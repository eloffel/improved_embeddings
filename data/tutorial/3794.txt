   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   21 march 2017 / [12]id21

id21 - machine learning's next frontier

   id21 - machine learning's next frontier

   this post gives an overview of id21 and motivate why it
   warrants our attention.

   table of contents:
     * [13]what is id21?
     * [14]why id21 now?
     * [15]a definition of id21
     * [16]id21 scenarios
     * [17]applications of id21
          + [18]learning from simulations
          + [19]adapting to new domains
          + [20]transferring knowledge across languages
     * [21]id21 methods
          + [22]using pre-trained id98 features
          + [23]learning domain-invariant representations
          + [24]making representations more similar
          + [25]confusing domains
     * [26]related research areas
          + [27]semi-supervised learning
          + [28]using available data more effectively
          + [29]improving models' ability to generalize
          + [30]making models more robust
          + [31]id72
          + [32]continuous learning
          + [33]zero-shot learning
     * [34]conclusion

   in recent years, we have become increasingly good at training deep
   neural networks to learn a very accurate mapping from inputs to
   outputs, whether they are images, sentences, label predictions, etc.
   from large amounts of labeled data.

   what our models still frightfully lack is the ability to generalize to
   conditions that are different from the ones encountered during
   training. when is this necessary? every time you apply your model not
   to a carefully constructed dataset but to the real world. the real
   world is messy and contains an infinite number of novel scenarios, many
   of which your model has not encountered during training and for which
   it is in turn ill-prepared to make predictions. the ability to transfer
   knowledge to new conditions is generally known as id21 and
   is what we will discuss in the rest of this post.

   over the course of this blog post, i will first contrast transfer
   learning with machine learning's most pervasive and successful
   paradigm, supervised learning. i will then outline reasons why transfer
   learning warrants our attention. subsequently, i will give a more
   technical definition and detail different id21 scenarios.
   i will then provide examples of applications of id21
   before delving into practical methods that can be used to transfer
   knowledge. finally, i will give an overview of related directions and
   provide an outlook into the future.

what is id21?

   in the classic supervised learning scenario of machine learning, if we
   intend to train a model for some task and domain \(a\), we assume that
   we are provided with labeled data for the same task and domain. we can
   see this clearly in figure 1, where the task and domain of the training
   and test data of our model \(a\) is the same. we will later define in
   more detail what exactly a task and a domain are). for the moment, let
   us assume that a task is the objective our model aims to perform, e.g.
   recognize objects in images, and a domain is where our data is coming
   from, e.g. images taken in san francisco coffee shops.
   traditional ml setup figure 1: the traditional supervised learning
   setup in ml

   we can now train a model \(a\) on this dataset and expect it to perform
   well on unseen data of the same task and domain. on another occasion,
   when given data for some other task or domain \(b\), we require again
   labeled data of the same task or domain that we can use to train a new
   model \(b\) so that we can expect it to perform well on this data.

   the traditional supervised learning paradigm breaks down when we do not
   have sufficient labeled data for the task or domain we care about to
   train a reliable model.
   if we want to train a model to detect pedestrians on night-time images,
   we could apply a model that has been trained on a similar domain, e.g.
   on day-time images. in practice, however, we often experience a
   deterioration or collapse in performance as the model has inherited the
   bias of its training data and does not know how to generalize to the
   new domain.
   if we want to train a model to perform a new task, such as detecting
   bicyclists, we cannot even reuse an existing model, as the labels
   between the tasks differ.

   id21 allows us to deal with these scenarios by leveraging
   the already existing labeled data of some related task or domain. we
   try to store this knowledge gained in solving the source task in the
   source domain and apply it to our problem of interest as can be seen in
   figure 2.
   id21 setup figure 2: the id21 setup

   in practice, we seek to transfer as much knowledge as we can from the
   source setting to our target task or domain. this knowledge can take on
   various forms depending on the data: it can pertain to how objects are
   composed to allow us to more easily identify novel objects; it can be
   with regard to the general words people use to express their opinions,
   etc.

why id21 now?

   andrew ng, chief scientist at baidu and professor at stanford, said
   during [35]his widely popular nips 2016 tutorial that id21
   will be -- after supervised learning -- the next driver of ml
   commercial success.
   andrew ng on id21 figure 3: andrew ng on id21
   at nips 2016

   in particular, he sketched out a chart on a whiteboard that i've sought
   to replicate as faithfully as possible in figure 4 below (sorry about
   the [36]unlabelled axes). according to andrew ng, id21
   will become a key driver of machine learning success in industry.
   drivers of ml success in industry figure 4: drivers of ml industrial
   success according to andrew ng

   it is indisputable that ml use and success in industry has so far been
   mostly driven by supervised learning. fuelled by advances in deep
   learning, more capable computing utilities, and large labeled datasets,
   supervised learning has been largely responsible for the wave of
   renewed interest in ai, funding rounds and acquisitions, and in
   particular the applications of machine learning that we have seen in
   recent years and that have become part of our daily lives. if we
   disregard naysayers and heralds of another ai winter and instead trust
   the prescience of andrew ng, this success will likely continue.

   it is less clear, however, why id21 which has been around
   for decades and is currently little utilized in industry, will see the
   explosive growth predicted by ng. even more so as id21
   currently receives relatively little visibility compared to other areas
   of machine learning such as unsupervised learning and reinforcement
   learning, which have come to enjoy increasing popularity: unsupervised
   learning -- the [37]key ingredient on the quest to general ai according
   to yann lecun as can be seen in figure 5 -- has seen a resurgence of
   interest, driven in particular by id3.
   id23, in turn, spear-headed by google deepmind has
   led to advances in game-playing ai exemplified by the success of
   [38]alphago and has already seen success in the real world, e.g. by
   [39]reducing google's data center cooling bill by 40%. both of these
   areas, while promising, will likely only have a comparatively small
   commercial impact in the foreseeable future and mostly remain within
   the confines of cutting-edge research papers as they still face
   [40]many challenges.
   yann lecun nips 2016 keynote cake slide figure 5: id21 is
   conspicuously absent as ingredient from yann lecun's cake

   what makes id21 different? in the following, we will look
   at the factors that -- in our opinion -- motivate ng's prognosis and
   outline the reasons why just now is the time to pay attention to
   id21.

   the current use of machine learning in industry is characterised by a
   dichotomy:
   on the one hand, over the course of the last years, we have obtained
   the ability to train more and more accurate models. we are now at the
   stage that for many tasks, state-of-the-art models have reached a level
   where their performance is so good that it is no longer a hindrance for
   users. how good? the newest residual networks ^[41][1] on id163
   achieve [42]superhuman performance at recognising objects; google's
   smart reply ^[43][2] automatically handles 10% of all mobile responses;
   id103 error has consistently dropped and is more accurate
   than typing ^[44][3]; we can automatically [45]identify skin cancer as
   well as dermatologists; google's id4 system ^[46][4] is used in
   production for more than 10 language pairs; baidu can generate
   realistic sounding speech [47]in real-time; the list goes on and on.
   this level of maturity has allowed the large-scale deployment of these
   models to millions of users and has enabled widespread adoption.

   on the other hand, these successful models are immensely data-hungry
   and rely on huge amounts of labeled data to achieve their performance.
   for some tasks and domains, this data is available as it has been
   painstakingly gathered over many years. in a few cases, it is public,
   e.g. id163 ^[48][5], but large amounts of labeled data are usually
   proprietary or expensive to obtain, as in the case of many speech or mt
   datasets, as they provide an edge over the competition.

   at the same time, when applying a machine learning model in the wild,
   it is faced with a myriad of conditions which the model has never seen
   before and does not know how to deal with; each client and every user
   has their own preferences, possesses or generates data that is
   different than the data used for training; a model is asked to perform
   many tasks that are related to but not the same as the task it was
   trained for. in all of these situations, our current state-of-the-art
   models, despite exhibiting human-level or even super-human performance
   on the task and domain they were trained on, suffer a significant loss
   in performance or even break down completely.

   id21 can help us deal with these novel scenarios and is
   necessary for production-scale use of machine learning that goes beyond
   tasks and domains were labeled data is plentiful. so far, we have
   applied our models to the tasks and domains that -- while impactful --
   are the low-hanging fruits in terms of data availability. to also serve
   the long tail of the distribution, we must learn to transfer the
   knowledge we have acquired to new tasks and domains.

   to be able to do this, we need to understand the concepts that transfer
   learning involves. for this reason, we will give a more technical
   definition in the following section.

a definition of id21

   for this definition, we will closely follow the excellent survey by pan
   and yang (2010) ^[49][6] with binary document classification as a
   running example.
   id21 involves the concepts of a domain and a task. a
   domain \(\mathcal{d}\) consists of a feature space \(\mathcal{x}\) and
   a marginal id203 distribution \(p(x)\) over the feature space,
   where \(x = {x_1, \cdots, x_n} \in \mathcal{x}\). for document
   classification with a bag-of-words representation, \(\mathcal{x}\) is
   the space of all id194s, \(x_i\) is the \(i\)-th term
   vector corresponding to some document and \(x\) is the sample of
   documents used for training.

   given a domain, \(\mathcal{d} = \{\mathcal{x},p(x)\}\), a task
   \(\mathcal{t}\) consists of a label space \(\mathcal{y}\) and a
   id155 distribution \(p(y|x)\) that is typically
   learned from the training data consisting of pairs \(x_i \in x\) and
   \(y_i \in \mathcal{y}\). in our document classification example,
   \(\mathcal{y}\) is the set of all labels, i.e. true, false and \(y_i\)
   is either true or false.

   given a source domain \(\mathcal{d}_s\), a corresponding source task
   \(\mathcal{t}_s\), as well as a target domain \(\mathcal{d}_t\) and a
   target task \(\mathcal{t}_t\), the objective of id21 now
   is to enable us to learn the target id155
   distribution \(p(y_t|x_t)\) in \(\mathcal{d}_t\) with the information
   gained from \(\mathcal{d}_s\) and \(\mathcal{t}_s\) where
   \(\mathcal{d}_s \neq \mathcal{d}_t\) or \(\mathcal{t}_s \neq
   \mathcal{t}_t\). in most cases, a limited number of labeled target
   examples, which is exponentially smaller than the number of labeled
   source examples are assumed to be available.

   as both the domain \(\mathcal{d}\) and the task \(\mathcal{t}\) are
   defined as tuples, these inequalities give rise to four transfer
   learning scenarios, which we will discus below.

id21 scenarios

   given source and target domains \(\mathcal{d}_s\) and \(\mathcal{d}_t\)
   where \(\mathcal{d} = \{\mathcal{x},p(x)\}\) and source and target
   tasks \(\mathcal{t}_s\) and \(\mathcal{t}_t\) where \(\mathcal{t} =
   \{\mathcal{y}, p(y|x)\}\) source and target conditions can vary in four
   ways, which we will illustrate in the following again using our
   document classification example:
    1. \(\mathcal{x}_s \neq \mathcal{x}_t\). the feature spaces of the
       source and target domain are different, e.g. the documents are
       written in two different languages. in the context of natural
       language processing, this is generally referred to as cross-lingual
       adaptation.
    2. \(p(x_s) \neq p(x_t)\). the marginal id203 distributions of
       source and target domain are different, e.g. the documents discuss
       different topics. this scenario is generally known as domain
       adaptation.
    3. \(\mathcal{y}_s \neq \mathcal{y}_t\). the label spaces between the
       two tasks are different, e.g. documents need to be assigned
       different labels in the target task. in practice, this scenario
       usually occurs with scenario 4, as it is extremely rare for two
       different tasks to have different label spaces, but exactly the
       same id155 distributions.
    4. \(p(y_s|x_s) \neq p(y_t|x_t)\). the id155
       distributions of the source and target tasks are different, e.g.
       source and target documents are unbalanced with regard to their
       classes. this scenario is quite common in practice and approaches
       such as over-sampling, under-sampling, or smote ^[50][7] are widely
       used.

   after we are now aware of the concepts relevant for id21
   and the scenarios in which it is applied, we will look to different
   applications of id21 that illustrate some of its
   potential.

applications of id21

learning from simulations

   one particular application of id21 that i'm very excited
   about and that i assume we'll see more of in the future is learning
   from simulations. for many machine learning applications that rely on
   hardware for interaction, gathering data and training a model in the
   real world is either expensive, time-consuming, or simply too
   dangerous. it is thus advisable to gather data in some other, less
   risky way.

   simulation is the preferred tool for this and is used towards enabling
   many advanced ml systems in the real world. learning from a simulation
   and applying the acquired knowledge to the real world is an instance of
   id21 scenario 2, as the feature spaces between source and
   target domain are the same (both generally rely on pixels), but the
   marginal id203 distributions between simulation and reality are
   different, i.e. objects in the simulation and the source look
   different, although this difference diminishes as simulations get more
   realistic. at the same time, the id155 distributions
   between simulation and real wold might be different as the simulation
   is not able to fully replicate all reactions in the real world, e.g. a
   physics engine can not completely mimic the complex interactions of
   real-world objects.
   google self-driving car figure 6: a google self-driving car (source:
   [51]google research blog)

   learning from simulations has the benefit of making data gathering easy
   as objects can be easily bounded and analyzed, while simultaneously
   enabling fast training, as learning can be parallelized across multiple
   instances. consequently, it is a prerequisite for large-scale machine
   learning projects that need to interact with the real world, such as
   self-driving cars (figure 6). according to zhaoyin jia, google's
   self-driving car tech lead, "simulation is essential if you really want
   to do a self-driving car". udacity has [52]open-sourced the simulator
   it uses for teaching its self-driving car engineer nanodegree, which
   can be seen in figure 7 and [53]openai's universe will potentially
   allows to train a self-driving car [54]using gta 5 or other video
   games.
   udacity self-driving car simulator figure 7: udacity's self-driving car
   simulator (source: [55]techcrunch)

   another area where learning from simulations is key is robotics:
   training models on a real robot is too slow and robots are expensive to
   train. learning from a simulation and transferring the knowledge to
   real-world robot alleviates this problem and has recently been
   garnering additional interest ^[56][8]. an example of a data
   manipulation task in the real world and in a simulation can be seen in
   figure 8.
   robot and simulation images figure 8: robot and simulation images (rusu
   et al., 2016)

   finally, another direction where simulation will be an integral part is
   on the path towards general ai. training an agent to achieve general
   artificial intelligence directly in the real world is too costly and
   hinders learning initially through unnecessary complexity. rather,
   learning may be more successful if it is based on a simulated
   environment such as [57]commai-env ^[58][9] that is visible in figure
   9.
   commai-env figure 9: facebook ai research's commai-env (mikolov et al.,
   2015)

adapting to new domains

   while learning from simulations is a particular instance of domain
   adaptation, it is worth outlining some other examples of domain
   adaptation.

   id20 is a common requirement in vision as often the data
   where labeled information is easily accessible and the data that we
   actually care about are different, whether this pertains to identifying
   bikes as in figure 10 or some other objects in the wild. even if the
   training and the the test data look the same, the training data may
   still contain a bias that is imperceptible to humans but which the
   model will exploit to overfit on the training data ^[59][10].
   visual domains figure 10: different visual domains (sun et al., 2016)

   another common id20 scenario pertains to adapting to
   different text types: standard nlp tools such as part-of-speech taggers
   or parsers are typically trained on news data such as the wall street
   journal, which has historically been used to evaluate these models.
   models trained on news data, however, have difficulty coping with more
   novel text forms such as social media messages and the challenges they
   present.
   different text types figure 11: different text types / genres

   even within one domain such as product reviews, people employ different
   words and phrases to express the same opinion. a model trained on one
   type of review should thus be able to disentangle the general and
   domain-specific opinion words that people use in order not to be
   confused by the shift in domain.
   [domain_adaptation_reviews.png] figure 12: different topics

   finally, while the above challenges deal with general text or image
   types, problems are amplified if we look at domains that pertain to
   individual or groups of users: consider the case of automatic speech
   recognition (asr). speech is poised to become [60]the next big
   platform, with 50% of all our searches predicted to be performed by
   voice by 2020. most asr systems are evaluated traditionally on the
   switchboard dataset, which comprises 500 speakers. most people with a
   standard accent are thus fortunate, while immigrants, people with
   non-standard accents, people with a speech impediment, or children have
   trouble being understood. now more than ever do we need systems that
   are able to adapt to individual users and minorities to ensure that
   everyone's voice is heard.
   different accents figure 13: different accents

transferring knowledge across languages

   finally, learning from one language and applying our knowledge to
   another language is -- in my opinion -- another killer application of
   id21, which i have written about before [61]here in the
   context of cross-lingual embedding models. reliable cross-lingual
   adaptation methods would allow us to leverage the vast amounts of
   labeled data we have in english and apply them to any language,
   particularly underserved and truly low-resource languages. given the
   [62]current state-of-the-art, this still seems utopian, but recent
   advances such as zero-shot translation ^[63][11] promise rapid progress
   in this area.

   while we have so far considered particular applications of transfer
   learning, we will now look at practical methods and directions in the
   literature that are used to solve some of the presented challenges.

id21 methods

   id21 has a long history of research and techniques exist
   to tackle each of the four id21 scenarios described above.
   the advent of deep learning has led to a range of new id21
   approaches, some of which we will review in the following. for a survey
   of earlier methods, refer to ^[64][6:1].

using pre-trained id98 features

   in order to motivate the most common way of id21 is
   currently applied, we must understand what accounts for the outstanding
   success of large convolutional neural networks on id163 ^[65][12].

understanding convolutional neural networks

   while many details of how these models work still remain a mystery, we
   are by now aware that lower convolutional layers capture low-level
   image features, e.g. edges (see figure 14), while higher convolutional
   layers capture more and more complex details, such as body parts,
   faces, and other compositional features.
   andrew ng on id21 figure 14: example filters learned by
   alexnet (krizhevsky et al., 2012).

   the final fully-connected layers are generally assumed to capture
   information that is relevant for solving the respective task, e.g.
   alexnet's fully-connected layers would indicate which features are
   relevant to classify an image into one of 1000 object categories.

   however, while knowing that a cat has whiskers, paws, fur, etc. is
   necessary for identifying an animal as a cat (for an example, see
   figure 15), it does not help us with identifying new objects or to
   solve other common vision tasks such as scene recognition, fine grained
   recognition, attribute detection and id162.
   token cat figure 15: this post's token cat

   what can help us, however, are representations that capture general
   information of how an image is composed and what combinations of edges
   and shapes it contains. this information is contained in one of the
   final convolutional layers or early fully-connected layers in large
   convolutional neural networks trained on id163 as we have described
   above.

   for a new task, we can thus simply use the off-the-shelf features of a
   state-of-the-art id98 pre-trained on id163 and train a new model on
   these extracted features. in practice, we either keep the pre-trained
   parameters fixed or tune them with a small learning rate in order to
   ensure that we do not unlearn the previously acquired knowledge. this
   simple approach has been shown to achieve astounding results on an
   array of vision tasks ^[66][13] as well as tasks that rely on visual
   input such as image captioning. a model trained on id163 seems to
   capture details about the way animals and objects are structured and
   composed that is generally relevant when dealing with images. as such,
   the id163 task seems to be a good proxy for general id161
   problems, as the same knowledge that is required to excel in it is also
   relevant for many other tasks.

learning the underlying structure of images

   a similar assumption is used to motivate generative models: when
   training generative models, we assume that the ability to generate
   realistic images requires an understanding of the underlying structure
   of images, which in turn can be applied to many other tasks. this
   assumption itself relies on the premise that all images lie on a
   low-dimensional manifold, i.e. that there is some underlying structure
   to images that can be extracted by a model. recent advances in
   generating photorealistic images with id3
   ^[67][14] indicate that such a structure might indeed exist, as
   evidenced by the model's ability to show realistic transitions between
   points in the bedroom image space in figure 16.
   walking the image manifold figure 16: walking along the bedroom image
   manifold

are pre-trained features useful beyond vision?

   off-the-shelf id98 features have seen unparalleled results in vision,
   but the question remains if this success can be replicated in other
   disciplines using other types of data, such as languages. currently,
   there are no off-the-shelf features that achieve results for natural
   language processing that are as astounding as their vision equivalent.
   why is that? do such features exist at all or -- if not -- why is
   vision more conducive to this form of transfer than language?

   the output of lower-level tasks such as part-of-speech tagging or
   chunking can be likened as off-the-shelf features, but these do not
   capture more fine-grained rules of language use beyond syntax and are
   not helpful for all tasks. as we have seen, the existence of
   generalizable off-the-shelf features seems to be intertwined with the
   existence of a task that can be seen as a prototype for many tasks in
   the field. in vision, object recognition occupies such a role. in
   language, the closest analogue might be language modelling: in order to
   predict the next word or sentence given a sequence of words, a model
   needs to possess knowledge of how language is structured, needs to
   understand what words likely are related to and likely follow each
   other, needs to model long-term dependencies, etc.

   while state-of-the-art language models increasingly approach human
   levels ^[68][15], their features are only of limited use. at the same
   time, advances in language modelling have led to positive results for
   other tasks: pre-training a model with a language model objective
   improves performance ^[69][16]. in addition, id27s
   pre-trained on a large unlabelled corpus with an approximated language
   modelling objective have become pervasive ^[70][17]. while they are not
   as effective as off-the-shelf features in vision, they still provide
   sizeable gains ^[71][18] and can be seen a simple form of transfer of
   general domain knowledge derived from a large unlabelled corpus.

   while a general proxy task seems currently out of reach in natural
   language processing, auxiliary tasks can take the form of local
   proxies. whether through multi-task objectives ^[72][19] or synthetic
   task objectives ^[73][20], ^[74][21], they can be used to inject
   additional relevant knowledge into the model.

   using pre-trained features is currently the most straightforward and
   most commonly used way to perform id21. however, it is by
   far not the only one.

learning domain-invariant representations

   pre-trained features are in practice mostly used for adaptation
   scenario 3 where we want to adapt to a new task. for the other
   scenarios, another way to transfer knowledge enabled by deep learning
   is to learn representations that do not change based on our domain.
   this approach is conceptually very similar to the way we have been
   thinking about using pre-trained id98 features: both encode general
   knowledge about our domain. however, creating representations that do
   not change based on the domain is a lot less expensive and more
   feasible for non-vision tasks than generating representations that are
   useful for all tasks. id163 has taken years and thousands of hours
   to create, while we typically only need unlabelled data of each domain
   for creating domain-invariant representations. these representations
   are generally learned using stacked denoising autoencoders and have
   seen success in natural language processing ^[75][22], ^[76][23] as
   well as in vision ^[77][24].

making representations more similar

   in order to improve the transferability of the learned representations
   from the source to the target domain, we would like the representations
   between the two domains to be as similar as possible so that the model
   does not take into account domain-specific characteristics that may
   hinder transfer but the commonalities between the domains.

   rather than just letting our autoencoder learn some representation, we
   can thus actively encourage the representations of both domains to be
   more similar to each other. we can apply this as a pre-processing step
   directly to the representations of our data ^[78][25], ^[79][26] and
   can then use the new representations for training. we can also
   encourage the representations of the domains in our model to be more
   similar to each other ^[80][27], ^[81][28].

confusing domains

   another way to ensure similarity between the representations of both
   domains that has recently become more popular is to add another
   objective to an existing model that encourages it to confuse the two
   domains ^[82][29], ^[83][30]. this domain confusion loss is a regular
   classification loss where the model tries to predict the domain of the
   input example. the difference to a regular loss, however, is that
   gradients that flow from the loss to the rest of the network are
   reversed, as can be seen in figure 17.
   confusing domains with gradient reversal layer figure 17: confusing
   domains with a gradient reversal layer (ganin and lempitsky, 2015)

   instead of learning to minimize the error of the domain classification
   loss, the gradient reversal layer causes the model to maximize the
   error. in practice, this means that the model learns representations
   that allow it to minimize its original objective, while not allowing it
   to differentiate between the two domains, which is beneficial for
   knowledge transfer. while a model trained only with the regular
   objective is shown in figure 18 to be clearly able to separate domains
   based on its learned representation, a model whose objective has been
   augmented with the domain confusion term is unable to do so.
   domain confusion figure 18: domain classifier score of a regular and a
   domain confusion model (tzeng et al, 2015)

related research areas

   while this post is about id21, id21 is by far
   not the only area of machine learning that seeks to leverage limited
   amounts of data, use learned knowledge for new endeavours, and enable
   models to generalize better to new settings. in the following, we will
   thus introduce other directions that are related or complementary to
   the goals of id21.

semi-supervised learning

   id21 seeks to leverage unlabelled data in the target task
   or domain to the most effect. this is also the maxim of semi-supervised
   learning, which follows the classical machine learning setup but
   assumes only a limited amount of labeled samples for training. insofar,
   semi-supervised id20 is essentially semi-supervised
   learning under domain shift. many lessons and insights from
   semi-supervised learning are thus equally applicable and relevant for
   id21. refer to ^[84][31] for a great survey on
   semi-supervised learning.

using available data more effectively

   another direction that is related to id21 and
   semi-supervised learning is to enable models to work better with
   limited amounts of data.

   this can be done in several ways: one can leverage unsupervised or
   semi-supervised learning to extract information from unlabelled data
   thereby reducing the reliance on labelled samples; one can give the
   model access to other features inherent in the data while reducing its
   tendency to overfit via id173; finally, one can leverage data
   that so far remains neglected or rests in non-obvious places.

   such fortuitous data ^[85][32] may be created as a side effect of
   user-generated content, such as hyperlinks that can be used to improve
   named entity and part-of-speech taggers; it may come as a by-product of
   annotation, e.g. annotator disagreement that may improve tagging or
   parsing; or it may be derived from user behaviour such as eye tracking
   or keystroke dynamics, which can inform nlp tasks. while such data has
   only been exploited in limited ways, such examples encourage us to look
   for data in unexpected places and to investigate new ways of retrieving
   data.

improving models' ability to generalize

   related to this is also the direction of making models generalize
   better. in order to achieve this, we must first better understand the
   behaviour and intricacies of large neural networks and investigate why
   and how they generalize. recent work has taken promising steps towards
   this end ^[86][33], but many questions are still left unanswered.

making models more robust

   while improving our models' generalization ability goes a long way, we
   might generalize well to similar instances but still fail
   catastrophically on unexpected or atypical inputs. therefore, a key
   complementary objective is to make our models more robust. this
   direction has seen increasing interest recently fuelled by advances in
   adversarial learning and recent approaches have investigated many ways
   of how models can be made more robust to worst-case or adversarial
   examples in different settings ^[87][34], ^[88][35].

id72

   in id21, we mainly care about doing well on our target
   task or domain. in id72, in contrast, the objective is
   to do well on all available tasks. alternatively, we can also use the
   knowledge acquired by learning from related tasks to do well on a
   target. crucially, in contrast to id21, some labeled data
   is usually assumed for each task. in addition, models are trained
   jointly on source and target task data, which is not the case for all
   id21 scenarios. however, even if target data is not
   available during training, insights about tasks that are beneficial for
   id72 ^[89][19:1] can still inform id21
   decisions.

   for a more thorough overview of id72, particularly as
   applied to deep neural networks, have a look at my other blog post
   [90]here.

continuous learning

   while id72 allows us to retain the knowledge across many
   tasks without suffering a performance penalty on our source tasks, this
   is only possible if all tasks are present at training time. for each
   new task, we would generally need to retrain our model on all tasks
   again.

   in the real world, however, we would like an agent to be able to deal
   with tasks that gradually become more complex by leveraging its past
   experience. to this end, we need to enable a model to learn
   continuously without forgetting. this area of machine learning is known
   as learning to learn ^[91][36], meta-learning, life-long learning, or
   continuous learning. it has seen some recent developments in the
   context of rl ^[92][37], ^[93][38], ^[94][39] most notably by
   [95]google deepmind on their quest towards general learning agents and
   is also being applied to sequence-to-sequence models ^[96][40].

zero-shot learning

   finally, if we take id21 to the extreme and aim to learn
   from only a few, one or even zero instances of a class, we arrive at
   few-shot, one-shot, and zero-shot learning respectively. enabling
   models to perform one-shot and zero-shot learning is admittedly among
   the hardest problems in machine learning. at the same time, it is
   something that comes naturally to us humans: toddlers only need to be
   told once what a dog is in order to be able to identify any other dog,
   while adults can understand the essence of an object just by reading
   about it in context, without ever having encountered it before.

   recent advances in id62 have leveraged the insight that
   models need to be trained explicitly to perform id62 in
   order to achieve good performance at test time ^[97][41], ^[98][42],
   while the more realistic generalized zero-shot learning setting where
   training classes are present at test time has garnered attention lately
   ^[99][43].

conclusion

   in summary, there are many exciting research directions that transfer
   learning offers and -- in particular -- many applications that are in
   need of models that can transfer knowledge to new tasks and adapt to
   new domains. i hope that i was able to provide you with an overview of
   id21 in this blog post and was able to pique your
   interest.

   some of the statements in this blog post are deliberately phrased
   slightly controversial. let me know your thoughts about any contentious
   issues and any errors that i undoubtedly made in writing this post in
   the comments below.

   note: title image is credited to ^[100][44].
     __________________________________________________________________

    1. szegedy, c., ioffe, s., vanhoucke, v., & alemi, a. (2016).
       inception-v4, inception-resnet and the impact of residual
       connections on learning. arxiv preprint arxiv:1602.07261. [101]      
    2. kannan, a., kurach, k., ravi, s., kaufmann, t., tomkins, a.,
       miklos, b.,     ramavajjala, v. (2016). smart reply: automated
       response suggestion for email. in kdd 2016.
       [102]http://doi.org/10.475/123 [103]      
    3. ruan, s., wobbrock, j. o., liou, k., ng, a., & landay, j. (2016).
       speech is 3x faster than typing for english and mandarin text entry
       on mobile devices. arxiv preprint arxiv:1608.07323. [104]      
    4. wu, y., schuster, m., chen, z., le, q. v, norouzi, m., macherey,
       w.,     dean, j. (2016). google   s id4 system:
       bridging the gap between human and machine translation. arxiv
       preprint arxiv:1609.08144. [105]      
    5. deng, j., dong, w., socher, r., li, l., li, k., & fei-fei, l.
       (2009). id163 : a large-scale hierarchical image database. in
       ieee conference on id161 and pattern recognition. [106]      
    6. pan, s. j., & yang, q. (2010). a survey on id21. ieee
       transactions on knowledge and data engineering, 22(10), 1345   1359.
       [107]       [108]      
    7. chawla, n. v, bowyer, k. w., hall, l. o., & kegelmeyer, w. p.
       (2002). smote : synthetic minority over-sampling technique. journal
       of artificial intelligence research, 16, 321   357. [109]      
    8. rusu, a. a., vecerik, m., roth  rl, t., heess, n., pascanu, r., &
       hadsell, r. (2016). sim-to-real robot learning from pixels with
       progressive nets. arxiv preprint arxiv:1610.04286. retrieved from
       [110]http://arxiv.org/abs/1610.04286 [111]      
    9. mikolov, t., joulin, a., & baroni, m. (2015). a roadmap towards
       machine intelligence. arxiv preprint arxiv:1511.08130. retrieved
       from [112]http://arxiv.org/abs/1511.08130 [113]      
   10. torralba, a., & efros, a. a. (2011). unbiased look at dataset bias.
       in 2011 ieee conference on id161 and pattern recognition
       (cvpr). [114]      
   11. johnson, m., schuster, m., le, q. v, krikun, m., wu, y., chen, z.,
           dean, j. (2016). google   s multilingual id4
       system: enabling zero-shot translation. arxiv preprint
       arxiv:1611.0455. [115]      
   12. krizhevsky, a., sutskever, i., & hinton, g. e. (2012). id163
       classification with deep convolutional neural networks. advances in
       neural information processing systems, 1   9. [116]      
   13. razavian, a. s., azizpour, h., sullivan, j., & carlsson, s. (2014).
       id98 features off-the-shelf: an astounding baseline for recognition.
       ieee computer society conference on id161 and pattern
       recognition workshops, 512   519. [117]      
   14. radford, a., metz, l., & chintala, s. (2016). unsupervised
       representation learning with deep convolutional generative
       adversarial networks. iclr. retrieved from
       [118]http://arxiv.org/abs/1511.06434 [119]      
   15. jozefowicz, r., vinyals, o., schuster, m., shazeer, n., & wu, y.
       (2016). exploring the limits of id38. arxiv preprint
       arxiv:1602.02410. retrieved from
       [120]http://arxiv.org/abs/1602.02410 [121]      
   16. ramachandran, p., liu, p. j., & le, q. v. (2016). unsupervised
       pretrainig for sequence to sequence learning. arxiv preprint
       arxiv:1611.02683. [122]      
   17. mikolov, t., chen, k., corrado, g., & dean, j. (2013). distributed
       representations of words and phrases and their compositionality.
       nips. [123]      
   18. kim, y. (2014). convolutional neural networks for sentence
       classification. proceedings of the conference on empirical methods
       in natural language processing, 1746   1751. retrieved from
       [124]http://arxiv.org/abs/1408.5882 [125]      
   19. bingel, j., & s  gaard, a. (2017). identifying beneficial task
       relations for id72 in deep neural networks. in eacl.
       retrieved from [126]http://arxiv.org/abs/1702.08303 [127]       [128]      
   20. plank, b., s  gaard, a., & goldberg, y. (2016). multilingual
       part-of-speech tagging with bidirectional long short-term memory
       models and auxiliary loss. in proceedings of the 54th annual
       meeting of the association for computational linguistics. [129]      
   21. yu, j., & jiang, j. (2016). learning sentence embeddings with
       auxiliary tasks for cross-domain sentiment classification.
       proceedings of the 2016 conference on empirical methods in natural
       language processing (emnlp2016), 236   246. retrieved from
       [130]http://www.aclweb.org/anthology/d/d16/d16-1023.pdf [131]      
   22. glorot, x., bordes, a., & bengio, y. (2011). id20 for
       large-scale sentiment classification: a deep learning approach.
       proceedings of the 28th international conference on machine
       learning, 513   520. retrieved from
       [132]http://www.icml-2011.org/papers/342_icmlpaper.pdf [133]      
   23. chen, m., xu, z., weinberger, k. q., & sha, f. (2012). marginalized
       denoising autoencoders for id20. proceedings of the
       29th international conference on machine learning (icml-12),
       767--774. [134]http://doi.org/10.1007/s11222-007-9033-z [135]      
   24. zhuang, f., cheng, x., luo, p., pan, s. j., & he, q. (2015).
       supervised representation learning: id21 with deep
       autoencoders. ijcai international joint conference on artificial
       intelligence, 4119   4125. [136]      
   25. daum   iii, h. (2007). frustratingly easy id20.
       association for computational linguistic (acl), (june), 256   263.
       [137]http://doi.org/10.1.1.110.2062 [138]      
   26. sun, b., feng, j., & saenko, k. (2016). return of frustratingly
       easy id20. in proceedings of the thirtieth aaai
       conference on artificial intelligence (aaai-16). retrieved from
       [139]http://arxiv.org/abs/1511.05547 [140]      
   27. bousmalis, k., trigeorgis, g., silberman, n., krishnan, d., &
       erhan, d. (2016). domain separation networks. nips. [141]      
   28. tzeng, e., hoffman, j., zhang, n., saenko, k., & darrell, t.
       (2014). deep domain confusion: maximizing for domain invariance.
       corr. retrieved from [142]https://arxiv.org/pdf/1412.3474.pdf
       [143]      
   29. ganin, y., & lempitsky, v. (2015). unsupervised id20
       by id26. in proceedings of the 32nd international
       conference on machine learning. (vol. 37). [144]      
   30. ganin, y., ustinova, e., ajakan, h., germain, p., larochelle, h.,
       laviolette, f.,     lempitsky, v. (2016). domain-adversarial training
       of neural networks. journal of machine learning research, 17, 1   35.
       [145]http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf
       [146]      
   31. zhu, x. (2005). semi-supervised learning literature survey. [147]      
   32. plank, b. (2016). what to do about non-standard (or non-canonical)
       language in nlp. konvens 2016. retrieved from
       [148]https://arxiv.org/pdf/1608.07836.pdf [149]      
   33. zhang, c., bengio, s., hardt, m., recht, b., & vinyals, o. (2017).
       understanding deep learning requires rethinking generalization.
       iclr 2017. [150]      
   34. kurakin, a., goodfellow, i., & bengio, s. (2017). adversarial
       examples in the physical world. in iclr 2017. retrieved from
       [151]http://arxiv.org/abs/1607.02533 [152]      
   35. huang, s., papernot, n., goodfellow, i., duan, y., & abbeel, p.
       (2017). adversarial attacks on neural network policies. in workshop
       track - iclr 2017. [153]      
   36. thrun, s., & pratt, l. (1998). learning to learn. springer science
       & business media. [154]      
   37. kirkpatrick, j., pascanu, r., rabinowitz, n., veness, j.,
       desjardins, g., rusu, a. a.,     hadsell, r. (2017). overcoming
       catastrophic forgetting in neural networks. pnas. [155]      
   38. rusu, a. a., rabinowitz, n. c., desjardins, g., soyer, h.,
       kirkpatrick, j., kavukcuoglu, k., ... deepmind, g. (2016).
       progressive neural networks. arxiv preprint arxiv:1606.04671.
       [156]      
   39. fernando, c., banarse, d., blundell, c., zwols, y., ha, d., rusu,
       a. a., ... wierstra, d. (2017). pathnet: evolution channels
       id119 in super neural networks. in arxiv preprint
       arxiv:1701.08734. [157]      
   40. kaiser,   ., nachum, o., roy, a., & bengio, s. (2017). learning to
       remember rare events. in iclr 2017. [158]      
   41. vinyals, o., blundell, c., lillicrap, t., kavukcuoglu, k., &
       wierstra, d. (2016). matching networks for one shot learning. nips
       2016. retrieved from [159]http://arxiv.org/abs/1606.04080 [160]      
   42. ravi, s., & larochelle, h. (2017). optimization as a model for
       few-shot learning. in iclr 2017. [161]      
   43. xian, y., schiele, b., akata, z., campus, s. i., & machine, a.
       (2017). zero-shot learning - the good, the bad and the ugly. in
       cvpr 2017. [162]      
   44. tzeng, e., hoffman, j., saenko, k., & darrell, t. (2017).
       adversarial discriminative id20. [163]      

   sebastian ruder

[164]sebastian ruder

   read [165]more posts by this author.
   [166]read more

       sebastian ruder    

[167]id21

     * [168]neural id21 for natural language processing (phd
       thesis)
     * [169]aaai 2019 highlights: dialogue, reproducibility, and more
     * [170]10 exciting ideas of 2018 in nlp

   [171]see all 12 posts    

   [172]an overview of id72 in deep neural networks

   id72

an overview of id72 in deep neural networks

   id72 is becoming more and more popular. this post gives
   a general overview of the current state of id72. in
   particular, it provides context for current neural network-based
   methods by discussing the extensive id72 literature.

     * sebastian ruder
       [173]sebastian ruder

   [174]highlights of nips 2016: adversarial learning, meta-learning, and
   more

   meta-learning

highlights of nips 2016: adversarial learning, meta-learning, and more

   the conference on neural information processing systems (nips) is one
   of the top ml conferences. this post discusses highlights of nips 2016
   including gans, the nuts and bolts of ml, id56s, improvements to classic
   algorithms, rl, meta-learning, and yann lecun's infamous cake.

     * sebastian ruder
       [175]sebastian ruder

   [176]sebastian ruder
      
   id21 - machine learning's next frontier
   share this
   please enable javascript to view the [177]comments powered by disqus.

   [178]sebastian ruder    2019

   [179]latest posts [180]twitter [181]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/transfer-learning/index.html
  13. http://ruder.io/transfer-learning/index.html#whatistransferlearning
  14. http://ruder.io/transfer-learning/index.html#whytransferlearningnow
  15. http://ruder.io/transfer-learning/index.html#adefinitionoftransferlearning
  16. http://ruder.io/transfer-learning/index.html#transferlearningscenarios
  17. http://ruder.io/transfer-learning/index.html#applicationsoftransferlearning
  18. http://ruder.io/transfer-learning/index.html#learningfromsimulations
  19. http://ruder.io/transfer-learning/index.html#adaptingtonewdomains
  20. http://ruder.io/transfer-learning/index.html#transferringknowledgeacrosslanguages
  21. http://ruder.io/transfer-learning/index.html#transferlearningmethods
  22. http://ruder.io/transfer-learning/index.html#usingpretrainedid98features
  23. http://ruder.io/transfer-learning/index.html#learningdomaininvariantrepresentations
  24. http://ruder.io/transfer-learning/index.html#makingrepresentationsmoresimilar
  25. http://ruder.io/transfer-learning/index.html#confusingdomains
  26. http://ruder.io/transfer-learning/index.html#relatedresearchareas
  27. http://ruder.io/transfer-learning/index.html#semisupervisedlearning
  28. http://ruder.io/transfer-learning/index.html#usingavailabledatamoreeffectively
  29. http://ruder.io/transfer-learning/index.html#improvingmodelsabilitytogeneralize
  30. http://ruder.io/transfer-learning/index.html#makingmodelsmorerobust
  31. http://ruder.io/transfer-learning/index.html#multitasklearning
  32. http://ruder.io/transfer-learning/index.html#continuouslearning
  33. http://ruder.io/transfer-learning/index.html#zeroshotlearning
  34. http://ruder.io/transfer-learning/index.html#conclusion
  35. http://ruder.io/highlights-nips-2016/index.html#thenutsandboltsofmachinelearning
  36. https://xkcd.com/833/
  37. http://ruder.io/highlights-nips-2016/index.html#generalartificialintelligence
  38. https://deepmind.com/research/alphago/
  39. https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/
  40. http://www.maluuba.com/blog/2017/3/14/the-next-challenges-for-reinforcement-learning
  41. http://ruder.io/transfer-learning/index.html#fn1
  42. http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-id163/
  43. http://ruder.io/transfer-learning/index.html#fn2
  44. http://ruder.io/transfer-learning/index.html#fn3
  45. http://news.stanford.edu/2017/01/25/artificial-intelligence-used-identify-skin-cancer/
  46. http://ruder.io/transfer-learning/index.html#fn4
  47. https://www.engadget.com/2017/03/09/baidu-deep-voice-natural-sounding-speec/
  48. http://ruder.io/transfer-learning/index.html#fn5
  49. http://ruder.io/transfer-learning/index.html#fn6
  50. http://ruder.io/transfer-learning/index.html#fn7
  51. https://googleblog.blogspot.ie/2014/04/the-latest-chapter-for-self-driving-car.html
  52. https://techcrunch.com/2017/02/08/udacity-open-sources-its-self-driving-car-simulator-for-anyone-to-use/
  53. https://universe.openai.com/
  54. https://techcrunch.com/2017/01/11/training-self-driving-cars-on-the-streets-of-los-santos-with-gta-v-just-got-easier/
  55. https://techcrunch.com/2017/02/08/udacity-open-sources-its-self-driving-car-simulator-for-anyone-to-use/
  56. http://ruder.io/transfer-learning/index.html#fn8
  57. https://github.com/facebookresearch/commai-env
  58. http://ruder.io/transfer-learning/index.html#fn9
  59. http://ruder.io/transfer-learning/index.html#fn10
  60. https://backchannel.com/voice-is-the-next-big-platform-unless-you-have-an-accent-6a787f7e8500#.koqx9pc2h
  61. http://ruder.io/cross-lingual-embeddings/index.html
  62. http://ruder.io/cross-lingual-embeddings/index.html#evaluation
  63. http://ruder.io/transfer-learning/index.html#fn11
  64. http://ruder.io/transfer-learning/index.html#fn6
  65. http://ruder.io/transfer-learning/index.html#fn12
  66. http://ruder.io/transfer-learning/index.html#fn13
  67. http://ruder.io/transfer-learning/index.html#fn14
  68. http://ruder.io/transfer-learning/index.html#fn15
  69. http://ruder.io/transfer-learning/index.html#fn16
  70. http://ruder.io/transfer-learning/index.html#fn17
  71. http://ruder.io/transfer-learning/index.html#fn18
  72. http://ruder.io/transfer-learning/index.html#fn19
  73. http://ruder.io/transfer-learning/index.html#fn20
  74. http://ruder.io/transfer-learning/index.html#fn21
  75. http://ruder.io/transfer-learning/index.html#fn22
  76. http://ruder.io/transfer-learning/index.html#fn23
  77. http://ruder.io/transfer-learning/index.html#fn24
  78. http://ruder.io/transfer-learning/index.html#fn25
  79. http://ruder.io/transfer-learning/index.html#fn26
  80. http://ruder.io/transfer-learning/index.html#fn27
  81. http://ruder.io/transfer-learning/index.html#fn28
  82. http://ruder.io/transfer-learning/index.html#fn29
  83. http://ruder.io/transfer-learning/index.html#fn30
  84. http://ruder.io/transfer-learning/index.html#fn31
  85. http://ruder.io/transfer-learning/index.html#fn32
  86. http://ruder.io/transfer-learning/index.html#fn33
  87. http://ruder.io/transfer-learning/index.html#fn34
  88. http://ruder.io/transfer-learning/index.html#fn35
  89. http://ruder.io/transfer-learning/index.html#fn19
  90. http://ruder.io/multi-task/index.html
  91. http://ruder.io/transfer-learning/index.html#fn36
  92. http://ruder.io/transfer-learning/index.html#fn37
  93. http://ruder.io/transfer-learning/index.html#fn38
  94. http://ruder.io/transfer-learning/index.html#fn39
  95. https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/
  96. http://ruder.io/transfer-learning/index.html#fn40
  97. http://ruder.io/transfer-learning/index.html#fn41
  98. http://ruder.io/transfer-learning/index.html#fn42
  99. http://ruder.io/transfer-learning/index.html#fn43
 100. http://ruder.io/transfer-learning/index.html#fn44
 101. http://ruder.io/transfer-learning/index.html#fnref1
 102. http://doi.org/10.475/123
 103. http://ruder.io/transfer-learning/index.html#fnref2
 104. http://ruder.io/transfer-learning/index.html#fnref3
 105. http://ruder.io/transfer-learning/index.html#fnref4
 106. http://ruder.io/transfer-learning/index.html#fnref5
 107. http://ruder.io/transfer-learning/index.html#fnref6
 108. http://ruder.io/transfer-learning/index.html#fnref6:1
 109. http://ruder.io/transfer-learning/index.html#fnref7
 110. http://arxiv.org/abs/1610.04286
 111. http://ruder.io/transfer-learning/index.html#fnref8
 112. http://arxiv.org/abs/1511.08130
 113. http://ruder.io/transfer-learning/index.html#fnref9
 114. http://ruder.io/transfer-learning/index.html#fnref10
 115. http://ruder.io/transfer-learning/index.html#fnref11
 116. http://ruder.io/transfer-learning/index.html#fnref12
 117. http://ruder.io/transfer-learning/index.html#fnref13
 118. http://arxiv.org/abs/1511.06434
 119. http://ruder.io/transfer-learning/index.html#fnref14
 120. http://arxiv.org/abs/1602.02410
 121. http://ruder.io/transfer-learning/index.html#fnref15
 122. http://ruder.io/transfer-learning/index.html#fnref16
 123. http://ruder.io/transfer-learning/index.html#fnref17
 124. http://arxiv.org/abs/1408.5882
 125. http://ruder.io/transfer-learning/index.html#fnref18
 126. http://arxiv.org/abs/1702.08303
 127. http://ruder.io/transfer-learning/index.html#fnref19
 128. http://ruder.io/transfer-learning/index.html#fnref19:1
 129. http://ruder.io/transfer-learning/index.html#fnref20
 130. http://www.aclweb.org/anthology/d/d16/d16-1023.pdf
 131. http://ruder.io/transfer-learning/index.html#fnref21
 132. http://www.icml-2011.org/papers/342_icmlpaper.pdf
 133. http://ruder.io/transfer-learning/index.html#fnref22
 134. http://doi.org/10.1007/s11222-007-9033-z
 135. http://ruder.io/transfer-learning/index.html#fnref23
 136. http://ruder.io/transfer-learning/index.html#fnref24
 137. http://doi.org/10.1.1.110.2062
 138. http://ruder.io/transfer-learning/index.html#fnref25
 139. http://arxiv.org/abs/1511.05547
 140. http://ruder.io/transfer-learning/index.html#fnref26
 141. http://ruder.io/transfer-learning/index.html#fnref27
 142. https://arxiv.org/pdf/1412.3474.pdf
 143. http://ruder.io/transfer-learning/index.html#fnref28
 144. http://ruder.io/transfer-learning/index.html#fnref29
 145. http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf
 146. http://ruder.io/transfer-learning/index.html#fnref30
 147. http://ruder.io/transfer-learning/index.html#fnref31
 148. https://arxiv.org/pdf/1608.07836.pdf
 149. http://ruder.io/transfer-learning/index.html#fnref32
 150. http://ruder.io/transfer-learning/index.html#fnref33
 151. http://arxiv.org/abs/1607.02533
 152. http://ruder.io/transfer-learning/index.html#fnref34
 153. http://ruder.io/transfer-learning/index.html#fnref35
 154. http://ruder.io/transfer-learning/index.html#fnref36
 155. http://ruder.io/transfer-learning/index.html#fnref37
 156. http://ruder.io/transfer-learning/index.html#fnref38
 157. http://ruder.io/transfer-learning/index.html#fnref39
 158. http://ruder.io/transfer-learning/index.html#fnref40
 159. http://arxiv.org/abs/1606.04080
 160. http://ruder.io/transfer-learning/index.html#fnref41
 161. http://ruder.io/transfer-learning/index.html#fnref42
 162. http://ruder.io/transfer-learning/index.html#fnref43
 163. http://ruder.io/transfer-learning/index.html#fnref44
 164. http://ruder.io/author/sebastian/index.html
 165. http://ruder.io/author/sebastian/index.html
 166. http://ruder.io/author/sebastian/index.html
 167. http://ruder.io/tag/transfer-learning/index.html
 168. http://ruder.io/thesis/index.html
 169. http://ruder.io/aaai-2019-highlights/index.html
 170. http://ruder.io/10-exciting-ideas-of-2018-in-nlp/index.html
 171. http://ruder.io/tag/transfer-learning/index.html
 172. http://ruder.io/index.html
 173. http://ruder.io/author/sebastian/index.html
 174. http://ruder.io/index.html
 175. http://ruder.io/author/sebastian/index.html
 176. http://ruder.io/
 177. https://disqus.com/?ref_noscript
 178. http://ruder.io/
 179. http://ruder.io/
 180. https://twitter.com/seb_ruder
 181. https://ghost.org/

   hidden links:
 183. https://twitter.com/seb_ruder
 184. http://ruder.io/rss/index.rss
 185. http://ruder.io/index.html
 186. http://ruder.io/index.html
 187. https://twitter.com/share?text=transfer%20learning%20-%20machine%20learning's%20next%20frontier&url=http://ruder.io/transfer-learning/
 188. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/transfer-learning/
