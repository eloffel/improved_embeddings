   #[1]github [2]recent commits to pix2pix:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]262
     * [35]star [36]5,983
     * [37]fork [38]975

[39]phillipi/[40]pix2pix

   [41]code [42]issues 45 [43]pull requests 2 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   image-to-image translation with conditional adversarial nets
   [47]https://phillipi.github.io/pix2pix/
   [48]computer-vision [49]computer-graphics [50]gan [51]pix2pix [52]dcgan
   [53]generative-adversarial-network [54]deep-learning
   [55]image-generation [56]image-manipulation
   [57]image-to-image-translation
     * [58]122 commits
     * [59]1 branch
     * [60]0 releases
     * [61]fetching contributors
     * [62]view license

    1. [63]lua 73.3%
    2. [64]python 17.5%
    3. [65]matlab 6.1%
    4. [66]tex 2.1%
    5. [67]shell 1.0%

   (button) lua python matlab tex shell
   branch: master (button) new pull request
   [68]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/p
   [69]download zip

downloading...

   want to be notified of new releases in phillipi/pix2pix?
   [70]sign in [71]sign up

launching github desktop...

   if nothing happens, [72]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [73]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [74]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [75]download the github extension for visual studio
   and try again.

   (button) go back
   [76]@tinghuiz
   [77]tinghuiz [78]update readme
   latest commit [79]ab759a4 mar 2, 2019
   [80]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [81]data [82]fix data/dataset.lua luajit dependency on mac mar 4, 2018
   [83]datasets [84]update dataset sep 2, 2018
   [85]imgs [86]smaller example image nov 22, 2016
   [87]models
   [88]scripts [89]merge pull request [90]#138 [91]from arthur-qiu/master
   apr 19, 2018
   [92]util [93]make model support cudnn7.0 mar 28, 2018
   [94].gitignore
   [95]license
   [96]readme.md
   [97]models.lua [98]update models.lua apr 28, 2018
   [99]test.lua
   [100]train.lua [101]delete useless code dec 6, 2017

readme.md

pix2pix

   [102]project | [103]arxiv | [104]pytorch

   torch implementation for learning a mapping from input images to output
   images, for example:

   [105][examples.jpg]

   image-to-image translation with conditional adversarial networks
   [106]phillip isola, [107]jun-yan zhu, [108]tinghui zhou, [109]alexei a.
   efros
   cvpr, 2017.

   on some tasks, decent results can be obtained fairly quickly and on
   small datasets. for example, to learn to generate facades (example
   shown above), we trained on just 400 images for about 2 hours (on a
   single pascal titan x gpu). however, for harder problems it may be
   important to train on far larger datasets, and for many hours or even
   days.

   note: please check out our [110]pytorch implementation for pix2pix and
   cyclegan. the pytorch version is under active development and can
   produce results comparable to or better than this torch version.

setup

prerequisites

     * linux or osx
     * nvidia gpu + cuda cudnn (cpu mode and cuda without cudnn may work
       with minimal modification, but untested)

getting started

     * install torch and dependencies from
       [111]https://github.com/torch/distro
     * install torch packages nngraph and display

luarocks install nngraph
luarocks install https://raw.githubusercontent.com/szym/display/master/display-s
cm-0.rockspec

     * clone this repo:

git clone git@github.com:phillipi/pix2pix.git
cd pix2pix

     * download the dataset (e.g., [112]cmp facades):

bash ./datasets/download_dataset.sh facades

     * train the model

data_root=./datasets/facades name=facades_generation which_direction=btoa th tra
in.lua

     * (cpu only) the same training command without using a gpu or cudnn.
       setting the environment variables gpu=0 cudnn=0 forces cpu only

data_root=./datasets/facades name=facades_generation which_direction=btoa gpu=0
cudnn=0 batchsize=10 save_epoch_freq=5 th train.lua

     * (optionally) start the display server to view results as the model
       trains. ( see [113]display ui for more details):

th -ldisplay.start 8000 0.0.0.0

     * finally, test the model:

data_root=./datasets/facades name=facades_generation which_direction=btoa phase=
val th test.lua

   the test results will be saved to an html file here:
   ./results/facades_generation/latest_net_g_val/index.html.

train

data_root=/path/to/data/ name=expt_name which_direction=atob th train.lua

   switch atob to btoa to train translation in opposite direction.

   models are saved to ./checkpoints/expt_name (can be changed by passing
   checkpoint_dir=your_dir in train.lua).

   see opt in train.lua for additional training options.

test

data_root=/path/to/data/ name=expt_name which_direction=atob phase=val th test.l
ua

   this will run the model named expt_name in direction atob on all images
   in /path/to/data/val.

   result images, and a webpage to view them, are saved to
   ./results/expt_name (can be changed by passing results_dir=your_dir in
   test.lua).

   see opt in test.lua for additional testing options.

datasets

   download the datasets using the following script. some of the datasets
   are collected by other researchers. please cite their papers if you use
   the data.
bash ./datasets/download_dataset.sh dataset_name

     * facades: 400 images from [114]cmp facades dataset. [[115]citation]
     * cityscapes: 2975 images from the [116]cityscapes training set.
       [[117]citation]
     * maps: 1096 training images scraped from google maps
     * edges2shoes: 50k training images from [118]ut zappos50k dataset.
       edges are computed by [119]hed edge detector + post-processing.
       [[120]citation]
     * edges2handbags: 137k amazon handbag images from [121]igan project.
       edges are computed by [122]hed edge detector + post-processing.
       [[123]citation]
     * night2day: around 20k natural scene images from [124]transient
       attributes dataset [[125]citation]. to train a day2night pix2pix
       model, you need to add which_direction=btoa.

models

   download the pre-trained models with the following script. you need to
   rename the model (e.g., facades_label2image to
   /checkpoints/facades/latest_net_g.t7) after the download has finished.
bash ./models/download_model.sh model_name

     * facades_label2image (label -> facade): trained on the cmp facades
       dataset.
     * cityscapes_label2image (label -> street scene): trained on the
       cityscapes dataset.
     * cityscapes_image2label (street scene -> label): trained on the
       cityscapes dataset.
     * edges2shoes (edge -> photo): trained on ut zappos50k dataset.
     * edges2handbags (edge -> photo): trained on amazon handbags images.
     * day2night (daytime scene -> nighttime scene): trained on around 100
       [126]webcams.

setup training and test data

generating pairs

   we provide a python script to generate training data in the form of
   pairs of images {a,b}, where a and b are two different depictions of
   the same underlying scene. for example, these might be pairs {label
   map, photo} or {bw image, color image}. then we can learn to translate
   a to b or b to a:

   create folder /path/to/data with subfolders a and b. a and b should
   each have their own subfolders train, val, test, etc. in
   /path/to/data/a/train, put training images in style a. in
   /path/to/data/b/train, put the corresponding images in style b. repeat
   same for other data splits (val, test, etc).

   corresponding images in a pair {a,b} must be the same size and have the
   same filename, e.g., /path/to/data/a/train/1.jpg is considered to
   correspond to /path/to/data/b/train/1.jpg.

   once the data is formatted this way, call:
python scripts/combine_a_and_b.py --fold_a /path/to/data/a --fold_b /path/to/dat
a/b --fold_ab /path/to/data

   this will combine each pair of images (a,b) into a single image file,
   ready for training.

notes on colorization

   no need to run combine_a_and_b.py for colorization. instead, you need
   to prepare some natural images and set preprocess=colorization in the
   script. the program will automatically convert each rgb image into lab
   color space, and create l -> ab image pair during the training. also
   set input_nc=1 and output_nc=2.

extracting edges

   we provide python and matlab scripts to extract coarse edges from
   photos. run scripts/edges/batch_hed.py to compute [127]hed edges. run
   scripts/edges/postprocesshed.m to simplify edges with additional
   post-processing steps. check the code documentation for more details.

evaluating labels2photos on cityscapes

   we provide scripts for running the evaluation of the labels2photos task
   on the cityscapes validation set. we assume that you have installed
   caffe (and pycaffe) in your system. if not, see the [128]official
   website for installation instructions. once caffe is successfully
   installed, download the pre-trained fcn-8s semantic segmentation model
   (512mb) by running
bash ./scripts/eval_cityscapes/download_fcn8s.sh

   then make sure ./scripts/eval_cityscapes/ is in your system's python
   path. if not, run the following command to add it
export pythonpath=${pythonpath}:./scripts/eval_cityscapes/

   now you can run the following command to evaluate your predictions:
python ./scripts/eval_cityscapes/evaluate.py --cityscapes_dir /path/to/original/
cityscapes/dataset/ --result_dir /path/to/your/predictions/ --output_dir /path/t
o/output/directory/

   images stored under --result_dir should contain your model predictions
   on the cityscapes validation split, and have the original cityscapes
   naming convention (e.g., frankfurt_000001_038418_leftimg8bit.png). the
   script will output a text file under --output_dir containing the
   metric.

   further notes: the pre-trained model is not supposed to work on
   cityscapes in the original resolution (1024x2048) as it was trained on
   256x256 images that are upsampled to 1024x2048. the purpose of the
   resizing was to 1) keep the label maps in the original high resolution
   untouched and 2) avoid the need of changing the standard fcn training
   code for cityscapes. to get the ground-truth numbers in the paper, you
   need to resize the original cityscapes images to 256x256 before running
   the evaluation code.

display ui

   optionally, for displaying images during training and test, use the
   [129]display package.
     * install it with: luarocks install
       https://raw.githubusercontent.com/szym/display/master/display-scm-0
       .rockspec
     * then start the server with: th -ldisplay.start
     * open this url in your browser: [130]http://localhost:8000

   by default, the server listens on localhost. pass 0.0.0.0 to allow
   external connections on any interface:
th -ldisplay.start 8000 0.0.0.0

   then open http://(hostname):(port)/ in your browser to load the remote
   desktop.

   l1 error is plotted to the display by default. set the environment
   variable display_plot to a comma-separated list of values errl1, errg
   and errd to visualize the l1, generator, and discriminator error
   respectively. for example, to plot only the generator and discriminator
   errors to the display instead of the default l1 error, set
   display_plot="errg,errd".

citation

   if you use this code for your research, please cite our paper
   [131]image-to-image translation using conditional adversarial networks:
@article{pix2pix2017,
  title={image-to-image translation with conditional adversarial networks},
  author={isola, phillip and zhu, jun-yan and zhou, tinghui and efros, alexei a}
,
  journal={cvpr},
  year={2017}
}

cat paper collection

   if you love cats, and love reading cool graphics, vision, and learning
   papers, please check out the cat paper collection:
   [132][github] [133][webpage]

acknowledgments

   code borrows heavily from [134]dcgan. the data loader is modified from
   [135]dcgan and [136]context-encoder.

     *    2019 github, inc.
     * [137]terms
     * [138]privacy
     * [139]security
     * [140]status
     * [141]help

     * [142]contact github
     * [143]pricing
     * [144]api
     * [145]training
     * [146]blog
     * [147]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [148]reload to refresh your
   session. you signed out in another tab or window. [149]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/phillipi/pix2pix/commits/master.atom
   3. https://github.com/phillipi/pix2pix#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/phillipi/pix2pix
  32. https://github.com/join
  33. https://github.com/login?return_to=/phillipi/pix2pix
  34. https://github.com/phillipi/pix2pix/watchers
  35. https://github.com/login?return_to=/phillipi/pix2pix
  36. https://github.com/phillipi/pix2pix/stargazers
  37. https://github.com/login?return_to=/phillipi/pix2pix
  38. https://github.com/phillipi/pix2pix/network/members
  39. https://github.com/phillipi
  40. https://github.com/phillipi/pix2pix
  41. https://github.com/phillipi/pix2pix
  42. https://github.com/phillipi/pix2pix/issues
  43. https://github.com/phillipi/pix2pix/pulls
  44. https://github.com/phillipi/pix2pix/projects
  45. https://github.com/phillipi/pix2pix/pulse
  46. https://github.com/join?source=prompt-code
  47. https://phillipi.github.io/pix2pix/
  48. https://github.com/topics/computer-vision
  49. https://github.com/topics/computer-graphics
  50. https://github.com/topics/gan
  51. https://github.com/topics/pix2pix
  52. https://github.com/topics/dcgan
  53. https://github.com/topics/generative-adversarial-network
  54. https://github.com/topics/deep-learning
  55. https://github.com/topics/image-generation
  56. https://github.com/topics/image-manipulation
  57. https://github.com/topics/image-to-image-translation
  58. https://github.com/phillipi/pix2pix/commits/master
  59. https://github.com/phillipi/pix2pix/branches
  60. https://github.com/phillipi/pix2pix/releases
  61. https://github.com/phillipi/pix2pix/graphs/contributors
  62. https://github.com/phillipi/pix2pix/blob/master/license
  63. https://github.com/phillipi/pix2pix/search?l=lua
  64. https://github.com/phillipi/pix2pix/search?l=python
  65. https://github.com/phillipi/pix2pix/search?l=matlab
  66. https://github.com/phillipi/pix2pix/search?l=tex
  67. https://github.com/phillipi/pix2pix/search?l=shell
  68. https://github.com/phillipi/pix2pix/find/master
  69. https://github.com/phillipi/pix2pix/archive/master.zip
  70. https://github.com/login?return_to=https://github.com/phillipi/pix2pix
  71. https://github.com/join?return_to=/phillipi/pix2pix
  72. https://desktop.github.com/
  73. https://desktop.github.com/
  74. https://developer.apple.com/xcode/
  75. https://visualstudio.github.com/
  76. https://github.com/tinghuiz
  77. https://github.com/phillipi/pix2pix/commits?author=tinghuiz
  78. https://github.com/phillipi/pix2pix/commit/ab759a46e306abdb7db79d31380afc8488661592
  79. https://github.com/phillipi/pix2pix/commit/ab759a46e306abdb7db79d31380afc8488661592
  80. https://github.com/phillipi/pix2pix/tree/ab759a46e306abdb7db79d31380afc8488661592
  81. https://github.com/phillipi/pix2pix/tree/master/data
  82. https://github.com/phillipi/pix2pix/commit/68ce7a4d43569c94f7553c388b2d86321f46fa94
  83. https://github.com/phillipi/pix2pix/tree/master/datasets
  84. https://github.com/phillipi/pix2pix/commit/35cc2482c607fb57840295910184a3b08101cc92
  85. https://github.com/phillipi/pix2pix/tree/master/imgs
  86. https://github.com/phillipi/pix2pix/commit/9fbcfb8d0c80583411f49f08c5046d2129062724
  87. https://github.com/phillipi/pix2pix/tree/master/models
  88. https://github.com/phillipi/pix2pix/tree/master/scripts
  89. https://github.com/phillipi/pix2pix/commit/fc2355db455782c7ec689dc20f445b74706e06e7
  90. https://github.com/phillipi/pix2pix/pull/138
  91. https://github.com/phillipi/pix2pix/commit/fc2355db455782c7ec689dc20f445b74706e06e7
  92. https://github.com/phillipi/pix2pix/tree/master/util
  93. https://github.com/phillipi/pix2pix/commit/fea1b7fbea575037c0a261f76ef1cf58d18aaff9
  94. https://github.com/phillipi/pix2pix/blob/master/.gitignore
  95. https://github.com/phillipi/pix2pix/blob/master/license
  96. https://github.com/phillipi/pix2pix/blob/master/readme.md
  97. https://github.com/phillipi/pix2pix/blob/master/models.lua
  98. https://github.com/phillipi/pix2pix/commit/dbcf70a8553f70ae0faa581b731f432630ac642b
  99. https://github.com/phillipi/pix2pix/blob/master/test.lua
 100. https://github.com/phillipi/pix2pix/blob/master/train.lua
 101. https://github.com/phillipi/pix2pix/commit/c428b1ce89c52c81d96e2cdd2e655ba3deb9ea86
 102. https://phillipi.github.io/pix2pix/
 103. https://arxiv.org/abs/1611.07004
 104. https://github.com/junyanz/pytorch-cyclegan-and-pix2pix
 105. https://github.com/phillipi/pix2pix/blob/master/imgs/examples.jpg
 106. http://web.mit.edu/phillipi/
 107. https://people.eecs.berkeley.edu/~junyanz/
 108. https://people.eecs.berkeley.edu/~tinghuiz/
 109. https://people.eecs.berkeley.edu/~efros/
 110. https://github.com/junyanz/pytorch-cyclegan-and-pix2pix
 111. https://github.com/torch/distro
 112. http://cmp.felk.cvut.cz/~tylecr1/facade/
 113. https://github.com/phillipi/pix2pix#display-ui
 114. http://cmp.felk.cvut.cz/~tylecr1/facade/
 115. https://github.com/phillipi/pix2pix/blob/master/datasets/bibtex/facades.tex
 116. https://www.cityscapes-dataset.com/
 117. https://github.com/phillipi/pix2pix/blob/master/datasets/bibtex/cityscapes.tex
 118. http://vision.cs.utexas.edu/projects/finegrained/utzap50k/
 119. https://github.com/s9xie/hed
 120. https://github.com/phillipi/pix2pix/blob/master/datasets/bibtex/shoes.tex
 121. https://github.com/junyanz/igan
 122. https://github.com/s9xie/hed
 123. https://github.com/phillipi/pix2pix/blob/master/datasets/bibtex/handbags.tex
 124. http://transattr.cs.brown.edu/
 125. https://github.com/phillipi/pix2pix/blob/master/datasets/bibtex/transattr.tex
 126. http://transattr.cs.brown.edu/
 127. https://github.com/s9xie/hed
 128. http://caffe.berkeleyvision.org/installation.html
 129. https://github.com/szym/display
 130. http://localhost:8000/
 131. https://arxiv.org/pdf/1611.07004v1.pdf
 132. https://github.com/junyanz/catpapers
 133. http://people.eecs.berkeley.edu/~junyanz/cat/cat_papers.html
 134. https://github.com/soumith/dcgan.torch
 135. https://github.com/soumith/dcgan.torch
 136. https://github.com/pathak22/context-encoder
 137. https://github.com/site/terms
 138. https://github.com/site/privacy
 139. https://github.com/security
 140. https://githubstatus.com/
 141. https://help.github.com/
 142. https://github.com/contact
 143. https://github.com/pricing
 144. https://developer.github.com/
 145. https://training.github.com/
 146. https://github.blog/
 147. https://github.com/about
 148. https://github.com/phillipi/pix2pix
 149. https://github.com/phillipi/pix2pix

   hidden links:
 151. https://github.com/
 152. https://github.com/phillipi/pix2pix
 153. https://github.com/phillipi/pix2pix
 154. https://github.com/phillipi/pix2pix
 155. https://help.github.com/articles/which-remote-url-should-i-use
 156. https://github.com/phillipi/pix2pix#pix2pix
 157. https://github.com/phillipi/pix2pix#setup
 158. https://github.com/phillipi/pix2pix#prerequisites
 159. https://github.com/phillipi/pix2pix#getting-started
 160. https://github.com/phillipi/pix2pix#train
 161. https://github.com/phillipi/pix2pix#test
 162. https://github.com/phillipi/pix2pix#datasets
 163. https://github.com/phillipi/pix2pix#models
 164. https://github.com/phillipi/pix2pix#setup-training-and-test-data
 165. https://github.com/phillipi/pix2pix#generating-pairs
 166. https://github.com/phillipi/pix2pix#notes-on-colorization
 167. https://github.com/phillipi/pix2pix#extracting-edges
 168. https://github.com/phillipi/pix2pix#evaluating-labels2photos-on-cityscapes
 169. https://github.com/phillipi/pix2pix#display-ui
 170. https://github.com/phillipi/pix2pix#citation
 171. https://github.com/phillipi/pix2pix#cat-paper-collection
 172. https://github.com/phillipi/pix2pix#acknowledgments
 173. https://github.com/
