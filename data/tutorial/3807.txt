   #[1]publisher [2]dataaspirant    feed [3]dataaspirant    comments feed
   [4]dataaspirant    how the id28 model works comments feed
   [5]alternate [6]alternate

[7]dataaspirant

   a data science portal for beginners
     * [8]about
     * [9]beginners guide
     * [10]data science courses
     * [11]data scientists interviews
     * [12]join us
     * [13]monthly newsletter
     * [14]partners

how the id28 model works

   march 2, 2017 [15]saimadhu polamuri
   [16]7 comments
   [17]machine learning
   id28 model id28 model

   id28 model

how the id28 model works in machine learning

   in this article, we are going to learn how the id28
   model works in machine learning. the id28 model is one
   member of the [18]supervised classification algorithm family. the
   building block concepts of id28 can be helpful in deep
   learning while building the neural networks.

   id28 classifier is more like a [19]linear classifier
   which uses the calculated logits (score ) to predict the target class.
   if you are not familiar with the concepts of the logits, don   t
   frighten. we are going to learn each and every block of logistic
   regression by the end of this post.

   before we begin, let   s check out the table of contents.

table of contents

     * what is id28?
     * dependent and independent variables?
     * id28 examples.
     * how the id28 classifier works.
          + binary classification with id28 model.
     * what is softmax function?
     * the special cases of softmax function.
     * implementing the softmax function in python.

   so let   s begin.
   [20]how the id28 model works in machine learning.
   [21]click to tweet

what is id28?

   below is the most accurate and well-defined definition of logistic
   regression from wikipedia.

        id28 measures the relationship between the
     categorical dependent variable and one or more independent variables
     by estimating probabilities using a [22]logistic
     function    ([23]wikipedia)

   let   s understand the above id28 model definition word by
   word. what id28 model will do is, it uses a black box
   function to understand the relation between the categorical dependent
   variable and the independent variables. this black box function is
   popularly known as the softmax funciton.

dependent and independent variables

   the dependent and the independent variables are the same which we were
   discussed in the [24]building simple id75 model. just to
   give you a glance. the dependent variable is the target class variable
   we are going to predict. however, the independent variables are the
   features or attributes we are going to use to predict the target class.

   suppose the shop owner would like to predict the customer who entered
   into the shop will buy the macbook or not. to predict whether the
   customer will buy the macbook or not. the shop owner will observe the
   customer features like.
     * gender:
          + probabilities wise male will high chances of purchasing a
            macbook than females.
     * age:
          + kids won   t purchase macbook.

   the shop owner will use the above, similar kind of features to predict
   the likelihood occurrence of the event  (will buy the macbook or not.)

   in the mathematical side, the id28 model will pass the
   likelihood occurrences through the logistic function to predict the
   corresponding target class. this popular logistic function is the
   softmax function. we are going to learn about the softmax function in
   the coming sections of this post.

   before that. let   s quickly see few examples to understand the sentence
   likelihood occurrence of an event.

examples of likelihood occurrence of an event

     * how likely a customer will buy ipod having iphone in his/her
       pocket.
     * how likely argentina team will win when [25]lionel andr  s messi in
       rest.
     * what is the id203 to get into best university by scoring
       decent marks in mathematics, physics?
     * what is the id203 to get a kiss from your girlfriend when you
       gifted her favorite dress on behalf of your birthday?

   hope the above examples gives you the better idea about the sentence
   predict the likelihood occurrence of an event.

   before drive into the underline mathematical concept of logistic
   regression. let   s brush up the id28 understanding level
   with an example.

id28 model example

   id28 example id28 example

   id28 example

   suppose hpenguin wants to know, how likely it will be happy based on
   its daily activities. if the penguin wants to build a logistic
   regression model to predict it happiness based on its daily activities.
   the penguin needs both the happy and sad activities. in [26]machine
   learning terminology these activities are known as the input parameters
   ( features ).

   so let   s create a table which contains penguin activities and the
   result of that activity like happy or sad.


   no. penguin activity penguin activity description how penguin felt (
   target )
   1 x1 eating squids happy
   2 x2 eating small fishes happy
   3 x3 hit by other penguin sad
   4 x4 eating crabs sad


   penguin is going to use the above activities ( features ) to train the
   id28 model. later the trained id28 model
   will predict how the penguin is feeling for the new penguin activities.

   as it   s not possible to use the above categorical data table to build
   the id28.  the above activities data table needs to
   convert into activities score, weights, and the corresponding target.

penguin activity data table

   no. penguin activity activity score weights target target description
   1   x1               6              0.6     1      happy
   2   x2               3              0.4     1      happy
   3   x3               7              -0.7    0      sad
   4   x4               3              -0.3    0      sad

   the updated dataset looks like this. before we drive further let   s
   understand more about the above data table.
     * penguin activity:
          + the activities penguin do daily like eating small fishes,
            eating crabs .. etc
     * activity score:
          + the activity score is more like the numerical equivalent to
            the penguin activity. for eating squids activity, the
            corresponding activity score is 6 and likewise, for other
            activities the scores are 3, 7, 3.
     * weights:
          + the weights more like the weightages corresponding to the
            particular target.
          + suppose for the activity x1 we have the weight as 0.6.
          + it means to say if the penguin performs the activity x1 the
            model is 60% confident to say the penguin will be happy.
          + if you observe the weights for the target class happy are
            positive, and the weights for the target class sad are
            negative.
          + this is because the problem we are addressing a binary
            classification. will talk about the binary classification in
            the next sections of this post.
     * target:
          + the target is just the binary values. the value 1 represents
            the target happy, and the value 0 represents the target sad.


   now we know the activity score for each activity and the corresponding
   weights. to predict how the penguin will feel given the activity we
   just need to multiply the activity score and the corresponding weight
   to get the score. the calculated score is also known as the logits
   which we talked earlier in the post.

   the logit (score) will pass into the softmax function to get the
   id203 for each target class. in our case, if we pass the logit
   through the softmax function will get the id203 for the target
   happy class and for the target sad class. later we can consider the
   target class with high id203 as the predicted target class for
   the given activity.

   in fact, we can predict whether the penguin is feeling happy or sad
   with the calculated logits (score ) in this case. as we were given the
   positive weights for the target class happy and the negative weights
   for the target class sad. we can say if the logit is greater than 0 the
   target class is happy and if the logit is less than  0 the target class
   is sad.
   id28 model for binary classification id28
   model for binary classification

   id28 model for binary classification

binary classification with id28 model

   in the penguin example, we pre-assigned the activity scores and the
   weights for the id28 model. this won   t be the simple
   while modeling the id28 model for real word problems.
   the weights will be calculated over the training data set. using the
   calculated the weights the logits will be computed. till here the model
   is similar to the [27]id75 model.

   note: the logits in the image were just for example, and not the
   calculated logits from the penguin example

   the calculated logits (score) for the id75 model will pass
   through the softmax function. the softmax function will return the
   probabilities for each target class. the high id203 target class
   will be the predicted target class.

   the target classes  in the penguin example, are having two target
   classes (happy and sad). if we are using the id28 model
   for predicting the binary targets like yes or no, 1 or 0. which is
   known as the binary classification.

   till now we talk about the softmax function as a black box which takes
   the calculated scores and returns the probabilities. now let   s learn
   how the softmax function calculates the probabilities. next, we are
   going to implement the simple softmax function to calculate the
   probabilities for given logits (scores)

what is softmax function?

   softmax function softmax function

   softmax function

   softmax function is the popular function to calculate the probabilities
   of the events. the other mathematical advantages of using the softmax
   function are the output range.  softmax function output values are
   always in the range of (0, 1). the sum of the output values will always
   equal to the 1. the softmax is also known as the normalized exponential
   function.

   the above is the softmax formula. which takes each value (logits) and
   find the id203. the numerator the e-power values of the logit and
   the denominator calculates the sum of the e-power values of all the
   logits.

softmax function used in:

     * [28]naive bayes classifier
     * multinomial logistic classifier
     * deep learning (while building neural networks)

   before we implementing the softmax function, let   s study the special
   cases of the softmax function inputs.

the special cases of softmax function input

   the two special cases we need to consider about the softmax function
   output, if we do the below modifications to the softmax function
   inputs.
     * multiplying the softmax function inputs (multiplying the logits
       with any value)
     * dividing the softmax function inputs (dividing the logits with any
       value)

multiplying the softmax function inputs:

   if we multiply the softmax function inputs, the inputs values will
   become large. so the id28 will be more confident (high
   id203 value) about the predicted target class.

dividing the softmax function inputs:

   if we divide the softmax function inputs, the inputs values will become
   small. so the id28 model will be not confident (less
   id203 value) of the predicted target class.

   enough of the theoretical concept of the softmax function. let   s do the
   fun part (coding).

implementing the softmax function in python

   let   s implement a softmax function which takes the logits in list or
   array and returns the softmax function outputs in a list.
   softmax function
   python

   # required python packages__________________________________
   import numpy as np__________________________________________
   ____________________________________________________________
   ____________________________________________________________
   def softmax(scores):________________________________________
       """_____________________________________________________
       calculate the softmax for the given scores______________
       :param scores:__________________________________________
       :return:________________________________________________
       """_____________________________________________________
       return np.exp(scores) / np.sum(np.exp(scores), axis=0)__
   ____________________________________________________________
   scores = [8, 5, 2]__________________________________________
   ____________________________________________________________
   if __name__ == "__main__":__________________________________
       logits = [8, 5, 2]______________________________________
       print "softmax :: ", softmax(logits)____________________
   1
   2
   3
   4
   5
   6
   7
   8
   9
   10
   11
   12
   13
   14
   15
   16
   17
   # required python packages
   import numpy as np


   def softmax(scores):
       """
       calculate the softmax for the given scores
       :param scores:
       :return:
       """
       return np.exp(scores) / np.sum(np.exp(scores), axis=0)

   scores = [8, 5, 2]

   if __name__ == "__main__":
       logits = [8, 5, 2]
       print "softmax :: ", softmax(logits)

   to implement the softmax function we just replicated the softmax
   formula.
     * the input to the softmax function is the logits in a list or array.
     * the numerator computes the exponential value of each logit in the
       array.
     * the denominator calculates the sum of all exponential values.
     * finally, we return the ratio of the numerator and the denominator
       values.

script output:

   softmax function output
   python

   softmax ::  [ 0.95033021  0.04731416  0.00235563]___________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   1
   softmax ::  [ 0.95033021  0.04731416  0.00235563]

summary

   in this post, we learned about the id28 model with a toy
   kind of example. further, we learned about the softmax function.
   finally, we implemented the simple softmax function with takes the
   logits as input and returns the probabilities as the outputs.

follow us:

[29]facebook| [30]quora |[31]twitter| [32]google+ | [33]linkedin| [34]reddit
| [35]flipboard | [36]medium | [37]github

   i hope you like this post. if you have any questions, then feel free to
   comment below.  if you want me to write on one particular topic, then
   do tell it to me in the comments below.

related data science courses

     * [38]building machine learning models with python
     * [39]python machine learning hands on experience
     * [40]complete machine learning courses

share this:

     * [41]click to share on twitter (opens in new window)
     * [42]click to share on facebook (opens in new window)
     * [43]click to share on google+ (opens in new window)
     * [44]click to share on reddit (opens in new window)
     * [45]click to share on pinterest (opens in new window)
     * [46]click to share on whatsapp (opens in new window)
     * [47]click to share on linkedin (opens in new window)
     * [48]click to email this to a friend (opens in new window)
     *

related

     * [49]classification algorithms
     * [50]id28

   [51]previous post
   gaussian naive bayes classifier implementation in python
   [52]machine learning
   [53]softmax functin vs sigmoid function softmax functin vs sigmoid
   function next post
   difference between softmax function and sigmoid function
   [54]machine learning

     manjunath says:
   [55]march 4, 2017 at 8:18 am

   if we multiply weights with activity score, it will be 6*.6 = 3.6,
   3*0.4 = 1.2 and so on and so forth. however, the logits are assigned
   0.5,0.8,    . . what am i missing?
   [56]reply
     * [57]saimadhu polamuri says:
       [58]march 4, 2017 at 8:51 am
       hi manjunath,
       you said true. if we multiply the weights with activities the score
       should be 6* 0.6 = 3.6 likewise, but the example image is for
       explaining the binary classification with id28 which
       is different from the penguin example.
       thanks for asking. will update the post with the clarification
       about the image.
       [59]reply
     * [60]saimadhu polamuri says:
       [61]march 4, 2017 at 8:55 am
       hi manjunath,
       updated the post: clarification about the binary classification
       with id28 image.
       [62]reply

     [63]difference between softmax function and sigmoid function says:
   [64]march 7, 2017 at 7:30 am

   [   ] the probabilities. as the calculated probabilities are used to
   predict the target class in id28 model. the two
   principal functions we frequently hear are softmax and sigmoid [   ]
   [65]reply

     [66]how multinomial id28 model works in machine
   learning says:
   [67]march 14, 2017 at 1:14 pm

   [   ] we discussed each and every block of binary id28
   classifier in our previous article. now we use the binary logistic
   regression knowledge to understand in [   ]
   [68]reply

     [69]four most popular coursera data science specializations says:
   [70]march 18, 2017 at 5:54 am

   [   ] the binary and multinomial classification techniques. you will be
   introduced to the concepts like id28, support vector
   machine [   ]
   [71]reply

     [72]how to implement id28 model in python for binary
   classification says:
   [73]april 15, 2017 at 4:36 pm

   [   ] how the id28 works. [   ]
   [74]reply

   ____________________ search

awarded top 75 data science blog

   dataaspirant awarded top 75 data science blog

   dataaspirant awarded top 75 data science blog

follow author

     * [75]view saimadhu.seven   s profile on facebook
     * [76]view saimadhup   s profile on twitter
     * [77]view saimadhu-polamuri   s profile on github

build your career in ai with andrew ng deep learning courses

   [78]andrew ng deep learning courses

   andrew ng deep learning courses

most popular posts

     * [79]how decision tree algorithm works
       [80]how decision tree algorithm works
     * [81]building decision tree algorithm in python with scikit learn
       [82]building decision tree algorithm in python with scikit learn
     * [83]how the id79 algorithm works in machine learning
       [84]how the id79 algorithm works in machine learning
     * [85]five most popular similarity measures implementation in python
       [86]five most popular similarity measures implementation in python
     * [87]difference between softmax function and sigmoid function
       [88]difference between softmax function and sigmoid function
     * [89]support vector machine classifier implementation in r with
       caret package
       [90]support vector machine classifier implementation in r with
       caret package
     * [91]how the naive bayes classifier works in machine learning
       [92]how the naive bayes classifier works in machine learning
     * [93]building id79 classifier with python scikit learn
       [94]building id79 classifier with python scikit learn
     * [95]visualize decision tree in python with graphviz
       [96]visualize decision tree in python with graphviz
     * [97]support vector machine (id166 classifier) implemenation in python
       with scikit-learn
       [98]support vector machine (id166 classifier) implemenation in python
       with scikit-learn

aegis graham bell awards 2018

   [99]coursera ds design 10

recent posts

     * [100]twitter id31 using r
     * [101]how id24 can be used in id23
     * [102]how to perform id23 with r
     * [103]feature selection techniques with r
     * [104]how to perform hierarchical id91 in r
     * [105]10 smart r programming tips to become better r programmer
     * [106]how to create histograms in r
     * [107]how to perform the principal component analysis in r
     * [108]building id79 classifier with python scikit learn
     * [109]how the id79 algorithm works in machine learning

   [110][edx.png]

follow on twitter

   [111]my tweets

categories

     * [112]amazon
     * [113]big data
     * [114]courses
     * [115]data science
     * [116]data science events
     * [117]datamining
     * [118]deep learning
     * [119]interviews
     * [120]machine learning
     * [121]newsletter
     * [122]python
     * [123]r
     * [124]recommendation engine

never miss a bit

   hey dude subscribe to dataaspirant. to get post updates in your inbox.

   email address ____________________

   subscribe

     *

quick links
          + [125]home
          + [126]about
          + [127]join us
          + [128]beginners guide
          + [129]data scientists interviews
          + [130]partners
          + [131]data science courses
     *

hours & info
       [132]whitefield, bangalore,
       india
       opens: 9:00 am
       closes: 11:00 pm
     *

categories
          + [133]amazon
          + [134]big data
          + [135]courses
          + [136]data science
          + [137]data science events
          + [138]datamining
          + [139]deep learning
          + [140]interviews
          + [141]machine learning
          + [142]newsletter
          + [143]python
          + [144]r
          + [145]recommendation engine
     *

tags
       [146]amazon go [147]big data [148]bigdata [149]classification
       [150]classification algorithms [151]id91 algorithms
       [152]datamining [153]data mining [154]datascience [155]data science
       [156]datasciencecongress2017 [157]data science courses [158]data
       science events [159]data scientist [160]decision tree [161]deep
       learning [162]hierarchical id91 [163]k-nearest neighbor
       [164]kaggle [165]id75 [166]id28
       [167]machine learning [168]monthly newsletter [169]multinomial
       id28 [170]naive bayes [171]pca [172]python
       [173]python programming language [174]id79
       [175]recommendation_engine [176]recommendation_systems
       [177]regression [178]regression coefficient [179]reinforcement
       learning [180]r programming [181]scala [182]scikit-learn [183]scipy
       [184]script output [185]similarity_distance [186]supervised
       learning [187]id166 [188]tensorflow [189]unsupervised learning
       [190]virtual_env

      copyright 2017 by dataaspirant.com. all rights reserved.

   (button) close dialog

   session expired

   [191]please log in again. the login page will open in a new window.
   after logging in you can close it and return to this page.

   >

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [192]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://plus.google.com/104087213513583475222
   2. http://dataaspirant.com/feed/
   3. http://dataaspirant.com/comments/feed/
   4. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/feed/
   5. http://dataaspirant.com/wp-json/oembed/1.0/embed?url=http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/
   6. http://dataaspirant.com/wp-json/oembed/1.0/embed?url=http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/&format=xml
   7. http://dataaspirant.com/
   8. http://dataaspirant.com/about/
   9. http://dataaspirant.com/for-beginners/
  10. http://dataaspirant.com/data-science-courses/
  11. http://dataaspirant.com/data-scientists-interviews/
  12. http://dataaspirant.com/join-us/
  13. http://dataaspirant.com/monthly-newsletter/
  14. http://dataaspirant.com/partners/
  15. http://dataaspirant.com/author/saimadhu/
  16. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments
  17. http://dataaspirant.com/category/machine-learning-2/
  18. https://dataaspirant.com/2014/09/19/supervised-and-unsupervised-learning/
  19. https://dataaspirant.com/2014/10/02/linear-regression/
  20. https://twitter.com/intent/tweet?url=https://wp.me/p7ijt0-aa&text=how the id28 model works in machine learning.&via=dataaspirant&related=dataaspirant
  21. https://twitter.com/intent/tweet?url=https://wp.me/p7ijt0-aa&text=how the id28 model works in machine learning.&via=dataaspirant&related=dataaspirant
  22. https://en.wikipedia.org/wiki/logistic_function
  23. https://en.wikipedia.org/wiki/logistic_regression
  24. https://dataaspirant.com/2017/02/15/simple-linear-regression-python-without-any-machine-learning-libraries/
  25. https://en.wikipedia.org/wiki/lionel_messi
  26. https://dataaspirant.com/category/machine-learning-2/
  27. https://dataaspirant.com/2014/10/02/linear-regression/
  28. https://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/
  29. https://www.facebook.com/dataaspirant
  30. http://dataaspirant.quora.com/
  31. https://twitter.com/dataaspirant
  32. https://plus.google.com/104087213513583475222
  33. https://www.linkedin.com/company/dataaspirant
  34. http://www.reddit.com/user/dataaspirant/
  35. https://flipboard.com/@dataaspirant/
  36. https://medium.com/@dataaspirant
  37. https://github.com/saimadhu-polamuri/dataaspirant_codes
  38. http://dataaspirant.com/recommends/data-science-courses/data-science-machine-learning-python-hands-course-udemy/
  39. http://dataaspirant.com/recommends/data-science-courses/machine-learning-z-hands-python-r-data-science-course-udemy/
  40. http://dataaspirant.com/recommends/data-science-courses/machine-learning-courses-udemy/
  41. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?share=twitter
  42. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?share=facebook
  43. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?share=google-plus-1
  44. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?share=reddit
  45. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?share=pinterest
  46. https://api.whatsapp.com/send?text=how the id28 model works http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/
  47. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?share=linkedin
  48. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?share=email
  49. http://dataaspirant.com/tag/classification-algorithms/
  50. http://dataaspirant.com/tag/logistic-regression/
  51. http://dataaspirant.com/2017/02/20/gaussian-naive-bayes-classifier-implementation-python/
  52. http://dataaspirant.com/category/machine-learning-2/
  53. http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/
  54. http://dataaspirant.com/category/machine-learning-2/
  55. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments/1985
  56. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?replytocom=1985#respond
  57. https://dataaspirant.com/
  58. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments/1987
  59. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?replytocom=1987#respond
  60. https://dataaspirant.com/
  61. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments/1990
  62. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?replytocom=1990#respond
  63. http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/
  64. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments/2013
  65. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?replytocom=2013#respond
  66. http://dataaspirant.com/2017/03/14/multinomial-logistic-regression-model-works-machine-learning/
  67. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments/2094
  68. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?replytocom=2094#respond
  69. https://dataaspirant.com/2015/10/24/four-coursera-data-science-specializations-starts-this-month/
  70. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments/2139
  71. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?replytocom=2139#respond
  72. http://dataaspirant.com/2017/04/15/implement-logistic-regression-model-python-binary-classification/
  73. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#comments/2574
  74. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/?replytocom=2574#respond
  75. https://www.facebook.com/saimadhu.seven/
  76. https://twitter.com/saimadhup/
  77. https://github.com/saimadhu-polamuri/
  78. http://dataaspirant.com/recommends/andrew-ng/deeplearning-course-1-coursera/
  79. http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/
  80. http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/
  81. http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/
  82. http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/
  83. http://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/
  84. http://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/
  85. http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/
  86. http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/
  87. http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/
  88. http://dataaspirant.com/2017/03/07/difference-between-softmax-function-and-sigmoid-function/
  89. http://dataaspirant.com/2017/01/19/support-vector-machine-classifier-implementation-r-caret-package/
  90. http://dataaspirant.com/2017/01/19/support-vector-machine-classifier-implementation-r-caret-package/
  91. http://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/
  92. http://dataaspirant.com/2017/02/06/naive-bayes-classifier-machine-learning/
  93. http://dataaspirant.com/2017/06/26/random-forest-classifier-python-scikit-learn/
  94. http://dataaspirant.com/2017/06/26/random-forest-classifier-python-scikit-learn/
  95. http://dataaspirant.com/2017/04/21/visualize-decision-tree-python-graphviz/
  96. http://dataaspirant.com/2017/04/21/visualize-decision-tree-python-graphviz/
  97. http://dataaspirant.com/2017/01/25/id166-classifier-implemenation-python-scikit-learn/
  98. http://dataaspirant.com/2017/01/25/id166-classifier-implemenation-python-scikit-learn/
  99. http://click.linksynergy.com/fs-bin/click?id=d1qcig2q3qi&offerid=479491.289&subid=0&type=4
 100. http://dataaspirant.com/2018/03/22/twitter-sentiment-analysis-using-r/
 101. http://dataaspirant.com/2018/02/08/id24-in-reinforcement-learning/
 102. http://dataaspirant.com/2018/02/05/reinforcement-learning-r/
 103. http://dataaspirant.com/2018/01/15/feature-selection-techniques-r/
 104. http://dataaspirant.com/2018/01/08/hierarchical-id91-r/
 105. http://dataaspirant.com/2017/11/04/smart-r-programming-tips/
 106. http://dataaspirant.com/2017/10/05/create-histograms-r/
 107. http://dataaspirant.com/2017/09/01/perform-principal-component-analysis-r/
 108. http://dataaspirant.com/2017/06/26/random-forest-classifier-python-scikit-learn/
 109. http://dataaspirant.com/2017/05/22/random-forest-algorithm-machine-learing/
 110. http://tidd.ly/3eaf0d92
 111. https://twitter.com/dataaspirant
 112. http://dataaspirant.com/category/amazon/
 113. http://dataaspirant.com/category/big-data/
 114. http://dataaspirant.com/category/courses/
 115. http://dataaspirant.com/category/data-science-2/
 116. http://dataaspirant.com/category/data-science-events/
 117. http://dataaspirant.com/category/datamining/
 118. http://dataaspirant.com/category/deep-learning/
 119. http://dataaspirant.com/category/interviews/
 120. http://dataaspirant.com/category/machine-learning-2/
 121. http://dataaspirant.com/category/newsletter/
 122. http://dataaspirant.com/category/python/
 123. http://dataaspirant.com/category/r/
 124. http://dataaspirant.com/category/recommendation_engine/
 125. http://dataaspirant.com/
 126. http://dataaspirant.com/about/
 127. http://dataaspirant.com/join-us/
 128. http://dataaspirant.com/for-beginners/
 129. http://dataaspirant.com/data-scientists-interviews/
 130. http://dataaspirant.com/partners/
 131. http://dataaspirant.com/data-science-courses/
 132. https://maps.google.com/maps?z=16&q=whitefield,+bangalore,+india
 133. http://dataaspirant.com/category/amazon/
 134. http://dataaspirant.com/category/big-data/
 135. http://dataaspirant.com/category/courses/
 136. http://dataaspirant.com/category/data-science-2/
 137. http://dataaspirant.com/category/data-science-events/
 138. http://dataaspirant.com/category/datamining/
 139. http://dataaspirant.com/category/deep-learning/
 140. http://dataaspirant.com/category/interviews/
 141. http://dataaspirant.com/category/machine-learning-2/
 142. http://dataaspirant.com/category/newsletter/
 143. http://dataaspirant.com/category/python/
 144. http://dataaspirant.com/category/r/
 145. http://dataaspirant.com/category/recommendation_engine/
 146. http://dataaspirant.com/tag/amazon-go/
 147. http://dataaspirant.com/tag/big-data-2/
 148. http://dataaspirant.com/tag/bigdata/
 149. http://dataaspirant.com/tag/classification/
 150. http://dataaspirant.com/tag/classification-algorithms/
 151. http://dataaspirant.com/tag/id91-algorithms/
 152. http://dataaspirant.com/tag/datamining-2/
 153. http://dataaspirant.com/tag/data-mining/
 154. http://dataaspirant.com/tag/datascience/
 155. http://dataaspirant.com/tag/data-science/
 156. http://dataaspirant.com/tag/datasciencecongress2017/
 157. http://dataaspirant.com/tag/data-science-courses/
 158. http://dataaspirant.com/tag/data-science-events/
 159. http://dataaspirant.com/tag/data-scientist/
 160. http://dataaspirant.com/tag/decision-tree/
 161. http://dataaspirant.com/tag/deep-learning/
 162. http://dataaspirant.com/tag/hierarchical-id91/
 163. http://dataaspirant.com/tag/k-nearest-neighbor/
 164. http://dataaspirant.com/tag/kaggle/
 165. http://dataaspirant.com/tag/linear-regression/
 166. http://dataaspirant.com/tag/logistic-regression/
 167. http://dataaspirant.com/tag/machine-learning/
 168. http://dataaspirant.com/tag/monthly-newsletter/
 169. http://dataaspirant.com/tag/multinomial-logistic-regression/
 170. http://dataaspirant.com/tag/naive-bayes/
 171. http://dataaspirant.com/tag/pca/
 172. http://dataaspirant.com/tag/python/
 173. http://dataaspirant.com/tag/python-programming-language/
 174. http://dataaspirant.com/tag/random-forest/
 175. http://dataaspirant.com/tag/recommendation_engine/
 176. http://dataaspirant.com/tag/recommendation_systems/
 177. http://dataaspirant.com/tag/regression/
 178. http://dataaspirant.com/tag/regression-coefficient/
 179. http://dataaspirant.com/tag/reinforcement-learning/
 180. http://dataaspirant.com/tag/r-programming/
 181. http://dataaspirant.com/tag/scala/
 182. http://dataaspirant.com/tag/scikit-learn/
 183. http://dataaspirant.com/tag/scipy/
 184. http://dataaspirant.com/tag/script-output/
 185. http://dataaspirant.com/tag/similarity_distance/
 186. http://dataaspirant.com/tag/supervised-learning/
 187. http://dataaspirant.com/tag/id166/
 188. http://dataaspirant.com/tag/tensorflow/
 189. http://dataaspirant.com/tag/unsupervised-learning/
 190. http://dataaspirant.com/tag/virtual_env/
 191. http://dataaspirant.com/wp-login.php
 192. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#cancel

   hidden links:
 194. http://dataaspirant.com/2017/03/02/how-logistic-regression-model-works/#sidr
 195. https://www.facebook.com/dataaspirant/
 196. http://bellaward.com/
 197. https://www.facebook.com/dataaspirant/
 198. https://plus.google.com/104087213513583475222
 199. https://twitter.com/dataaspirant
 200. https://www.linkedin.com/company/dataaspirant
 201. http://dataaspirant.quora.com/
 202. http://www.reddit.com/user/dataaspirant/
 203. https://flipboard.com/@dataaspirant/
 204. https://medium.com/@dataaspirant
