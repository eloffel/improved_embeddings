   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

q/a system         deep learning(2/2)

   [14]go to the profile of kashyap raval
   [15]kashyap raval (button) blockedunblock (button) followfollowing
   apr 11, 2017

   how lstm works?

   i think it   s unfair to say that neural network has no memory at all.
   after all, those learnt weights are some kind of memory of the training
   data. but this memory is more static. sometimes we want to remember an
   input for later use. there are many examples of such a situation, such
   as the stock market. to make a good investment judgement, we have to at
   least look at the stock data from a time window.
   [1*ceysrnsreg5azya3tqcbua.png]

   a lstm network has the following three aspects that differentiate it
   from an usual neuron in a recurrent neural network.

   1. it has control on deciding when to let the input enter the neuron.

   2. it has control on deciding when to remember what was computed in the
   previous time step.

   3. it has control on deciding when to let the output pass on to the
   next time stamp.

   simple example of lstm in python using keras: -
import numpy
from keras.models import sequential
from keras.layers import dense
from keras.layers import lstm
from keras.utils import np_utils
numpy.random.seed(7)
alphabet = "abcdefghijklmnopqrstuvwxyz"
char_to_int = dict((c, i) for i, c in enumerate(alphabet))
int_to_char = dict((i, c) for i, c in enumerate(alphabet))
# prepare the dataset of input to output pairs encoded as integers
# seq_length = 1
seq_length=1
datax = []
datay = []
for i in range(0, len(alphabet) - seq_length, 1):
 seq_in = alphabet[i:i + seq_length]
 seq_out = alphabet[i + seq_length]
 datax.append([char_to_int[char] for char in seq_in])
 datay.append(char_to_int[seq_out])
 print seq_in, '->', seq_out
# reshape x to be [samples, time steps, features]
x = numpy.reshape(datax, (len(datax), seq_length, 1))
x = x / float(len(alphabet))
y = np_utils.to_categorical(datay)
# create and fit the model
model = sequential()
model.add(lstm(32, input_shape=(x.shape[1], x.shape[2])))
model.add(dense(y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossid178', optimizer='adam', metrics=['accur
acy'])
model.fit(x, y, epochs=600, batch_size=1, verbose=2)
# summarize performance of the model
scores = model.evaluate(x, y, verbose=0)
print("model accuracy: %.2f%%" % (scores[1]*100))
for pattern in datax:
 x = numpy.reshape(pattern, (1, len(pattern), 1))
 x = x / float(len(alphabet))
 prediction = model.predict(x, verbose=0)
 index = numpy.argmax(prediction)
 result = int_to_char[index]
 seq_in = [int_to_char[value] for value in pattern]
 print seq_in, "->", result

   [1*56ief8a0-ez80y1whjs6xw.png]
   output
# if you set seq_length=3 then output:--

   [1*xnbhtutjpl9-zdypg03ypa.png]
   seq_length=3

   now lets work with some text data:

   create one python file :

   code:
import re
from nltk.tokenize import word_tokenize
from keras.preprocessing.sequence import pad_sequences
import numpy as np
def tokenize(sentence):
    return word_tokenize(sentence)
def vectorize_ques(data, word_id, test_max_length, ques_max_length):
    x = []
    xq = []
    for subtext, question in data:
        x = [word_id[w] for w in subtext]
        xq = [word_id[w] for w in question]

        x.append(x)
        xq.append(xq)
    return (pad_sequences(x, maxlen=test_max_length),
            pad_sequences(xq, maxlen=ques_max_length))
def vectorize_text(data, word_id, text_max_length, ques_max_length):
    x = []
    xq = []
    y = []
    for subtext, question, answer in data:
        x = [word_id[w] for w in subtext]
        # save the id of questions using subtext
        xq = [word_id[w] for w in question]
        # save the answers for the questions in "y" as "1"
        y = np.zeros(len(word_id) + 1)
        y[word_id[answer]] = 1
        x.append(x)
        xq.append(xq)
        y.append(y)
    return (pad_sequences(x, maxlen=text_max_length),
            pad_sequences(xq, maxlen=ques_max_length),
            np.array(y))
def read_text():
    text = []
    input_line = input('story, empty to stop: ')
    while input_line != '':
        # for now, lines have to be a full sentence
        if not input_line.endswith('.'):
            input_line += '.'
        text.extend(tokenize(input_line))
        input_line = input('story, empty to stop: ')
    return text

   another python file train.py:
# kashyap
# dataset from facebook ai research page
import os
from keras.models import sequential, model
from keras.layers.embeddings import embedding
from keras.preprocessing.sequence import pad_sequences
from keras.layers import input, activation, dense, permute, dropout, add, dot, c
oncatenate
from keras.layers import lstm
from keras.utils.data_utils import get_file
from functools import reduce
from nltk.tokenize import word_tokenize
import tarfile
from text_preprocessing import *
import numpy as np
import pickle
import keras
import re
import random
def parse_text(lines, only_supporting=false):

    data = []
    text = []

    for line in lines:

        line = line.decode('utf-8').strip()

        id, line = line.split(' ', 1)

        id = int(id)
        if id == 1:
            text = []

        if '\t' in line:
            ques, ans, supporting = line.split('\t')

            ques = tokenize(ques)
            subtext = none
            if only_supporting:

                supporting = list(map(int, supporting.split()))

                subtext = [text[i - 1] for i in supporting]
            else:

                subtext = [x for x in text if x]
            data.append((subtext, ques, ans))
            text.append('')
        else:
            sent = tokenize(line)
            text.append(sent)
    return data
def get_stories(file, only_supporting=false, max_length=none):

      data = parse_text(file.readlines(), only_supporting=only_supporting)
    flatten = lambda data: reduce(lambda x, y: x + y, data)
# flatten: takes two sentences and makes one array, 2nd array of question answer
 in a list
    # format: [([first sentence tokeinized, second sentence tokenized],[question
 with answer tokenized]), ....]
    data = [(flatten(text), question, answer) for text, question, answer in data
 if
            not max_length or len(flatten(text)) < max_length]
    return data
class memorynetwork(object):
    file_name = 'model'
    vocab_file_name = 'model_vocab'
def __init__(self):
        if (os.path.exists(memorynetwork.file_name) and
                os.path.exists(memorynetwork.vocab_file_name)):
            self.load()
        else:
            self.train()
            self.store()
def load(self):
        self.model = keras.models.load_model(memorynetwork.file_name)
        with open(memorynetwork.vocab_file_name, 'rb') as file:
            self.word_id = pickle.load(file)
def store(self):
        self.model.save(memorynetwork.file_name)
        with open(memorynetwork.vocab_file_name, 'wb') as file:
            pickle.dump(self.word_id, file)
def train(self):
        # load the babi dataset
        try:
            datapath = get_file('babi-tasks-v1-2.tar.gz',
                                origin='[16]https://s3.amazonaws.com/text-datase
ts/babi_tasks_1-20_v1-2.tar.gz')
        except:
            print('error downloading dataset, please download it manually:\n'
                  '$ wget [17]http://www.thespermwhale.com/jaseweston/babi/tasks
_1-20_v1-2.tar.gz\n'
                  '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2
.tar.gz')
            raise
tar = tarfile.open(datapath)
        challenges = {
            'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-sup
porting-fact_{}.txt',
            'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporti
ng-facts_{}.txt',
        }
challenge_type = 'single_supporting_fact_10k'
challenge = challenges[challenge_type]
# extract the text from single_supporting_fact_10k file
        print('extracting stories for the challenge:', challenge_type)
# load the testing and training text data
        train_stories = get_stories(tar.extractfile(challenge.format('train')))
        test_stories = get_stories(tar.extractfile(challenge.format('test')))
# initialize vocabulary as as set
        # create a vocabulary list with all words occuring only once
        vocab = set()
        for text, ques, answer in train_stories + test_stories:
            vocab |= set(text + ques + [answer])
# sort the words in vocabulary list
        vocab = sorted(vocab)
# get the max length of the vocabulary, text and questions
        vocab_size = len(vocab) + 1
# text_max_length: length of th subtext; no. of subtexts
        text_max_length = max(list(map(len, (x for x, _, _ in train_stories + te
st_stories))))
# ques_max_length: length of questions in input.
        ques_max_length = max(list(map(len, (x for _, x, _ in train_stories + te
st_stories))))
print('-')
print('vocab size:', vocab_size, 'unique words')
        print('story max length:', text_max_length, 'words')
        print('query max length:', ques_max_length, 'words')
        print('number of training stories:', len(train_stories))
        print('number of test stories:', len(test_stories))
print('-')
print('here\'s what a "story" tuple looks like (input, query, answer):')
        print(train_stories[0])
print('-')
print('vectorizing the word sequences...')
# vectorize the training and testing data
        self.word_id = dict((c, i + 1) for i, c in enumerate(vocab))
# inputs_train: matrix of arrays; arrays containing vectorized sentences
        # ques_train: matrix of arrays; each array has 4 values; each value corr
esponds to a character.
        # answers_train: matrix of arrays; each array contains a single "1", ind
ex corresponding to answer
        inputs_train, ques_train, answers_train = vectorize_text(train_stories,
                                                                 self.word_id,
                                                                 text_max_length
,
                                                                 ques_max_length
)
inputs_test, ques_test, answers_test = vectorize_text(test_stories,
                                                              self.word_id,
                                                              text_max_length,
                                                              ques_max_length)
# dataset analysis
        print('-')
print('inputs: integer tensor of shape (samples, max_length)')
        print('inputs_train shape:', inputs_train.shape)
        print('inputs_test shape:', inputs_test.shape)
print('-')
print('queries: integer tensor of shape (samples, max_length)')
        print('queries_train shape:', ques_train.shape)
        print('queries_test shape:', ques_test.shape)
print('-')
print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')
        print('answers_train shape:', answers_train.shape)
        print('answers_test shape:', answers_test.shape)
print('-')
print('compiling...')
# define placeholders
        input_sequence = input((text_max_length,))
        question = input((ques_max_length,))

        input_encoder_m = sequential()
input_encoder_m.add(embedding(input_dim=vocab_size,
                                      output_dim=64))
input_encoder_m.add(dropout(0.3))
input_encoder_c = sequential()
input_encoder_c.add(embedding(input_dim=vocab_size,
                                      output_dim=ques_max_length))
input_encoder_c.add(dropout(0.3))

question_encoder = sequential()
question_encoder.add(embedding(input_dim=vocab_size,
                                       output_dim=64,
                                       input_length=ques_max_length))
question_encoder.add(dropout(0.3))
        # output: (samples, query_maxlen, embedding_dim)
input_encoded_m = input_encoder_m(input_sequence)
input_encoded_c = input_encoder_c(input_sequence)
question_encoded = question_encoder(question)
# compute a 'match' between the first input vector sequence
        # and the question vector sequence
        # shape: `(samples, story_maxlen, query_maxlen)`
        match = dot([input_encoded_m, question_encoded], axes=(2, 2))
        match = activation('softmax')(match)
# add the match matrix with the second input vector sequence
        response = add([match, input_encoded_c])  # (samples, story_maxlen, quer
y_maxlen)
        response = permute((2, 1))(response)  # (samples, query_maxlen, story_ma
xlen)
        answer = concatenate([response, question_encoded])
        # we choose to use a id56 instead.
        answer = lstm(32)(answer)  # (samples, 32)
        answer = dropout(0.3)(answer)
        answer = dense(vocab_size)(answer)  # (samples, vocab_size)
        # we output a id203 distribution over the vocabulary
        answer = activation('softmax')(answer)
# build the final model
        self.model = model([input_sequence, question], answer)
        self.model.compile(optimizer='rmsprop', loss='categorical_crossid178',
                      metrics=['accuracy'])
# train the model
        self.model.fit([inputs_train, ques_train], answers_train,
                  batch_size=32,
                  epochs=120,
                  validation_data=([inputs_test, ques_test], answers_test))

   final.py :    
from text_preprocessing import *
from train import *
import numpy as np
memory_network = memorynetwork()
while true:
    print('use this vocabulary to form questions:\n' + ' , '.join(memory_network
.word_id.keys()))
    story = read_text()
    print('story: ' + ' '.join(story))
    question = input('q:')
    if question == '' or question == 'exit':
        break
    story_vector, query_vector = vectorize_ques([(story, tokenize(question))],
                                                  memory_network.word_id, 68, 4)
    prediction = memory_network.model.predict([np.array(story_vector), np.array(
query_vector)])
    prediction_word_index = np.argmax(prediction)
    for word, index in memory_network.word_id.items():
        if index == prediction_word_index:
            print('answer: ',word)

   python3 main.py
   [1*dgto2zn3obahqjz04xorwg.png]
   output

   please let me know if you are having any problem

   i will update code on github soon.

   iframe: [18]/media/47a0a8491046a6d8bdbb86880dd8fc1e?postid=c0ad60800e3

     * [19]machine learning
     * [20]deep learning
     * [21]python
     * [22]keras
     * [23]artificial intelligence

   (button)
   (button)
   (button) 30 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of kashyap raval

[25]kashyap raval

   computer geek ,ml-ai lover(python)

     (button) follow
   [26]becoming human: artificial intelligence magazine

[27]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 30
     * (button)
     *
     *

   [28]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [29]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c0ad60800e3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/q-a-system-deep-learning-2-2-c0ad60800e3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/q-a-system-deep-learning-2-2-c0ad60800e3&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_b5y9g7lvq4t2---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@kashyapraval?source=post_header_lockup
  15. https://becominghuman.ai/@kashyapraval
  16. https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz'
  17. http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n'
  18. https://becominghuman.ai/media/47a0a8491046a6d8bdbb86880dd8fc1e?postid=c0ad60800e3
  19. https://becominghuman.ai/tagged/machine-learning?source=post
  20. https://becominghuman.ai/tagged/deep-learning?source=post
  21. https://becominghuman.ai/tagged/python?source=post
  22. https://becominghuman.ai/tagged/keras?source=post
  23. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  24. https://becominghuman.ai/@kashyapraval?source=footer_card
  25. https://becominghuman.ai/@kashyapraval
  26. https://becominghuman.ai/?source=footer_card
  27. https://becominghuman.ai/?source=footer_card
  28. https://becominghuman.ai/
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/c0ad60800e3/share/twitter
  32. https://medium.com/p/c0ad60800e3/share/facebook
  33. https://medium.com/p/c0ad60800e3/share/twitter
  34. https://medium.com/p/c0ad60800e3/share/facebook
