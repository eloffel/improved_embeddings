   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

an introduction to tensorflow

   [14]go to the profile of liu songxiang
   [15]liu songxiang (button) blockedunblock (button) followfollowing
   mar 24, 2017
   [1*kr3uf_wc-lttg3oiptkfga.png]

   in this tutorial, i will first talk about some basic concepts in
   tensorflow, and then i will elaborate the mechanics of tensorflow using
   an example from [16]tensorflow, i will also digress to some machine
   learning notions like sgd, id26 and etc.
    1. what is tensorflow?

   tensorflow is an open source software library for numerical computation
   using data flow graphs. it was originally developed by google brain
   team to conduct machine learning and deep neural networks research, but
   the system is general enough to be applicable in a wide variety of
   other domains as well. the flexible architecture of tensorflow allows
   to deploy different-size training and id136 systems on a wide
   variety if different hardware platforms, such as single machines
   containing one or more gpu cards, android and ios.

   2. what is tensor?

   formally, tensors are multilinear maps from vector spaces to the real
   numbers (v is vector space, and v* is dual space), that is
   [1*y7m2dnz0wchcnuhqvvflpg.png]

   in tensorflow, a tensor can be represented as a multidimensional array
   of numbers (very similar to n-d array in numpy). a tensor has its rank
   and shape, rank is its number of dimensions and shape is the size of
   each dimension. to see the picture below for example.
   [1*7axsk5bdsfo9wzmswu5v1w.png]

   tensorflow uses tensor data structure to represent all data, only
   tensors are passes between operations in the computation graph. tensor
   data structure in tensorflow support a variety of element types,
   including signed and unsigned integers ranging in size from 8 bits to
   64 bits, ieee float and double types, a complex number type, and a
   string type (an arbitrary byte array). check the tables below for
   reference.
   [1*ni1ujhr5awff5co6c63udg.png]

   although few people make comparison between tensorflow and numpy, they
   are quite similar. (both are n-d array libraries). numpy is the
   fundamental package for scientific computing with python; it contains a
   powerful n-dimensional array object, sophisticated (broadcasting)
   functions and etc. i believe that most python users can not live
   without numpy. numpy has n-d array support, but it does not offer
   methods to create tensor functions and automatically compute
   derivatives and no gpu support, this is also one of the main reason for
   tensorflow   s existence. below is the numpy to tensorflow dictionary,
   you may easily find that many keywords are very similar.
   [1*c8vkrvnog07dk6d5wj3f3q.png]

   3. programming model in tensorflow

   the big idea of a tensorflow program is to express a numeric
   computation as a directed graph. the picture below is an example of
   computation graph, which represents the computation of h = relu(wx +
   b). this is a very classic component in many neural networks, which
   conducts an linear transformation of the input data and then feed to a
   linearity (rectified linear activation function in this case).
   [1*zpnkntgdfdndtregarovfq.png]

   the graph above represents a data-flow computation; each node is an
   operation which have zero or more inputs and zero or more outputs.
   graph edges are tensors which flow between nodes. clients typically
   construct a computational graph using one of the supported frontend
   languages such as python and c++ and then launch the graph to a session
   to execute (session is a very important notion in tensorflow, more
   about session below).

   let   s look at the computation graph above in detail. we truncate the
   graph and leave out the part above the relu node, we then get the graph
   below, which is exactly represents the computation h = relu(wx + b).
   [1*g9mxgom2jwl3soxuqrlv3a.png]

   now we are going to see what in the graph. we can view the graph as a
   system, which has inputs (the data x), output(h in this case), stateful
   variables(w and b)and a bunch of operations (matmul, add and relu). let
   me introduce them one by one.

   placeholders: in order to feed input data to train the model or make
   id136, we must have a input port to the graph. placeholders are
   nodes whose values are fed in at execution time. typically we want to
   feed data inputs, labels and hyper-parameters(eg. dropout rate) to the
   model.

   variables: when we train a model, we use variables to hold and update
   parameters. unlike many tensors flowing along the edges which do not
   survive past a single execution of the graph, a variable is a special
   kind of operation that return a handle to a persistent mutable tensor
   that survives across executions of a graph. in most machine learning
   models, there are many parameters we have to learn, which update during
   training. variables are stateful nodes which store parameters and
   output their current values from time to time. their states are
   retained across multiple executions of a graph. for example, the values
   of this nodes will not update until a full step of training using a
   mini batch of data finishes.

   mathematical operations: in this graph, there are three types math
   operations. matmul operation multiplies two matrix values; add
   operation adds element-wise (with broadcasting) and relu operation
   activates with element-wise rectified linear function.

   now lets implement this computation graph in python codes by two
   phases, one of which is building the computation graph and the other is
   running the computation graph.
     * building the computation graph:

   variables must be explicitly initialized. when create a variable, we
   pass a tensor as its initial value to the variable() constructor. the
   initializer can be constants, sequences and random values. in this
   case, we initialize the bias vector b by constants which are zeros, and
   initialize the weights matrix w by random uniform. note that all these
   ops require you to specify the shape of the tensors and that shape
   automatically becomes the shape of the variable. in this case, b is a
   rank 1 tensor with shape (100, ) and w is a rank 2 tensor with shape
   (784, 100).

   when use a tf.placeholder, its data type is required explicitly, while
   the shape is optional. if the shape is not specified, we can feed a
   tensor of any shape.
   [1*ug2uvo0kuy9bbtvh5rnrda.png]
     * running the computation graph:

   to execute the computation, we must launch the graph to a tf.session.
   what is a session? we can understand a session as an environment for
   running the graph. actually, to do efficient numerical computing in
   python, we typically use libraries like numpy that do expensive
   operations such as id127 outside python, using highly
   efficient code implemented in another language ( c ). unfortunately,
   there can still be a lot of overhead from switching back to python
   every operation. this overhead is especially bad if you want to run
   computations on gpus or in a distributed manner, where there can be a
   high cost to transferring data.

   tensorflow also does its heavy lifting outside python, but it takes
   things a step further to avoid this overhead. instead of running a
   single expensive operation independently from python, tensorflow lets
   us describe a graph of interacting operations that run entirely outside
   python.

   tensorflow relies on a highly efficient c++ backend to do its
   computation. the connection to this backend is called a session. the
   common usage for tensorflow programs is to first create a graph and
   then launch it in a session.

   in the codes below, by sess = tf.session() we construct a session
   object which encapsulates the environment in which operation objects
   are executed, and tensor objects are evaluated. because no graph
   argument is specified when constructing the session, the default graph
   will be launched in the session.

   variable initializers must be run explicitly before other ops in your
   model can be run. the easiest way to do that is to add an op that runs
   all the variable initializers, and run that op before using the model.
   use tf.global_variables_initializer() to add an op to run variable
   initializers. only run that op after you have fully constructed your
   model and launched it in a session.

   when execute a graph in a session, the typical format is
   sess.run(fetches,feeds). fetches is a list of graph nodes which returns
   the output of these nodes. feeds is a dictionary mapping from graph
   nodes to concrete values which specifies the value of each graph node
   given in the dictionary. the value returned by run() has the same shape
   as the fetches argument, where the leaves are replaced by the
   corresponding values returned by tensorflow.
   [1*jqd-fhknzm9e3ntxapqzhg.png]

   so what have we done so far? we have got some rough ideas about many
   basic concepts in tensorflow. to implement the example graph, we first
   built a graph using variables and placeholders, then we deployed the
   graph onto a session, which is the execution environment. in the next
   part, we will see how to train and evaluate a simple feed-forward
   neural network for handwritten digit classification.

   4. an example: handwritten digit classification

   in this part, we are going to use tensorflow to train and evaluate a
   simple feed-forward neural network for handwritten digit classification
   using the classic mnist data set. the [17]mnist database is a very
   classic dataset in the field of machine learning; it contains many
   grayscale 28x28 pixel images of handwritten digits. the task of this
   example is to classify the images to digits. some of the images are
   like this:
   [1*nfwljhq0zelzi34fjg68yw.png]

   we will deploy a two-hidden-layer neural network using tensorflow. the
   structure of the model is like this:
   [1*op5e0hn3foonbn-iuzjwpw.png]

   it takes the images as inputs and builds on top of it a pair of fully
   connected layers with relu activation followed by a ten node linear
   layer specifying the output logits. the relu activation is defined as
   f(x) = max(0,x), and it   s plot is as below.
   [1*qehx8klxwbwl-fa42qtima.png]

   first we have to load the data: the codes below will download the
   dataset to the local training folder and then unpack it to return a
   dictionary of dataset instances. the dataset contains a training set of
   55000 images&labels, a validation dataset with 5000 images&labels, and
   a test set with 10000 images&labels. the training set is for training
   the model and the validation set is for iterative validation training
   accuracy and the test set is for final testing the model performance.
   [1*sl-i80hbzuytognwuriria.png]

   in our example, we only have to feed the images and the labels to the
   computation graph. so we create two tf.placeholder ops that define the
   shape of the inputs. in our datasets, we have flatten every 28x28 image
   to 1-d array with shape (784, ). hence the images_paceholder has the
   shape (batch_size, 784), the labels_placeholder has the shape
   (batch_size, ).
   [1*tgodgt168n1stv95lged-w.png]

   we now at the stage of building the graph. the graph is built according
   to a 3-stage pattern:
     * id136(): builds the graph as far as is required for running the
       network forward to make predictions.

   [1*fi4fsy748fhkjrtt6ow1oq.png]
     * loss(): adds to the id136 graph the ops required to generate
       loss.

   [1*6gwwucmvvebepyahbgsqlq.png]
     * training(): adds to the loss graph the ops required to compute and
       apply gradients.

   [1*vb1z5ju7raslecyilco8mq.png]

   we are now to have a look at the codes.

   the first hidden layer takes images as inputs, then does a linear
   transformation, followed by a relu activation. the second hidden layer
   has the same pattern. for the output layer, there only has a linear
   transformation computation. so we have three sets of weights and biases
   variables. to use the same variable names and avoid clash, we make each
   layer be created beneath a unique tf.name_scope, so the complete name
   for the weights variable in the first hidden layer is hidden1/weights.
   in the variable constructors, we use the tf.truncated_normal ops to
   initialize the weights (see [18]neural networks part 2: setting up the
   data and the loss)and tf.zeros ops to initialize the biases.
   [1*8bphdfr3mcy_jbfwnhwx0a.png]

   in order to train our model, we need to define the cost or the loss to
   represent how far off out models is from desired outcome, we try to
   minimize that error, and the smaller the error margin, the better our
   model is. in many machine learning models, cross id178 loss is a very
   common way to measure how much the approximated distribution q(x)
   differs from a true distribution p(x). note that to reduce the cross
   id178 loss is equivalent to reduce the [19]k-l divergence, see the
   equation below: the first term in the right-hand side of the equation
   is the id178 of x, which is a constant, and the second term is the
   cross id178 (including the minus sign). if the cross id178 is
   reduced, which means that the k-l divergence of q from p decreases, the
   approximation performance gets better.
   [1*bsrs4sgwidzwkkk4z65-yg.png]
   [1*o6ibvrvnq9gcgi0ol-0tpa.png]
   [1*1rilt9usdxoapa-myhbbrg.png]

   digress to minibatch stochastic id119

   well, what is ordinary id119? it is a simple algorithm in
   which we repeatedly make small steps downward on an error surface
   defined by a id168 of some parameters. ordinary gradient
   descent looks at all of the training set at a time and then accumulate
   the error information to undergo one step of parameter update. when we
   have a very large training set, the training set reading in process can
   be very slow and the update step becomes a computationally very
   expensive procedure. this is why sgd comes to play. when we have to do
   training on a large scale dataset, to make one update step, we just
   look at one or a few training examples instead of the entire training
   set. in the purest form, we estimate the gradient from just a single
   example at a time and it has been proved by practice that sgd
   convergences more quickly than ordinary gradient version. in deep
   learning, people commonly use the variant of sgd, called mini-batch
   sgd, which works identically to sgd, except that we use more than one
   training examples to make each estimate of the gradient. in our case,
   we look at a batch_size of training examples in every training
   iteration.

   in the codes above, we apply sparse_softmax_cross_id178_with_logits
   on the unnormalized logits to compute the cross id178. it defines as:
   [1*t76epyv0nedgtux0a7hflg.png]

   where y is our predicted id203 distribution, and y    is the true
   distribution (the one-hot vector with the digit labels). actually, the
   sparse_softmax_cross_id178_with_logits method computes sparse softmax
   cross id178 between logits and labels, where logits is unscaled
   id203 and labels vector must provide a single specific index for
   the true class. a common use case is to have logits of shape
   [batch_size, num_classes] and labels of shape [batch_size] . note that
   if you want to compute the cross id178 of a id203 distribution,
   that is, the unnormalized logits has went through the softmax method,
   you may use the softmax_cross_id178_with_logits.

   so far, we have defined the loss or the objective that we want to
   minimize. the last step before training is to tell the model how to
   minimize it. in tensorflow, we can first create the id119
   optimizer with the given learning rate (which is a hyper-parameter we
   have to tune), and then we add the training op to apply the gradient to
   minimize the loss. see the codes below. actually, tensorflow provides a
   variety of methods to compute gradients for a loss and apply gradients
   to variables, such as adagrad and momentumgrad. this example just uses
   the most basic one.
   [1*whgnj0-koclm-7ancq2ujq.png]

   we have built the whole computation graph. tensorflow provides a very
   powerful suite of visualization tools called tensorboard to make it
   easier for us to understand, debug, and optimize tensorflow programs.
   in our example, tensorboard gives us the following computation graph if
   you launch the tensorboard. we can zone into some of the nodes and see
   what tensorboard has done for us.
   [1*5nuezvurrqb5shzig62a6a.png]

   we add all tf.operation objects, which represent units of computation
   and tf.tensor objects, which represent the units of data flow between
   operations to the default graph. so now it   s ready to train the model!
   remember that before we run this graph, we must launch it to a session.
   [1*m6s0vlp9yypvgypepdfdja.png]
   [1*6noeebchqnl-rxdezd85ag.png]

   note that before we get into the training iterations, we must first
   initialize all the variables in the graph, as up to now they are
   symbolic variables without any values. the
   tf.global_variables_initializer (this is just a shortcut for
   variable_initializers(global_variables())) collects all the variables
   from the graph and then returns an op that initializes the variables.
   after launch the graph in a session, we can run the returned op to
   initialize all the variables collected.
   [1*avguhwmxj5phpaabpghn8w.png]
   [1*mu8_f3k1sbhqnrbxe6clrq.png]

   so now we are to train the model, just as what i introduced in part 3,
   we slice up the training set to mini-batch and feed it to the graph for
   each training step. note that when we run the training process by
   sess.run(), the train op should be included in the fetches because we
   have to activate this op to conduct one step of parameter update. you
   can of course fetch many other ops as you want to inspect the values of
   your ops and variables and all you have to do is to include them in the
   fetched list and the value tensors will be returned in a tuple. in our
   example, we fetch the loss op to record what   s going on when training.
   [1*vqdhosbvlzv_ngxdoujk0w.png]

   in practice, during training, the value of the loss tensor may become
   nan if the model diverged, in which case we may have to stop training
   and think about what   s wrong in our model. in some cases where the loss
   no longer goes down after a long time of training, we may have to
   reduce the learning rate and make the loss goes down further. all of
   above things need us to check the training status and visualize it. in
   our example we print out the loss value every 100 iterations.
   [1*mztt15ow-hnfagxpoyr8pw.png]

   so here are the model parameters and the performance of the model.
   [1*su8ypoy0a-m23pmj58ty7w.png]

   after 2000 steps of training, the final precision of the model on the
   test data is just 0.9002, which is very bad compared to the
   state-of-the-art model (can get over 99.7% accuracy). but after all, we
   have got the idea how to train and evaluate a machine learning model
   using tensorflow!

   one last thing to think about in this example is why can tensorflow
   compute gradients automatically without any explicit user codes to
   specify. actually, tensorflow has built-in support for automatic
   gradient computation, which is the main reason for its existence. let   s
   first look at how we do back-propagation in our example.
     * forward pass to compute the cross id178 loss

   in this derivation, i will take one image for example and the image is
   represented by a flattened vector which has shape [728].

   notation: we use x to represent the input image. we denote the input to
   each layer by z(l), and the output by a(l). note that in the input
   layer, x = z(l) = a(l). we denote the weights and the bias between
   layer l and layer l + 1 by w (l) and b(l). y   is the prediction with
   shape [10]. y is the true label for that image, which is a one-hot
   vector with shape [10]. the cross id178 is denoted by ce(y, y  ).

   the dimensions for all this items are:
   [1*pn31wsmgkyk6bmzgwdkndg.png]

   the forward pass is as below:
   [1*luhkm0jxj_bx3t9t5dmjqw.png]
     * id26

   in this part, i will denote the error information for layer l by   (l).
   for the true label i,
   [1*fxdayqr-2xnugesg8v5vpg.png]

   so,
   [1*npt-8eby3mbjm_hzrqlnmq.png]

   to write the result above in vector form, we get
   [1*corfrgo3dkv7y1zgjdrhma.png]

   to compute
   [1*ym01bzb9_5xxyqu2kgkroa.png]

   we first look at element gradient. first note that
   [1*-uck4zltu_acj4zhr0a4jw.png]

   hence,
   [1*fahowrca3eg6_xfkrhyxrq.png]

   using chain rule, we get
   [1*yk_omlhtme40587ywv4d-g.png]

   in matrix form this looks like
   [1*c50bvpcddnwxgakdvrotwa.png]

   similarly, we can compute
   [1*a6qscxp9nwa946las6xngq.png]

   now, we are going to compute
   [1*lnxwbqpdh20zabewjbk5pg.png]

   similarly, we first look at a simple element wise derivative:
   [1*p2sohsbgf-zlqs4hm49lma.png]

   using chain rule again, we obtain:
   [1*tpfvp1x3932iiwfjomk0fq.png]

   so to vectorize:
   [1*34bs9i0wqzddh7wb6cerjq.png]

   then, we have to compute
   [1*vtpnucebiewfqczfzmpahq.png]

   where i(a(3) > 0) is the indicator function. so far, we have computed
   all the gradients for the hidden layer 2. any other gradients can be
   calculated in similar way. here i just give the final computation
   results for these other gradients.
   [1*_xtfmmco6qdtf6rqoyuh4a.png]

   so we have computed all the gradients for the id119 step to
   make one step of all the parameters during training. in the forward
   id136 process, tensorflow will add some nodes to store some
   intermediate values. during the gradient computation process, gradients
   will be computed from the loss to the input layer. this is the sugar
   that tensorflow gives to us and also one of the main reason why it is
   so user-friendly. see the example below:
   [1*uifj1zxlhe-bjo7vdjy6xa.png]

   in the picture above, the gray graph is to construct the computation
   graph by forward pass id136 to compute all the intermediate values
   needed for loss computation. the purple part is back-prop the gradients
   to all the variable nodes using the stored intermediate values when
   necessary.

   5. a short summary

   in this tutorial, i first introduced some basic concepts in tensorflow
   using a very simple computation graph, and then i elaborated the basic
   mechanics of tensorflow programming using a handwritten digits
   classification problem. finally i illustrate how tensorflow can conduct
   automatic gradients computation for us.

   so, happy to play with tensorflow!

   references:

   [1] [20]abadi, mart  n, et al.    tensorflow: large-scale machine learning
   on heterogeneous distributed systems.    arxiv preprint arxiv:1603.04467
   (2016).

   [2] [21]https://www.tensorflow.org/get_started/mnist/mechanics

   [3] [22]https://www.tensorflow.org/get_started/mnist/pros

   [4] [23]https://github.com/tensorflow/tensorflow

   [5] [24]http://cs231n.github.io/neural-networks-2/

   [6]
   [25]http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-tensorflo
   w.pdf

   [7]
   [26]http://web.donga.ac.kr/yjko/usefulthings/tensorflow-basic-concept_k
   o.pdf

   [8] [27]https://cs224d.stanford.edu/lectures/cs224d-lecture7.pdf

   iframe: [28]/media/a6edaa982bd7d35e5cbcc4b8ed2f9c2b?postid=f4f31e3ea1c0

     * [29]machine learning
     * [30]neural networks
     * [31]deep learning
     * [32]artificial intelligence
     * [33]tutorial

   (button)
   (button)
   (button) 222 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [34]go to the profile of liu songxiang

[35]liu songxiang

   i am a phd candidate

     (button) follow
   [36]becoming human: artificial intelligence magazine

[37]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 222
     * (button)
     *
     *

   [38]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [39]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f4f31e3ea1c0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/an-introduction-to-tensorflow-f4f31e3ea1c0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/an-introduction-to-tensorflow-f4f31e3ea1c0&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_rxwfqb8ivkyx---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@shaunliu23?source=post_header_lockup
  15. https://becominghuman.ai/@shaunliu23
  16. http://www.tensorflow.org/
  17. http://yann.lecun.com/exdb/mnist/
  18. http://cs231n.github.io/neural-networks-2/
  19. https://en.wikipedia.org/wiki/kullback   leibler_divergence
  20. https://arxiv.org/pdf/1603.04467.pdf
  21. https://www.tensorflow.org/get_started/mnist/mechanics
  22. https://www.tensorflow.org/get_started/mnist/pros
  23. https://github.com/tensorflow/tensorflow
  24. http://cs231n.github.io/neural-networks-2/
  25. http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-tensorflow.pdf
  26. http://web.donga.ac.kr/yjko/usefulthings/tensorflow-basic-concept_ko.pdf
  27. https://cs224d.stanford.edu/lectures/cs224d-lecture7.pdf
  28. https://becominghuman.ai/media/a6edaa982bd7d35e5cbcc4b8ed2f9c2b?postid=f4f31e3ea1c0
  29. https://becominghuman.ai/tagged/machine-learning?source=post
  30. https://becominghuman.ai/tagged/neural-networks?source=post
  31. https://becominghuman.ai/tagged/deep-learning?source=post
  32. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  33. https://becominghuman.ai/tagged/tutorial?source=post
  34. https://becominghuman.ai/@shaunliu23?source=footer_card
  35. https://becominghuman.ai/@shaunliu23
  36. https://becominghuman.ai/?source=footer_card
  37. https://becominghuman.ai/?source=footer_card
  38. https://becominghuman.ai/
  39. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  41. https://medium.com/p/f4f31e3ea1c0/share/twitter
  42. https://medium.com/p/f4f31e3ea1c0/share/facebook
  43. https://medium.com/p/f4f31e3ea1c0/share/twitter
  44. https://medium.com/p/f4f31e3ea1c0/share/facebook
