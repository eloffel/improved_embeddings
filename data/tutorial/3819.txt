math for machine learning

author: parameswaran raman

january 25, 2015

abstract

in this post, i want to discuss the connections between machine learning and various other    elds (especially
mathematics), citing speci   c examples where they come up. i have given very high-level explanations below
and cut corners at several places as i do not want to get into the depth. my intention here is not to explain any
concept precisely, but to merely lay down enough of them on the table to emphasize the role of mathematics
in this fast growing area.

1 ok, why so much math?

machine learning is an incredibly modern    eld that borrows heavily from several areas of mathematics. having
evolved as an inter-disciplinary    eld which is very applied (driven by data), it has captured concepts, intuition and
theory from several places. being at such an intersection of diverse areas of mathematics and computer science is
what makes research in machine learning so exciting and challenging!

at this point, i would like to mention physics as an analogy.
i consider machine learning to be very similar
to physics as a discipline, primarily because both are applied areas by nature, governed by deep mathematical
foundations. it turns out, much of the pre-requisite math for machine learning (multi-variable calculus, linear
algebra) applies to physics too. another similarity between the two    elds is their philosophy and connection to the
real world. in both cases, we try to model the real world phenomena by coming up with hypothesis and backing
it up with experiments. while we might not be able to    t most of the happenings in the real world exactly, we
try to get as accurate as possible. this in turn leads us to a better understanding of the black-box that generates
events in the real world (or data in case of machine learning).

   imagine being in a battle   eld without knowing how to use the weapons you have, (or worse still, not having the
weapons at all)!   

that   s exactly how it feels when you set out to do research in machine learning without knowing enough about
the fundamental areas underlying it. without the right intuition, it becomes very hard to build new algorithms
or extend existing ones.

below are the key useful areas:

1.1 algorithms & complexity

knowledge of basic data structures such as arrays/trees/hash tables, programming techniques like dynammic pro-
gramming, graphs, space and time complexity requirements for a given method, randomized algorithms, sublinear

1

algorithms.

how do you convince someone that your learning algorithm is more space and/or time e   cient (and hence scalable)
on big datasets? exploiting sparsity in the data sets might lead you to better performing algorithms; but how do
you qualitatively compare their performance? with data getting more and more massive, going through it entirely
is not feasible and therefore sometimes even linear time algorithms maybe too slow. this has led to the    eld of
sub-linear algorithms which work by inspecting only a tiny fraction of the entire data. property testing is a closely
related topic where the algorithm queries about some property of the input with a time complexity much smaller
than the size of the input. these are relatively modern areas in theoretical computer science which have a direct
impact on machine learning. randomized algorithms is another highly useful    eld that has helped solve several big
data problems, for eg large matrix problems have been very successfully dealt with using randomization techniques.

1.2 id202

rank of a matrix, matrix vector products, column spaces and null spaces of a matrix, eigen values and vectors,
svd factorization of a matrix, positive-de   niteness of a matrix .

id202 plays a super heavy role in understanding optimization methods used for machine learning.
lets take an example to see how. many problems in machine learning can be expressed as a simple least-squares
optimization problem. what is interesting is every least-squares problem can be turned into a quadratic program
(ie, an optimization problem involving quadratic function of the variables). this is illustrated below 1:

f (x) =

=

=

(cid:107)qx     c(cid:107)2
(qx     c)t (qx     c)
(xt qt qx     xt qt c     ct qx + ct c)

1
2
1
2
1
2

(1)

since ct c is a    xed quantity (constant), it is su   cient to solve the quadratic programming problem:

(2)
, where a = qt q and q =    qt c. observe that eqn(2) has a matrix a in its    rst term. this matrix a is
the hessian (generalization of second-order derivatives to higher dimensions) and when it is positive-de   nite, the
quadratic problem takes shape of a    bowl    in higher dimensions as shown below in figure 1.

f (x) =

xt ax + qt x

1
2

figure 1: three dimensional    bowl    shaped function.

as a consequence, it clearly has only one    unique    minima (global optimal solution). this concept is termed
as    convexity    (which is considered a blessing for optimization as such functions are well-understood and have
some very attractive properties). some of these ideas constitute the nuts and bolts of id76. like-
wise, when the matrix a is negative-de   nite, the function takes the shape of an inverted bowl and is not convex
(therefore, has no global and unique minimum, but has a global maximum). finally, when matrix a is inde   nite,
then the function takes an interesting form as below in figure 2. as we can imagine, there is neither a global

1note that q is a matrix, while c and x are vectors. this way of writing functions in terms of vectors and matrices is called as

vectorizing. this is described in 1.4.

2

figure 2: three dimensional function representing the saddle point problem.

maximum or global minimum here. the surface infact looks like a    saddle    and for that reason called a saddle
point.
although the data we obtain in the real world has very large dimensions (hundreds of thousands for example), it
can often be reduced to a handful of useful dimensions that we can work with. this is called low-rank approx-
imation (rank of a data matrix determines the true dimensions of your data or how diverse your data actually
is). id105 methods are based on this and typical recommender systems like the one net   ix uses
to predict movie ratings of a user, make use of it. low-rank approximations are also used in other areas like
id161 and information retrieval as a tool for extracting correlations in data and removing noise from
matrix-structured data.

another interesting id84 algorithm is pca (principal components analysis). a simple
way to understand pca is to visualize a bunch of red and blue data points dispersed in a 3-d space (assume the
red points represent spam emails and blue represent legit emails). these points can be projected onto a 2-d surface
in di   erent ways (for eg: onto the xy plane, onto the yz plane or onto the xz plane). if we examine each of these
projections, we will realize that the distribution of points in each of them are slightly di   erent. some of these
projections separate the red and blue points better than the rest. the job of pca is to identity such projections
which yield maximal separation (or capture maximum variance in the data) as they are the most useful to us. and
how can we do this? well, that   s where eigen vectors and values come to our rescue. the principal eigen vector of
the covariance matrix (computed using the data points we have) corresponds to the axis that provides maximum
separation / variance in our data. pca is used widely in id161 and image compression, to mention a
few.

algorithms in machine learning involve dozens of vector-vector multiplications (dot-products) and matrix-vector,
matrix-id127s. all of these operations can be extremely costly and a bottleneck when trying to
scale to big data. however, if we can cleverly manipulate or take advantage of special matrices which contains lots
of zeros (   sparsity   ), we can reduce such computations signi   cantly.

1.3 id203 theory & statistics

    id203 theory: counting and combinatorial methods, bayes    theorem, random variables, expection,
variance, conditional and joint distributions, moment generating functions, exponential family of dis-
tributions

    statistics: id113, map, prior and posterior, sampling methods, gibbs

as you would expect, this is the single-most important    eld which also conveys the essence of machine learning,
namely - estimating the parameters of the data-generating process. several machine learning methods have prob-
abilistic interpretations and its common to hear the words frequentist and bayesian ways of doing things. one way
to look at the di   erence between them is that the frequentist methods are concerned with estimating the param-
eters of their model that have the highest chance of generating the    current data   ; this is called the maximum
likelihood estimation (id113) and written as:

argmax

  

log l(  ) = argmax

log p (data|p arameters)

  

(3)

id113 has the tendency to over   t (generalize poorly on unseen data) and hence the bayesian approach proposes
incorporating historical evidence (based on    past data   ) into the current model. this is called the prior. the task

3

of estimation then boils down to using bayes    rule as below:

p (p arameters|data) =

p (data|p arameters)p (p arameters)

p (data)

this can be equivalently written as :

p osterior =

likelihood    p rior
n ormalizingconstant

(4)

(5)

posterior gives us a id203 distribution over the parameters and this is used in various ways to make predic-
tions on the new data. as is evident by now, random variables play a huge part in estimation and we often deal
with independence assumptions between them, work with their expected values and variances. it is also important
to know the functional forms of some key id203 distributions, for instance the most popularly used gaussian
distribution (or normal distribution)     n (  ,   2), which (in its univariate form) can be expressed as:

(cid:26)

(cid:27)

   
1
2  

  

exp

    (x       )2

2  2

(6)

methods of sampling play an important role in optimization algorithms. often the gradient (generalization of
the derivative - this has been described in a bit more detail further down below) needs to be calculated over the
entire data set and this is very expensive to compute in every iteration. to avoid this, algorithms like stochastic
id119 (sgd) randomly sample a data point and update its gradient alone, this makes the algorithm
independent of the number of data points which means it will scale well. but, now we only selected speci   c data
points; so our method has lot of variance. how can we reduce this and make sure randomly selecting one point in
each iteration will in the long run mimic the same behavior that we would have got if we had selected all the data
points and computed the exact gradient? this is where techniques of designing unbiased sampling mechanisms
come up.

1.4 multi-variable calculus (matrix-vector calculus)

vector-valued functions, partial-derivatives, gradient, directional gradient, hessian, jacobian, laplacian, la-
grange multipliers

we know from our high-school calculus that derivatives represent the    rate of change    of the function at a given
point. and, in order to minimize or maximize a function you set its derivative equal to zero. such a way of
obtaining the solution is called the    closed-form    solution and this was an easy thing to do because back then,
our functions then involved just handful of dimensions (often just a single variable). however, in machine learning
its very common to deal with functions that operate on variables having hundreds of dimensions. what does
derivative or slope even mean in such situations? that   s where partial-derivatives come into the picture. you can
look at them as derivatives of the function in each dimension of the variable. combine these partial-derivatives into
a vector and that gives us what is called the    gradient   . similarly, taking the second-order derivative of gradient
gives us a matrix termed as the    hessian   . also, in reality it is often not possible to obtain a    closed-form    solution
(by setting the gradient to zero) because the dimensions of the gradient could be extremely high and storage/time
required for this might be expensive. in such cases, knowledge of gradients and hessians help us de   ne things like
directions of descent and rate of descent which tell us how should we travel in our function space in order to get to
the bottom most point (which represents the optimal solution). thus at this point, we work using more abstract
objects -    vectors    and    matrices    and should be reasonably comfortable with applying di   erential and integral
calculus on them. this is also called    vectorizing the equations   , which prof. andrew ng discusses in detail in
one of his ml lectures (refer coursera). below is an example of a simple objective function (id75 with
weights x, n data points and d dimensions) both in the vectorized and expanded notation.

expanded notation:

vectorized notation:

n(cid:88)

(

d(cid:88)

i=1

j=1

1
2

j(w) =

aijxj     bi)2

j(w) =

(cid:107)ax     b(cid:107)2

1
2

(7)

(8)

as you can see the vectorized notation is not only compact and gets rid of the annoying summations but also is
more expressive and makes us think more abstractly in terms of higher level objects like vectors (x and b) and

4

matrices (a). such representations also help us leverage all the rich properties that vectors and matrices provide
us (that speed up our computation and make things easily parallelizable, to mention a few). what this also means
is that, we need to now get comfortable with doing arithmetic in this new space. for instance, while moving terms
around, we cannot divide by a matrix and instead have to compute the inverse (at this point, i should also mention
that computing inverses of big matrices are expensive too and there are workarounds and techniques to deal with
this). the method of lagrange multipliers is a standard way in calculus to maximize or minimize functions when
there are constraints involved. this comes up a lot in optimization.

1.5 real analysis

properties of sets and sequences, convergence of sequence, cauchy sequences, concepts from topology like open
and closed sets, metric spaces, limits and continuity of functions, di   erentiability

while this is the most rigorous of all mathematical    elds i have mentioned, its importance is highly understated.
for a person in machine learning, (quoting a friend of mine in his exact words) real analysis is a long-term
investment. it gives you su   cient practice with the art of writing mathematics proofs in a rigorous manner and
making precise statements without gaps and holes in your arguments. this is an extremely useful skill to acquire
that helps you when writing and reading research papers. although intuition and pictures are often good ways to
describe and understand ideas; compressing them in a clear mathematical way is sometimes valuable as it avoids
ambiguity. analysis also helps one navigate more comfortably through some fundamental concepts in numerical
optimization such as - sequences, what does it mean for a sequence to converge to a limit point, how do you de   ne
the rate of convergence, what does it mean for a sequence or set to have a sup and inf ? while    elds of linear
algebra and multi-variate calculus help you design optimization methods for machine learning, tools from real
analysis will help you verify their correctness theoretically and provide guarantees on convergence, etc.

1.6

id205

id178, mutual information, information gain, kl divergence

this branch of applied mathematics deals with studying how to quantify information. id178 for example
quanti   es the uncertainty involved in predicting the value of a random variable and intersects with other    elds like
proability theory. kl-divergence is a widely used metric to measure how di   erent two id203 distributions
are. a simple example is to consider id90 which is a popular class   cation method in machine learning.
the way decision tree works is very similar to the famous twenty questions game, where the player of the game
is expected to guess a person   s name by asking twenty yes-no answer questions. if you have played this game
before, you will realize that the    rst few questions one would ask would be the ones such as -    is the person male
or female?   ,    is the person dead or alive?   ,    is the person real or animated character?   , etc as opposed more
speci   c ones like    did the person act in movie xyz?   ,    is the person tall?   , etc. this is because the former
set of questions divide your mental search space to the maximum, or in id205 terms - they provide
the maximum information gain (or highest reduction in id178). the computation that happens so rapidly in
a split-second in our brain is picking all possible paths in the decision tree (in our analogy of the game, this is
equivalent to considering all possible questions that can be asked), computing the reduction in id178 assuming
we had gone down that path and then picking the one that gave maximum reduction in id178. the mathematical
tools that are used to analyze all of this have roots in id205.

for the sake of completeness, let me mention that there are few other areas such as di   erential geometry and
measure theory which make cameo appearances once in a while, but the show is predominently run by the ones
above!

note: if you    nd any mistakes or have suggestions on ways i could improve this document please send me an
email at: params@ucsc.edu. thanks for reading!

5

