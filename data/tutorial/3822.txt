   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id202 cheat sheet for deep learning

beginner   s guide to commonly used operations

   [16]go to the profile of brendan fortuner
   [17]brendan fortuner (button) blockedunblock (button) followfollowing
   mar 4, 2017
   [1*xlzoldzjkt3pr-v969kexq.png]

   during jeremy howard   s excellent [18]deep learning course i realized i
   was a little rusty on the prerequisites and my fuzziness was impacting
   my ability to understand concepts like id26. i decided to
   put together a few wiki pages on these topics to improve my
   understanding. here is a very basic intro to some of the more common
   id202 operations used in deep learning. new: check out
   [19]machine learning cheatsheet for more topics.

what is id202?

   in the context of deep learning, id202 is a mathematical
   toolbox that offers helpful techniques for manipulating groups of
   numbers simultaneously. it provides structures like vectors and
   matrices (spreadsheets) to hold these numbers and new rules for how to
   add, subtract, multiply, and divide them.

why is it useful?

   it turns complicated problems into simple, intuitive, efficiently
   calculated problems. here is an example of how id202 can
   achieve greater speed and simplicity.
# multiply two arrays
x = [1,2,3]
y = [2,3,4]
product = []
for i in range(len(x)):
    product.append(x[i]*y[i])
# id202 version
x = numpy.array([1,2,3])
y = numpy.array([2,3,4])
x * y

   after initializing the arrays, the id202 approach was 3x
   faster.

how is it used in deep learning?

   neural networks store weights in matrices. id202 makes matrix
   operations fast and easy, especially when training on gpus. in fact,
   gpus were created with vector and matrix operations in mind. similar to
   how images can be represented as arrays of pixels, video games generate
   compelling gaming experiences using enormous, constantly evolving
   matrices. instead of processing pixels one-by-one, gpus manipulate
   entire matrices of pixels in parallel.

vectors

   vectors are 1-dimensional arrays of numbers or terms. in geometry,
   vectors store the magnitude and direction of a potential change to a
   point. the vector [3, -2] says go right 3 and down 2. a vector with
   more than one dimension is called a matrix.

vector notation

   there are a variety of ways to represent vectors. here are a few you
   might come across in your reading.
   [1*tkihyegdrapnowywfsuiia.png]

vectors in geometry

   vectors typically represent movement from a point. they store both the
   magnitude and direction of potential changes to a point. the vector
   [-2,5] says move left 2 units and up 5 units. [20]source.
   [1*kvtcbmg81-sut60g8vvckw.png]
   v = [-2, 5]

   a vector can be applied to any point in space. the vector   s direction
   equals the slope of the hypotenuse created moving up 5 and left 2. its
   magnitude equals the length of the hypotenuse.

scalar operations

   scalar operations involve a vector and a number. you modify the vector
   in-place by adding, subtracting, or multiplying the number from all the
   values in the vector.
   [1*g7cc0gka7xrkhoh-__ok3g.png]
   scalar addition

elementwise operations

   in elementwise operations like addition, subtraction, and division,
   values that correspond positionally are combined to produce a new
   vector. the 1st value in vector a is paired with the 1st value in
   vector b. the 2nd value is paired with the 2nd, and so on. this means
   the vectors must have equal dimensions to complete the operation.*
   [1*foayy9lvg1fpfihihm7z5q.png]
   vector addition
y = np.array([1,2,3])
x = np.array([2,3,4])
y + x = [3, 5, 7]
y - x = [-1, -1, -1]
y / x = [.5, .67, .75]

   *see below for details on [21]broadcasting in numpy.

vector multiplication

   there are two types of vector multiplication: dot product and hadamard
   product.

dot product

   the dot product of two vectors is a scalar. dot product of vectors and
   matrices (id127) is one of the most important
   operations in deep learning.
   [1*m5enbfhctmzed5ian-yddq.png]
y = np.array([1,2,3])
x = np.array([2,3,4])
np.dot(y,x) = 20

hadamard product

   hadamard product is elementwise multiplication and it outputs a vector.
   [1*pu5ds3vf0f6xvehzie-x6a.png]
y = np.array([1,2,3])
x = np.array([2,3,4])
y * x = [2, 6, 12]

vector fields

   a vector field shows how far the point (x,y) would hypothetically move
   if we applied a vector function to it like addition or multiplication.
   given a point in space, a vector field shows the power and direction of
   our proposed change at a variety of points in a graph.
   [1*rahajnrqkk9-rdt7gdt2hw.png]
   [22]source

   this vector field is an interesting one since it moves in different
   directions depending the starting point. the reason is that the vector
   behind this field stores terms like 2x or x   instead of scalar values
   like -2 and 5. for each point on the graph, we plug the x-coordinate
   into 2x or x   and draw an arrow from the starting point to the new
   location. vector fields are extremely useful for visualizing machine
   learning techniques like id119.

matrices

   a matrix is a rectangular grid of numbers or terms (like an excel
   spreadsheet) with special rules for addition, subtraction, and
   multiplication.

matrix dimensions

   we describe the dimensions of a matrix in terms of rows by columns.
   [1*x3av7yi3kkxrdn6hakkefa.png]
a = np.array([
 [1,2,3],
 [4,5,6]
])
a.shape == (2,3)
b = np.array([
 [1,2,3]
])
b.shape == (1,3)

matrix scalar operations

   scalar operations with matrices work the same way as they do for
   vectors. simply apply the scalar to every element in the matrix         add,
   subtract, divide, multiply, etc.
   [1*vcxvsa9nqxwqw2pck368-g.png]
   matrix scalar addition
a = np.array(
[[1,2],
 [3,4]])
a + 1
[[2,3],
 [4,5]]

matrix elementwise operations

   in order to add, subtract, or divide two matrices they must have equal
   dimensions.* we combine corresponding values in an elementwise fashion
   to produce a new matrix.
   [1*c2_v94g-epco4oritmulia.png]
a = np.array([
 [1,2],
 [3,4]
])
b = np.array([
 [1,2],
 [3,4]
])
a + b
[[2, 4],
 [6, 8]]
a     b
[[0, 0],
 [0, 0]]

numpy broadcasting*

   i can   t escape talking about this, since it   s very relevant in
   practice. in numpy the dimension requirements for elementwise
   operations are relaxed via a mechanism called [23]broadcasting. two
   matrices are compatible if the corresponding dimensions in each matrix
   (rows vs rows, columns vs columns) meet the following requirements:
    1. the dimensions are equal, or
    2. one dimension is of size 1

a = np.array([
 [1],
 [2]
])
b = np.array([
 [3,4],
 [5,6]
])
c = np.array([
 [1,2]
])
# same no. of rows
# different no. of columns
# but a has one column so this works
a * b
[[ 3, 4],
 [10, 12]]
# same no. of columns
# different no. of rows
# but c has one row so this works
b * c
[[ 3, 8],
 [5, 12]]
# different no. of columns
# different no. of rows
# but both a and c meet the
# size 1 requirement rule
a + c
[[2, 3],
 [3, 4]]

   things get weirder in higher dimensions         3d, 4d, but for now we won   t
   worry about that. understanding 2d operations is a good start.

matrix hadamard product

   hadamard product of matrices is an elementwise operation. values that
   correspond positionally are multiplied to produce a new matrix.
   [1*mmmlhitl5gipe036wwuweq.png]
a = np.array(
[[2,3],
 [2,3]])
b = np.array(
[[3,4],
 [5,6]])
# uses python's multiply operator
a * b
[[ 6, 12],
 [10, 18]]

   in numpy you can take the hadamard product of a matrix and vector as
   long as their dimensions meet the requirements of broadcasting.
   [1*ouldhfvcet9gcoulczs0bg.png]

matrix transpose

   neural networks frequently process weights and inputs of different
   sizes where the dimensions do not meet the requirements of matrix
   multiplication. matrix transpose provides a way to    rotate    one of the
   matrices so that the operation complies with multiplication
   requirements and can continue. there are two steps to transpose a
   matrix:
    1. rotate the matrix right 90  
    2. reverse the order of elements in each row (e.g. [a b c] becomes [c
       b a])

   as an example, transpose matrix m into t:
   [1*oamy1ih28nwkvjk8ienjag.png]
a = np.array([
   [1, 2],
   [3, 4]])
a.t
[[1, 3],
 [2, 4]]

id127

   id127 specifies a set of rules for multiplying matrices
   together to produce a new matrix.

rules

   not all matrices are eligible for multiplication. in addition, there is
   a requirement on the dimensions of the resulting matrix output.
   [24]source.
    1. the number of columns of the 1st matrix must equal the number of
       rows of the 2nd
    2. the product of an m x n matrix and an n x k matrix is an m x k
       matrix. the new matrix takes the rows of the 1st and columns of the
       2nd

steps

   id127 relies on dot product to multiply various
   combinations of rows and columns. in the image below, taken from khan
   academy   s excellent id202 course, each entry in matrix c is
   the dot product of a row in matrix a and a column in matrix b.
   [1*6vfyszrssgbvvy88ap_wza.png]
   [25]source

   the operation a1    b1 means we take the dot product of the 1st row in
   matrix a (1, 7) and the 1st column in matrix b (3, 5).
   [1*spsizsebvvrh8xqf9wsoew.png]

   here   s another way to look at it:
   [1*xu8eqqn9xx60uz7dlzy14q.png]

test yourself with these examples

   [1*-p3n80ucmqxm6e0euvqofq.png]

id127 with numpy

   numpy uses the function np.dot(a,b) for both vector and matrix
   multiplication. it has some other interesting features and gotchas so i
   encourage you to read the documentation [26]here before use.
a = np.array([
 [1, 2]
 ])
a.shape == (1,2)
b = np.array([
 [3, 4],
 [5, 6]
 ])
b.shape == (2,2)
# multiply
mm = np.dot(a,b)
mm == [13, 16]
mm.shape == (1,2)

tutorials

   [27]khan academy id202
   [28]deep learning book math section
   [29]andrew ng   s course notes
   [30]explanation of id202
   [31]explanation of matrices
   [32]intro to id202
   [33]mini reference id202 in 4 pages

     * [34]machine learning
     * [35]deep learning
     * [36]neural networks
     * [37]data science
     * [38]id202

   (button)
   (button)
   (button) 2.8k claps
   (button) (button) (button) 13 (button) (button)

     (button) blockedunblock (button) followfollowing
   [39]go to the profile of brendan fortuner

[40]brendan fortuner

   exploring. previously software [41]@amazon.

     (button) follow
   [42]towards data science

[43]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.8k
     * (button)
     *
     *

   [44]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [45]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/cd67aba4526c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_e4gawqdkt3w5---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@bfortuner?source=post_header_lockup
  17. https://towardsdatascience.com/@bfortuner
  18. http://course.fast.ai/
  19. http://ml-cheatsheet.readthedocs.io/
  20. http://mathinsight.org/vector_introduction
  21. https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html
  22. https://en.wikipedia.org/wiki/vector_field
  23. https://docs.scipy.org/doc/numpy-1.10.0/user/basics.broadcasting.html
  24. https://www.khanacademy.org/math/precalculus/precalc-matrices/properties-of-matrix-multiplication/a/properties-of-matrix-multiplication
  25. https://www.khanacademy.org/math/precalculus/precalc-matrices/properties-of-matrix-multiplication/a/properties-of-matrix-multiplication
  26. https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html
  27. https://www.khanacademy.org/math/linear-algebra
  28. http://www.deeplearningbook.org/contents/part_basics.html
  29. https://www.coursera.org/learn/machine-learning/resources/jxwws
  30. https://betterexplained.com/articles/linear-algebra-guide/
  31. http://blog.stata.com/2011/03/03/understanding-matrices-intuitively-part-1/
  32. http://www.holehouse.org/mlclass/03_linear_algebra_review.html
  33. https://minireference.com/static/tutorials/linear_algebra_in_4_pages.pdf
  34. https://towardsdatascience.com/tagged/machine-learning?source=post
  35. https://towardsdatascience.com/tagged/deep-learning?source=post
  36. https://towardsdatascience.com/tagged/neural-networks?source=post
  37. https://towardsdatascience.com/tagged/data-science?source=post
  38. https://towardsdatascience.com/tagged/linear-algebra?source=post
  39. https://towardsdatascience.com/@bfortuner?source=footer_card
  40. https://towardsdatascience.com/@bfortuner
  41. http://twitter.com/amazon
  42. https://towardsdatascience.com/?source=footer_card
  43. https://towardsdatascience.com/?source=footer_card
  44. https://towardsdatascience.com/
  45. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  47. https://medium.com/p/cd67aba4526c/share/twitter
  48. https://medium.com/p/cd67aba4526c/share/facebook
  49. https://medium.com/p/cd67aba4526c/share/twitter
  50. https://medium.com/p/cd67aba4526c/share/facebook
