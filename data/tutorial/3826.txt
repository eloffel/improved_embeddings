    #[1]index [2]search [3]deep learning for nlp with pytorch
   [4]generating names with a character-level id56

     * [5]get started
     * [6]features
     * [7]ecosystem
     * [8]blog
     * [9]tutorials
     * [10]docs
     * [11]resources
     * [12]github

   table of contents

   1.0.0.dev20190327
   ____________________

   getting started
     * [13]deep learning with pytorch: a 60 minute blitz
     * [14]data loading and processing tutorial
     * [15]learning pytorch with examples
     * [16]id21 tutorial
     * [17]deploying a id195 model with the hybrid frontend
     * [18]saving and loading models
     * [19]what is torch.nn really?

   image
     * [20]finetuning torchvision models
     * [21]spatial transformer networks tutorial
     * [22]neural transfer using pytorch
     * [23]adversarial example generation
     * [24]transfering a model from pytorch to caffe2 and mobile using
       onnx

   text
     * [25]chatbot tutorial
     * [26]generating names with a character-level id56
     * [27]classifying names with a character-level id56
     * [28]deep learning for nlp with pytorch
     * [29]translation with a sequence to sequence network and attention

   generative
     * [30]dcgan tutorial

   id23
     * [31]id23 (id25) tutorial

   extending pytorch
     * [32]creating extensions using numpy and scipy
     * [33]custom c++ and cuda extensions
     * [34]extending torchscript with custom c++ operators

   production usage
     * [35]writing distributed applications with pytorch
     * [36]pytorch 1.0 distributed trainer with amazon aws
     * [37]onnx live tutorial
     * [38]loading a pytorch model in c++

   pytorch in other languages
     * [39]using the pytorch c++ frontend

     * [40]tutorials >
     * classifying names with a character-level id56
     * [41][view-page-source-icon.svg]

   shortcuts

   intermediate/char_id56_classification_tutorial
   [pytorch-colab.svg]
   run in google colab
   colab
   [pytorch-download.svg]
   download notebook
   notebook
   [pytorch-github.svg]
   view on github
   github

   note

   click [42]here to download the full example code

classifying names with a character-level id56[43]  

   author: [44]sean robertson

   we will be building and training a basic character-level id56 to
   classify words. a character-level id56 reads words as a series of
   characters - outputting a prediction and    hidden state    at each step,
   feeding its previous hidden state into each next step. we take the
   final prediction to be the output, i.e. which class the word belongs
   to.

   specifically, we   ll train on a few thousand surnames from 18 languages
   of origin, and predict which language a name is from based on the
   spelling:
$ python predict.py hinton
(-0.47) scottish
(-1.52) english
(-3.57) irish

$ python predict.py schmidhuber
(-0.19) german
(-2.48) czech
(-2.68) dutch

   recommended reading:

   i assume you have at least installed pytorch, know python, and
   understand tensors:
     * [45]https://pytorch.org/ for installation instructions
     * [46]deep learning with pytorch: a 60 minute blitz to get started
       with pytorch in general
     * [47]learning pytorch with examples for a wide and deep overview
     * [48]pytorch for former torch users if you are former lua torch user

   it would also be useful to know about id56s and how they work:
     * [49]the unreasonable effectiveness of recurrent neural networks
       shows a bunch of real life examples
     * [50]understanding id137 is about lstms specifically but
       also informative about id56s in general

preparing the data[51]  

   note

   download the data from [52]here and extract it to the current
   directory.

   included in the data/names directory are 18 text files named as
      [language].txt   . each file contains a bunch of names, one name per
   line, mostly romanized (but we still need to convert from unicode to
   ascii).

   we   ll end up with a dictionary of lists of names per language,
   {language: [names ...]}. the generic variables    category    and    line   
   (for language and name in our case) are used for later extensibility.
from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os

def findfiles(path): return glob.glob(path)

print(findfiles('data/names/*.txt'))

import unicodedata
import string

all_letters = string.ascii_letters + " .,;'"
n_letters = len(all_letters)

# turn a unicode string to plain ascii, thanks to https://stackoverflow.com/a/51
8232/2809427
def unicodetoascii(s):
    return ''.join(
        c for c in unicodedata.normalize('nfd', s)
        if unicodedata.category(c) != 'mn'
        and c in all_letters
    )

print(unicodetoascii('  lus  rski'))

# build the category_lines dictionary, a list of names per language
category_lines = {}
all_categories = []

# read a file and split into lines
def readlines(filename):
    lines = open(filename, encoding='utf-8').read().strip().split('\n')
    return [unicodetoascii(line) for line in lines]

for filename in findfiles('data/names/*.txt'):
    category = os.path.splitext(os.path.basename(filename))[0]
    all_categories.append(category)
    lines = readlines(filename)
    category_lines[category] = lines

n_categories = len(all_categories)

   out:
['data/names/italian.txt', 'data/names/german.txt', 'data/names/portuguese.txt',
 'data/names/chinese.txt', 'data/names/greek.txt', 'data/names/polish.txt', 'dat
a/names/french.txt', 'data/names/english.txt', 'data/names/spanish.txt', 'data/n
ames/arabic.txt', 'data/names/czech.txt', 'data/names/russian.txt', 'data/names/
irish.txt', 'data/names/dutch.txt', 'data/names/scottish.txt', 'data/names/vietn
amese.txt', 'data/names/korean.txt', 'data/names/japanese.txt']
slusarski

   now we have category_lines, a dictionary mapping each category
   (language) to a list of lines (names). we also kept track of
   all_categories (just a list of languages) and n_categories for later
   reference.
print(category_lines['italian'][:5])

   out:
['abandonato', 'abatangelo', 'abatantuono', 'abate', 'abategiovanni']

turning names into tensors[53]  

   now that we have all the names organized, we need to turn them into
   tensors to make any use of them.

   to represent a single letter, we use a    one-hot vector    of size <1 x
   n_letters>. a one-hot vector is filled with 0s except for a 1 at index
   of the current letter, e.g. "b" = <0 1 0 0 0 ...>.

   to make a word we join a bunch of those into a 2d matrix <line_length x
   1 x n_letters>.

   that extra 1 dimension is because pytorch assumes everything is in
   batches - we   re just using a batch size of 1 here.
import torch

# find letter index from all_letters, e.g. "a" = 0
def lettertoindex(letter):
    return all_letters.find(letter)

# just for demonstration, turn a letter into a <1 x n_letters> tensor
def lettertotensor(letter):
    tensor = torch.zeros(1, n_letters)
    tensor[0][lettertoindex(letter)] = 1
    return tensor

# turn a line into a <line_length x 1 x n_letters>,
# or an array of one-hot letter vectors
def linetotensor(line):
    tensor = torch.zeros(len(line), 1, n_letters)
    for li, letter in enumerate(line):
        tensor[li][0][lettertoindex(letter)] = 1
    return tensor

print(lettertotensor('j'))

print(linetotensor('jones').size())

   out:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0.]])
torch.size([5, 1, 57])

creating the network[54]  

   before autograd, creating a recurrent neural network in torch involved
   cloning the parameters of a layer over several timesteps. the layers
   held hidden state and gradients which are now entirely handled by the
   graph itself. this means you can implement a id56 in a very    pure    way,
   as regular feed-forward layers.

   this id56 module (mostly copied from [55]the pytorch for torch users
   tutorial) is just 2 linear layers which operate on an input and hidden
   state, with a logsoftmax layer after the output.
import torch.nn as nn

class id56(nn.module):
    def __init__(self, input_size, hidden_size, output_size):
        super(id56, self).__init__()

        self.hidden_size = hidden_size

        self.i2h = nn.linear(input_size + hidden_size, hidden_size)
        self.i2o = nn.linear(input_size + hidden_size, output_size)
        self.softmax = nn.logsoftmax(dim=1)

    def forward(self, input, hidden):
        combined = torch.cat((input, hidden), 1)
        hidden = self.i2h(combined)
        output = self.i2o(combined)
        output = self.softmax(output)
        return output, hidden

    def inithidden(self):
        return torch.zeros(1, self.hidden_size)

n_hidden = 128
id56 = id56(n_letters, n_hidden, n_categories)

   to run a step of this network we need to pass an input (in our case,
   the tensor for the current letter) and a previous hidden state (which
   we initialize as zeros at first). we   ll get back the output
   (id203 of each language) and a next hidden state (which we keep
   for the next step).
input = lettertotensor('a')
hidden =torch.zeros(1, n_hidden)

output, next_hidden = id56(input, hidden)

   for the sake of efficiency we don   t want to be creating a new tensor
   for every step, so we will use linetotensor instead of lettertotensor
   and use slices. this could be further optimized by pre-computing
   batches of tensors.
input = linetotensor('albert')
hidden = torch.zeros(1, n_hidden)

output, next_hidden = id56(input[0], hidden)
print(output)

   out:
tensor([[-2.8990, -2.8312, -2.9197, -2.9691, -2.8530, -2.8449, -2.9409, -2.7743,
         -2.8792, -2.8569, -2.9854, -2.9551, -2.8959, -2.8061, -3.0197, -2.9014,
         -2.8016, -2.9324]], grad_fn=<logsoftmaxbackward>)

   as you can see the output is a <1 x n_categories> tensor, where every
   item is the likelihood of that category (higher is more likely).

training[56]  

preparing for training[57]  

   before going into training we should make a few helper functions. the
   first is to interpret the output of the network, which we know to be a
   likelihood of each category. we can use tensor.topk to get the index of
   the greatest value:
def categoryfromoutput(output):
    top_n, top_i = output.topk(1)
    category_i = top_i[0].item()
    return all_categories[category_i], category_i

print(categoryfromoutput(output))

   out:
('english', 7)

   we will also want a quick way to get a training example (a name and its
   language):
import random

def randomchoice(l):
    return l[random.randint(0, len(l) - 1)]

def randomtrainingexample():
    category = randomchoice(all_categories)
    line = randomchoice(category_lines[category])
    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch
.long)
    line_tensor = linetotensor(line)
    return category, line, category_tensor, line_tensor

for i in range(10):
    category, line, category_tensor, line_tensor = randomtrainingexample()
    print('category =', category, '/ line =', line)

   out:
category = greek / line = close
category = vietnamese / line = luong
category = korean / line = ngai
category = vietnamese / line = duong
category = english / line = brewer
category = italian / line = salvatici
category = french / line = cavey
category = russian / line = badov
category = arabic / line = srour
category = irish / line = o'boyle

training the network[58]  

   now all it takes to train this network is show it a bunch of examples,
   have it make guesses, and tell it if it   s wrong.

   for the id168 nn.nllloss is appropriate, since the last layer
   of the id56 is nn.logsoftmax.
criterion = nn.nllloss()

   each loop of training will:
     * create input and target tensors
     * create a zeroed initial hidden state
     * read each letter in and
          + keep hidden state for next letter
     * compare final output to target
     * back-propagate
     * return the output and loss

learning_rate = 0.005 # if you set this too high, it might explode. if too low,
it might not learn

def train(category_tensor, line_tensor):
    hidden = id56.inithidden()

    id56.zero_grad()

    for i in range(line_tensor.size()[0]):
        output, hidden = id56(line_tensor[i], hidden)

    loss = criterion(output, category_tensor)
    loss.backward()

    # add parameters' gradients to their values, multiplied by learning rate
    for p in id56.parameters():
        p.data.add_(-learning_rate, p.grad.data)

    return output, loss.item()

   now we just have to run that with a bunch of examples. since the train
   function returns both the output and loss we can print its guesses and
   also keep track of loss for plotting. since there are 1000s of examples
   we print only every print_every examples, and take an average of the
   loss.
import time
import math

n_iters = 100000
print_every = 5000
plot_every = 1000



# keep track of losses for plotting
current_loss = 0
all_losses = []

def timesince(since):
    now = time.time()
    s = now - since
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)

start = time.time()

for iter in range(1, n_iters + 1):
    category, line, category_tensor, line_tensor = randomtrainingexample()
    output, loss = train(category_tensor, line_tensor)
    current_loss += loss

    # print iter number, loss, name and guess
    if iter % print_every == 0:
        guess, guess_i = categoryfromoutput(output)
        correct = '   ' if guess == category else '    (%s)' % category
        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, time
since(start), loss, line, guess, correct))

    # add current loss avg to list of losses
    if iter % plot_every == 0:
        all_losses.append(current_loss / plot_every)
        current_loss = 0

   out:
5000 5% (0m 11s) 2.9297 mazza / japanese     (italian)
10000 10% (0m 20s) 1.2793 yim / korean    
15000 15% (0m 28s) 1.4265 ahn / chinese     (korean)
20000 20% (0m 36s) 0.5924 mifsud / arabic    
25000 25% (0m 45s) 2.7584 carideo / spanish     (italian)
30000 30% (0m 53s) 0.6188 jagubsky / russian    
35000 35% (1m 1s) 0.7560 kosmatka / polish    
40000 40% (1m 9s) 0.4309 kamisaka / japanese    
45000 45% (1m 18s) 0.4546 pantelas / greek    
50000 50% (1m 26s) 1.9925 russo / italian    
55000 55% (1m 34s) 3.4536 alexander / irish     (scottish)
60000 60% (1m 43s) 1.9083 krawiec / polish     (czech)
65000 65% (1m 51s) 2.1582 ramecker / scottish     (dutch)
70000 70% (1m 59s) 0.7531 naser / arabic    
75000 75% (2m 8s) 0.9218 evangelista / italian    
80000 80% (2m 16s) 1.1249 johnston / english     (scottish)
85000 85% (2m 24s) 0.0361 sadowski / polish    
90000 90% (2m 32s) 1.8066 forsyth / german     (english)
95000 95% (2m 41s) 0.7888 nigro / italian    
100000 100% (2m 49s) 2.6297 venne / korean     (dutch)

plotting the results[59]  

   plotting the historical loss from all_losses shows the network
   learning:
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

plt.figure()
plt.plot(all_losses)

   ../_images/sphx_glr_char_id56_classification_tutorial_001.png

evaluating the results[60]  

   to see how well the network performs on different categories, we will
   create a confusion matrix, indicating for every actual language (rows)
   which language the network guesses (columns). to calculate the
   confusion matrix a bunch of samples are run through the network with
   evaluate(), which is the same as train() minus the backprop.
# keep track of correct guesses in a confusion matrix
confusion = torch.zeros(n_categories, n_categories)
n_confusion = 10000

# just return an output given a line
def evaluate(line_tensor):
    hidden = id56.inithidden()

    for i in range(line_tensor.size()[0]):
        output, hidden = id56(line_tensor[i], hidden)

    return output

# go through a bunch of examples and record which are correctly guessed
for i in range(n_confusion):
    category, line, category_tensor, line_tensor = randomtrainingexample()
    output = evaluate(line_tensor)
    guess, guess_i = categoryfromoutput(output)
    category_i = all_categories.index(category)
    confusion[category_i][guess_i] += 1

# normalize by dividing every row by its sum
for i in range(n_categories):
    confusion[i] = confusion[i] / confusion[i].sum()

# set up plot
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(confusion.numpy())
fig.colorbar(cax)

# set up axes
ax.set_xticklabels([''] + all_categories, rotation=90)
ax.set_yticklabels([''] + all_categories)

# force label at every tick
ax.xaxis.set_major_locator(ticker.multiplelocator(1))
ax.yaxis.set_major_locator(ticker.multiplelocator(1))

# sphinx_gallery_thumbnail_number = 2
plt.show()

   ../_images/sphx_glr_char_id56_classification_tutorial_002.png

   you can pick out bright spots off the main axis that show which
   languages it guesses incorrectly, e.g. chinese for korean, and spanish
   for italian. it seems to do very well with greek, and very poorly with
   english (perhaps because of overlap with other languages).

running on user input[61]  

def predict(input_line, n_predictions=3):
    print('\n> %s' % input_line)
    with torch.no_grad():
        output = evaluate(linetotensor(input_line))

        # get top n categories
        topv, topi = output.topk(n_predictions, 1, true)
        predictions = []

        for i in range(n_predictions):
            value = topv[0][i].item()
            category_index = topi[0][i].item()
            print('(%.2f) %s' % (value, all_categories[category_index]))
            predictions.append([value, all_categories[category_index]])

predict('dovesky')
predict('jackson')
predict('satoshi')

   out:
> dovesky
(-0.84) russian
(-1.03) czech
(-2.85) english

> jackson
(-0.36) scottish
(-2.18) english
(-2.95) russian

> satoshi
(-1.29) italian
(-1.93) japanese
(-2.03) scottish

   the final versions of the scripts [62]in the practical pytorch repo
   split the above code into a few files:
     * data.py (loads files)
     * model.py (defines the id56)
     * train.py (runs training)
     * predict.py (runs predict() with command line arguments)
     * server.py (serve prediction as a json api with bottle.py)

   run train.py to train and save the network.

   run predict.py with a name to view predictions:
$ python predict.py hazaki
(-0.42) japanese
(-1.39) polish
(-3.51) czech

   run server.py and visit [63]http://localhost:5533/yourname to get json
   output of predictions.

exercises[64]  

     * try with a different dataset of line -> category, for example:
          + any word -> language
          + first name -> gender
          + character name -> writer
          + page title -> blog or subreddit
     * get better results with a bigger and/or better shaped network
          + add more linear layers
          + try the nn.lstm and nn.gru layers
          + combine multiple of these id56s as a higher level network

   total running time of the script: ( 2 minutes 58.459 seconds)
   [65]download python source code: char_id56_classification_tutorial.py
   [66]download jupyter notebook: char_id56_classification_tutorial.ipynb

   [67]gallery generated by sphinx-gallery

   [68]next [chevron-right-orange.svg] [69][chevron-right-orange.svg]
   previous
     __________________________________________________________________

   was this helpful?
   yes
   no
   thank you
     __________________________________________________________________

      copyright 2017, pytorch.
   built with [70]sphinx using a [71]theme provided by [72]read the docs.
     * [73]classifying names with a character-level id56
          + [74]preparing the data
               o [75]turning names into tensors
          + [76]creating the network
          + [77]training
               o [78]preparing for training
               o [79]training the network
               o [80]plotting the results
          + [81]evaluating the results
               o [82]running on user input
          + [83]exercises

   [tr?id=243028289693773&amp;ev=pageview &amp;noscript=1]

docs

   access comprehensive developer documentation for pytorch
   [84]view docs

tutorials

   get in-depth tutorials for beginners and advanced developers
   [85]view tutorials

resources

   find development resources and get your questions answered
   [86]view resources

     * [87]pytorch
     * [88]get started
     * [89]features
     * [90]ecosystem
     * [91]blog
     * [92]resources

     * [93]support
     * [94]tutorials
     * [95]docs
     * [96]discuss
     * [97]github issues
     * [98]slack
     * [99]contributing

     * follow us
     * email address ____________________
       ____________________
       submit

   to analyze traffic and optimize your experience, we serve cookies on
   this site. by clicking or navigating, you agree to allow our usage of
   cookies. as the current maintainers of this site, facebook   s cookies
   policy applies. learn more, including about available controls:
   [100]cookies policy.
   [pytorch-x.svg]

     * [101]get started
     * [102]features
     * [103]ecosystem
     * [104]blog
     * [105]tutorials
     * [106]docs
     * [107]resources
     * [108]github

references

   visible links
   1. https://pytorch.org/tutorials/genindex.html
   2. https://pytorch.org/tutorials/search.html
   3. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
   4. https://pytorch.org/tutorials/intermediate/char_id56_generation_tutorial.html
   5. https://pytorch.org/get-started
   6. https://pytorch.org/features
   7. https://pytorch.org/ecosystem
   8. https://pytorch.org/blog/
   9. https://pytorch.org/tutorials
  10. https://pytorch.org/docs/stable/index.html
  11. https://pytorch.org/resources
  12. https://github.com/pytorch/pytorch
  13. https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
  14. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
  15. https://pytorch.org/tutorials/beginner/pytorch_with_examples.html
  16. https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html
  17. https://pytorch.org/tutorials/beginner/deploy_id195_hybrid_frontend_tutorial.html
  18. https://pytorch.org/tutorials/beginner/saving_loading_models.html
  19. https://pytorch.org/tutorials/beginner/nn_tutorial.html
  20. https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html
  21. https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html
  22. https://pytorch.org/tutorials/advanced/neural_style_tutorial.html
  23. https://pytorch.org/tutorials/beginner/fgsm_tutorial.html
  24. https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html
  25. https://pytorch.org/tutorials/beginner/chatbot_tutorial.html
  26. https://pytorch.org/tutorials/intermediate/char_id56_generation_tutorial.html
  27. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
  28. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
  29. https://pytorch.org/tutorials/intermediate/id195_translation_tutorial.html
  30. https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
  31. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
  32. https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html
  33. https://pytorch.org/tutorials/advanced/cpp_extension.html
  34. https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html
  35. https://pytorch.org/tutorials/intermediate/dist_tuto.html
  36. https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html
  37. https://pytorch.org/tutorials/advanced/onnxlive.html
  38. https://pytorch.org/tutorials/advanced/cpp_export.html
  39. https://pytorch.org/tutorials/advanced/cpp_frontend.html
  40. https://pytorch.org/tutorials/index.html
  41. https://pytorch.org/tutorials/_sources/intermediate/char_id56_classification_tutorial.rst.txt
  42. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#sphx-glr-download-intermediate-char-id56-classification-tutorial-py
  43. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#classifying-names-with-a-character-level-id56
  44. https://github.com/spro/practical-pytorch
  45. https://pytorch.org/
  46. https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
  47. https://pytorch.org/tutorials/beginner/pytorch_with_examples.html
  48. https://pytorch.org/tutorials/beginner/former_torchies_tutorial.html
  49. https://karpathy.github.io/2015/05/21/id56-effectiveness/
  50. https://colah.github.io/posts/2015-08-understanding-lstms/
  51. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#preparing-the-data
  52. https://download.pytorch.org/tutorial/data.zip
  53. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#turning-names-into-tensors
  54. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#creating-the-network
  55. https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net
  56. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#training
  57. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#preparing-for-training
  58. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#training-the-network
  59. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#plotting-the-results
  60. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#evaluating-the-results
  61. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#running-on-user-input
  62. https://github.com/spro/practical-pytorch/tree/master/char-id56-classification
  63. http://localhost:5533/yourname
  64. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#exercises
  65. https://pytorch.org/tutorials/_downloads/ccb15f8365bdae22a0a019e57216d7c6/char_id56_classification_tutorial.py
  66. https://pytorch.org/tutorials/_downloads/977c14818c75427641ccb85ad21ed6dc/char_id56_classification_tutorial.ipynb
  67. https://sphinx-gallery.readthedocs.io/
  68. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
  69. https://pytorch.org/tutorials/intermediate/char_id56_generation_tutorial.html
  70. http://sphinx-doc.org/
  71. https://github.com/rtfd/sphinx_rtd_theme
  72. https://readthedocs.org/
  73. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
  74. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#preparing-the-data
  75. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#turning-names-into-tensors
  76. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#creating-the-network
  77. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#training
  78. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#preparing-for-training
  79. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#training-the-network
  80. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#plotting-the-results
  81. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#evaluating-the-results
  82. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#running-on-user-input
  83. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html#exercises
  84. https://pytorch.org/docs/stable/index.html
  85. https://pytorch.org/tutorials
  86. https://pytorch.org/resources
  87. https://pytorch.org/
  88. https://pytorch.org/get-started
  89. https://pytorch.org/features
  90. https://pytorch.org/ecosystem
  91. https://pytorch.org/blog/
  92. https://pytorch.org/resources
  93. https://pytorch.org/support
  94. https://pytorch.org/tutorials
  95. https://pytorch.org/docs/stable/index.html
  96. https://discuss.pytorch.org/
  97. https://github.com/pytorch/pytorch/issues
  98. https://pytorch.slack.com/
  99. https://github.com/pytorch/pytorch/blob/master/contributing.md
 100. https://www.facebook.com/policies/cookies/
 101. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
 102. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
 103. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
 104. https://pytorch.org/blog/
 105. https://pytorch.org/tutorials
 106. https://pytorch.org/docs/stable/index.html
 107. https://pytorch.org/resources
 108. https://github.com/pytorch/pytorch

   hidden links:
 110. https://pytorch.org/
 111. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
 112. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
 113. https://pytorch.org/
 114. https://www.facebook.com/pytorch
 115. https://twitter.com/pytorch
 116. https://pytorch.org/
 117. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
