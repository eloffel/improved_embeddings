    #[1]index [2]search [3]deep learning with pytorch [4]deep learning for
   nlp with pytorch

     * [5]get started
     * [6]features
     * [7]ecosystem
     * [8]blog
     * [9]tutorials
     * [10]docs
     * [11]resources
     * [12]github

   table of contents

   1.0.0.dev20190327
   ____________________

   getting started
     * [13]deep learning with pytorch: a 60 minute blitz
     * [14]data loading and processing tutorial
     * [15]learning pytorch with examples
     * [16]id21 tutorial
     * [17]deploying a id195 model with the hybrid frontend
     * [18]saving and loading models
     * [19]what is torch.nn really?

   image
     * [20]finetuning torchvision models
     * [21]spatial transformer networks tutorial
     * [22]neural transfer using pytorch
     * [23]adversarial example generation
     * [24]transfering a model from pytorch to caffe2 and mobile using
       onnx

   text
     * [25]chatbot tutorial
     * [26]generating names with a character-level id56
     * [27]classifying names with a character-level id56
     * [28]deep learning for nlp with pytorch
     * [29]translation with a sequence to sequence network and attention

   generative
     * [30]dcgan tutorial

   id23
     * [31]id23 (id25) tutorial

   extending pytorch
     * [32]creating extensions using numpy and scipy
     * [33]custom c++ and cuda extensions
     * [34]extending torchscript with custom c++ operators

   production usage
     * [35]writing distributed applications with pytorch
     * [36]pytorch 1.0 distributed trainer with amazon aws
     * [37]onnx live tutorial
     * [38]loading a pytorch model in c++

   pytorch in other languages
     * [39]using the pytorch c++ frontend

     * [40]tutorials >
     * [41]deep learning for nlp with pytorch >
     * introduction to pytorch
     * [42][view-page-source-icon.svg]

   shortcuts

   beginner/nlp/pytorch_tutorial
   [pytorch-colab.svg]
   run in google colab
   colab
   [pytorch-download.svg]
   download notebook
   notebook
   [pytorch-github.svg]
   view on github
   github

   note

   click [43]here to download the full example code

introduction to pytorch[44]  

introduction to torch   s tensor library[45]  

   all of deep learning is computations on tensors, which are
   generalizations of a matrix that can be indexed in more than 2
   dimensions. we will see exactly what this means in-depth later. first,
   lets look what we can do with tensors.
# author: robert guthrie

import torch
import torch.autograd as autograd
import torch.nn as nn
import torch.nn.functional as f
import torch.optim as optim

torch.manual_seed(1)

creating tensors[46]  

   tensors can be created from python lists with the torch.tensor()
   function.
# torch.tensor(data) creates a torch.tensor object with the given data.
v_data = [1., 2., 3.]
v = torch.tensor(v_data)
print(v)

# creates a matrix
m_data = [[1., 2., 3.], [4., 5., 6]]
m = torch.tensor(m_data)
print(m)

# create a 3d tensor of size 2x2x2.
t_data = [[[1., 2.], [3., 4.]],
          [[5., 6.], [7., 8.]]]
t = torch.tensor(t_data)
print(t)

   out:
tensor([1., 2., 3.])
tensor([[1., 2., 3.],
        [4., 5., 6.]])
tensor([[[1., 2.],
         [3., 4.]],

        [[5., 6.],
         [7., 8.]]])

   what is a 3d tensor anyway? think about it like this. if you have a
   vector, indexing into the vector gives you a scalar. if you have a
   matrix, indexing into the matrix gives you a vector. if you have a 3d
   tensor, then indexing into the tensor gives you a matrix!

   a note on terminology: when i say    tensor    in this tutorial, it refers
   to any torch.tensor object. matrices and vectors are special cases of
   torch.tensors, where their dimension is 1 and 2 respectively. when i am
   talking about 3d tensors, i will explicitly use the term    3d tensor   .
# index into v and get a scalar (0 dimensional tensor)
print(v[0])
# get a python number from it
print(v[0].item())

# index into m and get a vector
print(m[0])

# index into t and get a matrix
print(t[0])

   out:
tensor(1.)
1.0
tensor([1., 2., 3.])
tensor([[1., 2.],
        [3., 4.]])

   you can also create tensors of other datatypes. the default, as you can
   see, is float. to create a tensor of integer types, try
   torch.longtensor(). check the documentation for more data types, but
   float and long will be the most common.

   you can create a tensor with random data and the supplied
   dimensionality with torch.randn()
x = torch.randn((3, 4, 5))
print(x)

   out:
tensor([[[-1.5256, -0.7502, -0.6540, -1.6095, -0.1002],
         [-0.6092, -0.9798, -1.6091, -0.7121,  0.3037],
         [-0.7773, -0.2515, -0.2223,  1.6871,  0.2284],
         [ 0.4676, -0.6970, -1.1608,  0.6995,  0.1991]],

        [[ 0.8657,  0.2444, -0.6629,  0.8073,  1.1017],
         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177],
         [-0.7981, -0.1316,  1.8793, -0.0721,  0.1578],
         [-0.7735,  0.1991,  0.0457,  0.1530, -0.4757]],

        [[-0.1110,  0.2927, -0.1578, -0.0288,  0.4533],
         [ 1.1422,  0.2486, -1.7754, -0.0255, -1.0233],
         [-0.5962, -1.0055,  0.4285,  1.4761, -1.7869],
         [ 1.6103, -0.7040, -0.1853, -0.9962, -0.8313]]])

operations with tensors[47]  

   you can operate on tensors in the ways you would expect.
x = torch.tensor([1., 2., 3.])
y = torch.tensor([4., 5., 6.])
z = x + y
print(z)

   out:
tensor([5., 7., 9.])

   see [48]the documentation for a complete list of the massive number of
   operations available to you. they expand beyond just mathematical
   operations.

   one helpful operation that we will make use of later is concatenation.
# by default, it concatenates along the first axis (concatenates rows)
x_1 = torch.randn(2, 5)
y_1 = torch.randn(3, 5)
z_1 = torch.cat([x_1, y_1])
print(z_1)

# concatenate columns:
x_2 = torch.randn(2, 3)
y_2 = torch.randn(2, 5)
# second arg specifies which axis to concat along
z_2 = torch.cat([x_2, y_2], 1)
print(z_2)

# if your tensors are not compatible, torch will complain.  uncomment to see the
 error
# torch.cat([x_1, x_2])

   out:
tensor([[-0.8029,  0.2366,  0.2857,  0.6898, -0.6331],
        [ 0.8795, -0.6842,  0.4533,  0.2912, -0.8317],
        [-0.5525,  0.6355, -0.3968, -0.6571, -1.6428],
        [ 0.9803, -0.0421, -0.8206,  0.3133, -1.1352],
        [ 0.3773, -0.2824, -2.5667, -1.4303,  0.5009]])
tensor([[ 0.5438, -0.4057,  1.1341, -0.1473,  0.6272,  1.0935,  0.0939,  1.2381]
,
        [-1.1115,  0.3501, -0.7703, -1.3459,  0.5119, -0.6933, -0.1668, -0.9999]
])

reshaping tensors[49]  

   use the .view() method to reshape a tensor. this method receives heavy
   use, because many neural network components expect their inputs to have
   a certain shape. often you will need to reshape before passing your
   data to the component.
x = torch.randn(2, 3, 4)
print(x)
print(x.view(2, 12))  # reshape to 2 rows, 12 columns
# same as above.  if one of the dimensions is -1, its size can be inferred
print(x.view(2, -1))

   out:
tensor([[[ 0.4175, -0.2127, -0.8400, -0.4200],
         [-0.6240, -0.9773,  0.8748,  0.9873],
         [-0.0594, -2.4919,  0.2423,  0.2883]],

        [[-0.1095,  0.3126,  1.5038,  0.5038],
         [ 0.6223, -0.4481, -0.2856,  0.3880],
         [-1.1435, -0.6512, -0.1032,  0.6937]]])
tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,
         -0.0594, -2.4919,  0.2423,  0.2883],
        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,
         -1.1435, -0.6512, -0.1032,  0.6937]])
tensor([[ 0.4175, -0.2127, -0.8400, -0.4200, -0.6240, -0.9773,  0.8748,  0.9873,
         -0.0594, -2.4919,  0.2423,  0.2883],
        [-0.1095,  0.3126,  1.5038,  0.5038,  0.6223, -0.4481, -0.2856,  0.3880,
         -1.1435, -0.6512, -0.1032,  0.6937]])

computation graphs and automatic differentiation[50]  

   the concept of a computation graph is essential to efficient deep
   learning programming, because it allows you to not have to write the
   back propagation gradients yourself. a computation graph is simply a
   specification of how your data is combined to give you the output.
   since the graph totally specifies what parameters were involved with
   which operations, it contains enough information to compute
   derivatives. this probably sounds vague, so let   s see what is going on
   using the fundamental flag requires_grad.

   first, think from a programmers perspective. what is stored in the
   torch.tensor objects we were creating above? obviously the data and the
   shape, and maybe a few other things. but when we added two tensors
   together, we got an output tensor. all this output tensor knows is its
   data and shape. it has no idea that it was the sum of two other tensors
   (it could have been read in from a file, it could be the result of some
   other operation, etc.)

   if requires_grad=true, the tensor object keeps track of how it was
   created. lets see it in action.
# tensor factory methods have a ``requires_grad`` flag
x = torch.tensor([1., 2., 3], requires_grad=true)

# with requires_grad=true, you can still do all the operations you previously
# could
y = torch.tensor([4., 5., 6], requires_grad=true)
z = x + y
print(z)

# but z knows something extra.
print(z.grad_fn)

   out:
tensor([5., 7., 9.], grad_fn=<addbackward0>)
<addbackward0 object at 0x7fe4f4c1c278>

   so tensors know what created them. z knows that it wasn   t read in from
   a file, it wasn   t the result of a multiplication or exponential or
   whatever. and if you keep following z.grad_fn, you will find yourself
   at x and y.

   but how does that help us compute a gradient?
# lets sum up all the entries in z
s = z.sum()
print(s)
print(s.grad_fn)

   out:
tensor(21., grad_fn=<sumbackward0>)
<sumbackward0 object at 0x7fe4f4c6fcc0>

   so now, what is the derivative of this sum with respect to the first
   component of x? in math, we want
   \[\frac{\partial s}{\partial x_0}\]

   well, s knows that it was created as a sum of the tensor z. z knows
   that it was the sum x + y. so
   \[s = \overbrace{x_0 + y_0}^\text{$z_0$} + \overbrace{x_1 +
   y_1}^\text{$z_1$} + \overbrace{x_2 + y_2}^\text{$z_2$}\]

   and so s contains enough information to determine that the derivative
   we want is 1!

   of course this glosses over the challenge of how to actually compute
   that derivative. the point here is that s is carrying along enough
   information that it is possible to compute it. in reality, the
   developers of pytorch program the sum() and + operations to know how to
   compute their gradients, and run the back propagation algorithm. an
   in-depth discussion of that algorithm is beyond the scope of this
   tutorial.

   lets have pytorch compute the gradient, and see that we were right:
   (note if you run this block multiple times, the gradient will
   increment. that is because pytorch accumulates the gradient into the
   .grad property, since for many models this is very convenient.)
# calling .backward() on any variable will run backprop, starting from it.
s.backward()
print(x.grad)

   out:
tensor([1., 1., 1.])

   understanding what is going on in the block below is crucial for being
   a successful programmer in deep learning.
x = torch.randn(2, 2)
y = torch.randn(2, 2)
# by default, user created tensors have ``requires_grad=false``
print(x.requires_grad, y.requires_grad)
z = x + y
# so you can't backprop through z
print(z.grad_fn)

# ``.requires_grad_( ... )`` changes an existing tensor's ``requires_grad``
# flag in-place. the input flag defaults to ``true`` if not given.
x = x.requires_grad_()
y = y.requires_grad_()
# z contains enough information to compute gradients, as we saw above
z = x + y
print(z.grad_fn)
# if any input to an operation has ``requires_grad=true``, so will the output
print(z.requires_grad)

# now z has the computation history that relates itself to x and y
# can we just take its values, and **detach** it from its history?
new_z = z.detach()

# ... does new_z have information to backprop to x and y?
# no!
print(new_z.grad_fn)
# and how could it? ``z.detach()`` returns a tensor that shares the same storage
# as ``z``, but with the computation history forgotten. it doesn't know anything
# about how it was computed.
# in essence, we have broken the tensor away from its past history

   out:
false false
none
<addbackward0 object at 0x7fe4f4c1c978>
true
none

   you can also stop autograd from tracking history on tensors with
   .requires_grad``=true by wrapping the code block in ``with
   torch.no_grad():
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)

   out:
true
true
false

   total running time of the script: ( 0 minutes 3.483 seconds)
   [51]download python source code: pytorch_tutorial.py
   [52]download jupyter notebook: pytorch_tutorial.ipynb

   [53]gallery generated by sphinx-gallery

   [54]next [chevron-right-orange.svg] [55][chevron-right-orange.svg]
   previous
     __________________________________________________________________

   was this helpful?
   yes
   no
   thank you
     __________________________________________________________________

      copyright 2017, pytorch.
   built with [56]sphinx using a [57]theme provided by [58]read the docs.
     * [59]introduction to pytorch
          + [60]introduction to torch   s tensor library
               o [61]creating tensors
               o [62]operations with tensors
               o [63]reshaping tensors
          + [64]computation graphs and automatic differentiation

   [tr?id=243028289693773&amp;ev=pageview &amp;noscript=1]

docs

   access comprehensive developer documentation for pytorch
   [65]view docs

tutorials

   get in-depth tutorials for beginners and advanced developers
   [66]view tutorials

resources

   find development resources and get your questions answered
   [67]view resources

     * [68]pytorch
     * [69]get started
     * [70]features
     * [71]ecosystem
     * [72]blog
     * [73]resources

     * [74]support
     * [75]tutorials
     * [76]docs
     * [77]discuss
     * [78]github issues
     * [79]slack
     * [80]contributing

     * follow us
     * email address ____________________
       ____________________
       submit

   to analyze traffic and optimize your experience, we serve cookies on
   this site. by clicking or navigating, you agree to allow our usage of
   cookies. as the current maintainers of this site, facebook   s cookies
   policy applies. learn more, including about available controls:
   [81]cookies policy.
   [pytorch-x.svg]

     * [82]get started
     * [83]features
     * [84]ecosystem
     * [85]blog
     * [86]tutorials
     * [87]docs
     * [88]resources
     * [89]github

references

   visible links
   1. https://pytorch.org/tutorials/genindex.html
   2. https://pytorch.org/tutorials/search.html
   3. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
   4. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
   5. https://pytorch.org/get-started
   6. https://pytorch.org/features
   7. https://pytorch.org/ecosystem
   8. https://pytorch.org/blog/
   9. https://pytorch.org/tutorials
  10. https://pytorch.org/docs/stable/index.html
  11. https://pytorch.org/resources
  12. https://github.com/pytorch/pytorch
  13. https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
  14. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
  15. https://pytorch.org/tutorials/beginner/pytorch_with_examples.html
  16. https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html
  17. https://pytorch.org/tutorials/beginner/deploy_id195_hybrid_frontend_tutorial.html
  18. https://pytorch.org/tutorials/beginner/saving_loading_models.html
  19. https://pytorch.org/tutorials/beginner/nn_tutorial.html
  20. https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html
  21. https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html
  22. https://pytorch.org/tutorials/advanced/neural_style_tutorial.html
  23. https://pytorch.org/tutorials/beginner/fgsm_tutorial.html
  24. https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html
  25. https://pytorch.org/tutorials/beginner/chatbot_tutorial.html
  26. https://pytorch.org/tutorials/intermediate/char_id56_generation_tutorial.html
  27. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
  28. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
  29. https://pytorch.org/tutorials/intermediate/id195_translation_tutorial.html
  30. https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
  31. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
  32. https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html
  33. https://pytorch.org/tutorials/advanced/cpp_extension.html
  34. https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html
  35. https://pytorch.org/tutorials/intermediate/dist_tuto.html
  36. https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html
  37. https://pytorch.org/tutorials/advanced/onnxlive.html
  38. https://pytorch.org/tutorials/advanced/cpp_export.html
  39. https://pytorch.org/tutorials/advanced/cpp_frontend.html
  40. https://pytorch.org/tutorials/index.html
  41. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
  42. https://pytorch.org/tutorials/_sources/beginner/nlp/pytorch_tutorial.rst.txt
  43. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#sphx-glr-download-beginner-nlp-pytorch-tutorial-py
  44. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#introduction-to-pytorch
  45. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#introduction-to-torch-s-tensor-library
  46. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#creating-tensors
  47. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#operations-with-tensors
  48. https://pytorch.org/docs/torch.html
  49. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#reshaping-tensors
  50. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#computation-graphs-and-automatic-differentiation
  51. https://pytorch.org/tutorials/_downloads/fea971ce34a8a2aa290ac3b68d066acc/pytorch_tutorial.py
  52. https://pytorch.org/tutorials/_downloads/7e00f82429a7b691e42bb5828dbb036e/pytorch_tutorial.ipynb
  53. https://sphinx-gallery.readthedocs.io/
  54. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
  55. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
  56. http://sphinx-doc.org/
  57. https://github.com/rtfd/sphinx_rtd_theme
  58. https://readthedocs.org/
  59. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
  60. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#introduction-to-torch-s-tensor-library
  61. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#creating-tensors
  62. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#operations-with-tensors
  63. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#reshaping-tensors
  64. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html#computation-graphs-and-automatic-differentiation
  65. https://pytorch.org/docs/stable/index.html
  66. https://pytorch.org/tutorials
  67. https://pytorch.org/resources
  68. https://pytorch.org/
  69. https://pytorch.org/get-started
  70. https://pytorch.org/features
  71. https://pytorch.org/ecosystem
  72. https://pytorch.org/blog/
  73. https://pytorch.org/resources
  74. https://pytorch.org/support
  75. https://pytorch.org/tutorials
  76. https://pytorch.org/docs/stable/index.html
  77. https://discuss.pytorch.org/
  78. https://github.com/pytorch/pytorch/issues
  79. https://pytorch.slack.com/
  80. https://github.com/pytorch/pytorch/blob/master/contributing.md
  81. https://www.facebook.com/policies/cookies/
  82. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
  83. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
  84. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
  85. https://pytorch.org/blog/
  86. https://pytorch.org/tutorials
  87. https://pytorch.org/docs/stable/index.html
  88. https://pytorch.org/resources
  89. https://github.com/pytorch/pytorch

   hidden links:
  91. https://pytorch.org/
  92. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
  93. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
  94. https://pytorch.org/
  95. https://www.facebook.com/pytorch
  96. https://twitter.com/pytorch
  97. https://pytorch.org/
  98. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
