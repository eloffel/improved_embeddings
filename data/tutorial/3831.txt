    #[1]index [2]search [3]id27s: encoding lexical semantics
   [4]introduction to pytorch

     * [5]get started
     * [6]features
     * [7]ecosystem
     * [8]blog
     * [9]tutorials
     * [10]docs
     * [11]resources
     * [12]github

   table of contents

   1.0.0.dev20190327
   ____________________

   getting started
     * [13]deep learning with pytorch: a 60 minute blitz
     * [14]data loading and processing tutorial
     * [15]learning pytorch with examples
     * [16]id21 tutorial
     * [17]deploying a id195 model with the hybrid frontend
     * [18]saving and loading models
     * [19]what is torch.nn really?

   image
     * [20]finetuning torchvision models
     * [21]spatial transformer networks tutorial
     * [22]neural transfer using pytorch
     * [23]adversarial example generation
     * [24]transfering a model from pytorch to caffe2 and mobile using
       onnx

   text
     * [25]chatbot tutorial
     * [26]generating names with a character-level id56
     * [27]classifying names with a character-level id56
     * [28]deep learning for nlp with pytorch
     * [29]translation with a sequence to sequence network and attention

   generative
     * [30]dcgan tutorial

   id23
     * [31]id23 (id25) tutorial

   extending pytorch
     * [32]creating extensions using numpy and scipy
     * [33]custom c++ and cuda extensions
     * [34]extending torchscript with custom c++ operators

   production usage
     * [35]writing distributed applications with pytorch
     * [36]pytorch 1.0 distributed trainer with amazon aws
     * [37]onnx live tutorial
     * [38]loading a pytorch model in c++

   pytorch in other languages
     * [39]using the pytorch c++ frontend

     * [40]tutorials >
     * [41]deep learning for nlp with pytorch >
     * deep learning with pytorch
     * [42][view-page-source-icon.svg]

   shortcuts

   beginner/nlp/deep_learning_tutorial
   [pytorch-colab.svg]
   run in google colab
   colab
   [pytorch-download.svg]
   download notebook
   notebook
   [pytorch-github.svg]
   view on github
   github

   note

   click [43]here to download the full example code

deep learning with pytorch[44]  

deep learning building blocks: affine maps, non-linearities and
objectives[45]  

   deep learning consists of composing linearities with non-linearities in
   clever ways. the introduction of non-linearities allows for powerful
   models. in this section, we will play with these core components, make
   up an objective function, and see how the model is trained.

affine maps[46]  

   one of the core workhorses of deep learning is the affine map, which is
   a function \(f(x)\) where
   \[f(x) = ax + b\]

   for a matrix \(a\) and vectors \(x, b\). the parameters to be learned
   here are \(a\) and \(b\). often, \(b\) is refered to as the bias term.

   pytorch and most other deep learning frameworks do things a little
   differently than traditional id202. it maps the rows of the
   input instead of the columns. that is, the \(i\)   th row of the output
   below is the mapping of the \(i\)   th row of the input under \(a\), plus
   the bias term. look at the example below.
# author: robert guthrie

import torch
import torch.nn as nn
import torch.nn.functional as f
import torch.optim as optim

torch.manual_seed(1)

lin = nn.linear(5, 3)  # maps from r^5 to r^3, parameters a, b
# data is 2x5.  a maps from 5 to 3... can we map "data" under a?
data = torch.randn(2, 5)
print(lin(data))  # yes

   out:
tensor([[ 0.1755, -0.3268, -0.5069],
        [-0.6602,  0.2260,  0.1089]], grad_fn=<addmmbackward>)

non-linearities[47]  

   first, note the following fact, which will explain why we need
   non-linearities in the first place. suppose we have two affine maps
   \(f(x) = ax + b\) and \(g(x) = cx + d\). what is \(f(g(x))\)?
   \[f(g(x)) = a(cx + d) + b = acx + (ad + b)\]

   \(ac\) is a matrix and \(ad + b\) is a vector, so we see that composing
   affine maps gives you an affine map.

   from this, you can see that if you wanted your neural network to be
   long chains of affine compositions, that this adds no new power to your
   model than just doing a single affine map.

   if we introduce non-linearities in between the affine layers, this is
   no longer the case, and we can build much more powerful models.

   there are a few core non-linearities. \(\tanh(x), \sigma(x),
   \text{relu}(x)\) are the most common. you are probably wondering:    why
   these functions? i can think of plenty of other non-linearities.    the
   reason for this is that they have gradients that are easy to compute,
   and computing gradients is essential for learning. for example
   \[\frac{d\sigma}{dx} = \sigma(x)(1 - \sigma(x))\]

   a quick note: although you may have learned some neural networks in
   your intro to ai class where \(\sigma(x)\) was the default
   non-linearity, typically people shy away from it in practice. this is
   because the gradient vanishes very quickly as the absolute value of the
   argument grows. small gradients means it is hard to learn. most people
   default to tanh or relu.
# in pytorch, most non-linearities are in torch.functional (we have it imported
as f)
# note that non-linearites typically don't have parameters like affine maps do.
# that is, they don't have weights that are updated during training.
data = torch.randn(2, 2)
print(data)
print(f.relu(data))

   out:
tensor([[-0.5404, -2.2102],
        [ 2.1130, -0.0040]])
tensor([[0.0000, 0.0000],
        [2.1130, 0.0000]])

softmax and probabilities[48]  

   the function \(\text{softmax}(x)\) is also just a non-linearity, but it
   is special in that it usually is the last operation done in a network.
   this is because it takes in a vector of real numbers and returns a
   id203 distribution. its definition is as follows. let \(x\) be a
   vector of real numbers (positive, negative, whatever, there are no
   constraints). then the i   th component of \(\text{softmax}(x)\) is
   \[\frac{\exp(x_i)}{\sum_j \exp(x_j)}\]

   it should be clear that the output is a id203 distribution: each
   element is non-negative and the sum over all components is 1.

   you could also think of it as just applying an element-wise
   exponentiation operator to the input to make everything non-negative
   and then dividing by the id172 constant.
# softmax is also in torch.nn.functional
data = torch.randn(5)
print(data)
print(f.softmax(data, dim=0))
print(f.softmax(data, dim=0).sum())  # sums to 1 because it is a distribution!
print(f.log_softmax(data, dim=0))  # theres also log_softmax

   out:
tensor([ 1.3800, -1.3505,  0.3455,  0.5046,  1.8213])
tensor([0.2948, 0.0192, 0.1048, 0.1228, 0.4584])
tensor(1.)
tensor([-1.2214, -3.9519, -2.2560, -2.0969, -0.7801])

objective functions[49]  

   the objective function is the function that your network is being
   trained to minimize (in which case it is often called a id168
   or cost function). this proceeds by first choosing a training instance,
   running it through your neural network, and then computing the loss of
   the output. the parameters of the model are then updated by taking the
   derivative of the id168. intuitively, if your model is
   completely confident in its answer, and its answer is wrong, your loss
   will be high. if it is very confident in its answer, and its answer is
   correct, the loss will be low.

   the idea behind minimizing the id168 on your training examples
   is that your network will hopefully generalize well and have small loss
   on unseen examples in your dev set, test set, or in production. an
   example id168 is the negative log likelihood loss, which is a
   very common objective for multi-class classification. for supervised
   multi-class classification, this means training the network to minimize
   the negative log id203 of the correct output (or equivalently,
   maximize the log id203 of the correct output).

optimization and training[50]  

   so what we can compute a id168 for an instance? what do we do
   with that? we saw earlier that tensors know how to compute gradients
   with respect to the things that were used to compute it. well, since
   our loss is an tensor, we can compute gradients with respect to all of
   the parameters used to compute it! then we can perform standard
   gradient updates. let \(\theta\) be our parameters, \(l(\theta)\) the
   id168, and \(\eta\) a positive learning rate. then:
   \[\theta^{(t+1)} = \theta^{(t)} - \eta \nabla_\theta l(\theta)\]

   there are a huge collection of algorithms and active research in
   attempting to do something more than just this vanilla gradient update.
   many attempt to vary the learning rate based on what is happening at
   train time. you don   t need to worry about what specifically these
   algorithms are doing unless you are really interested. torch provides
   many in the torch.optim package, and they are all completely
   transparent. using the simplest gradient update is the same as the more
   complicated algorithms. trying different update algorithms and
   different parameters for the update algorithms (like different initial
   learning rates) is important in optimizing your network   s performance.
   often, just replacing vanilla sgd with an optimizer like adam or
   rmsprop will boost performance noticably.

creating network components in pytorch[51]  

   before we move on to our focus on nlp, lets do an annotated example of
   building a network in pytorch using only affine maps and
   non-linearities. we will also see how to compute a id168, using
   pytorch   s built in negative log likelihood, and update parameters by
   id26.

   all network components should inherit from nn.module and override the
   forward() method. that is about it, as far as the boilerplate is
   concerned. inheriting from nn.module provides functionality to your
   component. for example, it makes it keep track of its trainable
   parameters, you can swap it between cpu and gpu with the .to(device)
   method, where device can be a cpu device torch.device("cpu") or cuda
   device torch.device("cuda:0").

   let   s write an annotated example of a network that takes in a sparse
   bag-of-words representation and outputs a id203 distribution over
   two labels:    english    and    spanish   . this model is just logistic
   regression.

example: id28 bag-of-words classifier[52]  

   our model will map a sparse bow representation to log probabilities
   over labels. we assign each word in the vocab an index. for example,
   say our entire vocab is two words    hello    and    world   , with indices 0
   and 1 respectively. the bow vector for the sentence    hello hello hello
   hello    is
   \[\left[ 4, 0 \right]\]

   for    hello world world hello   , it is
   \[\left[ 2, 2 \right]\]

   etc. in general, it is
   \[\left[ \text{count}(\text{hello}), \text{count}(\text{world})
   \right]\]

   denote this bow vector as \(x\). the output of our network is:
   \[\log \text{softmax}(ax + b)\]

   that is, we pass the input through an affine map and then do log
   softmax.
data = [("me gusta comer en la cafeteria".split(), "spanish"),
        ("give it to me".split(), "english"),
        ("no creo que sea una buena idea".split(), "spanish"),
        ("no it is not a good idea to get lost at sea".split(), "english")]

test_data = [("yo creo que si".split(), "spanish"),
             ("it is lost on me".split(), "english")]

# word_to_ix maps each word in the vocab to a unique integer, which will be its
# index into the bag of words vector
word_to_ix = {}
for sent, _ in data + test_data:
    for word in sent:
        if word not in word_to_ix:
            word_to_ix[word] = len(word_to_ix)
print(word_to_ix)

vocab_size = len(word_to_ix)
num_labels = 2


class bowclassifier(nn.module):  # inheriting from nn.module!

    def __init__(self, num_labels, vocab_size):
        # calls the init function of nn.module.  dont get confused by syntax,
        # just always do it in an nn.module
        super(bowclassifier, self).__init__()

        # define the parameters that you will need.  in this case, we need a and
 b,
        # the parameters of the affine mapping.
        # torch defines nn.linear(), which provides the affine map.
        # make sure you understand why the input dimension is vocab_size
        # and the output is num_labels!
        self.linear = nn.linear(vocab_size, num_labels)

        # note! the non-linearity log softmax does not have parameters! so we do
n't need
        # to worry about that here

    def forward(self, bow_vec):
        # pass the input through the linear layer,
        # then pass that through log_softmax.
        # many non-linearities and other functions are in torch.nn.functional
        return f.log_softmax(self.linear(bow_vec), dim=1)


def make_bow_vector(sentence, word_to_ix):
    vec = torch.zeros(len(word_to_ix))
    for word in sentence:
        vec[word_to_ix[word]] += 1
    return vec.view(1, -1)


def make_target(label, label_to_ix):
    return torch.longtensor([label_to_ix[label]])


model = bowclassifier(num_labels, vocab_size)

# the model knows its parameters.  the first output below is a, the second is b.
# whenever you assign a component to a class variable in the __init__ function
# of a module, which was done with the line
# self.linear = nn.linear(...)
# then through some python magic from the pytorch devs, your module
# (in this case, bowclassifier) will store knowledge of the nn.linear's paramete
rs
for param in model.parameters():
    print(param)

# to run the model, pass in a bow vector
# here we don't need to train, so the code is wrapped in torch.no_grad()
with torch.no_grad():
    sample = data[0]
    bow_vector = make_bow_vector(sample[0], word_to_ix)
    log_probs = model(bow_vector)
    print(log_probs)

   out:
{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'give': 6, '
it': 7, 'to': 8, 'no': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena':
14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21,
 'at': 22, 'yo': 23, 'si': 24, 'on': 25}
parameter containing:
tensor([[ 0.1194,  0.0609, -0.1268,  0.1274,  0.1191,  0.1739, -0.1099, -0.0323,
         -0.0038,  0.0286, -0.1488, -0.1392,  0.1067, -0.0460,  0.0958,  0.0112,
          0.0644,  0.0431,  0.0713,  0.0972, -0.1816,  0.0987, -0.1379, -0.1480,
          0.0119, -0.0334],
        [ 0.1152, -0.1136, -0.1743,  0.1427, -0.0291,  0.1103,  0.0630, -0.1471,
          0.0394,  0.0471, -0.1313, -0.0931,  0.0669,  0.0351, -0.0834, -0.0594,
          0.1796, -0.0363,  0.1106,  0.0849, -0.1268, -0.1668,  0.1882,  0.0102,
          0.1344,  0.0406]], requires_grad=true)
parameter containing:
tensor([0.0631, 0.1465], requires_grad=true)
tensor([[-0.5378, -0.8771]])

   which of the above values corresponds to the log id203 of
   english, and which to spanish? we never defined it, but we need to if
   we want to train the thing.
label_to_ix = {"spanish": 0, "english": 1}

   so lets train! to do this, we pass instances through to get log
   probabilities, compute a id168, compute the gradient of the
   id168, and then update the parameters with a gradient step.
   id168s are provided by torch in the nn package. nn.nllloss() is
   the negative log likelihood loss we want. it also defines optimization
   functions in torch.optim. here, we will just use sgd.

   note that the input to nllloss is a vector of log probabilities, and a
   target label. it doesn   t compute the log probabilities for us. this is
   why the last layer of our network is log softmax. the id168
   nn.crossid178loss() is the same as nllloss(), except it does the log
   softmax for you.
# run on test data before we train, just to see a before-and-after
with torch.no_grad():
    for instance, label in test_data:
        bow_vec = make_bow_vector(instance, word_to_ix)
        log_probs = model(bow_vec)
        print(log_probs)

# print the matrix column corresponding to "creo"
print(next(model.parameters())[:, word_to_ix["creo"]])

loss_function = nn.nllloss()
optimizer = optim.sgd(model.parameters(), lr=0.1)

# usually you want to pass over the training data several times.
# 100 is much bigger than on a real data set, but real datasets have more than
# two instances.  usually, somewhere between 5 and 30 epochs is reasonable.
for epoch in range(100):
    for instance, label in data:
        # step 1. remember that pytorch accumulates gradients.
        # we need to clear them out before each instance
        model.zero_grad()

        # step 2. make our bow vector and also we must wrap the target in a
        # tensor as an integer. for example, if the target is spanish, then
        # we wrap the integer 0. the id168 then knows that the 0th
        # element of the log probabilities is the log id203
        # corresponding to spanish
        bow_vec = make_bow_vector(instance, word_to_ix)
        target = make_target(label, label_to_ix)

        # step 3. run our forward pass.
        log_probs = model(bow_vec)

        # step 4. compute the loss, gradients, and update the parameters by
        # calling optimizer.step()
        loss = loss_function(log_probs, target)
        loss.backward()
        optimizer.step()

with torch.no_grad():
    for instance, label in test_data:
        bow_vec = make_bow_vector(instance, word_to_ix)
        log_probs = model(bow_vec)
        print(log_probs)

# index corresponding to spanish goes up, english goes down!
print(next(model.parameters())[:, word_to_ix["creo"]])

   out:
tensor([[-0.9297, -0.5020]])
tensor([[-0.6388, -0.7506]])
tensor([-0.1488, -0.1313], grad_fn=<selectbackward>)
tensor([[-0.2093, -1.6669]])
tensor([[-2.5330, -0.0828]])
tensor([ 0.2803, -0.5605], grad_fn=<selectbackward>)

   we got the right answer! you can see that the log id203 for
   spanish is much higher in the first example, and the log id203
   for english is much higher in the second for the test data, as it
   should be.

   now you see how to make a pytorch component, pass some data through it
   and do gradient updates. we are ready to dig deeper into what deep nlp
   has to offer.

   total running time of the script: ( 0 minutes 3.753 seconds)
   [53]download python source code: deep_learning_tutorial.py
   [54]download jupyter notebook: deep_learning_tutorial.ipynb

   [55]gallery generated by sphinx-gallery

   [56]next [chevron-right-orange.svg] [57][chevron-right-orange.svg]
   previous
     __________________________________________________________________

   was this helpful?
   yes
   no
   thank you
     __________________________________________________________________

      copyright 2017, pytorch.
   built with [58]sphinx using a [59]theme provided by [60]read the docs.
     * [61]deep learning with pytorch
          + [62]deep learning building blocks: affine maps,
            non-linearities and objectives
               o [63]affine maps
               o [64]non-linearities
               o [65]softmax and probabilities
               o [66]objective functions
          + [67]optimization and training
          + [68]creating network components in pytorch
               o [69]example: id28 bag-of-words classifier

   [tr?id=243028289693773&amp;ev=pageview &amp;noscript=1]

docs

   access comprehensive developer documentation for pytorch
   [70]view docs

tutorials

   get in-depth tutorials for beginners and advanced developers
   [71]view tutorials

resources

   find development resources and get your questions answered
   [72]view resources

     * [73]pytorch
     * [74]get started
     * [75]features
     * [76]ecosystem
     * [77]blog
     * [78]resources

     * [79]support
     * [80]tutorials
     * [81]docs
     * [82]discuss
     * [83]github issues
     * [84]slack
     * [85]contributing

     * follow us
     * email address ____________________
       ____________________
       submit

   to analyze traffic and optimize your experience, we serve cookies on
   this site. by clicking or navigating, you agree to allow our usage of
   cookies. as the current maintainers of this site, facebook   s cookies
   policy applies. learn more, including about available controls:
   [86]cookies policy.
   [pytorch-x.svg]

     * [87]get started
     * [88]features
     * [89]ecosystem
     * [90]blog
     * [91]tutorials
     * [92]docs
     * [93]resources
     * [94]github

references

   visible links
   1. https://pytorch.org/tutorials/genindex.html
   2. https://pytorch.org/tutorials/search.html
   3. https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html
   4. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
   5. https://pytorch.org/get-started
   6. https://pytorch.org/features
   7. https://pytorch.org/ecosystem
   8. https://pytorch.org/blog/
   9. https://pytorch.org/tutorials
  10. https://pytorch.org/docs/stable/index.html
  11. https://pytorch.org/resources
  12. https://github.com/pytorch/pytorch
  13. https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
  14. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html
  15. https://pytorch.org/tutorials/beginner/pytorch_with_examples.html
  16. https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html
  17. https://pytorch.org/tutorials/beginner/deploy_id195_hybrid_frontend_tutorial.html
  18. https://pytorch.org/tutorials/beginner/saving_loading_models.html
  19. https://pytorch.org/tutorials/beginner/nn_tutorial.html
  20. https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html
  21. https://pytorch.org/tutorials/intermediate/spatial_transformer_tutorial.html
  22. https://pytorch.org/tutorials/advanced/neural_style_tutorial.html
  23. https://pytorch.org/tutorials/beginner/fgsm_tutorial.html
  24. https://pytorch.org/tutorials/advanced/super_resolution_with_caffe2.html
  25. https://pytorch.org/tutorials/beginner/chatbot_tutorial.html
  26. https://pytorch.org/tutorials/intermediate/char_id56_generation_tutorial.html
  27. https://pytorch.org/tutorials/intermediate/char_id56_classification_tutorial.html
  28. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
  29. https://pytorch.org/tutorials/intermediate/id195_translation_tutorial.html
  30. https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html
  31. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
  32. https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html
  33. https://pytorch.org/tutorials/advanced/cpp_extension.html
  34. https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html
  35. https://pytorch.org/tutorials/intermediate/dist_tuto.html
  36. https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html
  37. https://pytorch.org/tutorials/advanced/onnxlive.html
  38. https://pytorch.org/tutorials/advanced/cpp_export.html
  39. https://pytorch.org/tutorials/advanced/cpp_frontend.html
  40. https://pytorch.org/tutorials/index.html
  41. https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html
  42. https://pytorch.org/tutorials/_sources/beginner/nlp/deep_learning_tutorial.rst.txt
  43. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#sphx-glr-download-beginner-nlp-deep-learning-tutorial-py
  44. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#deep-learning-with-pytorch
  45. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives
  46. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#affine-maps
  47. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#non-linearities
  48. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#softmax-and-probabilities
  49. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#objective-functions
  50. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#optimization-and-training
  51. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#creating-network-components-in-pytorch
  52. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier
  53. https://pytorch.org/tutorials/_downloads/da3e7a6653a5652ca1dd43be72d0ac2d/deep_learning_tutorial.py
  54. https://pytorch.org/tutorials/_downloads/755bf5a1deed0d3ab50f96bf7ca4ec7a/deep_learning_tutorial.ipynb
  55. https://sphinx-gallery.readthedocs.io/
  56. https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html
  57. https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html
  58. http://sphinx-doc.org/
  59. https://github.com/rtfd/sphinx_rtd_theme
  60. https://readthedocs.org/
  61. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
  62. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#deep-learning-building-blocks-affine-maps-non-linearities-and-objectives
  63. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#affine-maps
  64. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#non-linearities
  65. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#softmax-and-probabilities
  66. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#objective-functions
  67. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#optimization-and-training
  68. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#creating-network-components-in-pytorch
  69. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html#example-logistic-regression-bag-of-words-classifier
  70. https://pytorch.org/docs/stable/index.html
  71. https://pytorch.org/tutorials
  72. https://pytorch.org/resources
  73. https://pytorch.org/
  74. https://pytorch.org/get-started
  75. https://pytorch.org/features
  76. https://pytorch.org/ecosystem
  77. https://pytorch.org/blog/
  78. https://pytorch.org/resources
  79. https://pytorch.org/support
  80. https://pytorch.org/tutorials
  81. https://pytorch.org/docs/stable/index.html
  82. https://discuss.pytorch.org/
  83. https://github.com/pytorch/pytorch/issues
  84. https://pytorch.slack.com/
  85. https://github.com/pytorch/pytorch/blob/master/contributing.md
  86. https://www.facebook.com/policies/cookies/
  87. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
  88. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
  89. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
  90. https://pytorch.org/blog/
  91. https://pytorch.org/tutorials
  92. https://pytorch.org/docs/stable/index.html
  93. https://pytorch.org/resources
  94. https://github.com/pytorch/pytorch

   hidden links:
  96. https://pytorch.org/
  97. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
  98. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
  99. https://pytorch.org/
 100. https://www.facebook.com/pytorch
 101. https://twitter.com/pytorch
 102. https://pytorch.org/
 103. https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html
