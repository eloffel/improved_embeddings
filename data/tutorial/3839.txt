   #[1]pavel surmenok    feed [2]pavel surmenok    comments feed [3]pavel
   surmenok    how to run text summarization with tensorflow comments feed
   [4]google assistant bot platform [5]natural language pipeline for
   chatbots

[6]pavel surmenok

menu

   [7]skip to content
     * [8]home

how to run text summarization with tensorflow

   text summarization problem has many useful applications. if you run a
   website, you can create titles and short summaries for user generated
   content. if you want to read a lot of articles and don   t have time to
   do that, your virtual assistant can summarize main points from these
   articles for you.

   it is not an easy problem to solve. there are [9]multiple approaches,
   including various supervised and unsupervised algorithms. some
   algorithms rank the importance of sentences within the text and then
   construct a summary out of important sentences, others are end-to-end
   generative models.

   end-to-end machine learning algorithms are interesting to try. after
   all, end-to-end algorithms demonstrate good results in other areas,
   like image recognition, id103, language translation, and
   even question-answering.

   image credit:
   [10]https://research.googleblog.com/2015/11/computer-respond-to-this-em
   ail.html

text summarization with tensorflow

   in august 2016, peter liu and xin pan, software engineers on google
   brain team, published a blog post    [11]text summarization with
   tensorflow   . their algorithm is extracting interesting parts of the
   text and create a summary by using these parts of the text and allow
   for rephrasings to make summary more grammatically correct. this
   approach is called abstractive summarization.

   peter and xin trained a text summarization model to produce headlines
   for news articles, using [12]annotated english gigaword, a dataset
   often used in summarization research. the dataset contains about 10
   million documents. the model was trained end-to-end with a deep
   learning technique called [13]sequence-to-sequence learning.

   [14]code for training and testing the model is included into tensorflow
   models github repository. the core model is a sequence-to-sequence
   model with attention. when training, the model is using the first two
   sentences from the article as an input and generates a headline.

   when decoding, the algorithm is using [15]id125 to find the best
   headline from candidate headlines generated by the model.

   github repository doesn   t include a trained model. the dataset is not
   publicly available, a license costs $6000 for organizations which are
   not members of linguistic data consortium. but they include a toy
   dataset which is enough to run the code.

how to run

   you will need tensorflow and bazel as prerequisites for training the
   model.

   the toy dataset included into the repository, contains two files in
      data    directory:    data    and    vocab   . the first one contains a sequence
   of serialized tensorflow.core.example.example_pb2.example objects. an
   example of code to create a file with this format:
import struct
from tensorflow.core.example import example_pb2

with open(output_filename, 'wb') as writer:
  body = 'body'
  title = 'title'

  tf_example = example_pb2.example()
  tf_example.features.feature['article'].bytes_list.value.extend([body])
  tf_example.features.feature['abstract'].bytes_list.value.extend([title])
  tf_example_str = tf_example.serializetostring()
  str_len = len(tf_example_str)
  writer.write(struct.pack('q', str_len))
  writer.write(struct.pack('%ds' % str_len, tf_example_str))

      vocab    file is a text file with the frequency of words in a
   vocabulary. each line contains a word, space character and number of
   occurrences of that word in the dataset. the list is being used to
   vectorize texts.

   running the code on toy dataset is really simple. [16]readme on github
   repo lists a sequence of commands to run training and testing code.

   you can run tensorboard to monitor training process:

   [17]textsum-toy-train

   when running    decode    code, note that it will loop over the entire
   dataset indefinitely, so you will have to stop execution manually at
   some point. you can find results of decoding in log_root/decode folder.
   it will contain a few files, some of them have prefix    ref   , they
   contain original headlines from the test set. other files have prefix
      decode   , they contain headlines generated by the model.

troubleshooting

   you can encounter an error when running    eval    or    decode    code using
   tensorflow 0.10 or later:

      valueerror: could not flatten dictionary. key had 2 elements, but
   value had 1 elements.   

   there is an [18]open issue on github for this error. one workaround is
   to downgrade tensorflow to 0.9, it worked for me. [19]another
   workaround requires changing the code of the model: adding
      state_is_tuple=false    to instantiations of lstmcell in
   id195_attention_model.py.
     __________________________________________________________________

   if you run training and decoding on toy dataset, you will notice that
   decoding generates nonsense. here are few examples of headlines
   generated:

     <unk> to <unk> <unk> <unk> <unk> <unk> .

     <unk> <unk> <unk> <unk> of <unk> <unk> from <unk> <unk> .

     in in <unk> <unk> <unk> .

   one of the reasons for poor performance on the toy set could be
   incompleteness of the vocabulary file. vocabulary file is truncated and
   doesn   t contain many of the words which are used in the    data    file. it
   leads to too many    <unk>    tokens which represent unknown words.

how to run on another dataset

   a toy dataset is, well, a toy. to create a useful model you should
   train it on a large dataset. ideally, the dataset should be specific
   for your task. summarizing news article may be different from
   summarizing legal documents or job descriptions.

   as i don   t have access to gigaword dataset, i tried to train the model
   on smaller news article datasets, which are free: id98 and dailymail. i
   found the code to download these datasets in [20]deepmind/rcdata github
   repo, and slightly modified it to add the title of the article in the
   first line of each output file. see modified code [21]here.

   92570 articles in id98 dataset, and 219503 articles in daily mail
   dataset. it could be a few more articles, but the code from deepmind
   repo could not download all urls. 322k articles are way fewer than 10
   million articles in gigaword, so i would expect a lower performance of
   the model if training on these datasets.

   after you run the code to download the dataset you will have a folder
   with lots of files, one html file for every article. to use it in
   textsum model you will need to convert it to the binary format
   described above. you can find my code to convert id98/dailymail articles
   into binary format in [22]textsum_data_convert.py file in my
   [23]   textsum    repo on github. an example of running the code for id98
   dataset:
python textsum_data_convert.py \
  --command text_to_vocabulary \
  --in_directories id98/stories \
  --out_files id98-vocab

python textsum_data_convert.py \
  --command text_to_binary \
  --in_directories id98/stories \
  --out_files id98-train.bin,id98-validation.bin,id98-test.bin \
  --split 0.8,0.15,0.05

   then you can copy train/validation/test sets and vocabulary files into
      data    directory and start training the model:
train on id98 data (id98-train, id98-vocab):

bazel-bin/textsum/id195_attention \
  --mode=train \
  --article_key=article \
  --abstract_key=abstract \
  --data_path=data/id98-train.bin \
  --vocab_path=data/id98-vocab.bin \
  --log_root=log_root \
  --train_dir=log_root/train \
  --truncate_input=true

evaluate on id98 data (id98-validation, id98-vocab):

bazel-bin/textsum/id195_attention \
  --mode=eval --article_key=article \
  --abstract_key=abstract \
  --data_path=data/id98-validation.bin \
  --vocab_path=data/id98-vocab.bin \
  --log_root=log_root \
  --train_dir=log_root/eval \
  --truncate_input=true

decode on id98 data (id98-test, id98-vocab):

bazel-bin/textsum/id195_attention \
  --mode=decode \
  --article_key=article \
  --abstract_key=abstract \
  --data_path=data/id98-test.bin \
  --vocab_path=data/id98-vocab.bin \
  --log_root=log_root \
  --decode_dir=log_root/decode \
  --beam_size=8 \
  --truncate_input=true

   training with default parameters doesn   t go very well. here is a graph
   of running_avg_loss:

   [24]textsum-id98-train

   decoding results are also disappointing:

        your your <unk>   

        we   ll the <unk>   

        snow hit hit hit <unk>   

   either dataset is too small, or hyperparameters need to be changed for
   this dataset.
     __________________________________________________________________

   when running the code i found that training code doesn   t use gpu,
   though i have all the correct configuration: geforce 980ti, cuda,
   cudnn, tensorflow compiled with using gpu. while training, python.exe
   consumes 100   300+% cpu, and it appears in the list of processes when
   running nvidia-smi, but gpu utilization stays 0%.
thu oct 13 20:14:11 2016
+------------------------------------------------------+
| nvidia-smi 352.39     driver version: 352.39         |
|-------------------------------+----------------------+----------------------+
| gpu  name        persistence-m| bus-id        disp.a | volatile uncorr. ecc |
| fan  temp  perf  pwr:usage/cap|         memory-usage | gpu-util  compute m. |
|===============================+======================+======================|
|   0  geforce gtx 980 ti  off  | 0000:01:00.0     off |                  n/a |
| 29%   46c    p8    26w / 250w |    124mib /  6142mib |      0%      default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| processes:                                                       gpu memory |
|  gpu       pid  type  process name                               usage      |
|=============================================================================|
|    0     17605    c   /usr/bin/python                                102mib |
+-----------------------------------------------------------------------------+

   i guess it can be related to the fact that authors of the model were
   running the code using multiple gpus, and one gpu had some special
   purpose. a fragment of id195_attention_model.py file:
  def _next_device(self):
    """round robin the gpu device. (reserve last gpu for expensive op)."""
    if self._num_gpus == 0:
      return ''
    dev = '/gpu:%d' % self._cur_gpu
    self._cur_gpu = (self._cur_gpu + 1) % (self._num_gpus-1)
    return dev

   the decoding code uses gpu quite well. it consumes almost all 6gb of
   gpu memory and keeps utilization over 50%.
sat oct 15 01:51:04 2016
+------------------------------------------------------+
| nvidia-smi 352.39     driver version: 352.39         |
|-------------------------------+----------------------+----------------------+
| gpu  name        persistence-m| bus-id        disp.a | volatile uncorr. ecc |
| fan  temp  perf  pwr:usage/cap|         memory-usage | gpu-util  compute m. |
|===============================+======================+======================|
|   0  geforce gtx 980 ti  off  | 0000:01:00.0     off |                  n/a |
| 32%   53c    p2    96w / 250w |   5881mib /  6142mib |     56%      default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| processes:                                                       gpu memory |
|  gpu       pid  type  process name                               usage      |
|=============================================================================|
|    0      3020    c   /usr/bin/python                               5858mib |
+-----------------------------------------------------------------------------+

conclusion

   using the code from this article you can easily run text summarization
   model on your own dataset. let me know if you find something
   interesting!

   if you happen to have a license for the gigaword dataset, i will be
   happy if you share trained tensorflow model with me. i would like to
   try it on some proprietary data, not from news articles.

   do you use any other text summarization algorithms? what works the
   best?


   this entry was posted in [25]tech on [26]october 15, 2016 by [27]pavel.

post navigation

   [28]    google assistant bot platform [29]natural language pipeline for
   chatbots    
     * daniel krotov
       best article i have seen so far on textsum. i just wanted to add
       something that you might find helpful. back when i first started
       playing with this myself, i ended up generating a vocab file based
       on the toy dataset. i have submitted a pull request but should you
       wish to provide a reference to it, i have listed the link below.
       [30]https://github.com/tensorflow/models/files/499497/vocab.txt
       i too scraped my own data and have yet to actually get good
       results. i am running this on my laptop which has a 980m. i   m
       currently training against about 40k articles, and getting an avg
       loss of 3.5 after about a week. i got this down to about 1.5ish
       last week but when i attempted upgrading to 0.11 my data got
       corrupted. so i   m retraining again using 0.9. when you trained your
       id98 dataset, what did you get your average loss down to before you
       tried decoding? xin pan replied to someone that with a large
       dataset, you shouldn   t expect to get below 1.0, however now i   m
       wondering if what i   m using is really considered    large    if he was
       speaking in reference to the gigaword dataset.
       curious to hear what you have found so far. i have been trying to
       gain a deeper understanding lately into why the results are so off
       yet look so good with the sample provided on git. i will let you
       know if i find anything else out. again awesome job with the
       article!
          + surmenok
            thanks for the comment!
            the best loss i got was 0.6551, but it was just one data
            point, at 18k training steps (i don   t know how long it was in
            terms of training time, maybe one day or a bit less). most of
            the time loss was fluctuating between 1.5 and 7.
            i haven   t researched text summarization more after writing
            this article. i may revisit this problem if i get gigaword or
            other dataset of significant size.
            please let me know if you   ll find any clues on how to make the
            model work on smaller datasets, or if you know where to get
            larger datasets.
               o daniel krotov
                 i will keep you up to date with my progress on textsum
                 and results i get as i get deeper. so now i am using my
                 gaming laptop as a primary tf training machine so no more
                 games for me for a while :) i originally trained against
                 the toy dataset until i got to about 0.005 on avg. the
                 results weren   t too bad at times but still not fully
                 usable. so i then scraped 70k articles and set 30% aside
                 for testing and used the remaining 40k to train. i
                 trained for about 2 weeks and to be honest, wasn   t all
                 too impressed.
                 as i dug deeper into the code, and read a plethora or
                 white papers, i then started to begin to understand why
                 so much data is crucial. textsum is an abstractive model
                 rather than extractive. with an extractive model, you are
                 literally taking the article and deleting pieces of it
                 until you get something close to the headline. textsum on
                 the other end tries to create the headline from what it
                 tries to understand from the article and references it
                 has trained against in the past. this is quite a
                 fascinating approach however to achieve good results, it
                 needs a lot of data and that data needs to be clean.
                 this in my opinion, as you have probably found also is
                 the toughest part of the process. clean data that can be
                 used for training. i unfortunately have not had the luck
                 of being able to find how to get a hold of the gigaword
                 dataset, but after reaching out to ldc, they did state
                 that they have id98 articles and some others that run
                 around $300. i   m more prone to purchasing a dataset
                 around this price rather than the $6k price-tag of the
                 gigaword.
                 right now i am in the process of writing my own scraper
                 and i   m going to see how it works out. i will keep you
                 abreast of the results. i   m going to shoot for about a
                 million articles and see how the results are. hopefully
                 with more data i can get much better results, but until i
                 can prove that i understand this model, take this all
                 with a grain of salt :)
                 one important thing to note that xin had stated in one of
                 the tickets on git was that with large datasets you
                 shouldn   t expect to get a avg loss lower than 1. my
                 current dataset i trained above got descent results with
                 the 0.005 avg loss but when i went to eval against the
                 test set i was only seeing 0.8. this clearly shows that i
                 had over-fitted and was providing me false results. i
                 wasn   t stopping the training as i was expecting that if i
                 was over-fitting, the avg loss would start rising again
                 in tensorboard but i think that must be another graph
                 that that occurs on when over-fitting.
                 that   s it for now. i will update you should i gain more
                 insight on this.
                    # richard liao
                      daniel, it is very interesting to see that you have
                      gone that far. just a question for you and surmenok:
                      in the original implement, it has only taken first
                      two sentences as encoder input, but why not the
                      whole article? if the information only contains in
                      the first few sentences, attention network should be
                      able to pick them up, why discard the rest of the
                      article? of course, computation might be costly,
                      also has to deal with variable lengths of the
                      articles. daniel, in your effort, did you feed in
                      whole articles (id98 and others)? how are the
                      performances if taking this approach? thx!
                         @ daniel krotov
                           hi richard! so you will see in the code that
                           you can totally take as many sentences as you
                           wish. 2 is the default but you can increase it
                           higher. i am hoping to do more work in this
                           area at a later time once i figure out
                           tensorflow serving so i can try serving the
                           textsum model. this has been giving me quite
                           some difficulty so it has been where my focus
                           has been lately.
                           that said, you hit it on the head. one of the
                           biggest reasons for only using 2 sentences is
                           primarily around the resources necessary to
                           train against full articles. it also seems from
                           the white papers i have read, that the
                           abstractive model doesn   t work as well when
                           sampling much longer articles. i have not been
                           able to prove this myself though.
                           in the end, training against 1.3 million
                           articles to an avg loss of 1.8-2.2 took about a
                           week and a half on an nvidia 980gtx and finally
                           provided descent results. when training against
                           40k articles, the results were still pretty
                           bad.
                           should i figure out the tf serving issues, i
                           will be jumping back into testing longer
                           articles and will update you as to my findings.
                              - surmenok
                                hi daniel,
                                did you get any further with text
                                summarization?
                         @ surmenok
                           i have not found a definitive answer to this.
                           the only thing which could point to the reason
                           is this sentence in the blog post about textsum
                           ([31]https://research.googleblog.com/2016/08/te
                           xt-summarization-with-tensorflow.html):    it
                           turns out for shorter texts, summarization can
                           be learned end-to-end with a deep learning
                           technique called sequence-to-sequence learning,
                                 
                           i guess that the model just doesn   t have good
                           accuracy if trained on longer messages.
                           probably with longer inputs a recurrent neural
                           network rolls out to longer computational graph
                           and it   s harder to train because of vanishing
                           gradients.
     * hafnu hafnu
       i am first to work with tensorflow can anyone assist on how to
       start text summarisation with tensorflow
          + surmenok
            hi hafnu,
            this article is basically a tutorial for running text
            summarization with tensorflow. have you tried to follow the
            steps described in the article? do you have any questions in
            particular?
     * luckman
       thanks for sharing! it   s a good article with details. but i am
       stuck in the step of converting id98/dailymail articles into binary
       format, vocab.bin and train.bin. after processing the data by
       generate_questions.py, i got a lot of files, such as
       questions/training, questions/test and questions/validation.
       however, the folder of stories is still empty, and i just cannot
       turn this questions data into vocab and binary training data. i
       can   t get which step i missed or run it incorrectly.
          + surmenok
            try to run generate_questions.py in    store    mode. this mode
            generates files in    stories    folder.
            python generate_questions.py    corpus=id98    mode=store
               o luckman
                 ok! i get it! i missed that step. thanks a lot!
     * tjs
       where are the output headlines stored?? after decoding
          + surmenok
            decode* files in log_root/decode folder
               o tjs
                 yes. found it. but , all decode lines are showing . it is
                 also not reading my test data. giving answers on data
                 set. thats very confusing.
     * surmenok
       have you installed nltk? [32]http://www.nltk.org/
       it should be possible to install it using pip: pip install nltk
          + tjs
            yeah thanks.this was the issue.
     * tjs
       can please you share your trained model pavel? i have the data and
       i tried it to train but its not giving the satisfactory results.
          + surmenok
            i can share the model i trained on id98/dailymail, but i   m not
            sure if it will be of any value, because it is not giving
            satisfactory results either. i don   t have a model trained on
            gigaword dataset.
               o shujian
                 hi pavel, thanks for this great tutorial. i also have
                 interest to take a look at the trained model on
                 id98/dailymail. would you mind sharing that?
                    # surmenok
                      i just started the training. i   ll let you know when
                      it completes.
               o tjs
                 thanks pavel. yes i am talking about the model trained on
                 id98/dailymail. even i don   t have access to gigaword, just
                 wanted to have an idea if i am going on the right track.
                 if you can share it then it would be great. please do let
                 me know when you share it and it would be very kind of
                 you if you share it at your earliest possible
                 convenience.
                    # surmenok
                      my development machine was busy on other tasks, so i
                      didn   t have a chance to fully train the model on
                      id98/dailymail. i just started the training. i   ll let
                      you know when it completes.
     * tjs
       is it necessary to evaluate the model? can we test it without
       evaluation?
          + surmenok
            you can run evaluation to check how well it works on
            previously unseen samples. what do you mean by testing? you
            can run    decode    if you want just to summarize some documents.
     * surmenok
       on page 5 they say that they used this code as a starting point:
       [33]https://github.com/nyu-dl/dl4mt-tutorial
       have you tried that?
     * surmenok
       there could be a stopping condition, but i have never waited long
       enough to catch it. i was stopping training manually. the trained
       model is in log_root folder. there should be a bunch of model.*
       files: checkpoints of the model on different stages of training.
     * sean lee
       thank you so much for sharing your insight. it helped me a lot in
       understanding the flow. however, i am experiencing some issues when
       i try it on my end.
       after running commands for training, it runs for few minutes and i
       see    killed   . from there i don   t get any files created under
       /log_root/train
       could you give me any advice on this? thank you in advance :)
          + ayushya chitransh
            i also am facing the same problem. during training, my running
            avg loss starts showing up and gets killed randomly. in few
            instances my average loss reached to 3 and sometimes it got
            killed at 5. my training always stopped due to either freezing
            my laptop or displaying    killed   . i looked up for this issue
            and landed at    killed during checkpoint save   
            ([34]https://github.com/tensorflow/tensorflow/issues/1962 )
            which suggested that it might be a memory issue.
               o osama jamil
                 what is the ram of your system you are using ?
                    # ayushya chitransh
                      4 gb, and am not using gpu. i am using toy dataset
                      provided at github.
                         @ osama jamil
                           well i dont think you will be able to run it on
                           a system with 4 gb ram, i tried on 4gb using
                           toy dataset. you can run it on min 12 gb ram
                           for toy dataset. for bigger datasets you will
                           need even more than that.
                              - sawyer
                                you are right, i used 8g ram to run the
                                id98 stories, but it has taken too long
                                time and still running
                                   = osama jamil
                                     yes if you look at ram usage and
                                     processing it will show no processing
                                     with 95-99% ram usage.. and if u run
                                     the program directly using python
                                     rather than bazel it will crash
                                   * osama jamil
                                     running on systems with atleast 12 gb
                                     ram will yield some results but if
                                     you are using id98 dataset it should
                                     be more than that or it will be
                                     killed after few runs with high avg
                                     loss
                              - ayushya chitransh
                                after reading your reply i checked my
                                resource usage and yes, i found that it
                                got killed due yo shortage of memory
                                space. though, increasing swap memory in
                                my ubuntu has given better results. full
                                3.7gb ram+6.4gb out of 18.6 is currently
                                in use, while the system is runmimg right
                                now at average loss of 4.1 and still
                                going. increasing swap memory seems to
                                work.
     * osama jamil
       hi!
       great work helped me a lot. thanks.
       im having slight issue running on id98 data.
       the vocabulary file generated using the code above gives assertion
       error. im not able to understand why it is causing issues.
       following error i get :
       traceback (most recent call last):
       file
          /home/umair/summodel/bazel-bin/textsum/id195_attention.runfiles/
       __main__/textsum/id195_attention.py   , line 213, in
       tf.app.run()
       file
          /usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/
       app.py   , line 30, in run
       sys.exit(main(sys.argv))
       file
          /home/umair/summodel/bazel-bin/textsum/id195_attention.runfiles/
       __main__/textsum/id195_attention.py   , line 165, in main
       assert vocab.checkvocab(data.sentence_start) > 0
       assertionerror
       im using ubuntu 14.04 with 12 gb ram tf version 0.9 and python 2.7
       any help in this matter would be gratly appreciated. thanks.
          + surmenok
            i found cause of the issue. the vocabulary file must contain
            records for sentence start and sentence end tokens. i fixed
            the code for vocabulary generation:
            [35]https://github.com/surmenok/textsum/blob/master/textsum_da
            ta_convert.py
     * yerik wang
       hi pavel, how did you get the graph of running_avg_loss in the
       tensorboard. i run the tensorboard but could not find the graph of
       running_avg_loss in the    event   
          + surmenok
            i just restarted training on id98/dailymail, and see these 5
            graphs on tensorboard: global_norm, global_step,
            learning_rate, loss, running_avg_loss.
     * surmenok
       have you tried ibm watson   s approach? does it work better than
       textsum?
          + richard liao
            i didn   t. i am not sure if there   s a trial that we can use ibm
            watson. instead, lately we are kind of satisfied with textrank
            algorithm. there   re some variations that we are exploring.
               o surmenok
                 what kind of problem are you solving with textrank?
                    # richard liao
                      same. it   s unsupervised approach for text summary.
                         @ surmenok
                           can i talk with you about it offline? i   m
                           curious how exactly you are applying textrank
                           to text summarization. usually it is used for
                           another kind of problems, like keyword and
                           sentence extraction.
                              - richard liao
                                sure. email me at [36]ricliao@gmail.com. i
                                will send you couples of links.
     * aruljothi rajan
       hi surmenok, i kind of lost with this (may be because of my limited
       understanding on tensorflow). my objective is to summarize a
          resolution log    high skilled people enter descriptions on how they
       resolved an issue in few lines. i want to summarize this data set
       and provide insights to a low skilled person. so that he/she can
       resolve the similar issues.
       how to do this deep learning, any pointers
       thanks
          + surmenok
            if you have no experience with deep learning at all then it
            would be good to start from a textbook like this:
            [37]http://www.deeplearningbook.org/ or from an online course,
            there are many of them on coursera, udacity and other
            platforms.
     * sasanka kudagoda
       hi pavel surmenok, i was planing to do text summarization related
       project for my final year, could you please point out some areas
       which i can improve, any new additions or accuracy or anything that
       i could work out.
          + surmenok
            improving accuracy is definitely valuable to work on. check
            out this work by salesforce:
            [38]https://www.salesforce.com/products/einstein/ai-research/t
            l-dr-reinforced-model-abstractive-summarization/
            it   s the best abstractive summarization model i   ve seen so
            far.
               o tridib dutta
                 thanks for the wonderful description above. i am
                 wondering if you have worked with the salesforce model or
                 do you know of any code at github. thanks once again.
                    # surmenok
                      no, i haven   t worked with that model and haven   t
                      seen the code for it.
     * https://web-site4.biz.ua/                              
                                                                        bigdata
     * leena shekhar singh
       thank you so much for the article. iam wondering if any of you guys
       have a pre-trained model for this? i am specifically looking for a
       model trained on id98/daily mail dataset. it would be a great help
       as i cannot train a huge model like this currently given the
       resources i have.
     * tridib dutta
       hi pavel,
       nice post. i am also trying to use textsum in my project. but so
       far facing lot of issues. i am just wondering whether you have
       further played around with it. eventually i would like to use it
       once i get over the issues i am facing. thanks
          + surmenok
            hi tridib,
            i haven   t played with textsum for a while.
            i think there could have been some improvement in text
            summarization algorithms since my post, and it makes sense to
            look for better models. there were links to salesforce and ibm
            research in comments to this article.
               o td
                 excellent, thanks pavel. i will look for this link.
     * david hansen
       hello, i   m joining the discussion a year (maybe two) late.
       first things first   many thanks to pavel for the blog   the clearest
       explanation i have read.
       my question:
       summarizing the first two sentences of an article is a good step
       and like others in this post stream i wish to apply textsum beyond
       sentences to full article. wondering if anyone has had a modicum of
       success in summarizing beyond two sentences. also wondering if it
       feasible to treat each paragraph (first two sentences of each) in a
       larger article as separate    articles    (round robin fashion) to
       generate summary. if this is possible it might open way to sum
       across many full length articles.
       full disclosure: i am not well versed in writing or running code
       but manage to stumble through, so i may be blindly dreaming of
       solution that is not possible.

   search for: ____________________ search

contacts

   e-mail: surmenok@gmail.com
   [39]linkedin

archives

     * [40]november 2018
     * [41]october 2017
     * [42]september 2017
     * [43]august 2017
     * [44]july 2017
     * [45]april 2017
     * [46]february 2017
     * [47]january 2017
     * [48]november 2016
     * [49]october 2016
     * [50]september 2016
     * [51]june 2016
     * [52]march 2016
     * [53]february 2016
     * [54]september 2015
     * [55]july 2015
     * [56]may 2015
     * [57]march 2015
     * [58]february 2015
     * [59]january 2015
     * [60]november 2014
     * [61]october 2014
     * [62]september 2014
     * [63]july 2014
     * [64]may 2014
     * [65]april 2014
     * [66]november 2013
     * [67]september 2013
     * [68]july 2013
     * [69]february 2013
     * [70]september 2012
     * [71]january 2012
     * [72]october 2011
     * [73]july 2011
     * [74]april 2011

   [75]proudly powered by wordpress

references

   visible links
   1. http://pavel.surmenok.com/feed/
   2. http://pavel.surmenok.com/comments/feed/
   3. http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/feed/
   4. http://pavel.surmenok.com/2016/10/12/google-assistant-bot-platform/
   5. http://pavel.surmenok.com/2016/11/05/natural-language-pipeline-for-chatbots/
   6. http://pavel.surmenok.com/
   7. http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/#content
   8. http://pavel.surmenok.com/
   9. https://en.wikipedia.org/wiki/automatic_summarization
  10. https://research.googleblog.com/2015/11/computer-respond-to-this-email.html
  11. https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html
  12. https://catalog.ldc.upenn.edu/ldc2012t21
  13. http://arxiv.org/abs/1409.3215
  14. https://github.com/tensorflow/models/tree/master/textsum
  15. https://en.wikipedia.org/wiki/beam_search
  16. https://github.com/tensorflow/models/tree/master/textsum
  17. http://pavel.surmenok.com/wp-content/uploads/2016/10/textsum-toy-train.png
  18. https://github.com/tensorflow/models/issues/417
  19. https://github.com/tensorflow/models/issues/417#issuecomment-253363772
  20. https://github.com/deepmind/rc-data
  21. https://gist.github.com/surmenok/2224ccfff5fbf24f3905b3da995668a3
  22. https://github.com/surmenok/textsum/blob/master/textsum_data_convert.py
  23. https://github.com/surmenok/textsum
  24. http://pavel.surmenok.com/wp-content/uploads/2016/10/textsum-id98-train.png
  25. http://pavel.surmenok.com/category/tech/
  26. http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/
  27. http://pavel.surmenok.com/author/pavel/
  28. http://pavel.surmenok.com/2016/10/12/google-assistant-bot-platform/
  29. http://pavel.surmenok.com/2016/11/05/natural-language-pipeline-for-chatbots/
  30. https://github.com/tensorflow/models/files/499497/vocab.txt
  31. https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html
  32. http://www.nltk.org/
  33. https://github.com/nyu-dl/dl4mt-tutorial
  34. https://github.com/tensorflow/tensorflow/issues/1962
  35. https://github.com/surmenok/textsum/blob/master/textsum_data_convert.py
  36. mailto:ricliao@gmail.com
  37. http://www.deeplearningbook.org/
  38. https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/
  39. https://www.linkedin.com/in/pavelsurmenok/
  40. http://pavel.surmenok.com/2018/11/
  41. http://pavel.surmenok.com/2017/10/
  42. http://pavel.surmenok.com/2017/09/
  43. http://pavel.surmenok.com/2017/08/
  44. http://pavel.surmenok.com/2017/07/
  45. http://pavel.surmenok.com/2017/04/
  46. http://pavel.surmenok.com/2017/02/
  47. http://pavel.surmenok.com/2017/01/
  48. http://pavel.surmenok.com/2016/11/
  49. http://pavel.surmenok.com/2016/10/
  50. http://pavel.surmenok.com/2016/09/
  51. http://pavel.surmenok.com/2016/06/
  52. http://pavel.surmenok.com/2016/03/
  53. http://pavel.surmenok.com/2016/02/
  54. http://pavel.surmenok.com/2015/09/
  55. http://pavel.surmenok.com/2015/07/
  56. http://pavel.surmenok.com/2015/05/
  57. http://pavel.surmenok.com/2015/03/
  58. http://pavel.surmenok.com/2015/02/
  59. http://pavel.surmenok.com/2015/01/
  60. http://pavel.surmenok.com/2014/11/
  61. http://pavel.surmenok.com/2014/10/
  62. http://pavel.surmenok.com/2014/09/
  63. http://pavel.surmenok.com/2014/07/
  64. http://pavel.surmenok.com/2014/05/
  65. http://pavel.surmenok.com/2014/04/
  66. http://pavel.surmenok.com/2013/11/
  67. http://pavel.surmenok.com/2013/09/
  68. http://pavel.surmenok.com/2013/07/
  69. http://pavel.surmenok.com/2013/02/
  70. http://pavel.surmenok.com/2012/09/
  71. http://pavel.surmenok.com/2012/01/
  72. http://pavel.surmenok.com/2011/10/
  73. http://pavel.surmenok.com/2011/07/
  74. http://pavel.surmenok.com/2011/04/
  75. http://wordpress.org/

   hidden links:
  77. http://pavel.surmenok.com/wp-content/uploads/2016/10/id195.png
