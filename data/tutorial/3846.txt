   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]numerical-python-book-code
    2. [10]ch15-code-listing.ipynb

chapter 15: machine learning[11]  

   robert johansson

   source code listings for [12]numerical python - a practical techniques
   approach for industry (isbn 978-1-484205-54-9).

   the source code listings can be downloaded from
   [13]http://www.apress.com/9781484205549
   in [1]:
from sklearn import datasets
from sklearn import cross_validation
from sklearn import linear_model
from sklearn import metrics
from sklearn import tree
from sklearn import neighbors
from sklearn import id166
from sklearn import ensemble
from sklearn import cluster

   in [2]:
%matplotlib inline
import matplotlib.pyplot as plt

   in [3]:
import numpy as np

   in [4]:
import seaborn as sns

built in datasets[14]  

   in [5]:
datasets.load_boston

   out[5]:
<function sklearn.datasets.base.load_boston>

   in [6]:
datasets.fetch_california_housing

   out[6]:
<function sklearn.datasets.california_housing.fetch_california_housing>

   in [7]:
datasets.make_regression

   out[7]:
<function sklearn.datasets.samples_generator.make_regression>

regression[15]  

   in [8]:
np.random.seed(123)

   in [9]:
x_all, y_all = datasets.make_regression(n_samples=50, n_features=50, n_informati
ve=10) #, noise=2.5)

   in [10]:
x_train, x_test, y_train, y_test = cross_validation.train_test_split(x_all, y_al
l, train_size=0.5)

   in [11]:
x_train.shape, y_train.shape

   out[11]:
((25, 50), (25,))

   in [12]:
x_test.shape, y_test.shape

   out[12]:
((25, 50), (25,))

   in [13]:
model = linear_model.linearregression()

   in [14]:
model.fit(x_train, y_train)

   out[14]:
linearregression(copy_x=true, fit_intercept=true, n_jobs=1, normalize=false)

   in [15]:
def sse(resid):
    return sum(resid**2)

   in [16]:
resid_train = y_train - model.predict(x_train)
sse_train = sse(resid_train)
sse_train

   out[16]:
8.1332190417010494e-25

   in [17]:
resid_test = y_test - model.predict(x_test)
sse_test = sse(resid_train)
sse_test

   out[17]:
8.1332190417010494e-25

   in [18]:
model.score(x_train, y_train)

   out[18]:
1.0

   in [19]:
model.score(x_test, y_test)

   out[19]:
0.31407400675201746

   in [20]:
def plot_residuals_and_coeff(resid_train, resid_test, coeff):
    fig, axes = plt.subplots(1, 3, figsize=(12, 3))
    axes[0].bar(np.arange(len(resid_train)), resid_train)
    axes[0].set_xlabel("sample number")
    axes[0].set_ylabel("residual")
    axes[0].set_title("training data")
    axes[1].bar(np.arange(len(resid_test)), resid_test)
    axes[1].set_xlabel("sample number")
    axes[1].set_ylabel("residual")
    axes[1].set_title("testing data")
    axes[2].bar(np.arange(len(coeff)), coeff)
    axes[2].set_xlabel("coefficient number")
    axes[2].set_ylabel("coefficient")
    fig.tight_layout()
    return fig, axes

   in [21]:
fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
fig.savefig("ch15-regression-ols.pdf")

   [vtbwhnaoaeyovkaaaajkquaadaswqaaacorbqaambjzaaaai5efaaawchxx8ven28az
   aaaaaelftksuqmcc ]
   in [22]:
model = linear_model.ridge() #alpha=2.5)

   in [23]:
model.fit(x_train, y_train)

   out[23]:
ridge(alpha=1.0, copy_x=true, fit_intercept=true, max_iter=none,
   normalize=false, solver='auto', tol=0.001)

   in [24]:
resid_train = y_train - model.predict(x_train)
sse_train = sum(resid_train**2)
sse_train

   out[24]:
178.50695164950841

   in [25]:
resid_test = y_test - model.predict(x_test)
sse_test = sum(resid_test**2)
sse_test

   out[25]:
212737.00160105844

   in [26]:
model.score(x_train, y_train), model.score(x_test, y_test)

   out[26]:
(0.99945955150173349, 0.31670332736075468)

   in [27]:
fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
fig.savefig("ch15-regression-ridge.pdf")

   [gzgtzmksjgyaaaaasuvork5c yii= ]
   in [28]:
model = linear_model.lasso(alpha=1.0)

   in [29]:
model.fit(x_train, y_train)

   out[29]:
lasso(alpha=1.0, copy_x=true, fit_intercept=true, max_iter=1000,
   normalize=false, positive=false, precompute=false, random_state=none,
   selection='cyclic', tol=0.0001, warm_start=false)

   in [30]:
resid_train = y_train - model.predict(x_train)
sse_train = sse(resid_train)
sse_train

   out[30]:
309.74971389531891

   in [31]:
resid_test = y_test - model.predict(x_test)
sse_test = sse(resid_test)
sse_test

   out[31]:
1489.1176065002333

   in [32]:
fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
fig.savefig("ch15-regression-lasso.pdf")

   [bfo0anwv79+zfq1cg8
   9drt6ngjh95bcpw9vqlueokjj7bhwwzcvxovvr6+wlx4capbsmo8lajvjlgiwpkskudk5it
   xx39d
   6vciqjlgu0nernaitwtsg3bsgiksg9sdiikynrysrurererejcfeysiiiiiiipiqkywiiii
   iiiij
   mckiiiiiiiksejmsiiiiiiiicthjiiiiiiiikhctlciiiiiiign9p9cg4iaqbq8haaaaael
   ftksu qmcc ]
   in [33]:
alphas = np.logspace(-4, 2, 100)

   in [34]:
coeffs = np.zeros((len(alphas), x_train.shape[1]))
sse_train = np.zeros_like(alphas)
sse_test = np.zeros_like(alphas)

for n, alpha in enumerate(alphas):
    model = linear_model.lasso(alpha=alpha)
    model.fit(x_train, y_train)
    coeffs[n, :] = model.coef_
    resid = y_train - model.predict(x_train)
    sse_train[n] = sum(resid**2)
    resid = y_test - model.predict(x_test)
    sse_test[n] = sum(resid**2)

/users/rob/miniconda/envs/py27-npm/lib/python2.7/site-packages/sklearn/linear_mo
del/coordinate_descent.py:444: convergencewarning: objective did not converge. y
ou might want to increase the number of iterations
  convergencewarning)

   in [35]:
fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=true)

for n in range(coeffs.shape[1]):
    axes[0].plot(np.log10(alphas), coeffs[:, n], color='k', lw=0.5)

axes[1].semilogy(np.log10(alphas), sse_train, label="train")
axes[1].semilogy(np.log10(alphas), sse_test, label="test")
axes[1].legend(loc=0)

axes[0].set_xlabel(r"${\log_{10}}\alpha$", fontsize=18)
axes[0].set_ylabel(r"coefficients", fontsize=18)
axes[1].set_xlabel(r"${\log_{10}}\alpha$", fontsize=18)
axes[1].set_ylabel(r"sse", fontsize=18)
fig.tight_layout()
fig.savefig("ch15-regression-lasso-vs-alpha.pdf")

   [pfaaaaaelftksu qmcc ]
   in [36]:
model = linear_model.lassocv()

   in [37]:
model.fit(x_all, y_all)

   out[37]:
lassocv(alphas=none, copy_x=true, cv=none, eps=0.001, fit_intercept=true,
    max_iter=1000, n_alphas=100, n_jobs=1, normalize=false, positive=false,
    precompute='auto', random_state=none, selection='cyclic', tol=0.0001,
    verbose=false)

   in [38]:
model.alpha_

   out[38]:
0.06559238747534718

   in [39]:
resid_train = y_train - model.predict(x_train)
sse_train = sse(resid_train)
sse_train

   out[39]:
1.5450589323147632

   in [40]:
resid_test = y_test - model.predict(x_test)
sse_test = sse(resid_test)
sse_test

   out[40]:
1.5321417406216065

   in [41]:
model.score(x_train, y_train), model.score(x_test, y_test)

   out[41]:
(0.99999532217220677, 0.99999507886570982)

   in [42]:
fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
fig.savefig("ch15-regression-lasso-cv.pdf")

   [ioxzea i0qiaaaaaelftksuqmcc ]
   in [43]:
model = linear_model.elasticnetcv()

   in [44]:
model.fit(x_all, y_all)

   out[44]:
elasticnetcv(alphas=none, copy_x=true, cv=none, eps=0.001, fit_intercept=true,
       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,
       normalize=false, positive=false, precompute='auto',
       random_state=none, selection='cyclic', tol=0.0001, verbose=0)

   in [45]:
model.alpha_

   out[45]:
0.13118477495069433

   in [46]:
model.l1_ratio

   out[46]:
0.5

   in [47]:
resid_train = y_train - model.predict(x_train)
sse_train = sum(resid_train**2)
sse_train

   out[47]:
2183.8391729391255

   in [48]:
resid_test = y_test - model.predict(x_test)
sse_test = sum(resid_test**2)
sse_test

   out[48]:
2650.0504463382508

   in [49]:
model.score(x_train, y_train), model.score(x_test, y_test)

   out[49]:
(0.99338819810341106, 0.99148821954487831)

   in [50]:
fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)
fig.savefig("ch15-regression-elastic-net-cv.pdf")

   [axjuzqr+amclaaaaaelftksuqmcc ]

classification[16]  

   in [51]:
iris = datasets.load_iris()

   in [52]:
type(iris)

   out[52]:
sklearn.datasets.base.bunch

   in [53]:
iris.target_names

   out[53]:
array(['setosa', 'versicolor', 'virginica'],
      dtype='|s10')

   in [54]:
iris.feature_names

   out[54]:
['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']

   in [55]:
iris.data.shape

   out[55]:
(150, 4)

   in [56]:
iris.target.shape

   out[56]:
(150,)

   in [57]:
# print(iris['descr'])

   in [58]:
x_train, x_test, y_train, y_test = cross_validation.train_test_split(iris.data,
iris.target, train_size=0.7)

   in [59]:
classifier = linear_model.logisticregression()

   in [60]:
classifier.fit(x_train, y_train)

   out[60]:
logisticregression(c=1.0, class_weight=none, dual=false, fit_intercept=true,
          intercept_scaling=1, max_iter=100, multi_class='ovr',
          penalty='l2', random_state=none, solver='liblinear', tol=0.0001,
          verbose=0)

   in [61]:
y_test_pred = classifier.predict(x_test)

   in [62]:
print(metrics.classification_report(y_test, y_test_pred))

             precision    recall  f1-score   support

          0       1.00      1.00      1.00        16
          1       1.00      0.76      0.87        17
          2       0.75      1.00      0.86        12

avg / total       0.93      0.91      0.91        45


   in [63]:
np.bincount(y_test)

   out[63]:
array([16, 17, 12])

   in [64]:
metrics.confusion_matrix(y_test, y_test_pred)

   out[64]:
array([[16,  0,  0],
       [ 0, 13,  4],
       [ 0,  0, 12]])

   in [65]:
classifier = tree.decisiontreeclassifier()
classifier.fit(x_train, y_train)
y_test_pred = classifier.predict(x_test)
metrics.confusion_matrix(y_test, y_test_pred)

   out[65]:
array([[16,  0,  0],
       [ 0, 12,  5],
       [ 0,  0, 12]])

   in [66]:
classifier = neighbors.kneighborsclassifier()
classifier.fit(x_train, y_train)
y_test_pred = classifier.predict(x_test)
metrics.confusion_matrix(y_test, y_test_pred)

   out[66]:
array([[16,  0,  0],
       [ 0, 15,  2],
       [ 0,  0, 12]])

   in [67]:
classifier = id166.svc()
classifier.fit(x_train, y_train)
y_test_pred = classifier.predict(x_test)
metrics.confusion_matrix(y_test, y_test_pred)

   out[67]:
array([[16,  0,  0],
       [ 0, 15,  2],
       [ 0,  0, 12]])

   in [68]:
classifier = ensemble.randomforestclassifier()
classifier.fit(x_train, y_train)
y_test_pred = classifier.predict(x_test)
metrics.confusion_matrix(y_test, y_test_pred)

   out[68]:
array([[16,  0,  0],
       [ 0, 16,  1],
       [ 0,  0, 12]])

   in [69]:
train_size_vec = np.linspace(0.1, 0.9, 30)

   in [70]:
classifiers = [tree.decisiontreeclassifier,
               neighbors.kneighborsclassifier,
               id166.svc,
               ensemble.randomforestclassifier
              ]

   in [71]:
cm_diags = np.zeros((3, len(train_size_vec), len(classifiers)), dtype=float)

   in [72]:
for n, train_size in enumerate(train_size_vec):
    x_train, x_test, y_train, y_test = \
        cross_validation.train_test_split(iris.data, iris.target, train_size=tra
in_size)

    for m, classifier in enumerate(classifiers):
        classifier = classifier()
        classifier.fit(x_train, y_train)
        y_test_pred = classifier.predict(x_test)
        cm_diags[:, n, m] = metrics.confusion_matrix(y_test, y_test_pred).diagon
al()
        cm_diags[:, n, m] /= np.bincount(y_test)

   in [73]:
fig, axes = plt.subplots(1, len(classifiers), figsize=(12, 3))

for m, classifier in enumerate(classifiers):
    axes[m].plot(train_size_vec, cm_diags[2, :, m], label=iris.target_names[2])
    axes[m].plot(train_size_vec, cm_diags[1, :, m], label=iris.target_names[1])
    axes[m].plot(train_size_vec, cm_diags[0, :, m], label=iris.target_names[0])
    axes[m].set_title(type(classifier()).__name__)
    axes[m].set_ylim(0, 1.1)
    axes[m].set_xlim(0.1, 0.9)
    axes[m].set_ylabel("classification accuracy")
    axes[m].set_xlabel("training size ratio")
    axes[m].legend(loc=4)

fig.tight_layout()
fig.savefig("ch15-classification-comparison.pdf")

   [h5l6rbw6wdzeaaaa aelftksuqmcc ]

id91[17]  

   in [74]:
x, y = iris.data, iris.target

   in [75]:
np.random.seed(123)

   in [76]:
n_clusters = 3

   in [77]:
c = cluster.kmeans(n_clusters=n_clusters)

   in [78]:
c.fit(x)

   out[78]:
kmeans(copy_x=true, init='id116++', max_iter=300, n_clusters=3, n_init=10,
    n_jobs=1, precompute_distances='auto', random_state=none, tol=0.0001,
    verbose=0)

   in [79]:
y_pred = c.predict(x)

   in [80]:
y_pred[::8]

   out[80]:
array([1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0], dtype=int32)

   in [81]:
y[::8]

   out[81]:
array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])

   in [82]:
idx_0, idx_1, idx_2 = (np.where(y_pred == n) for n in range(3))

   in [83]:
y_pred[idx_0], y_pred[idx_1], y_pred[idx_2] = 2, 0, 1

   in [84]:
y_pred[::8]

   out[84]:
array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], dtype=int32)

   in [85]:
metrics.confusion_matrix(y, y_pred)

   out[85]:
array([[50,  0,  0],
       [ 0, 48,  2],
       [ 0, 14, 36]])

   in [86]:
n = x.shape[1]

fig, axes = plt.subplots(n, n, figsize=(12, 12), sharex=true, sharey=true)

colors = ["coral", "blue", "green"]
markers = ["^", "v", "o"]
for m in range(n):
    for n in range(n):
        for p in range(n_clusters):
            mask = y_pred == p
            axes[m, n].scatter(x[:, m][mask], x[:, n][mask],
                               marker=markers[p], s=30,
                               color=colors[p], alpha=0.25)

        for idx in np.where(y != y_pred):
            axes[m, n].scatter(x[idx, m], x[idx, n],
                               marker="s", s=30,
                               edgecolor="red",
                               facecolor=(1,1,1,0))


    axes[n-1, m].set_xlabel(iris.feature_names[m], fontsize=16)
    axes[m, 0].set_ylabel(iris.feature_names[m], fontsize=16)
fig.tight_layout()
fig.savefig("ch15-id91.pdf")

   [h6izkmbevssaaaaasuvork5cyii= ]

versions[18]  

   in [87]:
%reload_ext version_information

   in [88]:
%version_information sklearn, numpy, matplotlib, seaborn

   out[88]:
    software                      version
   python     2.7.10 64bit [gcc 4.2.1 (apple inc. build 5577)]
   ipython    3.2.1
   os         darwin 14.1.0 x86_64 i386 64bit
   sklearn    0.16.1
   numpy      1.9.2
   matplotlib 1.4.3
   seaborn    0.6.0

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [19]fastly, rendered by [20]rackspace

   nbviewer github [21]repository.

   nbviewer version: [22]33c4683

   nbconvert version: [23]5.4.0

   rendered (fri, 05 apr 2019 18:31:59 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb
   5. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb
   6. https://github.com/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb
   7. https://mybinder.org/v2/gh/jrjohansson/numerical-python-book-code/master?filepath=ch15-code-listing.ipynb
   8. https://raw.githubusercontent.com/jrjohansson/numerical-python-book-code/master/ch15-code-listing.ipynb
   9. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/tree/master
  10. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/tree/master/ch15-code-listing.ipynb
  11. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb#chapter-15:-machine-learning
  12. http://www.apress.com/9781484205549
  13. http://www.apress.com/9781484205549
  14. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb#built-in-datasets
  15. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb#regression
  16. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb#classification
  17. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb#id91
  18. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch15-code-listing.ipynb#versions
  19. http://www.fastly.com/
  20. https://developer.rackspace.com/?nbviewer=awesome
  21. https://github.com/jupyter/nbviewer
  22. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  23. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
