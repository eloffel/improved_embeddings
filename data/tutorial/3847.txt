   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]numerical-python-book-code
    2. [10]ch05-code-listing.ipynb

chapter 5: equation solving[11]  

   robert johansson

   source code listings for [12]numerical python - a practical techniques
   approach for industry (isbn 978-1-484205-54-9).

   the source code listings can be downloaded from
   [13]http://www.apress.com/9781484205549
   in [1]:
from scipy import linalg as la

   in [2]:
from scipy import optimize

   in [3]:
import sympy

   in [4]:
sympy.init_printing()

   in [5]:
import numpy as np

   in [6]:
import matplotlib.pyplot as plt
%matplotlib inline

import matplotlib as mpl
mpl.rcparams["font.family"] = "serif"
mpl.rcparams["font.size"] = "12"

   in [7]:
from __future__ import division

id202 - linear equation systems[14]  

   $$ 2 x_1 + 3 x_2 = 4 $$$$ 5 x_1 + 4 x_2 = 3 $$
   in [8]:
fig, ax = plt.subplots(figsize=(8, 4))

x1 = np.linspace(-4, 2, 100)

x2_1 = (4 - 2 * x1)/3
x2_2 = (3 - 5 * x1)/4

ax.plot(x1, x2_1, 'r', lw=2, label=r"$2x_1+3x_2-4=0$")
ax.plot(x1, x2_2, 'b', lw=2, label=r"$5x_1+4x_2-3=0$")

a = np.array([[2, 3], [5, 4]])
b = np.array([4, 3])
x = la.solve(a, b)

ax.plot(x[0], x[1], 'ko', lw=2)
ax.annotate("the intersection point of\nthe two lines is the solution\nto the eq
uation system",
            xy=(x[0], x[1]), xycoords='data',
            xytext=(-120, -75), textcoords='offset points',
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3, rad=-.3"))

ax.set_xlabel(r"$x_1$", fontsize=18)
ax.set_ylabel(r"$x_2$", fontsize=18)
ax.legend();

fig.tight_layout()
fig.savefig('ch5-linear-systems-simple.pdf')

   [+vf4n1xiebdaaaaaelftksuqmcc ]

symbolic approach[15]  

   in [9]:
a = sympy.matrix([[2, 3], [5, 4]])
b = sympy.matrix([4, 3])

   in [10]:
a.rank()

   out[10]:
   $$2$$
   in [11]:
a.condition_number()

   out[11]:
   $$\frac{\sqrt{2 \sqrt{170} + 27}}{\sqrt{- 2 \sqrt{170} + 27}}$$
   in [12]:
sympy.n(_)

   out[12]:
   $$7.58240137440151$$
   in [13]:
a.norm()

   out[13]:
   $$3 \sqrt{6}$$
   in [14]:
l, u, _ = a.ludecomposition()

   in [15]:
l

   out[15]:
   $$\left[\begin{matrix}1 & 0\\\frac{5}{2} & 1\end{matrix}\right]$$
   in [16]:
u

   out[16]:
   $$\left[\begin{matrix}2 & 3\\0 & - \frac{7}{2}\end{matrix}\right]$$
   in [17]:
l * u

   out[17]:
   $$\left[\begin{matrix}2 & 3\\5 & 4\end{matrix}\right]$$
   in [18]:
x = a.solve(b)

   in [19]:
x

   out[19]:
   $$\left[\begin{matrix}-1\\2\end{matrix}\right]$$

numerical approach[16]  

   in [20]:
a = np.array([[2, 3], [5, 4]])
b = np.array([4, 3])

   in [21]:
np.linalg.matrix_rank(a)

   out[21]:
   $$2$$
   in [22]:
np.linalg.cond(a)

   out[22]:
   $$7.5824013744$$
   in [23]:
np.linalg.norm(a)

   out[23]:
   $$7.34846922835$$
   in [24]:
p, l, u = la.lu(a)

   in [25]:
l

   out[25]:
array([[ 1. ,  0. ],
       [ 0.4,  1. ]])

   in [26]:
u

   out[26]:
array([[ 5. ,  4. ],
       [ 0. ,  1.4]])

   in [27]:
l*u

   out[27]:
array([[ 5. ,  0. ],
       [ 0. ,  1.4]])

   in [28]:
la.solve(a, b)

   out[28]:
array([-1.,  2.])

example : rank and condition numbers -> numerical errors[17]  

   in [29]:
p = sympy.symbols("p", positive=true)

   in [30]:
a = sympy.matrix([[1, sympy.sqrt(p)], [1, 1/sympy.sqrt(p)]])

   in [31]:
b = sympy.matrix([1, 2])

   in [32]:
sympy.simplify(a.solve(b))

   out[32]:
   $$\left[\begin{matrix}\frac{2 p - 1}{p - 1}\\- \frac{\sqrt{p}}{p -
   1}\end{matrix}\right]$$
   in [33]:
# symbolic problem specification
p = sympy.symbols("p", positive=true)
a = sympy.matrix([[1, sympy.sqrt(p)], [1, 1/sympy.sqrt(p)]])
b = sympy.matrix([1, 2])

# solve symbolically
x_sym_sol = a.solve(b)
x_sym_sol.simplify()
x_sym_sol
acond = a.condition_number().simplify()

# function for solving numerically
aa = lambda p: np.array([[1, np.sqrt(p)], [1, 1/np.sqrt(p)]])
bb = np.array([1, 2])
x_num_sol = lambda p: np.linalg.solve(aa(p), bb)

# graph the difference between the symbolic (exact) and numerical results.
p_vec = np.linspace(0.9, 1.1, 200)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

for n in range(2):
    x_sym = np.array([x_sym_sol[n].subs(p, pp).evalf() for pp in p_vec])
    x_num = np.array([x_num_sol(pp)[n] for pp in p_vec])
    axes[0].plot(p_vec, (x_num - x_sym)/x_sym, 'k')
axes[0].set_title("error in solution\n(numerical - symbolic)/symbolic")
axes[0].set_xlabel(r'$p$', fontsize=18)

axes[1].plot(p_vec, [acond.subs(p, pp).evalf() for pp in p_vec])
axes[1].set_title("condition number")
axes[1].set_xlabel(r'$p$', fontsize=18)

fig.tight_layout()
fig.savefig('ch5-linear-systems-condition-number.pdf')

   [f3gvreresqi+tkr4
   zn2jrooiiiiiiehz0giviiiiiihinhrqiyiiiiii5ekblyiiiiiisj4uuimiiiiiiorjazw
   iiiii ieieffcjiiiiiijksqgviiiiiihinv4felrlkmpc5b4aaaaasuvork5cyii= ]

rectangular systems[18]  

underdetermined[19]  

   in [34]:
unknown = sympy.symbols("x, y, z")

   in [35]:
a = sympy.matrix([[1, 2, 3], [4, 5, 6]])

   in [36]:
x = sympy.matrix(unknown)

   in [37]:
b = sympy.matrix([7, 8])

   in [38]:
aa = a * x - b

   in [39]:
sympy.solve(a*x - b, unknown)

   out[39]:
   $$\left \{ x : z - \frac{19}{3}, \quad y : - 2 z + \frac{20}{3}\right
   \}$$

overdetermined: least squares[20]  

   in [40]:
np.random.seed(1234)

# define true model parameters
x = np.linspace(-1, 1, 100)
a, b, c = 1, 2, 3
y_exact = a + b * x + c * x**2

# simulate noisy data points
m = 100
x = 1 - 2 * np.random.rand(m)
y = a + b * x + c * x**2 + np.random.randn(m)

# fit the data to the model using linear least square
a = np.vstack([x**0, x**1, x**2])  # see np.vander for alternative
sol, r, rank, sv = la.lstsq(a.t, y)
y_fit = sol[0] + sol[1] * x + sol[2] * x**2
fig, ax = plt.subplots(figsize=(12, 4))

ax.plot(x, y, 'go', alpha=0.5, label='simulated data')
ax.plot(x, y_exact, 'k', lw=2, label='true value $y = 1 + 2x + 3x^2$')
ax.plot(x, y_fit, 'b', lw=2, label='least square fit')
ax.set_xlabel(r"$x$", fontsize=18)
ax.set_ylabel(r"$y$", fontsize=18)
ax.legend(loc=2);

fig.savefig('ch5-linear-systems-least-square.pdf')

   [qgh2vfu55kqa aaaasuvork5cyii= ]
   in [41]:
# fit the data to the model using linear least square:
# 1st order polynomial
a = np.vstack([x**n for n in range(2)])
sol, r, rank, sv = la.lstsq(a.t, y)
y_fit1 = sum([s * x**n for n, s in enumerate(sol)])

# 15th order polynomial
a = np.vstack([x**n for n in range(16)])
sol, r, rank, sv = la.lstsq(a.t, y)
y_fit15 = sum([s * x**n for n, s in enumerate(sol)])

fig, ax = plt.subplots(figsize=(12, 4))
ax.plot(x, y, 'go', alpha=0.5, label='simulated data')
ax.plot(x, y_exact, 'k', lw=2, label='true value $y = 1 + 2x + 3x^2$')
ax.plot(x, y_fit1, 'b', lw=2, label='least square fit [1st order]')
ax.plot(x, y_fit15, 'm', lw=2, label='least square fit [15th order]')
ax.set_xlabel(r"$x$", fontsize=18)
ax.set_ylabel(r"$y$", fontsize=18)
ax.legend(loc=2);

fig.savefig('ch5-linear-systems-least-square-2.pdf')

   [ax9hufsqn13baaaaaelftksuqmcc ]

eigenvalue problems[21]  

   in [42]:
eps, delta = sympy.symbols("epsilon, delta")

   in [43]:
h = sympy.matrix([[eps, delta], [delta, -eps]])
h

   out[43]:
   $$\left[\begin{matrix}\epsilon & \delta\\\delta & -
   \epsilon\end{matrix}\right]$$
   in [44]:
eval1, eval2 = h.eigenvals()

   in [45]:
eval1, eval2

   out[45]:
   $$\left ( - \sqrt{\delta^{2} + \epsilon^{2}}, \quad \sqrt{\delta^{2} +
   \epsilon^{2}}\right )$$
   in [46]:
h.eigenvects()

   out[46]:
   $$\left [ \left ( - \sqrt{\delta^{2} + \epsilon^{2}}, \quad 1, \quad
   \left [ \left[\begin{matrix}- \frac{\delta}{\epsilon + \sqrt{\delta^{2}
   + \epsilon^{2}}}\\1\end{matrix}\right]\right ]\right ), \quad \left (
   \sqrt{\delta^{2} + \epsilon^{2}}, \quad 1, \quad \left [
   \left[\begin{matrix}- \frac{\delta}{\epsilon - \sqrt{\delta^{2} +
   \epsilon^{2}}}\\1\end{matrix}\right]\right ]\right )\right ]$$
   in [47]:
(eval1, _, evec1), (eval2, _, evec2) = h.eigenvects()

   in [48]:
sympy.simplify(evec1[0].t * evec2[0])

   out[48]:
   $$\left[\begin{matrix}0\end{matrix}\right]$$
   in [49]:
a = np.array([[1, 3, 5], [3, 5, 3], [5, 3, 9]])
a

   out[49]:
array([[1, 3, 5],
       [3, 5, 3],
       [5, 3, 9]])

   in [50]:
evals, evecs = la.eig(a)

   in [51]:
evals

   out[51]:
array([ 13.35310908+0.j,  -1.75902942+0.j,   3.40592034+0.j])

   in [52]:
evecs

   out[52]:
array([[ 0.42663918,  0.90353276, -0.04009445],
       [ 0.43751227, -0.24498225, -0.8651975 ],
       [ 0.79155671, -0.35158534,  0.49982569]])

   in [53]:
la.eigvalsh(a)

   out[53]:
array([ -1.75902942,   3.40592034,  13.35310908])

nonlinear equations[22]  

univariate[23]  

   in [54]:
x = np.linspace(-2, 2, 1000)

# four examples of nonlinear functions
f1 = x**2 - x - 1
f2 = x**3 - 3 * np.sin(x)
f3 = np.exp(x) - 2
f4 = 1 - x**2 + np.sin(50 / (1 + x**2))

# plot each function
fig, axes = plt.subplots(1, 4, figsize=(12, 3), sharey=true)

for n, f in enumerate([f1, f2, f3, f4]):
    axes[n].plot(x, f, lw=1.5)
    axes[n].axhline(0, ls=':', color='k')
    axes[n].set_ylim(-5, 5)
    axes[n].set_xticks([-2, -1, 0, 1, 2])
    axes[n].set_xlabel(r'$x$', fontsize=18)

axes[0].set_ylabel(r'$f(x)$', fontsize=18)

titles = [r'$f(x)=x^2-x-1$', r'$f(x)=x^3-3\sin(x)$',
          r'$f(x)=\exp(x)-2$', r'$f(x)=\sin\left(50/(1+x^2)\right)+1-x^2$']
for n, title in enumerate(titles):
    axes[n].set_title(title)

fig.tight_layout()
fig.savefig('ch5-nonlinear-plot-equations.pdf')

   [9ohaaaaaaeorvvcaizreaq5yaaacglaeaaaxzagaaglieaaawzakaagdiegaawahkvygwo
   5dd kqaaaabjru5erkjggg== ]

symbolic[24]  

   in [55]:
import sympy as s

   in [56]:
x, a, b, c = sympy.symbols("x, a, b, c")

   in [57]:
sympy.solve(a + b*x + c*x**2, x)

   out[57]:
   $$\left [ \frac{1}{2 c} \left(- b + \sqrt{- 4 a c + b^{2}}\right),
   \quad - \frac{1}{2 c} \left(b + \sqrt{- 4 a c + b^{2}}\right)\right ]$$
   in [58]:
sympy.solve(a * sympy.cos(x) - b * sympy.sin(x), x)

   out[58]:
   $$\left [ - 2 \operatorname{atan}{\left (\frac{1}{a} \left(b -
   \sqrt{a^{2} + b^{2}}\right) \right )}, \quad - 2
   \operatorname{atan}{\left (\frac{1}{a} \left(b + \sqrt{a^{2} +
   b^{2}}\right) \right )}\right ]$$
   in [59]:
sympy.solve(sympy.sin(x)-x, x)

---------------------------------------------------------------------------
notimplementederror                       traceback (most recent call last)
<ipython-input-59-5e1a5dc1567d> in <module>()
----> 1 sympy.solve(sympy.sin(x)-x, x)

/users/rob/miniconda/envs/py27-npm/lib/python2.7/site-packages/sympy/solvers/sol
vers.pyc in solve(f, *symbols, **flags)
    907     ####################################################################
#######
    908     if bare_f:
--> 909         solution = _solve(f[0], *symbols, **flags)
    910     else:
    911         solution = _solve_system(f, symbols, **flags)

/users/rob/miniconda/envs/py27-npm/lib/python2.7/site-packages/sympy/solvers/sol
vers.pyc in _solve(f, *symbols, **flags)
   1412     if result is false:
   1413         raise notimplementederror(msg +
-> 1414         "\nno algorithms are implemented to solve equation %s" % f)
   1415
   1416     if flags.get('simplify', true):

notimplementederror: multiple generators [x, sin(x)]
no algorithms are implemented to solve equation -x + sin(x)

bisection method[25]  

   in [60]:
# define a function, desired tolerance and starting interval [a, b]
f = lambda x: np.exp(x) - 2
tol = 0.1
a, b = -2, 2
x = np.linspace(-2.1, 2.1, 1000)

# graph the function f
fig, ax = plt.subplots(1, 1, figsize=(12, 4))

ax.plot(x, f(x), lw=1.5)
ax.axhline(0, ls=':', color='k')
ax.set_xticks([-2, -1, 0, 1, 2])
ax.set_xlabel(r'$x$', fontsize=18)
ax.set_ylabel(r'$f(x)$', fontsize=18)

# find the root using the bisection method and visualize
# the steps in the method in the graph
fa, fb = f(a), f(b)

ax.plot(a, fa, 'ko')
ax.plot(b, fb, 'ko')
ax.text(a, fa + 0.5, r"$a$", ha='center', fontsize=18)
ax.text(b, fb + 0.5, r"$b$", ha='center', fontsize=18)

n = 1
while b - a > tol:
    m = a + (b - a)/2
    fm = f(m)

    ax.plot(m, fm, 'ko')
    ax.text(m, fm - 0.5, r"$m_%d$" % n, ha='center')
    n += 1

    if np.sign(fa) == np.sign(fm):
        a, fa = m, fm
    else:
        b, fb = m, fm

ax.plot(m, fm, 'r*', markersize=10)
ax.annotate("root approximately at %.3f" % m,
            fontsize=14, family="serif",
            xy=(a, fm), xycoords='data',
            xytext=(-150, +50), textcoords='offset points',
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3, rad=-.5"))

ax.set_title("bisection method")

fig.tight_layout()
fig.savefig('ch5-nonlinear-bisection.pdf')

   [c9mnnas9jwaaaabjru5erkjggg== ]
   in [61]:
# define a function, desired tolerance and starting point xk
tol = 0.01
xk = 2

s_x = sympy.symbols("x")
s_f = sympy.exp(s_x) - 2

f = lambda x: sympy.lambdify(s_x, s_f, 'numpy')(x)
fp = lambda x: sympy.lambdify(s_x, sympy.diff(s_f, s_x), 'numpy')(x)

x = np.linspace(-1, 2.1, 1000)

# setup a graph for visualizing the root finding steps
fig, ax = plt.subplots(1, 1, figsize=(12,4))

ax.plot(x, f(x))
ax.axhline(0, ls=':', color='k')

# repeat newton's method until convergence to the desired tolerance has been rea
ched
n = 0
while f(xk) > tol:
    xk_new = xk - f(xk) / fp(xk)

    ax.plot([xk, xk], [0, f(xk)], color='k', ls=':')
    ax.plot(xk, f(xk), 'ko')
    ax.text(xk, -.5, r'$x_%d$' % n, ha='center')
    ax.plot([xk, xk_new], [f(xk), 0], 'k-')

    xk = xk_new
    n += 1

ax.plot(xk, f(xk), 'r*', markersize=15)
ax.annotate("root approximately at %.3f" % xk,
            fontsize=14, family="serif",
            xy=(xk, f(xk)), xycoords='data',
            xytext=(-150, +50), textcoords='offset points',
            arrowprops=dict(arrowstyle="->", connectionstyle="arc3, rad=-.5"))

ax.set_title("newton's method")
ax.set_xticks([-1, 0, 1, 2])
fig.tight_layout()
fig.savefig('ch5-nonlinear-newton.pdf')

   [waaaabjru5erkjggg== ]

scipy.optimize functions for root-finding[26]  

   in [62]:
optimize.bisect(lambda x: np.exp(x) - 2, -2, 2)

   out[62]:
   $$0.693147180559$$
   in [63]:
optimize.newton(lambda x: np.exp(x) - 2, 2)

   out[63]:
   $$0.69314718056$$
   in [64]:
x_root_guess = 2

   in [65]:
f = lambda x: np.exp(x) - 2

   in [66]:
fprime = lambda x: np.exp(x)

   in [67]:
optimize.newton(f, x_root_guess)

   out[67]:
   $$0.69314718056$$
   in [68]:
optimize.newton(f, x_root_guess, fprime=fprime)

   out[68]:
   $$0.69314718056$$
   in [69]:
optimize.brentq(lambda x: np.exp(x) - 2, -2, 2)

   out[69]:
   $$0.69314718056$$
   in [70]:
optimize.brenth(lambda x: np.exp(x) - 2, -2, 2)

   out[70]:
   $$0.69314718056$$
   in [71]:
optimize.ridder(lambda x: np.exp(x) - 2, -2, 2)

   out[71]:
   $$0.69314718056$$

multivariate[27]  

   in [72]:
def f(x):
    return [x[1] - x[0]**3 - 2 * x[0]**2 + 1, x[1] + x[0]**2 - 1]

   in [73]:
optimize.fsolve(f, [1, 1])

   out[73]:
array([ 0.73205081,  0.46410162])

   in [74]:
def f_jacobian(x):
    return [[-3*x[0]**2-4*x[0], 1], [2*x[0], 1]]

   in [75]:
optimize.fsolve(f, [1, 1], fprime=f_jacobian)

   out[75]:
array([ 0.73205081,  0.46410162])

   in [76]:
#import sympy as s

   in [77]:
x, y = sympy.symbols("x, y")

f_mat = sympy.matrix([y - x**3 -2*x**2 + 1, y + x**2 - 1])
f_mat.jacobian(sympy.matrix([x, y]))

   out[77]:
   $$\left[\begin{matrix}- 3 x^{2} - 4 x & 1\\2 x & 1\end{matrix}\right]$$
   in [78]:
#def f(x):
#    return [x[1] - x[0]**3 - 2 * x[0]**2 + 1, x[1] + x[0]**2 - 1]

x = np.linspace(-3, 2, 5000)
y1 = x**3 + 2 * x**2 -1
y2 = -x**2 + 1

fig, ax = plt.subplots(figsize=(8, 4))

ax.plot(x, y1, 'b', lw=1.5, label=r'$y = x^3 + 2x^2 - 1$')
ax.plot(x, y2, 'g', lw=1.5, label=r'$y = -x^2 + 1$')

x_guesses = [[-2, 2], [1, -1], [-2, -5]]
for x_guess in x_guesses:
    sol = optimize.fsolve(f, x_guess)
    ax.plot(sol[0], sol[1], 'r*', markersize=15)

    ax.plot(x_guess[0], x_guess[1], 'ko')
    ax.annotate("", xy=(sol[0], sol[1]), xytext=(x_guess[0], x_guess[1]),
                arrowprops=dict(arrowstyle="->", linewidth=2.5))

ax.legend(loc=0)
ax.set_xlabel(r'$x$', fontsize=18)
fig.tight_layout()
fig.savefig('ch5-nonlinear-system.pdf')

   [wrvnvv57l1wascbgayz8kjabhp0aaa4wkaaga8qqmajcdoaidxba0amj6g
   aqdgezqawhicbgayt9aaaop9axkxzfrevvmlaaaaaelftksuqmcc ]
   in [79]:
optimize.broyden2(f, x_guesses[1])

   out[79]:
array([ 0.73205079,  0.46410162])

   in [80]:
def f(x):
    return [x[1] - x[0]**3 - 2 * x[0]**2 + 1,
            x[1] + x[0]**2 - 1]

x = np.linspace(-3, 2, 5000)
y1 = x**3 + 2 * x**2 -1
y2 = -x**2 + 1

fig, ax = plt.subplots(figsize=(8, 4))

ax.plot(x, y1, 'k', lw=1.5, label=r'$y = x^3 + 2x^2 - 1$')
ax.plot(x, y2, 'k', lw=1.5, label=r'$y = -x^2 + 1$')

sol1 = optimize.fsolve(f, [-2,  2])
sol2 = optimize.fsolve(f, [ 1, -1])
sol3 = optimize.fsolve(f, [-2, -5])

colors = ['r', 'b', 'g']
for m in np.linspace(-4, 3, 80):
    for n in np.linspace(-15, 15, 40):
        x_guess = [m, n]
        sol = optimize.fsolve(f, x_guess)

        for idx, s in enumerate([sol1, sol2, sol3]):
            if abs(s-sol).max() < 1e-8:
                ax.plot(sol[0], sol[1], colors[idx]+'*', markersize=15)
                ax.plot(x_guess[0], x_guess[1], colors[idx]+'.')

ax.set_xlabel(r'$x$', fontsize=18)
fig.tight_layout()
fig.savefig('ch5-nonlinear-system-map.pdf')

/users/rob/miniconda/envs/py27-npm/lib/python2.7/site-packages/scipy/optimize/mi
npack.py:236: runtimewarning: the iteration is not making good progress, as meas
ured by the
  improvement from the last ten iterations.
  warnings.warn(msg, runtimewarning)

   [acua8kbxybuwwutghs9ourqb0a2noiikoijwo085kyqikipipbqhurrfurtfenrd
   oyikoiik9eigrleurveu69enjaioiqio1qmbgkvrfevrrec3niqikiqiwi9uabrfurrfsr7
   d0cik oiikyj26oveurveuxxr+h4ap2rndj2x7aaaaaelftksuqmcc ]

versions[28]  

   in [81]:
%reload_ext version_information

   in [82]:
%version_information sympy, scipy, numpy, matplotlib

   out[82]:
    software                      version
   python     2.7.10 64bit [gcc 4.2.1 (apple inc. build 5577)]
   ipython    3.2.1
   os         darwin 14.1.0 x86_64 i386 64bit
   sympy      0.7.6
   scipy      0.16.0
   numpy      1.9.2
   matplotlib 1.4.3

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [29]fastly, rendered by [30]rackspace

   nbviewer github [31]repository.

   nbviewer version: [32]33c4683

   nbconvert version: [33]5.4.0

   rendered (fri, 05 apr 2019 18:32:02 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb
   5. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb
   6. https://github.com/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb
   7. https://mybinder.org/v2/gh/jrjohansson/numerical-python-book-code/master?filepath=ch05-code-listing.ipynb
   8. https://raw.githubusercontent.com/jrjohansson/numerical-python-book-code/master/ch05-code-listing.ipynb
   9. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/tree/master
  10. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/tree/master/ch05-code-listing.ipynb
  11. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#chapter-5:-equation-solving
  12. http://www.apress.com/9781484205549
  13. http://www.apress.com/9781484205549
  14. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#linear-algebra---linear-equation-systems
  15. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#symbolic-approach
  16. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#numerical-approach
  17. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#example-:-rank-and-condition-numbers-->-numerical-errors
  18. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#rectangular-systems
  19. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#underdetermined
  20. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#overdetermined:-least-squares
  21. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#eigenvalue-problems
  22. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#nonlinear-equations
  23. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#univariate
  24. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#symbolic
  25. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#bisection-method
  26. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#scipy.optimize-functions-for-root-finding
  27. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#multivariate
  28. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch05-code-listing.ipynb#versions
  29. http://www.fastly.com/
  30. https://developer.rackspace.com/?nbviewer=awesome
  31. https://github.com/jupyter/nbviewer
  32. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  33. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
