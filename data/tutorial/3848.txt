   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]numerical-python-book-code
    2. [10]ch06-code-listing.ipynb

chapter 6: optimization[11]  

   robert johansson

   source code listings for [12]numerical python - a practical techniques
   approach for industry (isbn 978-1-484205-54-9).

   the source code listings can be downloaded from
   [13]http://www.apress.com/9781484205549
   in [1]:
%matplotlib inline
import matplotlib.pyplot as plt

   in [2]:
import numpy as np

   in [3]:
import sympy

   in [4]:
sympy.init_printing()

   in [5]:
from scipy import optimize

   in [6]:
import cvxopt

   in [7]:
from __future__ import division

univariate[14]  

   in [8]:
r, h = sympy.symbols("r, h")

   in [9]:
area = 2 * sympy.pi * r**2 + 2 * sympy.pi * r * h

   in [10]:
volume = sympy.pi * r**2 * h

   in [11]:
h_r = sympy.solve(volume - 1)[0]

   in [12]:
area_r = area.subs(h_r)

   in [13]:
rsol = sympy.solve(area_r.diff(r))[0]

   in [14]:
rsol

   out[14]:
   $$\frac{2^{\frac{2}{3}}}{2 \sqrt[3]{\pi}}$$
   in [15]:
_.evalf()

   out[15]:
   $$0.541926070139289$$
   in [16]:
# verify that the second derivative is positive, so that rsol is a minimum
area_r.diff(r, 2).subs(r, rsol)

   out[16]:
   $$12 \pi$$
   in [17]:
area_r.subs(r, rsol)

   out[17]:
   $$3 \sqrt[3]{2} \sqrt[3]{\pi}$$
   in [18]:
_.evalf()

   out[18]:
   $$5.53581044593209$$
   in [19]:
def f(r):
    return 2 * np.pi * r**2 + 2 / r

   in [20]:
r_min = optimize.brent(f, brack=(0.1, 4))

   in [21]:
r_min

   out[21]:
   $$0.541926077256$$
   in [22]:
f(r_min)

   out[22]:
   $$5.53581044593$$
   in [23]:
optimize.minimize_scalar(f, bracket=(0.1, 5))

   out[23]:
  fun: 5.5358104459320856
 nfev: 14
  nit: 13
    x: 0.54192606489766715

   in [24]:
r = np.linspace(0, 2, 100)

   in [25]:
fig, ax = plt.subplots(figsize=(8, 4))

ax.plot(r, f(r), lw=2, color='b')
ax.plot(r_min, f(r_min), 'r*', markersize=15)
ax.set_title(r"$f(r) = 2\pi r^2+2/r$", fontsize=18)
ax.set_xlabel(r"$r$", fontsize=18)
ax.set_xticks([0, 0.5, 1, 1.5, 2])
ax.set_ylim(0, 30)

fig.tight_layout()
fig.savefig('ch6-univariate-optimization-example.pdf')

/users/rob/miniconda/envs/py27-npm/lib/python2.7/site-packages/ipython/kernel/__
main__.py:2: runtimewarning: divide by zero encountered in true_divide
  from ipython.kernel.zmq import kernelapp as app

   [ d3fjmevzcr1oaaaaaelftksuqmcc ]

two-dimensional[15]  

   in [26]:
x1, x2 = sympy.symbols("x_1, x_2")

   in [27]:
f_sym = (x1-1)**4 + 5 * (x2-1)**2 - 2*x1*x2

   in [28]:
fprime_sym = [f_sym.diff(x_) for x_ in (x1, x2)]

   in [29]:
# gradient
sympy.matrix(fprime_sym)

   out[29]:
   $$\left[\begin{matrix}- 2 x_{2} + 4 \left(x_{1} - 1\right)^{3}\\- 2
   x_{1} + 10 x_{2} - 10\end{matrix}\right]$$
   in [30]:
fhess_sym = [[f_sym.diff(x1_, x2_) for x1_ in (x1, x2)] for x2_ in (x1, x2)]

   in [31]:
# hessian
sympy.matrix(fhess_sym)

   out[31]:
   $$\left[\begin{matrix}12 \left(x_{1} - 1\right)^{2} & -2\\-2 &
   10\end{matrix}\right]$$
   in [32]:
f_lmbda = sympy.lambdify((x1, x2), f_sym, 'numpy')

   in [33]:
fprime_lmbda = sympy.lambdify((x1, x2), fprime_sym, 'numpy')

   in [34]:
fhess_lmbda = sympy.lambdify((x1, x2), fhess_sym, 'numpy')

   in [35]:
def func_xy_x_y(f):
    """
    wrapper for f(x) -> f(x[0], x[1])
    """
    return lambda x: np.array(f(x[0], x[1]))

   in [36]:
f = func_xy_x_y(f_lmbda)

   in [37]:
fprime = func_xy_x_y(fprime_lmbda)

   in [38]:
fhess = func_xy_x_y(fhess_lmbda)

   in [39]:
x_opt = optimize.fmin_ncg(f, (0, 0), fprime=fprime, fhess=fhess)

optimization terminated successfully.
         current function value: -3.867223
         iterations: 8
         function evaluations: 10
         gradient evaluations: 17
         hessian evaluations: 8

   in [40]:
x_opt

   out[40]:
array([ 1.88292613,  1.37658523])

   in [41]:
fig, ax = plt.subplots(figsize=(6, 4))
x_ = y_ = np.linspace(-1, 4, 100)
x, y = np.meshgrid(x_, y_)
c = ax.contour(x, y, f_lmbda(x, y), 50)
ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15)
ax.set_xlabel(r"$x_1$", fontsize=18)
ax.set_ylabel(r"$x_2$", fontsize=18)
plt.colorbar(c, ax=ax)
fig.tight_layout()
fig.savefig('ch6-examaple-two-dim.pdf');

   [igmgb6ocfyvwrqu8ivgx7k3uqopmmjb0dxtpi8kdahcrz6ag7fktxdiydq zuxvvknzp
   y
   qxepae42tghzugjzcbgnfcceoapesid98lantgk2vrguo5q5csnkighxny3plwxan5m5dd6l
   pnbqn
   pkzqgqpradsamkeljkzza4ymbk8aryqq7wohmps3trpnzuanttfonbpnmaa9hi1go9gucvp
   wnbqn
   rlmmamhradqatzmgbuej0wg0zyiwhi1go9gucvpwnbqnrlmmamhradqatzmgbuej0wg0zyi
   whi1g o9gucf8hxibbttbqinqaaaaasuvork5cyii= ]

brute force search for initial point[16]  

   in [42]:
def f(x):
    x, y = x
    return (4 * np.sin(np.pi * x) + 6 * np.sin(np.pi * y)) + (x - 1)**2 + (y - 1
)**2

   in [43]:
x_start = optimize.brute(f, (slice(-3, 5, 0.5), slice(-3, 5, 0.5)), finish=none)

   in [44]:
x_start

   out[44]:
array([ 1.5,  1.5])

   in [45]:
f(x_start)

   out[45]:
   $$-9.5$$
   in [46]:
x_opt = optimize.fmin_bfgs(f, x_start)

optimization terminated successfully.
         current function value: -9.520229
         iterations: 4
         function evaluations: 28
         gradient evaluations: 7

   in [47]:
x_opt

   out[47]:
array([ 1.47586906,  1.48365788])

   in [48]:
f(x_opt)

   out[48]:
   $$-9.52022927306$$
   in [49]:
def func_x_y_to_xy(f, x, y):
    s = np.shape(x)
    return f(np.vstack([x.ravel(), y.ravel()])).reshape(*s)

   in [50]:
fig, ax = plt.subplots(figsize=(6, 4))
x_ = y_ = np.linspace(-3, 5, 100)
x, y = np.meshgrid(x_, y_)
c = ax.contour(x, y, func_x_y_to_xy(f, x, y), 25)
ax.plot(x_opt[0], x_opt[1], 'r*', markersize=15)
ax.set_xlabel(r"$x_1$", fontsize=18)
ax.set_ylabel(r"$x_2$", fontsize=18)
plt.colorbar(c, ax=ax)
fig.tight_layout()
fig.savefig('ch6-example-2d-many-minima.pdf');

   [j8oepikbftlmaaaaabjru5erkjggg== ]

nonlinear least square[17]  

   in [51]:
def f(x, beta0, beta1, beta2):
    return beta0 + beta1 * np.exp(-beta2 * x**2)

   in [52]:
beta = (0.25, 0.75, 0.5)

   in [53]:
xdata = np.linspace(0, 5, 50)

   in [54]:
y = f(xdata, *beta)

   in [55]:
ydata = y + 0.05 * np.random.randn(len(xdata))

   in [56]:
def g(beta):
    return ydata - f(xdata, *beta)

   in [57]:
beta_start = (1, 1, 1)

   in [58]:
beta_opt, beta_cov = optimize.leastsq(g, beta_start)

   in [59]:
beta_opt

   out[59]:
array([ 0.24647649,  0.76175477,  0.48895486])

   in [60]:
fig, ax = plt.subplots()

ax.scatter(xdata, ydata)
ax.plot(xdata, y, 'r', lw=2)
ax.plot(xdata, f(xdata, *beta_opt), 'b', lw=2)
ax.set_xlim(0, 5)
ax.set_xlabel(r"$x$", fontsize=18)
ax.set_ylabel(r"$f(x, \beta)$", fontsize=18)

fig.tight_layout()
fig.savefig('ch6-nonlinear-least-square.pdf')

   [3teyxkt+ik1aaaaaelf tksuqmcc ]
   in [61]:
beta_opt, beta_cov = optimize.curve_fit(f, xdata, ydata)

   in [62]:
beta_opt

   out[62]:
array([ 0.24647649,  0.76175477,  0.48895486])

constrained optimization[18]  

bounds[19]  

   in [63]:
def f(x):
    x, y = x
    return (x-1)**2 + (y-1)**2

   in [64]:
x_opt = optimize.minimize(f, (0, 0), method='bfgs').x

   in [65]:
bnd_x1, bnd_x2 = (2, 3), (0, 2)

   in [66]:
x_cons_opt = optimize.minimize(f, np.array([0, 0]), method='l-bfgs-b', bounds=[b
nd_x1, bnd_x2]).x

   in [67]:
fig, ax = plt.subplots(figsize=(6, 4))
x_ = y_ = np.linspace(-1, 3, 100)
x, y = np.meshgrid(x_, y_)
c = ax.contour(x, y, func_x_y_to_xy(f, x, y), 50)
ax.plot(x_opt[0], x_opt[1], 'b*', markersize=15)
ax.plot(x_cons_opt[0], x_cons_opt[1], 'r*', markersize=15)
bound_rect = plt.rectangle((bnd_x1[0], bnd_x2[0]),
                           bnd_x1[1] - bnd_x1[0], bnd_x2[1] - bnd_x2[0],
                           facecolor="grey")
ax.add_patch(bound_rect)
ax.set_xlabel(r"$x_1$", fontsize=18)
ax.set_ylabel(r"$x_2$", fontsize=18)
plt.colorbar(c, ax=ax)

fig.tight_layout()
fig.savefig('ch6-example-constraint-bound.pdf');

   [h+ri3mmnfkqtgaaaabjru5erkjggg== ]

lagrange multiplier[20]  

   in [68]:
x = x1, x2, x3, l = sympy.symbols("x_1, x_2, x_3, lambda")

   in [69]:
f = x1 * x2 * x3

   in [70]:
g = 2 * (x1 * x2 + x2 * x3 + x3 * x1) - 1

   in [71]:
l = f + l * g

   in [72]:
grad_l = [sympy.diff(l, x_) for x_ in x]

   in [73]:
sols = sympy.solve(grad_l)
sols

   out[73]:
   $$\left [ \left \{ \lambda : - \frac{\sqrt{6}}{24}, \quad x_{1} :
   \frac{\sqrt{6}}{6}, \quad x_{2} : \frac{\sqrt{6}}{6}, \quad x_{3} :
   \frac{\sqrt{6}}{6}\right \}, \quad \left \{ \lambda :
   \frac{\sqrt{6}}{24}, \quad x_{1} : - \frac{\sqrt{6}}{6}, \quad x_{2} :
   - \frac{\sqrt{6}}{6}, \quad x_{3} : - \frac{\sqrt{6}}{6}\right \}\right
   ]$$
   in [74]:
g.subs(sols[0])

   out[74]:
   $$0$$
   in [75]:
f.subs(sols[0])

   out[75]:
   $$\frac{\sqrt{6}}{36}$$
   in [76]:
def f(x):
    return -x[0] * x[1] * x[2]

   in [77]:
def g(x):
    return 2 * (x[0]*x[1] + x[1] * x[2] + x[2] * x[0]) - 1

   in [78]:
constraints = [dict(type='eq', fun=g)]

   in [79]:
result = optimize.minimize(f, [0.5, 1, 1.5], method='slsqp', constraints=constra
ints)

   in [80]:
result

   out[80]:
  status: 0
 success: true
    njev: 18
    nfev: 95
     fun: -0.068041368623352985
       x: array([ 0.40824187,  0.40825127,  0.40825165])
 message: 'optimization terminated successfully.'
     jac: array([-0.16666925, -0.16666542, -0.16666527,  0.        ])
     nit: 18

   in [81]:
result.x

   out[81]:
array([ 0.40824187,  0.40825127,  0.40825165])

inequality constraints[21]  

   in [82]:
def f(x):
    return (x[0] - 1)**2 + (x[1] - 1)**2

def g(x):
    return x[1] - 1.75 - (x[0] - 0.75)**4

   in [83]:
x_opt = optimize.minimize(f, (0, 0), method='bfgs').x

   in [84]:
constraints = [dict(type='ineq', fun=g)]

   in [85]:
x_cons_opt = optimize.minimize(f, (0, 0), method='slsqp', constraints=constraint
s).x

   in [86]:
x_cons_opt = optimize.minimize(f, (0, 0), method='cobyla', constraints=constrain
ts).x

   in [87]:
fig, ax = plt.subplots(figsize=(6, 4))
x_ = y_ = np.linspace(-1, 3, 100)
x, y = np.meshgrid(x_, y_)
c = ax.contour(x, y, func_x_y_to_xy(f, x, y), 50)
ax.plot(x_opt[0], x_opt[1], 'b*', markersize=15)

ax.plot(x_, 1.75 + (x_-0.75)**4, 'k-', markersize=15)
ax.fill_between(x_, 1.75 + (x_-0.75)**4, 3, color="grey")
ax.plot(x_cons_opt[0], x_cons_opt[1], 'r*', markersize=15)

ax.set_ylim(-1, 3)
ax.set_xlabel(r"$x_0$", fontsize=18)
ax.set_ylabel(r"$x_1$", fontsize=18)
plt.colorbar(c, ax=ax)

fig.tight_layout()
fig.savefig('ch6-example-constraint-inequality.pdf');

   [b4amrtx0k6a4aaaaaelftksuqmcc ]

id135[22]  

   in [88]:
c = np.array([-1.0, 2.0, -3.0])

a = np.array([[ 1.0, 1.0, 0.0],
              [-1.0, 3.0, 0.0],
              [ 0.0, -1.0, 1.0]])

b = np.array([1.0, 2.0, 3.0])

   in [89]:
a_ = cvxopt.matrix(a)
b_ = cvxopt.matrix(b)
c_ = cvxopt.matrix(c)

   in [90]:
sol = cvxopt.solvers.lp(c_, a_, b_)

optimal solution found.

   in [91]:
x = np.array(sol['x'])

   in [92]:
x

   out[92]:
array([[ 0.25],
       [ 0.75],
       [ 3.75]])

   in [93]:
sol

   out[93]:
{'dual infeasibility': 1.4835979218054372e-16,
 'dual objective': -10.0,
 'dual slack': 0.0,
 'gap': 0.0,
 'iterations': 0,
 'primal infeasibility': 0.0,
 'primal objective': -10.0,
 'primal slack': -0.0,
 'relative gap': 0.0,
 'residual as dual infeasibility certificate': none,
 'residual as primal infeasibility certificate': none,
 's': <3x1 matrix, tc='d'>,
 'status': 'optimal',
 'x': <3x1 matrix, tc='d'>,
 'y': <0x1 matrix, tc='d'>,
 'z': <3x1 matrix, tc='d'>}

   in [94]:
sol['primal objective']

   out[94]:
   $$-10.0$$

versions[23]  

   in [95]:
%reload_ext version_information

   in [96]:
%version_information numpy, scipy, cvxopt, sympy, matplotlib

   out[96]:
    software                      version
   python     2.7.10 64bit [gcc 4.2.1 (apple inc. build 5577)]
   ipython    3.2.1
   os         darwin 14.1.0 x86_64 i386 64bit
   numpy      1.9.2
   scipy      0.16.0
   cvxopt     1.1.7
   sympy      0.7.6
   matplotlib 1.4.3

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [24]fastly, rendered by [25]rackspace

   nbviewer github [26]repository.

   nbviewer version: [27]33c4683

   nbconvert version: [28]5.4.0

   rendered (fri, 05 apr 2019 18:32:04 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb
   5. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb
   6. https://github.com/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb
   7. https://mybinder.org/v2/gh/jrjohansson/numerical-python-book-code/master?filepath=ch06-code-listing.ipynb
   8. https://raw.githubusercontent.com/jrjohansson/numerical-python-book-code/master/ch06-code-listing.ipynb
   9. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/tree/master
  10. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/tree/master/ch06-code-listing.ipynb
  11. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#chapter-6:-optimization
  12. http://www.apress.com/9781484205549
  13. http://www.apress.com/9781484205549
  14. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#univariate
  15. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#two-dimensional
  16. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#brute-force-search-for-initial-point
  17. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#nonlinear-least-square
  18. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#constrained-optimization
  19. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#bounds
  20. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#lagrange-multiplier
  21. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#inequality-constraints
  22. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#linear-programming
  23. https://nbviewer.jupyter.org/github/jrjohansson/numerical-python-book-code/blob/master/ch06-code-listing.ipynb#versions
  24. http://www.fastly.com/
  25. https://developer.rackspace.com/?nbviewer=awesome
  26. https://github.com/jupyter/nbviewer
  27. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  28. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
