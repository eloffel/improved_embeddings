   (button) toggle navigation
   [1][nav_logo.svg?v=479cefe8d932fb14a67b93911b97d70f]
     * [2]jupyter
     * [3]faq
     * [4]view as code
     * [5]python 2 kernel
     * [6]view on github
     * [7]execute on binder
     * [8]download notebook

    1. [9]scientific-python-lectures
    2. [10]lecture-3-scipy.ipynb

scipy - library of scientific algorithms for python[11]  

   j.r. johansson (jrjohansson at gmail.com)

   the latest version of this [12]ipython notebook lecture is available at
   [13]http://github.com/jrjohansson/scientific-python-lectures.

   the other notebooks in this lecture series are indexed at
   [14]http://jrjohansson.github.io.
   in [1]:
# what is this line all about? answer in lecture 4
%matplotlib inline
import matplotlib.pyplot as plt
from ipython.display import image

introduction[15]  

   the scipy framework builds on top of the low-level numpy framework for
   multidimensional arrays, and provides a large number of higher-level
   scientific algorithms. some of the topics that scipy covers are:
     * special functions ([16]scipy.special)
     * integration ([17]scipy.integrate)
     * optimization ([18]scipy.optimize)
     * interpolation ([19]scipy.interpolate)
     * fourier transforms ([20]scipy.fftpack)
     * signal processing ([21]scipy.signal)
     * id202 ([22]scipy.linalg)
     * sparse eigenvalue problems ([23]scipy.sparse)
     * statistics ([24]scipy.stats)
     * multi-dimensional image processing ([25]scipy.ndimage)
     * file io ([26]scipy.io)

   each of these submodules provides a number of functions and classes
   that can be used to solve problems in their respective topics.

   in this lecture we will look at how to use some of these subpackages.

   to access the scipy package in a python program, we start by importing
   everything from the scipy module.
   in [2]:
from scipy import *

   if we only need to use part of the scipy framework we can selectively
   include only those modules we are interested in. for example, to
   include the id202 package under the name la, we can do:
   in [3]:
import scipy.linalg as la

special functions[27]  

   a large number of mathematical special functions are important for many
   computional physics problems. scipy provides implementations of a very
   extensive set of special functions. for details, see the list of
   functions in the reference documention at
   [28]http://docs.scipy.org/doc/scipy/reference/special.html#module-scipy
   .special.

   to demonstrate the typical usage of special functions we will look in
   more detail at the bessel functions:
   in [4]:
#
# the scipy.special module includes a large number of bessel-functions
# here we will use the functions jn and yn, which are the bessel functions
# of the first and second kind and real-valued order. we also include the
# function jn_zeros and yn_zeros that gives the zeroes of the functions jn
# and yn.
#
from scipy.special import jn, yn, jn_zeros, yn_zeros

   in [5]:
n = 0    # order
x = 0.0

# bessel function of first kind
print "j_%d(%f) = %f" % (n, x, jn(n, x))

x = 1.0
# bessel function of second kind
print "y_%d(%f) = %f" % (n, x, yn(n, x))

j_0(0.000000) = 1.000000
y_0(1.000000) = 0.088257

   in [6]:
x = linspace(0, 10, 100)

fig, ax = plt.subplots()
for n in range(4):
    ax.plot(x, jn(n, x), label=r"$j_%d(x)$" % n)
ax.legend();

   [aziodqbst2i5aaaaaelf tksuqmcc ]
   in [7]:
# zeros of bessel functions
n = 0 # order
m = 4 # number of roots to compute
jn_zeros(n, m)

   out[7]:
array([  2.40482556,   5.52007811,   8.65372791,  11.79153444])

integration[29]  

numerical integration: quadrature[30]  

   numerical evaluation of a function of the type

   $\displaystyle \int_a^b f(x) dx$

   is called numerical quadrature, or simply quadature. scipy provides a
   series of functions for different kind of quadrature, for example the
   quad, dblquad and tplquad for single, double and triple integrals,
   respectively.
   in [8]:
from scipy.integrate import quad, dblquad, tplquad

   the quad function takes a large number of optional arguments, which can
   be used to fine-tune the behaviour of the function (try help(quad) for
   details).

   the basic usage is as follows:
   in [9]:
# define a simple function for the integrand
def f(x):
    return x

   in [10]:
x_lower = 0 # the lower limit of x
x_upper = 1 # the upper limit of x

val, abserr = quad(f, x_lower, x_upper)

print "integral value =", val, ", absolute error =", abserr

integral value = 0.5 , absolute error = 5.55111512313e-15

   if we need to pass extra arguments to integrand function we can use the
   args keyword argument:
   in [11]:
def integrand(x, n):
    """
    bessel function of first kind and order n.
    """
    return jn(n, x)


x_lower = 0  # the lower limit of x
x_upper = 10 # the upper limit of x

val, abserr = quad(integrand, x_lower, x_upper, args=(3,))

print val, abserr

0.736675137081 9.3891268825e-13

   for simple functions we can use a lambda function (name-less function)
   instead of explicitly defining a function for the integrand:
   in [12]:
val, abserr = quad(lambda x: exp(-x ** 2), -inf, inf)

print "numerical  =", val, abserr

analytical = sqrt(pi)
print "analytical =", analytical

numerical  = 1.77245385091 1.42026367809e-08
analytical = 1.77245385091

   as show in the example above, we can also use 'inf' or '-inf' as
   integral limits.

   higher-dimensional integration works in the same way:
   in [13]:
def integrand(x, y):
    return exp(-x**2-y**2)

x_lower = 0
x_upper = 10
y_lower = 0
y_upper = 10

val, abserr = dblquad(integrand, x_lower, x_upper, lambda x : y_lower, lambda x:
 y_upper)

print val, abserr

0.785398163397 1.63822994214e-13

   note how we had to pass lambda functions for the limits for the y
   integration, since these in general can be functions of x.

ordinary differential equations (odes)[31]  

   scipy provides two different ways to solve odes: an api based on the
   function odeint, and object-oriented api based on the class ode.
   usually odeint is easier to get started with, but the ode class offers
   some finer level of control.

   here we will use the odeint functions. for more information about the
   class ode, try help(ode). it does pretty much the same thing as odeint,
   but in an object-oriented fashion.

   to use odeint, first import it from the scipy.integrate module
   in [14]:
from scipy.integrate import odeint, ode

   a system of odes are usually formulated on standard form before it is
   attacked numerically. the standard form is:

   $y' = f(y, t)$

   where

   $y = [y_1(t), y_2(t), ..., y_n(t)]$

   and $f$ is some function that gives the derivatives of the function
   $y_i(t)$. to solve an ode we need to know the function $f$ and an
   initial condition, $y(0)$.

   note that higher-order odes can always be written in this form by
   introducing new variables for the intermediate derivatives.

   once we have defined the python function f and array y_0 (that is $f$
   and $y(0)$ in the mathematical formulation), we can use the odeint
   function as:
y_t = odeint(f, y_0, t)


   where t is and array with time-coordinates for which to solve the ode
   problem. y_t is an array with one row for each point in time in t,
   where each column corresponds to a solution y_i(t) at that point in
   time.

   we will see how we can implement f and y_0 in python code in the
   examples below.

example: double pendulum[32]  

   let's consider a physical example: the double compound pendulum,
   described in some detail here:
   [33]http://en.wikipedia.org/wiki/double_pendulum
   in [15]:
image(url='http://upload.wikimedia.org/wikipedia/commons/c/c9/double-compound-pe
ndulum-dimensioned.svg')

   out[15]:
   [double-compound-pendulum-dimensioned.svg]

   the equations of motion of the pendulum are given on the wiki page:

   ${\dot \theta_1} = \frac{6}{m\ell^2} \frac{ 2 p_{\theta_1} - 3
   \cos(\theta_1-\theta_2) p_{\theta_2}}{16 - 9
   \cos^2(\theta_1-\theta_2)}$

   ${\dot \theta_2} = \frac{6}{m\ell^2} \frac{ 8 p_{\theta_2} - 3
   \cos(\theta_1-\theta_2) p_{\theta_1}}{16 - 9
   \cos^2(\theta_1-\theta_2)}.$

   ${\dot p_{\theta_1}} = -\frac{1}{2} m \ell^2 \left [ {\dot \theta_1}
   {\dot \theta_2} \sin (\theta_1-\theta_2) + 3 \frac{g}{\ell} \sin
   \theta_1 \right ]$

   ${\dot p_{\theta_2}} = -\frac{1}{2} m \ell^2 \left [ -{\dot \theta_1}
   {\dot \theta_2} \sin (\theta_1-\theta_2) + \frac{g}{\ell} \sin \theta_2
   \right]$

   to make the python code simpler to follow, let's introduce new variable
   names and the vector notation: $x = [\theta_1, \theta_2, p_{\theta_1},
   p_{\theta_2}]$

   ${\dot x_1} = \frac{6}{m\ell^2} \frac{ 2 x_3 - 3 \cos(x_1-x_2) x_4}{16
   - 9 \cos^2(x_1-x_2)}$

   ${\dot x_2} = \frac{6}{m\ell^2} \frac{ 8 x_4 - 3 \cos(x_1-x_2) x_3}{16
   - 9 \cos^2(x_1-x_2)}$

   ${\dot x_3} = -\frac{1}{2} m \ell^2 \left [ {\dot x_1} {\dot x_2} \sin
   (x_1-x_2) + 3 \frac{g}{\ell} \sin x_1 \right ]$

   ${\dot x_4} = -\frac{1}{2} m \ell^2 \left [ -{\dot x_1} {\dot x_2} \sin
   (x_1-x_2) + \frac{g}{\ell} \sin x_2 \right]$
   in [16]:
g = 9.82
l = 0.5
m = 0.1

def dx(x, t):
    """
    the right-hand side of the pendulum ode
    """
    x1, x2, x3, x4 = x[0], x[1], x[2], x[3]

    dx1 = 6.0/(m*l**2) * (2 * x3 - 3 * cos(x1-x2) * x4)/(16 - 9 * cos(x1-x2)**2)
    dx2 = 6.0/(m*l**2) * (8 * x4 - 3 * cos(x1-x2) * x3)/(16 - 9 * cos(x1-x2)**2)
    dx3 = -0.5 * m * l**2 * ( dx1 * dx2 * sin(x1-x2) + 3 * (g/l) * sin(x1))
    dx4 = -0.5 * m * l**2 * (-dx1 * dx2 * sin(x1-x2) + (g/l) * sin(x2))

    return [dx1, dx2, dx3, dx4]

   in [17]:
# choose an initial state
x0 = [pi/4, pi/2, 0, 0]

   in [18]:
# time coodinate to solve the ode for: from 0 to 10 seconds
t = linspace(0, 10, 250)

   in [19]:
# solve the ode problem
x = odeint(dx, x0, t)

   in [20]:
# plot the angles as a function of time

fig, axes = plt.subplots(1,2, figsize=(12,4))
axes[0].plot(t, x[:, 0], 'r', label="theta1")
axes[0].plot(t, x[:, 1], 'b', label="theta2")


x1 = + l * sin(x[:, 0])
y1 = - l * cos(x[:, 0])

x2 = x1 + l * sin(x[:, 1])
y2 = y1 - l * cos(x[:, 1])

axes[1].plot(x1, y1, 'r', label="pendulum1")
axes[1].plot(x2, y2, 'b', label="pendulum2")
axes[1].set_ylim([-1, 0])
axes[1].set_xlim([1, -1]);

   [zwaaaabjru5erkjggg== ]

   simple annimation of the pendulum motion. we will see how to make
   better animation in lecture 4.
   in [21]:
from ipython.display import display, clear_output
import time

   in [22]:
fig, ax = plt.subplots(figsize=(4,4))

for t_idx, tt in enumerate(t[:200]):

    x1 = + l * sin(x[t_idx, 0])
    y1 = - l * cos(x[t_idx, 0])

    x2 = x1 + l * sin(x[t_idx, 1])
    y2 = y1 - l * cos(x[t_idx, 1])

    ax.cla()
    ax.plot([0, x1], [0, y1], 'r.-')
    ax.plot([x1, x2], [y1, y2], 'b.-')
    ax.set_ylim([-1.5, 0.5])
    ax.set_xlim([1, -1])

    clear_output()
    display(fig)

    time.sleep(0.1)

   [hj8aaaaasuvork5cyii= ]
   [hj8aaaaasuvork5cyii= ]

example: damped harmonic oscillator[34]  

   ode problems are important in computational physics, so we will look at
   one more example: the damped harmonic oscillation. this problem is well
   described on the wiki page: [35]http://en.wikipedia.org/wiki/damping

   the equation of motion for the damped oscillator is:

   $\displaystyle \frac{\mathrm{d}^2x}{\mathrm{d}t^2} +
   2\zeta\omega_0\frac{\mathrm{d}x}{\mathrm{d}t} + \omega^2_0 x = 0$

   where $x$ is the position of the oscillator, $\omega_0$ is the
   frequency, and $\zeta$ is the damping ratio. to write this second-order
   ode on standard form we introduce $p =
   \frac{\mathrm{d}x}{\mathrm{d}t}$:

   $\displaystyle \frac{\mathrm{d}p}{\mathrm{d}t} = - 2\zeta\omega_0 p -
   \omega^2_0 x$

   $\displaystyle \frac{\mathrm{d}x}{\mathrm{d}t} = p$

   in the implementation of this example we will add extra arguments to
   the rhs function for the ode, rather than using global variables as we
   did in the previous example. as a consequence of the extra arguments to
   the rhs, we need to pass an keyword argument args to the odeint
   function:
   in [23]:
def dy(y, t, zeta, w0):
    """
    the right-hand side of the damped oscillator ode
    """
    x, p = y[0], y[1]

    dx = p
    dp = -2 * zeta * w0 * p - w0**2 * x

    return [dx, dp]

   in [24]:
# initial state:
y0 = [1.0, 0.0]

   in [25]:
# time coodinate to solve the ode for
t = linspace(0, 10, 1000)
w0 = 2*pi*1.0

   in [26]:
# solve the ode problem for three different values of the damping ratio

y1 = odeint(dy, y0, t, args=(0.0, w0)) # undamped
y2 = odeint(dy, y0, t, args=(0.2, w0)) # under damped
y3 = odeint(dy, y0, t, args=(1.0, w0)) # critial damping
y4 = odeint(dy, y0, t, args=(5.0, w0)) # over damped

   in [27]:
fig, ax = plt.subplots()
ax.plot(t, y1[:,0], 'k', label="undamped", linewidth=0.25)
ax.plot(t, y2[:,0], 'r', label="under damped")
ax.plot(t, y3[:,0], 'b', label=r"critical damping")
ax.plot(t, y4[:,0], 'g', label="over damped")
ax.legend();

   [q duuaaaaasuvork5cyii= ]

fourier transform[36]  

   fourier transforms are one of the universal tools in computational
   physics, which appear over and over again in different contexts. scipy
   provides functions for accessing the classic [37]fftpack library from
   netlib, which is an efficient and well tested fft library written in
   fortran. the scipy api has a few additional convenience functions, but
   overall the api is closely related to the original fortran library.

   to use the fftpack module in a python program, include it using:
   in [28]:
from numpy.fft import fftfreq
from scipy.fftpack import *

   to demonstrate how to do a fast fourier transform with scipy, let's
   look at the fft of the solution to the damped oscillator from the
   previous section:
   in [29]:
n = len(t)
dt = t[1]-t[0]

# calculate the fast fourier transform
# y2 is the solution to the under-damped oscillator from the previous section
f = fft(y2[:,0])

# calculate the frequencies for the components in f
w = fftfreq(n, dt)

   in [30]:
fig, ax = plt.subplots(figsize=(9,3))
ax.plot(w, abs(f));

   [8qpgqj9luryaaaaasuvork5cyii= ]

   since the signal is real, the spectrum is symmetric. we therefore only
   need to plot the part that corresponds to the postive frequencies. to
   extract that part of the w and f we can use some of the indexing tricks
   for numpy arrays that we saw in lecture 2:
   in [31]:
indices = where(w > 0) # select only indices for elements that corresponds to po
sitive frequencies
w_pos = w[indices]
f_pos = f[indices]

   in [32]:
fig, ax = plt.subplots(figsize=(9,3))
ax.plot(w_pos, abs(f_pos))
ax.set_xlim(0, 5);

   [wcicnyp0czpngaaaabj ru5erkjggg== ]

   as expected, we now see a peak in the spectrum that is centered around
   1, which is the frequency we used in the damped oscillator example.

id202[38]  

   the id202 module contains a lot of matrix related functions,
   including linear equation solving, eigenvalue solvers, matrix functions
   (for example matrix-exponentiation), a number of different
   decompositions (svd, lu, cholesky), etc.

   detailed documetation is available at:
   [39]http://docs.scipy.org/doc/scipy/reference/linalg.html

   here we will look at how to use some of these functions:

linear equation systems[40]  

   linear equation systems on the matrix form

   $a x = b$

   where $a$ is a matrix and $x,b$ are vectors can be solved like:
   in [33]:
from scipy.linalg import *

   in [34]:
a = array([[1,2,3], [4,5,6], [7,8,9]])
b = array([1,2,3])

   in [35]:
x = solve(a, b)

x

   out[35]:
array([-0.33333333,  0.66666667,  0.        ])

   in [36]:
# check
dot(a, x) - b

   out[36]:
array([ -1.11022302e-16,   0.00000000e+00,   0.00000000e+00])

   we can also do the same with

   $a x = b$

   where $a, b, x$ are matrices:
   in [37]:
a = rand(3,3)
b = rand(3,3)

   in [38]:
x = solve(a, b)

   in [39]:
x

   out[39]:
array([[ 1.19168749,  1.34543171,  0.38437594],
       [-0.88153715, -3.22735597,  0.66370273],
       [ 0.10044006,  1.0465058 ,  0.39801748]])

   in [40]:
# check
norm(dot(a, x) - b)

   out[40]:
2.0014830212433605e-16

eigenvalues and eigenvectors[41]  

   the eigenvalue problem for a matrix $a$:

   $\displaystyle a v_n = \lambda_n v_n$

   where $v_n$ is the $n$th eigenvector and $\lambda_n$ is the $n$th
   eigenvalue.

   to calculate eigenvalues of a matrix, use the eigvals and for
   calculating both eigenvalues and eigenvectors, use the function eig:
   in [41]:
evals = eigvals(a)

   in [42]:
evals

   out[42]:
array([ 1.08466629+0.j,  0.33612878+0.j, -0.28229973+0.j])

   in [43]:
evals, evecs = eig(a)

   in [44]:
evals

   out[44]:
array([ 1.08466629+0.j,  0.33612878+0.j, -0.28229973+0.j])

   in [45]:
evecs

   out[45]:
array([[-0.20946865, -0.48428024, -0.14392087],
       [-0.79978578,  0.8616452 , -0.79527482],
       [-0.56255275,  0.15178997,  0.58891829]])

   the eigenvectors corresponding to the $n$th eigenvalue (stored in
   evals[n]) is the $n$th column in evecs, i.e., evecs[:,n]. to verify
   this, let's try mutiplying eigenvectors with the matrix and compare to
   the product of the eigenvector and the eigenvalue:
   in [46]:
n = 1

norm(dot(a, evecs[:,n]) - evals[n] * evecs[:,n])

   out[46]:
3.243515426387745e-16

   there are also more specialized eigensolvers, like the eigh for
   hermitian matrices.

matrix operations[42]  

   in [47]:
# the matrix inverse
inv(a)

   out[47]:
array([[ 2.0031935 , -0.63411453,  0.49891784],
       [-4.63643938, -0.2212669 ,  3.35170585],
       [ 1.06421936,  1.37366073, -1.42726809]])

   in [48]:
# determinant
det(a)

   out[48]:
-0.10292296739753022

   in [49]:
# norms of various orders
norm(a, ord=2), norm(a, ord=inf)

   out[49]:
(1.3060382297688262, 1.591998214728641)

sparse matrices[43]  

   sparse matrices are often useful in numerical simulations dealing with
   large systems, if the problem can be described in matrix form where the
   matrices or vectors mostly contains zeros. scipy has a good support for
   sparse matrices, with basic id202 operations (such as equation
   solving, eigenvalue calculations, etc).

   there are many possible strategies for storing sparse matrices in an
   efficient way. some of the most common are the so-called coordinate
   form (coo), list of list (lil) form, and compressed-sparse column csc
   (and row, csr). each format has some advantanges and disadvantages.
   most computational algorithms (equation solving, matrix-matrix
   multiplication, etc) can be efficiently implemented using csr or csc
   formats, but they are not so intuitive and not so easy to initialize.
   so often a sparse matrix is initially created in coo or lil format
   (where we can efficiently add elements to the sparse matrix data), and
   then converted to csc or csr before used in real calcalations.

   for more information about these sparse formats, see e.g.
   [44]http://en.wikipedia.org/wiki/sparse_matrix

   when we create a sparse matrix we have to choose which format it should
   be stored in. for example,
   in [50]:
from scipy.sparse import *

   in [51]:
# dense matrix
m = array([[1,0,0,0], [0,3,0,0], [0,1,1,0], [1,0,0,1]]); m

   out[51]:
array([[1, 0, 0, 0],
       [0, 3, 0, 0],
       [0, 1, 1, 0],
       [1, 0, 0, 1]])

   in [52]:
# convert from dense to sparse
a = csr_matrix(m); a

   out[52]:
<4x4 sparse matrix of type '<type 'numpy.int64'>'
        with 6 stored elements in compressed sparse row format>

   in [53]:
# convert from sparse to dense
a.todense()

   out[53]:
matrix([[1, 0, 0, 0],
        [0, 3, 0, 0],
        [0, 1, 1, 0],
        [1, 0, 0, 1]])

   more efficient way to create sparse matrices: create an empty matrix
   and populate with using matrix indexing (avoids creating a potentially
   large dense matrix)
   in [54]:
a = lil_matrix((4,4)) # empty 4x4 sparse matrix
a[0,0] = 1
a[1,1] = 3
a[2,2] = a[2,1] = 1
a[3,3] = a[3,0] = 1
a

   out[54]:
<4x4 sparse matrix of type '<type 'numpy.float64'>'
        with 6 stored elements in linked list format>

   in [55]:
a.todense()

   out[55]:
matrix([[ 1.,  0.,  0.,  0.],
        [ 0.,  3.,  0.,  0.],
        [ 0.,  1.,  1.,  0.],
        [ 1.,  0.,  0.,  1.]])

   converting between different sparse matrix formats:
   in [56]:
a

   out[56]:
<4x4 sparse matrix of type '<type 'numpy.float64'>'
        with 6 stored elements in linked list format>

   in [57]:
a = csr_matrix(a); a

   out[57]:
<4x4 sparse matrix of type '<type 'numpy.float64'>'
        with 6 stored elements in compressed sparse row format>

   in [58]:
a = csc_matrix(a); a

   out[58]:
<4x4 sparse matrix of type '<type 'numpy.float64'>'
        with 6 stored elements in compressed sparse column format>

   we can compute with sparse matrices like with dense matrices:
   in [59]:
a.todense()

   out[59]:
matrix([[ 1.,  0.,  0.,  0.],
        [ 0.,  3.,  0.,  0.],
        [ 0.,  1.,  1.,  0.],
        [ 1.,  0.,  0.,  1.]])

   in [60]:
(a * a).todense()

   out[60]:
matrix([[ 1.,  0.,  0.,  0.],
        [ 0.,  9.,  0.,  0.],
        [ 0.,  4.,  1.,  0.],
        [ 2.,  0.,  0.,  1.]])

   in [61]:
a.todense()

   out[61]:
matrix([[ 1.,  0.,  0.,  0.],
        [ 0.,  3.,  0.,  0.],
        [ 0.,  1.,  1.,  0.],
        [ 1.,  0.,  0.,  1.]])

   in [62]:
a.dot(a).todense()

   out[62]:
matrix([[ 1.,  0.,  0.,  0.],
        [ 0.,  9.,  0.,  0.],
        [ 0.,  4.,  1.,  0.],
        [ 2.,  0.,  0.,  1.]])

   in [63]:
v = array([1,2,3,4])[:,newaxis]; v

   out[63]:
array([[1],
       [2],
       [3],
       [4]])

   in [64]:
# sparse matrix - dense vector multiplication
a * v

   out[64]:
array([[ 1.],
       [ 6.],
       [ 5.],
       [ 5.]])

   in [65]:
# same result with dense matrix - dense vector multiplcation
a.todense() * v

   out[65]:
matrix([[ 1.],
        [ 6.],
        [ 5.],
        [ 5.]])

optimization[45]  

   optimization (finding minima or maxima of a function) is a large field
   in mathematics, and optimization of complicated functions or in many
   variables can be rather involved. here we will only look at a few very
   simple cases. for a more detailed introduction to optimization with
   scipy see:
   [46]http://scipy-lectures.github.com/advanced/mathematical_optimization
   /index.html

   to use the optimization module in scipy first include the optimize
   module:
   in [66]:
from scipy import optimize

finding a minima[47]  

   let's first look at how to find the minima of a simple function of a
   single variable:
   in [67]:
def f(x):
    return 4*x**3 + (x-2)**2 + x**4

   in [68]:
fig, ax  = plt.subplots()
x = linspace(-5, 3, 100)
ax.plot(x, f(x));

   [d+umwo2 qcnzwqaaaabjru5erkjggg== ]

   we can use the fmin_bfgs function to find the minima of a function:
   in [69]:
x_min = optimize.fmin_bfgs(f, -2)
x_min

optimization terminated successfully.
         current function value: -3.506641
         iterations: 6
         function evaluations: 30
         gradient evaluations: 10

   out[69]:
array([-2.67298164])

   in [70]:
optimize.fmin_bfgs(f, 0.5)

optimization terminated successfully.
         current function value: 2.804988
         iterations: 3
         function evaluations: 15
         gradient evaluations: 5

   out[70]:
array([ 0.46961745])

   we can also use the brent or fminbound functions. they have a bit
   different syntax and use different algorithms.
   in [71]:
optimize.brent(f)

   out[71]:
0.46961743402759754

   in [72]:
optimize.fminbound(f, -4, 2)

   out[72]:
-2.6729822917513886

finding a solution to a function[48]  

   to find the root for a function of the form $f(x) = 0$ we can use the
   fsolve function. it requires an initial guess:
   in [73]:
omega_c = 3.0
def f(omega):
    # a transcendental equation: resonance frequencies of a low-q squid terminat
ed microwave resonator
    return tan(2*pi*omega) - omega_c/omega

   in [74]:
fig, ax  = plt.subplots(figsize=(10,4))
x = linspace(0, 3, 1000)
y = f(x)
mask = where(abs(y) > 50)
x[mask] = y[mask] = nan # get rid of vertical line when the function flip sign
ax.plot(x, y)
ax.plot([0, 3], [0, 0], 'k')
ax.set_ylim(-5,5);

/users/rob/miniconda/envs/py27-spl/lib/python2.7/site-packages/ipython/kernel/__
main__.py:4: runtimewarning: divide by zero encountered in divide

   [tsimaaaaabj ru5erkjggg== ]
   in [75]:
optimize.fsolve(f, 0.1)

   out[75]:
array([ 0.23743014])

   in [76]:
optimize.fsolve(f, 0.6)

   out[76]:
array([ 0.71286972])

   in [77]:
optimize.fsolve(f, 1.1)

   out[77]:
array([ 1.18990285])

interpolation[49]  

   interpolation is simple and convenient in scipy: the interp1d function,
   when given arrays describing x and y data, returns and object that
   behaves like a function that can be called for an arbitrary value of x
   (in the range covered by x), and it returns the corresponding
   interpolated y value:
   in [78]:
from scipy.interpolate import *

   in [79]:
def f(x):
    return sin(x)

   in [80]:
n = arange(0, 10)
x = linspace(0, 9, 100)

y_meas = f(n) + 0.1 * randn(len(n)) # simulate measurement with noise
y_real = f(x)

linear_interpolation = interp1d(n, y_meas)
y_interp1 = linear_interpolation(x)

cubic_interpolation = interp1d(n, y_meas, kind='cubic')
y_interp2 = cubic_interpolation(x)

   in [81]:
fig, ax = plt.subplots(figsize=(10,4))
ax.plot(n, y_meas, 'bs', label='noisy data')
ax.plot(x, y_real, 'k', lw=2, label='true function')
ax.plot(x, y_interp1, 'r', label='linear interp')
ax.plot(x, y_interp2, 'g', label='cubic interp')
ax.legend(loc=3);

   [u4rip89rqacj1trn0zrnswgmvlyoazqmazqw
   iekis9m0tdm0lqxoikvtne3tnc0f6cjl0zrn0zqtbegis9m0tdm0lqxoikvtne3tnc0f6cj
   l0zrn 0zqtbegis9m0tdm0lqx8h1mw7wsfvwduaaaaaelftksuqmcc ]

statistics[50]  

   the scipy.stats module contains a large number of statistical
   distributions, statistical functions and tests. for a complete
   documentation of its features, see
   [51]http://docs.scipy.org/doc/scipy/reference/stats.html.

   there is also a very powerful python package for statistical modelling
   called statsmodels. see [52]http://statsmodels.sourceforge.net for more
   details.
   in [82]:
from scipy import stats

   in [83]:
# create a (discreet) random variable with poissionian distribution

x = stats.poisson(3.5) # photon distribution for a coherent state with n=3.5 pho
tons

   in [84]:
n = arange(0,15)

fig, axes = plt.subplots(3,1, sharex=true)

# plot the id203 mass function (pmf)
axes[0].step(n, x.pmf(n))

# plot the commulative distribution function (cdf)
axes[1].step(n, x.cdf(n))

# plot histogram of 1000 random realizations of the stochastic variable x
axes[2].hist(x.rvs(size=1000));

   [dtqqspaxgxxjk0gkwszekbwbjl0klwmzekhaajb0klqab
   e0laadb2krqabowlaqh8fxn1zohskj+kaaaaaelftksuqmcc ]
   in [85]:
# create a (continous) random variable with normal distribution
y = stats.norm()

   in [86]:
x = linspace(-5,5,100)

fig, axes = plt.subplots(3,1, sharex=true)

# plot the id203 distribution function (pdf)
axes[0].plot(x, y.pdf(x))

# plot the commulative distributin function (cdf)
axes[1].plot(x, y.cdf(x));

# plot histogram of 1000 random realizations of the stochastic variable y
axes[2].hist(y.rvs(size=1000), bins=50);

   [yej
   gglmniv67vx3fytfniupa9+5u4e7os7ho8cxje2oiu4iihlvlkm0iiismwv4ezgmuoaxeck
   obxgr kyxsgbcrysgfebgrjfkafxhjkav4ezgm+n+s3srxwlpvuaaaaabjru5erkjggg==
   ]

   statistics:
   in [87]:
x.mean(), x.std(), x.var() # poission distribution

   out[87]:
(3.5, 1.8708286933869707, 3.5)

   in [88]:
y.mean(), y.std(), y.var() # normal distribution

   out[88]:
(0.0, 1.0, 1.0)

statistical tests[53]  

   test if two sets of (independent) random data comes from the same
   distribution:
   in [89]:
t_statistic, p_value = stats.ttest_ind(x.rvs(size=1000), x.rvs(size=1000))

print "t-statistic =", t_statistic
print "p-value =", p_value

t-statistic = -0.901953297251
p-value = 0.367190391714

   since the p value is very large we cannot reject the hypothesis that
   the two sets of random data have different means.

   to test if the mean of a single sample of data has mean 0.1 (the true
   mean is 0.0):
   in [90]:
stats.ttest_1samp(y.rvs(size=1000), 0.1)

   out[90]:
ttest_1sampresult(statistic=-3.1644288210071765, pvalue=0.0016008455559249511)

   low p-value means that we can reject the hypothesis that the mean of y
   is 0.1.
   in [91]:
y.mean()

   out[91]:
0.0

   in [92]:
stats.ttest_1samp(y.rvs(size=1000), y.mean())

   out[92]:
ttest_1sampresult(statistic=2.2098772438652992, pvalue=0.027339807364469011)

further reading[54]  

     * [55]http://www.scipy.org - the official web page for the scipy
       project.
     * [56]http://docs.scipy.org/doc/scipy/reference/tutorial/index.html -
       a tutorial on how to get started using scipy.
     * [57]https://github.com/scipy/scipy/ - the scipy source code.

versions[58]  

   in [93]:
%reload_ext version_information

%version_information numpy, matplotlib, scipy

   out[93]:
    software                      version
   python     2.7.10 64bit [gcc 4.2.1 (apple inc. build 5577)]
   ipython    3.2.1
   os         darwin 14.1.0 x86_64 i386 64bit
   numpy      1.9.2
   matplotlib 1.4.3
   scipy      0.16.0
   sat aug 15 11:13:18 2015 jst

   this website does not host notebooks, it only renders notebooks
   available on other websites.

   delivered by [59]fastly, rendered by [60]rackspace

   nbviewer github [61]repository.

   nbviewer version: [62]33c4683

   nbconvert version: [63]5.4.0

   rendered (fri, 05 apr 2019 18:32:11 utc)

references

   1. https://nbviewer.jupyter.org/
   2. http://jupyter.org/
   3. https://nbviewer.jupyter.org/faq
   4. https://nbviewer.jupyter.org/format/script/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb
   5. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb
   6. https://github.com/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb
   7. https://mybinder.org/v2/gh/jrjohansson/scientific-python-lectures/master?filepath=lecture-3-scipy.ipynb
   8. https://raw.githubusercontent.com/jrjohansson/scientific-python-lectures/master/lecture-3-scipy.ipynb
   9. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/tree/master
  10. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/tree/master/lecture-3-scipy.ipynb
  11. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#scipy---library-of-scientific-algorithms-for-python
  12. http://ipython.org/notebook.html
  13. http://github.com/jrjohansson/scientific-python-lectures
  14. http://jrjohansson.github.io/
  15. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#introduction
  16. http://docs.scipy.org/doc/scipy/reference/special.html
  17. http://docs.scipy.org/doc/scipy/reference/integrate.html
  18. http://docs.scipy.org/doc/scipy/reference/optimize.html
  19. http://docs.scipy.org/doc/scipy/reference/interpolate.html
  20. http://docs.scipy.org/doc/scipy/reference/fftpack.html
  21. http://docs.scipy.org/doc/scipy/reference/signal.html
  22. http://docs.scipy.org/doc/scipy/reference/linalg.html
  23. http://docs.scipy.org/doc/scipy/reference/sparse.html
  24. http://docs.scipy.org/doc/scipy/reference/stats.html
  25. http://docs.scipy.org/doc/scipy/reference/ndimage.html
  26. http://docs.scipy.org/doc/scipy/reference/io.html
  27. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#special-functions
  28. http://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special
  29. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#integration
  30. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#numerical-integration:-quadrature
  31. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#ordinary-differential-equations-(odes)
  32. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#example:-double-pendulum
  33. http://en.wikipedia.org/wiki/double_pendulum
  34. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#example:-damped-harmonic-oscillator
  35. http://en.wikipedia.org/wiki/damping
  36. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#fourier-transform
  37. http://www.netlib.org/fftpack/
  38. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#linear-algebra
  39. http://docs.scipy.org/doc/scipy/reference/linalg.html
  40. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#linear-equation-systems
  41. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#eigenvalues-and-eigenvectors
  42. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#matrix-operations
  43. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#sparse-matrices
  44. http://en.wikipedia.org/wiki/sparse_matrix
  45. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#optimization
  46. http://scipy-lectures.github.com/advanced/mathematical_optimization/index.html
  47. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#finding-a-minima
  48. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#finding-a-solution-to-a-function
  49. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#interpolation
  50. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#statistics
  51. http://docs.scipy.org/doc/scipy/reference/stats.html
  52. http://statsmodels.sourceforge.net/
  53. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#statistical-tests
  54. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#further-reading
  55. http://www.scipy.org/
  56. http://docs.scipy.org/doc/scipy/reference/tutorial/index.html
  57. https://github.com/scipy/scipy/
  58. https://nbviewer.jupyter.org/github/jrjohansson/scientific-python-lectures/blob/master/lecture-3-scipy.ipynb#versions
  59. http://www.fastly.com/
  60. https://developer.rackspace.com/?nbviewer=awesome
  61. https://github.com/jupyter/nbviewer
  62. https://github.com/jupyter/nbviewer/commit/33c4683164d5ee4c92dbcd53afac7f13ef033c54
  63. https://github.com/jupyter/nbconvert/releases/tag/5.4.0
