   #[1]neal caren    feed [2]neal caren    comments feed [3]an introduction
   to text analysis with python, part 2 [4]scraping comments from the new
   york times [5]alternate [6]alternate

   [7]neal caren
   [8]skip to content
     * [9]home
     * [10]research
     * [11]teaching
     * [12]big data
     * [13]links

   [14]    an introduction to text analysis with python, part 2
   [15]scraping comments from the new york times    

an introduction to text analysis with python, part 3

   posted on [16]april 10, 2012 by [17]neal caren

   two [18]earlier [19]tutorials looked at the basics of using python to
   analyze text data. this post explains how to expand the code written
   earlier so that you can use it to explore the positive and negative
   sentiment of any set of texts. specifically, we   ll look at looping over
   more than one tweet, incorporating a more complete dictionary, and
   exporting the results. [if you just want the final python script, you
   can just [20]download it.]
   earlier, we used a pretty nealcaren list of words to measure positive
   sentiment. while the study in science used the commercial [21]liwc
   dictionary, an alternate sentiment dictionary is produced by theresa
   wilson, janyce wiebe, and paul hoffmann at the university of pittsburgh
   and is freely [22]available. in both cases, the sentiment dictionaries
   are used in a fairly straightforward way: the more positive words in
   the text, the higher the text scores on the positive sentiment scale.
   while this has some drawbacks, the method is quite popular: the liwc
   database has over 1,000 cites in google scholar, and the wilson et al.
   database has more than 600.

downloading

   since the wislon et al. list combines negative and positive polarity
   words in one list, and includes both words and word stems, i will clean
   it up a little bit. you can download the [23]positive list and the
   [24]negative list using your browser, but you don   t have to. python can
   do that.

   first, you need to import one of the modules that python uses to
   communicate with the internet:
   >>> import urllib

   like many commands, python won   t return anything unless something went
   wrong. in this case, it should just respond with >>>, which means that
   the module was successfully brought into memory. next, store the web
   address that you want to access in a string. you don   t have to do this,
   but it   s the type of thing that makes your code easier to read and
   allows you to scale up quickly when you want to download thousands of
   urls.
   >>> url='http://www.unc.edu/~ncaren/haphazard/negative.txt'

   you can also create a string with the name you want the file to have on
   you hard drive:
   >>> file_name='negative.txt'

   to download and save the file:
   >>> urllib.urlretrieve(url,file_name)

   this will download the file into your current directory. if you want it
   to go somewhere else, you can put the full path in the file_name
   string. you didn   t have to enter the url and the file name in the prior
   lines. something like the following would have worked exactly the same:
   >>>
   urllib.urlretrieve('http://www.unc.edu/~ncaren/haphazard/negative.txt',
   'negative.txt')

   note that the location and filename are both surrounded by quotation
   marks because you want python to use this information literally; they
   aren   t referring to a string object, like in our previous code. this
   line of code is actually quite readable, and in most circumstances this
   would be the most efficient thing to do. but there are actually three
   files that we want to get: the negative list, the positive list, and
   the list of tweets. and we can download the three using a pretty simple
   loop:
   >>> files=['negative.txt','positive.txt','obama_tweets.txt']
   >>> path='http://www.unc.edu/~ncaren/haphazard/'
   >>> for file_name in files:
   ...     urllib.urlretrieve(path+file_name,file_name)
   ...

   the first line creates a new list with three items, the names of the
   three files to be downloaded. the second line creates a string object
   that stores the url path that they all share. the third line starts a
   loop over each of the items in the files list using file_name to
   reference each item in turn. the fourth line is indented, because it
   happens once for each item in the list as a result of the loop, and
   downloads the file. this is the same as the original download line,
   except the url is now the combination of two strings, path and
   file_name. as noted previously, python can combine strings with a plus
   sign, so the result from the first pass through the loop will be
   http://www.unc.edu/~ncaren/haphazard/negative.txt, which is where the
   file can be found. note that this takes advantage of the fact that we
   don   t mind reusing the original file name. if we wanted to change it,
   or if there were different paths to each of the files, things would get
   slightly trickier.

more fun with lists

   let   s take a look at the list of tweets that we just downloaded. first,
   open the file:
   >>> tweets = open("obama_tweets.txt").read()

   as you might have guessed, this line is actually doing double duty. it
   opens the file and reads it into memory before it is stored in tweets.
   since the file has one tweet on each line, we can turn it into a list
   of tweets by splitting it at the end of line character. the file was
   originally created on a mac, so the end of line character is an n
   (think n for new line). on a windows computer, the end of line
   character is an rn (think r for return and n for new line). so if the
   file was created on a windows computer, you might need to strip out the
   extra character with something like
   windows_file=windows_file.replace('r','') before you split the lines,
   but you don   t need to worry about that here, no matter what operating
   system you are using. the end of line character comes from the computer
   that made the file, not the computer you are currently using. to split
   the tweets into a list:
   >>> tweets_list = tweets.split('\n')

   as always, you can check how many items are in the list:
   >>> len(tweets_list)
   1365

   you can print the entire list by typing print tweets_list, but it will
   scroll by very fast. a more useful way to look at it is to print just
   some of the items. since it   s a list, we can loop through the first few
   item so they each print on the same line.
   >>> for tweet in tweets_list[0:5]:
   ...     print tweet
   ...
   obama has called the gop budget social darwinism. nice try, but they
   believe in social creationism.
   in his teen years, obama has been known to use marijuana and cocaine.
   ipa congratulates president barack obama for leadership regarding jobs
   act: washington, apr 05, 2012 (business w... http://t.co/8le3dc8e
   rt @professor_why: #whatsromneyhiding - his connection to supporters of
   critical race theory.... oh wait, that was obama, not romney...
   rt @wardollarshome: obama has approved more targeted assassinations
   than any modern us prez; read & rt: http://t.co/bfc4gbbw

   note the new [0:5] after the tweets_list but before the : that begins
   the loop. the first number tells python where to make the first cut in
   the list. the potentially counterintuitive part is that this number
   doesn   t reference an actual item in the list, but rather a position
   between each item in the list   think about where the comma goes when
   lists are created or printed. adding to the confusion, the position at
   the start of the list is 0. so, in this case, we are telling python we
   want to slice our list starting at the beginning and continuing until
   the fifth comma, which is after the fifth item in the list.

   so, if you wanted to just print the second item in the list, you could
   type:
   >>> print tweets_list[1:2]
   ['obama has called the gop budget social darwinism. nice try, but they
   believe in social creationism.']

   this slices the list from the first comma to the second comma, so the
   result is the second item in the list. unless you have a computer
   science background, this may be confusing as it   s not the common way to
   think of items in lists.

   as a shorthand, you can leave out the first number in the pair if you
   want to start at the very beginning or leave out the last number if you
   want to go until the end. so, if you want to print out the first five
   tweets, you could just type print tweet_list[:5]. there are several
   other shortcuts along these lines that are available. we will cover
   some of them in other tutorials.

   now that we have our tweet list expanded, let   s load up the positive
   sentiment list and print out the first few entries:
   >>> pos_sent = open("positive.txt").read()
   >>> positive_words=pos_sent.split('\n')
   >>> print positive_words[:10]
   ['abidance', 'abidance', 'abilities', 'ability', 'able', 'above',
   'above-average', 'abundant', 'abundance', 'acceptance']

   like the tweet list, this file contained each entry on its own line, so
   it loads exactly the same way. if you typed len(positive_words) you
   would find out that this list has 2,230 entries.

preprocessing

   in the earlier post, we explored how to preprocess the tweets: remove
   the punctuation, convert to lower case, and examine whether or not each
   word was in the positive sentiment list. we can use this exact same
   code here with our long list. the one alteration is that instead of
   having just one tweet, we now have a list of 1,365 tweets, so we have
   to loop over that list.
   >>> for tweet in tweets_list:
   ...     positive_counter=0
   ...     tweet_processed=tweet.lower()
   ...     for p in list(punctuation):
   ...         tweet_processed=tweet_processed.replace(p,'')
   ...         words=tweet_processed.split(' ')
   ...     for word in words:
   ...         if word in positive_words:
   ...             print word
   ...             positive_counter=positive_counter+1
   ...      print positive_counter/len(words)
   ...

   if you saw a string of numbers roll past you, it worked! to review, we
   start by looping over each item of the list. we set up a counter to
   hold the running total of the number of positive words found in the
   tweet. then we make everything lower case and store it in
   tweet_processed. to strip out the punctuation, we loop over every item
   of punctuation, swapping out the punctuation mark with nothing. if you
   haven   t already typed  from string import punctuation in your current
   python session, you might get an error with this line, so make sure to
   include that import. the cleaned tweet is then converted to a list of
   words, split at the white spaces. finally, we loop through each word in
   the tweet, and if the word is in our new and expanded list of positive
   words, we increase the counter by one. after cycling through each of
   the tweet words, the proportion of positive words is computed and
   printed. if you just get zeros, you might need to type from __future__
   import division again, so that the result isn   t rounded down.

   the major problem with this script is that it is currently useless. it
   prints the positive sentiment results, but then doesn   t do anything
   with it. a more practical solution would be to store the results
   somehow. in a standard statistical package, we would generate a new
   variable that held our results. we can do something similar here by
   storing the results in a new list. before we start the tweet loop, we
   add the line:
   >>> positive_counts=[]

   then, instead of printing the proportion, we can append it to the list:
   >>> positive_counts.append(positive_counter/word_count)

   the next time we run through the loop, it won   t produce any output, but
   it will create a list of the proportions. this still isn   t that useful,
   although you can use python to do most of your statistical analysis and
   plotting, but at this point you are probably ready to get your data out
   of python and back into your statistical package.

   the most convenient way to store data for use in multiple packages is
   as a plain text file where each case is its own row and variables are
   separated by commas. this file type commonly has a    csv    extension, and
   python can read and write these files quite easily.

   first, import the csv module:
   >>> import csv

   to write to csv file, you first    open    the file with the csv writer:
   >>> writer = csv.writer(open('tweet_sentiment.csv', 'wb'))

   in the 'open' part of the command, the first item is the name of the
   file you want to create, and the 'wb' tells python that this is a file
   you want to write. be careful with your file name, because if there is
   already a file with this name, python will write over it. if you wanted
   to read a csv file, you would just swap reader for writer and 'rb' for
   'wb', which creates a nice symmetry.

   sending your list of positive sentiment values to the file requires
   just one more line:
   >>> writer.writerows(positive_counts)

   you can now import this file into your statistical software package or
   just take a peak at it in excel. of course, having just one variable is
   not the most useful thing. usually, you will have more than one that
   you want to export, but for now we just have the one. at a minimum, you
   might also want to export the text of the original tweets. to combine
   more than one list together, you can zip them into one list. this is
   different from appending one list to the other, which would just make
   the one list twice as long.
   >>> output=zip(tweets_list,positive_counts)

   in this case, zip creates a new list output that is the same length as
   our tweets_list, but each entry has two items: the tweet and the
   positive count. you can use zip to combine as many lists as your like,
   although they all need to be the same length. technically, each item in
   the list is a tuple, or an ordered element list, which is a data format
   quite similar to a list but generally less useful for textual analysis.

   to write our final version of the output, we need to repeat the line
   that created our writer and then write the output list:
   >>> writer = csv.writer(open('tweet_sentiment.csv', 'wb'))
   >>> writer.writerows(output)

   that   s it. if you searched everyday for tweets mentioning president
   obama and ran this script, my guess is that your data would tell a
   pretty interesting story about trends over time. or, if you had your
   own text data arranged such that each text was its own line, you could
   just update the file name, and compute the sentiment scores.

   in case you were wondering, the top two most negative tweets, were
      hatch makes startling accusation against obama http://t.co/hvqfuzgr
   ..shocking headline   not    and    we need to tag obama & define him for nov
   battle. #obama #failedleader #incompetent #wasteful #divisive
   #desperate #flexible #arrogant #lazy   , which gives our little study
   some face validity.

   you have probably noticed that our code for this project has swelled to
   about 40 lines. not horrible, but not that easy to copy and paste. and
   if you mess up in a loop, you have to start all over again. while
   typing in commands this way in python is useful for playing around with
   new codes and commands, most of the time its not the most efficient way
   to do things. just like stata has .do files, you can similarly save a
   series of python commands as a text file and then run them all
   together. these sort of python files use a .py extension.

   i   ve compiled all the code for our id31 into one file,
   and you can download it [25]sentiment.py using your browser. at this
   point, you might want to make a directory for yourself where you can
   store all your python files.

   you can quit python by typing the exit(), which should bring you back
   to your operating system   s prompt. now, assuming you are in the
   directory where you download sentiment.py, you can run the entire
   program by typing:
   $ python sentiment.py

   remember the $ sign means that we are out of python. this command tells
   your computer that you want python to run the program sentiment.py. if
   all works according to plan, your computer should think for a couple of
   seconds, and then display the operating system prompt. python displays
   fewer things when run this way: only things with a print statement in
   front of them are displayed, so don   t expect your output to be as
   verbose as when you typed in each command. actually, you probably would
   want to add some print statements along the way so that you knew
   everything was working.

   assuming you didn   t get an error message, there should be new file
   called tweet_sentiment.csv in your current directory. you can confirm
   this by typing ls -l on a mac or dir in windows. this should display
   the contents of the current directory and you should see
   tweet_sentiment.csv listed along with the current time   which means that
   the file was just created. perfect.

   there are easier ways to run your .py files which i   ll discuss at a
   later point, and ways to improve the script, such as adding comments as
   notes to ourselves, speeding it up, and allowing different types of
   input files. but if you made it this far, you can proudly call yourself
   a    beginning python programmer.    congratulations.

about neal caren

   sociology
   [26]view all posts by neal caren    
   this entry was posted in [27]uncategorized and tagged [28]python,
   [29]sentiment, [30]tutorial. bookmark the [31]permalink.
   [32]    an introduction to text analysis with python, part 2
   [33]scraping comments from the new york times    

   comments are closed.
     * search for: ____________________ search
     * neal caren in an assistant professor of sociology at the university
       of north carolina, chapel hill.
       [34]email
       [35]twitter

     * note: header images found via google image search. i didn't take
       them.

   [36]neal caren
   [37]proudly powered by wordpress.

references

   1. http://nealcaren.web.unc.edu/feed/
   2. http://nealcaren.web.unc.edu/comments/feed/
   3. http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-2/
   4. http://nealcaren.web.unc.edu/scraping-comments-from-the-new-york-times/
   5. http://nealcaren.web.unc.edu/wp-json/oembed/1.0/embed?url=http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-3/
   6. http://nealcaren.web.unc.edu/wp-json/oembed/1.0/embed?url=http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-3/&format=xml
   7. http://nealcaren.web.unc.edu/
   8. http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-3/#content
   9. http://nealcaren.web.unc.edu/
  10. http://nealcaren.web.unc.edu/research/
  11. http://nealcaren.web.unc.edu/teaching/
  12. http://nealcaren.web.unc.edu/big-data/
  13. http://nealcaren.web.unc.edu/links/
  14. http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-2/
  15. http://nealcaren.web.unc.edu/scraping-comments-from-the-new-york-times/
  16. http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-3/
  17. http://nealcaren.web.unc.edu/author/ncaren/
  18. http://nealcaren.web.unc.edu/sentiment-classification-in-python-from-a-dictionary/
  19. http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-2/
  20. http://www.unc.edu/~ncaren/haphazard/sentiment.py
  21. http://www.liwc.net/
  22. http://www.cs.pitt.edu/mpqa/
  23. http://www.unc.edu/~ncaren/haphazard/positive.txt
  24. http://www.unc.edu/~ncaren/haphazard/negative.txt
  25. http://www.unc.edu/~ncaren/haphazard/sentiment.py
  26. http://nealcaren.web.unc.edu/author/ncaren/
  27. http://nealcaren.web.unc.edu/category/uncategorized/
  28. http://nealcaren.web.unc.edu/tag/python/
  29. http://nealcaren.web.unc.edu/tag/sentiment/
  30. http://nealcaren.web.unc.edu/tag/tutorial/
  31. http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-3/
  32. http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-2/
  33. http://nealcaren.web.unc.edu/scraping-comments-from-the-new-york-times/
  34. mailto:neal.caren@unc.edu
  35. http://twitter.com/haphazardsoc
  36. http://nealcaren.web.unc.edu/
  37. http://wordpress.org/
