   [1]cs231n convolutional neural networks for visual recognition

   table of contents:
     * [2]introduction
     * [3]simple expressions, interpreting the gradient
     * [4]compound expressions, chain rule, id26
     * [5]intuitive understanding of id26
     * [6]modularity: sigmoid example
     * [7]backprop in practice: staged computation
     * [8]patterns in backward flow
     * [9]gradients for vectorized operations
     * [10]summary

introduction

   motivation. in this section we will develop expertise with an intuitive
   understanding of id26, which is a way of computing gradients
   of expressions through recursive application of chain rule.
   understanding of this process and its subtleties is critical for you to
   understand, and effectively develop, design and debug neural networks.

   problem statement. the core problem studied in this section is as
   follows: we are given some function \(f(x)\) where \(x\) is a vector of
   inputs and we are interested in computing the gradient of \(f\) at
   \(x\) (i.e. \(\nabla f(x)\) ).

   motivation. recall that the primary reason we are interested in this
   problem is that in the specific case of neural networks, \(f\) will
   correspond to the id168 ( \(l\) ) and the inputs \(x\) will
   consist of the training data and the neural network weights. for
   example, the loss could be the id166 id168 and the inputs are
   both the training data \((x_i,y_i), i=1 \ldots n\) and the weights and
   biases \(w,b\). note that (as is usually the case in machine learning)
   we think of the training data as given and fixed, and of the weights as
   variables we have control over. hence, even though we can easily use
   id26 to compute the gradient on the input examples \(x_i\),
   in practice we usually only compute the gradient for the parameters
   (e.g. \(w,b\)) so that we can use it to perform a parameter update.
   however, as we will see later in the class the gradient on \(x_i\) can
   still be useful sometimes, for example for purposes of visualization
   and interpreting what the neural network might be doing.

   if you are coming to this class and you   re comfortable with deriving
   gradients with chain rule, we would still like to encourage you to at
   least skim this section, since it presents a rarely developed view of
   id26 as backward flow in real-valued circuits and any
   insights you   ll gain may help you throughout the class.

simple expressions and interpretation of the gradient

   lets start simple so that we can develop the notation and conventions
   for more complex expressions. consider a simple multiplication function
   of two numbers \(f(x,y) = x y\). it is a matter of simple calculus to
   derive the partial derivative for either input:

   interpretation. keep in mind what the derivatives tell you: they
   indicate the rate of change of a function with respect to that variable
   surrounding an infinitesimally small region near a particular point:

   a technical note is that the division sign on the left-hand side is,
   unlike the division sign on the right-hand side, not a division.
   instead, this notation indicates that the operator \( \frac{d}{dx} \)
   is being applied to the function \(f\), and returns a different
   function (the derivative). a nice way to think about the expression
   above is that when \(h\) is very small, then the function is
   well-approximated by a straight line, and the derivative is its slope.
   in other words, the derivative on each variable tells you the
   sensitivity of the whole expression on its value. for example, if \(x =
   4, y = -3\) then \(f(x,y) = -12\) and the derivative on \(x\)
   \(\frac{\partial f}{\partial x} = -3\). this tells us that if we were
   to increase the value of this variable by a tiny amount, the effect on
   the whole expression would be to decrease it (due to the negative
   sign), and by three times that amount. this can be seen by rearranging
   the above equation ( \( f(x + h) = f(x) + h \frac{df(x)}{dx} \) ).
   analogously, since \(\frac{\partial f}{\partial y} = 4\), we expect
   that increasing the value of \(y\) by some very small amount \(h\)
   would also increase the output of the function (due to the positive
   sign), and by \(4h\).

     the derivative on each variable tells you the sensitivity of the
     whole expression on its value.

   as mentioned, the gradient \(\nabla f\) is the vector of partial
   derivatives, so we have that \(\nabla f = [\frac{\partial f}{\partial
   x}, \frac{\partial f}{\partial y}] = [y, x]\). even though the gradient
   is technically a vector, we will often use terms such as    the gradient
   on x    instead of the technically correct phrase    the partial derivative
   on x    for simplicity.

   we can also derive the derivatives for the addition operation:

   that is, the derivative on both \(x,y\) is one regardless of what the
   values of \(x,y\) are. this makes sense, since increasing either
   \(x,y\) would increase the output of \(f\), and the rate of that
   increase would be independent of what the actual values of \(x,y\) are
   (unlike the case of multiplication above). the last function we   ll use
   quite a bit in the class is the max operation:

   that is, the (sub)gradient is 1 on the input that was larger and 0 on
   the other input. intuitively, if the inputs are \(x = 4,y = 2\), then
   the max is 4, and the function is not sensitive to the setting of
   \(y\). that is, if we were to increase it by a tiny amount \(h\), the
   function would keep outputting 4, and therefore the gradient is zero:
   there is no effect. of course, if we were to change \(y\) by a large
   amount (e.g. larger than 2), then the value of \(f\) would change, but
   the derivatives tell us nothing about the effect of such large changes
   on the inputs of a function; they are only informative for tiny,
   infinitesimally small changes on the inputs, as indicated by the
   \(\lim_{h \rightarrow 0}\) in its definition.

compound expressions with chain rule

   lets now start to consider more complicated expressions that involve
   multiple composed functions, such as \(f(x,y,z) = (x + y) z\). this
   expression is still simple enough to differentiate directly, but we   ll
   take a particular approach to it that will be helpful with
   understanding the intuition behind id26. in particular, note
   that this expression can be broken down into two expressions: \(q = x +
   y\) and \(f = q z\). moreover, we know how to compute the derivatives
   of both expressions separately, as seen in the previous section. \(f\)
   is just multiplication of \(q\) and \(z\), so \(\frac{\partial
   f}{\partial q} = z, \frac{\partial f}{\partial z} = q\), and \(q\) is
   addition of \(x\) and \(y\) so \( \frac{\partial q}{\partial x} = 1,
   \frac{\partial q}{\partial y} = 1 \). however, we don   t necessarily
   care about the gradient on the intermediate value \(q\) - the value of
   \(\frac{\partial f}{\partial q}\) is not useful. instead, we are
   ultimately interested in the gradient of \(f\) with respect to its
   inputs \(x,y,z\). the chain rule tells us that the correct way to
      chain    these gradient expressions together is through multiplication.
   for example, \(\frac{\partial f}{\partial x} = \frac{\partial
   f}{\partial q} \frac{\partial q}{\partial x} \). in practice this is
   simply a multiplication of the two numbers that hold the two gradients.
   lets see this with an example:
# set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y # q becomes 3
f = q * z # f becomes -12

# perform the backward pass (id26) in reverse order:
# first backprop through f = q * z
dfdz = q # df/dz = q, so gradient on z becomes 3
dfdq = z # df/dq = z, so gradient on q becomes -4
# now backprop through q = x + y
dfdx = 1.0 * dfdq # dq/dx = 1. and the multiplication here is the chain rule!
dfdy = 1.0 * dfdq # dq/dy = 1

   at the end we are left with the gradient in the variables
   [dfdx,dfdy,dfdz], which tell us the sensitivity of the variables x,y,z
   on f!. this is the simplest example of id26. going forward,
   we will want to use a more concise notation so that we don   t have to
   keep writing the df part. that is, for example instead of dfdq we would
   simply write dq, and always assume that the gradient is with respect to
   the final output.

   this computation can also be nicely visualized with a circuit diagram:
   -2-4x5-4y-43z3-4q+-121f*
   the real-valued "circuit" on left shows the visual representation of
   the computation. the forward pass computes values from inputs to output
   (shown in green). the backward pass then performs id26 which
   starts at the end and recursively applies the chain rule to compute the
   gradients (shown in red) all the way to the inputs of the circuit. the
   gradients can be thought of as flowing backwards through the circuit.

intuitive understanding of id26

   notice that id26 is a beautifully local process. every gate
   in a circuit diagram gets some inputs and can right away compute two
   things: 1. its output value and 2. the local gradient of its inputs
   with respect to its output value. notice that the gates can do this
   completely independently without being aware of any of the details of
   the full circuit that they are embedded in. however, once the forward
   pass is over, during id26 the gate will eventually learn
   about the gradient of its output value on the final output of the
   entire circuit. chain rule says that the gate should take that gradient
   and multiply it into every gradient it normally computes for all of its
   inputs.

     this extra multiplication (for each input) due to the chain rule can
     turn a single and relatively useless gate into a cog in a complex
     circuit such as an entire neural network.

   lets get an intuition for how this works by referring again to the
   example. the add gate received inputs [-2, 5] and computed output 3.
   since the gate is computing the addition operation, its local gradient
   for both of its inputs is +1. the rest of the circuit computed the
   final value, which is -12. during the backward pass in which the chain
   rule is applied recursively backwards through the circuit, the add gate
   (which is an input to the multiply gate) learns that the gradient for
   its output was -4. if we anthropomorphize the circuit as wanting to
   output a higher value (which can help with intuition), then we can
   think of the circuit as    wanting    the output of the add gate to be
   lower (due to negative sign), and with a force of 4. to continue the
   recurrence and to chain the gradient, the add gate takes that gradient
   and multiplies it to all of the local gradients for its inputs (making
   the gradient on both x and y 1 * -4 = -4). notice that this has the
   desired effect: if x,y were to decrease (responding to their negative
   gradient) then the add gate   s output would decrease, which in turn
   makes the multiply gate   s output increase.

   id26 can thus be thought of as gates communicating to each
   other (through the gradient signal) whether they want their outputs to
   increase or decrease (and how strongly), so as to make the final output
   value higher.

modularity: sigmoid example

   the gates we introduced above are relatively arbitrary. any kind of
   differentiable function can act as a gate, and we can group multiple
   gates into a single gate, or decompose a function into multiple gates
   whenever it is convenient. lets look at another expression that
   illustrates this point:

   as we will see later in the class, this expression describes a
   2-dimensional neuron (with inputs x and weights w) that uses the
   sigmoid activation function. but for now lets think of this very simply
   as just a function from inputs w,x to a single number. the function is
   made up of multiple gates. in addition to the ones described already
   above (add, mul, max), there are four more:

   where the functions \(f_c, f_a\) translate the input by a constant of
   \(c\) and scale the input by a constant of \(a\), respectively. these
   are technically special cases of addition and multiplication, but we
   introduce them as (new) unary gates here since we do need the gradients
   for the constants. \(c,a\). the full circuit then looks as follows:
   2.00-0.20w0-1.000.39x0-3.00-0.39w1-2.00-0.59x1-3.000.20w2-2.000.20*6.00
   0.20*4.000.20+1.000.20+-1.00-0.20*-10.37-0.53exp1.37-0.53+10.731.001/x
   example circuit for a 2d neuron with a sigmoid activation function. the
   inputs are [x0,x1] and the (learnable) weights of the neuron are
   [w0,w1,w2]. as we will see later, the neuron computes a dot product
   with the input and then its activation is softly squashed by the
   sigmoid function to be in range from 0 to 1.

   in the example above, we see a long chain of function applications that
   operates on the result of the dot product between w,x. the function
   that these operations implement is called the sigmoid function
   \(\sigma(x)\). it turns out that the derivative of the sigmoid function
   with respect to its input simplifies if you perform the derivation
   (after a fun tricky part where we add and subtract a 1 in the
   numerator):

   as we see, the gradient turns out to simplify and becomes surprisingly
   simple. for example, the sigmoid expression receives the input 1.0 and
   computes the output 0.73 during the forward pass. the derivation above
   shows that the local gradient would simply be (1 - 0.73) * 0.73 ~= 0.2,
   as the circuit computed before (see the image above), except this way
   it would be done with a single, simple and efficient expression (and
   with less numerical issues). therefore, in any real practical
   application it would be very useful to group these operations into a
   single gate. lets see the backprop for this neuron in code:
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (id26)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient deriva
tion
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit

   implementation protip: staged id26. as shown in the code
   above, in practice it is always helpful to break down the forward pass
   into stages that are easily backpropped through. for example here we
   created an intermediate variable dot which holds the output of the dot
   product between w and x. during backward pass we then successively
   compute (in reverse order) the corresponding variables (e.g. ddot, and
   ultimately dw, dx) that hold the gradients of those variables.

   the point of this section is that the details of how the
   id26 is performed, and which parts of the forward function
   we think of as gates, is a matter of convenience. it helps to be aware
   of which parts of the expression have easy local gradients, so that
   they can be chained together with the least amount of code and effort.

backprop in practice: staged computation

   lets see this with another example. suppose that we have a function of
   the form:

   to be clear, this function is completely useless and it   s not clear why
   you would ever want to compute its gradient, except for the fact that
   it is a good example of id26 in practice. it is very
   important to stress that if you were to launch into performing the
   differentiation with respect to either \(x\) or \(y\), you would end up
   with very large and complex expressions. however, it turns out that
   doing so is completely unnecessary because we don   t need to have an
   explicit function written down that evaluates the gradient. we only
   have to know how to compute it. here is how we would structure the
   forward pass of such expression:
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)
num = x + sigy # numerator                               #(2)
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)
xpy = x + y                                              #(4)
xpysqr = xpy**2                                          #(5)
den = sigx + xpysqr # denominator                        #(6)
invden = 1.0 / den                                       #(7)
f = num * invden # done!                                 #(8)

   phew, by the end of the expression we have computed the forward pass.
   notice that we have structured the code in such way that it contains
   multiple intermediate variables, each of which are only simple
   expressions for which we already know the local gradients. therefore,
   computing the backprop pass is easy: we   ll go backwards and for every
   variable along the way in the forward pass (sigy, num, sigx, xpy,
   xpysqr, den, invden) we will have the same variable, but one that
   begins with a d, which will hold the gradient of the output of the
   circuit with respect to that variable. additionally, note that every
   single piece in our backprop will involve computing the local gradient
   of that expression, and chaining it with the gradient on that
   expression with a multiplication. for each row, we also highlight which
   part of the forward pass it refers to:
# backprop f = num * invden
dnum = invden # gradient on numerator                             #(8)
dinvden = num                                                     #(8)
# backprop invden = 1.0 / den
dden = (-1.0 / (den**2)) * dinvden                                #(7)
# backprop den = sigx + xpysqr
dsigx = (1) * dden                                                #(6)
dxpysqr = (1) * dden                                              #(6)
# backprop xpysqr = xpy**2
dxpy = (2 * xpy) * dxpysqr                                        #(5)
# backprop xpy = x + y
dx = (1) * dxpy                                                   #(4)
dy = (1) * dxpy                                                   #(4)
# backprop sigx = 1.0 / (1 + math.exp(-x))
dx += ((1 - sigx) * sigx) * dsigx # notice += !! see notes below  #(3)
# backprop num = x + sigy
dx += (1) * dnum                                                  #(2)
dsigy = (1) * dnum                                                #(2)
# backprop sigy = 1.0 / (1 + math.exp(-y))
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)
# done! phew

   notice a few things:

   cache forward pass variables. to compute the backward pass it is very
   helpful to have some of the variables that were used in the forward
   pass. in practice you want to structure your code so that you cache
   these variables, and so that they are available during id26.
   if this is too difficult, it is possible (but wasteful) to recompute
   them.

   gradients add up at forks. the forward expression involves the
   variables x,y multiple times, so when we perform id26 we
   must be careful to use += instead of = to accumulate the gradient on
   these variables (otherwise we would overwrite it). this follows the
   multivariable chain rule in calculus, which states that if a variable
   branches out to different parts of the circuit, then the gradients that
   flow back to it will add.

patterns in backward flow

   it is interesting to note that in many cases the backward-flowing
   gradient can be interpreted on an intuitive level. for example, the
   three most commonly used gates in neural networks (add,mul,max), all
   have very simple interpretations in terms of how they act during
   id26. consider this example circuit:
   3.00-8.00x-4.006.00y2.002.00z-1.000.00w-12.002.00*2.002.00max-10.002.00
   +-20.001.00*2
   an example circuit demonstrating the intuition behind the operations
   that id26 performs during the backward pass in order to
   compute the gradients on the inputs. sum operation distributes
   gradients equally to all its inputs. max operation routes the gradient
   to the higher input. multiply gate takes the input activations, swaps
   them and multiplies by its gradient.

   looking at the diagram above as an example, we can see that:

   the add gate always takes the gradient on its output and distributes it
   equally to all of its inputs, regardless of what their values were
   during the forward pass. this follows from the fact that the local
   gradient for the add operation is simply +1.0, so the gradients on all
   inputs will exactly equal the gradients on the output because it will
   be multiplied by x1.0 (and remain unchanged). in the example circuit
   above, note that the + gate routed the gradient of 2.00 to both of its
   inputs, equally and unchanged.

   the max gate routes the gradient. unlike the add gate which distributed
   the gradient unchanged to all its inputs, the max gate distributes the
   gradient (unchanged) to exactly one of its inputs (the input that had
   the highest value during the forward pass). this is because the local
   gradient for a max gate is 1.0 for the highest value, and 0.0 for all
   other values. in the example circuit above, the max operation routed
   the gradient of 2.00 to the z variable, which had a higher value than
   w, and the gradient on w remains zero.

   the multiply gate is a little less easy to interpret. its local
   gradients are the input values (except switched), and this is
   multiplied by the gradient on its output during the chain rule. in the
   example above, the gradient on x is -8.00, which is -4.00 x 2.00.

   unintuitive effects and their consequences. notice that if one of the
   inputs to the multiply gate is very small and the other is very big,
   then the multiply gate will do something slightly unintuitive: it will
   assign a relatively huge gradient to the small input and a tiny
   gradient to the large input. note that in linear classifiers where the
   weights are dot producted \(w^tx_i\) (multiplied) with the inputs, this
   implies that the scale of the data has an effect on the magnitude of
   the gradient for the weights. for example, if you multiplied all input
   data examples \(x_i\) by 1000 during preprocessing, then the gradient
   on the weights will be 1000 times larger, and you   d have to lower the
   learning rate by that factor to compensate. this is why preprocessing
   matters a lot, sometimes in subtle ways! and having intuitive
   understanding for how the gradients flow can help you debug some of
   these cases.

gradients for vectorized operations

   the above sections were concerned with single variables, but all
   concepts extend in a straight-forward manner to matrix and vector
   operations. however, one must pay closer attention to dimensions and
   transpose operations.

   matrix-matrix multiply gradient. possibly the most tricky operation is
   the matrix-id127 (which generalizes all matrix-vector
   and vector-vector) multiply operations:
# forward pass
w = np.random.randn(5, 10)
x = np.random.randn(10, 3)
d = w.dot(x)

# now suppose we had the gradient on d from above in the circuit
dd = np.random.randn(*d.shape) # same shape as d
dw = dd.dot(x.t) #.t gives the transpose of the matrix
dx = w.t.dot(dd)

   tip: use dimension analysis! note that you do not need to remember the
   expressions for dw and dx because they are easy to re-derive based on
   dimensions. for instance, we know that the gradient on the weights dw
   must be of the same size as w after it is computed, and that it must
   depend on id127 of x and dd (as is the case when both
   x,w are single numbers and not matrices). there is always exactly one
   way of achieving this so that the dimensions work out. for example, x
   is of size [10 x 3] and dd of size [5 x 3], so if we want dw and w has
   shape [5 x 10], then the only way of achieving this is with
   dd.dot(x.t), as shown above.

   work with small, explicit examples. some people may find it difficult
   at first to derive the gradient updates for some vectorized
   expressions. our recommendation is to explicitly write out a minimal
   vectorized example, derive the gradient on paper and then generalize
   the pattern to its efficient, vectorized form.

   erik learned-miller has also written up a longer related document on
   taking matrix/vector derivatives which you might find helpful. [11]find
   it here.

summary

     * we developed intuition for what the gradients mean, how they flow
       backwards in the circuit, and how they communicate which part of
       the circuit should increase or decrease and with what force to make
       the final output higher.
     * we discussed the importance of staged computation for practical
       implementations of id26. you always want to break up
       your function into modules for which you can easily derive local
       gradients, and then chain them with chain rule. crucially, you
       almost never want to write out these expressions on paper and
       differentiate them symbolically in full, because you never need an
       explicit mathematical equation for the gradient of the input
       variables. hence, decompose your expressions into stages such that
       you can differentiate every stage independently (the stages will be
       matrix vector multiplies, or max operations, or sum operations,
       etc.) and then backprop through the variables one step at a time.

   in the next section we will start to define neural networks, and
   id26 will allow us to efficiently compute the gradients on
   the connections of the neural network, with respect to a id168.
   in other words, we   re now ready to train neural nets, and the most
   conceptually difficult part of this class is behind us! convnets will
   then be a small step away.

references

     * [12]automatic differentiation in machine learning: a survey

     * [13]cs231n
     * [14]cs231n
     * [15]karpathy@cs.stanford.edu

references

   1. http://cs231n.github.io/
   2. http://cs231n.github.io/optimization-2/#intro
   3. http://cs231n.github.io/optimization-2/#grad
   4. http://cs231n.github.io/optimization-2/#backprop
   5. http://cs231n.github.io/optimization-2/#intuitive
   6. http://cs231n.github.io/optimization-2/#sigmoid
   7. http://cs231n.github.io/optimization-2/#staged
   8. http://cs231n.github.io/optimization-2/#patterns
   9. http://cs231n.github.io/optimization-2/#mat
  10. http://cs231n.github.io/optimization-2/#summary
  11. http://cs231n.stanford.edu/vecderivs.pdf
  12. http://arxiv.org/abs/1502.05767
  13. https://github.com/cs231n
  14. https://twitter.com/cs231n
  15. mailto:karpathy@cs.stanford.edu
