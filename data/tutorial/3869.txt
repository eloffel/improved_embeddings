   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]#wecocreate
     * [9]mission
     * [10]solutions
     * [11]bots
     * [12]ai research
     * [13]the week!
     * [14]join the team
     * [15]botsupply.ai
     __________________________________________________________________

   [1*so-sp58t4bre9ehazhsega.png]

generative model chatbots

   [16]go to the profile of kumar shridhar
   [17]kumar shridhar (button) blockedunblock (button) followfollowing
   may 22, 2017

   as discussed in my previous post about the types of bots and it seemed
   that the generative bots are the smartest chatbots models out there.
   but are they solutions for our every problems?

     to know more about the types of chatbots and why generative models
     based chatbots are the most smart ones today, just follow my
     previous post:

   [18]rule based bots vs ai bots
   if the bot answers a question logically or solves a given task, it
   should be considered smart. a lot of personal   medium.com

sequence to sequence models:

   sequence to sequence based models was introduced first in the paper
   [19]learning phrase representations using id56 encoder-decoder for
   id151 by [20]kyunghyun cho et al in 2014.

   the proposed neural network architecture known as id56 encoder   decoder,
   consists of two recurrent neural networks (id56) that act as an encoder
   and a decoder pair. the encoder maps a variable-length source sequence
   to a fixed-length vector, and the decoder maps the vector
   representation back to a variable-length target sequence. the two
   networks are trained jointly to maximize the id155 of
   the target sequence given a source sequence. [2]
   [1*so-sp58t4bre9ehazhsega.png]
   image source: [21]deep learning for chatbots part 1
     __________________________________________________________________

the model

   id56 or recurrent neural network is a neural network where the output
   not only depends on the current input, but to a series of input given
   in the past. since the output is influenced by a series of past inputs,
   it makes id56 very effective in natural language processing as the
   contexts of next word does not necessarily rely only on the previous
   word but to a series of words before that.

   depending on the scenario, the id56s can be used to deal with a variety
   of tasks. some of them are listed down below.
   [1*ruwdm3-cuhpzuk3ga5hpfa.png]
   image source:[22] the unreasonable effectiveness of recurrent
   neural networks

   if you want to know more about id56 in detail and their use cases, visit
   [23]recurrent neural networks tutorial, part 1         introduction to id56s.
   [24]recurrent neural networks tutorial, part 1 - introduction to id56s
   recurrent neural networks (id56s) are popular models that have shown
   great promise in many nlp tasks. but despite their   www.wildml.com

   but, vanilla id56 faces the problem of [25]vanishing gradient which
   paved the way for lstms that handles the problem very well due to the
   introduction of memory cells and gates in lstms.

   lstms are type of id56s that handles the long term dependency problem of
   id56s very well due to the introduction of gates in lstm. it allows the
   cells to remember what information needs to be remembered from the
   previous cells and what needs to be updated. read more about lstms
   here:
   [26]understanding id137 -- colah's blog
   these loops make recurrent neural networks seem kind of mysterious.
   however, if you think a bit more, it turns out that   colah.github.io

   the sequence to sequence model uses two id137, one each for
   encoding and decoding respectively. i used three lstm layers with 512
   as layer sizes respectively. some other parameters that i used were:

     batch size: 128

     learning rate: played with few learning rates. chose a low learning
     rate overall. model did not performed very well. then i initialized
     a large learning rate with decaying factor and the performance
     improved.

     vocabulary size: 50000 words for both encoding and decoding models

     checkpoints saved after every 500 iterations.
     __________________________________________________________________

dataset used

   for training the model, two files were created where one acted as input
   and other as output. input file contained all the questions or the
   first part of the conversation whereas the output file contained all
   the respective answers or the other part of the conversation. each file
   had a vocabulary of 50ooo words and any word outside of the vocabulary
   was marked with an unk tag (unknown).

   an example of the lines in files are:

     input: hi!

     output: hi!

     input: how are you?

     output: i am good. thanks for asking.

     input: you   re asking me out. that   s so cute. what   s your name again?

     output: forget it.

     input: no, no, it   s my fault         we didn   t have a proper introduction

     output: cameron.

     input: the thing is, cameron         i   m at the mercy of a particularly
     hideous breed of loser. my sister. i can   t date until she does.

     output: seems like she could get a date easy enough   

   the datasets used for training the model are following:
    1. reddit conversation datasets for the months of september         december
       2016 which included all the comments on reddit and their replies
       and their respective replies or comments and so on. the dataset was
       cleaned where only those questions were taken into account where
       there was a reply for the question asked. all the other replies and
       sub comments were removed. the questions asked were saved in the
       input file and the respective comment/answer to the output file.

   [27]directory contents
   if you have any questions about the data formats of the files or any
   other questions, please feel free to contact me
   at   files.pushshift.io

     the [28]link is used to get the reddit datasets which is available
     in json format of all the years. please feel free to make a donation
     here ([29]https://pushshift.io/donations/) if you are using a lot of
     datasets to appreciate the efforts of the creators.

   2. [30]cornell movie         dialogs corpus which contains a large
   metadata-rich collection of fictional conversations extracted from raw
   movie scripts. it has 220,579 conversational exchanges between 10,292
   pairs of movie characters.
   [31]cornell movie-dialogs corpus
   this corpus contains a large metadata-rich collection of fictional
   conversations extracted from raw movie scripts:www.cs.cornell.edu

   3. twitter feed data with a [32]twitter scrapper and data pre processed
   and arranged in question-answer or conversational format.
   [33]marsan-ma/twitter_scraper
   twitter_scraper - scrap real time posts from twitter through the
   streaming apigithub.com

   4. other datasets include [34]ubuntu dialogue corpus v1.0 and
   [35]washington university law datasets which were cleaned and modified
   accordingly.
     __________________________________________________________________

training

   the model was trained on a nvidia titan x 12 gb gpu for 3 days.

     number of iterations : 90000

     learning rate at 90000 iteration: 0.0003 (starting learning rate:
     0.15 with a 0.9 learning rate decay factor)

     perplexity: 2.74 (starting perplexity close to 400)

     checkpoints saved after every 500 iterations
     __________________________________________________________________

performance

   the model performed well for few common sentences but it did not
   performed very good on uncommon sentences although it tried to generate
   some answers.
   [1*ai131am7dird_usfd0wxaw.png]
   the bot name and icon in browser

     human: hi!

     bot: hi!

     human: how are you?

     bot: why are you asking?

     human: who are you?

     bot: why are you doing this?

     human: i have a problem with my ubuntu computer.

     bot: what is that?

     human: i donot know.

     bot: so it all worked fine.

     human: are you a human ?

     bot: no , not real .

     human: are you a computer?

     bot: certainly , doctor .

     human: i am not a doctor.

     bot: i don     t believe it .

     human: am i a doctor?

     bot: sure but a can always be a.

   from the above sentences, we can see that the bot is performing
   reasonably well on the small talk sentences and it is generating
   answers based on the questions asked. it is also taking the previous
   context well into account while generating the next sentences which
   should not be true as the model does not have a memory to store
   previous contexts. but due to the fact that the model is trained on a
   conversation, it has learned the usage of pronouns reasonably well and
   is trying to incorporate into the answers in a similar way as the
   questions asked.
     __________________________________________________________________

     human: who is the president?

     bot: nice, me.

     human: will ai destroy humanity?

     bot: to find what happened.

     human: when will the world end

     bot: you mean last night would you?

     human: get lost!

     bot: you got it !

     human: can you teach me something?

     bot: what do i do to install ?

     human: you are not making sense

     bot: yeah , i know , : ) : ) )

   now when the model is asked some random question, it is generating some
   random answers which sometimes makes complete sense and sometimes it
   does not. (maybe bot   s level of intelligence is way higher than us and
   it is difficult to understand it.) also, a thing to note is that the
   bot has learned to use some smileys too which to much surprise goes
   well with the emotions it wants to convey. it has just learned the
   position of the smileys and a smiling face smiley :) is generally after
   a positive reply and it is doing that exactly without learning the
   context to do so. or, it might has associated the smiley with some
   words and it is attaching the smiley with the words which is a very
   good sign.
     __________________________________________________________________

     human: if i have a blue box, what color is my box?

     bot: what did you say ? are you all right ?

     human: are you drunk?

     bot: i     m him .

     human: tell me a good name for my son?

     bot: ah , i have windows , so it     s still good.

     human: tell me the name of your company

     bot: it     s a named horror de de de de .

     human: who is your daddy?

     bot: the name, then i must kill it ?

     human: are you gay?

     bot: yes, thanks :)

   in the above sentences, the bot is replying with the answers that
   syntactically make sense, but lacks the logical aspect. like for
   question    are you drunk   , the bot feels it is similar to    are you
   computer    and it replies with an affirmative answer which to structural
   logic it is correct as drunk word might be new to the bot (not in
   vocabulary) and it associated the word more close to computer than
   human and replies with an affirmative answer. similarly, for question
      tell me the name of your company   , the bot generates a fictitious name
      horror de de de de    which is a very good sign for a generative model.
   it is understanding the question to some extent and it came up with a
   great name, i would say.
     __________________________________________________________________

experimentation

    1. changing the word order in the input sentences and feeding them in
       reverse (last to first) to the encoder proved very useful in
       machine translation as it seemed that the decoder learns better due
       to last word in encoder gets closer to the first word in decoder
       and it learned better. i tried the same for the chatbot and i could
       not see a significant difference. it might be due to the fact that
       i am training on a very huge corpus and it might be difficult to
       evaluate the results. but upon training on both the ways and when
       asked with similar questions, i could not see much difference.
    2. i tried with more lstm layers and cells (4 lstms with 1024 cells)
       and the training time increased to a large extent which made to
       stop the model after 30k iterations and i switched back to 3 lstms
       with 512 cells. it might be said that more lstms layers might have
       performed better but there was a tradeoff between time and
       performance and i sticked to 3 layers at the moment. i will
       definitely try it with more lstms and will share the results in
       future.
    3. i tried with different learning rate and finally settled with a
       high learning rate at the start (0.15) and a learning rate decay of
       0.9 after that. a lot of learning rates can be tried.
    4. i kept my vocabulary limited to 50k words, which includes mostly
       used words in english. it can be increased or decreased based on
       the use case. also, the data clean up and pre processing of the
       data can be done according to the use cases.
     __________________________________________________________________

future scope

   in future, the model can be integrated with [36]memory networks like
   [37]end to end memory networks or integration of attention mechanism in
   the bot where the model learns to focus on certain words while
   generating answers.

references

    1. [38]learning phrase representations using id56 encoder-decoder for
       id151
    2. [39]end-to-end memory networks
    3. [40]memory networks
    4. [41]sequence to sequence learning with neural networks
    5. [42]deep learning for chatbots, part 1         introduction
    6. [43]understanding id137
    7. [44]tensorflow recurrent neural network
    8. [45]the unreasonable effectiveness of recurrent neural networks

   [1*njfizb8sumesnt02dvkwkg.png]

   iframe: [46]/media/805f1764462c707f814d691705bae075?postid=e422ab08461e

we   re a team of bot creatives and ai scientists with one common goal:

     blowing business objectives out of the water with bots and cognitive
     solutions

   [47][1*filktc1h4v_94ecdxj3vda.png]
   [48][1*ww49xuti9_fcxy8q9ml9mg.png]
   [49][1*qduacsrpqoj29pz9ffsljg.png]
   [1*yfanetnnkgbij0fvzixymq.gif]
   did you like the article? click     to recommend it to other
   medium readers!

     * [50]seq2seid24
     * [51]artificial intelligence
     * [52]recurrent neural network
     * [53]lstm
     * [54]cognitive

   (button)
   (button)
   (button) 500 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [55]go to the profile of kumar shridhar

[56]kumar shridhar

   helping machines in their quest to rule us! | nlp | id161|
   botsupply.ai | kumar-shridhar.github.io

     (button) follow
   [57]#wecocreate

[58]#wecocreate

   organizations partner with our network of ai scientists, bot engineers
   and creatives to co-create ai & bots

     * (button)
       (button) 500
     * (button)
     *
     *

   [59]#wecocreate
   never miss a story from #wecocreate, when you sign up for medium.
   [60]learn more
   never miss a story from #wecocreate
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e422ab08461e
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/botsupply/generative-model-chatbots-e422ab08461e&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/botsupply/generative-model-chatbots-e422ab08461e&source=--------------------------nav_reg&operation=register
   8. https://medium.com/botsupply?source=logo-lo_6f93wesw2omm---8a5c79a9c9e6
   9. https://medium.com/botsupply/mission/home
  10. https://medium.com/botsupply/solutions/home
  11. https://medium.com/botsupply/bots/home
  12. https://medium.com/botsupply/airesearch/home
  13. https://medium.com/botsupply/theweek/home
  14. https://medium.com/botsupply/jointheteam/home
  15. http://www.botsupply.ai/
  16. https://medium.com/@shridhar743?source=post_header_lockup
  17. https://medium.com/@shridhar743
  18. https://medium.com/@shridhar743/rule-based-bots-vs-ai-bots-b60cdb786ffa
  19. https://arxiv.org/abs/1406.1078
  20. https://arxiv.org/find/cs/1/au:+cho_k/0/1/0/all/0/1
  21. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  22. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  23. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  24. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  25. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  26. http://colah.github.io/posts/2015-08-understanding-lstms/
  27. http://files.pushshift.io/reddit/comments/
  28. http://files.pushshift.io/reddit/comments/
  29. https://pushshift.io/donations/
  30. https://www.cs.cornell.edu/~cristian/cornell_movie-dialogs_corpus.html
  31. https://www.cs.cornell.edu/~cristian/cornell_movie-dialogs_corpus.html
  32. https://github.com/marsan-ma/twitter_scraper
  33. https://github.com/marsan-ma/twitter_scraper
  34. http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/
  35. http://scdb.wustl.edu/data.php
  36. https://arxiv.org/abs/1410.3916
  37. https://arxiv.org/pdf/1503.08895.pdf
  38. https://arxiv.org/abs/1406.1078
  39. https://arxiv.org/pdf/1503.08895.pdf
  40. https://arxiv.org/abs/1410.3916
  41. https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
  42. http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/
  43. http://colah.github.io/posts/2015-08-understanding-lstms/
  44. https://www.tensorflow.org/tutorials/recurrent
  45. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  46. https://medium.com/media/805f1764462c707f814d691705bae075?postid=e422ab08461e
  47. https://www.facebook.com/botsupply
  48. https://www.linkedin.com/company/botsupply
  49. https://twitter.com/botsupplyhq
  50. https://medium.com/tag/seq2seid24?source=post
  51. https://medium.com/tag/artificial-intelligence?source=post
  52. https://medium.com/tag/recurrent-neural-network?source=post
  53. https://medium.com/tag/lstm?source=post
  54. https://medium.com/tag/cognitive?source=post
  55. https://medium.com/@shridhar743?source=footer_card
  56. https://medium.com/@shridhar743
  57. https://medium.com/botsupply?source=footer_card
  58. https://medium.com/botsupply?source=footer_card
  59. https://medium.com/botsupply
  60. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  62. https://medium.com/@shridhar743/rule-based-bots-vs-ai-bots-b60cdb786ffa
  63. http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/
  64. http://colah.github.io/posts/2015-08-understanding-lstms/
  65. http://files.pushshift.io/reddit/comments/
  66. https://www.cs.cornell.edu/~cristian/cornell_movie-dialogs_corpus.html
  67. https://github.com/marsan-ma/twitter_scraper
  68. https://medium.com/p/e422ab08461e/share/twitter
  69. https://medium.com/p/e422ab08461e/share/facebook
  70. https://medium.com/p/e422ab08461e/share/twitter
  71. https://medium.com/p/e422ab08461e/share/facebook
