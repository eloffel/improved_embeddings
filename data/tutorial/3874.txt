   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

using the lstm api in tensorflow (3/7)

   [9]go to the profile of erik hallstr  m
   [10]erik hallstr  m (button) blockedunblock (button) followfollowing
   nov 18, 2016

   [11]in the previous post we modified our to code to use the tensorflow
   native id56 api. now we will go about to build a modification of a id56
   that called a    recurrent neural network with long short-term memory    or
   id56-lstm. this architecture was pioneered by [12]j  rgen schmidhuber
   among others. one problem with the id56 when using long
   time-dependencies (truncated_backprop_length is large) is the
      [13]vanishing gradient problem   . one way to counter this is using a
   state that is    protected    and    selective   . the id56-lstm remembers,
   forgets and chooses what to pass on and output depending on the current
   state and input.

   since this primarily is a practical tutorial i won   t go into more
   detail about the theory, i recommend reading [14]this article again,
   continue with the    modern id56 architectures   . after you have done that
   read and look at the figures on [15]this page. notice that the last
   mentioned resource are using vector concatenation in their
   calculations.

   in the previous article we didn   t have to allocate the internal weight
   matrix and bias, that was done by tensorflow automatically    under the
   hood   . a lstm id56 has many more    moving parts   , but by using the native
   api it will also be very simple.

different state

   a lstm have a    cell state    and a    hidden state   , to account for this
   you need to remove _current_state on line 79 in the previous script and
   replace it with this:

   iframe: [16]/media/af1a97c85b5f61563422d4832c96ce6c?postid=5f2b97ca6b73

   tensorflow uses a data structure called lstmstatetuple internally for
   its lstm:s, where the first element in the tuple is the cell state, and
   the second is the hidden state. so you need to change line 28 where the
   init_state is placeholders are declared to these lines:

   iframe: [17]/media/6a42997a825b1cb44b063d2f293ea6d7?postid=5f2b97ca6b73

   changing the forward pass is now straight forward, you just change the
   function call to create a lstm and supply the initial state-tuple on
   line 38   39.

   iframe: [18]/media/9c841bfc7f526e07f4db662cc775fb6b?postid=5f2b97ca6b73

   the states_series will be a list of hidden states as tensors, and
   current_state will be a lstmstatetuple which shows both the hidden- and
   the cell state on the last time-step as shown below:
   [1*74hcrpbtjstecrqxzzjtdq.png]
   outputs of the previous states and the last lstmstatetuple

   so the current_state returns the cell- and hidden state in a tuple.
   they should be separated after calculation and supplied to the
   placeholders in the run-function on line 90.

   iframe: [19]/media/318f5c3bbea63ed67d3926d7bbfe9644?postid=5f2b97ca6b73

   iframe: [20]/media/cc7a9b8acba77d1b46f880e9eef98c3a?postid=5f2b97ca6b73

whole program

   this is the full code for creating a id56 with long short-term memory.

   iframe: [21]/media/cbcb559eaf8dfb7b2b5bb1425440373f?postid=5f2b97ca6b73

next step

   [22]in the next article we will create a multi-layered or    deep   
   recurrent neural network, also with long short-term memory.

     * [23]machine learning
     * [24]deep learning
     * [25]recurrent neural network
     * [26]tensorflow
     * [27]artificial intelligence

   (button)
   (button)
   (button) 472 claps
   (button) (button) (button) 14 (button) (button)

     (button) blockedunblock (button) followfollowing
   [28]go to the profile of erik hallstr  m

[29]erik hallstr  m

   studied engineering physics and in machine learning at royal institute
   of technology in stockholm. also been living in taiwan             . interested
   in deep learning.

     * (button)
       (button) 472
     * (button)
     *
     *

   [30]go to the profile of erik hallstr  m
   never miss a story from erik hallstr  m, when you sign up for medium.
   [31]learn more
   never miss a story from erik hallstr  m
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5f2b97ca6b73
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@erikhallstrm?source=post_header_lockup
  10. https://medium.com/@erikhallstrm
  11. https://medium.com/@erikhallstrm/tensorflow-id56-api-2bb31821b185#.j7hwrbnip
  12. https://www.youtube.com/watch?v=xkltshnd6xe
  13. http://neuralnetworksanddeeplearning.com/chap5.html
  14. https://arxiv.org/pdf/1506.00019.pdf
  15. http://colah.github.io/posts/2015-08-understanding-lstms/
  16. https://medium.com/media/af1a97c85b5f61563422d4832c96ce6c?postid=5f2b97ca6b73
  17. https://medium.com/media/6a42997a825b1cb44b063d2f293ea6d7?postid=5f2b97ca6b73
  18. https://medium.com/media/9c841bfc7f526e07f4db662cc775fb6b?postid=5f2b97ca6b73
  19. https://medium.com/media/318f5c3bbea63ed67d3926d7bbfe9644?postid=5f2b97ca6b73
  20. https://medium.com/media/cc7a9b8acba77d1b46f880e9eef98c3a?postid=5f2b97ca6b73
  21. https://medium.com/media/cbcb559eaf8dfb7b2b5bb1425440373f?postid=5f2b97ca6b73
  22. https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40#.3ik75ofe4
  23. https://medium.com/tag/machine-learning?source=post
  24. https://medium.com/tag/deep-learning?source=post
  25. https://medium.com/tag/recurrent-neural-network?source=post
  26. https://medium.com/tag/tensorflow?source=post
  27. https://medium.com/tag/artificial-intelligence?source=post
  28. https://medium.com/@erikhallstrm?source=footer_card
  29. https://medium.com/@erikhallstrm
  30. https://medium.com/@erikhallstrm
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/5f2b97ca6b73/share/twitter
  34. https://medium.com/p/5f2b97ca6b73/share/facebook
  35. https://medium.com/p/5f2b97ca6b73/share/twitter
  36. https://medium.com/p/5f2b97ca6b73/share/facebook
