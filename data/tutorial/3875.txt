   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

using the multilayered lstm api in tensorflow (4/7)

   [9]go to the profile of erik hallstr  m
   [10]erik hallstr  m (button) blockedunblock (button) followfollowing
   nov 19, 2016

   [11]in the previous article we learned how to use the tensorflow api to
   create a recurrent neural network with long short-term memory. in this
   post we will make that architecture deep, introducing a lstm with
   multiple layers.

   one thing to notice is that for every layer of the network we will need
   a hidden state and a cell state. typically the input to the next
   lstm-layer will be the previous state for that particular layer as well
   as the hidden activations of the    lower    or previous layer. there is a
   good diagram in [12]this article.

   we could continue to store the states for each layer in many
   lstmtuples, but that would require a lot of overhead. you can only
   input data to the placeholders trough the feed_dict as python lists or
   numpy arrays anyways (not as lstmtuples) so we still would have to
   convert between the datatypes. why not save the whole state for the
   network in a big tensor? in order to do this the first thing we want to
   do is to replace _current_cell_state and _current_hidden_state on line
   81   82 with the more generic:

   iframe: [13]/media/b5b2d4a71b232235375d589a5fb79169?postid=f6e7da7bbe40

   you also have to declare the new setting num_layers = 3 in the
   beginning of the file, but you may choose any number of layers. the    2   
   refers to the two states, cell- and hidden-state. so for each layer and
   each sample in a batch, we have both a cell state and a hidden state
   vector with the size state_size.

   now modify lines 93 to 103 (the run function and the separation of the
   state tuple) back to the original statement, since the state is now
   stored in a single tensor.

   iframe: [14]/media/6f05c59585476df54469b97b43a514da?postid=f6e7da7bbe40

   you can change these lines 28 to 30 in the previous post:

   iframe: [15]/media/6a42997a825b1cb44b063d2f293ea6d7?postid=f6e7da7bbe40

   to a single placeholder containing the whole state.

   iframe: [16]/media/e2dff4417ddafcd4b148c2af3872bb2f?postid=f6e7da7bbe40

   since the tensorflow multilayer-lstm-api accepts the state as a tuple
   of lstmtuples, we need to unpack the state state into this structure.
   for each layer in the state we then create a lstmtuple stated, and put
   these in a tuple, as shown below. add this just after the init_state
   placeholder.

   iframe: [17]/media/063014658275882d4b1cdc26a3c1ee54?postid=f6e7da7bbe40

   the forward pass on lines 40 and 41 should be changed to this:

   iframe: [18]/media/33230e4c28ff7ed44d369482ad92c47a?postid=f6e7da7bbe40

   the multi-layered lstm is created by first making a single lsmtcell,
   and then duplicating this cell in an array, supplying it to
   themultiid56cell api call. the forward pass uses the usual tf.nn.id56,
   let   s print the output of this function, the states_series and
   current_state variables.
   [1*_py2ipausp2amqva0xqbvw.png]
   output of the previous states and the last lstmstatetuples

   take a look at the tensor names between single quotes, we see that the
   id56 is unrolled 15 times. in the states_series all outputs have the
   name    cell2   , it means that we get the output of the last lstm layer   s
   hidden state in the list. furthermore the lstmstatetuple in the
   current_state gives the whole state of all layers in the network.
      cell0    refers to the first layer,    cell1    to the second and    cell2    to
   the third and final layer,    h    and    c    refers to hidden- and cell
   state.

whole program

   this is the whole self-contained script, just copy and run.

   iframe: [19]/media/31d11ad87edc748c78759fdcbf0eaaf1?postid=f6e7da7bbe40

next step

   [20]in the next article we will speed up the graph creation by not
   splitting up our inputs and labels into a python list.

     * [21]deep learning
     * [22]machine learning
     * [23]artificial intelligence
     * [24]neural networks
     * [25]recurrent neural network

   (button)
   (button)
   (button) 520 claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of erik hallstr  m

[27]erik hallstr  m

   studied engineering physics and in machine learning at royal institute
   of technology in stockholm. also been living in taiwan             . interested
   in deep learning.

     * (button)
       (button) 520
     * (button)
     *
     *

   [28]go to the profile of erik hallstr  m
   never miss a story from erik hallstr  m, when you sign up for medium.
   [29]learn more
   never miss a story from erik hallstr  m
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f6e7da7bbe40
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@erikhallstrm?source=post_header_lockup
  10. https://medium.com/@erikhallstrm
  11. https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73#.a6ai9917d
  12. https://arxiv.org/pdf/1409.2329v5.pdf
  13. https://medium.com/media/b5b2d4a71b232235375d589a5fb79169?postid=f6e7da7bbe40
  14. https://medium.com/media/6f05c59585476df54469b97b43a514da?postid=f6e7da7bbe40
  15. https://medium.com/media/6a42997a825b1cb44b063d2f293ea6d7?postid=f6e7da7bbe40
  16. https://medium.com/media/e2dff4417ddafcd4b148c2af3872bb2f?postid=f6e7da7bbe40
  17. https://medium.com/media/063014658275882d4b1cdc26a3c1ee54?postid=f6e7da7bbe40
  18. https://medium.com/media/33230e4c28ff7ed44d369482ad92c47a?postid=f6e7da7bbe40
  19. https://medium.com/media/31d11ad87edc748c78759fdcbf0eaaf1?postid=f6e7da7bbe40
  20. https://medium.com/@erikhallstrm/using-the-dynamicid56-api-in-tensorflow-7237aba7f7ea#.t9o6kvn3z
  21. https://medium.com/tag/deep-learning?source=post
  22. https://medium.com/tag/machine-learning?source=post
  23. https://medium.com/tag/artificial-intelligence?source=post
  24. https://medium.com/tag/neural-networks?source=post
  25. https://medium.com/tag/recurrent-neural-network?source=post
  26. https://medium.com/@erikhallstrm?source=footer_card
  27. https://medium.com/@erikhallstrm
  28. https://medium.com/@erikhallstrm
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/f6e7da7bbe40/share/twitter
  32. https://medium.com/p/f6e7da7bbe40/share/facebook
  33. https://medium.com/p/f6e7da7bbe40/share/twitter
  34. https://medium.com/p/f6e7da7bbe40/share/facebook
