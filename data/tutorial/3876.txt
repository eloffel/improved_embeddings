   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

using the dynamicid56 api in tensorflow (5/7)

   [9]go to the profile of erik hallstr  m
   [10]erik hallstr  m (button) blockedunblock (button) followfollowing
   nov 19, 2016

   [11]in the previous guide we built a multi-layered lstm id56. in this
   post we will speed it up by not splitting up our inputs and labels into
   a list, as done on line 41   42 in our code. you may remove these rows
   where inputs_series and labels_series are declared. next change the
   tf.nn.id56 call on line 47 to the following:

   iframe: [12]/media/7344b31543abcb98060e441b78878efe?postid=7237aba7f7ea

   the dynamic_id56 function takes the batch inputs of shape [batch_size,
   truncated_backprop_length, input_size], thus the addition of a single
   dimension on the end. output will be the last state of every layer in
   the network as an lstmstatetuple stored incurrent_state as well as a
   tensor states_series with the shape [batch_size,
   truncated_backprop_length, state_size] containing the hidden state of
   the last layer across all time-steps.
   [1*dh-jhqalfi7rnsexykc7bw.png]
   the states are not in lists anymore.

   the tensor states_series is reshaped on the second row in the code
   sample above to shape [batch_size*truncated_backprop_length,
   state_size], we will see the reason for this shortly. you may read more
   about dynamic_id56 in [13]the documentation.

   now input this two lines below the reshaping of the states_series .

   iframe: [14]/media/f66e08d637a8f8e15974af2beff9b2d0?postid=7237aba7f7ea

   notice that we are now only working with tensors, python lists were a
   thing of the past. the calculation of the logits and the labels are
   visualized below, notice the state_series variable that was reshaped
   earlier. in tensorflow reshaping is done in [15]c-like index order. it
   means that we read from the source tensor and    write    to the
   destination tensor with the last axis index changing fastest, and the
   first axis index changing slowest. the result of the reshaping will be
   as visualized in the figure below, where similar colors denote the same
   time-step, and the vertical grouped spacing of elements denote
   different batches.
   [1*mps2qmofkbmr0nu1wm5tjq.png]
   visualization of the calculations, similar color denote same time-step,
   vertical spacing denote new batch.

   let   s go trough all the tensors in the figure above, first let   s start
   with the sizes. we have that batch_size=3 , state_size=3 ,
   num_classes=2 and truncated_backprop_length=3. the tensor states_series
   have shape [batch_size*truncated_backprop_length, state_size], labels
   have shape [batch_size*truncated_backprop_length], logits have shape
   [batch_size*truncated_backprop_length, num_classes], w2 have shape
   [state_size, num_classes] and b2 have shape [1, num_classes]. it can be
   a bit tricky to keep track of all the tensors, but drawing and
   visualizing with colors definitely helps.

   next calculate the predictions for the visualization:

   iframe: [16]/media/a63b5ef96fd5debd7b4923957d4ae668?postid=7237aba7f7ea

   here we actually split the tensors into lists again. this is perhaps
   not the best way to do it, but it   s quick and dirty, and the plot
   function is already expecting a list.

   the sparse_softmax_cross_id178_with_logits can take the shape of our
   tensors! modify the losses calculation to this.

   iframe: [17]/media/8dc604bd1b8393635f42c926e4705e2f?postid=7237aba7f7ea

   as we can read in [18]the api the logits must have the shape
   [batch_size, num_classes] and labels must have the shape [batch_size].
   but now we are treating all time-steps as elements in our batch, so it
   will work out as we want.

whole program

   this is the whole self-contained script, just copy and run.

   iframe: [19]/media/f3a64d0096df414b76f873214a2b9e86?postid=7237aba7f7ea

next step

   [20]in the next part we will regularize the network to use dropout,
   making it less prone to overfitting.

     * [21]machine learning
     * [22]tensorflow
     * [23]neural networks
     * [24]artificial intelligence
     * [25]recurrent neural network

   (button)
   (button)
   (button) 493 claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of erik hallstr  m

[27]erik hallstr  m

   studied engineering physics and in machine learning at royal institute
   of technology in stockholm. also been living in taiwan             . interested
   in deep learning.

     * (button)
       (button) 493
     * (button)
     *
     *

   [28]go to the profile of erik hallstr  m
   never miss a story from erik hallstr  m, when you sign up for medium.
   [29]learn more
   never miss a story from erik hallstr  m
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7237aba7f7ea
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@erikhallstrm/using-the-dynamicid56-api-in-tensorflow-7237aba7f7ea&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@erikhallstrm/using-the-dynamicid56-api-in-tensorflow-7237aba7f7ea&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@erikhallstrm?source=post_header_lockup
  10. https://medium.com/@erikhallstrm
  11. https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40#.553dr9x54
  12. https://medium.com/media/7344b31543abcb98060e441b78878efe?postid=7237aba7f7ea
  13. https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#dynamic_id56
  14. https://medium.com/media/f66e08d637a8f8e15974af2beff9b2d0?postid=7237aba7f7ea
  15. https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html
  16. https://medium.com/media/a63b5ef96fd5debd7b4923957d4ae668?postid=7237aba7f7ea
  17. https://medium.com/media/8dc604bd1b8393635f42c926e4705e2f?postid=7237aba7f7ea
  18. https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#sparse_softmax_cross_id178_with_logits
  19. https://medium.com/media/f3a64d0096df414b76f873214a2b9e86?postid=7237aba7f7ea
  20. https://medium.com/@erikhallstrm/using-the-dropout-api-in-tensorflow-2b2e6561dfeb#.c4rgxnqhi
  21. https://medium.com/tag/machine-learning?source=post
  22. https://medium.com/tag/tensorflow?source=post
  23. https://medium.com/tag/neural-networks?source=post
  24. https://medium.com/tag/artificial-intelligence?source=post
  25. https://medium.com/tag/recurrent-neural-network?source=post
  26. https://medium.com/@erikhallstrm?source=footer_card
  27. https://medium.com/@erikhallstrm
  28. https://medium.com/@erikhallstrm
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/7237aba7f7ea/share/twitter
  32. https://medium.com/p/7237aba7f7ea/share/facebook
  33. https://medium.com/p/7237aba7f7ea/share/twitter
  34. https://medium.com/p/7237aba7f7ea/share/facebook
