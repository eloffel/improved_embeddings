   #[1]data science musing of kapild.    feed [2]data science musing of
   kapild.    comments feed [3]data science musing of kapild.    how
   cifar-10 data set trained me to become a deep learning scientist
   comments feed [4]how similar are neighborhoods of austin, texas.
   [5]image captioning using id56 and lstm. [6]alternate [7]alternate
   [8]data science musing of kapild. [9]wordpress.com

   [10]skip to content

[11]data science musing of kapild.

   (button) sidebar

   search for: ____________________ search

recent posts

     * [12]solving id161 problems
     * [13]learning to rank papers
     * [14]id161/deep learning papers
     * [15]run id97 on lotr movie books using skip gram approach
     * [16]covnets visualization: image gradients, deconvnets, fooling
       images, deepdream and more.

recent comments

   kranthi kiran on [17]k-mean id91 using silhou   
   kranthi kiran on [18]k-mean id91 using silhou   
   kranthi kiran on [19]k-mean id91 using silhou   
   [20]mukka teja on [21]solving id161 p   
   [22]solving computer vis    on [23]covnets visualization: image g   

archives

     * [24]june 2018
     * [25]june 2017
     * [26]january 2017
     * [27]november 2016
     * [28]june 2016
     * [29]may 2016
     * [30]march 2016
     * [31]december 2015
     * [32]november 2015
     * [33]september 2015
     * [34]july 2015
     * [35]june 2015
     * [36]may 2015

categories

     * [37]awk
     * [38]categorization
     * [39]cifar10
     * [40]classification
     * [41]id91
     * [42]id161
     * [43]data science
     * [44]deep learning
     * [45]find
     * [46]histogram
     * [47]image captioning
     * [48]id116
     * [49]lstm
     * [50]machine learning
     * [51]papers
     * [52]plot
     * [53]plotting
     * [54]id203
     * [55]id56
     * [56]sampling
     * [57]scikit learn
     * [58]shell
     * [59]silhouette analysis
     * [60]uncategorized
     * [61]unix

meta

     * [62]register
     * [63]log in
     * [64]entries rss
     * [65]comments rss
     * [66]wordpress.com

   advertisements

how cifar-10 data set trained me to become a deep learning scientist

   [67]november 6, 2016november 6, 2016
   [68]kapildalwani[69]classification, [70]deep learning, [71]machine
   learning, [72]vision

   machine learning: i have been solving complex problems using
   different machine learning (m.l.) algorithms like id75,
   id28, id166, id116, radom forest and learning to rank
   etc. i must say m.l. has come a long way.

   deep learning: for the last one year, i have been reading a lot about
   the impact of deep learning in the fields of vision, speech and nlp
   related field. so, i decided to give it a shot.

   in this post i share my story on how i used the [73]cifar-10 data set
   and cs231n stanford course ([74]convolutional neural networks for
   visual recognition) to train myself to become a deep learning
   scientist. (or, least make myself familiar with it algorithms and
   progress.)

results:

   from a simplest nearest neighbor algorithm (knn) to convolution network
   (with  relu, dropout, max pooling, batch norm etc) i was able to
   increase the accuracy of cifar-10 data set from mere  25.6 % to 65.7%.

   the progression of increase in validation accuracy using
   various algorithms is show below.

   marker

here are some of algorithms i implemented:

    1. id92 nearest neighbor algorithm
    2. id166 with softmax
    3. id166 with hinge loss
    4. neural networks
         1. with 2 fully connected layers.
         2. n fully connected neural networks.
    5. convolution network
         1. max pooling
         2. relu
         3. dropouts
         4. batch id172
         5. adamrmspprop
         6. momentum update

results

   deep learning is not about just neural networks but also about how to
   train effectively train your deeper networks so you don   t end with with
   vanishing gradients etc.

   before going ahead, lets see what the data looks likes for cifar-10
   data set:

   cifar-10 data set has small images with 10 different labels. here is a
   sample image set from each of the 10 labels. there are about 50 sample
   set

   screen shot 2016-11-02 at 2.12.34 pm.png

softmax and id166 classifier:

   after training a softmax and id166 classifier, you can visualize the
   weights below.


   weights from softmax classifier
   weights from softmax classifier
   weights from id166 classifier
   weights from id166 classifier


   here are some of the mis-classification for the images.

   screen shot 2016-11-02 at 2.23.55 pm.png

two layer neural networks:

   a two layer neural network has two layers: an input layer and a hidden
   layer. hidden layer is then connected to affine softmax layer which has
   10 units one each for its output class.

   hyper parametrization tuning: while training a neural network one
   should really try playing with different parameters to get the best
   model. for neural networks, i tried the following parameters:
   id173, learning rate and hidden units.

   plotting the validation and training accuracy on various hyper
   parameters gives you this:
   validation accuracy on various hyperparameters
   validation accuracy on various hyperparameters
   weights learned by 50 hidden neural networks units
   weights learned by 50 hidden neural networks units


  update rules for calculating loss:

    while running an iterative algorithm, the goal is to converge as fast
   as possible. there are many ways one can converge
     * id119
     * stochastic id119
     * stochastic id119 + momentum
     * rmsprop
     * adam

   sgd + momentum
   sgd + momentum
   sgd+momentum+ rmspprop + adam
   sgd+momentum+ rmspprop + adam

     * you can observe from the first graph that
          + sgd + momentum graph loss rate decreases faster than sgd
            alone.
          + additionally, higher training and validation accuracy is
            reached with less number of epos.
     * from the second graph: (with additional adam and rmsprop)
          + adam converges must faster than rest.
     * adam > rmsprop >> sgd + momentum >> sgd.

  batch id172:

   one way to easily train deeper networks we can use advance optimization
   techniques like rmsprop, adam etc.

   another strategy is to add additional layer    batch id172    to
   the current models. this uses a mini-batch data set to center the data
   with zero mean and unit variance. each layers takes a mini batch data
   set and stores the running average of these means and standard
   deviations. this helps in training a deeper neural network.

  dropout:

   one way to improve id173 in neural networks is to use
   dropouts. this technique with a x% id203 set some of the features
   of neural network to zero.

   you observe that the training accuracy is less with 0.75% dropout(as
   model is less overfitted) whereas the validation accuracy is same or
   more with dropouts.

   batch id172
   batch id172
   dropout at 0.75
   dropout at 0.75

   relu:

   rectified linear units are backbone of deep learning. they add
   non-linearity to nodes which fires only above a threshold. other
   non-linear units include tanh


convolution network:

   so far we have trained a fully connected deep neural network.
   additionally, we played with few optimization technique.

   finally, i will talk about convolution network.

   conv. nets works by applying filters to the original image by
   convolving a filter of certain width (w) and height(h) and channel
   width(c). there can be f such filters. convolving can be done using
   different strides(s) and applying padding (p).

   so, for an input image x of size (c, h, w) and f such filters of size
   c, hh, ww, you can get an output image of size

     h    = 1 + (h + 2 * pad     hh) / stride
     w    = 1 + (w + 2 * pad     ww) / stride

   pooling:

   next we can pool the convolution image by applying a max filter, which
   further decreases the size of the output image. similarly, for a
   pooling filter of size hh, ww after pooling the input images goes to a
   size of

     h2 = (h w- height)/stride + 1
     w2 = (ww     width)/stride + 1

   combining layers like blocks:

   you can combine the above layers to form one big layer. a typical
   combination include conv -> relu -> max pool


three layer convolution layers:

   once you implemented different layers, you can make a very complex deep
   network by just combining these layers like leggo blocks.


   i implemented a 3-layer conv. layer whose architecture looks like this.

     conv [conv_batch norm]      relu     2  2 max pool     affine [batch norm]
          relu     affine     softmax

   the first layer includes a covn. layer, following by an optional batch
   norm, followd by relu, max pool. second layer is nothing but a affine
   layer wih optional batch norm and relu layer. finally, the third layer
   is affine layer followed by a softmax for classification.

  results:

     * only with 3 epoch i was able to get an accuracy above 55%.
     * with batch norm and 3 epoch the accuracy went up to 64.9%.
     * hyper parameters with one epoch on number of filters and filter
       size i got an accuracy of 59.3%

   3conv-_filterspng filters for 3 layer conv. net3_covn_3_epoch conv.
   layer with just 3 epochsconv3_with_batch_ conv. layer with 3 epoch +
   batch norm3_conv_batch_combied 3 epoch baseline + batch
   normconvnet3_graph hyper parameters on number of filters and filter
   size

best accuracy so far: 65.7%

   finally, combining a 3 convolution layer with batch id172, and
   3 epochs and hyper tuned on number of parameters and filer size i was
   able to achieve an accuracy of 65.7%.
   finall_image.png validation accuracy of about 65.7%


deep learning === deep layers

   from a 3 layer conv. network, its very easy to implement a deeper layer
   conv. nets.

   one can implement various combination of layers to get a really complex
   but robust deep layer conv. nets


     [conv-relu-pool]xn     conv     relu     [affine]xm     [softmax or id166]
     [conv-relu-pool]xn     [affine]xm     [softmax or id166]
     [conv-relu-conv-relu-pool]xn     [affine]xm     [softmax or id166]
     [(conv-relu) * n- pool?]*m     (fc-relu) * k, softmax
     alexnet
     vgnet
     resnet
     googleletnet

  few interesting thing which i learned out it was

     * relu: this unit helps achieve non-linearity in the neural networks.
     * chain rule: chain rule can help implement forward and backward pass
       with ease.
     * adam vs rmsprop: how different iterative methods can help you
       converge faster on an experiment.
     * dropout: how randomly muting some of the nodes of neural network
       can help the model prevent overfitting.
     * batch id172: deeper neural networks have a problem of
       vanishing gradients.  thus in both forward and backward pass the
       gradients becomes almost zero and the model is unable to converge
       or train. with a simple initialization strategy batch id172
       helps train a deeper network.
     * convolution and pooling : can help train efficient and faster
       deeper network.
     * hyperparameter: you can find the best parameters to your model by
       performing a hyper parameter tuning approach.
     * and, much more.


   thanks for reading.


   appendix:
     * i would like to thank [75]andrew karpathy for providing such an
       excellent course for helping me understand the basics and nuances
       of deep learning.
     * also, would like to thank [76]standford for making such a wonderful
       course [77]cs231n: convolutional neural networks for visual
       recognition available online.



   advertisements

share this:

     * [78]twitter
     * [79]facebook
     * [80]reddit
     * [81]linkedin
     * [82]pocket
     *

like this:

   like loading...

related

   [83]how cifar-10 data set trained me to become a deep
   learning scientist

                                post navigation

   [84]    how similar are neighborhoods of austin, texas.
   [85]image captioning using id56 and lstm.    

2 thoughts on    how cifar-10 data set trained me to become a deep
learning scientist   

    1.
   [86]image captioning using id56 and lstm.     data science musing of
       kapild. says:
       [   ] how cifar-10 data set trained me to become a deep
       learning scientist [   ]
       [87]likelike
       [88]january 7, 2017 at 9:05 pm [89]reply
    2.
   [90]convolution network: from 3 layer covnets. to a deep layer(ed)
       network     data science musing of kapild. says:
       [   ] how cifar-10 data set trained me to become a deep
       learning scientist [   ]
       [91]likelike
       [92]january 8, 2017 at 1:10 am [93]reply

leave a reply [94]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [95]googleplus-sign-in

     *
     *

   [96]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [97]log out /
   [98]change )
   google photo

   you are commenting using your google account. ( [99]log out /
   [100]change )
   twitter picture

   you are commenting using your twitter account. ( [101]log out /
   [102]change )
   facebook photo

   you are commenting using your facebook account. ( [103]log out /
   [104]change )
   [105]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

   [106]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [107]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [108]cookie policy

   iframe: [109]likes-master

   %d bloggers like this:

references

   visible links
   1. https://kapilddatascience.wordpress.com/feed/
   2. https://kapilddatascience.wordpress.com/comments/feed/
   3. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/feed/
   4. https://kapilddatascience.wordpress.com/2016/06/05/how-similar-are-neighborhoods-of-austin-texas/
   5. https://kapilddatascience.wordpress.com/2017/01/07/image-captioning-using-id56-and-lstm/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/&for=wpcom-auto-discovery
   8. https://kapilddatascience.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#content
  11. https://kapilddatascience.wordpress.com/
  12. https://kapilddatascience.wordpress.com/2018/06/07/solving-computer-vision-problems/
  13. https://kapilddatascience.wordpress.com/2018/06/06/learning-to-rank-papers/
  14. https://kapilddatascience.wordpress.com/2018/06/05/computer-vision-deep-learning-papers/
  15. https://kapilddatascience.wordpress.com/2017/06/07/run-id97-on-lotr-movie-books-using-skip-gram/
  16. https://kapilddatascience.wordpress.com/2017/01/26/covnets-visualization-image-gradients-deconvnets-fooling-images-deepdream-and-more/
  17. https://kapilddatascience.wordpress.com/2015/12/08/k-mean-id91-using-silhouette-analysis-with-example-part-3/comment-page-1/#comment-198
  18. https://kapilddatascience.wordpress.com/2015/12/08/k-mean-id91-using-silhouette-analysis-with-example-part-3/comment-page-1/#comment-197
  19. https://kapilddatascience.wordpress.com/2015/12/08/k-mean-id91-using-silhouette-analysis-with-example-part-3/comment-page-1/#comment-196
  20. https://plus.google.com/111378058760853315492
  21. https://kapilddatascience.wordpress.com/2018/06/07/solving-computer-vision-problems/comment-page-1/#comment-66
  22. https://kapilddatascience.wordpress.com/2018/06/07/solving-computer-vision-problems/
  23. https://kapilddatascience.wordpress.com/2017/01/26/covnets-visualization-image-gradients-deconvnets-fooling-images-deepdream-and-more/comment-page-1/#comment-58
  24. https://kapilddatascience.wordpress.com/2018/06/
  25. https://kapilddatascience.wordpress.com/2017/06/
  26. https://kapilddatascience.wordpress.com/2017/01/
  27. https://kapilddatascience.wordpress.com/2016/11/
  28. https://kapilddatascience.wordpress.com/2016/06/
  29. https://kapilddatascience.wordpress.com/2016/05/
  30. https://kapilddatascience.wordpress.com/2016/03/
  31. https://kapilddatascience.wordpress.com/2015/12/
  32. https://kapilddatascience.wordpress.com/2015/11/
  33. https://kapilddatascience.wordpress.com/2015/09/
  34. https://kapilddatascience.wordpress.com/2015/07/
  35. https://kapilddatascience.wordpress.com/2015/06/
  36. https://kapilddatascience.wordpress.com/2015/05/
  37. https://kapilddatascience.wordpress.com/category/awk/
  38. https://kapilddatascience.wordpress.com/category/categorization/
  39. https://kapilddatascience.wordpress.com/category/cifar10/
  40. https://kapilddatascience.wordpress.com/category/classification/
  41. https://kapilddatascience.wordpress.com/category/id91/
  42. https://kapilddatascience.wordpress.com/category/machine-learning/computer-vision/
  43. https://kapilddatascience.wordpress.com/category/data-science/
  44. https://kapilddatascience.wordpress.com/category/deep-learning/
  45. https://kapilddatascience.wordpress.com/category/find/
  46. https://kapilddatascience.wordpress.com/category/histogram/
  47. https://kapilddatascience.wordpress.com/category/image-captioning/
  48. https://kapilddatascience.wordpress.com/category/id116/
  49. https://kapilddatascience.wordpress.com/category/lstm/
  50. https://kapilddatascience.wordpress.com/category/machine-learning/
  51. https://kapilddatascience.wordpress.com/category/papers/
  52. https://kapilddatascience.wordpress.com/category/data-science/plot/
  53. https://kapilddatascience.wordpress.com/category/plotting/
  54. https://kapilddatascience.wordpress.com/category/id203/
  55. https://kapilddatascience.wordpress.com/category/id56/
  56. https://kapilddatascience.wordpress.com/category/sampling/
  57. https://kapilddatascience.wordpress.com/category/scikit-learn/
  58. https://kapilddatascience.wordpress.com/category/shell/
  59. https://kapilddatascience.wordpress.com/category/silhouette-analysis/
  60. https://kapilddatascience.wordpress.com/category/uncategorized/
  61. https://kapilddatascience.wordpress.com/category/unix/
  62. https://wordpress.com/start?ref=wplogin
  63. https://kapilddatascience.wordpress.com/wp-login.php
  64. https://kapilddatascience.wordpress.com/feed/
  65. https://kapilddatascience.wordpress.com/comments/feed/
  66. https://wordpress.com/
  67. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/
  68. https://kapilddatascience.wordpress.com/author/kapildalwani/
  69. https://kapilddatascience.wordpress.com/tag/classification/
  70. https://kapilddatascience.wordpress.com/tag/deep-learning/
  71. https://kapilddatascience.wordpress.com/tag/machine-learning/
  72. https://kapilddatascience.wordpress.com/tag/vision/
  73. https://www.cs.toronto.edu/~kriz/cifar.html
  74. http://cs231n.stanford.edu/
  75. http://karpathy.github.io/
  76. https://cs.stanford.edu/
  77. http://cs231n.stanford.edu/
  78. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?share=twitter
  79. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?share=facebook
  80. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?share=reddit
  81. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?share=linkedin
  82. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?share=pocket
  83. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/
  84. https://kapilddatascience.wordpress.com/2016/06/05/how-similar-are-neighborhoods-of-austin-texas/
  85. https://kapilddatascience.wordpress.com/2017/01/07/image-captioning-using-id56-and-lstm/
  86. https://kapilddatascience.wordpress.com/2017/01/07/image-captioning-using-id56-and-lstm/
  87. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?like_comment=14&_wpnonce=f89d2a0270
  88. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#comment-14
  89. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?replytocom=14#respond
  90. https://kapilddatascience.wordpress.com/2017/01/08/convolution-network-from-3-layer-covnets-to-a-deep-layered-network/
  91. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?like_comment=15&_wpnonce=5838afccd9
  92. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#comment-15
  93. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/?replytocom=15#respond
  94. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#respond
  95. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://kapilddatascience.wordpress.com&color_scheme=light
  96. https://gravatar.com/site/signup/
  97. javascript:highlandercomments.doexternallogout( 'wordpress' );
  98. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/
  99. javascript:highlandercomments.doexternallogout( 'googleplus' );
 100. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/
 101. javascript:highlandercomments.doexternallogout( 'twitter' );
 102. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/
 103. javascript:highlandercomments.doexternallogout( 'facebook' );
 104. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/
 105. javascript:highlandercomments.cancelexternalwindow();
 106. https://wordpress.com/?ref=footer_blog
 107. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/
 108. https://automattic.com/cookies
 109. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 111. https://plus.google.com/111378058760853315492
 112. https://kapilddatascience.wordpress.com/2018/06/07/solving-computer-vision-problems/
 113. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/screen-shot-2016-11-02-at-2-17-35-pm/
 114. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/screen-shot-2016-11-02-at-2-18-54-pm/
 115. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/screen-shot-2016-11-02-at-2-34-29-pm/
 116. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/screen-shot-2016-11-02-at-2-35-14-pm/
 117. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/mometum_sgd/
 118. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/rmsprop_plus_adam/
 119. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/batchnorm/
 120. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/dropout/
 121. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#comment-form-guest
 122. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#comment-form-load-service:wordpress.com
 123. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#comment-form-load-service:twitter
 124. https://kapilddatascience.wordpress.com/2016/11/06/how-cifar-10-data-set-trained-me-to-become-a-deep-learning-scientist/#comment-form-load-service:facebook
