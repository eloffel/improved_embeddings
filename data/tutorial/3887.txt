    #[1]nvidia developer blog    feed [2]nvidia developer blog    comments
   feed [3]nvidia developer blog    introduction to neural machine
   translation with gpus (part 1) comments feed [4]alternate [5]alternate

   [6]nvidia accelerated computing [7]developer
     * ____________________
       [ ] search nvidia developer
     * [8]join
     * [9]login

   ____________________
   [ ] search nvidia developer

[10]nvidia developer blog

main menu

   [11]skip to primary content
   [12]skip to secondary content
   [13]developer news
   [14]subscribe
   [15]follow us
   [16]nvidiaaidev
   [17]nvidiahpcdev
   [18]nvidiagamedev
   [19]nvidiaembedded
   [20]nvidiadrive
   [21]nvidiadesign

   (button) toggle navigation

   topics
     * [22]accelerated computing
     * [23]artificial intelligence
     * [24]autonomous vehicles
     * [25]design & visualization
     * [26]features
     * [27]game development
     * [28]robotics
     * [29]smart cities
     * [30]virtual reality

   [31]accelerated computing
   [32]artificial intelligence
   [33]autonomous vehicles
   [34]design & visualization
   [35]features
   [36]game development
   [37]robotics
   [38]smart cities
   [39]virtual reality

   [40]artificial intelligence


introduction to id4 with gpus (part 1)

   by [41]kyunghyun cho | [42]may 27, 2015
   tags: [43]deep learning, [44]machine learning, [45]machine translation,
   [46]natural language processing, [47]natural language understanding,
   [48]neural networks, [49]theano

   note: this is the first part of a detailed three-part series on machine
   translation with neural networks by kyunghyun cho. you may
   enjoy [50]part 2 and [51]part 3.

   id4 is a recently proposed framework for machine
   translation based purely on [52]neural networks. this post is the first
   of a series in which i will explain a simple encoder-decoder model for
   building a id4 system [[53]cho et al.,
   2014; [54]sutskever et al., 2014; [55]kalchbrenner and blunsom, 2013].
   in a later post i will describe how an attention mechanism can be
   incorporated into the simple encoder-decoder model [bahdanau et al.,
   2015], leading to the state-of-the-art machine translation model for a
   number of language pairs including en-fr, en-de, en-tr and en-zh
   [[56]gulcehre et al., 2015; [57]jean et al., 2015]. furthermore, i will
   introduce recent work which has applied this framework of neural
   machine translation to image and video description generation [[58]xu
   et al., 2015; [59]li et al., 2015].

id151

   first, let   s start with a brief overview of machine translation. in
   fact, the name, machine translation, says everything. we want a machine
   to translate text in one language, which we will call the source
   sentence, to corresponding text in another language, which we call
   the target sentence. (although ideally the machine should be able to
   translate a whole document from one language to another, let us
   concentrate in this blog post on sentence-level machine translation.)

   there are multiple ways to build such a machine that can translate
   languages. for instance, we can ask a bilingual speaker to give us a
   set of rules transforming a source sentence into a correct translation.
   this is not a great solution, as you can imagine, because we don   t even
   know the set of rules underlying a single language, not to mention the
   rules underlying a pair of languages. it is simply hopeless to write an
   exhaustive set of rules for translating a source sentence into a
   correct translation. hence, in this blog post, we focus on a
   statistical approach where those rules, either implicitly or
   explicitly, are automatically extracted from a large corpus of text.

   this statistical approach to machine translation is called statistical
   machine translation. the goal is the same (build a machine that
   translates a sentence from one language to another), but we let the
   machine learn from data how to translate rather than design a set of
   rules for the machine (see fig. 1 for a graphical illustration.)
   learning is based on statistical methods, which should sound familiar
   to anyone who has taken a basic course on machine learning. in fact,
   id151 is nothing but a particular application
   of machine learning, where the task is to find a function that maps
   from a source sentence to a corresponding target.
   figure 1. id151 figure 1. statistical machine
   translation

   one important characteristics of machine translation is that the target
   (translation) function is neither one-to-one nor many-to-one as in many
   other applications of machine learning (such as classification, which
   is many-to-one), but one-to-many in the sense that one source sentence
   can be translated into many possible translations. because of this, we
   model the translation function not as a deterministic function but as a
   id155 p(y|x) of a target sentence (translation) y
   given x . the id155 may apply an equally high
   id203 to more than one well-separated configurations/sentences,
   leading to a one-to-many relationship between source and target
   sentences.

   now let   s say you want to build a id151
   system that translates a source sentence in english to a sentence in
   french. the first and probably most important job is to collect pairs
   of source sentences and their corresponding translations. i will use
   x^n and y^n to represent a pair of source and corresponding
   translation, respectively. the superscript n means that it   s the n -th
   pair in a set of many more pairs (often, we need tens to hundreds of
   thousands of pairs to train a good translation model.) i   ll use
   d={(x^1,y^1), ..., (x^n, y^n)} to denote the data set with n pairs.

   where can i get these training pairs? for widely used languages in
   machine translation, you probably want to check out the [60]workshop on
   id151 or the [61]international workshop on
   spoken language translation.

   with the training data d={(x^1,y^1), ..., (x^n, y^n)} in hand, we can
   now score a model by looking at how well the model works on the
   training data d . the score, which i   ll call the log-likelihood of the
   model, is the average of the log-likelihood of the model on each pair
   (x^n, y^n) . with the probabilistic interpretation of the machine
   translation model, the log-likelihood of the model on each pair is
   simply how high a log-id203 the model assigns to the pair: \log
   p(y^n | x^n, \theta) , where \theta is a set of parameters that defines
   the model. then, the overall score of the model on the training data is

   \mathcal{l}(\theta, d) = \sum_{(x^n, y^n) \in d} \log p(y^n|x^n,
   \theta).

   if the log-likelihood \mathcal{l} is low, the model is not giving
   enough id203 mass to the correctly translated pairs, meaning that
   it   s wasting its id203 mass on some wrong translations. thus, we
   want to find a configuration of the model, or the values of the
   parameters \theta that maximizes this log-likelihood, or score.

   in machine learning, this is known as a maximum likelihood estimator.
   but we   re left with a perhaps more important question: how do we model
   p(y|x, \theta) ?

id151 (almost) from scratch

   this question of how to model the conditional distribution p(y|x)  has
   been asked and answered for a long time, starting more than 20 years
   ago at ibm t.j. watson research center [[62]brown et al., 1993 and
   references therein]. the core of research on statistical machine
   translation (smt) since then has been a [63]log-linear model, where we
   approximate the logarithm of the true  p(y|x) with a linear combination
   of many features:

   \log p(y|x) \approx \log p(y|x, \theta) = \sum_i \theta_i f_i(x, y) +
   c(\theta),

   where c is the id172 constant. in this case, a large part of
   the research comes down to finding a good set of feature functions f_i
   , and there is a very well-written textbook that covers all the details
   about it [64][koehn, 2009].

   in this approach of id151, often the only
   thing left to machine learning is to find a nice set of coefficients
   \theta_i that balance among different features, or to filter/re-rank a
   set of potential translations decoded from the log-linear model
   [[65]schwenk, 2007]. more specifically, neural networks have been used
   both as a part of the feature functions or to re-rank so-called n -best
   lists of possible translations, as in the middle and right panels of
   fig. 2.

   figure 2. graphical illustration of neural mt, smt+reranking-by-nn and
   smt-nn. from [bahadanau et al., 2015] slides at iclr 2015. figure 2.
   graphical illustration of neural mt, smt+reranking-by-nn and smt-nn.
   from [bahadanau et al., 2015] slides at iclr 2015.in this blog post, on
   the other hand, i focus on a recently proposed approach, called neural
   machine translation, where machine learning, and more specifically a
   neural network, has more or even full control, as in the left panel of
   fig. 2.

id4

   as is usual with general [66]deep learning, id4
   (id4) does not rely on pre-designed feature functions. (by pre-designed
   feature functions, i mean those that are not learned.) rather, the goal
   of id4 is to design a fully trainable model of which every component
   is tuned based on training corpora to maximize its translation
   performance.

   a fully trainable id4 model \mathcal{m} starts from as raw a
   representation of a source sentence as possible and finishes by
   generating as raw a representation of a target sentence as possible.
   here, let   s consider a sequence of words as the most raw representation
   of a sentence. (this is not true for most natural languages, but
   without loss of generality, i will consider a word the smallest unit.)
   each word in a sequence is represented by its integer index in a
   vocabulary. for instance, in the vocabulary of english sorted according
   to frequency, there will be the first word, represented as an
   integer 1. let me use x=(x_1, x_2, \dots, x_t) to denote a source
   sentence, and y=(y_1, y_2, \dots, y_{t'}) a target sentence.

   given a source sequence x=(x_1, x_2, \dots, x_t) of word indices, the
   id4 model \mathcal{m} computes the id155 of y=(y_1,
   y_2, \dots, y_{t'}) . next i   ll discuss how we can build a neural
   network to approximate this id155 p(y|x) .

recurrent neural networks

   one important property of machine translation, or any task based on
   natural languages, is that we deal with variable-length input x=(x_1,
   x_2, \dots, x_t) and output y=(y_1, y_2, \dots, y_{t'}) . in other
   words, t and t' are not fixed.

   to deal with these types of variable-length input and output, we need
   to use a [67]recurrent neural network (id56). widely used feed-forward
   neural networks, such as convolutional neural networks, do not maintain
   internal state other than the network   s own parameters. whenever a
   single sample is fed into a feed-forward neural network, the network   s
   internal state, or the activations of the hidden units, is computed
   from scratch and is not influenced by the state computed from the
   previous sample. on the other hand, an id56 maintains its internal state
   while reading a sequence of inputs, which in our case will be a
   sequence of words, thereby being able to process an input of any
   length.

   let me explain this in more detail. the main idea behind id56s is to
   compress a sequence of input symbols into a fixed-dimensional vector by
   using recursion. assume at step  t that we have a vector h_{t-1} which
   is the history of all the preceding symbols. the id56 will compute the
   new vector, or its internal state, h_t which compresses all the
   preceding symbols \left( x_1, x_2, \dots, x_{t-1} \right) as well as
   the new symbol x_t by

   h_t = \phi_{\theta}(x_t, h_{t-1}),

   where \phi_{\theta} is a function parametrized by \theta which takes as
   input the new symbol x_t and the history h_{t-1} up to the (t-1) -th
   symbol. initially, we can safely assume that h_0 is an all-zero vector.

   figure 3. graphical illustration of different types of recurrent neural
   networks. from [pascanu et al., 2014] figure 3. graphical illustration
   of different types of recurrent neural networks. from [[68]pascanu et
   al., 2014]

   the recurrent activation function \phi is often implemented as, for
   instance, a simple affine transformation followed by an element-wise
   nonlinear function:

   h_t = \tanh(w x_t + u h_{t-1} + b).

   in this formulation, the parameters include the input weight matrix w ,
   the recurrent weight matrix u and the bias vector b . i must say that
   this is not the only possibility, and there is a very large room for
   designing a novel recurrent activation function. see fig. 3 for some
   examples from [[69]pascanu et al., 2014].

   this simple type of id56 can be implemented very easily using (for
   instance) [70]theano, which allows your id56 to be run on either the cpu
   or the gpu transparently. see [71]recurrent neural networks with word
   embeddings; note that the whole id56 code is written in [72]less than 10
   lines!

   recently, it has been observed that it is better, or easier, to train a
   recurrent neural network with more sophisticated id180
   such as long short-term memory units [[73]hochreiter and schmidhuber,
   1997]  and id149 [[74]cho et al., 2014].

   r_t = \sigma(w_r x_t + u_r h_{t-1} + b_r)
   u_t = \sigma(w_u x_t + r_t \odot (u_u h_{t-1}) + b_u)
   h_t = u_t \odot h_{t-1} + (1-u_t) \odot \tanh(w x_t + r_t \odot(u_u
   h_{t-1}) + b)

   as was the case with the simple recurrent activation function, the
   parameters here include the input weight matrices w , w_r and w_u , the
   recurrent weight matrices u , u_r and u_u and the bias vectors b , b_r
   and b_u .

   although these units look much more complicated than the simple id56,
   the implementation with theano, or any other deep learning framework,
   such as torch, is just as simple. for instance, see [75]id137
   for id31 (example [76]code of an [77]lstm network).

   i have explained a recurrent neural network (id56) as a history
   compressor, but it can also be used to probabilistically model a
   sequence. here, by probabilistically modeling a sequence i mean a
   machine learning model that computes the id203 p(x) of any given
   sequence x=(x_1, x_2, \dots, x_{t}) . how can we formulate p(x) such
   that it can be written as a recurrence?

   let   s start by rewriting p(x) = p(x_1, x_2, \dots, x_{t}) into

   p(x_1, x_2, \dots, x_{t}) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2)
   \cdots p(x_t | x_1, \dots, x_{t-1}),

   which comes from the definition of id155, p(x|y) =
   \frac{p(x, y)}{p(y)}.  from this, we can make a recursive formula such
   that

   p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2) \cdots p(x_t | x_1, \dots,
   x_{t-1}) = \prod_{t=1}^t p(x_t | x_{<t})

   now, we let an id56 model p(x_t | x_{<t}) at each time t by

   p(x_t | x_{<t}) = g_{\theta}(h_{t-1})
   h_{t-1} = \phi_{\theta}(x_{t-1}, h_{t-2}).

   g_{\theta} outputs a id203 distribution conditioned on the whole
   history up to the (t-1) -th symbol via h_{t-1} . in other words, at
   each time step, the id56 tries to predict the next symbol given the
   history of the input symbols.

   there are a lot of interesting properties and characteristics of a
   recurrent neural network that i would love to spend hours talking
   about, but i will have to stop here for this blog post, since after
   all, what i have described so far are all the things you need to start
   building a id4 system. for those who are more
   interested in recurrent neural networks, i suggest you to read the
   following papers. obviously, this list is definitely not exhaustive.
   or, you can also check out [78]my slides on how to use recurrent neural
   networks for id38.
     * graves, alex.    generating sequences with recurrent neural
       networks.    arxiv preprint arxiv:1308.0850 (2013).
     * pascanu, razvan et al.    how to construct deep recurrent neural
       networks.    arxiv preprint arxiv:1312.6026 (2013).
     * boulanger-lewandowski, nicolas, yoshua bengio, and pascal vincent.
          modeling temporal dependencies in high-dimensional sequences:
       application to polyphonic music generation and
       transcription.    arxiv preprint arxiv:1206.6392 (2012).
     * mikolov, tomas et al.    recurrent neural network based language
       model.    interspeech 2010, 11th annual conference of the
       international speech communication association, makuhari, chiba,
       japan, september 26-30, 2010 1 jan. 2010: 1045-1048.
     * hochreiter, sepp, and j  rgen schmidhuber.    long short-term
       memory.    neural computation 9.8 (1997): 1735-1780.
     * cho, kyunghyun et al.    learning phrase representations using id56
       encoder-decoder for id151.    arxiv
       preprint arxiv:1406.1078 (2014).
     * bengio, yoshua, patrice simard, and paolo frasconi.    learning
       long-term dependencies with id119 is difficult.    neural
       networks, ieee transactions on 5.2 (1994): 157-166.

the story so far, and what   s next

   in this post, i introduced machine translation, and described how
   id151 approaches the problem of machine
   translation. in the framework of id151, i
   have discussed how neural networks can be used to improve the overall
   translation performance.

   the goal of this blog series is to introduce a novel paradigm for
   id4; this post laid the groundwork,
   concentrating on two key capabilities of recurrent neural networks:
   sequence summarization and probabilistic modeling of sequences.

   based on these two properties, in the next post, i will describe
   the actual id4 system based on recurrent neural
   networks. i   ll also show you why gpus are so important for neural
   machine translation! stay tuned.
   [79]8 comments
   about the authors
   kyunghyun cho
   about kyunghyun cho
   kyunghyun cho is an assistant professor in the department of computer
   science, courant institute of mathematical sciences and the center for
   data science at new york university (nyu) (starting september, 2015).
   previously, he was a postdoctoral researcher at the university of
   montreal under the supervision of prof. yoshua bengio after obtaining a
   doctorate degree at aalto university (finland) in early 2014.
   kyunghyun's main research interests include neural networks, generative
   models and their applications, especially, to language understanding.
   [80]view all posts by kyunghyun cho
   related posts
   [81]introduction to id4 with gpus (part 3)
   by [82]kyunghyun cho | [83]july 26, 2015
   [84]figure 8. image id134 with attention mechanism.
   [85]introduction to id4 with gpus (part 2)
   by [86]kyunghyun cho | [87]june 14, 2015
   [88]figure 3. step 1: a word to a one-hot vector.
   [89]deep learning in a nutshell: core concepts
   by [90]tim dettmers | [91]november 3, 2015
   [92]dl_dog_340x340
   [93]understanding natural language with deep neural networks using
   torch
   by [94]soumith chintala | [95]march 3, 2015
   [96]torch_lstm_thumb
   comments

   copyright    2019 nvidia corporation
   [97]legal information [98]privacy policy

   [99]close

parallel forall

     * [100]features

   search for: ____________________ search

accelerated computing

     * [101]accelerated computing
     * [102]downloads
     * [103]training
     * [104]ecosystem
     * [105]forums
     * [106]register now
     * [107]login

references

   visible links
   1. https://devblogs.nvidia.com/feed/
   2. https://devblogs.nvidia.com/comments/feed/
   3. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/feed/
   4. https://devblogs.nvidia.com/wp-json/oembed/1.0/embed?url=https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
   5. https://devblogs.nvidia.com/wp-json/oembed/1.0/embed?url=https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/&format=xml
   6. https://developer.nvidia.com/
   7. https://developer.nvidia.com/
   8. https://developer.nvidia.com/accelerated-computing-developer
   9. https://developer.nvidia.com/user
  10. https://devblogs.nvidia.com/
  11. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/#content
  12. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/#secondary
  13. https://news.developer.nvidia.com/
  14. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  15. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  16. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  17. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  18. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  19. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  20. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  21. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  22. https://devblogs.nvidia.com/category/accelerated-computing/
  23. https://devblogs.nvidia.com/category/artificial-intelligence/
  24. https://devblogs.nvidia.com/category/autonomous-vehicles/
  25. https://devblogs.nvidia.com/category/design-visualization/
  26. https://devblogs.nvidia.com/category/features/
  27. https://devblogs.nvidia.com/category/game-development/
  28. https://devblogs.nvidia.com/category/robotics/
  29. https://devblogs.nvidia.com/category/smart-cities/
  30. https://devblogs.nvidia.com/category/virtual-reality/
  31. https://devblogs.nvidia.com/category/accelerated-computing/
  32. https://devblogs.nvidia.com/category/artificial-intelligence/
  33. https://devblogs.nvidia.com/category/autonomous-vehicles/
  34. https://devblogs.nvidia.com/category/design-visualization/
  35. https://devblogs.nvidia.com/category/features/
  36. https://devblogs.nvidia.com/category/game-development/
  37. https://devblogs.nvidia.com/category/robotics/
  38. https://devblogs.nvidia.com/category/smart-cities/
  39. https://devblogs.nvidia.com/category/virtual-reality/
  40. https://devblogs.nvidia.com/category/artificial-intelligence/
  41. https://devblogs.nvidia.com/author/kcho/
  42. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
  43. https://devblogs.nvidia.com/tag/deep-learning/
  44. https://devblogs.nvidia.com/tag/machine-learning/
  45. https://devblogs.nvidia.com/tag/machine-translation/
  46. https://devblogs.nvidia.com/tag/natural-language-processing/
  47. https://devblogs.nvidia.com/tag/natural-language-understanding/
  48. https://devblogs.nvidia.com/tag/neural-networks/
  49. https://devblogs.nvidia.com/tag/theano/
  50. https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/
  51. https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/
  52. https://developer.nvidia.com/discover/artificial-neural-network
  53. http://arxiv.org/abs/1406.1078
  54. http://arxiv.org/abs/1409.3215
  55. https://scholar.google.com/citations?view_op=view_citation&hl=en&user=lfyg0taaaaaj&citation_for_view=lfyg0taaaaaj:d1gkvwhdpl0c
  56. http://arxiv.org/abs/1503.03535
  57. http://arxiv.org/abs/1412.2007
  58. http://arxiv.org/abs/1502.03044
  59. http://arxiv.org/abs/1502.08029
  60. http://www.statmt.org/wmt15/
  61. https://sites.google.com/site/iwsltevaluation2014/home
  62. http://www.aclweb.org/anthology/j93-2003
  63. http://en.wikipedia.org/wiki/log-linear_model
  64. http://www.statmt.org/book/
  65. http://www.google.com/url?q=http://www.sciencedirect.com/science/article/pii/s0885230806000325&sa=d&sntz=1&usg=afqjcngsoe0l0ioibqvl9tmmfrvpnozynq
  66. https://developer.nvidia.com/deep-learning
  67. https://developer.nvidia.com/discover/recurrent-neural-network
  68. http://arxiv.org/abs/1312.6026
  69. http://www.google.com/url?q=http://arxiv.org/abs/1312.6026&sa=d&sntz=1&usg=afqjcngd8qr8ysn13g29o9hoqh9tgnhtgg
  70. http://www.google.com/url?q=http://deeplearning.net/software/theano/&sa=d&sntz=1&usg=afqjcnfjobcoa4ncy2q-yzlhuw1ho-wexw
  71. http://deeplearning.net/tutorial/id56slu.html#id56slu
  72. https://github.com/mesnilgr/is13/blob/master/id56/elman.py#l38-l45
  73. http://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735?journalcode=neco&
  74. http://arxiv.org/abs/1406.1078
  75. http://deeplearning.net/tutorial/lstm.html#lstm
  76. https://github.com/kyunghyuncho/deeplearningtutorials/blob/master/code/lstm.py#l155-l201
  77. https://developer.nvidia.com/discover/lstm
  78. https://drive.google.com/file/d/0b16rwcmqqrtdnehwbhn2bxjzdxm/view
  79. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/#comments
  80. https://devblogs.nvidia.com/author/kcho/
  81. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-3/
  82. https://devblogs.nvidia.com/author/kcho/
  83. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-3/
  84. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-3/
  85. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-2/
  86. https://devblogs.nvidia.com/author/kcho/
  87. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-2/
  88. https://devblogs.nvidia.com/introduction-neural-machine-translation-gpus-part-2/
  89. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  90. https://devblogs.nvidia.com/author/tdettmers/
  91. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  92. https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/
  93. https://devblogs.nvidia.com/understanding-natural-language-deep-neural-networks-using-torch/
  94. https://devblogs.nvidia.com/author/schintala/
  95. https://devblogs.nvidia.com/understanding-natural-language-deep-neural-networks-using-torch/
  96. https://devblogs.nvidia.com/understanding-natural-language-deep-neural-networks-using-torch/
  97. http://www.nvidia.com/object/legal_info.html
  98. http://www.nvidia.com/object/privacy_policy.html
  99. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/#sidr-left
 100. https://devblogs.nvidia.com/category/features/
 101. https://developer.nvidia.com/accelerated-computing
 102. https://developer.nvidia.com/accelerated-computing-toolkit
 103. https://developer.nvidia.com/accelerated-computing-training
 104. https://developer.nvidia.com/tools-ecosystem
 105. https://devtalk.nvidia.com/
 106. https://developer.nvidia.com/accelerated-computing-developer
 107. https://developer.nvidia.com/user

   hidden links:
 109. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
 110. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
 111. https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/
 112. file://localhost/users/mharris/downloads/id4/id4.html
 113. file://localhost/users/mharris/downloads/id4/id4.html
 114. file://localhost/users/mharris/downloads/id4/id4.html
 115. file://localhost/users/mharris/downloads/id4/id4.html
