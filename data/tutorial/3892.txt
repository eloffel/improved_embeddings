   [1]cs231n convolutional neural networks for visual recognition

   table of contents:
     * [2]intro to linear classification
     * [3]linear score function
     * [4]interpreting a linear classifier
     * [5]id168
          + [6]multiclass id166
          + [7]softmax classifier
          + [8]id166 vs softmax
     * [9]interactive web demo of linear classification
     * [10]summary

linear classification

   in the last section we introduced the problem of image classification,
   which is the task of assigning a single label to an image from a fixed
   set of categories. morever, we described the k-nearest neighbor (knn)
   classifier which labels images by comparing them to (annotated) images
   from the training set. as we saw, knn has a number of disadvantages:
     * the classifier must remember all of the training data and store it
       for future comparisons with the test data. this is space
       inefficient because datasets may easily be gigabytes in size.
     * classifying a test image is expensive since it requires a
       comparison to all training images.

   overview. we are now going to develop a more powerful approach to image
   classification that we will eventually naturally extend to entire
   neural networks and convolutional neural networks. the approach will
   have two major components: a score function that maps the raw data to
   class scores, and a id168 that quantifies the agreement between
   the predicted scores and the ground truth labels. we will then cast
   this as an optimization problem in which we will minimize the loss
   function with respect to the parameters of the score function.

parameterized mapping from images to label scores

   the first component of this approach is to define the score function
   that maps the pixel values of an image to confidence scores for each
   class. we will develop the approach with a concrete example. as before,
   let   s assume a training dataset of images \( x_i \in r^d \), each
   associated with a label \( y_i \). here \( i = 1 \dots n \) and \( y_i
   \in { 1 \dots k } \). that is, we have n examples (each with a
   dimensionality d) and k distinct categories. for example, in cifar-10
   we have a training set of n = 50,000 images, each with d = 32 x 32 x 3
   = 3072 pixels, and k = 10, since there are 10 distinct classes (dog,
   cat, car, etc). we will now define the score function \(f: r^d \mapsto
   r^k\) that maps the raw image pixels to class scores.

   linear classifier. in this module we will start out with arguably the
   simplest possible function, a linear mapping:

   in the above equation, we are assuming that the image \(x_i\) has all
   of its pixels flattened out to a single column vector of shape [d x 1].
   the matrix w (of size [k x d]), and the vector b (of size [k x 1]) are
   the parameters of the function. in cifar-10, \(x_i\) contains all
   pixels in the i-th image flattened into a single [3072 x 1] column, w
   is [10 x 3072] and b is [10 x 1], so 3072 numbers come into the
   function (the raw pixel values) and 10 numbers come out (the class
   scores). the parameters in w are often called the weights, and b is
   called the bias vector because it influences the output scores, but
   without interacting with the actual data \(x_i\). however, you will
   often hear people use the terms weights and parameters interchangeably.

   there are a few things to note:
     * first, note that the single id127 \(w x_i\) is
       effectively evaluating 10 separate classifiers in parallel (one for
       each class), where each classifier is a row of w.
     * notice also that we think of the input data \( (x_i, y_i) \) as
       given and fixed, but we have control over the setting of the
       parameters w,b. our goal will be to set these in such way that the
       computed scores match the ground truth labels across the whole
       training set. we will go into much more detail about how this is
       done, but intuitively we wish that the correct class has a score
       that is higher than the scores of incorrect classes.
     * an advantage of this approach is that the training data is used to
       learn the parameters w,b, but once the learning is complete we can
       discard the entire training set and only keep the learned
       parameters. that is because a new test image can be simply
       forwarded through the function and classified based on the computed
       scores.
     * lastly, note that classifying the test image involves a single
       id127 and addition, which is significantly faster
       than comparing a test image to all training images.

     foreshadowing: convolutional neural networks will map image pixels
     to scores exactly as shown above, but the mapping ( f ) will be more
     complex and will contain more parameters.

interpreting a linear classifier

   notice that a linear classifier computes the score of a class as a
   weighted sum of all of its pixel values across all 3 of its color
   channels. depending on precisely what values we set for these weights,
   the function has the capacity to like or dislike (depending on the sign
   of each weight) certain colors at certain positions in the image. for
   instance, you can imagine that the    ship    class might be more likely if
   there is a lot of blue on the sides of an image (which could likely
   correspond to water). you might expect that the    ship    classifier would
   then have a lot of positive weights across its blue channel weights
   (presence of blue increases score of ship), and negative weights in the
   red/green channels (presence of red/green decreases the score of ship).
   [imagemap.jpg]
   an example of mapping an image to class scores. for the sake of
   visualization, we assume the image only has 4 pixels (4 monochrome
   pixels, we are not considering color channels in this example for
   brevity), and that we have 3 classes (red (cat), green (dog), blue
   (ship) class). (clarification: in particular, the colors here simply
   indicate 3 classes and are not related to the rgb channels.) we stretch
   the image pixels into a column and perform id127 to get
   the scores for each class. note that this particular set of weights w
   is not good at all: the weights assign our cat image a very low cat
   score. in particular, this set of weights seems convinced that it's
   looking at a dog.

   analogy of images as high-dimensional points. since the images are
   stretched into high-dimensional column vectors, we can interpret each
   image as a single point in this space (e.g. each image in cifar-10 is a
   point in 3072-dimensional space of 32x32x3 pixels). analogously, the
   entire dataset is a (labeled) set of points.

   since we defined the score of each class as a weighted sum of all image
   pixels, each class score is a linear function over this space. we
   cannot visualize 3072-dimensional spaces, but if we imagine squashing
   all those dimensions into only two dimensions, then we can try to
   visualize what the classifier might be doing:
   [pixelspace.jpeg]
   cartoon representation of the image space, where each image is a single
   point, and three classifiers are visualized. using the example of the
   car classifier (in red), the red line shows all points in the space
   that get a score of zero for the car class. the red arrow shows the
   direction of increase, so all points to the right of the red line have
   positive (and linearly increasing) scores, and all points to the left
   have a negative (and linearly decreasing) scores.

   as we saw above, every row of \(w\) is a classifier for one of the
   classes. the geometric interpretation of these numbers is that as we
   change one of the rows of \(w\), the corresponding line in the pixel
   space will rotate in different directions. the biases \(b\), on the
   other hand, allow our classifiers to translate the lines. in
   particular, note that without the bias terms, plugging in \( x_i = 0 \)
   would always give score of zero regardless of the weights, so all lines
   would be forced to cross the origin.

   interpretation of linear classifiers as template matching. another
   interpretation for the weights \(w\) is that each row of \(w\)
   corresponds to a template (or sometimes also called a prototype) for
   one of the classes. the score of each class for an image is then
   obtained by comparing each template with the image using an inner
   product (or dot product) one by one to find the one that    fits    best.
   with this terminology, the linear classifier is doing template
   matching, where the templates are learned. another way to think of it
   is that we are still effectively doing nearest neighbor, but instead of
   having thousands of training images we are only using a single image
   per class (although we will learn it, and it does not necessarily have
   to be one of the images in the training set), and we use the (negative)
   inner product as the distance instead of the l1 or l2 distance.
   [templates.jpg]
   skipping ahead a bit: example learned weights at the end of learning
   for cifar-10. note that, for example, the ship template contains a lot
   of blue pixels as expected. this template will therefore give a high
   score once it is matched against images of ships on the ocean with an
   inner product.

   additionally, note that the horse template seems to contain a
   two-headed horse, which is due to both left and right facing horses in
   the dataset. the linear classifier merges these two modes of horses in
   the data into a single template. similarly, the car classifier seems to
   have merged several modes into a single template which has to identify
   cars from all sides, and of all colors. in particular, this template
   ended up being red, which hints that there are more red cars in the
   cifar-10 dataset than of any other color. the linear classifier is too
   weak to properly account for different-colored cars, but as we will see
   later neural networks will allow us to perform this task. looking ahead
   a bit, a neural network will be able to develop intermediate neurons in
   its hidden layers that could detect specific car types (e.g. green car
   facing left, blue car facing front, etc.), and neurons on the next
   layer could combine these into a more accurate car score through a
   weighted sum of the individual car detectors.

   bias trick. before moving on we want to mention a common simplifying
   trick to representing the two parameters \(w,b\) as one. recall that we
   defined the score function as:

   as we proceed through the material it is a little cumbersome to keep
   track of two sets of parameters (the biases \(b\) and weights \(w\))
   separately. a commonly used trick is to combine the two sets of
   parameters into a single matrix that holds both of them by extending
   the vector \(x_i\) with one additional dimension that always holds the
   constant \(1\) - a default bias dimension. with the extra dimension,
   the new score function will simplify to a single matrix multiply:

   with our cifar-10 example, \(x_i\) is now [3073 x 1] instead of [3072 x
   1] - (with the extra dimension holding the constant 1), and \(w\) is
   now [10 x 3073] instead of [10 x 3072]. the extra column that \(w\) now
   corresponds to the bias \(b\). an illustration might help clarify:
   [wb.jpeg]
   illustration of the bias trick. doing a id127 and then
   adding a bias vector (left) is equivalent to adding a bias dimension
   with a constant of 1 to all input vectors and extending the weight
   matrix by 1 column - a bias column (right). thus, if we preprocess our
   data by appending ones to all vectors we only have to learn a single
   matrix of weights instead of two matrices that hold the weights and the
   biases.

   image id174. as a quick note, in the examples above we
   used the raw pixel values (which range from [0   255]). in machine
   learning, it is a very common practice to always perform id172
   of your input features (in the case of images, every pixel is thought
   of as a feature). in particular, it is important to center your data by
   subtracting the mean from every feature. in the case of images, this
   corresponds to computing a mean image across the training images and
   subtracting it from every image to get images where the pixels range
   from approximately [-127     127]. further common preprocessing is to
   scale each input feature so that its values range from [-1, 1]. of
   these, zero mean centering is arguably more important but we will have
   to wait for its justification until we understand the dynamics of
   id119.

id168

   in the previous section we defined a function from the pixel values to
   class scores, which was parameterized by a set of weights \(w\).
   moreover, we saw that we don   t have control over the data \( (x_i,y_i)
   \) (it is fixed and given), but we do have control over these weights
   and we want to set them so that the predicted class scores are
   consistent with the ground truth labels in the training data.

   for example, going back to the example image of a cat and its scores
   for the classes    cat   ,    dog    and    ship   , we saw that the particular set
   of weights in that example was not very good at all: we fed in the
   pixels that depict a cat but the cat score came out very low (-96.8)
   compared to the other classes (dog score 437.9 and ship score 61.95).
   we are going to measure our unhappiness with outcomes such as this one
   with a id168 (or sometimes also referred to as the cost
   function or the objective). intuitively, the loss will be high if we   re
   doing a poor job of classifying the training data, and it will be low
   if we   re doing well.

multiclass support vector machine loss

   there are several ways to define the details of the id168. as a
   first example we will first develop a commonly used loss called the
   multiclass support vector machine (id166) loss. the id166 loss is set up so
   that the id166    wants    the correct class for each image to a have a score
   higher than the incorrect classes by some fixed margin \(\delta\).
   notice that it   s sometimes helpful to anthropomorphise the loss
   functions as we did above: the id166    wants    a certain outcome in the
   sense that the outcome would yield a lower loss (which is good).

   let   s now get more precise. recall that for the i-th example we are
   given the pixels of image \( x_i \) and the label \( y_i \) that
   specifies the index of the correct class. the score function takes the
   pixels and computes the vector \( f(x_i, w) \) of class scores, which
   we will abbreviate to \(s\) (short for scores). for example, the score
   for the j-th class is the j-th element: \( s_j = f(x_i, w)_j \). the
   multiclass id166 loss for the i-th example is then formalized as follows:

   example. lets unpack this with an example to see how it works. suppose
   that we have three classes that receive the scores \( s = [13, -7,
   11]\), and that the first class is the true class (i.e. \(y_i = 0\)).
   also assume that \(\delta\) (a hyperparameter we will go into more
   detail about soon) is 10. the expression above sums over all incorrect
   classes (\(j \neq y_i\)), so we get two terms:

   you can see that the first term gives zero since [-7 - 13 + 10] gives a
   negative number, which is then thresholded to zero with the
   \(max(0,-)\) function. we get zero loss for this pair because the
   correct class score (13) was greater than the incorrect class score
   (-7) by at least the margin 10. in fact the difference was 20, which is
   much greater than 10 but the id166 only cares that the difference is at
   least 10; any additional difference above the margin is clamped at zero
   with the max operation. the second term computes [11 - 13 + 10] which
   gives 8. that is, even though the correct class had a higher score than
   the incorrect class (13 > 11), it was not greater by the desired margin
   of 10. the difference was only 2, which is why the loss comes out to 8
   (i.e. how much higher the difference would have to be to meet the
   margin). in summary, the id166 id168 wants the score of the
   correct class \(y_i\) to be larger than the incorrect class scores by
   at least by \(\delta\) (delta). if this is not the case, we will
   accumulate loss.

   note that in this particular module we are working with linear score
   functions ( \( f(x_i; w) = w x_i \) ), so we can also rewrite the loss
   function in this equivalent form:

   where \(w_j\) is the j-th row of \(w\) reshaped as a column. however,
   this will not necessarily be the case once we start to consider more
   complex forms of the score function \(f\).

   a last piece of terminology we   ll mention before we finish with this
   section is that the threshold at zero \(max(0,-)\) function is often
   called the hinge loss. you   ll sometimes hear about people instead using
   the squared hinge loss id166 (or l2-id166), which uses the form
   \(max(0,-)^2\) that penalizes violated margins more strongly
   (quadratically instead of linearly). the unsquared version is more
   standard, but in some datasets the squared hinge loss can work better.
   this can be determined during cross-validation.

     the id168 quantifies our unhappiness with predictions on the
     training set

   [margin.jpg]
   the multiclass support vector machine "wants" the score of the correct
   class to be higher than all other scores by at least a margin of delta.
   if any class has a score inside the red region (or higher), then there
   will be accumulated loss. otherwise the loss will be zero. our
   objective will be to find the weights that will simultaneously satisfy
   this constraint for all examples in the training data and give a total
   loss that is as low as possible.

   id173. there is one bug with the id168 we presented
   above. suppose that we have a dataset and a set of parameters w that
   correctly classify every example (i.e. all scores are so that all the
   margins are met, and \(l_i = 0\) for all i). the issue is that this set
   of w is not necessarily unique: there might be many similar w that
   correctly classify the examples. one easy way to see this is that if
   some parameters w correctly classify all examples (so loss is zero for
   each example), then any multiple of these parameters \( \lambda w \)
   where \( \lambda > 1 \) will also give zero loss because this
   transformation uniformly stretches all score magnitudes and hence also
   their absolute differences. for example, if the difference in scores
   between a correct class and a nearest incorrect class was 15, then
   multiplying all elements of w by 2 would make the new difference 30.

   in other words, we wish to encode some preference for a certain set of
   weights w over others to remove this ambiguity. we can do so by
   extending the id168 with a id173 penalty \(r(w)\). the
   most common id173 penalty is the l2 norm that discourages
   large weights through an elementwise quadratic penalty over all
   parameters:

   in the expression above, we are summing up all the squared elements of
   \(w\). notice that the id173 function is not a function of the
   data, it is only based on the weights. including the id173
   penalty completes the full multiclass support vector machine loss,
   which is made up of two components: the data loss (which is the average
   loss \(l_i\) over all examples) and the id173 loss. that is,
   the full multiclass id166 loss becomes:

   or expanding this out in its full form:

   where \(n\) is the number of training examples. as you can see, we
   append the id173 penalty to the loss objective, weighted by a
   hyperparameter \(\lambda\). there is no simple way of setting this
   hyperparameter and it is usually determined by cross-validation.

   in addition to the motivation we provided above there are many
   desirable properties to include the id173 penalty, many of
   which we will come back to in later sections. for example, it turns out
   that including the l2 penalty leads to the appealing max margin
   property in id166s (see [11]cs229 lecture notes for full details if you
   are interested).

   the most appealing property is that penalizing large weights tends to
   improve generalization, because it means that no input dimension can
   have a very large influence on the scores all by itself. for example,
   suppose that we have some input vector \(x = [1,1,1,1] \) and two
   weight vectors \(w_1 = [1,0,0,0]\), \(w_2 = [0.25,0.25,0.25,0.25] \).
   then \(w_1^tx = w_2^tx = 1\) so both weight vectors lead to the same
   dot product, but the l2 penalty of \(w_1\) is 1.0 while the l2 penalty
   of \(w_2\) is only 0.25. therefore, according to the l2 penalty the
   weight vector \(w_2\) would be preferred since it achieves a lower
   id173 loss. intuitively, this is because the weights in
   \(w_2\) are smaller and more diffuse. since the l2 penalty prefers
   smaller and more diffuse weight vectors, the final classifier is
   encouraged to take into account all input dimensions to small amounts
   rather than a few input dimensions and very strongly. as we will see
   later in the class, this effect can improve the generalization
   performance of the classifiers on test images and lead to less
   overfitting.

   note that biases do not have the same effect since, unlike the weights,
   they do not control the strength of influence of an input dimension.
   therefore, it is common to only regularize the weights \(w\) but not
   the biases \(b\). however, in practice this often turns out to have a
   negligible effect. lastly, note that due to the id173 penalty
   we can never achieve loss of exactly 0.0 on all examples, because this
   would only be possible in the pathological setting of \(w = 0\).

   code. here is the id168 (without id173) implemented in
   python, in both unvectorized and half-vectorized form:
def l_i(x, y, w):
  """
  unvectorized version. compute the multiclass id166 loss for a single example (x,
y)
  - x is a column vector representing an image (e.g. 3073 x 1 in cifar-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
  - y is an integer giving index of correct class (e.g. between 0 and 9 in cifar
-10)
  - w is the weight matrix (e.g. 10 x 3073 in cifar-10)
  """
  delta = 1.0 # see notes about delta later in this section
  scores = w.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct_class_score = scores[y]
  d = w.shape[0] # number of classes, e.g. 10
  loss_i = 0.0
  for j in xrange(d): # iterate over all wrong classes
    if j == y:
      # skip for the true class to only loop over incorrect classes
      continue
    # accumulate loss for the i-th example
    loss_i += max(0, scores[j] - correct_class_score + delta)
  return loss_i

def l_i_vectorized(x, y, w):
  """
  a faster half-vectorized implementation. half-vectorized
  refers to the fact that for a single example the implementation contains
  no for loops, but there is still one loop over the examples (outside this func
tion)
  """
  delta = 1.0
  scores = w.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. we want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i

def l(x, y, w):
  """
  fully-vectorized implementation :
  - x holds all the training examples as columns (e.g. 3073 x 50,000 in cifar-10
)
  - y is array of integers specifying correct class (e.g. 50,000-d array)
  - w are weights (e.g. 10 x 3073)
  """
  # evaluate loss over all examples in x without using any for loops
  # left as exercise to reader in the assignment

   the takeaway from this section is that the id166 loss takes one
   particular approach to measuring how consistent the predictions on
   training data are with the ground truth labels. additionally, making
   good predictions on the training set is equivalent to minimizing the
   loss.

     all we have to do now is to come up with a way to find the weights
     that minimize the loss.

practical considerations

   setting delta. note that we brushed over the hyperparameter \(\delta\)
   and its setting. what value should it be set to, and do we have to
   cross-validate it? it turns out that this hyperparameter can safely be
   set to \(\delta = 1.0\) in all cases. the hyperparameters \(\delta\)
   and \(\lambda\) seem like two different hyperparameters, but in fact
   they both control the same tradeoff: the tradeoff between the data loss
   and the id173 loss in the objective. the key to understanding
   this is that the magnitude of the weights \(w\) has direct effect on
   the scores (and hence also their differences): as we shrink all values
   inside \(w\) the score differences will become lower, and as we scale
   up the weights the score differences will all become higher. therefore,
   the exact value of the margin between the scores (e.g. \(\delta = 1\),
   or \(\delta = 100\)) is in some sense meaningless because the weights
   can shrink or stretch the differences arbitrarily. hence, the only real
   tradeoff is how large we allow the weights to grow (through the
   id173 strength \(\lambda\)).

   relation to binary support vector machine. you may be coming to this
   class with previous experience with binary support vector machines,
   where the loss for the i-th example can be written as:

   where \(c\) is a hyperparameter, and \(y_i \in \{ -1,1 \} \). you can
   convince yourself that the formulation we presented in this section
   contains the binary id166 as a special case when there are only two
   classes. that is, if we only had two classes then the loss reduces to
   the binary id166 shown above. also, \(c\) in this formulation and
   \(\lambda\) in our formulation control the same tradeoff and are
   related through reciprocal relation \(c \propto \frac{1}{\lambda}\).

   aside: optimization in primal. if you   re coming to this class with
   previous knowledge of id166s, you may have also heard of kernels, duals,
   the smo algorithm, etc. in this class (as is the case with neural
   networks in general) we will always work with the optimization
   objectives in their unconstrained primal form. many of these objectives
   are technically not differentiable (e.g. the max(x,y) function isn   t
   because it has a kink when x=y), but in practice this is not a problem
   and it is common to use a subgradient.

   aside: other multiclass id166 formulations. it is worth noting that the
   multiclass id166 presented in this section is one of few ways of
   formulating the id166 over multiple classes. another commonly used form
   is the one-vs-all (ova) id166 which trains an independent binary id166 for
   each class vs. all other classes. related, but less common to see in
   practice is also the all-vs-all (ava) strategy. our formulation follows
   the [12]weston and watkins 1999 (pdf) version, which is a more powerful
   version than ova (in the sense that you can construct multiclass
   datasets where this version can achieve zero data loss, but ova cannot.
   see details in the paper if interested). the last formulation you may
   see is a structured id166, which maximizes the margin between the score
   of the correct class and the score of the highest-scoring incorrect
   runner-up class. understanding the differences between these
   formulations is outside of the scope of the class. the version
   presented in these notes is a safe bet to use in practice, but the
   arguably simplest ova strategy is likely to work just as well (as also
   argued by rikin et al. 2004 in [13]in defense of one-vs-all
   classification (pdf)).

softmax classifier

   it turns out that the id166 is one of two commonly seen classifiers. the
   other popular choice is the softmax classifier, which has a different
   id168. if you   ve heard of the binary id28
   classifier before, the softmax classifier is its generalization to
   multiple classes. unlike the id166 which treats the outputs \(f(x_i,w)\)
   as (uncalibrated and possibly difficult to interpret) scores for each
   class, the softmax classifier gives a slightly more intuitive output
   (normalized class probabilities) and also has a probabilistic
   interpretation that we will describe shortly. in the softmax
   classifier, the function mapping \(f(x_i; w) = w x_i\) stays unchanged,
   but we now interpret these scores as the unnormalized log probabilities
   for each class and replace the hinge loss with a cross-id178 loss
   that has the form:

   where we are using the notation \(f_j\) to mean the j-th element of the
   vector of class scores \(f\). as before, the full loss for the dataset
   is the mean of \(l_i\) over all training examples together with a
   id173 term \(r(w)\). the function \(f_j(z) =
   \frac{e^{z_j}}{\sum_k e^{z_k}} \) is called the softmax function: it
   takes a vector of arbitrary real-valued scores (in \(z\)) and squashes
   it to a vector of values between zero and one that sum to one. the full
   cross-id178 loss that involves the softmax function might look scary
   if you   re seeing it for the first time but it is relatively easy to
   motivate.

   id205 view. the cross-id178 between a    true   
   distribution \(p\) and an estimated distribution \(q\) is defined as:

   the softmax classifier is hence minimizing the cross-id178 between
   the estimated class probabilities ( \(q = e^{f_{y_i}} / \sum_j e^{f_j}
   \) as seen above) and the    true    distribution, which in this
   interpretation is the distribution where all id203 mass is on the
   correct class (i.e. \(p = [0, \ldots 1, \ldots, 0]\) contains a single
   1 at the \(y_i\) -th position.). moreover, since the cross-id178 can
   be written in terms of id178 and the id181 as
   \(h(p,q) = h(p) + d_{kl}(p||q)\), and the id178 of the delta function
   \(p\) is zero, this is also equivalent to minimizing the kl divergence
   between the two distributions (a measure of distance). in other words,
   the cross-id178 objective wants the predicted distribution to have
   all of its mass on the correct answer.

   probabilistic interpretation. looking at the expression, we see that

   can be interpreted as the (normalized) id203 assigned to the
   correct label \(y_i\) given the image \(x_i\) and parameterized by
   \(w\). to see this, remember that the softmax classifier interprets the
   scores inside the output vector \(f\) as the unnormalized log
   probabilities. exponentiating these quantities therefore gives the
   (unnormalized) probabilities, and the division performs the
   id172 so that the probabilities sum to one. in the
   probabilistic interpretation, we are therefore minimizing the negative
   log likelihood of the correct class, which can be interpreted as
   performing id113 (id113). a nice feature of this
   view is that we can now also interpret the id173 term \(r(w)\)
   in the full id168 as coming from a gaussian prior over the
   weight matrix \(w\), where instead of id113 we are performing the maximum
   a posteriori (map) estimation. we mention these interpretations to help
   your intuitions, but the full details of this derivation are beyond the
   scope of this class.

   practical issues: numeric stability. when you   re writing code for
   computing the softmax function in practice, the intermediate terms
   \(e^{f_{y_i}}\) and \(\sum_j e^{f_j}\) may be very large due to the
   exponentials. dividing large numbers can be numerically unstable, so it
   is important to use a id172 trick. notice that if we multiply
   the top and bottom of the fraction by a constant \(c\) and push it into
   the sum, we get the following (mathematically equivalent) expression:

   we are free to choose the value of \(c\). this will not change any of
   the results, but we can use this value to improve the numerical
   stability of the computation. a common choice for \(c\) is to set
   \(\log c = -\max_j f_j \). this simply states that we should shift the
   values inside the vector \(f\) so that the highest value is zero. in
   code:
f = np.array([123, 456, 789]) # example with 3 classes and each having large sco
res
p = np.exp(f) / np.sum(np.exp(f)) # bad: numeric problem, potential blowup

# instead: first shift the values of f so that the highest number is 0:
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer


   possibly confusing naming conventions. to be precise, the id166
   classifier uses the hinge loss, or also sometimes called the max-margin
   loss. the softmax classifier uses the cross-id178 loss. the softmax
   classifier gets its name from the softmax function, which is used to
   squash the raw class scores into normalized positive values that sum to
   one, so that the cross-id178 loss can be applied. in particular, note
   that technically it doesn   t make sense to talk about the    softmax
   loss   , since softmax is just the squashing function, but it is a
   relatively commonly used shorthand.

id166 vs. softmax

   a picture might help clarify the distinction between the softmax and
   id166 classifiers:
   [id166vssoftmax.png]
   example of the difference between the id166 and softmax classifiers for
   one datapoint. in both cases we compute the same score vector f (e.g.
   by id127 in this section). the difference is in the
   interpretation of the scores in f: the id166 interprets these as class
   scores and its id168 encourages the correct class (class 2, in
   blue) to have a score higher by a margin than the other class scores.
   the softmax classifier instead interprets the scores as (unnormalized)
   log probabilities for each class and then encourages the (normalized)
   log id203 of the correct class to be high (equivalently the
   negative of it to be low). the final loss for this example is 1.58 for
   the id166 and 1.04 (note this is 1.04 using the natural logarithm, not
   base 2 or base 10) for the softmax classifier, but note that these
   numbers are not comparable; they are only meaningful in relation to
   loss computed within the same classifier and with the same data.

   softmax classifier provides    probabilities    for each class. unlike the
   id166 which computes uncalibrated and not easy to interpret scores for
   all classes, the softmax classifier allows us to compute
      probabilities    for all labels. for example, given an image the id166
   classifier might give you scores [12.5, 0.6, -23.0] for the classes
      cat   ,    dog    and    ship   . the softmax classifier can instead compute the
   probabilities of the three labels as [0.9, 0.09, 0.01], which allows
   you to interpret its confidence in each class. the reason we put the
   word    probabilities    in quotes, however, is that how peaky or diffuse
   these probabilities are depends directly on the id173 strength
   \(\lambda\) - which you are in charge of as input to the system. for
   example, suppose that the unnormalized log-probabilities for some three
   classes come out to be [1, -2, 0]. the softmax function would then
   compute:

   where the steps taken are to exponentiate and normalize to sum to one.
   now, if the id173 strength \(\lambda\) was higher, the weights
   \(w\) would be penalized more and this would lead to smaller weights.
   for example, suppose that the weights became one half smaller ([0.5,
   -1, 0]). the softmax would now compute:

   where the probabilites are now more diffuse. moreover, in the limit
   where the weights go towards tiny numbers due to very strong
   id173 strength \(\lambda\), the output probabilities would be
   near uniform. hence, the probabilities computed by the softmax
   classifier are better thought of as confidences where, similar to the
   id166, the ordering of the scores is interpretable, but the absolute
   numbers (or their differences) technically are not.

   in practice, id166 and softmax are usually comparable. the performance
   difference between the id166 and softmax are usually very small, and
   different people will have different opinions on which classifier works
   better. compared to the softmax classifier, the id166 is a more local
   objective, which could be thought of either as a bug or a feature.
   consider an example that achieves the scores [10, -2, 3] and where the
   first class is correct. an id166 (e.g. with desired margin of \(\delta =
   1\)) will see that the correct class already has a score higher than
   the margin compared to the other classes and it will compute loss of
   zero. the id166 does not care about the details of the individual scores:
   if they were instead [10, -100, -100] or [10, 9, 9] the id166 would be
   indifferent since the margin of 1 is satisfied and hence the loss is
   zero. however, these scenarios are not equivalent to a softmax
   classifier, which would accumulate a much higher loss for the scores
   [10, 9, 9] than for [10, -100, -100]. in other words, the softmax
   classifier is never fully happy with the scores it produces: the
   correct class could always have a higher id203 and the incorrect
   classes always a lower id203 and the loss would always get
   better. however, the id166 is happy once the margins are satisfied and it
   does not micromanage the exact scores beyond this constraint. this can
   intuitively be thought of as a feature: for example, a car classifier
   which is likely spending most of its    effort    on the difficult problem
   of separating cars from trucks should not be influenced by the frog
   examples, which it already assigns very low scores to, and which likely
   cluster around a completely different side of the data cloud.

interactive web demo

   [14][classifydemo.jpeg]
   we have written an interactive web demo to help your intuitions with
   linear classifiers. the demo visualizes the id168s discussed in
   this section using a toy 3-way classification on 2d data. the demo also
   jumps ahead a bit and performs the optimization, which we will discuss
   in full detail in the next section.

summary

   in summary,
     * we defined a score function from image pixels to class scores (in
       this section, a linear function that depends on weights w and
       biases b).
     * unlike knn classifier, the advantage of this parametric approach is
       that once we learn the parameters we can discard the training data.
       additionally, the prediction for a new test image is fast since it
       requires a single id127 with w, not an exhaustive
       comparison to every single training example.
     * we introduced the bias trick, which allows us to fold the bias
       vector into the weight matrix for convenience of only having to
       keep track of one parameter matrix.
     * we defined a id168 (we introduced two commonly used losses
       for linear classifiers: the id166 and the softmax) that measures how
       compatible a given set of parameters is with respect to the ground
       truth labels in the training dataset. we also saw that the loss
       function was defined in such way that making good predictions on
       the training data is equivalent to having a small loss.

   we now saw one way to take a dataset of images and map each one to
   class scores based on a set of parameters, and we saw two examples of
   id168s that we can use to measure the quality of the
   predictions. but how do we efficiently determine the parameters that
   give the best (lowest) loss? this process is optimization, and it is
   the topic of the next section.

further reading

   these readings are optional and contain pointers of interest.
     * [15]deep learning using linear support vector machines from charlie
       tang 2013 presents some results claiming that the l2id166 outperforms
       softmax.

     * [16]cs231n
     * [17]cs231n
     * [18]karpathy@cs.stanford.edu

references

   1. http://cs231n.github.io/
   2. http://cs231n.github.io/linear-classify/#intro
   3. http://cs231n.github.io/linear-classify/#score
   4. http://cs231n.github.io/linear-classify/#interpret
   5. http://cs231n.github.io/linear-classify/#loss
   6. http://cs231n.github.io/linear-classify/#id166
   7. http://cs231n.github.io/linear-classify/#softmax
   8. http://cs231n.github.io/linear-classify/#id166vssoftmax
   9. http://cs231n.github.io/linear-classify/#webdemo
  10. http://cs231n.github.io/linear-classify/#summary
  11. http://cs229.stanford.edu/notes/cs229-notes3.pdf
  12. https://www.elen.ucl.ac.be/proceedings/esann/esannpdf/es1999-461.pdf
  13. http://www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf
  14. http://vision.stanford.edu/teaching/cs231n/linear-classify-demo
  15. http://arxiv.org/abs/1306.0239
  16. https://github.com/cs231n
  17. https://twitter.com/cs231n
  18. mailto:karpathy@cs.stanford.edu
