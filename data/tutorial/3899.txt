linear classification loss visualization

   these linear classifiers were written in javascript for stanford's
   [1]cs231n: convolutional neural networks for visual recognition.
   the class scores for linear classifiers are computed as \( f(x_i; w, b)
   = w x_i + b \), where the parameters consist of weights \(w\) and
   biases \(b\). the training data is \(x_i\) with labels \(y_i\). in this
   demo, the datapoints \(x_i\) are 2-dimensional and there are 3 classes,
   so the weight matrix is of size [3 x 2] and the bias vector is of size
   [3 x 1]. the multiclass id168 can be formulated in many ways.
   the default in this demo is an id166 that follows [weston and watkins
   1999]. denoting \( f \) as the [3 x 1] vector that holds the class
   scores, the loss has the form: $$ l = \underbrace{ \frac{1}{n} \sum_i
   \sum_{j \neq y_i} \max(0, f_j - f_{y_i} + 1)}_{\text{data loss}} +
   \lambda \underbrace{\sum_k\sum_l w_{k,l}^2 }_{\text{id173
   loss}} $$ where\( n \) is the number of examples, and \(\lambda\) is a
   hyperparameter that controls the strength of the l2 id173
   penalty \(r(w) = \sum_k\sum_l w_{k,l}^2\). on the bottom right of this
   demo you can also flip to different formulations for the multiclass id166
   including one vs all (ova) where a separate binary id166 is trained for
   every class independently (vs. other classes all labeled as negatives),
   and structured id166 which maximizes the margin between the correct score
   and the score of the highest runner-up class. you can also choose to
   use the cross-id178 loss which is used by the softmax classifier.
   these loses are explained the [2]cs231n notes on linear classification.
   datapoints are shown as circles colored by their class (red/gree/blue).
   the background regions are colored by whichever class is most likely at
   any point according to the current weights. each classifier is
   visualized by a line that indicates its zero score level set. for
   example, the blue classifier computes scores as \(w_{0,0} x_0 + w_{0,1}
   x_1 + b_0\) and the blue line shows the set of points \((x_0, x_1)\)
   that give score of zero. the blue arrow draws the vector \((w_{0,0},
   w_{0,1})\), which shows the direction of score increase and its length
   is proportional to how steep the increase is.
   note: you can drag the datapoints.
   parameters \(w,b\) are shown below. the value is in bold and its
   gradient (computed with backprop) is in red, italic below. click the
   triangles to control the parameters.
   visualization of the data loss computation. each row is loss due to one
   datapoint. the first three columns are the 2d data \(x_i\) and the
   label \(y_i\). the next three columns are the three class scores from
   each classifier \( f(x_i; w, b) = w x_i + b \) (e.g. s[0] = x[0] *
   w[0,0] + x[1] * w[0,1] + b[0]). the last column is the data loss for a
   single example, \(l_i\).
   l2 id173 strength:
     __________________________________________________________________

   step size:
   (button) single parameter update
   (button) start repeated update
   (button) stop repeated update
   (button) randomize parameters
   multiclass id166 loss formulation:
   (*) weston watkins 1999
   ( ) one vs. all
   ( ) structured id166
   ( ) softmax

references

   1. http://vision.stanford.edu/teaching/cs231n/
   2. http://cs231n.github.io/linear-classify/
