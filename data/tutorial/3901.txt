   in [1]:
# a bit of setup
import numpy as np
import matplotlib.pyplot as plt

%matplotlib inline
plt.rcparams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcparams['image.interpolation'] = 'nearest'
plt.rcparams['image.cmap'] = 'gray'

# for auto-reloading extenrnal modules
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipytho
n
%load_ext autoreload
%autoreload 2

   in [323]:
np.random.seed(0)
n = 100 # number of points per class
d = 2 # dimensionality
k = 3 # number of classes
x = np.zeros((n*k,d))
y = np.zeros(n*k, dtype='uint8')
for j in xrange(k):
  ix = range(n*j,n*(j+1))
  r = np.linspace(0.0,1,n) # radius
  t = np.linspace(j*4,(j+1)*4,n) + np.random.randn(n)*0.2 # theta
  x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
  y[ix] = j
fig = plt.figure()
plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap=plt.cm.spectral)
plt.xlim([-1,1])
plt.ylim([-1,1])
#fig.savefig('spiral_raw.png')

   [n5lk31eqvub6xugsajggndkiun5ez6aqyblyil0ofkykjnahy2
   pzt6cegivmwscd8ecit83vukokoqqgghhbuuh+ffiyqqqohst5iuiyqqqggrkkrlid35eemi
   kjoks
   qgghhlacsbqeeeiiiaxaki4hhbbcccuqpesiiyqqwgr+hyq9ep151pvwaaaaaelftksuqmc
   c ]
   in [329]:
#train a linear classifier

# initialize parameters randomly
w = 0.01 * np.random.randn(d,k)
b = np.zeros((1,k))

# some hyperparameters
step_size = 1e-0
reg = 1e-3 # id173 strength

# id119 loop
num_examples = x.shape[0]
for i in xrange(200):

  # evaluate class scores, [n x k]
  scores = np.dot(x, w) + b

  # compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true) # [n x k]

  # compute the loss: average cross-id178 loss and id173
  corect_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(corect_logprobs)/num_examples
  reg_loss = 0.5*reg*np.sum(w*w)
  loss = data_loss + reg_loss
  if i % 10 == 0:
    print "iteration %d: loss %f" % (i, loss)

  # compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples

  # backpropate the gradient to the parameters (w,b)
  dw = np.dot(x.t, dscores)
  db = np.sum(dscores, axis=0, keepdims=true)

  dw += reg*w # id173 gradient

  # perform a parameter update
  w += -step_size * dw
  b += -step_size * db

iteration 0: loss 1.096956
iteration 10: loss 0.917265
iteration 20: loss 0.851503
iteration 30: loss 0.822336
iteration 40: loss 0.807586
iteration 50: loss 0.799448
iteration 60: loss 0.794681
iteration 70: loss 0.791764
iteration 80: loss 0.789920
iteration 90: loss 0.788726
iteration 100: loss 0.787938
iteration 110: loss 0.787409
iteration 120: loss 0.787049
iteration 130: loss 0.786803
iteration 140: loss 0.786633
iteration 150: loss 0.786514
iteration 160: loss 0.786431
iteration 170: loss 0.786373
iteration 180: loss 0.786331
iteration 190: loss 0.786302


   in [330]:
# evaluate training set accuracy
scores = np.dot(x, w) + b
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))

training accuracy: 0.49


   in [331]:
# plot the resulting classifier
h = 0.02
x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
z = np.dot(np.c_[xx.ravel(), yy.ravel()], w) + b
z = np.argmax(z, axis=1)
z = z.reshape(xx.shape)
fig = plt.figure()
plt.contourf(xx, yy, z, cmap=plt.cm.spectral, alpha=0.8)
plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap=plt.cm.spectral)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
#fig.savefig('spiral_linear.png')

   [kwla
   0vm2d0hakekmifo5pluovpma6orzgqaaablhkraaacajnfyaaaazobecaadici0vaabarmi
   saaaa mkjjbqaakbeakwaagiz8a3azvnnlf9oiaaaaaelftksuqmcc ]
   in [332]:
# initialize parameters randomly
h = 100 # size of hidden layer
w = 0.01 * np.random.randn(d,h)
b = np.zeros((1,h))
w2 = 0.01 * np.random.randn(h,k)
b2 = np.zeros((1,k))

# some hyperparameters
step_size = 1e-0
reg = 1e-3 # id173 strength

# id119 loop
num_examples = x.shape[0]
for i in xrange(10000):

  # evaluate class scores, [n x k]
  hidden_layer = np.maximum(0, np.dot(x, w) + b) # note, relu activation
  scores = np.dot(hidden_layer, w2) + b2

  # compute the class probabilities
  exp_scores = np.exp(scores)
  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=true) # [n x k]

  # compute the loss: average cross-id178 loss and id173
  corect_logprobs = -np.log(probs[range(num_examples),y])
  data_loss = np.sum(corect_logprobs)/num_examples
  reg_loss = 0.5*reg*np.sum(w*w) + 0.5*reg*np.sum(w2*w2)
  loss = data_loss + reg_loss
  if i % 1000 == 0:
    print "iteration %d: loss %f" % (i, loss)

  # compute the gradient on scores
  dscores = probs
  dscores[range(num_examples),y] -= 1
  dscores /= num_examples

  # backpropate the gradient to the parameters
  # first backprop into parameters w2 and b2
  dw2 = np.dot(hidden_layer.t, dscores)
  db2 = np.sum(dscores, axis=0, keepdims=true)
  # next backprop into hidden layer
  dhidden = np.dot(dscores, w2.t)
  # backprop the relu non-linearity
  dhidden[hidden_layer <= 0] = 0
  # finally into w,b
  dw = np.dot(x.t, dhidden)
  db = np.sum(dhidden, axis=0, keepdims=true)

  # add id173 gradient contribution
  dw2 += reg * w2
  dw += reg * w

  # perform a parameter update
  w += -step_size * dw
  b += -step_size * db
  w2 += -step_size * dw2
  b2 += -step_size * db2

iteration 0: loss 1.098744
iteration 1000: loss 0.294946
iteration 2000: loss 0.259301
iteration 3000: loss 0.248310
iteration 4000: loss 0.246170
iteration 5000: loss 0.245649
iteration 6000: loss 0.245491
iteration 7000: loss 0.245400
iteration 8000: loss 0.245335
iteration 9000: loss 0.245292


   in [333]:
# evaluate training set accuracy
hidden_layer = np.maximum(0, np.dot(x, w) + b)
scores = np.dot(hidden_layer, w2) + b2
predicted_class = np.argmax(scores, axis=1)
print 'training accuracy: %.2f' % (np.mean(predicted_class == y))

training accuracy: 0.98


   in [336]:
# plot the resulting classifier
h = 0.02
x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1
y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))
z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], w) + b), w2) + b2
z = np.argmax(z, axis=1)
z = z.reshape(xx.shape)
fig = plt.figure()
plt.contourf(xx, yy, z, cmap=plt.cm.spectral, alpha=0.8)
plt.scatter(x[:, 0], x[:, 1], c=y, s=40, cmap=plt.cm.spectral)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
#fig.savefig('spiral_net.png')

   [wmqjz9sftdq9aaaaabjru5erkjggg== ]
   in []:
