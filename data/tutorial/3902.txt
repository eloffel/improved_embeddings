   #[1]rss [2]slideshare search [3]alternate [4]alternate [5]alternate
   [6]alternate [7]alternate [8]alternate [9]slideshow json oembed profile
   [10]slideshow xml oembed profile [11]alternate [12]alternate
   [13]alternate

   (button)

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our [14]user
   agreement and [15]privacy policy.

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our
   [16]privacy policy and [17]user agreement for details.

   [18]slideshare [19]explore search [20]you

     * [21]linkedin slideshare

     * [22]upload
     * [23]login
     * [24]signup

     *
     * ____________________ (button) submit search

     * [25]home
     * [26]explore

     * [27]presentation courses
     * [28]powerpoint courses
     *
     * by [29]linkedin learning

   ____________________
   successfully reported this slideshow.

   we use your linkedin profile and activity data to personalize ads and
   to show you more relevant ads. [30]you can change your ad preferences
   anytime.
   from natural language processing to artificial intelligence

   from natural language processing to artificial intelligence jonathan
   mugan, phd @jmugan data day texas january 14, 2017

       it is not my aim to surprise or shock you     but the simplest way i
   can summarize is to say that there are now in the wor...

       it is not my aim to surprise or shock you     but the simplest way i
   can summarize is to say that there are now in the wor...

       it is not my aim to surprise or shock you     but the simplest way i
   can summarize is to say that there are now in the wor...

   this is disappointing because we want to     interact with our world
   using natural language     current chatbots are an embarr...

   to understand language, computers need to understand the world why can
   you pull a wagon with a string but not push it? -- ...

   grounded understanding     we understand language in a way that is
   grounded in sensation and action. sensation representatio...

   sensation representation action two paths to meaning:

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   bag-of-words representation    the aardvark ate the zoo.    = [1,0,1, ...,
   0, 1] we can do a little better and count how often...

   give rare words a boost we can get fancier and say that rare words are
   more important than common words for characterizing...

   id96 (lda) id44     you pick the number
   of topics     each topic is a distribution over words...

   lda of my tweets shown in pyldavis i sometimes tweet about movies.
   https://github.com/bmabey/pyldavis

   id31: how the author feels about the text sentiment
   dictionary: word list with sentiment values associated w...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   manually constructing representations we tell the computer what things
   mean by manually specifying relationships between s...

   manually constructing representations vehicle honda civic tires is_ahas
   who might be in the market for tires? allows us to...

   manually constructing representations vehicle    ... the car ...       ... an
   automobile ...       ... my truck ...    honda civic    .....

   id138 auto, automobile, motorcar id138 search
   http://id138web.princeton.edu/perl/webwn ambulance hyponym
   (subordinat...

   id138 auto, automobile, machine, motorcar id138 search
   http://id138web.princeton.edu/perl/webwn ambulance hyponym (s...

   framenet     more integrative than id138: represents situations     one
   example is a child   s birthday party, another is comm...

   conceptnet bread toasteratlocation automobile relatedto     provides
   commonsense linkages between words     my experience: too...

   yago: yet another great ontology     built on id138 and dbpedia    
   http://www.mpi-inf.mpg.de/departments/databases-and- inf...

   sumo: suggested upper merged ontology there is also yago-sumo that
   merges the low- level organization of sumo with the ins...

   image schemas humans use image schemas comprehend spatial arrangements
   and movements in space [mandler, 2004] examples of ...

   semantic web (linked data)     broad, but not organized or deep     way too
   complicated     may eventually be streamlined (e.g. ...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   world models computers need causal models of how the world works and
   how we interact with it.     people don   t say everythin...

   dimensions of models     probabilistic     deterministic compared with
   stochastic     e.g., logic compared with probabilistic pr...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   merge representations with models explain your conception of
   conductivity? electrons are small spheres, and electricity is...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   sensation representation action meaningless tokens manual
   representations world models merged representations symbolic pat...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   id97 the id97 model learns a vector for each word in the
   vocabulary. the number of dimensions for each word vector...

   id97 1. initialize each word with a random vector 2. for each word
   w1 in the set of documents: 3. for each word w2 aro...

   id97 meaning    you shall know a word by the company it keeps.    j. r.
   firth [1957] the quote we often see: this seems at...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   id195 model the id195 (sequence-to-sequence) model can encode
   sequences of tokens, such as sentences, into single vect...

   encoding sentence meaning into a vector h0 the    the patient fell.   

   encoding sentence meaning into a vector h0 the h1 patient    the patient
   fell.   

   encoding sentence meaning into a vector h0 the h1 patient h2 fell    the
   patient fell.   

   encoding sentence meaning into a vector like a hidden markov model, but
   doesn   t make the markov assumption and benefits fr...

   decoding sentence meaning el h3 machine translation, or structure
   learning more generally.

   decoding sentence meaning el h3 h4 machine translation, or structure
   learning more generally.

   decoding sentence meaning el h3 paciente h4 machine translation, or
   structure learning more generally.

   decoding sentence meaning machine translation, or structure learning
   more generally. el h3 paciente h4 cay   h5 . h5 [cho e...

   generating image captions convolutional neural network sad h0 wet h1
   kid h2 . h3 [karpathy and fei-fei, 2015] [vinyals et ...

   attention [bahdanau et al., 2014] el h3 paciente h4 cay   h5 . h5 h0 the
   h1 patient h2 fell h3 .

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   deep learning and id53 id56s answer questions. what is the
   translation of this phrase to french? what is the ...

   deep learning and id53 bob went home. tim went to the
   junkyard. bob picked up the jar. bob went to town. whe...

   deep learning and id53 the network is learning linkages
   between sequences of symbols, but these kinds of sto...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   external world training if we want to talk to machines 1. we need to
   train them in an environment as much like our own as ...

   there has been work in this direction industry     openai     universe:
   train on screens with vnc     now with grand theft auto!...

   but we need more training centered in our world maybe if amazon alexa
   had a camera and rotating head? how far could we get...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   sensation representation action meaningless tokens manual
   representations world models merged representations id97 seq...

   sensation representation action symbolic path sub-symbolic path
   building world models based on large, deep, and organized ...

   sensation representation action final thought open problem: what is the
   simplest commercially viable task that requires co...

   thanks for listening jonathan mugan @jmugan www.deepgrammar.com
   upcoming slideshare
   []
   loading in    5
     
   [] 1
   (button)
   1 of 64 (button)
   (button) (button)
   like this presentation? why not share!
     * share
     * email
     *
     *

     * [31]natural language processing in arti... natural language
       processing in arti... by artivatic.ai 11484 views

   (button)

   share slideshare
     __________________________________________________________________

     * [32]facebook
     * [33]twitter
     * [34]linkedin

   embed
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   size (px)
   start on
   [x] show related slideshares at end
   wordpress shortcode ____________________
   link ____________________

from natural language processing to artificial intelligence

   4,340 views

     * (button) share
     * (button) like
     * (button) download
     * ...
          +

   [35]jonathan mugan

[36]jonathan mugan

   , computer scientist: ai and machine learning
   [37]follow

   (button) (button) (button)

   published on jan 14, 2017

   overview of natural language processing (nlp) from both symbolic and
   deep learning perspectives. covers tf-idf, id31, lda,
   id138, framenet, id97, and recurrent neural networks (id56s).
   (button) ...

   published in: [38]data & analytics

     * [39]2 comments
     * [40]13 likes
     * [41]statistics
     * [42]notes

     * full name
       full name
       comment goes here.
       12 hours ago   [43]delete [44]reply [45]block
       are you sure you want to [46]yes [47]no
       your message goes here

   no profile picture user
   ____________________
   [48](button) post
     * [49]johnmayer664
       [50]johnmayer664
       hi there! essay help for students | discount 10% for your first
       order! - check our website! https://vk.cc/80sako
       11 months ago    [51]reply
       are you sure you want to  [52]yes  [53]no
       your message goes here
     * [54]dwolfson04
       [55]dwolfson04
       i thought this was a great survey that put many of the technologies
       into a very useful framework!
       2 years ago    [56]reply
       are you sure you want to  [57]yes  [58]no
       your message goes here

     * [59]anupamkundu77
       [60]anupam kundu , global business strategist
       1 year ago
     * [61]chienjchienj
       [62]arthur chien , software research engineer     asus at asus
       1 year ago
     * [63]fairouzattiya
       [64]fairouz attiya , assistant professor at al emam islamic
       university at al emam islamic university
       2 years ago
     * [65]linekin
       [66]jia-yin lin , programmer analyst at galaxy software services at
       galaxy software services
       2 years ago
     * [67]chernushenko
       [68]yury chernushenko , data engineer at medigo gmbh at medigo gmbh
       2 years ago

   [69]show more
   no downloads
   views
   total views
   4,340
   on slideshare
   0
   from embeds
   0
   number of embeds
   135
   actions
   shares
   0
   downloads
   118
   comments
   2
   likes
   13
   embeds 0
   no embeds
   no notes for slide







     ai has gotten a lot smarter, in recent years, especially with the
   benefits of deep learning, but natural language understanding is still
   lacking.



     computers must understand our world to understand language.

     we don   t understand our biological structure, so we can   t just copy
   it in software, and we don   t know how to implement an alternative
   structure with equivalent capabilities, so we employ a bunch of parlor
   tricks to get us close to what we want. this is the field of natural
   language processing.

     for sub-symbolic pdp original book and pep at 25.

     for sub-symbolic pep original book and pep at 25.

     for sub-symbolic pep original book and pep at 25.





     there is some indirect meaning based on how people use symbols
   together.
   in real writing you have a thesis, a central thing to to say. a
   predicate.

     there is some indirect meaning based on how people use symbols
   together.
   in real writing you have a thesis, a central thing to to say. a
   predicate.



     for sub-symbolic pep original book and pep at 25.







     a set of meanings where each one is a set of sense entities called
   synsets.

     a set of meanings where each one is a set of sense entities called
   synsets.

     frames are more integrative; they represent more kinds of
   relationships between concepts, and those relationships are situation
   specific, so in a sense the representation is richer. however,
   commerce_buy doesn   t say that one person owns something and
   now has less money (confirm this).











     for sub-symbolic pdp original book and pep at 25.

     should i say they are causal models? we want causal because we want
   to know how the system would behave in new situations. causality is
   what happens if there is an exogenous change.
   compare
   i flipped the light switch. the light will go on.
   2. i made the rooster crow. the sun will not rise.



     for sub-symbolic pep original book and pep at 25.



     for sub-symbolic pdp original book and pdp at 25.











     should    happy    and    sad    have similar vectors when they are, in a
   sense, opposite?

from natural language processing to artificial intelligence

    1. 1. from natural language processing to artificial intelligence
       jonathan mugan, phd @jmugan data day texas january 14, 2017
    2. [70]2.     it is not my aim to surprise or shock you     but the
       simplest way i can summarize is to say that there are now in the
       world machines that think, that learn and that create. moreover,
       their ability to do these things is going to increase rapidly until
           in a visible future     the range of problems they can handle will
       be coextensive with the range to which the human mind has been
       applied.    
    3. [71]3.     it is not my aim to surprise or shock you     but the
       simplest way i can summarize is to say that there are now in the
       world machines that think, that learn and that create. moreover,
       their ability to do these things is going to increase rapidly until
           in a visible future     the range of problems they can handle will
       be coextensive with the range to which the human mind has been
       applied.     herbert simon, 1957
    4. [72]4.     it is not my aim to surprise or shock you     but the
       simplest way i can summarize is to say that there are now in the
       world machines that think, that learn and that create. moreover,
       their ability to do these things is going to increase rapidly until
           in a visible future     the range of problems they can handle will
       be coextensive with the range to which the human mind has been
       applied.     herbert simon, 1957 so what is going on here? there is
       no actual understanding. ai has gotten smarter     especially with
       deep learning, but     computers can   t read or converse intelligently
    5. [73]5. this is disappointing because we want to     interact with our
       world using natural language     current chatbots are an
       embarrassment     have computers read all those documents out there    
       so they can retrieve the best ones, answer our questions, and
       summarize what is new
    6. [74]6. to understand language, computers need to understand the
       world why can you pull a wagon with a string but not push it? --
       minsky why is it unusual that a gymnast competed with one leg? --
       schank why does it only rain outside? if a book is on a table, and
       you push the table, what happens? if bob went to the junkyard, is
       he at the airport? they need to be able to answer questions like:
    7. [75]7. grounded understanding     we understand language in a way
       that is grounded in sensation and action. sensation representation
       action     when someone says    chicken,    we map that to our experience
       with chickens.     we understand each other because we have had
       similar experiences.     this is the kind of understanding that
       computers need. see: benjamin bergen, steven pinker, mark johnson,
       jerome feldman, and murray shanahan    chicken   
    8. [76]8. sensation representation action two paths to meaning:
    9. [77]9. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   10. [78]10. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   11. [79]11. bag-of-words representation    the aardvark ate the zoo.    =
       [1,0,1, ..., 0, 1] we can do a little better and count how often
       the words occur. tf: term frequency, how often does the word occur.
          the aardvark ate the aardvark by the zoo.    = [2,0,1, ..., 0, 1]
       treat words as arbitrary symbols and look at their frequencies.
          dog bit man    will be the same as    man bit dog    consider a
       vocabulary of 50,000 words where:        aardvark    is position 0    
          ate    is position 2        zoo    is position 49,999 a bag-of-words can
       be a vector with 50,000 dimensions.
   12. [80]12. give rare words a boost we can get fancier and say that
       rare words are more important than common words for characterizing
       documents. multiply each entry by a measure of how common it is in
       the corpus. idf: inverse document frequency idf( term, document) =
       log(num. documents / num. with term) called a vector space model.
       you can throw these vectors into any classifier, or find similar
       documents based on similar vectors. 10 documents, only 1 has
          aardvark    and 5 have    zoo    and 5 have    ate    tf-idf: tf * idf    the
       aardvark ate the aardvark by the zoo.    =[4.6,0,0.7, ..., 0, 0.7]
   13. [81]13. id96 (lda) id44     you pick
       the number of topics     each topic is a distribution over words    
       each document is a distribution over topics easy to do in gensim
       https://radimrehurek.com/gensim/models/ldamodel.html
   14. [82]14. lda of my tweets shown in pyldavis i sometimes tweet about
       movies. https://github.com/bmabey/pyldavis
   15. [83]15. id31: how the author feels about the text
       sentiment dictionary: word list with sentiment values associated
       with all words that might indicate sentiment ... happy: +2 ...
       joyful: +2 ... pain: -3 painful: -3 ... we can do sentiment
       analysis using labeled data and meaningless tokens, with supervised
       learning over tf-idf. we can also do id31 by adding
       the first hint of meaning: some words are positive and some words
       are negative. one such word list is vader
       https://github.com/cjhutto/vadersentiment    i went to the junkyard
       and was happy to see joyful people.   
   16. [84]16. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   17. [85]17. manually constructing representations we tell the computer
       what things mean by manually specifying relationships between
       symbols 1. stores meaning using predefined relationships 2. maps
       multiple ways of writing something to the same representation
       allows us to code what the machine should do for a relatively small
       number of representations
   18. [86]18. manually constructing representations vehicle honda civic
       tires is_ahas who might be in the market for tires? allows us to
       code what the machine should do for a relatively small number of
       representations we tell the computer what things mean by manually
       specifying relationships between symbols 1. stores meaning using
       predefined relationships 2. maps multiple ways of writing something
       to the same representation
   19. [87]19. manually constructing representations vehicle    ... the car
       ...       ... an automobile ...       ... my truck ...    honda civic    ...
       cruising in my civic ...    tires is_ahas who might be in the market
       for tires? allows us to code what the machine should do for a
       relatively small number of representations we tell the computer
       what things mean by manually specifying relationships between
       symbols 1. stores meaning using predefined relationships 2. maps
       multiple ways of writing something to the same representation
   20. [88]20. id138 auto, automobile, motorcar id138 search
       http://id138web.princeton.edu/perl/webwn ambulance hyponym
       (subordinate) sports_car, sport_car hyponym (subordinate)
       motor_vehicle, automotive_vechicle hypernym (superordinate)
       automobile_horn, car_horn, motor_horn, horn, hooter meronym
       (has-part)     organizes sets of words into synsets, which are
       meanings     a word can be a member of more than one synset, such as
          bank    [synsets shown as boxes]
   21. [89]21. id138 auto, automobile, machine, motorcar id138 search
       http://id138web.princeton.edu/perl/webwn ambulance hyponym
       (subordinate) sports_car, sport_car hyponym (subordinate)
       motor_vehicle, automotive_vechicle hypernym (superordinate)
       automobile_horn, car_horn, motor_horn, horn, hooter meronym
       (has-part) other relations:     has-instance     car : honda civic    
       has-member     team : stringer (like    first stringer   )     antonym    
       ... a few more     organizes sets of words into synsets, which are
       meanings     a word can be a member of more than one synset, such as
          bank   
   22. [90]22. framenet     more integrative than id138: represents
       situations     one example is a child   s birthday party, another is
       commerce_buy     situations have slots (roles) that are filled    
       frames are triggered by keywords in text (more or less) framenet:
       https://framenet.icsi.berkeley.edu/fndrupal/intropage commerce_buy:
       https://framenet2.icsi.berkeley.edu/fnreports/data/frameindex.xml?f
       rame=commerce_buy commerce_buy triggered by the words: buy, buyer,
       client, purchase, or purchaser roles: buyer, goods, money, place
       (where bought), ... commerce_buy getting inherits from commerce_buy
       indicates a change of possession, but we need a world model to
       actually change a state.
   23. [91]23. conceptnet bread toasteratlocation automobile relatedto    
       provides commonsense linkages between words     my experience: too
       shallow and haphazard to be useful might be good for a conversation
       bot:    you know what you normally find by a toaster? bread.   
       conceptnet: http://conceptnet.io/
   24. [92]24. yago: yet another great ontology     built on id138 and
       dbpedia     http://www.mpi-inf.mpg.de/departments/databases-and-
       information-systems/research/yago-naga/yago/     dbpedia has a
       machine readable page for each wikipedia page     used by ibm watson
       to play jeopardy!     big on named entities, like entertainers    
       browse     https://gate.d5.mpi-inf.mpg.de/webyago3spotlx/browser
   25. [93]25. sumo: suggested upper merged ontology there is also
       yago-sumo that merges the low- level organization of sumo with the
       instance information of yago.http://people.mpi-
       inf.mpg.de/~gdemelo/yagosumo/ deep: organizes concepts down to the
       lowest level http://www.adampease.org/op/ example: cooking is a
       type of making that is a type of intentional process that is a type
       of process that is a physical thing that is an entity.
   26. [94]26. image schemas humans use image schemas comprehend spatial
       arrangements and movements in space [mandler, 2004] examples of
       image schemas include path, containment, blockage, and attraction
       [johnson, 1987] abstract concepts such as romantic relationships
       and arguments are represented as metaphors to this kind of
       experience [lakoff and johnson, 1980] image schemas are
       representations of human experience that are common across cultures
       [feldman, 2006]
   27. [95]27. semantic web (linked data)     broad, but not organized or
       deep     way too complicated     may eventually be streamlined (e.g.
       json-ld), and it could be very cool if it gets linked with deeper,
       better organized data     tools to map text:     fred     dbpedia
       spotlight     pikes semantic web layer cake spaghetti monster
   28. [96]28. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   29. [97]29. world models computers need causal models of how the world
       works and how we interact with it.     people don   t say everything to
       get a message across, just what is not covered by our shared
       conception     most efficient way to encode our shared conception is
       a model models express how the world changes based on events    
       recall the commerce_buy frame     afterward, one person has more
       money and another person has less     read such id136s right off
       the model
   30. [98]30. dimensions of models     probabilistic     deterministic
       compared with stochastic     e.g., logic compared with probabilistic
       programming     factor state     whole states compared with using
       variables     e.g., finite automata compared with dynamic bayesian
       networks     relational     id118 compared with
       id85     e.g., id110s compared with markov
       logic networks     concurrent     model one thing compared with
       multiple things     e.g., finite automata compared with petri nets    
       temporal     static compared with dynamic     e.g., id110s
       compared with dynamic id110s
   31. [99]31. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   32. [100]32. merge representations with models explain your conception
       of conductivity? electrons are small spheres, and electricity is
       small spheres going through a tube. conductivity is how little
       blockage is in the tube. cyc has a model that uses representations,
       but it is not clear if logic is sufficiently supple. why does it
       only rain outside? a roof blocks the path of things from above. the
       meaning of the word    chicken    is everything explicitly stated in
       the representation and everything that can be inferred from the
       world model.    chicken    the final step on this path is to create a
       robust model around rich representations.
   33. [101]33. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   34. [102]34. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   35. [103]35. sensation representation action meaningless tokens manual
       representations world models merged representations symbolic path
       sub-symbolic path neural networks (deep learning) great place to
       start is the landmark publication: parallel distributed processing,
       vols. 1 and 2, rumelhart and mcclelland, 1987
   36. [104]36. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   37. [105]37. id97 the id97 model learns a vector for each word
       in the vocabulary. the number of dimensions for each word vector is
       the same and is usually around 300. unlike the tf-idf vectors, word
       vectors are dense, meaning that most values are not 0.
   38. [106]38. id97 1. initialize each word with a random vector 2.
       for each word w1 in the set of documents: 3. for each word w2
       around w1: 4. move vectors for w1 and w2 closer together and move
       all others and w1 farther apart 5. goto 2 if not done     skip-gram
       model [mikolov et al., 2013]     note: there are really two vectors
       per word, because you don   t want a word to be likely to be around
       itself, see goldberg and levyhttps://arxiv.org/pdf/1402.3722v1.pdf
           first saw that double-for-loop explanation from christopher moody
   39. [107]39. id97 meaning    you shall know a word by the company it
       keeps.    j. r. firth [1957] the quote we often see: this seems at
       least kind of true.     vectors have internal structure [mikolov et
       al., 2013]     italy     rome = france     paris     king     queen = man    
       woman but ... words aren   t grounded in experience; they are only
       grounded in being around other words. can also do id97 on
       conceptnet, see https://arxiv.org/pdf/1612.03975v1.pdf
   40. [108]40. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   41. [109]41. id195 model the id195 (sequence-to-sequence) model can
       encode sequences of tokens, such as sentences, into single vectors.
       it can then decode these vectors into other sequences of tokens.
       both the encoding and decoding are done using recurrent neural
       networks (id56s). one obvious application for this is machine
       translation. for example, where the source sentences are english
       and the target sentences are spanish.
   42. [110]42. encoding sentence meaning into a vector h0 the    the
       patient fell.   
   43. [111]43. encoding sentence meaning into a vector h0 the h1 patient
          the patient fell.   
   44. [112]44. encoding sentence meaning into a vector h0 the h1 patient
       h2 fell    the patient fell.   
   45. [113]45. encoding sentence meaning into a vector like a hidden
       markov model, but doesn   t make the markov assumption and benefits
       from a vector representation. h0 the h1 patient h2 fell h3 .    the
       patient fell.   
   46. [114]46. decoding sentence meaning el h3 machine translation, or
       structure learning more generally.
   47. [115]47. decoding sentence meaning el h3 h4 machine translation, or
       structure learning more generally.
   48. [116]48. decoding sentence meaning el h3 paciente h4 machine
       translation, or structure learning more generally.
   49. [117]49. decoding sentence meaning machine translation, or
       structure learning more generally. el h3 paciente h4 cay   h5 . h5
       [cho et al., 2014] it keeps generating until it generates a stop
       symbol. note that the lengths don   t need to be the same. it could
       generate the correct    se cay  .        treats this task like it is
       devoid of meaning.     great that this can work on just about any
       kind of id195 problem, but this generality highlights its
       limitation for use as language understanding. no chomsky universal
       grammar.
   50. [118]50. generating image captions convolutional neural network sad
       h0 wet h1 kid h2 . h3 [karpathy and fei-fei, 2015] [vinyals et al.,
       2015]
   51. [119]51. attention [bahdanau et al., 2014] el h3 paciente h4 cay  
       h5 . h5 h0 the h1 patient h2 fell h3 .
   52. [120]52. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   53. [121]53. deep learning and id53 id56s answer
       questions. what is the translation of this phrase to french? what
       is the next word? attention is useful for id53. this
       can be generalized to which facts the learner should pay attention
       to when answering questions.
   54. [122]54. deep learning and id53 bob went home. tim
       went to the junkyard. bob picked up the jar. bob went to town.
       where is the jar? a: town     memory networks [weston et al., 2014]    
       updates memory vectors based on a question and finds the best one
       to give the output. the office is north of the yard. the bath is
       north of the office. the yard is west of the kitchen. how do you go
       from the office to the kitchen? a: south, east     neural reasoner
       [peng et al., 2015]     encodes the question and facts in many
       layers, and the final layer is put through a function that gives
       the answer.
   55. [123]55. deep learning and id53 the network is
       learning linkages between sequences of symbols, but these kinds of
       stories do not have sufficiently rich linkages to our world.
   56. [124]56. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   57. [125]57. external world training if we want to talk to machines 1.
       we need to train them in an environment as much like our own as
       possible 2. can   t just be dialog! harnad [1990]
       http://users.ecs.soton.ac.uk/harnad/papers/harnad/harnad90.sgproble
       m.html to understand    chicken    we need the machine to have had as
       much experience with chickens as possible. when we say    chicken    we
       don   t just mean the bird, we mean everything one can do with it and
       everything it represents in our culture.   chicken   
   58. [126]58. there has been work in this direction industry     openai    
       universe: train on screens with vnc     now with grand theft auto!
       https://openai.com/blog/gta-v-plus-universe/     google     mikolov et
       al., a roadmap towards machine intelligence. they define an
       artificial environment. https://arxiv.org/pdf/1511.08130v2.pdf    
       facebook     weston, memory networks to dialogs
       https://arxiv.org/pdf/1604.06045v7.pdf     kiela et al., virtual
       embodiment: a scalable long-term strategy for artificial
       intelligence res. advocate using video games    with a purpose.   
       https://arxiv.org/pdf/1610.07432v1.pdf academia     ray mooney     maps
       text to situations http://videolectures.net/aaai2013_moo
       ney_language_learning/     luc steels     robots come up with
       vocabulary and simple grammar     narasimhan et al.     train a neural
       net to play text- based adventure games
       https://arxiv.org/pdf/1506.08941v2.pdf icub
   59. [127]59. but we need more training centered in our world maybe if
       amazon alexa had a camera and rotating head? how far could we get
       without the benefit of a teacher?     could we use eye gaze as a cue?
       [yu and ballard, 2010]
   60. [128]60. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   61. [129]61. sensation representation action meaningless tokens manual
       representations world models merged representations id97
       id195 id53 external world training symbolic path
       sub-symbolic path
   62. [130]62. sensation representation action symbolic path sub-symbolic
       path building world models based on large, deep, and organized
       representations training large neural networks in an environment
       with similar objects, relationships, and dynamics as our own two
       paths from nlp to ai
   63. [131]63. sensation representation action final thought open
       problem: what is the simplest commercially viable task that
       requires commonsense knowledge and reasoning?
   64. [132]64. thanks for listening jonathan mugan @jmugan
       www.deepgrammar.com

          [133]recommended

     * core strategies for teaching in higher ed
       core strategies for teaching in higher ed
       online course - linkedin learning
     * insights from a college career coach
       insights from a college career coach
       online course - linkedin learning
     * office 2016 for educators
       office 2016 for educators
       online course - linkedin learning
     * natural language processing in artificial intelligence - codeup #5
       - payu natural language processing in artificial intelligence -
       codeup #5 - payu
       artivatic.ai
     * generating natural-language text with neural networks
       generating natural-language text with neural networks
       jonathan mugan
     * data day seattle, from nlp to ai
       data day seattle, from nlp to ai
       jonathan mugan
     * data day seattle, chatbots from first principles
       data day seattle, chatbots from first principles
       jonathan mugan
     * chatbots from first principles
       chatbots from first principles
       jonathan mugan
     * what deep learning means for artificial intelligence
       what deep learning means for artificial intelligence
       jonathan mugan
     * deep learning for natural language processing
       deep learning for natural language processing
       jonathan mugan

     * [134]english
     * [135]espa  ol
     * [136]portugu  s
     * [137]fran  ais
     * [138]deutsch

     * [139]about
     * [140]dev & api
     * [141]blog
     * [142]terms
     * [143]privacy
     * [144]copyright
     * [145]support

     *
     *
     *
     *
     *

   linkedin corporation    2019

     

share clipboard
     __________________________________________________________________

   [146]  
     * facebook
     * twitter
     * linkedin

   link ____________________

public clipboards featuring this slide
     __________________________________________________________________

   (button)   
   no public clipboards found for this slide

select another clipboard
     __________________________________________________________________

   [147]  

   looks like you   ve clipped this slide to already.
   ____________________

   create a clipboard

you just clipped your first slide!

   clipping is a handy way to collect important slides you want to go back
   to later. now customize the name of a clipboard to store your clips.
     __________________________________________________________________

   name* ____________________
   description ____________________
   visibility
   others can see my clipboard [ ]
   (button) cancel (button) save

   bizographics tracking image

references

   visible links
   1. https://www.slideshare.net/rss/latest
   2. https://www.slideshare.net/opensearch.xml
   3. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
   4. https://es.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
   5. https://fr.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
   6. https://de.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
   7. https://pt.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
   8. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
   9. https://www.slideshare.net/api/oembed/2?format=json&url=http://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  10. https://www.slideshare.net/api/oembed/2?format=xml&url=http://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  11. https://www.slideshare.net/mobile/jmugan/from-natural-language-processing-to-artificial-intelligence
  12. android-app://net.slideshare.mobile/slideshare-app/ss/71019044
  13. ios-app://917418728/slideshare-app/ss/71019044
  14. http://www.linkedin.com/legal/user-agreement
  15. http://www.linkedin.com/legal/privacy-policy
  16. http://www.linkedin.com/legal/privacy-policy
  17. http://www.linkedin.com/legal/user-agreement
  18. https://www.slideshare.net/
  19. https://www.slideshare.net/explore
  20. https://www.slideshare.net/login
  21. https://www.slideshare.net/
  22. https://www.slideshare.net/upload
  23. https://www.slideshare.net/login
  24. https://www.slideshare.net/w/signup
  25. https://www.slideshare.net/
  26. https://www.slideshare.net/explore
  27. https://www.linkedin.com/learning/topics/presentations?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  28. https://www.linkedin.com/learning/topics/powerpoint?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  29. https://www.linkedin.com/learning?trk=slideshare_subnav_learning
  30. https://www.linkedin.com/psettings/privacy
  31. https://public.slidesharecdn.com/artivatic/natural-language-processing-in-artificial-intelligence
  32. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  33. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  34. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  35. https://www.slideshare.net/jmugan?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  36. https://www.slideshare.net/jmugan?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  37. https://www.slideshare.net/signup?login_source=slideview.popup.follow&from=addcontact&from_source=https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  38. https://www.slideshare.net/featured/category/data-analytics
  39. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence#comments-panel
  40. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence#likes-panel
  41. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence#stats-panel
  42. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence#notes-panel
  43. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  44. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  45. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  46. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  47. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  48. https://www.slideshare.net/signup?login_source=slideview.popup.comment&from=comments&from_source=https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  49. https://www.slideshare.net/johnmayer664?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  50. https://www.slideshare.net/johnmayer664?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  51. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  52. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  53. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  54. https://www.slideshare.net/dwolfson04?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  55. https://www.slideshare.net/dwolfson04?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  56. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  57. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  58. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  59. https://www.slideshare.net/anupamkundu77?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  60. https://www.slideshare.net/anupamkundu77?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  61. https://www.slideshare.net/chienjchienj?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  62. https://www.slideshare.net/chienjchienj?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  63. https://www.slideshare.net/fairouzattiya?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  64. https://www.slideshare.net/fairouzattiya?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  65. https://www.slideshare.net/linekin?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  66. https://www.slideshare.net/linekin?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  67. https://www.slideshare.net/chernushenko?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  68. https://www.slideshare.net/chernushenko?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  69. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
  70. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-2-638.jpg?cb=1484421392
  71. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-3-638.jpg?cb=1484421392
  72. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-4-638.jpg?cb=1484421392
  73. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-5-638.jpg?cb=1484421392
  74. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-6-638.jpg?cb=1484421392
  75. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-7-638.jpg?cb=1484421392
  76. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-8-638.jpg?cb=1484421392
  77. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-9-638.jpg?cb=1484421392
  78. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-10-638.jpg?cb=1484421392
  79. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-11-638.jpg?cb=1484421392
  80. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-12-638.jpg?cb=1484421392
  81. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-13-638.jpg?cb=1484421392
  82. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-14-638.jpg?cb=1484421392
  83. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-15-638.jpg?cb=1484421392
  84. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-16-638.jpg?cb=1484421392
  85. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-17-638.jpg?cb=1484421392
  86. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-18-638.jpg?cb=1484421392
  87. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-19-638.jpg?cb=1484421392
  88. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-20-638.jpg?cb=1484421392
  89. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-21-638.jpg?cb=1484421392
  90. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-22-638.jpg?cb=1484421392
  91. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-23-638.jpg?cb=1484421392
  92. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-24-638.jpg?cb=1484421392
  93. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-25-638.jpg?cb=1484421392
  94. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-26-638.jpg?cb=1484421392
  95. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-27-638.jpg?cb=1484421392
  96. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-28-638.jpg?cb=1484421392
  97. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-29-638.jpg?cb=1484421392
  98. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-30-638.jpg?cb=1484421392
  99. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-31-638.jpg?cb=1484421392
 100. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-32-638.jpg?cb=1484421392
 101. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-33-638.jpg?cb=1484421392
 102. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-34-638.jpg?cb=1484421392
 103. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-35-638.jpg?cb=1484421392
 104. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-36-638.jpg?cb=1484421392
 105. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-37-638.jpg?cb=1484421392
 106. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-38-638.jpg?cb=1484421392
 107. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-39-638.jpg?cb=1484421392
 108. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-40-638.jpg?cb=1484421392
 109. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-41-638.jpg?cb=1484421392
 110. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-42-638.jpg?cb=1484421392
 111. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-43-638.jpg?cb=1484421392
 112. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-44-638.jpg?cb=1484421392
 113. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-45-638.jpg?cb=1484421392
 114. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-46-638.jpg?cb=1484421392
 115. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-47-638.jpg?cb=1484421392
 116. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-48-638.jpg?cb=1484421392
 117. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-49-638.jpg?cb=1484421392
 118. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-50-638.jpg?cb=1484421392
 119. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-51-638.jpg?cb=1484421392
 120. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-52-638.jpg?cb=1484421392
 121. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-53-638.jpg?cb=1484421392
 122. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-54-638.jpg?cb=1484421392
 123. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-55-638.jpg?cb=1484421392
 124. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-56-638.jpg?cb=1484421392
 125. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-57-638.jpg?cb=1484421392
 126. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-58-638.jpg?cb=1484421392
 127. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-59-638.jpg?cb=1484421392
 128. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-60-638.jpg?cb=1484421392
 129. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-61-638.jpg?cb=1484421392
 130. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-62-638.jpg?cb=1484421392
 131. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-63-638.jpg?cb=1484421392
 132. https://image.slidesharecdn.com/fromnlptoai-170114191309/95/from-natural-language-processing-to-artificial-intelligence-64-638.jpg?cb=1484421392
 133. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence#related-tab-content
 134. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
 135. https://es.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
 136. https://pt.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
 137. https://fr.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
 138. https://de.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
 139. https://www.slideshare.net/about
 140. https://www.slideshare.net/developers
 141. http://blog.slideshare.net/
 142. https://www.slideshare.net/terms
 143. https://www.slideshare.net/privacy
 144. http://www.linkedin.com/legal/copyright-policy
 145. https://www.linkedin.com/help/slideshare
 146. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
 147. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence

   hidden links:
 149. https://www.slideshare.net/jmugan/from-natural-language-processing-to-artificial-intelligence
 150. https://www.slideshare.net/signup?login_source=slideview.clip.like&from=clip&layout=foundation&from_source=
 151. https://www.slideshare.net/login?from_source=%2fjmugan%2ffrom-natural-language-processing-to-artificial-intelligence%3ffrom_action%3dsave&from=download&layout=foundation
 152. https://www.slideshare.net/signup?login_source=slideview.popup.flags&from=flagss&from_source=https%3a%2f%2fwww.slideshare.net%2fjmugan%2ffrom-natural-language-processing-to-artificial-intelligence
 153. https://www.linkedin.com/learning/core-strategies-for-teaching-in-higher-ed?trk=slideshare_sv_learning
 154. https://www.linkedin.com/learning/insights-from-a-college-career-coach?trk=slideshare_sv_learning
 155. https://www.linkedin.com/learning/office-2016-for-educators?trk=slideshare_sv_learning
 156. https://www.slideshare.net/artivatic/natural-language-processing-in-artificial-intelligence
 157. https://www.slideshare.net/jmugan/generating-naturallanguage-text-with-neural-networks
 158. https://www.slideshare.net/jmugan/data-day-seattle-from-nlp-to-ai
 159. https://www.slideshare.net/jmugan/data-day-seattle-chatbots-from-first-principles
 160. https://www.slideshare.net/jmugan/chatbots-from-first-principles
 161. https://www.slideshare.net/jmugan/what-deep-learning-means-for-artificial-intelligence
 162. https://www.slideshare.net/jmugan/deep-learning-for-natural-language-processing-62732431
 163. http://www.linkedin.com/company/linkedin
 164. http://www.facebook.com/linkedin
 165. http://twitter.com/slideshare
 166. http://www.google.com/+linkedin
 167. https://www.slideshare.net/rss/latest
