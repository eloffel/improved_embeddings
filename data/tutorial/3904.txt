lecture 1

introduction to kernel methods

bharath k. sriperumbudur

department of statistics, pennsylvania state university

machine learning summer school

t  ubingen, 2017

course outline

(cid:73) introduction to rkhs (lecture 1)

(cid:73) feature space vs. function space
(cid:73) kernel trick
(cid:73) application: ridge regression

(cid:73) generalization of kernel trick to probabilities (lecture 2)

(cid:73) hilbert space embedding of probabilities
(cid:73) mean element and covariance operator
(cid:73) application: two-sample testing

(cid:73) approximate kernel methods (lecture 3)
(cid:73) computational vs. statistical trade-o   
(cid:73) applications: ridge regression, principal component analysis

lecture outline

(cid:73) motivating examples

(cid:73) nonlinear classi   cation
(cid:73) statistical learning

(cid:73) feature space vs. function space

(cid:73) kernels and properties
(cid:73) rkhs and properties

(cid:73) application: ridge regression

(cid:73) kernel trick
(cid:73) representer theorem

motivating example: binary classi   cation

(cid:73) given: d := {(xj , yj )}n
(cid:73) goal: learn a function f : x     r such that

j=1, xj     x , yj     {   1, +1}

yj = sign(f (xj )),     j = 1, . . . , n.

linear classi   ers

(cid:73) linear classi   er: fw ,b(x) = (cid:104)w , x(cid:105)2 + b, w , x     rd , b     r
(cid:73) find w     rd and b     r such that

yj ((cid:104)w , xj(cid:105)2 + b)     0,     j = 1, . . . , n.

(cid:73) fisher discriminant analysis, support vector machine, id88, ...

nonlinear classi   cation: 1

(cid:73) there is no linear classi   er that separates red and blue regions.

nonlinear classi   cation: 1

(cid:73) there is no linear classi   er that separates red and blue regions.
(cid:73) however, the following function perfectly separates red and blue

regions

f (x) = x 2     r =

(cid:42)

(cid:43)
(cid:124) (cid:123)(cid:122) (cid:125)

, (x 2, 1)

  (x)

2

(1,   r )

(cid:124) (cid:123)(cid:122) (cid:125)

w

, a < r < b.

nonlinear classi   cation: 1

(cid:73) there is no linear classi   er that separates red and blue regions.
(cid:73) however, the following function perfectly separates red and blue

regions

f (x) = x 2     r =

(cid:42)

(cid:43)
(cid:124) (cid:123)(cid:122) (cid:125)

, (x 2, 1)

  (x)

2

(1,   r )

(cid:124) (cid:123)(cid:122) (cid:125)

w

, a < r < b.

(cid:73) by mapping x     r to   (x) = (x 2, 1)     r2, the nonlinear

classi   cation problem is turned into a linear problem.

(cid:73) we call    as the feature map (starting point of kernel trick)

nonlinear classi   cation: 2

(cid:73) there is no linear classi   er that separates red and blue regions.

nonlinear classi   cation: 2

(cid:73) there is no linear classi   er that separates red and blue regions.
(cid:73) a conic section, however, perfectly separates them

(cid:42)
(cid:124)

f (x) = ax 2

1 + bx1x2 + cx 2

2 + dx1 + ex2 + g

(a, b, c, d, e, g )

, (x 2

1 , x1x2, x 2

2 , x1, x2, 1)

(cid:123)(cid:122)

w

(cid:125)

(cid:124)

(cid:123)(cid:122)

  (x)

(cid:43)
(cid:125)

.

2

=

(cid:73)   (x)     r6.

motivating example: statistical learning

(cid:73) given: a set d := {(x1, y1), . . . , (xn, yn)} of input/output pairs

drawn independently from an unknown id203 distribution p on
x    y .

(cid:73) goal:    learn    a function f : x     y such that f (x) is a good

approximation of the possible response y for an arbitrary x.

(cid:73) we need a means to assess the quality of an estimated response

f (x) when the true input and output pair is (x, y ).

(cid:73) id168: l : y    y     [0,   )

(cid:73) squared-loss: l(y , f (x)) = (y     f (x))2
(cid:73) hinge-loss: l(y , f (x)) = max(0, 1     yf (x))

(cid:73) one common quality measure is the average loss or expected loss of

f , called the risk functional i.e.,

(cid:90)

rl,p(f ) :=

x  y

l(y , f (x)) dp(x, y ).

motivating example: statistical learning

(cid:73) given: a set d := {(x1, y1), . . . , (xn, yn)} of input/output pairs

drawn independently from an unknown id203 distribution p on
x    y .

(cid:73) goal:    learn    a function f : x     y such that f (x) is a good

approximation of the possible response y for an arbitrary x.

(cid:73) we need a means to assess the quality of an estimated response

f (x) when the true input and output pair is (x, y ).

(cid:73) id168: l : y    y     [0,   )

(cid:73) squared-loss: l(y , f (x)) = (y     f (x))2
(cid:73) hinge-loss: l(y , f (x)) = max(0, 1     yf (x))

(cid:73) one common quality measure is the average loss or expected loss of

f , called the risk functional i.e.,

(cid:90)

rl,p(f ) :=

x  y

l(y , f (x)) dp(x, y ).

motivating example: statistical learning

(cid:73) given: a set d := {(x1, y1), . . . , (xn, yn)} of input/output pairs

drawn independently from an unknown id203 distribution p on
x    y .

(cid:73) goal:    learn    a function f : x     y such that f (x) is a good

approximation of the possible response y for an arbitrary x.

(cid:73) we need a means to assess the quality of an estimated response

f (x) when the true input and output pair is (x, y ).

(cid:73) id168: l : y    y     [0,   )

(cid:73) squared-loss: l(y , f (x)) = (y     f (x))2
(cid:73) hinge-loss: l(y , f (x)) = max(0, 1     yf (x))

(cid:73) one common quality measure is the average loss or expected loss of

f , called the risk functional i.e.,

(cid:90)

rl,p(f ) :=

x  y

l(y , f (x)) dp(x, y ).

bayes risk and bayes function

(cid:73) idea: choose f that has the smallest risk.

f     := arg inf

f :x   rrl,p(f ),

where the in   mum is taken over the set of all measurable functions.
(cid:73) f     is called the bayes function and rl,p(f    ) is called the bayes risk.
(cid:73) if p is known,    nding f     is often a relatively easy task and there is

nothing to learn.

(cid:73) example: l(y , f (x)) = (y     f (x))2 and l(y , f (x)) = |y     f (x)|
(cid:73) exercise: what is f     for the above losses?

bayes risk and bayes function

(cid:73) idea: choose f that has the smallest risk.

f     := arg inf

f :x   rrl,p(f ),

where the in   mum is taken over the set of all measurable functions.
(cid:73) f     is called the bayes function and rl,p(f    ) is called the bayes risk.
(cid:73) if p is known,    nding f     is often a relatively easy task and there is

nothing to learn.

(cid:73) example: l(y , f (x)) = (y     f (x))2 and l(y , f (x)) = |y     f (x)|
(cid:73) exercise: what is f     for the above losses?

universal consistency

(cid:73) but p is unknown.

(cid:73) however    partially known    from the training set,

d := {(x1, y1), . . . , (xn, yn)}.

(cid:73) given d, the goal is to construct fd : x     r such that

rl,p(fd )     rl,p(f    ).

(cid:73) universally consistent learning algorithm: for all p on x    y , we

have

in id203.

rl,p(fd )     rl,p(f    ), n        

universal consistency

(cid:73) but p is unknown.

(cid:73) however    partially known    from the training set,

d := {(x1, y1), . . . , (xn, yn)}.

(cid:73) given d, the goal is to construct fd : x     r such that

rl,p(fd )     rl,p(f    ).

(cid:73) universally consistent learning algorithm: for all p on x    y , we

have

in id203.

rl,p(fd )     rl,p(f    ), n        

universal consistency

(cid:73) but p is unknown.

(cid:73) however    partially known    from the training set,

d := {(x1, y1), . . . , (xn, yn)}.

(cid:73) given d, the goal is to construct fd : x     r such that

rl,p(fd )     rl,p(f    ).

(cid:73) universally consistent learning algorithm: for all p on x    y , we

have

in id203.

rl,p(fd )     rl,p(f    ), n        

empirical risk minimization

(cid:73) since p is unknown but is known through d, it is tempting to

replace rl,p(f ) by

n(cid:88)

i=1

rl,d (f ) :=

1
n

l(yi , f (xi )),

called the empirical risk and    nd fd by

fd := arg min

f :x   rrl,d (f )

(cid:73) is it a good idea?
(cid:73) no! choose fd such that fd (x) = yi , x = xi ,     i and

fd (x) = 0, otherwise.

(cid:73) rl,d (fd ) = 0 but can be very far from rl,p(f    ).

over   tting!!

empirical risk minimization

(cid:73) since p is unknown but is known through d, it is tempting to

replace rl,p(f ) by

n(cid:88)

i=1

rl,d (f ) :=

1
n

l(yi , f (xi )),

called the empirical risk and    nd fd by

fd := arg min

f :x   rrl,d (f )

(cid:73) is it a good idea?
(cid:73) no! choose fd such that fd (x) = yi , x = xi ,     i and

fd (x) = 0, otherwise.

(cid:73) rl,d (fd ) = 0 but can be very far from rl,p(f    ).

over   tting!!

method of sieves (structural risk minimization)

(cid:73) how to avoid over   tting: perform erm on a small set f of

functions f : x     y (class of smooth functions) where the size of f
grows appropriately with n.

(cid:73) do minimization over f:

fd := arg inf
f    f

rl,d (f )

(cid:73) total error: de   ne r   

rl,p(fd )     r   

(cid:122)

l,p,f := inff    f rl,p(f )
(cid:125)(cid:124)
(cid:122)

l,p =

rl,p(fd )     r   

estimation error

approximation error

l,p,f

(cid:125)(cid:124)
r   
l,p,f     r   

(cid:123)

l,p

+

(cid:123)

approximation and estimation errors

approximation       errorestimation      errorhow to choose f?

fd = arg inf
f    f

rl,d (f ) = arg inf
f    f

1
n

n(cid:88)

i=1

l(yi , f (xi )
)

(cid:124)(cid:123)(cid:122)(cid:125)

  xi (f )

(cid:73) an evaluation functional is a linear functional   x that evaluates each

function in the space at the point x, i.e.,

  x (f ) = f (x),     f     f.

(cid:73) bounded evaluation functional: an evaluation functional is bounded

if there exists a m such that

|  x (f )| = |f (x)|     mx(cid:107)f (cid:107)f,     x,    x , f     f

where f is a normed vector space (continuity of   x ).

(cid:73) evaluation functionals are not always bounded.
(cid:73) example: l2[a, b]

(cid:73) (cid:107)f (cid:107)2 remains the same if f is changed at a countable set of points.

choice of f

(cid:73) various choices for f (with evaluation functional bounded):

(cid:73) lipschitz functions
(cid:73) bounded lipschitz functions
(cid:73) bounded continuous functions

(cid:73) if f is a hilbert space of functions with bounded evaluation

functionals for all x     x , computationally e   cient estimators can be
obtained.

reproducing kernel hilbert space

choice of f

(cid:73) various choices for f (with evaluation functional bounded):

(cid:73) lipschitz functions
(cid:73) bounded lipschitz functions
(cid:73) bounded continuous functions

(cid:73) if f is a hilbert space of functions with bounded evaluation

functionals for all x     x , computationally e   cient estimators can be
obtained.

reproducing kernel hilbert space

summary

points of view:

(cid:73) feature map,   : trick to achieve non-linear methods from linear ones

(cid:73) function space, f: statistical generalization and computational

e   ciency

history

(cid:73) mathematics (functional analysis): introduced in 1907 by stanis(cid:32)law zaremba for

studying boundary value problems; developed by mercer, szeg  o, bergman,
bochner, moore, aronszajn; reached maturity by late 1950   s.

(cid:73) statistics: started by emmanuel parzen (early 1960   s) and pursued by wahba

(between 1970 and 1990).

(cid:73) pattern recognition/machine learning: started by aizerman, braverman and
rozonoer (1964) but fury of activity following the work of boser, guyon and
vapnik (1992).

other areas: signal processing, control, id203 theory, stochastic processes,
numerical analysis

kernels

(feature space view point)

hilbert space

inner product: let h be a vector space over r. a map
(cid:104)  ,  (cid:105)h : h    h     r is an inner product on h if
(cid:73) linear in the    rst argument: for any f1, f2, g     h and   ,        r

(cid:104)  f1 +   f2, g(cid:105)h =   (cid:104)f1, g(cid:105)h +   (cid:104)f2, g(cid:105)h;

(cid:73) symmetric: for any f , g     h,

(cid:104)f , g(cid:105)h = (cid:104)g , f (cid:105)h;

(cid:73) positive de   niteness: for any f     h,

(cid:104)f , f (cid:105)h     0

and (cid:104)f , f (cid:105)h = 0     f = 0.

de   ne (cid:107)    (cid:107)h := (cid:104)  ,  (cid:105)h as the norm on h induced by the inner product.
a complete (by adding the limits of all cauchy sequences w.r.t. (cid:107)    (cid:107)h) inner product
space is de   ned as a hilbert space.

measure of similarity

hilbert space

inner product: let h be a vector space over r. a map
(cid:104)  ,  (cid:105)h : h    h     r is an inner product on h if
(cid:73) linear in the    rst argument: for any f1, f2, g     h and   ,        r

(cid:104)  f1 +   f2, g(cid:105)h =   (cid:104)f1, g(cid:105)h +   (cid:104)f2, g(cid:105)h;

(cid:73) symmetric: for any f , g     h,

(cid:104)f , g(cid:105)h = (cid:104)g , f (cid:105)h;

(cid:73) positive de   niteness: for any f     h,

(cid:104)f , f (cid:105)h     0

and (cid:104)f , f (cid:105)h = 0     f = 0.

de   ne (cid:107)    (cid:107)h := (cid:104)  ,  (cid:105)h as the norm on h induced by the inner product.
a complete (by adding the limits of all cauchy sequences w.r.t. (cid:107)    (cid:107)h) inner product
space is de   ned as a hilbert space.

measure of similarity

kernel

throughout, we assume that x is a non-empty set (input space)

(steinwart and christmann, 2008)

kernel: a function k : x    x     r is called a kernel if there exists a
hilbert space h and a map    : x     h such that

k(x, x(cid:48)) := (cid:104)  (x),   (x(cid:48))(cid:105)h,
    x, x(cid:48)     h.
  : feature map and h: feature space

non-uniqueness of    and h: suppose k(x, x(cid:48)) = xx(cid:48), x, x(cid:48)     r. then

  1(x) = x

and   2(x) =

1
2

(x, x)

are feature maps with corresponding feature spaces being r and r2.

kernel

throughout, we assume that x is a non-empty set (input space)

(steinwart and christmann, 2008)

kernel: a function k : x    x     r is called a kernel if there exists a
hilbert space h and a map    : x     h such that

k(x, x(cid:48)) := (cid:104)  (x),   (x(cid:48))(cid:105)h,
    x, x(cid:48)     h.
  : feature map and h: feature space

non-uniqueness of    and h: suppose k(x, x(cid:48)) = xx(cid:48), x, x(cid:48)     r. then

  1(x) = x

and   2(x) =

1
2

(x, x)

are feature maps with corresponding feature spaces being r and r2.

properties

(cid:73) for any    > 0,   k is a kernel.

  k(x, x(cid:48)) =   (cid:104)  (x),   (x(cid:48))(cid:105)h = (cid:104)   

    (x),

   

    (x(cid:48))(cid:105)h.

(cid:73) conic sum of kernels is a kernel: if (ki )m
m(cid:88)

then for any (  i )m

m(cid:88)

  i(cid:104)  i (x),   i (x(cid:48))(cid:105)hi =

  i ki (x, x(cid:48)) =

i=1     r+,(cid:80)m
m(cid:88)
= (cid:104)     (x),     (x(cid:48))(cid:105)   h

i=1

i=1

i=1

i=1   i ki is a kernel.

for all x, x(cid:48)     x where

i=1 is a collection of kernels,

(cid:104)   

  i   i (x),

   

  i   i (x(cid:48))(cid:105)hi

   

  1  1(x), . . . ,

   

  m  m(x))

and

  h = h1     . . .     hm

.

(cid:124)

(cid:123)(cid:122)

direct sum

(cid:125)

    (x) = (

(r     r = r2)

properties

(cid:73) for any    > 0,   k is a kernel.

  k(x, x(cid:48)) =   (cid:104)  (x),   (x(cid:48))(cid:105)h = (cid:104)   

    (x),

   

    (x(cid:48))(cid:105)h.

(cid:73) conic sum of kernels is a kernel: if (ki )m
m(cid:88)

then for any (  i )m

m(cid:88)

  i(cid:104)  i (x),   i (x(cid:48))(cid:105)hi =

  i ki (x, x(cid:48)) =

i=1     r+,(cid:80)m
m(cid:88)
= (cid:104)     (x),     (x(cid:48))(cid:105)   h

i=1

i=1

i=1

i=1   i ki is a kernel.

for all x, x(cid:48)     x where

i=1 is a collection of kernels,

(cid:104)   

  i   i (x),

   

  i   i (x(cid:48))(cid:105)hi

   

  1  1(x), . . . ,

   

  m  m(x))

and

  h = h1     . . .     hm

.

(cid:124)

(cid:123)(cid:122)

direct sum

(cid:125)

    (x) = (

(r     r = r2)

properties

(cid:73) for any    > 0,   k is a kernel.

  k(x, x(cid:48)) =   (cid:104)  (x),   (x(cid:48))(cid:105)h = (cid:104)   

    (x),

   

    (x(cid:48))(cid:105)h.

(cid:73) conic sum of kernels is a kernel: if (ki )m
m(cid:88)

then for any (  i )m

m(cid:88)

  i(cid:104)  i (x),   i (x(cid:48))(cid:105)hi =

  i ki (x, x(cid:48)) =

i=1     r+,(cid:80)m
m(cid:88)
= (cid:104)     (x),     (x(cid:48))(cid:105)   h

i=1

i=1

i=1

i=1   i ki is a kernel.

for all x, x(cid:48)     x where

i=1 is a collection of kernels,

(cid:104)   

  i   i (x),

   

  i   i (x(cid:48))(cid:105)hi

   

  1  1(x), . . . ,

   

  m  m(x))

and

  h = h1     . . .     hm

.

(cid:124)

(cid:123)(cid:122)

direct sum

(cid:125)

    (x) = (

(r     r = r2)

properties

(cid:73) di   erence of kernels is not a kernel:

(cid:73) suppose     x     x such that k1(x, x)     k2(x, x) < 0.
(cid:73) if k1     k2 is a kernel, then        and h such that for all x, x(cid:48)     h,

k1(x, x

(cid:48)

)     k2(x, x

(cid:48)

) = (cid:104)  (x),   (x

(cid:48)

)(cid:105)h.

(cid:73) choose x = x(cid:48).

(cid:73) product of kernels is a kernel: if k1 and k2 are kernels, then k1    k2 is

a kernel.

k((x1, x2), (x(cid:48)

1, x(cid:48)

2)) = k1(x1, x(cid:48)

1)    k2(x2, x(cid:48)
2)

= (cid:104)  1(x1),   1(x(cid:48)
= (cid:104)  1(x1)       2(x2),   1(x(cid:48)

1)(cid:105)h1    (cid:104)  2(x2),   2(x(cid:48)
1)       2(x(cid:48)

2)(cid:105)h2
2)(cid:105)h1   h2

where     denotes the tensor product.

properties

(cid:73) di   erence of kernels is not a kernel:

(cid:73) suppose     x     x such that k1(x, x)     k2(x, x) < 0.
(cid:73) if k1     k2 is a kernel, then        and h such that for all x, x(cid:48)     h,

k1(x, x

(cid:48)

)     k2(x, x

(cid:48)

) = (cid:104)  (x),   (x

(cid:48)

)(cid:105)h.

(cid:73) choose x = x(cid:48).

(cid:73) product of kernels is a kernel: if k1 and k2 are kernels, then k1    k2 is

a kernel.

k((x1, x2), (x(cid:48)

1, x(cid:48)

2)) = k1(x1, x(cid:48)

1)    k2(x2, x(cid:48)
2)

= (cid:104)  1(x1),   1(x(cid:48)
= (cid:104)  1(x1)       2(x2),   1(x(cid:48)

1)(cid:105)h1    (cid:104)  2(x2),   2(x(cid:48)
1)       2(x(cid:48)

2)(cid:105)h2
2)(cid:105)h1   h2

where     denotes the tensor product.

properties

(cid:73) suppose k1 is de   ned on {0, 1} and k2 is de   ned on {a, b, c}.

then clearly k1    k2 is de   ned on {0, 1}    {a, b, c}.

(cid:73) suppose for simplicity, we assume h1 = r2 and h2 = r5. then
2)(cid:105)r5

1)    k2(x2, x(cid:48)

k1(x1, x(cid:48)

=   (cid:62)

1)(cid:105)r2    (cid:104)  2(x2),   2(x(cid:48)
2) = (cid:104)  1(x1),   1(x(cid:48)
         
           2(x(cid:48)
2 (x2)  2(x(cid:48)
1 (x(cid:48)
1)  1(x1)  (cid:62)
2)
(cid:125)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:124)
  1(x1)  (cid:62)
2)  (cid:62)
= (cid:10)  1(x1)  (cid:62)
r5   r2
r2   r5
1)  (cid:62)
2 (x(cid:48)
2 (x2),   1(x(cid:48)
=: (cid:104)  1(x1)       2(x2),   1(x(cid:48)
1)       2(x(cid:48)

2)(cid:11)

1 (x(cid:48)
1)

2 (x2)

= tr

r2   r5
2)(cid:105)r2   r5

where r2     r5 is the space of 2    5 matrices.

properties

(cid:73) suppose k1 is de   ned on {0, 1} and k2 is de   ned on {a, b, c}.

then clearly k1    k2 is de   ned on {0, 1}    {a, b, c}.

(cid:73) suppose for simplicity, we assume h1 = r2 and h2 = r5. then
2)(cid:105)r5

1)    k2(x2, x(cid:48)

k1(x1, x(cid:48)

=   (cid:62)

1)(cid:105)r2    (cid:104)  2(x2),   2(x(cid:48)
2) = (cid:104)  1(x1),   1(x(cid:48)
         
           2(x(cid:48)
2 (x2)  2(x(cid:48)
1 (x(cid:48)
1)  1(x1)  (cid:62)
2)
(cid:125)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:123)(cid:122)
(cid:124)
  1(x1)  (cid:62)
2)  (cid:62)
= (cid:10)  1(x1)  (cid:62)
r5   r2
r2   r5
1)  (cid:62)
2 (x(cid:48)
2 (x2),   1(x(cid:48)
=: (cid:104)  1(x1)       2(x2),   1(x(cid:48)
1)       2(x(cid:48)

2)(cid:11)

1 (x(cid:48)
1)

2 (x2)

= tr

r2   r5
2)(cid:105)r2   r5

where r2     r5 is the space of 2    5 matrices.

properties

(cid:73) for any arbitrary function f : x     r,

  k(x, x(cid:48)) = f (x)k(x, x(cid:48))f (x(cid:48))

(1)

is a kernel.

= (cid:104)f (x)  (x)

  k(x, x(cid:48)) = f (x)k(x, x(cid:48))f (x(cid:48)) = f (x)(cid:104)  (x),   (x(cid:48))(cid:105)hf (x(cid:48))
(cid:105)h.

(cid:123)(cid:122)
(cid:73) k(x, x)     0: k(x, x) = (cid:104)  (x),   (x)(cid:105)h = (cid:107)  (x)(cid:107)2h     0.

(cid:124)
(cid:125)
(cid:73) cauchy-schwartz: |k(x, y )|    (cid:112)k(x, x)(cid:112)k(x(cid:48), x(cid:48))

, f (x(cid:48))  (x(cid:48))

(cid:123)(cid:122)

  f (x(cid:48))

(cid:124)

(cid:125)

  f (x)

|k(x, x(cid:48))| = |(cid:104)  (x),   (x(cid:48))(cid:105)h|     (cid:107)  (x)(cid:107)h(cid:107)  (x(cid:48))(cid:107)h.

properties

(cid:73) for any arbitrary function f : x     r,

  k(x, x(cid:48)) = f (x)k(x, x(cid:48))f (x(cid:48))

(1)

is a kernel.

= (cid:104)f (x)  (x)

  k(x, x(cid:48)) = f (x)k(x, x(cid:48))f (x(cid:48)) = f (x)(cid:104)  (x),   (x(cid:48))(cid:105)hf (x(cid:48))
(cid:105)h.

(cid:123)(cid:122)
(cid:73) k(x, x)     0: k(x, x) = (cid:104)  (x),   (x)(cid:105)h = (cid:107)  (x)(cid:107)2h     0.

(cid:124)
(cid:125)
(cid:73) cauchy-schwartz: |k(x, y )|    (cid:112)k(x, x)(cid:112)k(x(cid:48), x(cid:48))

, f (x(cid:48))  (x(cid:48))

(cid:123)(cid:122)

  f (x(cid:48))

(cid:124)

(cid:125)

  f (x)

|k(x, x(cid:48))| = |(cid:104)  (x),   (x(cid:48))(cid:105)h|     (cid:107)  (x)(cid:107)h(cid:107)  (x(cid:48))(cid:107)h.

properties

(cid:73) for any arbitrary function f : x     r,

  k(x, x(cid:48)) = f (x)k(x, x(cid:48))f (x(cid:48))

(1)

is a kernel.

= (cid:104)f (x)  (x)

  k(x, x(cid:48)) = f (x)k(x, x(cid:48))f (x(cid:48)) = f (x)(cid:104)  (x),   (x(cid:48))(cid:105)hf (x(cid:48))
(cid:105)h.

(cid:123)(cid:122)
(cid:73) k(x, x)     0: k(x, x) = (cid:104)  (x),   (x)(cid:105)h = (cid:107)  (x)(cid:107)2h     0.

(cid:124)
(cid:125)
(cid:73) cauchy-schwartz: |k(x, y )|    (cid:112)k(x, x)(cid:112)k(x(cid:48), x(cid:48))

, f (x(cid:48))  (x(cid:48))

(cid:123)(cid:122)

  f (x(cid:48))

(cid:124)

(cid:125)

  f (x)

|k(x, x(cid:48))| = |(cid:104)  (x),   (x(cid:48))(cid:105)h|     (cid:107)  (x)(cid:107)h(cid:107)  (x(cid:48))(cid:107)h.

properties

(cid:73) in   nite dimensional feature map:

(cid:88)

k(x, x(cid:48)) =

(cid:96)2(i ) :=(cid:80)

i   i   2

  i (x)  i (x(cid:48))

is a kernel

i   i
i (x) <     for all x     x .

if (cid:107)(  i (x))i(cid:107)2

(cid:73) proof:

k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h

where   (x) = (  i (x))i   i and h = (cid:96)2(i ), which is the space of
square summable sequences on i .

if i is countable, then   (x) is in   nite dimensional.

properties

(cid:73) in   nite dimensional feature map:

(cid:88)

k(x, x(cid:48)) =

(cid:96)2(i ) :=(cid:80)

i   i   2

  i (x)  i (x(cid:48))

is a kernel

i   i
i (x) <     for all x     x .

if (cid:107)(  i (x))i(cid:107)2

(cid:73) proof:

k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h

where   (x) = (  i (x))i   i and h = (cid:96)2(i ), which is the space of
square summable sequences on i .

if i is countable, then   (x) is in   nite dimensional.

examples

(cid:73) polynomial kernel: k(x, x(cid:48)) = (c + (cid:104)x, x(cid:48)(cid:105)2)m , x, x(cid:48)     rd for c     0

and m     n. use binomial theorem to expand, apply sum and
product rules.

(cid:73) linear kernel: c = 0 and m = 1.
(cid:73) exponential kernel: k(x, x(cid:48)) = exp((cid:104)x, x(cid:48)(cid:105)2), x, x(cid:48)     rd .

use taylor series expansion,

k(x, x(cid:48)) = exp((cid:104)x, x(cid:48)(cid:105)2) =

(cid:73) gaussian kernel: k(x, x(cid:48)) = exp

, x, x(cid:48)     rd . note that

2

.

(cid:104)x, x(cid:48)(cid:105)i

i!

i=0

   (cid:88)
(cid:16)   (cid:107)x   x(cid:48)(cid:107)2
(cid:17)
(cid:16)   2
(cid:19)
(cid:17)
(cid:16)   (cid:107)x(cid:107)2

exp

exp

=

  2

2

2

  2

(cid:17)
(cid:16)   (cid:107)x(cid:48)(cid:107)2

2

  2

(cid:17)

(cid:104)x,x(cid:48)(cid:105)2

  2

exp

(cid:18)

   (cid:107)x     x(cid:48)(cid:107)2

2

  2

k(x, x(cid:48)) = exp

and apply (1).

examples

(cid:73) polynomial kernel: k(x, x(cid:48)) = (c + (cid:104)x, x(cid:48)(cid:105)2)m , x, x(cid:48)     rd for c     0

and m     n. use binomial theorem to expand, apply sum and
product rules.

(cid:73) linear kernel: c = 0 and m = 1.
(cid:73) exponential kernel: k(x, x(cid:48)) = exp((cid:104)x, x(cid:48)(cid:105)2), x, x(cid:48)     rd .

use taylor series expansion,

k(x, x(cid:48)) = exp((cid:104)x, x(cid:48)(cid:105)2) =

(cid:73) gaussian kernel: k(x, x(cid:48)) = exp

, x, x(cid:48)     rd . note that

2

.

(cid:104)x, x(cid:48)(cid:105)i

i!

i=0

   (cid:88)
(cid:16)   (cid:107)x   x(cid:48)(cid:107)2
(cid:17)
(cid:16)   2
(cid:19)
(cid:17)
(cid:16)   (cid:107)x(cid:107)2

exp

exp

=

  2

2

2

  2

(cid:17)
(cid:16)   (cid:107)x(cid:48)(cid:107)2

2

  2

(cid:17)

(cid:104)x,x(cid:48)(cid:105)2

  2

exp

(cid:18)

   (cid:107)x     x(cid:48)(cid:107)2

2

  2

k(x, x(cid:48)) = exp

and apply (1).

examples

(cid:73) polynomial kernel: k(x, x(cid:48)) = (c + (cid:104)x, x(cid:48)(cid:105)2)m , x, x(cid:48)     rd for c     0

and m     n. use binomial theorem to expand, apply sum and
product rules.

(cid:73) linear kernel: c = 0 and m = 1.
(cid:73) exponential kernel: k(x, x(cid:48)) = exp((cid:104)x, x(cid:48)(cid:105)2), x, x(cid:48)     rd .

use taylor series expansion,

k(x, x(cid:48)) = exp((cid:104)x, x(cid:48)(cid:105)2) =

(cid:73) gaussian kernel: k(x, x(cid:48)) = exp

, x, x(cid:48)     rd . note that

2

.

(cid:104)x, x(cid:48)(cid:105)i

i!

i=0

   (cid:88)
(cid:16)   (cid:107)x   x(cid:48)(cid:107)2
(cid:17)
(cid:16)   2
(cid:19)
(cid:17)
(cid:16)   (cid:107)x(cid:107)2

exp

exp

=

  2

2

2

  2

(cid:17)
(cid:16)   (cid:107)x(cid:48)(cid:107)2

2

  2

(cid:17)

(cid:104)x,x(cid:48)(cid:105)2

  2

exp

(cid:18)

   (cid:107)x     x(cid:48)(cid:107)2

2

  2

k(x, x(cid:48)) = exp

and apply (1).

positive de   niteness

(cid:73) but given a bi-variate function k(x, x(cid:48)), it is not always easy to
verify that it is a kernel, i.e., it is not easy to establish that there
exists    and h such that

k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h.

(cid:73) a complete characterization is provided by moore-aronszajn

theorem (aronszajn, 1950)
a function k : x    x     r is a kernel if and only if it is symmetric

and positive de   nite.

(cid:73) symmetry: k(x, x(cid:48)) = k(x(cid:48), x), x, x(cid:48)     r
(cid:73) positive de   niteness: k is said to be positive de   nite if for all n     n, (  i )n

i=1     r

and all (xi )n

i=1     x ,

n(cid:88)

n(cid:88)

  i   j k(xi , xj )     0.

(cid:80)n

i=1

(cid:80)n
j=1   i   j k(xi , xj ) = 0       i = 0,     i.

j=1

i=1

k is said to be strictly positive de   nite if for mutually distinct xi ,

positive de   niteness

(cid:73) but given a bi-variate function k(x, x(cid:48)), it is not always easy to
verify that it is a kernel, i.e., it is not easy to establish that there
exists    and h such that

k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h.

(cid:73) a complete characterization is provided by moore-aronszajn

theorem (aronszajn, 1950)
a function k : x    x     r is a kernel if and only if it is symmetric

and positive de   nite.

(cid:73) symmetry: k(x, x(cid:48)) = k(x(cid:48), x), x, x(cid:48)     r
(cid:73) positive de   niteness: k is said to be positive de   nite if for all n     n, (  i )n

i=1     r

and all (xi )n

i=1     x ,

n(cid:88)

n(cid:88)

  i   j k(xi , xj )     0.

(cid:80)n

i=1

(cid:80)n
j=1   i   j k(xi , xj ) = 0       i = 0,     i.

j=1

i=1

k is said to be strictly positive de   nite if for mutually distinct xi ,

positive de   niteness

(cid:73) but given a bi-variate function k(x, x(cid:48)), it is not always easy to
verify that it is a kernel, i.e., it is not easy to establish that there
exists    and h such that

k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h.

(cid:73) a complete characterization is provided by moore-aronszajn

theorem (aronszajn, 1950)
a function k : x    x     r is a kernel if and only if it is symmetric

and positive de   nite.

(cid:73) symmetry: k(x, x(cid:48)) = k(x(cid:48), x), x, x(cid:48)     r
(cid:73) positive de   niteness: k is said to be positive de   nite if for all n     n, (  i )n

i=1     r

and all (xi )n

i=1     x ,

n(cid:88)

n(cid:88)

  i   j k(xi , xj )     0.

(cid:80)n

i=1

(cid:80)n
j=1   i   j k(xi , xj ) = 0       i = 0,     i.

j=1

i=1

k is said to be strictly positive de   nite if for mutually distinct xi ,

positive de   niteness

(cid:73) kernels are symmetric and positive de   nite: easy

(cid:73) symmetry: k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h = (cid:104)  (x(cid:48)),   (x)(cid:105)h = k(x(cid:48), x)
(cid:73) positive de   niteness:

n(cid:88)

n(cid:88)

n(cid:88)

n(cid:88)

i=1

j=1

i=1

j=1

  i   j k(xi , xj ) =

  i   j(cid:104)  (x),   (x

(cid:48)

)(cid:105)h =

  i   (xi )

    0.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

h

(cid:73) symmetric and positive de   nite functions are kernels: not

obvious

the proof is based on the construction of a reproducing kernel
hilbert space.

in general, checking for positive de   niteness is also not easy.

positive de   niteness

(cid:73) kernels are symmetric and positive de   nite: easy

(cid:73) symmetry: k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h = (cid:104)  (x(cid:48)),   (x)(cid:105)h = k(x(cid:48), x)
(cid:73) positive de   niteness:

n(cid:88)

n(cid:88)

n(cid:88)

n(cid:88)

i=1

j=1

i=1

j=1

  i   j k(xi , xj ) =

  i   j(cid:104)  (x),   (x

(cid:48)

)(cid:105)h =

  i   (xi )

    0.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

h

(cid:73) symmetric and positive de   nite functions are kernels: not

obvious

the proof is based on the construction of a reproducing kernel
hilbert space.

in general, checking for positive de   niteness is also not easy.

positive de   niteness

(cid:73) kernels are symmetric and positive de   nite: easy

(cid:73) symmetry: k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105)h = (cid:104)  (x(cid:48)),   (x)(cid:105)h = k(x(cid:48), x)
(cid:73) positive de   niteness:

n(cid:88)

n(cid:88)

n(cid:88)

n(cid:88)

i=1

j=1

i=1

j=1

  i   j k(xi , xj ) =

  i   j(cid:104)  (x),   (x

(cid:48)

)(cid:105)h =

  i   (xi )

    0.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

h

(cid:73) symmetric and positive de   nite functions are kernels: not

obvious

the proof is based on the construction of a reproducing kernel
hilbert space.

in general, checking for positive de   niteness is also not easy.

positive de   niteness: translation invariant kernels

let x = rd . a kernel k : x    x     rd is said to be translation invariant if

k(x, y ) =   (x     y ), x, y     rd ,

where    is a positive de   nite function on rd .

(cid:73) bochner   s theorem provides a complete characterization for the

positive de   niteness of   .

(cid:73) a continuous function    : rd     r is positive de   nite if and only if
   is the fourier transform of a    nite non-negative borel measure   ,
i.e.,

  (x) =

e

rd

      1(cid:104)x,  (cid:105)2 d  (  )
(cid:125)
(cid:123)(cid:122)

.

characteristic function of   

given a continuous integrable function   , i.e.,(cid:82)

rd |  (x)| dx <    , compute

    (  ) =

1

(2  )d

rd

e         1(cid:104)  ,x(cid:105)2   (x) dx.

if     (  ) is non-negative for all        rd , then    is positive de   nite and
k(x, x(cid:48)) =   (x     x(cid:48)) is a kernel.

(cid:90)
(cid:124)
(cid:90)

positive de   niteness: translation invariant kernels

let x = rd . a kernel k : x    x     rd is said to be translation invariant if

k(x, y ) =   (x     y ), x, y     rd ,

where    is a positive de   nite function on rd .

(cid:73) bochner   s theorem provides a complete characterization for the

positive de   niteness of   .

(cid:73) a continuous function    : rd     r is positive de   nite if and only if
   is the fourier transform of a    nite non-negative borel measure   ,
i.e.,

  (x) =

e

rd

      1(cid:104)x,  (cid:105)2 d  (  )
(cid:125)
(cid:123)(cid:122)

.

characteristic function of   

given a continuous integrable function   , i.e.,(cid:82)

rd |  (x)| dx <    , compute

    (  ) =

1

(2  )d

rd

e         1(cid:104)  ,x(cid:105)2   (x) dx.

if     (  ) is non-negative for all        rd , then    is positive de   nite and
k(x, x(cid:48)) =   (x     x(cid:48)) is a kernel.

(cid:90)
(cid:124)
(cid:90)

positive de   niteness: translation invariant kernels

let x = rd . a kernel k : x    x     rd is said to be translation invariant if

k(x, y ) =   (x     y ), x, y     rd ,

where    is a positive de   nite function on rd .

(cid:73) bochner   s theorem provides a complete characterization for the

positive de   niteness of   .

(cid:73) a continuous function    : rd     r is positive de   nite if and only if
   is the fourier transform of a    nite non-negative borel measure   ,
i.e.,

  (x) =

e

rd

      1(cid:104)x,  (cid:105)2 d  (  )
(cid:125)
(cid:123)(cid:122)

.

characteristic function of   

given a continuous integrable function   , i.e.,(cid:82)

rd |  (x)| dx <    , compute

    (  ) =

1

(2  )d

rd

e         1(cid:104)  ,x(cid:105)2   (x) dx.

if     (  ) is non-negative for all        rd , then    is positive de   nite and
k(x, x(cid:48)) =   (x     x(cid:48)) is a kernel.

(cid:90)
(cid:124)
(cid:90)

exercise

(cid:73) show that

  (x) = (1     |x|)1[   1,1](x), x     r

is positive de   nite.

(cid:73) show that

  (x) =

1
2

(2     |x|)21{(2   |x|)   [0,1]} +

is not positive de   nite.

(cid:19)

(cid:18)

1     x 2
2

1[   1,1](x), x     r

so far...

kernels     symmetric and positive de   nite functions

reproducing kernel hilbert space

(function space view point)

reproducing kernel hilbert space

(cid:73) a hilbert space h of real-valued functions on x is said to be a

reproducing kernel hilbert space (rkhs) with k : x    x     r as
the reproducing kernel, if
(cid:73)     x     x , k(  , x)     h;
(cid:73)    x     x ,     f     h, (cid:104)f , k(  , x)(cid:105)h = f (x).

(cid:73) the reproducing kernel (r.k.) k of h is a kernel:

k(x, x

(cid:48)

) =

(cid:43)

(cid:42)
(cid:124) (cid:123)(cid:122) (cid:125)

k(  , x)

  (x)

(cid:48)

(cid:124) (cid:123)(cid:122) (cid:125)

, k(  , x
  (x(cid:48))

)

, x, x

(cid:48)     x .

h

we refer to   (x) = k(  , x) as the canonical feature map.
(cid:73) every r.k. is a symmetric and positive de   nite function.
(cid:73) the evaluation functional is bounded:

|  x (f )| = |f (x)| = |(cid:104)f , k(  , x)(cid:105)h|     (cid:107)k(  , x)(cid:107)h(cid:107)f (cid:107)h

=(cid:112)k(x, x)(cid:107)f (cid:107)h,     x     x , f     h.

reproducing kernel hilbert space

(cid:73) a hilbert space h of real-valued functions on x is said to be a

reproducing kernel hilbert space (rkhs) with k : x    x     r as
the reproducing kernel, if
(cid:73)     x     x , k(  , x)     h;
(cid:73)    x     x ,     f     h, (cid:104)f , k(  , x)(cid:105)h = f (x).

(cid:73) the reproducing kernel (r.k.) k of h is a kernel:

k(x, x

(cid:48)

) =

(cid:43)

(cid:42)
(cid:124) (cid:123)(cid:122) (cid:125)

k(  , x)

  (x)

(cid:48)

(cid:124) (cid:123)(cid:122) (cid:125)

, k(  , x
  (x(cid:48))

)

, x, x

(cid:48)     x .

h

we refer to   (x) = k(  , x) as the canonical feature map.
(cid:73) every r.k. is a symmetric and positive de   nite function.
(cid:73) the evaluation functional is bounded:

|  x (f )| = |f (x)| = |(cid:104)f , k(  , x)(cid:105)h|     (cid:107)k(  , x)(cid:107)h(cid:107)f (cid:107)h

=(cid:112)k(x, x)(cid:107)f (cid:107)h,     x     x , f     h.

reproducing kernel hilbert space

(cid:73) a hilbert space h of real-valued functions on x is said to be a

reproducing kernel hilbert space (rkhs) with k : x    x     r as
the reproducing kernel, if
(cid:73)     x     x , k(  , x)     h;
(cid:73)    x     x ,     f     h, (cid:104)f , k(  , x)(cid:105)h = f (x).

(cid:73) the reproducing kernel (r.k.) k of h is a kernel:

k(x, x

(cid:48)

) =

(cid:43)

(cid:42)
(cid:124) (cid:123)(cid:122) (cid:125)

k(  , x)

  (x)

(cid:48)

(cid:124) (cid:123)(cid:122) (cid:125)

, k(  , x
  (x(cid:48))

)

, x, x

(cid:48)     x .

h

we refer to   (x) = k(  , x) as the canonical feature map.
(cid:73) every r.k. is a symmetric and positive de   nite function.
(cid:73) the evaluation functional is bounded:

|  x (f )| = |f (x)| = |(cid:104)f , k(  , x)(cid:105)h|     (cid:107)k(  , x)(cid:107)h(cid:107)f (cid:107)h

=(cid:112)k(x, x)(cid:107)f (cid:107)h,     x     x , f     h.

reproducing kernel hilbert space

(cid:73) a hilbert space h of real-valued functions on x is said to be a

reproducing kernel hilbert space (rkhs) with k : x    x     r as
the reproducing kernel, if
(cid:73)     x     x , k(  , x)     h;
(cid:73)    x     x ,     f     h, (cid:104)f , k(  , x)(cid:105)h = f (x).

(cid:73) the reproducing kernel (r.k.) k of h is a kernel:

k(x, x

(cid:48)

) =

(cid:43)

(cid:42)
(cid:124) (cid:123)(cid:122) (cid:125)

k(  , x)

  (x)

(cid:48)

(cid:124) (cid:123)(cid:122) (cid:125)

, k(  , x
  (x(cid:48))

)

, x, x

(cid:48)     x .

h

we refer to   (x) = k(  , x) as the canonical feature map.
(cid:73) every r.k. is a symmetric and positive de   nite function.
(cid:73) the evaluation functional is bounded:

|  x (f )| = |f (x)| = |(cid:104)f , k(  , x)(cid:105)h|     (cid:107)k(  , x)(cid:107)h(cid:107)f (cid:107)h

=(cid:112)k(x, x)(cid:107)f (cid:107)h,     x     x , f     h.

reproducing kernel hilbert space

(cid:73) every hilbert function space with a reproducing kernel is an rkhs.
(cid:73) the converse is true: every rkhs has a unique reproducing kernel.
(cid:73) (moore-aronszajn theorem)

if k is a positive de   nite kernel, then there exists a unique rkhs with k as the
reproducing kernel.

(proof: de   ne h = {f : f =(cid:80)n

i=1   i k(  , xi ),   i     r, xi     x} endowed with the

bilinear form

n(cid:88)

(cid:104)f , g(cid:105)h =

  i   j k(xi , xj ).

verify that (cid:104)  ,   (cid:105)h is an inner product and (cid:104)f , k(  , x)(cid:105)h = f (x) for any f     h.
complete h to obtain an rkhs.)

i,j=1

kernels     positive de   nite & symmetric functions     rkhs

reproducing kernel hilbert space

(cid:73) every hilbert function space with a reproducing kernel is an rkhs.
(cid:73) the converse is true: every rkhs has a unique reproducing kernel.
(cid:73) (moore-aronszajn theorem)

if k is a positive de   nite kernel, then there exists a unique rkhs with k as the
reproducing kernel.

(proof: de   ne h = {f : f =(cid:80)n

i=1   i k(  , xi ),   i     r, xi     x} endowed with the

bilinear form

n(cid:88)

(cid:104)f , g(cid:105)h =

  i   j k(xi , xj ).

verify that (cid:104)  ,   (cid:105)h is an inner product and (cid:104)f , k(  , x)(cid:105)h = f (x) for any f     h.
complete h to obtain an rkhs.)

i,j=1

kernels     positive de   nite & symmetric functions     rkhs

reproducing kernel hilbert space

(cid:73) every hilbert function space with a reproducing kernel is an rkhs.
(cid:73) the converse is true: every rkhs has a unique reproducing kernel.
(cid:73) (moore-aronszajn theorem)

if k is a positive de   nite kernel, then there exists a unique rkhs with k as the
reproducing kernel.

(proof: de   ne h = {f : f =(cid:80)n

i=1   i k(  , xi ),   i     r, xi     x} endowed with the

bilinear form

n(cid:88)

(cid:104)f , g(cid:105)h =

  i   j k(xi , xj ).

verify that (cid:104)  ,   (cid:105)h is an inner product and (cid:104)f , k(  , x)(cid:105)h = f (x) for any f     h.
complete h to obtain an rkhs.)

i,j=1

kernels     positive de   nite & symmetric functions     rkhs

functions in the rkhs

(cid:73) h = span{k(  , x) : x     x} (linear span of id81s)

i=1   i k(x, xi ) for arbitrary m     n, {  i}     r,

(cid:73) example: f (x) =(cid:80)m

x     x and {xi}     x .

k(x, y ) = e   (cid:107)x   y(cid:107)2/2  2

picture credit: a. gretton

   6   4   202468   0.4   0.200.20.40.60.81xf(x)properties of rkhs

(cid:73) k is bounded if and only every f     h is bounded.

(cid:112)k(x, x) d  (x) <    , then for every f     h,

(cid:73) if(cid:82)
(cid:82)
x
x f (x) d  (x) <    .

(cid:73) every f     h is continuous if and only if k(  , x) is continuous for all

x     x .

(cid:73) every f     h is m-times continuously di   erentiable if k is m-times

continuously di   erentiable.

k controls the properties of h

explicit realization of rkhs

function.

(cid:73) x = rd and k(x, y ) =   (x     y ) where    is a positive de   nite

(cid:73) assume    satis   es(cid:82)
rd |  (x)| dx <    . denote      to be the fourier
(cid:73) de   ne l2(rd ) := {f :(cid:82)
(cid:40)

rd |f (x)|2 dx <    }. then

transform of   .

(cid:41)

(cid:12)(cid:12)(cid:12)(cid:90)

f     l2(rd )

h =

endowed with

|  f (  )|2
    (  )

rd

d   <    

(cid:90)   f (  )  g (  )

    (  )

d  

(cid:104)f , g(cid:105)h = (2  )   d/2

is an rkhs with k as the r.k.

(wendland, 2005)

fourier transform

fourier transform

f   

f   

fourier transform

f   

f   

(cid:38)
(cid:37)

smooth function

(cid:109)

fast rate of decay of

fourier transform

gaussian rkhs

(cid:73) gaussian kernel:

k(x, y ) =   (x     y ) = e   (cid:107)x   y(cid:107)2

2/  2

, x, y     rd

(cid:73) fourier transform:

(cid:73)

h  (rd ) :=

(cid:18)   2

(cid:19)d/2

    (  ) =

2

                                 f     l2(rd ) :

e      2(cid:107)  (cid:107)2

2

4

,        rd

(cid:90)
(cid:124)

rd

|  f (  )|2e

(cid:123)(cid:122)

  2(cid:107)  (cid:107)2
2

4

<    

d  

(cid:125)

(cid:107)f (cid:107)2

h  

                                 

fast decay of          smooth h

gaussian rkhs

(cid:73) gaussian kernel:

k(x, y ) =   (x     y ) = e   (cid:107)x   y(cid:107)2

2/  2

, x, y     rd

e      2(cid:107)  (cid:107)2

2

4

,        rd

(cid:73) fourier transform:

(cid:73)

(cid:18)   2

(cid:19)d/2

    (  ) =

2

                                 f     l2(rd ) :

(cid:90)
(cid:124)

h  (rd ) :=

(cid:125)
(cid:73) {f : (cid:107)f (cid:107)h         }     {f : (cid:107)f (cid:107)h         }     h   for any    <   .

|  f (  )|2e

(cid:123)(cid:122)

  2(cid:107)  (cid:107)2
2

(cid:107)f (cid:107)2

d  

rd

h  

4

<    

                                 

more smoothness

sobolev rkhs

(cid:73) laplacian kernel:

k(x, y ) =   (x     y ) =

(cid:73) fourier transform:

(cid:114)   

2

e   |x   y|, x, y     r

(cid:73)

h2

1(r) :=

    (  ) =

                                       f     l2(r) :

(cid:73) {f : (cid:107)f (cid:107)h2

1

      }     {f : (cid:107)f (cid:107)h2

1

1

1 + |  |2 ,        r
(cid:90)
(cid:124)

(cid:123)(cid:122)

r

(cid:107)f (cid:107)2
      }     h2

h2
1

|  f (  )|2(1 + |  |2) d  

                                       

<    

(cid:125)

1 for any    <   .

extension to rd : mat  ern kernel

summing up

(cid:73) kernels: feature map    and feature space h

(cid:73) positive de   niteness and bochner   s theorem

(cid:73) rkhs: canonical feature map   (x) = k(  , x)

(cid:73) kernels     positive de   nite & symmetric functions    

rkhs

(cid:73) properties of k control the properties of the rkhs.

(cid:73) smoothness

application: ridge regression
(kernel trick: feature map point of view)

ridge regression

(cid:73) given: {(xi , yi )}n
(cid:73) task: find a linear regressor f = (cid:104)w ,  (cid:105)2 s.t. f (xi )     yi ,

i=1 where xi     rd , yi     r
n(cid:88)

((cid:104)w , xi(cid:105)2     yi )2 +   (cid:107)w(cid:107)2

(   > 0)

2

min
w   rd

1
n

i=1

(cid:73) solution: for x := (x1, . . . , xn)     rd  n and

y := (y1, . . . , yn)(cid:62)     rn,

(cid:19)   1

w =

1
n

(cid:124)

xx(cid:62) +   id

(cid:123)(cid:122)

primal

xy

(cid:125)

(cid:73) easy:

(cid:18) 1

n

(cid:18) 1

n

(cid:19)

x(cid:62)x +   in

(cid:19)   1

xx(cid:62) +   id

x = x

w =

x

1
n

(cid:124)

x(cid:62)x +   in

(cid:123)(cid:122)

dual

y

(cid:125)

(cid:18) 1

n

(cid:19)
(cid:18) 1

n

ridge regression

(cid:73) given: {(xi , yi )}n
(cid:73) task: find a linear regressor f = (cid:104)w ,  (cid:105)2 s.t. f (xi )     yi ,

i=1 where xi     rd , yi     r
n(cid:88)

((cid:104)w , xi(cid:105)2     yi )2 +   (cid:107)w(cid:107)2

(   > 0)

2

min
w   rd

1
n

i=1

(cid:73) solution: for x := (x1, . . . , xn)     rd  n and

y := (y1, . . . , yn)(cid:62)     rn,

(cid:19)   1

w =

1
n

(cid:124)

xx(cid:62) +   id

(cid:123)(cid:122)

primal

xy

(cid:125)

(cid:73) easy:

(cid:18) 1

n

(cid:18) 1

n

(cid:19)

x(cid:62)x +   in

(cid:19)   1

xx(cid:62) +   id

x = x

w =

x

1
n

(cid:124)

x(cid:62)x +   in

(cid:123)(cid:122)

dual

y

(cid:125)

(cid:18) 1

n

(cid:19)
(cid:18) 1

n

ridge regression

(cid:73) given: {(xi , yi )}n
(cid:73) task: find a linear regressor f = (cid:104)w ,  (cid:105)2 s.t. f (xi )     yi ,

i=1 where xi     rd , yi     r
n(cid:88)

((cid:104)w , xi(cid:105)2     yi )2 +   (cid:107)w(cid:107)2

(   > 0)

2

min
w   rd

1
n

i=1

(cid:73) solution: for x := (x1, . . . , xn)     rd  n and

y := (y1, . . . , yn)(cid:62)     rn,

(cid:19)   1

w =

1
n

(cid:124)

xx(cid:62) +   id

(cid:123)(cid:122)

primal

xy

(cid:125)

(cid:73) easy:

(cid:18) 1

n

(cid:18) 1

n

(cid:19)

x(cid:62)x +   in

(cid:19)   1

xx(cid:62) +   id

x = x

w =

x

1
n

(cid:124)

x(cid:62)x +   in

(cid:123)(cid:122)

dual

y

(cid:125)

(cid:18) 1

n

(cid:19)
(cid:18) 1

n

ridge regression

(cid:73) given: {(xi , yi )}n
(cid:73) task: find a linear regressor f = (cid:104)w ,  (cid:105)2 s.t. f (xi )     yi ,

i=1 where xi     rd , yi     r
n(cid:88)

((cid:104)w , xi(cid:105)2     yi )2 +   (cid:107)w(cid:107)2

(   > 0)

2

min
w   rd

1
n

i=1

(cid:73) solution: for x := (x1, . . . , xn)     rd  n and

y := (y1, . . . , yn)(cid:62)     rn,

(cid:19)   1

w =

1
n

(cid:124)

xx(cid:62) +   id

(cid:123)(cid:122)

primal

xy

(cid:125)

(cid:73) easy:

(cid:18) 1

n

(cid:18) 1

n

(cid:19)

x(cid:62)x +   in

(cid:19)   1

xx(cid:62) +   id

x = x

w =

x

1
n

(cid:124)

x(cid:62)x +   in

(cid:123)(cid:122)

dual

y

(cid:125)

(cid:18) 1

n

(cid:19)
(cid:18) 1

n

ridge regression

(cid:73) prediction: given t     rd

(cid:73) how does x(cid:62)x look like?

(cid:1)   1

t
x(cid:62)t

f (t) = (cid:104)w , t(cid:105)2 = y(cid:62)x(cid:62)(cid:0)xx(cid:62) + n  id
= y(cid:62)(cid:0)x(cid:62)x + n  in
(cid:1)   1
               
(cid:124)

(cid:104)x1, x2(cid:105)2
(cid:104)x2, x2(cid:105)2
(cid:104)xi , xj(cid:105)2
(cid:104)xn, x2(cid:105)2

(cid:104)x1, x1(cid:105)2
(cid:104)x2, x1(cid:105)1

      
      
. . .
      

(cid:104)xn, x1(cid:105)1

(cid:123)(cid:122)

...

x(cid:62)x =

               
(cid:125)

(cid:104)x1, xn(cid:105)2
(cid:104)x2, xn(cid:105)2

...

(cid:104)xn, xn(cid:105)2

matrix of inner products: gram matrix

ridge regression

(cid:73) prediction: given t     rd

(cid:73) how does x(cid:62)x look like?

(cid:1)   1

t
x(cid:62)t

f (t) = (cid:104)w , t(cid:105)2 = y(cid:62)x(cid:62)(cid:0)xx(cid:62) + n  id
= y(cid:62)(cid:0)x(cid:62)x + n  in
(cid:1)   1
               
(cid:124)

(cid:104)x1, x2(cid:105)2
(cid:104)x2, x2(cid:105)2
(cid:104)xi , xj(cid:105)2
(cid:104)xn, x2(cid:105)2

(cid:104)x1, x1(cid:105)2
(cid:104)x2, x1(cid:105)1

      
      
. . .
      

(cid:104)xn, x1(cid:105)1

(cid:123)(cid:122)

...

x(cid:62)x =

               
(cid:125)

(cid:104)x1, xn(cid:105)2
(cid:104)x2, xn(cid:105)2

...

(cid:104)xn, xn(cid:105)2

matrix of inner products: gram matrix

kernel ridge regression: feature map and kernel trick

(cid:73) given: {(xi , yi )}n
(cid:73) task: find a regressor f     h (some feature space) s.t. f (xi )     yi .
(cid:73) idea: map xi to   (xi ) and do id75,

i=1 where xi     x , yi     r
n(cid:88)
((cid:104)f ,   (xi )(cid:105)h     yi )2 +   (cid:107)f (cid:107)2h (   > 0)

min
f    h

1
n

i=1

(cid:73) solution: for   (x) := (  (x1), . . . ,   (xn))     rdim(h)  n and

y := (y1, . . . , yn)(cid:62)     rn,

  (x)  (x)(cid:62) +   idim(h)

(cid:18) 1

n

f =

=

1
n

(cid:124)
(cid:124)

1
n

(cid:18) 1

n

(cid:123)(cid:122)

primal

(cid:123)(cid:122)

dual

  (x)

  (x)(cid:62)  (x) +   in

(cid:19)   1
(cid:19)   1

  (x)y

(cid:125)

y

(cid:125)

kernel ridge regression: feature map and kernel trick

(cid:73) given: {(xi , yi )}n
(cid:73) task: find a regressor f     h (some feature space) s.t. f (xi )     yi .
(cid:73) idea: map xi to   (xi ) and do id75,

i=1 where xi     x , yi     r
n(cid:88)
((cid:104)f ,   (xi )(cid:105)h     yi )2 +   (cid:107)f (cid:107)2h (   > 0)

min
f    h

1
n

i=1

(cid:73) solution: for   (x) := (  (x1), . . . ,   (xn))     rdim(h)  n and

y := (y1, . . . , yn)(cid:62)     rn,

  (x)  (x)(cid:62) +   idim(h)

(cid:18) 1

n

f =

=

1
n

(cid:124)
(cid:124)

1
n

(cid:18) 1

n

(cid:123)(cid:122)

primal

(cid:123)(cid:122)

dual

  (x)

  (x)(cid:62)  (x) +   in

(cid:19)   1
(cid:19)   1

  (x)y

(cid:125)

y

(cid:125)

kernel ridge regression: feature map and kernel trick

(cid:73) given: {(xi , yi )}n
(cid:73) task: find a regressor f     h (some feature space) s.t. f (xi )     yi .
(cid:73) idea: map xi to   (xi ) and do id75,

i=1 where xi     x , yi     r
n(cid:88)
((cid:104)f ,   (xi )(cid:105)h     yi )2 +   (cid:107)f (cid:107)2h (   > 0)

min
f    h

1
n

i=1

(cid:73) solution: for   (x) := (  (x1), . . . ,   (xn))     rdim(h)  n and

y := (y1, . . . , yn)(cid:62)     rn,

  (x)  (x)(cid:62) +   idim(h)

(cid:18) 1

n

f =

=

1
n

(cid:124)
(cid:124)

1
n

(cid:18) 1

n

(cid:123)(cid:122)

primal

(cid:123)(cid:122)

dual

  (x)

  (x)(cid:62)  (x) +   in

(cid:19)   1
(cid:19)   1

  (x)y

(cid:125)

y

(cid:125)

kernel ridge regression: feature map and kernel trick

(cid:73) prediction: given t     x

f (t) = (cid:104)f ,   (t)(cid:105)h =

1
n

y(cid:62)  (x)(cid:62)(cid:18) 1
y(cid:62)(cid:18) 1

n

n

  (x)  (x)(cid:62) +   idim(h)

  (t)

(cid:19)   1

  (x)(cid:62)  (x) +   in

  (x)(cid:62)  (t)

as before

  (x)(cid:62)  (x) =

=

1
n

               
(cid:124)

(cid:104)  (x1),   (x1)(cid:105)h       
(cid:104)  (x2),   (x1)(cid:105)h       
. . .
(cid:104)  (xn),   (x1)(cid:105)h       

...

(cid:123)(cid:122)

(cid:104)  (x1),   (xn)(cid:105)h
(cid:104)  (x2),   (xn)(cid:105)h

...

(cid:104)  (xn),   (xn)(cid:105)h

k(xi ,xj )=(cid:104)  (xi ),  (xj )(cid:105)h

(cid:19)   1

               
(cid:125)

and

  (x)(cid:62)  (t) = [(cid:104)  (x1),   (t)(cid:105)h, . . . ,(cid:104)  (xn),   (t)(cid:105)h]

(cid:62)

kernel ridge regression: feature map and kernel trick

(cid:73) prediction: given t     x

f (t) = (cid:104)f ,   (t)(cid:105)h =

1
n

y(cid:62)  (x)(cid:62)(cid:18) 1
y(cid:62)(cid:18) 1

n

n

  (x)  (x)(cid:62) +   idim(h)

  (t)

(cid:19)   1

  (x)(cid:62)  (x) +   in

  (x)(cid:62)  (t)

as before

  (x)(cid:62)  (x) =

=

1
n

               
(cid:124)

(cid:104)  (x1),   (x1)(cid:105)h       
(cid:104)  (x2),   (x1)(cid:105)h       
. . .
(cid:104)  (xn),   (x1)(cid:105)h       

...

(cid:123)(cid:122)

(cid:104)  (x1),   (xn)(cid:105)h
(cid:104)  (x2),   (xn)(cid:105)h

...

(cid:104)  (xn),   (xn)(cid:105)h

k(xi ,xj )=(cid:104)  (xi ),  (xj )(cid:105)h

(cid:19)   1

               
(cid:125)

and

  (x)(cid:62)  (t) = [(cid:104)  (x1),   (t)(cid:105)h, . . . ,(cid:104)  (xn),   (t)(cid:105)h]

(cid:62)

feature map and kernel trick: remarks

(cid:73) the primal formulation requires the knowledge of feature map   

(and of course h) and these could be in   nite dimensional.

(cid:73) suppose we have access to a id81, k (recall: not easy to

verify that k is a kernel). then the dual formulation is entirely
determined by k (gram matrix or kernel matrix).

(cid:73) id75 in the dual uses a linear kernel.

replace (cid:104)xi , xj(cid:105)2 in your linear method by k(xi , xj ) where k is your favorite kernel

kernel trick or heuristic

feature map and kernel trick

same idea yields: (sch  olkopf and smola, 2002)

(cid:73) linear id166     kernel id166
(cid:73) principal component analysis (pca)     kernel pca
(cid:73) fisher discriminant analysis (fda)     kernel fda
(cid:73) canonical correlation analysis (cca)     kernel cca

many more ...

revisiting nonlinear classi   cation: 1

(cid:73) the following function perfectly separates red and blue regions

f (x) = x 2     r =

, a < r < b.

(cid:42)

(cid:43)
(cid:124) (cid:123)(cid:122) (cid:125)

, (x 2, 1)

  (x)

2

(1,   r )

(cid:124) (cid:123)(cid:122) (cid:125)

w

(cid:73) apply kernel trick with k(x, y ) = x 2y 2 + 1.

revisiting nonlinear classi   cation: 2

(cid:73) a conic section, however, perfectly separates them

(cid:42)
(cid:124)

f (x1, x2) = ax 2

1 + bx1x2 + cx 2

2 + dx1 + ex2 + g

=

(a, b, c, d, e, g )

, (x 2

1 , x1x2, x 2

2 , x1, x2, 1)

(cid:123)(cid:122)

w

(cid:125)

(cid:124)

(cid:123)(cid:122)

  (x)

(cid:43)
(cid:125)

.

2

(cid:73) apply kernel trick with k(x, y ). exercise: find the kernel k(x, y ).

application: ridge regression

(representer theorem: function space point of view)

learning theory: revisit

(cid:73) empirical risk: rl,d (f ) := 1

n

(cid:80)n
i=1 l(yi , f (xi ))
f :x   rrl,d (f )

fd := arg min

(cid:73) to avoid over   tting: perform erm on a small set f of functions

(class of smooth functions)

fd := arg inf
f    f

rl,d (f )

(cid:73) choice of f: evaluation functionals are bounded.

|  x (f )| = |f (x)|     mx(cid:107)f (cid:107)f,     x     x , f     f

pick f = {f : (cid:107)f (cid:107)h       }; h is an rkhs

classi   cation with lipschitz functions (von luxburg and bousquet, jmlr 2004)

penalized estimation

(cid:73) we have

fd = arg

inf

(cid:107)f (cid:107)h     

rl,d (f )

n(cid:88)

i=1

= arg

inf

(cid:107)f (cid:107)h     

1
n

l(yi , f (xi ))

(cid:73) in the lagrangian formulation, we have

fd =arg inf
f    h

rl,d (f ) +   (cid:107)f (cid:107)2

h

l(yi , f (xi )) +   (cid:107)f (cid:107)2

h

n(cid:88)

i=1

=arg inf
f    h

1
n

where    > 0.

optimization over (possibly in   nite dimensional) function space

representer theorem

consider the penalized estimation problem,

n(cid:88)

i=1

inf
f    h

1
n

l(yi , f (xi )) +     ((cid:107)f (cid:107)h)

where    : [0,   )     r is a non-decreasing function.
(cid:73) (kimeldorf, 1971; sch  olkopf et al., alt 2001) the solution to the above

minimization problem is achieved by a function of the form

n(cid:88)

f =

  i k(  , xi ),

where (  i )n

i=1     r.

i=1

the in   nite dimensional optimization problem reduces to a    nite

dimensional optimization problem in rn.

proof

(cid:73) decomposition:

h = h0     h   
0 ,

where h0 = span{k(  , x1), . . . , k(  , xn)}, h   
complement. decompose

0 : orthogonal

f = f0 + f    

accordingly.

(cid:73) the id168 l does not change by replacing f with f0 because

f (xi ) = (cid:104)f , k(  , xi )(cid:105)h = (cid:104)f0, k(  , xi )(cid:105)h + (cid:104)f    , k(  , xi )(cid:105)h

.

(cid:124)

(cid:123)(cid:122)

=0

(cid:125)

(cid:73) penalty term:

(cid:107)f0(cid:107)h     (cid:107)f (cid:107)h

   

  ((cid:107)f0(cid:107)h)       ((cid:107)f (cid:107)h).

(cid:73) thus the optimum lies in h0.

kernel ridge regression

(cid:73) f : x     r and l(y , f (x)) = (y     f (x))2 (squared loss)

n(cid:88)

i=1

inf
f    h

1
n

(yi     (cid:104)f , k(  , xi )(cid:105)h)2 +   (cid:107)f (cid:107)2

h

kernel ridge regression

(cid:73) f : x     r and l(y , f (x)) = (y     f (x))2 (squared loss)

n(cid:88)

i=1

inf
f    h

1
n

(yi     (cid:104)f , k(  , xi )(cid:105)h)2 +   (cid:107)f (cid:107)2

h

f =(cid:80)n

(cid:73) by representer theorem, the solution is of the form

i=1   i k(  , xi ) which on substitution yields
(cid:107)y     k  (cid:107)2 +     (cid:62)k  

1
n

inf
  

where k is the gram matrix with kij = k(xi , xj ).

kernel ridge regression

(cid:73) f : x     r and l(y , f (x)) = (y     f (x))2 (squared loss)

n(cid:88)

i=1

inf
f    h

1
n

(yi     (cid:104)f , k(  , xi )(cid:105)h)2 +   (cid:107)f (cid:107)2

h

f =(cid:80)n

(cid:73) by representer theorem, the solution is of the form

i=1   i k(  , xi ) which on substitution yields
(cid:107)y     k  (cid:107)2 +     (cid:62)k  

1
n

inf
  

where k is the gram matrix with kij = k(xi , xj ).

(cid:73) solution:      = (k + n  in)   1y (assuming k is invertible). for any

t     x ,

n(cid:88)

  f (t) =

    i k(t, xi ) = y(cid:62)(k + n  in)   1kt,

i=1

where (kt)i := k(t, xi ). (same solution as the feature map view point)

how to choose h?

large rkhs: universal kernel/rkhs

(cid:73) universal kernel: a kernel k on a compact metric space, x is said to be

universal if the rkhs, h is dense (w.r.t. uniform norm) in the space of
continuous functions on x .
any continous function on x can be approximated arbitrarily by a
function in h.

(cid:73) (steinwart and christmann, 2008) for certain conditions on l, if k is

universal, then

rl,p(f ) = rl,p(f    ),

inf
f    h

i.e., approximation error is zero.

(cid:73) squared loss, hinge loss,...

large rkhs: universal kernel/rkhs

(cid:73) universal kernel: a kernel k on a compact metric space, x is said to be

universal if the rkhs, h is dense (w.r.t. uniform norm) in the space of
continuous functions on x .
any continous function on x can be approximated arbitrarily by a
function in h.

(cid:73) (steinwart and christmann, 2008) for certain conditions on l, if k is

universal, then

rl,p(f ) = rl,p(f    ),

inf
f    h

i.e., approximation error is zero.

(cid:73) squared loss, hinge loss,...

when is k universal?

k is universal if and only if

(cid:90)

(cid:90)

x

x

k(x, y ) d  (x) d  (y ) > 0

for all non-zero    nite signed measures,    on x .

(carmeli et al., 2010; s et al., 2011)

generalization of strictly positive de   nite kernels

(cid:73) in lecture 2, we will explore more by relating it to the hilbert space

embedding of measures.

(cid:73) examples: gaussian, laplacian, etc. (no    nite dimensional rkhs is

universal!!)

references i

aronszajn, n. (1950).
theory of reproducing kernels.
trans. amer. math. soc., 68:337   404.

carmeli, c., vito, e. d., toigo, a., and umanit`a, v. (2010).
vector valued reproducing kernel hilbert spaces and universality.
analysis and applications, 8:19   61.

kimeldorf, g. s. and wahba, g. (1971).
some results on tchebyche   an spline functions.
journal of mathematical analysis and applications, 33:82   95.

sch  olkopf, b., herbrich, r., and smola, a. j. (2001).
a generalized representer theorem.
in proc. of the 14th annual conference on learning theory, pages 416   426.

sch  olkopf, b. and smola, a. j. (2002).
learning with kernels.
mit press, cambridge, ma.

sriperumbudur, b. k., fukumizu, k., and lanckriet, g. r. g. (2011).
universality, characteristic kernels and rkhs embedding of measures.
journal of machine learning research, 12:2389   2410.

steinwart, i. and christmann, a. (2008).
support vector machines.
springer.

von luxburg, u. and bousquet, o. (2004).
distance-based classi   cation with lipschitz functions.
journal for machine learning research, 5:669   695.

wendland, h. (2005).
scattered data approximation.
cambridge university press, cambridge, uk.

suggested readings

machine learning

(cid:73) sch  olkopf, b. and smola, a. j. (2002). learning with kernels. mit press, cambridge, ma.
(cid:73) shawe-taylor, j. and cristianini, n. (2004). kernel methods for pattern analysis. cambridge university press, cambridge, uk.
(cid:73) shawe-taylor, j. and cristianini, n. (2000). an introduction to support vector machines. cambridge university press,

cambridge, uk.

learning theory

(cid:73) cucker, f. and zhou, d-x. (2007). learning theory: an approximation theory viewpoint. cambridge university press,
(cid:73) steinwart, i. and christmann, a. (2008). support vector machines. springer, ny.

cambridge, uk.

non-parametric statistics

(cid:73) berlinet, a. and thomas-agnan, c. (2004.) reproducing kernel hilbert spaces in id203 and statistics. kluwer academic
(cid:73) wahba, g. (1990). spline models for observational data. siam, philadelphia.

publishers, ma.

mathematics

(cid:73) paulsen, v. and raghupathi, m. (2016). an introduction to the theory of reproducing kernel hilbert spaces. cambridge

university press, cambridge, uk.

