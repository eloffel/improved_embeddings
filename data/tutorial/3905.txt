lecture 2

hilbert space embedding of id203 measures

bharath k. sriperumbudur

department of statistics, pennsylvania state university

machine learning summer school

t  ubingen, 2017

recap of lecture 1

kernel method provides an elegant approach to achieve non-linear
algorithms from linear algorithms.

(cid:73) input space, x : the space of observed data on which learning is

performed.

(cid:73) feature map,   : de   ned through a positive de   nite id81,

k : x    x     r

x (cid:55)      (x),

x     x

(cid:73) constructing linear algorithms in the feature space   (x ) translates

as non-linear algorithms in x .

(cid:73) elegance: no explicit construction of    as (cid:104)  (x),   (y )(cid:105) = k(x, y ).
(cid:73) function space view: rkhs; smoothness and generalization

examples

(cid:73) ridge regression. in fact many more

(kernel+id166/pca/fda/cca/id88/id28, ...)

outline

(cid:73) motivating example: comparing distributions
(cid:73) hilbert space embedding of measures

(cid:73) mean element
(cid:73) distance on probabilities (mmd)
(cid:73) characteristic kernels
(cid:73) cross-covariance operator and measure of independence

(cid:73) applications

(cid:73) two-sample testing

(cid:73) choice of kernel

co-authors

(cid:73) sivaraman balakrishnan (statistics, carnegie mellon university)
(cid:73) kenji fukumizu (institute for statistical mathematics, tokyo)
(cid:73) arthur gretton (gatsby unit, university college london)
(cid:73) gert lanckriet (electrical engineering, university of california, san diego)
(cid:73) krikamol muandet (mathematics, mahidol university, bangkok)
(cid:73) massimiliano pontil (computer science, university college london)
(cid:73) bernhard sch  olkopf (max planck institute for intelligent systems, t  ubingen)
(cid:73) dino sejdinovic (statistics, university of oxford)
(cid:73) heiko strathmann (gatsby unit, university college london)
(cid:73) ilya tolstikhin (max planck institute for intelligent systems, t  ubingen)

motivating example: coin toss

(cid:73) toss 1: t h h h t t h t t h h t h

(cid:73) toss 2: h t t h t h t t h h h t t

are the coins/tosses statistically similar?

toss 1 is a sample from p:=bernoulli(p) and toss 2 is a sample from
q:=bernoulli(q).

(cid:90)

{0,1}

ep[x ] =

is p = q or not?, i.e., compare

x dp(x)

and

eq[x ] =

(cid:90)

{0,1}

x dq(x).

motivating example: coin toss

(cid:73) toss 1: t h h h t t h t t h h t h

(cid:73) toss 2: h t t h t h t t h h h t t

are the coins/tosses statistically similar?

toss 1 is a sample from p:=bernoulli(p) and toss 2 is a sample from
q:=bernoulli(q).

(cid:90)

{0,1}

ep[x ] =

is p = q or not?, i.e., compare

x dp(x)

and

eq[x ] =

(cid:90)

{0,1}

x dq(x).

coin toss example

in other words, we compare

(cid:90)

r

  (x) dp(x)

and

(cid:90)

r

  (x) dq(x)

where    is an identity map,

  (x) = x.

a positive de   nite kernel corresponding to    is

k(x, y ) = (cid:104)  (x),   (y )(cid:105)2 = xy ,

which is a linear kernel on {0, 1}. therefore, comparing two bernoulli is

equivalent to (cid:90)

k(y , x) dp(x) ?=

k(y , x) dq(x)

{0,1}

for all y     {0, 1}, i.e., compare the expectations of the kernel.

(cid:90)

{0,1}

comparing two gaussians

p = n(  1,   2
1)

and

q = n(  2,   2
2)

comparing p and q is equivalent to comparing   1,   2 and   2

1,   2

2, i.e.,

(cid:90)

r

(cid:90)

r

ep[x ] =

x dp(x) ?=

ep[x 2] =

(cid:90)

x 2 dp(x) ?=

  (x) dp(x) ?=

r

(cid:90)
(cid:90)
(cid:90)

r

r

r

x dq(x) = eq[x ]

x 2 dq(x) = eq[x 2].

  (x) dq(x)

  (x) = (x, x 2).

compare the    rst moment of the feature map

and

concisely

where

comparing two gaussians

p = n(  1,   2
1)

and

q = n(  2,   2
2)

comparing p and q is equivalent to comparing   1,   2 and   2

1,   2

2, i.e.,

(cid:90)

r

(cid:90)

r

ep[x ] =

x dp(x) ?=

ep[x 2] =

(cid:90)

x 2 dp(x) ?=

  (x) dp(x) ?=

r

(cid:90)
(cid:90)
(cid:90)

r

r

r

x dq(x) = eq[x ]

x 2 dq(x) = eq[x 2].

  (x) dq(x)

  (x) = (x, x 2).

compare the    rst moment of the feature map

and

concisely

where

comparing two gaussians

using the map   , we can construct a positive de   nite kernel as

k(x, y ) = (cid:104)  (x),   (y )(cid:105)r2 = xy + x 2y 2

which is a polynomial kernel of order 2.

therefore, comparing two gaussians is equivalent to

(cid:90)

r

(cid:90)

r

k(y , x) dp(x) ?=

k(y , x) dq(x)

for all y     r, i.e., compare the expectations of the kernel.

comparing general p and q

moment generating function is de   ned as

(cid:90)

r

mp(y ) =

exy dp(x)

and (if it exists) captures the information about a distribution, i.e.,

mp = mq     p = q.

choosing

(cid:18)

  (x) =

1, x,

x 2   
2!

, . . . ,

x i   
i!

(cid:19)

, . . .

    (cid:96)2(n),     x     r

it is easy to verify that

k(x, y ) = (cid:104)  (x),   (y )(cid:105)(cid:96)2(n) = exy

and so (cid:90)

r

(cid:90)

r

k(x, y ) dp(x) =

k(x, y ) dq(x),     y     r     p = q.

two-sample problem

(cid:73) given random samples {x1, . . . , xm} i.i.d.    p and

{y1, . . . , yn} i.i.d.    q.

(cid:73) determine: p = q or p (cid:54)= q ?

applications:

(cid:73) microarray data (aggregation problem)
(cid:73) speaker veri   cation
(cid:73) independence testing: given random samples

{(x1, y1), . . . , (xn, yn)} i.i.d    pxy . does pxy factorize into pxpy ?

(cid:73) feature selection (microarrays, image and text,. . .)

hilbert space embedding of measures

hilbert space embedding of measures

(cid:73) canonical feature map:

  (x) = k(  , x)     h,

x     x

where h is a reproducing kernel hilbert space (rkhs).

(cid:73) generalization to probabilities:

x (cid:55)    k(  , x)

   

  x(cid:124)(cid:123)(cid:122)(cid:125)

point mass at x

(cid:55)   

(cid:82)

k(  , x)

(cid:124) (cid:123)(cid:122) (cid:125)

x k(  ,y ) d  x (y )=e  x [k(  ,y )]

based on the above, the map is extended to id203 measures as

(cid:90)

x

(cid:90)
(cid:124)

x

  (x) dp(x) =

k(  , x) dp(x)

(cid:123)(cid:122)

ex   pk(  ,x )

(cid:125)

p (cid:55)      p :=

(smola et al., alt 2007)

properties

(cid:73)   p is the mean of the feature map and is called the kernel mean or

mean element of p.

(cid:73) when is   p well de   ned?

(cid:90)

proof:

x

(cid:107)  p(cid:107)h =

(cid:112)k(x, x) dp(x) <           p     h
(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)
(cid:90)

k(  , x) dp(x)

(cid:13)(cid:13)(cid:13)(cid:13)h

jensen(cid:48)s   

(cid:28)

(cid:90)

(cid:90)

x

x

(cid:104)f , k(  , x)(cid:105)h dp(x)

   
=

f ,

(cid:107)k(  , x)(cid:107)h dp(x).

(cid:29)

h

k(  , x) dp(x)

x

(cid:73) we know that for any f     h, f (x) = (cid:104)f , k(  , x)(cid:105)h. so, for any

f     h,

(cid:90)

x

f (x) dp(x) =

x

= (cid:104)f ,   p(cid:105)h.

properties

(cid:73)   p is the mean of the feature map and is called the kernel mean or

mean element of p.

(cid:73) when is   p well de   ned?

(cid:90)

proof:

x

(cid:107)  p(cid:107)h =

(cid:112)k(x, x) dp(x) <           p     h
(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)
(cid:90)

k(  , x) dp(x)

(cid:13)(cid:13)(cid:13)(cid:13)h

jensen(cid:48)s   

(cid:28)

(cid:90)

(cid:90)

x

x

(cid:104)f , k(  , x)(cid:105)h dp(x)

   
=

f ,

(cid:107)k(  , x)(cid:107)h dp(x).

(cid:29)

h

k(  , x) dp(x)

x

(cid:73) we know that for any f     h, f (x) = (cid:104)f , k(  , x)(cid:105)h. so, for any

f     h,

(cid:90)

x

f (x) dp(x) =

x

= (cid:104)f ,   p(cid:105)h.

properties

(cid:73)   p is the mean of the feature map and is called the kernel mean or

mean element of p.

(cid:73) when is   p well de   ned?

(cid:90)

proof:

x

(cid:107)  p(cid:107)h =

(cid:112)k(x, x) dp(x) <           p     h
(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)
(cid:90)

k(  , x) dp(x)

(cid:13)(cid:13)(cid:13)(cid:13)h

jensen(cid:48)s   

(cid:28)

(cid:90)

(cid:90)

x

x

(cid:104)f , k(  , x)(cid:105)h dp(x)

   
=

f ,

(cid:107)k(  , x)(cid:107)h dp(x).

(cid:29)

h

k(  , x) dp(x)

x

(cid:73) we know that for any f     h, f (x) = (cid:104)f , k(  , x)(cid:105)h. so, for any

f     h,

(cid:90)

x

f (x) dp(x) =

x

= (cid:104)f ,   p(cid:105)h.

interpretation

suppose k is translation invariant on rd , i.e.,
k(x, y ) =   (x     y ), x, y     rd . then

(cid:90)

  p =

rd

  (       x) dp(x) =    (cid:63) p,

where (cid:63) is the convolution of    and p.

(cid:73) convolution is a smoothing operation       p is a smoothed version

of p.

(cid:73) example: suppose p =   y , a point mass at y . then

  p =    (cid:63) p =   (       y ).

(cid:73) example: suppose        n(0,   2) and p = n(  ,    2). then

  p =    (cid:63) p     n(  ,   2 +    2).
  p is a wider gaussian than p

interpretation

suppose k is translation invariant on rd , i.e.,
k(x, y ) =   (x     y ), x, y     rd . then

(cid:90)

  p =

rd

  (       x) dp(x) =    (cid:63) p,

where (cid:63) is the convolution of    and p.

(cid:73) convolution is a smoothing operation       p is a smoothed version

of p.

(cid:73) example: suppose p =   y , a point mass at y . then

  p =    (cid:63) p =   (       y ).

(cid:73) example: suppose        n(0,   2) and p = n(  ,    2). then

  p =    (cid:63) p     n(  ,   2 +    2).
  p is a wider gaussian than p

interpretation

suppose k is translation invariant on rd , i.e.,
k(x, y ) =   (x     y ), x, y     rd . then

(cid:90)

  p =

rd

  (       x) dp(x) =    (cid:63) p,

where (cid:63) is the convolution of    and p.

(cid:73) convolution is a smoothing operation       p is a smoothed version

of p.

(cid:73) example: suppose p =   y , a point mass at y . then

  p =    (cid:63) p =   (       y ).

(cid:73) example: suppose        n(0,   2) and p = n(  ,    2). then

  p =    (cid:63) p     n(  ,   2 +    2).
  p is a wider gaussian than p

comparing kernel means

de   ne a distance (maximum mean discrepancy) on probabilities

mmdh(p, q) = (cid:107)  p       q(cid:107)h

(cid:90)
(cid:90)

(cid:90)

x

x

x

=

=

=

(gretton et al., nips 2006; smola et al., alt 2007)

mmd 2

h(p, q) = (cid:104)  p,   p(cid:105)h + (cid:104)  q,   q(cid:105)h     2(cid:104)  p,   q(cid:105)h

  p(x) dp(x) +

  q(x) dq(x)     2

(cid:90)

x

(cid:90)

x

  p(x) dq(x)

(cid:90)

(cid:90)

x

x

k(x, y ) dp(x) dp(y ) +

k(x, y ) dq(x) dq(y )

    2

(cid:90)
(cid:123)(cid:122)

x
epk(x , x

x
)

(cid:48)

(cid:124)

(cid:90)
(cid:125)

k(x, y ) dp(x) dq(y )

avg. similarity between points from p
    2   
ep,qk(x , y )

(cid:124)

(cid:123)(cid:122)

avg. similarity between points from p and q

+

(cid:125)

eqk(y , y

(cid:123)(cid:122)

(cid:48)

(cid:125)

)

(cid:124)

.

avg. similarity between points from q

comparing kernel means

de   ne a distance (maximum mean discrepancy) on probabilities

mmdh(p, q) = (cid:107)  p       q(cid:107)h

(cid:90)
(cid:90)

(cid:90)

x

x

x

=

=

=

(gretton et al., nips 2006; smola et al., alt 2007)

mmd 2

h(p, q) = (cid:104)  p,   p(cid:105)h + (cid:104)  q,   q(cid:105)h     2(cid:104)  p,   q(cid:105)h

  p(x) dp(x) +

  q(x) dq(x)     2

(cid:90)

x

(cid:90)

x

  p(x) dq(x)

(cid:90)

(cid:90)

x

x

k(x, y ) dp(x) dp(y ) +

k(x, y ) dq(x) dq(y )

    2

(cid:90)
(cid:123)(cid:122)

x
epk(x , x

x
)

(cid:48)

(cid:124)

(cid:90)
(cid:125)

k(x, y ) dp(x) dq(y )

avg. similarity between points from p
    2   
ep,qk(x , y )

(cid:124)

(cid:123)(cid:122)

avg. similarity between points from p and q

+

(cid:125)

eqk(y , y

(cid:123)(cid:122)

(cid:48)

(cid:125)

)

(cid:124)

.

avg. similarity between points from q

comparing kernel means

de   ne a distance (maximum mean discrepancy) on probabilities

mmdh(p, q) = (cid:107)  p       q(cid:107)h

(cid:90)
(cid:90)

(cid:90)

x

x

x

=

=

=

(gretton et al., nips 2006; smola et al., alt 2007)

mmd 2

h(p, q) = (cid:104)  p,   p(cid:105)h + (cid:104)  q,   q(cid:105)h     2(cid:104)  p,   q(cid:105)h

  p(x) dp(x) +

  q(x) dq(x)     2

(cid:90)

x

(cid:90)

x

  p(x) dq(x)

(cid:90)

(cid:90)

x

x

k(x, y ) dp(x) dp(y ) +

k(x, y ) dq(x) dq(y )

    2

(cid:90)
(cid:123)(cid:122)

x
epk(x , x

x
)

(cid:48)

(cid:124)

(cid:90)
(cid:125)

k(x, y ) dp(x) dq(y )

avg. similarity between points from p
    2   
ep,qk(x , y )

(cid:124)

(cid:123)(cid:122)

avg. similarity between points from p and q

+

(cid:125)

eqk(y , y

(cid:123)(cid:122)

(cid:48)

(cid:125)

)

(cid:124)

.

avg. similarity between points from q

comparing kernel means

de   ne a distance (maximum mean discrepancy) on probabilities

mmdh(p, q) = (cid:107)  p       q(cid:107)h

(cid:90)
(cid:90)

(cid:90)

x

x

x

=

=

=

(gretton et al., nips 2006; smola et al., alt 2007)

mmd 2

h(p, q) = (cid:104)  p,   p(cid:105)h + (cid:104)  q,   q(cid:105)h     2(cid:104)  p,   q(cid:105)h

  p(x) dp(x) +

  q(x) dq(x)     2

(cid:90)

x

(cid:90)

x

  p(x) dq(x)

(cid:90)

(cid:90)

x

x

k(x, y ) dp(x) dp(y ) +

k(x, y ) dq(x) dq(y )

    2

(cid:90)
(cid:123)(cid:122)

x
epk(x , x

x
)

(cid:48)

(cid:124)

(cid:90)
(cid:125)

k(x, y ) dp(x) dq(y )

avg. similarity between points from p
    2   
ep,qk(x , y )

(cid:124)

(cid:123)(cid:122)

avg. similarity between points from p and q

+

(cid:125)

eqk(y , y

(cid:123)(cid:122)

(cid:48)

(cid:125)

)

(cid:124)

.

avg. similarity between points from q

comparing kernel means

de   ne a distance (maximum mean discrepancy) on probabilities

mmdh(p, q) = (cid:107)  p       q(cid:107)h

(cid:90)
(cid:90)

(cid:90)

x

x

x

=

=

=

(gretton et al., nips 2006; smola et al., alt 2007)

mmd 2

h(p, q) = (cid:104)  p,   p(cid:105)h + (cid:104)  q,   q(cid:105)h     2(cid:104)  p,   q(cid:105)h

  p(x) dp(x) +

  q(x) dq(x)     2

(cid:90)

x

(cid:90)

x

  p(x) dq(x)

(cid:90)

(cid:90)

x

x

k(x, y ) dp(x) dp(y ) +

k(x, y ) dq(x) dq(y )

    2

(cid:90)
(cid:123)(cid:122)

x
epk(x , x

x
)

(cid:48)

(cid:124)

(cid:90)
(cid:125)

k(x, y ) dp(x) dq(y )

avg. similarity between points from p
    2   
ep,qk(x , y )

(cid:124)

(cid:123)(cid:122)

avg. similarity between points from p and q

+

(cid:125)

eqk(y , y

(cid:123)(cid:122)

(cid:48)

(cid:125)

)

(cid:124)

.

avg. similarity between points from q

comparing kernel means

de   ne a distance (maximum mean discrepancy) on probabilities

mmdh(p, q) = (cid:107)  p       q(cid:107)h

(cid:90)
(cid:90)

(cid:90)

x

x

x

=

=

=

(gretton et al., nips 2006; smola et al., alt 2007)

mmd 2

h(p, q) = (cid:104)  p,   p(cid:105)h + (cid:104)  q,   q(cid:105)h     2(cid:104)  p,   q(cid:105)h

  p(x) dp(x) +

  q(x) dq(x)     2

(cid:90)

x

(cid:90)

x

  p(x) dq(x)

(cid:90)

(cid:90)

x

x

k(x, y ) dp(x) dp(y ) +

k(x, y ) dq(x) dq(y )

    2

(cid:90)
(cid:123)(cid:122)

x
epk(x , x

x
)

(cid:48)

(cid:124)

(cid:90)
(cid:125)

k(x, y ) dp(x) dq(y )

avg. similarity between points from p
    2   
ep,qk(x , y )

(cid:124)

(cid:123)(cid:122)

avg. similarity between points from p and q

+

(cid:125)

eqk(y , y

(cid:123)(cid:122)

(cid:48)

(cid:125)

)

(cid:124)

.

avg. similarity between points from q

comparing kernel means

in the motivating examples, we compare p and q by comparing

  p(y ) =

k(y , x) dp(x)

and   q(y ) =

k(y , x) dq(x),     y     x .

(cid:90)

x

(cid:90)

x

for any f     h,

(cid:107)f (cid:107)    = sup
y   x

|f (y )| = sup
y   x

|(cid:104)f , k(  , y )(cid:105)h|     sup
y   x

(cid:112)k(y , y )(cid:107)  p       q(cid:107)h.

(cid:107)  p       q(cid:107)        sup
y   x

does (cid:107)  p       q(cid:107)h = 0     p = q? (more on this later)

(cid:112)k(y , y )(cid:107)f (cid:107)h.

comparing kernel means

in the motivating examples, we compare p and q by comparing

  p(y ) =

k(y , x) dp(x)

and   q(y ) =

k(y , x) dq(x),     y     x .

(cid:90)

x

(cid:90)

x

for any f     h,

(cid:107)f (cid:107)    = sup
y   x

|f (y )| = sup
y   x

|(cid:104)f , k(  , y )(cid:105)h|     sup
y   x

(cid:112)k(y , y )(cid:107)  p       q(cid:107)h.

(cid:107)  p       q(cid:107)        sup
y   x

does (cid:107)  p       q(cid:107)h = 0     p = q? (more on this later)

(cid:112)k(y , y )(cid:107)f (cid:107)h.

comparing kernel means

in the motivating examples, we compare p and q by comparing

  p(y ) =

k(y , x) dp(x)

and   q(y ) =

k(y , x) dq(x),     y     x .

(cid:90)

x

(cid:90)

x

for any f     h,

(cid:107)f (cid:107)    = sup
y   x

|f (y )| = sup
y   x

|(cid:104)f , k(  , y )(cid:105)h|     sup
y   x

(cid:112)k(y , y )(cid:107)  p       q(cid:107)h.

(cid:107)  p       q(cid:107)        sup
y   x

does (cid:107)  p       q(cid:107)h = 0     p = q? (more on this later)

(cid:112)k(y , y )(cid:107)f (cid:107)h.

integral id203 metric

the integral id203 metric between p and q is de   ned as

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)

f (x) dp(x)    

x

x
|epf (x )     eqf (x )| .

f (x) dq(x)

ipm(p, q, f) := sup
f    f
= sup
f    f

(m  uller, 1997)

(cid:73) f controls the degree of distinguishability between p and q.
(cid:73) related to the bayes risk of a certain classi   cation problem (s et al.,

nips 2009; ejs 2012)

integral id203 metric

the integral id203 metric between p and q is de   ned as

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

f (x) dp(x)    

x

x
|epf (x )     eqf (x )| .

f (x) dq(x)

ipm(p, q, f) := sup
f    f
= sup
f    f

(m  uller, 1997)

(cid:73) f controls the degree of distinguishability between p and q.
(cid:73) related to the bayes risk of a certain classi   cation problem (s et al.,

nips 2009; ejs 2012)

(cid:73) example: suppose f = {a    x, x     r : a     [   1, 1]}. then

ipm(p, q, f) = sup
a   [   1,1]

|a|

x dp(x)    

x dq(x)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

r

(cid:90)

r

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

integral id203 metric

example: suppose f = {a    x + b    x 2, x     r : a2 + b2 = 1}. then

ipm(p, q, f) = sup

x d(p     q) + b

x 2 d(p     q)

(cid:90)

(cid:12)(cid:12)(cid:12)(cid:12)a

r

a2+b2=1

(cid:34)(cid:18)(cid:90)

r

(cid:90)

r

(cid:19)2

(cid:18)(cid:90)

r

x d(p     q)

+

x 2 d(p     q)

.

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:19)2(cid:35) 1

2

=

how? exercise!

(cid:73) the richer the f is, the    ner is the resolvability of p and q.

we will explore the relation of mmdh(p, q) to ipm(p, q, f).

integral id203 metric

ipm(p, q, f) := sup
f    f

classical results:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

x

(cid:90)

x

(cid:12)(cid:12)(cid:12)(cid:12)

f (x) dq(x)

f (x) dp(x)    

(cid:73) f = unit lipschitz ball (wasserstein distance) (dudley, 2002)
(cid:73) f = unit bounded-lipschitz ball (dudley metric) (dudley, 2002)
(cid:73) f = {1(      ,t] : t     rd} (kolmogorov metric) (m  uller, 1997)
(cid:73) f = unit ball in bounded measurable functions (total variation distance)

(dudley, 2002)

for all these f, ipm(p, q, f) = 0     p = q.

(gretton et al., nips 2006, jmlr 2012; s et al., colt 2008): f = unit ball in an
rkhs, h with bounded kernel, k. then

proof: (cid:82)

mmdh(p, q) = ipm(p, q, f).

x f (x) d(p     q)(x) = (cid:104)f ,   p       q(cid:105)h     (cid:107)f (cid:107)h(cid:107)  p       q(cid:107)h.

integral id203 metric

ipm(p, q, f) := sup
f    f

classical results:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

x

(cid:90)

x

(cid:12)(cid:12)(cid:12)(cid:12)

f (x) dq(x)

f (x) dp(x)    

(cid:73) f = unit lipschitz ball (wasserstein distance) (dudley, 2002)
(cid:73) f = unit bounded-lipschitz ball (dudley metric) (dudley, 2002)
(cid:73) f = {1(      ,t] : t     rd} (kolmogorov metric) (m  uller, 1997)
(cid:73) f = unit ball in bounded measurable functions (total variation distance)

(dudley, 2002)

for all these f, ipm(p, q, f) = 0     p = q.

(gretton et al., nips 2006, jmlr 2012; s et al., colt 2008): f = unit ball in an
rkhs, h with bounded kernel, k. then

proof: (cid:82)

mmdh(p, q) = ipm(p, q, f).

x f (x) d(p     q)(x) = (cid:104)f ,   p       q(cid:105)h     (cid:107)f (cid:107)h(cid:107)  p       q(cid:107)h.

integral id203 metric

ipm(p, q, f) := sup
f    f

classical results:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:90)

x

(cid:90)

x

(cid:12)(cid:12)(cid:12)(cid:12)

f (x) dq(x)

f (x) dp(x)    

(cid:73) f = unit lipschitz ball (wasserstein distance) (dudley, 2002)
(cid:73) f = unit bounded-lipschitz ball (dudley metric) (dudley, 2002)
(cid:73) f = {1(      ,t] : t     rd} (kolmogorov metric) (m  uller, 1997)
(cid:73) f = unit ball in bounded measurable functions (total variation distance)

(dudley, 2002)

for all these f, ipm(p, q, f) = 0     p = q.

(gretton et al., nips 2006, jmlr 2012; s et al., colt 2008): f = unit ball in an
rkhs, h with bounded kernel, k. then

proof: (cid:82)

mmdh(p, q) = ipm(p, q, f).

x f (x) d(p     q)(x) = (cid:104)f ,   p       q(cid:105)h     (cid:107)f (cid:107)h(cid:107)  p       q(cid:107)h.

two-sample problem

(cid:73) given random samples {x1, . . . , xm} i.i.d.    p and

{y1, . . . , yn} i.i.d.    q.

(cid:73) determine: p = q or p (cid:54)= q ?

(cid:73) approach: de   ne    to be a distance on probabilities
h0 :   (p, q) = 0
h1 :   (p, q) > 0

h0 : p = q
h1 : p (cid:54)= q

   

(cid:73) if empirical    is

(cid:73) far from zero: reject h0
(cid:73) close to zero: accept h0

two-sample problem

(cid:73) given random samples {x1, . . . , xm} i.i.d.    p and

{y1, . . . , yn} i.i.d.    q.

(cid:73) determine: p = q or p (cid:54)= q ?

(cid:73) approach: de   ne    to be a distance on probabilities
h0 :   (p, q) = 0
h1 :   (p, q) > 0

h0 : p = q
h1 : p (cid:54)= q

   

(cid:73) if empirical    is

(cid:73) far from zero: reject h0
(cid:73) close to zero: accept h0

two-sample problem

(cid:73) given random samples {x1, . . . , xm} i.i.d.    p and

{y1, . . . , yn} i.i.d.    q.

(cid:73) determine: p = q or p (cid:54)= q ?

(cid:73) approach: de   ne    to be a distance on probabilities
h0 :   (p, q) = 0
h1 :   (p, q) > 0

h0 : p = q
h1 : p (cid:54)= q

   

(cid:73) if empirical    is

(cid:73) far from zero: reject h0
(cid:73) close to zero: accept h0

why mmdh?

(cid:73) related to the estimation of ipm(p, q,f).
(cid:73) recall

mmd 2

h(p, q) =

k(  , x) dp(x)    

k(  , x) dq(x)

(cid:90)

x

(cid:80)m

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:80)n

.

h

(cid:73) a trivial approximation: pm := 1

m

i=1   xi and qn := 1

n

i=1   yi ,

where   x represents the dirac measure at x.

mmd 2

h(pm, qn) =

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

h

k(  , yi )

n(cid:88)
n(cid:88)

i=1

1
n2

i,j=1

k(yi , yj )     2

(cid:88)

i,j

k(xi , yj )

k(  , xi )     1
n

k(xi , xj ) +

v-statistic; biased estimator of mmd2
h

(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

m

x

m(cid:88)
m(cid:88)

i=1

i,j=1

=

1
m2

why mmdh?

(cid:73) related to the estimation of ipm(p, q,f).
(cid:73) recall

mmd 2

h(p, q) =

k(  , x) dp(x)    

k(  , x) dq(x)

(cid:90)

x

(cid:80)m

(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:80)n

.

h

(cid:73) a trivial approximation: pm := 1

m

i=1   xi and qn := 1

n

i=1   yi ,

where   x represents the dirac measure at x.

mmd 2

h(pm, qn) =

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

h

k(  , yi )

n(cid:88)
n(cid:88)

i=1

1
n2

i,j=1

k(yi , yj )     2

(cid:88)

i,j

k(xi , yj )

k(  , xi )     1
n

k(xi , xj ) +

v-statistic; biased estimator of mmd2
h

(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

m

x

m(cid:88)
m(cid:88)

i=1

i,j=1

=

1
m2

why mmdh?

(cid:73) ipm(pm, qn, f) is obtained by solving a linear program for f =

lipschitz and bounded lipschitz balls. (s et al., ejs 2012)

(cid:73) quality of approximation (s et al., ejs 2012)

(cid:73) for f = lipschitz and bounded lipschitz balls,
|ipm(pm, qm, f)     ipm(p, q, f)| = op

(cid:16)

(cid:17)

    1

d+1

m

, d > 2

(cid:73) for f = unit rkhs ball,

|mmdh(pm, qm)     mmdh(p, q)| = op

(cid:16)

    1

2

m

(cid:17)

(cid:73) are there any other estimators of mmdh(p, q) that are statistically

better than mmdh(pm, qm)? no!! (tolstikhin et al., 2016)

(cid:73) in practice? yes!! (krikamol et al., jmlr 2016; s, bernoulli 2016)

why mmdh?

(cid:73) ipm(pm, qn, f) is obtained by solving a linear program for f =

lipschitz and bounded lipschitz balls. (s et al., ejs 2012)

(cid:73) quality of approximation (s et al., ejs 2012)

(cid:73) for f = lipschitz and bounded lipschitz balls,
|ipm(pm, qm, f)     ipm(p, q, f)| = op

(cid:16)

(cid:17)

    1

d+1

m

, d > 2

(cid:73) for f = unit rkhs ball,

|mmdh(pm, qm)     mmdh(p, q)| = op

(cid:16)

    1

2

m

(cid:17)

(cid:73) are there any other estimators of mmdh(p, q) that are statistically

better than mmdh(pm, qm)? no!! (tolstikhin et al., 2016)

(cid:73) in practice? yes!! (krikamol et al., jmlr 2016; s, bernoulli 2016)

why mmdh?

(cid:73) ipm(pm, qn, f) is obtained by solving a linear program for f =

lipschitz and bounded lipschitz balls. (s et al., ejs 2012)

(cid:73) quality of approximation (s et al., ejs 2012)

(cid:73) for f = lipschitz and bounded lipschitz balls,
|ipm(pm, qm, f)     ipm(p, q, f)| = op

(cid:16)

(cid:17)

    1

d+1

m

, d > 2

(cid:73) for f = unit rkhs ball,

|mmdh(pm, qm)     mmdh(p, q)| = op

(cid:16)

    1

2

m

(cid:17)

(cid:73) are there any other estimators of mmdh(p, q) that are statistically

better than mmdh(pm, qm)? no!! (tolstikhin et al., 2016)

(cid:73) in practice? yes!! (krikamol et al., jmlr 2016; s, bernoulli 2016)

why mmdh?

(cid:73) ipm(pm, qn, f) is obtained by solving a linear program for f =

lipschitz and bounded lipschitz balls. (s et al., ejs 2012)

(cid:73) quality of approximation (s et al., ejs 2012)

(cid:73) for f = lipschitz and bounded lipschitz balls,
|ipm(pm, qm, f)     ipm(p, q, f)| = op

(cid:16)

(cid:17)

    1

d+1

m

, d > 2

(cid:73) for f = unit rkhs ball,

|mmdh(pm, qm)     mmdh(p, q)| = op

(cid:16)

    1

2

m

(cid:17)

(cid:73) are there any other estimators of mmdh(p, q) that are statistically

better than mmdh(pm, qm)? no!! (tolstikhin et al., 2016)

(cid:73) in practice? yes!! (krikamol et al., jmlr 2016; s, bernoulli 2016)

beware of pitfalls

(cid:73) there are many other distances on probabilities:

(cid:73) total variation distance
(cid:73) hellinger distance
(cid:73) id181 and its variants
(cid:73) fisher divergence ...

(cid:73) estimating these distances is both computationally and statistically

di   cult.

(cid:73) mmdh is computationally simpler and appears statistically powerful

with no curse of dimensionality. in fact, it is not statistically
powerful. (ramdas et al., aaai 2015; s, bernoulli, 2016)

(cid:73) recall: mmdh is based on   p which is a smoothed version of p. even though p

and q can be distinguished (coming up!!) based on   p and   q, the
distinguishability is weak compared to that of the above distances. (s et al.,
jmlr 2010; s, bernoulli, 2016)

no free lunch!!

beware of pitfalls

(cid:73) there are many other distances on probabilities:

(cid:73) total variation distance
(cid:73) hellinger distance
(cid:73) id181 and its variants
(cid:73) fisher divergence ...

(cid:73) estimating these distances is both computationally and statistically

di   cult.

(cid:73) mmdh is computationally simpler and appears statistically powerful

with no curse of dimensionality. in fact, it is not statistically
powerful. (ramdas et al., aaai 2015; s, bernoulli, 2016)

(cid:73) recall: mmdh is based on   p which is a smoothed version of p. even though p

and q can be distinguished (coming up!!) based on   p and   q, the
distinguishability is weak compared to that of the above distances. (s et al.,
jmlr 2010; s, bernoulli, 2016)

no free lunch!!

beware of pitfalls

(cid:73) there are many other distances on probabilities:

(cid:73) total variation distance
(cid:73) hellinger distance
(cid:73) id181 and its variants
(cid:73) fisher divergence ...

(cid:73) estimating these distances is both computationally and statistically

di   cult.

(cid:73) mmdh is computationally simpler and appears statistically powerful

with no curse of dimensionality. in fact, it is not statistically
powerful. (ramdas et al., aaai 2015; s, bernoulli, 2016)

(cid:73) recall: mmdh is based on   p which is a smoothed version of p. even though p

and q can be distinguished (coming up!!) based on   p and   q, the
distinguishability is weak compared to that of the above distances. (s et al.,
jmlr 2010; s, bernoulli, 2016)

no free lunch!!

so far . . .

(cid:73) computation
(cid:73) estimation

(cid:90)

x

p (cid:55)      p :=

k(  , x) dp(x)

mmdh(p, q) = (cid:107)  p       q(cid:107)h

when is p (cid:55)      p one-to-one?, i.e., mmdh(p, q) = 0     p = q?

characteristic kernel

k is said to be characteristic if

mmdh(p, q) = 0     p = q

for any p and q.
not all kernels are characteristic.

(cid:73) example: if k(x, y ) = c > 0,     x, y     x , then

(cid:90)

  p =

k(  , x) dp(x) = c,   q = c

x
and mmdh(p, q) = 0,     p, q.

(cid:73) example: let k(x, y ) = xy , x, y     r. then

mmdh(p, q) = |ep[x ]     eq[x ]|.

characteristic for bernoulli   s but not for all p and q.

(cid:73) example: let k(x, y ) = (1 + xy )2, x, y     r. then

mmd2

h(p, q) = 2(ep[x ]     eq[x ])2 + (ep[x 2]     eq[x 2]).

characteristic for gaussian   s but not for all p and q.

characteristic kernel

k is said to be characteristic if

mmdh(p, q) = 0     p = q

for any p and q.
not all kernels are characteristic.

(cid:73) example: if k(x, y ) = c > 0,     x, y     x , then

(cid:90)

  p =

k(  , x) dp(x) = c,   q = c

x
and mmdh(p, q) = 0,     p, q.

(cid:73) example: let k(x, y ) = xy , x, y     r. then

mmdh(p, q) = |ep[x ]     eq[x ]|.

characteristic for bernoulli   s but not for all p and q.

(cid:73) example: let k(x, y ) = (1 + xy )2, x, y     r. then

mmd2

h(p, q) = 2(ep[x ]     eq[x ])2 + (ep[x 2]     eq[x 2]).

characteristic for gaussian   s but not for all p and q.

characteristic kernel

k is said to be characteristic if

mmdh(p, q) = 0     p = q

for any p and q.
not all kernels are characteristic.

(cid:73) example: if k(x, y ) = c > 0,     x, y     x , then

(cid:90)

  p =

k(  , x) dp(x) = c,   q = c

x
and mmdh(p, q) = 0,     p, q.

(cid:73) example: let k(x, y ) = xy , x, y     r. then

mmdh(p, q) = |ep[x ]     eq[x ]|.

characteristic for bernoulli   s but not for all p and q.

(cid:73) example: let k(x, y ) = (1 + xy )2, x, y     r. then

mmd2

h(p, q) = 2(ep[x ]     eq[x ])2 + (ep[x 2]     eq[x 2]).

characteristic for gaussian   s but not for all p and q.

characteristic kernel

k is said to be characteristic if

mmdh(p, q) = 0     p = q

for any p and q.
not all kernels are characteristic.

(cid:73) example: if k(x, y ) = c > 0,     x, y     x , then

(cid:90)

  p =

k(  , x) dp(x) = c,   q = c

x
and mmdh(p, q) = 0,     p, q.

(cid:73) example: let k(x, y ) = xy , x, y     r. then

mmdh(p, q) = |ep[x ]     eq[x ]|.

characteristic for bernoulli   s but not for all p and q.

(cid:73) example: let k(x, y ) = (1 + xy )2, x, y     r. then

mmd2

h(p, q) = 2(ep[x ]     eq[x ])2 + (ep[x 2]     eq[x 2]).

characteristic for gaussian   s but not for all p and q.

characteristic kernels on rd

(cid:73) translation invariant kernel: k(x, y ) =   (x     y ), x, y     rd ;

bounded and continuous.

(cid:73) bochner   s theorem:

  (x) =

(cid:90)

rd

      1(cid:104)x,  (cid:105)2 d  (  ), x     rd ,

e

where    is a non-negative    nite borel measure on rd .

then, k is characteristic     supp(  ) = rd . (s et al., colt 2008; jmlr,

2010)

(cid:73) corollary: compactly supported    are characteristic (s et al., colt

2008; jmlr, 2010).

key idea: fourier representation of mmdh

fourier representation of mmd 2
h

(cid:90)

rd

mmd 2

h(p, q) =

|  p(  )       q(  )|2 d  (  )

where   p is the characteristic function of p.

proof:

mmd2

h(p, q) =
(   )
=

(   )
=

=

(cid:90)
(cid:90)
(cid:90)

rd

rd

(cid:90)
(cid:90)
(cid:90)
(cid:90)

rd

rd

rd

rd

  (x     y ) d(p     q)(x) d(p     q)(y )

(cid:90)
e         1(cid:104)x,  (cid:105) d(p     q)(x)

rd

(cid:90)

e         1(cid:104)x   y ,  (cid:105) d  (  ) d(p     q)(x) d(p     q)(y )

      1(cid:104)y ,  (cid:105) d(p     q)(y ) d  (  )

e

rd

rd
|  p(  )       q(  )|2 d  (  ),

where bochner   s theorem is used in (   ) and fubini   s theorem in (   ).

(cid:73) suppose    = 1, i.e., uniform on rd (!!). then mmdh(p, q) is the

l2 distance between the densities (if they exist) of p and q.

fourier representation of mmd 2
h

(cid:90)

rd

mmd 2

h(p, q) =

|  p(  )       q(  )|2 d  (  )

where   p is the characteristic function of p.

proof:

mmd2

h(p, q) =
(   )
=

(   )
=

=

(cid:90)
(cid:90)
(cid:90)

rd

rd

(cid:90)
(cid:90)
(cid:90)
(cid:90)

rd

rd

rd

rd

  (x     y ) d(p     q)(x) d(p     q)(y )

(cid:90)
e         1(cid:104)x,  (cid:105) d(p     q)(x)

rd

(cid:90)

e         1(cid:104)x   y ,  (cid:105) d  (  ) d(p     q)(x) d(p     q)(y )

      1(cid:104)y ,  (cid:105) d(p     q)(y ) d  (  )

e

rd

rd
|  p(  )       q(  )|2 d  (  ),

where bochner   s theorem is used in (   ) and fubini   s theorem in (   ).

(cid:73) suppose    = 1, i.e., uniform on rd (!!). then mmdh(p, q) is the

l2 distance between the densities (if they exist) of p and q.

characteristic kernels on rd

proof:

(cid:73) suppose supp(  ) = rd . then

(cid:90)

rd

mmd 2

h(p, q) = 0    

|  p(  )       q(  )|2 d  (  ) = 0       p =   q a.e.

but characteristic functions are uniformly continuous and so
  p =   q which implies p = q.

(cid:73) suppose supp(  ) (cid:40) rd . then there exists an open set u (cid:40) rd such
that   (u) = 0. construct p and q such that   p and   q di   er only
in u, i.e., mmdh(p, q) > 0.

(cid:73) if    is compactly supported, its fourier transform is analytic, i.e.,

cannot vanish on an interval.

characteristic kernels on rd

proof:

(cid:73) suppose supp(  ) = rd . then

(cid:90)

rd

mmd 2

h(p, q) = 0    

|  p(  )       q(  )|2 d  (  ) = 0       p =   q a.e.

but characteristic functions are uniformly continuous and so
  p =   q which implies p = q.

(cid:73) suppose supp(  ) (cid:40) rd . then there exists an open set u (cid:40) rd such
that   (u) = 0. construct p and q such that   p and   q di   er only
in u, i.e., mmdh(p, q) > 0.

(cid:73) if    is compactly supported, its fourier transform is analytic, i.e.,

cannot vanish on an interval.

characteristic kernels on rd

proof:

(cid:73) suppose supp(  ) = rd . then

(cid:90)

rd

mmd 2

h(p, q) = 0    

|  p(  )       q(  )|2 d  (  ) = 0       p =   q a.e.

but characteristic functions are uniformly continuous and so
  p =   q which implies p = q.

(cid:73) suppose supp(  ) (cid:40) rd . then there exists an open set u (cid:40) rd such
that   (u) = 0. construct p and q such that   p and   q di   er only
in u, i.e., mmdh(p, q) > 0.

(cid:73) if    is compactly supported, its fourier transform is analytic, i.e.,

cannot vanish on an interval.

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

f   

f   

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

f   

f   

characteristic function di   erence

(cid:38)
(cid:37)

   30   20   10010203000.050.10.150.2  |  p       q|translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

gaussian kernel
|  p       q|

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

characteristic

picture credit: a. gretton

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

sinc kernel
|  p       q|

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

not characteristic

picture credit: a. gretton

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

b-spline kernel
|  p       q|

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

???

translation invariant kernels on rd

mmdh(p, q) = (cid:107)  p       q(cid:107)l2(rd ,  )

(cid:73) example: p di   ers from q at (roughly) one frequency

characteristic

picture credit: a. gretton

caution

chararacteristic property relates class of kernels and class of probabilities.

   := supp(  )

(s et al., colt 2008; jmlr 2010)

measuring (in)dependence

(cid:73) let x and y be gaussian random variables on r. then

x and y are independent     cov(x , y ) = e(xy )   e(x )e(y ) = 0

(cid:73) in general, cov(x , y ) = 0 (cid:59) x     y .
(cid:73) covariance captures the linear relationship between x and y .
(cid:73) feature space view point: how about cov(  (x ),   (y ))?
(cid:73) suppose

  (x ) = (1, x , x 2) and   (y ) = (1, y , y 2, y 3).

then cov(  (x ),   (y )) captures cov(x i , y j ) for i     {0, 1, 2} and
j     {0, 1, 2, 3}.

measuring (in)dependence

(cid:73) let x and y be gaussian random variables on r. then

x and y are independent     cov(x , y ) = e(xy )   e(x )e(y ) = 0

(cid:73) in general, cov(x , y ) = 0 (cid:59) x     y .
(cid:73) covariance captures the linear relationship between x and y .
(cid:73) feature space view point: how about cov(  (x ),   (y ))?
(cid:73) suppose

  (x ) = (1, x , x 2) and   (y ) = (1, y , y 2, y 3).

then cov(  (x ),   (y )) captures cov(x i , y j ) for i     {0, 1, 2} and
j     {0, 1, 2, 3}.

measuring (in)dependence

(cid:73) characterization of independence:

x     y     cov(f (x ), g (y )) = 0,     measurable functions f and g .

(cid:73) dependence measure:

|cov(f (x ), g (y ))| = sup

|e[f (x )g (y )]     e[f (x )]e[g (y )]|

f ,g

sup
f ,g

similar to the ipm between pxy and px py .

(cid:73) restricting functions in rkhs: (constrained covariance)

coco(pxy ; hx , hy ) := sup

(cid:107)f (cid:107)hx =1
(cid:107)g(cid:107)hy =1

|e[f (x )g (y )]     e[f (x )]e[g (y )]| .

(gretton et al., aistats 2005, jmlr 2005)

measuring (in)dependence

(cid:73) characterization of independence:

x     y     cov(f (x ), g (y )) = 0,     measurable functions f and g .

(cid:73) dependence measure:

|cov(f (x ), g (y ))| = sup

|e[f (x )g (y )]     e[f (x )]e[g (y )]|

f ,g

sup
f ,g

similar to the ipm between pxy and px py .

(cid:73) restricting functions in rkhs: (constrained covariance)

coco(pxy ; hx , hy ) := sup

(cid:107)f (cid:107)hx =1
(cid:107)g(cid:107)hy =1

|e[f (x )g (y )]     e[f (x )]e[g (y )]| .

(gretton et al., aistats 2005, jmlr 2005)

covariance operator

let kx and ky be the r.k.   s of hx and hy respectively. then
(cid:73) e[f (x )] = (cid:104)f ,   px(cid:105)hx and e[g (y )] = (cid:104)g ,   py (cid:105)hy
(cid:73)

e[f (x )]e[g (y )] = (cid:104)f ,   px(cid:105)hx(cid:104)g ,   py (cid:105)hy

= (cid:104)f     g ,   px       py (cid:105)hx   hy
= (cid:104)f , (  px       py )g(cid:105)hx
= (cid:104)g , (  py       px )f (cid:105)hy

(cid:73)

e[f (x )g (y )] = e[(cid:104)f , kx (  , x )(cid:105)hx(cid:104)g , ky (  , y )(cid:105)hy ]

= e[(cid:104)f     g , kx (  , x )     ky (  , y )(cid:105)hx   hy
= e[(cid:104)f , (kx (  , x )     ky (  , y ))g(cid:105)hx
= e[(cid:104)g , (ky (  , y )     kx (  , x ))f (cid:105)hy

]
]

]

covariance operator

let kx and ky be the r.k.   s of hx and hy respectively. then
(cid:73) e[f (x )] = (cid:104)f ,   px(cid:105)hx and e[g (y )] = (cid:104)g ,   py (cid:105)hy
(cid:73)

e[f (x )]e[g (y )] = (cid:104)f ,   px(cid:105)hx(cid:104)g ,   py (cid:105)hy

= (cid:104)f     g ,   px       py (cid:105)hx   hy
= (cid:104)f , (  px       py )g(cid:105)hx
= (cid:104)g , (  py       px )f (cid:105)hy

(cid:73)

e[f (x )g (y )] = e[(cid:104)f , kx (  , x )(cid:105)hx(cid:104)g , ky (  , y )(cid:105)hy ]

= e[(cid:104)f     g , kx (  , x )     ky (  , y )(cid:105)hx   hy
= e[(cid:104)f , (kx (  , x )     ky (  , y ))g(cid:105)hx
= e[(cid:104)g , (ky (  , y )     kx (  , x ))f (cid:105)hy

]
]

]

covariance operator

(cid:73) assuming e(cid:112)kx (x , x )ky (y , y ) <    , we obtain

e[f (x )g (y )] = (cid:104)f , e[kx (  , x )     ky (  , y )]g(cid:105)hx
= (cid:104)g , e[ky (  , y )     kx (  , x )]f (cid:105)hy

cov(f (x ), g (y )) = (cid:104)f , cx y g(cid:105)hx

= (cid:104)g , cy x f (cid:105)hy

cx y := e[kx (  , x )     ky (  , y )]       px       py

(cid:73)

where

is a cross-covariance operator from hy to hx and cy x = c   

x y .

compare to the feature space view point with canonical feature maps

dependence measures

(cid:73)

coco(pxy ; hx , hy ) = sup

|(cid:104)f , cx y g(cid:105)hx

|

(cid:107)f (cid:107)hx =1
(cid:107)g(cid:107)hy =1

= (cid:107)cx y(cid:107)op = (cid:107)cy x(cid:107)op,

which is the maximum singular value of cx y .

(cid:73) choosing kx (  , x ) = (cid:104)  , x(cid:105)2 and ky (  , y ) = (cid:104)  , y(cid:105)2, for gaussian

distributions,

(cid:73) in general,

x     y     cy x = 0

x     y ?    cy x = 0.

dependence measures

(cid:73)

coco(pxy ; hx , hy ) = sup

|(cid:104)f , cx y g(cid:105)hx

|

(cid:107)f (cid:107)hx =1
(cid:107)g(cid:107)hy =1

= (cid:107)cx y(cid:107)op = (cid:107)cy x(cid:107)op,

which is the maximum singular value of cx y .

(cid:73) choosing kx (  , x ) = (cid:104)  , x(cid:105)2 and ky (  , y ) = (cid:104)  , y(cid:105)2, for gaussian

distributions,

(cid:73) in general,

x     y     cy x = 0

x     y ?    cy x = 0.

dependence measures

(cid:73) how about we consider other singular values?
(cid:73) how about (cid:107)cy x(cid:107)2

hs , which is the sum of squared singular values of

cy x ?

hilbert-schmidt independence criterion (hsic) (gretton et al., alt

(cid:73) (cid:107)cy x(cid:107)op     (cid:107)cy x(cid:107)hs

2005, jmlr 2005)

dependence measures

(cid:73)

coco(pxy ; hx , hy ) := sup

(cid:107)f (cid:107)hx =1
(cid:107)g(cid:107)hy =1

|e[f (x )g (y )]     e[f (x )]e[g (y )]| .

(cid:73) how about we use di   erent constraint, i.e., (cid:107)f     g(cid:107)hx   hy     1?
(cid:104)f , cx y g(cid:105)hx
(cid:104)f     g , cx y(cid:105)hx   hy

(cid:107)f    g(cid:107)hx    hy    1

(cid:107)f    g(cid:107)hx    hy    1

cov(f (x ), g (y )) =

sup

sup

sup

(cid:107)f    g(cid:107)hx    hy    1

=
= (cid:107)cx y(cid:107)hx   hy = (cid:107)cx y(cid:107)hs

(cid:73)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)

(cid:107)cx y(cid:107)hx   hy = (cid:107)e[kx (  , x )     ky (  , y )]       px       px(cid:107)hx   hy
kx (  , x )     ky (  , y ) d(px y     px    py )

=
= mmdhx   hy (px y , px    py )

(cid:13)(cid:13)(cid:13)(cid:13)hx   hy

dependence measures

(cid:73)

coco(pxy ; hx , hy ) := sup

(cid:107)f (cid:107)hx =1
(cid:107)g(cid:107)hy =1

|e[f (x )g (y )]     e[f (x )]e[g (y )]| .

(cid:73) how about we use di   erent constraint, i.e., (cid:107)f     g(cid:107)hx   hy     1?
(cid:104)f , cx y g(cid:105)hx
(cid:104)f     g , cx y(cid:105)hx   hy

(cid:107)f    g(cid:107)hx    hy    1

(cid:107)f    g(cid:107)hx    hy    1

cov(f (x ), g (y )) =

sup

sup

sup

(cid:107)f    g(cid:107)hx    hy    1

=
= (cid:107)cx y(cid:107)hx   hy = (cid:107)cx y(cid:107)hs

(cid:73)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)

(cid:107)cx y(cid:107)hx   hy = (cid:107)e[kx (  , x )     ky (  , y )]       px       px(cid:107)hx   hy
kx (  , x )     ky (  , y ) d(px y     px    py )

=
= mmdhx   hy (px y , px    py )

(cid:13)(cid:13)(cid:13)(cid:13)hx   hy

dependence measures

(cid:73)

coco(pxy ; hx , hy ) := sup

(cid:107)f (cid:107)hx =1
(cid:107)g(cid:107)hy =1

|e[f (x )g (y )]     e[f (x )]e[g (y )]| .

(cid:73) how about we use di   erent constraint, i.e., (cid:107)f     g(cid:107)hx   hy     1?
(cid:104)f , cx y g(cid:105)hx
(cid:104)f     g , cx y(cid:105)hx   hy

(cid:107)f    g(cid:107)hx    hy    1

(cid:107)f    g(cid:107)hx    hy    1

cov(f (x ), g (y )) =

sup

sup

sup

(cid:107)f    g(cid:107)hx    hy    1

=
= (cid:107)cx y(cid:107)hx   hy = (cid:107)cx y(cid:107)hs

(cid:73)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:90)

(cid:107)cx y(cid:107)hx   hy = (cid:107)e[kx (  , x )     ky (  , y )]       px       px(cid:107)hx   hy
kx (  , x )     ky (  , y ) d(px y     px    py )

=
= mmdhx   hy (px y , px    py )

(cid:13)(cid:13)(cid:13)(cid:13)hx   hy

dependence measures

(cid:73) hx     hy is an rkhs with kernel kx ky .
(cid:73) if kx ky is characteristic, then

(cid:107)cx y(cid:107)hx   hy = 0     px y = px    py     x     y

(cid:73) if kx and ky are characteristic, then

(cid:107)cx y(cid:107)hs = 0     x     y .

(gretton, 2015)

(cid:73) using the reproducing property,

(cid:107)cx y(cid:107)2

hs = ex y ex (cid:48)y (cid:48)kx (x , x (cid:48))ky (y , y (cid:48))

+ex x (cid:48)kx (x , x (cid:48))eyy (cid:48)ky (y , y (cid:48))

   2    ex (cid:48)y (cid:48) [ex kx (x , x (cid:48))ey ky (y , y (cid:48))]

(cid:73) can be estimated using a v-statistic (empirical sums).

applications

(cid:73) two-sample testing
(cid:73) independence testing
(cid:73) conditional independence testing
(cid:73) supervised id84
(cid:73) kernel bayes rule (   ltering, prediction and smoothing)
(cid:73) kernel cca,....

review paper (muandet et al., 2016)

application: two-sample testing

two-sample problem

(cid:73) given random samples {x1, . . . , xm} i.i.d.    p and

{y1, . . . , yn} i.i.d.    q.

(cid:73) determine: p = q or p (cid:54)= q ?

(cid:73) approach:

h0 : p = q
h1 : p (cid:54)= q

   

h0 : mmdh(p, q) = 0
h1 : mmdh(p, q) > 0

(cid:73) if mmd 2

h(pm, qn) is

(cid:73) far from zero: reject h0
(cid:73) close to zero: accept h0

type-i and type-ii errors

(cid:73) given p = q, want threshold or critical value t1      such that

prh0(mmd 2

h(pm, qn) > t1     )       .

statistical test: large deviation bounds

(cid:73) given p = q, want threshold t such that

prh0(mmd 2

h(pm, qn) > t)       .
(cid:73) we showed that (s et al., ejs 2012)

(cid:16)(cid:12)(cid:12)mmd 2

pr

h(pm, qn)     mmd 2

h(p, q)(cid:12)(cid:12)
(cid:113)
(cid:16)

1 +

2 log 1
  

(cid:32)

(cid:114)

mn

   (cid:113) 2(m+n)
(cid:114)

(cid:17)(cid:17)       .
(cid:33)

1
  

mmd 2

h(pm, qn) <

2(m + n)

mn

1 +

2 log

(cid:73)   -level test: accept h0 if

otherwise reject.

too conservative!!

statistical test: large deviation bounds

(cid:73) given p = q, want threshold t such that

prh0(mmd 2

h(pm, qn) > t)       .
(cid:73) we showed that (s et al., ejs 2012)

(cid:16)(cid:12)(cid:12)mmd 2

pr

h(pm, qn)     mmd 2

h(p, q)(cid:12)(cid:12)
(cid:113)
(cid:16)

1 +

2 log 1
  

(cid:32)

(cid:114)

mn

   (cid:113) 2(m+n)
(cid:114)

(cid:17)(cid:17)       .
(cid:33)

1
  

mmd 2

h(pm, qn) <

2(m + n)

mn

1 +

2 log

(cid:73)   -level test: accept h0 if

otherwise reject.

too conservative!!

statistical test: asymptotic distribution (gretton et al., nips 2006,

jmlr 2012)

unbiased estimator of mmd 2

(cid:92)
mmd 2

h :=

1

m(m     1)

m(cid:88)

i(cid:54)=j

(cid:124)

h(p, q): u-statistic
k(xi , xj ) + k(yi , yj )     k(xi , yj )     k(xj , yi )

(cid:123)(cid:122)

(cid:125)

h((xi ,yi ),(xj ,yj ))

(cid:73) under h0,

   (cid:88)

(cid:0)  2
i     2(cid:1)

  i

(cid:92)
mmd 2
h

w   

m

as n        ,

where   i     n (0, 2) i.i.d., and   i are solutions to

  i (x) dp(x) =   i   i (y )

i=1

(cid:90)

x

(cid:101)k(x, y )
(cid:124) (cid:123)(cid:122) (cid:125)

centered

(cid:73) consistent (type-ii error goes to zero): under h1,

(cid:16) (cid:92)

mmd 2

   

m

h     mmd 2

h(p, q)

as n        .

(cid:17) w    n (0,   2

h)

statistical test: asymptotic distribution (gretton et al., nips 2006,

jmlr 2012)

unbiased estimator of mmd 2

(cid:92)
mmd 2

h :=

1

m(m     1)

m(cid:88)

i(cid:54)=j

(cid:124)

h(p, q): u-statistic
k(xi , xj ) + k(yi , yj )     k(xi , yj )     k(xj , yi )

(cid:123)(cid:122)

(cid:125)

h((xi ,yi ),(xj ,yj ))

(cid:73) under h0,

   (cid:88)

(cid:0)  2
i     2(cid:1)

  i

(cid:92)
mmd 2
h

w   

m

as n        ,

where   i     n (0, 2) i.i.d., and   i are solutions to

  i (x) dp(x) =   i   i (y )

i=1

(cid:90)

x

(cid:101)k(x, y )
(cid:124) (cid:123)(cid:122) (cid:125)

centered

(cid:73) consistent (type-ii error goes to zero): under h1,

(cid:16) (cid:92)

mmd 2

   

m

h     mmd 2

h(p, q)

as n        .

(cid:17) w    n (0,   2

h)

statistical test: asymptotic distribution (gretton et al., nips 2006,

jmlr 2012)

unbiased estimator of mmd 2

(cid:92)
mmd 2

h :=

1

m(m     1)

m(cid:88)

i(cid:54)=j

(cid:124)

h(p, q): u-statistic
k(xi , xj ) + k(yi , yj )     k(xi , yj )     k(xj , yi )

(cid:123)(cid:122)

(cid:125)

h((xi ,yi ),(xj ,yj ))

(cid:73) under h0,

   (cid:88)

(cid:0)  2
i     2(cid:1)

  i

(cid:92)
mmd 2
h

w   

m

as n        ,

where   i     n (0, 2) i.i.d., and   i are solutions to

  i (x) dp(x) =   i   i (y )

i=1

(cid:90)

x

(cid:101)k(x, y )
(cid:124) (cid:123)(cid:122) (cid:125)

centered

(cid:73) consistent (type-ii error goes to zero): under h1,

(cid:16) (cid:92)

mmd 2

   

m

h     mmd 2

h(p, q)

as n        .

(cid:17) w    n (0,   2

h)

statistical test: asymptotic distribution (gretton et al., nips 2006,

jmlr 2012)

(cid:73)   -level test: estimate 1        quantile of the null distribution using

bootstrap.

computationally intensive!!

picture credit: a. gretton

statistical test without bootstrap (gretton et al., nips 2009)

(cid:73) estimate the eigenvalues,   i from combined samples

(cid:73) de   ne z := (x1, . . . , xm, y1, . . . , ym)
(cid:73) kij := k(zi , zj )

(cid:73) compute the eigenvalues, (cid:98)  i of(cid:101)k = hkh

where h = i     1

2m 12m1t

2m

(cid:73)   -level test: compute the 1        quantile of the distribution

associated with

2m(cid:88)

(cid:0)  2
i     2(cid:1)

(cid:98)  i

(cid:73) test is asymptotically   -level consistent

i=1

experiments (gretton et al., nips 2009)

(cid:73) comparison example: canadian hansard corpus (agriculture,

   sheries and immigration)

(cid:73) samples: 5-line extracts
(cid:73) kernel: k-spectrum kernel with k = 10
(cid:73) sample size: 10
(cid:73) repetitions: 300
(cid:92)
mmd 2
h

(cid:73) compute

k-spectrum kernel: average type ii error 0 (   = 0.05)

bag of words kernel: average type ii error 0.18

first ever test on structured data

choice of characteristic kernel

choice of characteristic kernels

let x = rd . suppose k is a gaussian kernel, k  (x, y ) = e
(cid:73) mmdh   is a function of   .
(cid:73) so mmdh   is a family of metrics. which one should we use in

2  2

.

    (cid:107)x   y(cid:107)2

2

practice?

(cid:73) note that mmdh       0 as        0 or           .
therefore, the kernel choice is very critical in applications.

heuristics:

(cid:73) median:    = median(cid:0)(cid:107)x    

j (cid:107)2 : i (cid:54)= j, i, j = 1, . . . , m(cid:1) where

i     x    

x     = ((xi )i , (yi )i ) (gretton et al., nips 2006, nips 2009, jmlr 2012).

(cid:73) choose the test statistic to be mmdh      (pm, qm) where

      = arg max
     (0,   )

mmdh   (pm, qm)

(s et al., nips 2009)

choice of characteristic kernels

let x = rd . suppose k is a gaussian kernel, k  (x, y ) = e
(cid:73) mmdh   is a function of   .
(cid:73) so mmdh   is a family of metrics. which one should we use in

2  2

.

    (cid:107)x   y(cid:107)2

2

practice?

(cid:73) note that mmdh       0 as        0 or           .
therefore, the kernel choice is very critical in applications.

heuristics:

(cid:73) median:    = median(cid:0)(cid:107)x    

j (cid:107)2 : i (cid:54)= j, i, j = 1, . . . , m(cid:1) where

i     x    

x     = ((xi )i , (yi )i ) (gretton et al., nips 2006, nips 2009, jmlr 2012).

(cid:73) choose the test statistic to be mmdh      (pm, qm) where

      = arg max
     (0,   )

mmdh   (pm, qm)

(s et al., nips 2009)

choice of characteristic kernels

let x = rd . suppose k is a gaussian kernel, k  (x, y ) = e
(cid:73) mmdh   is a function of   .
(cid:73) so mmdh   is a family of metrics. which one should we use in

2  2

.

    (cid:107)x   y(cid:107)2

2

practice?

(cid:73) note that mmdh       0 as        0 or           .
therefore, the kernel choice is very critical in applications.

heuristics:

(cid:73) median:    = median(cid:0)(cid:107)x    

j (cid:107)2 : i (cid:54)= j, i, j = 1, . . . , m(cid:1) where

i     x    

x     = ((xi )i , (yi )i ) (gretton et al., nips 2006, nips 2009, jmlr 2012).

(cid:73) choose the test statistic to be mmdh      (pm, qm) where

      = arg max
     (0,   )

mmdh   (pm, qm)

(s et al., nips 2009)

choice of characteristic kernels

let x = rd . suppose k is a gaussian kernel, k  (x, y ) = e
(cid:73) mmdh   is a function of   .
(cid:73) so mmdh   is a family of metrics. which one should we use in

2  2

.

    (cid:107)x   y(cid:107)2

2

practice?

(cid:73) note that mmdh       0 as        0 or           .
therefore, the kernel choice is very critical in applications.

heuristics:

(cid:73) median:    = median(cid:0)(cid:107)x    

j (cid:107)2 : i (cid:54)= j, i, j = 1, . . . , m(cid:1) where

i     x    

x     = ((xi )i , (yi )i ) (gretton et al., nips 2006, nips 2009, jmlr 2012).

(cid:73) choose the test statistic to be mmdh      (pm, qm) where

      = arg max
     (0,   )

mmdh   (pm, qm)

(s et al., nips 2009)

classes of characteristic kernels (s et al., nips 2009)

more generally, we use

mmd(p, q) := sup
k   k

mmdhk (p, q).

examples for k :
(cid:73) kg := {e     (cid:107)x   y(cid:107)2

(cid:73) klin := {k   =(cid:80)(cid:96)
(cid:73) kcon := {k   =(cid:80)(cid:96)

2 , x, y     rd :        r+}.

i=1   i ki|k   is pd,(cid:80)(cid:96)
i=1   i ki|  i     0,(cid:80)(cid:96)

i=1   i = 1}.
i=1   i = 1}.

test:

(cid:73)   -level test: estimate 1        quantile of the null distribution of

mmd(pm, qm) using bootstrap.

(cid:73) test consistency: based on the functional central limit theorem for

u-processes indexed by vc -subgraph k.

computational disadvantage!!

classes of characteristic kernels (s et al., nips 2009)

more generally, we use

mmd(p, q) := sup
k   k

mmdhk (p, q).

examples for k :
(cid:73) kg := {e     (cid:107)x   y(cid:107)2

(cid:73) klin := {k   =(cid:80)(cid:96)
(cid:73) kcon := {k   =(cid:80)(cid:96)

2 , x, y     rd :        r+}.

i=1   i ki|k   is pd,(cid:80)(cid:96)
i=1   i ki|  i     0,(cid:80)(cid:96)

i=1   i = 1}.
i=1   i = 1}.

test:

(cid:73)   -level test: estimate 1        quantile of the null distribution of

mmd(pm, qm) using bootstrap.

(cid:73) test consistency: based on the functional central limit theorem for

u-processes indexed by vc -subgraph k.

computational disadvantage!!

experiments

(cid:73) q = n (0,   2
q).
(cid:73) p(x) = q(x)(1 + sin   x).

   = 0

   = 2

   = 7.5

(cid:73) k(x, y ) = exp(   (x     y )2/  ).
(cid:73) test statistics: mmd(pm, qm) and mmdh   (pm, qm) for various   .

   50500.10.20.3xq(x)   50500.10.20.30.4xp(x)   50500.10.20.30.4xp(x)experiments

mmd(p, q)

0.50.7511.251.502456  error (in %)  type   i errortype   ii errorexperiments

mmdh   (p, q)

   3   2   10123456510152025log   type   i error (in %)    =0.5  =0.75  =1.0  =1.25  =1.5   3   2   10123456050100log   type   ii error (in %)    =0.5  =0.75  =1.0  =1.25  =1.50.50.7511.251.50123log     0.50.7511.251.5891011median as     choice of characteristic kernels (gretton et al., nips 2012)

(cid:73) choose a kernel that minimizes the type-ii error for a given type-i

error:

k        arg

inf

k   k:typei (k)     

typeii (k).

(cid:73) not easy to compute with the asymptotic distributions of the

(cid:73) modi   ed statistic: average of u-statistics computed on independent

u-statistic,

(cid:92)
mmd 2
hk

blocks of size 2.
(cid:94)
mmd2
hk

(pm, qm).
m/2(cid:88)
(cid:124)

2
m

i=1

(pm, qm)=

k(x2i   1, x2i ) + k(y2i   1, y2i )
   k(x2i   1, y2i )     k(y2i   1, x2i )
,

(cid:125)

(cid:123)(cid:122)

hk (zi )

where zi = (x2i   1, x2i , y2i   1, y2i ).

(cid:73) recall
(cid:92)
mmd 2

h :=

m(cid:88)

i(cid:54)=j

1

m(m     1)

(cid:124)

k(xi , xj ) + k(yi , yj )     k(xi , yj )     k(xj , yi )

(cid:123)(cid:122)

h((xi ,yi ),(xj ,yj ))

(cid:125)

choice of characteristic kernels (gretton et al., nips 2012)

(cid:73) choose a kernel that minimizes the type-ii error for a given type-i

error:

k        arg

inf

k   k:typei (k)     

typeii (k).

(cid:73) not easy to compute with the asymptotic distributions of the

(cid:73) modi   ed statistic: average of u-statistics computed on independent

u-statistic,

(cid:92)
mmd 2
hk

blocks of size 2.
(cid:94)
mmd2
hk

(pm, qm).
m/2(cid:88)
(cid:124)

2
m

i=1

(pm, qm)=

k(x2i   1, x2i ) + k(y2i   1, y2i )
   k(x2i   1, y2i )     k(y2i   1, x2i )
,

(cid:125)

(cid:123)(cid:122)

hk (zi )

where zi = (x2i   1, x2i , y2i   1, y2i ).

(cid:73) recall
(cid:92)
mmd 2

h :=

m(cid:88)

i(cid:54)=j

1

m(m     1)

(cid:124)

k(xi , xj ) + k(yi , yj )     k(xi , yj )     k(xj , yi )

(cid:123)(cid:122)

h((xi ,yi ),(xj ,yj ))

(cid:125)

modi   ed statistic

advantages:
(cid:73) (cid:94)
mmd 2
computations.

h is computable in o(m) while

(cid:92)
mmd 2

h requires o(m2)

(cid:73) under h0,

where   2
hk

   

(cid:94)
mmd 2
m
hk
k (z )     (ez hk (z ))2 assuming 0 < ez h2
= ez h2

(pm, qm) w    n (0, 2  2

),

hk

k (z ) <    .

(cid:73) the asymptotic distribution is normal as against weighted sum of

in   nite   2. therefore, the test threshold is easy to compute.

disadvantages:

(cid:73) larger variance
(cid:73) smaller power

modi   ed statistic

advantages:
(cid:73) (cid:94)
mmd 2
computations.

h is computable in o(m) while

(cid:92)
mmd 2

h requires o(m2)

(cid:73) under h0,

where   2
hk

   

(cid:94)
mmd 2
m
hk
k (z )     (ez hk (z ))2 assuming 0 < ez h2
= ez h2

(pm, qm) w    n (0, 2  2

),

hk

k (z ) <    .

(cid:73) the asymptotic distribution is normal as against weighted sum of

in   nite   2. therefore, the test threshold is easy to compute.

disadvantages:

(cid:73) larger variance
(cid:73) smaller power

type-i and type-ii errors

(cid:73) test threshold: for a given k and   ,

   

tk,1      =

2  hk      1

n (1       )

where   n is the cdf of n (0, 1).

(cid:73) type-ii error:

  n

(cid:32)

n (1       )     mmd 2
   
     1

hk

(cid:33)

m

   
(p, q)
2  hk

best kernel: minimizes type-ii error

(cid:73) since   n is a strictly increasing function, the type-ii error is

minimized by maximizing

(cid:73) optimal kernel:

mmd 2

(p,q)

hk
  hk

.

k        arg sup
k   k

(p, q)

mmd 2
hk
  hk

.

(cid:73) since mmd 2
hk

and   hk depend on unknown p and q, we split the
data into train and test data to estimate k    on the train data as   k   
and evaluate the threshold t  k   ,1      on the test data.

data-dependent kernel

(cid:94)
mmd 2
hk

and     hk .

(cid:73) train data:
(cid:73) de   ne

  k        arg sup
k   k

for some   m     0 as m        .

(cid:94)
mmd 2
hk
    hk +   m

(cid:73) test data:

(cid:94)
mmd 2

h  k    ,     h  k    and t  k   ,1     .

(cid:73) if

(cid:94)
mmd 2

h  k    > t  k   ,1     , reject h0, else accept.

similar results are recently obtained for
2017)

(cid:92)
mmd 2
hk

(sutherland et al., iclr

data-dependent kernel

(cid:94)
mmd 2
hk

and     hk .

(cid:73) train data:
(cid:73) de   ne

  k        arg sup
k   k

for some   m     0 as m        .

(cid:94)
mmd 2
hk
    hk +   m

(cid:73) test data:

(cid:94)
mmd 2

h  k    ,     h  k    and t  k   ,1     .

(cid:73) if

(cid:94)
mmd 2

h  k    > t  k   ,1     , reject h0, else accept.

similar results are recently obtained for
2017)

(cid:92)
mmd 2
hk

(sutherland et al., iclr

learning the kernel

de   ne the family of kernels as follows:

(cid:40)

(cid:96)(cid:88)

i=1

k :=

k : k =

  i ki ,   i     0,     i     [(cid:96)]

.

(cid:41)

(cid:73) if all ki are characteristic and for some i     [(cid:96)],   i > 0, then k is

characteristic.

(p, q) =(cid:80)(cid:96)

(cid:73) mmd 2
hk

k =(cid:80)(cid:96)

(cid:73)   2

i=1   i mmd 2
hki

(p, q)

i,j=1   i   j cov(hki , hkj ) where

hki (x, x(cid:48), y , y(cid:48)) = ki (x, x(cid:48)) + ki (y , y(cid:48))     ki (x, y(cid:48))     ki (x(cid:48), y ).

(cid:73) objective:

      = arg max
  (cid:23)0

  t   (cid:112)  t w   

,

where    := (mmd 2

hki

(p, q))i and w := (cov(hki , hkj ))i,j .

learning the kernel

de   ne the family of kernels as follows:

(cid:40)

(cid:96)(cid:88)

i=1

k :=

k : k =

  i ki ,   i     0,     i     [(cid:96)]

.

(cid:41)

(cid:73) if all ki are characteristic and for some i     [(cid:96)],   i > 0, then k is

characteristic.

(p, q) =(cid:80)(cid:96)

(cid:73) mmd 2
hk

k =(cid:80)(cid:96)

(cid:73)   2

i=1   i mmd 2
hki

(p, q)

i,j=1   i   j cov(hki , hkj ) where

hki (x, x(cid:48), y , y(cid:48)) = ki (x, x(cid:48)) + ki (y , y(cid:48))     ki (x, y(cid:48))     ki (x(cid:48), y ).

(cid:73) objective:

      = arg max
  (cid:23)0

  t   (cid:112)  t w   

,

where    := (mmd 2

hki

(p, q))i and w := (cov(hki , hkj ))i,j .

learning the kernel

de   ne the family of kernels as follows:

(cid:40)

(cid:96)(cid:88)

i=1

k :=

k : k =

  i ki ,   i     0,     i     [(cid:96)]

.

(cid:41)

(cid:73) if all ki are characteristic and for some i     [(cid:96)],   i > 0, then k is

characteristic.

(p, q) =(cid:80)(cid:96)

(cid:73) mmd 2
hk

k =(cid:80)(cid:96)

(cid:73)   2

i=1   i mmd 2
hki

(p, q)

i,j=1   i   j cov(hki , hkj ) where

hki (x, x(cid:48), y , y(cid:48)) = ki (x, x(cid:48)) + ki (y , y(cid:48))     ki (x, y(cid:48))     ki (x(cid:48), y ).

(cid:73) objective:

      = arg max
  (cid:23)0

  t   (cid:112)  t w   

,

where    := (mmd 2

hki

(p, q))i and w := (cov(hki , hkj ))i,j .

optimization

(cid:73)

       
   = arg max
  (cid:23)0

(cid:113)

  t     

  t (   w +   i )  

(cid:73) if      has at least one positive element, the objective function is

strictly positive and so

       
   = arg min
  

  t (   w +   i )   :   t      = 1,    (cid:23) 0

.

(cid:110)

(cid:111)

(cid:73) on the test data:

h  k    using   k    =(cid:80)(cid:96)

(cid:94)
mmd 2

(cid:73) compute

       
  ,i ki .
(cid:73) compute test threshold   t  k   ,1      using       k    .

i=1

optimization

(cid:73)

       
   = arg max
  (cid:23)0

(cid:113)

  t     

  t (   w +   i )  

(cid:73) if      has at least one positive element, the objective function is

strictly positive and so

       
   = arg min
  

  t (   w +   i )   :   t      = 1,    (cid:23) 0

.

(cid:110)

(cid:111)

(cid:73) on the test data:

h  k    using   k    =(cid:80)(cid:96)

(cid:94)
mmd 2

(cid:73) compute

       
  ,i ki .
(cid:73) compute test threshold   t  k   ,1      using       k    .

i=1

experiments

(cid:73) p and q are mixtures of two-dimensional gaussians. p has unit

covariance in each component. q has correlated gaussians with   
being the ratio of largest to smallest covariance eigenvalues.

(cid:73) testing problem di   culty increases with        1 and the number of

mixture components.

competing approaches

(cid:73) median heuristic
(cid:73) max. mmd: supk   k mmd 2
hk

largest mmd 2
hk

(pm, qm)

(pm, qm)     choose k     k with the

(cid:73) same as maximizing   t      subject to (cid:107)  (cid:107)1     1.
(cid:73) (cid:96)2 statistic: maximize   t      subject to (cid:107)  (cid:107)2     1.
(cid:73) cross-validation on training set.

results

m = 10, 000 (for training and test). results are average over 617 trials.

results

optimize

(cid:92)
mmd 2
hk

    k

results

maximize

(cid:92)
mmd 2
hk

with    constraint

results

median heuristic

references i

dudley, r. m. (2002).
real analysis and id203.
cambridge university press, cambridge, uk.

fukumizu, k., gretton, a., sun, x., and sch  olkopf, b. (2008).
kernel measures of conditional dependence.
in platt, j., koller, d., singer, y., and roweis, s., editors, advances in neural information processing systems 20, pages 489   496,
cambridge, ma. mit press.

fukumizu, k., sriperumbudur, b. k., gretton, a., and sch  olkopf, b. (2009).
characteristic kernels on groups and semigroups.
in advances in neural information processing systems 21, pages 473   480.

gretton, a. (2015).
a simpler condition for consistency of a kernel independence test.
arxiv:1501.06103.

gretton, a., borgwardt, k. m., rasch, m., sch  olkopf, b., and smola, a. (2007).
a kernel method for the two sample problem.
in advances in neural information processing systems 19, pages 513   520. mit press.

gretton, a., borgwardt, k. m., rasch, m., sch  olkopf, b., and smola, a. (2012a).
a kernel two-sample test.
journal of machine learning research, 13:723   773.

gretton, a., bousquet, o., smola, a., and sch  olkopf, b. (2005a).
measuring statistical dependence with hilbert-schmidt norms.
in jain, s., simon, h. u., and tomita, e., editors, proceedings of algorithmic learning theory, pages 63   77, berlin. springer-verlag.

gretton, a., fukumizu, k., harchaoui, z., and sriperumbudur, b. k. (2010).
a fast, consistent kernel two-sample test.
in advances in neural information processing systems 22, cambridge, ma. mit press.

gretton, a., herbrich, r., smola, a., bousquet, o., and sch  olkopf, b. (2005b).
kernel methods for measuring independence.
journal of machine learning research, 6:2075   2129.

gretton, a., smola, a., bousquet, o., herbrich, r., belitski, a., augath, m., murayama, y., pauls, j., sch  olkopf, b., and logothetis,
n. (2005c).
kernel constrained covariance for dependence measurement.
in ghahramani, z. and cowell, r., editors, proc. 10th international workshop on arti   cial intelligence and statistics, pages 1   8.

references ii

gretton, a., sriperumbudur, b., sejdinovic, d., strathmann, h., balakrishnan, s., pontil, m., and fukumizu, k. (2012b).
optimal kernel choice for large-scale two-sample tests.
in advances in neural information processing systems 24, cambridge, ma. mit press.

muandet, k., fukumizu, k., sriperumbudur, b. k., and sch  olkopf, b. (2016a).
kernel mean embedding of distributions: a review and beyond.
arxiv:1605.09522.

muandet, k., sriperumbudur, b. k., fukumizu, k., gretton, a., and sch  olkopf, b. (2016b).
kernel mean shrinkage estimators.
journal of machine learning research, 17(48):1   41.

m  uller, a. (1997).
integral id203 metrics and their generating classes of functions.
advances in applied id203, 29:429   443.

ramdas, a., reddi, s. j., p  oczos, b., singh, a., and wasserman, l. (2015).
on the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions.
in proc. of 29th aaai conference on arti   cial intelligence, pages 3571   3577.

simon-gabriel, c. and sch  olkopf, b. (2016).
kernel distribution embeddings: universal kernels, characteristic kernels and kernel metrics on distributions.
arxiv:1604.05251.

smola, a. j., gretton, a., song, l., and sch  olkopf, b. (2007).
a hilbert space embedding for distributions.
in proc. 18th international conference on algorithmic learning theory, pages 13   31. springer-verlag, berlin, germany.

sriperumbudur, b. k. (2016).
on the optimal estimation of id203 measures in weak and strong topologies.
bernoulli, 22(3):1839   1893.

sriperumbudur, b. k., fukumizu, k., gretton, a., sch  olkopf, b., and lanckriet, g. r. g. (2012).
on the empirical estimation of integral id203 metrics.
electronic journal of statistics, 6:1550   1599.

sriperumbudur, b. k., fukumizu, k., and lanckriet, g. r. g. (2011).
universality, characteristic kernels and rkhs embedding of measures.
journal of machine learning research, 12:2389   2410.

references iii

sriperumbudur, b. k., gretton, a., fukumizu, k., lanckriet, g. r. g., and sch  olkopf, b. (2008).
injective hilbert space embeddings of id203 measures.
in servedio, r. and zhang, t., editors, proc. of the 21st annual conference on learning theory, pages 111   122.

sriperumbudur, b. k., gretton, a., fukumizu, k., sch  olkopf, b., and lanckriet, g. r. g. (2010).
hilbert space embeddings and metrics on id203 measures.
journal of machine learning research, 11:1517   1561.

steinwart, i. and christmann, a. (2008).
support vector machines.
springer.

sutherland, d. j., tung, h.-y., strathmann, h., de, s., ramdas, a., smola, a., and gretton, a. (2017).
generative models and model criticism via optimized maximum mean discrepancy.
in international conference on learning representations.

tolstikhin, i., sriperumbudur, b. k., and muandet, k. (2016).
minimax estimation of kernel mean embeddings.
arxiv:1602.04361.

