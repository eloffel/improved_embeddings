tutorial

october 28 - november 1, 2016

the twelfth conference of 

the association for machine translation 

in the americas 

http://www.amtaweb.org/amta-2016-in-austin-tx

dependency-based id151

october 28, 2016

qun liu

liangyou li

dependency-based statistical 

machine translation

qun liu 

liangyou li

adapt centre

dublin city university

{qun.liu,liangyou.li}@adaptcentre.ie

1

proceedings of amta 2016austin, oct 28 - nov 1, 2016speakers

2

qun liu
   
   
   

professor
dublin city university
chinese academy of science

http://computing.dcu.ie/~qliu/

liangyou li
   
    dublin city university

phd candidate

http://www.computing.dcu.ie/~liangyouli/

proceedings of amta 2016austin, oct 28 - nov 1, 2016outline

    introduction
    dependency-based evaluation
    translation models based on segmentation

coffee break

    translation models based on synchronous 

grammars 
    lab session
    conclusion

3

proceedings of amta 2016austin, oct 28 - nov 1, 2016id151
dependency structures
introduction

4

proceedings of amta 2016austin, oct 28 - nov 1, 2016id151

what is smt?
advantages of smt
framework of smt
smt approaches

   

   

   

   

5

proceedings of amta 2016austin, oct 28 - nov 1, 2016what is smt?

smt is a machine translation paradigm which replies on 
parallel corpora and machine learning techniques

6

[image from taus user conference 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016advantages of smt

    data driven
    language independent
    less dependent on language experts
    fully automatic
    fast prototype and deploy

7

proceedings of amta 2016austin, oct 28 - nov 1, 2016framework of smt

    noisy-channel model

t

p(t)

p(s|t)

s

        = argmax               

= argmax               (    |    )

language model

translation model

8

[brown 1988,1990,1993]

proceedings of amta 2016austin, oct 28 - nov 1, 2016framework of smt

    log-linear model

    generalization of the noisy-channel model
    add arbitrary number of features
    features are tunable

9

[och et al., 2002]

proceedings of amta 2016austin, oct 28 - nov 1, 2016smt approaches

inter-lingual

source semantic

target semantic

graph-based

source syntax

target syntax

source words

target words

tree-based

sequence-based

10

proceedings of amta 2016austin, oct 28 - nov 1, 2016phrase-based smt

    source sentences are segmented into phrases
    source phrases are translated into target phrases
    target phrases are reordered

11

[koehn, 2003]

proceedings of amta 2016austin, oct 28 - nov 1, 2016phrase-based smt

search space:

id125:

12

[koehn, 2003]

illustration as in [liu et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016tree-based smt

-based smt

motivation
hierarchical phrase
-to-tree smt
string
-to-string smt
tree
-to-tree smt
tree
-based smt
forest

   

   

   

   

   

   

13

proceedings of amta 2016austin, oct 28 - nov 1, 2016motivation

    phrase reordering

    reordering within phrases
    only continuous phrases

    generalizations

    french ne   pas to english not
    chinese yu   wuguan to english has nothing to do with

14

syntactic tree from [koehn, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016hierarchical phrase-based smt

   

rule form

             ,     , ~

   

glue rule

             1    2,     1    2
             1,     1

15

[chiang, 2005, 2007]

proceedings of amta 2016austin, oct 28 - nov 1, 2016hierarchical phrase-based smt

search hypergraph:

beam:

16

image from [koehn, 2010]

[chiang, 2005, 2007]

proceedings of amta 2016austin, oct 28 - nov 1, 2016string-to-tree smt

training
    target sentences are parsed 

into trees

    extract string   tree pairs
decoding
    parse source sentences using 

hierarchical phrases

    generate target trees using 

subtrees in rules

17

[galley et al., 2004, 2006]

proceedings of amta 2016austin, oct 28 - nov 1, 2016tree-to-string smt

training
   

source sentences are parsed 
into trees

decoding
    parse source sentences 

beforehand

    extract tree--string pairs

    generate target words

18

[liu et al., 2006]

proceedings of amta 2016austin, oct 28 - nov 1, 2016tree-to-tree smt

training
    source and target sentences 

are parsed into trees

    extract tree--tree pairs

decoding
   
   

parse source sentences 
generate target trees 
using subtrees in rules

19

[zhang et al., 2007]

proceedings of amta 2016austin, oct 28 - nov 1, 2016forest-based smt

20

[mi et al., 2008]

proceedings of amta 2016austin, oct 28 - nov 1, 2016graph-based smt

    semantic representation
    semantic-based smt

21

proceedings of amta 2016austin, oct 28 - nov 1, 2016semantic representation

    id15 (amr)

the boy wants to go

22

[laura et al., 2013]

proceedings of amta 2016austin, oct 28 - nov 1, 2016semantic-based smt

translation process:

edge-word alignments:

rules:

23

[jones et al., 2012]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency structures

   

   

dependency tree
why dependency in smt?

24

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency tree

flat

-node correspondence: one-to-one-or-

deep vs 
word
many vs one-to-one
simple in formalism yet having id18 equivalent 
formal generative capacity [ding et al., 2004]

   
   

   

25

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency tree

non-projective

projective

26

image from wikipedia

proceedings of amta 2016austin, oct 28 - nov 1, 2016why dependency in smt?

    semantic relation between words
    best inter-lingual phrase cohesion [fox, 2002]
    flexible translation units

27

proceedings of amta 2016austin, oct 28 - nov 1, 2016conclusion

    smt models benefit from syntactic structures

    hpb
    t2s
    s2t
    t2t

    dependency structures have the best inter-

lingual phrasal cohesion property

28

proceedings of amta 2016austin, oct 28 - nov 1, 2016references

peter f. brown, john cocke, stephen a. della pietra, vincent j. della pietra, frederick jelinek, john d. lafferty, robert l. mercer, and paul rossin (1990). a statistical approach to machine 
translation. in: computational linguistics 16.2, pages 76   85.
peter f. brown, john cocke, stephen a. della pietra, vincent j. della pietra, frederick jelinek, robert l. mercer, and paul roossin (1988). a statistical approach to language translation. in: 
proceedings of the 12th conference on computational linguistics - volume 1. budapest, hungry, pages 71   76.
peter f. brown, vincent j. della pietra, stephen a. della pietra, and robert l. mercer (1993). the mathematics of id151: parameter estimation. in: computational 
linguistics 19.2, pages 263   311.
david chiang (2005). a hierarchical phrase-based model for id151. in: proceedings of the 43rd annual meeting of the association for computational linguistics. 
ann arbor, michigan, usa, pages 263   270.
david chiang (2007). hierarchical phrase-based translation. in: computational linguistics 33.2, pages 201   228.
david chiang (2012). grammars for language and genes: theoretical and empirical investigations. springer.
michel galley, jonathan graehl, kevin knight, daniel marcu, steve deneefe, wei wang, and ignacio thayer (2006). scalable id136 and training of context-rich syntactic translation 
models. in: proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the association for computational linguistics. sydney, 
australia, pages 961   968.
michel galley, mark hopkins, kevin knight, and daniel marcu (2004). what   s in a translation rule? in: proceedings of human language technology conference of the north american 
chapter of the association for computational linguistics. boston, massachusetts, usa, pages 273   280.
liang huang, kevin knight, and aravind joshi (2006a). a syntax-directed translator with extended domain of locality. in: proceedings of the workshop on computationally hard problems 
and joint id136 in speech and language processing. new york city, new york, pages 1   8.
liang huang, kevin knight, and aravind joshi (2006b). statistical syntax-directed translation with extended domain of locality. in: proceedings of the 7th conference of the association 
for machine translation of the americas. cambridge, massachusetts, usa, pages 66   73.
bevan jones, jacob andreas, daniel bauer, karl moritz hermann, and kevin knight (2012). semantics-based machine translation with hyperedge replacement grammars. in: proceedings 
of coling 2012, the 24th international conference on computational linguistics: technical papers. mumbai, india, pages 1359   1376.
philipp koehn (2010). id151. 1st. new york, ny, usa: cambridge university press.
philipp koehn, franz josef och, and daniel marcu (2003). statistical phrase-based translation. in: proceedings of the 2003 conference of the north american chapter of the association 
for computational linguistics on human language technology - volume 1. edmonton, canada, pages 48   54.
yang liu and qun liu (2010). joint parsing and translation. in: proceedings of the 23rd international conference on computational linguistics (volume 2). beijing, china, pages 707   715.
yang liu, qun liu, and shouxun lin (2006). tree-to-string alignment template for id151. in: proceedings of the 21st international conference on computational 
linguistics and the 44th annual meeting of the association for computational linguistics. sydney, australia, pages 609   616.
daniel marcu,weiwang, abdessamad echihabi, and kevin knight (2006). spmt: id151 with syntactified target language phrases. in: proceedings of the 2006 
conference on empirical methods in natural language processing. sydney, australia, pages 44   52.
haitao mi, liang huang, and qun liu (2008). forest-based translation. in: proceedings of the 46th annual meeting of the association for computational linguistics: human language 
technologies. columbus, ohio, usa, pages 192   199.
franz josef och and hermann ney (2002). discriminative training and maximum id178 models for id151. in: proceedings of the 40th annual meeting on 
association for computational linguistics. philadelphia, pennsylvania, usa, pages 295   302.
chris quirk and simon corston-oliver (2006). the impact of parse quality on syntacticallyinformed id151. in: proceedings of the 2006 conference on empirical 
methods in natural language processing. sydney, australia, pages 62   69.
kenji yamada and kevin knight (2001). a syntax-based statistical translation model. in: proceedings of the 39th annual meeting on association for computational linguistics. toulouse, 
france, pages 523   530. 
kenji yamada and kevin knight (2002). a decoder for syntax-based statistical mt. in: proceedings of the 40th annual meeting on association for computational linguistics. philadelphia, 
pennsylvania, usa, pages 303   310.
min zhang, hongfei jiang, aiti aw, sun jun, sheng li, and chew lim tan (2007). a tree-to-tree alignment-based model for id151. in: proceedings of machine 
translation summit xi. copenhagen, denmark, pages 535   542.

   

   

   

   

   
   
   

   

   

   

   

   
   

   
   

   

   

   

   

   

   

   

29

proceedings of amta 2016austin, oct 28 - nov 1, 2016q&a

30

proceedings of amta 2016austin, oct 28 - nov 1, 2016mt evaluation introduction
human evaluation
automatic evaluation
dependency-based evaluation
dependency-based mt 
evaluation

31

proceedings of amta 2016austin, oct 28 - nov 1, 2016introduction of mt evaluation

goal: evaluate translation performance of smt 
systems

    meaning preserved
    grammatically correct

difficulty: no single right answer

32

image from [koehn, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016human evaluation

adequacy: same meaning?

fluency: grammatically correct?

33

tables and images from [koehn, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016human evaluation

    time-consuming
    expensive: e.g. professional translator?
    unrepeatable: precious human labor cannot be 

simply re-run

    low-agreement: both inter and intra judgement. 
    e.g. wmt11 en-cz task, multi-annotator agreement 

kappa value is very low; even the same strings 
produced by two systems were ranked differently 
each time by the same annotator [callison-burch, et al., 2011]

34

proceedings of amta 2016austin, oct 28 - nov 1, 2016automatic mt evaluation

    difficulty in automatic evaluation: 

    language variability, language ambiguity
    how to evaluate semantic and syntactic quality

    how to evaluate automatic id74: 

    usually calculate the correlation score with human judgements

    we expect:

    repeatable: can be re-used whenever we make some changes on smt 

systems

    fast: minutes or seconds for evaluating 3k sentences vs hours of 

human labor

    cheap: compared with employment of human judges
    stable: each time of running, with same score for un-changed output
    reliable: give a higher score for better translation output
    further benefit: tune system parameters with automatic metrics

35

proceedings of amta 2016austin, oct 28 - nov 1, 2016automatic mt evaluation

-based similarity metrics
[papineni et al., 2002]

[snover et al., 2006]

[lavie et al., 2007; denkowski et al., 2011]

-based similarity metrics:

lexicon
id7 
   
ter 
   
meteor 
   
semantic
   

[lo et al., 2012, 2013]. use semantic 
meant/hmeant series 
role labelling information, accuracy of labelling drops 
due to translation errors.

syntax
   
   

-based metrics

constituency structures
dependency structures

   

   

   

36

proceedings of amta 2016austin, oct 28 - nov 1, 2016id7

id165 precision:

length penalty:

language independent

    most widely used metric
   
    multiple references
    no recall
    geometric averaging 
    words are equally weighted
    weak at semantic equivalents
    document-level

37

[papineni et al., 2002; callison-burch et al., 2006]

proceedings of amta 2016austin, oct 28 - nov 1, 2016meteor

    precision, recall, f-measure
    word-order penalty
    matchers

    exact
    stem
    id138
    paraphrase

    function words, content words
    tunable

38

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-based evaluation

-word chain matching

advantages of dependency structures
subtree and head
dependency relation matching
red metrics
parsing as evaluation
id56

-based mt evaluation

   

   

   

   

   

   

39

proceedings of amta 2016austin, oct 28 - nov 1, 2016advantages of dependency structures

    syntactic equivalents

    structures and categories

    better structures for languages with freer 

word-order

    long-distance matching

40

proceedings of amta 2016austin, oct 28 - nov 1, 2016subtree and head-word chain matching

subtree matching:

head-word chain matching:

dependency

constituent

subtree

head-word chain

41

[liu et al., 2006]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency relation matching

lexical functional grammar:

subj(resign, john), pers(john, 3), num(john, sg)
tense(resign, past), adj(resign, yesterday)
pers(yesterday, 3), num(yesterday, sg)

42

[owczarzak et al., 2007]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency relation matching

id35

only parse references

dependent ordering score (dos):
    for each head word in the ref
    for each left dependent

   

if the head appears in the 
mt ouput and the depenent
is on the left, add value 1

    similar process for the right 

dependents

final score: 

recall in terms of dos * length penalty

43

[mehay et al., 2007]

proceedings of amta 2016austin, oct 28 - nov 1, 2016red metric

    red: reference dependency based mt 

evaluation metric 

    only use reference dependency tree
    two kinds of reference dependency 

structures:
    head-word chains: capture the long-distance 

dependency information

    fixed and floating structures [shen et al. 2010]: capture 

local continuous ngrams

44

[yu et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016red metric

45

figure 3: scoring head-word chain matching

stem and synonym 

extra resources redp (plus): 
   
    paraphrase
   

function word, content 
word

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: system-level correlation

tab 2: sentence-level correlation

46

proceedings of amta 2016austin, oct 28 - nov 1, 2016hpb mt tuned on red

47

[li et al., 2015]

proceedings of amta 2016austin, oct 28 - nov 1, 2016parsing as evaluation

   

   

   

   

48

-id178 model-based 

train a maximum
dependency parser on references
   
parse hypotheses and use 
parsing id203 as a score

references are parsed by 

the stanford parser
the normalized 

lexical score: unigram f
final score:

-score

[yu et al., 2015]

proceedings of amta 2016austin, oct 28 - nov 1, 2016parsing as evaluation

system-level

sentence-level

49

proceedings of amta 2016austin, oct 28 - nov 1, 2016id56-based mt evaluation

evaluation score

50

[gupta et al., 2015, tai et al., 2015]

proceedings of amta 2016austin, oct 28 - nov 1, 2016conclusion

-word chains

dependency structures are helpful on mt 
evaluation
subtrees
   
head
   
fixed/floating 
   
dependency 
   
id56
   
extra resources are important to evaluation 
performance but language-dependent.

structures

relations

thanks lifeng han for his help on this section.

   

   

51

proceedings of amta 2016austin, oct 28 - nov 1, 2016references

2010. statistical machine. translation. cambridge university press.

denkowski and alon lavie. 2011. meteor 1.3: automatic metric for reliable optimization and evaluation of machine translation systems. in proceedings of the 

-kiu lo and dekai wu. 2013. meant at wmt 2013: a tunable, accurate yet inexpensive semantic frame based mt evaluation metric. in proceedings of the eighth 

gildea. 2005. syntactic features for evaluation of machine translation. in proceedings of the acl workshop on intrinsic and extrinsic evaluation 

qun liu. 2015. mt tuning on red: a dependency-based evaluation metric. in proceedings of the tenth workshop on statistical machine 

he and andy way. 2009. learning labelled dependencies in machine translation evaluation. proceedings of the 13th annual conference of the eamt, pages 

gupta, constantin orasan, josef van genabith (2015). reval: a simple and effective machine translation evaluation metric based on recurrent neural 

lavie and abhaya agarwal. 2007. meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments. in proceedings of the 

   s gime   nez and llu     s ma`rquez. 2007. linguistic features for automatic evaluation of heterogenous mt systems. in proceedings of the second workshop on 

michael 
sixth workshop on id151, pages 85   91. association for computational linguistics.
jesu
id151, pages 256   264. association for computational linguistics.
rohit
networks. in:proceedings of the 2015 conference on empirical methods in natural language processing, pages 1066   1072, lisbon, portugal.
yifan
44   51, barcelona, may 2009
philipp koehn. 
alon
second workshop on id151, statmt    07, pages 228   231, stroudsburg, pa, usa. association for computational linguistics.
liangyou li, hui yu, 
translation, pages 428-433, lisboa, portugal, 17-18 september 2015.
ding liu and daniel 
measures for machine translation and/or summarization, pages 25   32.
chi
workshop on id151, pages 422   428, sofia, bulgaria, august. association for computational linguistics.
chi
translation, pages 243   252, montre   al, canada, june. association for computational linguistics.
dennis 
owczarzak, josef van genabith, and andy way. 2007. dependency-based automatic evaluation for machine translation. in proceedings of the naacl-hlt 
karolina 
2007/amta workshop on syntax and structure in sta- tistical translation, ssst    07, pages 80   87, stroudsburg, pa, usa. association for computational linguistics.
karolina 
owczarzak, josef van genabith and andy way. 2007. evaluating machine translation with lfg dependencies. j. machine translation. vol. 21, no. 2 (jun., 
2007), pp. 95-119.
karolina 
k. 
association for computational linguistics, pages 311   318. association for computational linguistics.
matthew 
proceedings of association for machine translation in the americas, pages 223   231.
matthew 
metric. in proceedings of the fourth workshop on id151, pages 259   268. association for computational linguistics.
milos 
h yu, x wu, j 
conference on computational linguistics: technical papers, pages 2042   2051, dublin, ireland, august 23-29 2014.
h yu, x wu, w jiang, q liu, sx lin. 

-kiu lo, anand karthik tumuluru, and dekai wu. 2012. fully automatic semantic mt evaluation. in proceedings of the seventh workshop on statistical machine 

snover, nitin madnani, bonnie j dorr, and richard schwartz. 2009. fluency, adequacy, or hter?: exploring different human judgments with a tunable mt

snover, bonnie dorr, richard schwartz, linnea micciulla, and john makhoul. 2006. a study of trans- lation edit rate with targeted human annotation. in 

stanojevic and khalil sima   an. 2014. beer: better evaluation as ranking. in proceedings of the ninth workshop on id151.

xie, w jiang, q liu, s lin. 2014. red: a reference dependency based mt evaluation metric. proceedings of coling 2014, the 25th international 

mehay and chris brew. 2007. id7atre: flattening syntactic dependencies for mt evaluation. in proceedings of the mt summit.

papineni, s. roukos, t. ward, and w.j. zhu. 2002. id7: a method for automatic evaluation of machine translation. in proceedings of the 40th annual meeting on 

2015. an automatic machine translation evaluation metric based on id33 model.arxiv preprint arxiv:1508.01996

owczarzak. 2008. a novel dependency-based evaluation metric for machine translation. phd thesis. dcu.

   

   

   

   

   
   

   

   

   

   

   
   

   

   
   

   

   

   
   

   

52

proceedings of amta 2016austin, oct 28 - nov 1, 2016q&a

53

proceedings of amta 2016austin, oct 28 - nov 1, 2016structure segmentation
why segmentation?
dependency tree segmentation
dependency graph segmentation
translation models based on 
segmentation

54

proceedings of amta 2016austin, oct 28 - nov 1, 2016structure segmentation

segmentation divides structures into units.

sentence -> phrases
phrase-based models

sentence segmentation

tree segmentation

tree -> treelets
treelet-based models

graph segmentation

graph -> subgraphs
graph-based models

55

proceedings of amta 2016austin, oct 28 - nov 1, 2016why segmentation?

intuitive
   

instead of translating a whole sentence at a time, 
translating parts and then combing them

not reply on recursive rules 

small model
   
flexible translation units
   

such as 
discontinuous spans.

treelets and subgraphs covering 

fast decoding in practice
   

-based model vs hierarchical phrase-based 

phrase
model

   

   

   

   

56

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency segmentation

    dependency tree segmentation

    edges, paths, treelets

    dependency graph segmentation

    subgraphs

subgraphs

treelets

paths

edges

57

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency edge model

bottom-up translate

subtree translations

non-terminal nodes for 
integrating subtree translations

different word order
for efficiency: 

distortion limitation

58

[chen et al., 2012]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency edge model

low distortion limit: 
   
   

less reordering is allowed.
target words are in the similar 
order with source words.
fast decoding

   

high distortion limit:
    allow too much reordering
   
   

introduce many bad translations
low efficiency

tab 1: id7 scores

59

[chen et al., 2012]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency path model

        

is a head word

monotonic

path

[lin, 2004]

not path

no edge

60

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency path model

rules:

decoding:
    a source sentence is parsed into 

   

   

a dependency tree
extract all paths and find transfer 
rules
find a sequence of transfer rules 
which

o cover the source tree
o generate a target tree
o have the highest id203

    obtain a target sequence from 

the target tree

61

[lin, 2014]

worse than the phrase-based model

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency treelet model

a treelet is defined to be an arbitrary connected subgraph of 
a dependency tree.

treelets

paths

edges

not a path but a treelet

62

[quirk et al., 2005]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency treelet model

projection based on word alignments

reattachment to keep target word order

example translation rule

63

[quirk et al., 2005]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency treelet models

    bottom-up decoding
    translations of treelets are 
attached together to form a 
complete translation

    attachment during decoding: 

combinatory problem

attach target trees to the head word
=insert translations into  installes sur
3*4=12 possibilities!

64

[quirk et al., 2005]

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: system comparison

tab 3: influence of treelet or phrase size

tab 2: influence of reordering

tab 4: continuity vs discontinuity

65

[quirk et al., 2005]

proceedings of amta 2016austin, oct 28 - nov 1, 2016allowing variables and gaps

variables: source -- target

gaps: only target

66

weak at reordering

[xiong et al., 2007]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency graph segmentation

    why graph segmentation?
    how to construct graphs?
    segmentational graph-based model
    context-aware segmentation

67

proceedings of amta 2016austin, oct 28 - nov 1, 2016why graph segmentation?

68

proceedings of amta 2016austin, oct 28 - nov 1, 2016why graph segmentation?

69

proceedings of amta 2016austin, oct 28 - nov 1, 2016why graph segmentation?

70

proceedings of amta 2016austin, oct 28 - nov 1, 2016why graph segmentation?

71

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016how to construct graphs?

72

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016how to construct graphs?

subgraphs

73

proceedings of amta 2016austin, oct 28 - nov 1, 2016segmentational graph-based models

    training

74

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016segmentational graph-based models

    training

75

proceedings of amta 2016austin, oct 28 - nov 1, 2016segmentational graph-based models

    training

76

proceedings of amta 2016austin, oct 28 - nov 1, 2016segmentational graph-based models

    decoding

77

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: id7 scores

tab 2: system rule number

78

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

79

   

   

tends to use 

treelet
smaller phrases. (only 
dependency relations, 
low coverage)

gbmt uses more larger 
phrase pairs. (+bigram 
relations)

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: rule number according to their types

tab 2: id7 scores

80

70%

42%--48%

share >30%

15%--17%

inconsistency: more 
treerules are extraced?

small contribution but 
the best

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

81

tab 1: influence of edge types

tab 1: rule number

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

less sensitive:
even though the distortion 
limit is small, subgraphs can 
cover long-distance 
discontinuous phrases.

82

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

83

proceedings of amta 2016austin, oct 28 - nov 1, 2016context-aware segmentation

    why need context-awareness?
    graph segmentation model
    context-aware rules

84

proceedings of amta 2016austin, oct 28 - nov 1, 2016why need context-awareness?

    better subgraph selection
    better rule selection

85

proceedings of amta 2016austin, oct 28 - nov 1, 2016graph segmentation model

basic assumption:

sparse features:

current node n

a node n    connected 
to the current node

where the node 
n    comes from 

connection 
directions

86

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016graph segmentation model

extract for each node

full generalization

87

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

88

proceedings of amta 2016austin, oct 28 - nov 1, 2016context-aware rules

rule form:

    ,     

    ,     ,     

rule types:

basic rule

segmenting rule

selecting rule

segmenting rules and selecting 
rules are extensions of basic 
rules by adding context 
information so that basic rules 
are split into different groups 
according to their contexts. 

89

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: id7 scores

tab 3: number of rules

tab 2: influence of context 

tab 4: influence of rules

90

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

91

[li et al., 2016]

proceedings of amta 2016austin, oct 28 - nov 1, 2016conclusion

segmentation-based models are flexible to use 
translation units. however, they are weak at phrase 
reordering.

main research lines:

    segmenting dependency tree

    edge
    path
    treelet

    segmenting dependency graph

    subgraph

92

proceedings of amta 2016austin, oct 28 - nov 1, 2016references

2010). id151. 1st. new york, ny, usa: cambridge university press.

lin (2004). a path-based transfer model for machine translation. in: proceedings of the 20th international 

chen, jun xie, fandong meng, wenbin jiang, and qun liu (2014). a dependency edge-based transfer model for 

hongshen
id151. in: proceedings of coling 2014, the 25th international conference on computational 
linguistics: technical papers. dublin, ireland, pages 1103   1113.
2010). accurate non-hierarchical phrase-based translation. in: human language 
michel galley and christopher d. manning (
technologies: the 2010 annual conference of the north american chapter of the association for computational linguistics. 
los angeles, california, usa, pages 966   974.
philipp koehn (
dekang
conference on computational linguistics. geneva, switzerland, pages 625   630.
arul menezes and chris quirk (
machine-translation? in: proceedings of the workshop on example-based machine translation. phuket, thailand.
2005). dependency treelet translation: syntactically informed phrasal smt. in: 
chris quirk, arul menezes, and colin cherry (
proceedings of the 43rd annual meeting of the association for computational linguistics. ann arbor, michigan, usa, pages 
271   279.
chris quirk and simon 
translation. in: proceedings of the 2006 conference on empirical methods in natural language processing. sydney, 
australia, pages 62   69.
deyi
translation. in: proceedings of the second workshop on id151. prague, pages 40   47.
liangyou li, 
meeting of the association for computational linguistics (volume 1: long papers). berlin, germany, pages 97   107, 
liangyou li, 
eacl. (submitted) 

xiong, qun liu, and shouxun lin (2007). a dependency treelet string correspondence model for statistical machine 

andyway, qun liu (2016). graph-based translation via graph segmentation. in proceedings of the 54th annual 

andyway, qun liu (2016). context-aware graph segmentation for graph-based translation. in proceedings of 

corston-oliver (2006). the impact of parse quality on syntacticallyinformed statistical machine 

2005). dependency treelet translation: the convergence of statistical and example-based 

   

   

   
   

   

   

   

   

   

   

93

proceedings of amta 2016austin, oct 28 - nov 1, 2016q&a

94

proceedings of amta 2016austin, oct 28 - nov 1, 2016synchronous grammars
string-to-dependency models
dependency-to-string models
dependency graph-to-string models
translation models based on 
synchronous grammars 

95

proceedings of amta 2016austin, oct 28 - nov 1, 2016synchronous grammars

    synchronous id18 (sid18)

    hierarchical phrase-based models

    synchronous tree substitution grammar 

(stsg)
    tree-to-string models
    string-to-tree models
    tree-to-tree models

96

proceedings of amta 2016austin, oct 28 - nov 1, 2016sid18

97

proceedings of amta 2016austin, oct 28 - nov 1, 2016stsg

98

proceedings of amta 2016austin, oct 28 - nov 1, 2016why synchronous grammars?

target phrase reordering
   

recursive rules

linguistic knowledge
   

syntax annotations

   

   

99

proceedings of amta 2016austin, oct 28 - nov 1, 2016string-to-dependency models

   

   

   

   

100

extension of hierarchical phrase
-formed dependency structures
well
dependency tree on the target side
dependency language model

-based model

[shen et al., 2008, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016well-formed dependency structures

head node + full subtrees
continuous span

ill-formed

101

[shen et al., 2008, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016well-formed dependency structures

sibling subtrees
continuous span

ill-formed

102
102

[shen et al., 2008, 2010]
[shen et al., 2008, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016construct target dependency tree

   

four operations:

   

examples

103

[shen et al., 2008, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency language model

root word

left dependents: from 
right to left

right dependents: 
from left to right

104

[shen et al., 2008, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016training and decoding 

    training

    similar to [chiang, 2007]
    keep target dependency structures
    only extract well-formed dependency structures

    decoding

    similar to [chiang, 2007]
    build target dependency trees

    non-terminal

    pos of the head in fixed structures
    x for floating structures

105

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: the number of rules

tab 2: evaluation results

106

only phrases 
covered by well-
formed structures

pos-based non-
terminals

worse but use fewer 
translation rules

dependency language 
model is useful

syntactic non-terminals 
are helpful

[shen et al., 2008, 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency forest

107

[tu et al., 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016why dependency forest?

108

[quirk et al., 2006]

proceedings of amta 2016austin, oct 28 - nov 1, 2016string-to-dependency models

tab1   evaluation result

tab2   model size

109

korean--chinese

[tu et al., 2010]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-to-string model

fast decoding
   

linear in practice 

[huang et al., 2008]

dependency
handling non

-to-string model
-syntactic phrases

   

   

   

110

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-to-string model

head-dependent (hd) fragment

hd rule

head rule

111

[xie et al., 2011]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-to-string model

lexicalized hd rule:

unlexicalized rule

112

[xie et al., 2011]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-to-string model

    decoding

    cyk algorithm
    post-order traverse

tab: evaluation results

113

[xie et al., 2011]

proceedings of amta 2016austin, oct 28 - nov 1, 2016handling non-syntactic phrases

dependency structures are flat.

non-syntactic phrases:
   
   
   

large number
local reordering
important to phrase 
coverage
improve systems 
performance

   

114

syntactic phrases:
    smaller amount
    reliable
   
    easy to use in models

long-distance reordering

proceedings of amta 2016austin, oct 28 - nov 1, 2016handling non-syntactic phrases

important to phrase coverage 
and systems performance

115

[koehn et al., 2003]

proceedings of amta 2016austin, oct 28 - nov 1, 2016handling non-syntactic phrases

    methods:

    using constituent trees
    integrating fixed/floating structures
    decomposing dependency structures

116

proceedings of amta 2016austin, oct 28 - nov 1, 2016using constituent tree

phrases that cannot be captured by a dependency tree 
can be captured by a constituency tree

117

[meng et al., 2013]

proceedings of amta 2016austin, oct 28 - nov 1, 2016using constituent tree

hd rule:

chd rule:

118

[meng et al., 2013]

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: evaluation results. (+phrase pairs)

tab 2: the proportion (%) of 1-best translations that 
employ chdr-phrasal rules (chdr-phrasal sent.) and 
the proportion (%) of chdr-phrasal rules in all chdr 
rules in these translations (chdr-phrasal rule)

119

[meng et al., 2013]

proceedings of amta 2016austin, oct 28 - nov 1, 2016integrating fixed/floating structures 

fixed

floating

the same number of rules:
   
   

use bilingual phases during decoding
but focus on phrases covered by 
fixed/floating structures

120

[xie et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency decomposition

formal definition:

example:

during training: extract more rules
during decoding: translate an hd fragment in two steps

121

[li et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016decomposition during decoding

122

[li et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: influence of decomposition

tab 3: rule number

tab 2: influence of phrase pairs

123

[li et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016revisit non-syntactic phrases

    non-syntactic phrases exist in linguistically 

syntax-based models
    stsg (over sid18)
    focus on subtrees
    same generative capability on string pairs
    stronger generative capability on tree pairs

    add patches to tree-based models [previous slides]

124

[chiang, 2012]

proceedings of amta 2016austin, oct 28 - nov 1, 2016revisit non-syntactic phrases

    graphs vs trees

    more complex structures
    more powerful to model sentences

    amr for semantic, graphs for feature structures

    graph grammars
    non-syntactic phrases could be connected
    subgraphs, without the definitions of syntactic 

and non-syntactic phrases

125

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency graph-to-string models

    graph grammars

    edge replacement grammar (erg)
    node replacement grammar (nrg)
    models based on graph grammars

    erg-based model
    nrg-based model

126

proceedings of amta 2016austin, oct 28 - nov 1, 2016graph grammars

hierarchy of graph grammars:

127

ignore node label in 
this tutorial

ignore edge label in 
this tutorial

[kukluket al., 2008]

proceedings of amta 2016austin, oct 28 - nov 1, 2016edge replacement grammar

    graph

    edge-labeled
    directed

    graph fragment definition

    basic deviation units
    graph
    external nodes
    prevent hyperedges

128

[li et al., 2015]

proceedings of amta 2016austin, oct 28 - nov 1, 2016edge replacement grammar

129

[li et al., 2015]

proceedings of amta 2016austin, oct 28 - nov 1, 2016edge replacement grammar

    derivation

130

proceedings of amta 2016austin, oct 28 - nov 1, 2016synchronous edge replacement grammar

131

proceedings of amta 2016austin, oct 28 - nov 1, 2016synchronous edge replacement grammar

    serg has a stronger generative capacity over 

structure pairs than both sid18 and stsg
    stsg has a stronger generative capacity over structures 

than sid18 [chiang, 2012]

    any stsg can easily be converted into an serg by labeling 

edges in tree structures

    the following serg generates a trivial example of a graph 

pair, which no stsg can generate

132

proceedings of amta 2016austin, oct 28 - nov 1, 2016node replacement grammar

    derivation

embedding mechanism which can be 
ignored during parsing [kukluket al., 2008]

133

proceedings of amta 2016austin, oct 28 - nov 1, 2016synchronous node replacement grammar

    for machine translation
    snrg has a stronger generative capacity over 

structure pairs than both sid18 and stsg

134

proceedings of amta 2016austin, oct 28 - nov 1, 2016erg-based model

    create edge-labeled graphs
    practical restrictions
    training
    decoding

135

proceedings of amta 2016austin, oct 28 - nov 1, 2016create edge-labeled graphs

136

[li et al., 2015]

proceedings of amta 2016austin, oct 28 - nov 1, 2016practical restrictions

    word-order restriction
    continuity restriction
    non-terminal restriction

137

proceedings of amta 2016austin, oct 28 - nov 1, 2016word-order restriction

    keep word order

(a) c b d

(b) b c d

138

proceedings of amta 2016austin, oct 28 - nov 1, 2016continuity restriction

   

subgraphs cover continuous phrase 
exponential to polynomial)

(from 

decoding process

      

1

2

k

    (2    ) subgraphs

continuity

    (    2) subgraphs

139

proceedings of amta 2016austin, oct 28 - nov 1, 2016non-terminal restriction

one head

juxing/vv

zai/p

chenggong/ad

nanfei/nr

shijiebei/vv zai/p

chenggong/ad

multiple head

nanfei/nr

140

proceedings of amta 2016austin, oct 28 - nov 1, 2016training

similar to [chiang, 2007], but:

    check if the source side is a valid graph
    keep dependency structures in rules
    induce non-terminals for the source side

invalid

invalid

141

proceedings of amta 2016austin, oct 28 - nov 1, 2016decoding

142

treelet, non-syntactic phrase

proceedings of amta 2016austin, oct 28 - nov 1, 2016decoding

143

non-syntactic phrase
not connected in the tree

allow phrase reordering

proceedings of amta 2016austin, oct 28 - nov 1, 2016nrg-based model

    node-labeled graphs

144

proceedings of amta 2016austin, oct 28 - nov 1, 2016nrg-based model

    the same practical restrictions
    similar training and decoding processes
    rule example:

pos non-terminal

embedding mechanism is ignored

phrase reordering

145

proceedings of amta 2016austin, oct 28 - nov 1, 2016nrg-based model

treelet, non-syntactic phrase

146

proceedings of amta 2016austin, oct 28 - nov 1, 2016nrg-based model

non-syntactic phrase
not connected in the tree

allow phrase reordering

147

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

tab 1: id7 scores

tab 3: influence of sibling edges

tab 2: influence of pos non-terminals

tab 4: influence of edge types

148

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

149

correct reordering

proceedings of amta 2016austin, oct 28 - nov 1, 2016evaluation

150

proceedings of amta 2016austin, oct 28 - nov 1, 2016conclusion

    models based on synchronous grammars can 

learn recursive rules.

    non-terminals in recursive rules are used for 

target-phrase reordering

    graph grammars

    serg
    snrg

151

proceedings of amta 2016austin, oct 28 - nov 1, 2016references

corston-oliver (2006). the impact of parse quality on syntactically informed statistical 

haitao mi (2010). efficient incremental decoding for tree-to-string translation. in: proceedings 

meng, jun xie, linfeng song, yajuan lu, and qun liu (2013). translation with source constituency and 

kukluk (2007). id136 of node and edge replacement graph grammars. phd thesis. university of texas 

2007). hierarchical phrase-based translation. in: computational linguistics 33.2, pages 201   228. 
2012). grammars for language and genes: theoretical and empirical investigations. springer 

david chiang (
david chiang (
liang huang and 
of the 2010 conference on empirical methods in natural language processing. cambridge, ma, pages 273   283 
jacek 
at arlington. 
fandong
dependency trees. in: proceedings of the 2013 conference on empirical methods in natural language processing. 
seattle, washington, usa, pages 1066   1076
chris quirk and simon 
machine translation. in: proceedings of the 2006 conference on empirical methods in natural language 
processing. sydney, australia, pages 62   69.
libin
computational linguistics 36.4, pages 649   671. 
zhaopeng
tu, yang liu, young-sook hwang, qun liu, and shouxun lin (2010). dependency forest for statistical 
machine translation. in: proceedings of the 23rd international conference on computational linguistics (volume 
2). beijing, china, pages 1092   1100 
xie, haitao mi, and qun liu (2011). a novel dependency-to-string model for id151. 
jun 
in: proceedings of the 2011 conference on empirical methods in natural language processing. edinburgh, united 
kingdom, pages 216   226.
jun 
structures. in: proceedings of coling 2014, the 25th international conference on computational linguistics: 
technical papers. dublin, ireland, pages 2217   2226 
liangyou
conference on empirical methods in natural language processing, pages 33   43, lisbon, portug

shen, jinxi xu, and ralph weischedel (2010). string-to-dependency id151. in: 

xie, jinan xu, and qun liu (2014). augment dependency-to-string translation with fixed and floating 

li, andy way, qun liu. (2015). dependency graph-to-string translation. in proceedings of the 2015 

   
   
   

   

   

   

   

   

   

   

   

152

proceedings of amta 2016austin, oct 28 - nov 1, 2016q&a

153

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-based models
dependency format
download and try
lab session

154

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-based models

    dependency tree-to-string model

    liangyou li, jun xie, andy way, qun liu. (2014). transformation and 

decomposition for efficiently implementing and improving 
dependency-to-string model in moses. in proceedings of ssst-8.

    segmentational graph-based model

    liangyou li, andy way, qun liu. (2016). graph-based translation via 

graph segmentation. in proceedings of acl.

    context-ware segmentational graph-based model

    liangyou li, andy way, qun liu. (2016). context-aware segmentation 

for graph-based translation. submitted to eacl 2017.

    serg-based dependency graph-to-string model

    liangyou li, andy way, qun liu. (2015). dependency graph-to-string 

translation. in proceedings of emnlp.

    snrg-based dependency graph-to-string model

    paper in preparation

155

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency format

    using factors

    word | pos | fid | relation

she|prp|3|root  is|vbz|3|cop  very|rb|3|advmod  smart|jj|-1|root

156

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency format

    moses-graph/scripts/training/stanford-dep-2-

factor.perl

nsubj(smart-4, she-1)
cop(smart-4, is-2) 
advmod(smart-4, very-3) 
root(root-0, smart-4)

she|prp|3|root is|vbz|3|cop very|rb|3|advmod smart|jj|-1|root

157

proceedings of amta 2016austin, oct 28 - nov 1, 2016download and try

    binaries, sample data, and lab instructions

    https://drive.google.com/drive/folders/0bzwibrtqhxll

z2hltjvkwnnqwkk?usp=sharing

    source codes

    git clone 

https://llysuda@bitbucket.org/llysuda/moses-
graph.git

please follow the instructions to 
build your models    

158

proceedings of amta 2016austin, oct 28 - nov 1, 2016conclusion

159

proceedings of amta 2016austin, oct 28 - nov 1, 2016smt benefits from structures

    sequence-based

    phrase-based

    tree-based

    hierarchical phrase-based
    tree-to-string
    string-to-tree
    tree-to-tree
    forest-based
    dependency-based

    graph-based

    semantic-based
    dependency graph-based

160

proceedings of amta 2016austin, oct 28 - nov 1, 2016dependency-based evaluation

    automatic evaluation is important

    lexical
    semantic
    syntactic

    dependency structures and relations provide 

rich information for evaluation
    subtree, head-word chain, fixed/float structures
    dependency relations
    id56

161

proceedings of amta 2016austin, oct 28 - nov 1, 2016segmentational dependency-based models

    segmenting dependency structures provides 

various translation units
    edge
    path
    treelet

    dependency graphs provides subgraphs as the 

basic translation units.

162

proceedings of amta 2016austin, oct 28 - nov 1, 2016recursive dependency-based models

    synchronous grammars provide theoretical 

foundation for smt

    recursive rules provide information on how to 

perform phrase reordering

    smt systems also benefit from linguistic non-

terminals

    tree-based models are weak at translating non-

syntactic phrases

    dependency graphs naturally takes various 

phrases into consideration

163

proceedings of amta 2016austin, oct 28 - nov 1, 2016lab session

    codes and instructions for:

    dependency-to-string models
    segmentational dependency graph-based models
    recursive dependency graph-to-string models

    please cite corresponding papers when you 

use these models    

164

proceedings of amta 2016austin, oct 28 - nov 1, 2016thank you very much !

q&a

165

proceedings of amta 2016austin, oct 28 - nov 1, 2016