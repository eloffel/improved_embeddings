representation learning 

with networks

jure leskovec (@jure)
stanford university

jure leskovec, stanford

1

quickly about me

   assoc. prof. cs stanford
   chief scientist at pinterest
   research group:

   ml in graphs, social networks, data 
science, computational social science

   12 phd students, 6 postdocs

   cs, ee, math, stats, nlp, compbio, soc. sci.

   2 staff, 10 ms and bs students
   http://snap.stanford.edu

jure leskovec, stanford

2

why networks?
networks are a general 
language for describing 
and modeling complex 

systems

jure leskovec, stanford

3

jure leskovec, stanford

4

network!

jure leskovec, stanford

5

society

jure leskovec, stanford

6

media	&	information

jure leskovec, stanford

7

world	economy

jure leskovec, stanford

8

roads

jure leskovec, stanford

9

human	cell

jure leskovec, stanford

10

brain

jure leskovec, stanford

11

background & motivation

complex social, technological, 

and biological systems 

represented as networks of 

interconnected entities

jure leskovec, stanford

12

many data are networks

social networks

economic networks

b

biomedical networks

a

c

information networks: 

web & citations

internet
jure leskovec, stanford

networks of neurons

figure 3: higher-order cluster in the c. elegans neuronal network (28). a: the 4-node
   bi-fan    motif, which is over-expressed in the neuronal networks (1). intuitively, this motif
describes a cooperative propagation of information from the nodes on the left to the nodes on
the right. b: the best higher-order cluster in the c. elegans frontal neuronal network based on
the motif in (a). the cluster contains three ring motor neurons (rmel/v/r; cyan) with many

13

networks are mysterious

   not just how to compute, predict, model network data
   but also, why are networks the way they are? 
what do networks reveal about the underlying system?

jure leskovec, stanford

14

why networks? why now?
   universal language for describing complex 

data
   networks from science, nature, and technology 

are more similar than one would expect 

   shared vocabulary between fields

   computer science, social science, physics, 

economics, statistics, biology

   data availability (+computational challenges)

   web/mobile, bio, health, and medical

   impact!

   social networking, social media, drug design

jure leskovec, stanford

15

networks: why now?

age and size of networks

jure leskovec, stanford

cs!!

16

detecting fraud

buyers

sellers

(thickness)	purchase	price

which	transactions	are	
likely	to	be	fraudulent?

jure leskovec, stanford

17

social and content networks

jure leskovec, stanford

18

which link is missing?

(a)	vanilla	extract,	celery
(b)	pepper,	onion

recipe recommendation using ingredient networks. teng et al., 2012.

jure leskovec, stanford

19

knowledge on wikipedia

montreal

markus	kr  tzsch,	tu	dresden

all	wikipedia	languages	
2m	geo-located	articles

jure leskovec, stanford

20

knowledge on wikipedia

markus	kr  tzsch,	tu	dresden

https://ddll.inf.tu-dresden.de/web/wikidata/maps-06-2015/en

arabic	wikipedia
87k			geo-located	articles
467m	native	speakers	

jure leskovec, stanford

21

can you detect a hoax?

disinformation	on	the	web:	impact,	characteristics,	and	detection	of	

wikipedia	hoaxes.	kumar	et	al.	www	   16.

jure leskovec, stanford

22

diffusion

cascades!

jure leskovec, stanford

23

modeling epidemics

http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040961

jure leskovec, stanford

24

linkedin adoption cascade

60-90%	of	linkedin	users	signed	up	due	to	an	invitation	from	another	user.
global	diffusion	via	cascading	invitations:	structure,	growth,	and	homophily.	

anderson	et	al.,	www	   15.

jure leskovec, stanford

25

facebook cascades

jure leskovec, stanford

26

time: early (red), late (blue)

jure leskovec, stanford

27

copies of the same post

jure leskovec, stanford

28

single copy of a post

jure leskovec, stanford

29

this tutorial:

ml with networks

jure leskovec, stanford

30

four fundamental problems
   how to compute over networks

   network manipulation system
   how to do ml on networks
   building ml features of nodes
   how to detect communities

   uncovering modular organization
   how to infer and build networks

   from data to graphs

jure leskovec, stanford

31

problem 1:

computing with 
network data

snap: a general purpose network analysis and graph mining library.
r. sosic, j. leskovec. acm tist 2016.
ringo: interactive graph analytics on big-memory machines y. perez, r. sosic, 
a. banerjee, r. puttagunta, m. raison, p. shah, j. leskovec. sigmod 2015.

jure leskovec, stanford

32

networks: common language

movie 1

actor 1

movie 2

actor 2
movie 3

actor 4

friend

co-worker

mary

peter

brothers

friend

tom

actor 3

albert

protein 1

protein 2

protein 5

protein 9

|n|=4
|e|=4

jure leskovec, stanford

33

network representations

email network >
facebook friendships >
id191 >
collaboration networks >
mobile phone calls >
protein interactions >

> directed multigraph with self-edges
> undirected, unweighted
> unweighted, directed, acyclic
> undirected multigraph or weighted graph
> directed, (weighted?) multigraph
> undirected, unweighted with self-interactions

jure leskovec, stanford

34

how do you define a network?
   how to build a graph:

   what are nodes?
   what are edges?
   choice of the proper network representation of 
a given domain/problem determines our ability 
to use networks successfully:
   in some cases there is a unique, unambiguous 
   in other cases, the representation is by no means 
   the way you assign links will determine the nature 

representation
unique
of the question you can study

jure leskovec, stanford

35

end-to-end graph analytics

data

graph analytics

new knowledge

and insights

need end-to-end graph analytics 
system that is flexible, scalable, 

and allows for easy implementation 

of new algorithms.

jure leskovec, stanford

36

end-to-end graph analytics

new knowledge

and insights

data

graph analytics

   stanford network analysis platform (snap)
general-purpose, high-performance system 
for analysis and manipulation of networks
   c++, python (bsd, open source)
   http://snap.stanford.edu
of nodes and billions of edges

   scales to networks with hundreds of millions 

jure leskovec, stanford

37

availability of hardware

can networks fit in ram of a single 
machine?
   big machines are  getting affordable:

   server 1tb ram, 80 cores, $25k

   big ram is eating big data:

   yearly increase of dataset sizes: 20%
   yearly increase of ram sizes: 50%
bottom line: want to do graph 
analytics? get a big machine!

jure leskovec, stanford

38

graph analytics workflow

raw data

video, text, sound, events, 

structured data
relational tables

graph analytics

sensor data, gene 
sequences, documents,    
   input: structured data
   output: results of network analyses

   node, edge, network properties
   expanded relational tables
   networks

jure leskovec, stanford

39

snap overview

high-level language user front-end

interface with graph 
processing engine

metadata 
(provenance)

provenance 

script

snap: in-memory graph processing engine

filters

graph 
methods

graph 
containers

graph, table 
conversions

table 
objects

secondary storage

jure leskovec, stanford

40

graphs and methods

generation

manipulation

analytics

graph methods

graphs

graph containers
   snap supports several graph types

networks

   directed, undirected, multigraph

   >200 graph algorithms
   any algorithm works on any container

jure leskovec, stanford

41

overview of network analytics
   how to get a network

   from a real-world dataset
   generate a synthetic network
   from an existing network

   calculate network properties

   quick summary of network properties
   global connectivity: connected components
   local connectivity: node degrees
   key nodes in the network: node centrality
   neighborhood connectivity: triads, id91 coefficient
   graph traversal: breadth and depth first search
   groups of nodes: community detection
   global graph properties: spectral graph analysis
   core nodes: k-core decomposition

jure leskovec, stanford

42

snap implementation

   high-level front end

   python module
   uses swig for c++ interface 

   high-performance graph engine

   c++ based on snap

   multi-core support

   openmp to parallelize loops
   fast hash table, vector operations

jure leskovec, stanford

43

snap: published benchmarks

system

graphchi
turbograph

spark
graphx

powergraph

snap

hosts cpus
host
4
1
1
1
50
2
1
16
2
64
1
4

host configuration

time

6x core intel, 12gb ram

8x core amd, 64gb ram 158s
30s
97s
15s
8x hyper intel, 23gb ram 3.6s
20x hyper intel, 32gb ram 6.0s

8x core intel, 68gb ram

twitter2010, one iteration of id95

jure leskovec, stanford

44

snap.py resources

   prebuilt packages available for mac, windows, linux
   snap.py documentation:

http://snap.stanford.edu/snappy/index.html

http://snap.stanford.edu/snappy/doc/index.html

   quick introduction, tutorial, reference manual

   snap user mailing list
   developer resources

http://groups.google.com/group/snap-discuss

   software available as open source under bsd license
   github repository

https://github.com/snap-stanford/snap-python

jure leskovec, stanford

45

snap c++ resources
   source code available for mac, windows, linux
   snap documentation

http://snap.stanford.edu/snap/download.html

http://snap.stanford.edu/snap/doc.html
   quick introduction, user reference manual
   source code, see tutorials

   snap user mailing list
   developer resources

http://groups.google.com/group/snap-discuss

   software available as open source under bsd license
   github repository

https://github.com/snap-stanford/snap

   snap c++ programming guide

jure leskovec, stanford

46

network datasets:

snap network datasets
collection of over 70 web and social 
http://snap.stanford.edu/data
mailing list: http://groups.google.com/group/snap-datasets
   social networks: online social networks, edges represent 
interactions between people
   twitter and memetracker : memetracker phrases, links 
and 467 million tweets
   id191: nodes represent papers, edges 
represent citations
   collaboration networks: nodes represent scientists, 
edges represent collaborations (co-authoring a paper)
   amazon networks : nodes represent products and edges 
link commonly co-purchased products

jure leskovec, stanford

47

problem 2:

automatic feature 
learning in graphs

jure leskovec, stanford

48

machine learning in networks

?

?

?

?

?

machine 
learning

node classification

jure leskovec, stanford

49

machine learning lifecycle
   (supervised) machine learning 

lifecycle: this feature, that feature. 
every single time!

raw 
data

structured 

data

learning 
algorithm  

model

feature 
engineering

automatically 
learn the features

downstream 
prediction task

jure leskovec, stanford

50

id171 in graphs

goal: efficient task-independent 

id171 for machine learning 

in networks!

2    :          &

node
u

vec

   &

feature representation, 

embedding

jure leskovec, stanford

51

why is it hard?

graph representations learning is hard:
   images are fixed size

   id197

   text is linear
   id97

   graphs are neither of these!
   node numbering is arbitrary 
(node isomorphism problem)

   much more complicated structure

jure leskovec, stanford

52

two ideas, three algorithms
   1)    linearizing    the graph

   create a    sentence    for each node 

using id93
   node2vec, ohmnet

   2) graph convolution networks

   propagate information between the 

nodes of the graph
   graphsage

jure leskovec, stanford

53

node2vec: random walk 

based (unsupervised) 

id171

node2vec: scalable id171 for networks
a. grover, j. leskovec. kdd 2016.

jure leskovec, stanford

54

unsupervised id171
   intuition: find embedding of nodes to 
d-dimensions that preserves similarity

   idea: learn node embedding such 

that nearby nodes are close together

   given a node u, how do we define 

      (         neighbourhood of u obtained 

nearby nodes?

by some strategy s

jure leskovec, stanford

55

(1)

max

max

two standard assumptions:

two standard assumptions:

in order to make the optimization problem tractable, we make

f is a matrix of size |v |     d parameters. for every source node
[25, 28]. we seek to optimize the following objective function,
in order to make the optimization problem tractable, we make
u 2 v , we de   ne ns(u)     v as a network neighborhood of node
which predicts which nodes are members of u   s network neighbor-
unsupervised id171
u generated through a neighborhood sampling strategy s.
hood ns(u) based on the learned node features f:
    conditional independence. we factorize the likelihood by as-
we proceed by extending the skip-gram architecture to networks
suming that the likelihood of observing a neighborhood node
log p r(ns(u)|f (u))
is independent of observing any other neighborhood node
[25, 28]. we seek to optimize the following objective function,
given the feature representation of the source.
which predicts which nodes are members of u   s network neighbor-
hood ns(u) based on the learned node features f:
p r(ni|f (u))
log p r(ns(u)|f (u))

   goal: find embedding     (    ) that 
predicts nearby nodes     (     :
f xu2v
p r(ns(u)|f (u)) = yni2ns (u)
f xu2v
p r(ns(u)|f (u)) = yni2ns (u)
estimate     (    ) using stochastic id119.
pv2v exp(f (v)    f (u))
f xu2v

    conditional independence. we factorize the likelihood by as-
suming that the likelihood of observing a neighborhood node
    symmetry in feature space. a source node and neighbor-
is independent of observing any other neighborhood node
   make independence assumption:
hood node have a symmetric effect over each other in fea-
given the feature representation of the source.
ture space. accordingly, we model the conditional likeli-
hood of every source-neighborhood node pair as a softmax
two standard assumptions:
   then softmax:
unit parametrized by a dot product of their features.
    conditional independence. we factorize the likelihood by as-
    symmetry in feature space. a source node and neighbor-
suming that the likelihood of observing a neighborhood node
p r(ni|f (u)) =
hood node have a symmetric effect over each other in fea-
is independent of observing any other neighborhood node
ture space. accordingly, we model the conditional likeli-
given the feature representation of the source.
with the above assumptions, the objective in eq. 1 simpli   es to:
hood of every source-neighborhood node pair as a softmax

in order to make the optimization problem tractable, we make

[  log zu + xni2ns (u)

exp(f (ni)    f (u))

p r(ni|f (u))

jure leskovec, stanford

56

between two kinds of similarities: structural equivalence and ho-
mophily [13]. under the homophily hypothesis [7, 42] nodes that
are highly interconnected and belong to similar network clusters
or communities tend to share features and thus should be embed-
ded closely together (e.g., nodes s1 and u in fig. 1 belong to the
same network community). in contrast, under the structural equiv-
alence [11, 40] nodes that have similar structural roles in networks
tend to share features and thus should be embedded closely together
(e.g., nodes u and s6 in fig. 1 act as hubs of their corresponding
communities).
lence does not emphasize connectivity; nodes could be far apart in
the network and still have the same structural role. in real-world,
these equivalence notions are not exclusive; networks commonly
exhibit both behaviors where properties of some nodes exhibit ho-
mophily while others re   ect structural equivalence.

in producing representations that re   ect either of the equivalences.
in particular, the neighborhoods sampled by the bfs lead to em-
beddings that correspond to structural equivalence. intuitively, we

jure@cs.stanford.edu

jure leskovec
stanford university

two classic strategies to define a 

how to determine     (    
neighborhood     (     of a given node     :
    +,(     ={	    1,    3,    4}
    6,(     ={	    7,    8,    9}

and edges. a typical solution involves hand-engineering domain-
speci   c features based on expert knowledge. even if one discounts
the tedious work of feature engineering, such features are usually
designed for speci   c tasks and do not generalize across different

s5 
local microscopic view
global macroscopic view

figure 1: bfs and dfs search strategies from node u (k = 3).

bfs 
dfs 

jure leskovec, stanford

s8 

s9 

s6 

s7 

s3 

s2 

s4 

s1 

u 

57

prediction tasks over nodes and edges in networks require careful
effort in engineering features for learning algorithms. recent re-
search in the broader    eld of representation learning has led to sig-
ni   cant progress in automating prediction by learning the features
themselves. however, present approaches are largely insensitive to

here we propose node2vec, an algorithmic framework for learn-
ing feature representations for nodes in networks. in node2vec, we
learn a mapping of nodes to a low-dimensional space of features
that maximizes the likelihood of preserving distances between net-
work neighborhoods of nodes. we de   ne a    exible notion of node   s
network neighborhood and design a biased random walk proce-

bfs vs. dfs

u

bfs:

micro-view of 
neighbourhood

dfs:

macro-view of 
neighbourhood

jure leskovec, stanford

58

interpolating bfs and dfs

biased random walk      that given a node 
     generates neighborhood     (    
   return parameter     :
   in-out parameter     :

   two parameters:

   return back to the previous node

   moving outwards (dfs) vs. inwards (bfs)

jure leskovec, stanford

59

biased id93

s5

s1

1
s4

biased 2nd-order id93 explore 
network neighborhoods:

u     s4    ?
   bfs-like: low value of     
   dfs-like: low value of     
    ,     can learned in a semi-supervised way

u
s1
s5

60

jure leskovec, stanford

1/q

1/p

u

node2vec algorithm
   1) compute random walk probs.

   2) simulate      id93 of length     

starting from each node u

   3) optimize the node2vec objective 
using stochastic id119

linear-time complexity. 
all 3 steps are individually parallelizable

jure leskovec, stanford

61

experiments: micro vs. macro
interactions of characters in a novel:

spectral id91
deepwalk
line
node2vec
node2vec settings (p,q)
gain of node2vec [%]

0.0405
0.2110
0.0784
0.2581
0.25, 0.25

0.0681
0.1768
0.1447
0.1791
4, 1
1.3

blogcatalog

algorithm

dataset

22.3

0.0395
0.1274
0.1164
0.1552
4, 0.5
21.8

ppi wikipedia

table 2: macro-f1 scores for multilabel classi   cation on blog-
catalog, ppi (homo sapiens) and wikipedia word cooccur-
rence networks with a balanced 50% train-test split.
labeled data with a grid search over p, q 2 {0.25, 0.50, 1, 2, 4}.
under the above experimental settings, we present our results for
two tasks under consideration.
4.3 multi-label classi   cation

figure 3: complementary visualizations of les mis  rables co-
appearance network generated by node2vec with label colors
re   ecting homophily (top) and structural equivalence (bottom).

p=1, q=2

microscopic view of the 
network neighbourhood

also exclude a recent approach, grarep [6], that generalizes line
to incorporate information from network neighborhoods beyond 2-
hops, but does not scale and hence, provides an unfair comparison
with other neural embedding based id171 methods. apart
from spectral id91 which has a slightly higher time complex-
ity since it involves id105, our experiments stand out

jure leskovec, stanford

in the multi-label classi   cation setting, every node is assigned
one or more labels from a    nite set l. during the training phase, we
observe a certain fraction of nodes and all their labels. the task is
to predict the labels for the remaining nodes. this is a challenging
task especially if l is large. we perform multi-label classi   cation
on the following datasets:
    blogcatalog [44]: this is a network of social relationships
macroscopic view of the 
of the bloggers listed on the blogcatalog website. the la-
bels represent blogger interests inferred through the meta-
network neighbourhood
data provided by the bloggers. the network has 10,312 nodes,
333,983 edges and 39 different labels.

p=1, q=0.5

    protein-protein interactions (ppi) [5]: we use a subgraph
of the ppi network for homo sapiens. the subgraph cor-
responds to the graph induced by nodes for which we could
obtain labels from the hallmark gene sets [21] and represent

62

scalability of node2vec

jure leskovec, stanford

63

incomplete network data (ppi)

0.20

0.15

0.10

0.05

e
r
o
c
s

1
f
-
o
r
c
a
m

0.00

0.0

0.20

0.15

0.10

0.05

e
r
o
c
s

1
f
-
o
r
c
a
m

0.5

0.6

0.00

0.0

jure leskovec, stanford

0.2

0.1
fraction of missing edges

0.3

0.4

0.2

0.1
fraction of additional edges

0.3

0.4

0.5

0.6

64

node2vec: discussion

general-purpose id171 in networks:
   an explicit locality preserving objective for 
id171.
   biased id93 capture diversity of 
network patterns.
   scalable and robust algorithm with excellent 
empirical performance.
   future extensions would involve designing 
random walk strategies entailed to network 
with specific structure such as 
heterogeneous networks and signed 
networks.

jure leskovec, stanford

65

node2vec: discussion
   there are many other methods in 

class of walk-based methods:
   line
   deepwalk
   structural deep network embedding

jure leskovec, stanford

66

ohmnet: extension to 
hierarchical networks

predicting	multicellular	function	through	multi-layer	tissue	
networks.	m.	zitnik,	j.	leskovec	bioinformatics	2017.

jure leskovec, stanford

67

multilayer networks

let   s generalize node2vec to 

multilayer networks!

jure leskovec, stanford

68

multi-layer networks

   each network is a layer     @=(    @,    @)
hierarchy    , map      encodes parent-

   similarities between layers are given in 

child relationships

jure leskovec, stanford

69

the approach

   computational framework that 

learns features of every node and at 
every scale based on:
  
  

edges within each layer
inter-layer relationships between 
nodes active on different layers

jure leskovec, stanford

70

features in multi-layer network

, hierarchy    
   given: layers     @
   layers     @ @e1..g are in leaves of    
   goal: learn functions:     @:    @      &

jure leskovec, stanford

71

features in multi-layer network
   approach has two components:

   per-layer objectives: nodes with similar 

network neighborhoods in each layer 
are embedded close together

   hierarchical dependency objectives: 

nodes in nearby layers in hierarchy are 
encouraged to share similar features

jure leskovec, stanford

72

node similarity

per-layer objective: node2vec
   intuition: for each layer, find a mapping 

of nodes to      dimensions that preserves 
   approach: similarity of nodes      and      is 
   given node      in layer      we define nearby 
nodes     @(    ) based on id93 
starting at node     

defined based on similarity of their 
network neighborhoods

jure leskovec, stanford

73

representation such that it predicts 

per-layer objective: node2vec

   given node      in layer     , learn        s 
nearby nodes     @(    ):
   given      layers, maximize:
   i = xu2vi

our goal is to take layer gi and learn fi which embeds nodes from
where the conditional likelihood of every node-neighborhood node pair
similar network regions, or nodes with similar structural roles, closely
is modeled as an independent softmax unit parameterized by a dot
together. in ohmnet, we aim to achieve this goal by specifying the
product of nodes    features, which corresponds to a single-layer feed-
following objective function for each layer gi. given a node u 2 vi,
forward neural network (grover and leskovec, 2016). given a node
the objective function !i seeks to predict, which nodes are members of
u, maximization of !i(u) tries to maximize classi   cation of nodes in
u   s network neighborhood ni(u) based on the learned node features fi:
u   s network neighborhood based on u   s learned representation. more
precisely, we use each current node as an input to a log-linear classi   er,
and predict nodes that are in the neighborhood of the current node.
where the conditional likelihood of every node-neighborhood node pair
is modeled as an independent softmax unit parameterized by a dot
product of nodes    features, which corresponds to a single-layer feed-
forward neural network (grover and leskovec, 2016). given a node
u, maximization of !i(u) tries to maximize classi   cation of nodes in

!i(u) = log p r(ni(u)|fi(u)),

the objective    i is de   ned for each layer i:

for
for i = 1, 2, . . . , t.

[grover et al. 2016]

!i(u),

jure leskovec, stanford

74

interdependent layers

   so far, we did not consider hierarchy    

   node representations in different layers are 

learned independently of each other

how to model dependencies between 
layers when learning node features?

jure leskovec, stanford

75

id173 for node u that resides in hierarchy i:

dependencies among the layers to de   ne a joint objective for id173
of the learned features of proteins.

we propose to use the hierarchy in the learning process by
ci(u) =
incorporating a recursive structure into the id173 term for every
object in the hierarchy. speci   cally, we propose the following form of
id173 for node u that resides in hierarchy i:

(3)

this recursive form of id173 enforces the features of node u in
the hierarchy i to be similar to the features of node u in i   s parent    (i)
under the euclidean norm. when regularizing features of all nodes in the
elements i of the hierarchy, we obtain:

1
2kfi(u)   f   (i)(u)k2
2.

interdependent layers
1
2kfi(u)   f   (i)(u)k2
2.

   given node     , learn        s representation in 
layer      to be close to        s representation in 
parent     (    ):
   multi-scale: repeat at every level of    
ci = xu2li
    @ has all layers appearing in sub-hierarchy rooted at     
ci = xu2li

ci(u) =

ci(u),

jure leskovec, stanford

76

this recursive form of id173 enforces the features of node u in
the hierarchy i to be similar to the features of node u in i   s parent    (i)
under the euclidean norm. when regularizing features of all nodes in the
elements i of the hierarchy, we obtain:

ci(u),

(4)

where li = vi if i 2 t is a leaf object in the hierarchy, and otherwise

learning model to a given multi-layer network and a given hierarchy, i.e.,
by    nding the mapping functions f1, f2, . . . , ft that maximize the data

ohmnet: final model

learning node features in multi-layer networks

given the data, ohmnet aims to solve the following maximum

likelihood optimization problem:

solve maximum likelihood problem:

max

f1,f2,...,f|m|xi2t

   i     xj2m

cj ,

per-layer 
network 
objectives

hierarchical 
dependency 
objectives

which includes the single-layer network objectives for all network layers,
and the hierarchical dependency objectives for all hierarchy objects.

jure leskovec, stanford

77

experiments: biological nets
   proteins are worker molecules
   understanding protein function 

has great biomedical and 
pharmaceutical implications

107 genome-wide  tissue-specific 
protein interaction networks
   584 tissue-specific cellular functions 
   examples (tissue, cellular function): 

   (renal cortex, cortex development)
   (artery, pulmonary artery morphogenesis)

jure leskovec, stanford

78

brain tissues

brain

brainstem

cerebellum

frontal
lobe

parietal

lobe

occipital

lobe

temporal

lobe

midbrain

substantia

nigra

pons

medulla
oblongata

9 brain tissue ppi networks

in two-level hierarchy

jure leskovec, stanford

79

meaningful node embeddings

jure leskovec, stanford

80

protein function prediction

42% improvement 
over state-of-the-art 
on the same dataset

jure leskovec, stanford

81

id21

   transfer functions to unannotated tissues
   task: predict functions in target tissue without 

access to any annotation/label in that tissue
target tissue
ohmnet tissue non-specific improvement
placenta
spleen
liver
forebrain
blood plasma
smooth muscle
average

0.758
0.779
0.741
0.755
0.703
0.729
0.746
reported are auc values

0.684
0.712
0.553
0.632
0.540
0.583
0.617

11%
10%
34%
20%
40%
25%
21%

jure leskovec, stanford

82

graphsage: supervised 

id171

inductive	representation	learning	on	large	graphs.	
w.	hamilton, r.	ying, j.	leskovec.	arxiv 2017.

jure leskovec, stanford

83

so far   

   node2vec and ohmnet are:

   unsupervised
   there is no model

   only the training (but no prediction phase)
   they directly learn node coordinates
   can   t generalize to unseen nodes. entire 

embedding has to be retrained

   too many parameters to scale to large graphs

jure leskovec, stanford

84

inductive id171
desiderata:
   scale to large networks
   generalize to new nodes
   build a model of node embedding

   training phase can be slow but 
prediction phase should be fast

jure leskovec, stanford

85

inductive id171

train on one graph

generalize to an entirely new graph

inductive node embedding          generalize to entirely unseen graphs

e.g., train on protein interaction graph from model organism a and 
generate embeddings on newly collected data about organism b

jure leskovec, stanford

86

inductive id171

train	at	t=0

new	node	arrives	at	t=1

generate	embedding	for	
new	node	at	t=2

many application settings constantly encounter previously unseen nodes.
e.g., reddit, youtube, googlescholar,    .
need to generate new embeddings    on the fly   

jure leskovec, stanford

87

what makes it hard?

   generalizing to unseen subgraphs 

requires    aligning    the new subgraph 
to data we   ve seen before
   closely related to the subgraph 

isomorphism problem

   node neighborhoods have no 

canonical ordering
   no straightforward way to    linearize    

neighborhood features

jure leskovec, stanford

88

id197
   id98 on an image:

goal is to generalize convolutions beyond simple lattices.
leverage node features/attributes (e.g., text, degrees)

niepert,	mathias,	mohamed	ahmed,	and	konstantin	kutzkov.	"learning	convolutional	neural	networks	for	graphs."	icml.	2016.	(image source)

jure leskovec, stanford

89

id197
   id197: 

learning convolutional neural networks for graphs

a sequence of words. however, for numerous graph col-
lections a problem-speci   c ordering (spatial, temporal, or
otherwise) is missing and the nodes of the graphs are not
in correspondence. in these instances, one has to solve two
problems: (i) determining the node sequences for which
neighborhood graphs are created and (ii) computing a nor-
malization of neighborhood graphs, that is, a unique map-
ping from a graph representation into a vector space rep-
resentation. the proposed approach, termed patchy-san,
addresses these two problems for arbitrary graphs. for each
input graph, it    rst determines nodes (and their order) for
which neighborhood graphs are created. for each of these
nodes, a neighborhood consisting of exactly k nodes is ex-
tracted and normalized, that is, it is uniquely mapped to a
space with a    xed linear order. the normalized neighbor-

problem: for a subgraph how to come with 
canonical node ordering

figure 2. an illustration of the proposed architecture. a node
sequence is selected from a graph via a graph labeling procedure.
for some nodes in the sequence, a local neighborhood graph is as-
sembled and normalized. the normalized neighborhoods are used
as receptive    elds and combined with existing id98 components.

niepert,	mathias,	mohamed	ahmed,	and	konstantin	kutzkov.	"learning	convolutional	neural	networks	for	graphs."	icml.	2016.	(image source)

jure leskovec, stanford

90

......neighborhood graph constructionconvolutional architecturenode sequence selectiongraph id172id197
   id197:

   has to also crop excess nodes, etc. 

niepert,	mathias,	mohamed	ahmed,	and	konstantin	kutzkov.	"learning	convolutional	neural	networks	for	graphs."	icml.	2016.	(image source)

jure leskovec, stanford

91

new idea: id197s+aggregation
   kipf et al. (2017) proposed id197s for 
semi-supervised learning on graphs:

   x     node feature vector
   a     graph adjacency matrix
   w(0) , w(1)     model parameters

    (no1)=    (        n    (n)), and     s =    

   more generally:

kipf,	thomas	n.,	and	max	welling.	"semi-supervised	classification	with	graph	convolutional	networks."	iclr.	2017.

jure leskovec, stanford

92

id197s + aggregation

    (no1)=    (        n    (n)), and     s =    

   notice this avoids coming up with a 
node ordering for a given subgraph

   essentially, a fixed, center-surround filter 

that    averages    the features of 
immediate neighbors

jure leskovec, stanford

93

graphsage

   adapt the id197 idea to inductive node 

embedding

   generalize beyond simple convolutions
   demonstrate that this generalization

   leads to significant performance gains
   allows the model to learn about local 
structures

jure leskovec, stanford

94

graphsage: sample & aggregate

       (no1)=    (        n    (n)), and     s =    

   important: directly leverage input node 

features (e.g., attributes, degrees)

jure leskovec, stanford

95

graphsage: sample & aggregate

   sample and aggregate:

  

   aggregate information from neighbors
   concatenate it with previous features 
   pass through a single layer network

      n and the activation function     

jure leskovec, stanford

96

graphsage algorithm

initialize representations as features
k =    search depth   

aggregate information from neighbors

concatenate neighborhood info with 
current representation and propagate

jure leskovec, stanford

97

wl isomorphism test
   the classic weisfeiler-lehman graph 
isomorphism test is a special case of 
graphsage
trainable neural nets:

   we replace the hash function with 

hash

x

x

shervashidze,	nino,	et	al.	"weisfeiler-lehman	graph	kernels."	journal	of	machine	learning	research	(2011).

jure leskovec, stanford

98

aggregation

?

jure leskovec, stanford

99

aggregation

   need a function that can aggregate 

over a set of points: 
   i.e., should be permutation invariant

mean: simple, element-wise mean
lstm: apply lstm to random permutation of the points
max-pooling:

element-wise max

jure leskovec, stanford

100

how do we learn the model?
   the essential part of the model:

   two types of parameters:

   aggregate function can have params.
   matrix wk

   how do we learn them?

   id119. we can use supervised or 

unsupervised objective function:

classification (cross-id178) loss

jure leskovec, stanford

101

graphsage vs. node2vec

   directly leverage feature information
   instead of optimizing embeddings directly, 

optimize a neural network that aggregates 
neighborhood features.

jure leskovec, stanford

102

graphsage vs. id197

   adapt to inductive setting (e.g., 
unsupervised loss, neighborhood 
sampling, minibatch optimization)

   generalized notion of    aggregating 

neighborhood   

jure leskovec, stanford

103

references

  

  

  
  

  

  

  
  

  

  

snap: a general purpose network analysis and graph mining library.
r. sosic, j. leskovec. acm tist 2016.
ringo: interactive graph analytics on big-memory machines y. perez, r. sosic, a. banerjee, 
r. puttagunta, m. raison, p. shah, j. leskovec. sigmod 2015.
node2vec: scalable id171 for networks. a. grover, j. leskovec. kdd 2016.
predicting multicellular function through multi-layer tissue networks. m. zitnik, j. leskovec 
bioinformatics 2017.
inductive representation learning on large graphs. w. hamilton, r. ying, j. leskovec. arxiv
2017.
deepwalk: online learning of social representations. b. perozzi, r. al-rfou, s. skiena. kdd 
2014.
line: large-scale information network embedding. j. tang, et al.  www 2015.
weisfeiler-lehman graph kernels. n. shervashidze, et al. journal of machine learning 
research 2011.
semi-supervised classification with id197. t. kipf, m. welling. iclr. 
2017.
code:

  
  

http://snap.stanford.edu/node2vec/
http://snap.stanford.edu/graphsage/

jure leskovec, stanford

104

