networks and 
optimization

jure leskovec (@jure)
joint work with austin benson, david hallac, 
aditya grover, rok sosi  , steven boyd

jure leskovec, stanford

1

optimization in networks
   large problems can often be 

represented as a network!
   nodes    a series of subproblems
   edges    relationships that define the 

coupling between different 
subproblems

   examples: cyber-physical and social 
networks, financial transactions, . . .

6/27/17

jure leskovec, stanford miner

2

application: house prices

house price prediction:
   nodes: houses
   edges: nearby houses
   each house i has 
its own price model: 

    "=    "%                   +    ",   #                        +    "4   #                           

   but, nearby houses have similar 

models:
   houses in the same    neighborhood    share 

a common model 

6/27/17

jure leskovec, stanford miner

3

house prices as network lasso

f(xj)

f(xi)

||xi-xj||2

xi    	house	i
model	parameters
f(xi)    	prediction	
error	at	house	i
||xi-xj||2    houses	i
and j	share	model
parameters

simultaneous house id91 (xi = xj) 
and network optimization!

6/27/17

jure leskovec, stanford miner

4

network lasso

f(xj)

j

f(xi)
i
||xi-xk||2

||xi-xj||2
f(xk)
k

simultaneous node id91 (xi = xj) 
and network optimization

6/27/17

jure leskovec, stanford miner

5

network lasso

   definition:

   network lasso penalty encourages nearby nodes 

to cluster together and share common values

6/27/17

jure leskovec, stanford miner

6

network lasso: nodes

6/27/17

jure leskovec, stanford miner

7

network lasso: edges

   not laplacian id173!

   incentivizes edge differences to be exactly zero
   when many edges are in consensus, the 
nodes are clustered into sets with equal 
values of xi
   houses share the exact same regression weights
   network lasso problem can be thought of as 

simultaneous id91 and optimization

6/27/17

jure leskovec, stanford miner

8

id173 penalty   

6/27/17

jure leskovec, stanford miner

9

solution: admm

   for large graphs, standard solvers don   t scale!
   alternating direction method of multipliers 

(admm) splits the problem into subproblems
   parallelizable & scalable
   each component (node/edge) solves its own private 

objective function, passes this solution on to its 
neighbors, and repeats

   without any global coordination, the entire network 

converges to the optimal solution!
   works for any convex fi, xi

6/27/17

jure leskovec, stanford miner

10

scalability

6/27/17

jure leskovec, stanford miner

11

housing: performance

   housing price prediction 

performance on held-out houses

6/27/17

jure leskovec, stanford miner

12

housing: id173 path

6/27/17

jure leskovec, stanford miner

13

housing: neighborhoods

6/27/17

jure leskovec, stanford miner

14

housing: neighborhoods

6/27/17

jure leskovec, stanford miner

15

housing: neighborhoods

6/27/17

jure leskovec, stanford miner

16

housing: neighborhoods

6/27/17

jure leskovec, stanford miner

17

how do we infer 

networks?

inferring time-varying networks via the graphical lasso
d. hallac, s. boyd, j. leskovec.

jure leskovec, stanford

18

sensing complex systems
sensor arrays measure complex systems

i in many applications, we generate large sequences of timestamped

sensors are everywhere

       sensors    have a broad de   nition

observations

introduction

infrastructure
machines
multidimensional time-series data!

biomedicine

2

jure leskovec, stanford

19

from time-series to networks

how do we encode structure and 
dependencies in temporal data?

networks

i networks are a great way of encoding structure in the data

i we use markov random    elds (mrfs), which denote conditional

independencies between di   erent entities

jure leskovec, stanford

20

network id136 from time series data

networks form time series
model dynamic dependency structure

i convert a sequence of timestamped sensor observations into a

time-varying network

jure leskovec, stanford

21

   goal: given a set of time-series 

our approach
   estimate inverse covariance matrix   
   if   "9=0 then      is conditionally indep. of     

data, learn the dependency network

   network can evolve over time

   allows us to gain insights into the 

process and detect anomalies

jure leskovec, stanford

22

benefits

to learn a time-varying network, we 
can encode different types of temporal 
evolution:

   single nodes rewiring
   smoothly varying graph over time
   a few sharp breakpoints where the 

entire network changes

darpa simplex pi meeting, september 19-21, 

2016                      miner project

23

our approach: tvgl
time-varying graphical lasso
   given: multivariate time series data
   goal: infer a sequence of networks 

(inverse covariance matrices)
   1) segment the time series
   2) infer the network for each segment

note: we have n,t parameters
n     number of sensors
t     number of time points

jure leskovec, stanford

24

time-varying graphical lasso

problem formulation

sequence of inv. cov. matrices   ":
tpi=2
      "  ":	log-likelihood of data at time     
     " bc,%: sparsity of the cov. matrix
      (  "     "h%): temporal consistency

tpi=1 li(   i) +  k   ikod,1 +  

    ni     number of observations at ti
    si     empirical covariance at ti

where (3 components):

i li(   i) = ni(log det    i   tr(si   i))     log likelihood (up to a

 (   i      i 1)

constant and scale)

minimize

i k    kod,1     o   -diagonal `1 norm

jure leskovec, stanford

25

time-varying graphical lasso

different types of network evolution:
   single edge rewiring:

temporal consistency       "     "h% models 
            =        9l
 9l
            =        9l,
 9l
            =   maxl     9l
 9
            =        9 ,
 9

   edges smoothly rewiring over time:

   block-wise restructuring

   entire graph to rewires:

jure leskovec, stanford

26

inferring the networks
alternating direction method of multipliers (admm)
   represent the problem as a graph:

   parallelizable and scalable
    parallelizable
    scalable

   distributed admm optimization solver:
i distributed optimization
   without any global coordination, the 
message passing algorithm quickly 
i without any global coordination, the message passing algorithm
converges to the optimal solution

quickly converges to the optimal solution

27
i we derived closed-form solutions for the admm subproblems,

jure leskovec, stanford

experiment: scalability

table 1: f1 score and temporal deviation (td) ratio for the lo-
cal and global shifts, using the two baselines methods and three

the baselines are unable to detect that there was a shift in the net-
work at t = 50, whereas this sudden shift was clearly discovered
by tvgl. we later use this idea in section 7 to detect signi   cant

selection of penalty parameter. while the tvgl outperformed
the two baselines regardless of the penalty type, even greater gains
can be achieved by selecting the correct evolutionary penalty. in

figure 3: scalability comparison of our tvgl solver with var-

jure leskovec, stanford

28

example application: stocks
   each stock is a    sensor   
   stock prices are correlated
   use stock data to find interesting 

patterns and changes in the 
correlation network

jure leskovec, stanford

29

stocks

i stock prices are correlated, so we can look at historical data to    nd

result: correlation network
   network reveals relationships 

i network sparsity shows the relationships between the di   erent

interesting patterns

companies

between the different companies

jure leskovec, stanford

30

i perturbed-node penalty:

result: evolution

    the event that had the largest single-node e   ect on the dynamics of

the network

on jan 27 network abruptly changes

what happened? apple rewired. why?

jure leskovec, stanford

31

result: ipad was introduced

i it occurred on the day that apple announced the original ipad!

what caused this shift?

jure leskovec, stanford

32

time series segmentation 

and id91

jure leskovec, stanford

33

using the resulting networks
   can we use these networks for other 

tasks as well?

   another high-value task in multivariate 
time series analysis is in segmentation 
and discovery of repeated motifs
   we can use networks to achieve this 

goal!

jure leskovec, stanford

34

task idea

   given: multivariate time series data
   goal: split the time series into a 

sequential timeline of a few key events
   for example, from a fitness tracking 

device:
      walking    for 30 minutes, then    running    for 1 

hour, then    walking    for 10 minutes, then 
   sitting    for 5 minutes

jure leskovec, stanford

35

the challenge

   can we simultaneously split the time 

series while also learning the 
structural    network signature    that 
defines each cluster?

   our solution: toeplitz inverse 

covariance-based id91 (ticc)

jure leskovec, stanford

36

solution idea

jure leskovec, stanford

37

solution idea

segment the time
series into a
sequence of 
clusters (i.e., 
the alphabet) with 
stable temporal 
dependency structure

   for each cluster infer a temporal 

multilayer network

jure leskovec, stanford

38

ticc

   jointly tries to optimize three goals:
   log likelihood     each point in the time 
series should    look like    the resulting 
network it is assigned to

   sparsity     the networks should be sparse
   temporal consistency     assignments 
should not change too frequently across 
the time series

jure leskovec, stanford

39

why toeplitz matrices?
   enforces time invariance and allows 

for    cross-time    edges!

   covariance structure of cluster i:

jure leskovec, stanford

40

solving the ticc problem
   ticc is highly non-convex
   we solve it using an em-like algorithm

   alternate between:

   id145 to assign points to 

clusters

   admm to update the cluster parameters

jure leskovec, stanford

41

assigning points to clusters

   find minimum cost path 1   t

   node cost: negative log-likelihood
   edge cost: beta

jure leskovec, stanford

42

scalability

   10m observations, each 50-dimensional, in 

just 25 minutes!
   scales linearly with the number of observations

6/27/17

43

case study: automobiles
   analyzed a 1 hour driving session

   36,000 samples @ 10hz

   7 sensors:

   brake pedal position
   forward acceleration
   lateral acceleration
   steering wheel angle
   vehicle velocity
   engine rpm
   gas pedal position

jure leskovec, stanford

44

running ticc
   run ticc with k=5 clusters
   we plot the centrality score of each 

node in each cluster

jure leskovec, stanford

45

running ticc
   run ticc with k=5 clusters
   we plot the centrality score of each 

node in each cluster

jure leskovec, stanford

46

plotting the resulting clusters
   green = straight, white = slowing down, red = 
   results are very consistent across the data!

turning, blue = speeding up

jure leskovec, stanford

47

conclusion

jure leskovec, stanford

48

many data are networks

social networks

collaboration networks

systems biology networks

a

b

c

information networks: 

web & citations

internet

networks of neurons

figure 3: higher-order cluster in the c. elegans neuronal network (28). a: the 4-node
   bi-fan    motif, which is over-expressed in the neuronal networks (1). intuitively, this motif
describes a cooperative propagation of information from the nodes on the left to the nodes on
the right. b: the best higher-order cluster in the c. elegans frontal neuronal network based on
the motif in (a). the cluster contains three ring motor neurons (rmel/v/r; cyan) with many

49

network data

network data brings several core data 
mining methodologies into play:
   working with network data is messy

   not just    wiring diagrams    but also dynamics 

and (meta)-data (features, attributes)

   computational challenges
   large-scale network data

   algorithmic models as vocabulary for 

expressing complex scientific questions 
   social science, physics, biology

jure leskovec, stanford

50

four fundamental problems
   how to compute over networks

   snap & big-ram machines

   how to infer and build networks
   time-varying graphical lasso, ticc

   how to do ml on networks

   node2vec, graphsage

   how to detect organization

   higher-order structures in networks

jure leskovec, stanford

51

tools for networks

stanford network analysis platform (snap) 
is a general purpose, high-performance 
system graph analytics system

   http://snap.stanford.edu
   scales to massive networks with hundreds of 
   released snap 3.0 for c++ and python

millions of nodes and billions of edges

   12k+ downloads in 2016
   610 stars, 320 forks on github
   904k hits on snap datasets in 2016

jure leskovec, stanford

52

references

  

  

  

  

  

network lasso: id91 and optimization in large graphs. d. hallac, j. 
leskovec, s. boyd. acm sigkdd international conference on knowledge 
discovery and data mining (kdd), 2015.

snapvx: a network-based id76 solver. d. hallac, c. wong, s. 
diamond, a. sharang, r. sosi  , s. boyd, j. leskovec. journal of machine 
learning research (jmlr), 18(4):1   5, 2017.

learning the network structure of heterogeneous data via pairwise exponential 
markov random fields. y. park, d. hallac, s. boyd, j. leskovec. artificial 
intelligence and statistics conference (aistats), 2017.

network id136 via the time-varying graphical lasso. d. hallac, y. park, s. 
boyd, j. leskovec.acm sigkdd international conference on knowledge 
discovery and data mining (kdd), 2017.

[code and data]

toeplitz inverse covariance-based id91 of multivariate time series data. d. 
hallac, s. vare, s. boyd, j. leskovec. acm sigkdd international conference on 
knowledge discovery and data mining (kdd), 2017.

[code and data]

jure leskovec, stanford

53

