human decisions and 
machine predictions

jure leskovec (@jure)
including joint work with himabindu lakkaraju,
sendhil mullainathan, jon kleinberg, jens ludwig

machine learning

humans vs. machines

humans vs. machines
   given the same data/features x 
machines tend to beat human:
   games: chess, alphago, poker
   image classification
   id53 (ibm watson)
   but humans tend see more than 

machines do. humans make 
decisions based on (x,z)!

humans make decisions
humans make decisions based on x,z!
   machine is trained based on p(y|x)
   but humans use p(y|x,z) to make 

decisions

   could this be a problem?

yes! why? because the data is 
selectively labeled and cross 
validation can severely overestimate 
model performance.

plan for the talk
1) can machines make better 
decisions than humans?

2) why do humans make mistakes?

3) can machines help humans make 
better decisions?

jure leskovec (@jure) stanford university

5

example problem:
criminal court bail 

assignment

jure leskovec, stanford university

6

example decision problem
u.s. police makes >12m arrests/year
q: where do people wait for trial? 
release vs. detain is high stakes:

   pre-trial detention spells avg. 2-3 months 

(can be up to 9-12 months)

   nearly 750,000 people in jails in us
   consequential for jobs, families 

as well as crime

jure leskovec (@jure) stanford university

7

judge   s decision problem
judge must decide whether to release 
(bail) or not (jail)
outcomes: defendant when out on bail 
can behave badly:

   fail to appear at trial
   commit a non-violent crime
   commit a violent crime
the judge is making a prediction!

jure leskovec (@jure) stanford university

8

bail as a decision problem
   not assessing guilt on this crime
   not a punishment
   judge can only pay attention to 
failure to appear and crime risk

jure leskovec (@jure) stanford university

9

the ml task

we want to build a predictor
that will based on defendant   s 

criminal history predict defendant   s 

future bad behavior

defendant

characteristics

learning 
algorithm

prediction

of bad
behavior
in the future

jure leskovec (@jure) stanford university

10

what characteristics to use?
only administrative features

   common across regions
   easy to get
   no    gaming the system    problem
only features that are legal to use

   no race, gender, religion

note: judge predictions may not obey either of these 

jure leskovec (@jure) stanford university

11

data: defendant features

  
  

  
  
  
  
  

  
  
  
  
  
  

age at first arrest
times sentenced residential 
correction
level of charge
number of active warrants
number of misdemeanor cases
number of past revocations
current charge domestic 
violence
is first arrest
prior jail sentence
prior prison sentence
employed at first arrest
currently on supervision
had previous revocation

  

  
  

  
  
  

  

  
  

arrest for new offense while on 
supervision or bond
has active warrant
has active misdemeanor 
warrant
has other pending charge
had previous adult conviction
had previous adult 
misdemeanor conviction
had previous adult felony 
conviction
had previous failure to appear
prior supervision within 10 years

(only legal features and easy to get)

jure leskovec (@jure) stanford university

12

data: kentucky & federal

jurisdiction number 
of cases

fraction 
released 
people

fraction 

of 
crime

failure to 
appear at 

trial

non-
violent 
crime

violent
crime

kentucky

362k

73%

17%

10%

4.2%

2.8%

federal 
pretrial 
system

1.1m

78%

19%

12%

5.4%

1.9%

jure leskovec (@jure) stanford university

13

applying machine learning
   just apply machine learning:

   use labeled dataset (released defendants)
   train a learning model to predict whether 
defendant when our on bail will misbehave

   report accuracy on the held-out test set

jure leskovec, stanford university

14

prediction model

                                                                                                                                 

         

            

                                    

                                                                              

                                    

         

            

         

            

                                    

         

            

                                                                                                   

                                                                        

         

            

         

            

            

         

                                                                              

                                                

                                                            

                                                         

                                          

                                    

                                                         

            

               

            

            

                  

               

            

               

            

               

            

               

            

               

            

               

            

         

         

            

            

            

            

            

            

            

         

            

but there is a problem with this   

top four levels the decision tree
id203 of not committing a crime

typical tree depth: 20-25

jure leskovec (@jure) stanford university

15

the problem

   issue: judge sees factors the machine 

does not
   judge makes decisions based on p(y|x,z)

   x     prior crime history
   z     other factors not seen by the machine

   machine makes decisions based on p(y|x)
   problem: judge is selectively labeling 

the data based on p(y|x,z)!

jure leskovec, stanford university

16

problem: selective labels
judge is selectively labeling the dataset

judge

release

crime?

jail

crime?

yes

no

?

?

   we can only train on released people
   by jailing judge is selectively hiding labels!

jure leskovec, stanford university

17

selection on unobservables
selective labels introduce bias:

   say young people with no tattoos have 
no risk for crime. judge releases them.
   but machine does not observe whether 

defendants have tattoos.

   so, the machine would falsely conclude 

that all young people do no crime.

   and falsely presume that by releasing all 
young people it does better than judge!

jure leskovec, stanford university

18

challenges: 

(1) selective labels
(2) unobservables z

jure leskovec, stanford university

19

solution: 3 properties
our solution depends on three key 
properties:
   multiple judges 
   natural variability between judges
   random assignment of cases to 

judges

   one-sidedness of the problem

solution: tree observations
1) problem is one-sided:

   releasing a jailed person is a problem
   jailing a released person is not

algorithm
release   jail

     
    

e
g
d
u
j

e
s
a
e
e
r

l

 
 
 
 
l
i

a
j

jure leskovec (@jure) stanford university

21

solution: three observations
2) in federal system cases are 
randomly assigned
   this means that on average all 

judges have similar cases

jure leskovec (@jure) stanford university

22

solution: three observations
3) natural variability between judges
   due to human nature there will be 
some variability between the judges
   some judges are more strict while 

others are more lenient

jure leskovec (@jure) stanford university

23

solution: contraction
solution: use most lenient judges!

   take released population of a lenient 

   which of those would we jail to become 

   compare crime rate to a strict human 

judge 

less lenient

judge

note: does not rely on judges having    similar    predictions

jure leskovec (@jure) stanford university

24

solution: contraction

making lenient judges strict:

released by a lenient judge

use algorithm to make a lenient judge more strict

strict human judge

defendants (ordered by crime id203) 

what makes contraction work:
   1) judges have similar cases 
(random assignment of cases)

   2) selective labels are one-sided

jure leskovec (@jure) stanford university

25

evaluating the prediction
predictions create a new release rule:

   parameterized by % released

   for every defendant predict p(crime)
   sort them by increasing p(crime) 

and    release    bottom k

   note: this is different than auc 

what is the fraction released vs. 

crime rate tradeoff?

jure leskovec (@jure) stanford university

26

overall predicted crime

judges
17%

machine learning
11%

release rate
release	rate

jure leskovec (@jure) stanford university

holding	release	rate	
constant,	crime	rate	is
reduced	by	0.102
(58.45%	decrease)

all	features	+
ml	algorithm

88%

73%
standard errors too small to display
id90 beat logreg by 30%
contrast with auc roc

27

)
a
c
v
n

	
,

a
c
e
n
t
a
	
,
r
a
 
e
t
m
f
	
s
i
r
e
c
d
 
l
u
l
a
l
c
r
e
n
v
i
(
o
	
e
t
a
r
e
m

	

i
r
c

predicted failure to appear

holding release rate 
constant, crime rate is
reduced by 0.06
(60% decrease)

e
t
a
r
a
t
f

 

judges decisions
10%

ml algorithm
4.1%

release rate
jure leskovec (@jure) stanford university

73%

28

predicted non-violent crime

holding release rate 
constant, crime rate is
reduced by 0.025
(58% decrease)

 
e
t
a
r
a
c
n

 

judges decisions
4.2%

1.7%

release rate

73%

notes:	standard	errors	too	small	to	display	on	graph
jure leskovec (@jure) stanford university

29

predicted violent crime

holding release rate 
constant, crime rate is
reduced by 0.014
(50% decrease)

judges decisions
2.8%

1.4%

 

 
e
t
a
r
a
c
v
n

release rate
notes:	standard	errors	too	small	to	display	on	graph
jure leskovec (@jure) stanford university

73%

30

effect of race

(cid:104)(cid:28)(cid:35)(cid:72)(cid:50) (cid:100)(cid:44) (cid:95)(cid:28)(cid:43)(cid:66)(cid:28)(cid:72) (cid:54)(cid:28)(cid:66)(cid:96)(cid:77)(cid:50)(cid:98)(cid:98)

(cid:95)(cid:50)(cid:72)(cid:50)(cid:28)(cid:98)(cid:50) (cid:95)(cid:109)(cid:72)(cid:50)
(cid:46)(cid:66)(cid:98)(cid:105)(cid:96)(cid:66)(cid:35)(cid:109)(cid:105)(cid:66)(cid:81)(cid:77) (cid:81)(cid:55) (cid:46)(cid:50)(cid:55)(cid:50)(cid:77)(cid:47)(cid:28)(cid:77)(cid:105)(cid:98) (cid:85)(cid:34)(cid:28)(cid:98)(cid:50) (cid:95)(cid:28)(cid:105)(cid:50)(cid:86)

(cid:67)(cid:109)(cid:47)(cid:59)(cid:50)

(cid:27)(cid:72)(cid:59)(cid:81)(cid:96)(cid:66)(cid:105)(cid:63)(cid:75)
(cid:108)(cid:98)(cid:109)(cid:28)(cid:72) (cid:95)(cid:28)(cid:77)(cid:70)(cid:66)(cid:77)(cid:59)

(cid:74)(cid:28)(cid:105)(cid:43)(cid:63) (cid:67)(cid:109)(cid:47)(cid:59)(cid:50) (cid:81)(cid:77) (cid:95)(cid:28)(cid:43)(cid:50)

(cid:49)(cid:91)(cid:109)(cid:28)(cid:72) (cid:95)(cid:50)(cid:72)(cid:50)(cid:28)(cid:98)(cid:50) (cid:95)(cid:28)(cid:105)(cid:50)(cid:98)
(cid:55)(cid:81)(cid:96) (cid:28)(cid:72)(cid:72) (cid:95)(cid:28)(cid:43)(cid:50)(cid:98)

(cid:42)(cid:96)(cid:66)(cid:75)(cid:50) (cid:95)(cid:28)(cid:105)(cid:50) (cid:46)(cid:96)(cid:81)(cid:84) (cid:95)(cid:50)(cid:72)(cid:28)(cid:105)(cid:66)(cid:112)(cid:50) (cid:83)(cid:50)(cid:96)(cid:43)(cid:50)(cid:77)(cid:105)(cid:28)(cid:59)(cid:50) (cid:81)(cid:55) (cid:67)(cid:28)(cid:66)(cid:72) (cid:83)(cid:81)(cid:84)(cid:109)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77)
(cid:62)(cid:66)(cid:98)(cid:84)(cid:28)(cid:77)(cid:66)(cid:43) (cid:74)(cid:66)(cid:77)(cid:81)(cid:96)(cid:66)(cid:105)(cid:118)
(cid:88)(cid:51)(cid:82)(cid:78)(cid:56)
(cid:88)(cid:106)(cid:106)(cid:82)(cid:51)

(cid:34)(cid:72)(cid:28)(cid:43)(cid:70)
(cid:88)(cid:57)(cid:51)(cid:100)(cid:100)

(cid:105)(cid:81) (cid:67)(cid:109)(cid:47)(cid:59)(cid:50)

(cid:88)(cid:82)(cid:82)(cid:106)(cid:57)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:82)(cid:121)(cid:86)

(cid:88)(cid:121)(cid:51)(cid:56)(cid:57)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:121)(cid:51)(cid:86)

(cid:88)(cid:121)(cid:51)(cid:56)(cid:56)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:121)(cid:51)(cid:86)

(cid:88)(cid:121)(cid:51)(cid:100)(cid:106)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:121)(cid:51)(cid:86)

(cid:121)(cid:87)

(cid:88)(cid:56)(cid:100)(cid:106)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:78)(cid:86)

(cid:88)(cid:106)(cid:82)(cid:101)(cid:107)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:100)(cid:86)

(cid:88)(cid:51)(cid:51)(cid:78)(cid:107)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:82)(cid:51)(cid:86)

(cid:64)(cid:107)(cid:57)(cid:88)(cid:101)(cid:51)(cid:87)

(cid:64)(cid:107)(cid:57)(cid:88)(cid:101)(cid:57)(cid:87)

(cid:64)(cid:107)(cid:106)(cid:88)(cid:121)(cid:107)(cid:87)

(cid:88)(cid:56)(cid:78)(cid:51)(cid:57)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:78)(cid:86)

(cid:88)(cid:56)(cid:100)(cid:106)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:78)(cid:86)

(cid:88)(cid:57)(cid:51)(cid:100)(cid:100)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:78)(cid:86)

(cid:88)(cid:106)(cid:121)(cid:107)(cid:106)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:100)(cid:86)

(cid:88)(cid:106)(cid:82)(cid:101)(cid:107)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:100)(cid:86)

(cid:88)(cid:106)(cid:106)(cid:82)(cid:51)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:51)(cid:86)

(cid:88)(cid:78)(cid:121)(cid:121)(cid:100)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:82)(cid:100)(cid:86)

(cid:88)(cid:51)(cid:51)(cid:78)(cid:107)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:82)(cid:51)(cid:86)

(cid:88)(cid:51)(cid:82)(cid:78)(cid:56)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:106)(cid:86)

(cid:74)(cid:28)(cid:105)(cid:43)(cid:63) (cid:71)(cid:81)(cid:114)(cid:50)(cid:96) (cid:81)(cid:55)
(cid:34)(cid:28)(cid:98)(cid:50) (cid:95)(cid:28)(cid:105)(cid:50) (cid:81)(cid:96) (cid:67)(cid:109)(cid:47)(cid:59)(cid:50)

ml does not beat the judge by 

(cid:88)(cid:51)(cid:121)(cid:106)(cid:78)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:106)(cid:86)
(cid:76)(cid:81)(cid:105)(cid:50)(cid:98)(cid:44) (cid:104)(cid:28)(cid:35)(cid:72)(cid:50) (cid:96)(cid:50)(cid:84)(cid:81)(cid:96)(cid:105)(cid:98) (cid:105)(cid:63)(cid:50) (cid:84)(cid:81)(cid:105)(cid:50)(cid:77)(cid:105)(cid:66)(cid:28)(cid:72) (cid:59)(cid:28)(cid:66)(cid:77)(cid:98) (cid:81)(cid:55) (cid:105)(cid:63)(cid:50) (cid:28)(cid:72)(cid:59)(cid:81)(cid:96)(cid:66)(cid:105)(cid:63)(cid:75)(cid:66)(cid:43) (cid:96)(cid:50)(cid:72)(cid:50)(cid:28)(cid:98)(cid:50) (cid:96)(cid:109)(cid:72)(cid:50) (cid:96)(cid:50)(cid:72)(cid:28)(cid:105)(cid:66)(cid:112)(cid:50) (cid:105)(cid:81) (cid:105)(cid:63)(cid:50) (cid:68)(cid:109)(cid:47)(cid:59)(cid:50) (cid:28)(cid:105) (cid:105)(cid:63)(cid:50) (cid:68)(cid:109)(cid:47)(cid:59)(cid:50)(cid:0)(cid:98) (cid:96)(cid:50)(cid:72)(cid:50)(cid:28)(cid:98)(cid:50) (cid:96)(cid:28)(cid:105)(cid:50) (cid:114)(cid:66)(cid:105)(cid:63)
(cid:96)(cid:50)(cid:98)(cid:84)(cid:50)(cid:43)(cid:105) (cid:105)(cid:81) (cid:43)(cid:96)(cid:66)(cid:75)(cid:50) (cid:96)(cid:50)(cid:47)(cid:109)(cid:43)(cid:105)(cid:66)(cid:81)(cid:77)(cid:98) (cid:28)(cid:77)(cid:47) (cid:98)(cid:63)(cid:28)(cid:96)(cid:50) (cid:81)(cid:55) (cid:105)(cid:63)(cid:50) (cid:68)(cid:28)(cid:66)(cid:72) (cid:84)(cid:81)(cid:84)(cid:109)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:105)(cid:63)(cid:28)(cid:105) (cid:66)(cid:98) (cid:35)(cid:72)(cid:28)(cid:43)(cid:70)(cid:45) (cid:62)(cid:66)(cid:98)(cid:84)(cid:28)(cid:77)(cid:66)(cid:43) (cid:81)(cid:96) (cid:50)(cid:66)(cid:105)(cid:63)(cid:50)(cid:96) (cid:35)(cid:72)(cid:28)(cid:43)(cid:70) (cid:81)(cid:96) (cid:62)(cid:66)(cid:98)(cid:84)(cid:28)(cid:77)(cid:66)(cid:43)(cid:88) (cid:104)(cid:63)(cid:50) (cid:125)(cid:96)(cid:98)(cid:105)
(cid:96)(cid:81)(cid:114) (cid:98)(cid:63)(cid:81)(cid:114)(cid:98) (cid:105)(cid:63)(cid:50) (cid:98)(cid:63)(cid:28)(cid:96)(cid:50) (cid:81)(cid:55) (cid:105)(cid:63)(cid:50) (cid:47)(cid:50)(cid:55)(cid:50)(cid:77)(cid:47)(cid:28)(cid:77)(cid:105) (cid:84)(cid:81)(cid:84)(cid:109)(cid:72)(cid:28)(cid:105)(cid:66)(cid:81)(cid:77) (cid:81)(cid:112)(cid:50)(cid:96)(cid:28)(cid:72)(cid:72) (cid:105)(cid:63)(cid:28)(cid:105) (cid:66)(cid:98) (cid:35)(cid:72)(cid:28)(cid:43)(cid:70) (cid:81)(cid:96) (cid:62)(cid:66)(cid:98)(cid:84)(cid:28)(cid:77)(cid:66)(cid:43)(cid:88) (cid:104)(cid:63)(cid:50) (cid:98)(cid:50)(cid:43)(cid:81)(cid:77)(cid:47) (cid:96)(cid:81)(cid:114) (cid:98)(cid:63)(cid:81)(cid:114)(cid:98) (cid:105)(cid:63)(cid:50) (cid:96)(cid:50)(cid:98)(cid:109)(cid:72)(cid:105)(cid:98) (cid:81)(cid:55) (cid:105)(cid:63)(cid:50)
(cid:81)(cid:35)(cid:98)(cid:50)(cid:96)(cid:112)(cid:50)(cid:47) (cid:68)(cid:109)(cid:47)(cid:59)(cid:50) (cid:47)(cid:50)(cid:43)(cid:66)(cid:98)(cid:66)(cid:81)(cid:77)(cid:98)(cid:88) (cid:104)(cid:63)(cid:50) (cid:105)(cid:63)(cid:66)(cid:96)(cid:47) (cid:96)(cid:81)(cid:114) (cid:98)(cid:63)(cid:81)(cid:114)(cid:98) (cid:105)(cid:63)(cid:50) (cid:96)(cid:50)(cid:98)(cid:109)(cid:72)(cid:105)(cid:98) (cid:81)(cid:55) (cid:105)(cid:63)(cid:50) (cid:109)(cid:98)(cid:109)(cid:28)(cid:72) (cid:28)(cid:72)(cid:59)(cid:81)(cid:96)(cid:66)(cid:105)(cid:63)(cid:75)(cid:66)(cid:43) (cid:96)(cid:50)(cid:64)(cid:96)(cid:28)(cid:77)(cid:70)(cid:66)(cid:77)(cid:59) (cid:96)(cid:50)(cid:72)(cid:50)(cid:28)(cid:98)(cid:50) (cid:96)(cid:109)(cid:72)(cid:50)(cid:45) (cid:114)(cid:63)(cid:66)(cid:43)(cid:63) (cid:47)(cid:81)(cid:50)(cid:98) (cid:77)(cid:81)(cid:105)
(cid:109)(cid:98)(cid:50) (cid:96)(cid:28)(cid:43)(cid:50) (cid:66)(cid:77) (cid:84)(cid:96)(cid:50)(cid:47)(cid:66)(cid:43)(cid:105)(cid:66)(cid:77)(cid:59) (cid:47)(cid:50)(cid:55)(cid:50)(cid:77)(cid:47)(cid:28)(cid:77)(cid:105) (cid:96)(cid:66)(cid:98)(cid:70) (cid:28)(cid:77)(cid:47) (cid:75)(cid:28)(cid:70)(cid:50)(cid:98) (cid:77)(cid:81) (cid:84)(cid:81)(cid:98)(cid:105)(cid:64)(cid:84)(cid:96)(cid:50)(cid:47)(cid:66)(cid:43)(cid:105)(cid:66)(cid:81)(cid:77) (cid:28)(cid:47)(cid:68)(cid:109)(cid:98)(cid:105)(cid:75)(cid:50)(cid:77)(cid:105)(cid:98) (cid:105)(cid:81) (cid:28)(cid:43)(cid:43)(cid:81)(cid:109)(cid:77)(cid:105) (cid:55)(cid:81)(cid:96) (cid:96)(cid:28)(cid:43)(cid:50)(cid:88) (cid:65)(cid:77) (cid:105)(cid:63)(cid:50) (cid:55)(cid:81)(cid:109)(cid:96)(cid:105)(cid:63) (cid:96)(cid:81)(cid:114) (cid:114)(cid:50)

racially discriminating

(cid:88)(cid:121)(cid:51)(cid:100)(cid:101)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:121)(cid:51)(cid:86)

(cid:88)(cid:57)(cid:51)(cid:100)(cid:100)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:78)(cid:86)

(cid:88)(cid:106)(cid:82)(cid:101)(cid:107)
(cid:85)(cid:88)(cid:121)(cid:121)(cid:107)(cid:100)(cid:86)

(cid:64)(cid:107)(cid:107)(cid:88)(cid:100)(cid:57)(cid:87)

source of errors

t

e
a
r
 
e
s
a
e
e
r
 
e
g
d
u
j

l

judges err on riskiest defendants

predicted risk

predicted risk vs. jailing

least likely to jail by judge

most likely to jail

k
s
i
r
 
d
e
t
c
d
e
r
p

i

why do judges do poorly?
odds are against algorithm: judge sees 
many things the algorithm does not:

   offender history:
observable by both

   demeanor:

      i looked in his eyes and saw his soul   
   unobservable by the algorithm

can we diagnose judges    mistakes and 
help them make better decisions?

jure leskovec (@jure) stanford university

34

why do judges do poorly?
two possible reasons why judges are 
making mistakes:

   misuse of observable features

   these are the administrative features 

available to the algorithm

    e.g.: previous crime history, etc.

   misuse of unobservable features
   features not seen by the algorithm

       i looked in his eyes and saw his soul   

jure leskovec (@jure) stanford university

35

a different test

predict judge   s decision 

   training data does not include whether 

defendant actually commits a crime

gives a release rule 
release if predicted judge would release
   this weights features the way judge does

what does it do differently then?
   does not use the unobservables!

jure leskovec (@jure) stanford university

36

predicting the judge

build a model to predict judge   s behavior

(and not if a defendant will commit a crime)

   we predict what the judge will do 

mix judge and the model:  

1       	                    *    	                	                                +	
                                                                         
does this beats the judge (for some     )?

   model will do better only if the judge 

misweighs unobservable features

jure leskovec (@jure) stanford university

37

artificial judge beats the judge

pure judge

pure ml model
of the judge

model of the judge beats 

the judge by 35%

jure leskovec (@jure) stanford university

38

putting it all together

0.0

   0.1

   0.2

 

a
t
f
e
g
n
a
h
c

 
t

n
e
c
r
e
p

   0.3

   

   

   

   0.4

   

   0.5

   

   

   

   0.20

      
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

         

         

         

         

judges
lenient judge + ml
predicted judge
random

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   0.15

   0.10

percentage point change in release rate

   0.05

0.00

recap so far   

algorithm beats human judge in the bail 
assignment problem
artificial judge beats the real judge
this means judge is misweighing
unobservable features

can we model human mistakes?

jure leskovec (@jure) stanford university

40

modeling human decisions
true labels
judge j 

items i

decisions of j
release

release

jail

jail

crime

no crime

crime

no crime

jure leskovec (@jure) stanford university

.
 
.
 
.

41

modeling human decisions
defendant i
judge j 

decision of 
judge j on 
defendant i

decision
pc,c
pn,c

pc,n
pn,n

h
t
u
r
t

true label ti

id203 that j 
decided    no 
crime    to a 
defendant who 
will do    crime   

jure leskovec (@jure) stanford university

42

problem setting

   judges and defendants described by 

input:

features

output:

   discover groups of judges and 
defendants that share common 
confusion matrices

jure leskovec (@jure) stanford university

43

joint confusion model

item i

cluster

di

judge j 

cluster 

cj

a3

a2

a1
a4
judge attributes 

decision of 
judge j on item i

decision

t

h
u
r
t

confusion matrix

a5

b3

b2

b1
item attributes 

b4

zi
item label

similar judges share confusion matrices 

when deciding on similar items

44

what do we learn on bail?
judge attributes: 
   year, county, number of previous cases, 
number of previous felony cases, 
number of previous misd. cases, number 
of previous minor cases

defendant attributes: 
   previous crime history, personal attributes 
(children/married etc.), social status 
(education/house own/moved a lot in past 
10 years), current offense details

jure leskovec (@jure) stanford university

45

why judges make mistakes

decision

0.9

0.1

0.1

0.9

h
t
u
r
t

0.6

0.4

0.4

0.6

less experienced judges make more mistakes
   left: judges with high number of cases
   right: judges with low number of cases

jure leskovec (@jure) stanford university

46

why judges make mistakes

decision

0.8

0.2

0.5

0.5

h
t
u
r
t

0.8

0.3

0.2

0.7

judges with many felony cases are stricter
   left: judges with many felony cases
   right: judges with few felony cases

jure leskovec (@jure) stanford university

47

why judges make mistakes

decision

0.9

0.1

0.1

0.9

h
t
u
r
t

0.5

0.5

0.5

0.5

defendants who are 
single, did felonies, 
and moved a lot are 
accurately judged 

defendants who have 
kids are confusing to 

judges

jure leskovec (@jure) stanford university

48

conclusion

new tool to understand human 
decision making
the framework can be applied to 
various other questions: 

   decisions about patient treatments
   assigning students to intervention 

programs

   hiring and promotion decisions

jure leskovec (@jure) stanford university

49

power of algorithms

algorithms can help us understand if 
human judges are biased

algorithms can be used to diagnose 
reason for bias

key insight

   more than prediction tools
   serve as behavioral diagnostic tools 

jure leskovec (@jure) stanford university

50

focusing on decisions

not just about prediction
key is starting with decision:

   performance benchmark: current 

   human    decisions
   not roc but    human roc   

   what are we really optimizing?

focus on decision also raises new 
concerns

jure leskovec (@jure) stanford university

51

reflections: new concerns
1) selective labels

   we don   t see labels of people that are 

jailed

   extremely common problem:

prediction -> decision -> action

   which outcomes we see depends on 

our decisions

jure leskovec (@jure) stanford university

52

reflections: new concerns
2) data responds to prediction
   whenever we make a 

prediction/decision we affect the 
labels/data we see in the future

   creates a self-reinforcing feedback 

loop

jure leskovec (@jure) stanford university

53

reflections: new concerns
3) constraints on input features:
for example, race is an illegal feature

   our models don   t use it

but, is that enough?

   many other characteristics correlate 

with race

how do we deal with this additional 
constraint?

jure leskovec (@jure) stanford university

54

reflections: new concerns
4) omitted payoff bias
   is minimizing the crime rate really the 

right goal?

   there are other important factors
   consequences of jailing on the family
   jobs and the workplace
   future behavior of the defendant

   how could we model these?

jure leskovec (@jure) stanford university

55

conclusion

bail is not the only prediction problem

many important problems  in public 
policy are predictive problems!

potential for large impact

jure leskovec (@jure) stanford university

56

references

  

  

  

the selective labels problem: evaluating algorithmic predictions in 
the presence of unobservables. h. lakkaraju, j. kleinberg, j. 
leskovec, j. ludwig, s. mullainathan. acm sigkdd international 
conference on knowledge discovery and data mining (kdd), 2017.
human decisions and machine predictions. j. kleinberg, h. 
lakkaraju, j. leskovec, j. ludwig, s. mullainathan. nber working 
paper, 2017.
a bayesian framework for modeling human evaluations by h. 
lakkaraju, j. leskovec, j. kleinberg, s. mullainathan. siam 
international conference on data mining (sdm) 2015.

   confusions over time: an interpretable bayesian model to 

characterize trends in decision making. h. lakkaraju, j. 
leskovec. neural information processing systems (nips), 2016.

jure leskovec (@jure) stanford university

57

