machine learning                                                             

                       

   

      srihari 

id202 for machine 

learning 

sargur n. srihari 

srihari@cedar.buffalo.edu 

1 

machine learning                                                             

                       

   

what is id202? 

       id202 is the branch of mathematics 

concerning linear equations such as  

      srihari 

 

 a1x1+   ..+anxn=b 

      in vector notation we say atx=b 
      called a linear transformation of x 

       id202 is fundamental to geometry, for 
defining objects such as lines, planes, rotations 

linear equation a1x1+   ..+anxn=b  
defines a plane in (x1,..,xn) space 
straight lines define common solutions 
to equations 

2 

machine learning                                                             

                       

   

why do we need to know it? 

      srihari 

       id202 is used throughout engineering 
      because it is based on continuous math rather than 
discrete math 
       computer scientists have little experience with it 

       essential for understanding ml algorithms 

      e.g., we convert input vectors (x1,..,xn) into outputs 
by a series of linear transformations 

       here we discuss: 

      concepts of id202 needed for ml 
      omit other aspects of id202 

3 

machine learning                                                             

   

      srihari 

id202 topics 

                       

      scalars, vectors, matrices and tensors 
      multiplying matrices and vectors 
      identity and inverse matrices 
      linear dependence and span 
      norms 
      special kinds of matrices and vectors 
      eigendecomposition 
      singular value decomposition 
      the moore penrose pseudoinverse 
      the trace operator 
      the determinant 
      ex: principal components analysis 

4 

machine learning                                                             

                       

   

      srihari 

scalar 

       single number 

      in contrast to other objects in id202, 
which are usually arrays of numbers 
       represented in lower-case italic x 

      they can be real-valued or be integers 

       e.g., let          be the slope of the line 

  x    !

       defining a real-valued scalar 

       e.g., let          be the number of units 

       defining a natural number scalar 

  n    !

5 

machine learning                                                             

                       

   

      srihari 

vector 

       an array of numbers arranged in order 
       each no. identified by an index 
       written in lower-case bold such as x 

      its elements are in italics lower case, subscripted 

x1
x2

xn

   
   
   
   
   
   
   
   
   
   

		

   
   
   
   
   
   
   
   
   
   

x =

		   

       if each element is in r then x is in rn 
       we can think of vectors as points in space 
      each element gives coordinate along an axis 

6 

machine learning                                                             

                       

   

      srihari 

matrices 

       2-d array of numbers 

      so each element identified by two indices 

       denoted by bold typeface a 

      elements indicated by name in italic but not bold  
      a1,1 is the top left entry and am,nis the bottom right entry 
       we can identify nos in vertical column j by writing : for the 
horizontal coordinate 
       e.g., 
a =

			

a1,1 a1,2
a2,1 a2,2

   
   
   
   

   
   
   
   

      ai: is ith row of a, a:j is jth column of a 

       if a has shape of height m and width n with 

real-values then  

	 	a   !m  n

7 

machine learning                                                             

                       

   

      srihari 

tensor 

       sometimes need an array with more than two 

axes 
      e.g., an rgb color image has three axes 

       a tensor is an array of numbers arranged on a 

regular grid with variable number of axes  
      see figure next 

       denote a tensor with this bold typeface: a 
       element (i,j,k) of tensor denoted by ai,j,k 

8 

machine learning                                                             

                       

   

      srihari 

shapes of tensors 

9 

machine learning                                                             

                       

   

transpose of a matrix 

      srihari 

       an important operation on matrices 
       the transpose of a matrix a is denoted as at 
       defined as 

 

 

 (at)i,j=aj,i 

      the mirror image across a diagonal line 

       called the main diagonal , running down to the right 
starting from upper left corner 

a1,1 a1,2 a1,3
a2,1 a2,2 a2,3
a3,1 a3,2 a3,3

   
   
   
   
   
   

   
   
   
   
   
   

a =

		

    at =

a1,1 a2,1 a3,1
a1,2 a2,2 a3,2
a1,3 a2,3 a3,3

   
   
   
   
   
   

   
   
   
   
   
   

a1,1 a1,2
a2,1 a2,2
a3,1 a3,2

   
   
   
   
   
   

a =

		

    at =

   
   
   
   
   
   

   
   
   
   
   
   

a1,1 a2,1 a3,1
a1,2 a2,2 a3,2

   
   
   
   
   
   

10 

 

machine learning                                                             

                       

   

      srihari 

vectors as special case of matrix 
       vectors are matrices with a single column 
       often written in-line using transpose 

 

 

 x = [x1,..,xn]t 
x1
x2

		    	x t = x1,x2,..xn

      

      

   
   
   
   
   
   
   
   
   
   

x =

		   

xn

   
   
   
   
   
   
   
   
   
   

       a scalar is a matrix with one element   

 

 a=at 

11 

machine learning                                                             

                       

   

      srihari 

matrix addition 

       we can add matrices to each other if they have 

the same shape, by adding corresponding 
elements 
      if a and b have same shape (height m, width n) 

c = a+b    ci,j = ai,j+bi,j
	 
d= ab+c    di,j = abi,j+c
	 
c = a+b    ci,j = ai,j +bj
	  

       less conventional notation used in ml: 

      vector added to matrix 

       a scalar can be added to a matrix or multiplied 

by a scalar 

       called broadcasting since vector b added to each row of a 

12 

machine learning                                                             

                       

   

      srihari 

multiplying matrices 

       for product c=ab to be defined, a has to have 
the same no. of columns as the no. of rows of b 
      if a is of shape mxn and b is of shape nxp then 
matrix product c is of shape mxp 

c = ab    ci,j = ai,k
	 

    bk,j

k

      note that the standard product of two matrices is 
not just the product of two individual elements 
       such a product does exist and is called the element-wise 
product or the hadamard product a  b 

13 

machine learning                                                             

                       

   

      srihari 

multiplying vectors 

       dot product between two vectors x and y of 

same dimensionality is the matrix product xty  

       we can think of matrix product c=ab as 

computing cij the dot product of row i of a and 
column j of b 

14 

machine learning                                                             

                       

   

      srihari 

matrix product properties 

       distributivity over addition: a(b+c)=ab+ac 
       associativity: a(bc)=(ab)c 
       not commutative: ab=ba is not always true 
       dot product between vectors is commutative: 

xty=ytx 

       transpose of a matrix product has a simple 

form: (ab)t=btat 

15 

machine learning                                                             

                       

   

example flow of tensors in ml 

      srihari 

a linear classifier  y= wxt+b 

vector x is converted 
into vector y by 
multiplying x by a matrix w 

a linear classifier with bias eliminated  y= wxt 

machine learning                                                             

                       

linear transformation 

   

      srihari 

       ax=b 

	 	a   !n  n

      where            and 
      more explicitly 
		 

	 	b    !n

a11x1+ a12x2 +....+ a1nxn = b1

a21x1+ a22x2 +....+ a2nxn = b2
an1x1+ am2x2 +....+ an,nxn = bn

								 x=

   
   
   
   
   
   

a =

		   

"

"

a1,1 ! a1,n
an,1 ! ann
n x n 

"

   
   
   
   
   
   

   
   
   
   
   
   

x1
"
xn

   
   
         b=
   
   
   
   
n x 1 

b1
"
bn

   
   
   
   
   
   
   
   
   
   
   
   
n x 1 

 

n equations in 
n unknowns 

can view a as a linear transformation 
of vector x to vector b 

       sometimes we wish to solve for the unknowns 
x ={x1,..,xn} when a and b provide constraints 

 

17 

machine learning                                                             

                       

   

      srihari 

identity and inverse matrices  

       matrix inversion is a powerful tool to analytically 

solve ax=b 

       needs concept of identity matrix 
       identity matrix does not change value of vector 
when we multiply the vector by identity matrix 
      denote identity matrix that preserves n-dimensional 
vectors as in 
1 0 0
      formally                      and 
0 1 0
in    !n  n
	 
0 0 1
      example of i3 
   
   
   
   
   

		     x    !n,inx = x

   
   
   
   
   

18 

	

machine learning                                                             

                       

   

matrix inverse 

       inverse of square matrix a defined as 
       we can now solve ax=b as follows: 

      srihari 

		a   1a = in

ax = b

a   1ax = a   1b
inx = a   1b
		 	
x = a   1b

 
       this depends on being able to find a-1 
       if a-1 exists there are several methods for 

finding it 

 
 

19 

machine learning                                                             

                       

   

      srihari 

solving simultaneous equations 
       ax = b 

where a is (m+1) x (m+1) 
x is (m+1) x 1: set of weights to be determined 
b is n x 1 

20 

machine learning                                                             

example: system of linear 

                       

   

equations in id75 

      srihari 

       instead of ax=b 
       we have 

   w = t

      where    is m x n design matrix of m features for n 
samples xj, j=1,..n 
       w is weight vector of m values 
       t is target values of sample, t=[t1,..tn] 
      we need weight w to be used with m features to 
determine output 

y(x,w)= wixi
		  
i=1m
   

21 

machine learning                                                             

                       

   

      srihari 

closed-form solutions 

       two closed-form solutions 

1.   matrix inversion x=a-1b 
2.   gaussian elimination 

22 

machine learning                                                             

      srihari 
linear equations: closed-form solutions 

                       

   

1. matrix formulation: ax=b 
solution: x=a-1b 

2. gaussian elimination  
followed by back-substitution 

l2-3l1  l2 

l3-2l1  l3 

-l2/4  l2 

machine learning                                                             

      srihari 
disadvantage of closed-form  solutions 
       if a-1 exists, the same a-1 can be used for any 

                       

   

given b 
      but a-1 cannot be represented with sufficient 
precision 
      it is not used in practice 

       gaussian elimination also has disadvantages 

      numerical instability (division by small no.) 
       o(n3) for n x n matrix 

       software solutions use value of b in finding x 

      e.g., difference (derivative) between b and output is 
used iteratively 

24 

		 

machine learning                                                             

      srihari 
how many solutions for ax=b exist? 
       system of equations with 
a21x1+ a22x2 +....+ a2nxn = b2

a11x1+ a12x2 +....+ a1nxn = b1

                       

   

        n variables and m equations is:  

am1x1+ am2x2 +....+ amnxn = bm

       solution is x=a-1b 
       in order for a-1 to exist  ax=b must have 
exactly one solution for every value of b 
      it is also possible for the system of equations to 
have no solutions or an infinite no. of solutions for 
some values of b 
       it is not possible to have more than one but fewer than 
infinitely many solutions 

      if x and y are solutions then z=   x + (1-  ) y   is a 
solution for any real    

25 

machine learning                                                             

                       

   

span of a set of vectors 

      srihari 

       span of a set of vectors: set of points obtained 

by a linear combination of those vectors 
      a linear combination of vectors {v(1),.., v(n)} with 
coefficients ci is 
      system of equations is ax=b 

    v(i)
	   

ci

i

       a column of a, i.e., a:i specifies travel in direction i 
       how much we need to travel is given by xi  
       this is a linear combination of vectors 

    a:, i

ax = xi
  

      thus determining whether ax=b has a solution is 
equivalent to determining whether b is in the span of 
columns of a 
       this span is referred to as column space or range of a 

i

      srihari 

machine learning                                                             

                       

   

conditions for a solution to ax=b 
       matrix must be square, i.e., m=n and all 

columns must be linearly independent 
      necessary condition is  

 n     m

       for a solution to exist when                we require the 
column space be all of 

	 	a   !m  n

	 !m

      sufficient condition 

       if columns are linear combinations of other columns, 
column space is less than 

       columns are linearly dependent or matrix is singular 

       for column space to encompass         at least one set 
of m linearly independent columns 

	 !m

	 !m

       for non-square and  singular matrices 

      methods other than matrix inversion are used 

machine learning                                                             

                       

   

      srihari 

use of a vector in regression 
       a design matrix 

      n samples, d features 

       feature vector has three dimensions 

       this is a regression problem 

28 

machine learning                                                             

                       

   

      srihari 

norms 

       used for measuring the size of a vector 
       norms map vectors to non-negative values 
       norm of vector x = [x1,..,xn]t is distance from 
origin to x 
      it is any function f  that satisfies: 

)=0   x =0

f x(
f( x+y)     f x(
        r   f   x(

	  

)+ f y( )     triangle	inequality

)=    f x(

)

 

29 

machine learning                                                             

                       

lp norm 
      
      

   
      

xi

1
p

p

i

       definition: 

       l2 norm 

=

p

x
  

       called euclidean norm 

       simply the euclidean distance 
    between the origin and the point x 
       written simply as ||x|| 
       squared euclidean norm is same as xtx 

       l1 norm 

   

      srihari 

  22 + 22 = 8 = 2 2

       useful when 0 and non-zero have to be distinguished  

       note that l2 increases slowly near origin, e.g., 0.12=0.01) 

       l    norm 

x
  

= max

i

xi

   

       called max norm 

30 

machine learning                                                             

use of norm in regression 
       id75 

                       

   

      srihari 

x: a vector, w: weight vector 

y(x,w) =  w0+w1x1+..+wd xd = wtx 
 with nonlinear basis functions   j 

 

m    1   
y(x,w) = w0 +
   
       id168 

j =1

wj  j(x)

 

 
 

!e(w) = 1
2
    

n   

n=1

{y(xn,w)     tn}2 +   
2

|| w 2 ||

second term is a weighted norm 

called a regularizer (to prevent overfitting) 

31 

machine learning                                                             

                       

lp norm and distance 

       norm is the length of a vector 

   

      srihari 

       we can use it to draw a unit circle from origin 

      different p values yield different shapes  

       euclidean norm yields a circle 

 
       distance between two vectors (v,w) 

       dist(v,w)=||v-w|| 
               = 
  

(v1     w1)2 + .. + (vn     wn)2

distance to origin would just be sqrt of sum of squares 

32 

machine learning                                                             

size of a matrix: frobenius norm 

                       

   

      srihari 

12

	 

2

ai,j

   
      

      
      

i,j

a

=

f

       similar to l2 norm 
 

   a = 4 + 1 + 25 + .. + 1 = 46

   
   
   
   
   

a =

  

2    1 5
1
0
3
1

2
1

   
   
   
   
   

       frobenius in ml 

      layers of neural network 
involve id127 
      id173:  

       minimize frobenius of weight 
matrices ||w(i)|| over l layers 

v matrix 

w matrix 

i 

j 

k 

i1  (i+1)    v(i+1)  j=netj 
hj=f(netj)     f(x)=1/(1+e-x) 

33 

machine learning                                                             

                       

   

angle between vectors 

      srihari 

       dot product of two vectors can be written in 
terms of their l2 norms and angle    between 
them 

		   x ty    ||x ||2||y ||2cos   

 
       cosine between two vectors is a measure of 

their similarity 

34 

      srihari 

machine learning                                                             

special kind of matrix: diagonal 
       diagonal matrix has mostly zeros, with non-

                       

   

zero entries only in diagonal 
      e.g., identity matrix, where all diagonal entries are 1 
 
      e.g., covariance matrix with independent features 

if cov(x,y)=0 then e(xy)=e(x)e(y) 

n(x |   ,  ) =
     

1

(2  )d/2

   
      
1
|    |1/2 exp    
   
         

1
2

   
      
(x       )t      1(x       )
   
         

machine learning                                                             

                       

   

efficiency of diagonal matrix 

      srihari 

       diag (v) denotes a square diagonal matrix with 
diagonal elements given by entries of vector  v 

       multiplying vector x by a diagonal matrix is 

efficient 
      to compute diag(v)x  we only need to scale each xi 
by vi 

    diag( v)x = v    x

       inverting a square diagonal matrix is efficient 

      inverse exists iff every diagonal entry is nonzero, in 
which case diag (v)-1=diag ([1/v1,..,1/vn]t) 

machine learning                                                             

      srihari 
special kind of matrix: symmetric 
       a symmetric matrix equals its transpose: a=at 

                       

   

      e.g., a distance matrix is symmetric with aij=aji 

      e.g., covariance matrices are symmetric 

machine learning                                                             

special kinds of vectors 

                       

   

      srihari 

       unit vector 

      a vector with unit norm 

       orthogonal vectors 

x 2=1
	 

      a vector x and a vector y are 
orthogonal to each other if xty=0 
       if vectors have nonzero norm, vectors at 
90 degrees to each other 

      orthonormal vectors 

       vectors are orthogonal & have unit norm 
       orthogonal matrix 

       a square matrix whose rows are mutually 

orthonormal:  ata=aat=i 

         a-1=at 

orthogonal matrices are of 
interest because their inverse is 
very cheap to compute 

   

      srihari 

machine learning                                                             

matrix decomposition 

                       

       matrices can be decomposed into factors to 
learn universal properties, just like integers: 
      properties not discernible from their representation 
1.   decomposition of integer into prime factors  

       from 12=2  2  3 we can discern that 

        12 is not divisible by 5 or  
       any multiple of 12 is divisible by 3 
       but representations of 12 in binary or decimal are different 

2.   decomposition of matrix a as a=vdiag(  )v-1 

       where v is formed of eigenvectors and    are eigenvalues, 
e.g, 

a = 2 1
1 2
   

   
   
   
   

   
   
   
   

has eigenvalues   =1 and   =3 and eigenvectors v: 

v  =1 = 1
   1
    

   
   
   
   

   
   
   ,v  =3 = 1
   
   
   
1
   
   

   
   
   
   

machine learning                                                             

                       

eigenvector 

       an eigenvector of a square matrix 
a is a non-zero vector v such that 
multiplication by a only changes 
the scale of v 

 

 av=  v 

      the scalar    is known as eigenvalue 

       if v is an eigenvector of a, so is 
any rescaled vector sv. moreover 
sv still has the same eigen value. 
thus look for a unit eigenvector 

   

      srihari 

wikipedia 

40 

machine learning                                                             

   

                       

      srihari 
eigenvalue and characteristic polynomial 
       consider av=w 
       if v and w are scalar multiples, i.e., if av=  v 
       then v is an eigenvector of the linear transformation a 
and the scale factor    is the eigenvalue corresponding 
to the eigen vector  

   
   
   
         w =
   
   
   

   
   
   
         v =
   
   
   

a1,1 l a1,n
m m m
an,1 l ann

w1
m
wn

v1
m
vn

       this is the eigenvalue equation of matrix a 

   
   
   
   
   
   

   
   
   
   
   
   

   
   
   
   
   
   

   
   
   
   
   
   

a=

  

      stated equivalently as   (a-  i)v=0    
      this has a non-zero solution if  |a-  i|=0           as 

       the polynomial of degree n can be factored as 
 
       the   1,   2     n  are roots of the polynomial and are 
eigenvalues of a 

 |a-  i| = (  1-  )(  2-  )   (  n-  )  

machine learning                                                             

      srihari 
example of eigenvalue/eigenvector 
       consider the matrix 

                       

   

a = 2 1
1 2
   

   
   
   
   

   
   
   
   

       taking determinant of (a-  i), the char poly is 
 

| a      i |= 2      
1
    

   
   
   
   

1

2      

   
   
    = 3     4   +   2
   

       it has roots   =1 and   =3 which are the two 

       the eigenvectors are found by solving for v in 

eigenvalues of a 

av=  v, which are 

v  =1 = 1
   1
    

   
   
   
   

   
   
   ,v  =3 = 1
   
   
   
1
   
   

   
   
   
   

42 

machine learning                                                             

                       

   

      srihari 

eigendecomposition 
       suppose that matrix a has n linearly 

independent eigenvectors {v(1),..,v(n)} with 
eigenvalues {  1,..,  n} 

       concatenate eigenvectors to form matrix v 
       concatenate eigenvalues to form vector 

  =[  1,..,  n] 

       eigendecomposition of a is given by 

 

 

 a=vdiag(  )v-1 

43 

                       

machine learning                                                             

      srihari 
decomposition of symmetric matrix 
       every real symmetric matrix a can be 

decomposed into real-valued eigenvectors and 
eigenvalues 

   

 

 

 a=q  qt 

where q is an orthogonal matrix composed of 
eigenvectors of a: {v(1),..,v(n)} 

orthogonal matrix: components are orthogonal or v(i)tv(j)=0 

   is a diagonal matrix of eigenvalues {  1,..,  n} 

       we can think of a as scaling space by   i in 
direction v(i) 
      see figure on next slide 

44 

machine learning                                                             

      srihari 
effect of eigenvectors and eigenvalues 

                       

   

       example of 2  2 matrix 
       matrix a with two orthonormal eigenvectors 

       v(1) with eigenvalue   1, v(2) with eigenvalue   2 

	  u    !

2

plot of unit vectors   
(circle) 

with two variables x1 and x2 

plot of vectors au   
(ellipse) 

45 

machine learning                                                             

      srihari 
eigendecomposition is not unique 
       eigendecomposition is a=q  qt 

                       

   

      where q is an orthogonal matrix composed of 
eigenvectors of a 

       decomposition is not unique when two 

eigenvalues are the same 

       by convention order entries of    in descending 

order: 
      under this convention, eigendecomposition is 
unique if all eigenvalues are unique  

46 

   

machine learning                                                             

      srihari 
what does eigendecomposition tell us? 
       tells us useful facts about the matrix: 

                       

1.    matrix is singular if & only if any eigenvalue is zero 
2.      useful to optimize quadratic expressions of form 

 

  f(x)=xtax subject to ||x||2=1 
whenever x is equal to an eigenvector, f is equal to the 
corresponding eigenvalue 
maximum value  of f  is max eigen value, minimum value is 
min eigen value 
example of such a quadratic form appears in multivariate 
gaussian 

n(x |   ,  ) =
     

1

(2  )d/2

   
      
1
|    |1/2 exp    
   
         

1
2

   
      
(x       )t      1(x       )
   
         

47 

machine learning                                                             

                       

   

      srihari 

positive definite matrix 

       a matrix whose eigenvalues are all positive is 

called positive definite 
      positive or zero is called positive semidefinite 

       if eigen values are all negative it is negative 

definite 
      positive definite matrices guarantee that xtax     0 

48 

machine learning                                                             

                       

   

      srihari 

singular value decomposition (svd) 
       eigendecomposition has form:  a=vdiag(  )v-1 

      if a is not square,  eigendecomposition is undefined 

       svd is a decomposition of the form a=udvt 
       svd is more general than eigendecomposition 
      used with any matrix rather than symmetric ones 
      every real matrix has a svd  

       same is not true of eigen decomposition  

machine learning                                                             

svd definition 

                       

       write a as a product of 3 matrices:  a=udvt 

      if a is m  n, then u is m  m, d is m  n,  v is n  n 
       each of these matrices have a special structure 

   

      srihari 

      u and v are orthogonal matrices 
      d is a diagonal matrix not necessarily square 

       elements of diagonal of d are called singular values of a 
       columns of u are called left singular vectors 
       columns of v are called right singular vectors 

       svd interpreted in terms of eigendecomposition 

       left singular vectors of a are eigenvectors of aat  
       right singular vectors of a are eigenvectors of ata 
       nonzero singular values of a are square roots of eigen 
values of ata. same is true of aat 

 

 

  

machine learning                                                             

                       

   

      srihari 

use of svd in ml 

1.    svd is used in generalizing matrix inversion 
       moore-penrose inverse (discussed next) 
2.    used in id126s 
       id185 (cf)  

       method to predict a rating for a user-item pair based on the 

history of ratings given by the user and given to the item 
       most cf algorithms are based on user-item rating matrix 
where each row represents a user, each column an item 
       entries of this matrix are ratings given by users to items 

       svd reduces no.of features of a data set by reducing space 

dimensions from n to k where k < n 

51 

machine learning                                                             

svd in id185 

                       

   

      srihari 

       x is the utility matrix 

       xij denotes how user i likes item j 
      cf fills blank (cell) in utility matrix that has no entry 
       scalability and sparsity is handled using svd 

      svd decreases dimension of utility matrix by 
extracting its latent factors 
       map each user and item into latent space of dimension r 

52 

machine learning                                                             

                       

   

      srihari 

moore-penrose pseudoinverse 
       most useful feature of svd is that it can be 
used to generalize matrix inversion to non-
square matrices 

       practical algorithms for computing the 
pseudoinverse of a are based on svd 

 

 

 a+=vd+ut 

      where u,d,v are the svd of a 

       pseudoinverse d+ of d is obtained by taking the 
reciprocal of its nonzero elements when taking transpose 
of resulting matrix 

 

53 

machine learning                                                             

                       

   

      srihari 

trace of a matrix 

       trace operator gives the sum of the elements 

along the diagonal 

       frobenius norm of a matrix can be represented 

as 

tr(a )=
		  
ai,i
i,i
   
= tr(a)
		 

a

(

)

f

12

54 

machine learning                                                             

                       

   

      srihari 

determinant of a matrix 

       determinant of a square matrix det(a) is a 

mapping to a scalar 

       it is equal to the product of all eigenvalues of 

the matrix 

       measures how much multiplication by the 

matrix expands or contracts space 

55 

machine learning                                                             

                       

   

      srihari 

example: pca 

       a simple ml algorithm is principal components 

       it can be derived using only knowledge of basic 

analysis 

id202 

56 

machine learning                                                             

                       

   

      srihari 

pca problem statement 

       given a collection of m points {x(1),..,x(m)} in 

rn represent them in a lower dimension. 
      for each point x(i) find a code vector c(i) in rl 
      if l is smaller than n it will take less memory to 
store the points 
      this is lossy compression 
      find encoding function f (x) = c and a decoding 
function x      g ( f (x) ) 

57 

machine learning                                                             

                       

   

      srihari 

pca using id127 

	 d    !n  l

       one choice of decoding function is to use 

id127: g(c) =dc  where 
       d is a matrix with l columns 

       to keep encoding easy, we require columns of 

d to be orthogonal to each other  
      to constrain solutions we require columns  of d to 
have unit norm 

       we need to find optimal code c* given d 
       then we need optimal d 

58 

machine learning                                                             

                       

   

      srihari 

finding optimal code given d 

       to generate optimal code point c* given input 
x, minimize the distance between input point x 
and its reconstruction g(c*) 

 

      using squared l2 instead of l2, function being 
minimized is equivalent to 

c

c* =argmin
x     g(c)2
	  
		  (x     g(c))t(x     g(c))
c* =argmin
	  

c

       using g(c)=dc optimal code can be shown to 

be equivalent to 

    2xt dc+ctc

59 

machine learning                                                             

                       

   

      srihari 

optimal encoding for pca 

   c(   2xt dc+ctc)= 0
		  

       using vector calculus 
 
       thus we can encode x using a matrix-vector 

   2dt x+2c = 0
c = dt x

operation 
      to encode we use f(x)=dtx 
      for pca reconstruction, since g(c)=dc we use 
r(x)=g(f(x))=ddtx 
      next we need to choose the encoding matrix d 

60 

      srihari 

machine learning                                                             

                       

   

method for finding optimal d 

       revisit idea of minimizing l2 distance between 

inputs and reconstructions 
      but cannot consider points in isolation 
       so minimize error over all points: frobenius norm 
12
2
)

d*=argmin
		  
       use design matrix x,  

       subject to dtd=il 

(i)     r x(i)
(

i,j
      
      

	  x    !m  n

      given by stacking all vectors describing the points 

 

) j

x j

   
      

(

d

       to derive algorithm for finding d* start by 

considering the case l =1 
      in this case d is just a single vector d 

61 

machine learning                                                             

                       

   

      srihari 

final solution to pca 

       for l =1, the optimization problem is solved 

using eigendecomposition 
      specifically the optimal d is given by the 
eigenvector of  xtx corresponding to the largest 
eigenvalue 

       more generally, matrix d is given by the l 

eigenvectors of x corresponding to the largest 
eigenvalues (proof by induction) 

62 

