hal daum  e iii

28 august 2009

math for machine learning

the goal of this document is to provide a    refresher    on continuous mathematics for computer science
students. it is by no means a rigorous course on these topics. the presentation, motivation, etc., are all
from a machine learning perspective. the hope, however, is that it   s useful in other contexts. the two major
topics covered are id202 and calculus (id203 is currently left o   )).

1 calculus

calculus is classically the study of the relationship between variables and their rates of change. however, this
is not what we use calculus for. we use di   erential calculus as a method for    nding extrema of functions;
we use integral calculus as a method for probabilistic modeling.

1.1 di   erential calculus

example 1. to be more concrete, a classical statistics problem is id75. suppose that i have
a bunch of points (x1, y1), (x2, y2), . . . , (xn , yn ), and i want to    t a line of the form y = mx + b. if i have
a lot of points, it   s pretty unlikely that there is going to be a line that actually passes exactly through all of
them. so we can ask instead for a line y = mx + b that lies as close to the points as possible. see figure ??.

id75.

one easy option is to use squared error as a measure of closeness. for a point (xn, yn) and a line de   ned
by m and b, we can measure the squared error as [(mxn + b)     yn]2. that is: our predicted value minus the
true value, all squared.1 we can easily sum all of the point-wise errors to get a total error (which, for some
strange reason, we   ll call    j   ) of:

n(cid:88)

(cid:2)(mxn + b)     yn

(cid:3)2

j(m, b) =

(1)

n=1

note that we have written the error j as a function of m and b, since, for any setting of m and b, we will
get a di   erent error.

now, our goal is to    nd values of m and b that minimize the error. how can we do this? di   erential
calculus tells us that the minimum of the j function can be computed by    nding the zeros of its derivatives.
(assuming it is convex: see section 1.3.)

the derivative of a function at a point is the slope of the function at that point (a derivative is like a
velocity). see figure ??. to be precise, suppose we have a function f that maps real numbers to real
numbers. (that is: f : r     r; see section ??). for instance, f(x) = 3x2     ex. the derivative of f with
respect to x, denoted    f /   x, is2:

derivative
velocity

   f
   x

(x0) = lim
h   0

f(x0 + h)     f(x0)

h

(2)

this essentially says that the derivative of f with respect to x, evaluated at a point x0, is the rate of change
of f at x0. it is fairly common to see    f /   x denoted by f(cid:48). the disadvantage to this notation is that when
f is a function of multiple variables (such as j in id75; see example 1), then f(cid:48) is ambiguous as

1as discussed in the notation (see section ??), we count data points by 1 . . . n and index them by n. in general, things that

range from 1 . . . x will be indexed by x.

2note that there are other de   nitions that catch interesting corner cases.

1

math for machine learning

2

to which variable the derivative is being taken with respect to. nevertheless, when clear from context, we
will also use f(cid:48).
also regarding notation, if we want to talk about the derivative of a function without naming the function,
we will write something like:

(3)
or, if we   re really trying to save space, will write    x for the derivative with respect to x, yielding:    x[3x2   ex].
in case you are a bit rusty taking derivatives by hand, the important rules are given below:

(cid:2)3x2     ex(cid:3)

   
   x

    scalar multiplication:    x[af(x)] = a[   xf(x)]
    polynomials:    x[xk] = kxk   1
    function addition:    x[f(x) + g(x)] = [   xf(x)] + [   xg(x)]
    function multiplication:    x[f(x)g(x)] = f(x)[   xg(x)] + [   xf(x)]g(x)
    function division:    x
    function composition:    x[f(g(x))] = [   xg(x)][   xf](g(x))
    exponentiation:    x[ex] = ex
   x[ax] = log(a)ex
    logarithms:    x[log x] = 1

= [   xf (x)]g(x)   f (x)[   xg(x)]

(cid:104) f (x)

and

[g(x)]2

(cid:105)

g(x)

x

note that throughout this document, log means natural log     that is, logarithm base e. you may have seen
this previously as ln, but we do not use this notation. if we intend a log base other than e, we will write,
eg., log10 x, which can be converted into natural log as log x/ log 10.
exercise 1. compute derivatives of the following functions:

natural log

1. f(x) = ex+1
2. f(x) = e    1
2 x2
3. f(x) = xax1   a
4. f(x) = (ex + x2 + 1/x)3
5. f(x) = log(x2 + x     1)
6. f(x) = ex+1
e   x

example 2. returning to example 1, we have a function j(m, b) and we want to compute its derivative
with respect to m and its derivative with respect to b. working through the case for m, we have:

   mj(m, b) =    m

(cid:33)

n=1

(cid:3)2

(cid:32) n(cid:88)
(cid:2)(mxn + b)     yn
n(cid:88)
(cid:2)(mxn + b)     yn
(cid:3)2
n(cid:88)
(cid:3)(cid:3)    m
(cid:2)2(cid:2)(mxn + b)     yn
n(cid:88)
(cid:3)(cid:3) xn
(cid:2)2(cid:2)(mxn + b)     yn

   m

n=1

n=1

=

=

=

n=1

(cid:2)(mxn + b)     yn

(cid:3)

(4)

(5)

(6)

(7)

math for machine learning

3

in the    rst step, we apply the function addition rule; in the second step, we apply the composition rule; in
the third step, we apply the polynomial rule.
exercise 2. compute    bj(m, b).

one nice thing about derivatives is that they allow us to    nd extreme points of functions in a straightfor-
ward way. (usually you can think of an exteme point as a maximum or minimum of a function.) consider maximum
again figure ??; here, we can easily see that the point at which the function is minimized has a derivative
minimum
(slope) of zero. thus, we we can    nd zeros of the derivative of a function, we can also    nd minima (or
maxima) of that function.
example 3. the example plotted in figure ?? is of the function f(x) = 2x2     3x + 1. we can compute
the derivative of this function as    xf(x) = 4x     3. we equate this to zero (4x     3 = 0) and apply algebra to
solve for x, yielding x = 3/4. as we can see from the plot, this is indeed a minimum of this function.
exercise 3. using    mj and    bj from previous examples and exercises, compute the values of m and b that
minimize the function j, thus solving the id75 problem!

extreme points

1.2 integral calculus

an integral is the    opposite    of a derivative. its most common use, at least by us, is in computing areas
under a curve. we will never actually have to compute integrals by hand, though you should be familiar
with their properties.

will write them as(cid:82) b
of    read such an integral as(cid:80)b

x=a f(x).

the    area computing    integral typically has two bounds, a (the lower bound) and b (the upper bound). we
a dxf(x) to mean the area under the curve given by the function f between a and b.3
you should think of these integrals as being the continuous analogues of simple sums. that is, you can    kind

the interpretation of an integral as a sum comes from the following thought experiment. suppose we were
to discretize the range [a, b] into r many units of width (a     b)/r. then, we could approximate the area
under the curve by a sum over these units, evaluating f(x) at each position (to get the height of a rectangle
there) and multiplying by (a     b)/r, which is the width. summing these rectangles (see figure ??) will
approximate the area. as we let r        , we   ll get a better and better approximation. however, as r        ,
the width of each rectangle will approach 0. we name this width    dx,    and thus the integral notation mimics
almost exactly the    rectangular sum    notation (we have width of dx times height of f(x), summed over the
range).

an common integral is that over an unbounded range, for instance(cid:82)    

       dxf(x). while it may seem crazy
to try to sum up things over an in   nite range, there are actually many functions f for which the result of
this integration is    nite. for instance, a half-bounded integral of 1/x2 is    nite:

(cid:90) b

1

(cid:20)
   1
b

(cid:21)

dx

1
x2 = lim
b      

dx

1
x2 = lim
b      

    (   1
1

)

= 0 + 1 = 1

a similar calculation can show the following (called gauss    integral):

(cid:90)    
(cid:90)    

1

      

dxe   x2 =

   

  

(8)

(9)

1.3 convexity

3you may be more used to the notation r b

have to       nd it    when f (x) is some long expression.

the notion of a convex function and a convex set will turn out to be incredibly important in our studies.

convex function
convex set

a f (x)dx     the reason for putting the d on the left is so that your brain doesn   t

math for machine learning

4

a convex function is, in many ways,    well behaved.    although not a precise de   nition, you can think of
a convex function as one that has a single point at which the derivative goes to zero, and this point is a
minimum. for instance, the function f(x) = 2x2     3x + 1 from figure ?? is convex. one usually thinks of
context functions as functions that    hold water        i.e., if you were to pour water into them, it wouldn   t spill
out.
the opposite of a convex function is a concave function. a function f is concave if the function    f is
convex. so convex functions look like valleys, concave functions like hills.

concave
function

the reason we care about convexity is because it means that    nding minima is easy. for instance, the fact
that f(x) = 2x2     3x + 1 is convex means that once we   ve found a point that has a zero derivative, we
have found the unique, global minimum. for instance, consider the function f(x) = x4 + x3     4x2, which is
plotted in figure ??. this function is non-convex. it has three points at which the derivative goes to zero.
the left-most corresponds to a global minimum, the middle to a local maximum and the right-most to a local
minimum. what this means is that even if we are able to    nd a point x for which    xf(x) = 0, it is not
necessarily true that x is a minimum (or maximum) of f.

more formally, a function f is convex on the range [a, b] if its second derivative is positive everywhere in
that range. the second derivative is simply the derivative of the derivative (and is physically associated with
acceleration). the second derivative of f with respect to x is typically denoted by one of the following:

   2f
   x   x

=

   2f
   x2

=

   
   x

(cid:20)    f

(cid:21)

   x

=

   x   xf

(10)

a function f is convex everywhere if f is convex on the range (      ,   ).
example 4. consider the function f(x) = 2x2     3x + 1. we   ve already computed the    rst derivative of this
function:    xf(x) = 4x     3. to compute the second derivative of f, we just re-di   erentiate the derivative,
yielding    x   xf(x) = 4. clearly, the function that maps everything to 4 is positive everywhere, so we know
that f is convex.
example 5. now, consider the non-convex function f(x) = x4 + x3     4x2. the    rst derivative is    xf(x) =
4x3 + 3x2     4x and the second derivative is 12x2 + 6x     4. it   s fairly easy to    nd a value of x for which the
second derivative is negative: 0 is such an example. it is moderately interesting to note that while this f is
not convex everywhere, it is convex in certain ranges, for instance the open intervals (      ,   1) and (0.5,   )
are ranges over which f is convex.

exercise 4. verify whether the functions from exercise 1 are convex, concave or neither.

convex

acceleration

convex
everywhere

an analogous notion to a convex function is a convex set. consider some subset a of the real line. we   ll
denote the real line by r, so we have a     r. we say that a is convex whenever the following holds: for all
x, y     a and        [0, 1], the point   x + (1       )y is also in a. in more mathy terms, a is convex if it is closed
under convex combination.
the way to think of this is as follows. given two points x and y on the plane, the function f(  ) =   x+(1     )y
on the range        [0, 1] denotes the line segment that joins x and y. a set a is convex if all points on all
such line segments are also contained in a.4
example 6. for example, the closed interval [1, 3] is convex. to show this, let x, y     [1, 3] be given, and
let        [0, 1] be given. let z =   x + (1       )y. first, we show that z     1. without loss of generality, x     y,
so z =   x + (1       )y       x + (1       )x = x     1. next, we show that z     3. similarly, z =   x + (1       )y    
  y + (1       )y = y     3.

convex set
convex

line segment

in general, all open and closed intervals of the real line are convex.

4except under strange conditions, it is su   cient to check that for all x, y     a, the point (x + y)/2 is also in a. this is

equivalent to just checking the case for    = 0.5.

math for machine learning

5

exercise 5. show that [   3,   1]     [1, 3] (the union of the closed interval [   3,   1] and the closed interval
[1, 3]) is not convex.

why do we care about convex sets? a lot of times we   re going to be trying to minimize some function f(x),
but under a constraint that x lies in some set a. if a is convex, the life is much easier. this is because it
means that if we have two solutions, both in a, we can try to    nd a better solution between them, and this
is guaranteed to still be in a (by convexity). (we   ll come back to this later in section 3.1.

an immediate question is: convex sets and convex functions share the word    convex.    this implies that
they have something in common. they do, but we   ll need to get to multidimensional analogues before we
can see this (see section 3.1.

1.4 wrap-up

the important concepts from this section are:

    di   erentiation as a tool to    nding maxima/minima of a function.
    integration as a tool for computing area under a function.
    convex functions hold water.

if you feel comfortable with these issues and can solve most of the exercises, you   re in good shape!

2 id202

a large part of statistics and machine learning has to do with modeling data. although not always the case,
for many problems, it is useful to think of data points as being points in some high dimensional space. for
instance, we might characterize a car by it   s length, width, height and maximum velocity. a given car can
then be realized by a point in 4-dimensional space, where the value in each dimension corresponds to one of
the properties we are measuring. id202 gives us a set of tools for describing and manipulating such
objects.

2.1 vector spaces

our presentation here is going to be focused on on particular type of vector space, namely d-dimensional
euclidean space; that is, the space rd. it   s very important to realize, however, that the ideas in linear
algebra are much more general. all of our examples will be from d     {2, 3}, since these are the only ones
for which we have a chance of drawing examples.

euclidean space

we begin by de   ning a vector. this is simply an element x that lives in our vector space. in two dimen-
sions, vectors are simply points on the plane. the vector x     rd has d-many components, denoted by
(cid:104)x1, x2, . . . , xd(cid:105). we   ll typically use small d to denote one particular dimension; thus, xd is the (real value!)
corresponding to the dth dimension of x.

vector

there are several things we might want to do with vectors: add them, and multiply them by a scalar. these
operations are de   ned by the following rules:

    for x     rd and a     r, the scalar product of x and a, denoted ax given by the vector in rd de   ned

component-wise by (cid:104)ax1, ax2, . . . , axd(cid:105).

scalar product

math for machine learning

6

    for x, y     rd, the vector sum (or simply the sum) of x and y, denoted x + y, is again a vector in

rd de   ned component-wise by (cid:104)x1 + y1, x2 + y2, . . . , xd + yd(cid:105).

vector sum

figure ?? shows a simple example of vector addition and scalar product.

as you can deduce from the above, there is a vector, called the zero vector, often denoted 0, de   ned as
(cid:104)0, 0, . . . , 0(cid:105)     rd that is the additive identity: that is, x + 0 = x for all x     rd.
figure ?? shows an example of a scalar product (left) of the vector (cid:104)1, 2(cid:105) with two values of a: 0.5 and 2.
note that multiplying by 0.5 brings the point    closer    to the origin, while multiplying by 2 pushes it out
further. on the right, we see an example of vector addition between the vector (cid:104)1, 2(cid:105) and the vector (cid:104)2, 1(cid:105) to
yield the vector (cid:104)3, 3(cid:105). as can be seen from this    gure, it is often helpful to think of vectors as    rays    that
point from the origin to the value x. this makes visualizing vector addition more straightforward.
given this construction, one can also de   ne, for instance, subtraction. x     y is just x + (   1)y, where (   1)
is the real value of negative one.

the rules that govern scalar produces and vector sums agree with what one might imagine:

zero vector
additive identity

    x + y = y + x
    a0 = 0
    0x = 0
    a(x + y) = ax + ay

todo. . . should we generalize here and talk about vector spaces in general, like (cid:96)2 and (cid:32)l2 and (cid:96)1, etc.?

2.2 vector norms

in many cases, we care about measuring the length of a vector, or the distance between two vectors. the
notion of a vector norm allows us to do this.
for instance, in the (uninteresting) vector space r, the standard norm would be absolute value. using
just the notion of absolute value, we can de   ne the size of a number x as |x| and the distance between two
numbers x and y as |x     y|.
we now need to generalize this notion to arbitrary vector spaces, such as rd. a norm is any function g
that maps vectors to real numbers that satis   es the following conditions:

length
distance
vector norm
absolute value

norm

    non-negativity: for all x     rd, g(x)     0
    strictly positive: for all x, g(x) = 0 implies that x = 0
    homogeneity: for all x and a, g(ax) = |a| g(x), where |a| is the absolute value.
    triangle inequality: for all x, y, g(x + y)     g(x) + g(y)

non-negativity

strictly positive

homogeneity

triangle
inequality

these conditions state, in turn, the following. first, lengths are always positive. second, a length of zero
implies that you are zero. third, scalar multiplication extends lengths in a predictable way. fourth, distances
add    reasonably.   
exercise 6. verify that for the vector space r, the absolute value norm g(x) = |x| satis   es all four conditions.

an immediate question is: are norms unique (i.e., for every space, is there a single, unique norm for that
space)? the answer is a resounding no!. for rd, there are lots of functions g that satisfy the above
conditions. here are some examples:

math for machine learning

7

1. euclidean norm: g(x) =

(cid:113)(cid:80)d
2. manhattan norm: g(x) =(cid:80)d
4. zero norm: g(x) =(cid:80)d

d=1 x2
d
d=1 |xd|
3. maximum norm: g(x) = maxd |xd|
d=1 1[xd (cid:54)= 0]

(here, to de   ne the zero norm, we have used an indicator function, denoted by    1[   ]   . the value of 1[   ]
is one whenever           is    true    and zero otherwise.)
these three norms behave quite di   erently. euclidean norm is probably the one you   re most familiar with.
it corresponds in two dimensions to the pythagorean theorem. essentially, it measures length by walking in
a straight line from the original to the point x.

indicator
function

manhattan norm (named because of the grid system for laying out streets in manhattan   not unlike salt
lake city!) measure length by walking along each dimension separately. you are not allowed to    cut across   
diagonally.

maximum norm measures the size of a vector as just the size of the maximum element in that vector.

zero norm simply counts the number of non-zero elements in the vector.

exercise 7. compute each of the four norms on the following vectors:

1. (cid:104)1, 2, 3(cid:105)
2. (cid:104)1,   1, 0(cid:105)
3. (cid:104)0, 0, 0(cid:105)
4. (cid:104)1, 5,   6(cid:105)

example 7. let   s verify that the euclidean norm is actually a norm (i.e., it satis   es the four conditions).

    non-negativity: let x be some vector; then look at

d. we know this value will be non-negative
so long as the sum is non-negative. but the sum is the sum of a bunch of values squared, so each of
them has to be positive. thus, euclidean norm is non-negative.

d=1 x2

    strictly positive: suppose x is such that g(x) = 0. for contradiction, suppose that x (cid:54)= 0, which means
there is some dimension d for which xd (cid:54)= 0. but now it cannot be the case that g(x) = 0 because
d > 0. this is a contradiction, so euclidean norm is strictly positive.
x2

(cid:113)(cid:80)d

(cid:113)(cid:80)d

let x and a be given. then compute: g(ax) =

d=1(axd)2 =

d=1 a2x2

d =

(cid:113)
a2(cid:80)d

    homogeneity:
d=1 x2

d = |a|(cid:113)(cid:80)d

d=1 x2

d = |a| g(x).

    triangle inequality: this follows directly from the pythagorean theorem.

exercise 8. verify that the manhattan norm, the maximum norm and the zero norm are actually norms.
(these are actually easier than the euclidean case.)

one nice thing about these four norms is that they   re actually speci   c cases of a family of norms called the
ell-p norms, sometimes denoted by (cid:96)p. (note that it   s a script (cid:96), not a roman l.) this is either pronounced
   ell p    or    little ell p   , depending on the context. (there is another set of norms called the lp norms, which

ell-p

(cid:113)(cid:80)d

math for machine learning

8

(cid:32) d(cid:88)

(cid:33)1/p

||x||p =

|xd|p

are often called the    big ell p    norms.) these are de   ned as follows. let p be in the range [0,   ]; then the
(cid:96)p norm of x, denoted by ||x||p, is de   ned by:

(11)

d=1

given this de   nition, it is easy to see that euclidean norm is the (cid:96)2 norm and manhattan norm is the (cid:96)1
norm.

maximum norm is a bit harder to see: it is actually the (cid:96)    norm. the way to think about this is as follows.
take a vector of (positive) numbers and raise them all to some gigantic (almost in   nite; say 1000000) power.
even if the vector is (cid:104)2, 2, 2, 2.001, 2, 2(cid:105), after raising it to a gigantic power, the fourth element, 2.0011000000
is going to totally dominate: it will be almost-in   nitely bigger than the others. so then when we add them
up, we   ll end up with a sum that is not really any di   erent from 2.0011000000. so then when we raise the
sum to 1/p, we   ll just get 2.001 back. (yes, this is hand-wavy. but you can prove it formally by taking a
limit as p tends toward in   nity.)

zero norm is also a bit tricky. again, to prove it formally you have to argue in terms of limits as p approaches
zero (from above). the intuition, however, is that as p goes to zero, any element xd that is non-zero will map
to x0
d = 1. on the other hand, any element xd that is zero, will map just to zero. so the non-zero elements
map to one and the zero elements map to zero and we sum the resulting vector. this gives us precisely the
zero norm.

which norm should you use? it depends on your application. de   nitely the most common are euclidean
((cid:96)2) and manhattan ((cid:96)1).
once we   ve chosen a norm, we immediately get a method for computing distances. we de   ne the distance
between two vectors x and y as ||x     y||, where ||   || denotes the norm of our choosing. thus, you can think
of the length of the vector ||x|| as the distance of that point to the origin.
we will often make use of unit vectors. these are vectors x that have unit norm: ||x|| = 1 (for whatever
particular norm we are using). so long as x is not the zero vector, we can normalize x by multiplying it by
1/||x||. this yields a unit vector x/||x|| in the    same direction    (see section 2.3) as x, but with unit norm.
exercise 9. verify that for non-zero x, we have that the norm of (1/||x||)x = 1. for simplicity,    rst show
that this is true for euclidean norm. next, show that it is true for any norm that satis   es the required
properties.

2.3 dot products

one thing that has been noticible absent from our discussion thus far is any notion of multiplying two vectors
together. a standard variety of multiplication of vectors is the dot product. before we de   ne it, however,
let   s motivate it a bit.

let   s say i hand you two vectors x and y and i want to know if they are perpendicular to each other
or not. in two dimensions, they are perpendicular if the angle between them is 90 degrees. that doesn   t
answer our question, though, because we don   t know how to generalize the notion of angle! moreover, we
might want to know if x and y are roughly in the same direction. the dot product allows us to answer these
questions.
let x and y be vectors in rd. we de   ne the dot product between x and y, denoted x    y as:

xdyd

(12)

d(cid:88)

d=1

x    y =

distance

unit vectors
normalize

dot product

perpendicular

dot product

math for machine learning

9

note that the dot product returns a real value, not another vector. (sometimes you will see di   erent notation
for the dot product. the two other standard notations are (cid:104)x, y(cid:105) or x(cid:62)y. we will actually use both of these
in the future, for di   erent purposes, so be fore-warned! the latter will make more sense when we talk about
matrices   see section 2.4).
example 8. we can compute the dot product between (cid:104)5, 3,   1(cid:105) and (cid:104)2, 0, 1(cid:105) as:

(5    2) + (3    0) + (   1    1) = 10 + 0     1 = 9

exercise 10. compute the following dot products:

1. (cid:104)1, 2, 3(cid:105)    (cid:104)4, 5, 6(cid:105)
2. (cid:104)4,   1, 2(cid:105)    (cid:104)1, 1, 1(cid:105)
3. (cid:104)0, 0, 1(cid:105)    (cid:104)1,   1, 1(cid:105)

note that the dot product has a nice relationship to the euclidean norm:

||x||2

2 = x    x

(13)

(14)

the dot product (at least in euclidean space) has a nice relationship to the angle between two vectors. let
x and y be two vectors. then it is easy to show that:

angle

x    y = ||x||2 ||y||2 cos   

where    is the angle between x and y. figure ?? has an example.

(15)

thus, now that we have the notion of a dot product in hand, we can answer a whole host of questions. we
say that two (non-zero) vectors are perpendicular if x    y = 0 (this means that cos    = 90). and, we can
measure the angle between x and y by their angle:

(cid:19)

(cid:18) x    y

||x||||y||

   = arccos

the dot product satis   es the following useful properties:

    commutativity: x    y = y    x
    distributivity: x    (y + z) = x    y + x    z

(16)

commutativity

distributivity

the more mathy term for    perpendicular    is orthogonal. we will stick with this word from now on.

orthogonal

exercise 11. we claimed earlier that you can normalize a (non-zero) vector x by multiplying it by the
scalar value (1/||x||), and that these two vectors are in the    same direction.    show that this is true by
demonstrating that the angle between x and (1/||x||)x is zero.

one signi   cant use of dot products is to evaluate projections. that is, u    v can be interpreted as a
projection of the vector u onto the vector v. it gives a scalar value that represents the distance that u goes
in the direction of v. see figure ?? for a geometric intuition. this makes the most sense when v is a unit
vector. see section 2.5 for more details.

projections

math for machine learning

10

2.4 matrices

a real-valued matrix is a rectangular collection of real values. for instance, we might de   ne the following matrix
matrix:

(17)

       5

10
   2
0
1    1

      

a =

an index into a matrix is in row-by-column notation. this example matrix a has 3 rows and 2 columns.
the value in the 2nd row and 1st column, denoted a2,1 is    2. (this is a bit confusing at    rst for those used
to thinking of x-by-y notation: it seems backwards.)
a matrix with n rows and m columns is usually called an n    m (   n by m   ) matrix, and we write
a     rn  m .
note that we can think of vectors as matrices where one of the dimensions is 1. the standard is to say that
a vector of length d is a matrix with d rows and 1 column. thus, vectors are    tall skinny    matrices that
live in rd  1.
matrices can be manipulated in very similar ways to vectors, in terms of sums and products:

    for a     rn  m and a     r, the matrix aa is also in rn  m with values given by (aa)n,m = aan,m.

(here, we denote the n, mth element of aa as (aa)n,m.)

    for a, b     rn  m , the sum a + b is of the same size with values given by (a + b)n,m = an,m + bn,m

one convenient piece of notation is the matrix cut. let a be n    m. then, we write an,    to denote the matrix cut
row vector obtained by taking the nth row of a. similarly, a   ,m is the column vector obtained by taking
the mth column of a.

just as we de   ned many di   erent norms over vectors, we can also de   ne di   erent matrix norms. however, matrix norms
since we won   t make use of these properties in any great depth, we   ll just mention the most common. this
is analogous to the euclidean ((cid:96)2) norm on vectors and is called the frobenius norm:

frobenius norm

a2

n,m

(18)

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

m(cid:88)

n=1

m=1

||a||fro =

there are two important notions of multiplication of matrices, each of which has di   erent properties. the
easiest to understand is the hadamard product, also called the element-wise product, typically denoted hadamard
(cid:12):

(a (cid:12) b)n,m = an,mbn,m

(19)
the hadamard product is only de   ned over matrices of equal size (say, n    m) and returns a matrix of that
size (again, n    m). the value in each cell of the returned matrix is just the product of the values in the
corresponding cells of the original two matrices.

product
element-wise
product

example 9. an easy example of hadamard product:

       5

10
   2
0
1    1

       (cid:12)

       2    1

   1
0

5
1

       =

      

10       1
5    2
0    5
   2       1
1    0    1    1

       =

       10    10

0
2
0    1

      

(20)

exercise 12. the two two matrices from the previous example and compute the hadamard product between
each and itself. how does this relate to frobenius norm?

math for machine learning

11

k(cid:88)

hadamard multiplication is commutative: a (cid:12) b = b (cid:12) a; associative: a (cid:12) (b (cid:12) c) = (a (cid:12) b) (cid:12) c); and
distributive: a (cid:12) (b + c) = a (cid:12) b + a (cid:12) c.
the second, perhaps more common, type of id127 is the inner product. let a be n   k matrix
and let b be k    m. then the inner product of a and b, written ab is a matrix of size n    m. it is
called an inner product because the inner dimensions of the multiplication must    match up.    the product
is de   ned as:

multiplication
inner product

(ab)n,m =

an,kbk,m

(21)

k=1

this can be though of in terms of matrix cuts. the n, mth cell in the resulting matrix is given by the (vector)
dot product (cid:104)an,   , b   ,m(cid:105). note that since the internal dimension size k must match up, this vector dot
product is well de   ned.

example 10. as an example of id127, we have:

(cid:20) 5    2
(cid:123)(cid:122)
(cid:124)

10

dims 2  3

1
0    1

(cid:21)
(cid:125)

       2    1
(cid:124)
(cid:123)(cid:122)

5
1
dims 3  2

   1
0

      
(cid:125)

dims 2  2

(cid:125)(cid:124)

(cid:122)
(cid:20) 5    2 + (   2)    (   1) + 1    0
(cid:20) 12    14

10    2 + 0    (   1) +    1    0

(cid:21)

20    11

=

=

(cid:123)
(cid:21)

5    (   1) + (   2)    5 + 1    1
10    (   1) + 0    5 +    1    1

exercise 13. compute the following matrix products:

(cid:20) 1 2 3
(cid:20)    1

4 5 6

1    1

1.

2.

(cid:21)          1

   2
   3

      

1    1

1
1    1

(cid:21)                1 1

   2 2
   3 3
   4 4

            

id127 is associative: a(bc) = (ab)c); and distributive: a(b + c) = ab + ac, but is
not commutative! in general, it is not true that ab = ba (even if a and b are square).
a useful matrix operation is the transpose operator, denoted    (cid:62)   . the transpose of a matrix a is simply
the matrix you get by    rotating    a. that is, if we start out with a     rn  m , then the transpose, denoted
a(cid:62), is now in rm  n . the values are de   ned by:

transpose

example 11. for instance, we can compute the following transpose:

(a(cid:62))m,n = an,m

       5

10
   2
0
1    1

      (cid:62)

(cid:20) 5    2

=

10

1
0    1

(cid:21)

(22)

(23)

the notion of transposition is where we get the previously mention notation for vector dot products. the
vector (cid:104)1, 2, 3(cid:105) corresponds to the 3    1 matrix [1 2 3](cid:62). similarly, the vector (cid:104)4, 5, 6(cid:105) corresponds to the

dot products

math for machine learning

12

3    1 matrix [4 5 6](cid:62). taking the transpose of the    rst, we get a 3    1 matrix and a 1    3 matrix. these
can be multiplied (in the id127 sense) to yield a scalar value:

       1

2
3

      (cid:62)       4

5
6

       = [1 2 3]

       4

5
6

       = (1    4) + (2    5) + (3    6) = 5 + 10 + 18 = 33

(24)

which is precisely the dot product between these two vectors. this is where the notation x(cid:62)y for the dot
product between two vectors comes from.
exercise 14. let a be the left-most matrix from eq (20). what is the value a(cid:62)a? what is the value
aa(cid:62)? how does these relate to the frobenius norm?

we say that a matrix is square if it has the same number of rows as columns. a square matrix a is
symmetric if a = a(cid:62).
exercise 15. which of the following matrices are symmetric:

4 1

(cid:21)
(cid:20) 1 4
       1 4 5
       1 4 5

4 2 7
5 2 6

4 2 7
5 7 6

      
      

1.

2.

3.

square
symmetric

2.5 projections

a very important geometric concept is the notion of an orthogonal projection. we   ve already (almost)
seen an example of this for vectors. in figure ??, we have two vectors x and y. one question we might
want to ask is: how far does x reach    in the direction of    y? the more mathy way to state this is: what is
the length of the projection of x onto y?
one way of thinking about this is the case when y is simply one of the axes. i.e., y = (cid:104)1, 0(cid:105) (the    x-axis   ) or
y = (cid:104)0, 1(cid:105) (the    y-axis   ). then, the question we   re asking is: how far does x point in the x direction or the
y direction? in the axis-aligned case, this is easy! it points x1 units along the    rst direction and x2 units
along the second direction! see figure ??.

things become only slightly more complicated when y is an arbitrary vector. the notion of vector projection
is shown pictorially in figure ??. the projection of x onto y is the component of x that points in the direction
of y. it turns out that the length of this projection (sometimes called the scalar component) is precisely
given by the dot project of the two vectors: x(cid:62)y!
that is, the dot product x(cid:62)y gives us the length of the component of x along y. it doesn   t give us the
actual vector. however, this is also easy to come by. we just need a vector that points in the same direction
as y but has length x(cid:62)y. as we   ve seen previously, we can obtain this by just multiplying y by the scalar
value (x(cid:62)y)/||y||.
example 12. take the simple case where y = (cid:104)1, 0(cid:105). let   s say x = (cid:104)0.5, 6(cid:105). it   s fairly intuitive in this case
that x points 0.5 units in the direction of y. we can verify this by computing x(cid:62)y = 0.5    1 + 6    0 = 0.5,
precisely as we wanted. moreover, we can    nd the actual vector corresponding to the projection of x onto y
as (0.5)y = (cid:104)0.5, 0(cid:105), again, as expected.

orthogonal
projection

projection

scalar
component

math for machine learning

13

exercise 16. take x from the previous example and project it onto the other axis. what is the length of
this projection? what is the vector corresponding to the projection?

exercise 17. compute the projection of x onto y (as a vector) for each of the following pairs:

1. x = (cid:104)6, 0.5(cid:105) and y = (cid:104)1, 2(cid:105)
2. x = (cid:104)1, 2(cid:105) and y = (cid:104)1, 2(cid:105)
3. x = (cid:104)2, 3(cid:105) and y = (cid:104)1, 0(cid:105)
4. x = (cid:104)   2, 3(cid:105) and y = (cid:104)1, 0(cid:105)

one way to think about projections of x onto y is in terms of projections of vectors onto the one-dimensional
subspace de   ned by {y}. if we think of the y a one dimensional subspace, then the projection of x onto y
is precisely the projection of x onto the subspace de   ned by y.
now, let   s consider some special cases. let u = (cid:104)2, 0.5 and let v = (cid:104)1, 0(cid:105) and w = (cid:104)0, 1(cid:105). note that both v
and w are unit vectors. what is the projection of u onto v? 2. and onto w? 0.5. we note an interesting
fact: we can write u as 2v + 0.5w. in fact, this is a general fact, so long as v and w are unit and orthogonal
(which you can easily verify). see figure ??.

let   s take this a step further. let v = (cid:104)   

   
2/2,
these are unit vectors and that they are orthogonal. now, let   s compute:

2/2(cid:105) and w = (cid:104)      

2/2,

   

subspace

2/2(cid:105). again, you can verify that

(25)
(26)

u    v =
u    w =

   
   

2 +

   
2        

   
   

2
2

2/4 = 5
4
2/4 = 3
4

this implies that we can write u = 5
4

   

2u + 3
4

   

2w, which we can verify geometrically in figure ??.

this way of thinking is useful to generalize the notion of projecting a vector onto a vector to that of projecting
a vector onto a matrix. let x be a vector in rd and let a be an n    d matrix. as before, we think of the
span of a as the span of the n-many d-dimensional vectors obtained through cuts of a. the projection of
x onto a is just the projection of x onto the subspace spanned by a: see figure ??.
computing the projection of x     rd onto a     rn  d for arbitrary a is actually di   cult. however, if we
can assume that a is orthonormal, then it is easy. for a to be orthonormal means that two things hold:

1. id172 constraint: for all n, ||an,   || = 1. that is, each d-vector in a is normalized.
2. orthogonality constraint: for all 1     n (cid:54)= n(cid:48)     n, an,    and an(cid:48),    are orthogonal: their dot product is

zero.

orthonormal

the nice thing about orthonormal sets is that they make various computations very easy. if a is orthonormal,
then we can project x independently onto each cut an,    of a. this is just a vector projection, so has length
an,   x. then, for each component n in a, we have the projection of x along that vector as (an,   x)an,   
(just as before, in the vector projection case).
putting this all together, the length of the projection of x onto a is ||ax|| (this is just the length of each of
n(an,   x)an,   . this projection indeed lies in the span

the individual vectors) and the projection itself is: (cid:80)

of a, since it is clearly a linear combination of the rows of a.

if a is not orthonormal, then we must    rst orthonormal-ize it and then do this projection. we will talk
about how to do this in section 2.7.

math for machine learning

14

2.6 important matrices

there are a few matrices that will come up over an over again. the    rst is the identity matrix. the
identity matrix is always square (that is, it has dimension d    d). the d-dimensional identity matrix is
the matrix that has zeros in every cell except the diagonal. it is denoted id, or, when the dimensionality is
clear from context, just i. a few examples:

identity matrix
square

(cid:21)

(cid:20) 1 0

0 1

       1 0

0 1
0 0

       ;

0
0
1

             1

0
0
0

            

0
1
0
0

0
0
1
0

0
0
0
1

i2 =

;

i3 =

i4 =

(27)

the identity matrix has several nice properties. assuming dimensions match up, we have:

    i(cid:62) = i
    ia = ai = a

exercise 18. prove the second claim.

another important matrix is the zero matrix, denoted 0 (not to be confused with the zero vector, 0). this
is the (not necessarily square) matrix of all zeros. again, assuming dimensions match up, the zero matrix
has the nice properties that:

zero matrix

    0(cid:62) = 0
    0a = a0 = 0

finally, there   s the ones matrix, denoted 1 (not to be confused with the indicator function 1[   ]). this is
the (not necessarily square) matrix of all ones. it only obeys the symmetry property (when it is square).

ones matrix

2.7 matrix properties: trace, determinant and rank

at this point, you   re going to have to suspend disbelief and just acknowledge that what we   re about to talk
about is at all important. as we progress, it will become obvious that these things are very important. but
for now, just have faith.
before progressing, we need to de   ne the diagonal of a matrix a. let a be an arbitrary square d    d diagonal
matrix. the diagonal of a, denoted diag(a) is the d-dimensional vector with compondent d equal to ad,d.
example 13. we can compute:

             1 4 5

4 2 7
5 2 6

             = (cid:104)1, 2, 6(cid:105)

diag

(28)

we say that a matrix a is diagonal if all of its o   -diagonal elements are zero. for instance, the identity
matrix and the zeros matrix are both diagonal, but the ones matrix is not. we say that a matrix is upper-
triangular if all of the elements below the diagonal are zero. similarly, it is lower-triangular if all of the
elements above the diagonal are zero. it is triangular if either condition holds.

the trace of a matrix a, denoted tr a, is the sum of the elements along the diagonal of a. that is:

tr a =(cid:80)d

d=1 ad,d.

diagonal

upper-triangular
lower-triangular
triangular
trace

exercise 19. compute the trace of the matrix from the previous example.

math for machine learning

15

the trace of a matrix, although simplistic, is often used as a measure of the    size    of a matrix. note that it
is not a norm, in that it doesn   t satisfy the necessary requirements, but it is sort of reasonable.
exercise 20. what is the value of tr[aa(cid:62)]? what about tr[a(cid:62)a]? what do these remind you of?

while the trace is fairly easy to comprehend, the determinant is quite a bit more complicated. we   ll start
with a special case of 2    2 matrices. the determinant of a matrix a, denoted det a, is de   ned as follows
for 2    2 matrices:

determinant

= ad     bc

(29)

(cid:20) a

det

b
c d

(cid:21)

note that if a is diagonal, the the determinant is the product of the elements along its diagonal. however,
if it   s not diagonal, the determinant is quite a bit di   erent.

exercise 21. compute the determinant of the following matrices:

1. i2

(cid:20) 5 1
(cid:20) 2 10

2 4

(cid:21)
(cid:21)

3

6

2.

3.

d(cid:88)

extending the notion of a determinant to larger matrices is a bit involved. we   ll give a recursive de   nition
and then give an example.
let a be a d    d matrix. denote by a   i,   j the matrix obtained by removing the ith row and jth column
from a. (thus, a   i,   j is a (d     1)    (d     1) matrix.) for arbitrary a, the determinant of a is:

det a =

(   1)1+da1,d det a   1,   d

(30)

d=1

we can end the recursion once we   ve gotten down to a 2    2 matrix, for which we know how to perform the
computation.

what this computation is doing is the following. we loop (d) over every column of a. on column d, we
remove the    rst row and dth column from a and recursively compute the determinant on that smaller
matrix. we multiply this value by a1,d. then, if d is odd, we add the resulting value to the running sum; if
d id even, we subtract the resulting value. (this last part is encoded in the (   1)1+d term.)
it is not terribly important for our purposes that you know how to compute a determinant (there are much
more e   cient methods than actually evaluating the recursion).

here are some important observations about determinants:

    if a is triangular, then the determinant is the product of the diagonal of a.
    if we take some row ad,    of a and add it to another row (perhaps scaled in some way), then the

determinant is unchanged!

    if two rows of a are swapped to produce b, then det a =     det b.
    if one row of a is multiplied by a scalar a to produce b, then det b = k det a.

math for machine learning

16

this last property tells us that det(aa) = ada, since the standard scalar product over matrices multiples
every row by the scalar a, and there are d-many rows.

these properties may seem mysterious at    rst (the certainly did to me!), but their importance will become
clear later on when we start using determinants to do computation.

the di   culty arises with a is not orthonormal. (actually, the id172 aspect is irrelevant. we only
are worried with a is not orthogonal.) in this case, we have to ask ourselves: what is the size of the smallest
matrix b that has the same span as a. such a matrix b will not be unique, but its dimensionality will be.
example 14. consider the matrix de   ned by the set of vectors in r3: {(cid:104)1, 0, 1(cid:105),(cid:104)0, 1, 0(cid:105),(cid:104)1, 2, 1}. this does
not de   ne an orthogonal set. in particular, the third element can be obtained as a linear combination of the
   rst two, and is therefore not orthogonal. in this case, we can see that if we simply drop the third element,
we are left with a set of 2 points that are orthogonal and do span the same space as the original set. this
means the the rank of the original matrix was actually 2.

the important property of the rank of a matrix is that it tells us the true size of the subspace spanned by
this matrix. we say that a matrix a     rn  d (with n     d) is full rank if the rank of a is equal to n. in
general, we like to work with full rank matrices: a matrix that is not full rank is somehow wasteful.

full rank

2.8 matrix inversion

we   ve seen previously (section 2.6) that the identity matrix i behaves in a reasonable way: ia = a for all
a. this is just like how, in real numbers, 1x = x for all x. another property that 1 (the real number) has
is that for any x (cid:54)= 0, there exists a y such that xy = 1. we call y the    inverse    of x and write it either as
1/x or as x   1.
the question that arises is: given a matrix a, does there exist a matrix b such that ab = i? if a is d   d,
it turns out that a is invertible if and only if a is full rank (see section 2.7)!
so, provided that a is full rank and square, we denote by a   1 the inverse of a. the inverse satis   es the
following properties:

invertible
full rank
inverse

1. commutativity: aa   1 = a   1a = i
2. scalar multiplication: (aa)   1 = a   1a   1, provided a (cid:54)= 0
3. transposition: (a(cid:62))   1 = (a   1)(cid:62)
4. product: (ab)   1 = b   1a   1

since id127 is not commutative, it is important to be careful when trying to use matrix
inverses to solve equations.

example 15. given the equation x = ab for given matrices a and x, we wish to solve for b. (suppose
everything is square and full rank.) in order to do this, we multiply both sides of the equation by a   1 on
the left. this yields: a   1x = a   1ab = b, which gives us a solution.
exercise 22. given x = a(b + c), where everything except b is known, solve for b. (assume everything
is square and full rank.)

one interesting fact is that the determinant of a square matrix a is non-zero if and only if it has full rank.
this means that an alternative way of checking to see if a matrix a is invertible is to check that det a (cid:54)= 0.
sometimes it is necessary to have an inverse-like quantity for non-square matrices. suppose that a is m   n.
we say that a    is a pseudo-inverse of a if the following four criteria hold:

pseudo-inverse

math for machine learning

17

1. aa   a = a
2. a   aa    = a
3. (aa   )(cid:62) = aa   
4. (a   a)(cid:62) = a   a

it turns out that the pseudo-inverse exists and is unique for any matrix a. in the case that a is invertible,
a   1 = a   , so the pseudo-inverse extends the notion of inverse.
sometimes the pseudo-inverse is called the moore-penrose inverse.

moore-penrose
inverse

2.9 eigenvectors and eigenvalues

todo. . . write this!

2.10 wrap-up

the most important concepts from this section are:

    de   nition of basic operations on vectors and matrices
    interpreting dot products as projections
    vector and matrix norms
    the properties of the matrix inverse (and that you can   t always invert a matrix!)

the matrix cookbook (http://matrixcookbook) provides lots more detail on all of these things and more!

3 multidimensional calculus

we   ve already hinted several times that di   erentiation in high dimensional spaces is going to be important.
for example, suppose we have vectors w and x and a function f(w, x) = w(cid:62)x. we might want to di   er-
entiate f with respect to w so that we can (for instance) maximize it. this is where di   erentiation with
respect to vectors becomes important.

we   ll start with a de   nition and then just convince you that di   erentiating in high dimensions is exactly the
same as di   erentiating in one dimension.
suppose f(w) is some function of a d-dimensional vector w = (cid:104)w1, w2, . . . , wd(cid:105). we can compute partial
derivatives of f with respect to each component of w:

   f
   w1

,

   f
   w2

,

   f
   w3

,

. . .

,

   f
   wd

(31)

each of these are just univariate derivatives. if we package them together into a vector, we get the gradient
of f with respect to the vector w. gradient is just the fancy term for multidimensional derivative. it is
denoted by    wf, and is formally de   ned as:

gradient

math for machine learning

                        

                        

   f
   w1
   f
   w2
   f

   w3...

   f
   wd

   wf =

18

(32)

example 16. let   s take a simple example. suppose f(w) = w(cid:62)(cid:104)1, 2, 3, 0,   1(cid:105) = 1w1+2w2+3w3+0w4   1w5.
we can compute:

   f
   w1
   f
   w2
   f
   w3
   f
   w4
   f
   w5

= 1

= 2

= 3

= 0

=    1

(33)

(34)

(35)

(36)

(37)

(38)

so the gradient is simply    wf = (cid:104)1, 2, 3, 0,   1(cid:105). note that this is complete analogous to a standard derivative.
f is just the    product    of w and (cid:104)1, 2, 3, 0,   1 and it   s    derivative    is just the vector (cid:104)1, 2, 3, 0,   1(cid:105).
example 17. let   s take another example that   s slightly more complicated: f(w) = w(cid:62)w (note that this
i . let   s compute

is just squared euclidean norm). rewriting in non-vector notation, we have f(w) =(cid:80)

i w2

partial derivatives:

   f
   w1
   f
   w2
...

   f
   wd

= 2w1

= 2w2

= 2wd

(39)

(40)

(41)

(42)

(43)

note, that, in general,    f /   wi = 2wi, which means that    wf = 2w! again, w(cid:62)w is basically like w2 and
the derivative is just like 2w!

f(w) =(cid:80)

example 18. let   s do one more really simple example. note that we can express    the sum of the elements
of w    as f(w) = 1(cid:62)w, where 1 is a vector of ones. can you guess what    wf is? let   s work it out.
i 1wi, so    f /   wj = 1 for all j. thus,    wf = 1. note that this is the ones vector, and is not

the constant one.

math for machine learning

19

as you might expect, function composition works in exactly the same way as in univariate calculus.
exercise 23. suppose f(w) = (w(cid:62)x)2 for some constant vector x. compute    wf. hint: begin by writing
f in non-vector form. then take the derivative with respect to some component wj. package these up into
a vector to get the gradient.
exercise 24. similar to the previous exercise, verify that when f(w) = exp[w(cid:62)x], we get    wf = exp[w(cid:62)x]x.

we can generalize this as follows. suppose f(w) = g(h(w)) for some arbitrary functions g and h. we wish
to compute    wf. first, we compute the gradient of the    inside    and let u =    wh. now, we take the
derivative of g     which we   ll call g(cid:48) here. the gradient of f is then just ug(cid:48)(h(w)), as in univarite calculus.
as you can see, most of these conform to our intuitions based on univariate calculus. there are lots of
identities listed in the matrix cookbook (http://matrixcookbook):

just as gradients are the multidimensional equivalents of derivatives, the hessian matrix is the multidi- hessian matrix
mensional equivalents of second derivatives. let f(w) be a d-dimensional function. the hessian of f is the
matrix of second derivatives:

                     

   2f
   w2
1
   2f

   w1   w2

...

   2f

h(f) =

   w1   w2

   2f

   2f
   w2
2

   2f

...
(cid:16)

. . .

. . .

. . .

...
(cid:17)

   w1   wd

   w2   wd

                     

   2f

   w1   wd

   2f

   w2   wd

...

   2f
   w2
d

(44)

=    2f

i,j

   wi   wj

h(f)

that is, it is a matrix where
. by physical analogy, the derivative of a function is it   s
rate of change (   velocity   ). the gradient of a function is its velocity in each available dimension. the second
derivative of a function is the rate of change of it   s velocity, aka it   s acceleration. the hessian of a function
is the rate at which di   erent dimension accelerate together. in other words, if h2,2 is high, then it means
that there is a high (positive) rate of change in the second dimension. if h1,2 is high, then it means that
as we accelerate in the    rst dimension, we simultaneously accelerate in the second dimension. if h1,2 is
negative, it means that as we accelerate in the    rst dimension, we decelerate in the second dimension.

3.1 multidimensional convexity
we discussed the idea of convex subsets of r in section 1.3. namely, a set a     r is convex if and only if
   x, y     a,          [0, 1] it holds that   x + (1      )y     a. the de   nition for high dimensional spaces is identical.
let rd be d-dimensional euclidean space, then a     rd is convex if and only if for all vectors x, y     a and
all        [0, 1], we have that   x + (1      )y     a. figure ?? shows an example of a convex set and a non-convex
set. convex sets look like balls, non-convex sets look like potatoes (or worse!).
example 19. let ap be a d dimensional (cid:96)p ball centered at the origin. namely ap is the set of all points
with (cid:96)p norm less than or equal to one; that is: ap = {x : ||x||p     1}. figure ?? shows examples of such
balls for di   erent p. as we can see from these    gures, the (cid:96)p balls for p < 1 are non-convex but the (cid:96)p balls
for p     1 are convex!.
exercise 25. prove the above claim for p = 0.5, p = 1 and p = 2.
exercise 26. prove the above claim for all p. (this is kind of hard.)

convex functions in higher dimensional space are actually harder to de   ne that convex sets.
now, recall from one dimension that f is convex if f(cid:48)(cid:48)     0 (it   s strictly convex if f(cid:48)(cid:48) > 0). this was easy,
because f(cid:48)(cid:48) is just a scalar value. how we have a matrix and we need some equivalent statement. the

math for machine learning

20

equivalent statement is that the matrix h is positive semi-de   nite, written h (cid:23) 0 (in latex, this is
succeq). (similarly, it is strictly convex if h (cid:31) 0, or h is positive de   nite.)

positive
semi-de   nite
positive de   nite

3.1.1 positive semi-de   nite-ness

the whole notion of being positive semi-de   nite is something that is hard to grasp, but it comes up all the
time. the reason it comes up all the time is because begin positive semi-de   nite is the analogue for matrices
of just being a non-negative real number: something that comes up all the time! it   s terribly annoying to
write    positive semi-de   nite    all the time, so we will just write psd in the future.

psd

there are actually lots of ways to de   ne what it means to be psd. we   ll do it by making an analogy to just
regular positive numbers. let a be a real value. we   ll de   ne a to be       u   y    if the following holds: for all
real values x, we have that ax2     0. this is a slightly odd de   nition, but the important thing to realize is
that a is    u   y if and only if a is non-negative! that is, we   ve essentially rede   ned non-negativeness in a
very strange way. note that we can equivalently write the    u   y requirement as being: for all x, xax     0.
(cid:80)
now, let   s move to matrices. de   ne a matrix a to be    multi-dimensional    u   y    if for all vectors x, we
have that x(cid:62)ax     0.
if we carry out the computation, this is just saying that for all x, we have that
i,j ai,jxixj     0. hopefully it   s clear that being multi-dimensional    u   y is a strict generalization of being

   u   y.

well, and here   s the magic: multi-dimensional    u   y is just the de   nition of being positive semi-de   nite!
that is, a psd matrix is just the multi-dimensional analogue of a non-negative real value!

3.1.2 convex sets and convex functions

so what do convex functions have to do with convex sets? the    rst thing they have in common is that
they   re easy to deal with. however, the real reason is that you can equivalently de   ne convex functions in
terms of convex sets. to see this, let f be some function. de   ne the set a = {(x, y)     r2 : f(x)     y}. in
e   ect, this set a is the set of all points that lie above the plot of the function f. it can be shown that f is a
convex function if and only if a is a convex set.

exercise 27. prove the above claim. (this is kind of hard!)

3.2 wrap-up

the things you should know are:

    gradients are multidimensional derivatives
    how to compute gradients
    hessians are multidimensional second derivatives
    convex sets are not potatoes
    positive de   nite matrices are just like positive numbers
    convex functions are those whose hessians are psd

math for machine learning

21

4 id203 & statistics

prml actually has a fairly good discussion of basic id203 and statistics. see section 1.2 (though but
not including the part on    bayesian probabilities   ) and section todo. . . .

the important things that you should really know are:

    the di   erence between computing the id203 of some event and sampling from the associated

distribution

    joint and conditional distributions
    marginal distributions
    chain rule and bayes    rule
    standard distributions: bernoulli, binomial, multinomial, uniform and gaussian
    expectation, variance and standard deviation
    covariance

one thing that always seems to trip students up who aren   t used to these this is that it is okay for a
continuous distribution to have density greater than one at some point.

example 20. let p be a gaussian distribution with zero mean and variance 0.1. let   s compute it   s density
at 0:

p(0)

= nor(0 | 0, 0.1)

= 1   

2  0.1

exp

(cid:104)    1
2(0.1)02(cid:105)
(cid:113) 1
(cid:113) 1

2  0.1

=

=

   

0.6283
=
1.5915
= 1.2615

(45)
(46)

(47)

(48)

(49)
(50)

this is very de   nitely above one! in fact, if we replace 0.1 with 0.01, we get a value of almost 4!

the thing to remember is that continuous id203 densities are not like discrete densities. that is, for
a continuous density, p(0) is not the id203 of drawing a zero from this distribution. if you think of
a gaussian, the id203 of drawing any value at all is always zero! this is because you   re drawing real
numbers, and you   re never going to draw exactly some given value (it will always di   er in the 100th decimal
place or something). the right way to interpret it is that the id203 of drawing some value in the range

a dxp(x). it is this value that should always be less than (or equal to) one.5

[a, b] is(cid:82) b

5keep in mind that it   s easy to have a function that takes values greater than one, but still integrates to one. simply recall

the example from section 1.2 of f (x) = 1/x2, which takes value 4 at x = 0.5, but still integrates to 1.

