optimization for machine learning

(lecture 2)

suvrit sra

massachusetts institute of technology

mpi-is t   ubingen

machine learning summer school, june 2017

course materials

my website (teaching)
some references:
(cid:4) introductory lectures on id76     nesterov
(cid:4) id76     boyd & vandenberghe
(cid:4) nonid135     bertsekas
(cid:4) convex analysis     rockafellar
(cid:4) fundamentals of convex analysis     urruty, lemar  echal
(cid:4) lectures on modern id76     nemirovski
(cid:4) optimization for machine learning     sra, nowozin, wright
(cid:4) nips 2016 optimization tutorial     bach, sra
some related courses:
(cid:4) ee227a, spring 2013, (sra, uc berkeley)
(cid:4) 10-801, spring 2014 (sra, cmu)
(cid:4) ee364a,b (boyd, stanford)
(cid:4) ee236b,c (vandenberghe, ucla)
venues: nips, icml, uai, aistats, siopt, math. prog.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

2 / 57

lecture plan

    introduction
    recap of convexity, sets, functions
    recap of duality, optimality, problems
    first-order optimization algorithms and techniques
    large-scale optimization (sgd and friends)
    directions in non-id76

suvrit sra (suvrit@mit.edu)

optimization for machine learning

3 / 57

ml optimization problems

(cid:73) data: n observations (xi, yi)n
(cid:73) prediction function: h(x,   )     r parameterized by        rd
(cid:73) motivating examples:

i=1     x    y

    linear predictions: h(x,   ) =   (cid:62)  (x) using features   (x)
    neural networks: h(x,   ) =   (cid:62)m   (  (cid:62)m   1  (         (cid:62)2   (  (cid:62)1 x))

(cid:73) estimating    parameters is an optimization problem

n(cid:88)

i=1

1
n

min
     rd

(cid:96)(cid:0)yi, h(xi,   )(cid:1) +      (  ) =

n(cid:88)

i=1

1
n

fi(  )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

4 / 57

ml optimization problems

(cid:73) data: n observations (xi, yi)n
(cid:73) prediction function: h(x,   )     r parameterized by        rd
(cid:73) motivating examples:

i=1     x    y

n(cid:88)

    linear predictions: h(x,   ) =   (cid:62)  (x) using features   (x)
    neural networks: h(x,   ) =   (cid:62)m   (  (cid:62)m   1  (         (cid:62)2   (  (cid:62)1 x))

(cid:73) estimating    parameters is an optimization problem

(cid:96)(cid:0)yi, h(xi,   )(cid:1) +      (  ) =
regression: y     r; quadratic loss: (cid:96)(cid:0)y, h(x,   )(cid:1) = 1
classf.: y     {  1}; logistic loss: (cid:96)(cid:0)y, h(x,   )(cid:1) = log(1 + exp(   yh(x,   )))

2 (y     h(x,   ))2

n(cid:88)

min
     rd

fi(  )

1
n

1
n

i=1

i=1

suvrit sra (suvrit@mit.edu)

optimization for machine learning

4 / 57

descent methods

minx

f (x)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

5 / 57

x      f(x   )=0xkxk+1...descent methods

minx

f (x)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

5 / 57

x      f(x   )=0xkxk+1...descent methods

suvrit sra (suvrit@mit.edu)

optimization for machine learning

6 / 57

xdescent methods

suvrit sra (suvrit@mit.edu)

optimization for machine learning

6 / 57

   f(x)      f(x)xdescent methods

suvrit sra (suvrit@mit.edu)

optimization for machine learning

6 / 57

   f(x)      f(x)xx        f(x)x        f(x)descent methods

suvrit sra (suvrit@mit.edu)

optimization for machine learning

6 / 57

   f(x)      f(x)xx        f(x)x        f(x)dx+  2diterative algorithm

1 start with some guess x0;
2 for each k = 0, 1, . . .
   guess      k and dk
xk+1     xk +   kdk
check when to stop (e.g., if    f (xk+1)     0)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

7 / 57

(batch) gradient methods

xk+1 = xk +   kdk,

k = 0, 1, . . .

stepsize   k     0, usually ensures f (xk+1) < f (xk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

8 / 57

(batch) gradient methods

xk+1 = xk +   kdk,

k = 0, 1, . . .

stepsize   k     0, usually ensures f (xk+1) < f (xk)
descent direction dk satis   es

(cid:104)   f (xk), dk(cid:105) < 0

suvrit sra (suvrit@mit.edu)

optimization for machine learning

8 / 57

(batch) gradient methods

xk+1 = xk +   kdk,

k = 0, 1, . . .

stepsize   k     0, usually ensures f (xk+1) < f (xk)
descent direction dk satis   es

(cid:104)   f (xk), dk(cid:105) < 0

numerous ways to select   k and dk

suvrit sra (suvrit@mit.edu)

optimization for machine learning

8 / 57

(batch) gradient methods

xk+1 = xk +   kdk,

k = 0, 1, . . .

stepsize   k     0, usually ensures f (xk+1) < f (xk)
descent direction dk satis   es

(cid:104)   f (xk), dk(cid:105) < 0

numerous ways to select   k and dk

usually (batch) methods seek monotonic descent

f (xk+1) < f (xk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

8 / 57

gradient methods     direction

xk+1 = xk +   kdk,

k = 0, 1, . . .

(cid:73) different choices of direction dk
    scaled gradient: dk =    dk   f (xk), dk (cid:31) 0
    newton   s method: (dk = [   2f (xk)]   1)
    quasi-newton: dk     [   2f (xk)]   1
(cid:17)   1
(cid:16)    2f (xk)
    steepest descent: dk = i
    diagonally scaled: dk diagonal with dk
    discretized newton: dk = [h(xk)]   1, h via    nite-diff.

ii    

(   xi)2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

9 / 57

gradient methods     direction

xk+1 = xk +   kdk,

k = 0, 1, . . .

(cid:73) different choices of direction dk
    scaled gradient: dk =    dk   f (xk), dk (cid:31) 0
    newton   s method: (dk = [   2f (xk)]   1)
    quasi-newton: dk     [   2f (xk)]   1
(cid:17)   1
(cid:16)    2f (xk)
    steepest descent: dk = i
    diagonally scaled: dk diagonal with dk
    discretized newton: dk = [h(xk)]   1, h via    nite-diff.
    . . .
exercise: verify that (cid:104)   f (xk), dk(cid:105) < 0 for above choices

ii    

(   xi)2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

9 / 57

gradient methods     stepsize

(cid:73) exact:   k := argmin
     0

f (xk +   dk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

10 / 57

gradient methods     stepsize

f (xk +   dk)

(cid:73) exact:   k := argmin
     0
(cid:73) limited min:   k = argmin
0        s

f (xk +   dk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

10 / 57

gradient methods     stepsize

f (xk +   dk)

(cid:73) exact:   k := argmin
     0
(cid:73) limited min:   k = argmin
0        s
(cid:73) armijo-rule. given    xed scalars, s,   ,    with 0 <    < 1 and

f (xk +   dk)

0 <    < 1 (chosen experimentally). set

  k =   mks,

where we try   ms for m = 0, 1, . . . until suf   cient descent

f (xk)     f (x +   msdk)            ms(cid:104)   f (xk), dk(cid:105)

(cid:73) constant:   k = 1/l (for suitable value of l)

(cid:73) diminishing:   k     0 but(cid:80)

k   k =    .

suvrit sra (suvrit@mit.edu)

optimization for machine learning

10 / 57

convergence

assumption: lipschitz continuous gradient; denoted f     c1

l

(cid:107)   f (x)        f (y)(cid:107)2     l(cid:107)x     y(cid:107)2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

11 / 57

f1(x)f2(x)f(x)yconvergence

assumption: lipschitz continuous gradient; denoted f     c1

l

(cid:107)   f (x)        f (y)(cid:107)2     l(cid:107)x     y(cid:107)2

    gradient vectors of closeby points are close to each other
    objective function has    bounded curvature   
    speed at which gradient varies is bounded

suvrit sra (suvrit@mit.edu)

optimization for machine learning

11 / 57

f1(x)f2(x)f(x)yconvergence

assumption: lipschitz continuous gradient; denoted f     c1

l

(cid:107)   f (x)        f (y)(cid:107)2     l(cid:107)x     y(cid:107)2

lemma (descent). let f     c1

l. then,

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105) + l

l be convex, and(cid:8)xk(cid:9) is sequence generated

theorem. let f     c1
as above, with   k = 1/l. then, f (xk+1)     f (x   ) = o(1/k).
remark: f     c1

l is    good    for nonconvex too, except for f     f    .

2(cid:107)y     x(cid:107)2

2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

12 / 57

strong convexity (faster convergence)

assumption: strong convexity; denote f     s1
2(cid:107)x     y(cid:107)2

f (x)     f (y) + (cid:104)   f (y), x     y(cid:105) +   

2

l,  

(cid:73) a twice diff. f : rd     r is convex if and only if

   x     rd, eigenvalues(cid:2)
   x     rd, eigenvalues(cid:2)

   2(x)(cid:3) (cid:62) 0.
   2f (x)(cid:3) (cid:62)   .

(cid:73) a twice diff. f : rd     r is   -strongly convex if and only if

suvrit sra (suvrit@mit.edu)

optimization for machine learning

13 / 57

strong convexity (faster convergence)

assumption: strong convexity; denote f     s1
2(cid:107)x     y(cid:107)2

f (x)     f (y) + (cid:104)   f (y), x     y(cid:105) +   

2

l,  

(cid:73) a twice diff. f : rd     r is convex if and only if

   x     rd, eigenvalues(cid:2)
   x     rd, eigenvalues(cid:2)

   2(x)(cid:3) (cid:62) 0.
   2f (x)(cid:3) (cid:62)   .

(cid:73) a twice diff. f : rd     r is   -strongly convex if and only if

condition number:    := l

       1 in   uences convergence speed.
  +l yields linear rate (   > 0) for gradient

setting   k = 2

descent. that is, f (xk)     f (x   ) = o(e   k).

suvrit sra (suvrit@mit.edu)

optimization for machine learning

13 / 57

strong convexity     linear rate

method generates a sequence(cid:8)xk(cid:9) that satis   es

if f     s1

theorem.

l,  , 0 <    < 2/(l +   ), then the gradient

(cid:107)xk     x   (cid:107)2

2    

1    

moreover, if    = 2/(l +   ) then

(cid:18)

2    l
   + l

(cid:19)k
(cid:18)        1

   + 1

(cid:107)x0     x   (cid:107)2.
(cid:19)2k

(cid:107)x0     x   (cid:107)2
2,

f (xk)     f        

l
2

where    = l/   is the condition number.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

14 / 57

gradient methods     lower bounds

xk+1 = xk       k   f (xk)

theorem. lower bound i (nesterov) for any x0     rn, and 1    
k     1

2 (n     1), there is a smooth f , s.t.

f (xk)     f (x   )    

3l(cid:107)x0     x   (cid:107)2
32(k + 1)2

2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

15 / 57

gradient methods     lower bounds

xk+1 = xk       k   f (xk)

theorem. lower bound i (nesterov) for any x0     rn, and 1    
k     1

2 (n     1), there is a smooth f , s.t.

f (xk)     f (x   )    

3l(cid:107)x0     x   (cid:107)2
32(k + 1)2

2

theorem. lower bound ii (nesterov). for class of smooth,
strongly convex, i.e., s   l,   (   > 0,    > 1)

(cid:19)2k

(cid:18)          1

      + 1

f (xk)     f (x   )    

  
2

(cid:107)x0     x   (cid:107)2
2.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

15 / 57

faster methods   

suvrit sra (suvrit@mit.edu)

optimization for machine learning

16 / 57

optimal gradient methods

    we saw ef   ciency estimates for the gradient method:

f     c1
l :
f     s1

l,   :

f (xk)     f        
f (xk)     f         l

2

2

k + 4

2l(cid:107)x0     x   (cid:107)2
(cid:19)2k

(cid:18)l       

l +   

(cid:107)x0     x   (cid:107)2
2.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

17 / 57

optimal gradient methods

    we saw ef   ciency estimates for the gradient method:

f     c1
l :
f     s1

l,   :

f (xk)     f        
f (xk)     f         l

2

2

k + 4

2l(cid:107)x0     x   (cid:107)2
(cid:19)2k

(cid:18)l       

(cid:107)x0     x   (cid:107)2
2.

l +   
    we also saw lower complexity bounds

f     c1
l :

fs   l,   :

f (xk)     f (x   )    

f (xk)     f (x   )    

  
2

2

3l(cid:107)x0     x   (cid:107)2
32(k + 1)2

(cid:32)   l          

   l +      

(cid:33)2k

(cid:107)x0     x   (cid:107)2
2.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

17 / 57

optimal gradient methods

    subgradient method upper and lower bounds

f (xk)     f (x   )     o(1/   k)
f (xk)     f (x   )    
2(1+   k+1)

ld

.

    composite objective problems: proximal gradient gives

same bounds as gradient methods.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

18 / 57

gradient with    momentum   
polyak   s method (aka heavy-ball) for f     s1

l,  

xk+1 = xk       k   f (xk) +   k(xk     xk   1)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

19 / 57

gradient with    momentum   
polyak   s method (aka heavy-ball) for f     s1

l,  

xk+1 = xk       k   f (xk) +   k(xk     xk   1)

(cid:73) converges (locally, i.e., for (cid:107)x0     x   (cid:107)2      ) as

(cid:107)xk     x   (cid:107)2

2    

(cid:107)x0     x   (cid:107)2
2,

(cid:33)2k
(cid:32)   l          
   l +      
(cid:17)2
(cid:16)   l        

   l+     

for   k =

4

(   l+     )2 and   k =

suvrit sra (suvrit@mit.edu)

optimization for machine learning

19 / 57

nesterov   s optimal gradient method

minx f (x), where s1

l,   with        0

suvrit sra (suvrit@mit.edu)

optimization for machine learning

20 / 57

nesterov   s optimal gradient method

minx f (x), where s1

l,   with        0

1. choose x0     rn,   0     (0, 1)
2. let y0     x0; set q =   /l

suvrit sra (suvrit@mit.edu)

optimization for machine learning

20 / 57

nesterov   s optimal gradient method

minx f (x), where s1

l,   with        0

1. choose x0     rn,   0     (0, 1)
2. let y0     x0; set q =   /l
3. k-th iteration (k     0):

a). compute intermediate update
xk+1 = yk     1

l   f (yk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

20 / 57

nesterov   s optimal gradient method

minx f (x), where s1

l,   with        0

1. choose x0     rn,   0     (0, 1)
2. let y0     x0; set q =   /l
3. k-th iteration (k     0):

a). compute intermediate update
xk+1 = yk     1

l   f (yk)

b). compute stepsize   k+1 by solving
  2
k+1 = (1       k+1)  2

k + q  k+1

suvrit sra (suvrit@mit.edu)

optimization for machine learning

20 / 57

nesterov   s optimal gradient method

minx f (x), where s1

l,   with        0

1. choose x0     rn,   0     (0, 1)
2. let y0     x0; set q =   /l
3. k-th iteration (k     0):

a). compute intermediate update
xk+1 = yk     1

l   f (yk)

b). compute stepsize   k+1 by solving
  2
k+1 = (1       k+1)  2

k + q  k+1

c). set   k =   k(1       k)/(  2
d). update solution estimate

k +   k+1)

yk+1 = xk+1 +   k(xk+1     xk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

20 / 57

optimal gradient method     rate

theorem. let(cid:8)xk(cid:9) be sequence generated by above algorithm.

(cid:112)  /l, then

if   0    

(cid:26)(cid:16)

(cid:114)   

(cid:17)k

f (xk)     f (x   )     c1 min

1    

l

,

4l

(2   l + c2k)2

(cid:27)

,

where constants c1, c2 depend on   0, l,   .

suvrit sra (suvrit@mit.edu)

optimization for machine learning

21 / 57

strongly convex case     simpli   cation

(cid:112)  /l. the two main steps get simpli   ed:
(cid:113)   

k +   k+1)

if    > 0, select   0 =
1. set   k =   k(1       k)/(  2
2. yk+1 = xk+1 +   k(xk+1     xk)
  k =

  k =

,

l

   l          
   l +      

k     0.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

22 / 57

strongly convex case     simpli   cation

(cid:112)  /l. the two main steps get simpli   ed:
(cid:113)   

k +   k+1)

if    > 0, select   0 =
1. set   k =   k(1       k)/(  2
2. yk+1 = xk+1 +   k(xk+1     xk)
  k =

  k =

,

l

   l          
   l +      

k     0.

optimal method simpli   es to
1. choose y0 = x0     rn
2. k-th iteration (k     0):

suvrit sra (suvrit@mit.edu)

optimization for machine learning

22 / 57

strongly convex case     simpli   cation

(cid:112)  /l. the two main steps get simpli   ed:
(cid:113)   

k +   k+1)

if    > 0, select   0 =
1. set   k =   k(1       k)/(  2
2. yk+1 = xk+1 +   k(xk+1     xk)
  k =

  k =

,

l

   l          
   l +      

k     0.

optimal method simpli   es to
1. choose y0 = x0     rn
2. k-th iteration (k     0):
l   f (yk)

a). xk+1 = yk     1
b). yk+1 = xk+1 +   (xk+1     xk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

22 / 57

strongly convex case     simpli   cation

(cid:112)  /l. the two main steps get simpli   ed:
(cid:113)   

k +   k+1)

if    > 0, select   0 =
1. set   k =   k(1       k)/(  2
2. yk+1 = xk+1 +   k(xk+1     xk)
  k =

  k =

,

l

   l          
   l +      

k     0.

optimal method simpli   es to
1. choose y0 = x0     rn
2. k-th iteration (k     0):
l   f (yk)

a). xk+1 = yk     1
b). yk+1 = xk+1 +   (xk+1     xk)

notice similarity to polyak   s method!

suvrit sra (suvrit@mit.edu)

optimization for machine learning

22 / 57

subgradient methods

suvrit sra (suvrit@mit.edu)

optimization for machine learning

23 / 57

subgradient method

xk+1 = xk       kgk

where gk        f (xk) is any subgradient

suvrit sra (suvrit@mit.edu)

optimization for machine learning

24 / 57

subgradient method

xk+1 = xk       kgk

where gk        f (xk) is any subgradient
stepsize   k > 0 must be chosen

suvrit sra (suvrit@mit.edu)

optimization for machine learning

24 / 57

subgradient method

xk+1 = xk       kgk

where gk        f (xk) is any subgradient
stepsize   k > 0 must be chosen

(cid:73) method generates sequence(cid:8)xk(cid:9)

(cid:73) does this sequence converge to an optimal solution x   ?
(cid:73) if yes, then how fast?
(cid:73) what if have constraints: x     x ?

k   0

suvrit sra (suvrit@mit.edu)

optimization for machine learning

24 / 57

example: lasso problem

min

1

2(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1

xk+1 = xk       k(at(axk     b) +    sgn(xk))

suvrit sra (suvrit@mit.edu)

optimization for machine learning

25 / 57

020406080100100101102example: lasso problem

min

1

2(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1

xk+1 = xk       k(at(axk     b) +    sgn(xk))

(more careful implementation)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

26 / 57

020406080100100101102subgradient method     stepsizes

(cid:73) constant set   k =    > 0, for k     0
(cid:73) scaled constant   k =   /(cid:107)gk(cid:107)2

((cid:107)xk+1     xk(cid:107)2 =   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

27 / 57

subgradient method     stepsizes

(cid:73) constant set   k =    > 0, for k     0
(cid:73) scaled constant   k =   /(cid:107)gk(cid:107)2
((cid:107)xk+1     xk(cid:107)2 =   )
(cid:88)
(cid:73) square summable but not summable
(cid:88)

k
(cid:73) diminishing scalar

  2
k <    ,

  k =    

(cid:88)

k

lim
k

  k = 0,

  k =    

k

(cid:73) adaptive stepsizes (not covered)

not a descent method!

work with best f k so far: f k

min := min0   i   k f i

suvrit sra (suvrit@mit.edu)

optimization for machine learning

27 / 57

exercise

support vector machines

(cid:73) let d := {(xi, yi) | xi     rn, yi     {  1}}
(cid:73) we wish to    nd w     rn and b     r such that

2 + c(cid:88)m

min
w,b

1

2(cid:107)w(cid:107)2

max[0, 1     yi(wtxi + b)]

i=1

(cid:73) derive and implement a subgradient method
(cid:73) plot evolution of objective function
(cid:73) experiment with different values of c > 0
(cid:73) plot and keep track of f k
min := min0   t   k f (xt)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

28 / 57

nonsmooth convergence rates

(cid:73) let   (x) = |x| for x     r

suvrit sra (suvrit@mit.edu)

optimization for machine learning

29 / 57

nonsmooth convergence rates

(cid:73) let   (x) = |x| for x     r
(cid:73) subgradient method xk+1 = xk       kgk, where gk        |xk|.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

29 / 57

nonsmooth convergence rates

(cid:73) let   (x) = |x| for x     r
(cid:73) subgradient method xk+1 = xk       kgk, where gk        |xk|.
(cid:73) if x0 = 1 and   k = 1   k+1 + 1   k+2 (this stepsize is known to be

optimal), then |xk| = 1   k+1

suvrit sra (suvrit@mit.edu)

optimization for machine learning

29 / 57

nonsmooth convergence rates

(cid:73) let   (x) = |x| for x     r
(cid:73) subgradient method xk+1 = xk       kgk, where gk        |xk|.
(cid:73) if x0 = 1 and   k = 1   k+1 + 1   k+2 (this stepsize is known to be

optimal), then |xk| = 1   k+1

(cid:73) thus, o( 1

 2 ) iterations are needed to obtain  -accuracy.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

29 / 57

nonsmooth convergence rates

(cid:73) let   (x) = |x| for x     r
(cid:73) subgradient method xk+1 = xk       kgk, where gk        |xk|.
(cid:73) if x0 = 1 and   k = 1   k+1 + 1   k+2 (this stepsize is known to be

optimal), then |xk| = 1   k+1

(cid:73) thus, o( 1
(cid:73) this behavior typical for the subgradient method which

 2 ) iterations are needed to obtain  -accuracy.

exhibits o(1/   k) convergence in general

suvrit sra (suvrit@mit.edu)

optimization for machine learning

29 / 57

nonsmooth convergence rates

(cid:73) let   (x) = |x| for x     r
(cid:73) subgradient method xk+1 = xk       kgk, where gk        |xk|.
(cid:73) if x0 = 1 and   k = 1   k+1 + 1   k+2 (this stepsize is known to be

optimal), then |xk| = 1   k+1

(cid:73) thus, o( 1
(cid:73) this behavior typical for the subgradient method which

 2 ) iterations are needed to obtain  -accuracy.

exhibits o(1/   k) convergence in general

can we do better in general?

suvrit sra (suvrit@mit.edu)

optimization for machine learning

29 / 57

nonsmooth convergence rates

theorem. (nesterov.) let b = (cid:8)x | (cid:107)x     x0(cid:107)2     d(cid:9). assume,

l(b) (with l > 0),

x        b. there exists a convex function f in c0
such that for 0     k     n     1, the lower-bound
ld
2(1+   k+1)

f (xk)     f (x   )    

,

holds for any algorithm that generates xk by linearly combining
the previous iterates and subgradients.

exercise: so design problems where we can do better!

suvrit sra (suvrit@mit.edu)

optimization for machine learning

30 / 57

constrained problems

suvrit sra (suvrit@mit.edu)

optimization for machine learning

31 / 57

constrained optimization

min

f (x)

s.t.

x     x

don   t want to be as slow as the subgradient method

suvrit sra (suvrit@mit.edu)

optimization for machine learning

32 / 57

projected subgradient method

xk+1 = px (xk       kgk)

where gk        f (xk) is any subgradient

(cid:73) projection: closest feasible point

px (y) = argmin

(cid:107)x     y(cid:107)2

x   x

(cid:73) great as long as projection is    easy   

suvrit sra (suvrit@mit.edu)

optimization for machine learning

33 / 57

projected subgradient method

xk+1 = px (xk       kgk)

where gk        f (xk) is any subgradient

(cid:73) projection: closest feasible point

px (y) = argmin

(cid:107)x     y(cid:107)2

x   x

(cid:73) great as long as projection is    easy   
(cid:73) questions we may have:

does it converge?
for which stepsizes?
how fast?

suvrit sra (suvrit@mit.edu)

optimization for machine learning

33 / 57

examples

1

2(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1

min
s.t. x     x
(cid:73) nonnegativity x     0

px (z) = [z]+
update step: xk+1 = [xk       k(at(axk     b) +    sgn(xk))]+

suvrit sra (suvrit@mit.edu)

optimization for machine learning

34 / 57

examples

1

2(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1

min
s.t. x     x
(cid:73) nonnegativity x     0

px (z) = [z]+
update step: xk+1 = [xk       k(at(axk     b) +    sgn(xk))]+
projection: min(cid:107)x     z(cid:107)2 s.t. x     1 and x        1

(cid:73) (cid:96)   -ball (cid:107)x(cid:107)        1

suvrit sra (suvrit@mit.edu)

optimization for machine learning

34 / 57

examples

1

2(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1

min
s.t. x     x
(cid:73) nonnegativity x     0

(cid:73) (cid:96)   -ball (cid:107)x(cid:107)        1

px (z) = [z]+
update step: xk+1 = [xk       k(at(axk     b) +    sgn(xk))]+
projection: min(cid:107)x     z(cid:107)2 s.t. x     1 and x        1
this is separable, so do it coordinate-wise:
px (z) = y where yi = sgn(zi) min{|zi|, 1}

suvrit sra (suvrit@mit.edu)

optimization for machine learning

34 / 57

examples

1

2(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1

min
s.t. x     x
(cid:73) nonnegativity x     0

(cid:73) (cid:96)   -ball (cid:107)x(cid:107)        1

px (z) = [z]+
update step: xk+1 = [xk       k(at(axk     b) +    sgn(xk))]+
projection: min(cid:107)x     z(cid:107)2 s.t. x     1 and x        1
this is separable, so do it coordinate-wise:
px (z) = y where yi = sgn(zi) min{|zi|, 1}
update step:

zk+1 = xk       k(at(axk     b) +    sgn(xk))
xk+1
i = sgn(zk+1

) min{|zk+1

|, 1}

i

i

suvrit sra (suvrit@mit.edu)

optimization for machine learning

34 / 57

examples

(cid:73) linear constraints ax = b (a     rn  m has rank n)

px (y) = y     a(cid:62)(aa(cid:62))   1(ay     b)

= (i     a(cid:62)(a(cid:62)a)   1a)y + a(cid:62)(aa(cid:62))   1b

suvrit sra (suvrit@mit.edu)

optimization for machine learning

35 / 57

examples

(cid:73) linear constraints ax = b (a     rn  m has rank n)

px (y) = y     a(cid:62)(aa(cid:62))   1(ay     b)

= (i     a(cid:62)(a(cid:62)a)   1a)y + a(cid:62)(aa(cid:62))   1b

update step, using axt = b:

xt+1 = px (xt       tgt)

= xt       t(i     a(cid:62)(aa(cid:62))   1a)gt

suvrit sra (suvrit@mit.edu)

optimization for machine learning

35 / 57

examples

(cid:73) linear constraints ax = b (a     rn  m has rank n)

px (y) = y     a(cid:62)(aa(cid:62))   1(ay     b)

= (i     a(cid:62)(a(cid:62)a)   1a)y + a(cid:62)(aa(cid:62))   1b

update step, using axt = b:

xt+1 = px (xt       tgt)

= xt       t(i     a(cid:62)(aa(cid:62))   1a)gt

(cid:73) simplex x(cid:62)1 = 1 and x     0

more complex but doable in o(n), similarly (cid:96)1-norm ball

suvrit sra (suvrit@mit.edu)

optimization for machine learning

35 / 57

subgradient method     remarks

(cid:73) why care?
simple
low-memory
large-scale versions possible

suvrit sra (suvrit@mit.edu)

optimization for machine learning

36 / 57

subgradient method     remarks

(cid:73) why care?
simple
low-memory
large-scale versions possible

(cid:73) another perspective

xk+1 = min

x   x (cid:104)x, gk(cid:105) +

1
2  k(cid:107)x     xk(cid:107)2

mirror descent

suvrit sra (suvrit@mit.edu)

optimization for machine learning

36 / 57

subgradient method     remarks

(cid:73) why care?
simple
low-memory
large-scale versions possible

(cid:73) another perspective

xk+1 = min

x   x (cid:104)x, gk(cid:105) +

1
2  k(cid:107)x     xk(cid:107)2

mirror descent

(cid:73) improvements using more information (heavy-ball,

   ltered subgradient, . . . )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

36 / 57

subgradient method     remarks

(cid:73) why care?
simple
low-memory
large-scale versions possible

(cid:73) another perspective

xk+1 = min

x   x (cid:104)x, gk(cid:105) +

1
2  k(cid:107)x     xk(cid:107)2

mirror descent

(cid:73) improvements using more information (heavy-ball,

   ltered subgradient, . . . )

(cid:73) don   t forget the dual

may be more amenable to optimization
duality gap?

suvrit sra (suvrit@mit.edu)

optimization for machine learning

36 / 57

what we did not cover

    adaptive stepsize tricks
    space dilation methods, quasi-newton style subgrads
    barrier subgradient method
    sparse subgradient method
    ellipsoid method, center of gravity, etc. as subgradient
    . . .

methods

suvrit sra (suvrit@mit.edu)

optimization for machine learning

37 / 57

feasible descent

min

f (x)

s.t. x     x

(cid:104)   f (x   ), x     x   (cid:105)     0,

   x     x .

suvrit sra (suvrit@mit.edu)

optimization for machine learning

38 / 57

x      f(x   )xf(x)xfeasible descent

xk+1 = xk +   kdk

suvrit sra (suvrit@mit.edu)

optimization for machine learning

39 / 57

feasible descent

xk+1 = xk +   kdk

(cid:73) dk     feasible direction, i.e., xk +   kdk     x

suvrit sra (suvrit@mit.edu)

optimization for machine learning

39 / 57

feasible descent

xk+1 = xk +   kdk

(cid:73) dk     feasible direction, i.e., xk +   kdk     x
(cid:73) dk must also be descent direction, i.e., (cid:104)   f (xk), dk(cid:105) < 0
(cid:73) stepsize   k chosen to ensure feasibility and descent.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

39 / 57

feasible descent

xk+1 = xk +   kdk

(cid:73) dk     feasible direction, i.e., xk +   kdk     x
(cid:73) dk must also be descent direction, i.e., (cid:104)   f (xk), dk(cid:105) < 0
(cid:73) stepsize   k chosen to ensure feasibility and descent.
since x is convex, all feasible directions are of the form

dk =   (z     xk),
where z     x is any feasible vector.

   > 0,

suvrit sra (suvrit@mit.edu)

optimization for machine learning

39 / 57

feasible descent

xk+1 = xk +   kdk

(cid:73) dk     feasible direction, i.e., xk +   kdk     x
(cid:73) dk must also be descent direction, i.e., (cid:104)   f (xk), dk(cid:105) < 0
(cid:73) stepsize   k chosen to ensure feasibility and descent.
since x is convex, all feasible directions are of the form

dk =   (z     xk),
where z     x is any feasible vector.

   > 0,

xk+1 = xk +   k(zk     xk),   k     (0, 1]

suvrit sra (suvrit@mit.edu)

optimization for machine learning

39 / 57

cone of feasible directions

suvrit sra (suvrit@mit.edu)

optimization for machine learning

40 / 57

xxdfeasibledirectionsatxfrank-wolfe / conditional gradient method

optimality: (cid:104)   f (xk), zk     xk(cid:105)     0 for all zk     x

suvrit sra (suvrit@mit.edu)

optimization for machine learning

41 / 57

frank-wolfe / conditional gradient method

optimality: (cid:104)   f (xk), zk     xk(cid:105)     0 for all zk     x
aim: if not optimal, then generate feasible direction
dk = zk     xk that obeys descent condition (cid:104)   f (xk), dk(cid:105) < 0.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

41 / 57

frank-wolfe / conditional gradient method

optimality: (cid:104)   f (xk), zk     xk(cid:105)     0 for all zk     x
aim: if not optimal, then generate feasible direction
dk = zk     xk that obeys descent condition (cid:104)   f (xk), dk(cid:105) < 0.

frank-wolfe (conditional gradient) method

(cid:78) let zk     argminx   x(cid:104)   f (xk), x     xk(cid:105)
(cid:78) use different methods to select   k
(cid:78) xk+1 = xk +   k(zk     xk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

41 / 57

frank-wolfe / conditional gradient method

optimality: (cid:104)   f (xk), zk     xk(cid:105)     0 for all zk     x
aim: if not optimal, then generate feasible direction
dk = zk     xk that obeys descent condition (cid:104)   f (xk), dk(cid:105) < 0.

frank-wolfe (conditional gradient) method

(cid:78) let zk     argminx   x(cid:104)   f (xk), x     xk(cid:105)
(cid:78) use different methods to select   k
(cid:78) xk+1 = xk +   k(zk     xk)
    practical when solving linear problem over x easy
    very popular in machine learning over recent years
    re   nements, several variants (including nonconvex)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

41 / 57

composite objectives

frequently ml problems take the regularized form

minimize f (x) := (cid:96)(x) + r(x)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

42 / 57

composite objectives

frequently ml problems take the regularized form

minimize f (x) := (cid:96)(x) + r(x)

(cid:96)    

+ r    

suvrit sra (suvrit@mit.edu)

optimization for machine learning

42 / 57

composite objectives

frequently ml problems take the regularized form

minimize f (x) := (cid:96)(x) + r(x)

(cid:96)    

+ r    

example: (cid:96)(x) = 1

2(cid:107)ax     b(cid:107)2 and r(x) =   (cid:107)x(cid:107)1
lasso, l1-ls, compressed sensing

suvrit sra (suvrit@mit.edu)

optimization for machine learning

42 / 57

composite objectives

frequently ml problems take the regularized form

minimize f (x) := (cid:96)(x) + r(x)

(cid:96)    

+ r    

example: (cid:96)(x) = 1

2(cid:107)ax     b(cid:107)2 and r(x) =   (cid:107)x(cid:107)1
lasso, l1-ls, compressed sensing

example: (cid:96)(x) : logistic loss, and r(x) =   (cid:107)x(cid:107)1

l1-id28, sparse lr

suvrit sra (suvrit@mit.edu)

optimization for machine learning

42 / 57

composite objective minimization

minimize f (x) := (cid:96)(x) + r(x)

subgradient: xk+1 = xk       kgk, gk        f (xk)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

43 / 57

composite objective minimization

minimize f (x) := (cid:96)(x) + r(x)

subgradient: xk+1 = xk       kgk, gk        f (xk)

subgradient: converges slowly at rate o(1/   k)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

43 / 57

composite objective minimization

minimize f (x) := (cid:96)(x) + r(x)

subgradient: xk+1 = xk       kgk, gk        f (xk)

subgradient: converges slowly at rate o(1/   k)

but: f is smooth plus nonsmooth

we should exploit: smoothness of (cid:96) for better method!

suvrit sra (suvrit@mit.edu)

optimization for machine learning

43 / 57

proximal gradient method

min

f (x) x     x
projected (sub)gradient
x     px (x          f (x))

suvrit sra (suvrit@mit.edu)

optimization for machine learning

44 / 57

proximal gradient method

min

f (x) x     x
projected (sub)gradient
x     px (x          f (x))
f (x) + h(x)

min

proximal gradient

x     prox  h(x          f (x))

prox  h denotes proximity operator for h

why? if we can compute proxh(x) easily, prox-grad con-
verges as fast gradient methods for smooth problems!

suvrit sra (suvrit@mit.edu)

optimization for machine learning

44 / 57

proximity operator

projection

px (y) := argmin
x   rn

1

2(cid:107)x     y(cid:107)2

2 + 1x (x)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

45 / 57

proximity operator

projection

px (y) := argmin
x   rn

1

2(cid:107)x     y(cid:107)2

2 + 1x (x)

proximity: replace 1x by a closed convex function
2 + r(x)

proxr(y) := argmin

1

2(cid:107)x     y(cid:107)2

x   rn

suvrit sra (suvrit@mit.edu)

optimization for machine learning

45 / 57

proximity operator

(cid:96)1-norm ball of radius   (  )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

46 / 57

proximity operators

exercise: let r(x) = (cid:107)x(cid:107)1. solve prox  r(y).

min
x   rn

1

2(cid:107)x     y(cid:107)2

2 +   (cid:107)x(cid:107)1.

hint 1: the above problem decomposes into n independent
subproblems of the form

min
x   r

1

2 (x     y)2 +   |x|.

hint 2: consider the two cases: either x = 0 or x (cid:54)= 0
exercise: moreau decomposition y = proxh y + proxh    y
(notice analogy to v = s + s    in id202)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

47 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0
0        f (x   ) +    h(x   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0
0        f (x   ) +    h(x   )
0          f (x   ) +      h(x   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0
0        f (x   ) +    h(x   )
0          f (x   ) +      h(x   )
x             f (x   ) + (i +      h)(x   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0
0        f (x   ) +    h(x   )
0          f (x   ) +      h(x   )
x             f (x   ) + (i +      h)(x   )

x             f (x   )     (i +      h)(x   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0
0        f (x   ) +    h(x   )
0          f (x   ) +      h(x   )
x             f (x   ) + (i +      h)(x   )
x    = (i +      h)   1(x             f (x   ))

x             f (x   )     (i +      h)(x   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0
0        f (x   ) +    h(x   )
0          f (x   ) +      h(x   )
x             f (x   ) + (i +      h)(x   )
x    = (i +      h)   1(x             f (x   ))
x    = prox  h(x             f (x   ))

x             f (x   )     (i +      h)(x   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

how to cook-up prox-grad?

lemma x    = prox  h(x             f (x   )),       > 0
0        f (x   ) +    h(x   )
0          f (x   ) +      h(x   )
x             f (x   ) + (i +      h)(x   )
x    = (i +      h)   1(x             f (x   ))
x    = prox  h(x             f (x   ))

x             f (x   )     (i +      h)(x   )

above    xed-point eqn suggests iteration

xk+1 = prox  kh(xk       k   f (xk))

suvrit sra (suvrit@mit.edu)

optimization for machine learning

48 / 57

convergence   

suvrit sra (suvrit@mit.edu)

optimization for machine learning

49 / 57

proximal-gradient works, why?

xk+1 = prox  kh(xk       k   f (xk))
xk+1 = xk       kg  k(xk).

suvrit sra (suvrit@mit.edu)

optimization for machine learning

50 / 57

proximal-gradient works, why?

xk+1 = prox  kh(xk       k   f (xk))
xk+1 = xk       kg  k(xk).

gradient mapping: the    gradient-like object   

g  (x) =

1
  

(x     p  h(x          f (x)))

suvrit sra (suvrit@mit.edu)

optimization for machine learning

50 / 57

proximal-gradient works, why?

xk+1 = prox  kh(xk       k   f (xk))
xk+1 = xk       kg  k(xk).

gradient mapping: the    gradient-like object   

g  (x) =

1
  

(x     p  h(x          f (x)))

(cid:73) our lemma shows: g  (x) = 0 if and only if x is optimal
(cid:73) so g   analogous to    f
(cid:73) if x locally optimal, then g  (x) = 0 (nonconvex f )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

50 / 57

convergence analysis

assumption: lipschitz continuous gradient; denoted f     c1

l

(cid:107)   f (x)        f (y)(cid:107)2     l(cid:107)x     y(cid:107)2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

51 / 57

convergence analysis

assumption: lipschitz continuous gradient; denoted f     c1

l

(cid:107)   f (x)        f (y)(cid:107)2     l(cid:107)x     y(cid:107)2

    gradient vectors of closeby points are close to each other
    objective function has    bounded curvature   
    speed at which gradient varies is bounded

suvrit sra (suvrit@mit.edu)

optimization for machine learning

51 / 57

convergence analysis

assumption: lipschitz continuous gradient; denoted f     c1

l

(cid:107)   f (x)        f (y)(cid:107)2     l(cid:107)x     y(cid:107)2

    gradient vectors of closeby points are close to each other
    objective function has    bounded curvature   
    speed at which gradient varies is bounded
lemma (descent). let f     c1

l. then,

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105) + l

2(cid:107)y     x(cid:107)2

2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

51 / 57

convergence analysis

assumption: lipschitz continuous gradient; denoted f     c1

l

(cid:107)   f (x)        f (y)(cid:107)2     l(cid:107)x     y(cid:107)2

    gradient vectors of closeby points are close to each other
    objective function has    bounded curvature   
    speed at which gradient varies is bounded
lemma (descent). let f     c1

l. then,

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105) + l

2(cid:107)y     x(cid:107)2

2

for convex f , compare with

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105).

suvrit sra (suvrit@mit.edu)

optimization for machine learning

51 / 57

descent lemma

l, by taylor   s theorem, for the vector

proof. since f     c1
zt = x + t(y     x) we have

f (y) = f (x) +(cid:82) 1

0 (cid:104)   f (zt), y     x(cid:105)dt.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

52 / 57

descent lemma

l, by taylor   s theorem, for the vector

proof. since f     c1
zt = x + t(y     x) we have

f (y) = f (x) +(cid:82) 1
f (y)     f (x)     (cid:104)   f (x), y     x(cid:105) = (cid:82) 1

0 (cid:104)   f (zt), y     x(cid:105)dt.
add and subtract (cid:104)   f (x), y     x(cid:105) on rhs we have

0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt

suvrit sra (suvrit@mit.edu)

optimization for machine learning

52 / 57

descent lemma

l, by taylor   s theorem, for the vector

proof. since f     c1
zt = x + t(y     x) we have

0 (cid:104)   f (zt), y     x(cid:105)dt.
add and subtract (cid:104)   f (x), y     x(cid:105) on rhs we have

f (y) = f (x) +(cid:82) 1
f (y)     f (x)     (cid:104)   f (x), y     x(cid:105) = (cid:82) 1
(cid:12)(cid:12)(cid:12)(cid:82) 1

|f (y)     f (x)     (cid:104)   f (x), y     x(cid:105)| =

0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt

(cid:12)(cid:12)(cid:12)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

52 / 57

descent lemma

l, by taylor   s theorem, for the vector

proof. since f     c1
zt = x + t(y     x) we have

0 (cid:104)   f (zt), y     x(cid:105)dt.
add and subtract (cid:104)   f (x), y     x(cid:105) on rhs we have

f (y) = f (x) +(cid:82) 1
f (y)     f (x)     (cid:104)   f (x), y     x(cid:105) = (cid:82) 1
(cid:12)(cid:12)(cid:12)(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
0 |(cid:104)   f (zt)        f (x), y     x(cid:105)|dt

|f (y)     f (x)     (cid:104)   f (x), y     x(cid:105)| =
   

(cid:12)(cid:12)(cid:12)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

52 / 57

descent lemma

l, by taylor   s theorem, for the vector

proof. since f     c1
zt = x + t(y     x) we have

0 (cid:104)   f (zt), y     x(cid:105)dt.
add and subtract (cid:104)   f (x), y     x(cid:105) on rhs we have

f (y) = f (x) +(cid:82) 1
f (y)     f (x)     (cid:104)   f (x), y     x(cid:105) = (cid:82) 1
(cid:12)(cid:12)(cid:12)(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
(cid:82) 1
0 |(cid:104)   f (zt)        f (x), y     x(cid:105)|dt
0 (cid:107)   f (zt)        f (x)(cid:107)2    (cid:107)y     x(cid:107)2dt

|f (y)     f (x)     (cid:104)   f (x), y     x(cid:105)| =
   
   

(cid:12)(cid:12)(cid:12)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

52 / 57

descent lemma

l, by taylor   s theorem, for the vector

proof. since f     c1
zt = x + t(y     x) we have

0 (cid:104)   f (zt), y     x(cid:105)dt.
add and subtract (cid:104)   f (x), y     x(cid:105) on rhs we have

f (y) = f (x) +(cid:82) 1
f (y)     f (x)     (cid:104)   f (x), y     x(cid:105) = (cid:82) 1
(cid:12)(cid:12)(cid:12)(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
(cid:82) 1
0 |(cid:104)   f (zt)        f (x), y     x(cid:105)|dt
    l(cid:82) 1
0 (cid:107)   f (zt)        f (x)(cid:107)2    (cid:107)y     x(cid:107)2dt
0 t(cid:107)x     y(cid:107)2
2dt

|f (y)     f (x)     (cid:104)   f (x), y     x(cid:105)| =
   
   

(cid:12)(cid:12)(cid:12)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

52 / 57

descent lemma

l, by taylor   s theorem, for the vector

proof. since f     c1
zt = x + t(y     x) we have

0 (cid:104)   f (zt), y     x(cid:105)dt.
add and subtract (cid:104)   f (x), y     x(cid:105) on rhs we have

f (y) = f (x) +(cid:82) 1
f (y)     f (x)     (cid:104)   f (x), y     x(cid:105) = (cid:82) 1
(cid:12)(cid:12)(cid:12)(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
(cid:82) 1
0 (cid:104)   f (zt)        f (x), y     x(cid:105)dt
(cid:82) 1
0 |(cid:104)   f (zt)        f (x), y     x(cid:105)|dt
    l(cid:82) 1
0 (cid:107)   f (zt)        f (x)(cid:107)2    (cid:107)y     x(cid:107)2dt
0 t(cid:107)x     y(cid:107)2
2dt

|f (y)     f (x)     (cid:104)   f (x), y     x(cid:105)| =
   
   

(cid:12)(cid:12)(cid:12)

2(cid:107)x     y(cid:107)2
2.
bounds f (y) around x with quadratic functions

= l

suvrit sra (suvrit@mit.edu)

optimization for machine learning

52 / 57

descent lemma     corollary

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105) + l

2(cid:107)y     x(cid:107)2

2

let y = x       g  (x), then

suvrit sra (suvrit@mit.edu)

optimization for machine learning

53 / 57

descent lemma     corollary

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105) + l

2(cid:107)y     x(cid:107)2

2

let y = x       g  (x), then

f (y)     f (x)       (cid:104)   f (x), g  (x)(cid:105) +

  2l
2 (cid:107)g  (x)(cid:107)2
2.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

53 / 57

descent lemma     corollary

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105) + l

2(cid:107)y     x(cid:107)2

2

let y = x       g  (x), then

f (y)     f (x)       (cid:104)   f (x), g  (x)(cid:105) +
corollary. so if 0            1/l, we have

  2l
2 (cid:107)g  (x)(cid:107)2
2.

f (y)     f (x)       (cid:104)   f (x), g  (x)(cid:105) +

  

2 (cid:107)g  (x)(cid:107)2
2.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

53 / 57

descent lemma     corollary

f (y)     f (x) + (cid:104)   f (x), y     x(cid:105) + l

2(cid:107)y     x(cid:107)2

2

let y = x       g  (x), then

f (y)     f (x)       (cid:104)   f (x), g  (x)(cid:105) +
corollary. so if 0            1/l, we have

  2l
2 (cid:107)g  (x)(cid:107)2
2.

f (y)     f (x)       (cid:104)   f (x), g  (x)(cid:105) +

2 (cid:107)g  (x)(cid:107)2
2.
lemma let y = x       g  (x). then, for any z we have

  

f (y) + h(y)     f (z) + h(z) + (cid:104)g  (x), x     z(cid:105)       

2 (cid:107)g  (x)(cid:107)2
2.

exercise: prove! (hint: f , h are convex, g  (x)        f (x)        h(y))

suvrit sra (suvrit@mit.edu)

optimization for machine learning

53 / 57

convergence analysis

we   ve actually shown x(cid:48) = x       g  (x) is a descent method.
write    = f + h; plug in z = x to obtain

  (x(cid:48))       (x)       

2 (cid:107)g  (x)(cid:107)2
2.

exercise: why this inequality suf   ces to show convergence.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

54 / 57

convergence analysis

we   ve actually shown x(cid:48) = x       g  (x) is a descent method.
write    = f + h; plug in z = x to obtain

  (x(cid:48))       (x)       

2 (cid:107)g  (x)(cid:107)2
2.

exercise: why this inequality suf   ces to show convergence.
use z = x    in corollary to obtain progress in terms of iterates:

  (x(cid:48))               (cid:104)g  (x), x     x   (cid:105)       

2 (cid:107)g  (x)(cid:107)2

2

suvrit sra (suvrit@mit.edu)

optimization for machine learning

54 / 57

convergence analysis

we   ve actually shown x(cid:48) = x       g  (x) is a descent method.
write    = f + h; plug in z = x to obtain

  (x(cid:48))       (x)       

2 (cid:107)g  (x)(cid:107)2
2.

exercise: why this inequality suf   ces to show convergence.
use z = x    in corollary to obtain progress in terms of iterates:

  (x(cid:48))               (cid:104)g  (x), x     x   (cid:105)       

2

2 (cid:107)g  (x)(cid:107)2

(cid:2)2(cid:104)  g  (x), x     x   (cid:105)     (cid:107)  g  (x)(cid:107)2
(cid:2)

(cid:3)
(cid:3)
2     (cid:107)x     x          g  (x)(cid:107)2

(cid:107)x     x   (cid:107)2

2

2

1
2  
1
2  

=

=

suvrit sra (suvrit@mit.edu)

optimization for machine learning

54 / 57

convergence analysis

we   ve actually shown x(cid:48) = x       g  (x) is a descent method.
write    = f + h; plug in z = x to obtain

  (x(cid:48))       (x)       

2 (cid:107)g  (x)(cid:107)2
2.

exercise: why this inequality suf   ces to show convergence.
use z = x    in corollary to obtain progress in terms of iterates:

  (x(cid:48))               (cid:104)g  (x), x     x   (cid:105)       

2

2 (cid:107)g  (x)(cid:107)2

(cid:2)2(cid:104)  g  (x), x     x   (cid:105)     (cid:107)  g  (x)(cid:107)2
(cid:2)
(cid:2)

(cid:3)
(cid:3)
2     (cid:107)x     x          g  (x)(cid:107)2
2     (cid:107)x(cid:48)     x   (cid:107)2

(cid:107)x     x   (cid:107)2
(cid:107)x     x   (cid:107)2

(cid:3) .

2

2

2

1
2  
1
2  
1
2  

=

=

=

suvrit sra (suvrit@mit.edu)

optimization for machine learning

54 / 57

convergence rate

set x     xk, x(cid:48)     xk+1, and    = 1/l. then add

suvrit sra (suvrit@mit.edu)

optimization for machine learning

55 / 57

convergence rate

(cid:88)k+1
(cid:2)
set x     xk, x(cid:48)     xk+1, and    = 1/l. then add
(cid:107)xi     x   (cid:107)2

(  (xi)          )     l

(cid:88)k+1

i=1

i=1

2

2     (cid:107)xi+1     x   (cid:107)2

2

(cid:3)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

55 / 57

convergence rate

(cid:88)k+1
(cid:88)k+1
(cid:2)
set x     xk, x(cid:48)     xk+1, and    = 1/l. then add
(cid:2)
(cid:107)xi     x   (cid:107)2
(cid:107)x1     x   (cid:107)2

(  (xi)          )     l
2
= l
2

i=1

i=1

2     (cid:107)xi+1     x   (cid:107)2

2

2     (cid:107)xk+1     x   (cid:107)2

2

(cid:3)

(cid:3)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

55 / 57

convergence rate

(cid:88)k+1
(cid:88)k+1
(cid:2)
set x     xk, x(cid:48)     xk+1, and    = 1/l. then add
(cid:2)
(cid:107)xi     x   (cid:107)2
(cid:107)x1     x   (cid:107)2
2     (cid:107)xk+1     x   (cid:107)2
2(cid:107)x1     x   (cid:107)2
2.

(  (xi)          )     l
2
= l
2
    l

i=1

i=1

2

(cid:3)

2     (cid:107)xi+1     x   (cid:107)2

2

(cid:3)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

55 / 57

convergence rate

(cid:3)

i=1

(cid:88)k+1
(cid:88)k+1
(cid:2)
set x     xk, x(cid:48)     xk+1, and    = 1/l. then add
(cid:2)
(  (xi)          )     l
(cid:107)xi     x   (cid:107)2
2
= l
(cid:107)x1     x   (cid:107)2
2     (cid:107)xk+1     x   (cid:107)2
2
    l
2(cid:107)x1     x   (cid:107)2
2.
k+1(cid:88)

since   (xk) is a decreasing sequence, it follows that

i=1

l

1

2

(cid:3)

2     (cid:107)xi+1     x   (cid:107)2

2

  (xk+1)              

k + 1

i=1

(  (xi)          )    

2(k + 1)(cid:107)x1     x   (cid:107)2
2.

this is the well-known o(1/k) rate.
(cid:73) but for c1

l convex functions, optimal rate is o(1/k2)!

suvrit sra (suvrit@mit.edu)

optimization for machine learning

55 / 57

accelerated proximal gradient

min   (x) = f (x) + h(x)

let x0 = y0     dom h. for k     1:

xk = prox  kh(yk   1       k   f (yk   1))
yk = xk +

k     1
k + 2 (xk     xk   1).

framework due to: nesterov (1983, 2004); also beck, teboulle (2009).
simpli   ed analysis: tseng (2008).

    uses extra    memory    for interpolation
    same computational cost as ordinary prox-grad
    convergence rate theoretically optimal

  (xk)              

(k + 1)2(cid:107)x0     x   (cid:107)2
2.

2l

suvrit sra (suvrit@mit.edu)

optimization for machine learning

56 / 57

proximal methods     cornucopia

douglas rachford splitting
admm (special case of dr on dual)
proximal-dykstra
proximal methods for f1 + f2 +        + fn
peaceman-rachford
proximal quasi-newton, newton
many other variation...

suvrit sra (suvrit@mit.edu)

optimization for machine learning

57 / 57

