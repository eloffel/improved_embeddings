optimization for machine learning

(lecture 3-a - convex)

suvrit sra

massachusetts institute of technology

special thanks: francis bach (inria, ens)
(for sharing this material, and permitting its use)

mpi-is t   ubingen

machine learning summer school, june 2017

course materials

http://suvrit.de/teaching.html
some references:
(cid:4) introductory lectures on id76     nesterov
(cid:4) id76     boyd & vandenberghe
(cid:4) nonid135     bertsekas
(cid:4) convex analysis     rockafellar
(cid:4) fundamentals of convex analysis     urruty, lemar  echal
(cid:4) lectures on modern id76     nemirovski
(cid:4) optimization for machine learning     sra, nowozin, wright
(cid:4) nips 2016 optimization tutorial     bach, sra
some related courses:
(cid:4) ee227a, spring 2013, (sra, uc berkeley)
(cid:4) 10-801, spring 2014 (sra, cmu)
(cid:4) ee364a,b (boyd, stanford)
(cid:4) ee236b,c (vandenberghe, ucla)
venues: nips, icml, uai, aistats, siopt, math. prog.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

2 / 22

lecture plan

    introduction
    recap of convexity, sets, functions
    recap of duality, optimality, problems
    first-order optimization algorithms and techniques
    large-scale optimization (sgd and friends)
    directions in non-id76

suvrit sra (suvrit@mit.edu)

optimization for machine learning

3 / 22

iteration complexity: smooth problems

(cid:73) assumption: f convex and l-smooth on rd
(cid:73) id119:   t =   t   1       k g(cid:48)(  t   1)

g(  t)     g(x   ) (cid:54) o(1/t)
g(  t)     g(x   ) (cid:54) o(e   t(  /l)) = o(e   t/  ) if   -strongly convex

(small    = l/  )

(large    = l/  )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

4 / 22

iteration complexity: smooth problems

    assumption: f convex and l-smooth on rd
    id119:   t =   t   1       k g(cid:48)(  t   1)

o(1/t) convergence rate for convex functions
o(e    t

   ) if strongly-convex     complexity = o(nd       log 1
   )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

5 / 22

iteration complexity: smooth problems

    assumption: f convex and l-smooth on rd
    id119:   t =   t   1       k g(cid:48)(  t   1)

o(1/t) convergence rate for convex functions
o(e    t
(cid:73) key insights for ml (bottou and bousquet, 2008)

   ) if strongly-convex     complexity = o(nd       log 1
   )

1 no need to optimize below statistical error

suvrit sra (suvrit@mit.edu)

optimization for machine learning

5 / 22

iteration complexity: smooth problems

    assumption: f convex and l-smooth on rd
    id119:   t =   t   1       k g(cid:48)(  t   1)

o(1/t) convergence rate for convex functions
o(e    t
(cid:73) key insights for ml (bottou and bousquet, 2008)

   ) if strongly-convex     complexity = o(nd       log 1
   )

1 no need to optimize below statistical error
2 cost functions are averages

suvrit sra (suvrit@mit.edu)

optimization for machine learning

5 / 22

iteration complexity: smooth problems

    assumption: f convex and l-smooth on rd
    id119:   t =   t   1       k g(cid:48)(  t   1)

o(1/t) convergence rate for convex functions
o(e    t
(cid:73) key insights for ml (bottou and bousquet, 2008)

   ) if strongly-convex     complexity = o(nd       log 1
   )

1 no need to optimize below statistical error
2 cost functions are averages
3 testing error is more important than training error

suvrit sra (suvrit@mit.edu)

optimization for machine learning

5 / 22

stochastic id119 for    nite sums

min
     rd

g(  ) =

1
n

fi(  )

n(cid:88)

i=1

    iteration:   t =   t   1       kf (cid:48)

i(t)(  t   1)

    sampling with replacement: i(t)     unif({1, . . . , n})
    polyak-ruppert averaging:     t = 1

u=0   u

t+1

    convergence rate if each fi is convex l-smooth and f

(cid:80)t

  -strongly-convex:

e[g(    t)     g(     )] (cid:54)

(cid:26) o(1/

   

k)

o(l/(  k)) = o(  /k)

   

if   k = 1/(l
if   k = 1/(  k)

k)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

6 / 22

stochastic vs. deterministic     strongly cvx

n(cid:88)

fi(  ) with fi(  ) = (cid:96)(cid:0)yi, h(xi,   )(cid:1) +      (  )

(cid:73) min g(  ) =
(cid:73) batch id119:

i=1

1
n

  t =   t   1       kg(cid:48)(  t   1) =   t   1       k
n
    linear (e.g., exponential) convergence rate in o(e   t/  )
    iteration complexity is linear in n

f (cid:48)
i (  t   1)

i=1

n(cid:88)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

7 / 22

stochastic vs. deterministic     strongly cvx

n(cid:88)

fi(  ) with fi(  ) = (cid:96)(cid:0)yi, h(xi,   )(cid:1) +      (  )

(cid:73) min g(  ) =
(cid:73) batch id119:

i=1

1
n

  t =   t   1       kg(cid:48)(  t   1) =   t   1       k
n
    linear (e.g., exponential) convergence rate in o(e   t/  )
    iteration complexity is linear in n

f (cid:48)
i (  t   1)

i=1

(cid:73) stochastic id119:   t =   t   1       kf (cid:48)

i(t)(  t   1)

    sampling with replacement: i(t) random element of {1, . . . , n}
    convergence rate in o(  /t)
    iteration complexity is independent of n

n(cid:88)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

7 / 22

stochastic vs. deterministic     strongly cvx

n(cid:88)

fi(  ) with fi(  ) = (cid:96)(cid:0)yi, h(xi,   )(cid:1) +      (  )

(cid:73) min g(  ) =
(cid:73) batch id119:

i=1

1
n

  t =   t   1       kg(cid:48)(  t   1) =   t   1       k
n
    linear (e.g., exponential) convergence rate in o(e   t/  )
    iteration complexity is linear in n

f (cid:48)
i (  t   1)

i=1

(cid:73) stochastic id119:   t =   t   1       kf (cid:48)

i(t)(  t   1)

    sampling with replacement: i(t) random element of {1, . . . , n}
    convergence rate in o(  /t)
    iteration complexity is independent of n

n(cid:88)

gd

sgd

suvrit sra (suvrit@mit.edu)

optimization for machine learning

7 / 22

stochastic vs. deterministic methods

goal = best of both worlds: linear rate with o(d) iteration cost

simple choice of step size

suvrit sra (suvrit@mit.edu)

optimization for machine learning

8 / 22

timelog(excesscost)deterministicstochasticstochastic vs. deterministic methods

goal = best of both worlds: linear rate with o(d) iteration cost

simple choice of step size

suvrit sra (suvrit@mit.edu)

optimization for machine learning

9 / 22

timelog(excesscost)deterministicstochasticnewlinearly convergent stochastic gradient algorithms

many related algorithms

sag (le roux, schmidt, and bach, 2012)
sdca (shalev-shwartz and zhang, 2013)
svrg (johnson and zhang, 2013; zhang et al., 2013)
miso (mairal, 2015)
finito (defazio et al., 2014b)
saga (defazio, bach, and lacoste-julien, 2014a)
      

similar rates of convergence and iterations

suvrit sra (suvrit@mit.edu)

optimization for machine learning

10 / 22

linearly convergent stochastic gradient algorithms

many related algorithms

sag (le roux, schmidt, and bach, 2012)
sdca (shalev-shwartz and zhang, 2013)
svrg (johnson and zhang, 2013; zhang et al., 2013)
miso (mairal, 2015)
finito (defazio et al., 2014b)
saga (defazio, bach, and lacoste-julien, 2014a)
      

similar rates of convergence and iterations

    different interpretations and proofs / proof lengths

    lazy gradient evaluations
    variance reduction

suvrit sra (suvrit@mit.edu)

optimization for machine learning

10 / 22

running-time comparisons (strongly-convex)

(cid:73) assumptions: g(  ) = 1
n

i=1 fi(  )

    each fi convex l-smooth and f is   -strongly convex

(cid:80)n

suvrit sra (suvrit@mit.edu)

optimization for machine learning

11 / 22

(cid:80)n

running-time comparisons (strongly-convex)

(cid:73) assumptions: g(  ) = 1
n

i=1 fi(  )

stochastic id119
id119

    each fi convex l-smooth and f is   -strongly convex
   1
   log 1
   log 1
   )    log 1

accelerated id119
sag/svrg

d   (cid:12)(cid:12)(cid:12) l
d   (cid:12)(cid:12)(cid:12)n l
d   (cid:12)(cid:12)(cid:12)n
(cid:113) l
d   (cid:12)(cid:12)(cid:12)(n + l

  

  

  

  

  

  

  

suvrit sra (suvrit@mit.edu)

optimization for machine learning

11 / 22

(cid:80)n

running-time comparisons (strongly-convex)

i=1 fi(  )

(cid:73) assumptions: g(  ) = 1
n

stochastic id119
id119

    each fi convex l-smooth and f is   -strongly convex
   1
   log 1
   log 1
   )    log 1
(cid:73) beating two lower bounds (nemirovski and yudin, 1983;

accelerated id119
sag/svrg

d   (cid:12)(cid:12)(cid:12) l
d   (cid:12)(cid:12)(cid:12)n l
d   (cid:12)(cid:12)(cid:12)n
(cid:113) l
d   (cid:12)(cid:12)(cid:12)(n + l

  

  

  

  

  

  

  

nesterov, 2004): with additional assumptions
(1) stochastic gradient: exponential rate for    nite sums
(2) full gradient: better exponential rate using the sum

structure

suvrit sra (suvrit@mit.edu)

optimization for machine learning

11 / 22

running-time comparisons (non-strongly-convex)

(cid:73) assumptions: g(  ) = 1
n

i=1 fi(  )

    each fi convex l-smooth
    ill conditioned problems: f may not be strongly-convex

(cid:80)n

stochastic id119

id119

accelerated id119

sag/svrg

d  
d  
d  
d  

(cid:12)(cid:12)(cid:12)(cid:12)1/  2
(cid:12)(cid:12)(cid:12)(cid:12)n/  
(cid:12)(cid:12)(cid:12)(cid:12)n/
(cid:12)(cid:12)(cid:12)(cid:12)   

   

  

n/  

suvrit sra (suvrit@mit.edu)

optimization for machine learning

12 / 22

running-time comparisons (non-strongly-convex)

(cid:73) assumptions: g(  ) = 1
n

i=1 fi(  )

    each fi convex l-smooth
    ill conditioned problems: f may not be strongly-convex

(cid:80)n

stochastic id119

id119

accelerated id119

sag/svrg

d  
d  
d  
d  

(cid:12)(cid:12)(cid:12)(cid:12)1/  2
(cid:12)(cid:12)(cid:12)(cid:12)n/  
(cid:12)(cid:12)(cid:12)(cid:12)n/
(cid:12)(cid:12)(cid:12)(cid:12)   

   

  

n/  

(cid:73) adaptivity to potentially hidden strong convexity
(cid:73) no need to know the local/global strong-convexity constant

suvrit sra (suvrit@mit.edu)

optimization for machine learning

12 / 22

experimental results (id28)

quantum dataset
(n = 50 000, d = 78)

rcv1 dataset

(n = 697 641, d = 47 236)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

13 / 22

                                                                                                                                                                                        afgl   bfgssgasgiag                                                                                                                                                                                                    afgl   bfgssgasgiagexperimental results (id28)

quantum dataset
(n = 50 000, d = 78)

rcv1 dataset

(n = 697 641, d = 47 236)

suvrit sra (suvrit@mit.edu)

optimization for machine learning

14 / 22

                                                                                                                                                                                                                                afgl   bfgssgasgiagsag   ls                                                                                                                                                                                            afgl   bfgssgasgiagsag   lskey idea: variance reduction

principle: reducing variance of sample of x by using a sample
from another random variable y with known expectation

z   =   (x     y) + ey

ez   =   ex + (1       )ey

var(z  ) =   2(cid:2) var(x) + var(y)     2cov(x, y)(cid:3)

   = 1: no bias,    < 1: potential bias (but reduced variance)
useful if y positively correlated with x

suvrit sra (suvrit@mit.edu)

optimization for machine learning

15 / 22

key idea: variance reduction

principle: reducing variance of sample of x by using a sample
from another random variable y with known expectation

z   =   (x     y) + ey

ez   =   ex + (1       )ey

var(z  ) =   2(cid:2) var(x) + var(y)     2cov(x, y)(cid:3)

   = 1: no bias,    < 1: potential bias (but reduced variance)
useful if y positively correlated with x

application to gradient estimation (johnson and zhang, 2013;
zhang, mahdavi, and jin, 2013)
i(t)(  t   1), y = f (cid:48)
i (    ) full gradient at     ;

i(t)(    ),    = 1, with      stored

(cid:80)n
svrg: x = f (cid:48)
i=1 f (cid:48)
ey = 1
n
i(t)(  t   1)     f (cid:48)
x     y = f (cid:48)

i(t)(    )

suvrit sra (suvrit@mit.edu)

optimization for machine learning

15 / 22

stochastic variance reduced gradient (svrg)

initialize          rd
for iepoch = 1 to # of epochs
compute all gradients f (cid:48)
initialize x0 =     
for t = 1 to length of epochs

i (    ) ; store g(cid:48)(    ) = 1

n

i (    )

(cid:104)

g(cid:48)(    ) +(cid:0)f (cid:48)

i(t)(  t   1)     f (cid:48)

(cid:80)n
i=1 f (cid:48)
i(t)(    )(cid:1)(cid:105)

  t =   t   1       

update      =   t

output:     

    two gradient evaluations per inner step; no need to store

gradients (sag needs storage)

    two parameters: length of epochs + step-size   
    same linear convergence rate as sag, simpler proof

suvrit sra (suvrit@mit.edu)

optimization for machine learning

16 / 22

svrg vs. saga

saga update:
  t =   t   1       
n
svrg update:
  t =   t   1       

(cid:104) 1
(cid:104) 1

n

(cid:80)n
i=1 yt   1
(cid:80)n
i=1 f (cid:48)

i +(cid:0)f (cid:48)
i (    ) +(cid:0)f (cid:48)

(cid:1)(cid:105)
i(t)(    )(cid:1)(cid:105)

i(t)

i(t)(  t   1)     yt   1

i(t)(  t   1)     f (cid:48)

storage of gradients
epoch-based
parameters
gradient evaluations per step
adaptivity to strong-convexity
robustness to ill-conditioning

saga
yes
no
step-size
1
yes
yes

svrg
no
yes
step-size & epoch lengths
at least 2
no
no

suvrit sra (suvrit@mit.edu)

optimization for machine learning

17 / 22

proximal extensions

n(cid:88)

fi(  )+h(  )

1
n

i=1

    composite optimization problems: min
x   rd

    fi smooth and convex
    h convex, potentially non-smooth
    constrained optimization: h an indicator function
    sparsity-inducing norms, e.g., h(  ) = (cid:107)  (cid:107)1
    proximal methods (a.k.a. splitting methods)

    projection / soft-thresholding step after gradient update
    see, e.g., combettes and pesquet (2011); bach, jenatton,
mairal, and obozinski (2012); parikh and boyd (2014)

    directly extends to variance-reduced gradient techniques

same rates of convergence

suvrit sra (suvrit@mit.edu)

optimization for machine learning

18 / 22

sgd minimizes the testing cost!

(cid:73) goal: minimize g(  ) = ep(x,y)(cid:96)(y,   (cid:62)  (x))
    given n independent samples (xi, yi)n
i=1, from p(x, y)
    given a single pass of stochastic id119
    bounds on the excess testing cost eg(    n)     infx   rd g(  )

   
(cid:73) optimal convergence rates: o(1/

n) and o(1/(n  ))
    optimal for non-smooth (nemirovski and yudin, 1983)
    attained by averaged sgd with decaying step-sizes

suvrit sra (suvrit@mit.edu)

optimization for machine learning

19 / 22

references

r. babanezhad, m. o. ahmed, a. virani, m. w. schmidt, j. konecn  y, and
s. sallinen. stopwasting my gradients: practical svrg. in advances in
neural information processing systems (nips), 2015.

f. bach, r. jenatton, j. mairal, and g. obozinski. optimization with

sparsity-inducing penalties. foundations and trends in machine learning, 4
(1):1   106, 2012.

l. bottou and o. bousquet. the tradeoffs of large scale learning. in adv.

nips, 2008.

p. l. combettes and j.-c. pesquet. proximal splitting methods in signal
processing. in fixed-point algorithms for inverse problems in science and
engineering, pages 185   212. springer, 2011.

a. defazio, f. bach, and s. lacoste-julien. saga: a fast incremental gradient

method with support for non-strongly convex composite objectives. in
advances in neural information processing systems (nips), 2014a.

a. defazio, j. domke, and t. s. caetano. finito: a faster, permutable

incremental gradient method for big data problems. in proc. icml, 2014b.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

20 / 22

r. johnson and t. zhang. accelerating stochastic id119 using

predictive variance reduction. in advances in neural information processing
systems, 2013.

n. le roux, m. schmidt, and f. bach. a stochastic gradient method with an
exponential convergence rate for strongly-id76 with    nite
training sets. in advances in neural information processing systems (nips),
2012.

j. mairal. incremental majorization-minimization optimization with

application to large-scale machine learning. siam journal on optimization,
25(2):829   855, 2015.

a. s. nemirovski and d. b. yudin. problem complexity and method ef   ciency in

optimization. wiley & sons, 1983.

y. nesterov. introductory lectures on id76: a basic course. kluwer,

2004.

n. parikh and s. p. boyd. proximal algorithms. foundations and trends in

optimization, 1(3):127   239, 2014.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

21 / 22

s. shalev-shwartz and t. zhang. stochastic dual coordinate ascent methods
for regularized loss minimization. journal of machine learning research, 14
(feb):567   599, 2013.

l. zhang, m. mahdavi, and r. jin. linear convergence with condition
number independent access of full gradients. in advances in neural
information processing systems, 2013.

suvrit sra (suvrit@mit.edu)

optimization for machine learning

22 / 22

