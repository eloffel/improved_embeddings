optimization for machine learning

(lecture 3-b - nonconvex)

suvrit sra   

massachusetts institute of technology

mpi-is t  bingen 

machine learning summer school, june 2017

ml.mit.edu

nonconvex problems are    

nonid76 problem with simple constraints

min    xi

s.t.

aizi   s   2

0     zi     1,

+xi

i = 1, . . . , n.

zi(1   zi)

question: is global min of this problem 0 or not?

does there exist a subset of                          that sums to s?

{a1, a2, . . . , an}

subset-sum problem, well-known np-complete prob.

min x>ax,

x   0

question: is x=0 a local minimum or not?

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

2

nonconvex    nite-sum problems

min
   2rd

1
n

nxi=1

` yi,dnn (xi,    )  +    (   )

x1

x2

xp

   

   

   

f (x;    )

   

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

3

nonconvex    nite-sum problems

g(   ) =

min
   2rd

1
n

nxi=1

fi(   )

related work

    original sgd paper    (robbins, monro 1951)   
    sgd with scaled gradients (                              ) + other tricks:   

(asymptotic convergence; no rates)

   t      thtrf (   t)

space dilation, (shor, 1972); variable metric sgd (uryasev 1988);  adagrad 
(duchi, hazan, singer, 2012);  adam (kingma, ba, 2015), and many others      
(typically asymptotic convergence for nonconvex)
    large number of other ideas, often for step-size tuning, initialization   

(see e.g., blog post: by s. ruder on id119 algorithms)

going beyond sgd (theoretically; ultimately in practice too)

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

4

nonconvex    nite-sum problems

g(   ) =

min
   2rd

1
n

nxi=1

fi(   )

related work (subset)

   

   

   

   

   

(solodov, 1997)                      incremental gradient,  smooth nonconvex   
                                         (asymptotic convergence; no rates proved)
(bertsekas, tsitsiklis, 2000)        id119 with errors;  incremental   
                                         (see   2.4, nonid135; no rates proved)
(sra, 2012)                           incremental nonconvex non-smooth    
                                         (asymptotic convergence only)
(ghadimi, lan, 2013)               sgd for nonconvex stochastic opt.   
                                          (   rst non-asymptotic rates to stationarity)
(ghadimi et al., 2013)              sgd for nonconvex non-smooth stoch. opt.   
                                          (non-asymptotic rates, but key limitations)                           

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

5

dif   culty of nonid76

3

2

1

0

-1

so, try to see how fast we can 
satisfy this necessary condition

-3

-2

-4

-6

-4

-2

0

2

4

6

dif   cult to optimize, but
rg(   ) = 0

necessary condition     local minima,   
maxima, saddle points satisfy it.

6

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

measuring ef   ciency of nonconvex opt.

convex:

nonconvex:

(nesterov 2003,  chap 1);   
(ghadimi, lan, 2012)

e[g(   t)   g   ]        
e[krg(   t)k2]        

(optimality gap)

(stationarity gap)

incremental first-order oracle (ifo)

(agarwal, bottou, 2014)   

(see also: nemirovski, yudin, 1983)

(x, i)

measure: #ifo calls to attain      accuracy

(fi(x),rfi(x))

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

7

ifo example: sgd vs gd (nonconvex)

g(   ) =

min
   2rd

1
n

nxi=1

fi(   )

sgd

   t+1 =    t      rfit(   t)
    o(1) ifo calls per iter

    o(1/    2) iterations
    total: o(1/    2) ifo calls

    independent of n
(ghadimi, lan, 2013,2014) 

 gd

   t+1 = xt      rg(   t)
    o(n) ifo calls per liter

    o(1/    ) iterations
    total: o(n/    ) ifo calls

    depends strongly on n
(nesterov, 2003; nesterov 2012) 

assuming lipschitz gradients

e[krg(   t)k2]        

8

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

?nonconvex    nite-sum problems

g(   ) =

min
   2rd

1
n

nxi=1

fi(   )

sgd

   t+1 =    t      rfit(   t)

 gd

   t+1 = xt      rg(   t)

do these bene   ts extend 
to nonconvex    nite-sums?
analysis depends heavily on convexity   

sag, svrg, saga, et al.

(especially for controlling variance)

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

9

svrg/saga work again!   

(with new analysis)

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

10

nonconvex svrg

for s=0 to s-1
   s+1
0      s
     s      s
for t=0 to m-1


m

m

uniformly randomly pick 

t      thrfi(t)(   s+1
   s+1
t+1 =    s+1

t

i(t) 2 {1, . . . , n}
nxi=1
)   rfi(t)(     s) +

1
n

end

end

the same algorithm as usual svrg (johnson, zhang, 2013)

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

rfi(     s)i

11

nonconvex svrg

for s=0 to s-1

end

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

12

   s+10    sm     s    smfor t=0 to m-1
uniformly randomly pick 
i(t)2{1,...,n}   s+1t+1=   s+1t    thrfi(t)(   s+1t) rfi(t)(     s)+1nnxi=1rfi(     s)iendnonconvex svrg

for s=0 to s-1
   s+1
0      s

m

end

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

13

     s    smfor t=0 to m-1
uniformly randomly pick 
i(t)2{1,...,n}   s+1t+1=   s+1t    thrfi(t)(   s+1t) rfi(t)(     s)+1nnxi=1rfi(     s)iendnonconvex svrg

for s=0 to s-1
   s+1
0      s
     s      s

m

m

end

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

14

for t=0 to m-1
uniformly randomly pick 
i(t)2{1,...,n}   s+1t+1=   s+1t    thrfi(t)(   s+1t) rfi(t)(     s)+1nnxi=1rfi(     s)iendnonconvex svrg

for t=0 to m-1

uniformly randomly pick 
i(t) 2 {1, . . . , n}
nxi=1
   s+1
t+1 =    s+1
end

t      thrfi(t)(   s+1

) rfi(t)(     s) +

1
n

t

rfi(     s)i

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

15

for s=0 to s-1   s+10    sm     s    smendnonconvex svrg

for t=0 to m-1

uniformly randomly pick 
i(t) 2 {1, . . . , n}
nxi=1
   s+1
t+1 =    s+1
end
{z

t      thrfi(t)(   s+1

) rfi(t)(     s) +

|

 t

1
n

t

rfi(     s)i
}

e[ t] = 0

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

16

for s=0 to s-1   s+10    sm     s    smendnonconvex svrg

for t=0 to m-1

uniformly randomly pick 
i(t) 2 {1, . . . , n}
nxi=1
   s+1
t+1 =    s+1
end

t      thrfi(t)(   s+1

) rfi(t)(     s) +

1
n

t

rfi(     s)i
}
{z

|

full gradient, computed
once every epoch

17

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

for s=0 to s-1   s+10    sm     s    smendnonconvex svrg

for s=0 to s-1

key quantities that determine
how the method operates

for t=0 to m-1


uniformly randomly pick 

t      thrfi(t)(   s+1
   s+1
t+1 =    s+1

t

) rfi(t)(     s) +

nxi=1

rfi(     s)i
}
{z

1
n

|

full gradient, computed
once every epoch

18

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

i(t)2{1,...,n}end   s+10    sm     s    smendkey ideas for analysis of nc-svrg

previous svrg proofs rely on convexity to control variance

larger step-size      smaller inner loop   
(full-gradient computation dominates epoch)   

smaller step-size     slower convergence

(longer inner loop)

(carefully) trading-off #inner-loop iterations m with
step-size    leads to lower #ifo calls!

(reddi, hefny, sra, poczos, smola, 2016; allen-zhu, hazan, 2016)

19

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

faster nonid76 via vr

(reddi, hefny, sra, poczos, smola, 2016; reddi et al., 2016)

algorithm

sgd
gd
svrg
saga
msvrg

remarks

nonconvex (lipschitz smooth)

   2 o  n
o  1
    
o n + n2/3
     
o n + n2/3
     
          
o   min    1
e[krg(   t)k2]        

   2 , n2/3

new results for convex case too; additional nonconvex results
for related results, see also (allen-zhu, hazan, 2016)

20

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

linear rates for nonconvex problems

g(   ) =

1
n

min
   2rd

fi(   )

nxi=1

the polyak-  ojasiewicz (pl) class of functions

g(   )   g(      )    

(polyak, 1963); (  ojasiewicz, 1963)

1
2  krg(   )k2

examples:

  -strongly convex (cid:15482) pl holds
stochastic pca**, some large-scale 
eigenvector problems

(more general than many other    restricted    strong convexity uses)

(karimi, nutini, schmidt, 2016)
(attouch, bolte, 2009)
(bertsekas, 2016)

proximal extensions; references
more general kurdya-  ojasiewicz class
textbook, more    growth conditions   

21

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

linear rates for nonconvex problems

g(   )   g(      )    
algorithm

1
2  krg(   )k2
nonconvex 

sgd
gd
svrg
saga
msvrg

e[g(   t)   g   ]        

nonconvex-pl

      

2   log 1

o  1
   2 
o    n
o   (n + n2/3
o   (n + n2/3

2   ) log 1
2   ) log 1
      

      
      

variant of nc-svrg attains this fast convergence!

(reddi, hefny, sra, poczos, smola, 2016; reddi et al., 2016)

22

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

o 1   2 o n    o n+n2/3    o n+n2/3    o   min 1   2 ,n2/3      empirical results

2
k
)
t
   
(
f
r
k

cifar10 dataset; 2-layer nn

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

23

some surprises!

min
   2rd

1
n

nxi=1

fi(   ) +    (   )

regularizer, e.g.,        for enforcing sparsity of
weights (in a neural net, or more generally); 
or an indicator function of a constraint set, etc.

k    k1

suvrit sra (ml.mit.edu)

beyond stochastic gradients and convexity: part 2

24

nonconvex composite objective problems

min
   2rd

fi(   ) +    (   )
convex

1
n
nonconvex

nxi=1
{z

}

|
   t+1 = prox t    (   t      trfit(   t))
prox-sgd convergence not known!*
1
2ku   vk2 +     (u)
prox    (v) := argminu

prox-sgd

prox: soft-thresholding for        ; projection for indicator function
    partial results: (ghadimi, lan, zhang, 2014)   
            (using growing minibatches, shrinking step sizes)

k    k1

* except in special cases (where even rates may be available)

25

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

empirical results: nn-pca

covtype (581012,54)
sgd
kwk   1, w 0 
saga
svrg

min

1
2

w>  nxi=1

rcv1 (677399, 47236)
sgd
saga
svrg

xix>i ! w

10-5

)
^x
(
f
!

)
x
(
f

10-10

10-15

0

5

10

# grad/n

15

20

5

10

# grad/n

15

10-5

10-10

)
^x
(
f
!

)
x
(
f

10-15

0

y-axis denotes distance                 to an approximate optimum

f (   )   f (     )

eigenvecs via sgd: (oja, karhunen 1985); via svrg (shamir, 2015,2016);   
(garber, hazan, jin, kakade, musco, netrapalli, sidford, 2016); and many more!

26

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

finite-sum problems with

nonconvex g(  ) and params   
lying on a known manifold  

g(   ) =

1
n

min
   2m

fi(   )

nxi=1

example: eigenvector problems (the ||  ||=1 constraint)

                 problems with orthogonality constraints

                 low-rank matrices

                 positive de   nite matrices / covariances 

suvrit sra (ml.mit.edu)

beyond stochastic gradients and convexity: part 2

27

nonid76 on manifolds

(zhang, reddi, sra, 2016)

g(   ) =

min
   2m

1
n

nxi=1

fi(   )

related work

    (udriste, 1994)                       batch methods; textbook
    (edelman, smith, arias, 1999)        classic paper; orthogonality constraints
    (absil, mahony, sepulchre, 2009)   textbook; convergence analysis
    (boumal, 2014)                           phd thesis, algos, theory, examples
    (mishra, 2014)                            phd thesis, algos, theory, examples 
    manopt                            excellent matlab toolbox
    (bonnabel, 2013)                     riemannnian sgd, asymptotic convg.
    and many more!

exploiting manifold structure yields speedups

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

28

example: gaussian mixture model

kxk=1

pmix(x) :=

   kpn (x;    k,   k)

likelihood
numerical challenge: positive de   nite constraint on   k

pmix(xi)

maxyi

riemannian

(new)

tx

sd
+

x

   x

[hosseini, sra, 2015]

em    
algo

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

cholesky
llt

29

careful use of manifold geometry helps!

k

2

5

10

em 
17s     29.28

202s     32.07
2159s     33.05

r-lbfgs
14s     29.28

117s     32.07
658s     33.06

riemannian-lbfgs (careful impl.)

github.com/utvisionlab/mixest

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

images dataset 
d=35, 
n=200,000

30

ations. right: 3 number of components. left: 7 number of components.

large-scale gaussian mixture models!

i

e
c
n
e
r
e
   
d
t
s
o
c
d
e
g
a
r
e
v
a

101

100

10 1

10 2

10 3

10 4

10 5

sgd (it=5)
sgd (it=20)
sgd (it=50)
em
lbfgs
cg

0

20

40

60

80

100

120

140

160

180

200

iterations

riemannian sgd for gmms

fig. 4: comparison of optimization methods on year predict data (d = 90, n = 515345).
y-axis: best cost minus current cost values. x-axis: number of function and gradient evalu-
ations. right: 3 number of components. left: 7 number of components.

(d=90, n=515345)

optimization for machine learning (mlss 2017)

suvrit sra (suvrit@mit.edu)

31

larger-scale optimization

g(   ) =

min
   2rd

1
n

nxi=1

fi(   )

suvrit sra (ml.mit.edu)

beyond stochastic gradients and convexity: part 2

32

simplest setting: using mini-batches

idea: use    b    stochastic gradients / ifo calls per iteration
useful in parallel and distributed settings
increases parallelism, reduces communication

sgd

   t+1 =    t  

rfj(   t)

   t

|it|xj2it

1/pb

for batch size b, sgd takes a factor          fewer iterations
(dekel, gilad-bachrach, shamir, xiao, 2012)
for batch size b, svrg takes a factor 1/b fewer iterations

theoretical linear speedup with parallelism

see also s2gd (convex case): (kone  n  , liu,richt  rik,tak    , 2015)

33

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

asynchronous stochastic algorithms

   t

sgd

   t+1 =    t  

|it|xj2it
    inherently sequential algorithm
    slow-downs in parallel/dist settings (synchronization)
classic results in asynchronous optimization: (bertsekas, tsitsiklis, 1987)

rfj(   t)

    asynchronous sgd implementation (hogwild!)   

avoids need to sync, operates in a    lock-free    manner

    key assumption: sparse data (often true in ml)

but

it is still sgd, thus has slow sublinear convergence

even for strongly convex functions

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

34

asynchronous algorithms: parallel

does variance reduction work with asynchrony?

yes!

asvrg (reddi, hefny, sra, poczos, smola, 2015)
asaga (leblond, pedregosa, lacoste-julien, 2016)
perturbed iterate analysis (mania et al, 2016)
    a few subtleties involved   
    some gaps between theory and practice
    more complex than async-sgd

bottomline: on sparse data, can get almost linear speedup
due to parallelism (   machines lead to ~    speedup)

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

35

asynchronous algorithms: distributed

common parameter
server architecture

(li,  andersen, smola, yu, 2014)

classic ref: (bertsekas, tsitsiklis, 1987)

d-sgd:

    workers compute (stochastic) gradients
    server computes parameter update
    widely used (centralized) design choice
    can have quite high communication cost

asynchrony via:  servers use delayed / stale gradients from workers
(nedic, bertsekas, borkar, 2000; agarwal, duchi 2011) and many others

(shamir, srebro 2014)     nice overview of distributed stochastic optimization
36

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

asynchronous algorithms: distributed

to reduce communication, following idea is useful:

data

d1

d2

workers

w1

w2

   

servers

s1

   

sk

dm

wm

worker nodes
solve compute
intensive
subproblems

servers perform
simple aggregation
(eg. full-gradients for
distributed svrg)

dane (shamir, srebro, zhang, 2013): distributed newton,    
view as having an svrg-like gradient correction

37

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

asynchronous algorithms: distributed

key point: use svrg (or related fast method)
to solve suitable subproblems at workers;  reduce    
#rounds of communication; (or just do d-svrg)

some related work

(lee, lin, ma, yang, 2015)

(ma, smith, jaggi, jordan,    
  richt  rik, tak    , 2015)

(shamir, 2016)

d-svrg, and accelerated version
for some special cases (applies in 
smaller condition number regime)

cocoa+: (updates m local dual variables
using m local data points; any local opt.
method can be used); higher runtime+comm.
d-svrg via cool application of without  
replacement svrg! regularized
least-squares problems only for now

several more: dane, disco, aide, etc.

38

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

summary

(cid:803) vr stochastic methods for nonconvex problems
(cid:803) surprises for proximal setup
(cid:803) nonconvex problems on manifolds
(cid:803) large-scale: parallel + sparse data
(cid:803) large-scale: distributed; svrg bene   ts, limitations

if there is a    nite-sum structure, can use vr ideas!

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

39

perspectives: did not cover these!

stochastic quasi-convex optim. (hazan, levy, shalev-shwartz, 2015)   
nonlinear eigenvalue-type problems (belkin, rademacher, voss, 2016)
frank-wolfe + svrg: (reddi, sra, poczos, smola,2016)
newton-type methods: (carmon, duchi, hinder, sidford, 2016); (agarwal, 
allen-zhu, bullins, hazan, ma, 2016);
many more, including robust optimization,
in   nite dimensional nonconvex problems
geodesic-convexity for global optimality
polynomial optimization
many more    it   s a rich    eld!

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

40

perspectives

impact of non-convexity on generalization
non-separable problems (e.g., maximize auc); saddle   
 point problems; robust optimization; heavy tails
convergence theory, local and global
lower-bounds for nonconvex    nite-sums
distributed algorithms (theory and implementations)
new applications (e.g., of riemannian optimization)
search for other more    tractable    nonconvex models
specialization to deep networks, software toolkits

suvrit sra (suvrit@mit.edu)

optimization for machine learning (mlss 2017)

41

