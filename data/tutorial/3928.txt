tutorial

october 28 - november 1, 2016

the twelfth conference of 

the association for machine translation 

in the americas 

http://www.amtaweb.org/amta-2016-in-austin-tx

november 1, 2016

advances in id4

rico sennrich
alexandra birch

marcin junczys-dowmunt

advances in id4

rico sennrich, alexandra birch, marcin junczys-dowmunt

institute for language, cognition and computation

university of edinburgh

november 1 2016

sennrich, birch, junczys-dowmunt

id4

1 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016machine translation

why we need mt

human translation industry:     666 million words / day
[pym et al., 2012]
mt indudstry: (cid:29) 100 billion words / day [turovsky, 2016]

demand for translation for outpaces what is humanly possible to produce
    we need fast, high-quality mt

sennrich, birch, junczys-dowmunt

id4

1 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

sennrich, birch, junczys-dowmunt

id4

2 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016edinburgh   s* wmt results over the years

)

e
d
   
n
e

30.0

20.0

20.3

19.4

20.9

20.2

22.0

20.8

18.9

10.0

0.0

(

3
1
0
2

t
s
e

t
s
w
e
n

n
o

u
e
l
b

24.7

21.5

22.1

2013

2014

2015

2016

phrase-based smt
syntax-based smt
neural mt

*id4 2015 from u. montr  al: https://sites.google.com/site/acl16id4/

sennrich, birch, junczys-dowmunt

id4

3 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016neural mt has already moved from academia into
production

sennrich, birch, junczys-dowmunt

id4

4 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016why neural mt?

single, end-to-end trained neural network replaces collection of weak
features
good generalization via continuous space representations
    modelling of dependencies over long distances

why now?

neural translation dates back to at least the 80s [allen, 1987]
large-scale neural mt is now possible thanks to

large amounts of training data
exponential growth in computational power (gpus!)
algorithmic advances

sennrich, birch, junczys-dowmunt

id4

5 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

1 neural networks     basics
2 recurrent neural networks and lstms
3 attention-based id4 model
4 where are we now? evaluation and chal-

lenges

evaluation results
comparing neural and phrase-based ma-
chine translation

5 recent research in neural machine transla-

tion

sennrich, birch, junczys-dowmunt

id4

6 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id75

parameters:    =(cid:20)   0

  1 (cid:21) model: h  (x) =   0 +   1x

sennrich, birch, junczys-dowmunt

id4

7 / 115

5101520population0510152025profitdataproceedings of amta 2016austin, oct 28 - nov 1, 2016id75

parameters:    =(cid:20)   0

  1 (cid:21) model: h  (x) =   0 +   1x

sennrich, birch, junczys-dowmunt

id4

7 / 115

5101520population0510152025profitdataproceedings of amta 2016austin, oct 28 - nov 1, 2016id75

parameters:    =(cid:20)   0

  1 (cid:21) model: h  (x) =   0 +   1x

sennrich, birch, junczys-dowmunt

id4

7 / 115

5101520population0510152025profity=   5.00+1.50xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016id75

parameters:    =(cid:20)   0

  1 (cid:21) model: h  (x) =   0 +   1x

sennrich, birch, junczys-dowmunt

id4

7 / 115

5101520population0510152025profity=   6.00+2.00xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016id75

parameters:    =(cid:20)   0

  1 (cid:21) model: h  (x) =   0 +   1x

sennrich, birch, junczys-dowmunt

id4

7 / 115

5101520population0510152025profity=   2.50+1.00xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016id75

parameters:    =(cid:20)   0

  1 (cid:21) model: h  (x) =   0 +   1x

sennrich, birch, junczys-dowmunt

id4

7 / 115

5101520population0510152025profity=   3.90+1.19xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

we try to    nd parameters          r2 such that the cost function j(  ) is
minimal:

j : r2     r

     = arg min

j(  )

     r2

sennrich, birch, junczys-dowmunt

id4

8 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

we try to    nd parameters          r2 such that the cost function j(  ) is
minimal:

j : r2     r

     = arg min

j(  )

mean square error:

     r2

j(  ) =

1
2m

m(cid:88)i=1(cid:16)h  (x(i))     y(i)(cid:17)2

sennrich, birch, junczys-dowmunt

id4

8 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

we try to    nd parameters          r2 such that the cost function j(  ) is
minimal:

j : r2     r

     = arg min

j(  )

mean square error:

     r2

j(  ) =

=

1
2m

1
2m

m(cid:88)i=1(cid:16)h  (x(i))     y(i)(cid:17)2
m(cid:88)i=1(cid:16)  0 +   1x(i)     y(i)(cid:17)2

sennrich, birch, junczys-dowmunt

id4

8 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

we try to    nd parameters          r2 such that the cost function j(  ) is
minimal:

j : r2     r

     = arg min

j(  )

mean square error:

     r2

j(  ) =

=

1
2m

1
2m

m(cid:88)i=1(cid:16)h  (x(i))     y(i)(cid:17)2
m(cid:88)i=1(cid:16)  0 +   1x(i)     y(i)(cid:17)2

where m is the number of data points in the training set.

sennrich, birch, junczys-dowmunt

id4

8 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

j((cid:20)    5.00

1.50 (cid:21)) = 6.1561

sennrich, birch, junczys-dowmunt

id4

9 / 115

5101520population0510152025profity=   5.00+1.50xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

j((cid:20)    6.00

2.00 (cid:21)) = 19.3401

sennrich, birch, junczys-dowmunt

id4

9 / 115

5101520population0510152025profity=   6.00+2.00xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

j((cid:20)    2.50

1.00 (cid:21)) = 4.7692

sennrich, birch, junczys-dowmunt

id4

9 / 115

5101520population0510152025profity=   2.50+1.00xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

j((cid:20)    3.90

1.19 (cid:21)) = 4.4775

sennrich, birch, junczys-dowmunt

id4

9 / 115

5101520population0510152025profity=   3.90+1.19xdataproceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

so, how do we    nd      = arg min

     r2

j(  ) computationally?

sennrich, birch, junczys-dowmunt

id4

10 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016the cost (or loss) function

so, how do we    nd      = arg min

     r2

j(  ) computationally?

sennrich, birch, junczys-dowmunt

id4

10 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 0,    = 0.01

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 1,    = 0.01

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 20,    = 0.01

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 200,    = 0.01

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 10000,    = 0.01

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 10000,    = 0.005

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 10000,    = 0.02

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

  j :=   j       

   
     j

j(  ) for each j

step 10,    = 0.025

sennrich, birch, junczys-dowmunt

id4

11 / 115

  01050510  1101234j(  )1000100200300400500600700800proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

how do we calculate

   
     j

j(  )?

sennrich, birch, junczys-dowmunt

id4

12 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

how do we calculate

   
     j

j(  )?

   
     j

j(  ) =

=

   
     j
1
m

1
2m

(h  (x(i))     y(i))2

m(cid:88)i=1
(h  (x(i))     y(i))x(i)

j

m(cid:88)i=1

sennrich, birch, junczys-dowmunt

id4

12 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016(stochastic) id119

how do we calculate

   
     j

j(  )?

   
     j

j(  ) =

   
     j

1
2m

(h  (x(i))     y(i))2

m(cid:88)i=1
m(cid:88)i=1
(h  (x(i))     y(i))   
   
(h  (x(i))     y(i))   
     j
(h  (x(i))     y(i))x(i)

j

= 2   
1
m

=

=

1
m

1
2m

m(cid:88)i=1
m(cid:88)i=1

(h  (x(i))     y(i))
  ix(i)
i

   
     j

n(cid:88)i=0

sennrich, birch, junczys-dowmunt

id4

12 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the update rule once again

for id75 we have the following model:

h  (x) =   0 +   1x

sennrich, birch, junczys-dowmunt

id4

13 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the update rule once again

for id75 we have the following model:

h  (x) =   0 +   1x

and we repeat until convergence (  0 and   1 should be updated
simultaneously):

  0

  1

:=   0       

:=   1       

1
m

1
m

m(cid:88)i=1
m(cid:88)i=1

(h  (x(i))     y(i))

(h  (x(i))     y(i))x(i)

sennrich, birch, junczys-dowmunt

id4

13 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model;

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model;
a suitable cost (or loss) function;

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model;
a suitable cost (or loss) function;
an optimization algorithm;

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model;
a suitable cost (or loss) function;
an optimization algorithm;
the gradient(s) of the cost function (if required by the optimization
algorithm).

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model; (here: a linear model)
a suitable cost (or loss) function; (here: mean square error)
an optimization algorithm; (here: a variant of sgd)
the gradient(s) of the cost function (if required by the optimization
algorithm).

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model; (here: a linear model)
a suitable cost (or loss) function; (here: mean square error)
an optimization algorithm; (here: a variant of sgd)
the gradient(s) of the cost function (if required by the optimization
algorithm).

side note: algorithms for    nding the minimum without the gradient

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model; (here: a linear model)
a suitable cost (or loss) function; (here: mean square error)
an optimization algorithm; (here: a variant of sgd)
the gradient(s) of the cost function (if required by the optimization
algorithm).

side note: algorithms for    nding the minimum without the gradient

for linear regession: the normal matrix (exact);

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model; (here: a linear model)
a suitable cost (or loss) function; (here: mean square error)
an optimization algorithm; (here: a variant of sgd)
the gradient(s) of the cost function (if required by the optimization
algorithm).

side note: algorithms for    nding the minimum without the gradient

for linear regession: the normal matrix (exact);
random search;

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016to summarize what we have learned

when approaching a machine learning problem, we need:

a suitable model; (here: a linear model)
a suitable cost (or loss) function; (here: mean square error)
an optimization algorithm; (here: a variant of sgd)
the gradient(s) of the cost function (if required by the optimization
algorithm).

side note: algorithms for    nding the minimum without the gradient

for linear regession: the normal matrix (exact);
random search;
id107;
...

sennrich, birch, junczys-dowmunt

id4

14 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016a neuron
features

  0

  1

  2

  n

1

x1

x2

      

xn

input layer

neuron

activation function

g(z)

output

input function

z =

  ixi

n(cid:88)i=0

layer 1

sennrich, birch, junczys-dowmunt

id4

15 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id75 and neural networks

features

  0

  1

  2

  n

1

x1

x2

      

xn

input layer

neuron

input function

activation function

  ixi

g(z) = z

output

z =

n(cid:88)i=0

layer 1

sennrich, birch, junczys-dowmunt

id4

16 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the logistic function (remember this one!)

sennrich, birch, junczys-dowmunt

id4

17 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016a more typical neuron (binary id28)

features

  0

  1

  2

  n

1

x1

x2

      

xn

input layer

input function

neuron

activation function

  ixi

g(z) =

1

1 + e   z

z =

n(cid:88)i=0

output

layer 1

sennrich, birch, junczys-dowmunt

id4

18 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016binary id28

model:

h  (x) = g((cid:80)n

i=0   ixi) =

1 + e   

(cid:80)n

1

i=0   ixi

sennrich, birch, junczys-dowmunt

id4

19 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016binary id28

model:

(cid:80)n

1

1 + e   

i=0   ixi) =

cost function (binary crossid178):

h  (x) = g((cid:80)n
[(cid:80)m
i=1 y(i) log h  (x(i)) + (1     y(i)) log(1     h  (x(i)))]

j(  ) =    

i=0   ixi

1
m

sennrich, birch, junczys-dowmunt

id4

19 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016binary id28

model:

(cid:80)n

1

i=0   ixi

1 + e   

i=0   ixi) =

cost function (binary crossid178):

h  (x) = g((cid:80)n
[(cid:80)m
i=1 y(i) log h  (x(i)) + (1     y(i)) log(1     h  (x(i)))]
m(cid:88)i=1
(h  (x(i))     y(i))x(i)

j(  ) =    
gradient:

   j(  )
     j

1
m

1
m

=

j

sennrich, birch, junczys-dowmunt

id4

19 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016multi-class id28 and neural networks

features

1

x1

x2

      

xn

input layer

  (0)
0
  (0)
1
  (0)
2

  (0)
n

  (2)
n

g(z) = softmax(z)

(cid:80)
(cid:80)
(cid:80)

layer 1

p (c = 0|  , x)

p (c = 1|  , x)

p (c = 2|  , x)

sennrich, birch, junczys-dowmunt

id4

20 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016multi-class id28

model: h  (x) = [p (k|x,   )]k=1,...,c = softmax(  x) where
   = (  (1), . . . ,   (c))

sennrich, birch, junczys-dowmunt

id4

21 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016multi-class id28

model: h  (x) = [p (k|x,   )]k=1,...,c = softmax(  x) where
   = (  (1), . . . ,   (c))
m(cid:80)m
cost function: j(  ) =     1
where   (x, y) =(cid:26) 1 if x = y

i=1(cid:80)c

0 otherwise

k=1   (y(i), k) log p (k|x(i),   )

sennrich, birch, junczys-dowmunt

id4

21 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016multi-class id28

model: h  (x) = [p (k|x,   )]k=1,...,c = softmax(  x) where
   = (  (1), . . . ,   (c))
i=1(cid:80)c
m(cid:80)m
cost function: j(  ) =     1
where   (x, y) =(cid:26) 1 if x = y
m(cid:80)m
i=1(  (y(i), k)     p (k|x(i),   )) x(i)

   j(  )
     j,k

0 otherwise

=     1

gradient:

k=1   (y(i), k) log p (k|x(i),   )

j

sennrich, birch, junczys-dowmunt

id4

21 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016multi-class id28

model: h  (x) = [p (k|x,   )]k=1,...,c = softmax(  x) where
   = (  (1), . . . ,   (c))
i=1(cid:80)c
m(cid:80)m
cost function: j(  ) =     1
where   (x, y) =(cid:26) 1 if x = y
m(cid:80)m
i=1(  (y(i), k)     p (k|x(i),   )) x(i)

may look complicated, but can be looked up!

   j(  )
     j,k

0 otherwise

=     1

gradient:

k=1   (y(i), k) log p (k|x(i),   )

j

sennrich, birch, junczys-dowmunt

id4

21 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016multi-class id28

model: h  (x) = [p (k|x,   )]k=1,...,c = softmax(  x) where
   = (  (1), . . . ,   (c))
i=1(cid:80)c
m(cid:80)m
cost function: j(  ) =     1
where   (x, y) =(cid:26) 1 if x = y
m(cid:80)m
i=1(  (y(i), k)     p (k|x(i),   )) x(i)

may look complicated, but can be looked up!

   j(  )
     j,k

0 otherwise

=     1

gradient:

k=1   (y(i), k) log p (k|x(i),   )

j

sennrich, birch, junczys-dowmunt

id4

21 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016multi-class id28 and neural networks

features

1

x1

x2

      

xn

input layer

  (0)
0
  (0)
1
  (0)
2

  (0)
n

  (2)
n

g(z) = softmax(z)

(cid:80)
(cid:80)
(cid:80)

layer 1

p (c = 0|  , x)

p (c = 1|  , x)

p (c = 2|  , x)

sennrich, birch, junczys-dowmunt

id4

22 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016deep learning: multi-layer neural networks

1

  (1)
1

1

  (2)
1

x1

x2

      

xn

  (1)
1,1
  (1)
2,1

  (1)
n,1

  (1)
n,3

a(1)
1

a(1)
2

a(1)
3

  (2)
1,1
  (2)
2,1
  (2)
3,1

  (2)
3,3

a(2)
1

a(2)
2

a(2)
3

1

  (3)
1

  (3)
1,1
  (3)
2,1
  (3)
3,1

a(3)
1

sennrich, birch, junczys-dowmunt

id4

23 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016why multiple-layers?

can a linear model separate these dots?

sennrich, birch, junczys-dowmunt

id4

24 / 115

1.00.50.00.51.01.00.50.00.51.0proceedings of amta 2016austin, oct 28 - nov 1, 2016why multiple-layers?

h(x) =   0 +   1x1 +   2x2

sennrich, birch, junczys-dowmunt

id4

24 / 115

1.00.50.00.51.01.00.50.00.51.0proceedings of amta 2016austin, oct 28 - nov 1, 2016why multiple-layers?

h(x) =   0 +   1x1 +   2x2 +   3x2

1 +   4x1x2 +   5x2
2

sennrich, birch, junczys-dowmunt

id4

24 / 115

1.00.50.00.51.01.00.50.00.51.0proceedings of amta 2016austin, oct 28 - nov 1, 2016why multiple-layers?

h(x) =   0 +   1x1 +   2x2 +   3x3 +   4x4 +   5x5 where x3 = x2

2, . . .

sennrich, birch, junczys-dowmunt

id4

24 / 115

1.00.50.00.51.01.00.50.00.51.0proceedings of amta 2016austin, oct 28 - nov 1, 2016why multiple-layers?

h(x) =   (  2  (  1x)) where |  1| = 3    3,|  2| = 3    1

sennrich, birch, junczys-dowmunt

id4

24 / 115

1.00.50.00.51.01.00.50.00.51.0proceedings of amta 2016austin, oct 28 - nov 1, 2016feed forward networks language models

source: philipp koehn, draft chapther on id4.

sennrich, birch, junczys-dowmunt

id4

25 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016feed forward networks language models

source: philipp koehn, draft chapther on id4.

sennrich, birch, junczys-dowmunt

id4

25 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id26     forward step

1

  (1)
1

1

  (2)
1

x1

x2

      

xn

  (1)
1,1
  (1)
1,2

  (1)
1,n

  (1)
3,n

a(1)
1

a(1)
2

a(1)
3

  (2)
1,1
  (2)
1,2
  (2)
1,3

  (2)
3,3

a(2)
1

a(2)
2

a(2)
3

1

  (3)
1

  (3)
1,1
  (3)
1,2
  (3)
1,3

a(3)
1

a(0) = x

z(1) =   (1)a(0) +   (1)
g(1)(x) = tanh(x)
a(1) = g(1)(z(1))

z(2) =   (2)a(1) +   (2)
g(2)(x) = tanh(x)
a(2) = g(2)(z(2))

z(3) =   (3)a(2) +   (3)
g(3)(x) = tanh(x)
a(3) = g(3)(z(3))

sennrich, birch, junczys-dowmunt

id4

26 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the four fundamental equations of id26

  l

  l

=    alj(  ) (cid:12) (gl)(cid:48)(zl)
= ((  l+1)t   l+1) (cid:12) (gl)(cid:48)(zl)

     lj(  ) =   l
     lj(  ) = al   1 (cid:12)   l

(bp 1)

(bp 2)

(bp 3)

(bp 4)

sennrich, birch, junczys-dowmunt

id4

27 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016the id26 algorithm

for one training example (x,y):

input: set the activations of the input layers a0 = x
forward step: for l = 1, . . . , l calculate

zl =   (l)al   1 +   l and al = gl(zl)

output error   l: calculate vector

  l =    alj(  ) (cid:12) (gl)(cid:48)(zl)

error id26: for l = l     1, l     2, . . . , 1 calculate

  l = ((  l+1)t   l+1) (cid:12) (gl)(cid:48)(zl)

gradients:

     lj(  ) = al   1 (cid:12)   l and      lj(  ) =   l

sennrich, birch, junczys-dowmunt

id4

28 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id26     backward step

1

  (1)
1

1

  (2)
1

x1

x2

      

xn

  (1)
1,1
  (1)
1,2

  (1)
1,n

  (1)
3,n

a(1)
1

a(1)
2

a(1)
3

  (2)
1,1
  (2)
1,2
  (2)
1,3

  (2)
3,3

a(2)
1

a(2)
2

a(2)
3

1

  (3)
1

  (3)
1,1
  (3)
1,2
  (3)
1,3

a(3)
1

  (3) = (a(3)     y) (cid:12) (1     tanh2(z(3)))

  (2) = (  (3))t   (3) (cid:12) (1     tanh2(z(2)))

  (1) = (  (2))t   (2) (cid:12) (1     tanh2(z(1)))

a(0) = x

z(1) =   (1)a(0) +   (1)
g(1)(x) = tanh(x)
a(1) = g(1)(z(1))

z(2) =   (2)a(1) +   (2)
g(2)(x) = tanh(x)
a(2) = g(2)(z(2))

z(3) =   (3)a(2) +   (3)
g(3)(x) = tanh(x)
a(3) = g(3)(z(3))

sennrich, birch, junczys-dowmunt

id4

29 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id26 and stochastic id119

one iteration:

for all parameters    = (  1, . . . ,   l) create zero-valued helper
matrices     = (   1, . . . ,    l) of the same size (   omitted for
simplicity).
for m examples in the batch, i = 1, . . . , m:

perform id26 for example (x(i), y(i)) and store the
gradients      j (i)(  )
    :=     +

1
m     j (i)(  )

update the weights:    :=             

sennrich, birch, junczys-dowmunt

id4

30 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016more complicated network architectures

textbook backprogagation is formulated in terms of layers, weights,
biases, activations, weighted inputs, ...
actual architectures can contain concatenation of bidirectional id56
states, ...
what   s the derivation of the "concatenation" operation?

sennrich, birch, junczys-dowmunt

id4

31 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016computing derivatives with reverse-mode autodiff

f (x1, x2) = sin(x1) + x1x2

sennrich, birch, junczys-dowmunt

id4

32 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016computing derivatives with reverse-mode autodiff

f (x1, x2) = sin(x1) + x1x2

   f
   x1
   f
   xx

= ?

= ?

sennrich, birch, junczys-dowmunt

id4

32 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016computation graphs to the rescue

f (x1, x2)

+

sin

   

x1

x2

sennrich, birch, junczys-dowmunt

id4

33 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016computation graphs to the rescue

f (x1, x2)

w5 = w3 + w4

+

w4 = sin(w1)

sin

   

w3 = w1    w2

w1 = x1

x1

x2

w2 = x2

sennrich, birch, junczys-dowmunt

id4

34 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016computation graphs to the rescue

f (x1, x2)

  f =   w5 = 1

w5 = w3 + w4

+

  w4 =   w5

   w5
   w4

=   w5

  w3 =   w5

   w5
   w3

=   w5

w4 = sin(w1)

sin

   

  wa

1 =   w4

   w4
   w1

=   w4    cos(w1)

  wb
1 =   w3    w2

w3 = w1    w2
  w2 =   w3

   w3
   w2

=   w3    w1

w1 = x1

x1

x2

w2 = x2

   f
   x1

1 +   wb
1

=   wa
= cos(x1) + x2

   f
   x2

=   w2 = x1

sennrich, birch, junczys-dowmunt

id4

35 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016computation graph for neural networks

a = softmax(x    w + b)
o = mean(sum(log(a) (cid:12) y))

sennrich, birch, junczys-dowmunt

id4

36 / 115

mean"cost"sum  loginput"y"softmax+   param"b"input"x"param"w"proceedings of amta 2016austin, oct 28 - nov 1, 2016computation graph for neural networks

a0 = x
a1 = relu(a0    w0 + b0)
a2 = relu(a1    w1 + b1)
a3 = a2    w2 + b2
o1 = softmax(a3)
o2 = mean(crossid178(a3, y))

sennrich, birch, junczys-dowmunt

id4

37 / 115

softmax"scores"+x-entmean"cost"input"y"   param"b2"reluparam"w2"+   param"b1"reluparam"w1"+   param"b0"input"x"param"w0"proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

1 neural networks     basics
2 recurrent neural networks and lstms
3 attention-based id4 model
4 where are we now? evaluation and chal-

lenges

evaluation results
comparing neural and phrase-based ma-
chine translation

5 recent research in neural machine transla-

tion

sennrich, birch, junczys-dowmunt

id4

38 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016recurrent neural networks (id56s)

source: http://colah.github.io/posts/2015-08-understanding-lstms/

sennrich, birch, junczys-dowmunt

id4

39 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016recurrent neural networks (id56s)

source: http://colah.github.io/posts/2015-08-understanding-lstms/

sennrich, birch, junczys-dowmunt

id4

39 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016recurrent neural networks (id56s)

ht = tanh(wh    ht   1 + wx    xt + b)

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

40 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016recurrent neural networks (id56s)

ht = tanh(wh    ht   1 + wx    xt + b)

ht = tanh(w    [ht   1, xt] + b)

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

40 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016recurrent neural networks language models

source: philipp koehn, draft chapther on id4.

sennrich, birch, junczys-dowmunt

id4

41 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016fun with id56s

andrej karpathy: http://karpathy.github.io/2015/05/21/id56-effectiveness/

character-level language models
python code generation
poetry generation
...

sennrich, birch, junczys-dowmunt

id4

42 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id56s and long distance dependencies

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

43 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id56s and long distance dependencies

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

43 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016long short-term memory (lstm)

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

44 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016long short-term memory (lstm)

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

44 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016long short-term memory (lstm)     step-by-step

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

45 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016long short-term memory (lstm)     step-by-step

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

45 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016long short-term memory (lstm)     step-by-step

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

45 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016long short-term memory (lstm)     step-by-step

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

45 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id149 (grus)

source: http://colah.github.io/posts/2015-08-understanding-lstms

sennrich, birch, junczys-dowmunt

id4

46 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id56s in encoder-decoder architectures

sennrich, birch, junczys-dowmunt

id4

47 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

1 neural networks     basics
2 recurrent neural networks and lstms
3 attention-based id4 model
4 where are we now? evaluation and chal-

lenges

evaluation results
comparing neural and phrase-based ma-
chine translation

5 recent research in neural machine transla-

tion

sennrich, birch, junczys-dowmunt

id4

48 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

sennrich, birch, junczys-dowmunt

id4

49 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016translation modelling

decomposition of translation problem (for id4)

a source sentence s of length m is a sequence x1, . . . , xm
a target sentence t of length n is a sequence y1, . . . , yn

t     = arg max

p(t|s)

t

p(t|s) = p(y1, . . . , yn|x1, . . . , xm)

=

p(yi|y0, . . . , yi   1, x1, . . . , xm)

n(cid:89)

i=1

sennrich, birch, junczys-dowmunt

id4

50 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016translation modelling

difference from language model

target-side language model:

p(t ) =

n(cid:89)i=1

p(yi|y0, . . . , yi   1)

translation model:

p(t|s) =

n(cid:89)i=1

p(yi|y0, . . . , yi   1, x1, . . . , xm)

we could just treat sentence pair as one long sequence, but:

we do not care about p(s) (s is given)
we may want different vocabulary, network architecture for source text

sennrich, birch, junczys-dowmunt

id4

51 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016translation modelling

difference from language model

target-side language model:

p(t ) =

n(cid:89)i=1

p(yi|y0, . . . , yi   1)

translation model:

p(t|s) =

n(cid:89)i=1

p(yi|y0, . . . , yi   1, x1, . . . , xm)

we could just treat sentence pair as one long sequence, but:

we do not care about p(s) (s is given)
we may want different vocabulary, network architecture for source text

sennrich, birch, junczys-dowmunt

id4

51 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016translating with id56s

encoder-decoder [sutskever et al., 2014, cho et al., 2014]

two id56s (lstm or gru):

encoder reads input and produces hidden state representations
decoder produces output, based on last encoder hidden state

encoder and decoder are learned jointly
    supervision signal from parallel text is backpropagated

sennrich, birch, junczys-dowmunt

id4

52 / 115

kyunghyun cho http://devblogs.nvidia.com/parallelforall/
introduction-neural-machine-translation-gpus-part-2/

proceedings of amta 2016austin, oct 28 - nov 1, 2016summary vector

last encoder hidden-state    summarizes    source sentence
with multilingual training, we can potentially learn
language-independent meaning representation

sennrich, birch, junczys-dowmunt

id4

53 / 115

[sutskever et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016summary vector as information bottleneck

can    xed-size vector represent meaning of arbitrarily long sentence?
empirically, quality decreases for long sentences
reversing source sentence brings some improvement
[sutskever et al., 2014]

sennrich, birch, junczys-dowmunt

id4

54 / 115

[sutskever et al., 2014]

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoder

encoder

goal: avoid bottleneck of summary vector
use bidirectional id56, and concatenate forward and backward states
    annotation vector hi
represent source sentence as vector of n annotations
    variable-length representation

sennrich, birch, junczys-dowmunt

id4

55 / 115

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoder

attention

problem: how to incorporate variable-length context into hidden state?
attention model computes context vector as weighted average of
annotations
weights are computed by feedforward neural network with softmax
activation

sennrich, birch, junczys-dowmunt

id4

56 / 115

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoder: math

simpli   cations of model by [bahdanau et al., 2015] (for illustration)

plain id56 instead of gru
simpler output layer
we do not show bias terms

notation

w , u, e, c, v are weight matrices (of different dimensionality)

e one-hot to embedding (e.g. 50000    512)
w embedding to hidden (e.g. 512    1024)
u hidden to hidden (e.g. 1024    1024)
c context (2x hidden) to hidden (e.g. 2048    1024)
vo hidden to one-hot (e.g. 1024    50000)

separate weight matrices for encoder and decoder (e.g. ex and ey)
input x of length tx; output y of length ty

sennrich, birch, junczys-dowmunt

id4

57 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoder: math

encoder

      h j =(cid:40)0,
      h j =(cid:40)0,

tanh(      w xexxj +       u xhj   1)
tanh(      w xexxj +       u xhj+1)

hj = (      h j,      h j)

, if j = 0
, if j > 0

, if j = tx + 1
, if j     tx

sennrich, birch, junczys-dowmunt

id4

58 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoder: math

decoder

si =(cid:40)tanh(ws      h i),

tanh(wyeyyi + uysi   1 + cci)
ti = tanh(uosi   1 + woeyyi   1 + coci)
yi = softmax(voti)

, if i = 0
, if i > 0

attention model

eij = v(cid:62)a tanh(wasi   1 + uahj)
  ij = softmax(eij)

ci =

tx(cid:88)j=1

  ijhj

sennrich, birch, junczys-dowmunt

id4

59 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attention model

attention model

side effect: we obtain alignment between source and target sentence
information can also    ow along recurrent connections, so there is no
guarantee that attention corresponds to alignment
applications:

visualisation
replace unknown words with back-off dictionary [jean et al., 2015]
...

sennrich, birch, junczys-dowmunt

id4

60 / 115

kyunghyun cho
http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/

proceedings of amta 2016austin, oct 28 - nov 1, 2016attention model

attention model also works with images:

sennrich, birch, junczys-dowmunt

id4

[cho et al., 2015]

61 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attention model

[cho et al., 2015]

sennrich, birch, junczys-dowmunt

id4

62 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016applications

score a translation
p(la, croissance,   conomique, s   est, ralentie, ces, derni  res, ann  es, . |
economic, growth, has, slowed, down, in, recent, year, .) = ?

generate the most probable translation of a source sentence
    decoding
y    = argmaxy p(y|economic, growth, has, slowed, down, in, recent, year, .)

sennrich, birch, junczys-dowmunt

id4

63 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016decoding

exact search

generate every possible sentence t in target language
compute score p(t|s) for each
pick best one

intractable: |vocab|n translations for output length n
    we need approximative search strategy

sennrich, birch, junczys-dowmunt

id4

64 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016decoding

approximative search/1

at each time step, compute id203 distribution p (yi|x, y<i)
select yi according to some heuristic:
sampling: sample from p (yi|x, y<i)
greedy search: pick argmaxy p(yi|x, y<i)

continue until we generate <eos>

ef   cient, but suboptimal

sennrich, birch, junczys-dowmunt

id4

65 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016decoding

approximative search/2: id125
maintain list of k hypotheses (beam)
at each time step, expand each hypothesis k: p(yk
select k hypotheses with highest total id203:

i |x, yk
<i)

(cid:89)i

p(yk

i |x, yk
<i)

relatively ef   cient
currently default search strategy in id4
small beam (k     10) offers good speed-quality trade-off

sennrich, birch, junczys-dowmunt

id4

66 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016ensembles

at each timestep, combine the id203 distribution of m different
ensemble components.
combine operator: typically average (log-)id203

log p (yi|x, y<i) = (cid:80)m

m=1 log pm(yi|x, y<i)

m

requirements:

same output vocabulary
same factorization of y

internal network architecture may be different
source representations may be different
(extreme example: ensemble-like model with different source
languages [junczys-dowmunt and grundkiewicz, 2016])

sennrich, birch, junczys-dowmunt

id4

67 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016ensembles

recent ensemble strategies in id4

ensemble of 8 independent training runs with different
hyperparameters/architectures [luong et al., 2015a]
ensemble of 8 independent training runs with different random
initializations [chung et al., 2016]
ensemble of 4 checkpoints of same training run
[sennrich et al., 2016a]
    probably less effective, but only requires one training run

23.7

24.8

31.6

33.1

28.1

28.2

24.3

26.0

30.1

31.4

36.2

37.5

33.3

33.9

26.9

28.0

40.0
30.0
20.0
10.0
0.0

u
e
l
b

en   cs en   de en   ro en   ru cs   en de   en ro   en ru   en

single model

ensemble

sennrich, birch, junczys-dowmunt

id4

[sennrich et al., 2016a]
68 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

1 neural networks     basics
2 recurrent neural networks and lstms
3 attention-based id4 model
4 where are we now? evaluation and chal-

lenges

evaluation results
comparing neural and phrase-based ma-
chine translation

5 recent research in neural machine transla-

tion

sennrich, birch, junczys-dowmunt

id4

69 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016state of neural mt

attentional encoder-decoder networks have become state of the art
on various mt tasks
your mileage may vary depending on

language pair and text type
amount of training data
type of training resources (monolingual?)
hyperparameters

sennrich, birch, junczys-dowmunt

id4

70 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoders (id4) are sota

system
uedin-id4
metamind
uedin-syntax
nyu-umontreal
online-b
kit/limsi
cambridge
online-a
promt-rule
kit
jhu-syntax
jhu-pbmt
uedin-pbmt
online-f
online-g

id7
34.2
32.3
30.6
30.8
29.4
29.1
30.6
29.9
23.4
29.0
26.6
28.3
28.4
19.3
23.8

of   cial rank

1
2
3
4

5-10
5-10
5-10
5-10
5-10
6-10
11-12
11-12
13-14
13-15
14-15

table: wmt16 results for en   de

system
uedin-id4
online-b
online-a
uedin-syntax
kit
uedin-pbmt
jhu-pbmt
online-g
jhu-syntax
online-f

id7
38.6
35.0
32.8
34.4
33.9
35.1
34.5
30.1
31.0
20.2

of   cial rank

1
2-5
2-5
2-5
2-6
5-7
6-7
8
9
10

table: wmt16 results for de   en

sennrich, birch, junczys-dowmunt

id4

71 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoders (id4) are sota

system
uedin-id4
metamind
uedin-syntax
nyu-umontreal
online-b
kit/limsi
cambridge
online-a
promt-rule
kit
jhu-syntax
jhu-pbmt
uedin-pbmt
online-f
online-g

id7
34.2
32.3
30.6
30.8
29.4
29.1
30.6
29.9
23.4
29.0
26.6
28.3
28.4
19.3
23.8

of   cial rank

1
2
3
4

5-10
5-10
5-10
5-10
5-10
6-10
11-12
11-12
13-14
13-15
14-15

system
uedin-id4
online-b
online-a
uedin-syntax
kit
uedin-pbmt
jhu-pbmt
online-g
jhu-syntax
online-f

id7
38.6
35.0
32.8
34.4
33.9
35.1
34.5
30.1
31.0
20.2

of   cial rank

1
2-5
2-5
2-5
2-6
5-7
6-7
8
9
10

table: wmt16 results for de   en

table: wmt16 results for en   de

pure id4

sennrich, birch, junczys-dowmunt

id4

71 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoders (id4) are sota

system
uedin-id4
metamind
uedin-syntax
nyu-umontreal
online-b
kit/limsi
cambridge
online-a
promt-rule
kit
jhu-syntax
jhu-pbmt
uedin-pbmt
online-f
online-g

id7
34.2
32.3
30.6
30.8
29.4
29.1
30.6
29.9
23.4
29.0
26.6
28.3
28.4
19.3
23.8

of   cial rank

1
2
3
4

5-10
5-10
5-10
5-10
5-10
6-10
11-12
11-12
13-14
13-15
14-15

table: wmt16 results for en   de

system
uedin-id4
online-b
online-a
uedin-syntax
kit
uedin-pbmt
jhu-pbmt
online-g
jhu-syntax
online-f

id7
38.6
35.0
32.8
34.4
33.9
35.1
34.5
30.1
31.0
20.2

of   cial rank

1
2-5
2-5
2-5
2-6
5-7
6-7
8
9
10

table: wmt16 results for de   en

pure id4
id4 component

sennrich, birch, junczys-dowmunt

id4

71 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoders (id4) are sota

uedin-id4
nyu-umontreal
jhu-pbmt
cu-chimera
cu-tamchyna
uedin-cu-syntax
online-b
online-a
cu-tectomt
cu-mergedtrees

25.8
23.6
23.6
21.0
20.8
20.9
22.7
19.5
14.7
8.2

1
2
3
4-5
4-5
6-7
6-7
15
16
18

table: wmt16 results for en   cs

online-b
uedin-id4
uedin-pbmt
uedin-syntax
online-a
jhu-pbmt
limsi

39.2
33.9
35.2
33.6
30.8
32.2
31.0

1-2
1-2
3
4-5
4-6
5-7
6-7

uedin-id4
jhu-pbmt
online-b
pjatk
online-a
cu-mergedtrees

31.4
30.4
28.6
28.3
25.7
13.3

1
2
3

8-10
11
12

table: wmt16 results for cs   en

uedin-id4
qt21-himl-syscomb
kit
uedin-pbmt
online-b
uedin-lmu-hiero
rwth-syscomb
limsi
lmu-cuni
jhu-pbmt
usfd-rescoring
online-a

28.1
28.9
25.8
26.8
25.4
25.9
27.1
23.9
24.3
23.5
23.1
19.2

1-2
1-2
3-7
3-7
3-7
3-7
3-7
8-10
8-10
8-11
10-12
11-12

table: wmt16 results for ro   en

table: wmt16 results for en   ro

sennrich, birch, junczys-dowmunt

id4

71 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016attentional encoder-decoders (id4) are sota

promt-rule
amu-uedin
online-b
uedin-id4
online-g
nyu-umontreal
jhu-pbmt
limsi
online-a
afrl-mitll-phr
afrl-mitll-verb
online-f

22.3
25.3
23.8
26.0
26.2
23.1
24.0
23.6
20.2
23.5
20.9
8.6

1
2-4
2-5
2-5
3-5
6
7-8
7-10
8-10
9-10
11
12

table: wmt16 results for en   ru

amu-uedin
online-g
nrc
online-b
uedin-id4
online-a
afrl-mitll-phr
afrl-mitll-contrast
promt-rule
online-f

29.1
28.7
29.1
28.1
28.0
25.7
27.6
27.0
20.4
13.5

1-2
1-3
2-4
3-5
4-5
6-7
6-7
8-9
8-9
10

uedin-pbmt
online-g
online-b
uh-opus
promt-smt
uh-factored
uedin-syntax
online-a
jhu-pbmt

23.4
20.6
23.6
23.1
20.3
19.3
20.4
19.0
19.1

1-4
1-4
1-4
1-4
5
6-7
6-7
8
9

table: wmt16 results for fi   en

online-g

abumatra-id4

online-b

abumatran-combo

uh-opus

nyu-umontreal
abumatran-pbsmt

online-a
jhu-pbmt

uh-factored

aalto

jhu-hltcoe

uut

15.4
17.2
14.4
17.4
16.3
15.1
14.6
13.0
13.8
12.8
11.6
11.9
11.6

1-3
1-4
1-4
3-5
4-5
6-8
6-8
6-8
9-10
9-12
10-13
10-13
11-13

table: wmt16 results for ru   en

table: wmt16 results for en   fi

sennrich, birch, junczys-dowmunt

id4

71 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

1 neural networks     basics
2 recurrent neural networks and lstms
3 attention-based id4 model
4 where are we now? evaluation and chal-

lenges

evaluation results
comparing neural and phrase-based ma-
chine translation

5 recent research in neural machine transla-

tion

sennrich, birch, junczys-dowmunt

id4

72 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

ambiguity
words are often polysemous, with different translations for different
meanings

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

sennrich, birch, junczys-dowmunt

id4

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0

73 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

ambiguity
words are often polysemous, with different translations for different
meanings

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

racket

sennrich, birch, junczys-dowmunt

id4

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0

73 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

ambiguity
words are often polysemous, with different translations for different
meanings

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

racket

attacker

sennrich, birch, junczys-dowmunt

id4

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0

73 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

ambiguity
words are often polysemous, with different translations for different
meanings

system
source
reference
uedin-id4
uedin-pbsmt

sentence
dort wurde er von dem schl  ger und einer weiteren m  nnlichen person erneut angegriffen.
there he was attacked again by his original attacker and another male.
there he was attacked again by the racket and another male person.
there, he was at the club and another male person attacked again.

schl  ger

racket

attacker

club

sennrich, birch, junczys-dowmunt

id4

racket https://www.   ickr.com/photos/128067141@n07/15157111178 / cc by 2.0

73 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

word order
there are systematic word order differences between languages. we need
to generate words in the correct order.

system
source
reference
uedin-id4
uedin-pbsmt

sentence
unsere digitalen leben haben die notwendigkeit, stark, lebenslustig und erfolgreich zu erscheinen, verdoppelt [...]
our digital lives have doubled the need to appear strong, fun-loving and successful [...]
our digital lives have doubled the need to appear strong, lifelike and successful [...]
our digital lives are lively, strong, and to be successful, doubled [...]

sennrich, birch, junczys-dowmunt

id4

74 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

grammatical marking system
grammatical distinctions can be marked in different ways, for instance
through word order (english), or in   ection (german). the translator needs
to produce the appropriate marking.

english
german

... because the dog chased the man.
... weil der hund den mann jagte.

sennrich, birch, junczys-dowmunt

id4

75 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

multiword expressions
the meaning of non-compositional expressions is lost in a word-to-word
translation
system
source
reference

sentence
he bends over backwards for the team, ignoring any pain.
er zerrei  t sich f  r die mannschaft, geht   ber schmerzen dr  ber.
(lit: he tears himself apart for the team)
er beugt sich r  ckw  rts f  r die mannschaft, ignoriert jeden schmerz.
(lit: he bends backwards for the team)
er macht alles f  r das team, den schmerz zu ignorieren.
(lit: he does everything for the team)

uedin-id4

uedin-pbsmt

sennrich, birch, junczys-dowmunt

id4

76 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

subcategorization
words only allow for speci   c categories of syntactic arguments, that often
differ between languages.

english
german
english
german

he remembers his medical appointment.
er erinnert sich an seinen arzttermin.
*he remembers himself to his medical appointment.
*er erinnert seinen arzttermin.

agreement
in   ected forms may need to agree over long distances to satisfy
grammaticality.

english
french

they can not be found
elles ne peuvent pas   tre trouv  es

sennrich, birch, junczys-dowmunt

id4

77 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

morphological complexity
translator may need to analyze/generate morphologically complex words
that were not seen before.

german abwasserbehandlungsanlage
english
french

waste water treatment plant
station d     puration des eaux r  siduaires

system
source
reference
uedin-id4
uedin-pbsmt

sentence
titelverteidiger ist drittligaabsteiger spvgg unterhaching.
the defending champions are spvgg unterhaching, who have been relegated to the third league.
defending champion is third-round pick spvgg underhaching.
title defender drittligaabsteiger week 2.

sennrich, birch, junczys-dowmunt

id4

78 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

open vocabulary
languages have an open vocabulary, and we need to learn translations for
words that we have only seen rarely (or never)

system
source
reference
uedin-id4
uedin-pbsmt

sentence
titelverteidiger ist drittligaabsteiger spvgg unterhaching.
the defending champions are spvgg unterhaching, who have been relegated to the third league.
defending champion is third-round pick spvgg underhaching.
title defender drittligaabsteiger week 2.

sennrich, birch, junczys-dowmunt

id4

79 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

discontinuous structures
a word (sequence) can map to a discontinuous structure in another
language.

english
french

i do not know
je ne sais pas

system
source
reference
uedin-id4
uedin-pbsmt

sentence
ein jahr sp  ter machten die fed-repr  sentanten diese k  rzungen r  ckg  ngig.
a year later, fed of   cials reversed those cuts.
a year later, fedex of   cials reversed those cuts.
a year later, the fed representatives made these cuts.

sennrich, birch, junczys-dowmunt

id4

80 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

discourse
the translation of referential expressions depends on discourse context,
which sentence-level translators have no access to.

english
french
french

i made a decision.
j   ai pris une d  cision. respectez-la s   il vous pla  t.
respectez-le s   il vous pla  t.
j   ai fait un choix.

please respect it.

sennrich, birch, junczys-dowmunt

id4

81 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016interlude: why is (machine) translation hard?

assorted other dif   culties

underspeci   cation
ellipsis
lexical gaps
language change
language variation (dialects, genres, domains)
ill-formed input

sennrich, birch, junczys-dowmunt

id4

82 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016comparison between phrase-based and neural mt

human analysis of id4 (reranking) [neubig et al., 2015]

id4 is more grammatical

word order
insertion/deletion of function words
morphological agreement

minor degradation in lexical choice?

sennrich, birch, junczys-dowmunt

id4

83 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, birch, junczys-dowmunt

id4

84 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, birch, junczys-dowmunt

id4

84 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, birch, junczys-dowmunt

id4

84 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016comparison between phrase-based and neural mt

analysis of iwslt 2015 results [bentivogli et al., 2016]

human-targeted translation error rate (hter) based on automatic
translation and human post-edit
4 error types: substitution, insertion, deletion, shift

system

pbsmt [ha et al., 2015]
id4 [luong and manning, 2015]

hter (no shift)

hter

word
28.3
21.7

lemma %    (shift only)
23.2
18.7

-18.0
-13.7

3.5
1.5

word-level is closer to lemma-level performance: better at
in   ection/agreement
improvement on lemma-level: better lexical choice
fewer shift errors: better word order

sennrich, birch, junczys-dowmunt

id4

84 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016fluency

100

80

60

y
c
a
u
q
e
d
a

75.4

75.8

72.2

70.8

73.9

71.2

72.8

71.1

cs   en de   en ro   en ru   en

online-b uedin-id4

figure: wmt16 direct assessment results

sennrich, birch, junczys-dowmunt

id4

85 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016fluency

+1%

100

80

60

y
c
a
u
q
e
d
a

75.4

75.8

72.2

70.8

73.9

71.2

72.8

71.1

cs   en de   en ro   en ru   en

online-b uedin-id4

figure: wmt16 direct assessment results

sennrich, birch, junczys-dowmunt

id4

85 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016fluency

+1%

75.4

75.8

72.2

70.8

73.9

71.2

72.8

71.1

100

80

60

y
c
a
u
q
e
d
a

100

80

64.6

60

y
c
n
e
u
f

l

78.7

77.5

68.4

71.9

66.7

67.8

74.3

cs   en de   en ro   en ru   en

online-b uedin-id4

cs   en de   en ro   en ru   en

online-b uedin-id4

figure: wmt16 direct assessment results

sennrich, birch, junczys-dowmunt

id4

85 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016fluency

+1%

+13%

100

80

60

y
c
a
u
q
e
d
a

75.4

75.8

72.2

70.8

73.9

71.2

72.8

71.1

100

80

64.6

60

y
c
n
e
u
f

l

78.7

77.5

68.4

71.9

66.7

67.8

74.3

cs   en de   en ro   en ru   en

online-b uedin-id4

cs   en de   en ro   en ru   en

online-b uedin-id4

figure: wmt16 direct assessment results

sennrich, birch, junczys-dowmunt

id4

85 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016why is neural mt output more grammatical?

phrase-based smt

log-linear combination of many    weak    features
data sparsenesss triggers back-off to smaller units
strong independence assumptions

neural mt

end-to-end trained model
generalization via continuous space representation
output conditioned on full source text and target history

sennrich, birch, junczys-dowmunt

id4

86 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016id4

1 neural networks     basics
2 recurrent neural networks and lstms
3 attention-based id4 model
4 where are we now? evaluation and chal-

lenges

evaluation results
comparing neural and phrase-based ma-
chine translation

5 recent research in neural machine transla-

tion

sennrich, birch, junczys-dowmunt

id4

87 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016ef   ciency

speed bottlenecks

id127
    use of highly parallel hardware (gpus)
size of output layer scales with vocabulary size. solutions:

lms: hierarchical softmax; noise-contrastive estimation;
self-id172
id4: approximate softmax through subset of vocabulary
[jean et al., 2015, mi et al., 2016, l   hostis et al., 2016]

id4 training vs. decoding (on fast gpu)

training: slow (1-3 weeks)
decoding: fast (100 000   500 000 sentences / day)a

awith nvidia titan x and amuid4 (https://github.com/emjotde/amuid4)

sennrich, birch, junczys-dowmunt

id4

88 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016ef   ciency

aggressive batching during decoding

compute all pre   xes in beam in single batch
compute multiple sentences in single batch

8-bit id136 [wu et al., 2016]
knowledge distillation: student network mimics teacher
[kim and rush, 2016]

sennrich, birch, junczys-dowmunt

id4

89 / 115

figure1:overviewofthedifferentknowledgedistillationapproaches.inword-levelknowledgedistillation(left)cross-id178isminimizedbetweenthestudent/teacherdistributions(yellow)foreachwordintheactualtargetsequence(ecd),aswellasbetweenthestudentdistributionandthedegeneratedatadistribution,whichhasallofitsprobabilitiymassononeword(black).insequence-levelknowledgedistillation(center)thestudentnetworkistrainedontheoutputfrombeamsearchoftheteachernetworkthathadthehighestscore(acf).insequence-levelinterpolation(right)thestudentistrainedontheoutputfrombeamsearchoftheteachernetworkthathadthehighestsimwiththetargetsequence(ece).thisobjectivecanbeseenasminimizingthecross-id178betweenthedegeneratedatadistribution(whichhasallofitsid203massononeclass)andthemodeldistributionp(y|x;  ).inknowledgedistillation,weassumeaccesstoalearnedteacherdistributionq(y|x;  t),possiblytrainedoverthesamedataset.insteadofminimiz-ingcross-id178withtheobserveddata,weinsteadminimizethecross-id178withtheteacher   sprob-abilitydistribution,lkd(  ;  t)=   |v|xk=1q(y=k|x;  t)  logp(y=k|x;  )where  tparameterizestheteacherdistributionandremains   xed.notethecross-id178setupisiden-tical,butthetargetdistributionisnolongerasparsedistribution.4trainingonq(y|x;  t)isattractivesinceitgivesmoreinformationaboutotherclassesforagivendatapoint(e.g.similaritybetweenclasses)andhaslessvarianceingradients(hintonetal.,2015).4insomecasestheid178oftheteacher/studentdistribu-tionisincreasedbyannealingitwithatemperatureterm  >1  p(y|x)   p(y|x)1  aftertesting     {1,1.5,2}wefoundthat  =1workedbest.sincethisnewobjectivehasnodirecttermforthetrainingdata,itiscommonpracticetointerpolatebetweenthetwolosses,l(  ;  t)=(1     )lnll(  )+  lkd(  ;  t)where  ismixtureparametercombiningtheone-hotdistributionandtheteacherdistribution.3knowledgedistillationforid4thelargesizesofneuralmachinetranslationsys-temsmakethemanidealcandidateforknowledgedistillationapproaches.inthissectionweexplorethreedifferentwaysthistechniquecanbeappliedtoid4.3.1word-levelknowledgedistillationid4systemsaretraineddirectlytominimizewordnll,lword-nll,ateachposition.thereforeifwehaveateachermodel,standardknowledgedistil-lationformulti-classcross-id178canbeapplied.wede   nethisdistillationforasentenceas,lword-kd=   jxj=1|v|xk=1q(tj=k|s,t<j)  logp(tj=k|s,t<j)wherevisthetargetvocabularyset.thestudentcanfurtherbetrainedtooptimizethemixtureofproceedings of amta 2016austin, oct 28 - nov 1, 2016open-vocabulary translation

why is vocabulary size a problem?

size of one-hot input/output vector is linear to vocabulary size
large vocabularies are space inef   cient
large output vocabularies are time inef   cient
typical network vocabulary size: 30 000   100 000

what about out-of-vocabulary words?

training set vocabulary typically larger than network vocabulary
(1 million words or more)
at translation time, we regularly encounter novel words:

names: barack obama
morph. complex words: hand|gep  ck|geb  hr (   carry-on bag fee   )
numbers, urls etc.

sennrich, birch, junczys-dowmunt

id4

90 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016open-vocabulary translation

solutions

copy unknown words, or translate with back-off dictionary
[jean et al., 2015, luong et al., 2015b, gulcehre et al., 2016]
    works for names (if alphabet is shared), and 1-to-1 aligned words
use subword units (characters or others) for input/output vocabulary
    model can learn translation of seen words on subword level
    model can translate unseen words if translation is transparent
active research area [sennrich et al., 2016c,
luong and manning, 2016, chung et al., 2016, ling et al., 2015,
costa-juss   and fonollosa, 2016, zhao and zhang, 2016,
lee et al., 2016]

sennrich, birch, junczys-dowmunt

id4

91 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016core idea: transparent translations

transparent translations

some translations are semantically/phonologically transparent
morphologically complex words (e.g. compounds):

solar system (english)
sonnen|system (german)
nap|rendszer (hungarian)

named entities:

obama(english; german)
           (russian)
    (o-ba-ma) (japanese)

cognates and loanwords:
claustrophobia(english)
klaustrophobie(german)
                           (russian)

sennrich, birch, junczys-dowmunt

id4

92 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016subword id4

flat representation [sennrich et al., 2016c, chung et al., 2016]

sentence is a sequence of subword units

hierarchical representation
[ling et al., 2015, luong and manning, 2016]

sentence is a sequence of words
words are a sequence of subword units

open question: should attention be on level of words or subwords?

sennrich, birch, junczys-dowmunt

id4

93 / 115

underreviewasaconferencepaperaticlr2016variables,thesourceattentionaandthetargetcontextlfp   1,theid203ofagivenwordtypetpbeingthenexttranslatedwordtpisgivenby:p(tp|a,lfp   1)=exp(estpaa+stpllfp   1)pj   [0,t]exp(esjaa+sjllfp   1),wheresaandslaretheparametersthatmaptheconditionedvectorsintoascoreforeachwordtypeinthetargetlanguagevocabularyt.theparametersforaspeci   cwordtypejareobtainedassjaandsjl,respectively.then,scoresarenormalizedintoaid203.2.2character-basedmachinetranslationwenowpresentouradaptationoftheword-basedneuralnetworkmodeltooperateovercharactersequencesratherthanwordsequences.however,unlikepreviousapproachesthatattempttodiscardthenotionofwordscompletely(vilaretal.,2007;neubigetal.,2013),weproposeanhierarhicalarchitecture,whichreplacesthewordlookuptables(steps1and3)andthewordsoftmax(step6)withcharacter-basedalternatives,whichcomposethenotionofwordsfromindividualcharacters.theadvantageofthisapproachisthatwebene   tfrompropertiesofcharacter-basedapproaches(e.g.compactnessandorthographicsensitivity),butcanalsoeasilybeincorporatedintoanyword-basedneuralapproaches.character-basedwordrepresentationtheworkin(lingetal.,2015;ballesterosetal.,2015)proposesacompositionalmodelforlearningwordvectorsfromcharacters.similartowordlookuptables,awordstringsjismappedintoads,w-dimensionalvector,butratherthanallocatingparam-etersforeachindividualwordtype,thewordvectorsjiscomposedbyaseriesoftransformationusingitscharactersequencesj,0,...,sj,x.* c2w compositional modelblstmwhereword vector for "where"figure2:illustrationofthec2wmodel.squareboxesrepresentvectorsofneuronactivations.theillustrationofthemodelisshownin2.essentially,themodelbuildsarepresentationofthewordusingcharacters,byreadingcharactersfroid113fttorightandvice-versa.moreformally,givenanin-putwordsj=sj,0,...,sj,x,themodelprojectseachcharacterintoacontinuousds,c-dimensionalvectorssj,0,...,sj,xusingacharacterlookuptable.then,itbuildsaforwardlstmstatese-quencehf0,...,hfkbyreadingthecharactervectorssj,0,...,sj,x.another,backwardlstmreadsthecharactervectorsinthereverseordergeneratingthebackwardstateshbk,...,hb0.finally,the4proceedings of amta 2016austin, oct 28 - nov 1, 2016subword id4

choice of subword unit

characters: small vocabulary, long sequences
morphemes (?): hard to control vocabulary size
hybrid choice: shortlist of words, subwords for rare words
variable-length character id165s: byte-pair encoding (bpe)

open research question which subid40 is best choice in
terms of ef   ciency and effectiveness.

sennrich, birch, junczys-dowmunt

id4

94 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding [gage, 1994]

algorithm
iteratively replace most frequent byte pair in sequence with unused byte

aaabdaaabac

sennrich, birch, junczys-dowmunt

id4

95 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding [gage, 1994]

algorithm
iteratively replace most frequent byte pair in sequence with unused byte

aaabdaaabac
zabdzabac

z=aa

sennrich, birch, junczys-dowmunt

id4

95 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding [gage, 1994]

algorithm
iteratively replace most frequent byte pair in sequence with unused byte

aaabdaaabac
zabdzabac
zydzyac

z=aa
y=ab

sennrich, birch, junczys-dowmunt

id4

95 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding [gage, 1994]

algorithm
iteratively replace most frequent byte pair in sequence with unused byte

aaabdaaabac
zabdzabac
zydzyac
xdxac

z=aa
y=ab
x=zy

sennrich, birch, junczys-dowmunt

id4

95 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

bottom-up character merging

iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: character vocabulary + one symbol per merge

freq

symbol pair

new symbol

word
   l o w </w>   
   l o w e r </w>   
   n e w e s t </w>   
   w i d e s t </w>   

freq
5
2
6
3

sennrich, birch, junczys-dowmunt

id4

96 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

bottom-up character merging

iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: character vocabulary + one symbol per merge

word
   l o w </w>   
   l o w e r </w>   
   n e w es t </w>   
   w i d es t </w>   

freq
5
2
6
3

freq
9

symbol pair
(   e   ,    s   )

new symbol

       es   

sennrich, birch, junczys-dowmunt

id4

96 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

bottom-up character merging

iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: character vocabulary + one symbol per merge

word
   l o w </w>   
   l o w e r </w>   
   n e w est </w>   
   w i d est </w>   

freq
5
2
6
3

freq
9
9

symbol pair
(   e   ,    s   )
(   es   ,    t   )

new symbol

       es   
       est   

sennrich, birch, junczys-dowmunt

id4

96 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

bottom-up character merging

iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: character vocabulary + one symbol per merge

word
   l o w </w>   
   l o w e r </w>   
   n e w est</w>   
   w i d est</w>   

freq
5
2
6
3

freq
9
9
9

new symbol

symbol pair
(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   

       es   
       est   

sennrich, birch, junczys-dowmunt

id4

96 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

bottom-up character merging

iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: character vocabulary + one symbol per merge

word
   lo w </w>   
   lo w e r </w>   
   n e w est</w>   
   w i d est</w>   

freq
5
2
6
3

freq
9
9
9
7

new symbol

symbol pair
(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )

       es   
       est   
       lo   

sennrich, birch, junczys-dowmunt

id4

96 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

bottom-up character merging

iteratively replace most frequent pair of symbols (   a   ,   b   ) with    ab   
apply on dictionary, not on full text (for ef   ciency)
output vocabulary: character vocabulary + one symbol per merge

word
   low </w>   
   low e r </w>   
   n e w est</w>   
   w i d est</w>   

freq
5
2
6
3

freq
9
9
9
7
7
...

new symbol

symbol pair
(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )
(   lo   ,    w   )

       es   
       est   
       lo   
       low   

sennrich, birch, junczys-dowmunt

id4

96 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

why bpe?

don   t waste time on frequent character sequences
    trade-off between text length and vocabulary sizes
open-vocabulary:
learned operations can be applied to unknown words
alternative view: character-level model on compressed text

   l o w e s t </w>   

(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )
(   lo   ,    w   )

       es   
       est   
       lo   
       low   

sennrich, birch, junczys-dowmunt

id4

97 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

why bpe?

don   t waste time on frequent character sequences
    trade-off between text length and vocabulary sizes
open-vocabulary:
learned operations can be applied to unknown words
alternative view: character-level model on compressed text

   l o w es t </w>   

(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )
(   lo   ,    w   )

       es   
       est   
       lo   
       low   

sennrich, birch, junczys-dowmunt

id4

97 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

why bpe?

don   t waste time on frequent character sequences
    trade-off between text length and vocabulary sizes
open-vocabulary:
learned operations can be applied to unknown words
alternative view: character-level model on compressed text

   l o w est </w>   

(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )
(   lo   ,    w   )

       es   
       est   
       lo   
       low   

sennrich, birch, junczys-dowmunt

id4

97 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

why bpe?

don   t waste time on frequent character sequences
    trade-off between text length and vocabulary sizes
open-vocabulary:
learned operations can be applied to unknown words
alternative view: character-level model on compressed text

   l o w est</w>   

(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )
(   lo   ,    w   )

       es   
       est   
       lo   
       low   

sennrich, birch, junczys-dowmunt

id4

97 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

why bpe?

don   t waste time on frequent character sequences
    trade-off between text length and vocabulary sizes
open-vocabulary:
learned operations can be applied to unknown words
alternative view: character-level model on compressed text

   lo w est</w>   

(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )
(   lo   ,    w   )

       es   
       est   
       lo   
       low   

sennrich, birch, junczys-dowmunt

id4

97 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016byte pair encoding for id40

why bpe?

don   t waste time on frequent character sequences
    trade-off between text length and vocabulary sizes
open-vocabulary:
learned operations can be applied to unknown words
alternative view: character-level model on compressed text

   low est</w>   

(   e   ,    s   )
(   es   ,    t   )
(   est   ,    </w>   )        est</w>   
(   l   ,    o   )
(   lo   ,    w   )

       es   
       est   
       lo   
       low   

sennrich, birch, junczys-dowmunt

id4

97 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016fully character-level id4 [lee et al., 2016]

character-to-character model requires no language-speci   c
segmentation
drawback: id56 over characters is slow (especially attention!)
(shorter) segment sequences are obtained from characters via
convolution and max-pooling layers

sennrich, birch, junczys-dowmunt

id4

98 / 115

__thesecondperson__convolution	+	relumax	pooling	with	stride	5highway	networkbidirectional	grucharacterembeddings   #  %&	   ()  (%&+,-#)	   /  %&	   /  (%&0   )	   /  (%&0   )	segment	embeddingsfigure1:encoderarchitectureschematics.underscoredenotespadding.adottedverticallinedelimitseachsegment.3.3challengessentencesareonaverage6(de,csandru)to8(fi)timeslongerwhenrepresentedincharacters.thisposesthreemajorchallengestoachievingfullycharacter-leveltranslation.(1)training/decodinglatencyforthedecoder,al-thoughthesequencetobegeneratedismuchlonger,eachcharacter-levelsoftmaxoperationcostsconsid-erablylesscomparedtoaword-orsubword-levelsoftmax.chungetal.(2016)reportthatcharacter-leveldecodingisonly14%slowerthansubword-leveldecoding.ontheotherhand,computationalcomplexityoftheattentionmechanismgrowsquadraticallywithrespecttothesentencelength,asitneedstoattendtoeverysourcetokenforeverytargettoken.thismakesanaivecharacter-levelapproach,suchasin(luongandmanning,2016),computationallyprohibitive.consequently,reducingthelengthofthesourcesequenceiskeytoensuringreasonablespeedinbothtraininganddecoding.(2)mappingcharactersequencetocontinuousrepresentationthearbitraryrelationshipbetweentheorthographyofawordanditsmeaningisawell-knownprobleminlinguistics(desaussure,1916).buildingacharacter-levelencoderisarguablyamoredif   cultproblem,astheencoderneedstolearnahighlynon-linearfunctionfromalongsequenceofcharactersymbolstoameaningrepresentation.(3)longrangedependenciesincharactersacharacter-levelencoderneedstomodeldependen-ciesoverlongertimespansthanaword-levelen-coderdoes.4fullycharacter-levelid44.1encoderwedesignanencoderthataddressesallthechal-lengesdiscussedabovebyusingconvolutionalandpoolinglayersaggressivelytoboth(1)drasticallyshortentheinputsentenceand(2)ef   cientlycapturelocalregularities.inspiredbythecharacter-levellanguagemodelfrom(kimetal.,2015),ourencoder   rstreducesthesourcesentencelengthwithaseriesofconvolutional,poolingandhighwaylayers.theshorterrepresentation,insteadofthefullcharactersequence,ispassedthroughabidirectionalgruto(3)helpitresolvelongtermdependencies.weillustratetheproposedencoderinfigure1anddiscusseachlayerindetailbelow.embeddingwemapthesourcesentence(x1,...,xtx)   r1  txtoasequenceofcharacterembeddingsx=(c(x1),...,c(xtx))   rdc  txwherecisthecharacterembeddinglookuptable:c   rdc  |c|.convolutionone-dimensionalconvolutionopera-proceedings of amta 2016austin, oct 28 - nov 1, 2016architecture variants

an incomplete selection

different encoder architectures:

convolution network
[kalchbrenner and blunsom, 2013, kalchbrenner et al., 2016]
treelstm [eriguchi et al., 2016]

modi   cations to attention mechanism
[luong et al., 2015a, feng et al., 2016, zhang et al., 2016]
deeper networks [zhou et al., 2016, wu et al., 2016]
coverage model [mi et al., 2016, tu et al., 2016b, tu et al., 2016a]

sennrich, birch, junczys-dowmunt

id4

99 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016sequence-level training

problem: at training time, target-side history is reliable;
at test time, it is not.
solution: instead of using gold context, sample from the model to
obtain target context [shen et al., 2016, ranzato et al., 2016,
bengio et al., 2015, wiseman and rush, 2016]
more ef   cient cross id178 training remains in use to initialize
weights

sennrich, birch, junczys-dowmunt

id4

100 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016trading-off target and source context

system
source
reference
uedin-id4
uedin-pbsmt

sentence
ein jahr sp  ter machten die fed-repr  sentanten diese k  rzungen r  ckg  ngig.
a year later, fed of   cials reversed those cuts.
a year later, fedex of   cials reversed those cuts.
a year later, the fed representatives made these cuts.

problem

id56 is locally normalized at each time step
given fed: as previous word, ex is very likely in training data: p(ex|fed:) = 0.55
label bias problem: locally-normalized models may ignore input in low-id178 state

potential solutions (speculative)

sampling at training time
bidirectional decoder [liu et al., 2016, sennrich et al., 2016a]
context gates to trade-off source and target context [tu et al., 2016]

sennrich, birch, junczys-dowmunt

id4

101 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016monolingual training data

why monolingual data for phrase-based smt?

more training data  
more appropriate training data (id20)  
relax independence assumptions  

why monolingual data for neural mt?

more training data  
more appropriate training data (id20)  
relax independence assumptions  

sennrich, birch, junczys-dowmunt

id4

102 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016training data: monolingual

solutions/1

shallow fusion: rescore beam with language model
[g  l  ehre et al., 2015]
deep fusion: extra, lm-speci   c hidden layer [g  l  ehre et al., 2015]

sennrich, birch, junczys-dowmunt

id4

[g  l  ehre et al., 2015]

103 / 115

(a)shallowfusion(sec.4.1)(b)deepfusion(sec.4.2)figure1:graphicalillustrationsoftheproposedfusionmethods.learnedbythelmfrommonolingualcorporaisnotoverwritten.itispossibletousemonolingualcorporaaswellwhile   netuningalltheparame-ters,butinthispaper,wealteronlytheoutputpa-rametersinthestageof   netuning.4.2.1balancingthelmandtminorderforthedecoderto   exiblybalancethein-putfromthelmandtm,weaugmentthedecoderwitha   controller   mechanism.theneedto   ex-iblybalancethesignalsarisesdependingontheworkbeingtranslated.forinstance,inthecaseofzh-en,therearenochinesewordsthatcorre-spondtoarticlesinenglish,inwhichcasethelmmaybemoreinformative.ontheotherhand,ifanounistobetranslated,itmaybebettertoig-noreanysignalfromthelm,asitmaypreventthedecoderfromchoosingthecorrecttranslation.in-tuitively,thismechanismhelpsthemodeldynami-callyweightthedifferentmodelsdependingonthewordbeingtranslated.thecontrollermechanismisimplementedasafunctiontakingthehiddenstateofthelmasinputandcomputinggt=  (cid:16)v>gslmt+bg(cid:17),(7)where  isalogisticsigmoidfunction.vgandbgarelearnedparameters.theoutputofthecontrolleristhenmultipliedwiththehiddenstateofthelm.thisletsthede-coderusethesignalfromthetmfully,whilethecontrollercontrolsthemagnitudeofthelmsig-nal.inourexperiments,weempiricallyfoundthatitwasbettertoinitializethebiasbgtoasmall,neg-ativenumber.thisallowsthedecodertodecidetheimportanceofthelmonlywhenitisdeemednecessary.5datasetsweevaluatetheproposedapproachesonfourdi-versetasks:chinesetoenglish(zh-en),turkishtoenglish(tr-en),germantoenglish(de-en)andczechtoenglish(cs-en).wedescribeeachofthesedatasetsinmoredetailbelow.5.1parallelcorpora5.1.1zh-en:opeid4   15weusetheparallelcorporamadeavailableasapartofthenistopeid4   15challenge.sentence-alignedpairsfromthreedomainsarecombinedtoformatrainingset:(1)sms/chatand(2)conversationaltelephonespeech(cts)fromdarpaboltproject,and(3)newsgroup-s/weblogsfromdarpagaleproject.intotal,thetrainingsetconsistsof430ksentencepairs(seetable1forthedetailedstatistics).wetraininallourexperiments,wesetbg=   1toensurethatgtisinitially0.26onaverage.proceedings of amta 2016austin, oct 28 - nov 1, 2016training data: monolingual

solutions/2

decoder is already a language model. train encoder-decoder with
added monolingual data [sennrich et al., 2016b]

ti = tanh(uosi   1 + voeyyi   1 + coci)
yi = softmax(woti)

how do we get approximation of context vector ci?

dummy source context (moderately effective)
automatically back-translate monolingual data into source language

name
pbsmt [haddow et al., 2015]
id4 [g  l  ehre et al., 2015]
shallow fusion [g  l  ehre et al., 2015]
deep fusion [g  l  ehre et al., 2015]
id4 baseline
+back-translated monolingual data

2014
28.8
23.6
23.7
24.0
25.9
29.5

2015
29.3

-
-
-

26.7
30.4

table: de   en translation performance (id7) on wmt training/test sets.

sennrich, birch, junczys-dowmunt

id4

104 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016training data: multilingual

multi-source translation [zoph and knight, 2016]
we can condition on multiple input sentences

bene   ts:

one source text may contain information that is undespeci   ed in other
    possible quality gains

drawbacks:

we need multiple source sentences at training and decoding time

sennrich, birch, junczys-dowmunt

id4

105 / 115

a b c <eos> w x y z <eos> z y x w a b c <eos> w x y z <eos> z y x w i j k figure2:multi-sourceencoder-decodermodelformt.wehavetwosourcesentences(cbaandkji)indifferentlanguages.eachlanguagehasitsownencoder;itpassesits   nalhiddenandcellstatetoasetofcombiners(inblack).theoutputofacombinerisahiddenstateandcellstateofthesamedimension.theinputgateofatypicallstmcell.inequa-tion4,therearetwoforgetgatesindexedbythesubscriptithatserveastheforgetgatesforeachoftheincomingcellsforeachoftheencoders.inequation5,orepresentstheoutputgateofanor-mallstm.i,f,o,anduareallsize-1000vectors.2.3multi-sourceattentionoursingle-sourceattentionmodelismodeledoffthelocal-pattentionmodelwithfeedinputfromluongetal.(2015b),wherehiddenstatesfromthetopdecoderlayercanlookbackatthetophiddenstatesfromtheencoder.thetopdecoderhiddenstateiscombinedwithaweightedsumoftheen-coderhiddenstates,tomakeabetterhiddenstatevector(  ht),whichispassedtothesoftmaxoutputlayer.withinput-feeding,thehiddenstatefromtheattentionmodelissentdowntothebottomde-coderlayeratthenexttimestep.thelocal-pattentionmodelfromluongetal.(2015b)worksasfollows.first,apositiontolookatinthesourceencoderispredictedbyequation9:pt=s  sigmoid(vtptanh(wpht))(9)sisthesourcesentencelength,andvpandwparelearnedparameters,withvpbeingavectorofdimension1000,andwpbeingamatrixofdimen-sion1000x1000.afterptiscomputed,awindowofsize2d+1islookedatinthetoplayerofthesourceencodercenteredaroundpt(d=10).foreachhiddenstateinthiswindow,wecomputeanalignmentscoreat(s),between0and1.thisalignmentscoreiscomputedbyequations10,11and12:at(s)=align(ht,hs)exp(cid:16)   (s   pt)22  2(cid:17)(10)align(ht,hs)=exp(score(ht,hs))ps0exp(score(ht,hs0))(11)score(ht,hs)=httwahs(12)inequation10,  issettobed/2andsisthesourceindexforthathiddenstate.waisalearn-ableparameterofdimension1000x1000.onceallofthealignmentsarecalculated,ctiscreatedbytakingaweightedsumofallsourcehid-denstatesmultipliedbytheiralignmentweight.the   nalhiddenstatesenttothesoftmaxlayerisgivenby:  ht=tanh(cid:16)wc[ht;ct](cid:17)(13)wemodifythisattentionmodeltolookatbothsourceencoderssimultaneously.wecreateacon-textvectorfromeachsourceencodernamedc1tandc2tinsteadofthejustctinthesingle-sourceattentionmodel:  ht=tanh(cid:16)wc[ht;c1t;c2t](cid:17)(14)inourmulti-sourceattentionmodelwenowhavetwoptvariables,oneforeachsourceencoder.proceedings of amta 2016austin, oct 28 - nov 1, 2016training data: multilingual

multilingual models [dong et al., 2015, firat et al., 2016a]
we can share layers (encoder/decoder/attention) of the model across
language pairs

bene   ts:

id21 from one language pair to the other
scalability: no need for n 2     n independent models for n languages

drawbacks:

no successful generalization to language pairs with no training data
(but: synthetic training data works: [firat et al., 2016b])

sennrich, birch, junczys-dowmunt

id4

106 / 115

figure2:multi-tasklearningframeworkformultiple-targetlanguagetranslationfigure3:optimizationforendtomulti-endmodel3.4translationwithbeamsearchalthoughparallelcorporaareavailablefortheencoderandthedecodermodelinginthetrainingphrase,thegroundtruthisnotavailableduringtesttime.duringtesttime,translationisproducedby   ndingthemostlikelysequenceviabeamsearch.  y=argmaxyp(ytp|stp)(15)giventhetargetdirectionwewanttotranslateto,beamsearchisperformedwiththesharedencoderandaspeci   ctargetdecoderwheresearchspacebelongstothedecodertp.weadoptbeamsearchalgorithmsimilarasitisusedinsmtsystem(koehn,2004)exceptthatweonlyutilizescoresproducedbyeachdecoderasfeatures.thesizeofbeamis10inourexperimentsforspeedupconsideration.beamsearchisendeduntiltheend-of-sentenceeossymbolisgenerated.4experimentsweconductedtwogroupsofexperimentstoshowtheeffectivenessofourframework.thegoalofthe   rstexperimentistoshowthatmulti-tasklearninghelpstoimprovetranslationperformancegivenenoughtrainingcorporaforalllanguagepairs.inthesecondexperiment,weshowthatforsomeresource-poorlanguagepairswithafewparalleltrainingdata,theirtranslationperformancecouldbeimprovedaswell.4.1datasettheeuroparlcorpusisamulti-lingualcorpusincluding21europeanlanguages.hereweonlychoosefourlanguagepairsforourexperiments.thesourcelanguageisenglishforalllanguagepairs.andthetargetlanguagesarespanish(es),french(fr),portuguese(pt)anddutch(nl).todemonstratethevalidityofourlearningframework,wedosomepreprocessingonthetrainingset.forthesourcelanguage,weuse30kofthemostfrequentwordsforsourcelanguagevocabularywhichissharedacrossdifferentlanguagepairsand30kmostfrequentwordsforeachtargetlanguage.out-of-vocabularywordsaredenotedasunknownwords,andwemaintaindifferentunknownwordlabelsfordifferentlanguages.fortestsets,wealsorestrictallwordsinthetestsettobefromourtrainingvocabularyandmarktheoovwordsasthecorrespondinglabelsasinthetrainingdata.thesizeoftrainingcorpusinexperiment1and2islistedintable1where1727proceedings of amta 2016austin, oct 28 - nov 1, 2016training data: multilingual

multilingual models [lee et al., 2016]

single, character level encoder trained on
multiple languages

more compact model
occasional quality improvements over single
language pairs
robust towards (synthetic) code-switched input

firat and cho: https://ufal.mff.cuni.cz/mtm16/files/
12-recent-advances-and-future-of-neural-mt-orhat-firat.pdf

sennrich, birch, junczys-dowmunt

id4

107 / 115

18/50fullycharacter-levelmultilingualid4jasonleeandkyunghyuncho,2016(inpreparation)modeldetails, id56searchmodel source-targetcharacterlevel id98+id56encoder bi-scaledecoder {fi,de,cs,ru}   entraining, mixmini-batches usebi-textonlyproceedings of amta 2016austin, oct 28 - nov 1, 2016training data: other tasks

multi-task models [luong et al., 2016]

other tasks can be modelled with sequence-to-sequence models
we can share layers between translation and other tasks

sennrich, birch, junczys-dowmunt

id4

108 / 115

publishedasaconferencepaperaticlr2016figure1:sequencetosequencelearningexamples   (left)machinetranslation(sutskeveretal.,2014)and(right)constituentparsing(vinyalsetal.,2015a).andgermanbyupto+1.5id7pointsoverstrongsingle-taskbaselinesonthewmtbenchmarks.furthermore,wehaveestablishedanewstate-of-the-artresultinconstituentparsingwith93.0f1.wealsoexploretwounsupervisedlearningobjectives,sequenceautoencoders(dai&le,2015)andskip-thoughtvectors(kirosetal.,2015),andrevealtheirinterestingpropertiesinthemtlsetting:autoencoderhelpslessintermsofperplexitiesbutmoreonid7scorescomparedtoskip-thought.2sequencetosequencelearningsequencetosequencelearning(id195)aimstodirectlymodeltheconditionalid203p(y|x)ofmappinganinputsequence,x1,...,xn,intoanoutputsequence,y1,...,ym.itaccomplishessuchgoalthroughtheencoder-decoderframeworkproposedbysutskeveretal.(2014)andchoetal.(2014).asillustratedinfigure1,theencodercomputesarepresentationsforeachinputsequence.basedonthatinputrepresentation,thedecodergeneratesanoutputsequence,oneunitatatime,andhence,decomposestheconditionalid203as:logp(y|x)=xmj=1logp(yj|y<j,x,s)(1)anaturalmodelforsequentialdataistherecurrentneuralnetwork(id56),whichisusedbymostoftherecentid195work.thesework,however,differintermsof:(a)architecture   fromunidirec-tional,tobidirectional,anddeepmulti-layerid56s;and(b)id56type   whicharelong-shorttermmemory(lstm)(hochreiter&schmidhuber,1997)andthegatedrecurrentunit(choetal.,2014).anotherimportantdifferencebetweenid195workliesinwhatconstitutestheinputrepresen-tations.theearlyid195work(sutskeveretal.,2014;choetal.,2014;luongetal.,2015b;vinyalsetal.,2015b)usesonlythelastencoderstatetoinitializethedecoderandsetss=[]ineq.(1).recently,bahdanauetal.(2015)proposesanattentionmechanism,awaytoprovideid195modelswitharandomaccessmemory,tohandlelonginputsequences.thisisaccomplishedbysettingsineq.(1)tobethesetofencoderhiddenstatesalreadycomputed.onthedecoderside,ateachtimestep,theattentionmechanismwilldecidehowmuchinformationtoretrievefromthatmemorybylearningwheretofocus,i.e.,computingthealignmentweightsforallinputpositions.recentworksuchas(xuetal.,2015;jeanetal.,2015a;luongetal.,2015a;vinyalsetal.,2015a)hasfoundthatitiscrucialtoempowerid195modelswiththeattentionmechanism.3multi-tasksequence-to-sequencelearningwegeneralizetheworkofdongetal.(2015)tothemulti-tasksequence-to-sequencelearningset-tingthatincludesthetasksofmachinetranslation(mt),constituencyparsing,andimagecaptiongeneration.dependingwhichtasksinvolved,weproposetocategorizemulti-taskid195learningintothreegeneralsettings.inaddition,wewilldiscusstheunsupervisedlearningtasksconsideredaswellasthelearningprocess.3.1one-to-manysettingthisschemeinvolvesoneencoderandmultipledecodersfortasksinwhichtheencodercanbeshared,asillustratedinfigure2.theinputtoeachtaskisasequenceofenglishwords.aseparatedecoderisusedtogenerateeachsequenceofoutputunitswhichcanbeeither(a)asequenceoftags2proceedings of amta 2016austin, oct 28 - nov 1, 2016id4 as a component in id148

id148

model ensembling is well-established
reranking output of phrase-based/syntax-based with id4
[neubig et al., 2015]
incorporating id4 as a feature function into pbsmt
[junczys-dowmunt et al., 2016]
    results depend on relative performance of pbsmt and id4

26.0

25.9

22.8

27.5

28.1

29.9

30.0

u
e
l
b

20.0

10.0

0.0

english   russian
phrase-based smt

russian   english
neural mt

hybrid

sennrich, birch, junczys-dowmunt

id4

109 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016linguistic features [sennrich and haddow, 2016]
a.k.a. factored id4

motivation: disambiguate words by pos

english
closeverb
closeadj
closenoun ende

german
schlie  en
nah

we thought a win like this might be closeadj.
source
wir dachten, dass ein solcher sieg nah sein k  nnte.
reference
baseline id4 *wir dachten, ein sieg wie dieser k  nnte schlie  en.

sennrich, birch, junczys-dowmunt

id4

110 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016linguistic features: architecture

use separate embeddings for each feature, then concatenate

baseline: only word feature

e(close) =            

0.5
0.2
0.3
0.1

            

|f| input features

e1(close) =      

0.4
0.1
0.2

       e2(adj) =(cid:2)0.1(cid:3) e1(close) (cid:107) e2(adj) =            

0.4
0.1
0.2
0.1

            

sennrich, birch, junczys-dowmunt

id4

111 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016linguistic features: results

27.8

28.4

32.9

31.4

23.8

24.8

u
e
l
b

40.0

30.0

20.0

10.0

0.0

english   german

german   english

english   romanian

baseline

+linguistic features

sennrich, birch, junczys-dowmunt

id4

112 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016further reading

secondary literature

lecture notes by kyunghyun cho: [cho, 2015]
chapter on neural network models in    id151   
by philipp koehn http://mt-class.org/jhu/assets/papers/neural-network-models.pdf

sennrich, birch, junczys-dowmunt

id4

113 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016(a small selection of) resources

id4 tools

dl4mt-tutorial (theano) https://github.com/nyu-dl/dl4mt-tutorial
(our branch: nematus https://github.com/rsennrich/nematus)
id4.matlab https://github.com/lmthang/id4.matlab
id195 (tensor   ow) https://www.tensorflow.org/versions/r0.8/tutorials/id195/index.html
neural monkey (tensor   ow) https://github.com/ufal/neuralmonkey
id195-attn (torch) https://github.com/harvardnlp/id195-attn

sennrich, birch, junczys-dowmunt

id4

114 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016do it yourself

sample    les and instructions for training id4 model
https://github.com/rsennrich/wmt16-scripts
pre-trained models to test decoding (and for further experiments)
http://statmt.org/rsennrich/wmt16_systems/
lab on installing/using nematus:
http://ufal.mff.cuni.cz/mtm16/files/
13-nematus-lab-rico-sennrich.pdf

sennrich, birch, junczys-dowmunt

id4

115 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography i

allen, r. b. (1987).
several studies on natural language and back-propagation.
in in proceedings of the ieee first international conference on neural networks, pages 335   341, san diego, ca. ieee.

bahdanau, d., cho, k., and bengio, y. (2015).
id4 by jointly learning to align and translate.
in proceedings of the international conference on learning representations (iclr).

bengio, s., vinyals, o., jaitly, n., and shazeer, n. (2015).
scheduled sampling for sequence prediction with recurrent neural networks.
corr, abs/1506.03099.

bentivogli, l., bisazza, a., cettolo, m., and federico, m. (2016).
neural versus phrase-based machine translation quality: a case study.
in emnlp 2016.

cho, k. (2015).
natural language understanding with distributed representation.
corr, abs/1511.07916.

cho, k., courville, a., and bengio, y. (2015).
describing multimedia content using attention-based encoder-decoder networks.

cho, k., van merrienboer, b., gulcehre, c., bahdanau, d., bougares, f., schwenk, h., and bengio, y. (2014).
learning phrase representations using id56 encoder   decoder for id151.
in proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1724   1734,
doha, qatar. association for computational linguistics.

sennrich, birch, junczys-dowmunt

id4

116 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography ii

chung, j., cho, k., and bengio, y. (2016).
a character-level decoder without explicit segmentation for id4.
corr, abs/1603.06147.

costa-juss  , r. m. and fonollosa, r. j. a. (2016).
character-based id4.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: short papers), pages
357   361. association for computational linguistics.

dong, d., wu, h., he, w., yu, d., and wang, h. (2015).
id72 for multiple language translation.
in
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers),
pages 1723   1732, beijing, china. association for computational linguistics.

eriguchi, a., hashimoto, k., and tsuruoka, y. (2016).
tree-to-sequence attentional id4.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
823   833. association for computational linguistics.

feng, s., liu, s., li, m., and zhou, m. (2016).
implicit distortion and fertility models for attention-based encoder-decoder id4 model.
corr, abs/1601.03317.

firat, o., cho, k., and bengio, y. (2016a).
multi-way, multilingual id4 with a shared attention mechanism.
in
proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies,
pages 866   875. association for computational linguistics.

sennrich, birch, junczys-dowmunt

id4

117 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography iii

firat, o., sankaran, b., al-onaizan, y., yarman-vural, f. t., and cho, k. (2016b).
zero-resource translation with multi-lingual id4.
corr, abs/1606.04164.

gage, p. (1994).
a new algorithm for data compression.
c users j., 12(2):23   38.

g  l  ehre, c., firat, o., xu, k., cho, k., barrault, l., lin, h., bougares, f., schwenk, h., and bengio, y. (2015).
on using monolingual corpora in id4.
corr, abs/1503.03535.

gulcehre, c., ahn, s., nallapati, r., zhou, b., and bengio, y. (2016).
pointing the unknown words.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
140   149. association for computational linguistics.

ha, t.-l., niehues, j., cho, e., mediani, m., and waibel, a. (2015).
the kit translation systems for iwslt 2015.
in proceedings of the international workshop on spoken language translation (iwslt), pages 62   69.

haddow, b., huck, m., birch, a., bogoychev, n., and koehn, p. (2015).
the edinburgh/jhu phrase-based machine translation systems for wmt 2015.
in proceedings of the tenth workshop on id151, pages 126   133, lisbon, portugal. association for
computational linguistics.

sennrich, birch, junczys-dowmunt

id4

118 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography iv

jean, s., cho, k., memisevic, r., and bengio, y. (2015).
on using very large target vocabulary for id4.
in
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers),
pages 1   10, beijing, china. association for computational linguistics.

junczys-dowmunt, m., dwojak, t., and sennrich, r. (2016).
the amu-uedin submission to the wmt16 news translation task: attention-based id4 models as feature functions in
phrase-based smt.
in proceedings of the first conference on machine translation, volume 2: shared task papers, pages 316   322, berlin,
germany. association for computational linguistics.

junczys-dowmunt, m. and grundkiewicz, r. (2016).
log-linear combinations of monolingual and bilingual id4 models for automatic post-editing.
in proceedings of the first conference on machine translation, pages 751   758, berlin, germany. association for computational
linguistics.

kalchbrenner, n. and blunsom, p. (2013).
recurrent continuous translation models.
in proceedings of the 2013 conference on empirical methods in natural language processing, seattle. association for
computational linguistics.

kalchbrenner, n., espeholt, l., simonyan, k., van den oord, a., graves, a., and kavukcuoglu, k. (2016).
id4 in linear time.
arxiv e-prints.

kim, y. and rush, a. m. (2016).
sequence-level knowledge distillation.
corr, abs/1606.07947.

sennrich, birch, junczys-dowmunt

id4

119 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography v

lee, j., cho, k., and hofmann, t. (2016).
fully character-level id4 without explicit segmentation.
arxiv e-prints.

l   hostis, g., grangier, d., and auli, m. (2016).
vocabulary selection strategies for id4.
arxiv e-prints.

ling, w., trancoso, i., dyer, c., and black, a. w. (2015).
character-based id4.
arxiv e-prints.

liu, l., utiyama, m., finch, a., and sumita, e. (2016).
agreement on target-bidirectional id4 .
in naacl hlt 16, san diego, ca.

luong, m., le, q. v., sutskever, i., vinyals, o., and kaiser, l. (2016).
multi-task sequence to sequence learning.
in iclr 2016.

luong, m.-t. and manning, c. d. (2015).
stanford id4 systems for spoken language domains.
in proceedings of the international workshop on spoken language translation 2015, da nang, vietnam.

luong, m.-t. and manning, d. c. (2016).
achieving open vocabulary id4 with hybrid word-character models.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
1054   1063. association for computational linguistics.

sennrich, birch, junczys-dowmunt

id4

120 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography vi

luong, t., pham, h., and manning, c. d. (2015a).
effective approaches to attention-based id4.
in proceedings of the 2015 conference on empirical methods in natural language processing, pages 1412   1421, lisbon,
portugal. association for computational linguistics.

luong, t., sutskever, i., le, q., vinyals, o., and zaremba, w. (2015b).
addressing the rare word problem in id4.
in
proceedings of the 53rd annual meeting of the association for computational linguistics and the 7th international joint conference on natural language processing (volume 1: long papers),
pages 11   19, beijing, china. association for computational linguistics.

mi, h., sankaran, b., wang, z., and ittycheriah, a. (2016).
a coverage embedding model for id4.
arxiv e-prints.

mi, h., wang, z., and ittycheriah, a. (2016).
vocabulary manipulation for id4.
corr, abs/1605.03209.

neubig, g., morishita, m., and nakamura, s. (2015).
neural reranking improves subjective quality of machine translation: naist at wat2015.
in proceedings of the 2nd workshop on asian translation (wat2015), pages 35   41, kyoto, japan.

pym, a., grin, f., and sfreddo, c. (2012).
the status of the translation profession in the european union, volume 7.
european commission, luxemburg.

sennrich, birch, junczys-dowmunt

id4

121 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography vii

ranzato, m., chopra, s., auli, m., and zaremba, w. (2016).
sequence level training with recurrent neural networks.
in iclr 2016.

sennrich, r. and haddow, b. (2016).
linguistic input features improve id4.
in proceedings of the first conference on machine translation, volume 1: research papers, pages 83   91, berlin, germany.
association for computational linguistics.

sennrich, r., haddow, b., and birch, a. (2016a).
edinburgh id4 systems for wmt 16.
in proceedings of the first conference on machine translation, volume 2: shared task papers, pages 368   373, berlin,
germany. association for computational linguistics.

sennrich, r., haddow, b., and birch, a. (2016b).
improving id4 models with monolingual data.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
86   96, berlin, germany. association for computational linguistics.

sennrich, r., haddow, b., and birch, a. (2016c).
id4 of rare words with subword units.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
1715   1725, berlin, germany. association for computational linguistics.

shen, s., cheng, y., he, z., he, w., wu, h., sun, m., and liu, y. (2016).
minimum risk training for id4.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), berlin,
germany.

sennrich, birch, junczys-dowmunt

id4

122 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography viii

sutskever, i., vinyals, o., and le, q. v. (2014).
sequence to sequence learning with neural networks.
in
advances in neural information processing systems 27: annual conference on neural information processing systems 2014,
pages 3104   3112, montreal, quebec, canada.

tu, z., liu, y., lu, z., liu, x., and li, h. (2016).
context gates for id4.
arxiv e-prints.

tu, z., lu, z., liu, y., liu, x., and li, h. (2016a).
coverage-based id4.
corr, abs/1601.04811.

tu, z., lu, z., liu, y., liu, x., and li, h. (2016b).
modeling coverage for id4.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
76   85. association for computational linguistics.

turovsky, b. (2016).
ten years of google translate.
https://googleblog.blogspot.co.uk/2016/04/ten-years-of-google-translate.html.

wiseman, s. and rush, a. m. (2016).
sequence-to-sequence learning as beam-search optimization.
corr, abs/1606.02960.

sennrich, birch, junczys-dowmunt

id4

123 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016bibliography ix

wu, y., schuster, m., chen, z., le, q. v., norouzi, m., macherey, w., krikun, m., cao, y., gao, q., macherey, k., klingner, j.,
shah, a., johnson, m., liu, x., kaiser,   ., gouws, s., kato, y., kudo, t., kazawa, h., stevens, k., kurian, g., patil, n., wang, w.,
young, c., smith, j., riesa, j., rudnick, a., vinyals, o., corrado, g., hughes, m., and dean, j. (2016).
google   s id4 system: bridging the gap between human and machine translation.
arxiv e-prints.

zhang, b., xiong, d., and su, j. (2016).
recurrent id4.
corr, abs/1607.08725.

zhao, s. and zhang, z. (2016).
an ef   cient character-level id4.
arxiv e-prints.

zhou, j., cao, y., wang, x., li, p., and xu, w. (2016).
deep recurrent models with fast-forward connections for id4.
transactions of the association of computational linguistics     volume 4, issue 1, pages 371   383.

zoph, b. and knight, k. (2016).
multi-source neural translation.
in naacl hlt 2016.

sennrich, birch, junczys-dowmunt

id4

124 / 115

proceedings of amta 2016austin, oct 28 - nov 1, 2016