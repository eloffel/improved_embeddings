deep	learning		

russ	salakhutdinov	

machine learning department 
carnegie mellon university 

canadian institute for advanced research

mlss 2017: lecture 1	

mining	for	structure	

massive	increase	in	both	computa:onal	power	and	the	amount	of	
data	available	from	web,	video	cameras,	laboratory	measurements.	

images	&	video	

text	&	language		

speech	&	audio	

gene	expression	

product		
recommenda:on	

rela:onal	data/		
social	network	

climate	change	

geological	data	

      	develop	sta:s:cal	models	that	can	discover	underlying	structure,	seman:c	
rela:ons,	constraints,	or	invariances	from	data.	
      	robust,	adap:ve	models	models	that	can	deal	with	missing	measurements,	
nonsta:onary	distribu:ons,	mul:modal	data.			

impact	of	deep	learning	

      	speech	recogni:on	
      	computer	vision	
      	recommender	systems		
      	language	understanding		
      	drug	discovery	and	medical	
image	analysis		

example:	understanding	images	

tags:	

strangers,		coworkers,		conven:oneers,		
anendants,		patrons	

nearest	neighbor	sentence:	

people	taking	pictures	of	a	crazy	person	

model	samples	
      	a	group	of	people	in	a	crowded	area	.	
      	a	group	of	people	are	walking	and	talking	.	
      	a	group	of	people,	standing	around	and	talking	.	

tutorial	roadmap	

part	1:	supervised	(discrimina:ve)	learning:	deep	
networks		

part	2:	unsupervised	learning:	deep	genera:ve	
models	

part	3:	open	research	ques:ons	

    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    dvalid dtest

supervised	learning		

hugo larochelle

      	given	a	set	of	labeled	training	examples:																								,	we	perform	
empirical	risk	minimiza:on:			

d  epartement d   informatique
universit  e de sherbrooke

feedforward neural network

hugo.larochelle@usherbrooke.ca

arg min

   

1

t xt

where	

l(f(x(t);    ), y(t)) +     (   )

september 13, 2012

loss	func:on		

abstract

math for my slides    feedforward neural network   .

     																					(non-linear)	func:on	mapping	inputs	to	outputs,	
parameterized	by	   ->	non-convex	op:miza:on
    f (x)
     																																								is	the	loss	func:on.	
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
       (   )

    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    dvalid dtest

supervised	learning		

      	given	a	set	of	labeled	training	examples:																								,	we	perform	
empirical	risk	minimiza:on:			

feedforward neural network

hugo larochelle

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
universit  e de sherbrooke
september 13, 2012

hugo.larochelle@usherbrooke.ca

arg min

   

1

t xt

l(f(x(t);    ), y(t)) +     (   )

september 13, 2012

abstract

where	

math for my slides    feedforward neural network   .

loss	func:on		

regularizer	

abstract

math for my slides    feedforward neural network   .

    f (x)
     																					(non-linear)	func:on	mapping	inputs	to	outputs,	
parameterized	by	   ->	non-convex	op:miza:on
    l(f(x(t);    ), y(t))
    f (x)
     																																								is	the	loss	func:on.	
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
     															is	a	regulariza:on	term.		
       (   )
       (   )
    r      (   )

    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    dvalid dtest

supervised	learning		

      	given	a	set	of	labeled	training	examples:																								,	we	perform	
empirical	risk	minimiza:on:			

arg min

   

1

t xt

l(f(x(t);    ), y(t)) +     (   )

      	learning	is	cast	as	op:miza:on.		

loss	func:on		

regularizer	

     	for	classi   ca:on	problems,	we	would	like	to	minimize	classi   ca:on	
error.	

     	loss	func:on	can	some:mes	be	viewed	as	a	surrogate	for	what	we	
want	to	op:mize	(e.g.	upper	bound)		

example:	id163	dataset	

      	1.2	million	(225	x	225)	images,	1000	classes	

examples	of	hammer	

(deng et al., id163: a large scale hierarchical image database, cvpr 2009)

    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    dvalid dtest

      	input:	225	x	225	image	
      	output:	so^max	over	1000	classes	

alexnet	

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke
hugo.larochelle@usherbrooke.ca
d  epartement d   informatique
universit  e de sherbrooke
september 13, 2012

hugo.larochelle@usherbrooke.ca

softmax output 

layer 7: full 

september 13, 2012

layer 6: full 
abstract

layer 5: conv + pool 

arg min

   

1

t xt

l(f(x(t);    ), y(t)) +     (   )

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

     																					di   eren:able,	non-nonlinear	
    f (x)
func:on	parameterized	by	  : 8	layers,	60m	
parameters	->	non-convex	op:miza:on	
    l(f(x(t);    ), y(t))
    f (x)
     																																								is	the	cross	id178	loss.	
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
     														:	l2,	early	stopping,	drop-out			
       (   )
      	achieves:	18.2%	top-5	error			
       (   )
    r      (   )
    r      (   )
    f(x)c = p(y = c|x)

abstract

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

(krizhevsky, sutskever, hinton, nips, 2012) 

important	breakthrough	

      	deep	convolu:onal	nets	for	vision	(supervised)		

krizhevsky, a., sutskever, i. and hinton, g. e., id163 classi   cation with deep 
convolutional neural networks, nips, 2012. 

unsupervised	learning	

      	given	a	set	of	unlabeled	training	examples													:	

vector	of	word	counts	
on	a	webpage	

latent	variables:	
hidden	topics	

european community 
monetary/economic  

interbank markets

energy markets

leading          
economic         
indicators       

disasters and 
accidents     

legal/judicial

804,414	unlabelled	
newswire	stories	

accounts/
earnings 

government 
borrowings 

(hinton & salakhutdinov, science, 2006) 

genera:ve	adversarial	net		

trained	on	id163	

training	

samples	

(salimans et. al., 2016)

boltzmann	machine	

observed	data	

25,000	characters	from	50	
alphabets	around	the	world.	

simulate	a	markov	chain	
whose	sta:onary	distribu:on	
is	

talk	roadmap	

part	1:	supervised	learning:	deep	networks		

       de   ni:on	and	training	neural	networks	
       recent	op:miza:on	/	regulariza:on	techniques	

part	2:	unsupervised	learning:	learning	deep	
genera:ve	models	

part	3:	open	research	ques:ons	

neural networks online course 

       disclaimer: some of the material and slides for this lecture were 
borrowed from hugo larochelle   s class on neural networks: 
https://sites.google.com/site/deeplearningsummerschool2016/ 

       hugo   s class covers 
many other topics: 
convolutional networks, 
neural language model, 
id82s, 
autoencoders, sparse 
coding, etc. 

       we will use his 
material for some of the 
other lectures.  

feedforward neural networks 

       definition of neural networks  

-    forward propagation 
-    types of units 
-    capacity of neural networks 

       how to train neural nets:  

-    id168 
-    id26 with id119 

       more recent techniques: 

-    dropout 
-    batch id172 
-    unsupervised pre-training 

d  epartement d   informatique

hugo.larochelle@usherbrooke.ca

september 6, 2012

universit  e de sherbrooke

math for my slides    feedforward neural network   .

september 6, 2012

abstract

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

abstract

abstract

artificial neuron 
abstract

hugo.larochelle@usherbrooke.ca
       neuron pre-activation (or input activation): 
september 6, 2012

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)
    a(x) = b +pi wixi = b + w>x
    x1 xd b w1 wd
       neuron output activation: 
abstract
    h(x) = g(a(x)) = g(b +pi wixi)
    x1 xd b w1 wd
    x1 xd b w1 wd
    w
    w
    w
    x1 xd b w1 wd
    {
where 
     
    {
    {
    w
    g(  ) b
     
    g(  ) b
    {
     
    g(  ) b
    h(x) = g(a(x))
    g(  ) b
    h(x) = g(a(x))
    h(x) = g(a(x))
    h(x) = g(a(x))

 are the weights (parameters) 
 is the bias term 
 is called the activation function   

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)

i,j xj   
i pj w (1)
i,j xj   
i,j xj   
i pj w (1)
i pj w (1)

math for my slides    feedforward neural network   .

    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)

abstract
abstract
artificial neuron 

math for my slides    feedforward neural network   .
math for my slides    feedforward neural network   .

       output activation of the neuron: 

    a(x) = b +pi wixi = b + w>x
    a(x) = b +pi wixi = b + w>x
    h(x) = g(a(x)) = g(b +pi wixi)
    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    x1 xd b w1 wd
    w
    w
range is 
    {
    {
determined  
    g(  ) b
by  
    g(  ) b
    h(x) = g(a(x))
    h(x) = g(a(x))

bias only changes 
the position of the 
riff 

(from pascal vincent   s slides) 

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    o(x) = g(out)(b(2) + w(2)>x)
    o(x) = g(out)(b(2) + w(2)>x)

i,j xj   
i,j xj   
i pj w (1)
i pj w (1)

    h(x) = g(a(x)) = g(b +pi wixi)

    x1 xd b w1 wd
    w
activation function  
    {
    g(a) = a
    g(a) = sigm(a) =
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b
    h(x) = g(a(x))

1

       sigmoid activation function:  

      squashes the neuron   s 
output between 0 and 1  

      always positive 
      bounded 
      strictly increasing  

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    o(x) = g(out)(b(2) + w(2)>x)

i pj w (1)

       rectified linear (relu) activation function:  

1+exp( a)
exp(a)+exp( a) = exp(2a) 1

exp(2a)+1

1

    g(a) = a
    g(a) = sigm(a) =
activation function  
    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    h(x) = g(a(x))

    a(x) = b(1) + w(1)x    a(x)i = b(1)
    o(x) = g(out)(b(2) + w(2)>x)

i pj w (1)

      bounded below by 0 

(always non-negative) 

     

tends to produce units 
with sparse activities 
      not upper bounded 
      strictly increasing  

i

i

i,j

i,j

b(2)

b(1)
i

exp(2a)+1

b(1)
i

exp(2a)+1
exp(2a)+1

exp(a)+exp( a) = exp(2a) 1
exp(a)+exp( a) = exp(2a) 1

    g(a) = tanh(a) = exp(a) exp( a)
    g(a) = tanh(a) = exp(a) exp( a)
exp(a)+exp( a) = exp(2a) 1
    g(a) = tanh(a) = exp(a) exp( a)
    g(  ) b
    g(a) = max(0, a)
    g(a) = max(0, a)
single hidden layer neural net 
    g(a) = max(0, a)
    w (1)
xj h(x)i w(2)
b(2)
    g(a) = reclin(a) = max(0, a)
    g(a) = reclin(a) = max(0, a)
    g(  ) b
    g(a) = reclin(a) = max(0, a)
       hidden layer pre-activation: 
    h(x) = g(a(x))
    w (1)
xj h(x)i w(2)
i,j xj   
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    p(y = 1|x)
i +pj w (1)
    p(y = 1|x)
    h(x) = g(a(x))
    g(  ) b
    a(x) = b(1) + w(1)x    a(x)i = b(1)
i,j xj   
i +pj w (1)
    g(  ) b
    f (x) = o(b(2) + w(2)>x)
    w (1)
xj h(x)i w(2)
    f (x) = o(b(2) + w(2)>x)
xj h(x)i w(2)
    w (1)
i
       hidden layer activation: 
i
    h(x) = g(a(x))
    h(x) = g(a(x))
    a(x) = b(1) + w(1)x    a(x)i = b(1)
    a(x) = b(1) + w(1)x    a(x)i = b(1)
       output layer activation: 
    f(x) = o   b(2) + w(2)>h(1)x   
    f(x) = o   b(2) + w(2)>x   

i,j xj   
i +pj w (1)
i,j xj   
i +pj w (1)

b(1)
b(1)
i
i

b(2)
b(2)

i,j
i,j

1

1

output activation 
function 

1
1

exp(ac )

exp(ac )

exp(ac )

pc exp(ac)i>
multilayer neural net 
pc exp(ac)i>
pc exp(ac)i>

    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
    p(y = c|x)
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
       consider a network with l hidden layers.  
    f (x)
pc exp(ac) . . .
    p(y = c|x)
-    layer pre-activation for k>0 
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    o(a) = softmax(a) =h exp(a1)
    f (x)
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    p(y = c|x)
-    hidden layer activation  
    f (x)
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
pc exp(ac)i>
    o(a) = softmax(a) =h exp(a1)
   from 1 to l: 
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
pc exp(ac) . . .
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
-    output layer activation (k=l+1): 
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)

    f (x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0)(x) = x)
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)

exp(ac )

capacity of neural nets 

       consider a single layer neural network 

r  eseaux de neurones

2

x2

1

z

1

0

-1

-1

0

0

-1

x1

1

zk

y1

1

0

-1

-1

0

1

0

-1

x1

x2

1

biais
bias 

.5

output 
sortie k
wkj

y2

y2

1

-1

0

cach  ee j
hidden 
wji
entr  ee i
input 

-1

y1

.7

-.4

-1.5

1 1

1

1

x1

x2

x2

1

z=-1

z=+1

r1

-1

r2

x1

1

x2

1

0

1

-1

x1

-1

0

(from pascal vincent   s slides) 

capacity of neural nets 

       consider a single layer neural network 

(from pascal vincent   s slides) 

feedforward neural networks 

       how neural networks predict f(x) given an input x: 

-    forward propagation 
-    types of units 
-    capacity of neural networks 

       how to train neural nets:  

-    id168 
-    id26 with id119 

       more recent techniques: 

-    dropout 
-    batch id172 
-    unsupervised pre-training 

    training set: dtrain = {(x(t), y(t))}
    f(x;    )
    dvalid dtest

       empirical risk minimization: 

training  

hugo larochelle

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

arg min

   

1

t xt

l(f(x(t);    ), y(t)) +     (   )

math for my slides    feedforward neural network   .

september 13, 2012

math for my slides    feedforward neural network   .

abstract

regularizer 

       to train a neural net, we need: 

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .
    f (x)
id168  
    f (x)
    l(f (x(t);    ), y(t))
    f (x)
    f (x)
    l(f(x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
       id168: 
    l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
       (   )
       a procedure to compute gradients: 
    r   l(f (x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
       regularizer and its gradient:          ,   
    r      (   )
       (   )
       (   )
       (   )
    f (x)c = p(y = c|x)
    r      (   )
    r      (   )
    r      (   )
    x(t) y(t)
    f(x)c = p(y = c|x)
    f (x)c = p(y = c|x)

feedforward neural network

d  epartement d   informatique
universit  e de sherbrooke

d  epartement d   informatique
universit  e de sherbrooke

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

hugo.larochelle@usherbrooke.ca

   

abstract
hugo larochelle

l(f (x(t);    ), y(t)) +     (   )

math for my slides    feedforward neural network   .

t pt r   l(f (x(t);    ), y(t))    r      (   )
t xt

    l(f (x(t);    ), y(t))
      =   1
       (   )
              +  
   
t pt r   l(f (x(t);    ), y(t))    r      (   )
stochastic gradient descend 
1
      =   1
    {x 2 rd | rxf (x) = 0}
arg min
    v>r2
xf (x)v > 0 8v
              +  
       perform updates after seeing each example:  
    f (x)
-    initialize:  
    v>r2
    {x 2 rd | rxf (x) = 0}
xf (x)v < 0 8v
    l(f (x(t);    ), y(t))
            {w(1), b(1), . . . , w(l+1), b(l+1)}
-    for t=1:t 
      =  r   l(f (x(t);    ), y(t))    r      (   )
    v>r2
xf (x)v > 0 8v
    l(f (x(t);    ), y(t))
       (   )
-    for each training example  
    (x(t), y(t))
    v>r2
xf (x)v < 0 8v
    r   l(f (x(t);    ), y(t))
      =   1
      =  r   l(f (x(t);    ), y(t))    r      (   )
math for my slides    feedforward neural network   .
       (   )
    f (x)
              +      
    f (x)
    r      (   )
    l(f (x(t);    ), y(t))
    {x 2 rd | rxf (x) = 0}
       to train a neural net, we need: 
    f (x)
5
    f (x)
    f (x)c = p(y = c|x)
    l(f(x(t);    ), y(t))
    r   l(f (x(t);    ), y(t))
    l(f (x(t);    ), y(t))
    v>r2
xf (x)v > 0 8v
       id168: 
    l(f (x(t);    ), y(t))
    x(t) y(t)
    r   l(f (x(t);    ), y(t))
       (   )
       a procedure to compute gradients: 
    r   l(f (x(t);    ), y(t))
    l(f (x), y) =  pc 1(y=c) log f (x)c =   log f (x)y =
    v>r2
xf (x)v < 0 8v
    r   l(f (x(t);    ), y(t))
       regularizer and its gradient:          ,   
    r      (   )
       (   )
       (   )
      =  r   l(f (x(t);    ), y(t))    r      (   )
       (   )
    f (x)c = p(y = c|x)
    r      (   )
    r      (   )
    (x(t), y(t))
    r      (   )

t pt r   l(f (x(t);    ), y(t))    r      (   )

5
math for my slides    feedforward neural network   .

iteration of all examples 

training epoch 

abstract

   

= 

september 13, 2012

@

f (x)c   log f (x)y =  1(y=c)

math for my slides    feedforward neural network   .

math for my slides    feedforward neural network   .

id26 algorithm: 
computational flow graph  

       forward propagation can be represented 
as an acyclic flow graph 

       forward propagation can be implemented 
in a modular way: 

      each box can be an object with an fprop 
method, that computes the value of the 
box given its children 

      calling the fprop method of each box in 
the right order yields forward propagation 

id26 algorithm: 
computational flow graph  

       each object also has a bprop method 
-    it computes the gradient of the loss with 
respect to each child box.  

       by calling bprop in the reverse order, we 
obtain id26 

    f(x;    )
    dvalid dtest

l(f(x(t);    ), y(t)) +     (   )

weight decay 
t xt
1

    g0(a) = g(a)(1   g(a))
    g0(a) = g(a)(1   g(a))
arg min
    g0(a) = 1   g(a)2
   
    g0(a) = 1   g(a)2
       l2 id173: 

       l1 id173: 
    rw(k)   (   ) = 2w(k)

    rw(k)   (   ) = 2w(k)

       (   ) =pkpipj   w (k)
i,j    2
       (   ) =pkpipj   w (k)
i,j    2
=pk ||w(k)||2
       (   ) =pkpipj |w (k)
       (   ) =pkpipj |w (k)

i,j |
i,j |
    rw(k)   (   ) = sign(w(k))
-    only applies to weights, not biases (weigh decay) 
    rw(k)   (   ) = sign(w(k))
    sign(w(k))i,j = 1 0
    sign(w(k))i,j = 1 0

=pk ||w(k)||2

f

f

       training protocol: 

machine learning

model selection 

t b    = 1
    t 1
    supervised learning example: (x, y) x y
    training set: dtrain = {(x(t), y(t))}
    f (x;    )

t pt(x(t)  b  )(x(t)  b  )>
t b    = 1
t pt(x(t)  b  )(x(t)  b  )>
    supervised learning example: (x, y) x y
t b    = 1
    t 1
    training set: dtrain = {(x(t), y(t))}
    f (x;    )
    supervised learning example: (x, y) x y
    dvalid dtest
    training set: dtrain = {(x(t), y(t))}
      hyper-parameter search: hidden layer size, learning rate, 
number of iterations/epochs, etc. 
    f (x;    )
-    estimate generalization performance using the test set 
    dvalid dtest

-    train your model on the training set  
-    for model selection, use validation set  

t pt(x(t)  b  )(x(t)  b  )>

       generalization is the behavior of the model on unseen 
examples.  

early stopping 

       to select the number of epochs, stop training when validation set 
error increases (with some look ahead). 

2   

       make updates based on a mini-batch of examples (instead of a 
single example): 

@x     f (x+   ) f (x    )
mini-batch, momentum 

    w(3) w(2) w(1) x f (x)
    @f (x)
    f (x) x    
    f (x +    ) f (x      )
    p1t=1    t = 1
the gradient is the average regularized loss for that mini-batch 
    p1t=1    2
can give a more accurate estimate of the gradient 
can leverage matrix/matrix operations, which are more efficient 
       t =    
       t =    
    r(t)

    = r   l(f (x(t)), y(t)) +  r(t 1)

       momentum: can use an exponential average of previous 
gradients: 

t  0.5 <       1  

t < 1    t

     
     
     

1+ t

   

can get pass plateaus more quickly, by       gaining momentum       

     

learning distributed representations 
       deep learning: learning models with multilayer representations 
      multilayer (feed-forward) neural networks  
      multilayer graphical model (deep belief network, deep boltzmann 

machine) 

       each layer learns       distributed representation       
      units in a layer are not mutually exclusive 

each unit is a separate feature of the input 
two units can be       active       at the same time 

      
      
 units do not correspond to a partitioning (id91) of the inputs 
      

in id91, an input can only belong to a single cluster 

     

inspiration from visual cortex 

feedforward neural networks 

       how neural networks predict f(x) given an input x: 

-    forward propagation 
-    types of units 
-    capacity of neural networks 

       how to train neural nets:  

-    id168 
-    id26 with id119 

       more recent techniques: 

-    dropout 
-    batch id172 

best practice  
       given a dataset d, pick a model so that:    
      you can achieve 0 training error    overfit on the training set  

       regularize the model (e.g. using dropout).  

       sgd with momentum, batch-id172, and dropout usually 
works very well. 

dropout 

       key idea: cripple neural network by removing hidden units 
stochastically 

     

     

     

each hidden unit is set to 0 with 
id203 0.5 

hidden units cannot co-adapt to 
other units 

hidden units must be more 
generally useful 

       could use a different dropout 
id203, but 0.5 usually works well 

(srivastava, hinton, krizhevsky, 
sutskever, salakhutdinov, jmlr 2014) 

exp(ac )

exp(ac )

layer pre-activation for k>0 

    p(y = c|x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
dropout 
    p(y = c|x)
pc exp(ac) . . .
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
    f (x)
       use random binary masks m(k)  
    p(y = c|x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
     
    f (x)
    o(a) = softmax(a) =h exp(a1)
pc exp(ac)i>
pc exp(ac) . . .
    a(k)(x) = b(k) + w(k)h(k 1)(x) (h(0)(x) = x)
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    f (x)
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
     
hidden layer activation (k=1 to l): 
    h(1)(x) h(2)(x) w(1) w(2) w(3) b(1) b(2) b(3)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
    h(k)(x) = g(a(k)(x))
    a(k)(x) = b(k) + w(k)h(k 1)x (h(0) = x)
    h(l+1)(x) = o(a(l+1)(x)) = f (x)
      output activation (k=l+1) 
    h(k)(x) = g(a(k)(x))
    h(l+1)(x) = o(a(l+1)(x)) = f (x)

exp(ac )

(srivastava, hinton, krizhevsky, 
sutskever, salakhutdinov, jmlr 2014) 

dropout at test time  

       at test time, we replace the masks by their expectation 

     
     

this is simply the constant vector 0.5 if dropout id203 is 0.5 
for single hidden layer: equivalent to taking the geometric average 
of all neural networks, with all possible binary masks 

       can be combined with unsupervised pre-training 
       beats regular id26 on many datasets 

       ensemble: can be viewed as a geometric average of exponential 
number of networks.  

batch id172 

       normalizing the inputs will speed up training (lecun et al. 1998) 
could id172 be useful at the level of the hidden layers? 

     

       batch id172 is an attempt to do that (ioffe and szegedy, 2015) 

     

     
     
     

each unit   s pre-activation is normalized (mean subtraction, stddev 
division) 
during training, mean and stddev is computed for each minibatch 
id26 takes into account the id172 
at test time, the global mean / stddev is used 

(ioffe and szegedy, icml 2015) 

batch id172 

activation function (     and    are trained) 

 and    are trained) 

learned linear transformation to adapt to non-linear 

(ioffe and szegedy, icml 2015) 

malize each dimension
!x(k) =

xi

1
m

  b    

// mini-batch mean

m#i=1
batch id172 
m#i=1
xi       b"  2

x(k)     e[x(k)]
"var[x(k)]
  2
b    
       why normalize the pre-activation? 
!xi    
yi       !xi +        bn  ,  (xi)
algorithm 1: batch normalizing transform, applied to
activation x over a mini-batch.

can help keep the pre-activation in a non-saturating regime 
(though the linear transform                             could cancel this 
effect) 

// normalize
// scale and shift

       use the global mean and stddev at test time. 

// mini-batch variance

(xi       b)2

b +   

1
m

     

where the expectation and variance are computed over the
training data set. as shown in (lecun et al., 1998b), such
id172 speeds up convergence, even when the fea-
tures are not decorrelated.
note that simply normalizing each input of a layer may
change what the layer can represent. for instance, nor-
malizing the inputs of a sigmoid would constrain them to
the linear regime of the nonlinearity. to address this, we
make sure that the transformation inserted in the network
can represent the identity transform. to accomplish this,

     

the bn transform can be added to a network to manip-
ulate any activation. in the notation y = bn  ,  (x), we

removes the stochasticity of the mean and stddev 

     

requires a final phase where, from the first to the last hidden layer 
      
      

propagate all training data to that layer 
compute and store the global mean and stddev of each unit 

3

     

for early stopping, could use a running average 

(ioffe and szegedy, icml 2015) 

optimization tricks 

       sgd with momentum, batch-id172, and dropout usually 
works very well 

       pick learning rate by running on a subset of the data 
      start with large learning rate & divide by 2 until loss does not diverge 
      decay learning rate by a factor of ~100 or more by the end of training  

       use relu nonlinearity  

        initialize parameters so that each feature across layers has 
similar variance. avoid units in saturation. 

[from marc'aurelio ranzato, cvpr 2014 tutorial] 

visualization 

       check gradients numerically by finite differences 

       visualize features (features need to be uncorrelated) and have 
high variance 

       good training: hidden units 
are sparse across samples  

[from marc'aurelio ranzato, cvpr 2014 tutorial] 

visualization 

       check gradients numerically by finite differences 

       visualize features (features need to be uncorrelated) and have 
high variance 

       bad training: many hidden 
units ignore the input and/or 
exhibit strong correlations 

debugging on small dataset 

       next, make sure your model can overfit on a smaller dataset     
(~ 500-1000 examples) 
       if not, investigate the following situations: 
      are some of the units saturated, even before the first update?  

scale down the initialization of your parameters for these units 
properly normalize the inputs 

      
      
is the training error bouncing up and down? 
      

decrease the learning rate 

     

       this does not mean that you have computed gradients correctly:  
      you could still overfit with some of the gradients being wrong 

id161  

       design algorithms that can process visual data to accomplish a given task:  

     

for example, object recognition: given an input image, identify 
which object it contains 

convnets: examples 

       id42, house number and traffic sign 
classification 

architecture  

       how can we select the right architecture: 
      manual tuning of features is now replaced with the manual tuning 

of architechtures 

       depth 
       width 
       parameter count 

how to choose architecture  

       many hyper-parameters: 
      number of layers, number of feature maps 

       cross validation 

       grid search (need lots of gpus) 

       smarter strategies  
      random search  
      bayesian optimization  

alexnet 

       8 layers total 

       trained on id163 
dataset [deng et al. cvpr   09] 

       18.2% top-5 error  

[from rob fergus    cifar 2016 tutorial] 

softmax output 

layer 7: full 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

alexnet 

       remove top fully connected layer 7  

softmax output 

       drop ~16 million parameters 

       only 1.1% drop in performance! 

[from rob fergus    cifar 2016 tutorial] 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

alexnet 

       let us remove upper feature extractor layers 
and fully connected: 

softmax output 

     

layers 3,4, 6 and 7 

       drop ~50 million parameters 

       33.5 drop in performance! 

       depth of the network is the key.  

[from rob fergus    cifar 2016 tutorial] 

layer 6: full 

layer 5: conv + pool 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

googlenet 

       24 layer model that uses so-called inception 
module.  

convolution 
pooling 
softmax 
other 

(szegedy et al., going deep with convolutions, 2014)

googlenet 

       width of inception modules ranges from 256 filters (in early modules) to 
1024 in top inception modules. 
       can remove fully connected layers on top completely 
       number of parameters is reduced to 5 million 
       6.7% top-5 validation error on imagnet 

(szegedy et al., going deep with convolutions, 2014)

deep residual learning for image recognition

xiangyu zhang

microsoft research

shaoqing ren

residual networks  

jian sun

{kahe, v-xiangz, v-shren, jiansun}@microsoft.com

really, really deep convnets do not train well,  
e.g. cifar10: 

top-1 err.
28.07

output 

size: 224

output 

size: 112

output 

size: 56

vgg-19

34-layer plain

34-layer residual

image

image

image

3x3 conv, 64

3x3 conv, 64

pool, /2

3x3 conv, 128

3x3 conv, 128

7x7 conv, 64, /2

7x7 conv, 64, /2

pool, /2

pool, /2

pool, /2

3x3 conv, 256

64-d

3x3 conv, 64

256-d

3x3 conv, 64

3x3 conv, 256

3x3, 64

relu
3x3 conv, 256

3x3, 64

3x3 conv, 256

3x3 conv, 64

1x1, 64

3x3 conv, 64

3x3 conv, 64

relu

3x3, 64

relu

3x3 conv, 64

3x3 conv, 64

1x1, 256

3x3 conv, 64

relu

relu

3x3 conv, 64

3x3 conv, 64

3x3 conv, 64

3x3 conv, 64

residual network. based on the above plain network, we
insert shortcut connections (fig. 3, right) which turn the
network into its counterpart residual version. the identity
shortcuts (eqn.(1)) can be directly used when the input and
output are of the same dimensions (solid line shortcuts in
fig. 3). when the dimensions increase (dotted line shortcuts
in fig. 3), we consider two options: (a) the shortcut still
performs identity mapping, with extra zero entries padded
for increasing dimensions. this option introduces no extra
parameter; (b) the projection shortcut in eqn.(2) is used to
match dimensions (done by 1   1 convolutions). for both
options, when the shortcuts go across feature maps of two
sizes, they are performed with a stride of 2.
3.4. implementation

20

10

)

%

(
 
r
o
r
r
e
 

g
n
i
n
i
a
r
t

 

0 
0

1

2

3

4
iter. (1e4)

 

56-layer

20-layer

5

6

20

10

)

%

(
 
r
o
r
r
e
 
t
s
e
t

 

0
0

1

model
vgg-16 [41]
googlenet [44]
prelu-net [13]
plain-34
resnet-34 a
resnet-34 b
resnet-34 c
resnet-50
resnet-101
resnet-152

56-layer

20-layer

-

 

24.27
28.54
25.03
24.52
24.19
22.85
21.75
21.43

top-5 err.

9.33
9.15
7.38
10.02
7.76
7.46
7.40
6.71
6.05
5.71

5

6

2

3

4
iter. (1e4)

table 3. error rates (%, 10-crop testing) on id163 validation.
vgg-16 is based on our test. resnet-50/101/152 are of option b
that only uses projections for increasing dimensions.

x

greatly bene   ted from very deep models.

figure 1. training error (left) and test error (right) on cifar-10
key idea: introduce    pass 
with 20-layer and 56-layer    plain    networks. the deeper network
method
through    into each layer 
has higher training error, and thus test error. similar phenomena
vgg [41] (ilsvrc   14)
on id163 is presented in fig. 4.
googlenet [44] (ilsvrc   14)
vgg [41] (v5)
prelu-net [13]
thus only residual now 
bn-inception [16]
resnet-34 b
needs to be learned 
resnet-34 c
resnet-50
resnet-101
resnet-152

driven by the signi   cance of depth, a question arises: is
learning better networks as easy as stacking more layers?
an obstacle to answering this question was the notorious
problem of vanishing/exploding gradients [1, 9], which
hamper convergence from the beginning. this problem,
f(x)
method
however, has been largely addressed by normalized initial-
vgg [41] (ilsvrc   14)
ization [23, 9, 37, 13] and intermediate id172 layers
googlenet [44] (ilsvrc   14)
vgg [41] (v5)
[16], which enable networks with tens of layers to start con-
prelu-net [13]
f(x)(cid:1)+(cid:1)x
verging for stochastic id119 (sgd) with back-
bn-inception [16]
resnet (ilsvrc   15)
propagation [22].
when deeper networks are able to start converging, a
degradation problem has been exposed: with the network
depth increasing, accuracy gets saturated (which might be

figure 2. residual learning: a building block.
(he, zhang, ren, sun, cvpr 2016)

weight layer

weight layer

identity

x

relu

relu

table 4. error rates (%) of single-model results on the id163
validation set (except     reported on the test set).

with ensembling, 3.57% top-5 
test error on id163 

table 5. error rates (%) of ensembles. the top-5 error is on the
test set of id163 and reported by the test server.

size: 28

output 

pool, /2

figure 5. a deeper residual function f for id163. left: a
building block (on 56   56 feature maps) as in fig. 3 for resnet-
34. right: a    bottleneck    building block for resnet-50/101/152.

3x3 conv, 128, /2

3x3 conv, 128, /2

3x3 conv, 128

3x3 conv, 128

3x3 conv, 512

3x3 conv, 512

3x3 conv, 128

3x3 conv, 128

3x3 conv, 512

3x3 conv, 128

3x3 conv, 128

output 

size: 14

pool, /2

3x3 conv, 256

3x3 conv, 256

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 256

3x3 conv, 256

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 128

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 512

3x3 conv, 512

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256, /2

3x3 conv, 256, /2

top-1 err.

-
-

top-5 err.
8.43   
7.89
7.1
5.71
5.81
5.71
5.60
5.25
4.60
4.49

parameter-free, identity shortcuts help with training. next
we investigate projection shortcuts (eqn.(2)). in table 3 we
compare three options: (a) zero-padding shortcuts are used
for increasing dimensions, and all shortcuts are parameter-
free (the same as table 2 and fig. 4 right); (b) projec-
tion shortcuts are used for increasing dimensions, and other
shortcuts are identity; and (c) all shortcuts are projections.
table 3 shows that all three options are considerably bet-
ter than the plain counterpart. b is slightly better than a. we
argue that this is because the zero-padded dimensions in a
indeed have no residual learning. c is marginally better than
b, and we attribute this to the extra parameters introduced
by many (thirteen) projection shortcuts. but the small dif-
ferences among a/b/c indicate that projection shortcuts are
not essential for addressing the degradation problem. so we
do not use option c in the rest of this paper, to reduce mem-
ory/time complexity and model sizes. identity shortcuts are
particularly important for not increasing the complexity of
the bottleneck architectures that are introduced below.
deeper bottleneck architectures. next we describe our
deeper nets for id163. because of concerns on the train-
ing time that we can afford, we modify the building block
as a bottleneck design4. for each residual function f, we
use a stack of 3 layers instead of 2 (fig. 5). the three layers
are 1   1, 3   3, and 1   1 convolutions, where the 1   1 layers
are responsible for reducing and then increasing (restoring)
dimensions, leaving the 3   3 layer a bottleneck with smaller
input/output dimensions. fig. 5 shows an example, where
both designs have similar time complexity.
the parameter-free identity shortcuts are particularly im-
figure 3. example network architectures for id163. left: the
portant for the bottleneck architectures. if the identity short-

24.4
21.59
21.99
21.84
21.53
20.74
id163 test set, and won the 1st place in the ilsvrc
19.87
19.38
2015 classi   cation competition. the extremely deep rep-
resentations also have excellent generalization performance
on other recognition tasks, and lead us to further win the
1st places on: id163 detection, id163 localization,
coco detection, and coco segmentation in ilsvrc &
coco 2015 competitions. this strong evidence shows that
the residual learning principle is generic, and we expect that
it is applicable in other vision and non-vision problems.

7.32
6.66
6.8
4.94
4.82
3.57

top-5 err. (test)

3x3 conv, 512, /2

3x3 conv, 512, /2

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 256

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

3x3 conv, 512

avg pool

avg pool

pool, /2

output 

output 

fc 1000

fc 4096

fc 4096

fc 1000

fc 1000

size: 1

size: 7

our implementation for id163 follows the practice
in [21, 41]. the image is resized with its shorter side ran-
domly sampled in [256, 480] for scale augmentation [41].
a 224   224 crop is randomly sampled from an image or its
horizontal    ip, with the per-pixel mean subtracted [21]. the
standard color augmentation in [21] is used. we adopt batch
id172 (bn) [16] right after each convolution and
before activation, following [16]. we initialize the weights
as in [13] and train all plain/residual nets from scratch. we
use sgd with a mini-batch size of 256. the learning rate
starts from 0.1 and is divided by 10 when the error plateaus,
and the models are trained for up to 60    104 iterations. we
use a weight decay of 0.0001 and a momentum of 0.9. we
do not use dropout [14], following the practice in [16].

in testing, for comparison studies we adopt the standard
10-crop testing [21]. for best results, we adopt the fully-
convolutional form as in [41, 13], and average the scores
at multiple scales (images are resized such that the shorter
side is in {224, 256, 384, 480, 640}).
4. experiments
4.1. id163 classi   cation

we evaluate our method on the id163 2012 classi   -
cation dataset [36] that consists of 1000 classes. the models
are trained on the 1.28 million training images, and evalu-
ated on the 50k validation images. we also obtain a    nal
result on the 100k test images, reported by the test server.
we evaluate both top-1 and top-5 error rates.
plain networks. we    rst evaluate 18-layer and 34-layer
plain nets. the 34-layer plain net is in fig. 3 (middle). the
18-layer plain net is of a similar form. see table 1 for de-

are comparably good or better than the constructed solution

resnet reduces the top-1 error by 3.5% (table 2), resulting
from the successfully reduced training error (fig. 4 right vs.

deeper neural networks are more dif   cult to train. we
present a residual learning framework to ease the training
of networks that are substantially deeper than those used
previously. we explicitly reformulate the layers as learn-
ing residual functions with reference to the layer inputs, in-
stead of learning unreferenced functions. we provide com-
prehensive empirical evidence showing that these residual
networks are easier to optimize, and can gain accuracy from
considerably increased depth. on the id163 dataset we
evaluate residual nets with a depth of up to 152 layers   8   
deeper than vgg nets [41] but still having lower complex-
ity. an ensemble of these residual nets achieves 3.57% error
on the id163 test set. this result won the 1st place on the
ilsvrc 2015 classi   cation task. we also present analysis

the depth of representations is of central importance
for many visual recognition tasks. solely due to our ex-
tremely deep representations, we obtain a 28% relative im-
provement on the coco id164 dataset. deep
residual nets are foundations of our submissions to ilsvrc
& coco 2015 competitions1, where we also won the 1st
places on the tasks of id163 detection, id163 local-
ization, coco detection, and coco segmentation.

end	of	part	1	

