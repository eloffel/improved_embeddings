deep	learning		

unsupervised	learning	

russ	salakhutdinov	

machine learning department 
carnegie mellon university 

canadian institute for advanced research	

tutorial	roadmap	

part	1:	supervised	(discrimina>ve)	learning:	deep	
networks		

part	2:	unsupervised	learning:	deep	genera>ve	
models	

part	3:	open	research	ques>ons	

unsupervised	learning	

non-probabilis>c	models	

      sparse	coding	
      autoencoders	
      others	(e.g.	id116)	

probabilis>c	(genera>ve)	
models	

tractable	models	
      fully	observed	

belief	nets	

      nade	
      pixelid56	

non-tractable	models	
      boltzmann	machines	
      varia>onal	

autoencoders	

      helmholtz	machines	
      many	others   	

      genera>ve	adversarial	

      moment	matching	

networks	

networks	

explicit	density	p(x)	

implicit	density	

tutorial	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	genera>ve	models	

      restricted	boltzmann	machines	
      deep	boltzmann	machines	
      helmholtz	machines	/	varia>onal	autoencoders		

      	genera>ve	adversarial	networks		

tutorial	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	genera>ve	models	

      restricted	boltzmann	machines	
      deep	boltzmann	machines	
      helmholtz	machines	/	varia>onal	autoencoders		

      	genera>ve	adversarial	networks		

sparse	coding	

      	sparse	coding	(olshausen	&	field,	1996).	originally	developed	
to	explain	early	visual	processing	in	the	brain	(edge	detec>on).		
      	objec>ve:	given	a	set	of	input	data	vectors																														
learn	a	dic>onary	of	bases																																such	that:					

sparse:	mostly	zeros	

      	each	data	vector	is	represented	as	a	sparse	linear	combina>on	
of	bases.	

sparse	coding	

				natural	images	

learned	bases:		   edges   	

new	example 

= 0.8 *                   + 0.3 *                     + 0.5 * 

          + 0.5 *       	

     x 

     = 0.8 *       						         +  0.3 *        					

	[0,	0,	   	0.8,	   ,	0.3,	   ,	0.5,	   ]	=	coe   cients	(feature	representa>on)		
slide	credit:	honglak	lee	

sparse	coding:	training	

      	input	image	patches:		
      	learn	dic>onary	of	bases:	

reconstruc>on	error	

sparsity	penalty	

      	alterna>ng	op>miza>on:	

1.    fix	dic>onary	of	bases																											and	solve	for	
2.    fix	ac>va>ons	a,	op>mize	the	dic>onary	of	bases	(convex	

ac>va>ons	a	(a	standard	lasso	problem).			

qp	problem).		

sparse	coding:	tes>ng	time	

      	input:	a	new		image	patch	x*	,	and	k	learned	bases			
      	output:	sparse	representa>on	a	of	an	image	patch	x*.		

= 0.8 *                   + 0.3 *                     + 0.5 * 

          + 0.5 *       	

     x*       = 0.8 *       						         +  0.3 *        					

	[0,	0,	   	0.8,	   ,	0.3,	   ,	0.5,	   ]	=	coe   cients	(feature	representa>on)		

image	classi   ca>on	
evaluated	on	caltech101	object	category	dataset.	

classification 

algorithm 

(id166) 

input	image 

learned		
bases 

features	(coe   cients) 

	9k	images,	101	classes	

algorithm	

accuracy	

baseline	(fei-fei	et	al.,	2004)	

pca	

sparse	coding	

16%	
37%	
47%	

slide	credit:	honglak	lee	

lee,	bakle,	raina,	ng,	2006 

interpre>ng	sparse	coding	

a	

sparse	features	

g(a)	

explicit	
linear	
decoding	

x   	

a	

x	

f(x)	

implicit	
nonlinear	
encoding	

      	sparse,	over-complete	representa>on	a.	
      	encoding	a	=	f(x)	is	implicit	and	nonlinear	func>on	of	x.		
      	reconstruc>on	(or	decoding)	x   	=	g(a)	is	linear	and	explicit.		

autoencoder	

feature representation 

feed-back, 
generative, 
top-down 
path	

decoder 

encoder 

feed-forward,  
bottom-up	

input image 

      	details	of	what	goes	insider	the	encoder	and	decoder	maker!	
      	need	constraints	to	avoid	learning	an	iden>ty.		

autoencoder	

 binary features z 

decoder 
filters d 

linear 
function 
path	

dz 

z=  (wx) 

encoder 
filters w. 

sigmoid 
function	

input image x 

autoencoder	

 binary features z 

dz 

z=  (wx) 

input image x 

      	an	autoencoder	with	d	inputs,	
d	outputs,	and	k	hidden	units,	
with	k<d.		

      	given	an	input	x,	its	
reconstruc>on	is	given	by:		

decoder	

encoder	

autoencoder	

      	an	autoencoder	with	d	inputs,	
d	outputs,	and	k	hidden	units,	
with	k<d.		

 binary features z 

dz 

z=  (wx) 

input image x 

      	we	can	determine	the	network	parameters	w	and	d	by	
minimizing	the	reconstruc>on	error:		

autoencoder	

 linear features z 

wz 

z=wx 

input image x 

      		if	the	hidden	and	output	layers	
are	linear,	it	will	learn	hidden	units	
that	are	a	linear	func>on	of	the	data	
and	minimize	the	squared	error.	
      	the	k	hidden	units	will	span	the	
same	space	as	the	   rst	k	principal	
components.	the	weight	vectors	
may	not	be	orthogonal.		

      	with	nonlinear	hidden	units,	we	have	a	nonlinear	
generaliza>on	of	pca.	

another	autoencoder	model	

 binary features z 

  (wtz) 

z=  (wx) 

encoder 
filters w. 

sigmoid 
function	

decoder 
filters d 
path	

binary input x 

      	need	addi>onal	constraints	to	avoid	learning	an	iden>ty.		
      	relates	to	restricted	boltzmann	machines	(later).		

predic>ve	sparse	decomposi>on	

l1 sparsity 

decoder 
filters d 
path	

at training 
time 
path	

 binary features z 

dz 

z=  (wx) 

encoder 
filters w. 

sigmoid 
function	

real-valued input x 

decoder	

encoder	

kavukcuoglu, ranzato, fergus, lecun, 2009 

stacked	autoencoders	

class labels 

decoder 

encoder 

features 

sparsity 

decoder 

encoder 

features 

sparsity 

decoder 

encoder 

input x 

stacked	autoencoders	

class labels 

decoder 

encoder 

features 

sparsity 

decoder 

encoder 

features 

greedy	layer-wise	learning.		

decoder 

encoder 

sparsity 

input x 

stacked	autoencoders	

class labels 

      	remove	decoders	and	
use	feed-forward	part.		
      	standard,	or	
convolu>onal	neural	
network	architecture.		
      	parameters	can	be	
   ne-tuned	using	
backpropaga>on.		

encoder 

features 
encoder 

features 
encoder 

input x 

deep	autoencoders	

decoder

top
rbm

rbm

rbm

30

w

500

4

500

w
1000

3

1000
w
2000

2

2000
w

1

t
1

t
2

t
3

t
4
code layer
4

w
2000
w
1000
w

500

w

30

w

500

w
1000
w
2000
w

3

2

1

2000

1000

w +(cid:1)

t
1

w +(cid:1)

t
2

w +(cid:1)

t
3

8

7

6

500

w

30

t
4

+(cid:1)

5

w +(cid:1)

4

500

w +(cid:1)

3

1000

w +(cid:1)

2

2000

4

3

2

w +(cid:1)

1

1

rbm

encoder

pretraining

unrolling

fine(cid:1)tuning

deep	autoencoders	

      	25x25	   	2000	   	1000	   	500	   	30	autoencoder	to	extract	30-d	real-
valued	codes	for	olives	face	patches.			

      	top:	random	samples	from	the	test	dataset.			
      	middle:	reconstruc>ons	by	the	30-dimensional	deep	autoencoder.	
      	bobom:	reconstruc>ons	by	the	30-dimen>noal	pca.		

informa>on	retrieval	

interbank markets

european community 
monetary/economic  

2-d	lsa	space	

energy markets

leading          
economic         
indicators       

disasters and 
accidents     

legal/judicial

accounts/
earnings 

government 
borrowings 

      	the	reuters	corpus	volume	ii	contains	804,414	newswire	stories	
(randomly	split	into	402,207	training	and	402,207	test).	
      	   bag-of-words   	representa>on:	each	ar>cle	is	represented	as	a	vector	
containing	the	counts	of	the	most	frequently	used	2000	words.	

(hinton and salakhutdinov, science 2006)

tutorial	roadmap	

      	basic	building	blocks:	

     
sparse	coding	
      autoencoders	

      	deep	genera>ve	models	

      restricted	boltzmann	machines	
      deep	boltzmann	machines	
      helmholtz	machines	/	varia>onal	autoencoders		

      	genera>ve	adversarial	networks		

maximum likelihood

fully-visible belief net

      	explicitly	model	condi>onal	probabili>es:	

   

fully	observed	models	
       = arg max
ex   pdata log pmodel(x |    )
nyi=2

pmodel(xi | x1, . . . , xi 1)

pmodel(x) = pmodel(x1)

each	condi>onal	can	be	a	
complicated	neural	network	

      	a	number	of	successful	models,	including		
      nade,	rnade	(larochelle,	et.al.	

20011)	
pixel	id98	(van	den	ord	et.	al.	2016)	
pixel	id56	(van	den	ord	et.	al.	2016)	

     
     

pixel	id98	

restricted	boltzmann	machines	

graphical	models:	powerful	
framework	for	represen>ng	
dependency	structure	between	
random	variables.	

		hidden	variables	

pair-wise	

unary	
feature	detectors	

image						visible	variables	

rbm	is	a	markov	random	field	with:	
      	stochas>c	binary	visible	variables																										
      	stochas>c	binary	hidden	variables																							
      	bipar>te	connec>ons.	

markov	random	   elds,	boltzmann	machines,	log-linear	models.		

learning	features	

observed		data		

subset	of	25,000	characters	

learned	w:		   edges   	
subset	of	1000	features	

new	image:	

=	

sparse	
representanons	

   .	

logis>c	func>on:	suitable	for	
modeling	binary	images	

rbms	for	real-valued	&	count	data	

4	million	unlabelled	images	

learned	features	(out	of	10,000)	

reuters	dataset:	
804,414	unlabeled	
newswire	stories	
bag-of-words		

learned	features:	``topics      	

russian	
russia	
moscow	
yeltsin	
soviet	

clinton	
house	
president	
bill	
congress	

computer	
system	
product	
sovware	
develop	

trade	
country	
import	
world	
economy	

stock	
wall	
street	
point	
dow	

collabora>ve	filtering	

binary	hidden:	user	preferences	

h

w1

v

mul>nomial	visible:	user	ra>ngs	

nexlix	dataset:		
480,189	users		
17,770	movies		
over	100	million	ra>ngs	

learned	features:	``genre      	

fahrenheit	9/11	
bowling	for	columbine	
the	people	vs.	larry	flynt	
canadian	bacon	
la	dolce	vita	

independence	day	
the	day	aver	tomorrow	
con	air	
men	in	black	ii	
men	in	black	

friday	the	13th	
the	texas	chainsaw	massacre	
children	of	the	corn	
child's	play	
the	return	of	michael	myers	

scary	movie	
naked	gun		
hot	shots!	
american	pie		
police	academy	

state-of-the-art	performance		
on	the	nexlix	dataset.		

(salakhutdinov, mnih, hinton, icml 2007)

di   erent	data	modali>es	

      	binary/gaussian/sovmax	rbms:	all	have	binary	hidden	
variables	but	use	them	to	model	di   erent	kinds	of	data.	

binary	

real-valued	

1-of-k	

0	
0	
0	
1	
0	

      	it	is	easy	to	infer	the	states	of	the	hidden	variables:		

product	of	experts	

the	joint	distribu>on	is	given	by:	

marginalizing	over	hidden	variables:	

product	of	experts	

government	
authority	
power	
empire	
federa>on	

clinton	
house	
president	
bill	
congress	

bribery	
corrup>on	
dishonesty	
corrupt	
fraud	

ma   a	
business	
gang	
mob	
insider	

stock	
wall	
street	
point	
dow	

   	

silvio	berlusconi	

topics	   government   ,	   corrup>on   	
and	   ma   a   	can	combine	to	give	very	
high	id203	to	a	word	   silvio	
berlusconi   .	

product	of	experts	

the	joint	distribu>on	is	given	by:	

marginalizing	over	hidden	variables:	

50

product	of	experts	

government	
authority	
power	
empire	
federa>on	

)

%

clinton	
(
house	
 
n
o
president	
s
bill	
c
e
congress	
r
p

i

i

40

30

20

10

replicated 
softmax 50   d

bribery	
corrup>on	
dishonesty	
lda 50   d
corrupt	
fraud	

ma   a	
business	
gang	
mob	
insider	

stock	
wall	
street	
point	
dow	

   	

0.001     0.006     0.051     0.4        1.6       6.4       25.6      100 

topics	   government   ,	   corrup>on   	
and	   ma   a   	can	combine	to	give	very	
high	id203	to	a	word	   silvio	
berlusconi   .	

recall (%) 

silvio	berlusconi	

deep	boltzmann	machines	

low-level	features:	
edges	

built	from	unlabeled	inputs.		

input:	pixels	

image	

(salakhutdinov 2008, salakhutdinov & hinton 2012)

deep	boltzmann	machines	

learn	simpler	representa>ons,	
then	compose	more	complex	ones	

higher-level	features:	
combina>on	of	edges	

low-level	features:	
edges	

built	from	unlabeled	inputs.		

input:	pixels	

image	

(salakhutdinov 2008, salakhutdinov & hinton 2009)

model	formula>on	

same	as	rbms	

model	parameters	

       dependencies	between	hidden	variables.	
       all	connec>ons	are	undirected.	
       bokom-up	and	top-down:	

w3

w2

w1

h3

h2

h1

v

input	

top-down	

bokom-up	

       hidden	variables	are	dependent	even	when	condinoned	on	

the	input.	

approximate	learning	

h3

h2

h1

v

(approximate)	maximum	likelihood:	

       both	expecta>ons	are	intractable!		

w3

w2

w1

not	factorial	any	more!	

approximate	learning	

h3

h2

h1

v

w3

w2

w1

(approximate)	maximum	likelihood:	

data	

not	factorial	any	more!	

approximate	learning	

h3

h2

h1

v

w3

w2

w1

(approximate)	maximum	likelihood:	

varia>onal	
	id136	

stochas>c	
approxima>on		
(mcmc-based)	

not	factorial	any	more!	

good	genera>ve	model?	

handwriken	characters	

good	genera>ve	model?	

handwriken	characters	

good	genera>ve	model?	

handwriken	characters	

simulated	

real	data	

good	genera>ve	model?	

handwriken	characters	

real	data	

simulated	

good	genera>ve	model?	

handwriken	characters	

handwri>ng	recogni>on	

mnist	dataset	

60,000	examples	of	10	digits	
learning	algorithm	
logis>c	regression	
id92		
neural	net	(plak	2005)	
id166	(decoste	et.al.	2002)	
deep	autoencoder	
(bengio	et.	al.	2007)		
deep	belief	net	
(hinton	et.	al.	2006)		
dbm		

error	
12.0%	
3.09%	
1.53%	
1.40%	
1.40%	

0.95%	

1.20%	

op>cal	character	recogni>on	

42,152	examples	of	26	english	lekers		

learning	algorithm	
logis>c	regression	
id92		
neural	net	
id166	(larochelle	et.al.	2009)	
deep	autoencoder	
(bengio	et.	al.	2007)		
deep	belief	net	
(larochelle	et.	al.	2009)		
dbm	

error	
22.14%	
18.92%	
14.62%	
9.70%	
10.05%	

9.68%	

8.40%	

permuta>on-invariant	version.	

3-d	object	recogni>on	

norb	dataset:	24,000	examples	

learning	algorithm	
logis>c	regression	
id92	(lecun	2004)	
id166	(bengio	&	lecun		2007)	
deep	belief	net	(nair	&	hinton		
2009)		
dbm	

error	
22.5%	
18.92%	
11.6%	
9.0%	

7.2%	

pakern	
comple>on	

data	   	collec>on	of	modali>es	

      	mul>media	content	on	the	web	-	
image	+	text	+	audio.	
      	product	recommenda>on	
systems.	

car,	
automobile	

      	robo>cs	applica>ons.	

touch	sensors	

motor	control	

sunset,	
paci   cocean,	
bakerbeach,	
seashore,	ocean	

vision	

audio	

challenges	-	i		

image	

text	

sunset,	paci   c	ocean,	
baker	beach,	seashore,	

ocean	

dense	

sparse	

very	di   erent	input	
representa>ons	
      	images	   	real-valued,	dense	
      	text	   	discrete,	sparse		

di   cult	to	learn	
cross-modal	features	
from	low-level	
representa>ons.	

challenges	-	ii		

image	

text	

noisy	and	missing	data	

pentax,	k10d,	
pentaxda50200,	
kangarooisland,	sa,	
australiansealion	

mickikrimmel,	
mickipedia,	
headshot	

<	no	text>	

unseulpixel,	
naturey	

challenges	-	ii		

image	

text	

text	generated	by	the	model	

pentax,	k10d,	
pentaxda50200,	
kangarooisland,	sa,	
australiansealion	

mickikrimmel,	
mickipedia,	
headshot	

<	no	text>	

unseulpixel,	
naturey	

beach,	sea,	surf,	strand,	
shore,	wave,	seascape,	
sand,	ocean,	waves	

portrait,	girl,	woman,	lady,	
blonde,	preky,	gorgeous,	
expression,	model	

night,	noke,	tra   c,	light,	
lights,	parking,	darkness,	
lowlight,	nacht,	glow	

fall,	autumn,	trees,	leaves,	
foliage,	forest,	woods,	
branches,	path	

mul>modal	dbm	

gaussian	model	

dense,	real-valued	
image	features	

replicated	sovmax	

word	
counts	

0	
0	
0	
1	
0	

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	

mul>modal	dbm	

gaussian	model	

dense,	real-valued	
image	features	

replicated	sovmax	

word	
counts	

0	
0	
0	
1	
0	

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	

mul>modal	dbm	

gaussian	model	

dense,	real-valued	
image	features	

replicated	sovmax	

word	
counts	

0	
0	
0	
1	
0	

(srivastava & salakhutdinov, nips 2012, jmlr 2014)	

text	generated	from	images	

given

		

given

generated 	
dog,	cat,	pet,	kiken,	
puppy,	ginger,	tongue,	
kiky,	dogs,	furry	

sea,	france,	boat,	mer,	
beach,	river,	bretagne,	
plage,	brikany	

portrait,	child,	kid,	
ritrako,	kids,	children,	
boy,	cute,	boys,	italy	

generated 	

		
insect,	buker   y,	insects,	
bug,	buker   ies,	
lepidoptera	

gra   >,	streetart,	stencil,	
s>cker,	urbanart,	gra   ,	
sanfrancisco	

canada,	nature,	
sunrise,	ontario,	fog,	
mist,	bc,	morning	

genera>ng	text	from	images	

samples	drawn	aver	
every	50	steps	of	
gibbs	updates	

text	generated	from	images	

given

generated 	

		

portrait,	women,	army,	soldier,	
mother,	postcard,	soldiers	

obama,	barackobama,	elec>on,	
poli>cs,	president,	hope,	change,	
sanfrancisco,	conven>on,	rally	

water,	glass,	beer,	bokle,	
drink,	wine,	bubbles,	splash,	
drops,	drop	

images	from	text	
retrieved	

given

water,	red,	
sunset	

nature,	   ower,	
red,	green	

blue,	green,	
yellow,	colors	

chocolate,	cake	

mir-flickr	dataset	
      	1	million	images	along	with	user-assigned	tags.	

sculpture,	beauty,	
stone	

d80	

nikon,	abigfave,	
goldstaraward,	d80,	
nikond80	

food,	cupcake,	
vegan	

anawesomeshot,	
theperfectphotographer,	
   ash,	damniwishidtakenthat,	
spiritofphotography	

nikon,	green,	light,	
photoshop,	apple,	d70	

white,	yellow,	
abstract,	lines,	bus,	
graphic	

sky,	geotagged,	
re   ec>on,	cielo,	
bilbao,	re   ejo	

huiskes	et.	al.	

results	

      	logis>c	regression	on	top-level	representa>on.	
      	mul>modal	inputs	

mean	average	precision	

learning	algorithm	
random	
lda	[huiskes	et.	al.]	
id166	[huiskes	et.	al.]	
dbm-labelled	
deep	belief	net	
autoencoder	
dbm	

map	
0.124	
0.492	
0.475	
0.526	
0.638	
0.638	
0.641	

precision@50	

0.124	
0.754	
0.758	
0.791	
0.867	
0.875	
0.873	

labeled	
25k	
examples	

+	1	million	
unlabelled	

helmholtz	machines	

      	hinton,	g.	e.,	dayan,	p.,	frey,	b.	j.	and	neal,	r.,	science	1995	

      	kingma	&	welling,	2014	
      	rezende,	mohamed,	daan,	2014	
      	mnih	&	gregor,	2014		
      	bornschein	&	bengio,	2015	
      	tang	&	salakhutdinov,	2013			

genera>ve	
process	

w3

w2

w1

approximate	
id136	
h3

h2

h1

v

input	data	

helmholtz	machines	vs.	dbms	

helmholtz machine

deep id82

approximate	
id136	
h3

h2

h1

v

genera>ve	
process	

w3

w2

w1

h3

h2

h1

v

input	data	

w3

w2

w1

varia>onal	autoencoders	(vaes)		
      	the	vae	de   nes	a	genera>ve	process	in	terms	of	ancestral	
sampling	through	a	cascade	of	hidden	stochas>c	layers:		

genera>ve	
process	

each	term	may	denote	a	
complicated	nonlinear	rela>onship		

h3

h2

h1

v

w3

w2

w1

input	data	

      

      

					denotes	parameters	
of	vae.		

				is	the	number	of	
stochasnc	layers.	

       sampling	and	id203	
evalua>on	is	tractable	for	
each																						.		

vae:	example	

      	the	vae	de   nes	a	genera>ve	process	in	terms	of	ancestral	
sampling	through	a	cascade	of	hidden	stochas>c	layers:		

stochas>c	layer	

determinis>c	
layer	

stochas>c	layer	

this	term	denotes	a	one-layer	
neural	net.	

      

      

					denotes	parameters	
of	vae.		

				is	the	number	of	
stochasnc	layers.	

       sampling	and	id203	
evalua>on	is	tractable	for	
each																						.		

varia>onal	bound	

      	the	vae	is	trained	to	maximize	the	varia>onal	lower	bound:	

       trading	o   	the	data	log-likelihood	and	the	kl	divergence	

from	the	true	posterior.		

h3

h2

h1

v

w3

w2

w1

input	data	

       hard	to	op>mize	the	varia>onal	bound	
with	respect	to	the	recogni>on	network	
(high-variance).		
       key	idea	of	kingma	and	welling	is	to	use	

reparameteriza>on	trick.		

reparameteriza>on	trick	
      	assume	that	the	recogni>on	distribu>on	is	gaussian:	

				with	mean	and	covariance	computed	from	the	state	of	the	hidden	

units	at	the	previous	layer.		

       alterna>vely,	we	can	express	this	in	term	of	auxiliary	variable:			

reparameteriza>on	trick	
      	assume	that	the	recogni>on	distribu>on	is	gaussian:	

       or	

       the	recogni>on	distribu>on																										can	be	expressed	in	

terms	of	a	determinis>c	mapping:				

determinis>c	
encoder	

distribu>on	of			
does	not	depend	on	

compu>ng	the	gradients	

       the	gradient	w.r.t	the	parameters:	both	recogni>on	and	

genera>ve:	

autoencoder	

gradients	can	be	
computed	by	backprop	

the	mapping	h	is	a	determinis>c	
neural	net	for	   xed				.		

importance	weighted	autoencoders	
       can	improve	vae	by	using	following	k-sample	importance	

weigh>ng	of	the	log-likelihood:		

unnormalized	
importance	weights		

				where																								are	sampled	

from	the	recogni>on	network.	

h3

h2

h1

v

w3

w2

w1

input	data	

	burda,	grosse,	salakhutdinov,	2015	

genera>ng	images	from	cap>ons	

stochas>c	
layer	

      	genera>ve	model:	stochas>c	recurrent	network,	chained	
sequence	of	varia>onal	autoencoders,	with	a	single	stochas>c	layer.	
      	recogni>on	model:	determinis>c	recurrent	network.	
gregor	et.	al.	2015		

(mansimov,	parisoko,	ba,	salakhutdinov,	2015)		

mo>va>ng	example	

      	can	we	generate	images	from	natural	language	descrip>ons?	

a	stop	sign	is	   ying	in	
blue	skies		

a	pale	yellow	school	bus	
is	   ying	in	blue	skies		

a	herd	of	elephants	is	
   ying	in	blue	skies		

a	large	commercial	airplane	
is	   ying	in	blue	skies		

(mansimov,	parisoko,	ba,	salakhutdinov,	2015)		

flipping	colors	

a	yellow	school	bus	parked	
in	the	parking	lot	

a	red	school	bus	parked	in	
the	parking	lot	

a	green	school	bus	parked	in	
the	parking	lot	

a	blue	school	bus	parked	in	
the	parking	lot	

(mansimov,	parisoko,	ba,	salakhutdinov,	2015)		

novel	scene	composi>ons	

a	toilet	seat	sits	open	in	the	
bathroom	

a	toilet	seat	sits	open	in	the	
grass	   eld	

ask	google?	

(some)	open	problems	

       reasoning,	aken>on,	and	memory	

       natural	language	understanding	

       deep	reinforcement	learning	

       unsupervised	learning	/	transfer	learning	/	

one-shot	learning	

(some)	open	problems	

       reasoning,	aken>on,	and	memory	

       natural	language	understanding	

       deep	reinforcement	learning	

       unsupervised	learning	/	transfer	learning	/	

one-shot	learning	

who-did-what	dataset	

       document:	      arrested	illinois	governor	rod	blagojevich	and	his	

chief	of	sta   	john	harris	on	corrup>on	charges	   	included	
blogojevich	allegedly	conspiring	to	sell	or	trade	the	senate	seat	lev	
vacant	by	president-elect	barack	obama      	

       query:	president-elect	barack	obama	said	tuesday	he	was	not	

aware	of	alleged	corrup>on	by	x	who	was	arrested	on	charges	of	
trying	to	sell	obama   s	senate	seat.	

       answer:	rod	blagojevich	

onishi, wang, bansal, gimpel, mcallester, emnlp, 2016

recurrent	neural	network		

nonlinearity		

hidden	state		at	
previous	>me	step	

input	at	>me	
step	t	

h1	

h2	

h3	

x1	

x2	

x3	

gated	aken>on	mechanism		

      	use	recurrent	neural	networks	(id56s)		to	encode	a	document	
and	a	query:	

      use	element-wise	mul>plica>on	

to	model	the	interac>ons	
between		document	and	query:	

(dhingra, liu, yang, cohen, salakhutdinov, acl 2017)

mul>-hop	architecture	
      	reasoning	requires	several	passes	over	the	context	

(dhingra, liu, yang, cohen, salakhutdinov, acl 2017)

analysis	of	aken>on		

       context:	      arrested	illinois	governor	rod	blagojevich	and	his	chief	of	sta   	john	
harris	on	corrup>on	charges	   	included	blogojevich	allegedly	conspiring	to	sell	
or	trade	the	senate	seat	lev	vacant	by	president-elect	barack	obama      	
       query:	   president-elect	barack	obama	said	tuesday	he	was	not	aware	of	

alleged	corrup>on	by	x	who	was	arrested	on	charges	of	trying	to	sell	obama   s	
senate	seat.   	

       answer:	rod	blagojevich	

layer	1	

layer	2	

analysis	of	aken>on		

       context:	      arrested	illinois	governor	rod	blagojevich	and	his	chief	of	sta   	john	
harris	on	corrup>on	charges	   	included	blogojevich	allegedly	conspiring	to	sell	
or	trade	the	senate	seat	lev	vacant	by	president-elect	barack	obama      	
       query:	   president-elect	barack	obama	said	tuesday	he	was	not	aware	of	

alleged	corrup>on	by	x	who	was	arrested	on	charges	of	trying	to	sell	obama   s	
senate	seat.   	

       answer:	rod	blagojevich	

layer	1	

layer	2	

code	+	data:	hkps://github.com/bdhingra/ga-reader	

incorpora>ng	prior	knowledge 

mary 

got 

the 

football 

she 

went 

to 

the 

kitchen 

she 

left 

the 

ball 

there 

id56	
coreference	
hyper/hyponymy	

 dhingra, yang, cohen, salakhutdinov 2017

incorpora>ng	prior	knowledge 

mary 

got 

the 

football 

she 

went 

to 

the 

kitchen 

she 

left 

the 

ball 

there 

id56	
coreference	
hyper/hyponymy	

memory	as	acyclic	graph	
encoding	(mage)	-	id56	

mt
. . .

e|e|

e1

h0
h1
...
ht 1
xt

mt+1

gt

	

n
n
r

ht

 dhingra, yang, cohen, salakhutdinov 2017

incorpora>ng	prior	knowledge 

her	plain	face	broke	into	
a	huge	smile	when	she	
saw	terry.		   terry!   	she	
called	out.		she	rushed	
to	meet	him	and	they	
embraced.		   hon,	i	want	
you	to	meet	an	old	
friend,	owen	mckenna.	
owen,	please	meet	
emily.'   		she	gave	me	a	
quick	nod	and	turned	
back	to	x	

core	nlp	

coreference	
dependency	parses	

freebase	

en>ty	rela>ons	

id138	

word	rela>ons	

recurrent	neural	network	

text	representa>on	

neural	story	telling	

sample	from	the	generanve	model	
(recurrent	neural	network):	
she	was	in	love	with	him	for	the	   rst	
>me	in	months,	so	she	had	no	
inten>on	of	escaping.		

the	sun	had	risen	from	the	ocean,	making	her	feel	more	alive	than	
normal.	she	is	beau>ful,	but	the	truth	is	that	i	do	not	know	what	to	
do.	the	sun	was	just	star>ng	to	fade	away,	leaving	people	scakered	
around	the	atlan>c	ocean.		

(kiros et al., nips 2015)

(some)	open	problems	

       reasoning,	aken>on,	and	memory	

       natural	language	understanding	

       deep	reinforcement	learning	

       unsupervised	learning	/	transfer	learning	/	

one-shot	learning	

learning	behaviors	

observa>on	

ac>on	

learning	to	map	sequences	of	observa>ons	to	ac>ons,	
for	a	par>cular	goal	

reinforcement	learning	with	

memory	

learned	
external	
memory	

ac>on	

reward	

observa>on	/	state	

differentiable neural computer, graves et al., nature, 2016; 
id63, graves et al., 2014

reinforcement	learning	with	

memory	

learned	
external	
memory	

ac>on	

reward	

learning	3-d	game	
without	memory	
 chaplot, lample, aaai 2017
observa>on	/	state	

differentiable neural computer, graves et al., nature, 2016; 
id63, graves et al., 2014

deep	rl	with	memory	

learned	
structured	
memory	

ac>on	

reward	

observa>on	/	state	

parisotto, salakhutdinov, 2017

random	maze	with	indicator	

      	indicator:	either	blue	or	pink	

     	if	blue,	   nd	the	green	block	
     	if	pink,	   nd	the	red	block	

      	nega>ve	reward	if	agent	does	not	   nd	correct	
block	in	n	steps	or	goes	to	wrong	block.	

parisotto, salakhutdinov, 2017

random	maze	with	indicator	

mt

write	

mt+1

write	

read	with	
aken>on	

parisotto, salakhutdinov, 2017

random	maze	with	indicator	

building	intelligent	agents	

learned	
external	
memory	

knowledge	
base	

ac>on	

reward	

observa>on	/	state	

building	intelligent	agents	

learned	
external	
memory	

knowledge	
base	

ac>on	

reward	

learning	from	fewer	
examples,	fewer	
experiences	

observa>on	/	state	

       e   cient	learning	algorithms	for	deep	unsupervised	models	

summary	

text	&	image	retrieval	/		
object	recogninon	

image	tagging	

learning	a	category	
hierarchy	

speech	recogninon	

id48	decoder	

mosque,	tower,	
building,	cathedral,	
dome,	castle	

mulnmodal	data	

object	detecnon	

sunset,	paci   c	ocean,	
beach,	seashore	

       deep	models	improve	the	current	state-of-the	art	in	many	

applica>on	domains:	
      object	recogni>on	and	detec>on,	text	and	image	retrieval,	handwriken	

character	and	speech	recogni>on,	and	others.	

thank	you	

