 
understanding machine learning     
a theory perspective 
 

shai ben-david 

university of waterloo 

 

 
 

 

 mlss at mpi tubingen, 2017 

 

disclaimer     warning    . 
  this talk is not about how cool machine 
learning is.  
i am sure you are already convinced of that. 
i am not going to show any videos of amazing 
applications of ml.  
you will hear a lot about the great applications of 
ml throughout this mlss.  
 

i wish to focus on understanding the 

principles underlying machine learning.  

high level view of 

(statistical)  machine learning 

   the purpose of science is  
to find meaningful simplicity  

in the midst of  

disorderly complexity    

herbert simon 

more concretely 

      statistical learning is concerned with algorithms that 

detect meaningful regularities in large complex data 
sets. 

     

 we focus on data that is too complex for humans to 
figure out its meaningful regularities. 

 we consider the task of finding such regularities  from  

     
     random samples of the data population. 

a typical setting 

      imagine you want a computer program to help  
   decide which email messages are spam and  
   which are important. 
      might represent each message by features.  
     (e.g., return address, keywords, spelling, etc.) 
      train on a sample s of emails, labeled 

according to whether they are/aren   t spam. 

      goal of algorithm is to produce good prediction  
    rule (program) to classify future emails.  

the concept learning setting 

the learner is given some training sample  
 
e.g.,  

the learner   s output     
a  classification rule 

given data, some reasonable rules might 
be: 
      predict spam if [unknown and (sex or sales)] 
      predict spam if [sales + sex     known > 0]. 
      ... 

 

these kind of tasks are called classification prediction  

some typical classification 
prediction tasks 

       medical diagnosis (patient info   high/low 

risk). 

        sequence-based classifications of proteins. 

        detection of fraudulent use of credit cards. 

        stock market prediction (today   s news   

tomorrow   s market trend). 

the formal setup  
(for label prediction tasks) 

l    domain set     x 

l    label set  -  y (often {0,1}) 

l    training data     s=((x1,y1),    (xm, ym)) 

l    learner   s output     h: x    y 

data generation  
and measures of success 
     an unknown distribution d generates 
    instances (x1, x2,    ) independently. 

     an unknown function f: x    y labels them. 

     the error of a classifier h is the id203 

(over d) that it will fail, prx~d [h(x)     f(x)] 

 
 
 

 empirical risk minimization (erm) 

given a labeled sample  
                  s=((x1,y1),    (xm, ym)) 
   and some candidate classifier h, 
define the empirical error of h as  
   ls(h) =|{i : h(xi)     f(xi)}|/m 
(the proportion of sample points on which 
h errs) 
erm     find h the minimizes ls(h). 

not so simple     risk of overfitting 

l    given any training sample  
       s=((x1,y1),    (xm, ym)) 
l    let, 
         h(x)=yi if x=xi for some i     m 
             and h(x)=0 for any other x. 
l    clearly ls(h) =0. 
l    it is also pretty obvious that in many 

cases this h has high error id203. 

 the missing component      

l    leaners need of some prior knowledge 

how is learning handled in nature (1)? 
bait shyness in rats 

successful animal learning 

  the bait shyness phenomena in rats: 
   when rats encounter poisoned food, 

they learn very fast the causal 
relationship between the taste and smell 
of the food and sickness that follows a 
few hours later. 

how is learning handled in nature (2)?
pigeon superstition (skinner 1948) 

what is the source of difference? 

l    in what way are the rats    smarter    than 

the pigeons? 

bait shyness and inductive bias 

    garcia et al (1989) : 
  replace the stimulus associated with the 
poisonous baits  by making a sound when 
they taste it (rather than having a slightly 
different taste or smell).  

 
how well do the rats pick the relation of 

sickness to bait in this experiment? 

   

surprisingly (?) 

 the rats fail to detect the 
association! 
 
they do not refrain from eating 
when the same warning sound 
occurs again.  

what about    improved rats   ? 

l    why aren   t there rats that will also pay 

attention to the noise when they are 
eating? 

l    and to light, and temperature, time-of-

day, and so on? 

l    wouldn   t such    improved rats    survive 

better? 

second thoughts about our  
improved rats 

l    but then every tasting of food will be an 

   outlier    in some respect   . 

l     how will they know which tasting should 

be blamed for the sickness? 

the basic no free lunch  principle 

 
 
no learning is possible without 
applying prior knowledge. 

 
(we will phrase and prove a precise 

statement later) 

first type of prior knowledge     
hypothesis classes 

l    a hypothesis class h is a set of 

hypotheses. 

l     we re-define the erm rule by searching 

only inside such a prescribed h. 

l    ermh(s) picks a classifier h in h that 

minimizes the empirical error over 
members of h 

our first theorem  

theorem: (guaranteed success for ermh) 
   
let h be a finite class, and assume further 
that the unknown labeling rule, f, is a 
member of h. 
then for every   ,    >0,  
if m>(log(|h|) + log(1/  ))/   , 
with id203 > 1-    (over the choice of s) 
any ermh(s) hypothesis has error below   . 

proof 

all we need to apply are two basic 
id203 rules: 
1)    the id203 of the and of 

independent events is the product of 
their probabilities. 

2)    the    unions bound        the id203 of 
the or of any events is at most sum of 
their probabilities. 

not only finite classes 

l    the same holds, for example, for  the 

class h of all intervals over the real line. 

(we will see a proof of that in the 
afternoon)(cid:1)

a formal definition of learnability 

   h is pac learnable  if  
   there is a function mh : (0,1)2    n  
   and a learning algorithm a,  
   such that for every distribution d over x,  
   every   ,    >0, and every f in h,  
   for samples s of size m>mh(  ,   )  
   generated by d and labeled by f,  
                  pr[ld(a(s)) >   ] <    

more realistic setups 

relaxing the realizability assumption. 
we wish to model scenarios in which the 
learner does not have a priori knowledge 
of a class to which the true classifier 
belongs. 
 
furthermore, often the labels are not fully 
determined by the instance attributes. 

general id168s 

 our learning formalism applies well 
beyond counting classification errors. 
let z be any domain set. 
and l : h x z    r quantify the loss of a 
   model    h on an instance z. 
given a id203 distribution p over z 
let lp(h) = exz~p(l (h, z)) 
 

examples of such losses 

     the 0-1 classification loss: 

l (h, (x,y))= 0 if h(x)=y and 1 otherwise. 

 

     regression square  loss: 
   l (h, (x,y)) = (y-h(x))2 

     id116 id91 loss: 
    l (c1,    ck), z)= mini (ci    z)2 

agnostic pac learnability 

 h is agnostic pac learnable  if  
   there is a function mh : (0,1)2    n  
   and a learning algorithm a,  
   such that for every distribution p over xxy  
   and every   ,    >0,  
   for samples s of size m>mh(  ,   )  
   generated by p,  
     pr[lp(a(s)) > inf[h in h] lp(h) +   ] <    

note the different philosophy 

rather than making an    absolute    
statement that is guaranteed to hold only 
when certain assumptions are met (like 
realizability), 

   provide a weaker, relative guarantee, 
that is guaranteed to always hold. 

general empirical loss 

l    for any loss l : h x z    r as above 
   and a finite domain subset s, define the  
   empirical loss w.r.t. s=(z1,    zm) as  
   ls(h) =   i   l (h, zi)/m 
 

representative samples 

we say that a sample s is  
  - representative of a class h w.r.t. a 
distribution p  
 
if for every h in h 
     
         | ls(h)     lp(h)|<    

representative samples and erm 

why care about representative samples? 

if s is an   - representative of a class h 
w.r.t. a distribution p  
   then for every ermh(s) classifier h, 
      lp(h) < inf[h    in h] lp(h   ) + 2     

 uniform convergence property 

l    we say that a class h has the 
 uniform convergence property 
 if there is a function mh : (0,1)2    n  
 such that for every distribution p over z  
   and every   ,    >0,  
   with id203> (1-  ), samples s of size  
   m>mh(  ,   ) generated by p, are  
  - representative of h w.r.t.  p  
 

learning via uniform convergence 

corollary: 

 if a class h has the uniform convergence 
property then it is agnostic pac learnable, 
and any erm algorithm is a successful 
pac learner for it. 

finite classes enjoy unif. conv. 
l   theorem: if h is finite, then it has the 

uniform convergence property. 

l   proof: hoeffding implies unif conv for 

single h   s and then the union bound 
handles the full class. 

l   corollary: finite classes are pac learnable 

and erm suffices for that. 

do we need the restriction to an h? 

l    note that agnostic pac learning requires  
   only relative-to-h accuracy 
   ld(a(s)) < inf[h in h] ld(h) +    
 

   why restrict to some h?  

   can we have a universal learner, 

capable of competing with every function? 

for the proof we need 

the hoeffding inequality: 
let   1      .  m be random variables over 
[0,1] with a common expectation   , then 
 
pr[ |1/m   i=1   m  i       | >   ] < 2 exp(-2m  2) 
 
we will apply it for   i = l (h, zi) 

the no-free-lunch theorem 

 let a be any learning algorithm over some 
domain set x. 
let m be < |x|/2, then  
there is a distribution p over x x {0,1} 
and f:x   {0,1} such that  
1)    lp(f)=0 and  
2)    for p- samples s of size m 
 with id203 > 1/7,   lp(a(s))> 1/8 

the bias-complexity tradeoff 

corollary: any class of infinite vc 
dimension is not pac learnable     we 
cannot learn w.r.t. the universal h. 
 
for every learner lp(a(s)) can be viewed 
as the sum of  
the approximation error inf[h in h] ld(h)  
and the generalization error     the    
 

1/15/14

want to know more? 

shai's   store    deals   store    gift   certificates    help   

udenrstanding   machine   learning

en   fran  ais

search

go

all

shop   by
department

understanding machine learning: from theory to algorithms: shai shalev-shwartz, shai ben-david: 9781107057135: books - amazon.ca

hello,   shai
your   account

0

cart

wish
list

books

advanced   search

browse   subjects

new   releases

bestsellers

bargain   books

textbooks

livres   en   fran  ais

understanding   machine   learning:   from   theory   to
algorithms   [hardcover]
shai   shalev-  shwartz   (author),   shai   ben-  david   (author)

vous   voulez   voir   cette   page   en
fran  ais   ?   cliquez   ici.

quantity:    1

list   price: cdn$   62.95

price: cdn$   50.36   

you   save: cdn$   12.59   (20%)

pre-  order   price   guarantee.   learn   more.

this   title   has   not   yet   been   released.
you   may   pre-  order   it   now   and   we   will   deliver   it   to   you   when   it   arrives.
ships   from   and   sold   by   amazon.ca.   gift-  wrap   available.

   

or

sign   in   to   turn   on   1-  click   ordering.

share

save   up   to   90%   on   textbooks
hit   the   books   in   amazon.ca's   textbook   store   and   save   up
to   90%   on   used   textbooks   and   35%   on   new   textbooks.
learn   more.

see   all   1   image(s)

publisher:   learn   how   customers   can   search   inside   this

book.

tell   the   publisher!   
i'd   like   to   read   this   book   on   kindle   

don't   have   a   kindle?   get   your
kindle   here,   or   download   a   free
kindle   reading   app.   

book   description
publication   date:   may   31   2014   |   isbn-  10:   1107057132   |   isbn-  13:   978-  1107057135

machine   learning   is   one   of   the   fastest   growing   areas   of   computer   science,   with   far-  reaching   applications.   the   aim   of   this   textbook   is   to
introduce   machine   learning,   and   the   algorithmic   paradigms   it   offers,   in   a   principled   way.   the   book   provides   an   extensive   theoretical   account
of   the   fundamental   ideas   underlying   machine   learning   and   the   mathematical   derivations   that   transform   these   principles   into   practical
algorithms.   following   a   presentation   of   the   basics   of   the   field,   the   book   covers   a   wide   array   of   central   topics   that   have   not   been   addressed   by
previous   textbooks.   these   include   a   discussion   of   the   computational   complexity   of   learning   and   the   concepts   of   convexity   and   stability;  
important   algorithmic   paradigms   including   stochastic   gradient   descent,   neural   networks,   and   structured   output   learning;     and   emerging
theoretical   concepts   such   as   the   pac-  bayes   approach   and   compression-  based   bounds.   designed   for   an   advanced   undergraduate   or   beginning
graduate   course,   the   text   makes   the   fundamentals   and   algorithms   of   machine   learning   accessible   to   students   and   non-  expert   readers   in
statistics,   computer   science,   mathematics,   and   engineering.

special   offers   and   product   promotions

pre-  order   price   guarantee!   order   now   and   if   the   amazon.ca   price   decreases   between   your   order   time   and   the   end   of   the   day   of   the
release   date,   you'll   receive   the   lowest   price.   here's   how   (restrictions   apply)

customers   who   bought   this   item   also   bought

