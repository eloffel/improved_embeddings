understanding	machine	learning	   	

a	theory	perspec6ve	

	

	

	
	

	
	

(part	3)	
shai	ben-david	

	

university	of	waterloo	

	mlss	at	mpi	tubingen,	2017	

the	fundamental	theorem	

(qualita6ve)	

theorem:	given	a	class	h	of	binary	valued	
func6ons	the	following	statements	are	
equivalent:	
a.    h	has	the	uniform	convergence	property	
b.    erm	is	an	agnos6c	pac	learner	for	h	
c.    h	is	agnos6c	pac	learnable	
d.    h	is	pac	learnable	
e.    vcdim(h)	is	   nite	

main	tool	for	(e)	implies	(a)	

the	sha6er	func:on	
for	a	class	h	de   ne	a	func6on	  h:	n	  n	
as		  h(m)	=	max{a:	|a|=m}|{h|a	:	h	in	h}|		
	
some	basic	proper6es	of	the	shaver	func6on:	
1.    for	every	m	   	vcdim(h),	  h(m)	=2m	
2.    for	every	m	>	vcdim(h),	  h(m)	<	2m	
	

the	sauer	(shelah,	perles)	lemma	

for	every	class	h	of	   nite	vc-dimension,	d,	
		
for	every	m,		
	
									  h(m)	   	  i=0
	
	

d		(m	choose	i)	   	md	

quan6ta6ve	version	of	the		

fundamental	theorem	

h(  ,	  )	<	c1(d+log(1/  ))/  2		

for	some	constants	c1,	c2,	for	every	d	and	every	class	h	
of	binary	valued	func6ons	such	that	vcdim(h)=d,		
1.		h	has	uniform	convergence	property	with	
					c1(d+log(1/  ))/  2	<	muc
	
	2.	h	is	agnos:c	pac	learnable	with	
					c1(d+log(1/  ))/  2	<	mh(  ,	  )	<	c1(d+log(1/  ))/  2		
	
	3.		h	is	pac	learnable	with	
				c1(d+log(1/  ))/  	<	mh(  ,	  )	<	c1(d+log(1/  ))/  		
	

how	to	compute	vc	dimension	

as	a	rule	of	thumb,	the	vc	dimension	of	a	class	
is	ofen	equal	to	the	number	of	parameters	
need	to	be	set	to	specify	a	speci   c	h	in	h.	
	
(think	of	hinit,	hintrvals,	hrectangles,	hsn		)	

is	the	story	complete	now?	

       issue	1	   	   nite	vc	classes	may	be	too	
restricted.	
       issue	2	   	computa6onal	complexity	

non-uniform	learn	-	de   ni6on	

a	class	h	is	nonuniformly	learnable	if	
		there	is	a	func4on	mh	:	hx(0,1)2	  	n		
			and	a	learning	algorithm	a,		
			s.t.	for	every	distribu4on	p	over	xxy		
			and	every	  ,	  	>0,	for	every	h	in	h	
			for	samples	s	of	size	m>mh(h,  ,	  )		
			generated	i.i.d.by	p,		
					pr[lp(a(s))	>	lp(h)	+	  ]	<	  	

non-uni	characteriza6on	-	statement	

theorem:	a	class	h	is	nonuniformly	learnable	if	
and	only	if	there	exist	classes	
{hn	:	n	in	n}	such	that:	
	
1.	each	hn	has	the	uniform	convergence	
property.		
and,	
2.	h	=	un	in	n	hn	
		
	

some	nonuni	learnable	classes	

     the	class	of	all	polynomials	epi-sets	
				h={hp:	p	a	polynonial	in	x}	
				where,	hp(x)	=	1	if	and	only	if	p(x)>0.	
     the	class	of	all	(characteris6c	func6ons	of)	

   nite	subsets	of	(any)	x.	

     the	class	of	all	   nite	unions	of	rectangles.	

some	classes	are	not	nul	

if	h	shavers	an	in   nite	set,	then	h	is	not	(even)	
nonuni	learnable.	

(in	par6cular,	the	class	of	all	func6ons	over	any	
in   nite	domain).	

proof	of	easy	direc6on	

assume	h	is	nonuni	learnable,	
	
de   ne,	for	every	n,		
hn	=	{h	in	h	:	mh	(h,	1/7,	1/8)	<	n}	
	
note	that	each	of	these	classes	must	have	   nite	
vcdim,	and	therefore	has	uniform	convergence,	
and	their	union	covers	h.	

hard	direc6on	

step	1:	weight	func6ons.	
	
we	de   ne	a	weight	func4on	to	be	any	func6on	
w	:n	  [0,1]	such	that		
  n	w(n)	   	1.	
	
examples:		w(n)	=	1/2n2			
																					or	w(n)	=	1/2n	

rewri6ng	the	m	func6on	

	given	a	class	h	and	a	representa6on	of	h	os	a	
union	of	hn   s,	each	enjoying	uniform	
convergence,	de   ne	for	any	n	
	
  n(m,	  )=min{  :	mhn(  ,	  )	<	m}	
	
(namely,	the	minimal	error	that	an	m-size	
sample	can	guarantee)	

hard	direc6on	

the	nonuniform	generaliza;on	(loss)	bound	
for	every	weight	func6on	w,	every	prob.	dist	p	
every	  	and	every	m,	with	id203		
>	(1-  ),	
for	all	h	in	h	
	
lp(h)	   	ls(h)	+	min{n:	h	in	hn}  n	(m,	w(n)	  )	

hard	direc6on	

the	bound	minimiza6on	algorithm	   	
structural	risk	minimiza6on	(srm):	
given	h,	a	decomposi6on	of	h	to	hn   s	of	   nite	
vcdim	each	and	a	weight	func6on	w,	
on	a	labeled	training	sample	s	of	size	m,	
find	h	in	h	that	minimizes	the	above	error	
bound:		
ls(h)	+	min{n:	h	in	hn}  n	(m,	w(n)	  )	
	
	

hard	direc6on	

the	resul6ng	sample	complexity	func6on:	
	
m(h,  ,  )	=	muc
	

hn(h)	(  /2,	w(n(h))	  )	

applica6ons	of	srm	

srm	has	many	applica6ons,	usually	referred	to	
as	   erm	with	regulariza6on   :	
	
adding	to	the	empirical	error	a	   penalty   	on	
complex	(or	otherwise,	undesirable)	h   s.		
example	include	
1.    norm	of	a	linear	classi   er	
2.    descrip6on	length	
3.    small	margins	
4.    low	prior	likelihood	
	

descrip6on	length	-	de   ni6on	

a	descrip6on	language	for	a	class	h	is	a	func6on	
	
g:	h	  finite	binary	strings	
	
such	that	the	range	of	g	is	pre   x-free.	

kraf	inequality	

any	collec6on	t	of	binary	strings	that	is	pre   x-free,	
sa6s   es	
	
    	in	t		2-|  |	   	1	
	
corollary:	for	h={h1,	h2,	   hn,   },	
we	can	use	any	descrip6on	language	for	
h	to	de   ne	weights	w(n)	=	2-|g(hn)|			

descrip6on	length	bound	and		

ocam   s	razor	

	
	
the	resul6ng	srm	algorithm	is	:	
pick	h	that	minimizes		
	
ls(h)	+	sqrt{(|g(h)|	+	ln(1/  ))/2m}	

