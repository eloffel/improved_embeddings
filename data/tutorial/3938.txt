submodularityand	
machine	learning

mlss t  bingen, june 2017

stefanie jegelka

mit

slides: people.csail.mit.edu/stefje/mlss/tuebingen2017.pdf
papers etc:  people.csail.mit.edu/stefje/mlss/literature.pdf

set	functions

ground set

v =

f : 2v ! r
f

(

) =

cost of buying items 
together, or
utility, or
id203,    

machine	learning

training examples

learn model

prediction

f (x, w)

f(

,   w) = train

f(

,   w) =

likely
awakening
effect

machine	learning

training examples

learn model

prediction

f (x, w)

f(

,   w) = train

informative	subsets

    compression
    summarization

    placing	sensors
    designing	experiments

f (s) =    information   

machine	learning

training examples

learn model

prediction

f (x, w)

f(

,   w) = train

variable	(coordinate)	selection

only use few coordinates of x in 

f (x, w)

f (x, w) =

wixi

dxi=1

f (s) =    coherence   

machine	learning

training examples

learn model

prediction

f (x, w)

f(

,   w) = train

summarization	&	recommendation

f (s) = relevance + diversity or coverage

machine	learning

training examples

learn model

prediction

f (x, w)

f(

,   w) = train

machine	learning	and	set	functions

common formalization: find a set s that 
maximizes / minimizes a set function f(s)

    difficult:		2100 possible	subsets	for	just	100	items	 l
    this	is	large!
fold	a	sheet	of	paper	100x.	height	of	the	final	pile:
2100x	0.1mm	= 13.4	billion	light	years!

machine	learning	and	set	functions

common formalization: find a set s that 
maximizes / minimizes a set function f(s)

    difficult:		2100 possible	subsets	for	just	100	items	 l

    special	properties	help!		(   10cm   ) j
submodularity

roadmap

    what	is	submodularity and	where	does	it	come	up?

    optimization	with	submodular	functions

    further	connections	&	directions

sensing

=	all	possible	locations

v
f(s)	=	information	gained	from	locations	in	s

14

marginal	gain

    given set function

    marginal gain:

f : 2v ! r
f (s|a) = f (a [ {s})   f (a)

1

2

s

new sensor s

15

diminishing	gains

placement	a	=	{1,2}

1

2

s

big	gain

placement	b	=	{1,2,3,4}

1

3

2

s

4

small	gain

for	all
a     b
and	s	not	in	b

a

b

f (a [ s)   f (a)

 

f (b [ s)   f (b)

diminishing	marginal	costs

a     b

a

|{z}

f (a [ s)   f (a)

.

b

| {z }

  f (b [ s)   f (b)

extra	cost:	
one	drink

extra	cost:	
free	refill	j

submodularset	functions

    diminishing	gains:		for	all

a

+    e

a     b

b

+    e

f (a [ e)   f (a)   f (b [ e)   f (b)

    union-intersection:		for	all	

s, t     v

f (s) + f (t )

  f (s [ t ) + f (s \ t )

example:	cover

f (s) =     [v2s

area(v)     

f (a [ v)   f (a)

 

f (b [ v)   f (b)

example:	sensing

x1

    =	random	variables	we	can	possibly	observe
    utility	to	have	sensors	in	locations	a:

f (a) = h(y)   h(y | xa)

uncertainty about 
temperature
before sensing

uncertainty about 
temperature
after sensing

= i(y; xa)
mutual
information

example:	id178

x1, . . . , xn

discrete	random	variables

f (s) = h(xs) = joint id178 of variables indexed by s

a     b
h(xa[e)   h(xa) = h(xe|xa)
    h(xe|xb)
= h(xb[e)   h(xb)

exercise:	meaning	of	diminishing	returns	here?

   information	never	hurts   

discrete	id178	is	submodular!

example:	id178

x1, . . . , xn

discrete	random	variables

f (s) = h(xs) = joint id178 of variables indexed by s

a     b
h(xa[e)   h(xa) = h(xe|xa)
    h(xe|xb)
= h(xb[e)   h(xb)

   information	never	hurts   

discrete	id178	is	submodular!

recommendation	&	summarization

we want:
relevance & coverage 
diversity
??
personalization

if you bought this,
you may want 
to add    

what	could	f(s)	be?
f (s) =xa2v

max
b2s

sa,b

s

sa,b

b

a

f (s) =xj q|s \ pj|

p2

p1

p3

example:	graph	cuts

cut	for	one	edge:

u

v

f (s) = xu2s,v /2s

wuv

f ({u}) + f ({v})

 

f ({u, v}) + f (;)

u

v

u

v

u

v

u

v

wuv

wuv

0

0

large	graph:		sum	of	edges

    cut	of	one	edge	is	submodular!
   
sum	of	submodular	functions	is	submodular

examples	of	submodular	functions

    discrete	id178
    mutual	information

    matrix	rank	(as	a	function	of	columns)

    coverage
    spread	in	social	networks

    graph	cuts
        many	others!

submodular	functions	(almost)	everywhere!

submodular functions, matroids, and certain

polyhedra   

jack edmonds

national bureau of standards, washington, d.c., u.s.a.

i

the viewpoint of the subject of matroids, and related areas of lattice theory,
has always been, in one way or another, abstraction of algebraic dependence or,
equivalently, abstraction of the incidence relations in geometric representations
of algebra. often one of the main derived facts is that all bases have the same
cardinality. (see van der waerden, section 33.)

cores  of  convex  games  1) 

by lloyd  s.  shapley2) 

from the viewpoint of mathematical programming, the equal cardinality
of all bases has special meaning     namely, that every basis is an optimum-
cardinality basis. we are thus prompted to study this simple property in the
context of id135.

submodular functions  and  convexity 
it turns out to be useful to regard    pure matroid theory   , which is only
incidentally related to the aspects of algebra which it abstracts, as the study of
certain classes of convex polyhedra.

l.  lovasz 
eotvos  lonind  university,  department of analysis  i,  muzeum krt.  6-8,  h-i088 
a matroid m = (e, f ) can be de   ned as a    nite set e and a nonempty
budapest,  hungary 

(1)
family f of so-called independent subsets of e such that

abstract:  the core of an n-person game is the set of feasible outcomes that cannot be improved upon 
by any coalition  of players.  a convex game is defined as one that is based  on  a  convex set function. 
in  this  paper  it  is shown that  the  core  of a  convex game is not  empty  and  that  it  has an  especially 
regular structure. it is further shown that certain other cooperative solution concepts are related in a 
simple way to the core: the value of a convex game is the center of gravity of the extreme points of the 
core, and the yon neumann-morgenstern stable set solution of a convex game is unique and coincides 

(a) every subset of an independent set is independent, and
(b) for every a     e, every maximal independent subset of a, i.e., every basis
o.  introduction 
of a, has the same cardinality, called the rank, r(a), of a (with respect
to m).
in "continuous" optimization convex functions playa central role. besides ele-
mentary tools like differentiation, various methods for finding the minimum of 
a  convex  function  constitute  the  main  body  of nonlinear  optimization.  but 
let re denote the space of real-valued vectors x = [xj], j     e.
even id135 may be viewed as the optimization of very special (li-

(this de   nition is not standard. it is prompted by the present interest).

(2)

submodular	functions	(almost)	everywhere!

graph	
theory
(frank	1993)

electrical	
networks
(narayanan	

1997)

submodular
functions

information

theory

(fujishige 1978	)

game	
theory

(shapley	1970)

matroid
theory

(whitney,	1935) stochastic	
processes
(macchi 1975,	
borodin	2003)

machine	
learning

l.	lov  sz

why	are	convex	functions	so	important?	(lov  sz,	1983)

       occur	in	many	models in	economy,	engineering	and	other	
sciences   ,	   often	the	only	nontrivial	property	that	can	be	stated	
in	general   
    preserved under	many	operations	and	transformations:	larger	
effective	range	of	results
    sufficient	structure	for	a	   mathematically	beautiful	and	
practically	useful	theory   
    efficient	minimization

   it	is	less	apparent,	but	we	claim	and	hope	to	prove	to	a	certain	
extent,	that	a	similar	role	is	played	in	discrete	optimization	by	
submodular	set-functions   	[   ]	
they	share	the	above	four	properties.

submodularity   

discrete	convexity	   .
convex	relaxation,
duality

   	or	concavity?

diminishing	   derivative   

30

roadmap

  what	is	submodularity and	where	does	it	comes	up?

    optimization	with	submodular	functions

    further	connections	&	directions

monotonicity

if s     t then f (s)     f (t )

3

5

1

maximizing	a	submodular	function?

max

s

f (s) s.t. |s|     k

np-hard	l

maximizing	a	submodular	function?

max

s

f (s) s.t. |s|     k

greedy algorithm:
s0 = ;
for i = 0,    , k-1

e    = arg max
e2v\si
si+1 = si [ {e   }

f (si [ {e})

algorithm 1 algorithm for pruning poor human-generated summaries.
require: con   dence level p, human summaries s, number of random summaries n

sample n uniformly at random size-10 image sets, to be used as summaries r = (r1, . . . , rn )
instantiate v-id8-score rs (  ) instantiated with summaries s
|r|pr2r 1{rs (r)>mins2s rs (s)} // fraction of random summaries better than worst human
o   1
while o > p do
s   s \ (argmins2s rs (s))
re-instantiate v-id8 score rs (  ) using updated pruned human summaries s.
recompute overlap o as above, but with updated v-id8 score.

s

end while
return pruned human summaries s

how    good    is          ?

sk

figure 1: three example 10   10 image collections from our new data set.

section 4 using features described in section 5. for weight optimization, we used adagrad [6], an
adaptive subgradient method allowing for informative gradient-based learning. we do 20 passes

how	good	is	greedy?	

empirically:

	

i

n
a
g
n
o
i
t
a
m
r
o
f
n

i

optimal

greedy

50

office

office

54

49

47

45

44

52

53

co nferenc e

st orage

elec

co py

51

48

46

ki tchen

39

37

43

40

9

7

4

10

6

3

1

33

8

5

2

35

11

lab

se rve r

31

12

quiet

ph one

13

14

15

18

19

21

16

17

20

22

23

29

27

42

41

38

36

34

32

30

28

26

25

24

theorem (nemhauser, wolsey, fisher 1978):
if f is monotone submodular, then 
greedy is guaranteed to achieve at least 
63% of optimum:

f (sk)     1  

1

e    f (s   )

why is this amazing?
does it always work?

greedy	can	fail	   without	submodularity

but: this never
happens with
diminishing 
returns! j

if s =

f (s) = 100
then                         .
otherwise, 

f (s) = 0

recap:	why	does	plain	greedy	work?

1. submodularity:	global	information	from	local	information

marginal	gain	of	single	item	gives	information	about	global	
value

2. monotonicity:	items	can	never	harm	(=	reduce	f)

beyond	greedy?		

    other	constraints?

    non-monotone	functions?

    large-scale	greedy?

greedy++

more	complex	constraints:			budget

1.
2.

run	greedy:
run	a	modified	greedy:

sgr

max f (s) s.t. xe2s

c(e)     b

smod
f (si [ {e})   f (si)

c(e)

e    = arg max

e

3. pick	better	of								,													
sgr smod

   approximation	factor:

1
pe

1  

even	better	but	less	fast:
partial	enumeration
(sviridenko,	   04) or
filtering (badanidiyuru &
vondr  k    14)

(leskovec-krause-guestrin-faloutsos-vanbriesen-glance	   07)

relax:	discrete	to	continuous

f (s)

max
s2i

max

x2conv(i)

fm (x)

1
0.8
0.6
f(x)
0.4
0.2
0
1

0.5

xb

0

0

0.5

xa

1

1

0.8

0.6

0.4

0.2

0
1

0.5

0

0

p

0.5

1

algorithm:	   continuous	greedy   

1. approximately	maximize	fm over	
2.

round	to	discrete	set

p = conv(i)

(vondr  k   08;	calinescu-chekuri-pal-vondr  k    11;	kulik-shachnai-tamir   11)

beyond	greedy?		greedy++

    other	constraints	for	monotone	submodular	functions?
variants	of	greedy	still	work	in	many	cases	(   downward	
closed   	constraints)

    non-monotone	functions?

    large-scale	greedy?

greedy	can	fail	   

greedy	solution:
f (a) = 40

f (a) =     [a2a

area(a)       xa2a

optimal	solution
f (a) = 95

c(a)

sensor	1

sensor	2

sensor	3

sensor	4

coverage:	100
cost:										-60
gain												40

coverage:			30
cost:										- 1
gain												29

coverage:			30
cost:										- 1
gain												29

coverage:			40
cost:										- 3
gain												37

non-monotone	maximization

    generally	inapproximable unless	f	is	nonnegative
    unconstrained	maximization:

    local	search	(feige-mirrokni-vondr  k   07)
    double	greedy:	optimal	  	approximation	
(buchbinder-feldman-naor-schwartz   12)

    constrained	maximization:	

    cardinality	constraints:	randomized	greedy	
(buchbinder-feldman-naor-schwartz   14)

    filtering	based	algorithms	(mirzasoleiman-badanidiyuru-karbasi   16)
    more	general	constraints:	continuous	local	search	via	multilinear	extension	

(chekuri   vondr  k-zenklusen   11)

    distributed	algorithms?	yes!

    divide-and-conquer	(de	ponte	barbosa-ene-nguyen-ward	   15)
    concurrency	control	/	hogwild (pan-jegelka-gonzalez-bradley-jordan	   14)

beyond	greedy?		greedy++

    other	constraints	for	monotone	submodular	functions?
variants	of	greedy	still	work	in	many	cases	(   downward	
closed   	constraints)

    non-monotone	functions?
monotone	greedy	can	fail,	but	other	types	of	greedy	(   double	
greedy   )	&	local	search	work

    large-scale	greedy?

distributed	greedy	algorithms

greedy	is	sequential.
pick	in	parallel??

pick	k	elements	
on	each	machine.

combine	and	run
greedy	again.

is	this	useful?

distributed	greedy	algorithms

pick	in	parallel
from	m machines

is	this	useful?

pick	the	best	of	m+1	solutions

random	partition:

1

2 (1   1
e )

even	better	with	
geometric	structure

for	any	partition:
min{pk,m}

1

(mirzasoleiman-karbasi-sarkar-krause   13,	da	ponte	barbosa-ene-nguyen-ward   15)

beyond	greedy?		greedy++

    other	constraints	for	monotone	submodular	functions?
variants	of	greedy	still	work	in	many	cases	(   downward	
closed   	constraints)

    non-monotone	functions?
monotone	greedy	can	fail,	but	other	types	of	greedy	(   double	
greedy   )	&	local	search	work

    large-scale	greedy?
distributed,	parallel,	streaming	versions	for	many	cases

roadmap

  what	is	submodularity and	where	does	it	comes	up?

    optimization	with	submodular	functions

  maximization:	greedy	algorithms	(diminishing	returns)
    minimization?

    further	connections	&	directions

submodular	minimization

f (s)

min
s   v

   maximize	coherence   

idea:	relaxation

f ({b})

1
0.8
0.6
f(x)
0.4
0.2
0
1

f ({a, b})

0.5

xb

0

0

0.5

xa

f ({a})

1

min

x2{0,1}n

f (x)

min
x2[0,1]n

f (x)

lovaszextension

    expectation:

    sample	threshold																			uniformly
   

   

e.g.     = 0.4

0.5
1.0
0.5
0.2
0.2

each	coordinate
corresponds	to	an	item

lov  szextension:	example

f({b})

)
x
(
f

1
0.8
0.6
0.4
0.2
0
1

f({a,b})

0.5

xb

f({})
0

0

0.5

xa

a
{}
{a}
{b}
{a,b}

f(a)
0
1
.8
.2

f({a})

1

51

submodularityand	convexity

f (x) = e      x[f (s   )]

if	f is	submodular,	this	is	equivalent	to:

f (x) = max
y2bf

y>x

1
0.8
0.6
f(x)
0.4
0.2
0
1

0.5

xb

0

0

0.5

xa

1

theorem (edmonds 1971, lov  sz 1983)
lov  sz extension	is	convex

f is	submodular.

,

examples	of	lovaszextensions

1.

f (s) = min{|s|, 1}
f (x) = max

xi

i

   

0.5
1.0
0.5
0.2
0.2

2. cut	function:	2	items	(nodes)

u

v

f (s) =(1

0

if |s| = 1
otherwise.

f (x) = |xu   xv|

base	polytopes

f (x) = e      x[f (s   )]

if	f is	submodular,	this	is	equivalent	to:

1
0.8
0.6
f(x)
0.4
0.2
0
1

0.5

xb

0

0

0.5

xa

1

f (x) = max
y2bf

y>x

3s

b(f)

p(f)

s2

base	polytope:	all	vectors	dominated	by	f(s)

s1

bf = {y 2 rn | 8s     v xi2s

yi     f (s) and

nxi=1

yi = f (v)}

examples	of	base	polytopes

1. id203	simplex
32

f (s) = min{|s|, 1}

2. permutahedron

f (s) =

|s|xi=1

(n   i + 1)

ii. submodular  systems  and base  polyhedra

putting	things	together

0.5
1.0
0.5
0.2
0.2

1
0.8
0.6
f(x)
0.4
0.2
0
1

f (s)

=

min
s   v

min

x2{0,1}n

f (x)

relaxation:	convex	optimization	
computable	subgradients

1.

2.

0.5

xb

=

1

0

0

0.5

xa

min
x2[0,1]n

f (x)

many	ways	to	do	step	1

relaxation	is	exact!
pick	elements	with	positive	coordinates

s    = {e | x   e > 0}

   submodular	minimization	in	polynomial	time!
(gr  tschel,	lov  sz,	schrijver 1981)

submodularminimization

combinatorial	methods
    first	polynomial-time:
(schrijver 00,	iwata-fleischer-fujishige-01)
   
o(n4t + n5 log m )
o(n6 + n5t )

(iwata	03)
(orlin 09)

   

convex	optimization
    ellipsoid	method
(gr  tschel-lovasz-schrijver 81)

    subgradient method	   
(   ,	chakrabarty-lee-sidford-wong	16)

    minimum-norm	point	/	fujishige-
wolfe	algorithm (different	
relaxation)
(fujishige-isotani 11)
       

latest:

o(n2t log nm + n3 logc nm )
o(n3t log2 n + n4 logc n)

(lee-sidford-wong	15)

submodularityand	convexity

    convex	lovasz extension

    easy	to	compute:	greedy	algorithm	(special	polyhedra!)

    submodular	minimization	via	convex	optimization:	exact
    duality	results

    structured	sparsity	(bach	10)
    decomposition	&	parallel	algorithms	
(komodakis-paragios-tziritas 11,	stobbe-krause	10,	jegelka-bach-sra 13,	
nishihara-jegelka-jordan	14,	ene-nguyen	15)
    variational id136	(djolonga-krause	14)
       

roadmap

  what	is	submodularity and	where	does	it	comes	up?

  optimization	with	submodular	functions

    maximization:	greedy	algorithms	(discrete	concavity)
constraints	manageable
    minimization:	convex	relaxation	(discrete	convexity)
constraints	are	hard

    further	connections	&	directions

    learning
    id203	distributions	&	set	functions
    integer	&	continuous	functions

log-supermodulardistributions

example:	ferromagnetic	ising model	/	conditional	random	field

sky

house

grass

0

0

0

0

0

0

0

0

0

1

1

1

0

1

1

1

   multivariate	totally	positive	of	order	2   ,	   affiliated   
benefits:
   
    approximating	partition	function	&	marginals    

finding	the	mode	=	minimizing	a	submodular	function

(fortuin-kasteleyn-ginibre 71,	kolmogorov-zabih 04,	djolonga-krause	14,	   )

log-submodular	distributions

p (s) / exp(f (s))
example:	determinantal point	processes	/	volume	sampling

sub-family:	   strongly	rayleigh   	distributions
benefits:	sampling	
(if	negative	association)

s

s

l

(macchi 75,	feder-mihail 82,	borodin	02,	deshpande-rademacher-vempala-wang	06,	borcea-br  nden 09,	
borcea-br  nden-liggett	09,	kulesza-taskar 12,	anari-oveis gharan-rezaei 16,	li-jegelka-sra 16,	   )

submodularitymore	generally

    integer	and	continuous	functions

    many	optimization	results	generalize	j

(milgrom-shannon	94;	topkis 98;	murota 03;	kapralov-post-vondrak 10;	soma	et	al	2014-16;	bach	2015;	
ene &	nguyen	2016;	bian-mirzasoleiman-buhmann-krause	16)	

submodularitymore	generally

    integer	and	continuous	functions

    equivalent	condition	for	differentiable	functions:

@2

@xi@xj

f (x)     0 8i 6= j

    subclass: diminishing	returns

@2

@xi@xj

f (x)     0 8i, j

    0

    0

    0

application:	robust	optimization

infer    from data. 
robust optimization?

nonconvex in    l   
but: submodular in    ! j   

y

(staib &	jegelka,	icml	2017)

nonconvex	optimization
lattice	/	continuous	submodularity
many	optimization	results	
generalize

id203	measures
log-supermodular (			positive	assoc.)
log-submodular			(			negative	assoc.)
sampling,	mode,	
approx.	partition	function

submodular	set	functions

convexity:	
minimization
maximize	coherence

dim.	returns	(concavity):	
maximization
maximize	diversity

many	examples:
    linear/modular	functions													
    id178
    mutual	information
    rank	functions

    coverage
    diffusion	in	networks	
    volume			
    graph	cut	   

