a tutorial for id23

abhijit gosavi

department of engineering management and systems engineering

missouri university of science and technology
219 engineering management, rolla, mo 65409

email:gosavia@mst.edu

february 11, 2017

if you    nd this tutorial useful, or the codes in c and matlab at

http://web.mst.edu/~gosavia/bookcodes.html

useful, please do cite my book (for which this material was prepared), now in its second
edition:

a. gosavi. simulation-based optimization: parametric optimization techniques and re-
inforcement learning, springer, new york, ny, second edition, 2014.

book website: http://web.mst.edu/  gosavia/book.html

1

contents

1 introduction

2 mdps and smdps

3 id23

3.1 average reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 selecting the appropriate learning rate or step size . . . . . . . . . . . . . . .
3.3 discounted reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 codes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 mdp example

4.1 average reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 discounted reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 conclusions

3

3

7
7
10
10
11

12
12
12

13

2

1 introduction

the tutorial is written for those who would like an introduction to id23
(rl). the aim is to provide an intuitive presentation of the ideas rather than concentrate
on the deeper mathematics underlying the topic.

rl is generally used to solve the so-called markov decision problem (mdp). in other
words, the problem that you are attempting to solve with rl should be an mdp or its
variant. the theory of rl relies on id145 (dp) and arti   cial intelligence
(ai). we will begin with a quick description of mdps. we will discuss what we mean by
   complex    and    large-scale    mdps. then we will explain why rl is needed to solve complex
and large-scale mdps. the semi-markov decision problem (smdp) will also be covered.

the tutorial is meant to serve as an introduction to these topics and is based mostly on
the book:    simulation-based optimization: parametric optimization techniques and rein-
forcement learning    [3]. the book discusses this topic in greater detail in the context of
simulators. there are at least two other textbooks that i would recommend you to read:
(i) neuro-id145 [1] (lots of details on convergence analysis) and (ii) rein-
forcement learning: an introduction [8] (lots of details on underlying ai concepts). a more
recent tutorial on this topic is [7]. this tutorial has 2 sections:

(cid:15) section 2 discusses mdps and smdps.
(cid:15) section 3 discusses rl.

by the end of this tutorial, you should be able to
(cid:15) identify problem structures that can be set up as mdps / smdps.
(cid:15) use some rl algorithms.

we will not discuss how to use function approximation, but will provide some general

advice towards the end.

2 mdps and smdps

the framework of the mdp has the following elements: (1) state of the system, (2) actions,
(3) transition probabilities, (4) transition rewards, (5) a policy, and (6) a performance metric.
we assume that the system is modeled by a so-called abstract stochastic process called the
markov chain. when we observe the system, we observe its markov chain, which is de   ned
by the states. we explain these ideas in more detail below.

state: the    state    of a system is a parameter or a set of parameters that can be used to
describe a system. for example the geographical coordinates of a robot can be used to
describe its    state.    a system whose state changes with time is called a dynamic system.
then it is not hard to see why a moving robot produces a dynamic system.

another example of a dynamic system is the queue that forms in a supermarket in front
of the counter. imagine that the state of the queuing system is de   ned by the number of

3

people in the queue. then, it should be clear that the state    uctuates with time, and then
this is dynamic system.

it is to be understood that the transition from one state to another in an mdp is usually
a random a   air. consider a queue in which there is one server and one waiting line. in this
queue, the state x, de   ned by the number of people in the queue, transitions to x + 1 with
some id203 and to x(cid:0) 1 with the remaining id203. the former type of transition
occurs when a new customer arrives, while the latter event occurs when one customer departs
from the system because of service completion.

actions: now, usually, the motion of the robot can be controlled, and in fact we are interested
in controlling it in an optimal manner. assume that the robot can move in discrete steps,
and that after every step the robot takes, it can go north, go south, go east, or go west.
these four options are called actions or controls allowed for the robot.

for the queuing system discussed above, an action could be as follows: when the number
of customers in a line exceeds some pre   xed number, (say 10), the remaining customers are
diverted to a new counter that is opened. hence, two actions for this system can be described
as: (1) open a new counter (2) do not open a new counter.

transition id203: assume that action a is selected in state i. let the next state be j.
let p(i, a, j) denote the id203 of going from state i to state j under the in   uence of
action a in one step. this quantity is also called the transition id203. if an mdp has
3 states and 2 actions, there are 9 transition probabilities per action.

immediate rewards: usually, the system receives an immediate reward (which could be
positive or negative) when it transitions from one state to another. this is denoted by
r(i, a, j).

policy: the policy de   nes the action to be chosen in every state visited by the system. note
that in some states, no actions are to be chosen. states in which decisions are to be made,
i.e., actions are to be chosen, are called decision-making states. in this tutorial, by states,
we will mean decision-making states.

performance metric: associated with any given policy, there exists a so-called performance
metric     with which the performance of the policy is judged. our goal is to select the policy
that has the best performance metric. we will    rst consider the metric called the average
reward of a policy. we will later discuss the metric called average reward. we will assume
that the system is run for a long time and that we are interested in a metric measured over
what is called the in   nite time horizon.
time of transition: we will assume for the mdp that the time of transition is unity (1),
which means it is the same for every transition. hence clearly 1 here does not have to mean
1 hour or minute or second. it is some    xed quantity    xed by the analyst. for the smdp,
this quantity is not    xed as we will see later.

let us assume that a policy named    is to be followed. then   (i) will denote the action
selected by this policy for state i. every time there is a jump in the markov chain (of the
system under the policy in consideration), we say a transition (or jump) has occurred. it is

4

important to understand that during a transition we may go from a state to itself !

let xs denote the state of the system before the sth transition. note that in a so-called
in   nite horizon problem, s will go from 1 to in   nity. then, the following quantity, in which
x1 = i, is called the average reward of the policy    if one starts at state i.

[   

]
s=1 r(xs,   (xs), xs+1)jx1 = i

k

e

  i = lim
k!1

k

(1)

this average reward essentially denotes the sum of the total immediate rewards earned
divided by the number of jumps (transitions), calculated over a very long time horizon (that
is k assumes a large value.) in the above, the starting state is i and   (xs) denotes the action
in state xs. also note that e[.] denotes the average value of the quantity inside the square
brackets.

it is not hard to show that the limit in (1) is such that its value is the same for any value
of x1, if the underlying markov chains in the system satisfy certain conditions (related to
the regularity of the markov chains); in many real-world problems such conditions are often
satis   ed. then

  i =    for any value of i.

the objective of the average-reward mdp is to    nd the policy that maximizes the per-

formance metric (average reward) of the policy.

another popular performance metric, commonly used in the literature, is discounted
reward. the following quantity is called the discounted reward of a policy   . again, let xs
denote the state of the system before the sth jump (transition). the total discounted reward
of the policy   , when one starts at state i, is given as:

[

k   

]

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) x1 = i

  i = lim

k!1 e

s=1

  s(cid:0)1r(xs,   (xs), xs+1)

,

(2)

where    denotes the discount factor; this factor,   , is less than 1 but greater than 0. eqn.
(2) has a simple interpretation:

  i = e[r(x1,   (x1), x2) +   r(x2,   (x2), x3) +   2r(x3,   (x3), x4) + (cid:1)(cid:1)(cid:1) ]

the discounted reward essentially measures the present value of the sum of the rewards
earned in the future over an in   nite time horizon, where    is used to discount money   s value.
we should point out that:

(

)

1

   =

,

(3)

1

1 +   

where    is the rate of interest; the rate is expressed as a fraction here and not in percent.
when    > 0, we have that 0 <    < 1.
it is worthwhile pointing out that in the mdp,
we have 1/(1 +   ) raised to the power 1 because in the mdp we assume a    xed rate of
discounting and that the time duration of each transition is    xed at 1. this mechanism thus
captures within the mdp framework the notion of time value of money.

the objective of the discounted-reward mdp is to    nd the policy that maximizes the

performance metric (discounted reward) of the policy starting from every state.

5

note that for average reward problems, the immediate reward in our algorithms can be earned
during the entire duration of the transition. however, for the discounted problems, we will
assume that the immediate reward is earned immediately after the transition starts.

the mdp can be solved with the classical method of id145 (dp). how-
ever, dp needs all the transition probabilities (the p(i, a, j) terms) and the transition rewards
(the r(i, a, j) terms) of the mdp.

for semi-markov decision problems (smdps), an additional parameter of interest is the
time spent in each transition. the time spent in transition from state i to state j under
the in   uence of action a is denoted by t(i, a, j). to solve smdps via dp one also needs
the transition times (the t(i, a, j) terms). for smdps, the average reward that we seek to
maximize is de   ned as:

]
[   
] .
[   
s=1 r(xs,   (xs), xs+1)jx1 = i
s=1 t(xs,   (xs), xs+1)jx1 = i

k

k

  i = lim
k!1

e

e

(4)

(technically, lim should be replaced by lim inf everywhere in this tutorial, but we will not
worry about such technicalities here.) it can be shown that the quantity has the same limit
for any starting state (under certain conditions). a possible unit for average reward here is
$ per hour.

for discounted reward, we will, as stated above, assume the immediate reward is earned
immediately after the transition starts and does not depend on the duration of the transition.
thus, the immediate reward is a lumpsum reward earned at the start of the transition (when
the immediate reward is a function of the time interval, see instead the algorithm in [2]).
also, because of the variability in time, we will assume continuously compounded rate of
interest. then, we seek to maximize:

[

k   

   

]

(cid:12)(cid:12)(cid:12)(cid:12) x1 = i

,

(cid:28)s+1

(cid:0)(cid:22)(cid:28) d  
e

  i = lim

k!1 e

r(x1,   (x1), x2) +

r(xs,   (xs), xs+1)

s=2

(cid:28)s

(cid:0)(cid:22)(cid:28) denotes the discount factor over a period of length    (under continuous com-
where e
pounding) and   s is the time of occurrence of the sth jump (transition). note that since
the immediate reward does not depend on the time    , it can be taken out of the integral.
essentially, what we have above is the sum of discounted rewards with the discount factor
in each transition appropriately calculated using the notion of continuous compounding.

curses: for systems which have a large number of governing random variables, it is often
hard to derive the exact values of the associated transition probabilities. this is called the
curse of modeling. for large-scale systems with millions of states, it is impractical to store
these values. this is called the curse of dimensionality.

dp breaks down on problems which su   er from any one of these curses because it needs

all these values.

id23 (rl) can generate near-optimal solutions to large and complex
mdps. in other words, rl is able to make inroads into problems which su   er from one or
more of these two curses and cannot be solved by dp.

6

3 id23

we will describe a basic rl algorithm that can be used for average reward smdps. note
that if t(i, a, j) = 1 for all values of i, j, and a, we have an mdp. hence our presentation will
be for an smdp, but it can easily be translated into that of an mdp by setting t(i, a, j) = 1
in the steps.

it is also important to understand that the transition probabilities and rewards of the

system are not needed if any one of the following is true:

1. we can play around in the real world system choosing actions and observing the rewards

2. if we have a simulator of the system.

the simulator of the system can usually be written on the basis of the knowledge of
some other easily accessible parameters. for example, the queue can be simulated with the
knowledge of the distribution functions of the inter-arrival time and the service time. thus
the transition probabilities of the system are usually not required for writing the simulation
program.

also, it is important to know that the rl algorithm that we will describe below requires
the updating of certain quantities (called q-factors) in its database whenever the system
visits a new state.

when the simulator is written in c or in any special package such as arena, it is possible

to update certain quantities that the algorithm needs whenever a new state is visited.

usually, the updating that we will need has to be performed immediately after a new
state is visited. in the simulator, or in real time, it is possible to keep track of the state of
the system so that when it changes, one can update the relevant quantities.

the key idea in rl is store a so-called q-factor for each state-action pair in the system.
thus, q(i, a) will denote the q-factor for state i and action a. the values of these q-factors
are initialized to suitable numbers in the beginning (e.g., zero or some small number to all the
q-factors). then the system is simulated (or controlled in real time) using the algorithm. in
each state visited, some action is selected and the system is allowed to transition to the next
state. the immediate reward (and the transition time) that is generated in the transition
is recorded as the feedback. the feedback is used to update the q-factor for the action
selected in the previous state. roughly speaking if the feedback is good, the q-factor of
that particular action and the state in which the action was selected is increased (rewarded)
using the relaxed-smart algorithm. if the feedback is poor, the q-factor is punished by
reducing its value.

then the same reward-punishment policy is carried out in the next state. this is done
for a large number of transitions. at the end of this phase, also called the learning phase,
the action whose q-factor has the highest value is declared to be the optimal action for that
state. thus the optimal policy is determined. note that this strategy does not require the
transition probabilities.

3.1 average reward

we begin with average reward. the algorithm that we will describe is called r-smart
(relaxed semi-markov average reward technique). the original version appeared in [5].

7

below, we present a modi   ed version from [4] that has better convergence properties.

steps in r-smart:

the steps in the learning phase are given below.
(cid:15) step 0 (inputs): set the q-factors to some arbitrary values (e.g. 0), that is:

q(i, a)   0 for all i and a.

set the iteration count, k, to 1. let   k denote the average reward in the kth iteration
of the algorithm. set   1 to 0 or any other value. let the    rst state be i. let a(i)
denote the set of actions allowed in state i. let   k and   k denote two learning rates
that we will need. how these values should be selected and updated is discussed in the
next subsection. set the total reward and total time to zero. set it erm ax, which
will denote the number of iterations for which the algorithm is run, to a large number.
set   , a scaling constant needed in the algorithm, to a small positive value close to 1
but less than 1, e.g., 0.99.

(cid:15) step 1 (q-factor update): determine the action associated to the q-factor that has
the highest value in state i. (for instance, if there are two actions in a state i and
their values are q(i, a) = 19 and q(i, 2) = 45, then, clearly, action 2 has the greatest
q-factor.) this is called the greedy action. select the greedy action with id203
(1 (cid:0) p(k)). one common approach to de   ning p(k) is as follows:

p(k) =

g1

g2 + k

in which g2 > g1, and g1 and g2 are large positive constants, e.g., 1000 and 2000
respectively. with a id203 of p(k), choose one of the other actions. (for the
two-action case, you can generate a random number between 0 and 1. if the number
is less than or equal to (1 (cid:0) pk), choose the greedy action; otherwise, choose the other
action.) the non-greedy actions are called exploratory actions, and selecting an
exploratory action is called exploration. our id203 of exploration will decay with
k, the number of iterations.

let the action selected be denoted by a. if a is a greedy action, set    = 0; otherwise,
set    = 1. simulate action a. let the next state be denoted by j. let r(i, a, j) denote
the transition reward and t(i, a, j) denote the transition time. then update q(i, a) as
follows:

[

]

q(i, a)   (1 (cid:0)   k)q(i, a) +   k

r(i, a, j) (cid:0)   kt(i, a, j) +    max
b2a(j)

q(j, b)

.

in the above, maxb2a(j) q(j, b) equals the maximum numeric value of all the q-factors
of state j.

8

(cid:15) step 2 (average reward update): if    = 1, i.e., the action a was non-greedy, go

to step 3. otherwise, update total reward and total time as follows.

then update the average reward as:

total reward   total reward + r(i, a, j),
total time   total time + t(i, a, j).
]

[

  k+1   (1 (cid:0)   k)  k +   k

total reward

total time

.

(cid:15) step 3 (check for termination): increment k by 1. set i   j. if k < it erm ax,

return to step 1. otherwise, go to step 4.

(cid:15) step 4 (outputs): declare the action for which q(i, .) is maximum to be the optimal
action for state i (do this for all values of i, i.e., for all states to generate a policy),
and stop.

in the above, the exploration id203 p(k) gradually decays to 1, and in the limit, the
algorithm selects the greedy actions. this decaying of the id203 has to be gradual. if
the decay is very quick (e.g., you use small values for g1 and g2), the algorithm will most
likely converge to an incorrect solution. in other words, the algorithm should be allowed to
explore su   ciently.

further note:    has to satisfy 0 <    < 1. the positive scalar    must be less than 1; it
enables the algorithm to converge gracefully to the optimal solution. its value should be as
close to 1 as possible and should not be changed during the learning. an example could be
   = 0.99. in practice,    = 1 may also generate convergent behavior, but there is no known
mathematical proof of this.

the next phase is called the frozen phase because the q-factors are not updated in it.
this phase is performed to estimate the average reward of the policy declared by the frozen
phase to be the optimal policy. (by optimal, of course, we only mean the best that rl can
generate; it may not necessarily be optimal, but hopefully is close enough to the optimal in
its performance.) steps in the frozen phase are as follows.

(cid:15) step 0 (inputs): use the q-factors learned in the learning phase. set iteration count
k to 0. it erm ax will denote the number of iterations for which the frozen phase
is run. it should be set to a large number. also set the following two quantities to 0:
total reward and total time.

(cid:15) step 1 (simulation): select for state i the action which has the maximum q-factor.
let that action be denoted by a. simulate action b. let the next state be denoted by
j. let r(i, a, j) denote the transition reward and t(i, a, j) denote the transition time.
then update total reward and total time as follows.

total reward   total reward + r(i, a, j);

total time   total time + t(i, a, j).

9

(cid:15) step 2 (check for termination): increment k by 1. set i   j. if k < it erm ax,

return to step 1. otherwise, go to step 3.

(cid:15) step 3 (output) calculate the average reward of the policy learned in the learning

phase as follows:

   =

total reward

total time

.

the value of   k at the end of the learning phase can also be used as an estimate of the
actual    while the learning is on. but typically a frozen phase is carried out to get a cleaner
estimate of the actual average reward of the policy learned.

3.2 selecting the appropriate learning rate or step size

the learning rates (  k and   k) should be positive values typically less than 1. the learning
rate should also be a function of k. the learning rates typically have to satisfy key conditions
rules described in [3]. common examples of step-sizes are:

  k = a/(b + k)

where for instance a = 90 and b = 100. another rule is the log rule, which is   k =
log(k + 1)/(k + 1) for k starting at 1 (see [6]). the simplest rule   k = 1/k, where a = 1 and
b = 0 in the above, may not lead to good behavior (see [6] for evidence).

in r-smart, we have two step sizes,   k and   k, within the same algorithm. for r-
smart, we must make sure that   k converges to 0 faster than   k. one example that
satis   es this condition is   k = log(k)/k and   k = 90/(100 + k). ideally, as k tends to 1,
  k/  k should tend to zero.

3.3 discounted reward

for discounted reward, the learning phase is su   cient. the steps we describe are from the
famous id24 algorithm of watkins [11]. they apply to the mdp; we will discuss the
smdp extension later.

(cid:15) step 0 (inputs): set the q-factors to 0:

q(i, a)   0 for all i and a.

let a(i) denote the set of actions allowed in state i. let   k denote the main learning
rate in the kth iteration. set k = 1. it erm ax will denote the number of iterations
for which the algorithm is run and should be set to a large number.

(cid:15) step 1 (q-factor update): select an action a in state i with id203 1/ja(i)j,
where ja(i)j denotes the number of elements in the set a(i). simulate action a. let

10

the next state be denoted by j. let r(i, a, j) denote the transition reward and t(i, a, j)
denote the transition time. update q(i, a) as follows:

[

q(i, a)   (1 (cid:0)   k)q(i, a) +   k

r(i, a, j) +    max
b2a(j)

q(j, b)

,

where you should compute   k using one of the rules discussed above. further,    here
denotes the discount factor.

(cid:15) step 2 (termination check): increment k by 1. set i   j if k < it erm ax,

return to step 1. otherwise, go to step 3.

(cid:15) step 3 (outputs): for each state i, declare the action for which q(i, .) is maximum

to be the optimal action.

note that in the algorithm above, there is no decay of exploration. this is because,
in id24, the exploration need not decay. however, in practice, to get the algorithm
to converge faster, decay of exploration if often employed. we do not recommended it in
simulators, unless there is a need to obtain a solution quickly. in online applications (such
as in robotics), decay may be needed.

for the smdp extension, you can use the following update in step 2 of the above algo-

]

]

[

rithm (see [3] for more details):

q(i, a)   (1 (cid:0)   k)q(i, a) +   k
(

where the exponential term arises as follows:

r(i, a, j) + e

)
(cid:28) (cid:25) e

(cid:0)(cid:22)(cid:28) .

  (cid:28) =

1

1 +   

(cid:0)(cid:22)t(i;a;j) max
b2a(j)

q(j, b)

,

in which    denotes the time and the discounting mechanism was explained above in equation
(3). note that in the above approximation to obtain the exponential term, we use the fact
that    is quite small.
note on the vanishing discount approach: you can actually use a discounted rl algorithm
to solve the average reward problem via the vanishing discount approach. in this heuristic
approach, one uses a discounted algorithm with    very close to 1 (for mdps) and    very
close to 0 (for smdps). this can work very well in practice.

3.4 codes

computer codes for these algorithms have been placed at the following website:

http://web.mst.edu/~gosavia/bookcodes.html

11

figure 1: a two state mdp

4 mdp example

we end with a simple example from gosavi [3]. figure 1 shows a simple mdp; the legend
in the    gure explains the transition rewards and probabilities.

this means that the transition data is given as follows: pa denotes the transition prob-

ability matrix for action a, while ra denotes the transition reward matrix for action a.

[
[

]
]

[
[

p1 =

r1 =

0.7 0.3
0.4 0.6
6 (cid:0)5
7 12

; p2 =

; r2 =

0.9 0.1
0.2 0.8

10
17
(cid:0)14 13

]
]

;

.

4.1 average reward

we    rst consider the average reward problem on which you could use r-smart. the optimal
policy for this mdp is: action 2 in state 1 and action 1 in state 2. the average reward of
the optimal policy is 10.56. when r-smart was used on the above, with    = 0.99, we
obtained the following results at the end of the learning phase:   it erm ax = 10.06;

q(1, 1) = 37.0754; q(1, 2) = 55.1019; q(2, 1) = 51.9332; q(2, 2) = 29.0445.

this implies that according to the algorithm, the best action is 2 for state 1 and is 1 for
state 2, which coincides with the optimal policy.

4.2 discounted reward

we consider the same problem as above but now with discounting, where    = 0.8. the
optimal policy is identical to that of the average reward case above. with id24, we

12

12(1,0.7,6)(2,0.9,10)(1,0.6,12)(2,0.8,13)(2,0.1,17)(1,0.3,-5)(1,0.4,7)(2,0.2,-14)legend:(a,p,r): a = action            p = transition                       id203            r = immediate                  rewardobtained the following results:

q(1, 1) = 42.5892; q(1, 2) = 53.2902; q(2, 1) = 51.5412; q(2, 2) = 45.9043.

here, the optimal values of the q-factors are:

(cid:3)
q

(cid:3)
(1, 1) = 44.84; q

(cid:3)
(1, 2) = 53.02; q

(cid:3)
(2, 1) = 51.87; q

(2, 2) = 49.28.

as is clear, the algorithm generates the optimal policy, as well as q-factors that approximate
their optimal values. the optimal values of the q-factors can be determined from q-factor
value iteration, which is discussed in the book.

5 conclusions

the tutorial presented above showed you one way to solve an mdp provided you have
the simulator of the system or if you can actually experiment in the real-world system.
transition probabilities of the state transitions were not needed in this approach; this is the
most attractive feature of this approach.

we did not discuss what is to be done for large-scale problems. that is beyond the
scope of this tutorial. what was discussed above is called the lookup-table approach in which
each q-factor is stored explicitly (separately). for large-scale problems, clearly it is not
possible to store the q-factors explicitly because there is too many of them. instead one
stores a few scalars, called basis functions, which on demand can generate the q-factor for
any state-action pair. function approximation when done improperly can become unstable
[3].

we can provide some advice to beginners in this regard. first, attempt to cluster states
so that you obtain a manageable state space for which you can actually use a lookup table.
in other words, divide the state space for each action into grids and use only one q-factor
for all the states in each grid. the total number of grids should typically be a manageable
number such as 10, 000. if this does not work well, produce a smaller number of grids but
use an incremental widrow-ho    algorithm, that is, a neuron (see [3]), in each grid. if you
prefer using id75, go ahead because that will work just as well. if this works
well, and you want to see additional improvement, do attempt to use neural networks, either
neurons or those based on back-propagation [10]. it is with function approximation that you
can scale your algorithm up to realistic problem sizes.

i wish you all the best for your new adventures with rl, but cannot promise any help

with homework or term papers     sorry :-(

references

[1] d. bertsekas and j. tsitsiklis. neuro-id145. athena, ma, 1996.

[2] s. j. bradtke and m. du   . id23 methods for continuous- time
markov decision problems. in advances in neural information processing systems 7.
mit press, cambridge, ma, usa, 1995.

13

[3] a. gosavi. simulation-based optimization:

niques
http://web.mst.edu/  gosavia/book.html

and id23,

parametric optimization tech-
2014.

springer, new york, ny,

[4] a. gosavi. target-sensitive control of markov and semi-markov processes. interna-

tional journal of control, automation, and systems, 9(5):1-11, 2011.

[5] a gosavi. id23 for long-run average cost. european jour-
nal of operational research. vol 155, pp. 654-674, 2004. can be found at:
http://web.mst.edu/  gosavia/rsmart.pdf

[6] a gosavi. on step sizes, stochastic shortest paths, and survival probabilities in
id23, conference proceedings of the winter simulation conference,
2009. available at: http://web.mst.edu/  gosavia/wsc 2008.pdf.

[7] a gosavi. id23: a tutorial survey and recent advances.
informs journal on computing. vol 21(2), pp. 178   192, 2009. available at:
http://web.mst.edu/  gosavia/joc.pdf

[8] r. sutton and a. g. barto. id23: an introduction. the mit press,

cambridge, ma, usa, 1998.

[9] matlab

repository

for
at missouri

reinforcement
of

learning,
science

and

created

by
technology,

gosavi

a.
http://web.mst.edu/  gosavia/mrrl website.html.

university

[10] p. j. werbos. beyond regression: new tools for prediction and analysis of behavioral

sciences.. ph.d. thesis, harvard university, usa, 1974.

[11] c. j. watkins. learning from delayed rewards. ph.d. thesis, kings college, cambridge,

england, may 1989.

14

