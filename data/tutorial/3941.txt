id203 theory for 

machine learning

chris cremer

september 2015

outline

    motivation
    id203 definitions and rules
    id203 distributions
    id113 for gaussian parameter estimation
    id113 and least squares

material

    pattern recognition and machine learning - christopher m. bishop 
    all of statistics     larry wasserman
    wolfram mathworld
    wikipedia 

motivation

    uncertainty arises through:

    noisy measurements
    finite size of data sets
    ambiguity: the word bank can mean (1) a financial institution, (2) the side of a river, 

or (3) tilting an airplane. which meaning was intended, based on the words that 
appear nearby?

    limited model complexity

    id203 theory provides a consistent framework for the quantification 

and manipulation of uncertainty

    allows us to make optimal predictions given all the information available to 

us, even though that information may be incomplete or ambiguous

sample space

    the sample space    is the set of possible outcomes of an experiment. 

points    in    are called sample outcomes, realizations, or elements. 
subsets of    are called events.

    example. if we toss a coin twice then    = {hh,ht, th, tt}. the event 

that the first toss is heads is a = {hh,ht}

    we say that events a1 and a2 are disjoint (mutually exclusive) if ai    

aj = {}

    example: first flip being heads and first flip being tails

id203

    we will assign a real number p(a) to every event a, called the 

id203 of a.

    to qualify as a id203, p must satisfy three axioms:

    axiom 1: p(a)     0 for every a 
    axiom 2: p(  ) = 1 
    axiom 3: if a1,a2, . . . are disjoint then

joint and conditional probabilities

    joint id203

    p(x,y)
    id203 of x and y

    id155

    p(x|y)
    id203 of x given y

independent and conditional probabilities

    assuming that p(b) > 0, the id155 of a given b:
    p(a|b)=p(ab)/p(b)
    p(ab) = p(a|b)p(b) = p(b|a)p(a) 

    product rule

    two events a and b are independent if
    p(ab) = p(a)p(b)

    joint = product of marginals

    two events a and b are conditionally independent given c if they are 

independent after conditioning on c 

    p(ab|c) = p(b|ac)p(a|c) = p(b|c)p(a|c)

example

    60% of ml students pass the final and 45% of ml students pass both the 

final and the midterm *

    what percent of students who passed the final also passed the 

midterm?

* these are made up values. 

example

    60% of ml students pass the final and 45% of ml students pass both the 

final and the midterm *

    what percent of students who passed the final also passed the 

midterm?

    reworded: what percent of students passed the midterm given they 

passed the final?

    p(m|f) = p(m,f) / p(f)
    = .45 / .60
    = .75

* these are made up values. 

marginalization and law of total id203

    marginalization (sum rule)

    law of total id203

bayes    rule

p(a|b) = p(ab) /p(b) 
p(a|b) = p(b|a)p(a) /p(b)                     (product rule)
p(a|b) = p(b|a)p(a) /    p(b|a)p(a)                (law of total id203)

(id155)

bayes    rule

example

    suppose you have tested positive for a disease; what is the 

id203 that you actually have the disease? 

    it depends on the accuracy and sensitivity of the test, and on the 

background (prior) id203 of the disease.

    p(t=1|d=1) = .95   (true positive)
    p(t=1|d=0) = .10   (false positive)
    p(d=1) = .01             (prior)

    p(d=1|t=1) = ?

example

    p(t=1|d=1) = .95   (true positive)
    p(t=1|d=0) = .10   (false positive)
    p(d=1) = .01             (prior)

bayes    rule
    p(d|t) = p(t|d)p(d) / p(t)
= .95 * .01 / .1085
= .087

law of total id203
    p(t) =    p(t|d)p(d)
= p(t|d=1)p(d=1) + p(t|d=0)p(d=0) 
= .95*.01 + .1*.99
= .1085

the id203 that you have the disease given you tested positive is 8.7%

random variable

    how do we link sample spaces and events to data?
    a random variable is a mapping that assigns a real number x(  ) to 

each outcome   

    example: flip a coin ten times. let x(  ) be the number of heads in the 

sequence   . if    = hhthhthhtt, then x(  ) = 6.

discrete vs continuous random variables

    discrete: can only take a countable number of values
    example: number of heads 
    distribution defined by id203 mass function (pmf)
    marginalization: 

    continuous:  can take infinitely many values (real numbers)
    example: time taken to accomplish task
    distribution defined by id203 density function (pdf)
    marginalization: 

id203 distribution statistics

    mean: e[x] =    = first moment  =

univariate continuous random variable

=

univariate discrete random variable

    variance: var(x) = 

    nth moment =

discrete distribution

bernoulli distribution

    input: x     {0, 1}
    parameter:   

    example: id203 of flipping heads (x=1)

    mean = e[x] =   
    variance =   (1       )

discrete distribution

binomial distribution

    input: m = number of successes
    parameters: n = number of trials

   = id203 of success

    example: id203 of flipping heads m times out of n independent 

flips with success id203   

    mean = e[x] = n  
    variance = n  (1       )

discrete distribution

multinomial distribution

    the multinomial distribution is a generalization of the binomial 

distribution to k categories instead of just binary (success/fail)

    for n independent trials each of which leads to a success for exactly 

one of k categories, the multinomial distribution gives the id203 
of any particular combination of numbers of successes for the various 
categories

    example: rolling a die n times

discrete distribution

multinomial distribution

    input: m1     mk (counts)
    parameters: n = number of trials

   =   1       k id203 of success for each category,     =1

    mean of mk: n  k
    variance of mk: n  k(1-  k)

continuous distribution

gaussian distribution

    aka the normal distribution
    widely used model for the distribution of continuous variables
    in the case of a single variable x, the gaussian distribution can be 

written in the form

    where    is the mean and   2 is the variance

gaussian distribution

    gaussians with different means and variances

multivariate gaussian distribution

    for a d-dimensional vector x, the multivariate gaussian distribution 

takes the form

    where    is a d-dimensional mean vector
       is a d    d covariance matrix
    |  | denotes the determinant of   

inferring parameters

    we have data x and we assume it comes from some distribution
    how do we figure out the parameters that    best    fit that distribution?

    id113 (id113)

    maximum a posteriori (map)

see    id150 for the uninitiated    for a straightforward introduction to parameter 
estimation: http://www.umiacs.umd.edu/~resnik/pubs/lamp-tr-153.pdf

i.i.d.

    random variables are independent and identically distributed (i.i.d.) if 

they have the same id203 distribution as the others and are all 
mutually independent.

    example: coin flips are assumed to be iid

id113 for parameter estimation

    the parameters of a gaussian distribution are the mean (  ) and 

variance (  2)

    we   ll estimate the parameters using id113
    given observations x1, . . . , xn , the likelihood of those observations 

for a certain    and   2 (assuming iid) is

likelihood = 

id113 for parameter estimation

likelihood = 

what   s the distribution   s mean 
and variance?

id113 for gaussian parameters

likelihood = 

    now we want to maximize this function wrt   
    instead of maximizing the product, we take the log of the likelihood so 

the product becomes a sum

log likelihood = log 

log

    we can do this because log is monotonically increasing
    meaning

id113 for gaussian parameters

    log likelihood simplifies to:

    now we want to maximize this function wrt   
    how?

to see proofs for these derivations: http://www.statlect.com/normal_distribution_maximum_likelihood.htm

id113 for gaussian parameters

    log likelihood simplifies to:

    now we want to maximize this function wrt   
    take the derivative, set to 0, solve for   

to see proofs for these derivations: http://www.statlect.com/normal_distribution_maximum_likelihood.htm

maximum likelihood and least squares

    suppose that you are presented with a 

sequence of data points (x1, t1), ..., (xn, tn), 
and you are asked to find the    best fit    line 
passing through those points.

    in order to answer this you need to know 

precisely how to tell whether one line is 
   fitter    than another

    a common measure of fitness is the squared-

error

for a good discussion of maximum likelihood estimators and least squares see
http://people.math.gatech.edu/~ecroot/3225/maximum_likelihood.pdf

maximum likelihood and least squares

y(x,w) is estimating the target t

red line

    error/loss/cost/objective function measures the squared error 

green lines

    least square regression

    minimize l(w) wrt w

maximum likelihood and least squares

    now we approach curve fitting from a probabilistic perspective
    we can express our uncertainty over the value of the target variable 

using a id203 distribution

    we assume, given the value of x, the corresponding value of t has a 

gaussian distribution with a mean equal to the value y(x,w)

   is the precision parameter (inverse variance)

maximum likelihood and least squares

maximum likelihood and least squares

    we now use the training data {x, t} to 
determine the values of the unknown 
parameters w and    by maximum likelihood

    log likelihood

maximum likelihood and least squares

    log likelihood

    maximize log likelihood wrt to w
    since last two terms, don   t depend on w, 

they can be omitted. 

    also, scaling the log likelihood by a positive 
constant   /2 does not alter the location of 
the maximum with respect to w, so it can be 
ignored

    result: maximize

maximum likelihood and least squares

    id113

    maximize

    least squares

    minimize

    therefore, maximizing likelihood is equivalent, so far as determining w is 

concerned, to minimizing the sum-of-squares error function

    significance: sum-of-squares error function arises as a consequence of 

maximizing likelihood under the assumption of a gaussian noise 
distribution

questions?

