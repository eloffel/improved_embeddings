review of id128 theory 1
winter 2017

1 author: guillaume genthial

keywords: differential, gradients, partial derivatives, jacobian,
chain-rule

this note is optional and is aimed at students who wish to have

a deeper understanding of id128. it de   nes and ex-
plains the links between derivatives, gradients, jacobians, etc. first,
: rn (cid:55)    r. then we
we go through de   nitions and examples for f
introduce the jacobian and generalize to higher dimension. finally,
we introduce the chain-rule.

1

introduction

we use derivatives all the time, but we forget what they mean. in
: r (cid:55)    r, we have
general, we have in mind that for a function f
something like

f (x + h)     f (x)     f (cid:48)(x)h

some people use different notation, especially when dealing with

higher dimensions, and there usually is a lot of confusion between
the following notations

f (cid:48)(x)
d f
dx
    f
   x
   x f

however, these notations refer to different mathematical objects,

and the confusion can lead to mistakes. this paper recalls some
notions about these objects.

scalar-product and dot-product

given two vectors a and b,
    scalar-product (cid:104)a|b(cid:105) =    n
i=1 aibi
    dot-product at    b = (cid:104)a|b(cid:105) =

   n

i=1 aibi

review of id128 theory

2

2 theory for f : rn (cid:55)    r

2.1 differential

formal de   nition
let   s consider a function f : rn (cid:55)    r de   ned on rn with the scalar
product (cid:104)  |  (cid:105). we suppose that this function is differentiable, which
means that for x     rn (   xed) and a small variation h (can change) we
can write:

notation
dx f is a linear form rn (cid:55)    r
this is the best linear approximation

of the function f

dx f is called the differential of f in x

f (x + h) = f (x) + dx f (h) + oh   0(h)

(1)

and dx f : rn (cid:55)    r is a linear form, which means that    x, y     rn ,

we have dx f (x + y) = dx f (x) + dx f (y).

oh   0(h) (landau notation) is equiva-
lent to the existence of a function  (h)
such that lim
h   0

 (h) = 0

(cid:32)

(cid:33)

x1
x2

: r2 (cid:55)    r such that f (

example

let f

(cid:32)

(cid:33)

a
b

    r2 and h =
(cid:32)

(cid:32)

h1
h2

(cid:33)
(cid:33)

f (

a + h1
b + h2

) = 3x1 + x2

2. let   s pick

    r2. we have

) = 3(a + h1) + (b + h2)2

= 3a + 3h1 + b2 + 2bh2 + h2
2
= 3a + b2 + 3h1 + 2bh2 + h2
2
= f (a, b) + 3h1 + 2bh2 + o(h)

(cid:33)

(cid:32)

h1
h2

then, d      a

b

       f (

) = 3h1 + 2bh2

h2 = h    h = oh   0(h)

2.2 link with the gradients

it can be shown that for all linear forms a : rn (cid:55)    r, there exists a

formal de   nition
vector ua     rn such that    h     rn

a(h) = (cid:104)ua|h(cid:105)

in particular, for the differential dx f , we can    nd a vector u     rn

such that

.

dx(h) = (cid:104)u|h(cid:105)

notation for x     rn, the gradient is
usually written    x f     rn

the dual of a vector space e    is isomor-
phic to e

see riesz representation theorem

the gradient has the same shape as x

review of id128 theory

3

we can thus de   ne the gradient of f in x

   x f := u

then, as a conclusion, we can rewrite equation 2.1

f (x + h) = f (x) + dx f (h) + oh   0(h)
= f (x) + (cid:104)   x f|h(cid:105) + oh   0(h)

(2)
(3)

gradients and differential of a func-
tion are conceptually very different.
the gradient is a vector, while the
differential is a function

(cid:33)

(cid:32)

x1
x2

) =

example

same example as before, f

3x1 + x2

2. we showed that

d      a

b

we can rewrite this as

       f (
(cid:32)

       f (

d      a

b

: r2 (cid:55)    r such that f (
(cid:32)
(cid:33)

) = 3h1 + 2bh2

h1
h2

(cid:33)

) = (cid:104)

(cid:32)

(cid:32)

(cid:33)

|

(cid:33)

(cid:105)

h1
h2

3
2b

h1
h2

(cid:33)

(cid:32)

3
2b

and thus our gradient is

       f =

         a

b

2.3 partial derivatives

formal de   nition

now, let   s consider an orthonormal basis (e1, . . . , en) of rn. let   s

de   ne the partial derivative

f (x1, . . . , xi   1, xi + h, xi+1, . . . , xn)     f (x1, . . . , xn)

(x) := lim
h   0

    f
   xi
note that the partial derivative     f
   xi

h

(x)     r and that it is de   ned

with respect to the i-th component and evaluated in x.

example
same example as before, f

3x1 + x2

2. let   s write

: r2 (cid:55)    r such that f (x1, x2) =

x

is a function rn (cid:55)    r

notation
partial derivatives are usually written
   x but you may also see    x f or f (cid:48)
    f
        f
   xi
   x = (     f
        f
   x1
rn (cid:55)    rn.
(x)     r
        f
   xi
   x (x) = (     f
        f
   x1

)t is a function

(x), . . . ,     f
   xn

(x))t     rn

, . . . ,     f
   xn

depending on the context, most people
omit to write the (x) evaluation and just
write
   x     rn instead of     f

   x (x)

    f

(cid:33)

(cid:32)

a
b

    f
   x1

(

(cid:33)

(cid:32)

f (

a + h

b

(cid:32)

(cid:33)

)

a
b

)     f (

h

3(a + h) + b2     (3a + b2)

h

3h
h

) = lim
h   0

= lim
h   0

= lim
h   0

= 3

in a similar way, we    nd that

    f
   x2

(

(cid:33)

(cid:32)

a
b

) = 2b

2.4 link with the partial derivatives

formal de   nition

it can be shown that

   x f =

=

(x)ei

    f
   xi

i=1

n   

                 f

   x1

    f
   xn

(x)
...
(x)

            

review of id128 theory

4

that   s why we usually write

   x f =
(same shape as x)

    f
   x

(x)

ei is a orthonormal basis. for instance,
in the canonical basis

ei = (0, . . . , 1, . . . 0)

with 1 at index i

where     f
   xi

ith component, evaluated in x.

(x) denotes the partial derivative of f with respect to the

example
we showed that

                                             

    f
   x1

(

    f
   x2

(

      a
      a

b

b

and that

         a

b

and then we verify that

      ) = 3
      ) = 2b
(cid:32)
(cid:33)

       f =

3
2b

review of id128 theory

5

                  

(cid:33)
(cid:33)

                  

)

)

(cid:32)
(cid:32)

a
b
a
b

    f
   x1

(

    f
   x2

(

       f =

         a

b

3 summary

formal de   nition

for a function f : rn (cid:55)    r, we have de   ned the following objects

which can be summarized in the following equation

recall that at    b = (cid:104)a|b(cid:105) =    n

i=1 aibi

f (x + h) = f (x) + dx f (h) + oh   0(h)
= f (x) + (cid:104)   x f|h(cid:105) + oh   0(h)
= f (x) + (cid:104)     f
(x)|h(cid:105) + oh   0
   x
(x)
...
(x)

             |h(cid:105) + oh   0

                 f

= f (x) + (cid:104)

   x1

    f
   xn

differential
gradient

partial derivatives

remark
let   s consider x : r (cid:55)    r such that x(u) = u for all u. then we can
easily check that dux(h) = h. as this differential does not depend on
u, we may simply write dx. that   s why the following expression has
some meaning,

the dx that we use refers to the differ-
ential of u (cid:55)    u, the identity mapping!

because

dx f (  ) =

(x)dx(  )

    f
   x

dx f (h) =

=

    f
   x
    f
   x

(x)dx(h)

(x)h

in higher dimension, we write

dx f =

n   

i=1

    f
   xi

(x)dxi

jacobian: generalization to f : rn (cid:55)    rm

4

for a function

             (cid:55)   

            x1

...
xn

f :

             f1(x1, . . . , xn)

...

fm(x1, . . . , xn)

review of id128 theory

6

            

we can apply the previous section to each fi(x) :

fi(x + h) = fi(x) + dx fi(h) + oh   0(h)
= fi(x) + (cid:104)   x fi|h(cid:105) + oh   0(h)
= fi(x) + (cid:104)     fi
(x)|h(cid:105) + oh   0
   x
(x), . . . ,     fi
    fi
= fi(x) + (cid:104)(
   x1
   xn

(x))t|h(cid:105) + oh   0

   x (x)t    h

    fm

   x (x)t    h

...

(x) . . .     f1
   xn

...

(x) . . .     fm
   xn

putting all this in the same vector yields

now, let   s de   ne the jacobian matrix as

f

...

xn + hn

            x1 + h1
             = f
                 f1
            x1
             = f

j(x) :=

...

...
xn

            x1
             +
                 f1
                 f1
             =
                 f1
             +

    fm
   x1

   x1

   x1

    fm
   x (x)t

   x (x)t

...
xn

            x1 + h1

...

xn + hn

f

then, we have that

(x) . . .     f1
   xn

(x)

...

    fm
   x1

(x) . . .     fm
   xn

(x)

= f (x) + j(x)    h + o(h)

(x)

             + o(h)
            
                h + o(h)
(cid:33)
(cid:32)

(x)

x1
x2

) =

example 1 : m = 1

let   s take our    rst function f

3x1 + x2

2. then, the jacobian of f is

(cid:16)     f

(x)

    f
   x2

   x1

: r2 (cid:55)    r such that f (
(cid:17)

(cid:17)

(x)

=

3

(cid:16)
(cid:32)

2x2

(cid:33)t

3
2x2

=
=     f (x)t

the jacobian matrix has dimensions
m    n and is a generalization of the
gradient

in the case where m = 1, the jacobian is
a row vector

(x)

(x) . . .     f1
   xn

    f1
   x1
remember that our gradient was
de   ned as a column vector with the
same elements. we thus have that
j(x) =    x f t

review of id128 theory

7

(cid:33)

y1 + 2y2 + 3y3

y1y2y3

example 2 : g : r3 (cid:55)    r2 let   s de   ne

then, the jacobian of g is

(cid:32)

g(

y2
y3

         y1
          (y1+2y2+3y3)
          (y1+2y2+3y3)
(cid:32)

   (y1y2y3)

   (y1y2y3)

   y1

   y1

   y

   y

         ) =
      
(cid:33)

(y)t

(y)

(y)t

(y)

1
y2y3

2
y1y3

3
y1y2

jg(y) =

=

=

   (y1+2y2+3y3)

   (y1+2y2+3y3)

(y)

(y)

   y2

   y2

   (y1y2y3)

(y)

   y3

   y3

   (y1y2y3)

(y)

      

5 generalization to f : rn  p (cid:55)    r
if a function takes as input a matrix a     rn  p, we can transform this
matrix into a vector a     rnp, such that

a[i, j] = a[i + nj]

then, we end up with a function   f

: rnp (cid:55)    r. we can apply
the results from 3 and we obtain for x, h     rnp corresponding to
x, h     rn  p,

  f (x + h) = f (x) + (cid:104)   x f|h(cid:105) + o(h)

where    x f =

            .

                 f

   x1

(x)
...

    f

   xnp(x)

now, we would like to give some meaning to the following equa-

tion

f (x + h) = f (x) + (cid:104)   x f|h(cid:105) + o(h)

now, you can check that if you de   ne

   x fij =

    f
   xij

(x)

that these two terms are equivalent

the gradient of f wrt to a matrix x is a
matrix of same shape as x and de   ned
by
   x fij =     f
   xij

(x)

review of id128 theory

8

(cid:104)   x f|h(cid:105) = (cid:104)   x f|h(cid:105)
    f
   xi

(x)hi =    
i,j

    f
   xij

(x)hij

np   

i=1

6 generalization to f : rn  p (cid:55)    rm

applying the same idea as before, we can write

f (x + h) = f (x) + j(x)    h + o(h)

where j has dimension m    n    p and is de   ned as

jijk(x) =

    fi
   xjk

(x)

writing the 2d-dot product    = j(x)    h     rm means that the i-th

component of    is

7 chain-rule

  i =

n   

p   

j=1

k=1

    fi
   xjk

(x)hjk

now let   s consider f

formal de   nition
: rn (cid:55)    rm and g : rp (cid:55)    rn. we want
to compute the differential of the composition h = f     g such that
h : x (cid:55)    u = g(x) (cid:55)    f (g(x)) = f (u), or
dx( f     g)

.

it can be shown that the differential is the composition of the dif-

ferentials

dx( f     g) = dg(x) f     dxg

where     is the composition operator. here, dg(x) f and dxg are lin-
ear transformations (see section 4). then, the resulting differential is
also a linear transformation and the jacobian is just the dot product
between the jacobians. in other words,

jh(x) = j f (g(x))    jg(x)

where    is the dot-product. this dot-product between two matrices

can also be written component-wise:

let   s generalize the generalization of
the previous section

you can apply the same idea to any
dimensions!

the chain-rule is just writing the
resulting jacobian as a dot product of
jacobians. order of the dot product is
very important!

review of id128 theory

9

jh(x)ij =

n   

k=1

example

j f (g(x))ik    jg(x)kj

let   s keep our example function f : (

) (cid:55)    3x1 + x2

2 and our

function g : (

y1 + 2y2 + 3y3

y1y2y3

the composition of f and g is h = f     g : r3 (cid:55)    r

(cid:33)

x1
x2

(cid:32)
(cid:33)

.

(cid:33)

)

y1 + 2y2 + 3y3

y1y2y3

y2
y3

(cid:32)

         y1
         ) =
         ) = f (
         y1

h(

y2
y3

(cid:32)

= 3(y1 + 2y2 + 3y3) + (y1y2y3)2

we can compute the three components of the gradient of h with

the partial derivatives

   h
   y1
   h
   y2
   h
   y3

(y) = 3 + 2y1y2

2y2
3

(y) = 6 + 2y2y2

1y2
3

(y) = 9 + 2y3y2

1y2
2

and then our gradient is

   yh =

         3 + 2y1y2

6 + 2y2y2
9 + 2y3y2

2y2
3
1y2
3
1y2
2

         

in this process, we did not use our previous calculation, and that   s
a shame. let   s use the chain-rule to make use of it. with examples 2.2
and 4, we had

(cid:16)
j f (x) =    x f t

=

3 2x2

(cid:17)

for a function f : rn (cid:55)    r, the jacobian
is the transpose of the gradient

   x f t = j f (x)

we also need the jacobian of g, which we computed in 4

(cid:32)

(cid:33)

jg(y) =

1
y2y3

2
y1y3

3
y1y2

review of id128 theory

10

applying the chain rule, we obtain that the jacobian of h is the

product j f    jg (in this order). recall that for a function rn (cid:55)    r, the
jacobian is formally the transpose of the gradient. then,

jh(y) = j f (g(y))    jg(y)
g(y) f    jg(y)

=    t

3 2y1y2y3

(cid:17)   

(cid:32)

1
y2y3

(cid:16)
(cid:16)

=

=

3 + 2y1y2

2y2
3

6 + 2y2y2

(cid:33)

2
y1y3
1y2
3

3
y1y2
9 + 2y3y2

1y2
2

(cid:17)

and taking the transpose we    nd the same gradient that we com-

puted before!

important remark

    the gradient is only de   ned for function with values in r.

    note that the chain rule gives us a way to compute the jacobian
and not the gradient. however, we showed that in the case of a
function f : rn (cid:55)    r, the jacobian and the gradient are directly
identi   able, because    x jt = j(x). thus, if we want to compute
the gradient of a function by using the chain-rule, the best way to
do it is to compute the jacobian.

    as the gradient must have the same shape as the variable against

which we derive, and

    we know that the jacobian is the transpose of the gradient
    and the jacobian is the dot product of jacobians

an ef   cient way of computing the gradient is to    nd the ordering
of jacobian (or the transpose of the jacobian) that yield correct
shapes!

    the notation      

dient or the jacobian.

      is often ambiguous and can refer to either the gra-

