id202 review and reference

zico kolter (updated by chuong do)

october 7, 2008

contents

1 basic concepts and notation

1.1 basic notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 id127

2.1 vector-vector products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 matrix-vector products
2.3 matrix-matrix products
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 operations and properties

. . . . . . . . . . . . . . . . . .
3.1 the identity matrix and diagonal matrices
3.2 the transpose
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 symmetric matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 the trace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.6 linear independence and rank . . . . . . . . . . . . . . . . . . . . . . . . .
3.7 the inverse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.8 orthogonal matrices
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.9 range and nullspace of a matrix . . . . . . . . . . . . . . . . . . . . . . . .
3.10 the determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.11 quadratic forms and positive semide   nite matrices . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
3.12 eigenvalues and eigenvectors
3.13 eigenvalues and eigenvectors of symmetric matrices
. . . . . . . . . . . . .

4 matrix calculus
4.1 the gradient
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 the hessian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 gradients and hessians of quadratic and linear functions . . . . . . . . . .
4.4 least squares . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 gradients of the determinant
. . . . . . . . . . . . . . . . . . . . . . . . . .
4.6 eigenvalues as optimization . . . . . . . . . . . . . . . . . . . . . . . . . . .

2
2

3
4
4
5

7
8
8
8
9
10
11
11
12
12
14
17
18
19

20
20
22
23
25
25
26

1

1 basic concepts and notation

id202 provides a way of compactly representing and operating on sets of linear
equations. for example, consider the following system of equations:

4x1     5x2 =    13

   2x1 + 3x2 = 9.

this is two equations and two variables, so as you know from high school algebra, you
can    nd a unique solution for x1 and x2 (unless the equations are somehow degenerate, for
example if the second equation is simply a multiple of the    rst, but in the case above there
is in fact a unique solution). in matrix notation, we can write the system more compactly
as

with

ax = b

3 (cid:21) ,
a =(cid:20) 4    5

   2

9 (cid:21) .
b =(cid:20)    13

as we will see shortly, there are many advantages (including the obvious space savings)

to analyzing linear equations in this form.

1.1 basic notation

we use the following notation:

    by a     rm  n we denote a matrix with m rows and n columns, where the entries of a

are real numbers.

    by x     rn, we denote a vector with n entries. by convention, an n-dimensional vector
is often thought of as a matrix with n rows and 1 column, known as a column vector .
if we want to explicitly represent a row vector     a matrix with 1 row and n columns
    we typically write xt (here xt denotes the transpose of x, which we will de   ne
shortly).

    the ith element of a vector x is denoted xi:

x1
x2
...
xn

x =   
            

.

   
            

2

    we use the notation aij (or aij, ai,j, etc) to denote the entry of a in the ith row and

jth column:

    we denote the jth column of a by aj or a:,j:

a =   
            
a =   
   

a11
a12
a21
a22
...
...
am1 am2

        
a1n
        
a2n
...
. . .
         amn

.

   
            

|
|
a1 a2
|
|

|
         an

|    
    .

    we denote the ith row of a by at

i or ai,::

    at
1    
    at
2    
...
    at
m    

a =   
            

.

   
            

    note that these de   nitions are ambiguous (for example, the a1 and at

1 in the previous
two de   nitions are not the same vector). usually the meaning of the notation should
be obvious from its use.

2 id127

the product of two matrices a     rm  n and b     rn  p is the matrix

where

c = ab     rm  p,

n

cij =

aikbkj.

xk=1

note that in order for the matrix product to exist, the number of columns in a must equal
the number of rows in b. there are many ways of looking at id127, and
we   ll start by examining a few special cases.

3

2.1 vector-vector products

given two vectors x, y     rn, the quantity xt y, sometimes called the inner product or dot
product of the vectors, is a real number given by

xt y     r =(cid:2) x1 x2

         xn (cid:3)

y1
x2
...
yn

   
            

   
            

=

n

xi=1

xiyi.

observe that inner products are really just special case of id127. note that
it is always the case that xt y = yt x.

given vectors x     rm, y     rn (not necessarily of the same size), xyt     rm  n is called
the outer product of the vectors. it is a matrix whose entries are given by (xyt )ij = xiyj,
i.e.,

xyt     rm  n =   
            

x1
x2
...
xm

   
            

        

(cid:2) y1 y2

yn (cid:3) =   
            

x1y1
x2y1
...

x1y2
x2y2
...

xmy1 xmy2

x1yn
x2yn
...

        
        
. . .
         xmyn

.

   
            

as an example of how the outer product can be useful, let 1     rn denote an n-dimensional
vector whose entries are all equal to 1. furthermore, consider the matrix a     rm  n whose
columns are all equal to some vector x     rm. using outer products, we can represent a
compactly as,

a =   
   

|

|
|
x x          x
|

|

    =   
|    
            

x1
x2
...

        
        
. . .

x1
x1
x2
x2
...
...
xm xm          xm

x1
x2
...
xm

   
            

=   
            

   
            

(cid:2) 1 1         

1 (cid:3) = x1t .

2.2 matrix-vector products

given a matrix a     rm  n and a vector x     rn, their product is a vector y = ax     rm.
there are a couple ways of looking at matrix-vector multiplication, and we will look at each
of them in turn.

if we write a by rows, then we can express ax as,

y = ax =   
            

    at
1    
    at
2    
...
    at
m    

   
            

x =   
            

at
1 x
at
2 x
...
at
mx

.

   
            

4

in other words, the ith entry of y is equal to the inner product of the ith row of a and x,
yi = at

i x.

alternatively, let   s write a in column form. in this case we see that,

x1
x2
...
xn

   
            

   
            

y = ax =   
   

|
|
a1 a2
|
|

|
         an

|    
   

=   
   

    x1 +   
a1    
   

    x2 + . . . +   
a2    
   

an    
    xn .

in other words, y is a linear combination of the columns of a, where the coe   cients of
the linear combination are given by the entries of x.

so far we have been multiplying on the right by a column vector, but it is also possible
to multiply on the left by a row vector. this is written, yt = xt a for a     rm  n, x     rm,
and y     rn. as before, we can express yt in two obvious ways, depending on whether we
express a in terms on its rows or columns. in the    rst case we express a in terms of its
columns, which gives

yt = xt a = xt    
   

|
|
a1 a2
|
|

|
         an

|    
    =(cid:2) xt a1 xt a2

         xt an (cid:3)

which demonstrates that the ith entry of yt is equal to the inner product of x and the ith
column of a.

finally, expressing a in terms of rows we get the    nal representation of the vector-matrix

product,

yt = xt a

= (cid:2) x1 x2
= x1(cid:2)     at

         xn (cid:3)
1     (cid:3) + x2(cid:2)     at

    at
1    
    at
2    
...
    at
m    

   
            
2     (cid:3) + ... + xn(cid:2)     at

   
            

n     (cid:3)

so we see that yt is a linear combination of the rows of a, where the coe   cients for the
linear combination are given by the entries of x.

2.3 matrix-matrix products

armed with this knowledge, we can now look at four di   erent (but, of course, equivalent)
ways of viewing the matrix-id127 c = ab as de   ned at the beginning of
this section.

first, we can view matrix-id127 as a set of vector-vector products. the
most obvious viewpoint, which follows immediately from the de   nition, is that the (i, j)th

5

entry of c is equal to the inner product of the ith row of a and the jth row of b. symbolically,
this looks like the following,

c = ab =   
            

    at
1    
    at
2    
...
    at
m    

   
            

|
b1
|

|
b2
|

   
   

        

|
bp

    =   
|    
            

1 b1 at
at
1 b2
at
2 b1 at
2 b2
...
...
at
mb1 at
mb2

at
        
1 bp
at
        
2 bp
...
. . .
         at
mbp

.

   
            

remember that since a     rm  n and b     rn  p, ai     rn and bj     rn, so these inner
products all make sense. this is the most    natural    representation when we represent a
by rows and b by columns. alternatively, we can represent a by columns, and b by rows.
this representation leads to a much trickier interpretation of ab as a sum of outer products.
symbolically,

c = ab =   
   

|
|
a1 a2
|
|

|
         an

|    
   

    bt
1    
    bt
2    
...
    bt
n    

   
            

   
            

=

n

xi=1

aibt
i

.

put another way, ab is equal to the sum, over all i, of the outer product of the ith column
of a and the ith row of b. since, in this case, ai     rm and bi     rp, the dimension of the
outer product aibt
is m    p, which coincides with the dimension of c. chances are, the last
i
equality above may appear confusing to you. if so, take the time to check it for yourself!

second, we can also view matrix-id127 as a set of matrix-vector products.
speci   cally, if we represent b by columns, we can view the columns of c as matrix-vector
products between a and the columns of b. symbolically,

c = ab = a   
   

|
b1
|

|
b2
|

        

|
bp

    =   
|    
   

|

|

ab1 ab2

|

|

         abp

|

|    
    .

here the ith column of c is given by the matrix-vector product with the vector on the right,
ci = abi. these matrix-vector products can in turn be interpreted using both viewpoints
given in the previous subsection. finally, we have the analogous viewpoint, where we repre-
sent a by rows, and view the rows of c as the matrix-vector product between the rows of a
and c. symbolically,

c = ab =   
            

    at
1    
    at
2    
...
    at
m    

   
            

b =   
            

    at
1 b    
    at
2 b    
...
    at
mb    

.

   
            

here the ith row of c is given by the matrix-vector product with the vector on the left,
ct
i = at

i b.

6

it may seem like overkill to dissect id127 to such a large degree, especially
when all these viewpoints follow immediately from the initial de   nition we gave (in about a
line of math) at the beginning of this section. however, virtually all of id202 deals
with id127s of some kind, and it is worthwhile to spend some time trying to
develop an intuitive understanding of the viewpoints presented here.

in addition to this, it is useful to know a few basic properties of id127 at

a higher level:

    id127 is associative: (ab)c = a(bc).

    id127 is distributive: a(b + c) = ab + ac.

    id127 is, in general, not commutative; that is, it can be the case that
ab 6= ba. (for example, if a     rm  n and b     rn  q, the matrix product ba does
not even exist if m and q are not equal!)

if you are not familiar with these properties, take the time to verify them for yourself.
for example, to check the associativity of id127, suppose that a     rm  n,
b     rn  p, and c     rp  q. note that ab     rm  p, so (ab)c     rm  q. similarly, bc     rn  q,
so a(bc)     rm  q. thus, the dimensions of the resulting matrices agree. to show that
id127 is associative, it su   ces to check that the (i, j)th entry of (ab)c is
equal to the (i, j)th entry of a(bc). we can verify this directly using the de   nition of
id127:

((ab)c)ij =

=

=

p

p

p

(ab)ikckj =

xk=1  n
ailblk! ckj
xl=1
xk=1
xl=1   p
xk=1  n
ailblkckj! =
xk=1
xl=1
ail  n
blkckj! =
xk=p
xl=1
xl=1

n

n

n

ailblkckj!

ail(bc)lj = (a(bc))ij.

here, the    rst and last two equalities simply use the de   nition of id127, the
third and    fth equalities use the distributive property for scalar multiplication over addition,
and the fourth equality uses the commutative and associativity of scalar addition. this
technique for proving matrix properties by reduction to simple scalar properties will come
up often, so make sure you   re familiar with it.

3 operations and properties

in this section we present several operations and properties of matrices and vectors. hope-
fully a great deal of this will be review for you, so the notes can just serve as a reference for
these topics.

7

3.1 the identity matrix and diagonal matrices

the identity matrix , denoted i     rn  n, is a square matrix with ones on the diagonal and
zeros everywhere else. that is,

it has the property that for all a     rm  n,

iij =(cid:26) 1 i = j

0 i 6= j

ai = a = ia.

note that in some sense, the notation for the identity matrix is ambiguous, since it does not
specify the dimension of i. generally, the dimensions of i are inferred from context so as to
make id127 possible. for example, in the equation above, the i in ai = a
is an n    n matrix, whereas the i in a = ia is an m    m matrix.

a diagonal matrix is a matrix where all non-diagonal elements are 0. this is typically

denoted d = diag(d1, d2, . . . , dn), with

dij =(cid:26) di

0

i = j
i 6= j

clearly, i = diag(1, 1, . . . , 1).

3.2 the transpose

the transpose of a matrix results from       ipping    the rows and columns. given a matrix
a     rm  n, its transpose, written at     rn  m, is the n    m matrix whose entries are given
by

(at )ij = aji.

we have in fact already been using the transpose when describing row vectors, since the
transpose of a column vector is naturally a row vector.

the following properties of transposes are easily veri   ed:

    (at )t = a

    (ab)t = bt at

    (a + b)t = at + bt

3.3 symmetric matrices

a square matrix a     rn  n is symmetric if a = at . it is anti-symmetric if a =    at .
it is easy to show that for any matrix a     rn  n, the matrix a + at is symmetric and the

8

matrix a     at is anti-symmetric. from this it follows that any square matrix a     rn  n can
be represented as a sum of a symmetric matrix and an anti-symmetric matrix, since

a =

1
2

(a + at ) +

1
2

(a     at )

and the    rst matrix on the right is symmetric, while the second is anti-symmetric. it turns out
that symmetric matrices occur a great deal in practice, and they have many nice properties
which we will look at shortly. it is common to denote the set of all symmetric matrices of
size n as sn, so that a     sn means that a is a symmetric n    n matrix;

3.4 the trace

the trace of a square matrix a     rn  n, denoted tr(a) (or just tra if the parentheses are
obviously implied), is the sum of diagonal elements in the matrix:

tra =

aii.

n

xi=1

as described in the cs229 lecture notes, the trace has the following properties (included
here for the sake of completeness):

    for a     rn  n, tra = trat .

    for a, b     rn  n, tr(a + b) = tra + trb.

    for a     rn  n, t     r, tr(ta) = t tra.

    for a, b such that ab is square, trab = trba.

    for a, b, c such that abc is square, trabc = trbca = trcab, and so on for the

product of more matrices.

as an example of how these properties can be proven, we   ll consider the fourth property
given above. suppose that a     rm  n and b     rn  m (so that ab     rm  m is a square
matrix). observe that ba     rn  n is also a square matrix, so it makes sense to apply the
trace operator to it. to verify that trab = trba, note that

trab =

=

=

(ab)ii =

m

n

m

xi=1
xi=1
xj=1
xj=1  m
xi=1

n

n

m

m

xi=1   n
aijbji!
xj=1
xj=1
xi=1
bjiaij! =
xj=1

bjiaij

n

aijbji =

(ba)jj = trba.

9

here, the    rst and last two equalities use the de   nition of the trace operator and matrix
multiplication. the fourth equality, where the main work occurs, uses the commutativity
of scalar multiplication in order to reverse the order of the terms in each product, and the
commutativity and associativity of scalar addition in order to rearrange the order of the
summation.

3.5 norms

a norm of a vector kxk is informally a measure of the    length    of the vector. for example,
we have the commonly-used euclidean or    2 norm,

n

kxk2 =vuut
xi=1

x2
i .

note that kxk2

2 = xt x.

more formally, a norm is any function f : rn     r that satis   es 4 properties:

1. for all x     rn, f (x)     0 (non-negativity).

2. f (x) = 0 if and only if x = 0 (de   niteness).

3. for all x     rn, t     r, f (tx) = |t|f (x) (homogeneity).

4. for all x, y     rn, f (x + y)     f (x) + f (y) (triangle inequality).

other examples of norms are the    1 norm,

and the        norm,

kxk1 =

|xi|

n

xi=1

kxk    = maxi |xi|.

in fact, all three norms presented so far are examples of the family of    p norms, which are
parameterized by a real number p     1, and de   ned as

kxkp =  n
xi=1

|xi|p!1/p

.

norms can also be de   ned for matrices, such as the frobenius norm,

m

kakf =vuut
xi=1

n

xj=1

a2

ij =ptr(at a).

many other norms exist, but they are beyond the scope of this review.

10

3.6 linear independence and rank

a set of vectors {x1, x2, . . . xn}     rm is said to be (linearly) independent if no vector can
be represented as a linear combination of the remaining vectors. conversely, if one vector
belonging to the set can be represented as a linear combination of the remaining vectors,
then the vectors are said to be (linearly) dependent. that is, if

xn =

  ixi

n   1

xi=1

for some scalar values   1, . . . ,   n   1     r, then we say that the vectors x1, . . . , xn are linearly
dependent; otherwise, the vectors are linearly independent. for example, the vectors

x1 =   
   

1
2
3

    x2 =   
   
   

4
1
5

    x3 =   
   
   

2
   3
   1

   
   

are linearly dependent because x3 =    2x1 + x2.

the column rank of a matrix a     rm  n is the size of the largest subset of columns of
a that constitute a linearly independent set. with some abuse of terminology, this is often
referred to simply as the number of linearly independent columns of a. in the same way,
the row rank is the largest number of rows of a that constitute a linearly independent set.
for any matrix a     rm  n, it turns out that the column rank of a is equal to the row
rank of a (though we will not prove this), and so both quantities are referred to collectively
as the rank of a, denoted as rank(a). the following are some basic properties of the rank:

    for a     rm  n, rank(a)     min(m, n). if rank(a) = min(m, n), then a is said to be

full rank .

    for a     rm  n, rank(a) = rank(at ).

    for a     rm  n, b     rn  p, rank(ab)     min(rank(a), rank(b)).

    for a, b     rm  n, rank(a + b)     rank(a) + rank(b).

3.7 the inverse

the inverse of a square matrix a     rn  n is denoted a   1, and is the unique matrix such
that

a   1a = i = aa   1.

note that not all matrices have inverses. non-square matrices, for example, do not have
inverses by de   nition. however, for some square matrices a, it may still be the case that

11

a   1 may not exist.
exists and non-invertible or singular otherwise.1

in particular, we say that a is invertible or non-singular if a   1

in order for a square matrix a to have an inverse a   1, then a must be full rank. we will
soon see that there are many alternative su   cient and necessary conditions, in addition to
full rank, for invertibility.

the following are properties of the inverse; all assume that a, b     rn  n are non-singular:
    (a   1)   1 = a

    (ab)   1 = b   1a   1

    (a   1)t = (at )   1. for this reason this matrix is often denoted a   t .

as an example of how the inverse is used, consider the linear system of equations, ax = b
where a     rn  n, and x, b     rn. if a is nonsingular (i.e., invertible), then x = a   1b. (what
if a     rm  n is not a square matrix? does this work?)

3.8 orthogonal matrices

two vectors x, y     rn are orthogonal if xt y = 0. a vector x     rn is normalized if
kxk2 = 1. a square matrix u     rn  n is orthogonal (note the di   erent meanings when
talking about vectors versus matrices) if all its columns are orthogonal to each other and are
normalized (the columns are then referred to as being orthonormal ).

it follows immediately from the de   nition of orthogonality and normality that

u t u = i = u u t .

in other words, the inverse of an orthogonal matrix is its transpose. note that if u is not
square     i.e., u     rm  n, n < m     but its columns are still orthonormal, then u t u = i,
but u u t 6= i. we generally only use the term orthogonal to describe the previous case,
where u is square.

another nice property of orthogonal matrices is that operating on a vector with an

orthogonal matrix will not change its euclidean norm, i.e.,

for any x     rn, u     rn  n orthogonal.

ku xk2 = kxk2

3.9 range and nullspace of a matrix

the span of a set of vectors {x1, x2, . . . xn} is the set of all vectors that can be expressed as
a linear combination of {x1, . . . , xn}. that is,

span({x1, . . . xn}) =(v : v =

  ixi,   i     r) .

n

xi=1

1it   s easy to get confused and think that non-singular means non-invertible. but in fact, it means the

opposite! watch out!

12

it can be shown that if {x1, . . . , xn} is a set of n linearly independent vectors, where each
xi     rn, then span({x1, . . . xn}) = rn. in other words, any vector v     rn can be written as
a linear combination of x1 through xn. the projection of a vector y     rm onto the span
of {x1, . . . , xn} (here we assume xi     rm) is the vector v     span({x1, . . . xn}), such that v
is as close as possible to y, as measured by the euclidean norm kv     yk2. we denote the
projection as proj(y; {x1, . . . , xn}) and can de   ne it formally as,

proj(y; {x1, . . . xn}) = argminv   span({x1,...,xn})ky     vk2.

the range (sometimes also called the columnspace) of a matrix a     rm  n, denoted

r(a), is the the span of the columns of a. in other words,

r(a) = {v     rm : v = ax, x     rn}.

making a few technical assumptions (namely that a is full rank and that n < m), the
projection of a vector y     rm onto the range of a is given by,

proj(y; a) = argminv   r(a)kv     yk2 = a(at a)   1at y .

this last equation should look extremely familiar, since it is almost the same formula we
derived in class (and which we will soon derive again) for the least squares estimation of
parameters. looking at the de   nition for the projection, it should not be too hard to
convince yourself that this is in fact the same objective that we minimized in our least
squares problem (except for a squaring of the norm, which doesn   t a   ect the optimal point)
and so these problems are naturally very connected. when a contains only a single column,
a     rm, this gives the special case for a projection of a vector on to a line:

proj(y; a) =

aat
at a

y .

the nullspace of a matrix a     rm  n, denoted n (a) is the set of all vectors that equal

0 when multiplied by a, i.e.,

n (a) = {x     rn : ax = 0}.

note that vectors in r(a) are of size m, while vectors in the n (a) are of size n, so vectors
in r(at ) and n (a) are both in rn. in fact, we can say much more. it turns out that

(cid:8)w : w = u + v, u     r(at ), v     n (a)(cid:9) = rn and r(at )     n (a) =     .

in other words, r(at ) and n (a) are disjoint subsets that together span the entire space of
rn. sets of this type are called orthogonal complements, and we denote this r(at ) =
n (a)   .

13

3.10 the determinant

the determinant of a square matrix a     rn  n, is a function det : rn  n     r, and is
denoted |a| or det a (like the trace operator, we usually omit parentheses). algebraically,
one could write down an explicit formula for the determinant of a, but this unfortunately
gives little intuition about its meaning. instead, we   ll start out by providing a geometric
interpretation of the determinant and then visit some of its speci   c algebraic properties
afterwards.

given a matrix

    at
1    
    at
2    
...
    at
n    

   
            

,

   
            

consider the set of points s     rn formed by taking all possible linear combinations of the
row vectors a1, . . . , an     rn of a, where the coe   cients of the linear combination are all
between 0 and 1; that is, the set s is the restriction of span({a1, . . . , an}) to only those
linear combinations whose coe   cients   1, . . . ,   n satisfy 0       i     1, i = 1, . . . , n. formally,

s = {v     rn : v =

  iai where 0       i     1, i = 1, . . . , n}.

n

xi=1

the absolute value of the determinant of a, it turns out, is a measure of the    volume    of
the set s.2

for example, consider the 2    2 matrix,

here, the rows of the matrix are

3 2 (cid:21) .
a =(cid:20) 1 3

3 (cid:21)
a1 =(cid:20) 1

2 (cid:21) .
a2 =(cid:20) 3

(1)

the set s corresponding to these rows is shown in figure 1. for two-dimensional matrices,
s generally has the shape of a parallelogram. in our example, the value of the determinant
is |a| =    7 (as can be computed using the formulas shown later in this section), so the area
of the parallelogram is 7. (verify this for yourself!)

in three dimensions, the set s corresponds to an object known as a parallelepiped (a three-
dimensional box with skewed sides, such that every face has the shape of a parallelogram).
the absolute value of the determinant of the 3    3 matrix whose rows de   ne s give the
three-dimensional volume of the parallelepiped. in even higher dimensions, the set s is an
object known as an n-dimensional parallelotope.

2admittedly, we have not actually de   ned what we mean by    volume    here, but hopefully the intuition
should be clear enough. when n = 2, our notion of    volume    corresponds to the area of s in the cartesian
plane. when n = 3,    volume    corresponds with our usual notion of volume for a three-dimensional object.

14

(4, 5)

(1, 3)

a1

(0, 0)

(3, 2)

a2

figure 1: illustration of the determinant for the 2    2 matrix a given in (1). here, a1 and a2
are vectors corresponding to the rows of a, and the set s corresponds to the shaded region
(i.e., the parallelogram). the absolute value of the determinant, |deta| = 7, is the area of
the parallelogram.

algebraically, the determinant satis   es the following three properties (from which all

other properties follow, including the general formula):

1. the determinant of the identity is 1, |i| = 1. (geometrically, the volume of a unit

hypercube is 1).

2. given a matrix a     rn  n, if we multiply a single row in a by a scalar t     r, then the

determinant of the new matrix is t|a|,

   
            

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
   
            

    t at
1    
    at
2    
...
    at
m    

= t|a|.

    at
2    
    at
1    
...
    at
m    

=    |a|.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
   
            

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
   
            

(geometrically, multiplying one of the sides of the set s by a factor t causes the volume
to increase by a factor t.)

3. if we exchange any two rows at

i and at

j of a, then the determinant of the new matrix

is    |a|, for example

in case you are wondering, it is not immediately obvious that a function satisfying the above
three properties exists. in fact, though, such a function does exist, and is unique (which we
will not prove here).

several properties that follow from the three properties above include:

15

    for a     rn  n, |a| = |at |.

    for a, b     rn  n, |ab| = |a||b|.

    for a     rn  n, |a| = 0 if and only if a is singular (i.e., non-invertible). (if a is singular
then it does not have full rank, and hence its columns are linearly dependent. in this
case, the set s corresponds to a       at sheet    within the n-dimensional space and hence
has zero volume.)

    for a     rn  n and a non-singular, |a   1| = 1/|a|.

before giving the general de   nition for the determinant, we de   ne, for a     rn  n, a\i,\j    
r(n   1)  (n   1) to be the matrix that results from deleting the ith row and jth column from a.
the general (recursive) formula for the determinant is

|a| =

=

n

n

xi=1
xj=1

(   1)i+jaij|a\i,\j|

(for any j     1, . . . , n)

(   1)i+jaij|a\i,\j|

(for any i     1, . . . , n)

with the initial case that |a| = a11 for a     r1  1.
if we were to expand this formula
completely for a     rn  n, there would be a total of n! (n factorial) di   erent terms. for this
reason, we hardly ever explicitly write the complete equation of the determinant for matrices
bigger than 3    3. however, the equations for determinants of matrices up to size 3    3 are
fairly common, and it is good to know them:

|[a11]| = a11

(cid:12)(cid:12)(cid:12)(cid:12)

a21 a22 (cid:21)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:20) a11 a12
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
   
   

a11 a12 a13
a21 a22 a23
a31 a32 a33

   
   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

= a11a22     a12a21

=

a11a22a33 + a12a23a31 + a13a21a32

   a11a23a32     a12a21a33     a13a22a31

the classical adjoint (often just called the adjoint) of a matrix a     rn  n, is denoted

adj(a), and de   ned as

adj(a)     rn  n,

(adj(a))ij = (   1)i+j|a\j,\i|

(note the switch in the indices a\j,\i). it can be shown that for any nonsingular a     rn  n,

a   1 =

1
|a|

adj(a) .

while this is a nice    explicit    formula for the inverse of matrix, we should note that, numer-
ically, there are in fact much more e   cient ways of computing the inverse.

16

3.11 quadratic forms and positive semide   nite matrices

given a square matrix a     rn  n and a vector x     rn, the scalar value xt ax is called a
quadratic form. written explicitly, we see that

note that,

xt ax =

n

n

aijxj! =

xi  n
xj=1

xi=1

xi(ax)i =

xi=1
xt ax = (xt ax)t = xt at x = xt (cid:18) 1

2

n

n

xi=1

xj=1
at(cid:19) x,

a +

1
2

aijxixj

.

where the    rst equality follows from the fact that the transpose of a scalar is equal to
itself, and the second equality follows from the fact that we are averaging two quantities
which are themselves equal. from this, we can conclude that only the symmetric part of
a contributes to the quadratic form. for this reason, we often implicitly assume that the
matrices appearing in a quadratic form are symmetric.

we give the following de   nitions:

    a symmetric matrix a     sn is positive de   nite (pd) if for all non-zero vectors
x     rn, xt ax > 0. this is usually denoted a     0 (or just a > 0), and often times the
set of all positive de   nite matrices is denoted sn

++.

    a symmetric matrix a     sn is positive semide   nite (psd) if for all vectors xt ax    
0. this is written a (cid:23) 0 (or just a     0), and the set of all positive semide   nite matrices
is often denoted sn
+.

    likewise, a symmetric matrix a     sn is negative de   nite (nd), denoted a     0 (or

just a < 0) if for all non-zero x     rn, xt ax < 0.

    similarly, a symmetric matrix a     sn is negative semide   nite (nsd), denoted

a (cid:22) 0 (or just a     0) if for all x     rn, xt ax     0.

    finally, a symmetric matrix a     sn is inde   nite, if it is neither positive semide   nite
1 ax1 > 0 and

nor negative semide   nite     i.e., if there exists x1, x2     rn such that xt
xt
2 ax2 < 0.

it should be obvious that if a is positive de   nite, then    a is negative de   nite and vice
versa. likewise, if a is positive semide   nite then    a is negative semide   nite and vice versa.
if a is inde   nite, then so is    a.

one important property of positive de   nite and negative de   nite matrices is that they
are always full rank, and hence, invertible. to see why this is the case, suppose that some
matrix a     rn  n is not full rank. then, suppose that the jth column of a is expressible as
a linear combination of other n     1 columns:

xiai,

aj =xi6=j

17

for some x1, . . . , xj   1, xj+1, . . . , xn     r. setting xj =    1, we have

ax =

n

xi=1

xiai = 0.

but this implies xt ax = 0 for some non-zero vector x, so a must be neither positive de   nite
nor negative de   nite. therefore, if a is either positive de   nite or negative de   nite, it must
be full rank.

finally, there is one type of positive de   nite matrix that comes up frequently, and so
deserves some special mention. given any matrix a     rm  n (not necessarily symmetric or
even square), the matrix g = at a (sometimes called a gram matrix ) is always positive
semide   nite. further, if m     n (and we assume for convenience that a is full rank), then
g = at a is positive de   nite.

3.12 eigenvalues and eigenvectors

given a square matrix a     rn  n, we say that        c is an eigenvalue of a and x     cn is
the corresponding eigenvector 3 if

ax =   x, x 6= 0.

intuitively, this de   nition means that multiplying a by the vector x results in a new vector
that points in the same direction as x, but scaled by a factor   . also note that for any
eigenvector x     cn, and scalar t     c, a(cx) = cax = c  x =   (cx), so cx is also an
eigenvector. for this reason when we talk about    the    eigenvector associated with   , we
usually assume that the eigenvector is normalized to have length 1 (this still creates some
ambiguity, since x and    x will both be eigenvectors, but we will have to live with this).

we can rewrite the equation above to state that (  , x) is an eigenvalue-eigenvector pair

of a if,

(  i     a)x = 0, x 6= 0.

but (  i     a)x = 0 has a non-zero solution to x if and only if (  i     a) has a non-empty
nullspace, which is only the case if (  i     a) is singular, i.e.,

|(  i     a)| = 0.

we can now use the previous de   nition of the determinant to expand this expression
into a (very large) polynomial in   , where    will have maximum degree n. we then    nd
the n (possibly complex) roots of this polynomial to    nd the n eigenvalues   1, . . . ,   n. to
   nd the eigenvector corresponding to the eigenvalue   i, we simply solve the linear equation
(  ii     a)x = 0.
it should be noted that this is not the method which is actually used

3note that    and the entries of x are actually in c, the set of complex numbers, not just the reals; we
will see shortly why this is necessary. don   t worry about this technicality for now, you can think of complex
vectors in the same way as real vectors.

18

in practice to numerically compute the eigenvalues and eigenvectors (remember that the
complete expansion of the determinant has n! terms); it is rather a mathematical argument.
the following are properties of eigenvalues and eigenvectors (in all cases assume a     rn  n

has eigenvalues   i, . . . ,   n and associated eigenvectors x1, . . . xn):

    the trace of a a is equal to the sum of its eigenvalues,

tra =

  i.

n

xi=1

    the determinant of a is equal to the product of its eigenvalues,

|a| =

  i.

n

yi=1

    the rank of a is equal to the number of non-zero eigenvalues of a.

    if a is non-singular then 1/  i is an eigenvalue of a   1 with associated eigenvector xi,
i.e., a   1xi = (1/  i)xi. (to prove this, take the eigenvector equation, axi =   ixi and
left-multiply each side by a   1.)

    the eigenvalues of a diagonal matrix d = diag(d1, . . . dn) are just the diagonal entries

d1, . . . dn.

we can write all the eigenvector equations simultaneously as

ax = x  

where the columns of x     rn  n are the eigenvectors of a and    is a diagonal matrix whose
entries are the eigenvalues of a, i.e.,

x     rn  n =   
   

|
|
x1 x2
|
|

|
         xn

|    
   

,    = diag(  1, . . . ,   n).

if the eigenvectors of a are linearly independent, then the matrix x will be invertible, so
a = x  x    1. a matrix that can be written in this form is called diagonalizable.

3.13 eigenvalues and eigenvectors of symmetric matrices

two remarkable properties come about when we look at the eigenvalues and eigenvectors
of a symmetric matrix a     sn. first, it can be shown that all the eigenvalues of a are
real. secondly, the eigenvectors of a are orthonormal, i.e., the matrix x de   ned above is an
orthogonal matrix (for this reason, we denote the matrix of eigenvectors as u in this case).

19

we can therefore represent a as a = u   u t , remembering from above that the inverse of
an orthogonal matrix is just its transpose.

using this, we can show that the de   niteness of a matrix depends entirely on the sign of

its eigenvalues. suppose a     sn = u   u t . then

xt ax = xt u   u t x = yt   y =

  iy2
i

n

xi=1

where y = u t x (and since u is full rank, any vector y     rn can be represented in this form).
because y2
i is always positive, the sign of this expression depends entirely on the   i   s. if all
  i > 0, then the matrix is positive de   nite; if all   i     0, it is positive semide   nite. likewise,
if all   i < 0 or   i     0, then a is negative de   nite or negative semide   nite respectively.
finally, if a has both positive and negative eigenvalues, it is inde   nite.

an application where eigenvalues and eigenvectors come up frequently is in maximizing
in particular, for a matrix a     sn, consider the following

some function of a matrix.
maximization problem,

maxx   rn xt ax

subject to kxk2

2 = 1

i.e., we want to    nd the vector (of norm 1) which maximizes the quadratic form. assuming
the eigenvalues are ordered as   1       2     . . .       n, the optimal x for this optimization
problem is x1, the eigenvector corresponding to   1. in this case the maximal value of the
quadratic form is   1. similarly, the optimal solution to the minimization problem,

minx   rn xt ax

subject to kxk2

2 = 1

is xn, the eigenvector corresponding to   n, and the minimal value is   n. this can be proved by
appealing to the eigenvector-eigenvalue form of a and the properties of orthogonal matrices.
however, in the next section we will see a way of showing it directly using matrix calculus.

4 matrix calculus

while the topics in the previous sections are typically covered in a standard course on linear
algebra, one topic that does not seem to be covered very often (and which we will use
extensively) is the extension of calculus to the vector setting. despite the fact that all the
actual calculus we use is relatively trivial, the notation can often make things look much
more di   cult than they are.
in this section we present some basic de   nitions of matrix
calculus and provide a few examples.

4.1 the gradient

suppose that f : rm  n     r is a function that takes as input a matrix a of size m    n and
returns a real value. then the gradient of f (with respect to a     rm  n) is the matrix of

20

partial derivatives, de   ned as:

   af (a)     rm  n =

i.e., an m    n matrix with

   f (a)
   a11
   f (a)
   a21

...

   f (a)
   a12
   f (a)
   a22

...

   f (a)
   am1

   f (a)
   am2

        
        
. . .
        

   
               

   f (a)
   a1n
   f (a)
   a2n

...

   f (a)
   amn

   
               

(   af (a))ij =

   f (a)
   aij

.

note that the size of    af (a) is always the same as the size of a. so if, in particular, a is
just a vector x     rn,

   xf (x) =

   
               

   f (x)
   x1
   f (x)
   x2

...

   f (x)
   xn

.

   
               

it is very important to remember that the gradient of a function is only de   ned if the function
is real-valued, that is, if it returns a scalar value. we can not, for example, take the gradient
of ax, a     rn  n with respect to x, since this quantity is vector-valued.

it follows directly from the equivalent properties of partial derivatives that:

       x(f (x) + g(x)) =    xf (x) +    xg(x).

    for t     r,    x(t f (x)) = t   xf (x).

in principle, gradients are a natural extension of partial derivatives to functions of mul-
tiple variables. in practice, however, working with gradients can sometimes be tricky for
notational reasons. for example, suppose that a     rm  n is a matrix of    xed coe   cients
and suppose that b     rm is a vector of    xed coe   cients. let f : rm     r be the function
de   ned by f (z) = zt z, such that    zf (z) = 2z. but now, consider the expression,

   f (ax).

how should this expression be interpreted? there are at least two possibilities:

1. in the    rst interpretation, recall that    zf (z) = 2z. here, we interpret    f (ax) as

evaluating the gradient at the point ax, hence,

   f (ax) = 2(ax) = 2ax     rm.

2. in the second interpretation, we consider the quantity f (ax) as a function of the input

variables x. more formally, let g(x) = f (ax). then in this interpretation,

   f (ax) =    xg(x)     rn.

21

here, we can see that these two interpretations are indeed di   erent. one interpretation yields
an m-dimensional vector as a result, while the other interpretation yields an n-dimensional
vector as a result! how can we resolve this?

here, the key is to make explicit the variables which we are di   erentiating with respect
to. in the    rst case, we are di   erentiating the function f with respect to its arguments z and
then substituting the argument ax. in the second case, we are di   erentiating the composite
function g(x) = f (ax) with respect to x directly. we denote the    rst case as    zf (ax) and
the second case as    xf (ax).4 keeping the notation clear is extremely important (as you   ll
   nd out in your homework, in fact!).

4.2 the hessian

suppose that f : rn     r is a function that takes a vector in rn and returns a real number.
then the hessian matrix with respect to x, written    2
xf (x) or simply as h is the n    n
matrix of partial derivatives,

   2

xf (x)     rn  n =

   
               

in other words,    2

xf (x)     rn  n, with

    2f (x)

   x2

1

    2f (x)
   x2   x1

...

    2f (x)
   x1   x2
    2f (x)

   x2

2

...

    2f (x)
   xn   x1

    2f (x)
   xn   x2

        

        
. . .
        

    2f (x)
   x1   xn
    2f (x)
   x2   xn

...

    2f (x)

   x2

n

.

   
               

(   2

xf (x))ij =

   2f (x)
   xi   xj

.

note that the hessian is always symmetric, since

   2f (x)
   xi   xj

=

   2f (x)
   xj   xi

.

similar to the gradient, the hessian is de   ned only when f (x) is real-valued.

it is natural to think of the gradient as the analogue of the    rst derivative for functions
of vectors, and the hessian as the analogue of the second derivative (and the symbols we
use also suggest this relation). this intuition is generally correct, but there a few caveats to
keep in mind.

4a drawback to this notation that we will have to live with is the fact that in the    rst case,    zf (ax) it
appears that we are di   erentiating with respect to a variable that does not even appear in the expression
being di   erentiated! for this reason, the    rst case is often written as    f (ax), and the fact that we are
di   erentiating with respect to the arguments of f is understood. however, the second case is always written
as    xf (ax).

22

first, for real-valued functions of one variable f : r     r, it is a basic de   nition that the

second derivative is the derivative of the    rst derivative, i.e.,

   2f (x)
   x2 =

   
   x

   
   x

f (x).

however, for functions of a vector, the gradient of the function is a vector, and we cannot
take the gradient of a vector     i.e.,

   x   xf (x) =    x

   
               

   f (x)
   x1
   f (x)
   x2

...

   f (x)
   x1

   
               

and this expression is not de   ned. therefore, it is not the case that the hessian is the
gradient of the gradient. however, this is almost true, in the following sense: if we look at
the ith entry of the gradient (   xf (x))i =    f (x)/   xi, and take the gradient with respect to
x we get

=

   x

   xi

   f (x)

    2f (x)
   xi   x1
    2f (x)
   xi   x2

   
               
xf (x) =(cid:2)    x(   xf (x))1    x(   xf (x))2

   f (x)
   xi   xn

...

   
               
            x(   xf (x))n (cid:3) .

   2

which is the ith column (or row) of the hessian. therefore,

if we don   t mind being a little bit sloppy we can say that (essentially)    2
xf (x) =    x(   xf (x))t ,
so long as we understand that this really means taking the gradient of each entry of (   xf (x))t ,
not the gradient of the whole vector.

finally, note that while we can take the gradient with respect to a matrix a     rn, for
the purposes of this class we will only consider taking the hessian with respect to a vector
x     rn. this is simply a matter of convenience (and the fact that none of the calculations
we do require us to    nd the hessian with respect to a matrix), since the hessian with respect
to a matrix would have to represent all the partial derivatives    2f (a)/(   aij   ak   ), and it is
rather cumbersome to represent this as a matrix.

4.3 gradients and hessians of quadratic and linear functions

now let   s try to determine the gradient and hessian matrices for a few simple functions. it
should be noted that all the gradients given here are special cases of the gradients given in
the cs229 lecture notes.

23

for x     rn, let f (x) = bt x for some known vector b     rn. then

n

so

f (x) =

   f (x)
   xk

=

   
   xk

bixi

bixi = bk.

xi=1
xi=1

n

from this we can easily see that    xbt x = b. this should be compared to the analogous
situation in single variable calculus, where    /(   x) ax = a.

now consider the quadratic function f (x) = xt ax for a     sn. remember that

n

n

to take the partial derivative, we   ll consider the terms including xk and x2

k factors separately:

f (x) =

aijxixj.

xi=1

xj=1

   f (x)
   xk

aijxixj

n

n

   
   xk

   

=

=

xi=1
xj=1
   xk "xi6=k xj6=k
= xi6=k
aikxi +xj6=k
xi=1
xj=1

aikxi +

=

n

n

akjxj + 2akkxk

n

akjxj = 2

akixi,

xi=1

aijxixj +xi6=k

aikxixk +xj6=k

akjxkxj + akkx2

k#

where the last equality follows since a is symmetric (which we can safely assume, since it is
appearing in a quadratic form). note that the kth entry of    xf (x) is just the inner product
of the kth row of a and x. therefore,    xxt ax = 2ax. again, this should remind you of
the analogous fact in single-variable calculus, that    /(   x) ax2 = 2ax.

finally, let   s look at the hessian of the quadratic function f (x) = xt ax (it should be

obvious that the hessian of a linear function bt x is zero). in this case,

   2f (x)
   xk   x   

=

   

   xk (cid:20)   f (x)

   x    (cid:21) =

   

   xk " n
xi=1

a   ixi# = 2a   k = 2ak   .

therefore, it should be clear that    2
again analogous to the single-variable fact that    2/(   x2) ax2 = 2a).

xxt ax = 2a, which should be entirely expected (and

to recap,
       xbt x = b

       xxt ax = 2ax (if a symmetric)

       2

xxt ax = 2a (if a symmetric)

24

4.4 least squares

let   s apply the equations we obtained in the last section to derive the least squares equations.
suppose we are given matrices a     rm  n (for simplicity we assume a is full rank) and a
vector b     rm such that b 6    r(a). in this situation we will not be able to    nd a vector
x     rn, such that ax = b, so instead we want to    nd a vector x such that ax is as close as
possible to b, as measured by the square of the euclidean norm kax     bk2
2.

using the fact that kxk2

2 = xt x, we have
kax     bk2

2 = (ax     b)t (ax     b)

= xt at ax     2bt ax + bt b

taking the gradient with respect to x we have, and using the properties we derived in the
previous section

   x(xt at ax     2bt ax + bt b) =    xxt at ax        x2bt ax +    xbt b

= 2at ax     2at b

setting this last expression equal to zero and solving for x gives the normal equations

x = (at a)   1at b

which is the same as what we derived in class.

4.5 gradients of the determinant

now let   s consider a situation where we    nd the gradient of a function with respect to
a matrix, namely for a     rn  n, we want to    nd    a|a|. recall from our discussion of
determinants that

n

|a| =

(   1)i+jaij|a\i,\j|

(for any j     1, . . . , n)

so

   

   ak   

|a| =

   

   ak   

xi=1
xi=1

n

(   1)i+jaij|a\i,\j| = (   1)k+   |a\k,\   | = (adj(a))   k.

from this it immediately follows from the properties of the adjoint that

   a|a| = (adj(a))t = |a|a   t .

now let   s consider the function f : sn

++     r, f (a) = log |a|. note that we have to
restrict the domain of f to be the positive de   nite matrices, since this ensures that |a| > 0,
so that the log of |a| is a real number. in this case we can use the chain rule (nothing fancy,
just the ordinary chain rule from single-variable calculus) to see that

    log |a|

   aij

=

    log |a|

   |a|

   |a|
   aij

=

1
|a|

   |a|
   aij

.

25

from this it should be obvious that

   a log |a| =

1
|a|

   a|a| = a   1,

where we can drop the transpose in the last expression because a is symmetric. note the
similarity to the single-valued case, where    /(   x) log x = 1/x.

4.6 eigenvalues as optimization

finally, we use matrix calculus to solve an optimization problem in a way that leads directly
to eigenvalue/eigenvector analysis. consider the following, equality constrained optimization
problem:

maxx   rn xt ax

subject to kxk2

2 = 1

for a symmetric matrix a     sn. a standard way of solving optimization problems with
equality constraints is by forming the lagrangian, an objective function that includes the
equality constraints.5 the lagrangian in this case can be given by

l(x,   ) = xt ax       xt x

where    is called the lagrange multiplier associated with the equality constraint. it can be
established that for x    to be a optimal point to the problem, the gradient of the lagrangian
has to be zero at x    (this is not the only condition, but it is required). that is,

   xl(x,   ) =    x(xt ax       xt x) = 2at x     2  x = 0.

notice that this is just the linear equation ax =   x. this shows that the only points which
can possibly maximize (or minimize) xt ax assuming xt x = 1 are the eigenvectors of a.

5don   t worry if you haven   t seen lagrangians before, as we will cover them in greater detail later in

cs229.

26

