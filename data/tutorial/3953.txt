cs229 lecture notes

andrew ng

part xi
principal components analysis

in our discussion of factor analysis, we gave a way to model data x     rn as
   approximately    lying in some k-dimension subspace, where k     n. specif-
ically, we imagined that each point x(i) was created by    rst generating some
z(i) lying in the k-dimension a   ne space {  z +   ; z     rk}, and then adding
  -covariance noise. factor analysis is based on a probabilistic model, and
parameter estimation used the iterative em algorithm.

in this set of notes, we will develop a method, principal components
analysis (pca), that also tries to identify the subspace in which the data
approximately lies. however, pca will do so more directly, and will require
only an eigenvector calculation (easily done with the eig function in matlab),
and does not need to resort to em.

suppose we are given a dataset {x(i); i = 1, . . . , m} of attributes of m dif-
ferent types of automobiles, such as their maximum speed, turn radius, and
so on. let x(i)     rn for each i (n     m). but unknown to us, two di   erent
attributes   some xi and xj   respectively give a car   s maximum speed mea-
sured in miles per hour, and the maximum speed measured in kilometers per
hour. these two attributes are therefore almost linearly dependent, up to
only small di   erences introduced by rounding o    to the nearest mph or kph.
thus, the data really lies approximately on an n     1 dimensional subspace.
how can we automatically detect, and perhaps remove, this redundancy?

for a less contrived example, consider a dataset resulting from a survey of
pilots for radio-controlled helicopters, where x(i)
is a measure of the piloting
1
skill of pilot i, and x(i)
captures how much he/she enjoys    ying. because
2
rc helicopters are very di   cult to    y, only the most committed students,
ones that truly enjoy    ying, become good pilots. so, the two attributes
indeed, we might posit that that the
x1 and x2 are strongly correlated.

1

data actually likes along some diagonal axis (the u1 direction) capturing the
intrinsic piloting    karma    of a person, with only a small amount of noise
lying o    this axis. (see    gure.) how can we automatically compute this u1
direction?

2

u
1

u
2

)
t
n
e
m
y
o
j
n
e
(

2
x

x1

(skill)

we will shortly develop the pca algorithm. but prior to running pca
per se, typically we    rst pre-process the data to normalize its mean and
variance, as follows:

2. replace each x(i) with x(i)       .

1. let    = 1

3. let   2

j = 1

i=1 x(i).

mpm
mpi(x(i)

j )2

4. replace each x(i)

j with x(i)

j /  j.

steps (1-2) zero out the mean of the data, and may be omitted for data
known to have zero mean (for instance, time series corresponding to speech
or other acoustic signals). steps (3-4) rescale each coordinate to have unit
variance, which ensures that di   erent attributes are all treated on the same
   scale.    for instance, if x1 was cars    maximum speed in mph (taking values
in the high tens or low hundreds) and x2 were the number of seats (taking
values around 2-4), then this reid172 rescales the di   erent attributes
to make them more comparable. steps (3-4) may be omitted if we had
apriori knowledge that the di   erent attributes are all on the same scale. one

3

example of this is if each data point represented a grayscale image, and each
x(i)
took a value in {0, 1, . . . , 255} corresponding to the intensity value of
pixel j in image i.

j

now, having carried out the id172, how do we compute the    ma-
jor axis of variation    u   that is, the direction on which the data approxi-
mately lies? one way to pose this problem is as    nding the unit vector u so
that when the data is projected onto the direction corresponding to u, the
variance of the projected data is maximized. intuitively, the data starts o   
with some amount of variance/information in it. we would like to choose a
direction u so that if we were to approximate the data as lying in the direc-
tion/subspace corresponding to u, as much as possible of this variance is still
retained.

consider the following dataset, on which we have already carried out the

id172 steps:

now, suppose we pick u to correspond the the direction shown in the
   gure below. the circles denote the projections of the original data onto this
line.

4

   
   
   
   
   
   
  
  
    
    
   
   
  
  
  
    
    
    
    
    
    
    
    
    
    
    
    

   
   
   
   
   
   
  
  
   
   
  
  

 
 
  
  
 
 
  
  
  
  
  
  
  
  
  
  
  
  

  
  
    
  

we see that the projected data still has a fairly large variance, and the
points tend to be far from zero. in contrast, suppose had instead picked the
following direction:

         
         
         
         
         
         
         
         
         
         
         
         
         
         
     
     
         
         
     
     
         
         
     
     
         
         
     
     
         
         
     
     
         
         
     
     
         
         
     
     
         
         
    
  
    
     
     
         
         
           
           
  
  
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           

         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
         
 
 
         
         
      
      
 
 
         
         
      
      
  
  
         
         
      
      
  
  
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      

here, the projections have a signi   cantly smaller variance, and are much

closer to the origin.

we would like to automatically select the direction u corresponding to
the    rst of the two    gures shown above. to formalize this, note that given a

5

unit vector u and a point x, the length of the projection of x onto u is given
by xt u. i.e., if x(i) is a point in our dataset (one of the crosses in the plot),
then its projection onto u (the corresponding circle in the    gure) is distance
xt u from the origin. hence, to maximize the variance of the projections, we
would like to choose a unit-length u so as to maximize:

(x(i)t

1
m

m

xi=1

u)2 =

ut x(i)x(i)t

u

m

1
m

xi=1
= ut   1

m

xi=1
i=1 x(i)x(i)t

m

x(i)x(i)t! u.

mpm

we easily recognize that the maximizing this subject to ||u||2 = 1 gives the
principal eigenvector of    = 1
, which is just the empirical
covariance matrix of the data (assuming it has zero mean).1

to summarize, we have found that if we wish to    nd a 1-dimensional
subspace with with to approximate the data, we should choose u to be the
principal eigenvector of   . more generally, if we wish to project our data
into a k-dimensional subspace (k < n), we should choose u1, . . . , uk to be the
top k eigenvectors of   . the ui   s now form a new, orthogonal basis for the
data.2

then, to represent x(i) in this basis, we need only compute the corre-

sponding vector

y(i) =   
            

1 x(i)
ut
2 x(i)
ut
...
k x(i)
ut

   
            

    rk.

thus, whereas x(i)     rn, the vector y(i) now gives a lower, k-dimensional,
approximation/representation for x(i). pca is therefore also referred to as
a id84 algorithm. the vectors u1, . . . , uk are called
the    rst k principal components of the data.

remark. although we have shown it formally only for the case of k = 1,
using well-known properties of eigenvectors it is straightforward to show that

1if you haven   t seen this before, try using the method of lagrange multipliers to max-
imize ut   u subject to that ut u = 1. you should be able to show that   u =   u, for some
  , which implies u is an eigenvector of   , with eigenvalue   .

2because    is symmetric, the ui   s will (or always can be chosen to be) orthogonal to

each other.

6

of all possible orthogonal bases u1, . . . , uk, the one that we have chosen max-
2. thus, our choice of a basis preserves as much variability

imizes pi ||y(i)||2

as possible in the original data.

in problem set 4, you will see that pca can also be derived by picking
the basis that minimizes the approximation error arising from projecting the
data onto the k-dimensional subspace spanned by them.

pca has many applications; we will close our discussion with a few exam-
ples. first, compression   representing x(i)   s with lower dimension y(i)   s   is
an obvious application. if we reduce high dimensional data to k = 2 or 3 di-
mensions, then we can also plot the y(i)   s to visualize the data. for instance,
if we were to reduce our automobiles data to 2 dimensions, then we can plot
it (one point in our plot would correspond to one car type, say) to see what
cars are similar to each other and what groups of cars may cluster together.
another standard application is to preprocess a dataset to reduce its
dimension before running a supervised learning learning algorithm with the
x(i)   s as inputs. apart from computational bene   ts, reducing the data   s
dimension can also reduce the complexity of the hypothesis class considered
and help avoid over   tting (e.g., linear classi   ers over lower dimensional input
spaces will have smaller vc dimension).

lastly, as in our rc pilot example, we can also view pca as a noise re-
duction algorithm. in our example it estimates the intrinsic    piloting karma   
from the noisy measures of piloting skill and enjoyment. in class, we also saw
the application of this idea to face images, resulting in eigenfaces method.
here, each point x(i)     r100  100 was a 10000 dimensional vector, with each co-
ordinate corresponding to a pixel intensity value in a 100x100 image of a face.
using pca, we represent each image x(i) with a much lower-dimensional y(i).
in doing so, we hope that the principal components we found retain the inter-
esting, systematic variations between faces that capture what a person really
looks like, but not the    noise    in the images introduced by minor lighting
variations, slightly di   erent imaging conditions, and so on. we then measure
distances between faces i and j by working in the reduced dimension, and
computing ||y(i)     y(j)||2. this resulted in a surprisingly good face-matching
and retrieval algorithm.

