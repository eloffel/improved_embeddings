   #[1]rss [2]slideshare search [3]alternate [4]alternate [5]alternate
   [6]alternate [7]alternate [8]alternate [9]slideshow json oembed profile
   [10]slideshow xml oembed profile [11]alternate [12]alternate
   [13]alternate

   (button)

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our [14]user
   agreement and [15]privacy policy.

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our
   [16]privacy policy and [17]user agreement for details.

   [18]slideshare [19]explore search [20]you

     * [21]linkedin slideshare

     * [22]upload
     * [23]login
     * [24]signup

     *
     * ____________________ (button) submit search

     * [25]home
     * [26]explore

     * [27]presentation courses
     * [28]powerpoint courses
     *
     * by [29]linkedin learning

   ____________________
   successfully reported this slideshow.

   we use your linkedin profile and activity data to personalize ads and
   to show you more relevant ads. [30]you can change your ad preferences
   anytime.
   recurrent neural networks ii (d2l3 deep learning for speech and
   language upc 2017)

   [course site] day 2 lecture 3 recurrent neural networks ii santiago
   pascual

   2 outline 1. lstm latest improvement: phased-lstm 2. causal
   convolutions 3. quasi-id56 4. main ideas to be kept

   recap lstm    an lstm cell is defined by two groups of neurons plus the
   cell state (memory unit): 1. gates 2. activation uni...

   4 phased lstm extend the lstm cell by adding a new time gate,
   controlled by a parametrized oscillation with a frequency ra...

   5 phased lstm extend the lstm cell by adding a new time gate,
   controlled by a parametrized oscillation with a frequency ra...

   6 phased lstm 3 phases in kt gate: in first 2 phases the    openness    of
   the gate rises from 0 to 1 (first phase) and from 1...

   7 phased lstm 3 phases in kt gate: in first 2 phases the    openness    of
   the gate rises from 0 to 1 (first phase) and from 1...

   8 phased lstm updates happen when the cell gate is open, as depicted in
   the colored figure below, where the different cell...

   phased lstm really faster convergence than vanilla lstm architecture,
   as well as more long-term memory retention, as they ...

   1d convolutions hi there how are you doing ? 100 dim let   s say we have
   a sequence of 100 dimensional vectors describing wo...

   1d convolutions we can apply a 1d convolutional activation over the 2d
   matrix: for an arbitrari kernel of width=3 100 dim ...

   1d convolutions keep in mind we are working with 100 dimensions
   although here we depict just one for simplicity 1 2 3 4 5 ...

   1d convolutions when we add zero padding, we normally do so on both
   sides of the sequence (as in image padding) 1 2 3 4 5 ...

   causal 1d convolutions add the zero padding just on the left side of
   the sequence, not symmetrically 1 2 3 4 5 6 7 8 w1 w2...

   quasi-id56 red blocks are parameterized projections (layer
   activations/dot products) and blue blocks are pooling operations...

   quasi-id56 qid56: merge the best of both worlds to speed up the
   operations for when we need to process large sequences. a qr...

   breaking down quasi-id56    having a sequence of length t with
   n-dimensional vectors compute the layer input activations (lik...

   breaking down quasi-id56    finally, pooling stage is the recursive part,
   where order is imposed (thus it becomes an id56-like...

   main ideas to be kept regarding id56s     id56s are very good to process
   sequences, so whenever order matters     think about in...

   20 thanks ! q&a ? @santty128
   upcoming slideshare
   []
   loading in    5
     
   [] 1
   (button)
   1 of 20 (button)
   (button) (button)
   like this presentation? why not share!
     * share
     * email
     *
     *

     * [31]recurrent neural networks i (d2l2 d... recurrent neural
       networks i (d2l2 d... by universitat polit... 1575 views
     * [32]the id88 (d1l2 deep learning ... the id88 (d1l2
       deep learning ... by universitat polit... 2002 views
     * [33]language model (d3l1 deep learning ... language model (d3l1
       deep learning ... by universitat polit... 816 views
     * [34]id103 with deep neural... id103 with
       deep neural... by universitat polit... 5777 views
     * [35]convolutional neural networks (d1l3... convolutional neural
       networks (d1l3... by universitat polit... 1274 views
     * [36]advanced id4... advanced neural machine
       translation... by universitat polit... 652 views

   (button)

   share slideshare
     __________________________________________________________________

     * [37]facebook
     * [38]twitter
     * [39]linkedin

   embed
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   size (px)
   start on
   [x] show related slideshares at end
   wordpress shortcode ____________________
   link ____________________

recurrent neural networks ii (d2l3 deep learning for speech and language upc
2017)

   2,188 views

     * (button) share
     * (button) like
     * (button) download
     * ...
          +

   [40]universitat polit  cnica de catalunya

[41]universitat polit  cnica de catalunya

   [42]follow

   (button) (button) (button)

   published on jan 25, 2017

   https://telecombcn-dl.github.io/2017-dlsl/
   winter school on deep learning for speech and language. upc
   barcelonatech etsetb telecombcn.
   the aim of this course is to train students in methods of deep learning
   for speech and language. recurrent neural networks (id56) will be
   presented and analyzed in detail to understand the potential of these
   state of the art tools for time series processing. engineering tips and
   scalability issues will be addressed to solve tasks such as machine
   translation, id103, id133 or question
   answering. hands-on sessions will provide development skills so that
   attendees can become competent in contemporary data analytics tools.
   (button) ...

   published in: [43]data & analytics

     * [44]0 comments
     * [45]2 likes
     * [46]statistics
     * [47]notes

     * full name
       full name
       comment goes here.
       12 hours ago   [48]delete [49]reply [50]block
       are you sure you want to [51]yes [52]no
       your message goes here

   no profile picture user
   ____________________
   [53](button) post
     * be the first to comment

     * [54]sungminyang6
       [55]sungmin yang , living in sweden, korean at junior data
       scientist
       1 year ago
     * [56]yutaroueno1
       [57]yutaro ueno
       1 year ago

   no downloads
   views
   total views
   2,188
   on slideshare
   0
   from embeds
   0
   number of embeds
   3
   actions
   shares
   0
   downloads
   139
   comments
   0
   likes
   2
   embeds 0
   no embeds
   no notes for slide

recurrent neural networks ii (d2l3 deep learning for speech and language upc
2017)

    1. 1. [course site] day 2 lecture 3 recurrent neural networks ii
       santiago pascual
    2. [58]2. 2 outline 1. lstm latest improvement: phased-lstm 2. causal
       convolutions 3. quasi-id56 4. main ideas to be kept
    3. [59]3. recap lstm    an lstm cell is defined by two groups of neurons
       plus the cell state (memory unit): 1. gates 2. activation units 3.
       cell state if we don   t have any input, the cell is still applying a
       forget factor!
    4. [60]4. 4 phased lstm extend the lstm cell by adding a new time
       gate, controlled by a parametrized oscillation with a frequency
       range that produces updates of the memory cell only during small
       percentage of the cycle. phased lstm: accelerating recurrent
       network training for long or event-based sequences (neil et al.
       2016)
    5. [61]5. 5 phased lstm extend the lstm cell by adding a new time
       gate, controlled by a parametrized oscillation with a frequency
       range that produces updates of the memory cell only during small
       percentage of the cycle. t ron = fraction of active period s
       learnable parameters
    6. [62]6. 6 phased lstm 3 phases in kt gate: in first 2 phases the
          openness    of the gate rises from 0 to 1 (first phase) and from 1
       to 0 (second phase). during third phase, gate closes and cell state
       is maintained.    parameter is a leaky factor to allow gradient
       flowing during training, 0 during test. t ron = fraction of active
       period s
    7. [63]7. 7 phased lstm 3 phases in kt gate: in first 2 phases the
          openness    of the gate rises from 0 to 1 (first phase) and from 1
       to 0 (second phase). during third phase, gate closes and cell state
       is maintained.    parameter is a leaky factor to allow gradient
       flowing during training, 0 during test. t ron = fraction of active
       period s
    8. [64]8. 8 phased lstm updates happen when the cell gate is open, as
       depicted in the colored figure below, where the different cell
       states are updated when the clock signal is burst.
    9. [65]9. phased lstm really faster convergence than vanilla lstm
       architecture, as well as more long-term memory retention, as they
       preserve info about some time-steps far back in time when kt gate
       is closed, till it   s open again, where current time-step receives
       fresh information about a possibly quite old state.
   10. [66]10. 1d convolutions hi there how are you doing ? 100 dim let   s
       say we have a sequence of 100 dimensional vectors describing words.
       sequence length = 8 arrangein 2d 100 dim sequence length = 8 ,
   11. [67]11. 1d convolutions we can apply a 1d convolutional activation
       over the 2d matrix: for an arbitrari kernel of width=3 100 dim
       sequence length = 8 k1 each 1d convolutional kernel is a 2d matrix
       of size (3, 100) 100 dim
   12. [68]12. 1d convolutions keep in mind we are working with 100
       dimensions although here we depict just one for simplicity 1 2 3 4
       5 6 7 8 w1 w2 w3 w1 w2 w3 w1 w2 w3 w1 w2 w3 w1 w2 w3 1 2 3 4 5 6 w1
       w2 w3 the length result of the convolution is well known to be:
       seqlength - kwidth + 1 = 8 - 3 + 1 = 6 so the output matrix will be
       (6, 100) because there was no padding
   13. [69]13. 1d convolutions when we add zero padding, we normally do so
       on both sides of the sequence (as in image padding) 1 2 3 4 5 6 7 8
       w1 w2 w3 w1 w2 w3 w1 w2 w3 w1 w2 w3 w1 w2 w3 1 2 3 4 5 6 7 8 w1 w2
       w3 the length result of the convolution is well known to be:
       seqlength - kwidth + 1 = 10 - 3 + 1 = 8 so the output matrix will
       be (8, 100) because we had padding 0 0 w1 w2 w3 w1 w2 w3
   14. [70]14. causal 1d convolutions add the zero padding just on the
       left side of the sequence, not symmetrically 1 2 3 4 5 6 7 8 w1 w2
       w3 w1 w2 w3 w1 w2 w3 w1 w2 w3 w1 w2 w3 1 2 3 4 5 6 7 8 w1 w2 w3 the
       length result of the convolution is well known to be: seqlength -
       kwidth + 1 = 10 - 3 + 1 = 8 so the output matrix will be (8, 100)
       because we had padding however: now every time-step t depends on
       the two previous inputs as well as the current time-step     every
       output is causal we make a causal convolution by padding left the
       sequence with (kwidth - 1) zeros 0 w1 w2 w3 w1 w2 w3 0
   15. [71]15. quasi-id56 red blocks are parameterized projections (layer
       activations/dot products) and blue blocks are pooling operations:    
       in the case of lstm it is applying the gates transforms (i, f, o)    
       in the case of id98 it is a parallel pooling regardless of the
       position (remember id98s don   t have a sense of order) advantage of
       id98: we can compute all convolutions in parallel advantage of lstm:
       it imposes the sense of order (appropriate for sequences) qid56s are
       up to x16 times faster than lstms!! quasi-recurrent neural networks
       (bradbury et al. 2016)
   16. [72]16. quasi-id56 qid56: merge the best of both worlds to speed up
       the operations for when we need to process large sequences. a qid56
       layer is composed of a convolutional stage and a pooling stage:    
       conv stage: perform the different activations in parallel (i.e.
       layer activations and gate activations) through time, such that
       each red sub-block is independent.     pooling stage: impose ordering
       information across time. it is a sequential process (as depicted by
       the arrow), yes, but the heavy computation has already been
       performed in a single forward pass in the convolutional stage! make
       all activations in parallel with convolutions and merge
       sequentially, with no recursive weight operations.
   17. [73]17. breaking down quasi-id56    having a sequence of length t with
       n-dimensional vectors compute the layer input activations (like
       lstm) with m conv 1d kernels of wz: we want to apply two learnable
       gates f (forget) and o (output):     note the sigmoid functions when
       it comes to gate activations     because we want them to act like
       switches! (between 0 and 1) consider weights matrices are thus
       containing m kernels, k kernel width each (amount of time-steps
       seen per convolution) and with n input dimensions (like previous
       slides example with 100 dim)     output z is to exemplify: what
       happens when k=2? we process the current time-step and the right
       previous one looks similar to lstm equations, huh?
   18. [74]18. breaking down quasi-id56    finally, pooling stage is the
       recursive part, where order is imposed (thus it becomes an id56-like
       mechanism). note this is just an element-wise product, so quite
       quicker than computing the gate activation at every time-step (like
       in lstm). three different flavors depending on the gates we
       computed:     f-pooling     fo-pooling     ifo-pooling
   19. [75]19. main ideas to be kept regarding id56s     id56s are very good
       to process sequences, so whenever order matters     think about
       including some recurrent module to keep track of temporal evolution
           generally lstm/gru perform similarly, though it is interesting to
       try both by yourselves with any task!     whenever you ask
       yourselves: which cell should i use? if you have lots of data you
       may want to go first with lstms (it has more parameters)     if you
       have to process very long sequences (e.g. 2.000 time-steps)     maybe
       try plstm     if you care about speed/efficiency during training    
       maybe try qid56
   20. [76]20. 20 thanks ! q&a ? @santty128

          [77]recommended

     * teaching techniques: classroom cloud strategy
       teaching techniques: classroom cloud strategy
       online course - linkedin learning
     * communication in the 21st century classroom
       communication in the 21st century classroom
       online course - linkedin learning
     * teaching technical skills through video
       teaching technical skills through video
       online course - linkedin learning
     * recurrent neural networks i (d2l2 deep learning for speech and
       language upc 2017)
       recurrent neural networks i (d2l2 deep learning for speech and
       language upc 2...
       universitat polit  cnica de catalunya
     * the id88 (d1l2 deep learning for speech and language)
       the id88 (d1l2 deep learning for speech and language)
       universitat polit  cnica de catalunya
     * language model (d3l1 deep learning for speech and language upc
       2017)
       language model (d3l1 deep learning for speech and language upc
       2017)
       universitat polit  cnica de catalunya
     * id103 with deep neural networks (d3l2 deep learning
       for speech and language upc 2017)
       id103 with deep neural networks (d3l2 deep learning
       for speech a...
       universitat polit  cnica de catalunya
     * convolutional neural networks (d1l3 deep learning for speech and
       language)
       convolutional neural networks (d1l3 deep learning for speech and
       language)
       universitat polit  cnica de catalunya
     * advanced id4 (d4l2 deep learning for speech
       and language upc 2017)
       advanced id4 (d4l2 deep learning for speech
       and langua...
       universitat polit  cnica de catalunya
     * id27s (d2l4 deep learning for speech and language upc
       2017)
       id27s (d2l4 deep learning for speech and language upc
       2017)
       universitat polit  cnica de catalunya

     * [78]english
     * [79]espa  ol
     * [80]portugu  s
     * [81]fran  ais
     * [82]deutsch

     * [83]about
     * [84]dev & api
     * [85]blog
     * [86]terms
     * [87]privacy
     * [88]copyright
     * [89]support

     *
     *
     *
     *
     *

   linkedin corporation    2019

     

share clipboard
     __________________________________________________________________

   [90]  
     * facebook
     * twitter
     * linkedin

   link ____________________

public clipboards featuring this slide
     __________________________________________________________________

   (button)   
   no public clipboards found for this slide

select another clipboard
     __________________________________________________________________

   [91]  

   looks like you   ve clipped this slide to already.
   ____________________

   create a clipboard

you just clipped your first slide!

   clipping is a handy way to collect important slides you want to go back
   to later. now customize the name of a clipboard to store your clips.
     __________________________________________________________________

   name* ____________________
   description ____________________
   visibility
   others can see my clipboard [ ]
   (button) cancel (button) save

   bizographics tracking image

references

   visible links
   1. https://www.slideshare.net/rss/latest
   2. https://www.slideshare.net/opensearch.xml
   3. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
   4. https://es.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
   5. https://fr.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
   6. https://de.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
   7. https://pt.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
   8. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
   9. https://www.slideshare.net/api/oembed/2?format=json&url=http://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  10. https://www.slideshare.net/api/oembed/2?format=xml&url=http://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  11. https://www.slideshare.net/mobile/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  12. android-app://net.slideshare.mobile/slideshare-app/ss/71385580
  13. ios-app://917418728/slideshare-app/ss/71385580
  14. http://www.linkedin.com/legal/user-agreement
  15. http://www.linkedin.com/legal/privacy-policy
  16. http://www.linkedin.com/legal/privacy-policy
  17. http://www.linkedin.com/legal/user-agreement
  18. https://www.slideshare.net/
  19. https://www.slideshare.net/explore
  20. https://www.slideshare.net/login
  21. https://www.slideshare.net/
  22. https://www.slideshare.net/upload
  23. https://www.slideshare.net/login
  24. https://www.slideshare.net/w/signup
  25. https://www.slideshare.net/
  26. https://www.slideshare.net/explore
  27. https://www.linkedin.com/learning/topics/presentations?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  28. https://www.linkedin.com/learning/topics/powerpoint?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  29. https://www.linkedin.com/learning?trk=slideshare_subnav_learning
  30. https://www.linkedin.com/psettings/privacy
  31. https://public.slidesharecdn.com/xavigiro/recurrent-neural-networks-1-d2l2-deep-learning-for-speech-and-language-upc-2017
  32. https://public.slidesharecdn.com/xavigiro/the-id88-d1l2-deep-learning-for-speech-and-language
  33. https://public.slidesharecdn.com/xavigiro/language-model-d3l1-deep-learning-for-speech-and-language-upc-2017
  34. https://public.slidesharecdn.com/xavigiro/speech-recognition-with-deep-neural-networks-d3l2-deep-learning-for-speech-and-language-upc-2017
  35. https://public.slidesharecdn.com/xavigiro/convolutional-neural-networks-d1l3-deep-learning-for-speech-and-language
  36. https://public.slidesharecdn.com/xavigiro/advanced-neural-machine-translation-d4l2-deep-learning-for-speech-and-language-upc-2017
  37. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  38. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  39. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  40. https://www.slideshare.net/xavigiro?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  41. https://www.slideshare.net/xavigiro?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  42. https://www.slideshare.net/signup?login_source=slideview.popup.follow&from=addcontact&from_source=https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  43. https://www.slideshare.net/featured/category/data-analytics
  44. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017#comments-panel
  45. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017#likes-panel
  46. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017#stats-panel
  47. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017#notes-panel
  48. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  49. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  50. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  51. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  52. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  53. https://www.slideshare.net/signup?login_source=slideview.popup.comment&from=comments&from_source=https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  54. https://www.slideshare.net/sungminyang6?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  55. https://www.slideshare.net/sungminyang6?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  56. https://www.slideshare.net/yutaroueno1?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  57. https://www.slideshare.net/yutaroueno1?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  58. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-2-638.jpg?cb=1485365099
  59. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-3-638.jpg?cb=1485365099
  60. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-4-638.jpg?cb=1485365099
  61. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-5-638.jpg?cb=1485365099
  62. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-6-638.jpg?cb=1485365099
  63. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-7-638.jpg?cb=1485365099
  64. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-8-638.jpg?cb=1485365099
  65. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-9-638.jpg?cb=1485365099
  66. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-10-638.jpg?cb=1485365099
  67. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-11-638.jpg?cb=1485365099
  68. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-12-638.jpg?cb=1485365099
  69. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-13-638.jpg?cb=1485365099
  70. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-14-638.jpg?cb=1485365099
  71. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-15-638.jpg?cb=1485365099
  72. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-16-638.jpg?cb=1485365099
  73. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-17-638.jpg?cb=1485365099
  74. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-18-638.jpg?cb=1485365099
  75. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-19-638.jpg?cb=1485365099
  76. https://image.slidesharecdn.com/dlsl2017d2l3recurrentneuralnetworksii-170125171005/95/recurrent-neural-networks-ii-d2l3-deep-learning-for-speech-and-language-upc-2017-20-638.jpg?cb=1485365099
  77. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017#related-tab-content
  78. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  79. https://es.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  80. https://pt.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  81. https://fr.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  82. https://de.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  83. https://www.slideshare.net/about
  84. https://www.slideshare.net/developers
  85. http://blog.slideshare.net/
  86. https://www.slideshare.net/terms
  87. https://www.slideshare.net/privacy
  88. http://www.linkedin.com/legal/copyright-policy
  89. https://www.linkedin.com/help/slideshare
  90. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  91. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017

   hidden links:
  93. https://www.slideshare.net/xavigiro/recurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  94. https://www.slideshare.net/signup?login_source=slideview.clip.like&from=clip&layout=foundation&from_source=
  95. https://www.slideshare.net/login?from_source=%2fxavigiro%2frecurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017%3ffrom_action%3dsave&from=download&layout=foundation
  96. https://www.slideshare.net/signup?login_source=slideview.popup.flags&from=flagss&from_source=https%3a%2f%2fwww.slideshare.net%2fxavigiro%2frecurrent-neural-networks-2-d2l3-deep-learning-for-speech-and-language-upc-2017
  97. https://www.linkedin.com/learning/teaching-techniques-classroom-cloud-strategy?trk=slideshare_sv_learning
  98. https://www.linkedin.com/learning/communication-in-the-21st-century-classroom?trk=slideshare_sv_learning
  99. https://www.linkedin.com/learning/teaching-technical-skills-through-video?trk=slideshare_sv_learning
 100. https://www.slideshare.net/xavigiro/recurrent-neural-networks-1-d2l2-deep-learning-for-speech-and-language-upc-2017
 101. https://www.slideshare.net/xavigiro/the-id88-d1l2-deep-learning-for-speech-and-language
 102. https://www.slideshare.net/xavigiro/language-model-d3l1-deep-learning-for-speech-and-language-upc-2017
 103. https://www.slideshare.net/xavigiro/speech-recognition-with-deep-neural-networks-d3l2-deep-learning-for-speech-and-language-upc-2017
 104. https://www.slideshare.net/xavigiro/convolutional-neural-networks-d1l3-deep-learning-for-speech-and-language
 105. https://www.slideshare.net/xavigiro/advanced-neural-machine-translation-d4l2-deep-learning-for-speech-and-language-upc-2017
 106. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
 107. http://www.linkedin.com/company/linkedin
 108. http://www.facebook.com/linkedin
 109. http://twitter.com/slideshare
 110. http://www.google.com/+linkedin
 111. https://www.slideshare.net/rss/latest
