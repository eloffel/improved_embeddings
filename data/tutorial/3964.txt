   #[1]rss [2]slideshare search [3]alternate [4]alternate [5]alternate
   [6]alternate [7]alternate [8]alternate [9]slideshow json oembed profile
   [10]slideshow xml oembed profile [11]alternate [12]alternate
   [13]alternate

   (button)

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our [14]user
   agreement and [15]privacy policy.

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our
   [16]privacy policy and [17]user agreement for details.

   [18]slideshare [19]explore search [20]you

     * [21]linkedin slideshare

     * [22]upload
     * [23]login
     * [24]signup

     *
     * ____________________ (button) submit search

     * [25]home
     * [26]explore

     * [27]presentation courses
     * [28]powerpoint courses
     *
     * by [29]linkedin learning

   ____________________
   successfully reported this slideshow.

   we use your linkedin profile and activity data to personalize ads and
   to show you more relevant ads. [30]you can change your ad preferences
   anytime.
   id27s (d2l4 deep learning for speech and language upc 2017)

   [course site] day 2 lecture 4 id27s id97 antonio
   bonafonte

   2 christopher olah visualizing representations

   3 representation of categorical features newsgroup task: input: 1000
   words (from a fixed vocabulary v, size |v|) variable ...

   4 one-hot (one-of-n) encoding example: letters. |v| = 30    a   : xt =
   [1,0,0, ..., 0]    b   : xt = [0,1,0, ..., 0]    c   : xt = [0,...

   5 one-hot (one-of-n) encoding example: words. cat: xt = [1,0,0, ..., 0]
   dog: xt = [0,1,0, ..., 0] . . mamaguy: xt = [0,0,0...

   6 one-hot encoding of words: limitations     large dimensionality    
   sparse representation (mostly zeros)     blind representat...

   7 id27s     represent words using vectors of dimension d (~100
   - 500)     meaningful (semantic, syntactic) distances...

   8 glove (stanford)

   9 glove (stanford)

   10 word analogy: a is to b as c is to    . find d such as wd is closest
   to wa - wb + wc     athens is to greece as berlin to    ...

   11 how to define word representation? you shall know a word by the
   company it keeps. firth, j. r. 1957 some relevant examp...

   using nn to define embeddings one-hot encoding + fully connected    
   embedding (projection) layer example: language model (p...

   from tensorflow id97 tutorial:
   https://www.tensorflow.org/tutorials/id97/ 13

   toy example: predict next word corpus: the dog saw a cat the dog chased
   a cat the cat climbed a tree |v| = 8 one-hot encod...

   toy example: predict next word architecture: input layer: h1 (x)= wi   
   x hidden layer: h2 (x)= g(wh    h1 (x)) output layer...

   x = y = zeros(8,1) x(2) = y(4) = 1 wi = rand(3,8) - 0.5; wo = rand(8,3)
   - 0.5; wh = rand(3,3); h1 = wi * x a2 = wh * h1 h2...

   input layer: h1 (x)= wi    x (projection layer) note that non-lineal
   activation function in projection layer is irrelevant ...

   hidden layer: hh (x)= g(wh    h1 (x)) softmax layer: z(x) = o(wo    h2
   (x)) 18

   19 computational complexity example: num training data: 1b vocabulary
   size: 100k context: 3 previous words embeddings: 100...

   we can get embeddings implicitly from any task that involves words.
   however, good generic embeddings are good for other ta...

   how to get good embedings:     very large lexicon     huge amount of
   learning data     unsupervised (or trivial labels)     comput...

   architecture specific for produccing embeddings. it is learnt with huge
   amount of data. simplify the architecture: remove ...

   cbow: continuous bag of words the cat climbed a tree given context: a,
   cat, the, tree estimate prob. of climbed 23

   skip-gram the cat climbed a tree given word: climbed estimate prob. of
   context words: a, cat, the, tree (it selects random...

   reduce cost: subsampling most frequent words (is, the) can appear
   hundred of millions of times. less important: paris ~ fr...

   simplify cost: negative sampling softmax is required to get
   probabilities. but our goal here is just to get good embedding...

   27

   id97 is not deep, but used in many tasks using deep learning there
   are other approaches: this is a very popular toolki...

   id97     mikolov, tomas; et al. "efficient estimation of word
   representations in vector space     linguistic regularities ...
   upcoming slideshare
   []
   loading in    5
     
   [] 1
   (button)
   1 of 29 (button)
   (button) (button)
   like this presentation? why not share!
     * share
     * email
     *
     *

     * [31]id97: from intuition to practic... id97: from intuition
       to practic... by edgar marca 528 views
     * [32]word representations in vector space word representations in
       vector space by abdullah khan zehady 2161 views

   (button)

   share slideshare
     __________________________________________________________________

     * [33]facebook
     * [34]twitter
     * [35]linkedin

   embed
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   size (px)
   start on
   [x] show related slideshares at end
   wordpress shortcode ____________________
   link ____________________

id27s (d2l4 deep learning for speech and language upc 2017)

   1,012 views

     * (button) share
     * (button) like
     * (button) download
     * ...
          +

   [36]universitat polit  cnica de catalunya

[37]universitat polit  cnica de catalunya

   [38]follow

   (button) (button) (button)

   published on jan 25, 2017

   https://telecombcn-dl.github.io/2017-dlsl/
   winter school on deep learning for speech and language. upc
   barcelonatech etsetb telecombcn.
   the aim of this course is to train students in methods of deep learning
   for speech and language. recurrent neural networks (id56) will be
   presented and analyzed in detail to understand the potential of these
   state of the art tools for time series processing. engineering tips and
   scalability issues will be addressed to solve tasks such as machine
   translation, id103, id133 or question
   answering. hands-on sessions will provide development skills so that
   attendees can become competent in contemporary data analytics tools.
   (button) ...

   published in: [39]data & analytics

     * [40]0 comments
     * [41]1 like
     * [42]statistics
     * [43]notes

     * full name
       full name
       comment goes here.
       12 hours ago   [44]delete [45]reply [46]block
       are you sure you want to [47]yes [48]no
       your message goes here

   no profile picture user
   ____________________
   [49](button) post
     * be the first to comment

     * [50]mariamelbadri
       [51]maraim masoud elbadri , student
       1 year ago

   no downloads
   views
   total views
   1,012
   on slideshare
   0
   from embeds
   0
   number of embeds
   0
   actions
   shares
   0
   downloads
   124
   comments
   0
   likes
   1
   embeds 0
   no embeds
   no notes for slide

id27s (d2l4 deep learning for speech and language upc 2017)

    1. 1. [course site] day 2 lecture 4 id27s id97 antonio
       bonafonte
    2. [52]2. 2 christopher olah visualizing representations
    3. [53]3. 3 representation of categorical features newsgroup task:
       input: 1000 words (from a fixed vocabulary v, size |v|) variable
       which can take a limited number of possible values e.g.: gender,
       blood types, countries,    , letters, words, phonemes id102
       transcription (cmu dict): input: letters: (a b c   . z     - .) (30
       symbols)
    4. [54]4. 4 one-hot (one-of-n) encoding example: letters. |v| = 30
          a   : xt = [1,0,0, ..., 0]    b   : xt = [0,1,0, ..., 0]    c   : xt =
       [0,0,1, ..., 0] . . .    .   : xt = [0,0,0, ..., 1]
    5. [55]5. 5 one-hot (one-of-n) encoding example: words. cat: xt =
       [1,0,0, ..., 0] dog: xt = [0,1,0, ..., 0] . . mamaguy: xt = [0,0,0,
          ,0,1,0,...,0] . . . number of words, |v| ? b2: 5k c2: 18k lvsr:
       50-100k wikipedia (1.6b): 400k crawl data (42b): 2m
    6. [56]6. 6 one-hot encoding of words: limitations     large
       dimensionality     sparse representation (mostly zeros)     blind
       representation     only operators:    !=    and    ==   
    7. [57]7. 7 id27s     represent words using vectors of
       dimension d (~100 - 500)     meaningful (semantic, syntactic)
       distances     dominant research topic in last years in nlp
       conferences (emnlp)     good embeddings are useful for many other
       tasks
    8. [58]8. 8 glove (stanford)
    9. [59]9. 9 glove (stanford)
   10. [60]10. 10 word analogy: a is to b as c is to    . find d such as wd
       is closest to wa - wb + wc     athens is to greece as berlin to    .    
       dance is to dancing as fly to    . word similarity: closest word to
       ... evaluation of the representation
   11. [61]11. 11 how to define word representation? you shall know a word
       by the company it keeps. firth, j. r. 1957 some relevant examples:
       latent semantic analysis (lsa):     define co-ocurrence matrix of
       words wi in documents wj     apply svd to reduce dimensionality glove
       (global vectors):     start with co-ocurrences of word wi and wj in
       context of wi     fit log-biid75 model of the embeddings
   12. [62]12. using nn to define embeddings one-hot encoding + fully
       connected     embedding (projection) layer example: language model
       (predict next word given previous words) produces id27s
       (bengio 2003) 12
   13. [63]13. from tensorflow id97 tutorial:
       https://www.tensorflow.org/tutorials/id97/ 13
   14. [64]14. toy example: predict next word corpus: the dog saw a cat
       the dog chased a cat the cat climbed a tree |v| = 8 one-hot
       encoding: a: [1,0,0,0,0,0,0,0] cat: [0,1,0,0,0,0,0,0] chased:
       [0,0,1,0,0,0,0,0] climbed: [0,0,0,1,0,0,0,0] dog: [0,0,0,0,1,0,0,0]
       saw: [0,0,0,0,0,1,0,0] the: [0,0,0,0,0,0,1,0] tree:
       [0,0,0,0,0,0,0,1] 14
   15. [65]15. toy example: predict next word architecture: input layer:
       h1 (x)= wi    x hidden layer: h2 (x)= g(wh    h1 (x)) output layer:
       z(x) = o(wo    h2 (x)) training sample: cat     climbed 15
   16. [66]16. x = y = zeros(8,1) x(2) = y(4) = 1 wi = rand(3,8) - 0.5; wo
       = rand(8,3) - 0.5; wh = rand(3,3); h1 = wi * x a2 = wh * h1 h2 =
       tanh(h2) a3 = wo * h2 z3 = exp(a3) z3 = h3/sum(z3) 16
   17. [67]17. input layer: h1 (x)= wi    x (projection layer) note that
       non-lineal activation function in projection layer is irrelevant 17
   18. [68]18. hidden layer: hh (x)= g(wh    h1 (x)) softmax layer: z(x) =
       o(wo    h2 (x)) 18
   19. [69]19. 19 computational complexity example: num training data: 1b
       vocabulary size: 100k context: 3 previous words embeddings: 100
       hidden layers: 300 units projection layer: - (copy row) hidden
       layer: 300    300 products, 300 tanh(.) softmax layer: 100    100k
       products, 100k exp(.) total: 90k + 10m !! the softmax is the
       network's main bottleneck.
   20. [70]20. we can get embeddings implicitly from any task that
       involves words. however, good generic embeddings are good for other
       tasks which may have much less data (id21). sometimes,
       the embeddings can be fine-tuned to the final task. word
       embeddings: requirements 20
   21. [71]21. how to get good embedings:     very large lexicon     huge
       amount of learning data     unsupervised (or trivial labels)    
       computational efficient id27s: requirements 21
   22. [72]22. architecture specific for produccing embeddings. it is
       learnt with huge amount of data. simplify the architecture: remove
       hidden layer. simplify the cost (softmax) two variants:     cbow
       (continuos bag of words)     skip-gram id97 [mikolov 2013] 22
   23. [73]23. cbow: continuous bag of words the cat climbed a tree given
       context: a, cat, the, tree estimate prob. of climbed 23
   24. [74]24. skip-gram the cat climbed a tree given word: climbed
       estimate prob. of context words: a, cat, the, tree (it selects
       randomly the context length, till max of 10 left + 10 right) 24
   25. [75]25. reduce cost: subsampling most frequent words (is, the) can
       appear hundred of millions of times. less important: paris ~ france
       ok paris ~ the ?     each input word, wi , is discarded with
       id203 25
   26. [76]26. simplify cost: negative sampling softmax is required to get
       probabilities. but our goal here is just to get good embeddings.
       cost function: maximize s(wo j    wi i ) but add term negative for
       words which do not appear in the context (randomly selected). 26
   27. [77]27. 27
   28. [78]28. id97 is not deep, but used in many tasks using deep
       learning there are other approaches: this is a very popular
       toolkit, with trained embeddings, but there are others (glove). why
       does it works? see paper from glove [pennington et al] discussion
       28
   29. [79]29. id97     mikolov, tomas; et al. "efficient estimation of
       word representations in vector space     linguistic regularities in
       continuous space word representations     tensorflow tutorial
       https://www.tensorflow.org/tutorials/id97/ glove:
       http://nlp.stanford.edu/projects/glove/ (& paper) blog sebastian
       ruder sebastianruder.com/word-embeddings-1/ references 29

          [80]recommended

     * teaching techniques: project-based learning
       teaching techniques: project-based learning
       online course - linkedin learning
     * teacher tips
       teacher tips
       online course - linkedin learning
     * visual aesthetics for elearning
       visual aesthetics for elearning
       online course - linkedin learning
     * id97: from intuition to practice using gensim
       id97: from intuition to practice using gensim
       edgar marca
     * word representations in vector space
       word representations in vector space
       abdullah khan zehady
     * self-supervised audiovisual learning - xavier giro - upc barcelona
       2019
       self-supervised audiovisual learning - xavier giro - upc barcelona
       2019
       universitat polit  cnica de catalunya
     * deep video object tracking - xavier giro - upc barcelona 2019
       deep video object tracking - xavier giro - upc barcelona 2019
       universitat polit  cnica de catalunya
     * self-supervised learning from video sequences - xavier giro - upc
       barcelona 2019
       self-supervised learning from video sequences - xavier giro - upc
       barcelona 2019
       universitat polit  cnica de catalunya
     * deep learning architectures for video - xavier giro - upc barcelona
       2019
       deep learning architectures for video - xavier giro - upc barcelona
       2019
       universitat polit  cnica de catalunya
     * deep video object segmentation - xavier giro - upc barcelona 2019
       deep video object segmentation - xavier giro - upc barcelona 2019
       universitat polit  cnica de catalunya

     * [81]english
     * [82]espa  ol
     * [83]portugu  s
     * [84]fran  ais
     * [85]deutsch

     * [86]about
     * [87]dev & api
     * [88]blog
     * [89]terms
     * [90]privacy
     * [91]copyright
     * [92]support

     *
     *
     *
     *
     *

   linkedin corporation    2019

     

share clipboard
     __________________________________________________________________

   [93]  
     * facebook
     * twitter
     * linkedin

   link ____________________

public clipboards featuring this slide
     __________________________________________________________________

   (button)   
   no public clipboards found for this slide

select another clipboard
     __________________________________________________________________

   [94]  

   looks like you   ve clipped this slide to already.
   ____________________

   create a clipboard

you just clipped your first slide!

   clipping is a handy way to collect important slides you want to go back
   to later. now customize the name of a clipboard to store your clips.
     __________________________________________________________________

   name* ____________________
   description ____________________
   visibility
   others can see my clipboard [ ]
   (button) cancel (button) save

   bizographics tracking image

references

   visible links
   1. https://www.slideshare.net/rss/latest
   2. https://www.slideshare.net/opensearch.xml
   3. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
   4. https://es.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
   5. https://fr.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
   6. https://de.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
   7. https://pt.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
   8. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
   9. https://www.slideshare.net/api/oembed/2?format=json&url=http://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  10. https://www.slideshare.net/api/oembed/2?format=xml&url=http://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  11. https://www.slideshare.net/mobile/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  12. android-app://net.slideshare.mobile/slideshare-app/ss/71385583
  13. ios-app://917418728/slideshare-app/ss/71385583
  14. http://www.linkedin.com/legal/user-agreement
  15. http://www.linkedin.com/legal/privacy-policy
  16. http://www.linkedin.com/legal/privacy-policy
  17. http://www.linkedin.com/legal/user-agreement
  18. https://www.slideshare.net/
  19. https://www.slideshare.net/explore
  20. https://www.slideshare.net/login
  21. https://www.slideshare.net/
  22. https://www.slideshare.net/upload
  23. https://www.slideshare.net/login
  24. https://www.slideshare.net/w/signup
  25. https://www.slideshare.net/
  26. https://www.slideshare.net/explore
  27. https://www.linkedin.com/learning/topics/presentations?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  28. https://www.linkedin.com/learning/topics/powerpoint?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  29. https://www.linkedin.com/learning?trk=slideshare_subnav_learning
  30. https://www.linkedin.com/psettings/privacy
  31. https://public.slidesharecdn.com/matiskay/id97-from-intuition-to-practice-using-gensim
  32. https://public.slidesharecdn.com/abdullahzehady/word-representations-in-vector-space
  33. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  34. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  35. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  36. https://www.slideshare.net/xavigiro?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  37. https://www.slideshare.net/xavigiro?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  38. https://www.slideshare.net/signup?login_source=slideview.popup.follow&from=addcontact&from_source=https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  39. https://www.slideshare.net/featured/category/data-analytics
  40. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017#comments-panel
  41. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017#likes-panel
  42. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017#stats-panel
  43. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017#notes-panel
  44. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  45. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  46. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  47. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  48. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  49. https://www.slideshare.net/signup?login_source=slideview.popup.comment&from=comments&from_source=https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  50. https://www.slideshare.net/mariamelbadri?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  51. https://www.slideshare.net/mariamelbadri?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  52. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-2-638.jpg?cb=1485364488
  53. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-3-638.jpg?cb=1485364488
  54. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-4-638.jpg?cb=1485364488
  55. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-5-638.jpg?cb=1485364488
  56. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-6-638.jpg?cb=1485364488
  57. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-7-638.jpg?cb=1485364488
  58. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-8-638.jpg?cb=1485364488
  59. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-9-638.jpg?cb=1485364488
  60. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-10-638.jpg?cb=1485364488
  61. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-11-638.jpg?cb=1485364488
  62. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-12-638.jpg?cb=1485364488
  63. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-13-638.jpg?cb=1485364488
  64. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-14-638.jpg?cb=1485364488
  65. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-15-638.jpg?cb=1485364488
  66. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-16-638.jpg?cb=1485364488
  67. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-17-638.jpg?cb=1485364488
  68. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-18-638.jpg?cb=1485364488
  69. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-19-638.jpg?cb=1485364488
  70. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-20-638.jpg?cb=1485364488
  71. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-21-638.jpg?cb=1485364488
  72. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-22-638.jpg?cb=1485364488
  73. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-23-638.jpg?cb=1485364488
  74. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-24-638.jpg?cb=1485364488
  75. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-25-638.jpg?cb=1485364488
  76. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-26-638.jpg?cb=1485364488
  77. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-27-638.jpg?cb=1485364488
  78. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-28-638.jpg?cb=1485364488
  79. https://image.slidesharecdn.com/dlsl2017d2l4wordembeddings-170125171006/95/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017-29-638.jpg?cb=1485364488
  80. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017#related-tab-content
  81. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  82. https://es.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  83. https://pt.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  84. https://fr.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  85. https://de.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  86. https://www.slideshare.net/about
  87. https://www.slideshare.net/developers
  88. http://blog.slideshare.net/
  89. https://www.slideshare.net/terms
  90. https://www.slideshare.net/privacy
  91. http://www.linkedin.com/legal/copyright-policy
  92. https://www.linkedin.com/help/slideshare
  93. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  94. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017

   hidden links:
  96. https://www.slideshare.net/xavigiro/word-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
  97. https://www.slideshare.net/signup?login_source=slideview.clip.like&from=clip&layout=foundation&from_source=
  98. https://www.slideshare.net/login?from_source=%2fxavigiro%2fword-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017%3ffrom_action%3dsave&from=download&layout=foundation
  99. https://www.slideshare.net/signup?login_source=slideview.popup.flags&from=flagss&from_source=https%3a%2f%2fwww.slideshare.net%2fxavigiro%2fword-embeddings-d2l4-deep-learning-for-speech-and-language-upc-2017
 100. https://www.linkedin.com/learning/teaching-techniques-project-based-learning?trk=slideshare_sv_learning
 101. https://www.linkedin.com/learning/teacher-tips?trk=slideshare_sv_learning
 102. https://www.linkedin.com/learning/visual-aesthetics-for-elearning?trk=slideshare_sv_learning
 103. https://www.slideshare.net/matiskay/id97-from-intuition-to-practice-using-gensim
 104. https://www.slideshare.net/abdullahzehady/word-representations-in-vector-space
 105. https://www.slideshare.net/xavigiro/selfsupervised-audiovisual-learning-xavier-giro-upc-barcelona-2019
 106. https://www.slideshare.net/xavigiro/deep-video-object-tracking-xavier-giro-upc-barcelona-2019
 107. https://www.slideshare.net/xavigiro/selfsupervised-learning-from-video-sequences-xavier-giro-upc-barcelona-2019
 108. https://www.slideshare.net/xavigiro/deep-learning-architectures-for-video-xavier-giroinieto-upc-barcelona
 109. https://www.slideshare.net/xavigiro/deep-video-object-segmentation-xavier-giroinieto-upc-2019
 110. http://www.linkedin.com/company/linkedin
 111. http://www.facebook.com/linkedin
 112. http://twitter.com/slideshare
 113. http://www.google.com/+linkedin
 114. https://www.slideshare.net/rss/latest
