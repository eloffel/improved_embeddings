   #[1]rss [2]slideshare search [3]alternate [4]alternate [5]alternate
   [6]alternate [7]alternate [8]alternate [9]slideshow json oembed profile
   [10]slideshow xml oembed profile [11]alternate [12]alternate
   [13]alternate

   (button)

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our [14]user
   agreement and [15]privacy policy.

   slideshare uses cookies to improve functionality and performance, and
   to provide you with relevant advertising. if you continue browsing the
   site, you agree to the use of cookies on this website. see our
   [16]privacy policy and [17]user agreement for details.

   [18]slideshare [19]explore search [20]you

     * [21]linkedin slideshare

     * [22]upload
     * [23]login
     * [24]signup

     *
     * ____________________ (button) submit search

     * [25]home
     * [26]explore

     * [27]presentation courses
     * [28]powerpoint courses
     *
     * by [29]linkedin learning

   ____________________
   successfully reported this slideshow.

   we use your linkedin profile and activity data to personalize ads and
   to show you more relevant ads. [30]you can change your ad preferences
   anytime.
   multimodal deep learning (d4l4 deep learning for speech and language
   upc 2017)

   day 4 lecture 4 multimodal deep learning xavier gir  -i-nieto [course
   site]

   2 multimedia text audio vision and ratings, geolocation, time stamps...

   3 multimedia text audio vision and ratings, geolocation, time stamps...

   4

   5 language & vision: encoder-decoder kyunghyun cho,    introduction to
   id4 with gpus    (2015) represen...

   6 language & vision: encoder-decoder

   7 captioning: deepimagesent (slides by marc bola  os): karpathy, andrej,
   and li fei-fei. "deep visual-semantic alignments f...

   8 captioning: deepimagesent (slides by marc bola  os): karpathy, andrej,
   and li fei-fei. "deep visual-semantic alignments f...

   9 captioning: show & tell vinyals, oriol, alexander toshev, samy
   bengio, and dumitru erhan. "show and tell: a neural image...

   10 captioning: show & tell vinyals, oriol, alexander toshev, samy
   bengio, and dumitru erhan. "show and tell: a neural imag...

   11 captioning: show, attend & tell xu, kelvin, jimmy ba, ryan kiros,
   kyunghyun cho, aaron c. courville, ruslan salakhutdin...

   12 captioning: show, attend & tell xu, kelvin, jimmy ba, ryan kiros,
   kyunghyun cho, aaron c. courville, ruslan salakhutdin...

   13 captioning: lstm with image & video jeffrey donahue, lisa anne
   hendricks, sergio guadarrama, marcus rohrbach, subhashin...

   14 johnson, justin, andrej karpathy, and li fei-fei. "densecap: fully
   convolutional localization networks for dense captio...

   15 captioning (+ detection): densecap johnson, justin, andrej karpathy,
   and li fei-fei. "densecap: fully convolutional loc...

   16 captioning (+ detection): densecap johnson, justin, andrej karpathy,
   and li fei-fei. "densecap: fully convolutional loc...

   17 captioning (+ retrieval): densecap johnson, justin, andrej karpathy,
   and li fei-fei. "densecap: fully convolutional loc...

   18 captioning: hrne ( slides by marc bola  os) pingbo pan, zhongwen xu,
   yi yang,fei wu,yueting zhuang hierarchical recurren...

   19 visual id53 [z1 , z2 ,     zn ] [y1 , y2 ,     ym ]    is
   economic growth decreasing ?       yes    encode encode dec...

   20 extract visual features embedding predict answermerge question what
   object is flying? answer kite visual question answe...

   21 visual id53 noh, h., seo, p. h., & han, b. image
   id53 using convolutional neural network wi...

   22 visual id53: dynamic (slides and slidecast by santi
   pascual): xiong, caiming, stephen merity, and richard...

   23 visual id53: dynamic (slides and slidecast by santi
   pascual): xiong, caiming, stephen merity, and richard...

   24 visual id53: grounded (slides and screencast by issey
   masuda): zhu, yuke, oliver groth, michael bernstein...

   25 challenges: visual id53 visual id53

   26 100%0% humans 83,30% uc berkeley & sony 66,47% baseline lstm&id98
   54,06% baseline nearest neighbor 42,85% baseline prior...

   27 vision and language: embeddings perronin, f., cvpr tutorial on lsvr
   @ cvpr   14, output embedding for lsvr

   28 vision and language: embeddings a krizhevsky, i sutskever, ge hinton
      id163 classification with deep convolutional n...

   29 vision and language: embeddings lecture d2l4 by toni bonafonte on
   id27s christopher olah visualizing represen...

   30 vision and language: devise one-hot encoding embedding space

   31 vision and language: devise a krizhevsky, i sutskever, ge hinton
      id163 classification with deep convolutional neura...

   32 vision and language: devise frome, andrea, greg s. corrado, jon
   shlens, samy bengio, jeff dean, and tomas mikolov. "dev...

   33 vision and language: embedding v  ctor campos, d  lia fern  ndez, jordi
   torres, xavier gir  -i-nieto, brendan jou and shih-...

   34 learn more julia hockenmeirer (uiuc), vision to language (@
   microsoft research)

   35 multimedia text audio vision and ratings, geolocation, time
   stamps...

   36 audio and video: soundnet aytar, yusuf, carl vondrick, and antonio
   torralba. "soundnet: learning sound representations ...

   37 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." nip...

   38 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." nip...

   39 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." in ...

   40 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." nip...

   41 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." nip...

   42 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." nip...

   43 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." nip...

   44 aytar, yusuf, carl vondrick, and antonio torralba. "soundnet:
   learning sound representations from unlabeled video." in ...

   45 audio and video: sonorizaton owens, andrew, phillip isola, josh
   mcdermott, antonio torralba, edward h. adelson, and wil...

   46 audio and video: visual sounds owens, andrew, phillip isola, josh
   mcdermott, antonio torralba, edward h. adelson, and w...

   47 audio and video: visual sounds owens, andrew, phillip isola, josh
   mcdermott, antonio torralba, edward h. adelson, and w...

   48 ephrat, ariel, and shmuel peleg. "v speech: speech reconstruction
   from silent video." icassp 2017 speech and video: v...

   49ephrat, ariel, and shmuel peleg. "v speech: speech reconstruction
   from silent video." icassp 2017 speech and video: vi...

   50 speech and video: v speech ephrat, ariel, and shmuel peleg.
   "v speech: speech reconstruction from silent video." ic...

   51 assael, yannis m., brendan shillingford, shimon whiteson, and nando
   de freitas. "lipnet: sentence-level lipreading." ar...

   52 assael, yannis m., brendan shillingford, shimon whiteson, and nando
   de freitas. "lipnet: sentence-level lipreading." ar...

   53 chung, joon son, andrew senior, oriol vinyals, and andrew zisserman.
   "lip reading sentences in the wild." arxiv preprin...

   54 chung, joon son, andrew senior, oriol vinyals, and andrew zisserman.
   "lip reading sentences in the wild." arxiv preprin...

   55 conclusions

   56 conclusions

   57 conclusions [course site]

   58 learn more ruus salakhutdinov,    multimodal machine learning    (nips
   2015 workshop)

   59 thanks ! q&a ? follow me at
   https://imatge.upc.edu/web/people/xavier-giro @docxavi /professorxavi
   upcoming slideshare
   []
   loading in    5
     
   [] 1
   (button)
   1 of 59 (button)
   (button) (button)
   like this presentation? why not share!
     * share
     * email
     *
     *

     * [31]gmm                                                                                                ...
       gmm                                                                                                ... by shinnosuke takamichi
       578 views
     * [32]id144-controllable id48-based spee... id144-controllable
       id48-based spee... by shinnosuke takamichi 923 views
     * [33]                  2017                                "                                    ...                   2017                               
       "                                    ... by shinnosuke takamichi 1039 views
     * [34]ph.d defence (shinnosuke takamichi) ph.d defence (shinnosuke
       takamichi) by shinnosuke takamichi 467 views
     * [35]icassp2017          (deep learning iii) [... icassp2017          (deep
       learning iii) [... by shinnosuke takamichi 644 views
     * [36]icassp2017          (acoustic modeling an... icassp2017          (acoustic
       modeling an... by shinnosuke takamichi 993 views

   (button)

   share slideshare
     __________________________________________________________________

     * [37]facebook
     * [38]twitter
     * [39]linkedin

   embed
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   size (px)
   start on
   [x] show related slideshares at end
   wordpress shortcode ____________________
   link ____________________

multimodal deep learning (d4l4 deep learning for speech and language upc
2017)

   2,146 views

     * (button) share
     * (button) like
     * (button) download
     * ...
          +

   [40]universitat polit  cnica de catalunya

[41]universitat polit  cnica de catalunya

   [42]follow

   (button) (button) (button)

   published on jan 27, 2017

   https://telecombcn-dl.github.io/2017-dlsl/
   winter school on deep learning for speech and language. upc
   barcelonatech etsetb telecombcn.
   the aim of this course is to train students in methods of deep learning
   for speech and language. recurrent neural networks (id56) will be
   presented and analyzed in detail to understand the potential of these
   state of the art tools for time series processing. engineering tips and
   scalability issues will be addressed to solve tasks such as machine
   translation, id103, id133 or question
   answering. hands-on sessions will provide development skills so that
   attendees can become competent in contemporary data analytics tools.
   (button) ...

   published in: [43]data & analytics

     * [44]1 comment
     * [45]6 likes
     * [46]statistics
     * [47]notes

     * full name
       full name
       comment goes here.
       12 hours ago   [48]delete [49]reply [50]block
       are you sure you want to [51]yes [52]no
       your message goes here

   no profile picture user
   ____________________
   [53](button) post
     * [54]johnmayer664
       [55]johnmayer664
       hello! high quality and affordable essays for you. starting at
       $4.99 per page - check our website! https://vk.cc/82gjd2
       11 months ago    [56]reply
       are you sure you want to  [57]yes  [58]no
       your message goes here

     * [59]smrutiranjansahu
       [60]smrutiranjan sahu , sr. software engineer (data science & ai)
       at talentica software at talentica software
       7 months ago
     * [61]seonyijeong
       [62]seonyi jeong , --
       10 months ago
     * [63]zeinabfarhoudi
       [64]zeinab farhoudi , research and development engineer at respina
       networks & beyond at respina networks & beyond
       1 year ago
     * [65]danielmichelsanti
       [66]daniel michelsanti , phd fellow at aalborg universitet at phd
       fellow
       1 year ago
     * [67]armandovieira4
       [68]armando vieira , data scientist at contextvision ab at
       contextvision ab
       2 years ago

   [69]show more
   no downloads
   views
   total views
   2,146
   on slideshare
   0
   from embeds
   0
   number of embeds
   7
   actions
   shares
   0
   downloads
   179
   comments
   1
   likes
   6
   embeds 0
   no embeds
   no notes for slide

multimodal deep learning (d4l4 deep learning for speech and language upc
2017)

    1. 1. day 4 lecture 4 multimodal deep learning xavier gir  -i-nieto
       [course site]
    2. [70]2. 2 multimedia text audio vision and ratings, geolocation,
       time stamps...
    3. [71]3. 3 multimedia text audio vision and ratings, geolocation,
       time stamps...
    4. [72]4. 4
    5. [73]5. 5 language & vision: encoder-decoder kyunghyun cho,
          introduction to id4 with gpus    (2015)
       representation or embedding lectures d3l4 & d4l2 by marta ruiz on
       id4
    6. [74]6. 6 language & vision: encoder-decoder
    7. [75]7. 7 captioning: deepimagesent (slides by marc bola  os):
       karpathy, andrej, and li fei-fei. "deep visual-semantic alignments
       for generating image descriptions." cvpr 2015
    8. [76]8. 8 captioning: deepimagesent (slides by marc bola  os):
       karpathy, andrej, and li fei-fei. "deep visual-semantic alignments
       for generating image descriptions." cvpr 2015 only takes into
       account image features in the first hidden state multimodal
       recurrent neural network
    9. [77]9. 9 captioning: show & tell vinyals, oriol, alexander toshev,
       samy bengio, and dumitru erhan. "show and tell: a neural image
       caption generator." cvpr 2015.
   10. [78]10. 10 captioning: show & tell vinyals, oriol, alexander
       toshev, samy bengio, and dumitru erhan. "show and tell: a neural
       image caption generator." cvpr 2015.
   11. [79]11. 11 captioning: show, attend & tell xu, kelvin, jimmy ba,
       ryan kiros, kyunghyun cho, aaron c. courville, ruslan
       salakhutdinov, richard s. zemel, and yoshua bengio. "show, attend
       and tell: neural image id134 with visual attention."
       icml 2015
   12. [80]12. 12 captioning: show, attend & tell xu, kelvin, jimmy ba,
       ryan kiros, kyunghyun cho, aaron c. courville, ruslan
       salakhutdinov, richard s. zemel, and yoshua bengio. "show, attend
       and tell: neural image id134 with visual attention."
       icml 2015
   13. [81]13. 13 captioning: lstm with image & video jeffrey donahue,
       lisa anne hendricks, sergio guadarrama, marcus rohrbach, subhashini
       venugopalan, kate saenko, trevor darrel. long-term recurrent
       convolutional networks for visual recognition and description, cvpr
       2015. code
   14. [82]14. 14 johnson, justin, andrej karpathy, and li fei-fei.
       "densecap: fully convolutional localization networks for dense
       captioning." cvpr 2016 captioning (+ detection): densecap
   15. [83]15. 15 captioning (+ detection): densecap johnson, justin,
       andrej karpathy, and li fei-fei. "densecap: fully convolutional
       localization networks for dense captioning." cvpr 2016
   16. [84]16. 16 captioning (+ detection): densecap johnson, justin,
       andrej karpathy, and li fei-fei. "densecap: fully convolutional
       localization networks for dense captioning." cvpr 2016 xavi:    man
       has short hair   ,    man with short hair    amaia:   a woman wearing a
       black shirt   ,     both:    two men wearing black glasses   
   17. [85]17. 17 captioning (+ retrieval): densecap johnson, justin,
       andrej karpathy, and li fei-fei. "densecap: fully convolutional
       localization networks for dense captioning." cvpr 2016
   18. [86]18. 18 captioning: hrne ( slides by marc bola  os) pingbo pan,
       zhongwen xu, yi yang,fei wu,yueting zhuang hierarchical recurrent
       neural encoder for video representation with application to
       captioning, cvpr 2016. lstm unit (2nd layer) time image t = 1 t = t
       hidden state at t = t first chunk of data
   19. [87]19. 19 visual id53 [z1 , z2 ,     zn ] [y1 , y2 ,    
       ym ]    is economic growth decreasing ?       yes    encode encode decode
   20. [88]20. 20 extract visual features embedding predict answermerge
       question what object is flying? answer kite visual question
       answering slide credit: issey masuda
   21. [89]21. 21 visual id53 noh, h., seo, p. h., & han, b.
       image id53 using convolutional neural network with
       dynamic parameter prediction. cvpr 2016 dynamic parameter
       prediction network (dppnet)
   22. [90]22. 22 visual id53: dynamic (slides and slidecast
       by santi pascual): xiong, caiming, stephen merity, and richard
       socher. "dynamic memory networks for visual and textual question
       answering." arxiv preprint arxiv:1603.01417 (2016).
   23. [91]23. 23 visual id53: dynamic (slides and slidecast
       by santi pascual): xiong, caiming, stephen merity, and richard
       socher. "dynamic memory networks for visual and textual question
       answering." icml 2016. main idea: split image into local regions.
       consider each region equivalent to a sentence. local region feature
       extraction: id98 (vgg-19): (1) rescale input to 448x448. (2) take
       output from last pooling layer     d=512x14x14     196 512-d local
       region vectors. visual feature embedding: w matrix to project image
       features to    q   -textual space.
   24. [92]24. 24 visual id53: grounded (slides and
       screencast by issey masuda): zhu, yuke, oliver groth, michael
       bernstein, and li fei-fei."visual7w: grounded id53 in
       images." cvpr 2016.
   25. [93]25. 25 challenges: visual id53 visual question
       answering
   26. [94]26. 26 100%0% humans 83,30% uc berkeley & sony 66,47% baseline
       lstm&id98 54,06% baseline nearest neighbor 42,85% baseline prior per
       question type 37,47% baseline all yes 29,88% 53,62% i. masuda-mora,
          open-ended visual question-answering   . bsc etsetb 2016. [keras]
       challenges: visual id53
   27. [95]27. 27 vision and language: embeddings perronin, f., cvpr
       tutorial on lsvr @ cvpr   14, output embedding for lsvr
   28. [96]28. 28 vision and language: embeddings a krizhevsky, i
       sutskever, ge hinton    id163 classification with deep
       convolutional neural networks    part of: advances in neural
       information processing systems 25 (nips 2012)
   29. [97]29. 29 vision and language: embeddings lecture d2l4 by toni
       bonafonte on id27s christopher olah visualizing
       representations
   30. [98]30. 30 vision and language: devise one-hot encoding embedding
       space
   31. [99]31. 31 vision and language: devise a krizhevsky, i sutskever,
       ge hinton    id163 classification with deep convolutional neural
       networks    part of: advances in neural information processing
       systems 25 (nips 2012)
   32. [100]32. 32 vision and language: devise frome, andrea, greg s.
       corrado, jon shlens, samy bengio, jeff dean, and tomas mikolov.
       "devise: a deep visual-semantic embedding model." nips 2013
   33. [101]33. 33 vision and language: embedding v  ctor campos, d  lia
       fern  ndez, jordi torres, xavier gir  -i-nieto, brendan jou and
       shih-fu chang (work under progress)
   34. [102]34. 34 learn more julia hockenmeirer (uiuc), vision to
       language (@ microsoft research)
   35. [103]35. 35 multimedia text audio vision and ratings, geolocation,
       time stamps...
   36. [104]36. 36 audio and video: soundnet aytar, yusuf, carl vondrick,
       and antonio torralba. "soundnet: learning sound representations
       from unlabeled video." in advances in neural information processing
       systems, pp. 892-900. 2016. object & scenes recognition in videos
       by analysing the audio track (only).
   37. [105]37. 37 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video."
       nips 2016. videos for training are unlabeled. relies on id98s
       trained on labeled images. audio and video: soundnet
   38. [106]38. 38 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video."
       nips 2016. videos for training are unlabeled. relies on id98s
       trained on labeled images. audio and video: soundnet
   39. [107]39. 39 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video." in
       advances in neural information processing systems, pp. 892-900.
       2016. audio and video: soundnet
   40. [108]40. 40 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video."
       nips 2016. hidden layers of soundnet are used to train a standard
       id166 classifier that outperforms state of the art. audio and video:
       soundnet
   41. [109]41. 41 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video."
       nips 2016. visualization of the 1d filters over raw audio in conv1.
       audio and video: soundnet
   42. [110]42. 42 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video."
       nips 2016. visualization of the 1d filters over raw audio in conv1.
       audio and video: soundnet
   43. [111]43. 43 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video."
       nips 2016. visualization of the 1d filters over raw audio in conv1.
       audio and video: soundnet
   44. [112]44. 44 aytar, yusuf, carl vondrick, and antonio torralba.
       "soundnet: learning sound representations from unlabeled video." in
       advances in neural information processing systems, pp. 892-900.
       2016. visualization of the video frames associated to the sounds
       that activate some of the last hidden units (conv7): audio and
       video: soundnet
   45. [113]45. 45 audio and video: sonorizaton owens, andrew, phillip
       isola, josh mcdermott, antonio torralba, edward h. adelson, and
       william t. freeman. "visually indicated sounds." cvpr 2016. learn
       synthesized sounds from videos of people hitting objects with a
       drumstick.
   46. [114]46. 46 audio and video: visual sounds owens, andrew, phillip
       isola, josh mcdermott, antonio torralba, edward h. adelson, and
       william t. freeman. "visually indicated sounds." cvpr 2016. no
       end-to-end
   47. [115]47. 47 audio and video: visual sounds owens, andrew, phillip
       isola, josh mcdermott, antonio torralba, edward h. adelson, and
       william t. freeman. "visually indicated sounds." cvpr 2016.
   48. [116]48. 48 ephrat, ariel, and shmuel peleg. "v speech: speech
       reconstruction from silent video." icassp 2017 speech and video:
       v speech id98 (vgg) frame from a slient video audio feature
   49. [117]49. 49ephrat, ariel, and shmuel peleg. "v speech: speech
       reconstruction from silent video." icassp 2017 speech and video:
       v speech no end-to-end
   50. [118]50. 50 speech and video: v speech ephrat, ariel, and shmuel
       peleg. "v speech: speech reconstruction from silent video."
       icassp 2017
   51. [119]51. 51 assael, yannis m., brendan shillingford, shimon
       whiteson, and nando de freitas. "lipnet: sentence-level
       lipreading." arxiv preprint arxiv:1611.01599 (2016). speech and
       video: lipnet
   52. [120]52. 52 assael, yannis m., brendan shillingford, shimon
       whiteson, and nando de freitas. "lipnet: sentence-level
       lipreading." arxiv preprint arxiv:1611.01599 (2016). speech and
       video: lipnet
   53. [121]53. 53 chung, joon son, andrew senior, oriol vinyals, and
       andrew zisserman. "lip reading sentences in the wild." arxiv
       preprint arxiv:1611.05358 (2016). speech and video: watch, listen,
       attend & spell
   54. [122]54. 54 chung, joon son, andrew senior, oriol vinyals, and
       andrew zisserman. "lip reading sentences in the wild." arxiv
       preprint arxiv:1611.05358 (2016). speech and video: watch, listen,
       attend & spell
   55. [123]55. 55 conclusions
   56. [124]56. 56 conclusions
   57. [125]57. 57 conclusions [course site]
   58. [126]58. 58 learn more ruus salakhutdinov,    multimodal machine
       learning    (nips 2015 workshop)
   59. [127]59. 59 thanks ! q&a ? follow me at
       https://imatge.upc.edu/web/people/xavier-giro @docxavi
       /professorxavi

          [128]recommended

     * powerpoint 2016 essential training
       powerpoint 2016 essential training
       online course - linkedin learning
     * powerpoint: designing better slides
       powerpoint: designing better slides
       online course - linkedin learning
     * learning management systems (lms) quick start
       learning management systems (lms) quick start
       online course - linkedin learning
     * gmm                                                                                                            
       gmm                                                                                                            
       shinnosuke takamichi
     * id144-controllable id48-based id133 using speech input
       id144-controllable id48-based id133 using speech input
       shinnosuke takamichi
     *                   2017                                "                                                "
                         2017                                "                                                "
       shinnosuke takamichi
     * ph.d defence (shinnosuke takamichi)
       ph.d defence (shinnosuke takamichi)
       shinnosuke takamichi
     * icassp2017          (deep learning iii) [                      ]
       icassp2017          (deep learning iii) [                      ]
       shinnosuke takamichi
     * icassp2017          (acoustic modeling and adaptation)
       icassp2017          (acoustic modeling and adaptation)
       shinnosuke takamichi
     *                   2017       moment-matching network                                                                           
                         2017       moment-matching network                                                                           
       shinnosuke takamichi

     * [129]english
     * [130]espa  ol
     * [131]portugu  s
     * [132]fran  ais
     * [133]deutsch

     * [134]about
     * [135]dev & api
     * [136]blog
     * [137]terms
     * [138]privacy
     * [139]copyright
     * [140]support

     *
     *
     *
     *
     *

   linkedin corporation    2019

     

share clipboard
     __________________________________________________________________

   [141]  
     * facebook
     * twitter
     * linkedin

   link ____________________

public clipboards featuring this slide
     __________________________________________________________________

   (button)   
   no public clipboards found for this slide

select another clipboard
     __________________________________________________________________

   [142]  

   looks like you   ve clipped this slide to already.
   ____________________

   create a clipboard

you just clipped your first slide!

   clipping is a handy way to collect important slides you want to go back
   to later. now customize the name of a clipboard to store your clips.
     __________________________________________________________________

   name* ____________________
   description ____________________
   visibility
   others can see my clipboard [ ]
   (button) cancel (button) save

   bizographics tracking image

references

   visible links
   1. https://www.slideshare.net/rss/latest
   2. https://www.slideshare.net/opensearch.xml
   3. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
   4. https://es.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
   5. https://fr.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
   6. https://de.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
   7. https://pt.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
   8. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
   9. https://www.slideshare.net/api/oembed/2?format=json&url=http://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  10. https://www.slideshare.net/api/oembed/2?format=xml&url=http://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  11. https://www.slideshare.net/mobile/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  12. android-app://net.slideshare.mobile/slideshare-app/ss/71471365
  13. ios-app://917418728/slideshare-app/ss/71471365
  14. http://www.linkedin.com/legal/user-agreement
  15. http://www.linkedin.com/legal/privacy-policy
  16. http://www.linkedin.com/legal/privacy-policy
  17. http://www.linkedin.com/legal/user-agreement
  18. https://www.slideshare.net/
  19. https://www.slideshare.net/explore
  20. https://www.slideshare.net/login
  21. https://www.slideshare.net/
  22. https://www.slideshare.net/upload
  23. https://www.slideshare.net/login
  24. https://www.slideshare.net/w/signup
  25. https://www.slideshare.net/
  26. https://www.slideshare.net/explore
  27. https://www.linkedin.com/learning/topics/presentations?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  28. https://www.linkedin.com/learning/topics/powerpoint?trk=slideshare_subnav_learning&entitytype=course&sortby=recency
  29. https://www.linkedin.com/learning?trk=slideshare_subnav_learning
  30. https://www.linkedin.com/psettings/privacy
  31. https://public.slidesharecdn.com/shinnosuketakamichi/gmm-73231298
  32. https://public.slidesharecdn.com/shinnosuketakamichi/id144controllable-id48based-speech-synthesis-using-speech-input
  33. https://public.slidesharecdn.com/shinnosuketakamichi/2017-80118132
  34. https://public.slidesharecdn.com/shinnosuketakamichi/phd-defence-shinnosuke-takamichi
  35. https://public.slidesharecdn.com/shinnosuketakamichi/icassp2017-deep-learning-iii
  36. https://public.slidesharecdn.com/shinnosuketakamichi/icassp2017-acoustic-modeling-and-adaptation
  37. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  38. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  39. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  40. https://www.slideshare.net/xavigiro?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  41. https://www.slideshare.net/xavigiro?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideview
  42. https://www.slideshare.net/signup?login_source=slideview.popup.follow&from=addcontact&from_source=https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  43. https://www.slideshare.net/featured/category/data-analytics
  44. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017#comments-panel
  45. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017#likes-panel
  46. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017#stats-panel
  47. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017#notes-panel
  48. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  49. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  50. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  51. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  52. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  53. https://www.slideshare.net/signup?login_source=slideview.popup.comment&from=comments&from_source=https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  54. https://www.slideshare.net/johnmayer664?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  55. https://www.slideshare.net/johnmayer664?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshare
  56. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  57. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  58. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  59. https://www.slideshare.net/smrutiranjansahu?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  60. https://www.slideshare.net/smrutiranjansahu?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  61. https://www.slideshare.net/seonyijeong?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  62. https://www.slideshare.net/seonyijeong?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  63. https://www.slideshare.net/zeinabfarhoudi?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  64. https://www.slideshare.net/zeinabfarhoudi?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  65. https://www.slideshare.net/danielmichelsanti?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  66. https://www.slideshare.net/danielmichelsanti?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  67. https://www.slideshare.net/armandovieira4?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  68. https://www.slideshare.net/armandovieira4?utm_campaign=profiletracking&utm_medium=sssite&utm_source=ssslideshow
  69. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
  70. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-2-638.jpg?cb=1485543803
  71. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-3-638.jpg?cb=1485543803
  72. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-4-638.jpg?cb=1485543803
  73. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-5-638.jpg?cb=1485543803
  74. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-6-638.jpg?cb=1485543803
  75. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-7-638.jpg?cb=1485543803
  76. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-8-638.jpg?cb=1485543803
  77. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-9-638.jpg?cb=1485543803
  78. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-10-638.jpg?cb=1485543803
  79. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-11-638.jpg?cb=1485543803
  80. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-12-638.jpg?cb=1485543803
  81. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-13-638.jpg?cb=1485543803
  82. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-14-638.jpg?cb=1485543803
  83. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-15-638.jpg?cb=1485543803
  84. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-16-638.jpg?cb=1485543803
  85. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-17-638.jpg?cb=1485543803
  86. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-18-638.jpg?cb=1485543803
  87. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-19-638.jpg?cb=1485543803
  88. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-20-638.jpg?cb=1485543803
  89. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-21-638.jpg?cb=1485543803
  90. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-22-638.jpg?cb=1485543803
  91. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-23-638.jpg?cb=1485543803
  92. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-24-638.jpg?cb=1485543803
  93. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-25-638.jpg?cb=1485543803
  94. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-26-638.jpg?cb=1485543803
  95. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-27-638.jpg?cb=1485543803
  96. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-28-638.jpg?cb=1485543803
  97. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-29-638.jpg?cb=1485543803
  98. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-30-638.jpg?cb=1485543803
  99. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-31-638.jpg?cb=1485543803
 100. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-32-638.jpg?cb=1485543803
 101. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-33-638.jpg?cb=1485543803
 102. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-34-638.jpg?cb=1485543803
 103. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-35-638.jpg?cb=1485543803
 104. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-36-638.jpg?cb=1485543803
 105. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-37-638.jpg?cb=1485543803
 106. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-38-638.jpg?cb=1485543803
 107. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-39-638.jpg?cb=1485543803
 108. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-40-638.jpg?cb=1485543803
 109. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-41-638.jpg?cb=1485543803
 110. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-42-638.jpg?cb=1485543803
 111. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-43-638.jpg?cb=1485543803
 112. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-44-638.jpg?cb=1485543803
 113. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-45-638.jpg?cb=1485543803
 114. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-46-638.jpg?cb=1485543803
 115. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-47-638.jpg?cb=1485543803
 116. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-48-638.jpg?cb=1485543803
 117. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-49-638.jpg?cb=1485543803
 118. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-50-638.jpg?cb=1485543803
 119. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-51-638.jpg?cb=1485543803
 120. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-52-638.jpg?cb=1485543803
 121. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-53-638.jpg?cb=1485543803
 122. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-54-638.jpg?cb=1485543803
 123. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-55-638.jpg?cb=1485543803
 124. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-56-638.jpg?cb=1485543803
 125. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-57-638.jpg?cb=1485543803
 126. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-58-638.jpg?cb=1485543803
 127. https://image.slidesharecdn.com/dlsl2017d4l4multimodaldeeplearning-170127184643/95/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017-59-638.jpg?cb=1485543803
 128. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017#related-tab-content
 129. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 130. https://es.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 131. https://pt.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 132. https://fr.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 133. https://de.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 134. https://www.slideshare.net/about
 135. https://www.slideshare.net/developers
 136. http://blog.slideshare.net/
 137. https://www.slideshare.net/terms
 138. https://www.slideshare.net/privacy
 139. http://www.linkedin.com/legal/copyright-policy
 140. https://www.linkedin.com/help/slideshare
 141. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 142. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017

   hidden links:
 144. https://www.slideshare.net/xavigiro/multimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 145. https://www.slideshare.net/signup?login_source=slideview.clip.like&from=clip&layout=foundation&from_source=
 146. https://www.slideshare.net/login?from_source=%2fxavigiro%2fmultimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017%3ffrom_action%3dsave&from=download&layout=foundation
 147. https://www.slideshare.net/signup?login_source=slideview.popup.flags&from=flagss&from_source=https%3a%2f%2fwww.slideshare.net%2fxavigiro%2fmultimodal-deep-learning-d4l4-deep-learning-for-speech-and-language-upc-2017
 148. https://www.linkedin.com/learning/powerpoint-2016-essential-training?trk=slideshare_sv_learning
 149. https://www.linkedin.com/learning/powerpoint-designing-better-slides?trk=slideshare_sv_learning
 150. https://www.linkedin.com/learning/learning-management-systems-lms-quick-start?trk=slideshare_sv_learning
 151. https://www.slideshare.net/shinnosuketakamichi/gmm-73231298
 152. https://www.slideshare.net/shinnosuketakamichi/id144controllable-id48based-speech-synthesis-using-speech-input
 153. https://www.slideshare.net/shinnosuketakamichi/2017-80118132
 154. https://www.slideshare.net/shinnosuketakamichi/phd-defence-shinnosuke-takamichi
 155. https://www.slideshare.net/shinnosuketakamichi/icassp2017-deep-learning-iii
 156. https://www.slideshare.net/shinnosuketakamichi/icassp2017-acoustic-modeling-and-adaptation
 157. https://www.slideshare.net/shinnosuketakamichi/2017-momentmatching-network
 158. http://www.linkedin.com/company/linkedin
 159. http://www.facebook.com/linkedin
 160. http://twitter.com/slideshare
 161. http://www.google.com/+linkedin
 162. https://www.slideshare.net/rss/latest
