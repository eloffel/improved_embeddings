0
1
0
2

 
r
a

m
4

 

 
 
]
l
c
.
s
c
[
 
 

1
v
1
4
1
1

.

3
0
0
1
:
v
i
x
r
a

journal of arti   cial intelligence research 37 (2010) 141-188

submitted 10/09; published 02/10

from frequency to meaning:

vector space models of semantics

peter d. turney
national research council canada
ottawa, ontario, canada, k1a 0r6

patrick pantel
yahoo! labs
sunnyvale, ca, 94089, usa

peter.turney@nrc-cnrc.gc.ca

me@patrickpantel.com

abstract

computers understand very little of the meaning of human language. this profoundly
limits our ability to give instructions to computers, the ability of computers to explain
their actions to us, and the ability of computers to analyse and process text. vector space
models (vsms) of semantics are beginning to address these limits. this paper surveys the
use of vsms for semantic processing of text. we organize the literature on vsms according
to the structure of the matrix in a vsm. there are currently three broad classes of vsms,
based on term   document, word   context, and pair   pattern matrices, yielding three classes
of applications. we survey a broad range of applications in these three categories and we
take a detailed look at a speci   c open source project in each category. our goal in this
survey is to show the breadth of applications of vsms for semantics, to provide a new
perspective on vsms for those who are already familiar with the area, and to provide
pointers into the literature for those who are less familiar with the    eld.

1. introduction

one of the biggest obstacles to making full use of the power of computers is that they
currently understand very little of the meaning of human language. recent progress in
search engine technology is only scratching the surface of human language, and yet the
impact on society and the economy is already immense. this hints at the transformative
impact that deeper semantic technologies will have. vector space models (vsms), surveyed
in this paper, are likely to be a part of these new semantic technologies.

in this paper, we use the term semantics in a general sense, as the meaning of a word, a
phrase, a sentence, or any text in human language, and the study of such meaning. we are
not concerned with narrower senses of semantics, such as the semantic web or approaches to
semantics based on formal logic. we present a survey of vsms and their relation with the
distributional hypothesis as an approach to representing some aspects of natural language
semantics.

the vsm was developed for the smart information retrieval system (salton, 1971)
by gerard salton and his colleagues (salton, wong, & yang, 1975). smart pioneered
many of the concepts that are used in modern search engines (manning, raghavan, &
sch  utze, 2008). the idea of the vsm is to represent each document in a collection as a
point in a space (a vector in a vector space). points that are close together in this space
are semantically similar and points that are far apart are semantically distant. the user   s

c(cid:13)2010 ai access foundation and national research council canada. reprinted with permission.

turney & pantel

query is represented as a point in the same space as the documents (the query is a pseudo-
document). the documents are sorted in order of increasing distance (decreasing semantic
similarity) from the query and then presented to the user.

the success of the vsm for information retrieval has inspired researchers to extend the
vsm to other semantic tasks in natural language processing, with impressive results. for
instance, rapp (2003) used a vector-based representation of word meaning to achieve a
score of 92.5% on multiple-choice synonym questions from the test of english as a foreign
language (toefl), whereas the average human score was 64.5%.1 turney (2006) used a
vector-based representation of semantic relations to attain a score of 56% on multiple-choice
analogy questions from the sat college entrance test, compared to an average human score
of 57%.2

in this survey, we have organized past work with vsms according to the type of matrix
involved: term   document, word   context, and pair   pattern. we believe that the choice of
a particular matrix type is more fundamental than other choices, such as the particular
linguistic processing or mathematical processing. although these three matrix types cover
most of the work, there is no reason to believe that these three types exhaust the possibilities.
we expect future work will introduce new types of matrices and higher-order tensors.3

1.1 motivation for vector space models of semantics

vsms have several attractive properties. vsms extract knowledge automatically from a
given corpus, thus they require much less labour than other approaches to semantics, such
as hand-coded knowledge bases and ontologies. for example, the main resource used in
rapp   s (2003) vsm system for measuring word similarity is the british national corpus
(bnc),4 whereas the main resource used in non-vsm systems for measuring word similarity
(hirst & st-onge, 1998; leacock & chodrow, 1998; jarmasz & szpakowicz, 2003) is a
lexicon, such as id1385 or roget   s thesaurus. gathering a corpus for a new language
is generally much easier than building a lexicon, and building a lexicon often involves also
gathering a corpus, such as semcor for id138 (miller, leacock, tengi, & bunker, 1993).
vsms perform well on tasks that involve measuring the similarity of meaning between
words, phrases, and documents. most search engines use vsms to measure the similarity
between a query and a document (manning et al., 2008). the leading algorithms for mea-
suring semantic relatedness use vsms (pantel & lin, 2002a; rapp, 2003; turney, littman,
bigham, & shnayder, 2003). the leading algorithms for measuring the similarity of seman-
tic relations also use vsms (lin & pantel, 2001; turney, 2006; nakov & hearst, 2008).
(section 2.4 discusses the di   erences between these types of similarity.)

we    nd vsms especially interesting due to their relation with the distributional hy-
pothesis and related hypotheses (see section 2.7). the distributional hypothesis is that

1. regarding the average score of 64.5% on the toefl questions, landauer and dumais (1997) note
that,    although we do not know how such a performance would compare, for example, with u.s. school
children of a particular age, we have been told that the average score is adequate for admission to many
universities.   

2. this is the average score for highschool students in their senior year, applying to us universities. for

more discussion of this score, see section 6.3 in turney   s (2006) paper.

3. a vector is a    rst-order tensor and a matrix is a second-order tensor. see section 2.5.
4. see http://www.natcorp.ox.ac.uk/.
5. see http://id138.princeton.edu/.

142

from frequency to meaning

words that occur in similar contexts tend to have similar meanings (wittgenstein, 1953;
harris, 1954; weaver, 1955; firth, 1957; deerwester, dumais, landauer, furnas, & harsh-
man, 1990). e   orts to apply this abstract hypothesis to concrete algorithms for measuring
the similarity of meaning often lead to vectors, matrices, and higher-order tensors. this
intimate connection between the distributional hypothesis and vsms is a strong motivation
for taking a close look at vsms.

not all uses of vectors and matrices count as vector space models. for the purposes of
this survey, we take it as a de   ning property of vsms that the values of the elements in a
vsm must be derived from event frequencies, such as the number of times that a given word
appears in a given context (see section 2.6). for example, often a lexicon or a knowledge
base may be viewed as a graph, and a graph may be represented using an adjacency matrix,
but this does not imply that a lexicon is a vsm, because, in general, the values of the
elements in an adjacency matrix are not derived from event frequencies. this emphasis
on event frequencies brings unity to the variety of vsms and explicitly connects them to
the distributional hypothesis; furthermore, it avoids triviality by excluding many possible
matrix representations.

1.2 vectors in ai and cognitive science

vectors are common in ai and cognitive science; they were common before the vsm was
introduced by salton et al. (1975). the novelty of the vsm was to use frequencies in a
corpus of text as a clue for discovering semantic information.

in machine learning, a typical problem is to learn to classify or cluster a set of items
(i.e., examples, cases, individuals, entities) represented as feature vectors (mitchell, 1997;
witten & frank, 2005).
in general, the features are not derived from event frequencies,
although this is possible (see section 4.6). for example, a machine learning algorithm can
be applied to classifying or id91 documents (sebastiani, 2002).

collaborative    ltering and recommender systems also use vectors (resnick, iacovou,
suchak, bergstrom, & riedl, 1994; breese, heckerman, & kadie, 1998; linden, smith, &
york, 2003).
in a typical recommender system, we have a person-item matrix, in which
the rows correspond to people (customers, consumers), the columns correspond to items
(products, purchases), and the value of an element is the rating (poor, fair, excellent) that
the person has given to the item. many of the mathematical techniques that work well
with term   document matrices (see section 4) also work well with person-item matrices, but
ratings are not derived from event frequencies.

in cognitive science, prototype theory often makes use of vectors. the basic idea of
prototype theory is that some members of a category are more central than others (rosch
& lloyd, 1978; lako   , 1987). for example, robin is a central (prototypical) member of
the category bird, whereas penguin is more peripheral. concepts have varying degrees of
membership in categories (graded categorization). a natural way to formalize this is to
represent concepts as vectors and categories as sets of vectors (nosofsky, 1986; smith, osh-
erson, rips, & keane, 1988). however, these vectors are usually based on numerical scores
that are elicited by questioning human subjects; they are not based on event frequencies.

another area of psychology that makes extensive use of vectors is psychometrics, which
studies the measurement of psychological abilities and traits. the usual instrument for

143

turney & pantel

measurement is a test or questionnaire, such as a personality test. the results of a test
are typically represented as a subject-item matrix, in which the rows represent the subjects
(people) in an experiment and the columns represent the items (questions) in the test
(questionnaire). the value of an element in the matrix is the answer that the corresponding
subject gave for the corresponding item. many techniques for vector analysis, such as factor
analysis (spearman, 1904), were pioneered in psychometrics.

in cognitive science, latent semantic analysis (lsa) (deerwester et al., 1990; lan-
dauer & dumais, 1997), hyperspace analogue to language (hal) (lund, burgess, &
atchley, 1995; lund & burgess, 1996), and related research (landauer, mcnamara, den-
nis, & kintsch, 2007) is entirely within the scope of vsms, as de   ned above, since this
research uses vector space models in which the values of the elements are derived from
event frequencies, such as the number of times that a given word appears in a given con-
text. cognitive scientists have argued that there are empirical and theoretical reasons for
believing that vsms, such as lsa and hal, are plausible models of some aspects of hu-
man cognition (landauer et al., 2007). in ai, computational linguistics, and information
retrieval, such plausibility is not essential, but it may be seen as a sign that vsms are a
promising area for further research.

1.3 motivation for this survey

this paper is a survey of vector space models of semantics. there is currently no com-
prehensive, up-to-date survey of this    eld. as we show in the survey, vector space models
are a highly successful approach to semantics, with a wide range of potential and actual
applications. there has been much recent growth in research in this area.

this paper should be of interest to all ai researchers who work with natural language,
especially researchers who are interested in semantics. the survey will serve as a general
introduction to this area and it will provide a framework     a uni   ed perspective     for
organizing the diverse literature on the topic. it should encourage new research in the area,
by pointing out open problems and areas for further exploration.

this survey makes the following contributions:
new framework: we provide a new framework for organizing the literature: term   
document, word   context, and pair   pattern matrices (see section 2). this framework shows
the importance of the structure of the matrix (the choice of rows and columns) in deter-
mining the potential applications and may inspire researchers to explore new structures
(di   erent kinds of rows and columns, or higher-order tensors instead of matrices).

new developments: we draw attention to pair   pattern matrices. the use of pair   
pattern matrices is relatively new and deserves more study. these matrices address some
criticisms that have been directed at word   context matrices, regarding lack of sensitivity
to word order.

breadth of approaches and applications: there is no existing survey that shows
the breadth of potential and actual applications of vsms for semantics. existing summaries
omit pair   pattern matrices (landauer et al., 2007).

focus on nlp and cl: our focus in this survey is on systems that perform practical
tasks in natural language processing and computational linguistics. existing overviews focus
on cognitive psychology (landauer et al., 2007).

144

from frequency to meaning

success stories: we draw attention to the fact that vsms are arguably the most

successful approach to semantics, so far.

1.4 intended readership

our goal in writing this paper has been to survey the state of the art in vector space models
of semantics, to introduce the topic to those who are new to the area, and to give a new
perspective to those who are already familiar with the area.

we assume our reader has a basic understanding of vectors, matrices, and id202,
such as one might acquire from an introductory undergraduate course in id202, or
from a text book (golub & van loan, 1996). the basic concepts of vectors and matrices
are more important here than the mathematical details. widdows (2004) gives a gentle
introduction to vectors from the perspective of semantics.

we also assume our reader has some familiarity with computational linguistics or infor-
mation retrieval. manning et al. (2008) provide a good introduction to information retrieval.
for computational linguistics, we recommend manning and sch  utze   s (1999) text.

if our reader is familiar with id202 and computational linguistics, this survey
should present no barriers to understanding. beyond this background, it is not necessary
to be familiar with vsms as they are used in information retrieval, natural language pro-
cessing, and computational linguistics. however, if the reader would like to do some further
background reading, we recommend landauer et al.   s (2007) collection.

1.5 highlights and outline

this article is structured as follows. section 2 explains our framework for organizing the
literature on vsms according to the type of matrix involved: term   document, word   context,
and pair   pattern. in this section, we present an overview of vsms, without getting into
the details of how a matrix can be generated from a corpus of raw text.

after the high-level framework is in place, sections 3 and 4 examine the steps involved
in generating a matrix. section 3 discusses linguistic processing and section 4 reviews
mathematical processing. this is the order in which a corpus would be processed in most
vsm systems (   rst linguistic processing, then mathematical processing).

when vsms are used for semantics, the input to the model is usually plain text. some
vsms work directly with the raw text, but most    rst apply some linguistic processing to the
text, such as id30, part-of-speech tagging, word sense tagging, or parsing. section 3
looks at some of these linguistic tools for semantic vsms.

in a simple vsm, such as a simple term   document vsm, the value of an element in a
document vector is the number of times that the corresponding word occurs in the given
document, but most vsms apply some mathematical processing to the raw frequency values.
section 4 presents the main mathematical operations: weighting the elements, smoothing
the matrix, and comparing the vectors. this section also describes optimization strategies
for comparing the vectors, such as distributed sparse id127 and randomized
techniques.

by the end of section 4, the reader will have a general view of the concepts involved in
vector space models of semantics. we then take a detailed look at three vsm systems in
section 5. as a representative of term   document vsms, we present the lucene information

145

turney & pantel

retrieval library.6 for word   context vsms, we explore the semantic vectors package, which
builds on lucene.7 as the representative of pair   pattern vsms, we review the latent
relational analysis module in the s-space package, which also builds on lucene.8 the
source code for all three of these systems is available under open source licensing.

we turn to a broad survey of applications for semantic vsms in section 6. this sec-
tion also serves as a short historical view of research with semantic vsms, beginning with
information retrieval in section 6.1. our purpose here is to give the reader an idea of the
breadth of applications for vsms and also to provide pointers into the literature, if the
reader wishes to examine any of these applications in detail.

in a term   document matrix, rows correspond to terms and columns correspond to doc-
uments (section 6.1). a document provides a context for understanding the term. if we
generalize the idea of documents to chunks of text of arbitrary size (phrases, sentences,
paragraphs, chapters, books, collections), the result is the word   context matrix, which in-
cludes the term   document matrix as a special case. section 6.2 discusses applications for
word   context matrices. section 6.3 considers pair   pattern matrices, in which the rows cor-
respond to pairs of terms and the columns correspond to the patterns in which the pairs
occur.

in section 7, we discuss alternatives to vsms for semantics. section 8 considers the
future of vsms, raising some questions about their power and their limitations. we conclude
in section 9.

2. vector space models of semantics

the theme that unites the various forms of vsms that we discuss in this paper can be
stated as the statistical semantics hypothesis: statistical patterns of human word usage can
be used to    gure out what people mean.9 this general hypothesis underlies several more
speci   c hypotheses, such as the bag of words hypothesis, the distributional hypothesis, the
extended distributional hypothesis, and the latent relation hypothesis, discussed below.

2.1 similarity of documents: the term   document matrix

in this paper, we use the following notational conventions: matrices are denoted by bold
capital letters, a. vectors are denoted by bold lowercase letters, b. scalars are represented
by lowercase italic letters, c.

if we have a large collection of documents, and hence a large number of document
vectors, it is convenient to organize the vectors into a matrix. the row vectors of the matrix
correspond to terms (usually terms are words, but we will discuss some other possibilities)

6. see http://lucene.apache.org/java/docs/.
7. see http://code.google.com/p/semanticvectors/.
8. see http://code.google.com/p/airhead-research/wiki/latentrelationalanalysis.
9. this phrase was taken from the faculty pro   le of george furnas at the university of michigan,
http://www.si.umich.edu/people/faculty-detail.htm?sid=41. the full quote is,    statistical semantics
    studies of how the statistical patterns of human word usage can be used to    gure out what people
mean, at least to a level su   cient for information access.    the term statistical semantics appeared in
the work of furnas, landauer, gomez, and dumais (1983), but it was not de   ned there.

146

from frequency to meaning

and the column vectors correspond to documents (web pages, for example). this kind of
matrix is called a term   document matrix.

in mathematics, a bag (also called a multiset) is like a set, except that duplicates are
allowed. for example, {a, a, b, c, c, c} is a bag containing a, b, and c. order does not matter
in bags and sets; the bags {a, a, b, c, c, c} and {c, a, c, b, a, c} are equivalent. we can represent
the bag {a, a, b, c, c, c} with the vector x = h2, 1, 3i, by stipulating that the    rst element of
x is the frequency of a in the bag, the second element is the frequency of b in the bag, and
the third element is the frequency of c. a set of bags can be represented as a matrix x, in
which each column x:j corresponds to a bag, each row xi: corresponds to a unique member,
and an element xij is the frequency of the i-th member in the j-th bag.

in a term   document matrix, a document vector represents the corresponding document
as a bag of words.
in information retrieval, the bag of words hypothesis is that we can
estimate the relevance of documents to a query by representing the documents and the
query as bags of words. that is, the frequencies of words in a document tend to indicate
the relevance of the document to a query. the bag of words hypothesis is the basis for
applying the vsm to information retrieval (salton et al., 1975). the hypothesis expresses
the belief that a column vector in a term   document matrix captures (to some degree) an
aspect of the meaning of the corresponding document; what the document is about.

let x be a term   document matrix. suppose our document collection contains n docu-
ments and m unique terms. the matrix x will then have m rows (one row for each unique
term in the vocabulary) and n columns (one column for each document). let wi be the i-th
term in the vocabulary and let dj be the j-th document in the collection. the i-th row in
x is the row vector xi: and the j-th column in x is the column vector x:j. the row vector
xi: contains n elements, one element for each document, and the column vector x:j contains
m elements, one element for each term. suppose x is a simple matrix of frequencies. the
element xij in x is the frequency of the i-th term wi in the j-th document dj.

in general, the value of most of the elements in x will be zero (the matrix is sparse),
since most documents will use only a small fraction of the whole vocabulary. if we randomly
choose a term wi and a document dj, it   s likely that wi does not occur anywhere in dj, and
therefore xij equals 0.

the pattern of numbers in xi: is a kind of signature of the i-th term wi; likewise, the
pattern of numbers in x:j is a signature of the j-th document dj. that is, the pattern of
numbers tells us, to some degree, what the term or document is about.

the vector x:j may seem to be a rather crude representation of the document dj. it tells
us how frequently the words appear in the document, but the sequential order of the words
is lost. the vector does not attempt to capture the structure in the phrases, sentences,
paragraphs, and chapters of the document. however, in spite of this crudeness, search
engines work surprisingly well; vectors seem to capture an important aspect of semantics.
the vsm of salton et al. (1975) was arguably the    rst practical, useful algorithm for
extracting semantic information from word usage. an intuitive justi   cation for the term   
document matrix is that the topic of a document will probabilistically in   uence the author   s
choice of words when writing the document.10 if two documents have similar topics, then
the two corresponding column vectors will tend to have similar patterns of numbers.

10. newer generative models, such as id44 (lda) (blei, ng, & jordan, 2003), directly

model this intuition. see sections 4.3 and 7.

147

turney & pantel

2.2 similarity of words: the word   context matrix

salton et al. (1975) focused on measuring document similarity, treating a query to a search
engine as a pseudo-document. the relevance of a document to a query is given by the
similarity of their vectors. deerwester et al. (1990) observed that we can shift the focus to
measuring word similarity, instead of document similarity, by looking at row vectors in the
term   document matrix, instead of column vectors.

deerwester et al. (1990) were inspired by the term   document matrix of salton et al. (1975),

but a document is not necessarily the optimal length of text for measuring word similarity.
in general, we may have a word   context matrix, in which the context is given by words,
phrases, sentences, paragraphs, chapters, documents, or more exotic possibilities, such as
sequences of characters or patterns.

the distributional hypothesis in linguistics is that words that occur in similar contexts
tend to have similar meanings (harris, 1954). this hypothesis is the justi   cation for ap-
plying the vsm to measuring word similarity. a word may be represented by a vector
in which the elements are derived from the occurrences of the word in various contexts,
such as windows of words (lund & burgess, 1996), grammatical dependencies (lin, 1998;
pad  o & lapata, 2007), and richer contexts consisting of dependency links and selectional
preferences on the argument positions (erk & pad  o, 2008); see sahlgren   s (2006) thesis for
a comprehensive study of various contexts. similar row vectors in the word   context matrix
indicate similar word meanings.

the idea that word usage can reveal semantics was implicit in some of the things that
wittgenstein (1953) said about language-games and family resemblance. wittgenstein was
primarily interested in the physical activities that form the context of word usage (e.g., the
word brick, spoken in the context of the physical activity of building a house), but the main
context for a word is often other words.11

weaver (1955) argued that id51 for machine translation should
be based on the co-occurrence frequency of the context words near a given target word (the
word that we want to disambiguate). firth (1957, p. 11) said,    you shall know a word
by the company it keeps.    deerwester et al. (1990) showed how the intuitions of wittgen-
stein (1953), harris (1954), weaver, and firth could be used in a practical algorithm.

2.3 similarity of relations: the pair   pattern matrix

in a pair   pattern matrix, row vectors correspond to pairs of words, such as mason : stone
and carpenter : wood, and column vectors correspond to the patterns in which the pairs co-
occur, such as    x cuts y     and    x works with y    . lin and pantel (2001) introduced the
pair   pattern matrix for the purpose of measuring the semantic similarity of patterns; that
is, the similarity of column vectors. given a pattern such as    x solves y    , their algorithm
was able to    nd similar patterns, such as    y is solved by x   ,    y is resolved in x   , and
   x resolves y    .

lin and pantel (2001) proposed the extended distributional hypothesis, that patterns
that co-occur with similar pairs tend to have similar meanings. the patterns    x solves y    

11. wittgenstein   s intuition might be better captured by a matrix that combines words with other modalities,
if the values of the elements are derived from event

such as images (monay & gatica-perez, 2003).
frequencies, we would include this as a vsm approach to semantics.

148

from frequency to meaning

and    y is solved by x    tend to co-occur with similar x : y pairs, which suggests that these
patterns have similar meanings. pattern similarity can be used to infer that one sentence
is a paraphrase of another (lin & pantel, 2001).

turney et al. (2003) introduced the use of the pair   pattern matrix for measuring the
semantic similarity of relations between word pairs; that is, the similarity of row vectors.
for example, the pairs mason : stone, carpenter : wood, potter : clay, and glassblower : glass
share the semantic relation artisan : material. in each case, the    rst member of the pair is
an artisan who makes artifacts from the material that is the second member of the pair.
the pairs tend to co-occur in similar patterns, such as    the x used the y to    and    the x
shaped the y into   .

the latent relation hypothesis is that pairs of words that co-occur in similar patterns
tend to have similar semantic relations (turney, 2008a). word pairs with similar row
vectors in a pair   pattern matrix tend to have similar semantic relations. this is the inverse
of the extended distributional hypothesis, that patterns with similar column vectors in the
pair   pattern matrix tend to have similar meanings.

2.4 similarities

pair   pattern matrices are suited to measuring the similarity of semantic relations between
pairs of words; that is, relational similarity. in contrast, word   context matrices are suited
to measuring attributional similarity. the distinction between attributional and relational
similarity has been explored in depth by gentner (1983).

the attributional similarity between two words a and b, sima(a, b)        , depends on the
degree of correspondence between the properties of a and b. the more correspondence there
is, the greater their attributional similarity. the relational similarity between two pairs of
words a : b and c : d, simr(a : b, c : d)        , depends on the degree of correspondence between
the relations of a : b and c : d. the more correspondence there is, the greater their relational
similarity. for example, dog and wolf have a relatively high degree of attributional similar-
ity, whereas dog : bark and cat : meow have a relatively high degree of relational similarity
(turney, 2006).

it is tempting to suppose that relational similarity can be reduced to attributional
similarity. for example, mason and carpenter are similar words and stone and wood are
similar words; therefore, perhaps it follows that mason : stone and carpenter : wood have
similar relations. perhaps simr(a : b, c : d) can be reduced to sima(a, c) + sima(b, d). however,
mason, carpenter, potter, and glassblower are similar words (they are all artisans), as are
wood, clay, stone, and glass (they are all materials used by artisans), but we cannot infer
from this that mason : glass and carpenter : clay have similar relations. turney (2006, 2008a)
presented experimental evidence that relational similarity does not reduce to attributional
similarity.

the term semantic relatedness in computational linguistics (budanitsky & hirst, 2001)
corresponds to attributional similarity in cognitive science (gentner, 1983). two words
are semantically related if they have any kind of semantic relation (budanitsky & hirst,
2001); they are semantically related to the degree that they share attributes (turney, 2006).
examples are synonyms (bank and trust company), meronyms (car and wheel), antonyms
(hot and cold), and words that are functionally related or frequently associated (pencil and

149

turney & pantel

paper). we might not usually think that antonyms are similar, but antonyms have a high
degree of attributional similarity (hot and cold are kinds of temperature, black and white
are kinds of colour, loud and quiet are kinds of sound). we prefer the term attributional
similarity to the term semantic relatedness, because attributional similarity emphasizes the
contrast with relational similarity, whereas semantic relatedness could be confused with
relational similarity.

in computational linguistics, the term semantic similarity is applied to words that share
a hypernym (car and bicycle are semantically similar, because they share the hypernym
vehicle) (resnik, 1995). semantic similarity is a speci   c type of attributional similarity. we
prefer the term taxonomical similarity to the term semantic similarity, because the term
semantic similarity is misleading. intuitively, both attributional and relational similarity
involve meaning, so both deserve to be called semantic similarity.

words are semantically associated if they tend to co-occur frequently (e.g., bee and
honey) (chiarello, burgess, richards, & pollock, 1990). words may be taxonomically simi-
lar and semantically associated (doctor and nurse), taxonomically similar but not semanti-
cally associated (horse and platypus), semantically associated but not taxonomically similar
(cradle and baby), or neither semantically associated nor taxonomically similar (calculus and
candy).

sch  utze and pedersen (1993) de   ned two ways that words can be distributed in a cor-
pus of text: if two words tend to be neighbours of each other, then they are syntagmatic
associates. if two words have similar neighbours, then they are paradigmatic parallels. syn-
tagmatic associates are often di   erent parts of speech, whereas paradigmatic parallels are
usually the same part of speech. syntagmatic associates tend to be semantically associ-
ated (bee and honey are often neighbours); paradigmatic parallels tend to be taxonomically
similar (doctor and nurse have similar neighbours).

2.5 other semantic vsms

the possibilities are not exhausted by term   document, word   context, and pair   pattern
matrices. we might want to consider triple   pattern matrices, for measuring the semantic
similarity between word triples. whereas a pair   pattern matrix might have a row mason :
stone and a column    x works with y    , a triple   pattern matrix could have a row mason :
stone : masonry and a column    x uses y to build z   . however, n-tuples of words grow
increasingly rare as n increases. for example, phrases that contain mason, stone, and
masonry together are less frequent than phrases that contain mason and stone together. a
triple   pattern matrix will be much more sparse than a pair   pattern matrix (ceteris paribus).
the quantity of text that we need, in order to have enough numbers to make our matrices
useful, grows rapidly as n increases. it may be better to break n-tuples into pairs. for
example, a : b : c could be decomposed into a : b, a : c, and b : c (turney, 2008a). the similarity
of two triples, a : b : c and d : e : f , could be estimated by the similarity of their corresponding
pairs. a relatively dense pair   pattern matrix could serve as a surrogate for a relatively
sparse triple   pattern matrix.

we may also go beyond matrices. the generalization of a matrix is a tensor (kolda
& bader, 2009; acar & yener, 2009). a scalar (a single number) is zeroth-order tensor, a
vector is    rst-order tensor, and a matrix is a second-order tensor. a tensor of order three or

150

from frequency to meaning

higher is called a higher-order tensor. chew, bader, kolda, and abdelali (2007) use a term   
document   language third-order tensor for multilingual information retrieval. turney (2007)
uses a word   word   pattern tensor to measure similarity of words. van de cruys (2009) uses
a verb   subject   object tensor to learn selectional preferences of verbs.

in turney   s (2007) tensor, for example, rows correspond to words from the toefl
multiple-choice synonym questions, columns correspond to words from basic english (og-
den, 1930),12 and tubes correspond to patterns that join rows and columns (hence we have
a word   word   pattern third-order tensor). a given word from the toefl questions is rep-
resented by the corresponding word   pattern matrix slice in the tensor. the elements in
this slice correspond to all the patterns that relate the given toefl word to any word
in basic english. the similarity of two toefl words is calculated by comparing the two
corresponding matrix slices. the algorithm achieves 83.8% on the toefl questions.

2.6 types and tokens

a token is a single instance of a symbol, whereas a type is a general class of tokens (manning
et al., 2008). consider the following example (from samuel beckett):

ever tried. ever failed.
no matter. try again.
fail again. fail better.

there are two tokens of the type ever, two tokens of the type again, and two tokens of
the type fail. let   s say that each line in this example is a document, so we have three
documents of two sentences each. we can represent this example with a token   document
matrix or a type   document matrix. the token   document matrix has twelve rows, one for
each token, and three columns, one for each line (figure 1). the type   document matrix
has nine rows, one for each type, and three columns (figure 2).

a row vector for a token has binary values: an element is 1 if the given token appears in
the given document and 0 otherwise. a row vector for a type has integer values: an element
is the frequency of the given type in the given document. these vectors are related, in that
a type vector is the sum of the corresponding token vectors. for example, the row vector
for the type ever is the sum of the two token vectors for the two tokens of ever.

in applications dealing with polysemy, one approach uses vectors that represent word
tokens (sch  utze, 1998; agirre & edmonds, 2006) and another uses vectors that represent
word types (pantel & lin, 2002a). typical id51 (wsd) algorithms
deal with word tokens (instances of words in speci   c contexts) rather than word types.
we mention both approaches to polysemy in section 6, due to their similarity and close
relationship, although a de   ning characteristic of the vsm is that it is concerned with
frequencies (see section 1.1), and frequency is a property of types, not tokens.

12. basic english is a highly reduced subset of english, designed to be easy for people to learn. the words

of basic english are listed at http://ogden.basic-english.org/.

151

turney & pantel

ever tried. no matter.
ever failed. try again.

fail again.
fail better.

ever
tried
ever
failed
no
matter
try
again
fail
again
fail
better

1
1
1
1
0
0
0
0
0
0
0
0

0
0
0
0
1
1
1
1
0
0
0
0

0
0
0
0
0
0
0
0
1
1
1
1

figure 1: the token   document matrix. rows are tokens and columns are documents.

ever tried. no matter.
ever failed. try again.

fail again.
fail better.

ever
tried
failed
no
matter
try
again
fail
better

2
1
1
0
0
0
0
0
0

0
0
0
1
1
1
1
0
0

0
0
0
0
0
0
1
2
1

figure 2: the type   document matrix. rows are types and columns are documents.

152

from frequency to meaning

2.7 hypotheses

we have mentioned    ve hypotheses in this section. here we repeat these hypotheses and
then interpret them in terms of vectors. for each hypothesis, we cite work that explicitly
states something like the hypothesis or implicitly assumes something like the hypothesis.

statistical semantics hypothesis: statistical patterns of human word usage can be
used to    gure out what people mean (weaver, 1955; furnas et al., 1983).     if units of text
have similar vectors in a text frequency matrix,13 then they tend to have similar meanings.
(we take this to be a general hypothesis that subsumes the four more speci   c hypotheses
that follow.)

bag of words hypothesis: the frequencies of words in a document tend to indicate
the relevance of the document to a query (salton et al., 1975).     if documents and pseudo-
documents (queries) have similar column vectors in a term   document matrix, then they
tend to have similar meanings.

distributional hypothesis: words that occur in similar contexts tend to have similar
meanings (harris, 1954; firth, 1957; deerwester et al., 1990).     if words have similar row
vectors in a word   context matrix, then they tend to have similar meanings.

extended distributional hypothesis: patterns that co-occur with similar pairs tend
to have similar meanings (lin & pantel, 2001).     if patterns have similar column vectors
in a pair   pattern matrix, then they tend to express similar semantic relations.

latent relation hypothesis: pairs of words that co-occur in similar patterns tend
to have similar semantic relations (turney et al., 2003).     if word pairs have similar row
vectors in a pair   pattern matrix, then they tend to have similar semantic relations.

we have not yet explained what it means to say that vectors are similar. we discuss

this in section 4.4.

3. linguistic processing for vector space models

we will assume that our raw data is a large corpus of natural language text. before we
generate a term   document, word   context, or pair   pattern matrix, it can be useful to apply
some linguistic processing to the raw text. the types of processing that are used can be
grouped into three classes. first, we need to tokenize the raw text; that is, we need to decide
what constitutes a term and how to extract terms from raw text. second, we may want to
normalize the raw text, to convert super   cially di   erent strings of characters to the same
form (e.g., car, car, cars, and cars could all be normalized to car). third, we may want
to annotate the raw text, to mark identical strings of characters as being di   erent (e.g.,    y
as a verb could be annotated as    y/vb and    y as a noun could be annotated as    y/nn).
grefenstette (1994) presents a good study of linguistic processing for word   context
vsms. he uses a similar three-step decomposition of linguistic processing: id121,
surface syntactic analysis, and syntactic attribute extraction.

13. by text frequency matrix, we mean any matrix or higher-order tensor in which the values of the elements
are derived from the frequencies of pieces of text in the context of other pieces of text in some collection
of text. a text frequency matrix is intended to be a general structure, which includes term   document,
word   context, and pair   pattern matrices as special cases.

153

turney & pantel

3.1 id121

id121 of english seems simple at    rst glance: words are separated by spaces. this
assumption is approximately true for english, and it may work su   ciently well for a basic
vsm, but a more advanced vsm requires a more sophisticated approach to id121.
an accurate english tokenizer must know how to handle punctuation (e.g., don   t, jane   s,
and/or), hyphenation (e.g., state-of-the-art versus state of the art), and recognize multi-word
terms (e.g., barack obama and ice hockey) (manning et al., 2008). we may also wish to
ignore stop words, high-frequency words with relatively low information content, such as
function words (e.g., of, the, and) and pronouns (e.g., them, who, that). a popular list of
stop words is the set of 571 common words included in the source code for the smart
system (salton, 1971).14

in some languages (e.g., chinese), words are not separated by spaces. a basic vsm
can break the text into character unigrams or bigrams. a more sophisticated approach
is to match the input text against entries in a lexicon, but the matching often does not
determine a unique id121 (sproat & emerson, 2003). furthermore, native speakers
often disagree about the correct segmentation. highly accurate id121 is a challenging
task for most human languages.

3.2 id172

the motivation for id172 is the observation that many di   erent strings of charac-
ters often convey essentially identical meanings. given that we want to get at the meaning
that underlies the words, it seems reasonable to normalize super   cial variations by con-
verting them to the same form. the most common types of id172 are case folding
(converting all words to lower case) and id30 (reducing in   ected words to their stem
or root form).

case folding is easy in english, but can be problematic in some languages. in french,
accents are optional for uppercase, and it may be di   cult to restore missing accents when
converting the words to lowercase. some words cannot be distinguished without accents; for
example, peche could be either p  eche (meaning    shing or peach) or p  ech  e (meaning sin).
even in english, case folding can cause problems, because case sometimes has semantic
signi   cance. for example, smart is an information retrieval system, whereas smart is a
common adjective; bush may be a surname, whereas bush is a kind of plant.

morphology is the study of the internal structure of words. often a word is composed
of a stem (root) with added a   xes (in   ections), such as plural forms and past tenses (e.g.,
trapped is composed of the stem trap and the a   x -ed). id30, a kind of morphological
analysis, is the process of reducing in   ected words to their stems. in english, a   xes are
simpler and more regular than in many other languages, and id30 algorithms based
on heuristics (rules of thumb) work relatively well (lovins, 1968; porter, 1980; minnen,
carroll, & pearce, 2001). in an agglutinative language (e.g., inuktitut), many concepts are
combined into a single word, using various pre   xes, in   xes, and su   xes, and morphological
analysis is complicated. a single word in an agglutinative language may correspond to a
sentence of half a dozen words in english (johnson & martin, 2003).

14. the source code is available at ftp://ftp.cs.cornell.edu/pub/smart/.

154

from frequency to meaning

the performance of an information retrieval system is often measured by precision and
recall (manning et al., 2008). the precision of a system is an estimate of the conditional
id203 that a document is truly relevant to a query, if the system says it is relevant.
the recall of a system is an estimate of the id155 that the system will say
that a document is relevant to a query, if it truly is relevant.

in general, id172 increases recall and reduces precision (kraaij & pohlmann,
1996). this is natural, given the nature of id172. when we remove super   cial
variations that we believe are irrelevant to meaning, we make it easier to recognize similar-
ities; we    nd more similar things, and so recall increases. but sometimes these super   cial
variations have semantic signi   cance; ignoring the variations causes errors, and so precision
decreases. id172 can also have a positive e   ect on precision in cases where variant
tokens are infrequent and smoothing the variations gives more reliable statistics.

if we have a small corpus, we may not be able to a   ord to be overly selective, and it may
be best to aggressively normalize the text, to increase recall. if we have a very large corpus,
precision may be more important, and we might not want any id172. hull (1996)
gives a good analysis of id172 for information retrieval.

3.3 annotation

annotation is the inverse of id172. just as di   erent strings of characters may have
the same meaning, it also happens that identical strings of characters may have di   erent
meanings, depending on the context. common forms of annotation include part-of-speech
tagging (marking words according to their parts of speech), word sense tagging (marking
ambiguous words according to their intended meanings), and parsing (analyzing the gram-
matical structure of sentences and marking the words in the sentences according to their
grammatical roles) (manning & sch  utze, 1999).

since annotation is the inverse of id172, we expect it to decrease recall and
increase precision. for example, by tagging program as a noun or a verb, we may be
able to selectively search for documents that are about the act of computer programming
(verb) instead of documents that discuss particular computer programs (noun); hence we
can increase precision. however, a document about computer programs (noun) may have
something useful to say about the act of computer programming (verb), even if the document
never uses the verb form of program; hence we may decrease recall.

large gains in ir performance have recently been reported as a result of query an-
notation with syntactic and semantic information. syntactic annotation includes query
segmentation (tan & peng, 2008) and id52 (barr, jones, & regelson,
2008). examples of semantic annotation are disambiguating abbreviations in queries (wei,
peng, & dumoulin, 2008) and    nding query keyword associations (lavrenko & croft, 2001;
cao, nie, & bai, 2005).

annotation is also useful for measuring the semantic similarity of words and concepts
(word   context matrices). for example, pantel and lin (2002a) presented an algorithm
that can discover word senses by id91 row vectors in a word   context matrix, using
contextual information derived from parsing.

155

turney & pantel

4. mathematical processing for vector space models

after the text has been tokenized and (optionally) normalized and annotated, the    rst step
is to generate a matrix of frequencies. second, we may want to adjust the weights of the
elements in the matrix, because common words will have high frequencies, yet they are less
informative than rare words. third, we may want to smooth the matrix, to reduce the
amount of random noise and to    ll in some of the zero elements in a sparse matrix. fourth,
there are many di   erent ways to measure the similarity of two vectors.

lowe (2001) gives a good summary of mathematical processing for word   context vsms.
he decomposes vsm construction into a similar four-step process: calculate the frequencies,
transform the raw frequency counts, smooth the space (id84), then
calculate the similarities.

4.1 building the frequency matrix

an element in a frequency matrix corresponds to an event: a certain item (term, word,
word pair) occurred in a certain situation (document, context, pattern) a certain number
of times (frequency). at an abstract level, building a frequency matrix is a simple matter
of counting events. in practice, it can be complicated when the corpus is large.

a typical approach to building a frequency matrix involves two steps. first, scan se-
quentially through the corpus, recording events and their frequencies in a hash table, a
database, or a search engine index. second, use the resulting data structure to generate the
frequency matrix, with a sparse matrix representation (gilbert, moler, & schreiber, 1992).

4.2 weighting the elements

the idea of weighting is to give more weight to surprising events and less weight to expected
events. the hypothesis is that surprising events, if shared by two vectors, are more dis-
criminative of the similarity between the vectors than less surprising events. for example,
in measuring the semantic similarity between the words mouse and rat, the contexts dissect
and exterminate are more discriminative of their similarity than the contexts have and like.
in id205, a surprising event has higher information content than an expected
event (shannon, 1948). the most popular way to formalize this idea for term   document
matrices is the tf-idf (term frequency    inverse document frequency) family of weighting
functions (sp  arck jones, 1972). an element gets a high weight when the corresponding term
is frequent in the corresponding document (i.e., tf is high), but the term is rare in other
documents in the corpus (i.e., df is low, and thus idf is high). salton and buckley (1988)
de   ned a large family of tf-idf weighting functions and evaluated them on information re-
trieval tasks, demonstrating that tf-idf weighting can yield signi   cant improvements over
raw frequency.

another kind of weighting, often combined with tf-idf weighting, is length id172
(singhal, salton, mitra, & buckley, 1996).
in information retrieval, if document length
is ignored, search engines tend to have a bias in favour of longer documents. length
id172 corrects for this bias.

term weighting may also be used to correct for correlated terms. for example, the
terms hostage and hostages tend to be correlated, yet we may not want to normalize them

156

from frequency to meaning

to the same term (as in section 3.2), because they have slightly di   erent meanings. as
an alternative to normalizing them, we may reduce their weights when they co-occur in a
document (church, 1995).

feature selection may be viewed as a form of weighting, in which some terms get a
weight of zero and hence can be removed from the matrix. forman (2003) provides a good
study of feature selection methods for text classi   cation.

an alternative to tf-idf is pointwise mutual information (pmi) (church & hanks, 1989;
turney, 2001), which works well for both word   context matrices (pantel & lin, 2002a)
and term   document matrices (pantel & lin, 2002b). a variation of pmi is positive pmi
(ppmi), in which all pmi values that are less than zero are replaced with zero (niwa &
nitta, 1994). bullinaria and levy (2007) demonstrated that ppmi performs better than a
wide variety of other weighting approaches when measuring semantic similarity with word   
context matrices. turney (2008a) applied ppmi to pair   pattern matrices. we will give the
formal de   nition of ppmi here, as an example of an e   ective weighting function.

let f be a word   context frequency matrix with nr rows and nc columns. the i-th row
in f is the row vector fi: and the j-th column in f is the column vector f:j. the row fi:
corresponds to a word wi and the column f:j corresponds to a context cj. the value of the
element fij is the number of times that wi occurs in the context cj. let x be the matrix
that results when ppmi is applied to f. the new matrix x has the same number of rows
and columns as the raw frequency matrix f. the value of an element xij in x is de   ned
as follows:

fij

pij =

i=1 fij

j=1 fij

j=1 fij

j=1 fij

i=1pnc
pnr
pi    = pnc
i=1pnc
pnr
p   j = pnr
i=1pnc
pnr
j=1 fij
pmiij = log(cid:18) pij
pi   p   j(cid:19)
xij = (cid:26) pmiij

if pmiij > 0

0 otherwise

(1)

(2)

(3)

(4)

(5)

in this de   nition, pij is the estimated id203 that the word wi occurs in the context
cj, pi    is the estimated id203 of the word wi, and p   j is the estimated id203 of
the context cj. if wi and cj are statistically independent, then pi   p   j = pij (by the de   nition
of independence), and thus pmiij is zero (since log(1) = 0). the product pi   p   j is what we
would expect for pij if wi occurs in cj by pure random chance. on the other hand, if there
is an interesting semantic relation between wi and cj, then we should expect pij to be larger
than it would be if wi and cj were indepedent; hence we should    nd that pij > pi   p   j, and
thus pmiij is positive. this follows from the distributional hypothesis (see section 2). if
the word wi is unrelated to the context cj, we may    nd that pmiij is negative. ppmi is
designed to give a high value to xij when there is an interesting semantic relation between

157

turney & pantel

wi and cj; otherwise, xij should have a value of zero, indicating that the occurrence of wi
in cj is uninformative.

a well-known problem of pmi is that it is biased towards infrequent events. consider
the case where wi and cj are statistically dependent (i.e., they have maximum association).
then pij = pi    = p   j. hence (4) becomes log (1/pi   ) and pmi increases as the id203 of
word wi decreases. several discounting factors have been proposed to alleviate this problem.
an example follows (pantel & lin, 2002a):

  ij =

fij
fij + 1   
newpmiij =   ij    pmiij

min (pnr
min (pnr

k=1 fkj,pnc
k=1 fkj,pnc

k=1 fik)

k=1 fik) + 1

(6)

(7)

another way to deal with infrequent events is laplace smoothing of the id203
estimates, pij, pi   , and p   j (turney & littman, 2003). a constant positive value is added to
the raw frequencies before calculating the probabilities; each fij is replaced with fij + k, for
some k > 0. the larger the constant, the greater the smoothing e   ect. laplace smoothing
pushes the pmiij values towards zero. the magnitude of the push (the di   erence between
pmiij with and without laplace smoothing) depends on the raw frequency fij.
if the
frequency is large, the push is small; if the frequency is small, the push is large. thus
laplace smoothing reduces the bias of pmi towards infrequent events.

4.3 smoothing the matrix

the simplest way to improve information retrieval performance is to limit the number of
vector components. keeping only components representing the most frequently occurring
content words is such a way; however, common words, such as the and have, carry little
semantic discrimination power. simple component smoothing heuristics, based on the prop-
erties of the weighting schemes presented in section 4.2, have been shown to both maintain
semantic discrimination power and improve the performance of similarity computations.

computing the similarity between all pairs of vectors, described in section 4.4, is a
computationally intensive task. however, only vectors that share a non-zero coordinate
must be compared (i.e., two vectors that do not share a coordinate are dissimilar). very
frequent context words, such as the word the, unfortunately result in most vectors matching
a non-zero coordinate. such words are precisely the contexts that have little semantic
discrimination power. consider the pointwise mutual information weighting described in
section 4.2. highly weighted dimensions co-occur frequently with only very few words and
are by de   nition highly discriminating contexts (i.e., they have very high association with
the words with which they co-occur). by keeping only the context-word dimensions with
a pmi above a conservative threshold and setting the others to zero, lin (1998) showed
that the number of comparisons needed to compare vectors greatly decreases while losing
little precision in the similarity score between the top-200 most similar words of every word.
while smoothing the matrix, one computes a reverse index on the non-zero coordinates.
then, to compare the similarity between a word   s context vector and all other words    context
vectors, only those vectors found to match a non-zero component in the reverse index must
be compared. section 4.5 proposes further optimizations along these lines.

158

from frequency to meaning

deerwester et al. (1990) found an elegant way to improve similarity measurements with a
mathematical operation on the term   document matrix, x, based on id202. the op-
eration is truncated singular value decomposition (svd), also called thin svd. deerwester
et al. brie   y mentioned that truncated svd can be applied to both document similarity
and word similarity, but their focus was document similarity. landauer and dumais (1997)
applied truncated svd to word similarity, achieving human-level scores on multiple-choice
synonym questions from the test of english as a foreign language (toefl). truncated
svd applied to document similarity is called id45 (lsi), but it is
called latent semantic analysis (lsa) when applied to word similarity.

there are several ways of thinking about how truncated svd works. we will    rst
present the math behind truncated svd and then describe four ways of looking at it:
latent meaning, noise reduction, high-order co-occurrence, and sparsity reduction.

svd decomposes x into the product of three matrices u  vt, where u and v are in
column orthonormal form (i.e., the columns are orthogonal and have unit length, utu =
vtv = i) and    is a diagonal matrix of singular values (golub & van loan, 1996). if x
is of rank r, then    is also of rank r. let   k, where k < r, be the diagonal matrix formed
from the top k singular values, and let uk and vk be the matrices produced by selecting the
corresponding columns from u and v. the matrix uk  kvt
k is the matrix of rank k that
best approximates the original matrix x, in the sense that it minimizes the approximation
k minimizes k   x     xkf over all matrices   x of rank k, where
errors. that is,   x = uk  kvt
k . . . kf denotes the frobenius norm (golub & van loan, 1996).
latent meaning: deerwester et al. (1990) and landauer and dumais (1997) describe
truncated svd as a method for discovering latent meaning. suppose we have a word   
context matrix x. the truncated svd,   x = uk  kvt
k , creates a low-dimensional linear
mapping between row space (words) and column space (contexts). this low-dimensional
mapping captures the latent (hidden) meaning in the words and the contexts. limiting the
number of latent dimensions (k < r) forces a greater correspondence between words and
contexts. this forced correspondence between words and contexts improves the similarity
measurement.

noise reduction: rapp (2003) describes truncated svd as a noise reduction technique.
we may think of the matrix   x = uk  kvt
k as a smoothed version of the original matrix x.
the matrix uk maps the row space (the space spanned by the rows of x) into a smaller
k-dimensional space and the matrix vk maps the column space (the space spanned by
the columns of x) into the same k-dimensional space. the diagonal matrix   k speci   es
the weights in this reduced k-dimensional space. the singular values in    are ranked in
descending order of the amount of variation in x that they    t. if we think of the matrix
x as being composed of a mixture of signal and noise, with more signal than noise, then
uk  kvt
k mostly captures the variation in x that is due to the signal, whereas the remaining
vectors in u  vt are mostly    tting the variation in x that is due to the noise.

high-order co-occurrence: landauer and dumais (1997) also describe truncated
svd as a method for discovering high-order co-occurrence. direct co-occurrence (   rst-
order co-occurrence) is when two words appear in identical contexts. indirect co-occurrence
(high-order co-occurrence) is when two words appear in similar contexts. similarity of
contexts may be de   ned recursively in terms of lower-order co-occurrence. lemaire and
denhi`ere (2006) demonstrate that truncated svd can discover high-order co-occurrence.

159

turney & pantel

sparsity reduction: in general, the matrix x is very sparse (mostly zeroes), but the
truncated svd,   x = uk  kvt
k , is dense. sparsity may be viewed as a problem of insu   cient
data: with more text, the matrix x would have fewer zeroes, and the vsm would perform
better on the chosen task. from this perspective, truncated svd is a way of simulating the
missing text, compensating for the lack of data (vozalis & margaritis, 2003).

these di   erent ways of viewing truncated svd are compatible with each other; it is
possible for all of these perspectives to be correct. future work is likely to provide more
views of svd and perhaps a unifying view.

a good c implementation of svd for large sparse matrices is rohde   s svdlibc.15
another approach is brand   s (2006) incremental truncated svd algorithm.16 yet another
approach is gorrell   s (2006) hebbian algorithm for incremental truncated svd. brand   s
and gorrell   s algorithms both introduce interesting new ways of handling missing values,
instead of treating them as zero values.

for higher-order tensors, there are operations that are analogous to truncated svd,
such as parallel factor analysis (parafac) (harshman, 1970), canonical decomposition
(candecomp) (carroll & chang, 1970) (equivalent to parafac but discovered inde-
pendently), and tucker decomposition (tucker, 1966). for an overview of tensor decompo-
sitions, see the surveys of kolda and bader (2009) or acar and yener (2009). turney (2007)
gives an empirical evaluation of how well four di   erent tucker decomposition algorithms
scale up for large sparse third-order tensors. a low-ram algorithm, multislice projection,
for large sparse tensors is presented and evaluated.17

since the work of deerwester et al. (1990), subsequent research has discovered many
alternative matrix smoothing processes, such as nonnegative id105 (nmf)
(lee & seung, 1999), probabilistic id45 (plsi) (hofmann, 1999), iter-
ative scaling (is) (ando, 2000), kernel principal components analysis (kpca) (scholkopf,
smola, & muller, 1997), id44 (lda) (blei et al., 2003), and discrete
component analysis (dca) (buntine & jakulin, 2006).

the four perspectives on truncated svd, presented above, apply equally well to all of
these more recent matrix smoothing algorithms. these newer smoothing algorithms tend
to be more computationally intensive than truncated svd, but they attempt to model
word frequencies better than svd. truncated svd implicitly assumes that the elements
in x have a gaussian distribution. minimizing the the frobenius norm k   x     xkf will
minimize the noise, if the noise has a gaussian distribution. however, it is known that
word frequencies do not have gaussian distributions. more recent algorithms are based on
more realistic models of the distribution for word frequencies.18

4.4 comparing the vectors

the most popular way to measure the similarity of two frequency vectors (raw or weighted)
is to take their cosine. let x and y be two vectors, each with n elements.

15. svdlibc is available at http://tedlab.mit.edu/   dr/svdlibc/.
16. matlab source code is available at http://web.mit.edu/   wingated/www/resources.html.
17. matlab source code is available at http://www.apperceptual.com/multislice/.
18. in our experience, pmiij appears to be approximately gaussian, which may explain why pmi works well
with truncated svd, but then ppmi is puzzling, because it is less gaussian than pmi, yet it apparently
yields better semantic models than pmi.

160

from frequency to meaning

x = hx1, x2, . . . , xni
y = hy1, y2, . . . , yni

the cosine of the angle    between x and y can be calculated as follows:

cos(x, y) =

=

=

i=1 y2

i

pn
i=1 xi    yi
qpn
i   pn
i=1 x2
x    y
   x    x       y    y
y
kxk   
kyk

x

(8)

(9)

(10)

(11)

(12)

in other words, the cosine of the angle between two vectors is the inner product of the
vectors, after they have been normalized to unit length. if x and y are frequency vectors
for words, a frequent word will have a long vector and a rare word will have a short vector,
yet the words might be synonyms. cosine captures the idea that the length of the vectors
is irrelevant; the important thing is the angle between the vectors.

the cosine ranges from    1 when the vectors point in opposite directions (   is 180
degrees) to +1 when they point in the same direction (   is 0 degrees). when the vectors
are orthogonal (   is 90 degrees), the cosine is zero. with raw frequency vectors, which
necessarily cannot have negative elements, the cosine cannot be negative, but weighting
and smoothing often introduce negative elements. ppmi weighting does not yield negative
elements, but truncated svd can generate negative elements, even when the input matrix
has no negative values.

a measure of distance between vectors can easily be converted to a measure of similarity

by inversion (13) or subtraction (14).

sim(x, y) = 1/dist(x, y)
sim(x, y) = 1     dist(x, y)

(13)

(14)

many similarity measures have been proposed in both ir (jones & furnas, 1987) and
lexical semantics circles (lin, 1998; dagan, lee, & pereira, 1999; lee, 1999; weeds, weir,
& mccarthy, 2004). it is commonly said in ir that, properly normalized, the di   erence
in retrieval performance using di   erent measures is insigni   cant (van rijsbergen, 1979).
often the vectors are normalized in some way (e.g., unit length or unit id203) before
applying any similarity measure.

popular geometric measures of vector distance include euclidean distance and manhat-
tan distance. distance measures from id205 include hellinger, bhattacharya,
and kullback-leibler. bullinaria and levy (2007) compared these    ve distance measures
and the cosine similarity measure on four di   erent tasks involving word similarity. overall,
the best measure was cosine. other popular measures are the dice and jaccard coe   cients
(manning et al., 2008).

161

turney & pantel

lee (1999) proposed that, for    nding word similarities, measures that focused more on
overlapping coordinates and less on the importance of negative features (i.e., coordinates
where one word has a nonzero value and the other has a zero value) appear to perform
better.
in lee   s experiments, the jaccard, jensen-shannon, and l1 measures seemed to
perform best. weeds et al. (2004) studied the linguistic and statistical properties of the
similar words returned by various similarity measures and found that the measures can be
grouped into three classes:

1. high-frequency sensitive measures (cosine, jensen-shannon,   -skew, recall),

2. low-frequency sensitive measures (precision), and

3. similar-frequency sensitive methods (jaccard, jaccard+mi, lin, harmonic mean).

given a word w0, if we use a high-frequency sensitive measure to score other words wi
according to their similarity with w0, higher frequency words will tend to get higher scores
than lower frequency words. if we use a low-frequency sensitive measure, there will be a
bias towards lower frequency words. similar-frequency sensitive methods prefer a word wi
that has approximately the same frequency as w0. in one experiment on determining the
compositionality of collocations, high-frequency sensitive measures outperformed the other
classes (weeds et al., 2004). we believe that determining the most appropriate similarity
measure is inherently dependent on the similarity task, the sparsity of the statistics, the
frequency distribution of the elements being compared, and the smoothing method applied
to the matrix.

4.5 e   cient comparisons

computing the similarity between all rows (or columns) in a large matrix is a non-trivial
problem, with a worst case cubic running time o(n2
rnc), where nr is the number of rows and
nc is the number of columns (i.e., the dimensionality of the feature space). optimizations
and parallelization are often necessary.

4.5.1 sparse-id127

one optimization strategy is a generalized sparse-id127 approach (sarawagi
& kirpal, 2004), which is based on the observation that a scalar product of two vectors
depends only on the coordinates for which both vectors have nonzero values. further, we
observe that most commonly used similarity measures for vectors x and y, such as cosine,
overlap, and dice, can be decomposed into three values: one depending only on the nonzero
values of x, another depending only on the nonzero values of y, and the third depending on
the nonzero coordinates shared both by x and y. more formally, commonly used similarity
scores, sim(x, y), can be expressed as follows:

sim(x, y) = f0 (pn

i=1f1(xi, yi), f2(x), f3(y))

(15)

for example, the cosine measure, cos(x, y), de   ned in (10), can be expressed in this model
as follows:

162

from frequency to meaning

i=1f1(xi, yi), f2(x), f3(y))

f0(a, b, c) =

cos(x, y) = f0 (pn

a
b    c
f1(a, b) = a    b
f2(a) = f3(a) = qpn

i=1a2

i

(16)

(17)

(18)

(19)

is pi n 2

let x be a matrix for which we want to compute the pairwise similarity, sim(x, y),
between all rows or all columns x and y. e   cient computation of the similarity matrix s
can be achieved by leveraging the fact that sim(x, y) is determined solely by the nonzero
coordinates shared by x and y (i.e., f1(0, xi) = f1(xi, 0) = 0 for any xi) and that most of
the vectors are very sparse. in this case, calculating f1(xi, yi) is only required when both
vectors have a shared nonzero coordinate, signi   cantly reducing the cost of computation.
determining which vectors share a nonzero coodinate can easily be achieved by    rst building
an inverted index for the coordinates. during indexing, we can also precompute f2(x) and
f3(y) without changing the algorithm complexity. then, for each vector x we retrieve in
constant time, from the index, each vector y that shares a nonzero coordinate with x and
we apply f1(xi, yi) on the shared coordinates i. the computational cost of this algorithm
i where ni is the number of vectors that have a nonzero i-th coordinate. its worst
case time complexity is o(ncv) where n is the number of vectors to be compared, c is the
maximum number of nonzero coordinates of any vector, and v is the number of vectors
that have a nonzero i-th coordinate where i is the coordinate which is nonzero for the most
vectors. in other words, the algorithm is e   cient only when the density of the coordinates
is low. in our own experiments of computing the semantic similarity between all pairs of
words in a large web crawl, we observed near linear average running time complexity in n.
the computational cost can be reduced further by leveraging the element weighting
techniques described in section 4.2. by setting to zero all coordinates that have a low
ppmi, pmi or tf-idf score, the coordinate density is dramatically reduced at the cost of
losing little discriminative power. in this vein, bayardo, ma, and srikant (2007) described
a strategy that omits the coordinates with the highest number of nonzero values. their
algorithm gives a signi   cant advantage only when we are interested in    nding solely the
similarity between highly similar vectors.

4.5.2 distributed implementation using mapreduce

the algorithm described in section 4.5.1 assumes that the matrix x can    t into memory,
which for large x may be impossible. also, as each element of x is processed indepen-
dently, running parallel processes for non-intersecting subsets of x makes the processing
faster. elsayed, lin, and oard (2008) proposed a mapreduce implementation deployed us-
ing hadoop, an open-source software package implementing the mapreduce framework and
distributed    le system.19 hadoop has been shown to scale to several thousands of machines,
allowing users to write simple code, and to seaid113ssly manage the sophisticated parallel ex-
ecution of the code. dean and ghemawat (2008) provide a good primer on mapreduce
programming.

19. hadoop is available for download at http://lucene.apache.org/hadoop/.

163

turney & pantel

the mapreduce model   s map step is used to start m    n map tasks in parallel, each
caching one m-th part of x as an inverted index and streaming one n-th part of x through
it. the actual inputs are read by the tasks directly from hdfs (hadoop distributed
file system). the value of m is determined by the amount of memory dedicated for the
inverted index, and n should be determined by trading o    the fact that, as n increases,
more parallelism can be obtained at the increased cost of building the same inverted index
n times.

the similarity algorithm from section 4.5.1 runs in each task of the map step of a

mapreduce job. the reduce step groups the output by the rows (or columns) of x.

4.5.3 randomized algorithms

other optimization strategies use randomized techniques to approximate various similar-
ity measures. the aim of randomized algorithms is to improve computational e   ciency
(memory and time) by projecting high-dimensional vectors into a low-dimensional subspace.
truncated svd performs such a projection, but svd can be computationally intensive.20
the insight of randomized techniques is that high-dimensional vectors can be randomly pro-
jected into a low-dimensional subspace with relatively little impact on the    nal similarity
scores. signi   cant reductions in computational cost have been reported with little aver-
age error to computing the true similarity scores, especially in applications such as word
similarity where we are interested in only the top-k most similar vectors to each vector
(ravichandran, pantel, & hovy, 2005; gorman & curran, 2006).

random indexing, an approximation technique based on sparse distributed memory
(kanerva, 1993), computes the pairwise similarity between all rows (or vectors) of a matrix
with complexity o(nrnc  1), where   1 is a    xed constant representing the length of the index
vectors assigned to each column. the value of   1 controls the tradeo    of accuracy versus
e   ciency. the elements of each index vector are mostly zeros with a small number of
randomly assigned +1   s and    1   s. the cosine measure between two rows r1 and r2 is then
approximated by computing the cosine between two    ngerprint vectors,    ngerprint(r1)
and    ngerprint(r2), where    ngerprint(r) is computed by summing the index vectors of
each non-unique coordinate of r. random indexing was shown to perform as well as lsa
on a word synonym selection task (karlgren & sahlgren, 2001).

locality sensitive hashing (lsh) (broder, 1997) is another technique that approximates
the similarity matrix with complexity o(n2
r  2), where   2 is a constant number of random
projections, which controls the accuracy versus e   ciency tradeo   .21 lsh is a general class of
techniques for de   ning functions that map vectors (rows or columns) into short signatures or
   ngerprints, such that two similar vectors are likely to have similar    ngerprints. de   nitions
of lsh functions include the min-wise independent function, which preserves the jaccard
similarity between vectors (broder, 1997), and functions that preserve the cosine similarity
between vectors (charikar, 2002). on a word similarity task, ravichandran et al. (2005)
showed that, on average, over 80% of the top-10 similar words of random words are found
in the top-10 results using charikar   s functions, and that the average cosine error is 0.016

20. however, there are e   cient forms of svd (brand, 2006; gorrell, 2006).
21. lsh stems from work by rabin (1981), who proposed the use of hash functions from random irreducible
polynomials to create short    ngerprints of collections of documents. such techniques are useful for many
tasks, such as removing duplicate documents (deduping) in a web crawl.

164

from frequency to meaning

(using   2 = 10,000 random projections). gorman and curran (2006) provide a detailed
comparison of random indexing and lsh on a distributional similarity task. on the bnc
corpus, lsh outperformed random indexing; however, on a larger corpora combining bnc,
the reuters corpus, and most of the english news holdings of the ldc in 2003, random
indexing outperformed lsh in both e   ciency and accuracy.

4.6 machine learning

if the intended application for a vsm is id91 or classi   cation, a similarity measure
such as cosine (section 4.4) may be used. for classi   cation, a nearest-neighbour algorithm
can use cosine as a measure of nearness (dasarathy, 1991). for id91, a similarity-
based id91 algorithm can use cosine as a measure of similarity (jain, murty, & flynn,
1999). however, there are many machine learning algorithms that can work directly with
the vectors in a vsm, without requiring an external similarity measure, such as cosine.
in e   ect, such machine learning algorithms implicitly use their own internal approaches to
measuring similarity.

any machine learning algorithm that works with real-valued vectors can use vectors
from a vsm (witten & frank, 2005). linguistic processing (section 3) and mathematical
processing (section 4) may still be necessary, but the machine learning algorithm can handle
vector comparison (sections 4.4 and 4.5).

in addition to unsupervised (id91) and supervised (classi   cation) machine learn-
ing, vectors from a vsm may also be used in semi-supervised learning (ando & zhang,
2005; collobert & weston, 2008). in general, there is nothing unique to vsms that would
compel a choice of one machine learning algorithm over another, aside from the algorithm   s
performance on the given task. therefore we refer our readers to the machine learning
literature (witten & frank, 2005), since we have no advice that is speci   c to vsms.

5. three open source vsm systems

to illustrate the three types of vsms discussed in section 2, this section presents three open
source systems, one for each vsm type. we have chosen to present open source systems so
that interested readers can obtain the source code to    nd out more about the systems and
to apply the systems in their own projects. all three systems are written in java and are
designed for portability and ease of use.

5.1 the term   document matrix: lucene

lucene22 is an open source full-featured text search engine library supported by the apache
software foundation. it is arguably the most ubiquitous implementation of a term   document
matrix, powering many search engines such as at cnet, sourceforge, wikipedia, disney,
aol and comcast. lucene o   ers e   cient storage, indexing, as well as retrieval and ranking
functionalities. although it is primarily used as a term   document matrix, it generalizes to
other vsms.

22. apache lucene is available for download at http://lucene.apache.org/.

165

turney & pantel

content, such as webpages, pdf documents, images, and video, are programmatically
decomposed into    elds and stored in a database. the database implements the term   
document matrix, where content corresponds to documents and    elds correspond to terms.
fields are stored in the database and indices are computed on the    eld values. lucene
uses    elds as a generalization of content terms, allowing any other string or literal to index
documents. for example, a webpage could be indexed by all the terms it contains, and also
by the anchor texts pointing to it, its host name, and the semantic classes in which it is
classi   ed (e.g., spam, product review, news, etc.). the webpage can then be retrieved by
search terms matching any of these    elds.

columns in the term   document matrix consist of all the    elds of a particular instance
of content (e.g., a webpage). the rows consist of all instances of content in the index.
various statistics such as frequency and tf-idf are stored in the matrix. the developer
de   nes the    elds in a schema and identi   es those to be indexed by lucene. the developer
also optionally de   nes a content ranking function for each indexed    eld.

once the index is built, lucene o   ers functionalities for retrieving content. users can
issue many query types such as phrase queries, wildcard queries, proximity queries, range
queries (e.g., date range queries), and    eld-restricted queries. results can be sorted by any
   eld and index updates can occur simultaneously during searching. lucene   s index can be
directly loaded into a tomcat webserver and it o   ers apis for common programming lan-
guages. solr,23 a separate apache software foundation project, is an open source enterprise
webserver for searching a lucene index and presenting search results. it is a full-featured
webserver providing functionalities such as xml/http and json apis, hit highlighting,
faceted search, caching, and replication.

a simple recipe for creating a web search service, using nutch, lucene and solr, consists
of crawling a set of urls (using nutch), creating a term   document matrix index (using
lucene), and serving search results (using solr). nutch,24 the apache software foundation
open source web search software, o   ers functionality for id190 from a seed set
of urls, for building a link-graph of the web crawl, and for parsing web documents such
as html pages. a good set of seed urls for nutch can be downloaded freely from the
open directory project.25 crawled pages are html-parsed, and they are then indexed
by lucene. the resulting indexed collection is then queried and served through a solr
installation with tomcat.

for more information on lucene, we recommend gospodnetic and hatcher   s (2004)
book. konchady (2008) explains how to integrate lucene with lingpipe and gate for
sophisticated semantic processing.26

23. apache solr is available for download at http://lucene.apache.org/solr/.
24. apache nutch is available for download at http://lucene.apache.org/nutch/.
25. see http://www.dmoz.org/.
26. information about lingpipe is available at http://alias-i.com/lingpipe/. the gate (general architec-

ture for text engineering) home page is at http://gate.ac.uk/.

166

from frequency to meaning

5.2 the word   context matrix: semantic vectors

semantic vectors27 is an open source project implementing the random projection approach
to measuring word similarity (see section 4.5.3). the package uses lucene to create a term   
document matrix, and it then creates vectors from lucene   s term   document matrix, using
random projection for id84. the random projection vectors can be
used, for example, to measure the semantic similarity of two words or to    nd the words
that are most similar to a given word.

the idea of random projection is to take high-dimensional vectors and randomly project
them into a relatively low-dimensional space (sahlgren, 2005). this can be viewed as a
kind of smoothing operation (section 4.3), but the developers of the semantic vectors
package emphasize the simplicity and e   ciency of random projection (section 4.5), rather
than its smoothing ability. they argue that other matrix smoothing algorithms might
smooth better, but none of them perform as well as random indexing, in terms of the
computational complexity of building a smooth matrix and incrementally updating the
matrix when new data arrives (widdows & ferraro, 2008). their aim is to encourage
research and development with semantic vectors by creating a simple and e   cient open
source package.

the semantic vectors package is designed to be convenient to use, portable, and easy
to extend and modify. the design of the software incorporates lessons learned from the
earlier stanford infomap project.28 although the default is to generate random projection
vectors, the system has a modular design that allows other kinds of word   context matrices
to be used instead of random projection matrices.

the package supports two basic functions: building a word   context matrix and searching
through the vectors in the matrix.
in addition to generating word vectors, the building
operation can generate document vectors by calculating weighted sums of the word vectors
for the words in each document. the searching operation can be used to search for similar
words or to search for documents that are similar to a query. a query can be a single word or
several words can be combined, using various mathematical operations on the corresponding
vectors. the mathematical operations include vector negation and disjunction, based on
quantum logic (widdows, 2004). widdows and ferraro (2008) provide a good summary of
the semantic vectors software.

5.3 the pair   pattern matrix: latent relational analysis in s-space

latent relational analysis29 (lra) is an open source project implementing the pair   pattern
matrix.
it is a component of the s-space package, a library of tools for building and
comparing di   erent semantic spaces.

lra takes as input a textual corpus and a set of word pairs. a pair   pattern matrix is
built by deriving lexical patterns that link together the word pairs in the corpus. for exam-
ple, consider the word pair hkorea, japani and the following retrieved matching sentences:

27. semantic vectors is a software package for measuring word similarity, available under the simpli   ed bsd

license at http://code.google.com/p/semanticvectors/.

28. see http://infomap-nlp.sourceforge.net/.
29. latent relational analysis is part of the s-space package and is distributed under the gnu general
public license version 2. it is available at http://code.google.com/p/airhead-research/. at the time of
writing, the lra module was under development.

167

turney & pantel

    korea looks to new japan prime minister   s e   ect on korea-japan relations.
    what channel is the korea vs. japan football game?

from these two sentences, lra extracts two patterns:    x looks to new y     and    x vs. y    .
these patterns become two columns in the pair   pattern matrix, and the word pair hkorea,
japani becomes a row. pattern frequencies are counted and then smoothed using svd (see
section 4.3).
in order to mitigate the sparseness of occurrences of word pairs, a thesaurus such as
id138 is used to expand the seed word pairs to alternatives. for example the pair
hkorea, japani may be expanded to include hsouth korea, japani, hrepublic of korea,
japani, hkorea, nipponi, hsouth korea, nipponi, and hrepublic of korea, nipponi.
lra uses lucene (see section 5.1) as its backend to store the matrix, index it, and serve
its contents. for a detailed description of the lra algorithm, we suggest turney   s (2006)
paper.

6. applications

in this section, we will survey some of the semantic applications for vsms. we will aim for
breadth, rather than depth; readers who want more depth should consult the references. our
goal is to give the reader an impression of the scope and    exibility of vsms for semantics.
the following applications are grouped according to the type of matrix involved: term   
document, word   context, or pair   pattern. note that this section is not exhaustive; there
are many more references and applications than we have space to list here.

6.1 term   document matrices

term   document matrices are most suited to measuring the semantic similarity of documents
and queries (see section 2.1). the usual measure of similarity is the cosine of column vectors
in a weighted term   document matrix. there are a variety of applications for measures of
document similarity.

document retrieval: the term   document matrix was    rst developed for document
retrieval (salton et al., 1975), and there is now a large body of literature on the vsm
for document retrieval (manning et al., 2008), including several journals and conferences
devoted to the topic. the core idea is, given a query, rank the documents in order of
decreasing cosine of the angles between the query vector and the document vectors (salton
et al., 1975). one variation on the theme is cross-lingual document retrieval, where a
query in one language is used to retrieve a document in another language (landauer &
littman, 1990). an important technical advance was the discovery that smoothing the
term   document matrix by truncated svd can improve precision and recall (deerwester
et al., 1990), although few commercial systems use smoothing, due to the computational
expense when the document collection is large and dynamic. random indexing (sahlgren,
2005) or incremental svd (brand, 2006) may help to address these scaling issues. another
important development in document retrieval has been the addition of collaborative    ltering,
in the form of id95 (brin & page, 1998).

document id91: given a measure of document similarity, we can cluster the
documents into groups, such that similarity tends to be high within a group, but low across

168

from frequency to meaning

groups (manning et al., 2008). the clusters may be partitional (   at) (cutting, karger,
pedersen, & tukey, 1992; pantel & lin, 2002b) or they may have a hierarchical structure
(groups of groups) (zhao & karypis, 2002); they may be non-overlapping (hard) (croft,
1977) or overlapping (soft) (zamir & etzioni, 1999). id91 algorithms also di   er in how
clusters are compared and abstracted. with single-link id91, the similarity between
two clusters is the maximum of the similarities between their members. complete-link
id91 uses the minimum of the similarities and average-link id91 uses the average
of the similarities (manning et al., 2008).

document classi   cation: given a training set of documents with class labels and a
testing set of unlabeled documents, the task of document classi   cation is to learn from the
training set how to assign labels to the testing set (manning et al., 2008). the labels may
be the topics of the documents (sebastiani, 2002), the sentiment of the documents (e.g.,
positive versus negative product reviews) (pang, lee, & vaithyanathan, 2002; kim, pantel,
chklovski, & pennacchiotti, 2006), spam versus non-spam (sahami, dumais, heckerman, &
horvitz, 1998; pantel & lin, 1998), or any other labels that might be inferred from the words
in the documents. when we classify documents, we are implying that the documents in a
class are similar in some way; thus document classi   cation implies some notion of document
similarity, and most machine learning approaches to document classi   cation involve a term   
document matrix (sebastiani, 2002). a measure of document similarity, such as cosine, can
be directly applied to document classi   cation by using a nearest-neighbour algorithm (yang,
1999).

essay grading: student essays may be automatically graded by comparing them to
one or more high-quality reference essays on the given essay topic (wolfe, schreiner, rehder,
laham, foltz, kintsch, & landauer, 1998; foltz, laham, & landauer, 1999). the student
essays and the reference essays can be compared by their cosines in a term   document matrix.
the grade that is assigned to a student essay is proportional to its similarity to one of the
reference essays; a student essay that is highly similar to a reference essay gets a high grade.
document segmentation: the task of document segmentation is to partition a doc-
ument into sections, where each section focuses on a di   erent subtopic of the document
(hearst, 1997; choi, 2000). we may treat the document as a series of blocks, where a block
is a sentence or a paragraph. the problem is to detect a topic shift from one block to the
next. hearst (1997) and choi (2000) both use the cosine between columns in a word   block
frequency matrix to measure the semantic similarity of blocks. a topic shift is signaled by
a drop in the cosine between consecutive blocks. the word   block matrix can be viewed as
a small term   document matrix, where the corpus is a single document and the documents
are blocks.

id53: given a simple question, the task in id53 (qa)
is to    nd a short answer to the question by searching in a large corpus. a typical ques-
tion is,    how many calories are there in a big mac?    most algorithms for qa have four
components, question analysis, document retrieval, passage retrieval, and answer extraction
(tellex, katz, lin, fern, & marton, 2003; dang, lin, & kelly, 2006). vector-based similar-
ity measurements are often used for both document retrieval and passage retrieval (tellex
et al., 2003).

call routing: chu-carroll and carpenter (1999) present a vector-based system for
automatically routing telephone calls, based on the caller   s spoken answer to the question,

169

turney & pantel

   how may i direct your call?    if the caller   s answer is ambiguous, the system automatically
generates a question for the caller, derived from the vsm, that prompts the caller for further
information.

6.2 word   context matrices

word   context matrices are most suited to measuring the semantic similarity of words (see
section 2.2). for example, we can measure the similarity of two words by the cosine of the
angle between their corresponding row vectors in a word   context matrix. there are many
applications for measures of word similarity.

word similarity: deerwester et al. (1990) discovered that we can measure word simi-
larity by comparing row vectors in a term   document matrix. landauer and dumais (1997)
evaluated this approach with 80 multiple-choice synonym questions from the test of en-
glish as a foreign language (toefl), achieving human-level performance (64.4% correct
for the word   context matrix and 64.5% for the average non-english us college applicant).
the documents used by landauer and dumais had an average length of 151 words, which
seems short for a document, but long for the context of a word. other researchers soon
switched to much shorter lengths, which is why we prefer to call these word   context matri-
ces, instead of term   document matrices. lund and burgess (1996) used a context window
of ten words. sch  utze (1998) used a    fty-word window (  25 words, centered on the target
word). rapp (2003) achieved 92.5% correct on the 80 toefl questions, using a four-word
context window (  2 words, centered on the target word, after removing stop words). the
toefl results suggest that performance improves as the context window shrinks. it seems
that the immediate context of a word is much more important than the distant context for
determining the meaning of the word.

word id91: pereira, tishby, and lee (1993) applied soft hierarchical id91
to row-vectors in a word   context matrix. in one experiment, the words were nouns and the
contexts were verbs for which the given nouns were direct objects. in another experiment,
the words were verbs and the contexts were nouns that were direct objects of the given
verbs. sch  utze   s (1998) seminal word sense discrimination model used hard    at id91
for row-vectors in a word   context matrix, where the context was given by a window of   25
words, centered on the target word. pantel and lin (2002a) applied soft    at id91 to
a word   context matrix, where the context was based on parsed text. these algorithms are
able to discover di   erent senses of polysemous words, generating di   erent clusters for each
sense. in e   ect, the di   erent clusters correspond to the di   erent concepts that underlie the
words.

word classi   cation: turney and littman (2003) used a word   context matrix to clas-
sify words as positive (honest, intrepid) or negative (disturbing, super   uous). they used the
general inquirer (gi) lexicon (stone, dunphy, smith, & ogilvie, 1966) to evaluate their
algorithms. the gi lexicon includes 11,788 words, labeled with 182 categories related to
opinion, a   ect, and attitude.30 turney and littman hypothesize that all 182 categories can
be discriminated with a word   context matrix.

automatic thesaurus generation: id138 is a popular tool for research in natural
language processing (fellbaum, 1998), but creating and maintaing such lexical resources

30. the gi lexicon is available at http://www.wjh.harvard.edu/   inquirer/spreadsheet guide.htm.

170

from frequency to meaning

is labour intensive, so it is natural to wonder whether the process can be automated to
some degree.31 this task can seen as an instance of word id91 (when the thesaurus
is generated from scratch) or classi   cation (when an existing thesaurus is automatically
extended), but it is worthwhile to consider the task of automatic thesaurus generation
separately from id91 and classi   cation, due to the speci   c requirements of a thesaurus,
such as the particular kind of similarity that is appropriate for a thesaurus (see section 2.4).
several researchers have used word   context matrices speci   cally for the task of assisting or
automating thesaurus generation (crouch, 1988; grefenstette, 1994; ruge, 1997; pantel &
lin, 2002a; curran & moens, 2002).

id51: a typical id51 (wsd) system
(agirre & edmonds, 2006; pedersen, 2006) uses a feature vector representation in which
each vector corresponds to a token of a word, not a type (see section 2.6). however, leacock,
towell, and voorhees (1993) used a word   context frequency matrix for wsd, in which each
vector corresponds to a type annotated with a sense tag. yuret and yatbaz (2009) applied
a word   context frequency matrix to unsupervised wsd, achieving results comparable to
the performance of supervised wsd systems.

context-sensitive id147: people frequently confuse certain sets of
words, such as there, they   re, and their. these confusions cannot be detected by a sim-
ple dictionary-based spelling checker; they require context-sensitive id147. a
word   context frequency matrix may be used to correct these kinds of spelling errors (jones
& martin, 1997).

id14: the task of id14 is to label parts of a sen-
tence according to the roles they play in the sentence, usually in terms of their connection
to the main verb of the sentence. erk (2007) presented a system in which a word   context
frequency matrix was used to improve the performance of id14. pennac-
chiotti, cao, basili, croce, and roth (2008) show that word   context matrices can reliably
predict the semantic frame to which an unknown lexical unit refers, with good levels of
accuracy. such lexical unit induction is important in id14, to narrow the
candidate set of roles of any observed lexical unit.

id183: queries submitted to search engines such as google and yahoo!
often do not directly match the terms in the most relevant documents. to alleviate this
problem, the process of id183 is used for generating new search terms that are
consistent with the intent of the original query. vsms form the basis of query semantics
models (cao, jiang, pei, he, liao, chen, & li, 2008). some methods represent queries
by using session contexts, such as query cooccurrences in user sessions (huang, chien, &
oyang, 2003; jones, rey, madani, & greiner, 2006), and others use click contexts, such as
the urls that were clicked on as a result of a query (wen, nie, & zhang, 2001).

textual advertising: in pay-per-click advertising models, prevalent in search engines
such as google and yahoo!, users pay for keywords, called bidterms, which are then used to
display their ads when relevant queries are issued by users. the scarcity of data makes ad
matching di   cult and, in response, several techniques for bidterm expansion using vsms
have been proposed. the word   context matrix consists of rows of bidterms and the columns

31. id138 is available at http://id138.princeton.edu/.

171

turney & pantel

(contexts) consist of advertiser identi   ers (gleich & zhukov, 2004) or co-bidded bidterms
(second order co-occurrences) (chang, pantel, popescu, & gabrilovich, 2009).

information extraction: the    eld of information extraction (ie) includes named
entity recognition (ner: recognizing that a chunk of text is the name of an entity, such as
a person or a place), id36, event extraction, and fact extraction. pa  sca et
al. (2006) demonstrate that a word   context frequency matrix can facilitate fact extraction.
vyas and pantel (2009) propose a semi-supervised model using a word   context matrix for
building and iteratively re   ning arbitrary classes of named entities.

6.3 pair   pattern matrices

pair   pattern matrices are most suited to measuring the semantic similarity of word pairs
and patterns (see section 2.3). for example, we can measure the similarity of two word
pairs by the cosine of the angle between their corresponding row vectors in a pair   pattern
matrix. there are many applications for measures of relational similarity.

relational similarity: just as we can measure attributional similarity by the cosine
of the angle between row vectors in a word   context matrix, we can measure relational
similarity by the cosine of the angle between rows in a pair   pattern matrix. this approach
to measuring relational similarity was introduced by turney et al. (2003) and examined
in more detail by turney and littman (2005). turney (2006) evaluated this approach
to relational similarity with 374 multiple-choice analogy questions from the sat college
entrance test, achieving human-level performance (56% correct for the pair   pattern matrix
and 57% correct for the average us college applicant). this is the highest performance so
far for an algorithm. the best algorithm based on attributional similarity has an accuracy
of only 35% (turney, 2006). the best non-vsm algorithm achieves 43% (veale, 2004).

pattern similarity: instead of measuring the similarity between row vectors in a pair   
pattern matrix, we can measure the similarity between columns; that is, we can measure
pattern similarity. lin and pantel (2001) constructed a pair   pattern matrix in which the
patterns were derived from parsed text. pattern similarity can be used to infer that one
phrase is a paraphrase of another phrase, which is useful for id86,
text summarization, information retrieval, and id53.

relational id91: bi  cici and yuret (2006) clustered word pairs by representing
them as row vectors in a pair   pattern matrix. davidov and rappoport (2008)    rst clustered
contexts (patterns) and then identi   ed representative pairs for each context cluster. they
used the representative pairs to automatically generate multiple-choice analogy questions,
in the style of sat analogy questions.

relational classi   cation: chklovski and pantel (2004) used a pair   pattern matrix
to classify pairs of verbs into semantic classes. for example, taint : poison is classi   ed as
strength (poisoning is stronger than tainting) and assess : review is classi   ed as enablement
(assessing is enabled by reviewing). turney (2005) used a pair   pattern matrix to classify
noun compounds into semantic classes. for example,    u virus is classi   ed as cause (the
virus causes the    u), home town is classi   ed as location (the home is located in the town),
and weather report is classi   ed as topic (the topic of the report is the weather).

relational search: cafarella, banko, and etzioni (2006) described relational search
as the task of searching for entities that satisfy given semantic relations. an example of

172

from frequency to meaning

a query for a relational search engine is    list all x such that x causes cancer   .
in this
example, the relation, cause, and one of the terms in the relation, cancer, are given by the
user, and the task of the search engine is to    nd terms that satisfy the user   s query. the
organizers of task 4 in semeval 2007 (girju, nakov, nastase, szpakowicz, turney, & yuret,
2007) envisioned a two-step approach to relational search:    rst a conventional search engine
would look for candidate answers, then a relational classi   cation system would    lter out
incorrect answers. the    rst step was manually simulated by the task 4 organizers and the
goal of task 4 was to design systems for the second step. this task attracted 14 teams who
submitted 15 systems. nakov and hearst (2007) achieved good results using a pair   pattern
matrix.

automatic thesaurus generation: we discussed automatic thesaurus generation in
section 6.2, with word   context matrices, but arguably relational similarity is more relevant
than attributional similarity for thesaurus generation. for example, most of the informa-
tion in id138 is in the relations between the words rather than in the words individually.
snow, jurafsky, and ng (2006) used a pair   pattern matrix to build a hypernym-hyponym
taxonomy, whereas pennacchiotti and pantel (2006) built a meronymy and causation tax-
onomy. turney (2008b) showed how a pair   pattern matrix can distinguish synonyms from
antonyms, synonyms from non-synonyms, and taxonomically similar words (hair and fur)
from words that are merely semantically associated (cradle and baby).

analogical mapping: proportional analogies have the form a : b :: c : d, which means    a
is to b as c is to d   . for example, mason : stone :: carpenter : wood means    mason is to stone
as carpenter is to wood   . the 374 multiple-choice analogy questions from the sat college
entrance test (mentioned above) all involve proportional analogies. with a pair   pattern
matrix, we can solve proportional analogies by selecting the choice that maximizes relational
similarity (e.g., simr(mason : stone, carpenter : wood) has a high value). however, we often
encounter analogies that involve more than four terms. the well-known analogy between
the solar system and the rutherford-bohr model of the atom contains at least fourteen
terms. for the solar system, we have planet, attracts, revolves, sun, gravity, solar system,
and mass. for the atom, we have revolves, atom, attracts, electromagnetism, nucleus,
charge, and electron. turney (2008a) demonstrated that we can handle these more complex,
systematic analogies by decomposing them into sets of proportional analogies.

7. alternative approaches to semantics

the applications that we list in section 6 do not necessarily require a vsm approach. for
each application, there are many other possible approaches.
in this section, we brie   y
consider a few of the main alternatives.

underlying the applications for term   document matrices (section 6.1) is the task of
measuring the semantic similarity of documents and queries. the main alternatives to
vsms for this task are probabilistic models, such as the traditional probabilistic retrieval
models in information retrieval (van rijsbergen, 1979; baeza-yates & ribeiro-neto, 1999)
and the more recent statistical language models inspired by id205 (liu &
croft, 2005). the idea of statistical language models for information retrieval is to measure
the similarity between a query and a document by creating a probabilistic language model

173

turney & pantel

of the given document and then measuring the id203 of the given query according to
the language model.

with progress in information retrieval, the distinction between the vsm approach and
the probabilistic approach is becoming blurred, as each approach borrows ideas from the
other. language models typically involve multiplying probabilities, but we can view this as
adding logs of probabilities, which makes some language models look similar to vsms.

the applications for word   context matrices (section 6.2) share the task of measuring the
semantic similarity of words. the main alternatives to vsms for measuring word similarity
are approaches that use lexicons, such as id138 (resnik, 1995; jiang & conrath, 1997;
hirst & st-onge, 1998; leacock & chodrow, 1998; budanitsky & hirst, 2001). the idea is
to view the lexicon as a graph, in which nodes correspond to word senses and edges represent
relations between words, such as hypernymy and hyponymy. the similarity between two
words is then proportional to the length of the path in the graph that joins the two words.
several approaches to measuring the semantic similarity of words combine a vsm with
a lexicon (turney et al., 2003; pantel, 2005; patwardhan & pedersen, 2006; mohammad &
hirst, 2006). humans use both dictionary de   nitions and observations of word usage, so it
is natural to expect the best performance from algorithms that use both distributional and
lexical information.

pair   pattern matrices (section 6.3) have in common the task of measuring the semantic
similarity of relations. as with word   context matrices, the main alternatives are approaches
that use lexicons (rosario & hearst, 2001; rosario, hearst, & fillmore, 2002; nastase
& szpakowicz, 2003; veale, 2003, 2004). the idea is to reduce relational similarity to
attributional similarity, simr(a : b, c : d)     sima(a, c) + sima(b, d), and then use a lexicon
to measure attributional similarity. as we discuss in section 2.4, this reduction does not
work in general. however, the reduction is often a good approximation, and there is some
evidence that a hybrid approach, combining a vsm with a lexicon, can be bene   cial (turney
et al., 2003; nastase, sayyad-shirabad, sokolova, & szpakowicz, 2006).

8. the future of vector space models of semantics

several authors have criticized vsms (french & labiouse, 2002; pad  o & lapata, 2003;
morris & hirst, 2004; budanitsky & hirst, 2006). most of the criticism stems from the
fact that term   document and word   context matrices typically ignore word order. in lsa,
for instance, a phrase is commonly represented by the sum of the vectors for the individual
words in the phrase; hence the phrases house boat and boat house will be represented by
the same vector, although they have di   erent meanings. in english, word order expresses
relational information. both house boat and boat house have a tool-purpose relation, but
house boat means tool-purpose(boat, house) (a boat that serves as a house), whereas boat
house means tool-purpose(house, boat) (a house for sheltering and storing boats).

landauer (2002) estimates that 80% of the meaning of english text comes from word
choice and the remaining 20% comes from word order. however, vsms are not inherently
limited to 80% of the meaning of text. mitchell and lapata (2008) propose composition
models sensitive to word order. for example, to make a simple additive model become
syntax-aware, they allow for di   erent weightings of the contributions of the vector compo-
nents. constituents that are more important to the composition therefore can participate

174

from frequency to meaning

more actively. clark and pulman (2007) assigned distributional meaning to sentences us-
ing the hilbert space tensor product. widdows and ferraro (2008), inspired by quantum
mechanics, explores several operators for modeling composition of meaning. pair   pattern
matrices are sensitive to the order of the words in a pair (turney, 2006). thus there are
several ways to handle word order in vsms.

this raises the question, what are the limits of vsms for semantics? can all semantics
be represented with vsms? there is much that we do not yet know how to represent with
vsms. for example, widdows (2004) and van rijsbergen (2004) show how disjunction,
conjunction, and negation can be represented with vectors, but we do not yet know how to
represent arbitrary statements in    rst-order predicate calculus. however, it seems possible
that future work may discover answers to these limitations.

in this survey, we have assumed that vsms are composed of elements with values that
are derived from event frequencies. this ties vsms to some form of distributional hypothesis
(see sections 1.1 and 2.7); therefore the limits of vsms depend on the limits of the family
of distributional hypotheses. are statistical patterns of word usage su   cient to    gure out
what people mean? this is arguably the major open question of vsms, and the answer will
determine the future of vsms. we do not have a strong argument one way or the other,
but we believe that the continuing progress with vsms suggests we are far from reaching
their limits.

9. conclusions

when we want information or help from a person, we use words to make a request or
describe a problem, and the person replies with words. unfortunately, computers do not
understand human language, so we are forced to use arti   cial languages and unnatural user
interfaces. in science    ction, we dream of computers that understand human language, that
can listen to us and talk with us. to achieve the full potential of computers, we must enable
them to understand the semantics of natural language. vsms are likely to be part of the
solution to the problem of computing semantics.

many researchers who have struggled with the problem of semantics have come to the
conclusion that the meaning of words is closely connected to the statistics of word usage
(section 2.7). when we try to make this intuition precise, we soon    nd we are working with
vectors of values derived from event frequencies; that is, we are dealing with vsms.

in this survey, we have organized past work with vsms according to the structure of
the matrix (term   document, word   context, or pair   pattern). we believe that the structure
of the matrix is the most important factor in determining the types of applications that
are possible. the linguistic processing (section 3) and mathematical processing (section 4)
play smaller (but important) roles.

our goal in this survey has been to show the breadth and power of vsms, to introduce
vsms to those who less familiar with them, and to provide a new perspective on vsms to
those who are already familiar with them. we hope that our emphasis on the structure of
the matrix will inspire new research. there is no reason to believe that the three matrix
types we present here exhaust the possibilities. we expect new matrix types and new tensors
will open up more applications for vsms. it seems possible to us that all of the semantics
of human language might one day be captured in some kind of vsm.

175

turney & pantel

acknowledgments

thanks to annie zaenen for prompting this paper. thanks to saif mohammad and mariana
so   er for their comments. thanks to arkady borkovsky and eric crestan for developing
the distributed sparse-id127 algorithm, and to marco pennacchiotti for his
invaluable comments. thanks to the anonymous reviewers of jair for their very helpful
comments and suggestions.

references

acar, e., & yener, b. (2009). unsupervised multiway data analysis: a literature survey.

ieee transactions on knowledge and data engineering, 21 (1), 6   20.

agirre, e., & edmonds, p. g. (2006). id51: algorithms and appli-

cations. springer.

ando, r. k. (2000). latent semantic space: iterative scaling improves precision of inter-
document similarity measurement. in proceedings of the 23rd annual acm sigir
conference on research and development in information retrieval (sigir-2000), pp.
216   223.

ando, r. k., & zhang, t. (2005). a framework for learning predictive structures from
multiple tasks and unlabeled data. journal of machine learning research, 6, 1817   
1853.

baeza-yates, r., & ribeiro-neto, b. (1999). modern information retrieval. addison wesley.

barr, c., jones, r., & regelson, m. (2008). the linguistic structure of english web-
search queries. in conference on empirical methods in natural language processing
(emnlp).

bayardo, r. j., ma, y., & srikant, r. (2007). scaling up all pairs similarity search.

in
proceedings of the 16th international conference on world wide web (www    07),
pp. 131   140, new york, ny. acm.

bi  cici, e., & yuret, d. (2006). id91 word pairs to answer analogy questions.

in
proceedings of the fifteenth turkish symposium on arti   cial intelligence and neural
networks (tainn 2006), akyaka, mugla, turkey.

blei, d. m., ng, a. y., & jordan, m. i. (2003). id44. journal of

machine learning research, 3, 993   1022.

brand, m. (2006). fast low-rank modi   cations of the thin singular value decomposition.

id202 and its applications, 415 (1), 20   30.

breese, j., heckerman, d., & kadie, c. (1998). empirical analysis of predictive algorithms
for collaborative    ltering. in proceedings of the 14th conference on uncertainty in
arti   cial intelligence, pp. 43   52. morgan kaufmann.

brin, s., & page, l. (1998). the anatomy of a large-scale hypertextual web search engine.
in proceedings of the seventh world wide web conference (www7), pp. 107   117.

broder, a. (1997). on the resemblance and containment of documents. in in compression
and complexity of sequences (sequences   97), pp. 21   29. ieee computer society.

176

from frequency to meaning

budanitsky, a., & hirst, g. (2001). semantic distance in id138: an experimental,
application-oriented evaluation of    ve measures.
in proceedings of the workshop
on id138 and other lexical resources, second meeting of the north american
chapter of the association for computational linguistics (naacl-2001), pp. 29   24,
pittsburgh, pa.

budanitsky, a., & hirst, g. (2006). evaluating id138-based measures of semantic dis-

tance. computational linguistics, 32 (1), 13   47.

bullinaria, j., & levy, j. (2007). extracting semantic representations from word co-
occurrence statistics: a computational study. behavior research methods, 39 (3),
510   526.

buntine, w., & jakulin, a. (2006). discrete component analysis.

in subspace, latent
structure and feature selection: statistical and optimization perspectives workshop
at slsfs 2005, pp. 1   33, bohinj, slovenia. springer.

cafarella, m. j., banko, m., & etzioni, o. (2006). relational web search. tech. rep., uni-
versity of washington, department of computer science and engineering. technical
report 2006-04-02.

cao, g., nie, j.-y., & bai, j. (2005). integrating word relationships into language models.
in proceedings of the 28th annual international acm sigir conference on research
and development in information retrieval (sigir    05), pp. 298   305, new york, ny.
acm.

cao, h., jiang, d., pei, j., he, q., liao, z., chen, e., & li, h. (2008). context-aware query
suggestion by mining click-through and session data. in proceeding of the 14th acm
sigkdd international conference on knowledge discovery and data mining (kdd
   08), pp. 875   883. acm.

carroll, j. d., & chang, j.-j. (1970). analysis of individual di   erences in multidimensional
scaling via an n-way generalization of    eckart-young    decomposition. psychometrika,
35 (3), 283   319.

chang, w., pantel, p., popescu, a.-m., & gabrilovich, e. (2009). towards intent-driven

bidterm suggestion. in proceedings of www-09 (short paper), madrid, spain.

charikar, m. s. (2002). similarity estimation techniques from rounding algorithms. in pro-
ceedings of the thiry-fourth annual acm symposium on theory of computing (stoc
   02), pp. 380   388. acm.

chew, p., bader, b., kolda, t., & abdelali, a. (2007). cross-language information re-
trieval using parafac2. in proceedings of the 13th acm sigkdd international
conference on knowledge discovery and data mining (kdd07), pp. 143   152. acm
press.

chiarello, c., burgess, c., richards, l., & pollock, a. (1990). semantic and associative
priming in the cerebral hemispheres: some words do, some words don   t . . . sometimes,
some places. brain and language, 38, 75   104.

chklovski, t., & pantel, p. (2004). verbocean: mining the web for    ne-grained semantic
verb relations. in proceedings of experimental methods in natural language process-
ing 2004 (emnlp-04), pp. 33   40, barcelona, spain.

177

turney & pantel

choi, f. y. y. (2000). advances in domain independent linear text segmentation.

in
proceedings of the 1st meeting of the north american chapter of the association for
computational linguistics, pp. 26   33.

chu-carroll, j., & carpenter, b. (1999). vector-based natural language call routing. com-

putational linguistics, 25 (3), 361   388.

church, k. (1995). one term or two?.

in proceedings of the 18th annual international
acm sigir conference on research and development in information retrieval, pp.
310   318.

church, k., & hanks, p. (1989). word association norms, mutual information, and lexicog-
raphy. in proceedings of the 27th annual conference of the association of computa-
tional linguistics, pp. 76   83, vancouver, british columbia.

clark, s., & pulman, s. (2007). combining symbolic and distributional models of meaning.

in proceedings of aaai spring symposium on quantum interaction, pp. 52   55.

collobert, r., & weston, j. (2008). a uni   ed architecture for natural language processing:
deep neural networks with multitask learning. in proceedings of the 25th international
conference on machine learning (icml-08), pp. 160   167.

croft, w. b. (1977). id91 large    les of documents using the single-link method.

journal of the american society for information science, 28 (6), 341   344.

crouch, c. j. (1988). a cluster-based approach to thesaurus construction. in proceedings
of the 11th annual international acm sigir conference, pp. 309   320, grenoble,
france.

curran, j. r., & moens, m. (2002). improvements in automatic thesaurus extraction. in
unsupervised lexical acquisition: proceedings of the workshop of the acl special
interest group on the lexicon (siglex), pp. 59   66, philadelphia, pa.

cutting, d. r., karger, d. r., pedersen, j. o., & tukey, j. w. (1992). scatter/gather: a
cluster-based approach to browsing large document collections. in proceedings of the
15th annual international acm sigir conference, pp. 318   329.

dagan, i., lee, l., & pereira, f. c. n. (1999). similarity-based models of word cooccurrence

probabilities. machine learning, 34 (1   3), 43   69.

dang, h. t., lin, j., & kelly, d. (2006). overview of the trec 2006 id53

track. in proceedings of the fifteenth text retrieval conference (trec 2006).

dasarathy, b. (1991). nearest neighbor (nn) norms: nn pattern classi   cation techniques.

ieee computer society press.

davidov, d., & rappoport, a. (2008). unsupervised discovery of generic relationships using
pattern clusters and its evaluation by automatically generated sat analogy questions.
in proceedings of the 46th annual meeting of the acl and hlt (acl-hlt-08), pp.
692   700, columbus, ohio.

dean, j., & ghemawat, s. (2008). mapreduce: simpli   ed data processing on large clusters.

communications of the acm, 51 (1), 107   113.

178

from frequency to meaning

deerwester, s. c., dumais, s. t., landauer, t. k., furnas, g. w., & harshman, r. a.
indexing by latent semantic analysis. journal of the american society for

(1990).
information science (jasis), 41 (6), 391   407.

elsayed, t., lin, j., & oard, d. (2008). pairwise document similarity in large collections
with mapreduce.
in proceedings of association for computational linguistics and
human language technology conference 2008 (acl-08: hlt), short papers, pp.
265   268, columbus, ohio. association for computational linguistics.

erk, k. (2007). a simple, similarity-based model for selectional preferences. in proceedings
of the 45th annual meeting of the association of computational linguistics, pp. 216   
223,, prague, czech republic.

erk, k., & pad  o, s. (2008). a structured vector space model for word meaning in context.
in proceedings of the 2008 conference on empirical methods in natural language
processing (emnlp-08), pp. 897   906, honolulu, hi.

fellbaum, c. (ed.). (1998). id138: an electronic lexical database. mit press.

firth, j. r. (1957). a synopsis of linguistic theory 1930   1955.

in studies in linguistic

analysis, pp. 1   32. blackwell, oxford.

foltz, p. w., laham, d., & landauer, t. k. (1999). the intelligent essay assessor: ap-
interactive multimedia electronic journal of

plications to educational technology.
computer-enhanced learning, 1 (2).

forman, g. (2003). an extensive empirical study of feature selection metrics for text clas-

si   cation. journal of machine learning research, 3, 1289   1305.

french, r. m., & labiouse, c. (2002). four problems with extracting human semantics from
in proceedings of the 24th annual conference of the cognitive

large text corpora.
science society.

furnas, g. w., landauer, t. k., gomez, l. m., & dumais, s. t. (1983). statistical se-
mantics: analysis of the potential performance of keyword information systems. bell
system technical journal, 62 (6), 1753   1806.

gentner, d. (1983). structure-mapping: a theoretical framework for analogy. cognitive

science, 7 (2), 155   170.

gilbert, j. r., moler, c., & schreiber, r. (1992). sparse matrices in matlab: design and
implementation. siam journal on matrix analysis and applications, 13 (1), 333   356.

girju, r., nakov, p., nastase, v., szpakowicz, s., turney, p., & yuret, d. (2007). semeval-
2007 task 04: classi   cation of semantic relations between nominals. in proceedings
of the fourth international workshop on semantic evaluations (semeval 2007), pp.
13   18, prague, czech republic.

gleich, d., & zhukov, l. (2004). svd based term suggestion and ranking system.

in
proceedings of the fourth ieee international conference on data mining (icdm
   04), pp. 391   394. ieee computer society.

golub, g. h., & van loan, c. f. (1996). matrix computations (third edition). johns

hopkins university press, baltimore, md.

179

turney & pantel

gorman, j., & curran, j. r. (2006). scaling distributional similarity to large corpora. in
proceedings of the 21st international conference on computational linguistics and
the 44th annual meeting of the association for computational linguistics (acl 2006),
pp. 361   368. association for computational linguistics.

gorrell, g. (2006). generalized hebbian algorithm for incremental singular value decom-
position in natural language processing. in proceedings of the 11th conference of the
european chapter of the association for computational linguistics (eacl-06), pp.
97   104.

gospodnetic, o., & hatcher, e. (2004). lucene in action. manning publications.

grefenstette, g. (1994). explorations in automatic thesaurus discovery. kluwer.

harris, z. (1954). distributional structure. word, 10 (23), 146   162.

harshman, r. (1970). foundations of the parafac procedure: models and conditions for an
   explanatory    multi-modal factor analysis. ucla working papers in id102, 16.

hearst, m. (1997). texttiling: segmenting text into multi-paragraph subtopic passages.

computational linguistics, 23 (1), 33   64.

hirst, g., & st-onge, d. (1998). lexical chains as representations of context for the detection
in fellbaum, c. (ed.), id138: an electronic

and correction of malapropisms.
lexical database, pp. 305   332. mit press.

hofmann, t. (1999). probabilistic id45. in proceedings of the 22nd
annual acm conference on research and development in information retrieval (si-
gir    99), pp. 50   57, berkeley, california.

huang, c.-k., chien, l.-f., & oyang, y.-j. (2003). relevant term suggestion in interactive
web search based on contextual information in query session logs. journal of the
american society for information science and technology, 54 (7), 638   649.

hull, d. (1996). id30 algorithms: a case study for detailed evaluation. journal of the

american society for information science, 47 (1), 70   84.

jain, a., murty, n., & flynn, p. (1999). data id91: a review. acm computing

surveys, 31 (3), 264   323.

jarmasz, m., & szpakowicz, s. (2003). roget   s thesaurus and semantic similarity.

in
proceedings of the international conference on recent advances in natural language
processing (ranlp-03), pp. 212   219, borovets, bulgaria.

jiang, j. j., & conrath, d. w. (1997). semantic similarity based on corpus statistics
and lexical taxonomy. in proceedings of the international conference on research in
computational linguistics (rocling x), pp. 19   33, tapei, taiwan.

johnson, h., & martin, j. (2003). unsupervised learning of morphology for english and

inuktitut. in proceedings of hlt-naacl 2003, pp. 43   45.

jones, m. p., & martin, j. h. (1997). contextual id147 using latent seman-
in proceedings of the fifth conference on applied natural language

tic analysis.
processing, pp. 166   173, washington, dc.

180

from frequency to meaning

jones, r., rey, b., madani, o., & greiner, w. (2006). generating query substitutions. in
proceedings of the 15th international conference on world wide web (www    06),
pp. 387   396, new york, ny. acm.

jones, w. p., & furnas, g. w. (1987). pictures of relevance: a geometric analysis of
similarity measures. journal of the american society for information science, 38 (6),
420   442.

kanerva, p. (1993). sparse distributed memory and related models. in hassoun, m. h.
(ed.), associative neural memories, pp. 50   76. oxford university press, new york,
ny.

karlgren, j., & sahlgren, m. (2001). from words to understanding. in uesaka, y., kanerva,
p., & asoh, h. (eds.), foundations of real-world intelligence, pp. 294   308. csli
publications.

kim, s.-m., pantel, p., chklovski, t., & pennacchiotti, m. (2006). automatically assessing
review helpfulness. in proceedings of the 2006 conference on empirical methods in
natural language processing, pp. 423   430.

kolda, t., & bader, b. (2009). tensor decompositions and applications. siam review,

51 (3), 455   500.

konchady, m. (2008). building search applications: lucene, lingpipe, and gate. mustru

publishing.

kraaij, w., & pohlmann, r. (1996). viewing id30 as recall enhancement. in proceed-

ings of the 19th annual international acm sigir conference, pp. 40   48.

lako   , g. (1987). women, fire, and dangerous things. university of chicago press,

chicago, il.

landauer, t. k. (2002). on the computational basis of learning and cognition: arguments
from lsa. in ross, b. h. (ed.), the psychology of learning and motivation: advances
in research and theory, vol. 41, pp. 43   84. academic press.

landauer, t. k., & dumais, s. t. (1997). a solution to plato   s problem: the latent se-
mantic analysis theory of the acquisition, induction, and representation of knowledge.
psychological review, 104 (2), 211   240.

landauer, t. k., & littman, m. l. (1990). fully automatic cross-language document
retrieval using id45. in proceedings of the sixth annual conference
of the uw centre for the new oxford english dictionary and text research, pp. 31   
38, waterloo, ontario.

landauer, t. k., mcnamara, d. s., dennis, s., & kintsch, w. (2007). handbook of latent

semantic analysis. lawrence erlbaum, mahwah, nj.

lavrenko, v., & croft, w. b. (2001). relevance based language models. in proceedings of
the 24th annual international acm sigir conference on research and development
in information retrieval (sigir    01), pp. 120   127, new york, ny. acm.

leacock, c., & chodrow, m. (1998). combining local context and id138 similarity for
in fellbaum, c. (ed.), id138: an electronic lexical

word sense identi   cation.
database. mit press.

181

turney & pantel

leacock, c., towell, g., & voorhees, e. (1993). corpus-based statistical sense resolution.
in proceedings of the arpa workshop on human language technology, pp. 260   265.

lee, d. d., & seung, h. s. (1999). learning the parts of objects by nonnegative matrix

factorization. nature, 401, 788   791.

lee, l. (1999). measures of distributional similarity.

in proceedings of the 37th annual

meeting of the association for computational linguistics, pp. 25   32.

lemaire, b., & denhi`ere, g. (2006). e   ects of high-order co-occurrences on word semantic

similarity. current psychology letters: behaviour, brain & cognition, 18 (1).

lin, d. (1998). automatic retrieval and id91 of similar words. in roceedings of the
17th international conference on computational linguistics, pp. 768   774. association
for computational linguistics.

lin, d., & pantel, p. (2001). dirt     discovery of id136 rules from text. in proceedings
of acm sigkdd conference on knowledge discovery and data mining 2001, pp.
323   328.

linden, g., smith, b., & york, j. (2003). amazon.com recommendations: item-to-item

collaborative    ltering. ieee internet computing, 76   80.

liu, x., & croft, w. b. (2005). statistical id38 for information retrieval.

annual review of information science and technology, 39, 3   28.

lovins, j. b. (1968). development of a id30 algorithm. mechanical translation and

computational linguistics, 11, 22   31.

lowe, w. (2001). towards a theory of semantic space. in proceedings of the twenty-   rst

annual conference of the cognitive science society, pp. 576   581.

lund, k., & burgess, c. (1996). producing high-dimensional semantic spaces from lexical
co-occurrence. behavior research methods, instruments, and computers, 28 (2), 203   
208.

lund, k., burgess, c., & atchley, r. a. (1995). semantic and associative priming in high-
in proceedings of the 17th annual conference of the

dimensional semantic space.
cognitive science society, pp. 660   665.

manning, c., & sch  utze, h. (1999). foundations of statistical natural language processing.

mit press, cambridge, ma.

manning, c. d., raghavan, p., & sch  utze, h. (2008). introduction to information retrieval.

cambridge university press, cambridge, uk.

miller, g., leacock, c., tengi, r., & bunker, r. (1993). a semantic concordance.

in
proceedings of the 3rd darpa workshop on human language technology, pp. 303   
308.

minnen, g., carroll, j., & pearce, d. (2001). applied morphological processing of english.

natural language engineering, 7 (3), 207   223.

mitchell, j., & lapata, m. (2008). vector-based models of semantic composition. in proceed-
ings of acl-08: hlt, pp. 236   244, columbus, ohio. association for computational
linguistics.

182

from frequency to meaning

mitchell, t. (1997). machine learning. mcgraw-hill, columbus, oh.

mohammad, s., & hirst, g. (2006). distributional measures of concept-distance: a task-
oriented evaluation. in proceedings of the conference on empirical methods in natural
language processing (emnlp-2006), pp. 35   43.

monay, f., & gatica-perez, d. (2003). on image auto-annotation with latent space models.
in proceedings of the eleventh acm international conference on multimedia, pp.
275   278.

morris, j., & hirst, g. (2004). non-classical lexical semantic relations. in workshop on

computational lexical semantics, hlt-naacl-04, boston, ma.

nakov, p., & hearst, m. (2007). ucb: system description for semeval task 4. in proceed-
ings of the fourth international workshop on semantic evaluations (semeval 2007),
pp. 366   369, prague, czech republic.

nakov, p., & hearst, m. (2008). solving relational similarity problems using theweb as a

corpus. in proceedings of acl-08: hlt, pp. 452   460, columbus, ohio.

nastase, v., sayyad-shirabad, j., sokolova, m., & szpakowicz, s. (2006). learning noun-
modi   er semantic relations with corpus-based and id138-based features. in pro-
ceedings of the 21st national conference on arti   cial intelligence (aaai-06), pp.
781   786.

nastase, v., & szpakowicz, s. (2003). exploring noun-modi   er semantic relations.

in
fifth international workshop on computational semantics (iwcs-5), pp. 285   301,
tilburg, the netherlands.

niwa, y., & nitta, y. (1994). co-occurrence vectors from corpora vs. distance vectors from
dictionaries. in proceedings of the 15th international conference on computational
linguistics, pp. 304   309, kyoto, japan.

nosofsky, r. (1986). attention, similarity, and the identi   cation-categorization relationship.

journal of experimental psychology: general, 115 (1), 39   57.

ogden, c. k. (1930). basic english: a general introduction with rules and grammar.

kegan paul, trench, trubner and co.

pad  o, s., & lapata, m. (2003). constructing semantic space models from parsed corpora.
in proceedings of the 41st annual meeting of the association for computational lin-
guistics, pp. 128   135, sapporo, japan.

pad  o, s., & lapata, m. (2007). dependency-based construction of semantic space models.

computational linguistics, 33 (2), 161   199.

pang, b., lee, l., & vaithyanathan, s. (2002). thumbs up? sentiment classi   cation using
machine learning techniques. in proceedings of the conference on empirical methods
in natural language processing (emnlp), pp. 79   86, philadelphia, pa.

pantel, p. (2005). inducing ontological co-occurrence vectors. in proceedings of association

for computational linguistics (acl-05), pp. 125   132.

pantel, p., & lin, d. (1998). spamcop: a spam classi   cation and organization program. in
learning for text categorization: papers from the aaai 1998 workshop, pp. 95   98.

183

turney & pantel

pantel, p., & lin, d. (2002a). discovering word senses from text. in proceedings of the
eighth acm sigkdd international conference on knowledge discovery and data
mining, pp. 613   619, edmonton, canada.

pantel, p., & lin, d. (2002b). document id91 with committees. in proceedings of the

25th annual international acm sigir conference, pp. 199   206.

pa  sca, m., lin, d., bigham, j., lifchits, a., & jain, a. (2006). names and similarities on
the web: fact extraction in the fast lane. in proceedings of the 21st international
conference on computational linguistics and 44th annual meeting of the acl, pp.
809   816, sydney, australia.

patwardhan, s., & pedersen, t. (2006). using id138-based context vectors to estimate
the semantic relatedness of concepts.
in proceedings of the workshop on making
sense of sense at the 11th conference of the european chapter of the association for
computational linguistics (eacl-2006), pp. 1   8.

pedersen, t. (2006). unsupervised corpus-based methods for wsd. in word sense dis-

ambiguation: algorithms and applications, pp. 133   166. springer.

pennacchiotti, m., cao, d. d., basili, r., croce, d., & roth, m. (2008). automatic induction
of framenet lexical units. in proceedings of the 2008 conference on empirical methods
in natural language processing (emnlp-08), pp. 457   465, honolulu, hawaii.

pennacchiotti, m., & pantel, p. (2006). ontologizing semantic relations. in proceedings of
the 21st international conference on computational linguistics and the 44th annual
meeting of the association for computational linguistics, pp. 793   800. association
for computational linguistics.

pereira, f., tishby, n., & lee, l. (1993). distributional id91 of english words. in
proceedings of the 31st annual meeting on association for computational linguistics,
pp. 183   190.

porter, m. (1980). an algorithm for su   x stripping. program, 14 (3), 130   137.

rabin, m. o. (1981). fingerprinting by random polynomials. tech. rep., center for research

in computing technology, harvard university. technical report tr-15-81.

rapp, r. (2003). word sense discovery based on sense descriptor dissimilarity. in proceed-

ings of the ninth machine translation summit, pp. 315   322.

ravichandran, d., pantel, p., & hovy, e. (2005). randomized algorithms and nlp: using
locality sensitive hash function for high speed noun id91. in proceedings of the
43rd annual meeting of the association for computational linguistics (acl    05), pp.
622   629, morristown, nj. association for computational linguistics.

resnick, p., iacovou, n., suchak, m., bergstrom, p., & riedl, j. (1994). grouplens: an open
architecture for collaborative    ltering of netnews. in proceedings of the acm 1994
conference on computer supported cooperative work, pp. 175   186. acm press.

resnik, p. (1995). using information content to evaluate semantic similarity in a taxonomy.
in proceedings of the 14th international joint conference on arti   cial intelligence
(ijcai-95), pp. 448   453, san mateo, ca. morgan kaufmann.

184

from frequency to meaning

rosario, b., & hearst, m. (2001). classifying the semantic relations in noun-compounds
in proceedings of the 2001 conference on

via a domain-speci   c lexical hierarchy.
empirical methods in natural language processing (emnlp-01), pp. 82   90.

rosario, b., hearst, m., & fillmore, c. (2002). the descent of hierarchy, and selection in
relational semantics. in proceedings of the 40th annual meeting of the association
for computational linguistics (acl-02), pp. 247   254.

rosch, e., & lloyd, b. (1978). cognition and categorization. lawrence erlbaum, hillsdale,

nj.

ruge, g. (1997). automatic detection of thesaurus relations for information retrieval ap-
plications. in freksa, c., jantzen, m., & valk, r. (eds.), foundations of computer
science, pp. 499   506. springer.

sahami, m., dumais, s., heckerman, d., & horvitz, e. (1998). a bayesian approach to
   ltering junk e-mail. in proceedings of the aaai-98 workshop on learning for text
categorization.

sahlgren, m. (2005). an introduction to random indexing. in proceedings of the methods
and applications of semantic indexing workshop at the 7th international conference
on terminology and knowledge engineering (tke), copenhagen, denmark.

sahlgren, m. (2006). the word-space model: using distributional analysis to represent syn-
tagmatic and paradigmatic relations between words in high-dimensional vector spaces.
ph.d. thesis, department of linguistics, stockholm university.

salton, g. (1971). the smart retrieval system: experiments in automatic document pro-

cessing. prentice-hall, upper saddle river, nj.

salton, g., & buckley, c. (1988). term-weighting approaches in automatic text retrieval.

information processing and management, 24 (5), 513   523.

salton, g., wong, a., & yang, c.-s. (1975). a vector space model for automatic indexing.

communications of the acm, 18 (11), 613   620.

sarawagi, s., & kirpal, a. (2004). e   cient set joins on similarity predicates. in proceed-
ings of the 2004 acm sigmod international conference on management of data
(sigmod    04), pp. 743   754, new york, ny. acm.

scholkopf, b., smola, a. j., & muller, k.-r. (1997). kernel principal component analysis. in
proceedings of the international conference on arti   cial neural networks (icann-
1997), pp. 583   588, berlin.

sch  utze, h. (1998). automatic word sense discrimination. computational linguistics, 24 (1),

97   124.

sch  utze, h., & pedersen, j. (1993). a vector model for syntagmatic and paradigmatic
relatedness. in making sense of words: proceedings of the conference, pp. 104   113,
oxford, england.

sebastiani, f. (2002). machine learning in automated text categorization. acm computing

surveys (csur), 34 (1), 1   47.

shannon, c. (1948). a mathematical theory of communication. bell system technical

journal, 27, 379   423, 623   656.

185

turney & pantel

singhal, a., salton, g., mitra, m., & buckley, c. (1996). document length id172.

information processing and management, 32 (5), 619   633.

smith, e., osherson, d., rips, l., & keane, m. (1988). combining prototypes: a selective

modi   cation model. cognitive science, 12 (4), 485   527.

snow, r., jurafsky, d., & ng, a. y. (2006). semantic taxonomy induction from heteroge-
nous evidence. in proceedings of the 21st international conference on computational
linguistics and the 44th annual meeting of the acl, pp. 801   808.

sp  arck jones, k. (1972). a statistical interpretation of term speci   city and its application

in retrieval. journal of documentation, 28 (1), 11   21.

spearman, c. (1904).    general intelligence,    objectively determined and measured. amer-

ican journal of psychology, 15, 201   293.

sproat, r., & emerson, t. (2003). the    rst international chinese id40 bake-
o   . in proceedings of the second sighan workshop on chinese language processing,
pp. 133   143, sapporo, japan.

stone, p. j., dunphy, d. c., smith, m. s., & ogilvie, d. m. (1966). the general inquirer:

a computer approach to content analysis. mit press, cambridge, ma.

tan, b., & peng, f. (2008). unsupervised query segmentation using generative language
models and wikipedia. in proceeding of the 17th international conference on world
wide web (www    08), pp. 347   356, new york, ny. acm.

tellex, s., katz, b., lin, j., fern, a., & marton, g. (2003). quantitative evaluation of
passage retrieval algorithms for id53. in proceedings of the 26th annual
international acm sigir conference on research and development in information
retrieval (sigir), pp. 41   47.

tucker, l. r. (1966). some mathematical notes on three-mode factor analysis. psychome-

trika, 31 (3), 279   311.

turney, p. d. (2001). mining the web for synonyms: pmi-ir versus lsa on toefl. in
proceedings of the twelfth european conference on machine learning (ecml-01),
pp. 491   502, freiburg, germany.

turney, p. d. (2005). measuring semantic similarity by latent relational analysis. in pro-
ceedings of the nineteenth international joint conference on arti   cial intelligence
(ijcai-05), pp. 1136   1141, edinburgh, scotland.

turney, p. d. (2006). similarity of semantic relations. computational linguistics, 32 (3),

379   416.

turney, p. d. (2007). empirical evaluation of four tensor decomposition algorithms. tech.
rep., institute for information technology, national research council of canada.
technical report erb-1152.

turney, p. d. (2008a). the latent relation mapping engine: algorithm and experiments.

journal of arti   cial intelligence research, 33, 615   655.

turney, p. d. (2008b). a uniform approach to analogies, synonyms, antonyms, and as-
in proceedings of the 22nd international conference on computational

sociations.
linguistics (coling 2008), pp. 905   912, manchester, uk.

186

from frequency to meaning

turney, p. d., & littman, m. l. (2003). measuring praise and criticism: id136 of
semantic orientation from association. acm transactions on information systems,
21 (4), 315   346.

turney, p. d., & littman, m. l. (2005). corpus-based learning of analogies and semantic

relations. machine learning, 60 (1   3), 251   278.

turney, p. d., littman, m. l., bigham, j., & shnayder, v. (2003). combining independent
modules to solve multiple-choice synonym and analogy problems. in proceedings of
the international conference on recent advances in natural language processing
(ranlp-03), pp. 482   489, borovets, bulgaria.

van de cruys, t. (2009). a non-negative tensor factorization model for selectional preference
induction. in proceedings of the workshop on geometric models for natural language
semantics (gems-09), pp. 83   90, athens, greece.

van rijsbergen, c. j. (2004). the geometry of information retrieval. cambridge university

press, cambridge, uk.

van rijsbergen, c. j. (1979). information retrieval. butterworths.

veale, t. (2003). the analogical thesaurus.

in proceedings of the 15th innovative ap-
plications of arti   cial intelligence conference (iaai 2003), pp. 137   142, acapulco,
mexico.

veale, t. (2004). id138 sits the sat: a knowledge-based approach to lexical analogy. in
proceedings of the 16th european conference on arti   cial intelligence (ecai 2004),
pp. 606   612, valencia, spain.

vozalis, e., & margaritis, k. (2003). analysis of recommender systems algorithms.

in
proceedings of the 6th hellenic european conference on computer mathematics and
its applications (hercma-2003), athens, greece.

vyas, v., & pantel, p. (2009). semi-automatic entity set re   nement.

in proceedings of

naacl-09, boulder, co.

weaver, w. (1955). translation. in locke, w., & booth, d. (eds.), machine translation

of languages: fourteen essays. mit press, cambridge, ma.

weeds, j., weir, d., & mccarthy, d. (2004). characterising measures of lexical distri-
butional similarity. in proceedings of the 20th international conference on compu-
tational linguistics (coling    04), pp. 1015   1021. association for computational
linguistics.

wei, x., peng, f., & dumoulin, b. (2008). analyzing web text association to disambiguate
abbreviation in queries. in proceedings of the 31st annual international acm sigir
conference on research and development in information retrieval (sigir    08), pp.
751   752, new york, ny. acm.

wen, j.-r., nie, j.-y., & zhang, h.-j. (2001). id91 user queries of a search engine. in
proceedings of the 10th international conference on world wide web (www    01),
pp. 162   168, new york, ny. acm.

widdows, d. (2004). geometry and meaning. center for the study of language and

information, stanford, ca.

187

turney & pantel

widdows, d., & ferraro, k. (2008). semantic vectors: a scalable open source package and
online technology management application. in proceedings of the sixth international
conference on language resources and evaluation (lrec 2008), pp. 1183   1190.

witten, i. h., & frank, e. (2005). data mining: practical machine learning tools and

techniques with java implementations. morgan kaufmann, san francisco.

wittgenstein, l. (1953). philosophical investigations. blackwell. translated by g.e.m.

anscombe.

wolfe, m. b. w., schreiner, m. e., rehder, b., laham, d., foltz, p. w., kintsch, w., &
landauer, t. k. (1998). learning from text: matching readers and texts by latent
semantic analysis. discourse processes, 25, 309   336.

yang, y. (1999). an evaluation of statistical approaches to text categorization. information

retrieval, 1 (1), 69   90.

yuret, d., & yatbaz, m. a. (2009). the id87 for unsupervised word sense

disambiguation. computational linguistics. under review.

zamir, o., & etzioni, o. (1999). grouper: a dynamic id91 interface to web search
results. computer networks: the international journal of computer and telecom-
munications networking, 31 (11), 1361   1374.

zhao, y., & karypis, g. (2002). evaluation of hierarchical id91 algorithms for docu-
ment datasets. in proceedings of the eleventh international conference on informa-
tion and knowledge management, pp. 515   524, mclean, virginia.

188

