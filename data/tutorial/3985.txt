18.657: mathematics of machine learning

lecturer: philippe rigollet
scribe: philippe rigollet

lecture

1
sep. 9, 2015

1. what is machine learning (in this course)?

this course focuses on statistical learning theory, which roughly means understanding the
amount of data required to achieve a certain prediction accuracy. to better understand
what this means, we    rst focus on stating some di   erences between statistics and machine
learning since the two    elds share common goals.

indeed, both seem to try to use data to improve decisions. while these    elds have evolved
in the same direction and currently share a lot of aspects, they were at the beginning quite
di   erent. statistics was around much before machine learning and statistics was already
a fully developed scienti   c discipline by 1920, most notably thanks to the contributions of
r. fisher, who popularized id113 (id113) as a systematic tool for
statistical id136. however, id113 requires essentially knowing the id203 distribution
from which the data is draw, up to some unknown parameter of interest. often, the unknown
parameter has a physical meaning and its estimation is key in better understanding some
phenomena. enabling id113 thus requires knowing a lot about the data generating process:
this is known as modeling. modeling can be driven by physics or prior knowledge of the
problem. in any case, it requires quite a bit of domain knowledge.

more recently (examples go back to the 1960   s) new types of datasets (demographics,
social, medical,. . . ) have become available. however, modeling the data that they contain
is much more hazardous since we do not understand very well the input/output process
thus requiring a distribution free approach. a typical example is image classi   cation where
the goal is to label an image simply from a digitalization of this image. understanding
what makes an image a cat or a dog for example is a very complicated process. however,
for the classi   cation task, one does not need to understand the labelling process but rather
to replicate it. in that sense, machine learning favors a blackbox approach (see figure 1).

input x

blackbox

output y

input x

y = f (x) +   

output y

figure 1: the machine learning blackbox (left) where the goal is to replicate input/output
pairs from past observations, versus the statistical approach that opens the blackbox and
models the relationship.

these di   erences between statistics and machine learning have receded over the last
couple of decades. indeed, on the one hand, statistics is more and more concerned with
   nite sample analysis, model misspeci   cation and computational considerations. on the
other hand, probabilistic modeling is now inherent to machine learning. at the intersection
of the two    elds, lies statistical learning theory, a    eld which is primarily concerned with
sample complexity questions, some of which will be the focus of this class.

12. statistical learning theory

2.1 binary classi   cation

a large part of this class will be devoted to one of the simplest problem of statistical learning
theory: binary classi   cation (aka pattern recognition [dgl96]). in this problem, we observe
(x1, y1), . . . , (xn, yn) that are n independent random copies of (x, y )     x    {0, 1}. denote
by px,y the joint distribution of (x, y ). the so-called feature x lives in some abstract
space x (think ird) and y     {0, 1} is called label. for example, x can be a collection of
gene expression levels measured on a patient and y indicates if this person su   ers from
obesity. the goal of binary classi   cation is to build a rule to predict y given x using
only the data at hand. such a rule is a function h : x     {0, 1} called a classi   er. some
classi   ers are better than others and we will favor ones that have low classi   cation error
r(h) = ip(h(x) = y ). let us make some important remarks.

fist of all, since y     {0, 1} then y has a bernoulli distribution: so much for distribution
free assumptions! however, we will not make assumptions on the marginal distribution of
x or, what matters for prediction, the conditional distribution of y given x. we write,
y |x     ber(  (x)), where   (x) = ip(y = 1|x) = ie[y |x] is called the regression function
of y onto x.

next, note that we did not write y =   (x). actually we have y =   (x) +   , where
   = y      (x) is a    noise    random variable that satis   es ie[  |x] = 0. in particular, this noise
accounts for the fact that x may not contain enough information to predict y perfectly.
this is clearly the case in our genomic example above:
it not whether there is even any
information about obesity contained in a patient   s genotype. the noise vanishes if and only
if   (x)     {0, 1} for all x     x . figure 2.1 illustrates the case where there is no noise and the
the more realistic case where there is noise. when   (x) is close to .5, there is essentially no
information about y in x as the y is determined essentially by a toss up. in this case, it
is clear that even with an in   nite amount of data to learn from, we cannot predict y well
since there is nothing to learn. we will see what the e   ect of the noise also appears in the
sample complexity.

  (x)

1

.5

x

figure 2: the thick black curve corresponds to the noiseless case where y =   (x)     {0, 1}
and the thin red curve corresponds to the more realistic case where        [0, 1]. in the latter
case, even full knowledge of    does not guarantee a perfect prediction of y .

in the presence of noise, since we cannot predict y accurately, we cannot drive the
classi   cation error r(h) to zero, regardless of what classi   er h we use. what is the smallest
value that can be achieved? as a thought experiment, assume to begin with that we have all

26
the information that we may ever hope to get, namely we know the regression function   (  ).
for a given x to classify, if   (x) = 1/2 we may just toss a coin to decide our prediction
and discard x since it contains no information about y . however, if   (x) = 1/2, we have
an edge over random guessing: if   (x) > 1/2, it means that ip(y = 1|x) > ip(y = 0|x)
or, in words, that 1 is more likely to be the correct label. we will see that the classi   er
h   (x) = 1i(  (x) > 1/2) (called bayes classi   er ) is actually the best possible classi   er in
the sense that

r(h   ) = inf r(h) ,

h(  )

where the in   mum is taken over all classi   ers, i.e. functions from x to {0, 1}. note that
unless   (x)     {0, 1} for all x     x (noiseless case), we have r(h   ) = 0. however, we can
always look at the excess risk e(h) of a classi   er h de   ned by

e(h) = r(h)     r(h   )     0 .

in particular, we can hope to drive the excess risk to zero with enough observations by
mimicking h    accurately.

2.2 empirical risk

the bayes classi   er h   , while optimal, presents a major drawback: we cannot compute it
because we do not know the regression function   .
instead, we have access to the data
(x1, y1), . . . , (xn, yn), which contains some (but not all) information about    and thus h   .
in order to mimic the properties of h    recall that it minimizes r(h) over all h. but the
function r(  ) is unknown since it depends on the unknown distribution px,y of (x, y ). we
estimate it by the empirical classi   cation error, or simply empirical risk rn(  ) de   ned for
any classi   er h by

  

  rn(h) = x 1i(h(xi) = yi) .

1
n

n

i=1

  

  

since ie[1i(h(xi) = yi)] = ip(h(xi) = yi) = r(h), we have ie[rn(h)] = r(h) so rn(h) is
an unbiased estimator of r(h). moreover, for any h, by the law of large numbers, we have
  
rn(h)     r(h) as n        , almost surely. this indicates that if n is large enough, rn(h)
should be close to r(h).

  

  

  

as a result, in order to mimic the performance of h   , let us use the empirical risk
minimizer (erm) h de   ned to minimize rn(h) over all classi   ers h. this is an easy enough
task: de   ne h such h(xi) = yi for all i = 1, . . . , n and h(x) = 0 if x    / {x1, . . . , xn}. we
have rn(h) = 0, which is clearly minimal. the problem with this classi   er is obvious: it
does not generalize outside the data. rather, it predicts the label 0 for any x that is not in
the data. we could have predicted 1 or any combination of 0 and 1 and still get rn(h) = 0.
in particular it is unlikely that ie[r(h)] will be small.

     

     

  

  

  

36
6
6
6
6
important remark: recall that r(h) = ip(h(x) 6= y ).

  

  

if h(  ) = h({(x1, y1), . . . , (xn, yn)} ;   ) is constructed from the data, r(h) denotes

  

the id155

  

  

r(h) = ip(h(x) 6= y |(x1, y1), . . . , (xn, yn)) .

  

rather than ip(h(x) 6= y ). as a result r(h) is a random variable since it depends on the
randomness of the data (x1, y1), . . . , (xn, yn). one way to view this is to observe that
  
we compute the deterministic function r(  ) and then plug in the random classi   er h.

  

this problem is inherent to any method if we are not willing to make any assumption
on the distribution of (x, y ) (again, so much for distribution freeness!). this can actually
be formalized in theorems, known as no-free-lunch theorems.

theorem:
any    > 0, there exists a distribution px,y for (x, y ) such that r(h   ) = 0 and

for any integer n     1, any classi   er h built from (x1, y1), . . . , (xn, yn) and

  

ier(hn)     1/2        .

  

to be fair, note that here the distribution of the pair (x, y ) is allowed to depend on
n which is cheating a bit but there are weaker versions of the no-free-lunch theorem that
essentially imply that it is impossible to learn without further assumptions. one such
theorem is the following.

for any classi   er h built from (x1, y1), . . . , (xn, yn) and any sequence
theorem:
{an}n > 0 that converges to 0, there exists a distribution px,y for (x, y ) such that
r(h   ) = 0 and

  

ier(hn)     an ,

  

for all n     1

in the above theorem, the distribution of (x, y ) is allowed to depend on the whole sequence
{an}n > 0 but not on a speci   c n. the above result implies that the convergence to zero of
the classi   cation error may be arbitrarily slow.

2.3 generative vs discriminative approaches

both theorems above imply that we need to restrict the distribution px,y of (x, y ). but
isn   t that exactly what statistical modeling is? the is answer is not so clear depending on
how we perform this restriction. there are essentially two schools: generative which is the
statistical modeling approach and discriminative which is the machine learning approach.

generative: this approach consists in restricting the set of candidate distributions px,y .
this is what is done in discriminant analysis 1where it is assumed that the condition dis-

1amusingly, the generative approach is called discriminant analysis but don   t let the terminology fool

you.

4tributions of x given y (there are only two of them: one for y = 0 and one for y = 1) are
gaussians on x = ird (see for example [htf09] for an overview of this approach).

discriminative: this is the machine learning approach. rather than making assumptions
directly on the distribution, one makes assumptions on what classi   ers are likely to perform
correctly. in turn, this allows to eliminate classi   ers such as the one described above and
that does not generalize well.

while it is important to understand both, we will focus on the discriminative approach
in this class. speci   cally we are going to assume that we are given a class h of classi   ers
such that r(h) is small for some h     h.

2.4 estimation vs. approximation

assume that we are given a class h in which we expect to    nd a classi   er that performs well.
this class may be constructed from domain knowledge or simply computational convenience.
we will see some examples in the class. for any candidate classi   er hn built from the data,
we can decompose its excess risk as follows:

  

  

  

e(hn) = r(hn)     r(h   ) = r(hn)     inf r(h) + inf r(h)     r(h   ) .
|approxim{aztion error}

| estimat{iozn error }

h   h

h   h

  

on the one hand, estimation error accounts for the fact that we only have a    nite
amount of observations and thus a partial knowledge of the distribution px,y . hopefully
we can drive this error to zero as n        . but we already know from the no-free-lunch
theorem that this will not happen if h is the set of all classi   ers. therefore, we need to
take h small enough. on the other hand, if h is too small, it is unlikely that we will
   nd classi   er with performance close to that of h   . a tradeo    between estimation and
approximation can be made by letting h = hn grow (but not too fast) with n.

for now, assume that h is    xed. the goal of statistical learning theory is to understand
how the estimation error drops to zero as a function not only of n but also of h. for the
   rst argument, we will use concentration inequalities such as hoe   ding   s and bernstein   s
inequalities that allow us to control how close the empirical risk is to the classi   cation error
by bounding the random variable

n

1
n

(cid:12)
(cid:12) x 1i(h(x
(cid:12)

i=1

i) = yi)     ip (x) = y )(cid:12)
(cid:12)
(cid:12)

(h

with high id203. more generally we will be interested in results that allow to quantify
how close the average of independent and identically distributed (i.i.d) random variables is
to their common expected value.

indeed, since by de   nition, we have rn(h)     rn(h) for all h     h, the estimation error
can be controlled as follows. de   ne h     h to be any classi   er that minimizes r(  ) over h
(assuming that such a classi   er exist).

  

     

  

  
r(hn)     inf r(h) = r(hn)     r(h)

  

  

h   h

  

     

= rn(hn)     rn(h) +r(hn)     r  n(h ) + r  

     
|
rn(hn)     r(hn)(cid:12)(cid:12) + (cid:12)(cid:12)rn(h)     r(h)(cid:12)(cid:12) .
    (cid:12)(cid:12)      

     

   {z0

}

  

  

  

n

  
n(h)     r(h)

  

56
6
  

since h is deterministic, we can use a concentration inequality to control (cid:12)(cid:12)rn(h)     r(h)(cid:12)(cid:12).

     

  

however,

rn(hn) = x 1i(hn(xi) = yi)
     

  

1
n

n

i=1

is not
the average of independent random variables since hn depends in a complicated
manner on all of the pairs (xi, yi), i = 1, . . . , n. to overcome this limitation, we often use
a blunt, but surprisingly accurate tool: we    sup out    hn,

  

  

(cid:12)(cid:12)      
rn(hn)     r(hn)(cid:12)(cid:12)     sup (cid:12)

h    (cid:12)rn(hn)     r(hn)(cid:12)
(cid:12) .

     

  

  

h

controlling this supremum falls in the scope of suprema of empirical processes that we will
study in quite a bit of detail. clearly the supremum is smaller as h is smaller but h should
be kept large enough to have good approximation properties. this is the tradeo    between
approximation and estimation. it is also know in statistics as the bias-variance tradeo   .

66
18.657: mathematics of machine learning

lecturer: philippe rigollet
scribe: jonathan weed

lecture

2
sep. 14, 2015

part i
statistical learning theory

1. binary classification

in the last lecture, we looked broadly at the problems that machine learning seeks to solve
and the techniques we will cover in this course. today, we will focus on one such problem,
binary classi   cation, and review some important notions that will be foundational for the
rest of the course.

our present focus on the problem of binary classi   cation is justi   ed because both binary
classi   cation encompasses much of what we want to accomplish in practice and because the
response variables in the binary classi   cation problem are bounded. (we will see a very
important application of this fact below.) it also happens that there are some nasty surprises
in non-binary classi   cation, which we avoid by focusing on the binary case here.

1.1 bayes classi   er

recall the setup of binary classi   cation: we observe a sequence (x1, y1), . . . , (xn, yn) of n
independent draws from a joint distribution px,y . the variable y (called the label ) takes
values in {0, 1}, and the variable x takes values in some space x representing    features    of
the problem. we can of course speak of the marginal distribution px of x alone; moreover,
since y is supported on {0, 1}, the conditional random variable y |x is distributed according
to a bernoulli distribution. we write y |x     bernoulli(  (x)), where

  (x) = ip(y = 1|x) = ie[y |x].

(the function    is called the regression function.)

we begin by de   ning an optimal classi   er called the bayes classi   er. intuitively, the
bayes classi   er is the classi   er that    knows         it is the classi   er we would use if we had
perfect access to the distribution y |x.
de   nition: the bayes classi   er of x given y , denoted h   , is the function de   ned by the
rule

(cid:26)1

0

   (x) =

h

(

if    x) > 1/2
if   (x)     1/2.

in other words, h   (x) = 1 whenever ip(y = 1|x) > ip(y = 0|x).
our measure of performance for any classi   er h (that is, any function mapping x to
{0, 1}) will be the classi   cation error : r(h) = ip(y = h(x)). the bayes risk is the value
r    = r(h   ) of the classi   cation error associated with the bayes classi   er. the following
theorem establishes that the bayes classi   er is optimal with respect to this metric.

7(cid:54)
(cid:90)

h=h   

theorem: for any classi   er h, the following identity holds:

r(h)     r(h   ) =

|2  (x)     1| px(dx) = iex [|2  (x)     1|1(h(x) = h   (x))]

(1.1)

where h = h    is the (measurable) set {x     x | h(x) = h   (x)}.

in particular, since the integrand is nonnegative, the classi   cation error r    of the

bayes classi   er is the minimizer of r(h) over all classi   ers h.

moreover,

r(h   ) = ie[min(  (x), 1       (x))]     .
1
2

(1.2)

proof. we begin by proving equation (1.2). the de   nition of r(h) implies

r(h) = ip(y = h(x)) = ip(y = 1, h(x) = 0) + ip(y = 0, h(x) = 1),

where the second equality follows since the two events are disjoint. by conditioning on x
and using the tower law, this last quantity is equal to

ie[ie[1(y = 1, h(x) = 0)|x]] + ie[ie[1(y = 0, h(x) = 1)|x]]

now, h(x) is measurable with respect to x, so we can factor it out to yield

ie[1(h(x) = 0)  (x) + 1(h(x) = 1)(1       (x))]],

(1.3)

where we have replaced ie[y |x] by   (x).

in particular, if h = h   , then equation 1.3 becomes

ie[1(  (x)     1/2)  (x) + 1(  (x) > 1/2)(1       (x))].

but   (x)     1/2 implies   (x)     1       (x) and conversely, so we    nally obtain

r(h   ) = ie[1(  (x)     1/2)  (x) + 1(  (x) > 1/2)(1       (x))]

= ie[(1(  (x)     1/2) + 1(  (x) > 1/2)) min(  (x), 1       (x))]
= ie[min(  (x), 1       (x))],

as claimed. since min(  (x), 1       (x))     1/2, its expectation is also certainly at most 1/2
as well.

now, given an arbitrary h, applying equation 1.3 to both h and h    yields
r(h)     r(h   ) = ie[1(h(x) = 0)  (x) + 1(h(x) = 1)(1       (x))]
   1(h   (x) = 0)  (x) + 1(h   (x) = 1)(1       (x))]],

which is equal to

ie[(1(h(x) = 0)     1(h   (x) = 0))  (x) + (1(h(x) = 1)     1(h   (x) = 1))(1       (x))].

since h(x) takes only the values 0 and 1, the second term can be rewritten as    (1(h(x) =
0)     1(h   (x) = 0)). factoring yields

ie[(2  (x)     1)(1(h(x) = 0)     1(h   (x) = 0))].

8(cid:54)
(cid:54)
(cid:54)
(cid:54)
the term 1(h(x) = 0)     1(h   (x) = 0) is equal to    1, 0, or 1 depending on whether h
and h    agree. when h(x) = h   (x), it is zero. when h(x) = h   (x), it equals 1 whenever
h   (x) = 0 and    1 otherwise. applying the de   nition of the bayes classi   er, we obtain
ie[(2  (x)     1)1(h(x) = h   (x)) sign(       1/2)] = ie[|2  (x)     1|1(h(x) = h   (x))],

as desired.

we make several remarks. first, the quantity r(h)     r(h   ) in the statement of the
theorem above is called the excess risk of h and denoted e(h). (   excess,    that is, above
the bayes classi   er.) the theorem implies that e(h)     0.
second, the risk of the bayes classi   er r    equals 1/2 if and only if   (x) = 1/2 almost
surely. this maximal risk for the bayes classi   er occurs precisely when y    contains no
information    about the feature variable x. equation (1.1) makes clear that the excess risk
weighs the discrepancy between h and h    according to how far    is from 1/2. when    is
close to 1/2, no classi   er can perform well and the excess risk is low. when    is far from
1/2, the bayes classi   er performs well and we penalize classi   ers that fail to do so more
heavily.

as noted last time, id156 attacks binary classi   cation by putting
some model on the data. one way to achieve this is to impose some distributional assump-
tions on the conditional distributions x|y = 0 and x|y = 1.

we can reformulate the bayes classi   er in these terms by applying bayes    rule:

  (x) = ip(y = 1

|x = x) =

ip(x = x|y = 1)ip(y = 1) + ip(x = x|y = 0)ip(y = 0)

.

ip(x = x y = 1)ip(y = 1)

|

(in general, when px is a continuous distribution, we should consider in   nitesimal proba-
bilities ip(x     dx).)
assume that x|y = 0 and x|y = 1 have densities p0 and p1, and ip(y = 1) =    is
some constant re   ecting the underlying tendency of the label y . (typically, we imagine
that    is close to 1/2, but that need not be the case: in many applications, such as anomaly
detection, y = 1 is a rare event.) then h   (x) = 1 whenever   (x)     1/2, or, equivalently,
whenever

p1(x)
p0(x)

   

1       
  

.

when    = 1/2, this rule amounts to reporting 1 or 0 by comparing the densities p1
and p0. for instance, in figure 1, if    = 1/2 then the bayes classi   er reports 1 whenever
p1     p0, i.e., to the right of the dotted line, and 0 otherwise.

on the other hand, when    is far from 1/2, the bayes classi   er is weighed towards the

underlying bias of the label variable y .

1.2 empirical risk minimization

the above considerations are all probabilistic, in the sense that they discuss properties of
some underlying id203 distribution. the statistician does not have access to the true
id203 distribution px,y ; she only has access to i.i.d. samples (x1, y1), . . . , (xn, yn).
we consider now this statistical perspective. note that the underlying distribution px,y
still appears explicitly in what follows, since that is how we measure our performance: we
judge the classi   ers we produced on future i.i.d. draws from px,y .

9(cid:54)
(cid:54)
(cid:54)
figure 1: the bayes classi   er when    = 1/2.

given data dn = {

(x1, y1), . . . , (xn, yn)}, we build a classi   er hn(x), which is random
in two senses: it is a function of a random variable x and also depends implicitly on the
random data dn. as above, we judge a classi   er according to the quantity e(hn). this is
a random variable: though we have integrated out x, the excess risk still depends on the
data dn. we therefore will consider bounds both on its expected value and bounds that
hold in high id203. in any case, the bound e(hn)     0 always holds. (this inequality
does not merely hold    almost surely,    since we proved that r(h)     r(h   ) uniformly over
all choices of classi   er h.)

  

  

  

last time, we proposed two di   erent philosophical approaches to this problem.

in
particular, generative approaches make distributional assumptions about the data, attempt
to learn parameters of these distributions, and then plug the resulting values into the model.
the discriminative approach   the one taken in machine learning   will be described in great
detail over the course of this semester. however, there is some middle ground, which is worth
mentioning brie   y. this middle ground avoids making explicit distributional assumptions
about x while maintaining some of the    avor of the generative model.
the central insight of this middle approach is the following: since by de   nition h   (x) =
1(  (x) > 1/2), we estimate    by some     n and thereby produce the estimator hn =
1(    n(x) > 1/2). the result is called a plug-in estimator.

  

of course, achieving good performance with a plug-in estimator requires some assump-
(no-free-lunch theorems imply that we can   t avoid making an assumption some-
tions.
where!) one possible assumption is that   (x) is smooth; in that case, there are many
nonparamteric regression techniques available (nadaraya-watson kernel regression, wavelet
bases, etc.).

we could also assume that   (x) is a function of a particular form. since   (x) is only
supported on [0, 1], standard linear models are generally inapplicable; rather, by applying
the logit transform we obtain id28, which assumes that    satis   es an identity
of the form

(cid:18)   (x)

(cid:19)

log

1       (x)

=   t x.

plug-in estimators are called    semi-paramteric    since they avoid making any assumptions
about the distribution of x. these estimators are widely used because they perform fairly
well in practice and are very easy to compute. nevertheless, they will not be our focus here.
in what follows, we focus here on the discriminative framework and empirical risk min-
imization. our benchmark continues to be the risk function r(h) = ie1(y = h(x)), which

10(cid:54)
is clearly not computable based on the data alone; however, we can attempt to use a na    ve
statistical    hammer    and replace the expectation with an average.

de   nition: the empirical risk of a classi   er h is given by

(cid:88)

n1
n

i=1

  rn(h) =

1(yi = h(xi)).

minimizing the empirical risk over the family of all classi   ers is useless, since we can
always minimize the empirical risk by mimicking the data and classifying arbitrarily other-
wise. we therefore limit our attention to classi   ers in a certain family h.
de   nition: the empirical risk minimizer (erm) over h is any element1 herm of the set
argminh rn(h).

   h

  

  

in order for our results to be meaningful, the class h must be much smaller than the
space of all classi   ers. on the other hand, we also hope that the risk of herm will be close
to the bayes risk, but that is unlikely if h is too small. the next section will give us tools
for quantifying this tradeo   .

  

1.3 oracle inequalities

an oracle is a mythical classi   er, one that is impossible to construct from data alone but
whose performance we nevertheless hope to mimic. speci   cally, given h we de   ne h to be
h that minimizes the true risk. of course,
an element of argminh r(h)   a classi   er in
we cannot determine h, but we can hope to prove a bound of the form

   h
  

  

  r(h)       r(h) + something small.

  

  

  

(1.4)
since h is the best minimizer in h given perfect knowledge of the distribution, a bound of
the form given in equation 1.4 would imply that h has performance that is almost best-in-
class. we can also apply such an inequality in the so-called improper learning framework,
    h; in that case, we still get nontrivial
where we allow h to lie in a slightly larger class h(cid:48)
  
guarantees on the performance of h if we know how to control r(h)
there is a natural tradeo    between the two terms on the right-hand side of equation 1.4.
when h
is small, we expect the performance of the oracle h to su   er, but we may hope
(indeed, at the limit where h is a single function, the
to approximate h quite closely.
   something small    in equation 1.4 is equal to zero.) on the other hand, as h grows the
oracle will become more powerful but approximating it becomes more statistically di   cult.
(in other words, we need a larger sample size to achieve the same measure of performance.)
since r(h) is a random variable, we ultimately want to prove a bound in expectation

  

  

  

  

or tail bound of the form

ip(r(h)       r(h) +    n,  (h))     1       ,

  

where    n,  (h) is some explicit term depending on our sample size and our desired level of
con   dence.

1in fact, even an approximate solution will do: our bounds will still hold whenever we produce a classi   er

h satisfying rn(h)     inf h r   h n(h) +   .
  

     

11(cid:54)
in the end, we should recall that
   

e   
(h) = r(h) r(h ) = (r(h) r(h)) + (r(h)     r(h   )).

         

      

  

the second term in the above equation is the approximation error, which is unavoidable
once we    x the class h. oracle inequalities give a means of bounding the    rst term, the
stochastic error.

1.4 hoe   ding   s theorem

our primary building block is the following important result, which allows us to understand
how closely the average of random variables matches their expectation.

theorem (hoe   ding   s theorem): let x1, . . . , xn be n independent random vari-
ables such that xi     [0, 1] almost surely.

then for any t > 0,

ip

(cid:32)(cid:12)(cid:12)
(cid:12)
(cid:12)
(cid:12)

n1(cid:88)

n

i=1

(cid:33)

(cid:12)(cid:12)
(cid:12)(cid:12)
(cid:12)

   

xi

iexi

t     2e   2nt .

2

>

in other words, deviations from

y exponentially fast in n and t.
proof. de   ne centered random variables zi = xi     iexi. it su   ces to show that

the mean deca

(cid:18) 1(cid:88) (cid:19)

zi > t

ip

       2nt2

e

,

since the lower tail bound follows analogously. (exercise!)

tion, we have for any s > 0

(cid:18) 1(cid:88)

ip

n

(cid:19)

(cid:16)

(cid:88)

(cid:17)

z

i > t

= ip

exp

s

z

i

stn

> e

n

(cid:16)

(cid:17)     e    ie[e
(cid:89)

= e   stn

stn

(cid:80)

zi

]

(markov)

s

ie[eszi],

(1.5)

we apply cherno    bounds. since the exponential function is an order-preserving bijec-

where in the last equality we have used the independence of the

zi.

we therefore need to control the term ie[eszi], known as the moment-generating func-
tion of zi. if the zi were normally distributed, we could compute the moment-generating
function analytically. the following lemma establishes that we can do something similar
when the zi are bounded.

lemma (hoe   ding   s lemma): if z     [a, b] almost surely and iez = 0, then

ieesz

2

    s (b   a)

e

8

2

.

proof of lemma. consider the log-moment generating function   (s) = log ie[esz], and note
that it su   ces to show that   (s)     s2(b     a)2/8. we will investigate    by computing the

12   rst several terms of its taylor expansion. standard regularity conditions imply that we
can interchange the order of di   erentiation and integration to obtain

  (cid:48)(s) =

  (cid:48)(cid:48)(s) =

,

ie[zesz]
ie[esz]
ie[z2esz]ie[esz]

   
ie[esz]2

(cid:20)

(cid:21)

(cid:18)

(cid:20)

ie z

   

(cid:21)(cid:19)

2

.

esz

ie[esz]

z2

esz

ie[esz]

ie[zesz]2

= ie

sz integrates to 1, we can interpret   (cid:48)(cid:48)(s) as the variance of z under the id203

since esz
ie[e
measure df = esz

]

sz die. we obtain

ie[e

]

(cid:18)

(cid:19)

2 |     b   a almost surely since

2

  (cid:48)(cid:48)(s) = varf(z) = varf

z    

a + b

,

2
since the variance is una   ected under shifts. but |z     a+b
z     [a, b] almost surely, so

(cid:19)

(cid:34)(cid:18)

(cid:19) (cid:35)

2

(cid:18)

varf

z    

a

+ b
2

    f

z    

a + b

2

   

(b     a)2

4

.

finally, the fundamental theorem of calculus yields

(cid:90)

(cid:90)

s

0

0

  (s) =

u

  (cid:48)(cid:48)

(u du

)

s2(b

   

   
8

a)2

.

this concludes the proof of the lemma.

applying hoe   ding   s lemma to equation (1.5), we obtain

(cid:18) 1(cid:88)

ip

n

z > t

i

(cid:19)
(cid:18) 1(cid:88)

    e   stn(cid:89)
(cid:19)

zi > t

n

    e   2nt2

,

    ,
es2/8 = ens /8 stn

2

for any s > 0. plugging in s = 4t > 0 yields

ip

as desired.

hoe   ding   s theorem implies that, for any classi   er h, the bound

|   rn(h)     r(h)|    

log(2/  )

2n

holds with id203 1       . we can immediately apply this formula to yield a maximal
if h is a    nite family, i.e., h = {h1, . . . , hm}, then with id203 1       /m
inequality:
the bound

|   rn(hj)     r(hj)|    

log

(2m/  )
2n

(cid:114)

(cid:114)

13holds. the event that maxj |rn(hj)   r(hj)|
t for j = 1, . . . , m , so the union bound immediately implies that

> t is the union of the events |rn(hj)   r(hj)| >

  

  

(cid:114)

max|   rn(hj)     r(hj)

j

|    

log(2m/  )

2n

with id203 1     . in other words, for such a family, we can be assured that the empirical
risk and the true risk are close. moreover, the logarithmic dependence on m implies that
we can increase the size of the family h exponentially quickly with n and maintain the
same guarantees on our estimate.

1418.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: james hirst

lecture

3
sep. 16, 2015

1.5 learning with a    nite dictionary

recall from the end of last lecture our setup: we are working with a    nite dictionary
h = {h1, . . . , hm } of estimators, and we would like to understand the scaling of this problem
with respect to m and the sample size n. given h, one idea is to simply try to minimize
the empirical risk based on the samples, and so we de   ne the empirical risk minimizer, herm,
by

  

  
herm     argmin rn(h).

  

h   h

in what follows, we will simply write h instead of herm when possible. also recall the

  

  

de   nition of the oracle, h, which (somehow) minimizes the true risk and is de   ned by

  

  h     argmin r(h).

h   h

the following theorem shows that, although h cannot hope to do better than h in
general, the di   erence should not be too large as long as the sample size is not too small
compared to m .

  

  

theorem: the estimator h satis   es

  

r(h)     r(h) + r 2 log(2m/  )

  

  

n

with id203 at least 1       . in expectation, it holds that
r 2 log(2m )

ie[r(h)]     r(h) +

  

  

.

n

proof.

from the de   nition of h, we have rn(h)     rn(h), which gives

     

     

  

r(h)     r(h) + [rn(h)     r(h)] + [r(h)     rn(h)].

  

     

     

  

  

  

the only term here that we need to control is the second one, but since we don   t have
any real information about h, we will bound it by a maximum over h and then apply
hoe   ding:

  

     
[rn(h)     r(h)] + [r(h)     rn(h)]     2 max |rn(hj )     r(hj)|     2

     

  

  

  

j

r

log(2m/  )

2n

with id203 at least 1       , which completes the    rst part of the proof.

15to obtain the bound in expectation, we start with a standard trick from id203
which bounds a max by its sum in a slightly more clever way. here, let {zj}j be centered
random variables, then
(cid:21)

ie max |zj| = log exp sie max |zj|(cid:21)(cid:19)     log ie(cid:20)exp(cid:18)s max |zj|
(cid:20)

(cid:19)(cid:21) ,

(cid:18)

(cid:20)

j

j

j

1
s

1
s

where the last inequality comes from applying jensen   s inequality to the convex function
exp(  ). now we bound the max by a sum to get

2m

    log x

1
s

j=1

ie [exp(szj)]     log (cid:18)2m exp(cid:18) (cid:19)(cid:19) =

s2
8n

1
s

log(2m

)

s

s
+ ,
8n

where we used zj = rn(hj)     r(hj) in our case and then applied hoe   ding   s lemma. bal-
ancing terms by minimizing over s, this gives s = 2p2n log(2m ) and plugging in produces

  

ie max |rn(hj )     r(hj)|    

  

(cid:20)

j

(cid:21) r

log(2m )

2n

,

which    nishes the proof.

2. concentration inequalities

concentration inequalities are results that allow us to bound the deviations of a function of
random variables from its average. the    rst of these we will consider is a direct improvement
to hoe   ding   s inequality that allows some dependence between the random variables.

2.1 azuma-hoe   ding inequality

given a    ltration {fi}i of our underlying space x , recall that {   i}i are called martingale
di   erences if, for every i, it holds that    i     fi and ie [   i|fi] = 0. the following theorem
gives a very useful concentration bound for averages of bounded martingale di   erences.

theorem (azuma-hoe   ding): suppose that {   i}i are margingale di   erences with
respect to the    ltration {fi}i, and let ai, bi     fi   1 satisfy ai        i     bi almost surely
for every i. then

ip

" 1 x

n

i

#

   i > t

    exp

(cid:18)   pn

2n

2t2

i=1 kbi     aik2

   

(cid:19) .

in comparison to hoe   ding   s inequality, azuma-hoe   ding a   ords not only the use of
non-uniform boundedness, but additionally requires no independence of the random vari-
ables.

proof. we start with a typical cherno    bound.

"

ip

x    i > t     iehes p    ii e   st = ie

#

hiehes p    i|fn   1ii e   st

i

16= iehes p    iie[es   n|fn 1] e   st

i

n   1

   

    ie[es p    i    es (bn   an) /8]e   st,

2

2

n   1

where we have used the fact that the    
hoe   ding   s lemma on the inner expectation.
applying hoe   ding   s lemma, we get
#

ip

    > t

i

    exp

"
x

i

n

  x kb     a k2 !

   

i

i

s2
8

i=1

e   st

.

i, i < n, are all fn measureable, and then applied
iteratively isolating each    i like this and

optimizing over s as usual then gives the result.

2.2 bounded di   erences inequality

although azuma-hoe   ding is a powerful result, its full generality is often wasted and can
be cumbersome to apply to a given problem. fortunately, there is a natural choice of the
{fi}i and {   i}i, giving a similarly strong result which can be much easier to apply. before
we get to this, we need one de   nition.

de   nition (bounded di   erences condition): let g : x     ir and constants ci be
given. then g is said to satisfy the bounded di   erences condition (with constants ci) if

sup

x

1,...,xn,xi

   

for every i.

|g(x , . . . , x )     g(x , . . . , x   

n

1

1

i, . . . , xn)|     ci

intuitively, g satis   es the bounded di   erences condition if changing only one coordinate
of g at a time cannot make the value of g deviate too far. it should not be too surprising
that these types of functions thus concentrate somewhat strongly around their average, and
this intuition is made precise by the following theorem.

theorem (bounded di   erences inequality): if g : x     ir satis   es the bounded
di   erences condition, then

ip [|g(x1, . . . , xn)     ie[g(x1, . . . , xn)| > t]     2 exp (cid:18)   pi c2

2t2

i

(cid:19) .

proof. let {fi}i be given by fi =   (x1, . . . , xi), and de   ne the martingale di   erences
{   i}i by

   i = ie [g(x1, . . . , xn)|fi]     ie [g(x1, . . . , xn)|fi   1] .

then

"
|x    i| > t

#

ip

i

(cid:12) > t ,
= ip (cid:12)g(x1, . . . , xn)     ie[g(x1, . . . , xn)
(cid:3)
(cid:12)

(cid:2)(cid:12)

exactly the quantity we want to bou

nd. now, note that

   i     ie(cid:20)sup g(x1, . . . , xi, . . . , xn)|fi     ie [g(x1, . . . , xn)|fi   1]

(cid:21)

xi

17= ie(cid:20)sup g(x1, . . . , xi, . . . , xn)     g(x1, . . . , xn)|fi   1

xi

(cid:21) =: bi.

similarly,

   i     ie(cid:20)inf g(x1, . . . , xi, . . . , xn)     g(x1, . . . , xn)|fi   1 =: ai.

(cid:21)

xi

at this point, our assumption on g implies that kbi     aik        ci for every i, and since
ai        i     bi with ai, bi     fi   1, an application of azuma-hoe   ding gives the result.

2.3 bernstein   s inequality

hoe   ding   s inequality is certainly a powerful concentration inequality for how little it as-
sumes about the random variables. however, one of the major limitations of hoe   ding is
just this: since it only assumes boundedness of the random variables, it is completely obliv-
ious to their actual variances. when the random variables in question have some known
variance, an ideal concentration inequality should capture the idea that variance controls
concentration to some degree. bernstein   s inequality does exactly this.

theorem (bernstein   s inequality): let x1, . . . , xn be independent, centered ran-
p
dom variables with |x |     c for every i, and write   2 = n   1
i var(xi) for the average
variance. then
!

nt2

 

#

i

ip

" 1 x

n

i

xi > t

    exp

   

2  2 + 2 tc

3

.

here, one should think of t as being    xed and relatively small compared to n, so that

strength of the inequality indeed depends mostly on n and 1/  2.

proof. the idea of the proof is to do a cherno    bound as usual, but to    rst use our
assumptions on the variance to obtain a slightly better bound on the moment generating
functions. to this end, we expand

ie[esxi] = 1 + ie[sxi] + ie

"

   

x

k=2

#

(s

k
x )
i
k!

    1 + var(xi)x

   

k=2

skck   2

k!

,

where we have used ie[x k
exponential, we get

i ]     ie[x 2

i |xi|k   2]     var(x k   2

i)c

. rewriting the sum as an

sxi

ie[e

2

]     s var(xi)g(s),

g(s) :=

esc

    sc     1
c2s2

.

the cherno    bound now gives

" 1 x xi > t

#

 

    exp

!
inf [s2(x var(xi))g(s)     nst]

ip

n

i

s>0

i

= exp(cid:18)n    inf [s2  2g(s)     st]
(cid:19)

s>0

,

and optimizing this over s (a fun calculus exercise) gives exactly the desired result.

183. noise conditions and fast rates

  

  

to measure the e   ectiveness of the estimator h, we would like to obtain an upper bound
on the excess risk e(h) = r(h)     r(h   ). it should be clear, however, that this must depend
signi   cantly on the amount of noise that we allow. in particular, if   (x) is identically equal
to 1/2, then we should not expect to be able to say anything meaningful about e(h) in
general. understanding this trade-o    between noise and rates will be the main subject of
this chapter.

  

  

3.1 the noiseless case

a natural (albeit somewhat na    ve) case to examine is the completely noiseless case. here,
we will have   (x)     {0, 1} everywhere, var(y |x) = 0, and

e(h) = r(h)     r(h   ) = ie[|2  (x)     1|1i(h(x) = h   (x))] = ip[h(x) = h   (x)].

let us now denote

zi = 1i(h(xi) = yi)     1i(h(xi) = yi),

  

  

and write zi = zi     ie[zi]. then notice that we have

  

|zi| = 1i(h(xi) = h(xi)),

  

  

and

var(zi)     ie[z 2

i ] = ip[h(xi) = h(xi)].

  

  

for any classi   er hj     h, we can similarly de   ne zi(hj) (by replacing h with hj through-

  

out). then, to set up an application of bernstein   s inequality, we can compute

n

1 x
n

i=1

var(zi(hj))     ip[hj(xi) = h(xi)] =:   2
j .

  

at this point, we will make a (fairly strong) assumption about our dictionary h, which
is that h        h, which further implies that h = h   . since the random variables zi compare
to h, this will allow us to use them to bound e(h), which rather compares to h   . now,
applying bernstein (with c = 2) to the {zi(hj )}i for every j gives

  

  

  

  

"

ip

1
n

n

x

i=1

#

  
zi(hj) > t     exp

 

   

!

nt2
j + 4 t3

2  2

=

:

  
m

,

and a simple computation here shows that it is enough to take
   
    =: t0(j)

j log(m/  ) 4
3n

t     max   

   s 2  2

log(m/  )

n

,

for this to hold. from here, we may use the assumption h = h    to conclude that

  

iphe(h) > t0(  j)i       ,

  

  
h   = h.

j

196
6
6
6
6
6
6
however, we also know that   2
       e(h), which implies that
j

  

  e(h)     max   

   s   

2e(h) log(m/  ) 4
3n

n

,

log(m/  )

   
   

with id203 1       , and solving for e(h) gives the improved rate

  

  e(h)     2

log(m/  )

n

.

2018.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: cheng mao

lecture

4
sep. 21, 2015

  

  

  

in this lecture, we continue to discuss the e   ect of noise on the rate of the excess risk
e(h) = r(h)     r(h   ) where h is the empirical risk minimizer. in the binary classi   cation
model, noise roughly means how close the regression function    is from 1
2 . in particular, if
   = 1 then we observe only noise, and if        {0, 1} we are in the noiseless case which has
been studied last time. especially, we achieved the fast rate log m in the noiseless case by
assuming h        h which implies that h = h   . this assumption was essential for the proof
and we will see why it is necessary again in the following section.

  

n

2

3.2 noise conditions

the noiseless assumption is rather unrealistic, so it is natural to ask what the rate of excess
risk is when the noise is present but can be controlled. instead of the condition        {0, 1},
we can control the noise by assuming that    is uniformly bounded away from 1
2 , which is
the motivation of the following de   nition.

de   nition (massart   s noise condition): the noise in binary classi   cation is said
to satisfy massart   s condition with constant        (0, 1
2 ] if |  (x)     1 |        almost surely.

2

once uniform boundedness is assumed, the fast rate simply follows from last proof with

appropriate modi   cation of constants.

theorem: let ce(h) denote the excess risk of the empirical risk minimizer h = herm.
if massart   s noise condition is satis   ed with constant   , then

  

  

  

  e(h)    

log(m/  )

  n

with id203 at least 1       . (in particular    = 1 gives exactly the noiseless case.)

2

de   ne zi(h) = 1i(h(xi) = yi)     1i(h(xi) = yi). by the assumption h = h    and the

  

  

proof.
de   nition of h = herm,

  

  

  
e(h) = r(h)     r(h)

  

  

     

(cid:1)
  
= rn(h)     rn(h) + rn(h)     rn(h)     r(h)     r(h)

     

     

     

(cid:0)

  

   

1
n

n

x(cid:0)

i=1

  

zi(h)     ie[zi(h ](cid:1).

  

)

(3.1)

(3.2)

hence it su   ces to bound the deviation of pi zi from its expectation. to this end, we

hope to apply bernstein   s inequality. since

var[zi(h)]     ie[z

i(h) ] = ip[h(xi) = h(xi)],

2

  

216
6
6
(cid:0)

,

we have that for any 1     j     m ,

n

1 x
n

i=1

var[zi(hj)]     ip[hj(x) = h(x)] =:   2
j .

  

bernstein   s inequality implies that

ip

n

(cid:2) 1 x

n

i=1

(zi(hj)     ie[zi(hj )]) > t     exp    

(cid:3)

nt2
3 t(cid:1)
j + 2

2  2

=:

  
m

.

applying a union bound over 1     j     m and taking

t = t0(j) := max

s

(cid:0)

2  2

j log(m/  ) 2 log(m/  )

n

3n

we get that

1 x(zi(hj )     ie[zi(hj)])     t0(j)
n

n

i=1

(cid:1),

(3.3)

for all 1     j     m with id203 at least 1       .

suppose h = h  . it follows from (3.2) and (3.3) that with id203 at least 1       ,

  

j

  e(h)     t   0(j).

(note that so far the proof is exactly the same as the noiseless case.) since |  (x)     1
a.s. and h = h   ,

  

2 |       

  

e(h) = ie[|2  (x)     1|1i(h(x) = h   (x))]     2  ip[h  (x) = h(x)] = 2    2
  .
j

j

  

  

therefore,

  e(h)     max

s   e(h) log(m/  ) 2 log(m/  )
(cid:0)

,

  n

3n

(cid:1)

,

(3.4)

so we conclude that with probabilit

y at least 1       ,

  e(h)    

log(m/  )

  n

.

  

the assumption that h = h    was used twice in the proof. first it enables us to ignore
the approximation error and only study the stochastic error. more importantly, it makes
the excess risk appear on the right-hand side of (3.4) so that we can rearrange the excess
risk to get the fast rate.

massart   s noise condition is still somewhat strong because it assumes uniform bounded-
2 but only with small id203,

2 . instead, we can allow    to be close to 1

ness of    from 1
and this is the content of next de   nition.

226
6
6
de   nition (tsybakov   s noise condition or mammen-tsybakov noise condi-
tion): the noise in binary classi   cation is said to satisfy tsybakov   s condition if there
exists        (0, 1), c

0 > 0 and t0     (0, 2 ] such that

1

ip[|  (x)     |     t]     c 10t      

  

1
2

for all t     [0, t0].

  

in particular, as        1, t 1          0

, so this recovers massart   s condition with    = t0 and
we have the fast rate. as        0, t 1          1, so the condition is void and we have the slow
rate. in between, it is natural to expect fast rate (meaning faster than slow rate) whose
order depends on   . we will see that this is indeed the case.

  

lemma: under tsybakov   s noise condition with constants   , c0 and t0, we have

ip[h(x) = h   (x)]     ce(h)  

for any classi   er h where c = c(  , c0, t0) is a constant.

proof. we have

e(h) = ie[|2  (x)     1|1i(h(x) = h   (x))]

    ie[|2  (x)     1|1i(|  (x)     | > t)1i(h(x) = h   (x))]

1
2

    2tip[|  (x)     | > t, h(x) = h   (x)]

1
2

    2tip[h(x) = h   (x)]     2tip[|  (x)     |     t]

1
2

    2tip[h(x) = h   (x)]     2c0t 1     

1

where tsybakov   s condition was used in the last step. take t = cip[h(x) = h   (x)]
for
some positive c = c(  , c0, t0) to be chosen later. we assume that c     t0 to guarantee that
t     [0, t0]. since        (0, 1),

  

1

     

e(h)     2cip[h(x) = h   (x)]1/  
    cip[h(x) = h   (x)]1/  

1

    2c c 1      ip[h(x) = h   

(x)] /  

1

0

by selecting c su   ciently small depending on    and c0. therefore

ip[h(x) = h   (x)]     e(h)  

1
c  

and choosing c = c(  , c0, t0) := c      completes the proof.

having established the key lemma, we are ready to prove the promised fast rate under

tsybakov   s noise condition.

236
6
6
6
6
6
6
6
6
6
6
theorem: if tsybakov   s noise condition is satis   ed with constant   , c0 and t0, then
there exists a constant c = c(  , c0, t0) such that
  e h)     c(cid:0) og(m/  

(cid:1) 2     

(

)

l

1

n

with id203 at least 1       .

this rate of excess risk parametrized by    is indeed an interpolation of the slow (       0)
and the fast rate (       1). futhermore, note that the empirical risk minimizer h does not
depend on the parameter    at all! it automatically adjusts to the noise level, which is a
very nice feature of the empirical risk minimizer.

  

proof. the majority of last proof remains valid and we will explain the di   erence. after
establishing that

  e(h)     t0(  j),

we note that the lemma gives

  2
   = ip[h(x) 6= h(x)]     ce(h)  .
j

  

  

  

it follows that

and thus

  e(h)     max

s

(cid:0)

2ce(h)   log(m/  ) 2 log(m/  )

  

,

3n

(cid:1)

n

  

e(h)     max (cid:0)

(cid:16) 2c log m

1

   (cid:1) 2     

n

,

2

log(m/  )

3n

(cid:17).

4. vapnik-chervonenkis (vc) theory

the upper bounds proved so far are meaningful only for a    nite dictionary h, because if
m = |h| is in   nite all of the bounds we have will simply be in   nity. to extend previous
results to the in   nite case, we essentially need the condition that only a    nite number of
elements in an in   nite dictionary h really matter. this is the objective of the vapnik-
chervonenkis (vc) theory which was developed in 1971.

4.1 empirical measure

recall from previous proofs (see (3.1) for example) that the key quantity we need to control
is

2 sup (cid:0)rn(h)     r(h) .
(cid:1)

  

h   h

instead of the union bound which would not work in th
e in   nite case, we seek some bound
that potentially depends on n and the complexity of the set h. one approach is to consider
some metric structure on h and hope that if two elements in h are close, then the quantity
evaluated at these two elements are also close. on the other hand, the vc theory is more
combinatorial and does not involve any metric space structure as we will see.

24by de   nition

  rn(h)     r(h) = x(cid:0)1i(h(xi) = yi)     ie[1i(h(xi) = yi)]
(cid:1).

1
n

n

i=1

let z = (x, y ) and zi = (xi, yi), and let a denote the class of measurable sets in the
sample space x    {0, 1}. for a classi   er h, de   ne ah     a by

moreover, de   ne measures   n and    on a by

{zi     ah} = {h(xi) = yi}.

  n(a) = x 1i(zi     a) and   (a) = ip[zi     a]

1
n

n

i=1

for a     a. with this notation, the slow rate we proved is just
r

sup rn(h)     r(h) = sup |  n(a)       (a)|    
h   h

a   a

  

log(2|a|/  )

.

2n

since this is not accessible in the in   nite case, we hope to use one of the concentration
inequalities to give an upper bound. note that   n(a) is a sum of random variables that may
not be independent, so the only tool we can use now is the bounded di   erence inequality.

if we change the value of only one zi in the function

z1, . . . , zn 7    sup |  n(a)       (a)|,

a   a

r

the value of the function will di   er by at most 1/n. hence it satis   es the bounded di   erence
assumption with ci = 1/n for all 1     i     n. applying the bounded di   erence inequality, we
get that

(cid:12)
(cid:12)(cid:12)
(cid:12) sup |  n(a)       (a)|     ie[ sup |  n(a)       (a)|]    
(cid:12)
(cid:12)
des any fast rate (faster than
at least 1       . note that this already preclu
with id203
n   1/2). to achieve fast rate, we need talagrand inequality and localization techniques which
are beyond the scope of this section.

log(2/  )

a   a

a   a

2n

it follows that with id203 at least 1       ,

sup |  n(a)       (a)|     ie[ sup |  n(a)       (a)|] +
a   

a   a

a

r

log(2/  )

2n

.

we will now focus on bounding the    rst term on the right-hand side. to this end, we need
a technique called symmetrization, which is the subject of the next section.

4.2 symmetrization and rademacher complexity

symmetrization is a frequently used technique in machine learning. let d = {z1, . . . , zn}
be the sample set. to employ symmetrization, we take another independent copy of the
sample set d    = {z    
n}. this sample only exists for the proof, so it is sometimes
referred to as a ghost sample. then we have

1, . . . , z    

  (a) = ip[z     a] = ie[ x 1i(z    

i     a)] = ie[

1
n

n

i=1

1
n

n

x

i=1

1i(z    

i     a)|d] = ie[     

n(a)|d]

256
6
6
where      

n := 1 pi=1 1i(z    

n

n

i     a). thus by jensen   s inequality,
ie[ sup |  n(a)       (a)|] = ie(cid:2) sup (cid:12)(cid:12)  n(a)     ie[     
    ie sup ie[|  n(a)          

a   a

a   a

(cid:12)(cid:12)
n(a)|d]
n(a)| |d

(cid:3)
(cid:3)
]

(cid:2)

a   a

   

ie(cid:2) sup |  

a   a

(cid:3)
n(a)       n(a)|

   

n

= ie(cid:2) sup (cid:12) x(cid:0)1i(z

1
(cid:12)
a   a n

i=1

i     a)     1i(z

   
i     a)

(cid:1)(cid:12)(cid:3).
(cid:12)

since d    has the same distribution of d, by sy
distribution as   i(cid:0)1i(zi     a)     1i(z    

mmetry 1i(zi     a)     1i(z    

i     a) has the same

i     a)(cid:1) where   1, . . . ,   n are i.i.d. rad( 1

2 ), i.e.

1
ip[  i = 1] = ip[  i =    1] = ,
2

and   i   s are taken to be independent of both samples. therefore,

ie[ sup |  n(a)       (a)|]     ie

a a

   

(cid:2) sup

a   a

(cid:12)(cid:12)

n

1 x   i(cid:0)1i(zi     a)     1i(z

n

i=1
n

    2ie(cid:2)

1
sup (cid:12) x   i1i(zi     a) .
(cid:12)(cid:3)
(cid:12)
(cid:12)
a   a n

i=1

   
i     a)

(cid:1)(cid:12)(cid:3)
(cid:12)

(4.5)

using symmetrization we have bounded ie[sup
] by a much nicer quantity.
yet we still need an upper bound of the last quantity that depends only on the structure
of a but not on the random sample {zi}. this is achieved by taking the supremum over
all zi     x    {0, 1} =: y.

a   a |  n(a)     (a)|

de   nition: the rademacher complexity of a family of sets a in a space y is de   ned
to be the quantity

rn(a) = sup

z1,...,zn   y

ie(cid:2)

sup (cid:12) 1
(cid:12)
a   a n

n

x   i1i(zi     a)
(cid:12)(cid:3).
(cid:12)

i=1

the rademacher complexity of a set b     i

rn is de   ned to b

e

rn(b) = ie(cid:2) sup

b   b

n

1

(cid:12)(cid:12) n x   ibi

i=1

(cid:12)(cid:12)(cid:3).

we conclude from (4.5) and the de   nition that

ie[ sup |  n(a)       (a)|]     2rn(a).

a   a

(cid:12)(cid:12)
in the de   nition of rademacher complexity of a set, the quantity 1
n

n
i=1   ibi measure
s
how well a vector b     b correlates with a random sign pattern {  i}. the more complex
b is, the better some vector in b can replicate a sign pattern. in
f b is the
full hypercube [   1, 1]n, then rn(b) = 1. however, if b     [   1, 1]n contains only k-sparse

rticular, i

p

pa

(cid:12)(cid:12)

26vectors, then rn(b) = k/n. hence rn(b) is indeed a measurement of the complexity of
the set b.

the set of vectors to our interest in the de   nition of rademacher complexity of a is

t (z) := {(1i(z1     a), . . . , 1i(zn     a))t , a     a}.

thus the key quantity here is the cardinality of t (z), i.e., the number of sign patterns these
vectors can replicate as a ranges over a. although the cardinality of a may be in   nite,
the cardinality of t (z) is bounded by 2n.

2718.657: mathematics of machine learning

lecturer: philippe rigollet
scribe: vira semenova and philippe rigollet

lecture

5
sep. 23, 2015

in this lecture, we complete the analysis of the performance of the empirical risk mini-
mizer under a constraint on the vc dimension of the family of classi   ers. to that end, we
will see how to control rademacher complexities using shatter coe   cients. moreover, we
will see how the problem of controlling uniform deviations of the empirical measure   n from
the true measure    as done by vapnik and chervonenkis relates to our original classi   cation
problem.

4.1 shattering

recall from the previous lecture that we are interested in sets of the form

t (z) := (cid:8)(1i(z1     a), . . . , 1i(zn     a)), a     a ,
(cid:9)

z = (z1, . . . , zn) .

(4.1)

in particular, the cardinality of t (z), i.e., the numbe

r of binary patterns these vectors
can replicate as a ranges over a, will be of critical importance, as it will arise when
controlling the rademacher complexity. although the cardinality of a may be in   nite, the
cardinality of t (z) is always at most 2n. when it is of the size 2n, we say that a shatters
the set z1, . . . , zn. formally, we have the following de   nition.

de   nition: a collection of sets a shatters the set of points {z1, z2, ..., zn}

card{(1i(z1     a), . . . , 1i(zn     a)), a     a} = 2n .

the sets of points {z1, z2, ..., zn} that we are interested are realizations of the pairs z1 =
(x1, y1), . . . , zn = (xn, yn) and may, in principle take any value over the sample space.
therefore, we de   ne the shatter coe   cient to be the largest cardinality that we may obtain.

de   nition: the shatter coe   cients of a class of sets a is the sequence of numbers
{sa(n)}n   1, where for any n     1,

sa(n) = sup card (1i(z1 a), . . . , 1i(zn a)), a

   

   

{

z1,...,zn

    a}

and the suprema are taken over the whole sample space.

by de   nition, the nth shatter coe   cient sa(n) is equal to 2n if there exists a set {z1, z2, ..., zn}
that a shatters. the largest of such sets is precisely the vapnik-chervonenkis or vc di-
mension.

de   nition: the vapnik-chervonenkis dimension, or vc-dimension of
integer d such that sa(d) = 2 . we write

(a) = d.

vc

d

a is the largest

28if sa(n) = 2n for all positive integers n, then vc(a) :=    

in words, a shatters some set of points of cardinality d but shatters no set of points of
cardinality d + 1. in particular, a also shatters no set of points of cardinality d    > d so that
the vc dimension is well de   ned.

in the sequel, we will see that the vc dimension will play the role similar to of cardinality,
but on an exponential scale. for interesting classes a such that card(a) =    , we also may
have vc(a) <    . for example, assume that a is the class of half-lines, a = {(      , a], a    
ir}     {[a,    ), a     ir}, which is clearly in   nite. then, we can clearly shatter a set of size
2 but we for three points z1, z2, z3,     ir, if for example z1 < z2 < z3, we cannot create the
pattern (0, 1, 0) (see figure 4.1). indeed, half lines can can only create patterns with zeros
followed by ones or with ones followed by zeros but not an alternating pattern like (0, 1, 0).

00

10

01

11

000

100

110

111

001

011

101

figure 1:
if a = {hal   ines}, then any set of size n = 2 is shattered because we can
create all 2n = 4 0/1 patterns (left); if n = 3 the pattern (0, 1, 0) cannot be reconstructed:
sa(3) = 7 < 23 (right). therefore, vc(a) = 2.

4.2 the vc inequality

we have now introduced all the ingredients necessary to state the main result of this section:
the vc inequality.

theorem (vc inequality): for any family of sets a with vc dimension vc(a) = d,
it holds

ie sup |  n(a)       (a)|     2

a   a

n

r 2d log(2en/d)

note that this result holds even if a is in   nite as long as its vc dimension is    nite. moreover,
(cid:1)
observe that log(|a|) has been replaced by a term of order d log 2en/d .

(cid:0)

to prove the vc inequality, we proceed in three steps:

291. symmetrization, to bound the quantity of interest by the rademacher complexity:

ie[ sup |  n(a)       (a)|]     2rn(

)
a .

a   a

we have already done this step in the previous lecture.

2. control of the rademacher complexity using shatter coe   cients. we are going to

show that

r (a)    

n

s 2 lo
g

(cid:0)2sa(n)
(cid:1)
n

3. we are going to need the sauer-shelah lemma to bound the shatter coe   cients by

the vc dimension. it will yield

s (n)     (cid:16) en(cid:17)d

a

d

,

(
d = vc
a) .

put together, these three steps yield the vc inequality.

step 2: control of the rademacher complexity

we prove the following lemma.

lemma: for any b     irn, such that |b| <     :, it holds

rn(

b

n

) = ie max(cid:12) x

(cid:12) 1
b   b n

(cid:2)

i=1

   (cid:12)
(cid:3)
ibi(cid:12)     max

b   b

|b|2

p

2

)
|

log 2 b

|

(
n

where |    |2 denotes the euclidean norm.

proof. note that

rn(b) = ie

1

(cid:12)(cid:12)
n (cid:2) max zb

b   b

(cid:3)
|

,

where zb = pn
implies that the moment generating function of zb is controlled by

i=1   ibi. in particular, since

   |bi|       i|bi|     |bi|, a.s., hoe   ding   s lemma

ie(cid:2) exp(szb)(cid:3) = y ie

n

(cid:2) exp(s  ibi)(cid:3)     y exp(s2b2

i /2) = exp(s2 b 2
| |

2/2)

n

(4.2)

i=1

i=1

b = b    (cid:12)(cid:12)
(cid:2)
next, to control ie maxb   b zb| , we use the same technique as in lecture 3, section 1.5.
to that end, de   ne
ie(cid:20)

max |zb|(cid:21) = ie(cid:20)max zb(cid:21) = log exp(cid:18)sie(cid:20)max zb(cid:21)(cid:19) 1

} and observe that for any s > 0,

    log ie exp s max zb

(cid:3)
{   b

(cid:19)(cid:21)

(cid:18)

(cid:20)

  

,

b      b

s

b      b

b   b

b   b  

1
s

where the last inequality follows from jensen   s inequality. now we bound the max by a
sum to get

ie max |zb|     log

ie [exp(szb)]    

(cid:20)

b   b

(cid:21)

1 x
s

b   b  

log |   b|

s

+

s b 2
| |2 ,
2n

where in the last inequality, we used (4.2). optimizing over s > 0 yields the desired
result.

30we apply this result to our problem by observing that

rn(a) = sup
,

z1,... zn

rn(t z))

(

where t (z) is de   ned in (4.1).
|2     n
for all b     t (z). moreover, by de   nition of the shatter coe   cients, if b = t (z), then
|   b|     2|t (z)|     2sa(n). together with the above lemma, it yields the desired inequality:

in particular, since t (z)     {0, 1}, we have |b

   

rn(a)    

r 2 log(2sa(n))

n

.

step 3: sauer-shelah lemma

we need to use a lemma from combinatorics to relate the shatter coe   cients to the vc
dimension. a priori, it is not clear from its de   nition that the vc dimension may be at
all useful to get better bounds. recall that steps 1 and 2 put together yield the following
bound

ie[ sup
a

a   

|  

n(a)

      (a) ]

|    

2r

2 log(2s (n))

a

n

(4.3)

in particular, if sa(n) is exponential in n, the bound (4.3) is not informative, i.e., it does
not imply that the uniform deviations go to zero as the sample size n goes to in   nity. the
vc inequality suggest that this is not the case as soon as vc(a) <     but it is not clear a
sa(n) = 2n for n     d and sa(n) = 2n     1 for n > d,
priori. indeed, it may be the case that
which would imply that
(a) = d <     but that the right-hand side in (4.3) is larger than
2 for all n. it turns our that this can never be the case: if the vc dimension is    nite, then
the shatter coe   cients are at most polynomial in n. this result is captured by the sauer-
shelah lemma, whose proof is omitted. the reading section of the course contains pointers
to various proofs, speci   cally the one based on shifting which is an important technique in
enumerative combinatorics.

vc

lemma (sauer-shelah): if vc(a) = d, then    n     1,

d

sa(n)     x(cid:18)n

(cid:19)    

k

k=0

(cid:16)

(cid:17)
en d
d

.

together with (4.3), it clearly yields the vc inequality. by applying the bounded di   erence
inequality, we also obtain the following vc inequality that holds with high id203. this
is often the preferred from for this inequality in the literature.

corollary (vc inequality): for any family of sets a such that vc(a) = d and any
       (0, 1), it holds with id203 at least 1       ,

|

sup   n a)       (a)|     2
a   a

(

r 2d log(2en/d) r log(2/  )

n

+

.

2n

31note that the logarithmic term log(2en/d) is actually super   uous and can be replaced
by a numerical constant using a more careful bounding technique. this is beyond the scope
of this class and the interested reader should take a look at the recommending readings.

4.3 application to erm

the vc inequality provides an upper bound for supa   a |  n(a)       (a)| in terms of the vc
dimension of the class of sets a. this result translates directly to our quantity of interest:

  

sup |rn h)    
h   h

(

s
2

r(h)
|

   

2vc(

a

) log

n

(cid:0)

2en

vc(a)

(cid:1)

r

+

log(2/  

)

2n

(4.4)

where a = {ah : h     h} and ah = {(x, y)     x    {0, 1} : h(x) = y}. unfortunately, the
vc dimension of this class of subsets of x    {0, 1} is not very natural. since, a classi   er h
is a {0, 1} valued function, it is more natural to consider the vc dimension of the family
a = (cid:8){h = 1} : h     h(cid:9).

de   nition: let h be a collection of classi   ers and de   ne

a   = {h = 1} : h     h

(cid:8)

(cid:9) = (cid:8)a :     h     h, h(  ) = 1i(       a) .
(cid:9)
  
a.

f h to be the vc dimension of

we de   ne the vc d

imension vc(

h

) o

it is not clear how vc(a) relates to the quantity vc(a), where a = {ah : h     h} and
ah = {(x, y)     x    {0, 1} : h(x) = y} that appears in the vc inequality. fortunately, these
two are actually equal as indicated in the following lemma.

  

  

lemma: de   ne the two families for sets:
h : h
ah = (x, y)     x    {0, 1} : h(x) = y} and a = (cid:8){h = 1 : h
(cid:9)
  
}
vc a  
).
) = vc(a

then, a  (n) = a  (n) for all n

1. it implies

    h}    
    h    

= a

a

   

s

s

{

{

(

x   
2
2x .

{0,1} where

proof. fix x = (x1, ..., xn)     x n and y = (y1, y2, ..., yn)     {0, 1}n and de   ne

t (x, y) = {(1i(h(x1) = y1), . . . , 1i(h(xn) = yn)), h     h}

and

  t (x) = {(1i(h(x1) = 1), . . . , 1i(h(xn) = 1)), h     h}

to that end,    x v     {0, 1} and recall the xor (exclusive or) boolean function from {0, 1}
to {0, 1} de   ned by u     v = 1i(u = v). it is clearly1 a bijection since (u     v)     v = u.

1one way to see that is to introduce the    spinned    variables u   = 2u     1 and v   = 2v     1 that live in
{   1, 1}. then u     v = u      v  , and the claim follows by observing that (u      v  )    v   = u  . another way is to simply
write a truth table.

^

326
6
6
6
6
6
when applying xor componentwise, we have

    1i(h(x1) = y1)
   
   
      
      
   

..
.
1i(h(xi)
..
.

1i(h(xn) = yn)

= yi)

)

..
.

   
    1i(h(x1 = 1)
      
   
         
   
1i(h(xi) = 1)
       
          
        1i(h(xn) = 1)

=

.
.
.

   
   
      
   
   
      

   

1

   

               

y
...
yi
..
   
    .
yn

   

               

   
   

since xor is a bijection, we must have card[t (x, y)] = card[t (x)]. the lemma follows

  

by taking the supremum on each side of the equality.

it yields the following corollary to the vc inequality.

corollary: let h be a family of classi   ers with vc dimension d. then the empirical
risk classi   er herm over h satis   es

  

  r(h

erm

with id203 1       .

)     min r(h) + 4

h   h

r 2d log(2en/d)

n

r log(2/  )

2n

+

proof. recall from lecture 3 that

  r(herm)     min
h   h

r(h

)    

2 sup
h   h

  

(cid:12)(cid:12)rn(h)     r(h)(cid:12)
(cid:12)

the proof follows directly by applyi

ng (4.4) and the above lemma.

336
6
6
18.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: ali makhdoumi

lecture

6
sep. 28, 2015

5. learning with a general id168

in the previous lectures we have focused on binary losses for the classi   cation problem and
developed vc theory for it. in particular, the risk for a classi   cation function h : x     {0, 1}
and binary id168 the risk was

r(h) = ip(h(x) = y ) = ie[1i(h(x) = y )].

in this lecture we will consider a general id168 and a general regression model where
y is not necessarily a binary variable. for the binary classi   cation problem, we then used
the followings:

    hoe   ding   s inequality: it requires boundedness of the id168s.

    bounded di   erence inequality: again it requires boundedness of the id168s.

    vc theory: it requires binary nature of the id168.

limitations of the vc theory:

    hard to    nd the optimal classi   cation: the empirical risk minimization optimization,

i.e.,

1

min

h n x 1i(h(xi) = yi)

n

i=1

is a di   cult optimization. even though it is a hard optimization, there are some
algorithms that try to optimize this function such as id88 and adaboost.

    this is not suited for regression. we indeed know that classi   cation problem is a
subset of regression problem as in regression the goal is to    nd ie[y |x] for a general
y (not necessarily binary).

in this section, we assume that y     [   1, 1] (this is not a limiting assumption as all the
results can be derived for any bounded y ) and we have a regression problem where (x, y )    
x    [   1, 1]. most of the results that we preset here are the analogous to the results we had
in binary classi   cation. this would be a good place to review those materials and we will
refer to the techniques we have used in classi   cation when needed.

5.1 empirical risk minimization

5.1.1 notations

id168: in binary classi   cation the id168 was 1i(h(x) = y ). here, we
replace this id168 by    (y, f (x)) which we assume is symmetric, where f     f,
f : x     [   1, 1] is the regression functions. examples of id168 include

346
6
6
6
       (a, b) = 1i(a = b) ( this is the classi   cation id168).

       (a, b) = |a     b|.

       (a, b) = (a     b)2.

       (a, b) = |a     b|p, p     1.

we further assume that 0        (a, b)     1.
risk: risk is the expectation of the id168, i.e.,

r(f ) = iex,y [   (y, f (x))],

where the joint distribution is typically unknown and it must be learned from data.
data: we observe a sequence (x1, y1), . . . , (xn, yn) of n independent draws from a joint
distribution px,y , where (x, y )     x    [   1, 1]. we denote the data points by dn =
{(x1, y1), . . . , (xn, yn)}.
empirical risk: the empirical risk is de   ned as

  rn(f ) =

n

1

n x    (yi, f (xi)),

i=1

and the empirical risk minimizer denoted by f erm (or f ) is de   ned as the minimizer of
empirical risk, i.e.,

  

  

argmin rn(f ).

  

f    f

in order to control the risk of f we shall compare its performance with the following oracle:

  

  f     argmin r(f ).

f    f

note that this is an oracle as in order to    nd it one need to have access to pxy and then
optimize r(f ) (we only observe the data dn). since f is the minimizer of the empirical
risk minimizer, we have that rn(f )     rn(f ), which leads to

  

  

  

  

  

  r(f )    

   

  

  

  r(f )       

  
rn(f ) + rn(f )     rn(f ) + rn(f )     r(f ) + r(f )
  
|rn(f )     r(f )|.

r(f ) + r(f )     rn(f ) + rn(f )     r(f )     r(f ) + 2 sup
f    f

  

  

  

  

  

  

  

  

  

  

  

  

  

  

therefore, the quantity of interest that we need to bound is

sup |   rn(f )     r(f )
|.
f    f

moreover, from the bounded di   erence inequality, we know that since the id168    (  ,   )
is bounded by 1, supf    f |rn(f )     r(f )| has the bounded di   erence property with ci = 1
n
for i = 1, . . . , n, and the bounded di   erence inequality establishes
    exp(cid:18)

(cid:19) = e p
x

rn(f )     r(f ) |     ie

p"

#
|

    t

"

#

  

  

(cid:0)

(cid:1)
   2nt2 ,

sup |rn(f )     r(f )
f    f

sup |   
f    f

2t2
2
c
i

i

   
p

which in turn yields

|   

sup rn(f )     r(f )|     i
e
f    f

"

#
|
sup rn(f )     r(f )
f
   

|   

f

r

+

log (1/delta)

2n

, w.p. 1

  
    .

as a result we only need to bound the expectation ie[supf    f |rn(f )     r(f )|].

  

356
5.1.2 symmetrization and rademacher complexity

similar to the binary loss case we    rst use symmetrization technique and then intro-
duce rademacher random variables. let dn = {(x1, y1), . . . (xn, yn)} be the sample set
and de   ne an independent sample (ghost sample) with the same distribution denoted by
d   
i ) is independent from dn with the same
distribution as of (xi, yi)). also, let   i     {   1, +1} be i.i.d. rad( 1 ) random variables
independent of dn and d   

n)}( for each i, (x    

1), . . . (x    

n = {(x    

n, y    

1, y    

i, y    

2

ie

n. we have
n

1

sup |

"
f    f n x (yi, f (x ))
"

i=1

1

   

n

i

= ie

|

sup
f    f

"

= ie

sup |ie
f    f

"

(a)
    ie

sup ie
f    f

   

n

i=1

i))

   (yi, f (x

n x
" x    (yi, f (x
"
1
| x    (yi, f (x
n

i=1
n

1
n

i=1

#
    ie [   (yi, f (xi))] |

"

ie

n

1

n x

i=1

   (y    

i , f (x    

i)) dn

|

   

   (yi , f (x    

i)) dn

|

#

#
|

#

#
|

i))

   

i))     x    (y , f (x    

   
i

i))| |dn

##

n

1

n x

i=1
n

1
n

i=1

n

"
1
sup | x
f    f n
"

i=1
n

    ie

(b)
= ie

   (yi, f (xi))

n

    x

1
n

i=1

   (y    

i , f (x    

#
|
i))

#
(cid:1) |

   
i))

1
sup | x   i (cid:0)   (yi, f (xi))        (y     x
i , f (
f f n
   
"

i=1
n

1

|

n x   i   (yi, f (xi))

#
|

sup
f    f

i=1

(c)
    2ie

    2 sup ie

dn

n

#
"
1
sup | x   i   (yi, f (xi))
|
f    f n

.

i=1

where (a) follows from jensen   s inequality with convex function f (x) = x , (b) follows from
the fact that (x , y ) and (x    
i, yi ) has the same distributions, and (c) follows from triangle
inequality.
rademacher complexity: of a class f of functions for a given id168    (  ,   ) and
samples dn is de   ned as

|

|

i

i

   

rn(        f) = sup ie

dn

n

"
#
1
sup | x   i   (yi, f (xi))
|
f    f n

i=1

.

therefore, we have
"
1
sup | x    (yi, f (xi))
f    f n

ie

n

i=1

#
    ie[   (yi, f (xi))]|

    2rn(        f)

and we only require to bound the rademacher complexity.

5.1.3 finite class of functions

suppose that the class of functions f is    nite. we have the following bound.

36theorem: assume that f is    nite and that     takes values in [0, 1]. we have

rn(        f)

   

r 2 log(2
n

|f|)

.

proof. from the previous lecture, for b     nr , we have that

rn(b) = ie

"

max
b   b

|

n

1

n x   ibi

i=1

#
|

    max
b   b

|b|2

p

|
2 log(2 b )

|

n

.

here, we have

b =

         (y
      
      

   
   

(x

1))

1, f
.
.
.

   (yn, f (xn )
)

   
    f     f
   

,

   
   
    .
      

   

since     takes values in [0, 1], this
previous inequality completes the proof.

im

plies b

    {b : |b|2

   

n}. plugging this bound in the

5.2 the general case

  erm

recall that for the classi   cation problem, we had f     {0, 1}x . we have seen that the
cardinality of the set {(f (x1), . . . , f (xn)), f
    f} plays an important role in bounding the
risk of f
(this is not exactly what we used but the xor argument of the previous lecture
allows us to show that the cardinality of this set is the same as the cardinality of the set
that interests us). in this lecture, this set might be uncountable. therefore, we need to
introduce a metric on this set so that we can treat the close points in the same manner. to
this end we will de   ne covering numbers (which basically plays the role of vc dimension
in the classi   cation).

5.2.1 covering numbers

de   nition: given a set of functions f and a pseudo metric d on f ((f, d) is a metric
space) and    > 0. an   -net of (f, d) is a set v such that for any f     f, there exists
g     v such that d(f, g)       . moreover, the covering numbers of (f, d) are de   ned by

n (f, d,   ) = inf{|v | : v is an   -net}.

for instance, for the f shown in the figure 5.2.1 the set of points {1, 2, 3, 4, 5, 6} is a
covering. however, the covering number is 5 as point 6 can be removed from v and the
resulting points are still a covering.

de   nition: given x = (x1, . . . , xn), the conditional rademacher average of a class of

371

2

f

6

3

  

5

4

functions f is de   ned as

r   x

n = ie

"

sup
f    f

n

1

#(cid:12)(cid:12)
(cid:12)(cid:12) n x if (xi)

  

i=1

.

note that in what follows we consider a general class of functions f. however, for
applying the results in order to bound empirical risk minimization, we take xi to be (xi, yi)
and f to be         f. we de   ne the empirical l1 distance as

dx
1(f, g) =

n

1

n x i

|f (x )

i

=1

    g(xi)|.

theorem: if 0     f     1 for all f     f, then for any x = (x1, . . . , xn), we have

  rx

n(f)     inf
     0

(cid:8)
   +

r

x
2 log (2n (f, d
1 ,   ))

n

(cid:9).

proof. fix x = (x1, . . . , xn) and    > 0. let v be a minimal   -net of (f, dx
by de   nition we have that |v | = n (f, dx

1 ). thus,
1 ,   ). for any f     f, de   ne f         v such that

38dx
1(f, f    )       . we have that

rx

n(f) = ie

"

sup
f    f

|

#
|
  if (xi)

n

1

n x

i=1

    ie

n

"
1
sup | x   i(f (xi)
f    f n

i=1

"

       + ie

max
f    v

|

n

1

n x

i=1

#
|
  if (xi)

   

f    (xi)) + ie sup

#
|

"

n

1
| x
f    f n

i=1

#
  if    (xi)
|

       +

=    +

r 2 log(2
n

|v |)

r 2 log(2n (
f, dx
n

1 ,   ))

.

since the previous bound holds for any   , we can take the in   mum over all        0 to obtain

x

rn(f)     inf    +

     0

r(cid:8)

2 log(2n (f, dx

1 ,   ))

n

(cid:9).

the previous bound clearly establishes a trade-o    because as    decreases n (f, dx
creases.

1 ,   ) in-

5.2.2 computing covering numbers

as a warm-up, we will compute the covering number of the    2 ball of radius 1 in dr denoted
by b2. we will show that the covering is at most ( 3
   )d. there are several techniques to
prove this result: one is based on a probabilistic method argument and one is based on
greedily    nding an   -net. we will describe the later approach here. we select points in v
one after another so that at step k, we have uk     b2 \    k
j=1b(uj,   ). we will continue this
procedure until we run out of points. let it be step n . this means that v = {u1, . . . , un }
is an   -net. we claim that the balls b(ui,    ) and b(uj,    ) for any i, j
    { , . . . , n } are
disjoint. the reason is that if v     b(ui,    )     b(uj,    ), then we would have

1

2

2

2

2

kui     ujk2     kui     vk2 + kv     ujk2     + =   ,

  
2

  
2

which contradicts the way we have chosen the points. on the other hand, we have that
j=1b(uj,    )     (1 +    )b2. comparing the volume of these two sets leads to
   n

2

2

|v |( )dvol(b2)     (1 + )dvol(b2) ,

  
2

  
2

where vol(b2) denotes the volume of the unit euclidean ball in d dimensions. it yields,

|v |     (cid:0)1 +    d
(cid:1)
(cid:1)d

(cid:0)   

2

2

=

(cid:18)

2
  

d

(cid:19)
+ 1

    (cid:18)

(cid:19)
3 d
  

.

39for any p     1, de   ne

and for p =    , de   ne

dx
p(f, g) =

  n

1
n

i=1

x |f (xi)

   

g(x ) p
|

i

1

p

!

,

dx
|.
   (f, g) = max |f (xi)     g(xi)

i

  
using the previous theorem, in order to bound rx
with dx
norm. in order to show this, we will compare the covering number of the norms dx
(cid:0) 1
n pn
bound on n (f, dx

n we need to bound the covering number
1 norm. we claim that it is su   cient to bound the covering number for the in   nity-
p(f, g) =
   ,   ) implies a

i)     g(xi)| (cid:1) p for p     1 and conclude that a bound on n (f, dx

i=1 |f (x

p ,   ) for any p     1.

p

1

proposition: for any 1     p     q and    > 0, we have that

n (f, dx

p ,   )     n (f, dx

q ,   ).

proof. first note that if q =    , then the inequality evidently holds. because, we have

1
( x 1
n

|zi|p) p     max

i

|zi|,

n

i=1

which leads to b(f, dx
1     p     q <    . using h  older   s inequality with r = q

   ,   )     b(f, dx

p,   ) and n (f, d   ,   )     n (f, dp,   ). now suppose that

p     1 we obtain

 

n

1

n x

i=1

! 1

p

|z |p

i

n

 

1

       n p x

i 1
=

   

!(1 1 ) 1  
1

r p

n

x

i=1

! 1

pr

 

=

|zi|pr

! 1

q

.

|zi|q

1
n

n

x

i

=1

this inequality yeilds

b(f, dx

q ,   ) = {g : dx

q (f, g)       }     b(f, dx

p,   ),

which leads to n (f, dq,   )     n (f, dp,   ).

using this propositions we only need to bound n (f, dx

   ,   ).

let the function class be f = {f (x) = hf, xi, f     bd, x     bd}, where 1
p

p

q

+ = 1. this

1
q

leads to |f |     1.
claim: n (f, dx
this leads to

   ,   )     ( 2 )d.

  

x

  rn(f)     inf
{   +
  >
0

r 2d log(4/  )

n

.
}

taking    = o(q d log n ), we obtain

n

n(f)     o(r log n
  rx

d

n

).

4018.657: mathematics of machine learning

lecturer: philippe rigollet
scribe: zach izzo

lecture

7
sep. 30, 2015

in this lecture, we continue our discussion of covering numbers and compute upper
n(f). we then discuss chaining and

bounds for speci   c conditional rademacher averages rx
  
conclude by applying it to learning.

recall the following de   nitions. we de   ne the risk function

r(f ) = ie[(cid:96)(x, f (x))],

(x, y )     x    [   1, 1] ,

for some id168 (cid:96)(  ,  ). the conditiona rademacher average that we need to control
is

r((cid:96)l     f) =

sup

(x1,y1),...,(xn,yn)

ie sup
   f

f

  i(cid:96)(yi, f (xi))

.

(cid:12)(cid:12)(cid:35)
(cid:12)(cid:12)
(cid:12)

furthermore, we de   ned the conditional rademacher average for a poin
to be

t x = (x1, . . . , xn)

(cid:88)

n1

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n
(cid:88)

(cid:34)
(cid:12)(cid:12)
(cid:12)(cid:12)
(cid:12)

(cid:12)(cid:12)(cid:35)
(cid:12)(cid:12)
(cid:12)

  if (xi)

.

(cid:34)

n1
sup
f   f
n
f, d,
lastly, we de   ne the   -covering number n (
respect to the metric d) of radius    needed to

r   x
n(f) = ie

i=1

  ) to be the m

inimum number of balls (with
f. we proved the following theorem:

cover

theorem: assume |f|     1 for all f     f. then

(cid:40)

(cid:114)

r   x
n(f)

    inf

  >0

  

+

f
2 log(2n (
n

, dx,   ))

1

(cid:41)

,

where dx

1 is given by

(cid:88)

n

i=1

1
n

dx
1(f, g) =

|f (xi)     g(xi)|.

we make use of this theorem in the following example. de   ne bd
then take f (x) = (cid:104)a, x(cid:105), set f = {(cid:104)a,  (cid:105) : a     bd }, and x = bd
we have

   
|f (x)|     |a| x   |
|1     1,

p = {x     ird : |x|p     1}.
1 . by h  older   s inequality,

so the theorem above holds. we need to compute the covering number n (f, dx
that for all a     bd , there exists v = (v1, . . . , vn) such that vi = g(xi) and

   

1,   ). note

(cid:88)

n1
n

i=1

|(cid:104)
a, xi

(cid:105)     |    

vi

  

for some function g. for this case, we will take g(x) = (cid:104)b, x(cid:105), so vi = (cid:104)b, xi(cid:105). now, note the
following. given this de   nition of g, we have

(cid:88)

n

i=1

(cid:105)| (cid:88)

n

1
n

i=1

dx
1(f, g) =

1
n

|(cid:104)
a, x1

(cid:105)     (cid:104)

b, xi =

|(cid:104)    
a

b, xi

(cid:105)|     |     |   

a

b

41by h  older   s inequality and the fact that |x|1 = 1. so if |a    b|          , we can take vi = (cid:104)b, xi(cid:105).
we just need to    nd a set of {b1, . . . , bm}     ird such that, for any a there exists bj such
that |a     bj| <    . we can do this by dividing bd into cubes with side length    and
taking the b    s to be the set of vertices of these cubes. then any a     bd
   
must land in one
of these cubes, so |a     bj|        as desired. there are c/  d of such b
j   s for some constant
c > 0. thus

   
j

   

   

we now plug this value into the theorem to obtain

n (bd , dx    1,   )     c/  d.

(cid:40)

(cid:114)

r   x
n(f)     inf
   0
  

   +

2 log(c/  d)

n

(cid:41)

.

optimizing over all choices of    gives

(cid:114)

      = c

d log(n)

n

   

(cid:114)
n(f)     c
r   x

d log(n)

n

.

note that in this    nal inequality, the conditional empirical risk no longer depends on
x, since we    sup   d    x out of the bound during our computations. in general, one should
ignore x unless it has properties which will guarantee a bound which is better than the sup.
another important thing to note is that we are only considering one granularity of f in our
   nal result, namely the one associated to      . it is for this reason that we pick up an extra
log factor in our risk bound. in order to remove this term, we will need to use a technique
called chaining.

5.4 chaining

we have the following theorem.

theorem: assume that |f|     1 for all f     f. then

(cid:26)

12 (cid:90) 1

(cid:113)

n

  

r   x
n     inf

  >0

   

4   +

f
log(n (

(cid:27)

, dx ))dt

2, t

.

(note that the integrand decays with t.)

proof. fix x = (x1, . . . , xn), and for all j = 1, . . . , n , let vj be a minimal 2   j-net of f
2 metric. (the number n will be determined later.) for a    xed f     f, this
under the dx
process will give us a    chain    of points fi

    which converges to f : dx
de   ne f = {(f (x1), . . . , f (xn))(cid:62), f     f}     [   1, 1]n. note that

   , f )     2   j.

2(fi

n(f) = ie sup
r   x
1
n f   f

(cid:104)  , f(cid:105)

where    = (  1, . . . ,   n). observe that for all n , we can rewrite (cid:104)  , f(cid:105) as a telescoping sum:

(cid:104)  , f(cid:105) = (cid:104)  , f     fn

    (cid:105) + (cid:104)  , fn

        fn
   

   1(cid:105) + . . . + (cid:104)  , f1

        f0
   (cid:105)

42where f0

    := 0. thus

r   x
n(f)     ie sup |(cid:104)  , f     fn

    (cid:105)| +

1
n f   f

(cid:88)

n

j=1

1
n

|(cid:104)
e sup   ,
i

   
f f

   
j

f

   

   
   1
fj

(cid:105)|

.

we can control the two terms in this inequality separately. note    rst that by the cauchy-
schwarz inequality,

since |  |2 =

   

n and dx

2(f, fn

ie sup |(cid:104)  , f     fn
1
n f
    )     2   n , we have

   f

    (cid:105)|     |  |2

    )
dx
   
2(f, fn

.

n

(cid:105)|        n .

2

ie sup |(cid:104)  , f     fn
   
1
n f   f
(cid:88) 1

n

now we turn our attention to the second term in the inequality, that is

(cid:112)2 log(2

|b|)

s =

ie sup |(cid:104)  , fj
n f
        vj and fj
   1     vj
   1, there are at most | j||vj   1| possible di   erences
   
note that since fj
j|/2, |vj||vj 1|     |vj| /2 and we    nd ourselves in the    nite
j 1|     |v
        fj
.   1 since |v
   
   
fj
dictionary case. we employ a risk bound from earlier in the course to obtain the inequality

        fj
   1(cid:105)|.
   

   f

j=1

   

v

2

in the present case, b = {fj

|b|2

rn(b)     max
   
b b
(cid:113)
   1 , f     f} so that |b|     |vj|2/2. it yields
        fj
   

n

.

(cid:112)

|2

|vj
2
2

= 2r

  

|
vj

|

,

log
n

)

2 log(

  

    r

r
n(b)
        fj
   1|2. next, observe that
   
   
    n(dx
   x
2(fj
n d

2(fj

   

n

where r = supf   f |fj
1|2 =
|fj
        fj
   
   

   , f ) + dx
by the triangle inequality and the fact that dx(f   , f )     2   j
bound for rn(b), we have

2

j

   , fj
   
   1)
(cid:114)

log

(cid:114)

r

n

(b)

      

6 2   j

||

      
2 j

= 6

|vj
n

log(n f dx

(

2, 2   j

,
n

))

since v

j was chosen to be a minimal 2    -net.

j

the proof is almost complete. note that 2   j = 2(2   j     2   j   1) so that

2   j(cid:113)

log(n (f, dx

2, 2   j)) =     (cid:88)

n

12
n

j=1

(cid:113)

next, by comparing sums and integrals (figure 1), we see that

(cid:113)

(cid:90) /2

1

(cid:113)

2   (n +1)

(2   j

    2   j   1)

log(n (f, dx

2, 2   j))    

log(n (

f

, dx

2, t))dt.

n

n6    (cid:88)
(cid:88)n

j=1

j=1

(2   j     2   j   1)

log(n (f, dx

2, 2   j)) .

   1       
   
2(f, fj

3 2   j n .

))

   

. substituting this back into our

43figure 1: a comparison of the sum and integral in question.

so we choose n such that 2   (n +2)            2   (n +1), and by combining our bounds we obtain

(cid:90)

(cid:113)

r   x
n(f

)     2   n

+    
12
n

1/2

2   ( +1)

n

log(n (f, dx

2, t))dt

    4   +

(cid:90) (cid:112)log(n,

1

  

f, t)dt

since the integrand is non-negative. (note: this integral is known as the    dudley id178
integral.   )

returning to our earlier example, since n (f, dx

r   x
n(f)     inf

since(cid:82) 1(cid:112)log(c/t)dt = c   is    nite, we then have

  >0

0

(cid:26)

1

4   +    
12

log((c(cid:48)/t)d)dt

(cid:27)
(cid:90) (cid:113)
2,   )     c/  d, we have
n(f)     12c  (cid:112)d/n.

n   

.

r   x

using chaining, we   ve been able to remove the log factor!

5.5 back to learning

we want to bound

rn((cid:96)     f) =

(cid:34)

ie sup
f   f

(cid:12)(cid:12)(cid:12)
(cid:12)
(cid:12)

(cid:88)

n1
n

i

=1

(cid:12)(cid:12)(cid:3)

(x1,y1),...,(xn,yn)

sup

(cid:2)supf

(cid:12)(cid:12) 1(cid:80)

(cid:12)(cid:12)(cid:35)(cid:12)
(cid:12)(cid:12)

  i(cid:96)(yi, f (xi))

.

    f (x )
we consider
  , that is |  (a)       (b)|     l|a     b| for all a, b     [   1, 1]. we

r  
n(       f

n
i=1   

) = ie

   f

i  

n

x

i

for some l
have the following lemma.

-lipschitz function

44theorem: (contraction inequality) let    be l-lipschitz and such that   (0) = 0,
then

r   x
n(       f)    

2l   

r  

x

n(f) .

the proof is omitted and the interested reader should take a look at [lt91, kol11] for
example.
of r-valued id168s, for example (cid:96)(y,  ) = (y       )2.

as a    nal remark, note that requiring the id168 to be lipschitz prohibits the use

4518.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: quan li

lecture

8
oct. 5, 2015

part ii
convexity

1. convex relaxation of the empirical risk minimization

in the previous lectures, we have proved upper bounds on the excess risk r(herm)     r(h   )
of the empirical risk minimizer

  

  herm

= argmin

h   h

n

1

n x

i=1

1i(yi = h(xi)).

(1.1)

however due to the nonconvexity of the objective function, the optimization problem
(1.1) in general can not be solved e   ciently. for some choices of h and the classi   cation
error function (e.g. 1i(  )), the optimization problem can be np-hard. however, the problem
we deal with has some special features:

1. since the upper bound we obtained on the excess risk is o(q d log n ), we only need to

approximate the optimization problem with error up to o(q d log n

).

n

n

2. the optimization problem corresponds to the average case problem where the data

(xi, yi)     px,y .

i.i.d

3. h can be chosen to be some    natural    classi   ers, e.g. h = {half spaces}.

these special features might help us bypass the computational issue. computational
issue in machine learning have been studied for quite some time (see, e.g. [kea90]), especially
in the context of pac learning. however, many of these problems are somewhat abstract
and do not shed much light on the practical performance of machine learning algorithms.

to avoid the computational problem, the basic idea is to minimize a convex upper bound
of the classi   cation error function 1i(  ) in (1.1). for the purpose of computation, we shall
also require that the function class h be a convex set. hence the resulting minimization
becomes a id76 problem which can be solved e   ciently.

1.1 convexity

de   nition: a set c is convex if for all x, y     c and        [0, 1],   x + (1       )y     c.

466
de   nition: a function f : d     ir on a convex domain d is convex if it satis   es

f (  x + (1       )y)       f (x) + (1       )f (y),

   x, y     d, and        [0, 1].

1.2 convex relaxation

the convex relaxation takes three steps.

step 1: spinning.

using a mapping y 7    2y     1, the i.i.d. data (x1, y1), (x2, y2), . . . , (xn, yn) is transformed
to lie in x    {   1, 1}. these new labels are called spinned labels. correspondingly, the task
becomes to    nd a classi   er h : x 7    {   1, 1}. by the relation

we can rewrite the objective function in (1.1) by

h(x) 6= y        h(x)y > 0,

n

1 x
n

i=1

1i(h(xi) = yi) = x   1i( h

    (xi)yi)

1
n

n

i=1

(1.2)

where   1i(z) = 1i(z > 0).

step 2: soft classi   ers.

the set h of classi   ers in (1.1) contains only functions taking values in {   1, 1}. as a result,
it is non convex if it contains at least two distinct classi   ers. soft classi   ers provide a way
to remedy this nuisance.

de   nition: a soft classi   er is any measurable function f : x     [   1, 1]. the hard
classi   er (or simply    classi   er   ) associated to a soft classi   er f is given by h = sign(f ).

let f     irx be a convex set soft classi   ers. several popular choices for f are:

    linear functions:

f := {ha, xi : a     a}.

for some convex set a     ird. the associated hard classi   er h = sign(f ) splits ird into
two half spaces.

    majority votes: given weak classi   ers h1, . . . , hm ,

m

f := nx   jhj(x) :   j

m

    0, x   j = 1o.

j=

1

j=1

    let   j, j = 1, 2, . . . a family of functions, e.g., fourier basis or wavelet basis. de   ne

   

f := {x   j  j(x) : (  1,   2, . . .)

      },

j=1

where    is some convex set.

476
step 3: convex surrogate.

given a convex set f of soft classi   ers, using the rewriting in (1.2), we need to solve that
minimizes the empirical classi   cation error

min
f    f

n

1

n x

i=1

  1i( f (xi)yi),

   

however, while we are now working with a convex constraint, our objective is still not
convex: we need a surrogate for the classi   cation error.

de   nition: a function    : ir 7    ir+ is called a convex surrogate if it is a convex
non-decreasing function such that   (0) = 1 and   (z)       1i(z) for all z     ir.

the following is a list of convex surrogates of id168s.

    hinge loss:   (z) = max(1 + z, 0).

    exponential loss:   (z) = exp(z).

    logistic loss:   (z) = log2(1 + exp(z)).

to bypass the nonconvexity of   1i(  ), we may use a convex surrogate   (  ) in place of

  1i(  ) and consider the minimizing the empirical   -risk rn,   de   ned by

  

  rn,  (f ) =

1
n

n

x

i=1

  (   yif (xi))

it is the empirical counterpart of the   -risk r   de   ned by

r  (f ) = ie[  (   y f (x))].

1.3   -risk minimization

in this section, we will derive the relation between the   -risk r  (f ) of a soft classi   er f and
the classi   cation error r(h) = ip(h(x) = y ) of its associated hard classi   er h = sign(f )

let

f    
   = argmin e[  ( y

    f (x))]

f    irx

where the in   mum is taken over all measurable functions f : x     ir.

to verify that minimizing the    serves our purpose, we will    rst show that if the convex
  (x))     0 is equivalent to   (x)     1/2 where

surrogate   (  ) is di   erentiable, then sign(f    
  (x) = ip(y = 1 | x). conditional on {x = x}, we have

ie[  (   y f (x)) | x = x] =   (x)  (   f (x)) + (1       (x))  (f (x)).

let

h  (  ) =   (x)  (     ) + (1       (x))  (  )

(1.3)

486
so that

f    
  (x) = argmin h

  

   ir

  (  ) ,

and

   

r   = min r  (f ) = min h  

(x)(   .
)

f    irx

     ir

since   (  ) is di   erentiable, setting the derivative of h

  (  ) to zero gives f  (x) =     , where

   

h    

  (    ) =      (x)     (       ) + (1       (x))     (    ) = 0,

which gives

  (x)

1       (x)

=

     (    )
     (       )

since   (  ) is a convex function, its derivative      (  ) is non-decreasing. then from the equation
above, we have the following equivalence relation

  (x)                  0     sign(f    

  (x))     0.

1
2

(1.4)

since the equivalence relation holds for all x     x ,

  (x)         sign(f    

  (x))

    0.

1
2

the following lemma shows that if the excess   -risk r (f )     r   

  

   of a soft classi   er f is

small, then the excess-risk of its associated hard classi   er sign(f ) is also small.

lemma (zhang   s lemma [zha04]): let    : ir 7    ir+ be a convex non-decreasing
function such that   (0) = 1. de   ne for any        [0, 1],

if there exists c > 0 and        [0, 1] such that

   (  ) := inf h  (  ).

     ir

|      

1
2

|     (1        (  ))   ,

c

          [0, 1] ,

(1.5)

then

r(sign(f ))     r        2c(r  (f )     r   

  )  

proof. note    rst that    (  )     h  (0) =   (0) = 1 so that condition (2.5) is well de   ned.

next, let h    = argminh   {   1,1}x ip[h(x) = y ] = sign(      1/2) denote the bayes classi   er,

where    = ip[y = 1|x = x], . then it is easy to verify that

r(sign(f ))     r    = ie[|2  (x)     1|1i(sign(f (x)) = h   (x))]

= ie[|2  (x)     1|1i(f (x)(  (x)     1/2) < 0)]
    2cie[((1        (  (x)))1i(f (x)(  (x)     1/2) < 0))  ]
    2c (ie[(1        (  (x)))1i(f (x)(  (x)     1/2) < 0)])   ,

where the last inequality above follows from jensen   s inequality.

496
6
we are going to show that for any x     x , it holds

(1        (  ))1i(f (x)(  (x)     1/2) < 0)]     ie[  (   y f (x)) | x = x]     r   

   .

(1.6)

this will clearly imply the result by integrating with respect to x.

recall    rst that

ie[  (   y f (x)) | x = x] = h  (x)(f (x))

and

so that (2.6) is equivalent to

r   

   = min h  (x)(  ) =    (  (x)) .

     ir

(1        (  ))1i(f (x)(  (x)     1/2) < 0)]     h  (x)(  )        (  (x))

since the right-hand side above is nonnegative, the case where f (x)(  (x)     1/2)     0 follows
trivially. if f (x)(  (x)     1/2) < 0, (2.6) follows if we prove that h  (x)(  )     1. the convexity
of   (  ) gives

h  (x)(  ) =   (x)  (   f (x)) + (1       (x))  (f (x))

      (     (x)f (x) + (1       (x))f (x))

=   ((1     2  (x))f (x))

      (0) = 1 ,

where the last inequality follows from the fact that    is non decreasing and f (x)(  (x)    
1/2) < 0. this completes the proof of (2.6) and thus of the lemma.

it is not hard to check the following values for the quantities    (  ), c and    for the three

losses introduced above:

    hinge loss:    (  ) = 1     |1     2  | with c = 1/2 and    = 1.
   
    exponential loss:    (  ) = 2p  (1       ) with c = 1/
    logistic loss:    (  ) =       log        (1       ) log(1       ) with c = 1/

   

2 and    = 1/2.

2 and    = 1/2.

5018.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: xuhong zhang

lecture

9
oct. 7, 2015

recall that last lecture we talked about convex relaxation of the original problem

n

  h = argmin

h   h n x

1

i=1

1i(h(xi) = yi)

by considering soft classi   ers (i.e. whose output is in [   1, 1] rather than in {0, 1}) and
convex surrogates of the id168 (e.g. hinge loss, exponential loss, logistic loss):

  
f = argmin r  ,n(f ) = argmin

  

  ( yif (xi))

   

f    f

n

1

f    f n x

i=1

and h = sign(f ) will be used as the    hard    classi   er.

  

  

we want to bound the quantity r  (f )     r  (f ), where f = argminf    f r  (f ).

  

  

  

(1) f = argminf    f r  ,n(f ), thus

  

  

  
r  (f ) = r  (f ) + r  ,n(f )     r  ,n(f ) + r  ,n(f )     r  ,n(f ) + r  (f )     r  (f )

  

  

  

  

  

  

  

  

  

  

  

   

   

  
r  (f ) + r  ,n(f )     r  ,n(f ) + r  (f )     r  (f )

  

  

  

  

  

  

r  (f ) + 2 sup |r  ,n(f )     r  (f )
|

  

  

f    f

(2) let us    rst focus on e[supf    f |r  ,n(f )     r  (f )|]. using the symmetrization trick as
before, we know it is upper-bounded by 2rn(     f), where the rademacher complexity

  

rn(       f) =

sup

x1,...,xn,y1,...,yn

e[sup | x   i  (   yif (xi)) ]
|

1
f    f n

n

i=1

one thing to notice is that   (0) = 1 for the id168s we consider (hinge loss,
exponential loss and logistic loss), but in order to apply contraction inequality later,
we require   (0) = 0. let us de   ne   (  ) =   (  )     1. clearly   (0) = 0, and

e[sup | x(  (   yif (xi))     e[  (   yif (xi))]) ]
|

= e[sup | x(  (   yif (x )     e

i )

1
f    f n

[  (

   yif (x ))])

i

|]

n

1
f    f n

i=1

n

i=
1
    2rn(       f)

(3) the rademacher complexity of        f is still di   cult to deal with. let us assume
that   (  ) is l-lipschitz, (as a result,   (  ) is also l-lipschitz), apply the contraction
inequality, we have

rn(       f)     2lrn(f)

516
(4) let zi = (xi, yi), i = 1, 2, ..., n and

g(z1, z2

, ..., zn) = sup |r  ,n(f )   r  (f ) =

|

  

f

   f

n

1

n x(  (

i=1

   yif (xi))   e[  (   yif (xi))])|

|

sup
f    f

since   (  ) is monotonically increasing, it is not di   cult to verify that    z1, z2, ..., zn, z    
i

|g(z1, ..., zi, ..., zn)     g(z1, ..., z    

i, ..., zn)|     (  (1)       (

   1))    

1
n

2l
n

the last inequality holds since g is l-lipschitz. apply bounded di   erence inequality,

p(| s
f

up r  ,n(f )     r  (f )|     e[sup |r  ,n(f )     r  (f )|] >
   
f

   f

|

f

|   

  

t)     2 exp(

   

2t2
i= ( 2l )2

1 n

pn

)

set the rhs of above equation to   , we get:

  
sup r
f    f

|

  ,n(f ) r  (f )

   

|    

e

with id203 1       .

(5) combining (1) - (4), we have

  

|

[sup r  ,n(f )
f    f

    r  (f )|] + 2lr

log(2/  )

2n

r  (f )     r  (f ) + 8lrn(f) + 2lr log(2/  )

  

  

2n

with id203 1       .

1.4 boosting

in this section, we will specialize the above analysis to a particular learning model: boosting.
the basic idea of boosting is to convert a set of weak learners (i.e. classi   ers that do better
than random, but have high error id203) into a strong one by using the weighted
average of weak learners    opinions. more precisely, we consider the following function class

m

f = {x   jhj(  ) : |  |1     1, hj : x 7    [   1, 1], j     {1, 2, ..., m a

} re classi   ers}

j=1

and we want to upper bound rn(f) for this choice of f.

rn(f) = sup e[sup
f    f

z1,...,zn

|

n

1

n x

i=

1

  iyif (xi) ] =

|

1
n z

sup e[ sup
1,...,z
|  |1   1

n

m

|x

j=1

  j

n

x

i=1

yi  ihj(xi) ]
|

let g(  ) = |pm
i=1 yi  ihj(xi)|. it is easy to see that g(  ) is a convex function, thus
sup|  |1   1 g(  ) is achieved at a vertex of the unit    1 ball {   : k  k1     1}. de   ne the    nite set

j=1   j pn

bx,y ,

   y1h1(x1)       y1h2(x1)   
(    y2h1(x2)       y2h2(x2)   
   
   
      
      

   
             
   

  

  

.
.
.

.
.
.

,

ynh2(xn)

ynh1(xn)

, . . . ,

  

   y1hm (x1)
   y2hm (x2)
         

ynhm (xn)

.
.
.

   
   
         

)

52then

notice maxb   bx,y b
we get

   

r

n(bx,y)     max
b   bx,y

(cid:2)

thus for boosting,

rn(f) = sup rn(bx,y) .

x,y

| |2     n and |bx,y| = 2m . therefore, using a lemma from lecture 5,

p

|b|2

(cid:3)

2 log(2 b

|
n

x,y )
|

    r

2

log( m )

4
n

r  (f )     r  (f ) + 8lr

  

  

2 log(4m )

n

+ 2lr

log(2/  )

2n

with id203 1 -   

to get some ideas of what values l usually takes, consider the following examples:

(1) for hinge loss, i.e.   (x) = (1 + x)+, l = 1.

(2) for exponential loss, i.e.   (x) = ex, l = e.

(3) for logistic loss, i.e.   (x) = log2(1 + ex), l = e

1+e log2(e)     2.43

now we have bounded r  (f )     r  (f ), but this is not yet the excess risk. excess risk is
de   ned as r(f )     r(f    ), where f     = argminf r  (f ). the following theorem provides a
bound for excess risk for boosting.

  

  

  

theorem: let f = {pm
lipschitz convex surrogate. de   ne f = argminf    f r  ,n(f ) and h = sign(f ). then

j=1   jhj : k  k1     1, hj s are weak classi   ers} and    is an l-

  

  

  

  r(h)     r     2c inf r  (f )     r  (f ) + 2c 8l

   

    (cid:1)  

(cid:0)

f    f

 

r

2 log(4m )

n

  

!

 
2c

2lr

+

log(2/  )

  

!

2n

with id203 1       

proof.
r  (f )     r  (f    )(cid:1)
  r(h)     r        2c(cid:0)
 
    2c

   

  

inf r  (f ) r  (f ) + 8l
f    f

   

r

2 log(4m )

n

+ 2l

 
    2c inf r  (f )     r  (f ) + 2c

  

   

(cid:1)

(cid:0)

f    f

8lr

2 log(4m )

  

!

n

r

log(2/  )

  

!

2n
 
+ 2c

2lr

log(2/  )

!  

2n

here the    rst inequality uses zhang   s lemma and the last one uses the fact that for ai     0
and        [0, 1], (a1 + a

2 + a3)     a  

1 + a  

2 + a  
3.

  

1.5 support vector machines

in this section, we will apply our analysis to another important learning model: support
vector machines (id166s). we will see that hinge loss   (x) = (1 + x)+ is used and the
associated function class is f = {f : kf kw       } where w is a hilbert space. before
analyzing id166s, let us    rst introduce reproducing kernel hilbert spaces (rkhs).

531.5.1 reproducing kernel hilbert spaces (rkhs)

de   nition: a function k : x    x 7    ir is called a positive symmetric de   nite kernel
(psd kernel) if

(1)    x, x        x , k(x, x   ) = k(x   , x)

(2)    n     z+,    x1, x2, ..., xn, the n

   n matrix with k(xi, xj) as its element in ith row
column is positive semi-de   nite. in other words, for any a1, a2, ..., an     ir,

and j

th

x aiajk(xi, xj)

   

0

i,j

let us look at a few examples of psd kernels.

example 1 let x = ir, k(x, x   ) = hx, x   iird is a psd kernel, since    a1, a2, ..., an     ir

x aiajhxi, xjiird = xhaixi, ajxjiird = hx aixi,x ajxjiird = kx a

2

ixikird

   

0

i,j

i,j

i

j

i

example 2 the gaussian kernel k(x, x   ) = exp(    1
2 k
  

2 x     x kird) is also a psd kernel.

   

2

note that here and in the sequel, k    kw and h  ,   iw denote the norm and inner product
of hilbert space w .

de   nition: let w be a hilbert space of functions x 7    ir. a symmetric kernel k(  ,   )
is called reproducing kernel of w if

(1)    x     x , the function k(x,   )     w .

(2)    x     x , f     w , hf (  ), k(x,   )iw = f (x).

if such a k(x,   ) exists, w is called a reproducing kernel hilbert space (rkhs).

claim: if k(  ,   ) is a reproducing kernel for some hilbert space w , then k(  ,   ) is a
psd kernel.

proof.    a1, a2, ..., an     ir, we have

x aiajk(xi, xj) = x aiajhk(xi,   ), k(xj ,   )i (since k( , ) is reproducing)

     

i,j

i,j

= hx aik(xi, ),

   x

ajk(xj, ) w

   i

i

= kx aik(xi,

j
  )k2

w     0

i

54in fact, the above claim holds both directions, i.e.
reproducing kernel.

if a kernel k(  ,   ) is psd, it is also a

a natural question to ask is, given a psd kernel k(  ,   ), how can we build the corresponding
hilbert space (for which k(  ,   ) is a reproducing kernel)? let us look at a few examples.

example 3 let   1,   2, ...,   m be a set of orthonormal functions in l2([0, 1]), i.e. for any
j, k     {1, 2, ..., m }

z   j(x)  k(x)dx =

h  j,   ki =   jk

let k(x, x   ) = pm

j=1   j(x)  j (x   ). we claim that the hilbert space

x

m

w = {x aj  j(

  ) a1, a2, ..., am

:

    ir}

equipped with inner product h  ,   il2 is a rkhs with reproducing kernel k(  ,   ).

j

=1

proof.

(1) k(x,   ) = j=1   j(x)  j (  )     w . (choose aj =   j(x)).

m

p

(2) if f (  ) = pm

j=1 aj

  j(  ),

hf (  ), k(x,   )il2 = hx aj  j(  ), x   k(x)  k(  )il2 = x aj  j(x) = f (x)

m

m

m

j=1

k=1

j=1

(3) k(x, x   ) is a psd kernel:    a1, a2, ..., an     ir,

x aiajk(x

i, xj) = x aiaj  k(xi)  k(xj) = x(x ai  k(xi))

2

    0

i,j

i,j,k

k

i

example 4 if x = ird, and k(x, x   ) = hx, x   iird, the corresponding hilbert space is
w = {hw,   i : w     ird} (i.e. all linear functions) equipped with the following inner product:
if f = hw,   i, g = hv,   i, hf, gi , hw, viird .

proof.

(1)    x     ird, k(x,   ) = hx,   iird     w .

(2)    f = hw,   iird     w ,    x     ird, hf, k(x,   )i = hw, xiird = f (x)

(3) k(x, x   ) is a psd kernel:    a1, a2, ..., an     ir,

x aiajk(xi, xj) = x aiaj

hxi xj

,

i = hx aixi,

x ajxj

iird = kx aix 2

k    
i ird

0

i

,j

i,j

i

j

i

5518.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: aden forrow

lecture

10
oct. 13, 2015

recall the following de   nitions from last time:

de   nition: a function k : x    x 7    r is called a positive symmetric de   nite kernel
(psd kernel) if

1.    x, x        x , k(x, x   ) = k(x   , x)

2.    n     z+,    x1, x2, . . . , xn, the n    n matrix with entries k(xi, xj) is positive de   -

nite. equivalently,    a

1, a2, . . . , an     ,
r

n

x aiajk(xi, xj)

    0

i,j=1

de   nition: let w be a hilbert space of functions x 7    r. a symmetric kernel k(  ,   )
is called a reproducing kernel of w if

1.    x     x , the function k(x,   )     w .

2.    x     x ,    f     w , hf (  ), k(x,   )iw = f (x).

if such a k(x,   ) exists, w is called a reproducing kernel hilbert space (rkhs).

as before, h  ,   iw and k    kw respectively denote the inner product and norm of w . the
subscript w will occasionally be omitted. we can think of the elements of w as in   nite
linear combinations of functions of the form k(x,   ). also note that

hk(x,   ), k(y,   )iw = k(x, y)

since so many of our tools rely on functions being bounded, we   d like to be able to
bound the functions in w . we can do this uniformly over x     x if the diagonal k(x, x) is
bounded.

proposition: let w be a rkhs with psd k such that supx   x k(x, x) = kmax is
   nite. then    f     w ,

sup |f (x)|     kf kwpkmax

x   x

.

proof. we rewrite f (x) as an inner product and apply cauchy-schwartz.

f (x) = hf, k(x,   )iw     kf kw kk(x,   )kw

now kk(x,   )k2

w = hk(x,   ), k(x,   )iw = k(x, x)     kmax. the result follows immediately.

561.5.2 risk bounds for id166

we now analyze support vector machines (id166) the same way we analyzed boosting. the
general idea is to choose a linear classi   er that maximizes the margin (distance to classi   ers)
while minimizing empirical risk. classes that are not linearly separable can be embedded
in a higher dimensional space so that they are linearly separable. we won   t go into that,
however; we   ll just consider the abstract optimization over a rkhs w .

explicitly, we minimize the empirical   -risk over a ball in w with radius   :

  
f =

min

f    w,kf kw      

  
rn,  (f )

  

the soft classi   er f is then turned into a hard classi   er h = sign(f ). typically in id166   
is the hinge loss, though all our convex surrogates behave similarly. to choose w (the only
other free parameter), we choose a psd k(x1, x2) that measures the similarity between two
points x1 and x2.

  

  

as written, this is an intractable minimum over an in   nite dimensional ball {f, kf kw    

  }. the minimizers, however, will all be contained in a    nite dimensional subset.

theorem: representer theorem. let w be a rkhs with psd k and let g :
nr 7    r be any function. then

min g(f (x1), . . . , f (xn)) =

f    w,kf k     

min

      

f wn, kf k     

g(f (x1), . . . , f (xn))

=

min

     rn,      ik       2

g(g  (x1), . . . , g  (xn)),

where

and ikij = k(xi, xj).

  wn = {f     w |f (  ) = g  (  ) = x   ik(xi,

n

  )}

i=1

proof.
  
f = f + f with f wn and f       w    

         

   

   

  

since wn is a linear subspace of w , we can decompose any f w uniquely as

   

n . the pythagorean theorem then gives

kf k2

w = k   f k2

w + kf    k2
w

moreover, since k(xi,   )     wn,

  

f    (xi) = hf    , k(xi,   )iw = 0

so f (xi) = f (xi) and

  

g(f (x1

), . . . , f (xn)) = g(f (x1), . . . , f (xn)).

  

  

because f     does not contribute to g, we can remove it from the constraint:

min

f    w,kf k2+kf    k2     2

  

g(f (x1), . . . , f (xn)) =

min

f    

k   w, f k2     2

  

g(f (x1), . . . , f (xn)).

  

57restricting to f     wn now does not change the minimum, which gives us the    rst equality.
for the second, we need to show that kg  kw        is equivalent to      ik         2.

  

kg  k2 = hg  , g  

i

n

= hx   ik(xi,

n

x   jk(xj,

  ),

  )
i

i=1
n

j

=1

= x   i  jhk(xi,

  ), k xj ,

(

  )i

i

,j=1

n

= x   i  jk(xi, xj)

i,j=1

=      ik  

we   ve reduced the in   nite dimensional problem to a minimization over        nr . this
works because we   re only interested in g evaluated at a    nite set of points. the matrix
ik here is a gram matrix, though we will not not use that. ik should be a measure of the
similarity of the points xi. for example, we could have w = {hx,   ird , x     dr } with k(x, y
the usual inner product k(x, y) = hx, yird.

we   ve shown that f only depends on k through ik, but does rn,   depend on k(x, y)

  

  

for x, y    / {xi}? it turns out not to:

  rn,   = x

  (   yig  (xi)) = x   (   yi x   jk(xj, xi)).

n

n

1
n

1
n

n

i=1

i=1

j=1

the last expression only involves ik. this makes it easy to encode all the knowledge about
our problem that we need. the hard classi   er is

h(x) = sign(f (x)) = sign(g    (x)) = sign(x     jk(xj, x))
  

  

n

j=1

if we are given a new point xn+1, we need to compute a new column for ik. note that
xn+1 must be in some way comparable or similar to the previous {xi} for the whole idea of
extrapolating from data to make sense.

the expensive part of id166s is calculating the n    n matrix ik. in some applications,
ik may be sparse; this is faster, but still not as fast as deep learning. the minimization
over the ellipsoid      ik   requires quadratic programming, which is also relatively slow. in
practice, it   s easier to solve the lagrangian form of the problem

     = argmin x   (   yig  (x

1
     rn n

n

i=1

i)) +       ik  

       

this formulation is equivalent to the constrained one. note that    and       are di   erent.
id166s have few tuning parameters and so have less    exibility than other methods.
we now turn to analyzing the performance of id166.

58  

theorem: excess risk for id166. let    be an l-lipschitz convex surrogate and
w a rkhs with psd k such that maxx |k(x, x)| = kmax <    . let hn,   = sign fn,  ,
where fn,   is the empirical   -risk minimizer over f = {f
    w.kf kw       } (that is,
  
kmax     1. then
rn,  (fn,  )     rn,  (f )   f     f). suppose   
!

2 log(2/  )

kmax

 

!

   

  

  

  

  

  

  

  

r

  r(hn,  )     r        2c(cid:18)

  r
2l

  )(cid:19) + 2c

+

2c

8l  

inf (r  (f )     r   
f    

f

n

n

with id203 1       . the constants c and    are those from zhang   s lemma. for the
hinge loss, c = 1

2 and    = 1.

proof. the    rst term comes from optimizing over a restricted set f instead of all classi   ers.
the third term comes from applying the bounded di   erence inequality. these arise in
exactly the same way as they do for boosting, so we will omit the proof for those parts. for
the middle term, we need to show that rn,  (f)        kmax
n .

q
f     f, so we can use the contraction
kmax     1 for all
inequality to replace rn,  (f) with rn(f). next we   ll expand f (xi) inside the rademacher
complexity and bound inner products using cauchy-schwartz.

first, |f (x)|     kf kw

kmax       

   

   

rn(f) = sup e sup
f    f

x1,...,xn

"

n

n

i=1

x

(cid:12)(cid:12)#
(cid:12)
  if (xi)
(cid:12)
(cid:12)
i,   ), f
i k(x
h

(cid:12)(cid:12)
1
(cid:12) n
(cid:12)
(cid:12)
sup (cid:12)(cid:12)
(cid:12)x   
(cid:12)(cid:12)
f    f i=1
(cid:12)(cid:12)(cid:12)hx   ik(xi,   ), f
n
(cid:12)(cid:12)
v
te" n
#
uu
kx   ik(x

sup
f    f i=1

2
  )kw

i,

i=1

(cid:12)(cid:12)#
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)(cid:12)#
(cid:12)
i
(cid:12)
(cid:12)

1
n x1,...,xn

sup e"
"

sup e

x1,...,xn

1
n

=

=

   

  
n x1,...,xn

sup

now,

e"

n

kx   ik(xi,   )kw

2

#

= e

i=1

n

   
   hx   ik(xi,   ), x   jk(xj,

n

   
   

  )iw

j=1

i=1

n

= x hk(x

i,j=1

i,   ), k(xj ,   )i [  i  j]

e

n

x k(xi, xj)  ij

=

i

,j=1

    nkmax

so rn(f)        kmax
as with boosti
zhang   s lemma.

q
n and we are done with the new parts of the proof. the remainder follows
ng, using symmetrization, contraction, the bounded di   erence inequality, and

5918.657: mathematics of machine learning

lecturer: philippe rigollet
scribe: kevin li

lecture

11
oct. 14, 2015

2. id76 for machine learning

in this lecture, we will cover the basics of id76 as it applies to machine
learning. there is much more to this topic than will be covered in this class so you may be
interested in the following books.

id76 by boyd and vandenberghe
lecture notes on id76 by nesterov
id76: algorithms and complexity by bubeck
online id76 by hazan

the last two are drafts and can be obtained online.

2.1 convex problems

a convex problem is an optimization problem of the form min f (x) where f and

x   c

convex. first, we will debunk the idea that convex problems are easy by showing that
virtually all optimization problems can be written as a convex problem. we can rewrite an
optimization problem as follows.

c

are

   
min f (x) ,
x   x

min

t   f (x),x   x

   
t ,

min

(x,t)   epi(f )

t

where the epigraph of a function is de   ned by

}
epi(f ) = f(x, t) 2 x (cid:2) ir : t (cid:21) f (x)g

      

   

{

figure 1:

an example of an epigraph.

source: https://en.wikipedia.org/wiki/epigraph_(mathematics)

60now we observe that for linear functions,

min c(cid:62)x = min
x   d

x   conv(d)

c(cid:62)x

where the convex hull is de   ned

conv(d) = fy : 9n 2 z+, x1, . . . , xn 2 d,   i (cid:21) 0,

       

   

   

{

(cid:88)

n

i=1

  i = 1, y =

(cid:88)

n

i=1

}
g

  ixi

to prove this, we know that the left side is a least as big as the right side since d (cid:26) conv(d).
for the other direction, we have

   

min

x   conv(d)

c(cid:62)x = min min

min c(cid:62)

n x1,...,xn   d   1,...,  n

  ixi

n

i=1

(cid:88)
(cid:88)
(cid:88)

=1
n

n

i

i=1

  ic(cid:62)xi min c(cid:62)x

   
(cid:21)

x   d

  i min c(cid:62)x

   d

x

= min min

min
n x1,...,xn   d   1,...,  n

min
n x1,...,xn   d   1,...,  n

   
(cid:21) min min
= min c(cid:62)x

x   d

therefore we have

which is a convex problem.

min f (x)
x   x

   
,

min

(x,t)   conv(epi(f ))

t

why do we want convexity? as we will show, convexity allows us to infer global infor-

mation from local information. first, we must de   ne the notion of subgradient.

de   nition (subgradient): let c (cid:26) ird, f : c ! ir. a vector g 2 ird is called a
subgradient of f at x 2 c if

   

   

   

   

f (x) (cid:0) f (y) (cid:20) g(cid:62)(x (cid:0) y)

   

   

   

   
8y 2 c .

   

the set of such vectors g is denoted by    f (x).

subgradients essentially correspond to gradients but unlike gradients, they always ex-
ist for convex functions, even when they are not di   erentiable as illustrated by the next
theorem.

(cid:54)=    
theorem: if f : c ! ir is convex, then for all x,    f (x) = ;.
}
di   erentiable at x, then    f (x) = frf (x)g.

{   

   

in addition, if f is

proof. omitted. requires separating hyperplanes for convex sets.

616
(cid:54)
theorem: let f,c be convex. if x is a local minimum of f on c, then it is also global
minimum. furthermore this happens if and only if 0 2    f (x).

   

   

proof. 0 2    f (x) if and only if f (x)     f (y) (cid:20) 0 for all y 2 c. this is clearly equivalent to
x being a global minimizer.

next assume(cid:16) x is a local minimum. then for all y 2 c there exists    small enough such

(cid:17) (cid:20) (1       )f (x) +   f (y) =) f (x) (cid:20) f (y) for all y 2 c.

that f (x) (cid:20) f (1       )x +   y

   
   

   

   

   

   

   

   

not only do we know that local minimums are global minimums, looking at the subgra-
dient also tells us where the minimum can be. if g(cid:62)(x     y) < 0 then f (x) < f (y). this
means f (y) cannot possibly be a minimum so we can narrow our search to ys such that
g(cid:62)(x     y). in one dimension, this corresponds to the half line fy 2 ir : y (cid:20) xg if g > 0 and
the half line fy 2 ir : y (cid:21) xg if g < 0 . this concept leads to the idea of id119.

    }

    }

{    

{    

2.2 id119
y (cid:25) x and f di   erentiable the    rst order taylor expansion of f at x yields f (y) (cid:25) f (x) +
   
g(cid:62)(y     x). this means that

   

min f (x +       ) (cid:25) min f (x) + g(cid:62)(      )
|    |2=1

   

which is minimized at      =     . therefore to minimizes the linear approximation of f at
x, one should move in direction opposite to the gradient.
id119 is an algorithm that produces a sequence of points fxjgj   1 such that

{ }

g
|g|2

(hopefully) f (xj+1) < f (xj).

figure 2: example where the subgradient of x1 is a singleton and and the subgradient of

x2 contains multiple elements.

source: https://optimization.mccormick.northwestern.edu/index.php/

subgradient_optimization

62algorithm 1 id119 algorithm
{ }

input: x1 2 c, positive sequence f  sgs   1
for s = 1 to k (cid:0) 1 do
xs+1 = xs (cid:0)   sgs ,

gs 2    f (xs)

   
   

   

   

end for

return either x   =

(cid:88)

k1
k

s=1

xs or x   

2 argmin f (x)
   

x   {x1,...,xk}

theorem: let f be a convex l-lipschitz function on ird such that x    2 argminird f (x)
exists. assume that jx1 (cid:0) x   j2 (cid:20) r. then if   

|     |    

(cid:21) 1, then
   

s =    =

   

    for all s
r
k
l

(cid:88)

k1
k

s=1

f (

xs)

(cid:0) f (x   ) (cid:20) p
   
   
lr
k

   

and

min f (xs)
       
1 s k

(cid:0) f (x   ) (cid:20) p
       
   
lr
k

proof. using the fact that gs = 1 (x

s+1

  

(cid:0) xs) and the equality 2a(cid:62)b = kak
   
(cid:107) (cid:107)

2

+

kbk2(cid:0)ka(cid:0) bk2,
(cid:107) (cid:107)
(cid:107)

       

(cid:107)

   

   

(cid:104)k (cid:0)

(cid:62)(xs (cid:0) x   ) = (xs
f (xs) (cid:0) f (x   ) (cid:20) gs
   
1
  
k
(cid:107)    
(cid:107)
1
x
2  
kg 2
sk + (  2
(cid:107) (cid:107)
  
2

2   s (cid:0)   2
   

=

=

1

xs+1 + x

2

s

s+1)

(cid:0) xs+1)(cid:62)(xs (cid:0) x   )
   
   
k (cid:0) k (cid:0) k
(cid:107)     (cid:107)     (cid:107)

x    2

x

s+1

s

(cid:105)

(cid:0) k
    (cid:107)
x   

2

where we have de   ned   s = kxs (cid:0) x   k. using the lipschitz condition

(cid:107)     (cid:107)

f (xs) (cid:0) f (x   ) (cid:20) l2 + (  2

   

   

1

2   s (cid:0)   2
   

s+1)

  
2

taking the average from 1, to k we get

f (xs)

(cid:0)
   

f (x   ) (cid:20) l2

   

  
2

+

1
k
2

  

1 (cid:0)   2
   
(  2

s

+1

)

(cid:20) l2
   

  
2

+

1

2k   1 (cid:20) l2 +

   

  
2

  2

r2
2k  

    to minimize the expression, we obtain
taking    = r
k
l

(cid:88)

k1
k

f (xs)

(cid:0) f (x   ) (cid:20) p
       
   
lr
k

noticing that the left-hand side of the inequality is larger than both f ((cid:80) xs) (cid:0) f (x   ) by

   

s=1

k

jensen   s inequality and min f (xs)

1   s   k

(cid:0) f (x   ) respectively, completes the proof.
   

s=1

k1(cid:88)

k

s=1

63s

k

(

s=1

   

(cid:88)k

(cid:88)

one    aw with this theorem is that the step size depends on k. we would rather have
step sizes   s that does not depend on k so the inequalities hold for all k. with the new step

(cid:20)(cid:16)(cid:88)

s (cid:0) s+1)
   
(  2

  s f (x ) (cid:0) f x   )] (cid:20)
   
[

sizes, (cid:88)k
after dividing by (cid:80)k(cid:80) s=1   s, we(cid:80)would like the right-hand side to approach 0. for this to
happen we need (cid:80) s ! 0 and
(cid:80)
then (cid:80)
   
s (cid:20) c1g2 log(k) and
  2
k   1(cid:88)

  s ! 1. one candidate for the step size is   s = g

       
   
(cid:21)

(cid:16)(cid:88)k

    since
s

l
  2
s 2

k. so we get

r2
2

  2
s
2

(cid:17)

(cid:17)

c2g

   

   
p

   

  2
  s

2
l

1
2

  2

s=1

s=1

s=1

s=1

  s

c1

+

+

=1

k

k

k

s

  s

  s[f (xs)

   
(cid:0)

f (x   )]

   
(cid:20)

   
p

gl log k
2c2

k

+

r2
   
p
2c2g

k

s=1

s=1

choosing g appropriately, the right-hand side approaches 0 at the rate of lr log k . notice
that we get an extra factor of
2 to k instead

   
p
log k. however, if we look at the sum from k/

k

of 1 to k, (cid:80)

k

s

= k
2

1g2 and (cid:80)   s (cid:21) c(cid:48)

   

k

s=1

   
p

   
s (cid:20) c(cid:48)
  2

(cid:113)

min f (xs)
1   s   k

   
(cid:0)

f (x   )

   
(cid:20)

   
(cid:0)

f (x   )

min f (xs)
       
s k

k
2

  s[f (xs)

   
(cid:0)

f (x   )]

       
(cid:20) p
clr
k

2g

k. now we have

(cid:20)(cid:16)(cid:88)

   

k

  s

(cid:17) (cid:88)

k   1

s=

k
2

s=

k
2

which is the same rate as in the theorem and the step sizes are independent of k.

important remark: note this rate only holds if we can ensure that jxk/2 (cid:0) x   j2 (cid:20) r
since we have replaced x1 by xk/2 in the telescoping sum. in general, this is not true for
id119, but it will be true for projected id119 in the next lecture.

    |    

|

one    nal remark is that the dimension d does not appear anywhere in the proof. how-
ever, the dimension does have an e   ect because for larger dimensions, the conditions f is
l-lipschitz and jx1 (cid:0) x   j2 (cid:20) r are stronger conditions in higher dimensions.

|    

   

|

6418.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: michael traub

lecture 12
oct. 19, 2015

2.3 projected id119

in the original id119 formulation, we hope to optimize minx
c a d
f are convex, but we did not constrain the intermediate xk. projected id119 will
incorporate this condition.

f (x) where

n

   

c

2.3.1 projection onto closed convex set

first we must establish that it is possible to always be able to keep xk in the convex set c.
one approach is to take the closest point   (xk)     c.

de   nition: let c be a closed convex subset of ird. then     x     ird, let   (x)     c be
the minimizer of

kx       (x)k = min x

k     k

z

z   c

where k    k denotes the euclidean norm. then   (x) is unique and,

h  (x)     x,   (x)     zi     0     z     c

(2.1)

proof. from the de   nition of    :=   (x), we have kx       k2     kx     vk2 for any v     c. fix
w     c and de   ne v = (1     t)   + tw for t     (0, 1]. observe that since c is convex we have
v     c so that

k     k2     k     k2
x

x

  

v = kx            t(w       )k

2

expanding the right-hand side yields

x       k     kx       k     2t hx       , w       i + t2 kw       k
k

2

2

2

this is equivalent to

hx       , w     i    

  

t kw       k

2

since this is valid for all t     (0, 1), letting t     0 yields (2.1).

proof of uniqueness. assume   1,   2     c satisfy

h  1     x,   1     zi     0     z     c
h  2     x,   2     zi     0     z     c

taking z =   2 in the    rst inequality and z =   1 in the second, we get

h  1     x,   1       2i     0
hx       2,   1       2i     0

adding these two inequalities yields k  1       2k2     0 so that   1 =   2.

652.3.2 projected id119

algorithm 1 projected id119 algorithm

input: x1     c, positive sequence {  s}s   1
for s = 1 to k     1 do

ys+1 = xs       sgs ,
xs+1 =   (ys+1)

gs        f (xs)

end for
return either x   = x xs or x        argmin f (x)

k

x

   

{x1,...,x

k}

1
k

s=

1

theorem: let c be a closed, nonempty convex subset of ird such that diam(c)     r.
let f be a convex l-lipschitz function on
f (x) exists.
then if   s        =

c such that x        argminx

r
    then

   c

l

k

lr
f (x  )     f (x   )        
k

and

f (x     )     f (x   )

       

lr
k

moreover, if   s = r    , then    c > 0 such that

l s

lr
f (x  )     f (x   )     c    

k

and

f (x     )

   

f (x   )

lr

    c    

k

proof. again we will use the identity that 2a   b = kak2 + kbk2     k

2
a     bk .

by convexity, we have

f (xs)     f (x   )     gs
   (xs     x   )
1
  
1
2  

hk

=

= (xs     ys+1)   (xs

    x   )

xs     ys+1k + kxs     x   k2     kys+1     x   k2i

2

next,

kys+1     x   k2 = k

ys+1     xs+1k + kxs+1     x   k2 + 2 hys+1

2

    xs+1, xs+1     x   i

2

= kys+1     xs+1k + k
    kxs+1     x   k2

xs+1     x   k + 2 hys+1       (ys+1),   (ys+1)     x   i

2

where we used that hx       (x),   (x)     zi     0     z     c, and x   
kxs    
y
we    nd

    c. also notice that
s+1k =    kgsk        l since f is l-lipschitz with respect to k  k. using this

2 2

2

2

2

k

1

k x

s=1

f xs

(

)     (    )     x 2 2 +

   l

f x

1
k

k

s=1

h

1
2  

xs
k     k     k

x

xs+1

    2

    k i

    2

x

   

  l2
2

+

1
2 k
  

k 1     x   
x

2
k    

  l2
2

+

r2
2  k

66minimizing over    we get l = r

2 =       = r    , completing the proof

2

2

2

2   k

l k

f (x  )     f (x   )        

rl
k

moreover, the proof of the bound for f (pk
r2 as well.

s=

(cid:13)(cid:13) 2
k xs)     f (x   ) is identical because x k
(cid:13)

2

       
x

2

(cid:13)(cid:13)    
(cid:13)

2.3.3 examples

support vector machines

the id166 minimization as we have shown before is

minn x max (0, 1

   

yif  (xi))

1
n

n

i=1

   r

   i

   

   ik   c

    2

=1   jk(x ,

where f  (xi) =      ikei = pn
in this case executing the projection onto the ellipsoid {   :      ik       c 2} is not too hard,
but we do not know about c, r, or l. we must determine these we can know that our
bound is not exponential with respect to n. first we    nd l and start with the gradient of
gi(  ):

j xi). for convenience, call gi(  ) = max (0, 1     yif  (xi)).

j

with this we bound the gradient of the   -risk rn,  (f  ) = 1
n

   gi(  ) = 1i(1     yif  (xi)     0)yiikei
pn

  

i

=1 gi(  

).

n

     

(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)    
(cid:13) r  n,  (f  )(cid:13)(cid:13) x(cid:13) =
1
(cid:13) n
(cid:13)
(cid:13) i=1

(cid:13)
(cid:13)(cid:13)     x ike
   gi(  )
(cid:13)
(cid:13)
    yif  (xi)     0)yi     1. we can now
by the triangle inequality and the fact that that 1i(1
k is the    2 norm of the ith column so
use the properties of our kernel k. notice that kikei
2 = (cid:16)pj=1 k(xj, xi)2(cid:17) . we also know that

kikeik

1
n

k
2

(cid:13)

i=1

k

n

n

i

1

2

k(xj, xi)2 = hk(x

j,   ), k(xi,   )i     kk(xj,   )k kh k (xi,   )kh     kmax

2

combining all of these we get

(cid:13) r  n,  (f  )(cid:13)
(cid:13)
(cid:13)    
(cid:13)(cid:13)
(cid:13)      
(cid:13)

   

1
n

n

n

x x

   
    k2

j=1

i=1

max

1

2

   
    = kmax

   

n = l

to    nd r we try to evaluate diam{     ik       c 2} = 2 max

     

ik     c 2

   

  

     . we can use the

condition to put bounds on the diameter

c 2

         ik         min(ik)        =    diam{     ik       c 2}     p  min(ik)

2c

we need to understand how small   min can get. while it is true that these exist random
samples selected by an adversary that make   min = 0, we will consider a random sample of

67i.i.d

x1, . . . , xn     n (0, id). this we can write these d-dimensional samples as a d    n matrix
x. we can rewrite the matrix ik with entries ikij = k(xi, xj) = hxi, xjiird as a wishart
matrix ik = x   x (in particular, 1 x
   x is wishart). using results from random matrix
theory, if we take n, d         but hold n as a constant   , then   
      ) . taking
an approximation since we cannot take n, d to in   nity, we get

(
d     1

( ik

min

   

)

d

d

2

  min(ik)     d(cid:18)

1     2r

(cid:19)    

n
d

d
2

using the fact that d     n. this means that   min becoming too small is not a problem when
we model our samples as coming from multivariate gaussians.

now we turn our focus to the number of iterations k. looking at our bound on the

excess risk

r  n,  (f

r)     min rn,  (f  ) + cr

  

     

     ik     c 2

n

k  min(ik)

kmax

we notice that our all of the constants in our stochastic term can be computed given the
number of points and the kernel. since statistical error is often    1 , to be generous we want
to have precision up to 1

n to allow for fast rates in special cases. this gives us

n

k    

max

n3k2 c 2
  min(ik)

which is not bad since n is often not very big.

in [bub15], the rates for many a wide rage of problems with various assumptions are
available. for example, if we assume strong convexity and lipschitz we can get an exponen-
tial rate so k     log n. if gradient is lipschitz, then we get get 1
k instead of    1 in the bound.
however, often times we are not optimizing over functions with these nice properties.

k

boosting
we already know that    is l-lipschitz for boosting because we required it before.

remember that our optimization problem is

n

1
min x   (   yif  (xi))
   rn n
   i
|  
|1   

i=1

1

p
where f   = n
j=1   jfj and fj is the jth weak classi   er. remember before we had some rate
like q log n
and we would hope to get some other rate that grows with log n since n can
be very large. taking the gradient of the   -loss in this case we    nd

c

n

   r  n,  (f  ) = x      (   yif  (xi))(   yi)f (xi)

1
n

n

i=1

where f (x) is the column vector [f1(x), . . . , fn (x)]   . since |yi|     1 and           l, we can
bound the    2 norm of the gradient as

(cid:13)
(cid:13)   r  
(cid:13)

n,  (f  )(cid:13)(cid:13)(cid:13)2

   

   

(cid:13)
(cid:13)
(cid:13)
i)
(cid:13)(cid:13)

n

l
n

(cid:13)
(cid:13)(cid:13)x f (x
(cid:13)
(cid:13) i=1
n x

kf (xi

l

n

i=

1

)
k     l

   

n

68using triangle inequality and the fact that f (xi) is a n -dimensional vector with each
component bounded in absolute value by 1.

using the fact th   at the diameter of the    1 ball is 2, r = 2 and the lipschitz associated
with our   -risk is l n where l is the lipschitz constant for   . our stochastic term r   l
k
becomes 2lq n
n error as before we    nd that k     n 2n, which is very
bad especially since we want log n .

k . imposing the same 1

2.4 mirror descent

   

boosting is an example of when we want to do id119 on a non-euclidean space,
in particular a    1 space. while the dual of the    2-norm is itself, the dual of the    1 norm is
the    
or sup norm. we want this appear if we have an    1 constraint. the reason for this
is not intuitive because we are taking about measures on the same space ird, but when we
consider optimizations on other spaces we want a procedure that does is not indi   erent to
the measure we use. mirror descent accomplishes this.

2.4.1 bregman projections

de   nition: if k  k is some norm on ird, then k  k is its dual norm.

   

example: if dual norm of the    p norm k  kp is the    q norm k  kq, then 1
limiting case of h  older   s inequality.

p + 1

q = 1. this is the

in general we can also re   ne our bounds on inner products in ird to x   y     kxk kyk if   
we consider x to be the primal and y to be the dual. thinking like this, gradients live in
   (x     x   ), x     x    is in the primal space, so gs is in the dual. the
the dual space, e.g. in gs
transpose of the vectors suggest that these vectors come from spaces with di   erent measure,
even though all the vectors are in ird.

de   nition: convex function    on a convex set d is said to be
(i) l-lipschitz with respect to k  k if kgk        l     g          (x)     x     d
(ii)   -strongly convex with respect to k  k if

  (y)       (x) + g   (y     x) +

  
2

ky     xk2

for all x, y     d and for g        f (x)

example:
eig(h)       .

if    is twice di   erentiable with hessian h and k  k is the    2 norm, then all

de   nition (bregman divergence): for a given convex function    on a convex set
d with x, y     d, the bregman divergence of y from x is de   ned as

d  (y, x) =   (y)       (x)          (x)   (y     x)

69this divergence is the error of the function   (y) from the linear approximation at x.
also note that this quantity is not symmetric with respect to x and y. if    is convex then
d  (y, x)     0 because the hessian is positive semi-de   nite. if    is   -strongly convex then
2 ky     xk2 and if the quadratic approximation is good then this approximately
d  (y, x)       
holds in equality and this divergence behaves like euclidean norm.

proposition: given convex function    on d with x, y, z     d

(     (x)          (y))    (x     z) = d  (x, y) + d  (z, x)     d  (z, y)

proof. looking at the right hand side

=   (x)       (y)          (y)   (x     y) +   (z)       (x)          (x)   (z     x)

    h  (z)       (y)          (y)   (z     y)i

=      (y)   (y     x + z     y)          (x)   (z     x)
= (     (x)          (y))    (x     z)

de   nition (bregman projection): given x     ird,    a convex di   erentiable function
on d    

ird and convex c     , the bregman projection of x with respect to    is

d  

    (x)     argmin d  (x, z)

z   c

7018.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: mina karzand

lecture

13
oct. 21, 2015

previously, we analyzed the convergence of the projected id119 algorithm.
we proved that optimizing the convex l-lipschitz function f on a closed, convex set c with
diam(c)     r with step sizes   s = r    would give us accuracy of f (x)     f (x   ) + lr
    after
k
k iterations.

l k

although it might seem that projected id119 algorithm provides dimension-
free convergence rate, it is not always true. reviewing the proof of convergence rate, we
realize that dimension-free convergence is possible when the objective function f and the
constraint set c are well-behaved in euclidean norm (i.e., for all x     c and g        f (x), we
have that |x|2 and |g|2 are independent of the ambient dimension). we provide an examples
of the cases that these assumptions are not satis   ed.

    consider the di   erentiable, convex function f on the euclidean ball b2,n such that
n and the projected
n
k . using the

k   f (x)k     1,    x     b2,n. this implies that
f (x)
id119 converges to the minimum of f in b

|2    
2,

|   

   

   

p

method of mirror descent we can get convergence rate of

n at rate
q

log(n)

k

to get better rates of convergence in the optimization problem, we can use the mirror
descent algorithm. the idea is to change the euclidean geometry to a more pertinent
geometry to a problem at hand. we will de   ne a new geometry by using a function which
is sometimes called potential function   (x). we will use bregman projection based on
bregman divergence to de   ne this geometry.

the geometric intuition behind the mirror descent algorithm is the following: the
projected gradient described in previous lecture works in any arbitrary hilbert space h so
that the norm of vectors is associated with an inner product. now, suppose we are interested
in optimization in a banach space d. in other words, the norm (or the measure of distance)
that we use does not derive from an inner product. in this case, the id119 does
not even make sense since the gradient    f (x) are elements of dual space. thus, the term
x          f (x) cannot be performed. (note that in hilbert space used in projected gradient
descent, the dual space of h is isometric to h. thus, we didn   t have any such problems.)
the geometric insight of the mirror descent algorithm is that to perform the optimiza-
tion in the primal space d, one can    rst map the point x     d in primal space to the dual
space d   , then perform the gradient update in the dual space and    nally map the optimal
point back to the primal space. note that at each update step, the new point in the primal
space d might be outside of the constraint set c     d, in which case it should be projected
into the constraint set c. the projection associate with the mirror descent algorithm is
bergman projection de   ned based on the notion of bergman divergence.

de   nition (bregman divergence): for given di   erentiable,   -strongly convex func-
tion   (x) : d     r, we de   ne the bregman divergence associated with    to be:

d  (y, x) =   (y)       (x)          (x)t (y     x)

71we will use the convex open set d     nr whose closure contains the constraint set c     d.
bregman divergence is the error term of the    rst order taylor expansion of the function   
in d.

also, note that the function   (x) is said to be   -strongly convex w.r.t. a norm k.k if

  (y)       (x)          (x)t

(y     x)    

  
2

ky     xk2 .

we used the following property of the euclidean norm:

2a   b = kak2 + kbk2     ka     bk2

in the proof of convergence of projected id119, where we chose a = xs     ys+1 and
b = xs     x   .

to prove the convergence of the mirror descent algorithm, we use the following property
of the bregman divergence in a similar fashion. this proposition shows that the bregman di-
vergence essentially behaves as the euclidean norm squared in terms of projections:

proposition: given   -strongly di   erentiable convex function    : d     r, for all
x, y, z     d,

[     (x)          (y)]    (x     z) = d  (x, y) + d  (z, x)     d  (z, y) .

as described previously, the bregman divergence is used in each step of the mirror descent
algorithm to project the updated value into the constraint set.

de   nition (bregman projection): given   -strongly di   erentiable convex function
   : d     r and for all x     d and closed convex set c     d

    (x) = argmin d

c

  (z, x)

z   c   d

2.4.2 mirror descent algorithm

algorithm 1 mirror descent algorithm
d

  (x),    :

r

c   d

input: x1     argmin
for s = 1,          , k do

   

d

r such that   (x) =   (x)

   

  (ys+1) =   (xs)       gs for gs        f (xs)
xs+1 =     (yc

s+1)

end for
return either x = 1

k pk

s=1 xs or x        argminx

   {

x1,

      

,xk f (x)

}

proposition: let z     c     d, then    y     d,

(     (  (y)          (y))    (  (y)     z)     0

72moreover, d  (z,   (y))     d  (z, y).

proof. de   ne    =     (y) and h(t) = d  (   + t(z       ), y) . since h(t) is minimized at t = 0
(due to the de   nition of projection), we have

c

h   (0) =    xd  (x, y)|x=  (z       )     0

where suing the de   nition of bregman divergence,

thus,

   xd  (x, y) =      (x)          (y)

(     (  )          (y))    (       z)     0 .

using proposition 1, we know that

(     (  )          (y))    (       z) = d  (  , y) + d  (z,   )     d  (z, y)     0 ,

and since d  (  , y)     0, we would have d  (z,   )     d  (z, y).

theorem: assume that f is convex and l-lipschitz w.r.t. k.k. assume that    is
  -strongly convex on c     d w.r.t. k.k and

r2 = sup   (x) m

    in   (x)

x

   c   d

x   c   d

ta
ke x1 = argminx
q
r
l

2  
r gives,

   c   d

  (x) (assume that it exists). then, mirror descent with    =

f (x)     f (x   )     rlr

2
  k

and f (x   )

    f (x   )     rlr ,

2
  k

proof. take x        c     d. similar to the proof of the projected id119, we have:

(i)

   

    x

   (xs     x   )

s)       (ys+1))    (xs

f (xs)     f (x   )     gs
(ii) 1
= (  (x
  
(iii) 1
= (   (xs)
  
= hd
(iv) 1
  
(v) 1
    hd  (xs, ys+1) + d  (x   , xs)     d  (x   , xs+1)
i
  

   
    d  (x , ys+1)i

  (ys+1))    (xs

  (xs, ys+1) + d

  (x , xs)

       

x   )

   

)

   

   

(vi)   l2
   
2  2

+ hd  (x   , xs)

1
  

    d  (x   , xs+1)i

where (i) is due to convexity of the function f .

73equations (ii) and (iii) are direct results of mirror descent algorithm.
equation (iv) is the result of applying proposition 1.
=     
inequality (v) is a result of the fact that x
d  (x , ys+1)     d  (x , xs+1).
we will justify the following derivations to prove inequality (vi):

s+1), thus for x

(yc

s+1

   

   

   

    c     d, we have

d  (xs, ys+1) =   (xs)       (ys+1)          (ys+1)   (xs     ys+1)

(a)

(b)
    [     (x

s)          (ys+1)]    (xs     ys+1)    

  
2

2
kys+1     xsk

(c)
      kgsk   kxs     ys+1k    

  
2

kys+1     xsk2

(d)   2l2
   
2  

.

equation (a) is the de   nition of bregman divergence.
to show inequality (b), we used the fact that    is   -strongly convex which implies that
  (ys+1)       (xs)          (xs)t (ys+1     xs)   
according to the mirror descent algorithm,      (xs)          (ys+1) =   gs. we use h  older   s
   (xs     ys+1)     kgsk   kxs     ys+1k and derive inequality (c).
inequality to show that gs
looking at the quadratic term ax   bx2 for a, b > 0 , it is not hard to show that max ax
a
4
b
inequality (d).

. we use this statement with x = kys+1     xsk , a =    g

    bx2 =
        and b =    to derive

2
s+1     xsk .

k sk

2 ky

l

2

2

again, we use telescopic sum to get

k

1 x
k

s=1

[f (xs)

   

f (x   )]

   

  l2 d (x   
2  
k  

+

  

, x1)

.

(2.1)

we use the de   nition of bregman divergence to get

d  (x   , x1) =   (x   )       (x1)          (x1)(x        x1)

      (x   )       (x1)
    sup   (x) min   (x)
   

x

   c   d

x

   c   d

    r2 .

where we used the fact x1     argmin
  (x) in the description of the mirror descent
algorithm to prove      (x1)(x     x1)     0. we optimize the right hand side of equation (2.1)
for    to get

c   d

   

k

1 x
k

s=

1

[f (xs)     f

(x   

)]

    rl

r .

2
  k

to conclude the proof, let x        x        c.

note that with the right geometry, we can get projected id119 as an instance

the mirror descent algorithm.

742.4.3 remarks

the mirror descent is sometimes called mirror prox. we can write xs+1 as

xs+1 = argmin d  (x, ys+1)

x   c   d

= argmin   (x)

x

   c   d

= argmin   (x)

x

   c   d

            (ys+1)x

    [     (

xs

)       gs]   x

= argmin   (gs

   x) +   (x)

            (xs)x

x

   c   d

= argmin   (gs

   x) + d  (x, xs)

x   c   d

thus, we have

xs+1 = argmin   (gs

   x) + d  (x, xs) .

x   c   d

to get xs+1, in the    rst term on the right hand side we look at linear approximations
close to xs in the direction determined by the subgradient gs. if the function is linear, we
would just look at the linear approximation term. but if the function is not linear, the
linear approximation is only valid in a small neighborhood around xs. thus, we penalized
by adding the term d  (x, xs). we can penalized by the square norm when we choose
d  (x, xs) = kx     xsk2. in this case we get back the projected id119 algorithm
as an instance of mirror descent algorithm.

but if we choose a di   erent divergence d  (x, xs), we are changing the geometry and we

can penalize di   erently in di   erent directions depending on the geometry.

thus, using the mirror descent algorithm, we could replace the 2-norm in projected
id119 algorithm by another norm, hoping to get less constraining lipschitz con-
stant. on the other hand, the norm is a lower bound on the strong convexity parameter.
thus, there is trade o    in improvement of rate of convergence.

2.4.4 examples

euclidean setup:
  (x) = 1 x 2,

2 k k

the id119.

d

= dr ,

   

  (x) =   (x) = x. thus, the updates will be similar to

d  (y, x) = kyk2

    kx

1
2
= kx     yk2 .

1
2
1
2

k2

2
       y + kxk

x

thus, bregman projection with this potential function   (x) is the same as the usual eu-
clidean projection and the mirror descent algorithm is exactly the same as the projected
descent algorithm since it has the same update and same projection operator.

note that    = 1 since d

  (y, x)     2 kx     yk2.

1

   1 setup:

we look at d = dr

+ \ {0}.

75de   ne   (x) to be the negative id178 so that:

d

  (x) = x xi log(xi),

i=1

  (x) =      (x) = {1 + log(x

i)

d
}i=1

thus, looking at the update function y

(s+1)

(s)

log(xi )     (s)
  gi

and for all i = 1,          , d, we have yi

(s)    
=   (x )
   
(s+1)
(s)
= xi

  gs, we get log(yi

exp(     gi

(s)

). thus,

(s+1)

) =

y(s) = x(s) exp(     g(s)) .

we call this setup exponential id119 or mirror descent with multiplicative
weights.

the bregman divergence of this mirror map is given by

d  (y, x) =   (y)       (x)             (x)(y     x)

d

x

=

yi log(yi)    

d

x

xi log(xi)

   

i

=1
d

x

i=1

=

yi log(

yi
xi

) +

i

=
1
d

x

i=1

(yi

    xi)

d

x

i

=1

(1 + log(xi))(yi

   

xi)

note that pd

i=1 yi log( yi

x

i

and x.

) is call the id181 (kl-div) between y

we show that the projection with respect to this bregman divergence on the simplex
p
d
   d = {x     dr :
i=1 xi = 1, xi     0} amounts to a simple reid172 y 7    y/|y|1. to
prove so, we prov
ide the lagrangian:

l

=

d

x

i=1

yi log(

y
i
xi

) +

d

x

(xi

    yi) +   (

    1) .

d

x xi

i=

1

i=1

to    nd the bregman projection, for all i = 1,          , d we write

   
   xi

y
l     i
xi

=

+ 1 +    = 0

thus, for all i, we have xi =   yi. we know that pd

i=1 xi = 1. thus,    = 1
p yi

.

thus, we have     

   d(y) =

y
|y|

projection would be:

. the mirror descent algorithm with this update and

1

ys+1 = xs exp(     gs)

xs+1 =

y
|y|1

.

to analyze the rate of convergence, we want to study the    1 norm on    d. thus, we have

to show that for some   ,    is   -strongly convex w.r.t |    |1 on    d.

76d  (y, x) = kl(y, x) + x(xi

    yi)

i

= kl(y, x)

   

1
2

|x     y|2
1

where we used the fact that x, y        d to show
s,    is 1-strongly conve
inequality show the result. thu
p
remembering that   (x) = d

that     log(d)       (x)     0 for x        d. thus,

i(xi     yi) = 0 and used pinsker

p
x w.r.t. |    |1 on    d.

i=1 xi log(xi) was de   ned to be negative id178, we know

r2 = max   (x)

x      d

    min   (x) = log(d) .

x      d

corollary: let f be a convex function on    d such that

kgk        l,
then, mirror descent with    = 1 q

l

   g        f (x),

   x        d .

2 log(d)

k

gives

f (xk)     f (x   )     lr

2 log(d)

k

,

f (x   

k)     f (x   )

    lr

2 log(d)

k

boosting: for weak classi   ers f1(x),          , fn (x) and           n, we de   ne

f   =

n

x   jfj

j=1

and

f (x) =

   
      

f1(x)

.
..

fn (x)

   
      

so that f  (x) is the weighted majority vote classi   er. note that |f |        1.
as shown before, in boosting, we have:

g =    rb

n,  (f  ) = x      (   yif  (xi))(   yi)f (xi) ,

1
n

n

i=1

since |f |     1 and |y|     1, then |g|     l where l is the lipschitz constant of   

   

   
(e.g., a constant like e or 2).

   

rn,  (f     
b

k )     min rn,  (f  ) l

   

        n

b

r

2 log(n )

k

we need the number of iterations k     n2 log(n ).

   

the functions fj   s could hit all the vertices. thus, if we want to    t them in a ball, the
ball has to be radius
n . this is why the projected id119 would give the rate of
q
n
k . but by looking at the gradient we can determine the right geometry. in this case, the
gradient is bounded by sup-norm which is usually the most constraining norm in projected

77id119. thus, using mirror descent would be most bene   cial.

other potential functions:
there are other potential functions which are strongly convex w.r.t    1 norm. in partic-

ular, for

  (x) = |x|p
p,

1
p

p = 1 +

1

log(d)

then    is cplog(d)-strongly convex w.r.t    1 norm.

7818.657: mathematics of machine learning

lecturer: philippe rigollet
scribe: sylvain carpentier

lecture

14
oct. 26, 2015

in this lecture we will wrap up the study of optimization techniques with stochastic
optimization. the tools that we are going to develop will turn out to be very e   cient in
minimizing the   -risk when we can bound the noise on the gradient.

3. stochastic optimization

3.1 stochastic id76

we are considering random functions x 7       (x, z) where x is the optimization parameter and
z a random variable. let pz be the distribution of z and let us assume that x 7       (x, z) is
convex pz a.s. in particular, ie[   (x, z)] will also be convex. the goal of stochastic convex
optimization is to approach minx
is convex. for our purposes, will
be a deterministic convex set. however, stochastic id76 can be de   ned more
broadly. the constraint can be itself stochastic :

ie[   (x, z)] when

   c

c

c

c = {x, ie[g(x, z)]     0},

g convex pz a.s.

c = {x, ip[g(x, z)     0]     1       },

   chance constraint   

the second constraint is not convex a priori but remedies are possible (see [ns06, nem12]).
in the following, we will stick to the case where x is deterministic. a few optimization
problems we tackled can be interpreted in this new framework.

3.1.1 examples

boosting. recall that the goal in boosting is to minimize the   -risk:

min ie[  (
  

     

   y   (x))] ,

f

where    is the simplex of ird. de   ne z = (x, y ) and the random function    (  , z) =
  (   y f  (x)), convex pz a.s.

id75. here the goal is the minimize the    2 risk:

min ie[(y     f
     ird

  (x)) ] .

2

de   ne z = (x, y ) and the random function    (  , z) = (y     f  (x))2, convex pz a.s.

maximum likelihood. we consider samples z1, . . . , zn iid with density p  ,          . for
instance, z n (  , 1). the likelihood functions associated to this set of samples is   
7   
qn
(it does not have to be of the form p  
for some          . then

=1 p  (zi). let p   (z) denote the true density of z

   

i

1
n

ie[log

n

y

i=1

p  (zi)] =     log(

z

p   (z)
p  (z)

)p   (z)dz + c =

   kl(p   , p  ) + c

79where c is a constant in   . hence maximizing the expected log-likelihood is equivalent to
minimizing the expected id181:

maxie[log y p  (zi)]

  

n

i=1

       kl(p   , p  )

external randomization. assume that we want to minimize a function of the form

f (x) =

1
n

n

x fi(x) ,

i=1

where the functions f1, . . . , fn are convex. as we have seen, this arises a lot in empirical
risk minimization. in this case, we treat this problem as deterministic problem but inject
arti   cial randomness as follows. let i be a random variable uniformly distributed on
[n] =: {1, . . . , n}. we have the representation f (x) = ie[fi(x)], which falls into the context
of stochastic id76 with z = i and    (x, i) = fi (x).

important remark: there is a key di   erence between the case where we assume that
we are given independent random variables and the case where we generate arti   cial ran-
domness. let us illustrate this di   erence for boosting. we are given (x1, y1), . . . , (xn, yn)
in the    rst example, our aim is to minimize
i.i.d from some unknown distribution.
ie[  (   y f  (x))] based on these n observations and we will that the stochastic gradient
allows to do that by take one pair (xi, yi) in each iteration. in particular, we can use
each pair at most once. we say that we do one pass on the data.

we could also leverage our statistical analysis of the empirical risk minimizer from

previous lectures and try to minimize the empirical   -risk

r  n,  (f  ) =

1
n

n

x   (

   yif (xi))

  

i=1

by generating k independent random variables i1, . . . , ik uniform over [n] and run the
stochastic id119 to us one random variable ij in each iteration. the di   erence
here is that k can be arbitrary large, regardless of the number n of observations (we make
multiple passes on the data). however, minimizing iei[  (   yi f  (xi ))|x1, y1, . . . , xn, yn]
will perform no better than the empirical risk minimizer whose statistical performance
is limited by the number n of observations.

3.2 stochastic id119

if the distribution of z was known, then the function x 7    ie[   (x, z)] would be known and
we could apply id119, projected id119 or any other optimization tool
seen before in the deterministic setup. however this is not the case in reality where the
true distribution pz is unknown and we are only given the samples z1, . . . , zn and the
random function    (x, z). in what follows, we denote by       (x, z) the set of subgradients of
the function y 7       (y, z) at point x.

80algorithm 1 stochastic id119 algorithm

input: x1     c, positive sequence {  s}s 1, independent random variables z , . . . , z
with distribution pz .
for s = 1 to k     1 do

   

1

k

ys+1 = xs       sg  s ,
xs+1 =    (yc

s+1)

g  s           (xs, zs)

end for

return x  k =

k

1

k x xs

s=1

note the di   erence here with the deterministic id119 which returns either
in the stochastic framework, the function f (x) = ie[   (x,   )] is

k = argmin f (x).

x  k or x   

x1,...,xn

typically unknown and x  k cannot be computed.

theorem: let c be a closed convex subset of ird such that diam(c)     r. assume that
he convex function f (x) = ie[   (x, z)] attains its minimum on c at x        ird. assume
that    (x, z) is convex pz a.s. and that iekg  k2     l2 for all g             (x, z) for all x. then
if   s        = r
l

    ,
k

lr
ie[f (x  k)]     f (x   )        

k

proof.

f xs

(

)     (    )        (        )

gs xs

f x

x

= ie[g  s

   (xs     x   )|xs]

= ie[(ys+1

   

xs)   (xs

   

x   ) xs]

|

= ie[kx

s     y

2

s+1k +

kxs     x   

2
k     kys+1     x   k |xs]

2

    (  2ie[kg  sk2|xs] + ie[kx

s     x   k |xs]     ie[kxs+1     x   

2

k2

x
| s]

1
  
1
  
2
1
  
2

taking expectations and summing over s we get

k

1 x
k

s=1

f (xs)

    f x   )

(

   

  l2
2

+

r2
2  k

.

using jensen   s inequality and chosing    = r
l

k

    , we get

lr
ie[f (x  k)]     f (x   )        
k

813.3 stochastic mirror descent

we can also extend the mirror descent to a stochastic version as follows.

algorithm 2 mirror descent algorithm

input: x1     argmin
random variables z1, . . . , zk with distribution pz .
for s = 1,          , k do

  (x),    :

c   d

dr     dr such that   (x) =      (x), independent

  (ys+1) =   (xs)       g  s for g  s           (xs, zs)
s+1 =    (yc
x

s+1)

  

end for
return x = 1

k

k ps=1 xs

theorem: assume that    is   -strongly convex on c     d w.r.t. k    k and

r2 = sup   (x)

x

   c   d

    min
x   c   d

   x)

(

   c   d

  (x) (assume that it exists). then, stochastic mirror descent

take x1 = argminx
with    = r

lq 2  

r outputs   k, such that

x

ie[f (x  k)]     f (x   )     rlr 2

  k

.

proof. we essentially reproduce the proof for the mirror descent algorithm.

take x        c     d. we have

f (xs

)     f (x   )     g   (x     x   )

s

      (ys+1))    (xs     x   )|xs]

= ie[(  (xs)

s
   (xs     x   )|xs]
ie[g  s
1
  
1
  
1
  
1
  

= ie[(     (xs)          (ys+1))    (xs
= iehd
    iehd  (x

    2 ie[kg  sk2

   

  
2  

|xs] + ie

1
  

s, ys+1) + d  (x , xs)     d  (x   , xs+1) x

   

(cid:12)(cid:12)xsi
(cid:12)(cid:12) si
hd  (x   , xs)     d  (x   , xs+1)(cid:12)(cid:12)xsi

  (xs, ys+1) + d  (x   , xs)     d  (x , ys+1)

   

    x   )|xs]

82where the last inequality comes from

d  (xs, ys+1) =   (xs)       (ys+1)          (ys+1)   (xs     ys+1)

    [     (x

s)          (ys+1)]    (xs     ys+1)    

      kg  sk x

   k s     s+1k    

y

  
2

x 2
y
k s+1     sk

  
2

2
kys+1     xsk

   

sk2
    .

  2kg  
2  

summing and taking expectations, we get

k

1 x
k

s=1

[f (x

s)

   
]
    f (x )

   

  l2 d  (x   , x1)
2  

k  

+

.

(3.1)

we conclude as in the previous lecture.

3.4 stochastic coordinate descent

let f be a convex l-lipschitz and di   erentiable function on ird. let us denote by    if the
partial derivative of f in the direction ei. one drawback of the id119 algorithm
is that at each step one has to update every coordinate    if of the gradient. the idea of
the stochastic coordinate descent is to pick at each step a direction ej uniformly and to
choose that ej to be the direction of the descent at that step. more precisely, of i is drawn
uniformly on [d], then ie[d   i f (x)ei ] =    f (x). therefore, the vector d   i f (x)ei that has
only one nonzero coordinate is an unbiased estimate of the gradient    f (x). we can use
this estimate to perform stochastic id119.

algorithm 3 stochastic coordinate descent algorithm

input: x1     c, positive sequence {  s}s 1, independent random variables i , . . . , i
uniform over [d].
for s = 1 to k     1 do

   

1

k

ys+1 = xs       sd   i f (x)ei ,
xs+1 =    (ys+1)

c

g  s           (xs, zs)

end for

return x  k =

k

1

k x xs

s=1

if we apply stochastic id119 to this problem for    = rq 2 , we directly

l

dk

obtain

ie[f (x  k)]     f (x   )     rlr

2d
k

we are in a trade-o    situation where the updates are much easier to implement but where
we need more steps to reach the same precision as the id119 alogrithm.

8318.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: zach izzo

lecture

15
oct. 27, 2015

part iii

online learning

it is often the case that we will be asked to make a sequence of predictions, rather
than just one prediction given a large number of data points.
in particular, this situa-
tion will arise whenever we need to perform online classi   cation: at time t, we have
(x1, y1), . . . , (xt   1, yt   1) iid random variables, and given xt, we are asked to predict
yt     {0, 1}. consider the following examples.

online shortest path: we have a graph g = (v, e) with two distinguished vertices
s and t, and we wish to    nd the shortest path from s to t. however, the edge weights
e1, . . . , et change with time t. our observations after time t may be all of the edge weights
e1, . . . , et; or our observations may only be the weights of edges through which our path
traverses; or our observation may only be the sum of the weights of the edges we   ve traversed.

dynamic pricing: we have a sequence of customers, each of which places a value vt
on some product. our goal is to set a price pt for the tth customer, and our reward for
doing so is pt if pt     vt (in which case the customer buys the product at our price) or 0
otherwise (in which case the customer chooses not to buy the product). our observations
after time t may be v1, . . . , vt; or, perhaps more realistically, our observations may only be
1i(p1 < v1), . . . , 1i(pt < vt). (in this case, we only know whether or not the customer bought
the product.)

i=1

sequential investment: given n assets, a portfolio is           n = {x     irn : xi    
0,pn x
i = 1}. (   tells what percentage of our funds to invest in each stock. we could
also allow for negative weights, which would correspond to shorting a stock.) at each time
t, we wish to create a portfolio   t        n to maximize   t
t zt, where zt     irn is a random
variable which speci   es the return of each asset at time t.

there are two general modelling approaches we can take: statistical or adversarial.
statistical methods typically require that the observations are iid, and that we can learn
something about future points from past data. for example, in the dynamic pricing example,
we could assume vt     n (v, 1). another example is the markowitz model for the sequential
investment example, in which we assume that log(zt)     n (  ,   ).

in this lecture, we will focus on adversarial models. we assume that zt can be any
bounded sequence of numbers, and we will compare our predictions to the performance of
some benchmark. in these types of models, one can imagine that we are playing a game
against an opponent, and we are trying to minimize our losses regardless of the moves he
plays. in this setting, we will frequently use optimization techniques such as mirror descent,
as well as approaches from game theory and id205.

841. prediction with expert advice

1.1 cumulative regret

let a be a convex set of actions we can take. for example, in the sequential investment
example, a =    n . if our options are discrete   for instance, choosing edges in a graph   then
think of a as the convex hull of these options, and we can play one of the choices randomly
according to some distribution. we will denote our adversary   s moves by z. at time t,
we simultaneously reveal at     a and zt     z. denote by    (at, zt) the loss associated to the
player/decision maker taking action at and his adversary playing zt.

in the general case, pn

   =1 (at, zt) can be arbitrarily large. therefore, rather than looking
t
at the absolute loss for a series of n steps, we will compare our loss to the loss of a benchmark
called an expert. an expert is simply some vector b     an, b = (b1, . . . , bt, . . . , b t
n) . if we
choose k experts b(1), . . . , b(k), then our benchmark value will be the minimum cumulative
loss amongst of all the experts:

benchmark = min
   j   k

1

the cumulative regret is then de   ned as

n

x    (bt

(j)

, zt).

t=1

n

rn = x

t=1

   (at, zt)     min
1   j   k

n

x

t=1

(j)

   (bt

, zt).

at time t, we have access to the following information:

1. all of our previous moves, i.e. a1, . . . , at   1,

2. all of our adversary   s previous moves, i.e. z1, . . . , zt   1, and

3. all of the experts    strategies, i.e. b(1), . . . , b(k).

t

t , where b

naively, one might try a strategy which chooses a = b   
is the expert which
has incurred minimal total loss for times 1, . . . , t     1. unfortunately, this strategy is easily
exploitable by the adversary: he can simply choose an action which maximizes the loss for
that move at each step. to modify our approach, we will instead take a convex combination
of the experts    suggested moves, weighting each according to the performance of that expert
thus far. to that end, we will replace    (at, zt) by    (p, (bt, zt)), where p        k denotes a
)t     ak is the vector of the experts    moves at time
convex combination, bt = (bt
t, and zt     z is our adversary   s move. then

, . . . , bt

(k)

(1)

   

rn = x    (pt, zt)     min

1   j   k

n

t=1

n

x

t=1

   (ej, zt)

where ej is the vector whose jth entry is 1 and the rest of the entries are 0. since we are
restricting ourselves to convex combinations of the experts    moves, we can write a =    k.
we can now reduce our goal to an optimization problem:

k

      k x   j

min
  

j=1

n

x    (ej , zt).

t=1

85from here, one option would be to use a projected id119 type algorithm: we
de   ne

qt+1 = pt       (   (e

t
1, zt), . . . ,    (ek , zt ))

and then p

t+1 =   

k

   

(pt) to be the projection of qt+1 onto the simplex.

1.2 exponential weights

suppose we instead use stochastic mirror descent with    = negative id178. then

qt+1,j = pt+1,j exp(        (ej , zt)),

pt+1,j =

where we have de   ned

qt

pk

l

q=1 t+1,l

,

pt =

k   wt,j
x
k w
l=1

p

j=1

!

,

ej

,l
t

wt,j =

exp

  t   1
!
      x    (ej, zs)

.

s=1

s from each expert and downweights it exponentially according
this process looks at the los
to the fraction of total loss incurred. for this reason, this method is called an exponential
weighting (ew) strategy.

recall the de   nition of the cumulative regret rn:

n

rn = x    (pt, zt)     min

1   j   k

then we have the following theorem.

t=1

n

x    (ej, zt).

t=1

theorem: assume    (  , z) is convex for all z     z and that    (p, z)     [0, 1] for all p    
   k , z     z. then the ew strategy has regret

rn    

in particular, for    = q 2 log k ,

n

log k   n
2

+

  

.

rn     p2n log k.

proof. we will recycle much of the mirror descent proof. de   ne

k

ft(p) = x pj   (ej, zt).

j=1

denote k    k := |    |1. then

n

1 x
n

t=1

ft(

pt)     f

t

(
p

   

)     n pt=1

   1

n

2

kg k2
   

t

+

log k

  n

,

86where gt        ft(pt) and k    k    is the dual norm (in this case k    k    = |    |   ). the 2 in the
denominator of the    rst term of this sum comes from setting    = 1 in the mirror descent
proof. now,

gt        ft(pt)     gt = (   (e1, zt), . . . ,    (e

t
k , zt)) .

furthermore, since    (p, z)     [0, 1], we have kgtk    = |gt|        1 for all t. thus

n

n pt=1 kgtk2
   1

   

substituting for ft yields

2

+

log k   
2

n  

    +

log k

  n

.

n k

xx

t=1 j=1

pt,j   (ej, zt)     min x pj   (ej, zt)    

x

k n

p      k

j=1

t=1

  n
2

+

log k

  

.

note that the boxed term is actually min1   j   k pn
jensen   s to the unboxed term gives

t=1 (ej, zt). furthermore, applying

   

n k

n

xx pt,j   (ej, zt)     x    (pt, zt).

substituting these expressions then yields

t=1 j=1

t=1

rn    

  n
2

+

log k

  

.

we optimize over    to reach the desired conclusion.

we now o   er a di   erent proof of the same theorem which will give us the optimal

constant in the error bound. de   ne

wt,j = exp      

  t   1
x

!
   (ej , zs)

, wt =

s=1

k

x

j=1

wt,j,

pt =

p wt,je

k
j=1

j

wt

.

for t = 1, we initialize w1,j = 1, so w1 = k. it should be noted that the starting values for
w1,j are uniform, so we   re starting at the correct point (i.e. maximal id178) for mirrored
descent. now we have

log

(cid:18)

w
t+1
w

t

(cid:19) = lo
g

   p
   

(cid:16)
k exp    
j=1
p

p
t   1
, z
s=1 (ej
   
xp(cid:16)     
pj
k
= e
1
l
= log (iej    pt [exp(        (ej , zt))])

  

(cid:17)
s) exp(        (ej , zt))

(cid:17)
s)

, z

    e
( l

   1
t
=1

   
  
   

hoe   ding   s lemma         log (cid:16)   

1 2
8

e

ie
e      j

   (e ,zt)

j

(cid:17)

=

jensen   s        

  2
8
  2
8

      iej    (ej , zt)

         (iej ej , zt) =

  2
8

         (pt, zt)

87since iej ej = pk p
with

j=1 t,jej. if we sum over t, the sum telescopes. since w1 = k, we are left

log(wn+1)     log(k)    

n  2
8

n

       x    (pt, zt).

t=1

log(wn+1) = log

   
   

k

x exp

 

n

      x    (ej , zs)

j=1

s=1

!   
   

,

we have

so setting

j    = argmin1   j   k p    (e
j, zt), we obtain
 

n
t=1

 

n

log(wn+1)     log

exp

     

x    (ej   , zs)

!!

n

=       x    (ej    , zt).

s=1

t=1

rearranging, we have

n

x

t=

1

   (pt, zt)    

n

x (ej    , zt)    

   

t=1

  n
8

+

log k

  

.

finally, we optimize over    to arrive at

   = r 8 log k

n

   

rn    

r n l g k

o
2

.

the improved constant comes from the assumption that our loss lies in an interval of size
1 (namely [0, 1]) rather than in an interval of size 2 (namely [   1, 1]).

8818.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: haihao (sean) lu

16
lecture
nov. 2, 2015

recall that in last lecture, we talked about prediction with expert advice. remember
that l(ej, zt) means the loss of expert j at time t, where zt is one adversary   s move. in this
lecture, for simplexity we replace the notation zt and denote by zt the loss associated to all
experts at time t:

zt =

       (e1, zt)    
   
   
   
   

   (ek , zt)

...

,

whereby for p        k, p
j=1 j   ( j, zt). this gives an alternative de   nition of ft(p)
in last lecture. actually it is easy to check ft(p) = p   zt, thus we can rewrite the theorem
for exponential weighting(ew

) strategy as

   zt = k p

e

p

rn     x p   

t zt     min

      k x    zt

p

p

    p

2n log k,

n

t=1

n

t=1

where the    rst inequality is jensen inequality:

n

x p   
z zt

t=1

n

    x    (pz, zt) .

t=1

we consider ew strategy for bounded convex losses. without loss of generality, we
assume    (p, z)     [0, 1], for all (p, z)        k    z, thus in notation here, we expect pt        k
and z
. indeed if    (p, z)     [m, m ] then one can work with a rescaled loss    (a, z) =
   (a,z)   m . note that now we have bounded gradient on pt, since zt is bounded.
m    m

t     [0, 1]

  

k

2. follow the perturbed leader (fpl)

in this section, we consider a di   erent strategy, called follow the perturbed leader.

at    rst, we introduce follow the leader strategy, and give an example to show that

follow the leader can be hazardous sometimes. at time t, assume that choose

pt = argmin
p      k

t   1

x p   zs.

s=1

note that the function to be optimized is linear in p, whereby the optimal solution should
be a vertex of the simplex. this method can be viewed as a greedy algorithm, however, it
might not be a good strategy.

consider the following example. let k = 2, z1 = (0,   )   , z2 = (0, 1)   , z3 = (1, 0)   ,
z4 = (0, 1)    and so on (alternatively having (0, 1)    and (1, 0)    when t     2), where    is small
enough. then with following the leader strategy, we have that p1 is arbitrary and in the
best case p1 = (1, 0)   , and p2 = (1, 0)   , p3 = (0, 1)   , p4 = (1, 0)    and so on (alternatively
having (0, 1)    and (1, 0)    when t     2).

89in the above example, we have

n

x

t=1

p   
t zt min
p    k

   

   

n

x

t=1

p   zt

               

n

1

n
2

n
2

   

1 ,

which gives raise to linear regret.

now let   s consider fpl. fpl regularizes fl by adding a small amount of noise, which

can guarantee square root regret under oblivious adversary situation.

algorithm 1 follow the perturbed leader (fpl)

input: let    be a random variables uniformly drawn on [0, 1 ]k.
for t = 1 to n do

  

pt = argmin
p      k

t   1

x

s=1

(cid:0)p   zs +   (cid:1).

end for

we analyze this strategy in oblivious adversaries, which means the sequence zt is chosen
ahead of time, rather than adaptively given. the following theorem gives a bound for regret
of fpl:

theorem: fpl with    =    1 yields expected regret:

kn

   

ie  [rn]     2

2nk .

before proving the theorem, we introduce the so-called be-the-leader lemma at    rst.

lemma: (be-the-leader)
for all id168    (p, z), let

p   
t = arg min

t

p      k x

s=1

   (p, zs) ,

then we have

n

x    (p   

t , zt)

n

    x    (pn

    , zt)

t=1

t=1

proof. the proof goes by induction on n. for n = 1, it is clearly true. from n to n + 1, it

90follows from:

n+1

x    (pt

   , zt) = x   (p   

   
t , zt) +    (pn

+1, zn+1)

t=1

i=1

n

n

    x   (p   

n, zt) +    (p   

n+1, zn+1)

i=1

n

    x   (p   

n+1, zt) +    (p   

n+1, zn+1) ,

where the    rst inequality uses induction and the second inequality follows from the de   nition
of p   
n.

i=1

proof of theorem. de   ne

qt = argmin p   (   +

p      k

t

x zs) .

s=1

using the be-the-leader lemma with

   (p, zt) =

(cid:26) pt (   + z1)

pt zt

if t = 1
if t > 1 ,

we have

n

      + x qt
q1

   zt     min q   (   +

q

      k

n

x zt) ,

t=1

whereby for any q        k,

t=

1

n

x(cid:16)
   zt     q   zt(cid:17)     (cid:16)q        q1
qt

   (cid:17)        kq

i

=1

    q1

k k  k        ,
1

2
  

where the second inequality uses h  older   s inequality and the third inequality is from the
fact that q and q1 are on the simplex and    is in the box.

now let

and

therefore,

qt = arg min p     

p      k

t

   + zt + x zs

!

s=1

pt = arg min p   

p

      

k

 

   + 0

+

!

.

t

x zs

s=1

n

ie[rn]     xp   
t zt

i

=1

    min
p      k

n

x

i=1

p   zt

n (cid:16)
    x qt

   zt     p   t zt + xie[(pt

(cid:17)

n

i=1

i

=1

    + xie[(pt     qt)   zt] ,

2
  

n

i=1

    q )   zt]

t

(2.1)

91where p    = arg minp      k pn
now let

t=1

   

p z
t.
     

h(  ) = zt

arg min p   [   +

p      k

t   1

!
x zs]

,

s=1

then we have a easy observation that

ie[zt

   (pt     qt)] = ie[h(  )]     ie[h(   + zt)] .

hence,

ie[zt

   (pt     q

k

t)] =    z
      k z

     [0, 1 ]k

  

     [0, 1 ]k

  

h(  )d          z

k

h(  )d  

     z +[0, 1 ]k

t

  

h(  )d  

\nz

t+[0   

, 1 ]k

o

      k z

     [0, 1 ]k \nzt+[0, 1 ]k

  

  

1d  

o

= ip (   i     [k],   (i)     zt(i))

k

    x ip(cid:18)unif (cid:18) 1

[0,

i=1

      kzt(i)       k ,

  

](cid:19)     zt(i)
(cid:19)

(2.2)

where the    rst inequality is from the fact that h(  )     0, the second inequality uses

h(  )     1, the second equation is just geometry and the last inequality is due to zt(i)     1.

combining (2.1) and (2.2) together, we have

ie[rn]     +   kn .

2
  

in particular, with    = q 2 , we have

kn

   

ie[rn]     2

2kn ,

which completes the proof.

92online learning with structured experts   a biased

survey

g  abor lugosi

icrea and pompeu fabra university, barcelona

93on-line prediction

a repeated game between forecaster and environment.
at each round t,

i

the forecaster chooses an action t 2{ 1, . . . , };
(actions are often called experts)
the environment chooses losses `t(1), . . . ,` t(n) 2 [0, 1];
the forecaster su   ers loss `t(it).

n

the goal is to minimize the regret

rn = xn

=1

t

`t(it)   min
   n

i

! .

n

x `t(i )

t=1

7

94simplest example

is it possible to make (1/n)rn ! 0 for all loss assignments?
let n = 2 and de   ne, for all t = 1, . . . , n,

`t(1) =    0 if it = 2

1 if it = 1

and `t(2) = 1   `t(1).
then

so

xn

t=1

`t(it) =n

and min
i =1 2
,

1
1
id56   .
2

n

`

x t(i )

t=1

n
2

   

95randomized prediction

key to solution: randomization.
at time t, the forecaster chooses a id203 distribution
pt 1 = (p1,t 1, . . . , pn,t 1)
and chooses action i with id203 pi ,t 1.
simplest model: all losses `s(i ), i = 1, . . . , n, s < t, are
observed: full information.

12

96hannan and blackwell

hannan (1957) and blackwell (1956) showed that the forecaster
has a strategy such that

1 xn

=1

n

t

`t(it)

  min
   n

i

! ! 0

n

x `t(i )

t=1

almost surely for all strategies of the environment.

13

97basic ideas

expected loss of the forecaster:

by martingale convergence,

1
n

n

n

`

i =1

`t(pt 1) =x pi ,t
 xn
t(it)  x
n xn

`t(pt

t=1

t=1

=1

1

`

t

( 1`t i ) =e t`t(it)

!

t 1) = op(n  )

1 2

/

t(p

) min

 1      n

i

!

` (i )

n

x t

t=1

so it su ces to study

98weighted average prediction

idea: assign a higher id203 to better-performing actions.
vovk (1990), littlestone and warmuth (1989).

a popular choice is

, . . . ,n .

s

exp       pt 1
=1 exp       p

`t(pt 1)   min

i

   

i

s i )

= 1

=1 ` (
t 1
k
=1 `s( )
s

   
! =rln n
   nx `t(i )

2n

t=1

n

n
k

pi ,t = 1

where    > 0. then

p
n 
1 xn
with     =p8 ln n/n.

t=1

99proof

t

n

let li ,t =ps=1 `s(i ) and
wt =x
 
xn

wn
w
0

= ln

i =1

i =1

ln

for t   1, andw 0 = n. first observe that

wi ,t =

e    li ,t

xn

i =1

    li n

,

e

  ln    max e    li ,n
=      min li ,n

i =1,...,n

i =1,...,n

!   ln n
      n
ln
  ln n .

100proof

on the other hand, for each t = 1, . . . ,n

ln

wt
wt 1

= ln

   

 

 

i =1 wi ,t 1e    `t (i )

pn
pn
p
p

j =1 wj ,t 1
n
t i )
=1 w
i ,t 1` (
i
n
j =1 wj ,t 1
   2
8

   

=     `t(pt 1) +

+

   2
8

by hoe   ding   s inequality.
hoe   ding (1963): if x 2 [0, 1],

ln ee    

x         ex +

   2
8

101proof

for each t = 1, . . . , n

ln

w
t
w
t 1

        `t(p

t

 1

) +

2

   
8

summing over t = 1, . . . , n,

ln

w
n
w0

n

        x

t=1

`t(pt 1) + n .

 

2

   
8

combining these, we get

`t(pt 1)     min li ,n +

i =1,...,n

 

ln n
   

+ n

   
8

xn

t=1

102lower bound

the upper bound is optimal: for all predictors,

idea: choose `t(i ) to be i.i.d. symmetric bernoulli coin    ips.

sup

n,n,`t (i )

n
=1 t(i )
t

`

1 .

 

t

i   n

) min

(n/2) ln np

pn
=1 `t(itp 
sup xn
"
  e x `t(it)   min
n
2   min bi

   nx `t(i )
xn

  min

`t(it)

   n

t=1

t=1

t=1

t=1

t (i )

=

n

n

i

i

`

i   n

!

`t(i )

#

where b1, . . . , bn are independent binomial (n, 1/2).
use the central limit theorem.

103follow the perturbed leader

t 1

it = arg minx `s(i ) + zi ,t

i =1,...,n s=1

where the zi ,t are random noise variables.
the original forecaster of hannan (1957) is based on this idea.

104follow the perturbed leader

if the zi ,t are i.i.d. uniform [0, pnn], then

1

id56     2r + op(n 1/2) .

n
n

if the z

i ,t are i.i.d. with density (   /2)e    | |, then for

z

       plog n/n,

n     cr

r

log n

n

+ op(n  ) .

1/2

1
n

kalai and vempala (2003).

105combinatorial experts

often the class of experts is very large but has some combinatorial
structure. can the structure be exploited?

   google. all rights reserved. this content is excluded from our
creative commons license. for more information, see
http://ocw.mit.edu/fairuse.

path planning. at each time
instance, the forecaster chooses a
path in a graph between two
   xed nodes. each edge has an
associated loss. loss of a path is
the sum of the losses over the
edges in the path.
n is huge!!!

106assignments: learning permutations

given a complete
bipartite graph
km,m, the
forecaster chooses a
perfect matching.
the loss is the sum
of the losses over
the edges.

this image has been removed due to copyright restrictions.
please see the image at
http://38.media.tumblr.com/tumblr_m0ol5tggjz1qir7tc.gif

helmbold and warmuth (2007): full information case.

107spanning trees

the forecaster chooses a
spanning tree in the complete
graph km. the cost is the sum
of the losses over the edges.

108combinatorial experts

d

t

formally, the class of experts is a set s   { 0, 1} of cardinality
|s| = n.
2 rd
at each time , a loss is assigned to each component: `t
loss of expert v 2s is `t(v ) =` >t v .
forecaster chooses it 2s .
the goal is to control the regret

.

xn

t=1

`t(it)   min
k=1,...,n

`t(k) .

xn

t=1

109computing the exponentially weighted average forecaster

one needs to draw a random element of s with distribution
proportional to

wt(v ) = exp 

t

     lt(v )  = exp 
 

exp

 1

!
     x `>t v .
! .
     x `t,j vj

s=1
 1

s=1

jyd

=

=1

t

110computing the exponentially weighted average forecaster

path planning: sampling may be done by id145.

assignments: sum of weights (partition function) is the permanent
of a non-negative matrix. sampling may be done by a fpas of
jerrum, sinclair, and vigoda (2004).

spanning trees: propp and wilson (1998) de   ne an exact sampling
algorithm. expected running time is the average hitting time of
the markov chain de   ned by the edge weights wt(v ).

111computing the follow-the-perturbed leader forecaster

in general, much easier. one only needs to solve a linear
optimization problem over s. this may be hard but it is well
understood.

in our examples it becomes either a shortest path problem, or an
assignment problem, or a minimum spanning tree problem.

112follow the leader: random walk perturbation

suppose n experts, no structure. de   ne

t

it = arg min

i =1,...,n x  

s=1

(`i ,s 1 + xs)

where the xs are either i.i.d. normal or   1 coin   ips.
this is like follow-the-perturbed-leader but with random walk
advantage: foprecaster rarely changes actions!
perturbation:

t
s=1 xt.

113follow the leader: random walk perturbation

if rn is the regret and cn is the number of times it 6= it 1, then

ern     2ecn     8p2n log n + 16 log n + 16 .

devroye, lugosi, and neu (2015).
key tool: number of leader changes in n independent random
walks with drift.

114follow the leader: random walk perturbation

this also works in the    combinatorial    setting: just add an
independent n(0, d ) at each time to every component.

and

ern = oe(b 3/2pn log d )

ecn = o(bpn log d ) ,

where b = maxv2s kvk1.

115why exponentially weighted averages?

may be adapted to many di   erent variants of the problem,
including bandits, tracking, etc.

116multi-armed bandits

the forecaster only observes `t(it) but not `t(i ) for i 6= it.

this image has been removed due to copyright restrictions. please see the image at
https://en.wikipedia.org/wiki/herbert_robbins#/media/file:1966-herbertrobbins.jpg

herbert robbins (1952).

117multi-armed bandits

trick: estimate `t(i ) by

this is an unbiased estimate:

) =

`t(it)

i =i
{ t
pit ,t 1

}

t(i

`e
xn

=1

j

et`t(i ) =

e

pj ,t 1
 

`t(j )
p t 1
j

,

{j =i} = `t(i )

use the estimated losses to de   ne exponential weights and mix
with uniform (auer, cesa-bianchi, freund, and schapire, 2002):

pi ,t

 

1 = (

1    

)

p
|

exp

n
k=1 exp

 1 ` ( )
s=1

   

     pt
   
    p
{z

t 1

   
es i
s=1 `s(k)   
e
}

exploitation

+

 
n

|{z}

exploration

118multi-armed bandits

e  

1
n

xn

t=1

`t(pt 1)   minxn

   n

t=1

 

i

`t(i )! = o rn ln n! ,

n

119multi-armed bandits

lower bound:

sup e  xn

1
`t (i ) n

=1

t

n

`t(pt 1)   minx `t(i )

i n
    =1

 

t

!   cr n

n

,

dependence on n is not logarithmic anymore!
audibert and bubeck (2009) constructed a forecaster with

   n n 

max e
i

1

n

x

t=1

`t(pt

 1)

 

n

x

t=1

`t(i ) = o

!

n

 r n!

,

120calibration

sequential id203 assignment.
a binary sequencex 1, x2, . . . is revealed one by one.
after observing x1, . . . ,x t 1, the forecaster issues prediction
it 2{ 0, 1, . . . , n}.
meaning:    chance of rain is it/n   .
forecast is calibrated if

n
t=1 xt
t=1

  p 
pn
 
 

{it =i}
{it =i}

 

1
2n

+ o(1)

i

n         

0
{it =i > .

}

whenever lim supn(1/n)pn

t=1

is there a forecaster that is calibrated for all possible sequences?
no. (dawid, 1985).

121randomized calibration

however, if the forecaster is allowed to randomize then it is
possible! (foster and vohra, 1997).

this can be achieved by a simple modi   cation of any regret
minimization procedure.
set of actions (experts): {0, 1, . . . ,n }.
at time t, assign loss `t(i ) = (xt   i /n)2 to action i .
one can now de   ne a forecaster. minimizing regret is not
su cient.

122internal regret

recall that the (expected) regret is

xn

t=1

`t(pt

1)   min
i

 

xn

t=1

`t(i ) = max

i

n

x pj ,t (`t(j )   `t(i ))
t=1xj

the internal regret is de   ned by

n

maxx pj ,t (`t(j )   `t(i ))

t=1

i ,j

pj ,t (`t(j )   `t(i )) = et

{it =j (}

`

t(j )   `t(i ))

is the expected regret of having taken action j instead of action i .

123internal regret and calibration

by guaranteeing small internal regret, one obtains a calibrated
forecaster.

this can be achieved by an exponentially weighted average
forecaster de   ned over n 2 actions.
can be extended even for calibration with checking rules.

124prediction with partial monitoring

for each round t = 1, . . . , n,

the environment chooses the next outcome jt 2{ 1, . . . ,m }
without revealing it;
the forecaster chooses a id203 distribution pt
 1
draws an action it 2{ 1, . . . ,n } according to pt 1;
the forecaster incurs loss `(it, jt) and each action i incurs loss
`(i , jt). none of these values is revealed to the forecaster;
the feedback h(it, jt) is revealed to the forecaster.

and

h = [h(i , j )]n   m is the feedback matrix.
l = [`(i , j )]n   m is the loss matrix.

125examples

dynamic pricing. here m = n, andl = [`(i , j )]n   n where

`

i
( , ) =

j

(j

)

  i

{i   j + c

}
n

{i >j

} .

and h(i , j ) = {i >j or}
i
{     }

h(i , j ) =a

j + b

,

i >j

{

}

i , j = 1, . . . , n .

multi-armed bandit problem. the only information the forecaster
receives is his own loss: h = l.

126examples

apple tasting. = = 2.

n m

l =    0 1
1 0  
h =    a a
b c   .

the predictor only receives feedback when he chooses the second
action.
label e cient prediction. n = 3, m = 2.

1 0 35
l =2 1 1
4 0 1
h =2 a b
c 3 .
4 c
c 5

c

127a general predictor

a forecaster    rst proposed by piccolboni and schindelhauer (2001).
crucial assumption: h can be encoded such that there exists an
n     n matrix k = [k(i , j )]n   n such that

l = k    h .

thus,

then we may estimate the losses by

`(i , j ) =xn

l =1

k(i , l )h(l , j ) .

k(i , it)h(it, jt)

pit ,t

.

(i , jt) =

`e

128a general predictor

observe

et`(i , jt) =

e

=

pk,t 1

k(i , k)h(k, jt)

pk,t 1

k

(i , k)h(k, jt) =` (i , jt) ,

k=1

xn
kxn

=1

let

`e(i , jt) is an unbiased estimate of `(i , jt).
e
e    li ,t 1
n
=1 e    lek,t 1
k
where li ,t =pt

pi ,t 1 = (1    )

=1 `(i , jt).

p

s

e

e

+

 
n

129performance bound

with id203 at least 1    ,

n

t=1

`(it, jt)

  min
i =1,...,n

1xn
xn
    cn 1/3n 2/3pln(n/ ) .

1
n t=1

 

`(i , jt)

where c depends on k . (cesa-bianchi, lugosi, stoltz (2006))
hannan consistency is achieved with rate o(n 1/3) whenever
l = k    h.
this solves the dynamic pricing problem.
bart  ok, p  al, and szepesv  ari (2010): if m = 2, only possible rates
are n 1/2, n 1/3, 1

130imperfect monitoring: a general framework

s is a    nite set of signals.
feedback matrix: h : {1, . . . ,n }   { 1, . . . ,m }!p (s).
for each round t = 1, 2 . . . ,n ,

the environment chooses the next outcome jt 2{ 1, . . . ,m }
without revealing it;
the forecaster chooses pt 1 and draws an action
it 2{ 1, . . . ,n } according to it;
the forecaster receives loss `(it, jt) and each action i su   ers
loss `(i , jt), none of these values is revealed to the forecaster;
a feedbacks t drawn at random according to h(it, jt) is
revealed to the forecaster.

131target

de   ne

`(p, q) =x pi qj `(i , j )

i ,j

h(  , q) = (h(1, q), . . . ,h (n, q))

where h(i , q) = j qj h(i , j ) .

denote by f the setp of those   that can be written as h(  , q) for

some q.
f is the set of    observable    vectors of signal distributions  .
the key quantity is

   (p,  ) = max

q : h(  ,q)= 

`(p, q)

    is convex in p and concave in  .

132rustichini   s theorem

the value of the base one-shot game is

min max `(p, q) = min max    (p,  )
p

q

p  2f

if qn is the empirical distribution of j1, . . . ,j n, even with the
knowledge of h(  , qn) we cannot hope to do better than
minp    (p, h(  , qn)).
rustichini (1999) proved that there exists a strategy such that for
all strategies of the opponent, almost surely,

lim sup0@ 1 x `(it, jt)   min     (p, h(

t=1,...,n

!1

n

p

n

  , qn))

1a     0

133rustichini   s theorem

rustichini   s proof relies on an approachability theorem for a
continuum of types (mertens, sorin, and zamir, 1994).

it is non-constructive.

it does not imply any convergence rate.

lugosi, mannor, and stoltz (2008) construct e ciently computable
strategies that guarantee fast rates of convergence.

134combinatorial bandits

, 1} of cardinality |s| = n.
d
, a loss is assigned to each component: `t 2 r .

d

the class of actions is a set s   { 0
at each time t
loss of expert v 2s is `t(v ) =` >t v .
forecaster chooses it 2s .
the goal is to control the regret

xn

t=1

`t(it)   min
k=1,...,n

n

x `t(k) .

t=1

135combinatorial bandits

three models.
(full information.) all d components of the loss vector are
observed.

(semi-bandit.) only the components corresponding to the chosen
object are observed.

(bandit.) only the total loss of the chosen object is observed.
challenge: is o(n 1/2poly(d )) regret achievable for the
semi-bandit and bandit problems?

136combinatorial prediction game

adversary

player

137combinatorial prediction game

adversary

player

   google. all rights reserved. this content is excluded from our
creative commons license. for more information, see
http://ocw.mit.edu/fairuse.

138combinatorial prediction game

adversary

player

   google. all rights reserved. this content is excluded from our
creative commons license. for more information, see
http://ocw.mit.edu/fairuse.

139combinatorial prediction game

adversary

`1

`2

`3

`8

`4

`5

`6

`7

`9

`d 2

`d 1
`d

player

   google. all rights reserved. this content is excluded from our
creative commons license. for more information, see
http://ocw.mit.edu/fairuse.

140combinatorial prediction game

adversary

`1

`2

`3

`8

`4

`5

`6

`7

`9

`d 2

`d 1
`d

player

   google. all rights reserved. this content is excluded from our
creative commons license. for more information, see
http://ocw.mit.edu/fairuse.

loss su   ered: `2 + `7 + . . . + `d

141combinatorial prediction game

adversary

feedback:

`1

`2

`3

`8

`4

`5

`6

`7

`9

<8 full info: `1,` 2, . . . ,` d
:

`d 2

`d 1
`d

player

   google. all rights reserved. this content is excluded from our
creative commons license. for more information, see
http://ocw.mit.edu/fairuse.

loss su   ered: `2 + `7 + . . . + `d

142combinatorial prediction game

adversary

b ck: 8 full i fo:

`1,` 2, . . . ,`
d
semi-bandit: `2,` 7, . . . ,` d
bandit:

n

`2 + `7 + . . . + `d

feed a

<
:

`5

`

4

`6

`7

`9

`1

`2

`3

`8

`d 2

`d 1
`d

player

   google. all rights reserved. this content is excluded from our
creative commons license. for more information, see
http://ocw.mit.edu/fairuse.

loss su   ered: `2 + `7 + . . . + `d

143notation

`1

`2

`3

`8

`1

`2

`3

`8

`4

`5

`6

`7

`9

`4

`5

`6

`7

`9

regret:

`d 2

`d 1
`d

`d 2

`d 1
`d

r

n =

s   { 0, 1}d

`t 2 rd

+

vt 2s

, loss su   ered: `t vt

t

n

ex

t=1
|   

u

t v  
`t
1 for all v

t min
2s
2s

n

ex t

`t u

t=1
and t = 1, . . . ,n .

t
loss assumption: `t v

|

144weighted average forecaster

at time t assign a weight wt,i to each i = 1, . . . ,d .
the weight of each v k 2s is
w t(k) =

wt,i .

i :vky(i )=1

let qt 1(k) =w t 1(k

at each time t, drawk t fromp the distribution

n
k=1 w t 1(k).

)/

pt 1(k) = (1    )qt

k 1( ) +    (k)
where    is a    xed distribution on s and  > 0. here

where lt,i = `1,i +

e

e

+ `t,i and  `t,i is a n estimated loss.
e
wt,i = exp      lt,i
e

e

        

145loss estimates

dani, hayes, and kakade (2008).
de   ne the scaled incidence vector

x t = `t(kt)v kt
where kt is distributed according to pt 1.

let pt 1 = e   v kt v > be the

hence

   

kt

pt 1

  (i , j ) =

d     d correlation matrix.

k : vk (ix pt 1(k) .

)=vk (j )=1

similarly, let qt 1 and m be the correlation matrices of e v v >
when v has law, qt 1 and   . then

   

   

pt 1(i , j ) = (1    )qt 1(i , j ) +   m(i , j ) .

the vector of loss estimates is de   ned by
t 1x t

where p +

t 1 is the pseudo-inverse of pt 1.

e`t = p +

146key properties

m m +v = v for all v 2s .
qt 1 is positive semide   nite for every t.
pt 1 p +
t 1v = v fo
 
by de   nition,

2s .

and

ll t

r a

 

v

et x t = pt 1 `t

and therefore

an unbiased estimate!

t 1et x t = `t

ete`t = p +

147performance bound

the regret of the forecaster satis   es

1   

n

where

elb
n   min ln(k)

k=1,...,n

        s   d  min(m)

2b 2

2

+ 1

d n

ln

    n

.

 min(m) =

min

x2span(s):kxk=1

t
x mx > 0

is the smallest    relevant    eigenvalue of m. (cesa-bianchi and
lugosi, 2009.)
large  min(m) is needed to make sure no `t,i

is too large.

|e |

148performance bound

other bounds:

sampling is over a barycentric spanner.

bpd ln n/n (dani, hayes, and kakade). no condition on s.
dp(    ln n)/n (abernethy, hazan, and rakhlin). computationally

e cient.

149eigenvalue bounds

 min(m) =

min

x2span(s):kxk=1

e(v , x)2 .

where v has distribution    over s.
in many cases it su ces to take    uniform.

150multitask bandit problem

the decision maker acts in m games in parallel.
in each game, the decision maker selects one of r possible actions.
after selecting the m actions, the sum of the losses is observed.

 min =

1
r

max e ln   ln(k)
k

hberving the su
i

    2mp3nr

ln r .

the price of only obs

m of losses is a factor of m.

generating a random joint action can be done in polynomial time.

151assignments

perfect matchings of km,m.
at each time one of the n = m! perfect matchings of km,m is
selected.

 min(m) =

1

m   1

only a factor of mh

max e ln   ln(k)     2m 3n ln(m!) .
k

i
aive full-ipnformation bound.
wborse than n

152spanning trees

in a network of m nodes, the cost of communication between two
nodes joined by edge e is `t(e) at time t. at each time a minimal
connected subnetwork (a spanning tree) is selected. the goal is to
minimize the total cost. n = mm 2.

 min(m) =

the entries of m are

1

m      m2    .

o

1

p{vi = 1}=
vi = 1, vj = 1 =
 

p vi = 1, vj = 1 =

p 
 

2
m
3
m2
4
m
2

if

if

i     j
i 6    j

.

153stars

at each time a central node of a network of m nodes is selected.
cost is the total cost of the edges adjacent to the node.

 min   1   o   m    .

1

154cut sets

a balanced cut ink 2m is the collection of all edges between a set

of m vertices and its comp lement. each balanced cut has m2
edges and there are n = m  balanced cuts.
   m2   

 min(m) =   o

1
4

2m

1

.

choosing from the exponentially weighted average distribution is
equivalent to sampling from ferromagnetic ising model. fpas by
randall and wilson (1999).

155hamiltonian cycles

a hamiltonian cycle in km is a cycle that visits each vertex exactly
once and returns to the starting vertex. n = (m   1)!

2
 min   m

e cient computation is hopeless.

156sampling paths

in all these examples    is uniform over s.
for path planning it does not always work.

what is the optimal choice of   ?
what is the optimal way of exploration?

157minimax regret

rn = inf

max

sup rn

strategy s   {0,1}d adversary

theorem
let n   d 2. in the full information and semi-bandit games, we
have

0.008 dpn     rn     dp2n,

and in the bandit game,

0.01 d 3/2pn     rn     2 d 5/2p2n.

158proof

upper bounds:
information but it is only optipmal up to a logarithmic factor in the
d = [0, +1)d , f (x) = 1
semi-bandit case.
in the bandit case it does not work at all! exponentially weighted
average forecaster is used.

xi works for full

d
i =1 xi l

og

   

lower bounds:
careful construction of randomly chosen set s in each case.

159a general strategy

be a convex subset of r with nonempty interior int(d).

let d
a function f : d! r is legendre if

d

    f is strictly convex and admits continuous    rst partial
derivatives on int(d),
    for u 2 @d, andv 2 int(d), we have

lim (u   v t) rf (1   s)u + sv = +

 

s!0,s>0

1.

the bregman divergence df : d    int(
legendre function f is

d) associated to a

df (u

, v ) =f (u)   f (v )   (u   v ) rf (v ).

t

160cleb (combinatorial learning with bregman divergences)

parameter: f legendre on d  conv (s)
(1) wt0+1 2d :

rf (w0+1) =rf (

t

)

wt   `  t

(2) wt+1 2 arg min df (w , wt0+1)

w2conv (s)

(3) pt+1 2  (s) :w t+1 = ev   pt+1v

pt+1

 (s)

pt

d

w0t+1

wt

wt+1

conv (s)

161general regret bound for cleb

theorem
if f admits a hessian r2f always invertible then,
   r f (wt)

df (s) +e x `t

rn / diam

  t

=1

n

2

t

 1   
`t.

   

162di   erent instances of cleb: linexp (id178 function)

d

= [0, + )d , f (x) = 1
   

1

semi-bandit=bandit: exp3
auer et al. [2002]

i =1 xi log xi
full info: exponentially weighted average

pd
8>><
>>:
8>>>
>>>> koolen, warmuth and kivinen [2010]
><>>
>> kale, reyzin and schapire [2010]
>>>>:

full info: component hedge

bandit: new algorithm

semi-bandit: mw

163di   erent instances of cleb: lininf (exchangeable
hessian)

d = [0, +1)d , f (x) =pd

i =1r xi   1( )

  s ds

0

inf, audibert and bubeck [2009]

     (x) = exp(   x) :linexp
 (x) = (    x) q, q > 1 : linpoly

164di   erent instances of cleb: follow the regularized leader

d = conv (s), then

wt+1 2 arg min xt

w2d

s=1

  t`s w + f (w )

!

particularly interesting choice: f self-concordant barrier function,
abernethy, hazan and rakhlin [2008]

16518.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: haihao (sean) lu

8
1
lecture
nov. 2, 2015

3. stochastic bandits

3.1 setup

the stochastic multi-armed bandit is a classical model for decision making and is de   ned
as follows:

there are k arms(di   erent actions). iteratively, a decision maker chooses an arm k    
{1, . . . , k}, yielding a sequence xk,1, . . . , xk,t, . . ., which are i.i.d random variables with
mean   k. de   ne       = maxj   j or         arg max. a policy    is a sequence {  t}t   1, which
indicates which arm to be pulled at time t.   t     {1, . . . , k} and it depends only on the
observations strictly interior to t. the regret is then de   ned as:

n

x

n

x

rn = max ie[ xk,t]     ie[ x  t,t]

k

t=1

t=1

n

= n          ie[x x  t,t]

t=1

= n          ie[ie[

n

x x  t,t |   t]]

t=1

k

= x    kie[tk(n)] ,

k=1

where    k =             k and tk(n) = pt=1 1i(  t = k) is the number of time when arm k was
pulled.

n

3.2 warm up: full info case

assume in this subsection that k = 2 and we observe the full information

time t after choosing   t. so in each iteration, a normal idea is to choose
highest average return so far. that is

  t = argmax xk,t

  

k=1,2

x1,t

.
..

   
      

xk,t
the arm

   
      

at

with

where

  xk,t =

1
t

t

x

s=1

xk,s

assume from now on that all random variable xk,t are subgaussian with variance proxy
for all u     ir. for example, n (0,   2) is subgaussian with

  2, which means ie[eux

]     e 2

2 2

u   

166variance proxy   2 and any bounded random variable x     [a, b] is subgaussian with variance
proxy (b     a)2/4 by hoe   ding   s lemma.

therefore,

where     =   1       2. besides,

rn =    ie[t2(n)] ,

(3.1)

n

t2(n) = 1 + x 1i(x2,t > x1,t)

  

  

t=2

= 1 + x   

1i(x2,t     x1,t     (  2       1)        ) .

  

n

t=2

it is easy to check that (x2,t     x1,t)     (  2       1) is centered subgaussian with variance proxy
2  2, whereby

  

  

  
ie[1i(x

    t   
2,t > x1,t)]     e 4  

  

2

2

by a simple cherno    bound. therefore,

   

rn        (1 + x

t=0

2

    t   
e

24   )         +

4  2
   

,

(3.2)

whereby the benchmark is

rn         +

4  2
   

.

3.3 upper con   dence bound (ucb)

without loss of generality, from now on we assume    = 1. a trivial idea is that after s
pulls on arm k, we use     k,s = 1 pj   {pulls of k} xk,j and choose the one with largest     k,s.
the problem of this trivial policy is that for some arm, we might try it for only limited
times, which give a bad average and then we never try it again. in order to overcome this
limitation, a good idea is to choose the arm with highest upper bound estimate on the mean
of each arm at some id203 lever. note that the arm with less tries would have a large
deviations from its mean. this is called upper con   dence bound policy.

s

167algorithm 1 upper con   dence bound (ucb)

for t = 1 to k do

  t = t
end for
for t = k + 1 to n do

t   1

tk(t) = x 1i(  t = k)

s=1

(number of time we have pull arm k before time t)

t   1

    k,t = x xk,t   s

1

tk(t)

s=1

  t     argmax

k   [k]

(
    k,t + 2

s

)

,

log t)
(
2
tk(t)

end for

theorem: the ucb policy has regret

rn     8

x log n
   k

k,   k>0

  2
+ (1 + )
3

k

x    k

k=1

proof. from now on we    x k such that    k > 0. then

ie[tk(n)] = 1 +

n

x ip(  t = k) .

t=

k+1

note that for t > k,
s

{  t = k}     {    k,t + 2

2 log t
tk(t)

s

           ,t + 2

2 log t
t   (t)

}

(

    {  k         k,t + 2

s 2 log t
tk(t)

s

{                 ,t + 2

[

}

2 log t
t   (t)

}[{            k + 2

s

)
,   t = k}

2 log t
tk(t)

and from a union bound, we have

ip(    k,t       k <    2

s 2 log t
tk(t)

) = ip(    k,t       k < 2

s

2 log t
tk(t)

)

   s 8 log t

s

2

)

t

    x exp(

s=1
1
t3

=

168thus ip(  k >     k t + 2
whereby

,

q

2 log t
t (t)

k

)     1
3 and similarly we have ip(      >        ,t + 2
t

q

2 log t
t
   (t)

)     1 ,

3
t

n

x

t=k+1

ip(  t = k)     2

n

n

x + x ip(         

s

  k + 2

2 log t
tk(t)

,   t = k)

8 log t

   2
k

,   t = k)

g n

8 lo
   2
k

,   t = k)

1
t3

1
t3

t=1
   

t=1
   

t=1

n

    2x + x ip(tk(t)    

t=1

n

+ x

ip(tk(t)    

    2x 1
t3

    2

t=1
   

x 1
t

3

t=1
   

t=1
   

x

s=1

+

+

    2x 1
t2

t=1

=

  2
3

+

8 log n

   2
k

8 log n

   2
k

,

ip s    

(

8 o
l g n
   2
k

)

where s is the counter of pulling arm k. therefore we have

k

rn = x

k

=1

   kie[tk(n)]    

x

k,   k

>0

   k(1 +

  2
3

+

8 log n

   2
k

) ,

which furnishes the proof.

consider the case k = 2 at    rst, then from the theorem above we know rn     log n ,   
which is consistent with intuition that when the di   erence of two arm is small, it is hard to
distinguish which to choose. on the other hand, it always hold that rn     n   . combining
these two results, we have rn    
up to a constant.
actually it turns out to be the optimal bound. when k     3, we can similarly get the
result that rn     p
. this, however, is not the optimal bound. the optimal bound
should be p log(n/h)

, which includes the harmonic sum and h =

    n   , whereby rn    

1 . see [lat15].

pk    2

log(n   2 )

log(n   2)

log n

   

   

   

   

k

k

k

k

k

k

3.4 bounded regret

from above we know ucb policy can give regret that increases with at most rate log n with
n. in this section we would consider whether it is possible to have bounded regret. actually
it turns out that if there is a known separator between the expected reward of optimal arm
and other arms, there is a bounded regret policy.

we would only consider the case when k = 2 here. without loss of generality, we

assume   1 =     and   

2

2 =     , then there is a natural separator 0.

   
2

169algorithm 2 bounded regret policy (brp)

  1 = 1 and   2 = 2
for t = 3 to n do

if maxk     k,t > 0 then

then   t = argmaxk     k,t

else

  t = 1,   t+1 = 2

end if
end for

theorem: brp has regret

rn         +

16
   

.

proof.

note that

ip(  t = 2) = ip(    2,t > 0,   t = 2) + ip(    2,t     0,   t = 2)

n

x ip(    2,t > 0,   t = 2)     ie

t=3

n

x

t=3

1i(    2,t > 0,   t = 2)

n

    iex 1i(    2,t       2 > 0,   t = 2)

t=3

   

    x 2
e    s   

8

s=1
8
   2

,

=

where s is the counter of pulling arm 2 and the third inequality is a cherno    bound.
similarly,

n

n

x ip(    2,t     0,   t = 2) = x ip(    1,t     0,   t   1 = 1)

t=3

t=3
8
   2

,

   

combining these two inequality, we have

rn        (1 +

16
   2

) ,

17018.657: mathematics of machine learning

lecturer: alexander rakhlin

scribe: kevin li

lecture

19
nov. 16, 2015

4. prediction of individual sequences

in this lecture, we will try to predict the next bit given the previous bits in the sequence.
given completely random bits, it would be impossible to correctly predict more than half
of the bits. however, certain cases including predicting bits generated by a human can
be correct greater than half the time due to the inability of humans to produce truly
random bits. we will show that the existence of a prediction algorithm that can predict
better than a given threshold exists if and only if the threshold sati   es certain probabilistic
inequalities. for more information on this topic, you can look at the lecture notes at
http://stat.wharton.upenn.edu/~rakhlin/courses/stat928/stat928_notes.pdf

4.1 the problem

to state the problem formally, given a sequence y1, . . . , yn, . . .     {   1, +1}, we want to    nd
a prediction algorithm y  t = y  t(y1, . . . , yt 1) that correctly predicts yt as much as possible.

   

in order to get a grasp of the problem, we will consider the case where y1, . . . , yn     ber(p).
it is easy to see that we can get

iid

"

ie

n

1

n x =

{y  t

1

=1
t

#
}

yt

    min{p, 1     p}

by letting y  t equal majority vote of the    rst t     1 bits. eventually, the bit that occurs
with higher id203 will alway
s have occurred more times. so the central limit theorem
shows that our loss will approach min{

p, 1     p} at the rate of o(     ).n

1

knowing that the distribution of the bits are iid bernoulli random variables made the
prediction problem fairly easy. more surprisingly is the fact that we can achieve the same
for any individual sequence.

claim: there is an algorithm such that the following holds for any sequence y1, . . . , yn, . . ..

lim sup x {y  t = yt}     min{y  n, 1

1

    y   }     0 a.s.

n

1
n

n

t=1

n

      

it is clear that no deterministic strategy can achieve this bound. for any deterministic
strategy, we can just choose yt =    y  t and the predictions would be wrong every time. so
we need a non-deterministic algorithm that chooses q  t = ie[y  t]     [   1, 1].

to prove this claim, we will look at a more general problem. take a    xed horizon n     1,
and function    : {  1}n     r. does
there exist a randomized prediction strategy such that
for any y1, . . . , yn

ie[ x {y  t = yt

1

1
n

n

t=1

}]       (y1, . . . , yn) ?

1716
6
6
for certain    such as        0, it is clear that no randomized strategy exists. however for
       , the strategy of randomly predicting the next bit (q  t = 0) satis   es the inequality.

1
2

lemma: for a stable   , the following are equivalent

a)    (q  t)t=1,...,n   y1, . . . , yn

ie[ x {y  t = yt}]       (y1, . . . , yn)

1

1
n

n

t=1

b) ie[  (  1, . . . ,   n)]     where   1, . . . ,   n are rademacher random variables

1
2

where stable is de   ned as follows

de   nition (stable function): a function    : {  1}n     is stable if

r

|  (. . . , yi, . . .)       (. . . ,    yi, . . . )|    

1
n

b) suppose ie   < . take (y1, . . . , yn) = (  1, . . .

   

proof. (a =
  t}] = > ie[  ] so there must exist a sequence (
  (  1, . . . ,   n).

1
2

  1, . . . ,   

n) such that ie[

,   n). then ie[ 1
n

1
n

pt

n
=1

1
2

p
n
t=1 {  t =
{y  t =   t}] >

y

1

1

(b =    a) recu

rsively de   ne v (y1, . . . , yt) such that    y1, . . . , yn

v (y1, . . . , yt 1) = min max (cid:16) ie[ {y  t = yt}] + v (y1, . . . y

1
qt   [   1,1] yt     1 n

   

1

(cid:17)
n)

p
n
looking at the de   nition, we can see that ie[ 1
t=1 {y  t = yt}] = v ( )
n
now we note that v (y1, . . . , yt) =     t     ie[  (y1,
. . , yt,   t
.
de   nition since

        v (y1, . . . , yn).
+1, . . . ,   n)] satis   es the recursive

n

2

1

min max ie[

1

{
y  t = yt ]

}    

q  t

ie[  (y1, . . . , yt,   t+1, . . . ,   n)]

   

t
2n

1
yt n
   

q  t

yt

= min max

q  tyt
2n
q  
= min m x{    t
2n

a

q  t

    ie[  (y1, . . . , yt,   t+1, . . . ,   n)]

   

t

1
   
2n

   

ie[  (y1, . . . , yt 1, 1,   t+1, . . . ,   n)]    

   

t

1 q
  
   
t
2n 2n

,

    ie[  (y .
1,

. . , yt    1,   t+1, . . . ,   n)]

   1,

   

t

1
   
2n

}

=     ie[  (y1, . . . , yt 1,   t,   t+1, . . . ,   

   

n)]

   

t

1
   
2n

=v (y1, . . . , yt 1)   

a = b} =    ab , the second uses the
the    rst equality uses the fact that for a, b     {  1}, {
fact that yt     {  1}, the third minimizes the entire expression by choosing q  t so that the
two expressions in the max are equal. here the fact that    is stable means q  t     [   1, 1] and
is the only place where we need    to be stable.

1

2

1

therefore we have
ie[ x

n

1

1
n

t=1

{y  t = yt}] = v (   )   v (y1, . . . , yn) =    ie[  (  1, . . . ,   n)]+ +  (y1, . . . , yn)

      (y1, . . . , yn)

1
2

1726
6
6
6
6
6
6
6
by b).

by choosing    = min{y  , 1     y  } + c    , this shows there is an algorithm that satis   es our

n

original claim.

4.2 extensions

4.2.1 supervised learning

we can extend the problem to a regression type problem by observing xt and trying to
predict yt. in this case, the objective we are trying to minimize would be

1

n x (y  t, yt)     inf

f    f

l

1

n x l(f (xt) yt)

,

it turns out that the best achievable performance in such problems is governed by martin-
gale (or, sequential) analogues of rademacher averages, covering numbers, combinatorial
dimensions, and so on. much of statistical learning techniques extend to this setting of
online learning. in addition, the minimax/relaxation framework gives a systematic way of
developing new prediction algorithms (in a way similar to the bit prediction problem).

4.2.2 equivalence to tail bounds

we can also obtain probabilistic tail bound on functions    on hypercube by using part a) of
the earlier lemma. rearranging part a) of the lemma we get 1     2  (
q  tyt.
this implies

y1, . . . , yn)    

p

1
n

ip(cid:0)

  (  1, . . . ,   n) <

1

  

    (cid:1) = ip(cid:0)1     2  (  1, . . . ,   n) >        ip

(cid:1)

2

(cid:0)

so ie       1 =    existence of a strategy =    tail boun

2

d for   

1

2

  

(cid:1)

n x q  t  t >        e    2n
1< .
2

we can extend the results to higher dimensions. consider z1, . . . , zn     b2 where b2 is
a ball in a hilbert space. we can de   ne recursively y  0 = 0 and y  t+1 = projb2(y  t     1    zt).
based on the properties of projections, for every        
    .n

, we have 1 ph          

i     1

y , zt

b2

yt

y

n

n

taking y   

=

k

z
t

p
p ztk

,

   z1, . . . , zn, x zt

k

n

n

   

k     n     x y  t,

zt
h     i

take a martingale di   erence sequence z1, . . . , zn with values in b2. then

t=1

t=1

n

ip(cid:0)kx

zt

t=1

integrating out the tail,

   

k     n >   (cid:1)     ip(x

n

hy  t,    zti >   )     e    2

2

n  

t=1

n

iekx zt

   

k     c

n

t=1

173it can be shown using von neumann minimax theorem that

   (y  t)   z1, . . . , zn, y        b2 x

hy  t     y   , zti    

n

t
=1

sup

mdsw

1,...,w

n

e

n

kx

t=1

wt

k    

   
c n

where the supremum is over all martingale di   erence sequences (mds) with values in b2.
by the previous part, this upper bound is c
n. we conclude an interesting equivalence of
(a) deterministic statements that hold for all sequences, (b) tail bounds on the size of a
martingale, and (c) in-expectation bound on this size.

   

in fact, this connection between probabilistic bounds and existence of prediction strate-

gies for individual sequences is more general and requires further investigation.

17418.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: vira semenova

lecture

20
nov. 23, 2015

in this lecture, we talk about the adversarial bandits under limited feedback. adver-
sarial bandit is a setup in which the id168 l(a, z) : axz is determinitic. lim-
ited feedbacid116 that the information available to the dm after the step t is it =
{l(a1, z1), ..., l(at   1, zt)}, namely consits of the realised losses of the past steps only.

5. adversarial bandits

consider the problem of prediction with expert advice. let the set of adversary moves be z
and the set of actions of a decision maker a = {e1, ..., ek }. at time t, at     a and zt     z are
simultaneously revealed.denote the loss associated to the decision at     a and his adversary
playing zt by l(at, zt). we compare the total loss after n steps to the minimum expert loss,
namely:

min
1 j k
       

n

x lt(ej , zt),

t=1

where ej is the choice of expert j     {1, 2, .., k}.
the cumulative regret is then de   ned as

rn = x lt(at, zt)     min x lt(ej, zt)

n

1   j   k

t=1

n

t=1

   

the feedback at step t can be either full or limited. the full feedback setup means that
the vector f = (l(e1, z
t), ..., l(ek , zt)) of losses incurred at a pair of adversary   s choice zt and
each bandit ej     {e1, ..., ek } is observed after each step t. hence, the information available
to the dm after the step t is i =    t
t    )}. the limited feedbacid116
that the time    t feedback consists of the realised loss l(at, zt) only. namely, the information
available to the dm is it = {l(a1, z1), ..., l(at, zt)}. an example of the    rst setup is portfolio
optimization problems, where the loss of all possible portfolios is observed at time t. an
example of the second setup is a path planning problem and dynamic pricing, where the
loss of the chosen decision only is observed. this lecture has limited feedback setup.

{l(a1, zt), ..., l(ak , z

t =1

t

   

   

the two strategies, de   ned in the past lectures, were exponential weights, which yield
n log k and follow the perturbed leader. we would like to

   

the regret of order rn     c
play exponential weights, de   ned as:

pt,j

=

1

  

   

t
s=1 (ej, zs))

p     l

exp(
l=1 exp(      pt   1 l

s=1 (ej, zs))

pk

this decision rule is not feasible, since the loss l(ej, zt) are not part of the feedback if

ej = at. we will estimate it by

  
l(ej, zt) =

l(ej, zt)1i(a

t = ej)

p (at = ej)

1756
lemma: l(ej, zt) is an unbiased estimator of l(ej, zt)

  

p

roof. e

atl(ej , zt) = p
  

k
k=1

l(e

i(

k,zt)1 k et)
e =
p (at=ej)

p

(a = e ) = l(e , z )
t

j

t

k

de   nition (exp 3 algorithm): let    > 0 be    xed. de   ne the exponential weights
as

p

(j)
t+1,j =

    pt   1
  

exp(
l=1 exp(      pt   1   l

  
l(ej , zs))
s=1 (ej, zs))

s=1

pk

(exp3 stands for exponential weights for exploration and exploitation.)
we will show that the regret of exp3 is bounded by

k
times bigger than the bound on the regret under the full feedback. the
k multiplier is
the price of have smaller information set at the time t. the are methods that allow to get
2nk is the
rid of log k term in this expression. on the other hand, it can be shown that
optimal regret.

2nk log k. this bound is

   

   

   

   

proof. let wt,j = exp(

t,j, and p = j=1
w

t

t

.

pk

w e

t,j j

log(

w
t+1
w

t

    pt
  

) = log(

1

l(e ,

k
j=1

pk

   1   
=
s

j zs)), w

t = p w
l   e , z
t
s=1 ( j
s)) exp(
xp(    p    1
t
s=1 (ej, z
= log(iej    pt exp(      x l(ej , zs)))

(      p    1
pk e

exp

j=1

j=1

t   1

  l

  

  

   

  l   e , z

( j

t))

s))

s=1

      

log(1    

  

iej    ptl(ej , z

s) + iej    ptl (ej , zs)

  
2

  

  2
2

)

(5.1)

(5.2)

(5.3)

where

   

inequality is obtained by plugging in iej    ptl(ej , zt) into the inequality

  

.

exp x     1       x +

  2x2

2

iej    ptl(ej zt) = x   

pt,jl(ej , zt) = x pt,j

  

,

j=1

j=1

l(ej, zt)1i(a

e )
t = j

p (at = ej)

= l(at, zt)

(5.4)

k

k

k

k

iej    ptl (ej , zt) = x pt,jl2(ej , zt) = x pt,j

  
2

  

l2

(ej, zt)1i(a

t = ej

)

p 2(at = ej)

(5.5)

(5.6)

j=1
l2(ej, zt)

pat,t

=

   

1

pat,t

j=1

summing from 1 through n, we get
log(wt+1)    

1)        p l

log(

t=1 (at, zt) +   

w

n

2

2 p 1

p

a ,t

t

.

176t

1

j

p
p

=

pa ,t pk

for t = 1, we initialize w1,j = 1, so w1 = k.
since ie

j,t k
wn+1)     log k           pt
n
l a
=1 ( t, zt) + 2
j=1 exp(      pt   1 l e , z
s))
t=1 (ej, zt), we obtain:

ie log(
noting that log(wn+1) = log(pk
and de   ning j    = argmin1   j   k pn

s=1 ( j

  2kn

= , the expression above becomes

j=1

  

l

j,t

log(wn+1)     log(x exp(      x l(ej, zs))) =       x l(ej, zs)

k

t   1

t   1

j=1

s=1

s=1

together:

the choice of    :=

   

x l(at, zt)     min

1

   j   

k

n

=1
t

t=1

n

x l(ej, z )

t

   

2 log knk yields the bound rn

log k   kn

+

2

  

   

    2k log kn.

17718.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: ali makhdoumi

lecture

21
nov. 25, 2015

6. linear bandits

recall form last lectures that in prediction with expert advise, at each time t, the player
plays at     {e1, . . . , ek} and the adversary plays zt such that l(at, zt)     1 for some loss
function. one example of such id168 is linear function l(at, zt) = at
t zt where |zt|       
1. linear bandits are a more general setting where the player selects an action at     a     rk,
where a is a convex set and the adversary selects z
t     z such that |zt at|     1. similar to
the prediction with expert advise, the regret is de   ned as

t

rn = e" n

x tat zt

#

t=1

    min
a   k

n

x

t=1

ta zt,

where at is a random variable in a. note that in the prediction with expert advise, the set
t. however, in
a was essentially a polyhedron and we had min
the linear bandit setting the minimizer of at zt can be any point of the set a and essentially
the umber experts that the player tries to    comp
ete    with are in   nity. similar, to the
prediction with expert advise we have two settings:

t = min1   j   k e zj

n
t=1

at z

p

a   k

t

1 full feedback: after time t, the player observes zt.

2 bandit feedback: after time t, the player observes at

t zt, where at is the action that

player has chosen at time t.

we next, see if we can use the bounds we have developed in the prediction with expert
advise in this setting.
in particular, we have shown the following bounds for prediction
with expert advise:

1 prediction with k expert advise, full feedback: rn

    2n log k.

2 prediction with k expert advise, bandit feedback: rn

   

    2nk log k.

the idea to deal with linear bandits is to discretize the set a. suppose that a is bounded
(e.g., a     b2, where b2 is the l2 ball in rk). we can use a 1 -covering of
a which we
have shown to be of size (smaller than)o(nk). this means there exist y1, . . . , y|n | such that
for any a     a, there exist yi such that ||yi     a||     1 . we now can bound the regret for
general case, where the experts can be any point in a, based on the regret on the discrete
set, n = {y1, . . . , y|n |},as follows.

n

n

x tat zt

rn = e" n
" n

=
t 1

= e x tat zt

t=1

#

#

    min
a   a

    min
a   n

n

x

t=1

n

x

t=1

ta zt

ta zt + o(1).

therefore, we restrict actions at to a combination of the actions that belong to {y1, . . . , y|n |}
(we can always do this), then using the bounds for the prediction with expert advise, we
obtain the following bounds:

   

178   
1 linear bandit, full feedback: rn     p2n log(nk) = o(

   

of dependency to n is of order o(

n) that is what we expect to have.

kn log n), which in terms

2 linear bandit, bandit feedback: rn     p2nnk log(nk) =    (n), which is useless in

terms of dependency of n as we expect to obtain o(

n) behavior.

   

the topic of this lecture is to provide bounds for the linear bandit in the bandit feedback.
problem setup: let us recap the problem formulation:

    at time t, player chooses action at     a     [   1, 1]k.

    at time t, adversary chooses zt     z     rk, where at

t zt = hat, zti     [0, 1].

    bandit feedback: player observes hat, zti( rather than zt in the full feedback setup).

literature: o(n3/4) regret bound has been shown in [bb04]. later on this bound has
been improved to o(n2/3) in [bk04] and [vh06] with    geometric hedge algorithm   , which
we will describe and analyze below. we need the following assumption to show the results:
assumption: there exist    such that   e1, . . . ,   ek     a. this assumption guarantees that
a has full-dimension around zero.
we also discretize a with a 1 -net of size cnk and only consider the resulting discrete set
and denote it by a, where |a|     (3n)k. all we need to do is to bound

n

rn = e" n

x tat zt

#

t=1

for any t and a, we de   ne

n

    minx ta zt.

a   a

t=1

t(
p a) =

(cid:16)   

exp

p

a   a exp

p
  
(cid:16)     

(cid:17)
t   1
s=1   t
z
s a
s a(cid:17)
p
t   1
s=1   t
z

,

where    is a parameter (that we will
d z  t is de   ned to incorporate the idea of
exploration versus exploitation. the algorithm which is termed geometric hedge algorithm
is as follows:
at time t we have

choose later) an

    exploitation: with id203 1        draw at according to pt and let z  t = 0.

    exploration: with id203    let at =   ej

k

k
2 h
  

a

  

t, zt a

j)
(
k
i t = z
   t

e
j.

for some 1

    j     k and z  t =

note that    is the the parameter that we have by assumption on the set a, and    and   
are the parameters of the algorithm that we shall choose later.

theorem: using geometric hedge algorithm for linear bandit with bandit feedback,
with    = 1
n

1/3 and    q lo n

g
4/3 , we have
kn

=

e[rn]    

cn plog

2/3

n k

3/2

.

179proof. let the overall distribution of at be qt de   ned as qt = (1       )pt +   u , where u is a
uniform distribution over the set {  e1, . . . ,   ek}. under this distribution, z  t is an unbiased
estimator of zt, i.e.,

eat   qt[z  t] = 0(1    

  ) + x zt ej = zt.

j)

   k (
k   

k

j=1

following the same lines of the proof that we had for analyzing exponential weight algorithm,
we will de   ne

wt =

x

a   a

1

  t   
x

  

e p
x    

!

.

t
a   s
z

s=1

we then have

log (cid:18) wt+1(cid:19) = log

wt

 x

a   

a

pt(a) exp

(cid:0)   

t

  a z  t

(cid:1)

!

e   

x   1   x+ x

2

2

   

log

 x

a   a

( )(cid:18)1    
pt a

t

  a zt

   + 2( t

2
   a z  s)

1
2

(cid:19)!

 

1 + x )(cid:18)

(
pt a

= log

a   a

    t

  a zt

   + 2( t

2
   a z  t)

1
2

(cid:19)!

log(1+x)   x

    x

a   a

a)(cid:18)     a z  t +    (a z  t)

t

t

2

(cid:19) .

pt(

1 2
2

taking expectation from both sides leads to

eat   qt

(cid:20)

log (cid:18)

wt+1
wt

(cid:19)(cid:21)          eat   qt

"

x

a   a

pt(a) ta z  t

#

+

  2
2

"

eat   qt x

a   a

2
pt(a)(a z  t)

t

#

=    

  e

t   pt at z  t(cid:3) +

(cid:2)

t

a

2

  
2

e

a    q

t

t

"

x

a

   a

pt(a)( ta z  t)
2

#

qt=(1     )pt+  u

=

   
  
      
1

eat   qt (cid:2) t
at z  
t

(cid:3) +   

  
   

1

  

e

a

t

t

at zt   1      
1       

   

eat   qt at z  t +

t

(cid:2)

(cid:3)

    
1    

  

+

  2
2

eat

   qt

   u

a

(cid:2)
"x

a   a

t
t z  

(cid:3)
t + e

2

  
2

a

t   qt

"

x

a   a

2
pt(a)(a z  t)

t

#

pt(a)(a z  

t

2
t)

#

.

we next, take summation of the previous relation for t = 1 up to n and use a telescopic
cancellation to obtain

e [log w

+ ]     [log w1]    
n 1

e

  
1    

e

  

n

"x at z  t

t

t=1

#

+

    
   

  

1

n + e

  2
2

n

"x

x

t=1 a

   a

pt(a)(a z  

t

2
t)

#

    e [log

w1

]       e"x t

n

at z  
t

t=1

#

+

    
   

1

  

n + e

  2
2

n

"x

x

t=1 a   a

2
pt(a)(a z  t)

t

#

.

(6.1)

180note that for all a        a we have
 

log(wn+1) = log

using e[z  s] = zs, leads to

 

x exp      

n

x ta z  s

!!

a

   a

s=1

n

          x

h    a , z  s

s=

1

e [log(wn+1)]           x    a , zs .
i

h

n

we also have that

s=1

log(w1) = log |a|     2k log n.

plugging (6.2) and (6.3) into (6.1), leads to

e[rn]    

  

1       

n + e" n

  
2

x x pt a)( t
a

(

t=1 a   a

#

+

t)2
z  

2
k log

n

  

.

i
.

(6.2)

(6.3)

(6.4)

it remains to control the quadratic term e(cid:2)p
| (j)zt

|     1 to obtain

|, | (j)at

n
t=1

p

a   a t( )(

p a at z 2
  t)

(cid:3)

. we use the fact that

e" n

x x

t=1 a   a

n

= x x

t=1 a   a

pt(a)

(j)

|ajzt

n

|   1

    x x

pt(a)(a z  t) =

t

n

# x

2

x

p a)e
t(

2
qt[(a z  t) ]

t

t=1 a

   a

2

k

(cid:18)

   
   (1       )0 + x (cid:19) [ j
(a)(cid:18)

= n

(cid:19)

  
k

k
  

j=1

p

2

.

t

k2
  

k
  

(j)

a zt

   
   

2
]

plugging this bound into (6.4), we have

t=1 a   a

e[rn]       n + n +

   k2
2
  

2k log n

  

.

letting

   = 1
n1

/3 and    =

q

log n
kn4/3 leads to

e[

rn]     ck

3/2 2/3

n plog n.

literature: the bound we just proved has been improved in [vkh07] where they show
o(d3/2   
n log n) bound with a better exploration in the algorithm. the exploration that we
used in the algorithm was coordinate-wise. the key is that we have a linear problem and we
can use better tools from id75 such as least square estimation. however, we will
describe a slightly di   erent approach in which we never explore and the exploration is com-
pletely done with the exponential weighting. this approach also gives a better performance
in terms of the dependency on k. in particular, we obtain the bound o(d
n log n) which
coincides with the bound recently shown in [bck 12] using a john   s ellipsoid.

   

181theorem: let ct = eat   qt[a at
t zt)ct at, and    = 0 (so that pt = qt). using
geometric hedge algorithm with    = 2q log n for linear bandit with bandit feedback
leads to

t ], z  t = (at

n

t

   1

e[rn]     ckpn log n.

proof. we follow the same lines of the proof as the previous theorem to obtain (6.4). note
that the only fact that we used in order to obtain (6.4) is unbiasedness, i.e., e[z  t] = zt,
which holds here as well since

e[z  

t] = [

   e

ct atat zt] =    1c e

t

1

t

tatat ]zt = zt.
[

note that we can use pseudo-inverse instead of inverse so that invertibility is not an issue.
therefore, rewriting (6.4) with    = 0, we obtain

e[rn]     eat   pt

  
2

" n
x x

t=1 a a

   

2
pt(a)(a z  t)

t

#

+

2k log n

  

.

we now bound the quadratic term as follows

n

"x x pt(a)(a z  t)

t

2

#

n

= x x p a)e

t(

eat   pt

t=1 a   a

t=1 a   a

2
at   pt (a z  t)

t

(cid:2)

(cid:3)

c t

t =c

t, z  t=(at zt)ct at

t

   1

=

n

x

x

p

t=1 a

   a

|a

t

t zt|   1

   

n

x x t    1
a c e

p (a)

t

t

(cid:2)

t

   1
atat ct a

(cid:3)

t=1 a
   a
n

[a
e

t ]=ct
tat
=

x

x

t    1
pt(a)a ct a

a) t

a e

t(

(cid:2)

z   t
t   
z
t

n

(cid:3) x
a =

x

pt(

a) t

(cid:2)
a e (

t
at zt) ct atat ct

   1

t    1

2

(cid:3)

a

t=1 a   a

t=1 a

   a

=

n

x x

t=1 a   a

p

( )tr( t    1
t a

a c a)

t

tr(ab)=tr(ba)

= x x pt(a)t

r(    

t
ct aa )

1

n

n

= x tr(    1c e

t

a   pt[

taa ]) = x tr(    1ct ct) =

x

tr(ik) = kn.

n

t=1 a   a
n

t=1

t=1

t=1

plugging this bound into previous bound yields

e[rn]     nk +

  
2

2k log n

  

.

letting    = 2q , leads to e[rn]

log n

n

    ck

   

n log n.

18218.657: mathematics of machine learning

lecturer: philippe rigollet

scribe: aden forrow

lecture

22
nov. 30, 2015

7. blackwell   s approachability

7.1 vector losses

david blackwell introduced approachability in 1956 as a generalization of zero sum game
theory to vector payo   s. born in 1919, blackwell was the    rst black tenured professor at
uc berkeley and the seventh black phd in math in the us.

recall our setup for online linear optimization. at time t, we choose an action at        k
and the adversary chooses zt     b   (1). we then get a loss    (at, zt) = hat, zti. in the full
information case, where we observe zt and not just    (at, zt), this is the same as prediction
with expert advice. exponential weights leads to a regret bound

rn     r n

2

log(k).

the setup of a zero sum game is nearly identical:

    player 1 plays a mixed strategy p        n.

    player 2 plays q        m.

    player 1   s payo    is p   m q.

here m is the game   s payo    matrix.

theorem: von neumann minimax theorem

max min    p m q = min max    p m q.
p      n q      m

q      m p      n

the minimax is called the value of the game. each player can prevent the other from doing
any better than this. the minimax theorem implies that if there is a good response pq to
any individual q, then there is a silver bullet strategy p that works for any q.

corollary: if    q        n,    p such that p   m q     c, then    p such that    q, p   m q     c.

von neumann   s minimax theorem can be extended to more general sets. the following
theorem is due to sion (1958).

theorem: sion   s minimax theorem let a and z be convex, compact spaces, and
f : a    z     r. if f (a,   ) is upper semicontinuous and quasiconcave on z    a     a and

183f (  , z) is lower semicontinuous and quasiconvex on a    z     z, then

inf sup f (a, z) = sup inf f (a, z).
a   a z   z

z    az    a

(note - this wasn   t given explicitly in lecture, but we do use it later.) quasiconvex and
quasiconcave are weaker conditions than convex and concave respectively.

blackwell looked at the case with vector losses. we have the following setup:

    player 1 plays a     a

    player 2 plays z     z

    player 1   s payo    is    (a, z)     dr

we suppose a and z are both compact and convex, that    (a, z) is bilinear, and that
k   (a, z)k     r    a     a, z     z. all norms in this section are euclidean norms. can we
translate the minimax theorem directly to this new setting? that is, if we    x a set s     dr ,
and if    z    a such that    (a, z)     s, does there exist an a such that    z    (a, z)     s?

no. we   ll construct a counterexample. let a = z = [0, 1],    (a, z) = (a, z), and
s = {(a, z)     [0, 1]2 : a = z}. clearly, for any z     z there is an a     a such that a = z and
   (a, z)     s, but there is no a     a such that    z, a = z.

instead of looking for a single best strategy, we   ll play a repeated game. at time t,
player 1 plays at = at(a1, z1, . . . , at   1, zt   1) and player 2 plays zt = zt(a1, z1, . . . , at   1, zt   1).
player 1   s average loss after n iterations is

     n =

1
n

n

x    (at, zt)

t=1

let d(x, s) be the distance between a point x     dr and the set s, i.e.

d(x, s) = inf
s   s

kx     sk.

if s is convex, the in   mum is a minimum attained only at the projection of x in s.

de   nition: a set s is approachable if there exists a strategy at = at(a1, z1, . . . , at   1, zt   1)
such that limn       d(   n, s) = 0.

  

whether a set is approachable depends on the id168    (a, z). in our example, we can
choose a0 = 0 and at = zt   1 to get

lim n = lim x(zt   1, zt) = (z  , z  )     s.

     

n      

1
n       n

n

t=1

so this s is approachable.

7.2 blackwell   s theorem

we have the same conditions on a, z, and    (a, z) as before.

184theorem: blackwell   s theorem let s be a closed convex set of
   x     s. if    z,    a such that    (a, z)     s, then s is approachable.

2r with kxk     r

moreover, there exists a strategy such that

2r
d      ( n, s)        
n

proof. we   ll prove the rate; approachability of s follows immediately. the idea here is to
transform the problem to a scalar one where sion   s theorem applies by using half spaces.

suppose we have a half space h = {x     dr : hw, xi     c} with s     h. by assumption,

   z    a such that    (a, z)     h. that is,    z    a such that hw,    (a, z)i     c, or

by sion   s theorem,

so    a   

h such that    z    (a, z)     h.

max minhw,    (a, z)i     c.
z   z a   a

min maxhw,    (a, z)
a   a z   z

i     c.

this works for any h containing s. we want to choose ht so that    (at, zt) brings the
  
average    
t closer to s than    t   1. an intuitive choice is to have the hyperplane w bounding
h
t be the separating hyperplane between s and    t   1 closest to s. this is blackwell   s
strategy: let w be the hyperplane through   t     argmin     s k   t   1       k with normal vector
     t   1       t. then

  

  

  

h = {x     dr : hx       t,      t   1       ti     0}.

find a   

h and play it.

we need one more equality before proving convergence. the average loss can be ex-

panded:

t

t

     t

=

=

1

t   1

          +
t
    1
t

1
t

   

t

  (   t   1       t) +

1

t

   
t

  t +    t

1
t

now we look at the distance of the average from s, using the above equation and the
de   nition of   t+1:

d   (    , s)2

t

     

= k t       t+1k2
    k     t       tk2

= (cid:13)(cid:13)(cid:13)
t     1
(cid:13) t
= (cid:18) t     1(cid:19)2

(cid:13)
(cid:13)
  (   t   1       t) + (   t       t)
(cid:13)(cid:13)
   2
k
t

1
t

2

t   1, s) +

d   (   

2

k    
   t
t2

t

+ 2

1

t

   
t2

  
h   t       t,    
t   1       ti

since    t     h, the last term is negative; since    t and   t are both bounded by r, the middle
term is bounded by 4r
2 . letting   2
t

t, s) , we have a recurrence relation

t = t2d(     

2

2

  2
t       2

t   1 + 4r2,

185implying

rewriting in terms of the distance gives the desired bound,

  2
n     4nr2.

2r
d      ( t, s)        
n

note that this proof fails for nonconvex s.

7.3 regret minimization via approachability

consider the case a =    
k , z = b   (1). as we showed before, exponential weights rn    
cpn log(k). we can get the same dependence on n with an approachability-based strategy.
first recall that

k

1
n

rn = x

1
n

   (at, zt)     min

1
j n

n

t=1

n

x

t=1

   (ej, zt)

n

= m x

a x

j

" 1
n

t=1

   (at, zt)

n

    x

1
n

t=1

#
   (ej, zt)

if we de   ne a vector average loss

     n = x (   (at, zt)        (e1, zt), . . . ,    (a

1
n

n

t=1

t, zt)

       ( k , zt))

e

   

kr

,

  
rn
n     0 if and only if all components of    n are nonpositive. that is, we need d(
where    o = {x     kr :    1     xi     0,
approachability strategy, we get

  
   n, ok )     0,
    } is the nonpositive orthant. using blackwell   s

k

   

i

rn     d(     
n

n, o

   
k

)     c

.

r k
n
   

the k dependence is worse than exponential weights,

p
k instead of
   
h? as a concrete example, let k = 2. we need a
h tp satisfy

log(k).

how do we    nd a   

h
w,    (

   
ah , z)i = hw, hah , ziy     zi     c

   

for all z. here y is the vector of all ones. note that c     0 since 0 is in s and therefore in
h. rearranging,

h    ah, zihw, yi     hw, zi + c,

choosing a   

h = w will work; the inequality reduces to

hw,yi

approachability in the bandit setting with only partial feedback is still an open problem.

hw, zi     hw, zi + c.

18618.657: mathematics of machine learning

lecturer: philippe rigollet
scribe: jonathan weed

lecture
23
dec. 2, 2015

1. potential based approachability

last lecture, we saw blackwell   s celebrated approachability theorem, which establishes a
procedure by which a player can ensure that the average (vector) payo    in a repeated game
approaches a convex set. the central idea was to construct a hyperplane separating the
convex set from the point (cid:96)t 1, the average loss so far. by projecting perpendicular to
this hyperplane, we obtained a scalar-valued problem to which von neumann   s minimax
theorem could be applied. the set s is approachable as long as we can always    nd a    silver
bullet,    a choice of action at for which the loss vector (cid:96)t lies on the side of the hyperplane
containing s. (see figure 1.)

   

  

figure 1: blackwell approachability

  

concretely, blackwell   s theorem also implied the existence of a regret-minimizing algo-
rithm for expert advice. indeed, if we de   ne the vector loss (cid:96)t by ((cid:96)t)i = (cid:96)(at, zt)     (cid:96)(ei, zt),
then the average regret at time t is equivalent to the sup-norm distance between the average
loss (cid:96)t and the negative orthant. approaching the negative orthant therefore corresponds
to achieving sublinear regret.

however, this reduction yielded suboptimal rates. to bound average regret, w

   e replaced
the sup-norm distance by the euclidean distance, which led to an extra factor of
k appear-
ing in our bound. in the sequel, we develop a more sophisticated version of approachability
that allows us to adapt to the geometry of our problem. (much of what follows resem-
bles out development of the mirror descent algorithm, though the two approaches di   er in
crucial details.)

1.1 potential functions

we recall the setup of mirror descent,    rst described in lecture 13. mirror descent achieved
accelerated rates by employing a potential function which was strongly convex with respect

187to the given norm. in this case, we seek what is in some sense the opposite: a function
whose gradient does not change too quickly. in particular, we make the following de   nition.

de   nition: a function    : ird     ir is a potential for s     ir if it satis   es the following
properties:

       is convex.
      (x)     0 for x     s.
      (y) = 0 for y        s.
      (y)       (x)     (cid:104)     (x), y     x(cid:105)     h x2(cid:107)     y(cid:107)2, where by abuse of notation we use
     (x) to denote a subgradient of    at x.

given such a function, we recall two associated notions from the mirror descent algo-

rithm. the bregman divergence associated to    is given by

d  (y, x) =   (y)       (x)     (cid:104)     (x), y     x(cid:105) .

likewise, the associated bregman projection is

  (x) = argmin d  (y, x) .

y   s

we aim to use the function    as a stand-in for the euclidean distance that we employed
in our proof of blackwell   s theorem. to that end, the following lemma establishes several
properties that will allow us to generalize the notion of a separating hyperplane.

lemma: for any convex, closed set s and z     s, x     sc, the following properties
hold.

    (cid:104)z       (x),     (x)(cid:105)     0,
    (cid:104)x       (x),     (x)(cid:105)       (x).
in particular, if    is positive on sc, then h := {y | (cid:104)y       (x),     (x)(cid:105) = 0} is a

separating hyperplane.

our proof requires the following proposition, whose proof appears in our analysis of the
mirror descent algorithm and is omitted here.

proposition: for all z     s, it holds

(cid:104)     (  (x))          (x),   (x)     z(cid:105)     0 .

proof of lemma. denote by    the projection   (x). the    rst claim follows upon expanding
the expression on the left-hand side as follows

(cid:104)z       ,     (x)(cid:105) = (cid:104)z       ,     (x)          (  )(cid:105) + (cid:104)z       ,     (  )(cid:105).

188the above proposition implies that the    rst term is nonpositive. since the function    is
convex, we obtain

0       (z)       (  ) + (cid:104)z       ,     (  )(cid:105) .

since    lies on the boundary of s, by assumption   (  ) = 0 and the claim follows.

for the second claim, we again use convexity:

  (  )       (x) + (cid:104)       x,     (x)(cid:105) .

since   (  ) = 0, the claim follows.

1.2 potential based approachability

with the de   nitions in place, the algorithm for approachability is essentially the same as it
before we introduced the potential function. as before, we will use a projection de   ned by
the hyperplane h = {y | (cid:104)y       
= 0 and von neumann   s minmax theorem
to    nd a    silver bullet    a   

}

  ((cid:96)t   1),     ((cid:96)t
   1(cid:105)
  
t such that (cid:96)t = (cid:96)(a   
t , zt) satis   es
  t,     ((cid:96)t   1)(cid:105)     0 .

(cid:104)(cid:96)t    

  

all that remains to do is to analyze this procedure   s performance. we have the following

theorem.

theorem: if (cid:107)(cid:96)(a, z)(cid:107)     r holds for all z     a, z     z and all assumptions above are
satis   ed, then

    ((cid:96)n)    

4r2h log n

.

n

proof. the de   nition of the potential    required that    be upper bounded by a quadratic
function. the proof below is a simple application of that bound.

as before, we note the identity

  
(cid:96)t = (cid:96)t   1 +

  

(cid:96)

t

t   1

      (cid:96)
t

.

this expression and the de   nition of    imply.

         
)

  ((cid:96)t   1) + (cid:96)t       
(cid:96)

(cid:104)

         ((cid:96)t 1 (cid:105) +
t 1,

   

  

)

  ((cid:96)t

1
t

(cid:107)(cid:96)

t

    t   1

  
(cid:96)

(cid:107) .

2

h
2 2
t

the last term is the easiest to control. by assumption, (cid:96)t and (cid:96)t 1 are contained in a ball
of radius r, so (cid:107)(cid:96)t       (cid:96)t   1(cid:107)2     4r2.
to bound the second term, write

  

   

1
t

(cid:104)
(cid:96)t       

(cid:96)t 1,     ((cid:96)t 1)(cid:105) = (cid:104)(cid:96)t       t,     ((cid:96)t 1) +   
(cid:104) t
   

    (cid:105)

   

  

  

    t   1     t   1 (cid:105)
) .

  
,   ((cid:96)

  
(cid:96)

1
t

1
t

the    rst term is nonpositive by assumption, since this is how the algorithm constructs
the silver bullet. by the above lemma, the inner product in the second term is at most
        ((cid:96)t   1).

we obtain

    ((cid:96)t)    

(cid:18) t     1

(cid:19)

t

    ((cid:96)t

)   1 +

2

hr2
t2

.

189de   ning ut = t  ((cid:96)t) and rearranging, we obtain the recurrence

  

so

(cid:88)

n

t=1

un =

ut     ut   1 +

2hr2

t

ut     ut   1     2hr2(cid:88) 1

n

t

t=1

,

    4hr2 log n .

applying the de   nition of un proves the claim.

   

1.3 application to regret minimization
we now show that potential based approachability provides    an improved bound on regret
minimization. our ultimate goal is to replace the bound nk (which we proved last lecture)
by
n log k (which we know to be the optimal bound for prediction with expert advice).
we will be able to achieve this goal up to logarithmic terms in n. (a more careful analysis
of the potential de   ned below does actually yields an optimal rate.)
    ), where rn is the cumulative regret after n rounds and o   
is the negative orthant. it is not hard to see that d = (cid:107)x+(cid:107) , where x+ is the positive
part of the vector x.

recall that rn
n

= d ((cid:96)n, o

   

   

   

  

k

k

we de   ne the following potential function:

   
   

   
   

.

(cid:88)

k

j=1

e  (xj )+

  (x) = log

1
  

1
k

the function    is a kind of    soft max    of the
of x. (note that this de   nition
does not agree with the use of the term soft max in the literature   the di   erence is the
presence of the factor 1 .) the terminology soft max is justi   ed by noting that

positive entries

k

(cid:107)x+(cid:107) = max(x

   

j)+     max

1
  

log

1
k

e  (xj ) +

+

log k

  

      (x) +

log k

  

.

j

j

the potential function therefore serves as an upper bound on the sup distance, up to an
additive logarithmic factor.

the function    de   ned in this way is clearly convex and zero on the negative orthant.
to verify that it is a potential, it remains to show that    can be bounded by a quadratic.
away from the negative orthant,    is twice di   erentiable and we can compute the

hessian explicitly:

   2  (x) =    diag(     (x))                 (cid:62) .

for any vector u such that (cid:107)u(cid:107)2 = 1, we therefore have

u(cid:62)   2  (x)u =   

j (     (x))j       (u(cid:62)
u2

     (x))2       

(cid:88)

k

j

=1

(cid:88)

k

j=1

     (x))j        ,
u2
j (

since (cid:107)u(cid:107)2 = 1 and (cid:107)     (x)

(cid:107)1     1.

we conclude that       (x) (cid:22)   i, which for nonnegative x and y implies the bound

2

  (y)       (x)     (cid:104)     (x), y     x(cid:105)    

(cid:107)y     x(cid:107)2 .

  
2

190in fact, this bound holds everywhere. therefore    is a valid potential function for the
negative orthant, with h =   .

the above theorem then implies that we can ensure

to optimize this bound, we pick    = 1 (cid:113) n log k and obtain the bound

+

n

  

  

.

log k 4r2   log n

log k

        ((cid:96)n) +

rn
n

   

2r

rn     4r(cid:112)n log n log k .

log n

as alluded to earlier, a more careful analysis can remove the log n term. indeed, for this

particular choice of   , we can modify the above lemma to obtain the sharper bound

(cid:104)x       (x),     (x)(cid:105)     2  (x) .

when we substitute this expression into the above proof, we obtain the recurrence

relation

    ((cid:96)t)

t

   

    2
t

c
    ((cid:96)t   1) + .
t2

this small change is enough to prevent the appearance of log n in the    nal bound.

191references

[bb04] mcmahan, h. brendan, and avrim blum. online geometric optimization in the
bandit setting against an adaptive adversary. conference on learning theory
(colt) 2004.

[bck 12] bubeck, sbastien, nicolo cesa-bianchi, and sham m. kakade. towards mini-
max policies for online linear optimization with bandit feedback. arxiv preprint
arxiv:1202.3079 (2012). apa

[bk04] awerbuch, baruch, and robert d. kleinberg. adaptive routing with end-to-end
feedback: distributed learning and geometric approaches.proceedings of the thirty-
sixth annual acm symposium on theory of computing. acm, 2004.

[bla56] d. blackwell, an analog of the minimax theorem for vector payo   s, paci   c j.

math. 6 (1956), no. 1, 1   8

[bub15] s  ebastien bubeck, id76: algorithms and complexity, now publish-

ers inc., 2015.

[dgl96] l. devroye, l. gyo  r   , and g. lugosi, a probabilistic theory of pattern recognition,
applications of mathematics (new york), vol. 31, springer-verlag, new york,
1996. mr mr1383093 (97d:68196)

[htf09] trevor hastie, robert tibshirani, and jerome friedman, the elements of statis-
tical learning, second ed., springer series in statistics, springer, new york, 2009,
data mining, id136, and prediction. mr 2722294 (2012d:62081)

[kol11] vladimir koltchinskii. oracle inequalities in empirical risk minimization and sparse
recovery problems. ecole d   et  e de probabilit  es de saint-flour xxxviii-2008. lec-
ture notes in mathematics 2033. berlin: springer. ix, 254 p. eur 48.10 , 2011.

  

  

[kea90] michael j kearns. the computational complexity of machine learning. phd thesis,

harvard university, 1990.

[lt91] michel ledoux and michel talagrand. id203 in banach spaces, volume 23 of
ergebnisse der mathematik und ihrer grenzgebiete (3) [results in mathematics and
related areas (3)]. springer-verlag, berlin, 1991. isoperimetry and processes.

[nem12] arkadi nemirovski, on safe tractable approximations of chance constraints, euro-

pean j. oper. res. 219 (2012), no. 3, 707   718. mr 2898951 (2012m:90133)

[ns06] arkadi nemirovski and alexander shapiro, convex approximations of chance
constrained programs, siam j. optim. 17 (2006), no. 4, 969   996. mr 2274500
(2007k:90077)

[sio58] m. sion, on general minimax theorems. paci   c j. math. 8 (1958), no. 1, 171   176.

[vh06] dani, varsha, and thomas p. hayes. robbing the bandit: less regret in online geo-
metric optimization against an adaptive adversary. proceedings of the seventeenth
annual acm-siam symposium on discrete algorithm. society for industrial and
applied mathematics, 2006.

[vkh07] dani, varsha, sham m. kakade, and thomas p. hayes, the price of bandit in-
formation for online optimization, advances in neural information processing
systems. 2007.

1926
6
6
6
[zha04] tong zhang. statistical behavior and consistency of classi   cation methods based

on convex risk minimization. ann. statist., 32(1):56   85, 2004.

1936
6
mit opencourseware
http://ocw.mit.edu

18.657 mathematics of machine learning
fall 2015

for information about citing these materials or our terms of use, visit: http://ocw.mit.edu/terms.

