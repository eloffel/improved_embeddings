introduction to machine learning

introduction to machine learning

alex smola and s.v.n. vishwanathan

yahoo! labs
santa clara

   and   

departments of statistics and computer science

purdue university

   and   

college of engineering and computer science

australian national university

p u b l i s h e d b y t h e p r e s s s y n d i c a t e o f t h e u n i v e r s i t y o f c a m b r i d g e

the pitt building, trumpington street, cambridge, united kingdom

c a m b r i d g e u n i v e r s i t y p r e s s

the edinburgh building, cambridge cb2 2ru, uk

40 west 20th street, new york, ny 10011   4211, usa

477 williamstown road, port melbourne, vic 3207, australia

ruiz de alarc  on 13, 28014 madrid, spain

dock house, the waterfront, cape town 8001, south africa

http://www.cambridge.org

c(cid:13) cambridge university press 2008

this book is in copyright. subject to statutory exception

and to the provisions of relevant collective licensing agreements,

no reproduction of any part may take place without

the written permission of cambridge university press.

first published 2008

printed in the united kingdom at the university press, cambridge

typeface monotype times 10/13pt system latex 2  
vishwanathan]

[alexander j. smola and s.v.n.

a catalogue record for this book is available from the british library

library of congress cataloguing in publication data available

isbn 0 521 82583 0 hardback

author: vishy
revision: 252
timestamp: october 1, 2010
url: svn://smola@repos.stat.purdue.edu/thebook/trunk/book/thebook.tex

contents

preface

1

introduction
1.1 a taste of machine learning

1.1.1 applications
1.1.2 data
1.1.3 problems

1.2 id203 theory

1.2.1 random variables
1.2.2 distributions
1.2.3 mean and variance
1.2.4 marginalization, independence, conditioning, and

bayes rule
1.3 basic algorithms

1.3.1 naive bayes
1.3.2 nearest neighbor estimators
1.3.3 a simple classi   er
1.3.4 id88
1.3.5 id116

2

density estimation
limit theorems
2.1
2.1.1 fundamental laws
2.1.2 the characteristic function
2.1.3 tail bounds
2.1.4 an example

2.2 parzen windows

2.2.1 discrete density estimation
2.2.2 smoothing kernel
2.2.3 parameter estimation
2.2.4 silverman   s rule
2.2.5 watson-nadaraya estimator

2.3 exponential families

2.3.1 basics

page 1

3
3
3
7
9
12
12
13
15

16
20
22
24
27
29
32

37
37
38
42
45
48
51
51
52
54
57
59
60
60

v

vi

3

2.3.2 examples

2.4 estimation

2.4.1 id113
2.4.2 bias, variance and consistency
2.4.3 a bayesian approach
2.4.4 an example
sampling
2.5.1
2.5.2 rejection sampler

inverse transformation

2.5

0 contents

62
66
66
68
71
75
77
78
82

optimization
3.1 preliminaries

3.2 unconstrained smooth convex minimization

91
91
92
3.1.1 convex sets
92
3.1.2 convex functions
96
3.1.3 subgradients
3.1.4 strongly convex functions
97
3.1.5 convex functions with lipschitz continous gradient 98
3.1.6 fenchel duality
98
100
3.1.7 bregman divergence
102
102
104
104
108
111
115
121
125
125
127
131
135
136
137
137
139

3.2.1 minimizing a one-dimensional convex function
3.2.2 coordinate descent
3.2.3 id119
3.2.4 mirror descent
3.2.5 conjugate gradient
3.2.6 higher order methods
3.2.7 bundle methods

3.3.1 projection based methods
3.3.2 lagrange duality
3.3.3 linear and quadratic programs
stochastic optimization
3.4.1 stochastic id119

3.5.1 concave-convex procedure
some practical advice

3.4

3.6

3.5 nonid76

3.3 constrained optimization

4

online learning and boosting
4.1 halving algorithm
4.2 weighted majority

143
143
144

contents

5

conditional densities
5.1
id28
5.2 regression

5.2.1 conditionally normal models
5.2.2 posterior distribution
5.2.3 heteroscedastic estimation

5.3 multiclass classi   cation

5.3.1 conditionally multinomial models

5.4 what is a crf?

5.4.1 linear chain crfs
5.4.2 higher order crfs
5.4.3 kernelized crfs
5.5 optimization strategies
5.5.1 getting started
5.5.2 optimization algorithms
5.5.3 handling higher order crfs

5.6 id48
5.7 further reading

5.7.1 optimization

6

kernels and function spaces
6.1 the basics

6.1.1 examples

6.2 kernels

6.2.1 feature maps
6.2.2 the kernel trick
6.2.3 examples of kernels

6.3 algorithms

6.3.1 kernel id88
6.3.2 trivial classi   er
6.3.3 kernel principal component analysis

6.4 reproducing kernel hilbert spaces

6.4.1 hilbert spaces
6.4.2 theoretical properties
6.4.3 id173

6.5 banach spaces

6.5.1 properties
6.5.2 norms and convex sets

7

linear models
7.1

support vector classi   cation

vii

149
150
151
151
151
151
151
151
152
152
152
152
152
152
152
152
153
153
153

155
155
156
161
161
161
161
161
161
161
161
161
163
163
163
164
164
164

165
165

viii

0 contents

7.1.1 a regularized risk minimization viewpoint
7.1.2 an exponential family interpretation
7.1.3 specialized algorithms for training id166s

7.2 extensions

7.2.1 the    trick
7.2.2 squared hinge loss
7.2.3 ramp loss
support vector regression
7.3.1
7.3.2

incorporating general id168s
incorporating the    trick

7.3

7.4 novelty detection
7.5 margins and id203
7.6 beyond binary classi   cation

7.7

7.6.1 multiclass classi   cation
7.6.2 multilabel classi   cation
7.6.3 ordinal regression and ranking
large margin classi   ers with structure
7.7.1 margin
7.7.2 penalized margin
7.7.3 nonconvex losses

7.8 applications

7.8.1 sequence annotation
7.8.2 matching
7.8.3 ranking
7.8.4 shortest path planning
7.8.5
7.8.6 contingency table loss

image annotation

7.9 optimization

7.9.1 column generation
7.9.2 bundle methods
7.9.3 overrelaxation in the dual

7.10 crfs vs structured large margin models

7.10.1 id168
7.10.2 dual connections
7.10.3 optimization

appendix 1 id202 and functional analysis

appendix 2 conjugate distributions

appendix 3 id168s
bibliography

170
170
172
177
177
179
180
181
184
186
186
189
189
190
191
192
193
193
193
193
193
193
193
193
193
193
193
193
193
193
193
194
194
194
194

197

201

203
221

preface

since this is a textbook we biased our selection of references towards easily
accessible work rather than the original references. while this may not be
in the interest of the inventors of these concepts, it greatly simpli   es access
to those topics. hence we encourage the reader to follow the references in
the cited works should they be interested in    nding out who may claim
intellectual ownership of certain key ideas.

1

2

structure of the book

0 preface

canberra, august 2008

introductiondensity estimationid114kernelsoptimizationconditional densitiesid49linear modelsstructured estimationduality and estimationmomentmethodsid23introductiondensity estimationid114kernelsoptimizationconditional densitiesid49linear modelsstructured estimationduality and estimationmomentmethodsid23introductiondensity estimationid114kernelsoptimizationconditional densitiesid49linear modelsstructured estimationduality and estimationmomentmethodsid231

introduction

over the past two decades machine learning has become one of the main-
stays of information technology and with that, a rather central, albeit usually
hidden, part of our life. with the ever increasing amounts of data becoming
available there is good reason to believe that smart data analysis will become
even more pervasive as a necessary ingredient for technological progress.

the purpose of this chapter is to provide the reader with an overview over
the vast range of applications which have at their heart a machine learning
problem and to bring some degree of order to the zoo of problems. after
that, we will discuss some basic tools from statistics and id203 theory,
since they form the language in which many machine learning problems must
be phrased to become amenable to solving. finally, we will outline a set of
fairly basic yet e   ective algorithms to solve an important problem, namely
that of classi   cation. more sophisticated tools, a discussion of more general
problems and a detailed analysis will follow in later parts of the book.

1.1 a taste of machine learning

machine learning can appear in many guises. we now discuss a number of
applications, the types of data they deal with, and    nally, we formalize the
problems in a somewhat more stylized fashion. the latter is key if we want to
avoid reinventing the wheel for every new application. instead, much of the
art of machine learning is to reduce a range of fairly disparate problems to
a set of fairly narrow prototypes. much of the science of machine learning is
then to solve those problems and provide good guarantees for the solutions.

1.1.1 applications

most readers will be familiar with the concept of web page ranking. that
is, the process of submitting a query to a search engine, which then    nds
webpages relevant to the query and which returns them in their order of
relevance. see e.g. figure 1.1 for an example of the query results for    ma-
chine learning   . that is, the search engine returns a sorted list of webpages
given a query. to achieve this goal, a search engine needs to    know    which

3

4

1 introduction

fig. 1.1. the 5 top scoring webpages for the query    machine learning   

pages are relevant and which pages match the query. such knowledge can be
gained from several sources: the link structure of webpages, their content,
the frequency with which users will follow the suggested links in a query, or
from examples of queries in combination with manually ranked webpages.
increasingly machine learning rather than guesswork and clever engineering
is used to automate the process of designing a good search engine [rpb06].
a rather related application is collaborative    ltering. internet book-
stores such as amazon, or video rental sites such as net   ix use this informa-
tion extensively to entice users to purchase additional goods (or rent more
movies). the problem is quite similar to the one of web page ranking. as
before, we want to obtain a sorted list (in this case of articles). the key dif-
ference is that an explicit query is missing and instead we can only use past
purchase and viewing decisions of the user to predict future viewing and
purchase habits. the key side information here are the decisions made by
similar users, hence the collaborative nature of the process. see figure 1.2
for an example. it is clearly desirable to have an automatic system to solve
this problem, thereby avoiding guesswork and time [bk07].

an equally ill-de   ned problem is that of automatic translation of doc-
uments. at one extreme, we could aim at fully understanding a text before
translating it using a curated set of rules crafted by a computational linguist
well versed in the two languages we would like to translate. this is a rather
arduous task, in particular given that text is not always grammatically cor-
rect, nor is the document understanding part itself a trivial one. instead, we
could simply use examples of translated documents, such as the proceedings
of the canadian parliament or other multilingual entities (united nations,
european union, switzerland) to learn how to translate between the two

web images maps news shopping gmail more !     sponsored linksmachine learninggoogle sydney needs machinelearning experts. apply today!www.google.com.au/jobssign in search  advanced search  preferences web    scholar   results 1 - 10 of about 10,500,000 for machine learning. (0.06 seconds) machine learning - wikipedia, the free encyclopediaas a broad subfield of artificial intelligence, machine learning is concerned with the designand development of algorithms and techniques that allow ...en.wikipedia.org/wiki/machine_learning - 43k - cached - similar pagesmachine learning textbookmachine learning is the study of computer algorithms that improve automatically throughexperience. applications range from datamining programs that ...www.cs.cmu.edu/~tom/mlbook.html - 4k - cached - similar pagesmachine learningwww.aaai.org/aitopics/html/machine.html - similar pagesmachine learninga list of links to papers and other resources on machine learning.www.machinelearning.net/ - 14k - cached - similar pagesintroduction to machine learningthis page has pointers to my draft book on machine learning and to its individualchapters. they can be downloaded in adobe acrobat format. ...ai.stanford.edu/~nilsson/mlbook.html - 15k - cached - similar pagesmachine learning - artificial intelligence (incl. robotics ...machine learning - artificial intelligence. machine learning is an international forum forresearch on computational approaches to learning.www.springer.com/computer/artificial/journal/10994 - 39k - cached - similar pagesmachine learning (theory)graduating students in statistics appear to be at a substantial handicap compared tograduating students in machine learning, despite being in substantially ...hunch.net/ - 94k - cached - similar pagesamazon.com: machine learning: tom m. mitchell: booksamazon.com: machine learning: tom m. mitchell: books.www.amazon.com/machine-learning-tom-m-mitchell/dp/0070428077 - 210k -cached - similar pagesmachine learning journalmachine learning publishes articles on the mechanisms through which intelligent systemsimprove their performance over time. we invite authors to submit ...pages.stern.nyu.edu/~fprovost/mlj/ - 3k - cached - similar pagescs 229: machine learningstanford. cs229 machine learning autumn 2007. announcements. final reports fromthis year's class projects have been posted here. ...cs229.stanford.edu/ - 10k - cached - similar pages12345678910next  searchsearch within results | language tools | search tips | dissatisfied? help us improve | try google experimental  2008 google - google home - advertising programs - business solutions - about googlemachine learningmachine learninggoogle1.1 a taste of machine learning

5

languages. in other words, we could use examples of translations to learn
how to translate. this machine learning approach proved quite successful
[?].

many security applications, e.g. for access control, use face recognition as
one of its components. that is, given the photo (or video recording) of a
person, recognize who this person is. in other words, the system needs to
classify the faces into one of many categories (alice, bob, charlie, . . . ) or
decide that it is an unknown face. a similar, yet conceptually quite di   erent
problem is that of veri   cation. here the goal is to verify whether the person
in question is who he claims to be. note that di   erently to before, this
is now a yes/no question. to deal with di   erent lighting conditions, facial
expressions, whether a person is wearing glasses, hairstyle, etc., it is desirable
to have a system which learns which features are relevant for identifying a
person.

another application where learning helps is the problem of named entity
recognition (see figure 1.4). that is, the problem of identifying entities,
such as places, titles, names, actions, etc. from documents. such steps are
crucial in the automatic digestion and understanding of documents. some
modern e-mail clients, such as apple   s mail.app nowadays ship with the
ability to identify addresses in mails and    ling them automatically in an
address book. while systems using hand-crafted rules can lead to satisfac-
tory results, it is far more e   cient to use examples of marked-up documents
to learn such dependencies automatically, in particular if we want to de-
ploy our system in many languages. for instance, while    bush    and    rice   

fig. 1.2. books recommended by amazon.com when viewing tom mitchell   s ma-
chine learning book [mit97]. it is desirable for the vendor to recommend relevant
books which a user might purchase.

fig. 1.3. 11 pictures of the same person taken from the yale face recognition
database. the challenge is to recognize that we are dealing with the same per-
son in all 11 cases.

your amazon.com today's dealsgifts & wish lists gift cards your account  |  helpadvertise on amazon5 star: (23)4 star: (2)3 star: (3)2 star: (2)1 star:  (0)   quantity: 1 orsign in to turn on 1-click ordering.   more buying choices16 used & new from$52.00have one to sell?      share your own customer imagessearch inside another edition of this bookare you an author orpublisher? find out how to publishyour own kindle books   hello. sign in to get personalized recommendations. new customer? start here.   books   booksadvanced searchbrowse subjectshot new releasesbestsellersthe new york times   best sellerslibros en espa  olbargain bookstextbooksjoin amazon prime and ship two-day for free and overnight for $3.99. already a member? sign in.machine learning (mcgraw-hill international edit)(paperback)by thomas mitchell (author) "ever since computers were invented, we have wondered whetherthey might be made to learn..." (more)    (30 customer reviews)  list price:$87.47price:$87.47 & this item ships for free with super saver shipping.detailsavailability: usually ships within 4 to 7 weeks. ships from and sold by amazon.com. gift-wrap available.16 used & new available from $52.00also available in:list price:our price:other offers:hardcover (1)$153.44$153.4434 used & new from $67.00   better togetherbuy this book with introduction to machine learning (adaptive computation and machine learning) by ethem alpaydin today!buy together today: $130.87customers who bought this item also boughtpattern recognition andmachine learning(information science andstatistics) by christopherm. bishop (30)  $60.50artificial intelligence: amodern approach (2ndedition) (prentice hallseries in artificialintelligence) by stuartrussell (76)  $115.00the elements of statisticallearning by t. hastie (25)  $72.20pattern classification (2ndedition) by richard o.duda (25)  $115.00data mining: practicalmachine learning toolsand techniques, secondedition (morgan kaufmannseries in datamanagement systems) byian h. witten (21)  $39.66    explore similar items : books (50)editorial reviewsbook descriptionthis exciting addition to the mcgraw-hill series in computer science focuses on the concepts and techniques that contribute to the rapidlychanging field of machine learning--including id203 and statistics, artificial intelligence, and neural networks--unifying them all in a logicaland coherent manner. machine learning serves as a useful reference tool for software developers and researchers, as well as an outstanding textfor college students. --this text refers to the hardcover edition. book infopresents the key algorithms and theory that form the core of machine learning. discusses such theoretical issues as how does learningperformance vary with the number of training examples presented? and which learning algorithms are most appropriate for various types oflearning tasks? dlc: computer algorithms. --this text refers to the hardcover edition.product detailspaperback: 352 pagespublisher: mcgraw-hill education (ise editions); 1st edition (october 1, 1997)language: englishisbn-10: 0071154671isbn-13: 978-0071154673product dimensions: 9 x 5.9 x 1.1 inchesshipping weight: 1.2 pounds (view shipping rates and policies)average customer review:   (30 customer reviews)amazon.com sales rank: #104,460 in books (see bestsellers in books)popular in this category: (what's this?)#11 in books > computers & internet > computer science > artificial intelligence > machine learning(publishers and authors: improve your sales)in-print editions: hardcover (1) |  all editions would you like to update product info or give feedback on images? (we'll ask you to sign in so we can get back to you)inside this book (learn more) browse and search another edition of this book.first sentence:ever since computers were invented, we have wondered whether they might be made to learn. read the first pagebrowse sample pages:front cover | copyright | table of contents | excerpt | index | back cover | surprise me!search inside this book: customers viewing this page may be interested in these sponsored links (what's this?)online law degreehttp://www.edu-onlinedegree.org juris doctor jd & llm masters low tuition, free textbooks learning cdswww.mindperk.com save on powerful mind-boosting cds & dvds. huge selection video edit magicwww.deskshare.com/download video editing software trim, modify color, and merge video tags customers associate with this product (what's this?)click on a tag to find related items, discussions, and people.machine learning (6)artificial intelligence (2)computer science (1)pattern recognition (1)your tags: add your first taghelp others find this product - tag it for amazon searchno one has tagged this product for amazon search yet. why not be the first tosuggest a search for which it should appear?search products tagged with are you the publisher or author? learn how amazon can help you make this book an ebook. if you are a publisher or author and hold the digital rights to a book, you can make it available as an ebook on amazon.com. learn morerate this item to improve your recommendationsi own itnot ratedyour ratingdon't like it < > i love it!save yourrating  ?12345 customer reviews30 reviews  average customer review(30 customer reviews)    share your thoughts with other customers:most helpful customer reviews 44 of 44 people found the following review helpful: an excellent overview for the adv. undergrad or beg. grad,september 30, 2002by todd ebert (long beach california) - see all my reviewsthis review is from: machine learning (hardcover)i agree with some of the previous reviews which criticize the book for its lack ofdepth, but i believe this to be an asset rather than a liability given its targetaudience (seniors and beginning grad. students). the average college senior typicallyknows very little about subjects like neural networks, id107, or baysiannetworks, and this book goes a long way in demystifying these subjects in a veryclear, concise, and understandable way. moreover, the first-year grad. student who isinterested in possibly doing research in this field needs more of an overview than todive deeply into one of the many branches which themselves have had entire books written aboutthem. this is one of the few if only books where one will find diverse areas oflearning (e.g. analytical, reinforcment, bayesian, neural-network, genetic-algorithmic)all within the same cover.but more than just an encyclopedic introduction, the author makes a number ofconnections between the different paradigms. for example, he explains thatassociated with each paradigm is the notion of an inductive-learning bias, i.e. theunderlying assumptions that lend validity to a given learning approach. these end-of-chapter discussions on bias seem very interesting and unique to this book.finally, i used this book for part of the reading material for an intro. ai class, andreceived much positive feedback from the students, although some did find thepresentation a bit too abstract for their undergraduate tastes comment | permalink | was this review helpful to you?   (report this)  22 of 27 people found the following review helpful: great compilation, may 18, 2001by steven burns (-) - see all my reviewsthis review is from: machine learning (hardcover)this book is completely worth the price, and worth the hardcover to take care of it.the main chapters of the book are independent, so you can read them in any order.the way it explains the different learning approaches is beautiful because: 1)itexplains them nicely 2)it gives examples and 3)it presents pseudocode summaries ofthe algorithms. as a software developer, what else could i possibly ask for? comment | permalink | was this review helpful to you?   (report this)  23 of 23 people found the following review helpful: venerable, in both senses, april 4, 2004by eldil (albuquerque nm) - see all my reviewsthis review is from: machine learning (hardcover)it's pretty well done, it covers theory and core areas but - maybe it was more thestate of the field when it was written - i found it unsatisfyingly un-synthesized,unconnected, and short of detail (but this is subjective). i found the 2nd edition ofrussell and norvig to be a better introduction where it covers the same topic, whichit does for everything i can think of, except vc dimension.the book sorely needs an update, it was written in 1997 and the field has movedfast. a comparison with mitchell's current course (materials generously availableonline) shows that about 1/4 of the topics taught have arisen since the book waspublished; boosting, support vector machines and id48 to namethe best-known. the book also does not cover statistical or data mining methods.despite the subjective complaint about lack of depth it does give the theoreticalroots and many fundamental techniques decently and readably. for many purposesthough it may have been superceded by r&n 2nd ed. comment | permalink | was this review helpful to you?   (report this) share your thoughts with other customers:     see all 30 customer reviews...  most recent customer reviews outstandingi read this book about 7 years ago while inthe phd program at stanford university. iconsider this book not only the bestmachine learning book, but one of the bestbooks in all... read morepublished 6 months ago by husam abu-haimed great start to machine learningi have used this book during my mastersand found it to be an extremely helpful anda gentle introduction to the thick and thingsof machine learning applications.read morepublished 6 months ago by subrat nanda best book i've seen on topici have this book listed as one of the bestand most interesting i've ever read. i lovedthe book just as much as i loved the coursewe used it in. read morepublished 13 months ago by lars kristensson too expensive i would saygreat book if you wanna start sth anywherein machine learning, but it is tooooooexpensive.published 17 months ago by x. wu excellent book, concise andreadablethis is a great book if you're starting outwith machine learning. it's rare to comeacross a book like this that is very wellwritten and has technical depth. read morepublished 20 months ago by part time reader great bookthis is a great book because it focuses onmachine learning techniques. it has beenused as textbook in my class.published on november 11, 2005 by jay great introduction book forstudents in data mining and machinelearning classalthough this text book is not required inmy data mining class, but i found it is veryhelpful for my study. read morepublished on october 24, 2005 by thanh doan excellently writteni am using this textbook for a machinelearning class. while my professor isexcellent, i must say that this book is awelcome addition to class. read morepublished on october 12, 2005 by gregor kronenberger just a brief introduction to ml...first of all, the statistical part of machinelearning is just a real subset ofmathematical statisitcs, whatever bayesianor frequentist. read morepublished on september 12, 2005 by supercutepig excellent reference booki liked the book. but i think author mustprovide more figures in the book like dudaand hart's pattern classification book.read morepublished on december 25, 2004 by fatih narsearch customer reviews only search this product's reviews    see all 30 customer reviews... customer discussions beta (what's this?)new! see recommended discussions for youthis product's forum (0 discussions)discussionreplieslatest postno discussions yetask questions, share opinions, gain insightstart a new discussiontopic:   related forumsmachine learning (start the discussion)artificial intelligence  (1 discussion)product information from the amapedia community beta (what's this?)be the first person to add an article about this item at amapedia.com.     see featured amapedia.com articles listmania! machine learning and graphs: a list by j. chan "phd student (computerscience)" id110 books: a list by tincture of iodine "toi" books on algorithms on a variety of topics: a list by calvinnme "texan refugee"create a listmania! listsearch listmania!so you'd like to... learn advanced mathematics on your own: a guide by gal gross "wir m  ssenwissen, wir werden wissen. - david hilbert" learn more about artificial intelligence (ai) and games: a guide by john funge study curriculum of b.s. computer science (honors mode): a guide by"josie_roberts"create a guidesearch guideslook for similar items by categorycomputers & internet > computer science > artificial intelligence > machine learninglook for similar items by subject machine learning computer books: generalfind books matching all checked subjects i.e., each book must be in subject 1 and subject 2 and ... harry potter storeour harrypotterstorefeaturesall thingsharry,includingbooks, audio cds andcassettes, dvds,soundtracks, and more. got your neti pot?give yoursinuses abath withone of the many netipots in our health &personal care store.   see more drop it like it'swaterproofandshockproof,crushproof,andfreezeproof. all that, inaddition to 7-megapixelresolution and brightcapture technology,makes the olympusstylus 770sw theperfect vacationcompanion. plus, it's nowavailable for only$289.94 fromamazon.com. editors' faves inbookssave40%on thesignificant 7, our favoritepicks for the month.    feedback  if you need help or have a question for customer service, contact us. would you like to update product info or give feedback on images? (we'll ask you to sign in so we can get back to you) is there any other feedback you would like to provide? click herewhere's my stuff?track your recent orders.view or change your orders in your account.shipping & returnssee our shipping rates & policies.return an item (here's our returns policy).need help?forgot your password? click here.redeem or buy a gift certificate/card.visit our help department.search amazon.com    your recent history (what's this?) recently viewed productsafter viewing product detail pages or search results, look here to find an easy way to navigate back to pages you are interested in.look to the right column to find helpful suggestions for your shopping session.    view & edit your browsing history    amazon.com home  |   directory of all storesinternational sites:  canada  |  united kingdom  |  germany  |  japan  |  france  |  chinahelp  |  view cart  |  your account  |  sell items  |  1-click settings         6

1 introduction

havana (reuters) - the european union   s top development aid official
left cuba on sunday convinced that eu diplomatic sanctions against
the communist island should be dropped after fidel castro   s
retirement, his main aide said.

<type="organization">havana</> (<type="organization">reuters</>) - the
<type="organization">european union</>   s top development aid official left
<type="organization">cuba</> on sunday convinced that eu diplomatic sanctions
against the communist <type="location">island</> should be dropped after
<type="person">fidel castro</>   s retirement, his main aide said.

fig. 1.4. named entity tagging of a news article (using lingpipe). the relevant
locations, organizations and persons are tagged for further information extraction.

are clearly terms from agriculture, it is equally clear that in the context of
contemporary politics they refer to members of the republican party.

other applications which take advantage of learning are speech recog-
nition (annotate an audio sequence with text, such as the system shipping
with microsoft vista), the recognition of handwriting (annotate a sequence
of strokes with text, a feature common to many pdas), trackpads of com-
puters (e.g. synaptics, a major manufacturer of such pads derives its name
from the synapses of a neural network), the detection of failure in jet en-
gines, avatar behavior in computer games (e.g. black and white), direct
marketing (companies use past purchase behavior to guesstimate whether
you might be willing to purchase even more) and    oor cleaning robots (such
as irobot   s roomba). the overarching theme of learning problems is that
there exists a nontrivial dependence between some observations, which we
will commonly refer to as x and a desired response, which we refer to as y,
for which a simple set of deterministic rules is not known. by using learning
we can infer such a dependency between x and y in a systematic fashion.

we conclude this section by discussing the problem of classi   cation,
since it will serve as a prototypical problem for a signi   cant part of this
book. it occurs frequently in practice: for instance, when performing spam
   ltering, we are interested in a yes/no answer as to whether an e-mail con-
tains relevant information or not. note that this issue is quite user depen-
dent: for a frequent traveller e-mails from an airline informing him about
recent discounts might prove valuable information, whereas for many other
recipients this might prove more of an nuisance (e.g. when the e-mail relates
to products available only overseas). moreover, the nature of annoying e-
mails might change over time, e.g. through the availability of new products
(viagra, cialis, levitra, . . . ), di   erent opportunities for fraud (the nigerian
419 scam which took a new twist after the iraq war), or di   erent data types
(e.g. spam which consists mainly of images). to combat these problems we

1.1 a taste of machine learning

7

fig. 1.5. binary classi   cation; separate stars from diamonds. in this example we
are able to do so by drawing a straight line which separates both sets. we will see
later that this is an important example of what is called a linear classi   er.

want to build a system which is able to learn how to classify new e-mails.
a seemingly unrelated problem, that of cancer diagnosis shares a common
structure: given histological data (e.g. from a microarray analysis of a pa-
tient   s tissue) infer whether a patient is healthy or not. again, we are asked
to generate a yes/no answer given a set of observations. see figure 1.5 for
an example.

1.1.2 data

it is useful to characterize learning problems according to the type of data
they use. this is a great help when encountering new challenges, since quite
often problems on similar data types can be solved with very similar tech-
niques. for instance natural language processing and bioinformatics use very
similar tools for strings of natural language text and for dna sequences.
vectors constitute the most basic entity we might encounter in our work.
for instance, a life insurance company might be interesting in obtaining the
vector of variables (blood pressure, heart rate, height, weight, cholesterol
level, smoker, gender) to infer the life expectancy of a potential customer.
a farmer might be interested in determining the ripeness of fruit based on
(size, weight, spectral data). an engineer might want to    nd dependencies
in (voltage, current) pairs. likewise one might want to represent documents
by a vector of counts which describe the occurrence of words. the latter is
commonly referred to as bag of words features.

one of the challenges in dealing with vectors is that the scales and units
of di   erent coordinates may vary widely. for instance, we could measure the
height in kilograms, pounds, grams, tons, stones, all of which would amount
to multiplicative changes. likewise, when representing temperatures, we
have a full class of a   ne transformations, depending on whether we rep-
resent them in terms of celsius, kelvin or farenheit. one way of dealing

8

1 introduction

with those issues in an automatic fashion is to normalize the data. we will
discuss means of doing so in an automatic fashion.

lists: in some cases the vectors we obtain may contain a variable number
of features. for instance, a physician might not necessarily decide to perform
a full battery of diagnostic tests if the patient appears to be healthy.

sets may appear in learning problems whenever there is a large number of
potential causes of an e   ect, which are not well determined. for instance, it is
relatively easy to obtain data concerning the toxicity of mushrooms. it would
be desirable to use such data to infer the toxicity of a new mushroom given
information about its chemical compounds. however, mushrooms contain a
cocktail of compounds out of which one or more may be toxic. consequently
we need to infer the properties of an object given a set of features, whose
composition and number may vary considerably.

matrices are a convenient means of representing pairwise relationships.
for instance, in collaborative    ltering applications the rows of the matrix
may represent users whereas the columns correspond to products. only in
some cases we will have knowledge about a given (user, product) combina-
tion, such as the rating of the product by a user.

a related situation occurs whenever we only have similarity information
between observations, as implemented by a semi-empirical distance mea-
sure. some homology searches in bioinformatics, e.g. variants of blast
[agml90], only return a similarity score which does not necessarily satisfy
the requirements of a metric.

images could be thought of as two dimensional arrays of numbers, that is,
matrices. this representation is very crude, though, since they exhibit spa-
tial coherence (lines, shapes) and (natural images exhibit) a multiresolution
structure. that is, downsampling an image leads to an object which has very
similar statistics to the original image. id161 and psychooptics
have created a raft of tools for describing these phenomena.

video adds a temporal dimension to images. again, we could represent
them as a three dimensional array. good algorithms, however, take the tem-
poral coherence of the image sequence into account.

trees and graphs are often used to describe relations between collec-
tions of objects. for instance the ontology of webpages of the dmoz project
(www.dmoz.org) has the form of a tree with topics becoming increasingly
re   ned as we traverse from the root to one of the leaves (arts     animation
    anime     general fan pages     o   cial sites). in the case of gene ontol-
ogy the relationships form a directed acyclic graph, also referred to as the
go-dag [abb+00].

both examples above describe estimation problems where our observations

1.1 a taste of machine learning

9

are vertices of a tree or graph. however, graphs themselves may be the
observations. for instance, the dom-tree of a webpage, the call-graph of
a computer program, or the protein-protein interaction networks may form
the basis upon which we may want to perform id136.

strings occur frequently, mainly in the area of bioinformatics and natural
language processing. they may be the input to our estimation problems, e.g.
when classifying an e-mail as spam, when attempting to locate all names of
persons and organizations in a text, or when modeling the topic structure
of a document. equally well they may constitute the output of a system.
for instance, we may want to perform document summarization, automatic
translation, or attempt to answer natural language queries.

compound structures are the most commonly occurring object. that
is, in most situations we will have a structured mix of di   erent data types.
for instance, a webpage might contain images, text, tables, which in turn
contain numbers, and lists, all of which might constitute nodes on a graph of
webpages linked among each other. good statistical modelling takes such de-
pendencies and structures into account in order to tailor su   ciently    exible
models.

1.1.3 problems

the range of learning problems is clearly large, as we saw when discussing
applications. that said, researchers have identi   ed an ever growing number
of templates which can be used to address a large set of situations. it is those
templates which make deployment of machine learning in practice easy and
our discussion will largely focus on a choice set of such problems. we now
give a by no means complete list of templates.

binary classi   cation is probably the most frequently studied problem
in machine learning and it has led to a large number of important algorithmic
and theoretic developments over the past century. in its simplest form it
reduces to the question: given a pattern x drawn from a domain x, estimate
which value an associated binary random variable y     {  1} will assume.
for instance, given pictures of apples and oranges, we might want to state
whether the object in question is an apple or an orange. equally well, we
might want to predict whether a home owner might default on his loan,
given income data, his credit history, or whether a given e-mail is spam or
ham. the ability to solve this basic problem already allows us to address a
large variety of practical settings.

there are many variants exist with regard to the protocol in which we are

required to make our estimation:

10

1 introduction

fig. 1.6. left: binary classi   cation. right: 3-class classi   cation. note that in the
latter case we have much more degree for ambiguity. for instance, being able to
distinguish stars from diamonds may not su   ce to identify either of them correctly,
since we also need to distinguish both of them from triangles.

1, . . . , x(cid:48)

is known as active learning.

x(cid:48) =(cid:8)x(cid:48)

model. this is commonly referred to as transduction.

m(cid:48)(cid:9). this is commonly referred to as batch learning.

    we might see a sequence of (xi, yi) pairs for which yi needs to be estimated
in an instantaneous online fashion. this is commonly referred to as online
learning.
    we might observe a collection x := {x1, . . . xm} and y := {y1, . . . ym} of
pairs (xi, yi) which are then used to estimate y for a (set of) so-far unseen
    we might be allowed to know x(cid:48) already at the time of constructing the
    we might be allowed to choose x for the purpose of model building. this
    we might not have full information about x, e.g. some of the coordinates
of the xi might be missing, leading to the problem of estimation with
missing variables.
    the sets x and x(cid:48) might come from di   erent data sources, leading to the
    we might be given observations id30 from two problems at the same
time with the side information that both problems are somehow related.
this is known as co-training.
    mistakes of estimation might be penalized di   erently depending on the
type of error, e.g. when trying to distinguish diamonds from rocks a very
asymmetric loss applies.

problem of covariate shift correction.

multiclass classi   cation is the logical extension of binary classi   ca-
tion. the main di   erence is that now y     {1, . . . , n} may assume a range
of di   erent values. for instance, we might want to classify a document ac-
cording to the language it was written in (english, french, german, spanish,
hindi, japanese, chinese, . . . ). see figure 1.6 for an example. the main dif-
ference to before is that the cost of error may heavily depend on the type of

1.1 a taste of machine learning

11

fig. 1.7. regression estimation. we are given a number of instances (indicated by
black dots) and would like to    nd some function f mapping the observations x to
r such that f (x) is close to the observed values.

error we make. for instance, in the problem of assessing the risk of cancer, it
makes a signi   cant di   erence whether we mis-classify an early stage of can-
cer as healthy (in which case the patient is likely to die) or as an advanced
stage of cancer (in which case the patient is likely to be inconvenienced from
overly aggressive treatment).

structured estimation goes beyond simple multiclass estimation by
assuming that the labels y have some additional structure which can be used
in the estimation process. for instance, y might be a path in an ontology,
when attempting to classify webpages, y might be a permutation, when
attempting to match objects, to perform collaborative    ltering, or to rank
documents in a retrieval setting. equally well, y might be an annotation of
a text, when performing id39. each of those problems
has its own properties in terms of the set of y which we might consider
admissible, or how to search this space. we will discuss a number of those
problems in chapter ??.
regression is another prototypical application. here the goal is to esti-
mate a real-valued variable y     r given a pattern x (see e.g. figure 1.7). for
instance, we might want to estimate the value of a stock the next day, the
yield of a semiconductor fab given the current process, the iron content of
ore given mass spectroscopy measurements, or the heart rate of an athlete,
given accelerometer data. one of the key issues in which regression problems
di   er from each other is the choice of a loss. for instance, when estimating
stock values our loss for a put option will be decidedly one-sided. on the
other hand, a hobby athlete might only care that our estimate of the heart
rate matches the actual on average.

novelty detection is a rather ill-de   ned problem. it describes the issue
of determining    unusual    observations given a set of past measurements.
clearly, the choice of what is to be considered unusual is very subjective.
a commonly accepted notion is that unusual events occur rarely. hence a
possible goal is to design a system which assigns to each observation a rating

12

1 introduction

fig. 1.8. left: typical digits contained in the database of the us postal service.
right: unusual digits found by a novelty detection algorithm [spst+01] (for a
description of the algorithm see section 7.4). the score below the digits indicates
the degree of novelty. the numbers on the lower right indicate the class associated
with the digit.

as to how novel it is. readers familiar with density estimation might contend
that the latter would be a reasonable solution. however, we neither need a
score which sums up to 1 on the entire domain, nor do we care particularly
much about novelty scores for typical observations. we will later see how this
somewhat easier goal can be achieved directly. figure 1.8 has an example of
novelty detection when applied to an id42 database.

1.2 id203 theory

in order to deal with the instances of where machine learning can be used, we
need to develop an adequate language which is able to describe the problems
concisely. below we begin with a fairly informal overview over id203
theory. for more details and a very gentle and detailed discussion see the
excellent book of [bt03].

1.2.1 random variables

assume that we cast a dice and we would like to know our chances whether
we would see 1 rather than another digit. if the dice is fair all six outcomes
x = {1, . . . , 6} are equally likely to occur, hence we would see a 1 in roughly
1 out of 6 cases. id203 theory allows us to model uncertainty in the out-
come of such experiments. formally we state that 1 occurs with id203
1
6 .

in many experiments, such as the roll of a dice, the outcomes are of a
numerical nature and we can handle them easily. in other cases, the outcomes
may not be numerical, e.g., if we toss a coin and observe heads or tails. in
these cases, it is useful to associate numerical values to the outcomes. this
is done via a random variable. for instance, we can let a random variable

1.2 id203 theory
13
x take on a value +1 whenever the coin lands heads and a value of    1
otherwise. our notational convention will be to use uppercase letters, e.g.,
x, y etc to denote random variables and lower case letters, e.g., x, y etc to
denote the values they take.

fig. 1.9. the random variable    maps from the set of outcomes of an experiment
(denoted here by x) to real numbers. as an illustration here x consists of the
patients a physician might encounter, and they are mapped via    to their weight
and height.

1.2.2 distributions

perhaps the most important way to characterize a random variable is to
associate probabilities with the values it can take. if the random variable is
discrete, i.e., it takes on a    nite number of values, then this assignment of
probabilities is called a id203 mass function or pmf for short. a pmf
must be, by de   nition, non-negative and must sum to one. for instance,
if the coin is fair, i.e., heads and tails are equally likely, then the random
variable x described above takes on values of +1 and    1 with id203
0.5. this can be written as

p r(x = +1) = 0.5 and p r(x =    1) = 0.5.

(1.1)

when there is no danger of confusion we will use the slightly informal no-
tation p(x) := p r(x = x).

in case of a continuous random variable the assignment of probabilities
results in a id203 density function or pdf for short. with some abuse
of terminology, but keeping in line with convention, we will often use density
or distribution instead of id203 density function. as in the case of the
pmf, a pdf must also be non-negative and integrate to one. figure 1.10
shows two distributions: the uniform distribution
if x     [a, b]
otherwise,

(cid:40) 1

b   a
0

p(x) =

(1.2)

xweightheight  (x)x14

1 introduction

fig. 1.10. two common densities. left: uniform distribution over the interval
[   1, 1]. right: normal distribution with zero mean and unit variance.

and the gaussian distribution (also called normal distribution)

(cid:18)
    (x       )2

(cid:19)

2  2

p(x) =

1   
2    2

exp

.

(1.3)

closely associated with a pdf is the inde   nite integral over p. it is com-
monly referred to as the cumulative distribution function (cdf).

de   nition 1.1 (cumulative distribution function) for a real valued
random variable x with pdf p the associated cumulative distribution func-
tion f is given by

f (x(cid:48)) := pr(cid:8)x     x(cid:48)(cid:9) =

(cid:90) x(cid:48)

      

dp(x).

(1.4)

(cid:90) b

the cdf f (x(cid:48)) allows us to perform range queries on p e   ciently. for
instance, by integral calculus we obtain

pr(a     x     b) =

dp(x) = f (b)     f (a).

(1.5)

the values of x(cid:48) for which f (x(cid:48)) assumes a speci   c value, such as 0.1 or 0.5
have a special name. they are called the quantiles of the distribution p.

a

de   nition 1.2 (quantiles) let q     (0, 1). then the value of x(cid:48) for which
pr(x < x(cid:48))     q and pr(x > x(cid:48))     1     q is the q-quantile of the distribution
p. moreover, the value x(cid:48) associated with q = 0.5 is called the median.

-4-20240.00.10.20.30.40.5-4-20240.00.10.20.30.40.51.2 id203 theory

15

fig. 1.11. quantiles of a distribution correspond to the area under the integral of
the density p(x) for which the integral takes on a pre-speci   ed value. illustrated
are the 0.1, 0.5 and 0.9 quantiles respectively.

1.2.3 mean and variance

a common question to ask about a random variable is what its expected
value might be. for instance, when measuring the voltage of a device, we
might ask what its typical values might be. when deciding whether to ad-
minister a growth hormone to a child a doctor might ask what a sensible
range of height should be. for those purposes we need to de   ne expectations
and related quantities of distributions.

de   nition 1.3 (mean) we de   ne the mean of a random variable x as

e[x] :=

xdp(x)

(1.6)
more generally, if f : r     r is a function, then f (x) is also a random
variable. its mean is mean given by
e[f (x)] :=

f (x)dp(x).

(cid:90)

(1.7)

(cid:88)

e[x] =

whenever x is a discrete random variable the integral in (1.6) can be re-
placed by a summation:

xp(x).

(1.8)

x

for instance, in the case of a dice we have equal probabilities of 1/6 for all
6 possible outcomes. it is easy to see that this translates into a mean of
(1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5.

the mean of a random variable is useful in assessing expected losses and
bene   ts. for instance, as a stock broker we might be interested in the ex-
pected value of our investment in a year   s time. in addition to that, however,
we also might want to investigate the risk of our investment. that is, how
likely it is that the value of the investment might deviate from its expecta-
tion since this might be more relevant for our decisions. this means that we

(cid:90)

p(x)16

1 introduction

need a variable to quantify the risk inherent in a random variable. one such
measure is the variance of a random variable.

de   nition 1.4 (variance) we de   ne the variance of a random variable
x as

(1.9)
as before, if f : r     r is a function, then the variance of f (x) is given by

var[x] := e(cid:104)
var[f (x)] := e(cid:104)

(x     e[x])2(cid:105)
(f (x)     e[f (x)])2(cid:105)

.

.

(1.10)

the variance measures by how much on average f (x) deviates from its ex-
pected value. as we shall see in section 2.1, an upper bound on the variance
can be used to give guarantees on the id203 that f (x) will be within
  of its expected value. this is one of the reasons why the variance is often
associated with the risk of a random variable. note that often one discusses
properties of a random variable in terms of its standard deviation, which is
de   ned as the square root of the variance.

1.2.4 marginalization, independence, conditioning, and bayes

rule

given two random variables x and y , one can write their joint density
p(x, y). given the joint density, one can recover p(x) by integrating out y.
this operation is called marginalization:

p(x) =

dp(x, y).

(1.11)

if y is a discrete random variable, then we can replace the integration with
a summation:

p(x) =

p(x, y).

(1.12)

y

we say that x and y are independent, i.e., the values that x takes does

not depend on the values that y takes whenever

p(x, y) = p(x)p(y).

(1.13)

independence is useful when it comes to dealing with large numbers of ran-
dom variables whose behavior we want to estimate jointly. for instance,
whenever we perform repeated measurements of a quantity, such as when

(cid:90)

y

(cid:88)

1.2 id203 theory

17

fig. 1.12. left: a sample from two dependent random variables. knowing about
   rst coordinate allows us to improve our guess about the second coordinate. right:
a sample drawn from two independent random variables, obtained by randomly
permuting the dependent sample.

measuring the voltage of a device, we will typically assume that the individ-
ual measurements are drawn from the same distribution and that they are
independent of each other. that is, having measured the voltage a number
of times will not a   ect the value of the next measurement. we will call such
random variables to be independently and identically distributed, or in short,
iid random variables. see figure 1.12 for an example of a pair of random
variables drawn from dependent and independent distributions respectively.
conversely, dependence can be vital in classi   cation and regression prob-
lems. for instance, the tra   c lights at an intersection are dependent of each
other. this allows a driver to perform the id136 that when the lights are
green in his direction there will be no tra   c crossing his path, i.e. the other
lights will indeed be red. likewise, whenever we are given a picture x of a
digit, we hope that there will be dependence between x and its label y.

especially in the case of dependent random variables, we are interested
in conditional probabilities, i.e., id203 that x takes on a particular
value given the value of y . clearly p r(x = rain|y = cloudy) is higher than
p r(x = rain|y = sunny). in other words, knowledge about the value of y
signi   cantly in   uences the distribution of x. this is captured via conditional
probabilities:

p(x|y) :=

p(x, y)

p(y)

.

(1.14)

equation 1.14 leads to one of the key tools in statistical id136.

theorem 1.5 (bayes rule) denote by x and y random variables then

-0.50.00.51.01.52.0-0.50.00.51.01.52.0-0.50.00.51.01.52.0-0.50.00.51.01.52.018

the following holds

1 introduction

p(y|x) =

p(x|y)p(y)

(1.15)
this follows from the fact that p(x, y) = p(x|y)p(y) = p(y|x)p(x). the key
consequence of (1.15) is that we may reverse the conditioning between a
pair of random variables.

p(x)

.

1.2.4.1 an example

we illustrate our reasoning by means of a simple example     id136 using
an aids test. assume that a patient would like to have such a test carried
out on him. the physician recommends a test which is guaranteed to detect
hiv-positive whenever a patient is infected. on the other hand, for healthy
patients it has a 1% error rate. that is, with id203 0.01 it diagnoses
a patient as hiv-positive even when he is, in fact, hiv-negative. moreover,
assume that 0.15% of the population is infected.

now assume that the patient has the test carried out and the test re-
turns    hiv-negative   . in this case, logic implies that he is healthy, since the
test has 100% detection rate. in the converse case things are not quite as
straightforward. denote by x and t the random variables associated with
the health status of the patient and the outcome of the test respectively. we
are interested in p(x = hiv+|t = hiv+). by bayes rule we may write
p(t = hiv+|x = hiv+)p(x = hiv+)

p(x = hiv+|t = hiv+) =

p(t = hiv+)

while we know all terms in the numerator, p(t = hiv+) itself is unknown.
that said, it can be computed via

(cid:88)
(cid:88)

p(t = hiv+) =

=

x   {hiv+,hiv-}

p(t = hiv+, x)

p(t = hiv+|x)p(x)

x   {hiv+,hiv-}

= 1.0    0.0015 + 0.01    0.9985.

substituting back into the conditional expression yields

p(x = hiv+|t = hiv+) =

1.0    0.0015

1.0    0.0015 + 0.01    0.9985

= 0.1306.

in other words, even though our test is quite reliable, there is such a low
prior id203 of having been infected with aids that there is not much
evidence to accept the hypothesis even after this test.

1.2 id203 theory

19

fig. 1.13. a graphical description of our hiv testing scenario. knowing the age of
the patient in   uences our prior on whether the patient is hiv positive (the random
variable x). the outcomes of the tests 1 and 2 are independent of each other given
the status x. we observe the shaded random variables (age, test 1, test 2) and
would like to infer the un-shaded random variable x. this is a special case of a
graphical model which we will discuss in chapter ??.

let us now think how we could improve the diagnosis. one way is to ob-
tain further information about the patient and to use this in the diagnosis.
for instance, information about his age is quite useful. suppose the patient
is 35 years old. in this case we would want to compute p(x = hiv+|t =
hiv+, a = 35) where the random variable a denotes the age. the corre-
sponding expression yields:

p(t = hiv+|x = hiv+, a)p(x = hiv+|a)

p(t = hiv+|a)

here we simply conditioned all random variables on a in order to take addi-
tional information into account. we may assume that the test is independent
of the age of the patient, i.e.

p(t|x, a) = p(t|x).

what remains therefore is p(x = hiv+|a). recent us census data pegs this
number at approximately 0.9%. plugging all data back into the conditional
expression yields
1  0.009+0.01  0.991 = 0.48. what has happened here is that
by including additional observed random variables our estimate has become
more reliable. combination of evidence is a powerful tool. in our case it
helped us make the classi   cation problem of whether the patient is hiv-
positive or not more reliable.

1  0.009

a second tool in our arsenal is the use of multiple measurements. after
the    rst test the physician is likely to carry out a second test to con   rm the
diagnosis. we denote by t1 and t2 (and t1, t2 respectively) the two tests.
obviously, what we want is that t2 will give us an    independent    second
opinion of the situation. in other words, we want to ensure that t2 does
not make the same mistakes as t1. for instance, it is probably a bad idea
to repeat t1 without changes, since it might perform the same diagnostic

agextest 1test 220

1 introduction

mistake as before. what we want is that the diagnosis of t2 is independent
of that of t2 given the health status x of the patient. this is expressed as

p(t1, t2|x) = p(t1|x)p(t2|x).

(1.16)

see figure 1.13 for a graphical illustration of the setting. random variables
satisfying the condition (1.16) are commonly referred to as conditionally
independent. in shorthand we write t1, t2        x. for the sake of the argument
we assume that the statistics for t2 are given by

x = hiv- x = hiv+

p(t2|x)
0.95
t2 = hiv-
t2 = hiv+ 0.05

0.01
0.99

= 0.95.

clearly this test is less reliable than the    rst one. however, we may now
combine both estimates to obtain a very reliable estimate based on the
combination of both events. for instance, for t1 = t2 = hiv+ we have
p(x = hiv+|t1 = hiv+, t2 = hiv+) =

1.0    0.99    0.009 + 0.01    0.05    0.991
in other words, by combining two tests we can now con   rm with very high
con   dence that the patient is indeed diseased. what we have carried out is a
combination of evidence. strong experimental evidence of two positive tests
e   ectively overcame an initially very strong prior which suggested that the
patient might be healthy.

1.0    0.99    0.009

tests such as in the example we just discussed are fairly common. for
instance, we might need to decide which manufacturing procedure is prefer-
able, which choice of parameters will give better results in a regression es-
timator, or whether to administer a certain drug. note that often our tests
may not be conditionally independent and we would need to take this into
account.

1.3 basic algorithms

we conclude our introduction to machine learning by discussing four simple
algorithms, namely naive bayes, nearest neighbors, the mean classi   er,
and the id88, which can be used to solve a binary classi   cation prob-
lem such as that described in figure 1.5. we will also introduce the id116
algorithm which can be employed when labeled data is not available. all
these algorithms are readily usable and easily implemented from scratch in
their most basic form.
for the sake of concreteness assume that we are interested in spam    lter-
ing. that is, we are given a set of m e-mails xi, denoted by x := {x1, . . . , xm}

1.3 basic algorithms

21

from: "lucindaparkison497072" <lucindaparkison497072@hotmail.com>
to: <kargr@earthlink.net>
subject: we think acgu is our next winner
date: mon, 25 feb 2008 00:01:01 -0500
mime-version: 1.0
x-originalarrivaltime: 25 feb 2008 05:01:01.0329 (utc) filetime=[6a931810:01c8776b]
return-path: lucindaparkison497072@hotmail.com

(acgu) .045 up 104.5%

i do think that (acgu) at it   s current levels looks extremely attractive.

asset capital group, inc., (acgu) announced that it is expanding the marketing of bio-remediation fluids and cleaning equipment. after
its recent acquisition of interest in american bio-clean corporation and an 80

news is expected to be released next week on this growing company and could drive the price even higher. buy (acgu) monday at open. i
believe those involved at this stage could enjoy a nice ride up.

fig. 1.14. example of a spam e-mail

x1: the quick brown fox jumped over the lazy dog.
x2: the dog hunts a fox.

the

quick

brown

fox

jumped

over

lazy

dog

hunts

x1
x2

2
1

1
0

1
0

1
1

1
0

1
0

1
0

1
1

0
1

a

0
1

fig. 1.15. vector space representation of strings.

and associated labels yi, denoted by y := {y1, . . . , ym}. here the labels sat-
isfy yi     {spam, ham}. the key assumption we make here is that the pairs
(xi, yi) are drawn jointly from some distribution p(x, y) which represents
the e-mail generating process for a user. moreover, we assume that there
is su   ciently strong dependence between x and y that we will be able to
estimate y given x and a set of labeled instances x, y.

before we do so we need to address the fact that e-mails such as figure 1.14
are text, whereas the three algorithms we present will require data to be
represented in a vectorial fashion. one way of converting text into a vector
is by using the so-called bag of words representation [mar61, lew98]. in its
simplest version it works as follows: assume we have a list of all possible
words occurring in x, that is a dictionary, then we are able to assign a unique
number with each of those words (e.g. the position in the dictionary). now
we may simply count for each document xi the number of times a given
word j is occurring. this is then used as the value of the j-th coordinate
of xi. figure 1.15 gives an example of such a representation. once we have
the latter it is easy to compute distances, similarities, and other statistics
directly from the vectorial representation.

22

1.3.1 naive bayes

1 introduction

in the example of the aids test we used the outcomes of the test to infer
whether the patient is diseased. in the context of spam    ltering the actual
text of the e-mail x corresponds to the test and the label y is equivalent to
the diagnosis. recall bayes rule (1.15). we could use the latter to infer

p(y|x) =

p(x|y)p(y)

p(x)

.

we may have a good estimate of p(y), that is, the id203 of receiving
a spam or ham mail. denote by mham and mspam the number of ham and
spam e-mails in x. in this case we can estimate

p(ham)     mham
m

and p(spam)     mspam
m

.

the key problem, however, is that we do not know p(x|y) or p(x). we may
dispose of the requirement of knowing p(x) by settling for a likelihood ratio

l(x) :=

p(spam|x)
p(ham|x)

=

p(x|spam)p(spam)
p(x|ham)p(ham)

.

(1.17)

whenever l(x) exceeds a given threshold c we decide that x is spam and
consequently reject the e-mail. if c is large then our algorithm is conservative
and classi   es an email as spam only if p(spam|x) (cid:29) p(ham|x). on the other
hand, if c is small then the algorithm aggressively classi   es emails as spam.
the key obstacle is that we have no access to p(x|y). this is where we make
our key approximation. recall figure 1.13. in order to model the distribution
of the test outcomes t1 and t2 we made the assumption that they are
conditionally independent of each other given the diagnosis. analogously,
we may now treat the occurrence of each word in a document as a separate
test and combine the outcomes in a naive fashion by assuming that

p(x|y) =

p(wj|y),

(1.18)

j=1

where wj denotes the j-th word in document x. this amounts to the as-
sumption that the id203 of occurrence of a word in a document is
independent of all other words given the category of the document. even
though this assumption does not hold in general     for instance, the word
   york    is much more likely to after the word    new        it su   ces for our
purposes (see figure 1.16).
this assumption reduces the di   culty of knowing p(x|y) to that of esti-
mating the probabilities of occurrence of individual words w. estimates for

# of words in x(cid:89)

1.3 basic algorithms

23

fig. 1.16. naive bayes model. the occurrence of individual words is independent
of each other, given the category of the text. for instance, the word viagra is fairly
frequent if y = spam but it is considerably less frequent if y = ham, except when
considering the mailbox of a p   zer sales representative.

(cid:111)

p(w|y) can be obtained, for instance, by simply counting the frequency oc-
currence of the word within documents of a given class. that is, we estimate

(cid:80)m

i=1

(cid:110)

(cid:80)# of words in xi
(cid:80)m
(cid:111)

(cid:80)# of words in xi

j=1

j=1

i=1

p(w|spam)    

(cid:110)

yi = spam and wj
{yi = spam}

i = w

yi = spam and wj

i = w

here
equals 1 if and only if xi is labeled as spam
and w occurs as the j-th word in xi. the denominator is simply the total
number of words in spam documents. similarly one can compute p(w|ham).
in principle we could perform the above summation whenever we see a new
document x. this would be terribly ine   cient, since each such computation
requires a full pass through x and y. instead, we can perform a single pass
through x and y and store the resulting statistics as a good estimate of the
conditional probabilities. algorithm 1.1 has details of an implementation.
note that we performed a number of optimizations: firstly, the normaliza-
tion by m   1
ham respectively is independent of x, hence we incor-
porate it as a    xed o   set. secondly, since we are computing a product over
a large number of factors the numbers might lead to numerical over   ow or
under   ow. this can be addressed by summing over the logarithm of terms
rather than computing products. thirdly, we need to address the issue of
estimating p(w|y) for words w which we might not have seen before. one
way of dealing with this is to increment all counts by 1. this method is
commonly referred to as laplace smoothing. we will encounter a theoretical
justi   cation for this heuristic in section 2.3.

spam and m   1

this simple algorithm is known to perform surprisingly well, and variants
of it can be found in most modern spam    lters. it amounts to what is
commonly known as    bayesian spam    ltering   . obviously, we may apply it
to problems other than document categorization, too.

yword 1word 2...word nword 324

1 introduction

algorithm 1.1 naive bayes
train(x, y) {reads documents x and labels y}

compute dictionary d of x with n words.
compute m, mham and mspam.
initialize b := log c + log mham    log mspam to o   set the rejection threshold
initialize p     r2  n with pij = 1, wspam = n, wham = n.
{count occurrence of each word}
{here xj
for i = 1 to m do

i denotes the number of times word j occurs in document xi}

if yi = spam then
for j = 1 to n do
p0,j     p0,j + xj
wspam     wspam + xj

i

i

end for

else

for j = 1 to n do
p1,j     p1,j + xj
wham     wham + xj

i

i

end for

end if
end for
{normalize counts to yield word probabilities}
for j = 1 to n do
p0,j     p0,j/wspam
p1,j     p1,j/wham

end for
classify(x) {classi   es document x}
initialize score threshold t =    b
for j = 1 to n do
t     t + xj(log p0,j     log p1,j)

end for
if t > 0 return spam else return ham

1.3.2 nearest neighbor estimators

an even simpler estimator than naive bayes is nearest neighbors. in its most
basic form it assigns the label of its nearest neighbor to an observation x
(see figure 1.17). hence, all we need to implement it is a distance measure
d(x, x(cid:48)) between pairs of observations. note that this distance need not even
be symmetric. this means that nearest neighbor classi   ers can be extremely

1.3 basic algorithms

25

fig. 1.17. 1 nearest neighbor classi   er. depending on whether the query point x is
closest to the star, diamond or triangles, it uses one of the three labels for it.

fig. 1.18. k-nearest neighbor classi   ers using euclidean distances. left: decision
boundaries obtained from a 1-nearest neighbor classi   er. middle: color-coded sets
of where the number of red / blue points ranges between 7 and 0. right: decision
boundary determining where the blue or red dots are in the majority.

   exible. for instance, we could use string id153s to compare two
documents or id205 based measures.

however, the problem with nearest neighbor classi   cation is that the esti-
mates can be very noisy whenever the data itself is very noisy. for instance,
if a spam email is erroneously labeled as nonspam then all emails which
are similar to this email will share the same fate. see figure 1.18 for an
example. in this case it is bene   cial to pool together a number of neighbors,
say the k-nearest neighbors of x and use a majority vote to decide the class
membership of x. algorithm 1.2 has a description of the algorithm. note
that nearest neighbor algorithms can yield excellent performance when used
with a good distance measure. for instance, the technology underlying the
net   ix progress prize [bk07] was essentially nearest neighbours based.

note that it is trivial to extend the algorithm to regression. all we need
to change in algorithm 1.2 is to return the average of the values yi instead
of their majority vote. figure 1.19 has an example.

note that the distance computation d(xi, x) for all observations can be-

26

1 introduction

algorithm 1.2 k-nearest neighbor classi   cation
classify(x, y, x) {reads documents x, labels y and query x}

for i = 1 to m do

compute distance d(xi, x)

end for
compute set i containing indices for the k smallest distances d(xi, x).
return majority label of {yi where i     i}.

fig. 1.19. k-nearest neighbor regression estimator using euclidean distances. left:
some points (x, y) drawn from a joint distribution. middle: 1-nearest neighbour
classi   er. right: 7-nearest neighbour classi   er. note that the regression estimate is
much more smooth.

come extremely costly, in particular whenever the number of observations is
large or whenever the observations xi live in a very high dimensional space.
random projections are a technique that can alleviate the high computa-
tional cost of nearest neighbor classi   ers. a celebrated lemma by johnson
and lindenstrauss [dg03] asserts that a set of m points in high dimensional
euclidean space can be projected into a o(log m/ 2) dimensional euclidean
space such that the distance between any two points changes only by a fac-
tor of (1     ). since euclidean distances are preserved, running the nearest
neighbor classi   er on this mapped data yields the same results but at a
lower computational cost [gim99].

the surprising fact is that the projection relies on a simple randomized
algorithm: to obtain a d-dimensional representation of n-dimensional ran-
dom observations we pick a matrix r     rd  n where each element is drawn
independently from a normal distribution with n    1
2 variance and zero mean.
multiplying x with this projection matrix can be shown to achieve this prop-
erty with high id203. for details see [dg03].

1.3 basic algorithms

27

fig. 1.20. a trivial classi   er. classi   cation is carried out in accordance to which of
the two means       or   + is closer to the test point x. note that the sets of positive
and negative labels respectively form a half space.

1.3.3 a simple classi   er

we can use geometry to design another simple classi   cation algorithm [ss02]
for our problem. for simplicity we assume that the observations x     rd, such
as the bag-of-words representation of e-mails. we de   ne the means   + and
      to correspond to the classes y     {  1} via

(cid:88)

yi=   1

      :=

1
m   

xi and   + :=

1
m+

(cid:88)

yi=1

xi.

here we used m    and m+ to denote the number of observations with label
yi =    1 and yi = +1 respectively. an even simpler approach than using the
nearest neighbor classi   er would be to use the class label which corresponds
to the mean closest to a new query x, as described in figure 1.20.

for euclidean distances we have

(1.19)

(cid:107)          x(cid:107)2 = (cid:107)     (cid:107)2 + (cid:107)x(cid:107)2     2(cid:104)     , x(cid:105) and
(cid:107)  +     x(cid:107)2 = (cid:107)  +(cid:107)2 + (cid:107)x(cid:107)2     2(cid:104)  +, x(cid:105) .

(1.20)
here (cid:104)  ,  (cid:105) denotes the standard dot product between vectors. taking di   er-
ences between the two distances yields

f (x) := (cid:107)  +     x(cid:107)2     (cid:107)          x(cid:107)2 = 2(cid:104)            +, x(cid:105) + (cid:107)     (cid:107)2     (cid:107)  +(cid:107)2 .

(1.21)

this is a linear function in x and its sign corresponds to the labels we esti-
mate for x. our algorithm sports an important property: the classi   cation
rule can be expressed via dot products. this follows from
(cid:107)  +(cid:107)2 = (cid:104)  +,   +(cid:105) = m   2

(cid:104)xi, xj(cid:105) and (cid:104)  +, x(cid:105) = m   1

(cid:104)xi, x(cid:105) .

(cid:88)

(cid:88)

+

+

yi=yj =1

yi=1

w  -  +x28

1 introduction

fig. 1.21. the feature map    maps observations x from x into a feature space h.
the map    is a convenient way of encoding pre-processing steps systematically.

m(cid:88)

i=1

analogous expressions can be computed for      . consequently we may ex-
press the classi   cation rule (1.21) as

f (x) =

  i (cid:104)xi, x(cid:105) + b

where b = m   2    (cid:80)

yi=yj =   1 (cid:104)xi, xj(cid:105)    m   2

+

(cid:80)
yi=yj =1 (cid:104)xi, xj(cid:105) and   i = yi/myi.

(1.22)

this o   ers a number of interesting extensions. recall that when dealing
with documents we needed to perform pre-processing to map e-mails into a
vector space. in general, we may pick arbitrary maps    : x     h mapping
the space of observations into a feature space h, as long as the latter is
endowed with a dot product (see figure 1.21). this means that instead of
dealing with (cid:104)x, x(cid:48)(cid:105) we will be dealing with (cid:104)  (x),   (x(cid:48))(cid:105).

as we will see in chapter 6, whenever h is a so-called reproducing kernel
hilbert space, the inner product can be abbreviated in the form of a kernel
function k(x, x(cid:48)) which satis   es

k(x, x(cid:48)) :=(cid:10)  (x),   (x(cid:48))(cid:11) .

(1.23)

this small modi   cation leads to a number of very powerful algorithm and
it is at the foundation of an area of research called kernel methods. we
will encounter a number of such algorithms for regression, classi   cation,
segmentation, and density estimation over the course of the book. examples
of suitable k are the polynomial kernel k(x, x(cid:48)) = (cid:104)x, x(cid:48)(cid:105)d for d     n and the
gaussian rbf kernel k(x, x(cid:48)) = e     (cid:107)x   x(cid:48)(cid:107)2

for    > 0.

the upshot of (1.23) is that our basic algorithm can be kernelized. that

is, we may rewrite (1.21) as

f (x) =

m(cid:88)

where as before   i = yi/myi and the o   set b is computed analogously. as

i=1

  ik(xi, x) + b

(1.24)

x  (x)xh1.3 basic algorithms

29

algorithm 1.3 the id88
id88(x, y) {reads stream of observations (xi, yi)}

initialize w = 0 and b = 0
while there exists some (xi, yi) with yi((cid:104)w, xi(cid:105) + b)     0 do

w     w + yixi and b     b + yi

end while

algorithm 1.4 the kernel id88
kernelid88(x, y) {reads stream of observations (xi, yi)}

initialize f = 0
while there exists some (xi, yi) with yif (xi)     0 do

f     f + yik(xi,  ) + yi

end while

a consequence we have now moved from a fairly simple and pedestrian lin-
ear classi   er to one which yields a nonlinear function f (x) with a rather
nontrivial decision boundary.

1.3.4 id88

in the previous sections we assumed that our classi   er had access to a train-
ing set of spam and non-spam emails. in real life, such a set might be di   cult
to obtain all at once. instead, a user might want to have instant results when-
ever a new e-mail arrives and he would like the system to learn immediately
from any corrections to mistakes the system makes.

to overcome both these di   culties one could envisage working with the
following protocol: as emails arrive our algorithm classi   es them as spam or
non-spam, and the user provides feedback as to whether the classi   cation is
correct or incorrect. this feedback is then used to improve the performance
of the classi   er over a period of time.

this intuition can be formalized as follows: our classi   er maintains a
parameter vector. at the t-th time instance it receives a data point xt, to
which it assigns a label   yt using its current parameter vector. the true label
yt is then revealed, and used to update the parameter vector of the classi   er.
such algorithms are said to be online. we will now describe perhaps the
simplest classi   er of this kind namely the id88 [heb49, ros58].
let us assume that the data points xt     rd, and labels yt     {  1}. as
before we represent an email as a bag-of-words vector and we assign +1 to
spam emails and    1 to non-spam emails. the id88 maintains a weight

30

1 introduction

fig. 1.22. the id88 without bias. left: at time t we have a weight vector wt
denoted by the dashed arrow with corresponding separating plane (also dashed).
for reference we include the linear separator w    and its separating plane (both
denoted by a solid line). as a new observation xt arrives which happens to be
mis-classi   ed by the current weight vector wt we perform an update. also note the
margin between the point xt and the separating hyperplane de   ned by w   . right:
this leads to the weight vector wt+1 which is more aligned with w   .

vector w     rd and classi   es xt according to the rule

  yt := sign{(cid:104)w, xt(cid:105) + b},

(1.25)
where (cid:104)w, xt(cid:105) denotes the usual euclidean dot product and b is an o   set. note
the similarity of (1.25) to (1.21) of the simple classi   er. just as the latter,
the id88 is a linear classi   er which separates its domain rd into two
halfspaces, namely {x|(cid:104)w, x(cid:105) + b > 0} and its complement. if   yt = yt then
no updates are made. on the other hand, if   yt (cid:54)= yt the weight vector is
updated as

w     w + ytxt and b     b + yt.

(1.26)

figure 1.22 shows an update step of the id88 algorithm. for simplicity
we illustrate the case without bias, that is, where b = 0 and where it remains
unchanged. a detailed description of the algorithm is given in algorithm 1.3.
an important property of the algorithm is that it performs updates on w
by multiples of the observations xi on which it makes a mistake. hence we
i   error yixi. just as before, we can replace xi and x
by   (xi) and   (x) to obtain a kernelized version of the id88 algorithm
[fs99] (algorithm 1.4).

may express w as w =(cid:80)

if the dataset (x, y) is linearly separable, then the id88 algorithm

w*wtw*wt+1xtxt1.3 basic algorithms

31

eventually converges and correctly classi   es all the points in x. the rate of
convergence however depends on the margin. roughly speaking, the margin
quanti   es how linearly separable a dataset is, and hence how easy it is to
solve a given classi   cation problem.
de   nition 1.6 (margin) let w     rd be a weight vector and let b     r be
an o   set. the margin of an observation x     rd with associated label y is

  (x, y) := y ((cid:104)w, x(cid:105) + b) .

(1.27)

moreover, the margin of an entire set of observations x with labels y is

  (x, y) := min

i

  (xi, yi).

(1.28)

geometrically speaking (see figure 1.22) the margin measures the distance
of x from the hyperplane de   ned by {x|(cid:104)w, x(cid:105) + b = 0}. larger the margin,
the more well separated the data and hence easier it is to    nd a hyperplane
with correctly classi   es the dataset. the following theorem asserts that if
there exists a linear classi   er which can classify a dataset with a large mar-
gin, then the id88 will also correctly classify the same dataset after
making a small number of mistakes.

theorem 1.7 (noviko       s theorem) let (x, y) be a dataset with at least
one example labeled +1 and one example labeled    1. let r := maxt (cid:107)xt(cid:107), and
assume that there exists (w   , b   ) such that (cid:107)w   (cid:107) = 1 and   t := yt((cid:104)w   , xt(cid:105) +
b   )        for all t. then, the id88 will make at most (1+r2)(1+(b   )2)
mistakes.

  2

this result is remarkable since it does not depend on the dimensionality
of the problem. instead, it only depends on the geometry of the setting,
as quanti   ed via the margin    and the radius r of a ball enclosing the
observations. interestingly, a similar bound can be shown for support vector
machines [vap95] which we will be discussing in chapter 7.
proof we can safely ignore the iterations where no mistakes were made
and hence no updates were carried out. therefore, without loss of generality
assume that the t-th update was made after seeing the t-th observation and
let wt denote the weight vector after the update. furthermore, for simplicity
assume that the algorithm started with w0 = 0 and b0 = 0. by the update
equation (1.26) we have

(cid:104)wt, w   (cid:105) + btb    = (cid:104)wt   1, w   (cid:105) + bt   1b    + yt((cid:104)xt, w   (cid:105) + b   )

    (cid:104)wt   1, w   (cid:105) + bt   1b    +   .

32

1 introduction

by induction it follows that (cid:104)wt, w   (cid:105)+btb        t  . on the other hand we made
an update because yt((cid:104)xt, wt   1(cid:105) + bt   1) < 0. by using ytyt = 1,

(cid:107)wt(cid:107)2 + b2

t = (cid:107)wt   1(cid:107)2 + b2
    (cid:107)wt   1(cid:107)2 + b2

t   1 + y2
t   1 + (cid:107)xt(cid:107)2 + 1

t (cid:107)xt(cid:107)2 + 1 + 2yt((cid:104)wt   1, xt(cid:105) + bt   1)

t(cid:2)r2 + 1(cid:3). combining the upper and the lower bounds, using the cauchy-

since (cid:107)xt(cid:107)2 = r2 we can again apply induction to conclude that (cid:107)wt(cid:107)2+b2
schwartz inequality, and (cid:107)w   (cid:107) = 1 yields

t    

(cid:21)
(cid:28)(cid:20) wt
(cid:21)(cid:13)(cid:13)(cid:13)(cid:13) =
(cid:113)

bt

(cid:20) w   

b   

,

(cid:21)(cid:29)
(cid:112)

(cid:107)wt(cid:107)2 + b2

t

1 + (b   )2

t       (cid:104)wt, w   (cid:105) + btb    =

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) wt
   (cid:112)

   

bt

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) w   
(cid:112)

t(r2 + 1)

b   
1 + (b   )2.

squaring both sides of the inequality and rearranging the terms yields an
upper bound on the number of updates and hence the number of mistakes.

the id88 was the building block of research on neural networks
[hay98, bis95]. the key insight was to combine large numbers of such net-
works, often in a cascading fashion, to larger objects and to fashion opti-
mization algorithms which would lead to classi   ers with desirable properties.
in this book we will take a complementary route. instead of increasing the
number of nodes we will investigate what happens when increasing the com-
plexity of the feature map    and its associated kernel k. the advantage of
doing so is that we will reap the bene   ts from convex analysis and linear
models, possibly at the expense of a slightly more costly function evaluation.

1.3.5 id116

all the algorithms we discussed so far are supervised, that is, they assume
that labeled training data is available. in many applications this is too much
to hope for; labeling may be expensive, error prone, or sometimes impossi-
ble. for instance, it is very easy to crawl and collect every page within the
www.purdue.edu domain, but rather time consuming to assign a topic to
each page based on its contents. in such cases, one has to resort to unsuper-
vised learning. a prototypical unsupervised learning algorithm is id116,
which is id91 algorithm. given x = {x1, . . . , xm} the goal of id116
is to partition it into k clusters such that each point in a cluster is similar
to points from its own cluster than with points from some other cluster.

1.3 basic algorithms

33

towards this end, de   ne prototype vectors   1, . . . ,   k and an indicator
vector rij which is 1 if, and only if, xi is assigned to cluster j. to cluster our
dataset we will minimize the following distortion measure, which minimizes
the distance of each point from the prototype vector:

m(cid:88)

k(cid:88)

i=1

j=1

j(r,   ) :=

1
2

rij(cid:107)xi       j(cid:107)2,

(1.29)

where r = {rij},    = {  j}, and (cid:107)    (cid:107)2 denotes the usual euclidean square
norm.

our goal is to    nd r and   , but since it is not easy to jointly minimize j

with respect to both r and   , we will adapt a two stage strategy:

stage 1 keep the       xed and determine r. in this case, it is easy to see
that the minimization decomposes into m independent problems.
the solution for the i-th data point xi can be found by setting:

rij = 1 if j = argmin

j(cid:48)

(cid:107)xi       j(cid:48)(cid:107)2,

(1.30)

and 0 otherwise.

stage 2 keep the r    xed and determine   . since the r   s are    xed, j is an
quadratic function of   . it can be minimized by setting the derivative
with respect to   j to be 0:

m(cid:88)

rearranging obtains

i=1

since(cid:80)

rij(xi       j) = 0 for all j.

(cid:80)
i rijxi(cid:80)

i rij

.

  j =

(1.31)

(1.32)

i rij counts the number of points assigned to cluster j, we are
essentially setting   j to be the sample mean of the points assigned
to cluster j.

the algorithm stops when the cluster assignments do not change signi   -
cantly. detailed pseudo-code can be found in algorithm 1.5.

two issues with id116 are worth noting. first, it is sensitive to the
choice of the initial cluster centers   . a number of practical heuristics have
been developed. for instance, one could randomly choose k points from the
given dataset as cluster centers. other methods try to pick k points from x
which are farthest away from each other. second, it makes a hard assignment
of every point to a cluster center. variants which we will encounter later in

34

1 introduction

algorithm 1.5 id116
cluster(x) {cluster dataset x}

initialize cluster centers   j for j = 1, . . . , k randomly
repeat

for i = 1 to m do

compute j(cid:48) = argminj=1,...,k d(xi,   j)
set rij(cid:48) = 1 and rij = 0 for all j(cid:48) (cid:54)= j

end for
for j = 1 to k do
compute   j =

(cid:80)
i rij xi(cid:80)

i rij

end for

until cluster assignments rij are unchanged
return {  1, . . . ,   k} and rij

the book will relax this. instead of letting rij     {0, 1} these soft variants
will replace it with the id203 that a given xi belongs to cluster j.

the id116 algorithm concludes our discussion of a set of basic machine
learning methods for classi   cation and regression. they provide a useful
starting point for an aspiring machine learning researcher. in this book we
will see many more such algorithms as well as connections between these
basic algorithms and their more advanced counterparts.

problems

problem 1.1 (eyewitness) assume that an eyewitness is 90% certain
that a given person committed a crime in a bar. moreover, assume that
there were 50 people in the restaurant at the time of the crime. what is the
posterior id203 of the person actually having committed the crime.

problem 1.2 (dna test) assume the police have a dna library of 10
million records. moreover, assume that the false recognition id203 is
below 0.00001% per record. suppose a match is found after a database search
for an individual. what are the chances that the identi   cation is correct? you
can assume that the total population is 100 million people. hint: compute
the id203 of no match occurring    rst.

problem 1.3 (bomb threat) suppose that the id203 that one of a
thousand passengers on a plane has a bomb is 1 : 1, 000, 000. assuming that
the id203 to have a bomb is evenly distributed among the passengers,

1.3 basic algorithms
35
the id203 that two passengers have a bomb is roughly equal to 10   12.
therefore, one might decide to take a bomb on a plane to decrease chances
that somebody else has a bomb. what is wrong with this argument?

problem 1.4 (monty-hall problem) assume that in a tv show the
candidate is given the choice between three doors. behind two of the doors
there is a pencil and behind one there is the grand prize, a car. the candi-
date chooses one door. after that, the showmaster opens another door behind
which there is a pencil. should the candidate switch doors after that? what
is the id203 of winning the car?

problem 1.5 (mean and variance for random variables) denote by
xi random variables. prove that in this case

(cid:34)(cid:88)

(cid:35)

(cid:88)

(cid:34)(cid:88)

(cid:35)

(cid:88)

ex1,...xn

xi

=

exi[xi] and varx1,...xn

xi

=

varxi[xi]

i

i

i

i

to show the second equality assume independence of the xi.

problem 1.6 (two dices) assume you have a game which uses the max-
imum of two dices. compute the id203 of seeing any of the events
{1, . . . , 6}. hint: prove    rst that the cumulative distribution function of the
maximum of a pair of random variables is the square of the original cumu-
lative distribution function.

problem 1.7 (matching coins) consider the following game: two play-
ers bring a coin each. the    rst player bets that when tossing the coins both
will match and the second one bets that they will not match. show that even
if one of the players were to bring a tainted coin, the game still would be
fair. show that it is in the interest of each player to bring a fair coin to the
game. hint: assume that the second player knows that the    rst coin favors
heads over tails.

problem 1.8 (randomized maximization) how many observations do
you need to draw from a distribution to ensure that the maximum over them
is larger than 95% of all observations with at least 95% id203? hint:
generalize the result from problem 1.6 to the maximum over n random vari-
ables.

application: assume we have 1000 computers performing mapreduce [dg08]

and the reducers have to wait until all 1000 mappers are    nished with their
job. compute the quantile of the typical time to completion.

36

1 introduction

problem 1.9 prove that the normal distribution (1.3) has mean    and
variance   2. hint: exploit the fact that p is symmetric around   .

problem 1.10 (cauchy distribution) prove that for the density

p(x) =

1

  (1 + x2)

(1.33)

mean and variance are unde   ned. hint: show that the integral diverges.

problem 1.11 (quantiles) find a distribution for which the mean ex-
ceeds the median. hint: the mean depends on the value of the high-quantile
terms, whereas the median does not.

n(cid:89)

i=1

problem 1.12 (multicategory naive bayes) prove that for multicate-
gory naive bayes the optimal decision is given by

y   (x) := argmax

p(y)

y

p([x]i|y)

(1.34)

where y     y is the class label of the observation x.
problem 1.13 (bayes optimal decisions) denote by y   (x) = argmaxy p(y|x)
the label associated with the largest conditional class id203. prove that
for y   (x) the id203 of choosing the wrong label y is given by

l(x) := 1     p(y   (x)|x).

moreover, show that y   (x) is the label incurring the smallest misclassi   cation
error.

problem 1.14 (nearest neighbor loss) show that the expected loss in-
curred by the nearest neighbor classi   er does not exceed twice the loss of the
bayes optimal decision.

2

density estimation

2.1 limit theorems

assume you are a gambler and go to a casino to play a game of dice. as
it happens, it is your unlucky day and among the 100 times you toss the
dice, you only see    6    eleven times. for a fair dice we know that each face
should occur with equal id203 1
6 . hence the expected value over 100
6     17, which is considerably more than the eleven times that we
draws is 100
observed. before crying foul you decide that some mathematical analysis is
in order.

the id203 of seeing a particular sequence of m trials out of which n
are a    6    is given by 1
n!(m   n)! di   erent
sequences of    6    and    not 6    with proportions n and m   n respectively. hence
6
we may compute the id203 of seeing a    6    only 11 or less via

m   n. moreover, there are(cid:0)m
(cid:21)i(cid:20) 5

(cid:18)100

(cid:19)(cid:20) 1

(cid:1) =
(cid:21)100   i     7.0%

pr(x     11) =

11(cid:88)

11(cid:88)

p(i) =

(2.1)

n 5
6

m!

n

i=0

i=0

i

6

6

after looking at this    gure you decide that things are probably reasonable.
and, in fact, they are consistent with the convergence behavior of a sim-
ulated dice in figure 2.1. in computing (2.1) we have learned something
useful: the expansion is a special case of a binomial series. the    rst term

fig. 2.1. convergence of empirical means to expectations. from left to right: em-
pirical frequencies of occurrence obtained by casting a dice 10, 20, 50, 100, 200, and
500 times respectively. note that after 20 throws we still have not observed a single

   6   , an event which occurs with only(cid:2) 5

(cid:3)20     2.6% id203.

6

37

1234560.00.10.20.3m=101234560.00.10.20.3m=201234560.00.10.20.3m=501234560.00.10.20.3m=1001234560.00.10.20.3m=2001234560.00.10.20.3m=50038

2 density estimation

counts the number of con   gurations in which we could observe i times    6    in a
sequence of 100 dice throws. the second and third term are the probabilities
of seeing one particular instance of such a sequence.

note that in general we may not be as lucky, since we may have con-
siderably less information about the setting we are studying. for instance,
we might not know the actual probabilities for each face of the dice, which
would be a likely assumption when gambling at a casino of questionable
reputation. often the outcomes of the system we are dealing with may be
continuous valued random variables rather than binary ones, possibly even
with unknown range. for instance, when trying to determine the average
wage through a questionnaire we need to determine how many people we
need to ask in order to obtain a certain level of con   dence.

to answer such questions we need to discuss limit theorems. they tell
us by how much averages over a set of observations may deviate from the
corresponding expectations and how many observations we need to draw to
estimate a number of probabilities reliably. for completeness we will present
proofs for some of the more fundamental theorems in section 2.1.2. they
are useful albeit non-essential for the understanding of the remainder of the
book and may be omitted.

2.1.1 fundamental laws

the law of large numbers developed by bernoulli in 1713 is one of the
fundamental building blocks of statistical analysis. it states that averages
over a number of observations converge to their expectations given a su   -
ciently large number of observations and given certain assumptions on the
independence of these observations. it comes in two    avors: the weak and
the strong law.

theorem 2.1 (weak law of large numbers) denote by x1, . . . , xm
random variables drawn from p(x) with mean    = exi[xi] for all i. moreover
let

m(cid:88)

i=1

  xm :=

1
m

xi

(2.2)

be the empirical average over the random variables xi. then for any   > 0
the following holds

m       pr(cid:0)(cid:12)(cid:12)   xm       (cid:12)(cid:12)      (cid:1) = 1.

lim

(2.3)

2.1 limit theorems

39

fig. 2.2. the mean of a number of casts of a dice. the horizontal straight line
denotes the mean 3.5. the uneven solid line denotes the actual mean   xn as a
function of the number of draws, given as a semilogarithmic plot. the crosses denote
the outcomes of the dice. note how   xn ever more closely approaches the mean 3.5
are we obtain an increasing number of observations.

this establishes that, indeed, for large enough sample sizes, the average will
converge to the expectation. the strong law strengthens this as follows:

of theorem 2.1 we have pr(cid:0)limm         xm =   (cid:1) = 1.

theorem 2.2 (strong law of large numbers) under the conditions

the strong law implies that almost surely (in a measure theoretic sense)   xm
converges to   , whereas the weak law only states that for every   the random
variable   xm will be within the interval [      ,   + ]. clearly the strong implies
the weak law since the measure of the events   xm =    converges to 1, hence
any  -ball around    would capture this.

both laws justify that we may take sample averages, e.g. over a number
of events such as the outcomes of a dice and use the latter to estimate their
means, their probabilities (here we treat the indicator variable of the event
as a {0; 1}-valued random variable), their variances or related quantities. we
postpone a proof until section 2.1.2, since an e   ective way of proving theo-
rem 2.1 relies on the theory of characteristic functions which we will discuss
in the next section. for the moment, we only give a pictorial illustration in
figure 2.2.

once we established that the random variable   xm = m   1(cid:80)m

i=1 xi con-
verges to its mean   , a natural second question is to establish how quickly it
converges and what the properties of the limiting distribution of   xm      are.
note in figure 2.2 that the initial deviation from the mean is large whereas
as we observe more data the empirical mean approaches the true one.

10110210312345640

2 density estimation

fig. 2.3. five instantiations of a running average over outcomes of a toss of a dice.
note that all of them converge to the mean 3.5. moreover note that they all are

well contained within the upper and lower envelopes given by      (cid:112)varx [x]/m.

the central limit theorem answers this question exactly by addressing a
slightly more general question, namely whether the sum over a number of
independent random variables where each of them arises from a di   erent
distribution might also have a well behaved limiting distribution. this is
the case as long as the variance of each of the random variables is bounded.
the limiting distribution of such a sum is gaussian. this a   rms the pivotal
role of the gaussian distribution.

theorem 2.3 (central limit theorem) denote by xi independent ran-
dom variables with means   i and standard deviation   i. then

(cid:34) m(cid:88)

(cid:35)    1
2(cid:34) m(cid:88)

(cid:35)

xi       i

zm :=

  2
i

(2.4)

i=1

i=1

converges to a normal distribution with zero mean and unit variance.

note that just like the law of large numbers the central limit theorem (clt)
is an asymptotic result. that is, only in the limit of an in   nite number of
observations will it become exact. that said, it often provides an excellent
approximation even for    nite numbers of observations, as illustrated in fig-
ure 2.4. in fact, the central limit theorem and related limit theorems build
the foundation of what is known as asymptotic statistics.

example 2.1 (dice) if we are interested in computing the mean of the
values returned by a dice we may apply the clt to the sum over m variables

1011021031234562.1 limit theorems

41

we now study the random variable wm := m   1(cid:80)m

which have all mean    = 3.5 and variance (see problem 2.1)
varx [x] = ex [x2]     ex [x]2 = (1 + 4 + 9 + 16 + 25 + 36)/6     3.52     2.92.
i=1[xi     3.5]. since each
of the terms in the sum has zero mean, also wm   s mean vanishes. moreover,
wm is a multiple of zm of (2.4). hence we have that wm converges to a
normal distribution with zero mean and standard deviation 2.92m    1
2 .

consequently the average of m tosses of the dice yields a random vari-
able with mean 3.5 and it will approach a normal distribution with variance
m    1
2 2.92. in other words, the empirical mean converges to its average at
rate o(m    1
2 ). figure 2.3 gives an illustration of the quality of the bounds
implied by the clt.

one remarkable property of functions of random variables is that in many
conditions convergence properties of the random variables are bestowed upon
the functions, too. this is manifest in the following two results: a variant
of slutsky   s theorem and the so-called delta method. the former deals with
limit behavior whereas the latter deals with an extension of the central limit
theorem.

theorem 2.4 (slutsky   s theorem) denote by xi, yi sequences of ran-
dom variables with xi     x and yi     c for c     r in id203. moreover,
denote by g(x, y) a function which is continuous for all (x, c). in this case
the random variable g(xi, yi) converges in id203 to g(x, c).

for a proof see e.g. [bil68]. theorem 2.4 is often referred to as the continuous
mapping theorem (slutsky only proved the result for a   ne functions). it
means that for functions of random variables it is possible to pull the limiting
procedure into the function. such a device is useful when trying to prove
asymptotic normality and in order to obtain characterizations of the limiting
distribution.
theorem 2.5 (delta method) assume that xn     rd is asymptotically
n     0. moreover, assume that
normal with a   2
g : rd     rl is a mapping which is continuously di   erentiable at b. in this
case the random variable g(xn) converges

n (xn     b)     n(0,   ) for a2

n (g(xn)     g(b))     n(0, [   xg(b)]  [   xg(b)](cid:62)).
a   2

proof via a taylor expansion we see that

n [g(xn)     g(b)] = [   xg(  n)](cid:62)a   2
a   2

n (xn     b)

(2.5)

(2.6)

42

2 density estimation

n [g(xn)     g(b)]     [   xg(b)](cid:62)a   2

here   n lies on the line segment [b, xn]. since xn     b we have that   n     b,
too. since g is continuously di   erentiable at b we may apply slutsky   s the-
n (xn     b). as a con-
orem to see that a   2
sequence, the transformed random variable is asymptotically normal with
covariance [   xg(b)]  [   xg(b)](cid:62).
we will use the delta method when it comes to investigating properties of
maximum likelihood estimators in exponential families. there g will play the
role of a mapping between expectations and the natural parametrization of
a distribution.

2.1.2 the characteristic function

the fourier transform plays a crucial role in many areas of mathematical
analysis and engineering. this is equally true in statistics. for historic rea-
sons its applications to distributions is called the characteristic function,
which we will discuss in this section. at its foundations lie standard tools
from functional analysis and signal processing [rud73, pap62]. we begin by
recalling the basic properties:

de   nition 2.6 (fourier transform) denote by f : rn     c a function
de   ned on a d-dimensional euclidean space. moreover, let x,        rn. then
the fourier transform f and its inverse f    1 are given by

f [f ](  ) := (2  )    d

2

f    1[g](x) := (2  )    d

2

f (x) exp(   i(cid:104)  , x(cid:105))dx

g(  ) exp(i(cid:104)  , x(cid:105))d  .

(2.7)

(2.8)

(cid:90)
(cid:90)

rn

rn

the key insight is that f    1     f = f     f    1 = id. in other words, f and
f    1 are inverses to each other for all functions which are l2 integrable on
rd, which includes id203 distributions. one of the key advantages of
fourier transforms is that derivatives and convolutions on f translate into
multiplications. that is f [f     g] = (2  )
2 f [f ]    f [g]. the same rule applies
to the inverse transform, i.e. f    1[f     g] = (2  )

2 f    1[f ]f    1[g].

d

d

the bene   t for statistical analysis is that often problems are more easily
expressed in the fourier domain and it is easier to prove convergence results
there. these results then carry over to the original domain. we will be
exploiting this fact in the proof of the law of large numbers and the central
limit theorem. note that the de   nition of fourier transforms can be extended
to more general domains such as groups. see e.g. [bcr84] for further details.

2.1 limit theorems

43

we next introduce the notion of a characteristic function of a distribution.1

de   nition 2.7 (characteristic function) denote by p(x) a distribution
of a random variable x     rd. then the characteristic function   x (  ) with
       rd is given by

  x (  ) := (2  )

d

2 f    1[p(x)] =

exp(i(cid:104)  , x(cid:105))dp(x).

(2.9)

(cid:90)

in other words,   x (  ) is the inverse fourier transform applied to the prob-
ability measure p(x). consequently   x (  ) uniquely characterizes p(x) and
moreover, p(x) can be recovered from   x (  ) via the forward fourier trans-
form. one of the key utilities of characteristic functions is that they allow
us to deal in easy ways with sums of random variables.

theorem 2.8 (sums of random variables and convolutions) denote
by x, y     r two independent random variables. moreover, denote by z :=
x + y the sum of both random variables. then the distribution over z sat-
is   es p(z) = p(x)     p(y). moreover, the characteristic function yields:

  z(  ) =   x (  )  y (  ).

(2.10)

proof z is given by z = x + y . hence, for a given z = z we have
the freedom to choose x = x freely provided that y = z     x. in terms of
distributions this means that the joint distribution p(z, x) is given by

p(z, x) = p(y = z     x)p(x)

(cid:90)

and hence p(z) =

p(y = z     x)dp(x) = [p(x)     p(y)](z).

the result for characteristic functions follows form the property of the
fourier transform.

for sums of several random variables the characteristic function is the prod-
uct of the individual characteristic functions. this allows us to prove both
the weak law of large numbers and the central limit theorem (see figure 2.4
for an illustration) by proving convergence in the fourier domain.
proof [weak law of large numbers] at the heart of our analysis lies
a taylor expansion of the exponential into

exp(iwx) = 1 + i(cid:104)w, x(cid:105) + o(|w|)

and hence   x (  ) = 1 + iwex [x] + o(|w|).

1 in chapter ?? we will discuss more general descriptions of distributions of which   x is a special
case. in particular, we will replace the exponential exp(i (cid:104)  , x(cid:105)) by a id81 k(x, x(cid:48)).

44

2 density estimation

fig. 2.4. a working example of the central limit theorem. the top row contains
distributions of sums of uniformly distributed random variables on the interval
[0.5, 0.5]. from left to right we have sums of 1, 2, 4, 8 and 16 random variables. the
m, where
bottom row contains the same distribution with the means rescaled by
m is the number of observations. note how the distribution converges increasingly
to the normal distribution.

   

(cid:80)m

(cid:18)

given m random variables xi with mean ex [x] =    this means that their
average   xm := 1
m

i=1 xi has the characteristic function

w   + o(m   1 |w|)

i
m

1 +

     xm(  ) =

(2.11)
in the limit of m         this converges to exp(iw  ), the characteristic func-
tion of the constant distribution with mean   . this proves the claim that in
the large sample limit   xm is essentially constant with mean   .

(cid:19)m

proof [central limit theorem] we use the same idea as above to prove
the clt. the main di   erence, though, is that we need to assume that the
second moments of the random variables xi exist. to avoid clutter we only
prove the case of constant mean exi[xi] =    and variance varxi[xi] =   2.

-5050.00.51.0-5050.00.51.0-5050.00.51.0-5050.00.51.0-5050.00.51.0-1010.00.51.01.5-1010.00.51.01.5-1010.00.51.01.5-1010.00.51.01.5-1010.00.51.01.52.1 limit theorems
let zm := 1   

(cid:80)m
i=1(xi       ). our proof relies on showing convergence
of the characteristic function of zm, i.e.   zm to that of a normally dis-
tributed random variable w with zero mean and unit variance. expanding
the exponential to second order yields:

m  2

45

exp(iwx) = 1 + iwx     1
2

w2x2 + o(|w|2)

and hence   x (  ) = 1 + iwex [x]     1
2

w2varx [x] + o(|w|2)

since the mean of zm vanishes by centering (xi       ) and the variance per
variable is m   1 we may write the characteristic function of zm via

(cid:18)

(cid:19)m

  zm(  ) =

1     1
2m

w2 + o(m   1 |w|2)

as before, taking limits m         yields the exponential function. we have
that limm         zm(  ) = exp(    1
2   2) which is the characteristic function of
the normal distribution with zero mean and variance 1. since the character-
istic function transform is injective this proves our claim.

note that the characteristic function has a number of useful properties. for
instance, it can also be used as moment generating function via the identity:

   n
    x (0) = i   nex [xn].

(2.12)

its proof is left as an exercise. see problem 2.2 for details. this connection
also implies (subject to regularity conditions) that if we know the moments
of a distribution we are able to reconstruct it directly since it allows us
to reconstruct its characteristic function. this idea has been exploited in
density estimation [cra46] in the form of edgeworth and gram-charlier
expansions [hal92].

2.1.3 tail bounds

in practice we never have access to an in   nite number of observations. hence
the central limit theorem does not apply but is just an approximation to the
real situation. for instance, in the case of the dice, we might want to state
worst case bounds for    nite sums of random variables to determine by how
much the empirical mean may deviate from its expectation. those bounds
will not only be useful for simple averages but to quantify the behavior of
more sophisticated estimators based on a set of observations.

the bounds we discuss below di   er in the amount of knowledge they
assume about the random variables in question. for instance, we might only

46

2 density estimation

know their mean. this leads to the gauss-markov inequality. if we know
their mean and their variance we are able to state a stronger bound, the
chebyshev inequality. for an even stronger setting, when we know that
each variable has bounded range, we will be able to state a cherno    bound.
those bounds are progressively more tight and also more di   cult to prove.
we state them in order of technical sophistication.
theorem 2.9 (gauss-markov) denote by x     0 a random variable and
let    be its mean. then for any   > 0 we have
pr(x      )       
 

(2.13)

.

proof we use the fact that for nonnegative random variables

(cid:90)    

(cid:90)    

pr(x      ) =

dp(x)    

 

 

dp(x)         1

x
 

xdp(x) =

  
 

.

(cid:90)    

0

this means that for random variables with a small mean, the proportion of
samples with large value has to be small.
consequently deviations from the mean are o(    1). however, note that this
bound does not depend on the number of observations. a useful application
of the gauss-markov inequality is chebyshev   s inequality. it is a statement
on the range of random variables using its variance.

theorem 2.10 (chebyshev) denote by x a random variable with mean
   and variance   2. then the following holds for   > 0:

pr(|x       |      )       2
 2 .

(2.14)
proof denote by y := |x       |2 the random variable quantifying the
deviation of x from its mean   . by construction we know that ey [y] =   2.
next let    :=  2. applying theorem 2.9 to y and    yields pr(y >   )       2/  
which proves the claim.

note the improvement to the gauss-markov inequality. where before we had
bounds whose con   dence improved with o(    1) we can now state o(    2)
bounds for deviations from the mean.

example 2.2 (chebyshev bound) assume that   xm := m   1(cid:80)m

i=1 xi is
the average over m random variables with mean    and variance   2. hence
  xm also has mean   . its variance is given by

var   xm[  xm] =

m   2varxi[xi] = m   1  2.

m(cid:88)

i=1

2.1 limit theorems

47

applying chebyshev   s inequality yields that the id203 of a deviation
of   from the mean    is bounded by   2
m 2 . for    xed failure id203    =
pr(|   xm       | >  ) we have

   
         2m   1    2 and equivalently         /

m  .

this bound is quite reasonable for large    but it means that for high levels
of con   dence we need a huge number of observations.

much stronger results can be obtained if we are able to bound the range
of the random variables. using the latter, we reap an exponential improve-
ment in the quality of the bounds in the form of the mcdiarmid [mcd89]
inequality. we state the latter without proof:
theorem 2.11 (mcdiarmid) denote by f : xm     r a function on x
and let xi be independent random variables. in this case the following holds:

pr (|f (x1, . . . , xm)     ex1,...,xm[f (x1, . . . , xm)]| >  )     2 exp(cid:0)   2 2c   2(cid:1) .
here the constant c2 is given by c2 =(cid:80)m
(cid:12)(cid:12)f (x1, . . . , xi, . . . , xm)     f (x1, . . . , x(cid:48)

i, . . . , xm)(cid:12)(cid:12)     ci

i where

i=1 c2

for all x1, . . . , xm, x(cid:48)

i and for all i.

this bound can be used for averages of a number of observations when
they are computed according to some algorithm as long as the latter can be
encoded in f . in particular, we have the following bound [hoe63]:

range xi     [a, b] and mean   . let   xm := m   1(cid:80)m

theorem 2.12 (hoe   ding) denote by xi iid random variables with bounded

i=1 xi be their average.

then the following bound holds:

pr(cid:0)(cid:12)(cid:12)   xm       (cid:12)(cid:12) >  (cid:1)     2 exp

(cid:18)

(cid:19)

    2m 2
(b     a)2

.

(2.15)

proof this is a corollary of theorem 2.11. in   xm each individual random
variable has range [a/m, b/m] and we set f (x1, . . . , xm) :=   xm. straight-
forward algebra shows that c2 = m   2(b     a)2. plugging this back into
mcdiarmid   s theorem proves the claim.

note that (2.15) is exponentially better than the previous bounds. with
increasing sample size the con   dence level also increases exponentially.

example 2.3 (hoe   ding bound) as in example 2.2 assume that xi are
iid random variables and let   xm be their average. moreover, assume that

48

2 density estimation

xi     [a, b] for all i. as before we want to obtain guarantees on the id203
that |   xm       | >  . for a given level of con   dence 1        we need to solve

(cid:16)    2m 2

(b   a)2

(cid:17)

       2 exp

      |b     a|(cid:112)[log 2     log   ] /2m

(2.16)

(2.17)

for  . straightforward algebra shows that in this case   needs to satisfy

in other words, while the con   dence level only enters logarithmically into the
inequality, the sample size m improves our con   dence only with   = o(m    1
2 ).
that is, in order to improve our con   dence interval from   = 0.1 to   = 0.01
we need 100 times as many observations.

while this bound is tight (see problem 2.5 for details), it is possible to ob-
tain better bounds if we know additional information. in particular knowing
a bound on the variance of a random variable in addition to knowing that it
has bounded range would allow us to strengthen the statement considerably.
the bernstein inequality captures this connection. for details see [bbl05]
or works on empirical process theory [vdvw96, sw86, vap82].

2.1.4 an example

it is probably easiest to illustrate the various bounds using a concrete exam-
ple. in a semiconductor fab processors are produced on a wafer. a typical
300mm wafer holds about 400 chips. a large number of processing steps
are required to produce a    nished microprocessor and often it is impossible
to assess the e   ect of a design decision until the    nished product has been
produced.

assume that the production manager wants to change some step from
process    a    to some other process    b   . the goal is to increase the yield of
the process, that is, the number of chips of the 400 potential chips on the
wafer which can be sold. unfortunately this number is a random variable,
i.e. the number of working chips per wafer can vary widely between di   erent
wafers. since process    a    has been running in the factory for a very long
time we may assume that the yield is well known, say it is   a = 350 out
of 400 processors on average. it is our goal to determine whether process
   b    is better and what its yield may be. obviously, since production runs
are expensive we want to be able to determine this number as quickly as
possible, i.e. using as few wafers as possible. the production manager is risk
averse and wants to ensure that the new process is really better. hence he
requires a con   dence level of 95% before he will change the production.

2.1 limit theorems

49

a    rst step is to formalize the problem. since we know process    a    exactly
we only need to concern ourselves with    b   . we associate the random variable
xi with wafer i. a reasonable (and somewhat simplifying) assumption is to
posit that all xi are independent and identically distributed where all xi
have the mean   b. obviously we do not know   b     otherwise there would
be no reason for testing! we denote by   xm the average of the yields of m
wafers using process    b   . what we are interested in is the accuracy   for
which the id203

   = pr(|   xm       b| >  ) satis   es        0.05.

let us now discuss how the various bounds behave. for the sake of the
argument assume that   b       a = 20, i.e. the new process produces on
average 20 additional usable chips.

chebyshev in order to apply the chebyshev inequality we need to bound
the variance of the random variables xi. the worst possible variance would
occur if xi     {0; 400} where both events occur with equal id203. in
other words, with equal id203 the wafer if fully usable or it is entirely
broken. this amounts to   2 = 0.5(200     0)2 + 0.5(200     400)2 = 40, 000.
since for chebyshev bounds we have

         2m   1    2

(2.18)
we can solve for m =   2/   2 = 40, 000/(0.05   400) = 20, 000. in other words,
we would typically need 20,000 wafers to assess with reasonable con   dence
whether process    b    is better than process    a   . this is completely unrealistic.
slightly better bounds can be obtained if we are able to make better
assumptions on the variance. for instance, if we can be sure that the yield
of process    b    is at least 300, then the largest possible variance is 0.25(300   
0)2 + 0.75(300     400)2 = 30, 000, leading to a minimum of 15,000 wafers
which is not much better.
hoe   ding since the yields are in the interval {0, . . . , 400} we have an ex-
plicit bound on the range of observations. recall the inequality (2.16) which
bounds the failure probably    = 0.05 by an exponential term. solving this
for m yields

m     0.5|b     a|2    2 log(2/  )     737.8

(2.19)

in other words, we need at lest 738 wafers to determine whether process    b   
is better. while this is a signi   cant improvement of almost two orders of
magnitude, it still seems wasteful and we would like to do better.

50

2 density estimation

central limit theorem the central limit theorem is an approximation.
this means that our reasoning is not accurate any more. that said, for
large enough sample sizes, the approximation is good enough to use it for
practical predictions. assume for the moment that we knew the variance   2
exactly. in this case we know that   xm is approximately normal with mean
  b and variance m   1  2. we are interested in the interval [       ,    +  ] which
contains 95% of the id203 mass of a normal distribution. that is, we
need to solve the integral

(cid:90)   + 

1

2    2

      

exp

(cid:18)
    (x       )2

(cid:19)

2  2

dx = 0.95

(2.20)

this can be solved e   ciently using the cumulative distribution function of
a normal distribution (see problem 2.3 for more details). one can check
that (2.20) is solved for   = 2.96  . in other words, an interval of   2.96  
contains 95% of the id203 mass of a normal distribution. the number
of observations is therefore determined by

   
  = 2.96  /

m and hence m = 8.76

  2
 2

(2.21)

again, our problem is that we do not know the variance of the distribution.
using the worst-case bound on the variance, i.e.   2 = 40, 000 would lead to
a requirement of at least m = 876 wafers for testing. however, while we do
not know the variance, we may estimate it along with the mean and use the
empirical estimate, possibly plus some small constant to ensure we do not
underestimate the variance, instead of the upper bound.

assuming that    uctuations turn out to be in the order of 50 processors,
i.e.   2 = 2500, we are able to reduce our requirement to approximately 55
wafers. this is probably an acceptable number for a practical test.

rates and constants the astute reader will have noticed that all three
con   dence bounds had scaling behavior m = o(    2). that is, in all cases
the number of observations was a fairly ill behaved function of the amount
of con   dence required. if we were just interested in convergence per se, a
statement like that of the chebyshev inequality would have been entirely
su   cient. the various laws and bounds can often be used to obtain con-
siderably better constants for statistical con   dence guarantees. for more
complex estimators, such as methods to classify, rank, or annotate data,
a reasoning such as the one above can become highly nontrivial. see e.g.
[mya94, vap98] for further details.

2.2 parzen windows

2.2 parzen windows

2.2.1 discrete density estimation

51

the convergence theorems discussed so far mean that we can use empir-
ical observations for the purpose of density estimation. recall the case of
the naive bayes classi   er of section 1.3.1. one of the key ingredients was
the ability to use information about word counts for di   erent document
classes to estimate the id203 p(wj|y), where wj denoted the number
of occurrences of word j in document x, given that it was labeled y. in the
following we discuss an extremely simple and crude method for estimating
probabilities. it relies on the fact that for random variables xi drawn from
distribution p(x) with discrete values xi     x we have

lim
m         px (x) = p(x)
where   px (x) := m   1

m(cid:88)

i=1

{xi = x} for all x     x.

(2.22)

(2.23)

let us discuss a concrete case. we assume that we have 12 documents and
would like to estimate the id203 of occurrence of the word    dog    from
it. as raw data we have:

document id

occurrences of    dog   

1

1

2

0

3

2

4

0

5

4

6

6

7

3

8

0

9

6

10

11

12

2

0

1

this means that the word    dog    occurs the following number of times:

occurrences of    dog   

number of documents

0

4

1

2

2

2

3

1

4

1

5

0

6

2

something unusual is happening here: for some reason we never observed
5 instances of the word dog in our documents, only 4 and less, or alter-
natively 6 times. so what about 5 times? it is reasonable to assume that
the corresponding value should not be 0 either. maybe we did not sample
enough. one possible strategy is to add pseudo-counts to the observations.
this amounts to the following estimate:

  px (x) := (m + |x|)   1(cid:104)

m(cid:88)

1 +

{xi = x} = p(x)

(2.24)

(cid:105)

clearly the limit for m         is still p(x). hence, asymptotically we do not
lose anything. this prescription is what we used in algorithm 1.1 used a
method called laplace smoothing. below we contrast the two methods:

i=1

52

2 density estimation

occurrences of    dog   

0

1

2

3

4

5

6

0
0
0.05

2
0.17
0.16

2
0.17
0.16

4
0.33
0.26

1
0.083
0.11

1
0.083
0.11

number of documents
2
frequency of occurrence
0.17
laplace smoothing
0.16
the problem with this method is that as |x| increases we need increasingly
more observations to obtain even a modicum of precision. on average, we
will need at least one observation for every x     x. this can be infeasible for
large domains as the following example shows.
example 2.4 (curse of dimensionality) assume that x = {0, 1}d, i.e.
x consists of binary bit vectors of dimensionality d. as d increases the size of
x increases exponentially, requiring an exponential number of observations
to perform density estimation. for instance, if we work with images, a 100   
100 black and white picture would require in the order of 103010 observations
to model such fairly low-resolution images accurately. this is clearly utterly
infeasible     the number of particles in the known universe is in the order
of 1080. bellman [bel61] was one of the    rst to formalize this dilemma by
coining the term    curse of dimensionality   .

this example clearly shows that we need better tools to deal with high-
dimensional data. we will present one of such tools in the next section.

2.2.2 smoothing kernel

we now proceed to proper density estimation. assume that we want to
estimate the distribution of weights of a population. sample data from a
population might look as follows: x = {57, 88, 54, 84, 83, 59, 56, 43, 70, 63,
90, 98, 102, 97, 106, 99, 103, 112}. we could use this to perform a density
estimate by placing discrete components at the locations xi     x with weight
1/|x| as what is done in figure 2.5. there is no reason to believe that weights
are quantized in kilograms, or grams, or miligrams (or pounds and stones).
and even if it were, we would expect that similar weights would have similar
densities associated with it. indeed, as the right diagram of figure 2.5 shows,
the corresponding density is continuous.

the key question arising is how we may transform x into a realistic
estimate of the density p(x). starting with a    density estimate    with only
discrete terms

  p(x) =

1
m

  (x     xi)

(2.25)

m(cid:88)

i=1

2.2 parzen windows

53

we may choose to smooth it out by a smoothing kernel h(x) such that the
id203 mass becomes somewhat more spread out. for a density estimate
on x     rd this is achieved by

r   dh(cid:0) x   xi

(cid:1) .

r

m(cid:88)

i=1

  p(x) =

1
m

(2.26)

this expansion is commonly known as the parzen windows estimate. note
that obviously h must be chosen such that h(x)     0 for all x     x and

moreover that(cid:82) h(x)dx = 1 in order to ensure that (2.26) is a proper prob-

ability distribution. we now formally justify this smoothing. let r be a
small region such that

(cid:90)

r

q =

p(x) dx.

out of the m samples drawn from p(x), the id203 that k of them fall
in region r is given by the binomial distribution

(cid:18)m
(cid:19)

k

qk(1     q)m   k.

the expected fraction of points falling inside the region can easily be com-
puted from the expected value of the binomial distribution: e[k/m] = q.
similarly, the variance can be computed as var[k/m] = q(1     q)/m. as
m         the variance goes to 0 and hence the estimate peaks around the
expectation. we can therefore set

k     mq.

if we assume that r is so small that p(x) is constant over r, then

q     p(x)    v,

where v is the volume of r. rearranging we obtain

let us now set r to be a cube with side length r, and de   ne a function

.

(2.27)

observe that h(cid:0) x   xi

r

h(u) =

(cid:1) is 1 if and only if xi lies inside a cube of size r centered

p(x)     k
mv

(cid:40)

1 if |ui|     1
2
0 otherwise.

54

around x. if we let

k =

then one can use (2.27) to estimate p via
r   dh

  p(x) =

h

m(cid:88)
m(cid:88)

i=1

(cid:19)
(cid:18) x     xi
(cid:18) x     xi

r

,

r

1
m

i=1

2 density estimation

(cid:19)

,

where rd is the volume of the hypercube of size r in d dimensions. by symme-
try, we can interpret this equation as the sum over m cubes centered around
m data points xn. if we replace the cube by any smooth id81 h(  )
this recovers (2.26).

there exists a large variety of di   erent kernels which can be used for the
kernel density estimate. [sil86] has a detailed description of the properties
of a number of kernels. popular choices are

2 x2

    1
2 e    1
2 e   |x|
4 max(0, 1     x2)
2   [   1,1](x)

h(x) = (2  )
h(x) = 1
h(x) = 3
h(x) = 1
h(x) = max(0, 1     |x|)

gaussian kernel

laplace kernel

epanechnikov kernel

uniform kernel

triangle kernel.

(2.28)

(2.29)

(2.30)

(2.31)

(2.32)

further kernels are the triweight and the quartic kernel which are basically
powers of the epanechnikov kernel. for practical purposes the gaussian ker-
nel (2.28) or the epanechnikov kernel (2.30) are most suitable. in particular,
the latter has the attractive property of compact support. this means that
for any given density estimate at location x we will only need to evaluate
terms h(xi     x) for which the distance (cid:107)xi     x(cid:107) is less than r. such expan-
sions are computationally much cheaper, in particular when we make use of
fast nearest neighbor search algorithms [gim99, im98]. figure 2.7 has some
examples of kernels.

2.2.3 parameter estimation

so far we have not discussed the issue of parameter selection. it should be
evident from figure 2.6, though, that it is quite crucial to choose a good
kernel width. clearly, a kernel that is overly wide will oversmooth any    ne
detail that there might be in the density. on the other hand, a very narrow
kernel will not be very useful, since it will be able to make statements only
about the locations where we actually observed data.

2.2 parzen windows

55

fig. 2.5. left: a naive density estimate given a sample of the weight of 18 persons.
right: the underlying weight distribution.

fig. 2.6. parzen windows density estimate associated with the 18 observations of
the figure above. from left to right: gaussian kernel density estimate with kernel
of width 0.3, 1, 3, and 10 respectively.

fig. 2.7. some kernels for parzen windows density estimation. from left to right:
gaussian kernel, laplace kernel, epanechikov kernel, and uniform density.

moreover, there is the issue of choosing a suitable id81. the
fact that a large variety of them exists might suggest that this is a crucial
issue. in practice, this turns out not to be the case and instead, the choice
of a suitable kernel width is much more vital for good estimates. in other
words, size matters, shape is secondary.

the problem is that we do not know which kernel width is best for the
data. if the problem is one-dimensional, we might hope to be able to eyeball
the size of r. obviously, in higher dimensions this approach fails. a second

4050607080901001100.000.050.104050607080901001100.000.010.020.030.040.054060801000.0000.0250.0504060801000.0000.0250.0504060801000.0000.0250.0504060801000.0000.0250.050-2-10120.00.51.0-2-10120.00.51.0-2-10120.00.51.0-2-10120.00.51.0m(cid:89)

m(cid:88)

m(cid:88)

(cid:16) xi   xj

(cid:17)

r

56

2 density estimation

option would be to choose r such that the log-likelihood of the data is
maximized. it is given by

log

p(xi) =    m log m +

log

r   dh

(2.33)

i=1

i=1

j=1

remark 2.13 (log-likelihood) we consider the logarithm of the likeli-
hood for reasons of computational stability to prevent numerical under   ow.
while each term p(xi) might be within a suitable range, say 10   2, the prod-
uct of 1000 of such terms will easily exceed the exponent of    oating point
representations on a computer. summing over the logarithm, on the other
hand, is perfectly feasible even for large numbers of observations.

unfortunately computing the log-likelihood is equally infeasible: for decreas-
ing r the only surviving terms in (2.33) are the functions h((xi     xi)/r) =
h(0), since the arguments of all other id81s diverge. in other
words, the log-likelihood is maximized when p(x) is peaked exactly at the
locations where we observed the data. the graph on the left of figure 2.6
shows what happens in such a situation.

what we just experienced is a case of over   tting where our model is too
   exible. this led to a situation where our model was able to explain the
observed data    unreasonably well   , simply because we were able to adjust
our parameters given the data. we will encounter this situation throughout
the book. there exist a number of ways to address this problem.

validation set: we could use a subset of our set of observations as an
estimate of the log-likelihood. that is, we could partition the obser-
vations into x := {x1, . . . , xn} and x(cid:48) := {xn+1, . . . , xm} and use
the second part for a likelihood score according to (2.33). the second
set is typically called a validation set.

n-fold cross-validation: taking this idea further, note that there is no
particular reason why any given xi should belong to x or x(cid:48) respec-
tively. in fact, we could use all splits of the observations into sets
x and x(cid:48) to infer the quality of our estimate. while this is compu-
tationally infeasible, we could decide to split the observations into
n equally sized subsets, say x1, . . . , xn and use each of them as a
validation set at a time while the remainder is used to generate a
density estimate.

typically n is chosen to be 10, in which case this procedure is

2.2 parzen windows

57

referred to as 10-fold cross-validation. it is a computationally at-
tractive procedure insofar as it does not require us to change the
basic estimation algorithm. nonetheless, computation can be costly.
leave-one-out estimator: at the extreme end of cross-validation we could
choose n = m. that is, we only remove a single observation at a time
and use the remainder of the data for the estimate. using the average
over the likelihood scores provides us with an even more    ne-grained
estimate. denote by pi(x) the density estimate obtained by using
x := {x1, . . . , xm} without xi. for a parzen windows estimate this
is given by

pi(xi) = (m     1)   1(cid:88)

r   dh

j(cid:54)=i

(cid:16) xi   xj

(cid:17)

r

(cid:104)

= m
m   1

p(xi)     r   dh(0)

.

(cid:105)

(2.34)
note that this is precisely the term r   dh(0) that is removed from
the estimate. it is this term which led to divergent estimates for
r     0. this means that the leave-one-out log-likelihood estimate
can be computed easily via

m(cid:88)

(cid:104)

(cid:105)

l(x) = m log m

m   1 +

log

p(xi)     r   dh(0)

.

(2.35)

i=1

we then choose r such that l(x) is maximized. this strategy is very
robust and whenever it can be implemented in a computationally
e   cient manner, it is very reliable in performing model selection.

an alternative, probably more of theoretical interest, is to choose the scale r
a priori based on the amount of data we have at our disposition. intuitively,
we need a scheme which ensures that r     0 as the number of observations
increases m        . however, we need to ensure that this happens slowly
enough that the number of observations within range r keeps on increasing in
order to ensure good statistical performance. for details we refer the reader
to [sil86]. chapter ?? discusses issues of model selection for estimators in
general in considerably more detail.

2.2.4 silverman   s rule

assume you are an aspiring demographer who wishes to estimate the popu-
lation density of a country, say australia. you might have access to a limited
census which, for a random portion of the population determines where they
live. as a consequence you will obtain a relatively high number of samples

58

2 density estimation

fig. 2.8. nonuniform density. left: original density with samples drawn from the
distribution. middle: density estimate with a uniform kernel. right: density estimate
using silverman   s adjustment.

of city dwellers, whereas the number of people living in the countryside is
likely to be very small.

if we attempt to perform density estimation using parzen windows, we
will encounter an interesting dilemma: in regions of high density (i.e. the
cities) we will want to choose a narrow kernel width to allow us to model
the variations in population density accurately. conversely, in the outback,
a very wide kernel is preferable, since the population there is very low.
unfortunately, this information is exactly what a density estimator itself
could tell us. in other words we have a chicken and egg situation where
having a good density estimate seems to be necessary to come up with a
good density estimate.

fortunately this situation can be addressed by realizing that we do not
actually need to know the density but rather a rough estimate of the latter.
this can be obtained by using information about the average distance of the
k nearest neighbors of a point. one of silverman   s rules of thumb [sil86] is
to choose ri as

ri =

c
k

x   kn n (xi)

(cid:107)x     xi(cid:107) .

(2.36)

typically c is chosen to be 0.5 and k is small, e.g. k = 9 to ensure that the
estimate is computationally e   cient. the density estimate is then given by

(cid:16) x   xi

(cid:17)

ri

p(x) =

1
m

r   d
i h

.

(2.37)

(cid:88)

m(cid:88)

i=1

figure 2.8 shows an example of such a density estimate. it is clear that a
locality dependent kernel width is better than choosing a uniformly constant
kernel density estimate. however, note that this increases the computational
complexity of performing a density estimate, since    rst the k nearest neigh-
bors need to be found before the density estimate can be carried out.

2.2 parzen windows

59

2.2.5 watson-nadaraya estimator

now that we are able to perform density estimation we may use it to perform
classi   cation and regression. this leads us to an e   ective method for non-
parametric data analysis, the watson-nadaraya estimator [wat64, nad65].
the basic idea is very simple: assume that we have a binary classi   cation
problem, i.e. we need to distinguish between two classes. provided that we
are able to compute density estimates p(x) given a set of observations x we
could appeal to bayes rule to obtain

i:yi=y r   dh(cid:0) xi   x
(cid:80)
i=1 r   dh(cid:0) xi   x
(cid:1)
(cid:80)m
m    1

my
1
m

r

r

my

(cid:1)

.

(2.38)

p(y|x) =

p(x|y)p(y)

p(x)

=

here we only take the sum over all xi with label yi = y in the numerator.
the advantage of this approach is that it is very cheap to design such an
estimator. after all, we only need to compute sums. the downside, similar
to that of the k-nearest neighbor classi   er is that it may require sums (or
search) over a large number of observations. that is, evaluation of (2.38) is
potentially an o(m) operation. fast tree based representations can be used
to accelerate this [bkl06, km00], however their behavior depends signi   -
cantly on the dimensionality of the data. we will encounter computationally
more attractive methods at a later stage.
for binary classi   cation (2.38) can be simpli   ed considerably. assume
that y     {  1}. for p(y = 1|x) > 0.5 we will choose that we should estimate
y = 1 and in the converse case we would estimate y =    1. taking the
di   erence between twice the numerator and the denominator we can see
that the function

i yih(cid:0) xi   x
(cid:1)
(cid:80)
(cid:1) =
i h(cid:0) xi   x
(cid:80)

r

r

(cid:88)

i

h(cid:0) xi   x
(cid:1)
i h(cid:0) xi   x
(cid:80)

r

r

yi

(cid:1) =:

(cid:88)

i

yiwi(x)

(2.39)

f (x) =

can be used to achieve the same goal since f (x) > 0        p(y = 1|x) > 0.5.
note that f (x) is a weighted combination of the labels yi associated with
weights wi(x) which depend on the proximity of x to an observation xi.
in other words, (2.39) is a smoothed-out version of the k-nearest neighbor
classi   er of section 1.3.2. instead of drawing a hard boundary at the k closest
observation we use a soft weighting scheme with weights wi(x) depending
on which observations are closest.

note furthermore that the numerator of (2.39) is very similar to the simple
classi   er of section 1.3.3. in fact, for kernels k(x, x(cid:48)) such as the gaussian
rbf kernel, which are also kernels in the sense of a parzen windows den-
sity estimate, i.e. k(x, x(cid:48)) = r   dh
the two terms are identical. this

(cid:16) x   x(cid:48)

(cid:17)

r

60

2 density estimation

fig. 2.9. watson nadaraya estimate. left: a binary classi   er. the optimal solution
would be a straight line since both classes were drawn from a normal distribution
with the same variance. right: a regression estimator. the data was generated from
a sinusoid with additive noise. the regression tracks the sinusoid reasonably well.

means that the watson nadaraya estimator provides us with an alternative
explanation as to why (1.24) leads to a usable classi   er.

yi     r to obtain the regression estimator(cid:80)

in the same fashion as the watson nadaraya classi   er extends the k-
nearest neighbor classi   er we also may construct a watson nadaraya re-
gression estimator by replacing the binary labels yi by real-valued values
i yiwi(x). figure 2.9 has an ex-
ample of the workings of both a regression estimator and a classi   er. they
are easy to use and they work well for moderately dimensional data.

2.3 exponential families

distributions from the exponential family are some of the most versatile
tools for statistical id136. gaussians, poisson, gamma and wishart dis-
tributions all form part of the exponential family. they play a key role in
dealing with id114, classi   cation, regression and conditional ran-
dom    elds which we will encounter in later parts of this book. some of the
reasons for their popularity are that they lead to id76 prob-
lems and that they allow us to describe id203 distributions by linear
models.

2.3.1 basics

densities from the exponential family are de   ned by

p(x;   ) := p0(x) exp ((cid:104)  (x),   (cid:105)     g(  )) .

(2.40)

2.3 exponential families

61

(cid:90)

here p0(x) is a density on x and is often called the base measure,   (x) is
a map from x to the su   cient statistics   (x).    is commonly referred to as
the natural parameter. it lives in the space dual to   (x). moreover, g(  ) is a
id172 constant which ensures that p(x) is properly normalized. g is
often referred to as the log-partition function. the name stems from physics
where z = eg(  ) denotes the number of states of a physical ensemble. g can
be computed as follows:

g(  ) = log

exp ((cid:104)  (x),   (cid:105)) dx.

(2.41)

x

example 2.5 (binary model) assume that x = {0; 1} and that   (x) =

x. in this case we have g(  ) = log(cid:2)e0 + e  (cid:3) = log(cid:2)1 + e  (cid:3). it follows that

p(x = 0;   ) = 1
di   erent values of    one can recover di   erent bernoulli distributions.

1+e   and p(x = 1;   ) = e  

1+e   . in other words, by choosing

one of the convenient properties of exponential families is that the log-
partition function g can be used to generate moments of the distribution
itself simply by taking derivatives.

theorem 2.14 (log partition function) the function g(  ) is convex.
moreover, the distribution p(x;   ) satis   es
     g(  ) = ex [  (x)] and    2

  g(  ) = varx [  (x)] .

(2.42)

  g(  ) = varx [  (x)] implies that g is convex, since the

proof note that    2
covariance matrix is positive semide   nite. to show (2.42) we expand
     g(  ) =

(cid:82)
(cid:82)
x   (x) exp(cid:104)  (x),   (cid:105) dx

  (x)p(x;   )dx = ex [  (x)] .

(cid:90)

=

next we take the second derivative to obtain
(cid:62)
  (x) [  (x)          g(  )]

x exp(cid:104)  (x),   (cid:105)
(cid:104)
  (x)  (x)(cid:62)(cid:105)     ex [  (x)] ex [  (x)]

   2
  g(  ) =

p(x;   )dx

= ex

(cid:90)

x

(cid:62)

(2.43)

(2.44)

(2.45)

which proves the claim. for the    rst equality we used (2.43). for the second
line we used the de   nition of the variance.
one may show that higher derivatives    n
   g(  ) generate higher order cu-
mulants of   (x) under p(x;   ). this is why g is often also referred as the
cumulant-generating function. note that in general, computation of g(  )

62

2 density estimation

is nontrivial since it involves solving a highdimensional integral. for many
cases, in fact, the computation is np hard, for instance when x is the do-
main of permutations [fj95]. throughout the book we will discuss a number
of approximation techniques which can be applied in such a case.
let us brie   y illustrate (2.43) using the binary model of example 2.5.
we have that       = e  
(1+e  )2 . this is exactly what we would
have obtained from direct computation of the mean p(x = 1;   ) and variance
p(x = 1;   )     p(x = 1;   )2 subject to the distribution p(x;   ).

1+e   and    2

   = e  

2.3.2 examples

a large number of densities are members of the exponential family. note,
however, that in statistics it is not common to express them in the dot
product formulation for historic reasons and for reasons of notational com-
pactness. we discuss a number of common densities below and show why
they can be written in terms of an exponential family. a detailed description
of the most commonly occurring types are given in a table.
gaussian let x,        rd and let        rd  d where    (cid:31) 0, that is,    is a
positive de   nite matrix. in this case the normal distribution can be
expressed via

(cid:18)
x(cid:62)(cid:2)     1  (cid:3) + tr

2|  |    1

2 exp

    1
2

(cid:18)

p(x) = (2  )    d

= exp

(cid:19)

(cid:18)(cid:20)

(x       )(cid:62)     1(x       )

xx(cid:62)(cid:21)(cid:2)     1(cid:3)(cid:19)

    1
2

    c(  ,   )

(2.46)

(cid:19)

2 log |  |. by combining the
2   (cid:62)     1   + d
2 log 2   + 1
where c(  ,   ) = 1
terms in x into   (x) := (x,    1
2 xx(cid:62)) we obtain the su   cient statistics
of x. the corresponding linear coe   cients (     1  ,      1) constitute the
natural parameter   . all that remains to be done to express p(x) in
terms of (2.40) is to rewrite g(  ) in terms of c(  ,   ). the summary
table on the following page contains details.
multinomial another popular distribution is one over k discrete events.
in this case x = {1, . . . , k} and we have in completely generic terms
x   x = 1. now denote by ex     rk the
x-th unit vector of the canonical basis, that is (cid:104)ex, ex(cid:48)(cid:105) = 1 if x = x(cid:48)
and 0 otherwise. in this case we may rewrite p(x) via

p(x) =   x where   x     0 and(cid:80)

p(x) =   x = exp ((cid:104)ex, log   (cid:105))

(2.47)

where log    = (log   1, . . . , log   k). in other words, we have succeeded

2.3 exponential families

63

in rewriting the distribution as a member of the exponential family
where   (x) = ex and where    = log   . note that in this de   nition   
is restricted to a k   1 dimensional manifold (the k dimensional prob-
ability simplex). if we relax those constraints we need to ensure that
p(x) remains normalized. details are given in the summary table.

poisson this distribution is often used to model distributions over discrete
events. for instance, the number of raindrops which fall on a given
surface area in a given amount of time, the number of stars in a
given volume of space, or the number of prussian soldiers killed by
horse-kicks in the prussian cavalry all follow this distribution. it is
given by

p(x) =

e       x

x!

=

1
x!

exp (x log          ) where x     n0 .

(2.48)

by de   ning   (x) = x we obtain an exponential families model. note
that things are a bit less trivial here since 1
is the nonuniform
x!
counting measure on n0. the case of the uniform measure which
leads to the exponential distribution is discussed in problem 2.16.

the reason why many discrete processes follow the poisson distri-
bution is that it can be seen as the limit over the average of a large
number of bernoulli draws: denote by z     {0, 1} a random variable
with p(z = 1) =   . moreover, denote by zn the sum over n draws
from this random variable. in this case zn follows the multinomial
we let n         such that the expected value of zn remains constant.
that is, we rescale    =   

distribution with p(zn = k) = (cid:0)n

(cid:1)  k(1       )n   k. now assume that
(cid:18)
(cid:19)n(cid:34)

n . in this case we have

(cid:19)n   k

(cid:19)k(cid:35)

1       
n

(cid:18)

(cid:18)

(2.49)

  k
nk

n!

k

p(zn = k) =

=

(n     k)!k!
  k
1       
n
k!

n!

nk(n     k)!

1       
n

for n         the second term converges to e     . the third term con-
verges to 1, since we have a product of only 2k terms, each of which
converge to 1. using the exponential families notation we may check
that e[x] =    and that moreover also var[x] =   .

beta this is a distribution on the unit interval x = [0, 1] which is very
versatile when it comes to modelling unimodal and bimodal distri-

64

2 density estimation

fig. 2.10. left: poisson distributions with    = {1, 3, 10}. right: beta distributions
with a = 2 and b     {1, 2, 3, 5, 7}. note how with increasing b the distribution
becomes more peaked close to the origin.

butions. it is given by

p(x) = xa   1(1     x)b   1   (a + b)
  (a)  (b)

.

(2.50)

taking logarithms we see that this, too, is an exponential families
distribution, since p(x) = exp((a     1) log x + (b     1) log(1     x) +
log   (a + b)     log   (a)     log   (b)).

figure 2.10 has a graphical description of the poisson distribution and the
beta distribution. for a more comprehensive list of exponential family dis-
tributions see the table below and [fel71, ft94, mn83]. in principle any
map   (x), domain x with underlying measure    are suitable, as long as the
log-partition function g(  ) can be computed e   ciently.

theorem 2.15 (convex feasible domain) the domain of de   nition   
of g(  ) is convex.

proof by construction g is convex and di   erentiable everywhere. hence the
below-sets for all values c with {x|g(x)     c} exist. consequently the domain
of de   nition is convex.

having a convex function is very valuable when it comes to parameter infer-
ence since convex minimization problems have unique minimum values and
global minima. we will discuss this notion in more detail when designing
maximum likelihood estimators.

0510152025300.000.050.100.150.200.250.300.350.400.00.20.40.60.81.00.00.51.01.52.02.53.03.52.3 exponential families

65

)

)
0
,
   
   
(

n
r

r

)
0
,
   
   
(

   
,
0
(
  
r

n
c
  

n
r

2
)

   
,
0
(

2
)

   
,
0
(

n
c
  
r

2
r

)

n
)
+
r
(

   
,
0
(

)
0
,
   
   
(

)
i
  

1
=
ni

(cid:80)

(cid:1)
i

2
  
g
o
l

2

   
1

2
g
o
l

+

1
  

n
1
  
+

)
2
  
(
  
)
1
  
(
  

(cid:0)

  
g
o
l

|
2
  
|
g
o
l

1
  
)
   
2
  
+
)
1
1
  
  
(
(
  
  
(cid:80)
g
o
+
l

1
  
   

1
=
ni

g
o
l

(cid:80)

(
  
g
o
l

)
1
   
  
(
g
o
l

   
)
i
  
(
  
g
o
l

+
2
g
o
l
)
1
1
=
   
ni
  
(

)
)
  
e
   
1
(
g
o
l

   

(
g
o
l

c
i
r
e
n
e
g

1
  
1
   

2
  
(cid:62)1
  
2
12
  

21
  

2
  
g
o
l

12

12

+

+

2
  
g
o
l

|
2
  
|
g
o
l
   

12

12

   

2
  
1
  

   
  
2
g
o
l

   
  
2
g
o
l

2
   
  
g
o
l

12

n2

12

  
g
o
l

i
  
e

(cid:1)
  
e
   
1
=
1
ni

(cid:0)

e
+
1

g
o
l

(cid:80)

g
o
l

g
o
l

   

  
e

(cid:1)
(cid:62)

(cid:1)
(cid:1)
2
x
12

x
x
12

1x
   

   

   

,

,

,

x
   

x

x
e

x

x
(cid:0)

x
(cid:0)

x
(cid:0)

x

)
)
x
   
(cid:1)
1
(
g
o
l

)
x
   

   

,
|

x
12

x

,

,

x
g
o
l
(cid:0)
(

x
g
o
l
(

|
g
o
l

)
n
x
g
o
l

,

.

.

.

,
1
x
g
o
l
(

x
g
o
l

   

)
)
  
(
g
   

,
  
(

x

g
n

i
t
n
u
o
c

g
n

i
t
n
u
o
c

g
n

i
t
n
u
o
c

!

1x

e
u
g
s
e
b
e
l

e
u
g
s
e
b
e
l

e
u
g
s
e
b
e
l

32
   

x

)
x
   
1
1
(
x

1
+
n

2

   

|

(cid:81)
x

1

x
2
   

1x

|

(

e

1x

1
   

)
i

x
1
=
ni

}
n

.
.
1
{

}
1
,
0
{

)

   

+0
n

+0
n

,
0
[

r

n
r

l
a
i
m
o
n

i
t
l

u
m

l
a
i
t
n
e
n
o
p
x
e

i
l
l

u
o
n
r
e
b

n
o
s
s
i
o
p

e
c
a
l

p
a
l

n
a
i
s
s
u
a
g

)

   

,
0
[

l
a
m
r
o
n
e
s
r
e
v
n
i

)

   

]
1
,
0
[

,
0
[

n
c

n
s

+
r

n

a
m
m
a
g

t
r
a
h
s
i

w

a
t
e
b

2
  

e
s
r
e
v
n
i

t
e
l

h
c
i
r
i
d

c
i
m
h
t
i
r
a
g
o
l

e
u
g
s
e
b
e
l

  

e
t
a
g
u
j
n
o
c

.
n
  
n
r
n
i

s
e
c
i
r
t
a
m
e
t
i
n
   
e
d
i
m
e
s

e
v
i
t
i
s
o
p

f
o

e
n
o
c

e
h
t

s
i

n
c

.
s
n
o
i
s
n
e
m
d

i

n

n

i

x
e
l

p
m

i
s

y
t
i
l
i

b
a
b
o
r
p

e
h
t

s
e
t
o
n
e
d

n
s

66

2.4 estimation

2 density estimation

in many statistical problems the challenge is to estimate parameters of in-
terest. for instance, in the context of exponential families, we may want
to estimate a parameter      such that it is close to the    true    parameter      
in the distribution. while the problem is fully general, we will describe the
relevant steps in obtaining estimates for the special case of the exponential
family. this is done for two reasons        rstly, exponential families are an
important special case and we will encounter slightly more complex variants
on the reasoning in later chapters of the book. secondly, they are of a su   -
ciently simple form that we are able to show a range of di   erent techniques.
in more advanced applications only a small subset of those methods may be
practically feasible. hence exponential families provide us with a working
example based on which we can compare the consequences of a number of
di   erent techniques.

2.4.1 id113

whenever we have a distribution p(x;   ) parametrized by some parameter
   we may use data to    nd a value of    which maximizes the likelihood that
the data would have been generated by a distribution with this choice of
parameter.
for instance, assume that we observe a set of temperature measurements
x = {x1, . . . , xm}. in this case, we could try    nding a normal distribution
such that the likelihood p(x;   ) of the data under the assumption of a normal
distribution is maximized. note that this does not imply in any way that the
temperature measurements are actually drawn from a normal distribution.
instead, it means that we are attempting to    nd the gaussian which    ts the
data in the best fashion.

while this distinction may appear subtle, it is critical: we do not assume
that our model accurately re   ects reality. instead, we simply try doing the
best possible job at modeling the data given a speci   ed model class. later
we will encounter alternative approaches at estimation, namely bayesian
methods, which make the assumption that our model ought to be able to
describe the data accurately.
de   nition 2.16 (maximum likelihood estimator) for a model p(  ;   )
parametrized by    and observations x the maximum likelihood estimator
(id113) is

    ml[x] := argmax

  

p(x;   ).

(2.51)

2.4 estimation

67

in the context of exponential families this leads to the following procedure:
given m observations drawn iid from some distribution, we can express the
joint likelihood as

p(x;   ) =

p(xi;   ) =

exp ((cid:104)  (xi),   (cid:105)     g(  ))

= exp (m ((cid:104)  [x],   (cid:105)     g(  )))

m(cid:89)

i=1

m(cid:89)

i=1

m(cid:88)

i=1

where   [x] :=

1
m

  (xi).

(2.52)

(2.53)

(2.54)

here   [x] is the empirical average of the map   (x). maximization of p(x;   )
is equivalent to minimizing the negative log-likelihood     log p(x;   ). the
latter is a common practical choice since for independently drawn data,
the product of probabilities decomposes into the sum of the logarithms of
individual likelihoods. this leads to the following objective function to be
minimized

    log p(x;   ) = m [g(  )     (cid:104)  ,   [x](cid:105)]

(2.55)
since g(  ) is convex and (cid:104)  ,   [x](cid:105) is linear in   , it follows that minimization
of (2.55) is a id76 problem. using theorem 2.14 and the    rst
order optimality condition      g(  ) =   [x] for (2.55) implies that

   = [     g]

   1 (  [x]) or equivalently ex   p(x;  )[  (x)] =      g(  ) =   [x].

(2.56)

put another way, the above conditions state that we aim to    nd the distribu-
tion p(x;   ) which has the same expected value of   (x) as what we observed
empirically via   [x]. under very mild technical conditions a solution to
(2.56) exists.

in general, (2.56) cannot be solved analytically. in certain special cases,
though, this is easily possible. we discuss two such choices in the following:
multinomial and poisson distributions.

example 2.6 (poisson distribution) for the poisson distribution1 where
x! exp(  x    e  ) it follows that g(  ) = e   and   (x) = x. this allows
p(x;   ) = 1

1 often the poisson distribution is speci   ed using    := log    as its rate parameter. in this case we
have p(x;   ) =   xe     /x! as its parametrization. the advantage of the natural parametrization
using    is that we can directly take advantage of the properties of the log-partition function as
generating the cumulants of x.

68

2 density estimation

m(cid:88)

i=1

1
m

us to solve (2.56) in closed form using

     g(  ) = e   =

xi and hence    = log

m(cid:88)

xi     log m.

(2.57)

have that

bution the log-partition function is given by g(  ) = log(cid:80)n

example 2.7 (multinomial distribution) for the multinomial distri-
i=1 e  i, hence we

i=1

   ig(  ) =

e  i(cid:80)n

m(cid:88)

1
m

{xj = i} .

=

j=1

(2.58)

j=1 e  j

j=1 {xj = i}. in other
words, the id113 for a discrete distribution simply given by the empirical
frequencies of occurrence.

it is easy to check that (2.58) is satis   ed for e  i =(cid:80)m
ponential families:    rstly, choosing   i = c + log(cid:80)m
of   (x) are linearly dependent     for any x we have that (cid:80)
j=1 {xj = i}(cid:105)
(cid:104)(cid:80)m

the multinomial setting also exhibits two rather important aspects of ex-
i=1 {xj = i} for any c     r
will lead to an equivalent distribution. this is the case since the su   cient
statistic   (x) is not minimal. in our context this means that the coordinates
j[  (x)]j = 1,
hence we could eliminate one dimension. this is precisely the additional
degree of freedom which is re   ected in the scaling freedom in   .

secondly, for data where some events do not occur at all, the expression
= log 0 is ill de   ned. this is due to the fact that this
log
particular set of counts occurs on the boundary of the convex set within
which the natural parameters    are well de   ned. we will see how di   erent
types of priors can alleviate the issue.

using the id113 is not without problems. as we saw in figure 2.1, conver-
gence can be slow, since we are not using any side information. the latter
can provide us with problems which are both numerically better conditioned
and which show better convergence, provided that our assumptions are ac-
curate. before discussing a bayesian approach to estimation, let us discuss
basic statistical properties of the estimator.

2.4.2 bias, variance and consistency
when designing any estimator     (x) we would like to obtain a number of
desirable properties: in general it should not be biased towards a particular
solution unless we have good reason to believe that this solution should
be preferred. instead, we would like the estimator to recover, at least on

2.4 estimation

69

average, the    correct    parameter, should it exist. this can be formalized in
the notion of an unbiased estimator.

secondly, we would like that, even if no correct parameter can be found,
e.g. when we are trying to    t a gaussian distribution to data which is not
normally distributed, that we will converge to the best possible parameter
choice as we obtain more data. this is what is understood by consistency.
finally, we would like the estimator to achieve low bias and near-optimal
estimates as quickly as possible. the latter is measured by the e   ciency
of an estimator. in this context we will encounter the cram  er-rao bound
which controls the best possible rate at which an estimator can achieve this
goal. figure 2.11 gives a pictorial description.

fig. 2.11. left: unbiased estimator; the estimates, denoted by circles have as mean
the true parameter, as denoted by a star. middle: consistent estimator. while the
true model is not within the class we consider (as denoted by the ellipsoid), the
estimates converge to the white star which is the best model within the class that
approximates the true model, denoted by the solid star. right: di   erent estimators
have di   erent regions of uncertainty, as made explicit by the ellipses around the
true parameter (solid star).

de   nition 2.17 (unbiased estimator) an estimator     [x] is unbiased
if for all    where x     p(x;   ) we have ex[    [x]] =   .

in other words, in expectation the parameter estimate matches the true pa-
rameter. note that this only makes sense if a true parameter actually exists.
for instance, if the data is poisson distributed and we attempt modeling it
by a gaussian we will obviously not obtain unbiased estimates.

for    nite sample sizes id113 is often biased. for instance, for the normal
distribution the variance estimates carry bias o(m   1). see problem 2.19
for details. in general, under fairly mild conditions, id113 is asymptotically
unbiased [dgl96]. we prove this for exponential families. for more general
settings the proof depends on the dimensionality and smoothness of the
family of densities that we have at our disposition.

70

2 density estimation

theorem 2.18 (id113 for exponential families) assume that x is an
m-sample drawn iid from p(x;   ). the estimate     [x] = g   1(  [x]) is asymp-
totically normal with

2 [    [x]       ]     n(0,(cid:2)   2

  g(  )(cid:3)   1

m    1

).

(2.59)

in other words, the estimate     [x] is asymptotically normal, it converges to
the true parameter   , and moreover, the variance at the correct parameter
is given by the inverse of the covariance matrix of the data, as given by the
second derivative of the log-partition function    2
proof denote by    =      g(  ) the true mean. moreover, note that    2
  g(  ) is
the covariance of the data drawn from p(x;   ). by the central limit theorem
(theorem 2.3) we have that n    1
now note that     [x] = [     g]

2 [  [x]       ]     n(0,   2
   1 (  [x]). therefore, by the delta method
(theorem 2.5) we know that     [x] is also asymptotically normal. moreover,
   1 (  ) =
by the inverse function theorem the jacobian of g   1 satis   es       [     g]

  g(  )(cid:3)   1. applying slutsky   s theorem (theorem 2.4) proves the claim.
(cid:2)   2

  g(  )).

  g(  ).

now that we established the asymptotic properties of the id113 for exponen-
tial families it is only natural to ask how much variation one may expect in
    [x] when performing estimation. the cramer-rao bound governs this.

theorem 2.19 (cram  er and rao [rao73]) assume that x is drawn from
p(x;   ) and let     [x] be an asymptotically unbiased estimator. denote by i
the fisher information matrix and by b the variance of     [x] where

(cid:104)    [x]
(cid:105)

.

(2.60)

i := cov [      log p(x;   )] and b := var

in this case det ib     1 for all estimators     [x].

proof we prove the claim for the scalar case. the extension to matrices is
straightforward. using the cauchy-schwarz inequality we have

cov2(cid:104)      log p(x;   ),     [x]

(cid:105)     var [      log p(x;   )] var

(cid:104)    [x]

= ib. (2.61)

(cid:105)

note that at the true parameter the expected log-likelihood score vanishes

ex[      log p(x;   )] =      

p(x;   )dx =      1 = 0.

(2.62)

(cid:90)

hence we may simplify the covariance formula by dropping the means via

2.4 estimation

(cid:104)      log p(x;   ),     [x]
(cid:105)

cov

71

(cid:104)      log p(x;   )    [x]
(cid:105)
(cid:90)

p(x;   )    (x)      log p(x;   )d  

p(x;   )    (x)dx =         = 1.

(cid:90)

= ex

=
=      

here the last equality follows since we may interchange integration by x
and the derivative with respect to   .

the cram  er-rao theorem implies that there is a limit to how well we may
estimate a parameter given    nite amounts of data. it is also a yardstick by
which we may measure how e   ciently an estimator uses data. formally, we
de   ne the e   ciency as the quotient between actual performance and the
cram  er-rao bound via

e := 1/det ib.

(2.63)

the closer e is to 1, the lower the variance of the corresponding estimator
    (x). theorem 2.18 implies that for exponential families id113 is asymptot-
ically e   cient. it turns out to be generally true.

theorem 2.20 (e   ciency of id113 [cra46, gw92, ber85]) the max-
imum likelihood estimator is asymptotically e   cient (e = 1).

so far we only discussed the behavior of     [x] whenever there exists a true   
generating p(  ; x). if this is not true, we need to settle for less: how well     [x]
approaches the best possible choice of within the given model class. such
behavior is referred to as consistency. note that it is not possible to de   ne
consistency per se. for instance, we may ask whether      converges to the
optimal parameter      , or whether p(x;     ) converges to the optimal density
p(x;      ), and with respect to which norm. under fairly general conditions
this turns out to be true for    nite-dimensional parameters and smoothly
parametrized densities. see [dgl96, vdg00] for proofs and further details.

2.4.3 a bayesian approach

the analysis of the maximum likelihood method might suggest that in-
ference is a solved problem. after all, in the limit, id113 is unbiased and it
exhibits as small variance as possible. empirical results using a    nite amount
of data, as present in figure 2.1 indicate otherwise.

while not making any assumptions can lead to interesting and general

72

2 density estimation

theorems, it ignores the fact that in practice we almost always have some
idea about what to expect of our solution. it would be foolish to ignore such
additional information. for instance, when trying to determine the voltage
of a battery, it is reasonable to expect a measurement in the order of 1.5v
or less. consequently such prior knowledge should be incorporated into the
estimation process. in fact, the use of side information to guide estimation
turns out to be the tool to building estimators which work well in high
dimensions.

recall bayes    rule (1.15) which states that p(  |x) = p(x|  )p(  )

. in our con-
text this means that if we are interested in the posterior id203 of   
assuming a particular value, we may obtain this using the likelihood (often
referred to as evidence) of x having been generated by    via p(x|  ) and our
prior belief p(  ) that    might be chosen in the distribution generating x.
observe the subtle but important di   erence to id113: instead of treating   
as a parameter of a density model, we treat    as an unobserved random
variable which we may attempt to infer given the observations x.

p(x)

this can be done for a number of di   erent purposes: we might want to
infer the most likely value of the parameter given the posterior distribution
p(  |x). this is achieved by

    map(x) := argmax

  

p(  |x) = argmin

    log p(x|  )     log p(  ).

(2.64)

  

the second equality follows since p(x) does not depend on   . this estimator
is also referred to as the maximum a posteriori, or map estimator. it di   ers
from the maximum likelihood estimator by adding the negative log-prior
to the optimization problem. for this reason it is sometimes also referred
to as penalized id113. e   ectively we are penalizing unlikely choices    via
    log p(  ).

note that using     map(x) as the parameter of choice is not quite accurate.
after all, we can only infer a distribution over    and in general there is no
guarantee that the posterior is indeed concentrated around its mode. a more
accurate treatment is to use the distribution p(  |x) directly via

p(x|x) =

p(x|  )p(  |x)d  .

(2.65)

(cid:90)

in other words, we integrate out the unknown parameter    and obtain the
density estimate directly. as we will see, it is generally impossible to solve
(2.65) exactly, an important exception being conjugate priors. in the other
cases one may resort to sampling from the posterior distribution to approx-
imate the integral.

while it is possible to design a wide variety of prior distributions, this book

2.4 estimation

73

focuses on two important families: norm-constrained prior and conjugate
priors. we will encounter them throughout, the former sometimes in the
guise of id173 and gaussian processes, the latter in the context of
exchangeable models such as the dirichlet process.

norm-constrained priors take on the form

p(  )     exp(     (cid:107)         0(cid:107)d

p) for p, d     1 and    > 0.

(2.66)

that is, they restrict the deviation of the parameter value    from some guess
  0. the intuition is that extreme values of    are much less likely than more
moderate choices of    which will lead to more smooth and even distributions
p(x|  ).

a popular choice is the gaussian prior which we obtain for p = d = 1
and    = 1/2  2. typically one sets   0 = 0 in this case. note that in (2.66)
we did not spell out the id172 of p(  )     in the context of map
estimation this is not needed since it simply becomes a constant o   set in
the optimization problem (2.64). we have

  

m [g(  )     (cid:104)  ,   [x](cid:105)] +   (cid:107)         0(cid:107)d

    map[x] = argmin

(2.67)
for d, p     1 and        0 the resulting optimization problem is convex and it
has a unique solution. moreover, very e   cient algorithms exist to solve this
problem. we will discuss this in detail in chapter 3. figure 2.12 shows the
regions of equal prior id203 for a range of di   erent norm-constrained
priors.

p

as can be seen from the diagram, the choice of the norm can have profound
consequences on the solution. that said, as we will show in chapter ??, the
estimate     map is well concentrated and converges to the optimal solution
under fairly general conditions.
an alternative to norm-constrained priors are conjugate priors. they are
designed such that the posterior p(  |x) has the same functional form as the
prior p(  ). in exponential families such priors are de   ned via
p(  |n,   ) = exp ((cid:104)n  ,   (cid:105)     ng(  )     h(  , n)) where

(2.68)

exp ((cid:104)n  ,   (cid:105)     ng(  )) d  .

h(  , n) = log

(2.69)
note that p(  |n,   ) itself is a member of the exponential family with the
feature map   (  ) = (  ,   g(  )). hence h(  , n) is convex in (n  , n). moreover,
the posterior distribution has the form

(cid:90)

p(  |x)     p(x|  )p(  |n,   )     exp ((cid:104)m  [x] + n  ,   (cid:105)     (m + n)g(  )) .

(2.70)

74

2 density estimation

fig. 2.12. from left to right: regions of equal prior id203 in r2 for priors using
the (cid:96)1, (cid:96)2 and (cid:96)    norm. note that only the (cid:96)2 norm is invariant with regard to the
coordinate system. as we shall see later, the (cid:96)1 norm prior leads to solutions where
only a small number of coordinates is nonzero.

m+n

that is, the posterior distribution has the same form as a conjugate prior
with parameters m  [x]+n  
and m + n. in other words, n acts like a phantom
sample size and    is the corresponding mean parameter. such an interpreta-
tion is reasonable given our desire to design a prior which, when combined
with the likelihood remains in the same model class: we treat prior knowl-
edge as having observed virtual data beforehand which is then added to the
actual set of observations. in this sense data and prior become completely
equivalent     we obtain our knowledge either from actual observations or
from virtual observations which describe our belief into how the data gen-
eration process is supposed to behave.

eq. (2.70) has the added bene   t of allowing us to provide an exact nor-

malized version of the posterior. using (2.68) we obtain that

(cid:16)(cid:104)m  [x] + n  ,   (cid:105)     (m + n)g(  )     h

(cid:16) m  [x]+n  

m+n

p(  |x) = exp

(cid:17)(cid:17)

, m + n

.

the main remaining challenge is to compute the id172 h for a range
of important conjugate distributions. the table on the following page pro-
vides details. besides attractive algebraic properties, conjugate priors also
have a second advantage     the integral (2.65) can be solved exactly:

exp ((cid:104)  (x),   (cid:105)     g(  ))  

(cid:16)(cid:104)m  [x] + n  ,   (cid:105)     (m + n)g(  )     h

exp

(cid:16) m  [x]+n  

m+n

(cid:17)(cid:17)

d  

, m + n

combining terms one may check that the integrand amounts to the normal-

(cid:90)

p(x|x) =

ization in the conjugate distribution, albeit   (x) added. this yields

2.4 estimation

(cid:16)

(cid:16) m  [x]+n  +  (x)

m+n+1

p(x|x) = exp

h

(cid:17)     h

(cid:16) m  [x]+n  

m+n

75

(cid:17)(cid:17)

, m + n + 1

, m + n

such an expansion is very useful whenever we would like to draw x from
p(x|x) without the need to obtain an instantiation of the latent variable   .
we provide explicit expansions in appendix 2. [gs04] use the fact that   
can be integrated out to obtain what is called a collapsed gibbs sampler for
topic models [bnj03].

2.4.4 an example

assume we would like to build a language model based on available doc-
uments. for instance, a linguist might be interested in estimating the fre-
quency of words in shakespeare   s collected works, or one might want to
compare the change with respect to a collection of webpages. while mod-
els describing documents by treating them as bags of words which all have
been obtained independently of each other are exceedingly simple, they are
valuable for quick-and-dirty content    ltering and categorization, e.g. a spam
   lter on a mail server or a content    lter for webpages.
hence we model a document d as a multinomial distribution: denote by
wi for i     {1, . . . , md} the words in d. moreover, denote by p(w|  ) the
id203 of occurrence of word w, then under the assumption that the
words are independently drawn, we have

p(d|  ) =

p(wi|  ).

(2.71)

md(cid:89)

i=1

it is our goal to    nd parameters    such that p(d|  ) is accurate. for a given
collection d of documents denote by mw the number of counts for word w
in the entire collection. moreover, denote by m the total number of words
in the entire collection. in this case we have

(cid:89)

(cid:89)

p(d|  ) =

p(di|  ) =

p(w|  )mw .

(2.72)

i

w

finding suitable parameters    given d proceeds as follows: in a maximum
likelihood model we set

p(w|  ) =

mw
m

.

(2.73)

in other words, we use the empirical frequency of occurrence as our best
guess and the su   cient statistic of d is   (w) = ew, where ew denotes the unit
vector which is nonzero only for the    coordinate    w. hence   [d]w = mw
m .

76

2 density estimation

we know that the conjugate prior of the multinomial model is a dirichlet
model. it follows from (2.70) that the posterior mode is obtained by replacing
. denote by nw :=   w    n the pseudo-counts arising from
  [d] by m  [d]+n  
the conjugate prior with parameters (  , n). in this case we will estimate the
id203 of the word w as
p(w|  ) =

mw + n  w

mw + nw

(2.74)

m+n

=

.

m + n

m + n

in other words, we add the pseudo counts nw to the actual word counts mw.
this is particularly useful when the document we are dealing with is brief,
that is, whenever we have little data: it is quite unreasonable to infer from
a webpage of approximately 1000 words that words not occurring in this
page have zero id203. this is exactly what is mitigated by means of
the conjugate prior (  , n).

finally, let us consider norm-constrained priors of the form (2.66). in this

case, the integral required for

p(d) =
   

p(d|  )p(  )d  

(cid:16)     (cid:107)         0(cid:107)d

exp

p + m(cid:104)  [d],   (cid:105)     mg(  )

(cid:17)

d  

(cid:90)
(cid:90)

is intractable and we need to resort to an approximation. a popular choice
is to replace the integral by p(d|     ) where       maximizes the integrand. this
is precisely the map approximation of (2.64). hence, in order to perform
estimation we need to solve

minimize

  

g(  )     (cid:104)  [d],   (cid:105) +

(cid:107)         0(cid:107)d
p .

  
m

(2.75)

a very simple strategy for minimizing (2.75) is id119. that is for
a given value of    we compute the gradient of the objective function and take
a    xed step towards its minimum. for simplicity assume that d = p = 2 and
   = 1/2  2, that is, we assume that    is normally distributed with variance
  2 and mean   0. the gradient is given by

      [    log p(d,   )] = ex   p(x|  )[  (x)]       [d] +

1

m  2 [         0]

(2.76)

in other words, it depends on the discrepancy between the mean of   (x)
with respect to our current model and the empirical average   [x], and the
di   erence between    and the prior mean   0.
unfortunately, convergence of the procedure                       [. . .] is usually
very slow, even if we adjust the steplength    e   ciently. the reason is that
the gradient need not point towards the minimum as the space is most likely

2.5 sampling

77

distorted. a better strategy is to use newton   s method (see chapter 3 for
a detailed discussion and a convergence proof). it relies on a second order
taylor approximation

1
2

  (cid:62)h  

    log p(d,    +   )         log p(d,   ) + (cid:104)  , g(cid:105) +

(2.77)
where g and h are the    rst and second derivatives of     log p(d,   ) with
respect to   . the quadratic expression can be minimized with respect to   
by choosing    =    h   1g and we can fashion an update algorithm from this
by letting             h   1g. one may show (see chapter 3) that algorithm 2.1
is quadratically convergent. note that the prior on    ensures that h is well
conditioned even in the case where the variance of   (x) is not. in practice this
means that the prior ensures fast convergence of the optimization algorithm.

algorithm 2.1 id77 for map estimation
newtonmap(d)
initialize    =   0
while not converged do

compute g = ex   p(x|  )[  (x)]       [d] + 1
compute h = varx   p(x|  )[  (x)] + 1
m  2 1
update               h   1g

m  2 [         0]

end while
return   

2.5 sampling

so far we considered the problem of estimating the underlying id203
density, given a set of samples drawn from that density. now let us turn to
the converse problem, that is, how to generate random variables given the
underlying id203 density. in other words, we want to design a random
variable generator. this is useful for a number of reasons:

we may encounter id203 distributions where optimization over suit-
able model parameters is essentially impossible and where it is equally im-
possible to obtain a closed form expression of the distribution. in these cases
it may still be possible to perform sampling to draw examples of the kind
of data we expect to see from the model. chapter ?? discusses a number of
id114 where this problem arises.

secondly, assume that we are interested in testing the performance of a
network router under di   erent load conditions. instead of introducing the
under-development router in a live network and wreaking havoc, one could

78

2 density estimation

estimate the id203 density of the network tra   c under various load
conditions and build a model. the behavior of the network can then be
simulated by using a probabilistic model. this involves drawing random
variables from an estimated id203 distribution.

carrying on, suppose that we generate data packets by sampling and see
an anomalous behavior in your router. in order to reproduce and debug
this problem one needs access to the same set of random packets which
caused the problem in the    rst place. in other words, it is often convenient
if our random variable generator is reproducible; at    rst blush this seems
like a contradiction. after all, our random number generator is supposed
to generate random variables. this is less of a contradiction if we consider
how random numbers are generated in a computer     given a particular
initialization (which typically depends on the state of the system, e.g. time,
disk size, bios checksum, etc.) the random number algorithm produces a
sequence of numbers which, for all practical purposes, can be treated as iid.
a simple method is the linear congruential generator [ptvf94]

xi+1 = (axi + b) mod c.

the performance of these iterations depends signi   cantly on the choice of the
constants a, b, c. for instance, the gnu c compiler uses a = 1103515245, b =
12345 and c = 232. in general b and c need to be relatively prime and a     1
needs to be divisible by all prime factors of c and by 4. it is very much
advisable not to attempt implementing such generators on one   s own unless
it is absolutely necessary.

useful desiderata for a pseudo random number generator (prng) are that
for practical purposes it is statistically indistinguishable from a sequence of
iid data. that is, when applying a number of statistical tests, we will accept
the null-hypothesis that the random variables are iid. see chapter ?? for
a detailed discussion of statistical testing procedures for random variables.
in the following we assume that we have access to a uniform rng u [0, 1]
which draws random numbers uniformly from the range [0, 1].

2.5.1 inverse transformation

we now consider the scenario where we would like to draw from some dis-
tinctively non-uniform distribution. whenever the latter is relatively simple
this can be achieved by applying an inverse transform:
theorem 2.21 for z     p(z) with z     z and an injective transformation
   : z     x with inverse transform      1 on   (z) it follows that the random

2.5 sampling

79

discrete id203 distribution

cumulative density function

0.3

0.2

0.1

0

1

2

3

4

5

1

0.8

0.6

0.4

0.2

0

1

2

3

4

5

6

fig. 2.13. left: discrete id203 distribution over 5 possible outcomes. right:
associated cumulative distribution function. when sampling, we draw x uniformly
at random from u [0, 1] and compute the inverse of f .

denotes the determinant of the jacobian of      1.

variable x :=   (z) is drawn from (cid:12)(cid:12)   x     1(x)(cid:12)(cid:12)    p(     1(x)). here (cid:12)(cid:12)   x     1(x)(cid:12)(cid:12)
sure, i.e. we change dp(z) to dp(     1(x))(cid:12)(cid:12)   x     1(x)(cid:12)(cid:12). such a conversion strat-
bution function f (x(cid:48)) = (cid:82) x(cid:48)

corollary 2.22 denote by p(x) a distribution on r with cumulative distri-
       dp(x). then the transformation x =   (z) =

this follows immediately by applying a variable transformation for a mea-

egy is particularly useful for univariate distributions.

f    1(z) converts samples z     u [0, 1] to samples drawn from p(x).

we now apply this strategy to a number of univariate distributions. one of
the most common cases is sampling from a discrete distribution.

example 2.8 (discrete distribution) in the case of a discrete distribu-
tion over {1, . . . , k} the cumulative distribution function is a step-function
with steps at {1, . . . , k} where the height of each step is given by the corre-
sponding id203 of the event.
the implementation works as follows: denote by p     [0, 1]k the vector of
probabilities and denote by f     [0, 1]k with fi = fi   1 + pi and f1 = p1 the
steps of the cumulative distribution function. then for a random variable z
drawn from u [0, 1] we obtain x =   (z) := argmini {fi     z}. see figure 2.13
for an example of a distribution over 5 events.

80

1

0.8

0.6

0.4

0.2

exponential distribution

cumulative distribution function

2 density estimation

1

0.8

0.6

0.4

0.2

0

0

2

4

6

8

10

0

0

2

4

6

8

10

fig. 2.14. left: exponential distribution with    = 1. right: associated cumulative
distribution function. when sampling, we draw x uniformly at random from u [0, 1]
and compute the inverse.

example 2.9 (exponential distribution) the density of a exponential-
distributed random variable is given by

p(x|  ) =    exp(     x) if    > 0 and x     0.

(2.78)

this allows us to compute its cdf as

f (x|  ) = 1     exp(     x)if    > 0 for x     0.

(2.79)
therefore to generate a exponential random variable we draw z     u [0, 1]
and solve x =   (z) = f    1(z|  ) =         1 log(1     z). since z and 1     z are
drawn from u [0, 1] we can simplify this to x =         1 log z.

we could apply the same reasoning to the normal distribution in order to
draw gaussian random variables. unfortunately, the cumulative distribution
function of the gaussian is not available in closed form and we would need
resort to rather nontrivial numerical techniques. it turns out that there exists
a much more elegant algorithm which has its roots in gauss    proof of the
id172 constant of the normal distribution. this technique is known
as the box-m  uller transform.

example 2.10 (box-m  uller transform) denote by x, y independent gaus-
sian random variables with zero mean and unit variance. we have

p(x, y) =

1   
2  

e    1

2 x2 1   
2  

e    1
2 y2

=

1
2  

e    1

2 (x2+y2)

(2.80)

2.5 sampling

81

fig. 2.15. red: true density of the standard normal distribution (red line) is con-
trasted with the histogram of 20,000 random variables generated by the box-m  uller
transform.

the key observation is that the joint distribution p(x, y) is radially symmet-
ric, i.e. it only depends on the radius r2 = x2 + y2. hence we may perform
a variable substitution in polar coordinates via the map    where
x = r cos    and y = r sin    hence (x, y) =      1(r,   ).

(2.81)

this allows us to express the density in terms of (r,   ) via

p(r,   ) = p(     1(r,   ))(cid:12)(cid:12)   r,       1(r,   )(cid:12)(cid:12) =

e    1
2 r2

1
2  

r
2  
the fact that p(r,   ) is constant in    means that we can easily sample       
[0, 2  ] by drawing a random variable, say z   from u [0, 1] and rescaling it with
2  . to obtain a sampler for r we need to compute the cumulative distribution
function for p(r) = re    1
2 r2

cos   
   r sin   

sin   
r cos   

:

(cid:21)(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:20)

and hence r = f    1(z) =(cid:112)   2 log(1     z).

f (r(cid:48)) =

re    1
2 r2

dr = 1     e    1

2 r(cid:48)2

(cid:90) r(cid:48)

e    1
2 r2

.

0

(2.82)
observing that z     u [0, 1] implies that 1     z     u [0, 1] yields the following
sampler: draw z  , zr     u [0, 1] and compute x and y by

x =(cid:112)   2 log zr cos 2  z   and y =(cid:112)   2 log zr sin 2  z  .

note that the box-m  uller transform yields two independent gaussian ran-
dom variables. see figure 2.15 for an example of the sampler.

43210123450.000.050.100.150.200.250.300.350.400.4582

2 density estimation

example 2.11 (uniform distribution on the disc) a similar strategy
can be employed when sampling from the unit disc. in this case the closed-
form expression of the distribution is simply given by
if x2 + y2     1
otherwise

(cid:40) 1

p(x, y) =

(2.83)

  
0

using the variable transform (2.81) yields

p(r,   ) = p(     1(r,   ))(cid:12)(cid:12)   r,       1(r,   )(cid:12)(cid:12) =

(cid:40) r

  
0

if r     1
otherwise

(2.84)

integrating out    yields p(r) = 2r for r     [0, 1] with corresponding cdf
f (r) = r2 for r     [0, 1]. hence our sampler draws zr, z       u [0, 1] and then
computes x =

   
zr cos 2  z   and y =

zr sin 2  z  .

   

2.5.2 rejection sampler

all the methods for random variable generation that we looked at so far re-
quire intimate knowledge about the pdf of the distribution. we now describe
a general purpose method, which can be used to generate samples from an
arbitrary distribution. let us begin with sampling from a set:
example 2.12 (rejection sampler) denote by x     x a set and let p be
a density on x. then a sampler for drawing from px (x)     p(x) for x     x
and px (x) = 0 for x (cid:54)    x, that is, px (x) = p(x|x     x) is obtained by the
procedure:
repeat
until x     x
return x

draw x     p(x)

that is, the algorithm keeps on drawing from p until the random variable is
contained in x. the id203 that this occurs is clearly p(x). hence the
larger p(x) the higher the e   ciency of the sampler. see figure 2.16.

example 2.13 (uniform distribution on a disc) the procedure works
trivially as follows: draw x, y     u [0, 1]. accept if (2x     1)2 + (2y     1)2     1
and return sample (2x     1, 2y     1). this sampler has e   ciency 4
   since this
is the surface ratio between the unit square and the unit ball.

note that this time we did not need to carry out any sophisticated measure

2.5 sampling

83

fig. 2.16. rejection sampler. left: samples drawn from the uniform distribution on
[0, 1]2. middle: the samples drawn from the uniform distribution on the unit disc
are all the points in the grey shaded area. right: the same procedure allows us to
sample uniformly from arbitrary sets.

fig. 2.17. accept reject sampling for the beta(2, 5) distribution. left: samples are
generated uniformly from the blue rectangle (shaded area). only those samples
which fall under the red curve of the beta(2, 5) distribution (darkly shaded area)
are accepted. right: the true density of the beta(2, 5) distribution (red line) is
contrasted with the histogram of 10,000 samples drawn by the rejection sampler.

transform. this mathematical convenience came at the expense of a slightly
less e   cient sampler     about 21% of all samples are rejected.

the same reasoning that we used to obtain a hard accept/reject procedure
can be used for a considerably more sophisticated rejection sampler. the
basic idea is that if, for a given distribution p we can    nd another distribution
q which, after rescaling, becomes an upper envelope on p, we can use q to
sample from and reject depending on the ratio between q and p.

theorem 2.23 (rejection sampler) denote by p and q distributions on
x and let c be a constant such that such that cq(x)     p(x) for all x     x.

0.00.20.40.60.81.00.00.51.01.52.02.50.00.20.40.60.81.00.00.51.01.52.02.53.084
2 density estimation
then the algorithm below draws from p with acceptance id203 c   1.

repeat

draw x     q(x) and t     u [0, 1]

until ct     p(x)
return x

q(x)

proof denote by z the event that the sample drawn from q is accepted.
then by bayes rule the id203 pr(x|z) can be written as follows

pr(z|x) pr(x)

p(x)

cq(x)    q(x)

pr(x|z) =

here we used that pr(z) =(cid:82) pr(z|x)q(x)dx =(cid:82) c   1p(x)dx = c   1.

= p(x)

pr(z)

c   1

=

(2.85)

note that the algorithm of example 2.12 is a special case of such a rejection
sampler     we majorize px by the uniform distribution rescaled by

1
p(x) .

example 2.14 (beta distribution) recall that the beta(a, b) distribution,
as a member of the exponential family with su   cient statistics (log x, log(1   
x)), is given by

p(x|a, b) =

  (a + b)
  (a)  (b)

xa   1(1     x)b   1,

for given (a, b) one can verify (problem 2.25) that

m := argmax

x

p(x|a, b) =

a     1
a + b     2

.

(2.86)

(2.87)

provided a > 1. hence, if we use as proposal distribution the uniform distri-
bution u [0, 1] with scaling factor c = p(m|a, b) we may apply theorem 2.23.
as illustrated in figure 2.17, to generate a sample from beta(a, b) we    rst
generate a pair (x, t), uniformly at random from the shaded rectangle. a
sample is retained if ct     p(x|a, b), and rejected otherwise. the acceptance
rate of this sampler is 1
c .

example 2.15 (normal distribution) we may use the laplace distri-
bution to generate samples from the normal distribution. that is, we use

q(x|  ) =

e     |x|

  
2

(2.88)

as the proposal distribution. for a normal distribution p = n(0, 1) with zero

2.5 sampling

85

mean and unit variance it turns out that choosing    = 1 yields the most
e   cient sampling scheme (see problem 2.27) with

(cid:114) 2e

  

p(x)    

q(x|   = 1)

as illustrated in figure 2.18, we    rst generate x     q(x|   = 1) using the
inverse transform method (see example 2.9 and problem 2.21) and t    

u [0, 1]. if t    (cid:112)2e/  p(x) we accept x, otherwise we reject it. the e   ciency
of this scheme is(cid:112)   

2e .

(cid:113) 2e
   g(x|0, 1)

p(x)

0.6

0.4

0.2

   4
0

   2

0

2

4

generated uniformly from the laplace distribution rescaled by(cid:112)2e/  . only those

fig. 2.18. rejection sampling for the normal distribution (red curve). samples are

samples which fall under the red curve of the standard normal distribution (darkly
shaded area) are accepted.

while rejection sampling is fairly e   cient in low dimensions its e   ciency is
unsatisfactory in high dimensions. this leads us to an instance of the curse of
dimensionality [bel61]: the pdf of a d-dimensional gaussian random variable
centered at 0 with variance   2 1 is given by
2      de

p(x|  2) = (2  )    d

    1
2  2 (cid:107)x(cid:107)2

now suppose that we want to draw from p(x|  2) by sampling from another
gaussian q with slightly larger variance   2 >   2. in this case the ratio
between both distributions is maximized at 0 and it yields

q(0|  2)
p(0|  2)

=

c =

(cid:104)   

(cid:105)d

  

86

2 density estimation

   = 1.01, and d = 1000, we    nd that c     20960. in other words,

if suppose   
we need to generate approximately 21,000 samples on the average from q to
draw a single sample from p. we will discuss a more sophisticated sampling
algorithms, namely id150, in section ??. it allows us to draw from
rather nontrivial distributions as long as the distributions in small subsets
of random variables are simple enough to be tackled directly.

erf(x) =(cid:112)2/  

(cid:90) x

problems
problem 2.1 (bias variance decomposition {1}) prove that the vari-
ance varx [x] of a random variable can be written as ex [x2]     ex [x]2.
problem 2.2 (moment generating function {2}) prove that the char-
acteristic function can be used to generate moments as given in (2.12). hint:
use the taylor expansion of the exponential and apply the di   erential oper-
ator before the expectation.
problem 2.3 (cumulative error function {2})

e   x2

dx.

(2.89)

0

problem 2.4 (weak law of large numbers {2}) in analogy to the proof
of the central limit theorem prove the weak law of large numbers. hint: use
a    rst order taylor expansion of ei  t = 1 + i  t + o(t) to compute an approx-
imation of the characteristic function. next compute the limit m         for
     xm. finally, apply the inverse fourier transform to associate the constant
distribution at the mean    with it.
problem 2.5 (rates and con   dence bounds {3}) show that the rate
of hoe   ding is tight     get bound from central limit theorem and compare to
the hoe   ding rate.

problem 2.6 why can   t we just use each chip on the wafer as a random
variable? give a counterexample. give bounds if we actually were allowed to
do this.

problem 2.7 (union bound) work on many bounds at the same time.
we only have logarithmic penalty.
problem 2.8 (randomized rounding {4}) solve the linear system of
equations ax = b for integral x.

2.5 sampling
87
problem 2.9 (randomized projections {3}) prove that the random-
ized projections converge.
problem 2.10 (the count-min sketch {5}) prove the projection trick
problem 2.11 (parzen windows with triangle kernels {1}) suppose
you are given the following data: x = {2, 3, 3, 5, 5}. plot the estimated den-
sity using a kernel density estimator with the following kernel:

(cid:40)

k(u) =

0.5     0.25     |u| if |u|     2
0 otherwise.

problem 2.12 gaussian process link with gaussian prior on natural pa-
rameters

problem 2.13 optimization for gaussian id173

problem 2.14 conjugate prior (student-t and wishart).
problem 2.15 (multivariate gaussian {1}) prove that    (cid:31) 0 is a nec-
essary and su   cient condition for the normal distribution to be well de   ned.
problem 2.16 (discrete exponential distribution {2})   (x) = x and
uniform measure.

problem 2.17 exponential random graphs.

problem 2.18 (maximum id178 distribution) show that exponen-
tial families arise as the solution of the maximum id178 estimation prob-
lem.

problem 2.19 (maximum likelihood estimates for normal distributions)
derive the maximum likelihood estimates for a normal distribution, that is,
show that they result in

m(cid:88)

i=1

m(cid:88)

i=1

     =

1
m

xi and     2 =

1
m

(xi         )2

(2.90)

using the exponential families parametrization. next show that while the
mean estimate      is unbiased, the variance estimate has a slight bias of o( 1
m ).
to see this, take the expectation with respect to     2.

88
2 density estimation
problem 2.20 (cdf of logistic random variable {1}) show that the cdf
of the logistic random variable (??) is given by (??).
problem 2.21 (double-exponential (laplace) distribution {1}) use
the inverse-transform method to generate a sample from the double-exponential
(laplace) distribution (2.88).
problem 2.22 (normal random variables in polar coordinates {1})
if x1 and x2 are standard normal random variables and let (r,   ) de-
note the polar coordinates of the pair (x1, x2). show that r2       2
2 and
       unif[0, 2  ].
problem 2.23 (monotonically increasing mappings {1}) a mapping
t : r     r is one-to-one if, and only if, t is monotonically increasing, that
is, x > y implies that t (x) > t (y).
problem 2.24 (monotonically increasing multi-maps {2}) let t : rn    
rn be one-to-one. if x     px (x), then show that the distribution py (y) of
y = t (x) can be obtained via (??).
problem 2.25 (argmax of the beta(a, b) distribution {1}) show that
the mode of the beta(a, b) distribution is given by (2.87).
problem 2.26 (accept reject sampling for the unit disk {2}) give at
least two di   erent accept-reject based sampling schemes to generate sam-
ples uniformly at random from the unit disk. compute their e   ciency.
problem 2.27 (optimizing laplace for standard normal {1}) optimize
the ratio p(x)/g(x|  ,   ), with respect to    and   , where p(x) is the standard
normal distribution (??), and g(x|  ,   ) is the laplace distribution (2.88).
problem 2.28 (normal random variable generation {2}) the aim
of this problem is to write code to generate standard normal random vari-
ables (??) by using di   erent methods. to do this generate u     unif[0, 1]
and apply

(i) the box-muller transformation outlined in section ??.
(ii) use the following approximation to the inverse cdf

     1(  )     t     a0 + a1t

1 + b1t + b2t2 ,

(2.91)

2.5 sampling

where t2 = log(     2) and

89

a0 = 2.30753, a1 = 0.27061, b1 = 0.99229, b2 = 0.04481

(iii) use the method outlined in example 2.15.

plot a histogram of the samples you generated to con   rm that they are nor-
mally distributed. compare these di   erent methods in terms of the time
needed to generate 1000 random variables.
problem 2.29 (non-standard normal random variables {2}) describe
a scheme based on the box-muller transform to generate d dimensional nor-
mal random variables p(x|0, i). how can this be used to generate arbitrary
normal random variables p(x|  ,   ).
problem 2.30 (uniform samples from a disk {2}) show how the ideas
described in section ?? can be generalized to draw samples uniformly at ran-
dom from an axis parallel ellipse: {(x, y) : x2

a2 + x2

b2     1}.

1

2

3

optimization

optimization plays an increasingly important role in machine learning. for
instance, many machine learning algorithms minimize a regularized risk
functional:

min

f

j(f ) :=      (f ) + remp(f ),

with the empirical risk

m(cid:88)

i=1

1
m

l(f (xi), yi).

remp(f ) :=

(3.1)

(3.2)

here xi are the training instances and yi are the corresponding labels. l the
id168 measures the discrepancy between y and the predictions f (xi).
finding the optimal f involves solving an optimization problem.

this chapter provides a self-contained overview of some basic concepts and
tools from optimization, especially geared towards solving machine learning
problems. in terms of concepts, we will cover topics related to convexity,
duality, and lagrange multipliers. in terms of tools, we will cover a variety
of optimization algorithms including id119, stochastic gradient
descent, newton   s method, and quasi-id77s. we will also look
at some specialized algorithms tailored towards solving id135
and quadratic programming problems which often arise in machine learning
problems.

3.1 preliminaries

minimizing an arbitrary function is, in general, very di   cult, but if the ob-
jective function to be minimized is convex then things become considerably
simpler. as we will see shortly, the key advantage of dealing with convex
functions is that a local optima is also the global optima. therefore, well
developed tools exist to    nd the global minima of a convex function. conse-
quently, many machine learning algorithms are now formulated in terms of
id76 problems. we brie   y review the concept of convex sets
and functions in this section.

91

92

3 optimization

3.1.1 convex sets
de   nition 3.1 (convex set) a subset c of rn is said to be convex if
(1       )x +   y     c whenever x     c, y     c and 0 <    < 1.

intuitively, what this means is that the line joining any two points x and y
from the set c lies inside c (see figure 3.1). it is easy to see (exercise 3.1)
that intersections of convex sets are also convex.

fig. 3.1. the convex set (left) contains the line joining any two points that belong
to the set. a non-convex set (right) does not satisfy this property.

a vector sum(cid:80)

i   ixi is called a convex combination if   i     0 and(cid:80)

i   i =

1. convex combinations are helpful in de   ning a convex hull:

de   nition 3.2 (convex hull) the convex hull, conv(x), of a    nite sub-
set x = {x1, . . . , xn} of rn consists of all convex combinations of x1, . . . , xn.

3.1.2 convex functions
let f be a real valued function de   ned on a set x     rn. the set

{(x,   ) : x     x,        r,        f (x)}

(3.3)

is called the epigraph of f . the function f is de   ned to be a convex function
if its epigraph is a convex set in rn+1. an equivalent, and more commonly
used, de   nition (exercise 3.5) is as follows (see figure 3.2 for geometric
intuition):

de   nition 3.3 (convex function) a function f de   ned on a set x is
called convex if, for any x, x(cid:48)     x and any 0 <    < 1 such that   x + (1    
  )x(cid:48)     x, we have

f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48)).

(3.4)

3.1 preliminaries

a function f is called strictly convex if

f (  x + (1       )x(cid:48)) <   f (x) + (1       )f (x(cid:48))

93

(3.5)

in fact, the above de   nition can be extended to show that if f is a convex

whenever x (cid:54)= x(cid:48).

function and   i     0 with(cid:80)
(cid:32)(cid:88)

i   i = 1 then

(cid:33)

   (cid:88)

f

  ixi

  if (xi).

(3.6)

the above inequality is called the jensen   s inequality (problem ).

i

i

fig. 3.2. a convex function (left) satis   es (3.4); the shaded region denotes its epi-
graph. a nonconvex function (right) does not satisfy (3.4).

if f : x     r is di   erentiable, then f is convex if, and only if,

f (x(cid:48))     f (x) +(cid:10)x(cid:48)     x,   f (x)(cid:11) for all x, x(cid:48)     x.

(3.7)

in other words, the    rst order taylor approximation lower bounds the convex
function universally (see figure 3.4). here and in the rest of the chapter
(cid:104)x, y(cid:105) denotes the euclidean dot product between vectors x and y, that is,

xiyi.

(3.8)

i

if f is twice di   erentiable, then f is convex if, and only if, its hessian is

positive semi-de   nite, that is,

   2f (x) (cid:23) 0.

(3.9)

for twice di   erentiable strictly convex functions, the hessian matrix is pos-
itive de   nite, that is,    2f (x) (cid:31) 0. we brie   y summarize some operations
which preserve convexity:

(cid:88)

(cid:104)x, y(cid:105) :=

6420246x02004006008001000f(x)3210123x1.51.00.50.00.51.01.5f(x)94

3 optimization

addition if f1 and f2 are convex, then f1 + f2 is also convex.

scaling

if f is convex, then   f is convex for    > 0.

a   ne transform if f is convex, then g(x) = f (ax + b) for some matrix
adding a linear function if f is convex, then g(x) = f (x)+(cid:104)a, x(cid:105) for some vector
subtracting a linear function if f is convex, then g(x) = f (x)   (cid:104)a, x(cid:105) for some vector

a and vector b is also convex.

a is also convex.

a is also convex.

pointwise maximum if fi are convex, then g(x) = maxi fi(x) is also convex.
scalar composition if f (x) = h(g(x)), then f is convex if a) g is convex,
and h is convex, non-decreasing or b) g is concave, and
h is convex, non-increasing.

fig. 3.3. left: convex function in two variables. right: the corresponding convex
below-sets {x|f (x)     c}, for di   erent values of c. this is also called a contour plot.

there is an intimate relation between convex functions and convex sets.
for instance, the following lemma show that the below sets (level sets) of
convex functions, sets for which f (x)     c, are convex.
lemma 3.4 (below-sets of convex functions) denote by f : x     r
a convex function. then the set

xc := {x| x     x and f (x)     c}, for all c     r,

(3.10)

is convex.
proof for any x, x(cid:48)     xc, we have f (x), f (x(cid:48))     c. moreover, since f is
convex, we also have

f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48))     c for all 0 <    < 1.

(3.11)
hence, for all 0 <    < 1, we have (  x + (1       )x(cid:48))     xc, which proves the
claim. figure 3.3 depicts this situation graphically.

-3-2-1 0 1 2 3-3-2-1 0 1 2 3 0 2 4 6 8 10 12 14 16 18-3-2-1 0 1 2 3-3-2-1 0 1 2 33.1 preliminaries

95

as we hinted in the introduction of this chapter, minimizing an arbitrary
function on a (possibly not even compact) set of arguments can be a di   cult
task, and will most likely exhibit many local minima. in contrast, minimiza-
tion of a convex objective function on a convex set exhibits exactly one global
minimum. we now prove this property.
theorem 3.5 (minima on convex sets) if the convex function f : x    
r attains its minimum, then the set of x     x, for which the minimum value
is attained, is a convex set. moreover, if f is strictly convex, then this set
contains a single element.
proof denote by c the minimum of f on x. then the set xc := {x|x    
x and f (x)     c} is clearly convex.
if f is strictly convex, then for any two distinct x, x(cid:48)     xc and any 0 <

   < 1 we have

f (  x + (1       )x(cid:48)) <   f (x) + (1       )f (x(cid:48)) =   c + (1       )c = c,

which contradicts the assumption that f attains its minimum on xc. there-
fore xc must contain only a single element.

as the following lemma shows, the minimum point can be characterized
precisely.
lemma 3.6 let f : x     r be a di   erentiable convex function. then x is
a minimizer of f , if, and only if,

(cid:10)x(cid:48)     x,   f (x)(cid:11)     0 for all x(cid:48).

(3.12)

proof to show the forward implication, suppose that x is the optimum
but (3.12) does not hold, that is, there exists an x(cid:48) for which

consider the line segment z(  ) = (1       )x +   x(cid:48), with 0 <    < 1. since x
is convex, z(  ) lies in x. on the other hand,

(cid:10)x(cid:48)     x,   f (x)(cid:11) < 0.
(cid:12)(cid:12)(cid:12)(cid:12)  =0

=(cid:10)x(cid:48)     x,   f (x)(cid:11) < 0,

d
d  

f (z(  ))

which shows that for small values of    we have f (z(  )) < f (x), thus showing
that x is not optimal.
the reverse implication follows from (3.7) by noting that f (x(cid:48))     f (x),

whenever (3.12) holds.

96
3 optimization
one way to ensure that (3.12) holds is to set    f (x) = 0. in other words,
minimizing a convex function is equivalent to    nding a x such that    f (x) =
0. therefore, the    rst order conditions are both necessary and su   cient
when minimizing a convex function.

3.1.3 subgradients

so far, we worked with di   erentiable convex functions. the subgradient is a
generalization of gradients appropriate for convex functions, including those
which are not necessarily smooth.

de   nition 3.7 (subgradient) suppose x is a point where a convex func-
tion f is    nite. then a subgradient is the normal vector of any tangential
supporting hyperplane of f at x. formally    is called a subgradient of f at
x if, and only if,

f (x(cid:48))     f (x) +(cid:10)x(cid:48)     x,   (cid:11) for all x(cid:48).

(3.13)

the set of all subgradients at a point is called the subdi   erential, and is de-
noted by    xf (x). if this set is not empty then f is said to be subdi   erentiable
at x. on the other hand, if this set is a singleton then, the function is said
to be di   erentiable at x. in this case we use    f (x) to denote the gradient
of f . convex functions are subdi   erentiable everywhere in their domain. we
now state some simple rules of subgradient calculus:

addition    x(f1(x) + f2(x)) =    xf1(x) +    xf2(x)

a   ne transform if g(x) = f (ax + b) for some matrix a and vector b,

scaling

   x  f (x) =      xf (x), for    > 0
then    xg(x) = a(cid:62)   yf (y).
i(cid:48)     argmaxi fi(x).

pointwise maximum if g(x) = maxi fi(x) then    g(x) = conv(   xfi(cid:48)) where

the de   nition of a subgradient can also be understood geometrically. as
illustrated by figure 3.4, a di   erentiable convex function is always lower
bounded by its    rst order taylor approximation. this concept can be ex-
tended to non-smooth functions via subgradients, as figure 3.5 shows.

by using more involved concepts, the proof of lemma 3.6 can be extended
to subgradients. in this case, minimizing a convex nonsmooth function en-
tails    nding a x such that 0        f (x).

3.1 preliminaries

97

3.1.4 strongly convex functions

when analyzing optimization algorithms, it is sometimes easier to work with
strongly convex functions, which generalize the de   nition of convexity.

de   nition 3.8 (strongly convex function) a convex function f is   -
strongly convex if, and only if, there exists a constant    > 0 such that the
function f (x)       

2 (cid:107)x(cid:107)2 is convex.

the constant    is called the modulus of strong convexity of f . if f is twice
di   erentiable, then there is an equivalent, and perhaps easier, de   nition of
strong convexity: f is strongly convex if there exists a    such that

   2f (x) (cid:23)   i.

(3.14)

in other words, the smallest eigenvalue of the hessian of f is uniformly
lower bounded by    everywhere. some important examples of strongly con-
vex functions include:

example 3.1 (squared euclidean norm) the function f (x) =   
is   -strongly convex.

example 3.2 (negative id178) let    n = {x s.t. (cid:80)

be the n dimensional simplex, and f :    n     r be the negative id178:

i xi = 1 and xi     0}

2 (cid:107)x(cid:107)2

(cid:88)

f (x) =

xi log xi.

(3.15)

then f is 1-strongly convex with respect to the (cid:107)  (cid:107)1 norm on the simplex
(see problem 3.7).

i

if f is a   -strongly convex function then one can show the following prop-
erties (exercise 3.8). here x, x(cid:48) are arbitrary and           f (x) and   (cid:48)        f (x(cid:48)).

(cid:13)(cid:13)x(cid:48)     x(cid:13)(cid:13)2
f (x(cid:48))     f (x) +(cid:10)x(cid:48)     x,   (cid:11) +
(cid:13)(cid:13)  (cid:48)       (cid:13)(cid:13)2
f (x(cid:48))     f (x) +(cid:10)x(cid:48)     x,   (cid:11) +
(cid:10)x     x(cid:48),          (cid:48)(cid:11)       (cid:13)(cid:13)x     x(cid:48)(cid:13)(cid:13)2
(cid:13)(cid:13)         (cid:48)(cid:13)(cid:13)2 .
(cid:10)x     x(cid:48),          (cid:48)(cid:11)     1

  
2
1
2  

  

(3.16)

(3.17)

(3.18)

(3.19)

98

3 optimization

3.1.5 convex functions with lipschitz continous gradient

a somewhat symmetric concept to strong convexity is the lipschitz conti-
nuity of the gradient. as we will see later they are connected by fenchel
duality.

de   nition 3.9 (lipschitz continuous gradient) a di   erentiable con-
vex function f is said to have a lipschitz continuous gradient, if there exists
a constant l > 0, such that

(cid:13)(cid:13)   f (x)        f (x(cid:48))(cid:13)(cid:13)     l(cid:13)(cid:13)x     x(cid:48)(cid:13)(cid:13)

   x, x(cid:48).

(3.20)

as before, if f is twice di   erentiable, then there is an equivalent, and perhaps
easier, de   nition of lipschitz continuity of the gradient: f has a lipschitz
continuous gradient strongly convex if there exists a l such that

li (cid:23)    2f (x).

(3.21)

in other words, the largest eigenvalue of the hessian of f is uniformly upper
bounded by l everywhere. if f has a lipschitz continuous gradient with
modulus l, then one can show the following properties (exercise 3.9).

(cid:13)(cid:13)x     x(cid:48)(cid:13)(cid:13)2
f (x(cid:48))     f (x) +(cid:10)x(cid:48)     x,   f (x)(cid:11) +
(cid:13)(cid:13)   f (x)        f (x(cid:48))(cid:13)(cid:13)2
f (x(cid:48))     f (x) +(cid:10)x(cid:48)     x,   f (x)(cid:11) +
(cid:10)x     x(cid:48),   f (x)        f (x(cid:48))(cid:11)     l(cid:13)(cid:13)x     x(cid:48)(cid:13)(cid:13)2
(cid:13)(cid:13)   f (x)        f (x(cid:48))(cid:13)(cid:13)2 .
(cid:10)x     x(cid:48),   f (x)        f (x(cid:48))(cid:11)     1

l
2
1
2l

l

3.1.6 fenchel duality

the fenchel conjugate of a function f is given by

f   (x   ) = sup

{(cid:104)x, x   (cid:105)     f (x)} .

x

even if f is not convex, the fechel conjugate which is written as a supremum
over linear functions is always convex. some rules for computing fenchel
duals are summarized in table 3.1.6. if f is convex and its epigraph (3.3) is
a closed convex set, then f       = f . if f and f    are convex, then they satisfy
the so-called fenchel-young inequality

f (x) + f   (x   )     (cid:104)x, x   (cid:105) for all x, x   .

(3.27)

(3.22)

(3.23)

(3.24)

(3.25)

(3.26)

3.1 preliminaries

99

fig. 3.4. a convex function is always lower bounded by its    rst order taylor ap-
proximation. this is true even if the function is not di   erentiable (see figure 3.5)

fig. 3.5. geometric intuition of a subgradient. the nonsmooth convex function
(solid blue) is only subdi   erentiable at the    kink    points. we illustrate two of its
subgradients (dashed green and red lines) at a    kink    point which are tangential to
the function. the normal vectors to these lines are subgradients. observe that the
   rst order taylor approximations obtained by using the subgradients lower bounds
the convex function.

this inequality becomes an equality whenever x           f (x), that is,

f (x) + f   (x   ) = (cid:104)x, x   (cid:105) for all x and x           f (x).

(3.28)

strong convexity (section 3.1.4) and lipschitz continuity of the gradient

4321012341012345100

3 optimization

table 3.1. rules for computing fenchel duals

scalar addition if g(x) = f (x) +    then g   (x   ) = f   (x   )       .
function scaling
parameter scaling

if    > 0 and g(x) =   f (x) then g   (x   ) =   f   (x   /  ).
if    (cid:54)= 0 and g(x) = f (  x) then g   (x   ) = f   (x   /  )
linear transformation if a is an invertible matrix then (f   a)    = f      (a   1)   .
if g(x) = f (x     x0) then g   (x   ) = f   (x   ) + (cid:104)x   , x0(cid:105).
then g   (x   ) =
inf {f   

2 = x   }.
pointwise in   mum if g(x) = inf fi(x) then g   (x   ) = supi f   

g(x) = f1(x) + f2(x)
1 (x   
1 + x   

shift
sum if

2) s.t. x   

1) + f   

i (x   ).

2 (x   

(section 3.1.5) are related by fenchel duality according to the following
lemma, which we state without proof.

lemma 3.10 (theorem 4.2.1 and 4.2.2 [hul93])

(i) if f is   -strongly convex, then f    has a lipschitz continuous gradient

(ii) if f is convex and has a lipschitz continuous gradient with modulus

with modulus 1
   .
l, then f    is 1

l -strongly convex.

next we describe some convex functions and their fenchel conjugates.

example 3.3 (squared euclidean norm) whenever f (x) = 1
have f   (x   ) = 1
jugate.

2 (cid:107)x(cid:107)2 we
2 (cid:107)x   (cid:107)2, that is, the squared euclidean norm is its own con-

example 3.4 (negative id178) the fenchel conjugate of the negative
id178 (3.15) is

(cid:88)

f   (x   ) = log

exp(x   
i ).

3.1.7 bregman divergence

i

let f be a di   erentiable convex function. the bregman divergence de   ned
by f is given by

   f (x, x(cid:48)) = f (x)     f (x(cid:48))    (cid:10)x     x(cid:48),   f (x(cid:48))(cid:11) .

(3.29)

also see figure 3.6. here are some well known examples.

example 3.5 (square euclidean norm) set f (x) = 1
   f (x) = x and therefore

(cid:13)(cid:13)x(cid:48)(cid:13)(cid:13)2    (cid:10)x     x(cid:48), x(cid:48)(cid:11) =

(cid:13)(cid:13)x     x(cid:48)(cid:13)(cid:13)2 .

1
2

   f (x, x(cid:48)) =

1
2

(cid:107)x(cid:107)2     1
2

2 (cid:107)x(cid:107)2. clearly,

3.1 preliminaries

101

fig. 3.6. f (x) is the value of the function at x, while f (x(cid:48))+(cid:104)x     x(cid:48),   f (x(cid:48))(cid:105) denotes
the    rst order taylor expansion of f around x(cid:48), evaluated at x. the di   erence
between these two quantities is the bregman divergence, as illustrated.

(cid:88)

i

example 3.6 (relative id178) let f be the un-normalized id178

f (x) =

(xi log xi     xi) .

(3.30)

one can calculate    f (x) = log x, where log x is the component wise loga-
rithm of the entries of x, and write the bregman divergence
   f (x, x(cid:48)) =

i    (cid:10)x     x(cid:48), log x(cid:48)(cid:11)

xi    (cid:88)

(cid:88)

i +

x(cid:48)

(cid:88)
(cid:88)

i

xi log xi    (cid:88)
(cid:19)
(cid:18) xi
(cid:18)

i

xi log

x(cid:48)

i

=

i

i

+ x(cid:48)

i     xi

example 3.7 (p-norm) let f be the square p-norm

f (x) =

(cid:107)x(cid:107)2

p =

1
2

1
2

.

(3.31)

.

i log x(cid:48)
x(cid:48)
(cid:19)
(cid:32)(cid:88)

xp
i

i

i

(cid:33)2/p

f(x0)f(x)f(x0)+   x   x0,   f(x0)      f(x,x0)102

3 optimization

we say that the q-norm is dual to the p-norm whenever 1
verify (problem 3.12) that the i-th component of the gradient    f (x) is

q = 1. one can

p + 1

   xif (x) =

sign(xi)|xi|p   1

(cid:107)x(cid:107)p   2

p

.

(3.32)

the corresponding bregman divergence is

   f (x, x(cid:48)) =

1
2

(cid:107)x(cid:107)2

p     1
2

p    (cid:88)
(cid:13)(cid:13)x(cid:48)(cid:13)(cid:13)2

i

(xi     x(cid:48)
i)

i|p   1

sign(x(cid:48)

i)|x(cid:48)
(cid:107)x(cid:48)(cid:107)p   2

p

.

the following properties of the bregman divergence immediately follow:
       f (x, x(cid:48)) is convex in x.
       f (x, x(cid:48))     0.
       f may not be symmetric, that is, in general    f (x, x(cid:48)) (cid:54)=    f (x(cid:48), x).
       x   f (x, x(cid:48)) =    f (x)        f (x(cid:48)).
the next lemma establishes another important property.

lemma 3.11 the bregman divergence (3.29) de   ned by a di   erentiable
convex function f satis   es

   f (x, y) +    f (y, z)        f (x, z) = (cid:104)   f (z)        f (y), x     y(cid:105) .

(3.33)

proof
   f (x, y) +    f (y, z) = f (x)     f (y)     (cid:104)x     y,   f (y)(cid:105) + f (y)     f (z)     (cid:104)y     z,   f (z)(cid:105)

= f (x)     f (z)     (cid:104)x     y,   f (y)(cid:105)     (cid:104)y     z,   f (z)(cid:105)
=    f (x, z) + (cid:104)   f (z)        f (y), x     y(cid:105) .

3.2 unconstrained smooth convex minimization

in this section we will describe various methods to minimize a smooth convex
objective function.

3.2.1 minimizing a one-dimensional convex function

as a warm up let us consider the problem of minimizing a smooth one di-
mensional convex function j : r     r in the interval [l, u ]. this seemingly

3.2 unconstrained smooth convex minimization

103

algorithm 3.1 interval bisection

1: input: l, u , precision  
2: set t = 0, a0 = l and b0 = u
3: while (bt     at)    j(cid:48)(u ) >   do

if j(cid:48)( at+bt

2

) > 0 then

at+1 = at and bt+1 = at+bt

2

else

at+1 = at+bt

2

and bt+1 = bt

4:

5:

6:

7:

8:

end if
t = t + 1
9:
10: end while
11: return: at+bt

2

simple problem has many applications. as we will see later, many optimiza-
tion methods    nd a direction of descent and minimize the objective function
along this direction1; this subroutine is called a line search. algorithm 3.1
depicts a simple line search routine based on interval bisection.

before we show that algorithm 3.1 converges, let us    rst derive an im-
portant property of convex functions of one variable. for a di   erentiable
one-dimensional convex function j (3.7) reduces to

j(w)     j(w(cid:48)) + (w     w(cid:48))    j(cid:48)(w(cid:48)),

(3.34)
where j(cid:48)(w) denotes the gradient of j. exchanging the role of w and w(cid:48) in
(3.34), we can write

j(w(cid:48))     j(w) + (w(cid:48)     w)    j(cid:48)(w).

(3.35)

adding the above two equations yields

(w     w(cid:48))    (j(cid:48)(w)     j(cid:48)(w(cid:48)))     0.

(3.36)
if w     w(cid:48), then this implies that j(cid:48)(w)     j(cid:48)(w(cid:48)). in other words, the gradient
of a one dimensional convex function is monotonically non-decreasing.

recall that minimizing a convex function is equivalent to    nding w    such
that j(cid:48)(w   ) = 0. furthermore, it is easy to see that the interval bisection
maintains the invariant j(cid:48)(at) < 0 and j(cid:48)(bt) > 0. this along with the
monotonicity of the gradient su   ces to ensure that w        (at, bt). setting
w = w    in (3.34), and using the monotonicity of the gradient allows us to

1 if the objective function is convex, then the one dimensional function obtained by restricting

it along the search direction is also convex (exercise 3.10).

104

write for any w(cid:48)     (at, bt)

3 optimization

j(w(cid:48))     j(w   )     (w(cid:48)     w   )    j(cid:48)(w(cid:48))     (bt     at)    j(cid:48)(u ).

(3.37)
since we halve the interval (at, bt) at every iteration, it follows that (bt   at) =
(u     l)/2t. therefore

j(w(cid:48))     j(w   )     (u     l)    j(cid:48)(u )

(3.38)
for all w(cid:48)     (at, bt). in other words, to    nd an  -accurate solution, that is,
j(w(cid:48))     j(w   )       we only need log(u     l) + log j(cid:48)(u ) + log(1/ ) < t itera-
tions. an algorithm which converges to an   accurate solution in o(log(1/ ))
iterations is said to be linearly convergent.

2t

,

for multi-dimensional objective functions, one cannot rely on the mono-
tonicity property of the gradient. therefore, one needs more sophisticated
optimization algorithms, some of which we now describe.

3.2.2 coordinate descent

coordinate descent is conceptually the simplest algorithm for minimizing a
multidimensional smooth convex function j : rn     r. at every iteration
select a coordinate, say i, and update

wt+1 = wt       tei.

(3.39)

here ei denotes the i-th basis vector, that is, a vector with one at the i-th co-
ordinate and zeros everywhere else, while   t     r is a non-negative scalar step
size. one could, for instance, minimize the one dimensional convex function
j(wt       ei) to obtain the stepsize   t. the coordinates can either be selected
cyclically, that is, 1, 2, . . . , n, 1, 2, . . . or greedily, that is, the coordinate which
yields the maximum reduction in function value.

even though coordinate descent can be shown to converge if j has a lip-
schitz continuous gradient [lt92], in practice it can be quite slow. however,
if a high precision solution is not required, as is the case in some machine
learning applications, coordinate descent is often used because a) the cost
per iteration is very low and b) the speed of convergence may be acceptable
especially if the variables are loosely coupled.

3.2.3 id119

id119 (also widely known as steepest descent) is an optimization
technique for minimizing multidimensional smooth convex objective func-
tions of the form j : rn     r. the basic idea is as follows: given a location

3.2 unconstrained smooth convex minimization

wt at iteration t, compute the gradient    j(wt), and update

wt+1 = wt       t   j(wt),

105

(3.40)

where   t is a scalar stepsize. see algorithm 3.2 for details. di   erent variants
of id119 depend on how   t is chosen:
exact line search: since j(wt          j(wt)) is a one dimensional convex
function in   , one can use the algorithm 3.1 to compute:

  t = argmin

  

j(wt          j(wt)).

(3.41)

instead of the simple bisecting line search more sophisticated line searches
such as the more-thuente line search or the golden bisection rule can also
be used to speed up convergence (see [nw99] chapter 3 for an extensive
discussion).

instead of minimizing j(wt          j(wt)) we could
inexact line search:
simply look for a stepsize which results in su   cient decrease in the objective
function value. one popular set of su   cient decrease conditions is the wolfe
conditions
j(wt+1)     j(wt) + c1  t (cid:104)   j(wt), wt+1     wt(cid:105) (su   cient decrease)
(cid:104)   j(wt+1), wt+1     wt(cid:105)     c2 (cid:104)   j(wt), wt+1     wt(cid:105) (curvature)

(3.42)

(3.43)

with 0 < c1 < c2 < 1 (see figure 3.7). the wolfe conditions are also called
the armijio-goldstein conditions. if only su   cient decrease (3.42) alone is
enforced, then it is called the armijio rule.

fig. 3.7. the su   cient decrease condition (left) places an upper bound on the
acceptable stepsizes while the curvature condition (right) places a lower bound on
the acceptable stepsizes.

acceptable stepsizeacceptable stepsize106

3 optimization

algorithm 3.2 id119

1: input: initial point w0, gradient norm tolerance  
2: set t = 0
3: while (cid:107)   j(wt)(cid:107)       do
wt+1 = wt       t   j(wt)
t = t + 1
5:
6: end while
7: return: wt

4:

decaying stepsize:
instead of performing a line search at every itera-
   
tion, one can use a stepsize which decays according to a    xed schedule, for
example,   t = 1/
t. in section 3.2.4 we will discuss the decay schedule and
convergence rates of a generalized version of id119.

fixed stepsize: suppose j has a lipschitz continuous gradient with mod-
ulus l. using (3.22) and the id119 update wt+1 = wt       t   j(wt)
one can write

j(wt+1)     j(wt) + (cid:104)   j(wt), wt+1     wt(cid:105) +

(cid:107)wt+1     wt(cid:107)

l
2

= j(wt)       t (cid:107)   j(wt)(cid:107)2 +

l  2
t
2

(cid:107)   j(wt)(cid:107)2 .

(3.44)

(3.45)

minimizing (3.45) as a function of   t clearly shows that the upper bound on
j(wt+1) is minimized when we set   t = 1
l , which is the    xed stepsize rule.

theorem 3.12 suppose j has a lipschitz continuous gradient with modu-
lus l. then algorithm 3.2 with a    xed stepsize   t = 1
l will return a solution
wt with (cid:107)   j(wt)(cid:107)       in at most o(1/ 2) iterations.
proof plugging in   t = 1

l and rearranging (3.45) obtains
(cid:107)   j(wt)(cid:107)2     j(wt)     j(wt+1)

(3.46)

1
2l
summing this inequality

(cid:107)   j(wt)(cid:107)2     j(w0)     j(wt )     j(w0)     j(w   ),

which clearly shows that (cid:107)   j(wt)(cid:107)     0 as t        . furthermore, we can
write the following simple inequality:

(cid:114) 2l(j(w0)     j(w   ))

t + 1

.

(cid:107)   j(wt )(cid:107)    

t(cid:88)

t=0

1
2l

3.2 unconstrained smooth convex minimization

solving for

(cid:114) 2l(j(w0)     j(w   ))

107

=  

shows that t is o(1/ 2) as claimed.

t + 1

if in addition to having a lipschitz continuous gradient, if j is   -strongly
convex, then more can be said. first, one can translate convergence in
(cid:107)   j(wt)(cid:107) to convergence in function values. towards this end, use (3.17) to
write

j(wt)     j(w   ) +

(cid:107)   j(wt)(cid:107)2 .

1
2  

therefore, it follows that whenever (cid:107)   j(wt)(cid:107) <   we have j(wt)     j(w   ) <
 2/2  . furthermore, we can strengthen the rates of convergence.

theorem 3.13 assume everything as in theorem 3.12. moreover assume
that j is   -strongly convex, and let c := 1       
after at most

l . then j(wt)     j(w   )      

log((j(w0)     j(w   ))/ )

log(1/c)

(3.47)

iterations.
proof combining (3.46) with (cid:107)   j(wt)(cid:107)2     2  (j(wt)     j(w   )), and using
the de   nition of c one can write

c(j(wt)     j(w   ))     j(wt+1)     j(w   ).

applying the above equation recursively

ct (j(w0)     j(w   ))     j(wt )     j(w   ).

solving for

  = ct (j(w0)     j(w   ))

and rearranging yields (3.47).

when applied to practical problems which are not strongly convex gra-
dient descent yields a low accuracy solution within a few iterations. how-
ever, as the iterations progress the method    stalls    and no further increase
in accuracy is obtained because of the o(1/ 2) rates of convergence. on
the other hand, if the function is strongly convex, then id119
converges linearly, that is, in o(log(1/ )) iterations. however, the number

108

3 optimization

of iterations depends inversely on log(1/c). if we approximate log(1/c) =
    log(1       /l)       /l, then it shows that convergence depends on the ratio
l/  . this ratio is called the condition number of a problem. if the problem
is well conditioned, i.e.,        l then id119 converges extremely
fast. in contrast, if    (cid:28) l then id119 requires many iterations.
this is best illustrated with an example: consider the quadratic objective
function

(3.48)
where a     rn  n is a symmetric positive de   nite matrix, and b     rn is any
arbitrary vector.

j(w) =

w(cid:62)aw     bw,

1
2

recall that a twice di   erentiable function is   -strongly convex and has a
lipschitz continuous gradient with modulus l if and only if its hessian sat-
is   es li (cid:23)    2j(w) (cid:23)   i (see (3.14) and (3.21)). in the case of the quadratic
function (3.48)    2j(w) = a and hence    =   min and l =   max, where   min
(respectively   max) denotes the minimum (respectively maximum) eigen-
value of a. one can thus change the condition number of the problem by
varying the eigen-spectrum of the matrix a. for instance, if we set a to
the n    n identity matrix, then   max =   min = 1 and hence the problem is
well conditioned. in this case, id119 converges very quickly to the
optimal solution. we illustrate this behavior on a two dimensional quadratic
function in figure 3.8 (right).
on the other hand, if we choose a such that   max (cid:29)   min then the
problem (3.48) becomes ill-conditioned. in this case id119 exhibits
zigzagging and slow convergence as can be seen in figure 3.8 (left). because
of these shortcomings, id119 is not widely used in practice. a
number of di   erent algorithms we described below can be understood as
explicitly or implicitly changing the condition number of the problem to
accelerate convergence.

3.2.4 mirror descent

one way to motivate id119 is to use the following quadratic ap-
proximation of the objective function

qt(w) := j(wt) + (cid:104)   j(wt), w     wt(cid:105) +

(3.49)
where, as in the previous section,    j(  ) denotes the gradient of j. mini-
mizing this quadratic model at every iteration entails taking gradients with

(w     wt)(cid:62)(w     wt),

1
2

3.2 unconstrained smooth convex minimization

109

fig. 3.8. convergence of id119 with exact line search on two quadratic
problems (3.48). the problem on the left is ill-conditioned, whereas the problem
on the right is well-conditioned. we plot the contours of the objective function,
and the steps taken by id119. as can be seen id119 converges
fast on the well conditioned problem, while it zigzags and takes many iterations to
converge on the ill-conditioned problem.

respect to w and setting it to zero, which gives
w     wt :=       j(wt).

(3.50)
performing a line search along the direction       j(wt) recovers the familiar
id119 update

wt+1 = wt       t   j(wt).

(3.51)

the closely related mirror descent method replaces the quadratic penalty
in (3.49) by a bregman divergence de   ned by some convex function f to
yield

qt(w) := j(wt) + (cid:104)   j(wt), w     wt(cid:105) +    f (w, wt).

(3.52)
computing the gradient, setting it to zero, and using    w   f (w, wt) =    f (w)   
   f (wt), the minimizer of the above model can be written as

   f (w)        f (wt) =       j(wt).

(3.53)

as before, by using a stepsize   t the resulting updates can be written as

wt+1 =    f   1(   f (wt)       t   j(wt)).

(3.54)

it is easy to verify that choosing f (  ) = 1
descent updates. on the other hand if we choose f to be the un-normalized
id178 (3.30) then    f (  ) = log and therefore (3.54) specializes to

2 (cid:107)  (cid:107)2 recovers the usual gradient

wt+1 = exp(log(wt)       t   j(wt)) = wt exp(     t   j(wt)),

(3.55)

which is sometimes called the exponentiated gradient (eg) update.

110
3 optimization
theorem 3.14 let j be a convex function and j(w   ) denote its minimum
value. the mirror descent updates (3.54) with a   -strongly convex function
f satisfy

   f (w   , w1) + 1

t (cid:107)   j(wt)(cid:107)2

t   2

    min

t

j(wt)     j(w   ).

(cid:80)

t   t

(cid:80)

2  

proof using the convexity of j (see (3.7)) and (3.54) we can write

j(w   )     j(wt) + (cid:104)w        wt,   j(wt)(cid:105)

    j(wt)     1
  t

(cid:104)w        wt, f (wt+1)     f (wt)(cid:105) .

now applying lemma 3.11 and rearranging

   f (w   , wt)        f (w   , wt+1) +    f (wt, wt+1)       t(j(wt)     j(w   )).

(cid:88)

   f (wt, wt+1)    (cid:88)

summing over t = 1, . . . , t
   f (w   , w1)        f (w   , wt +1) +
  t(j(wt)     j(w   )).
noting that    f (w   , wt +1)     0, j(wt)     j(w   )     mint j(wt)     j(w   ), and
rearranging it follows that

t

t

t    f (wt, wt+1)

    min

t

j(wt)     j(w   ).

(3.56)

   f (w   , w1) +(cid:80)
(cid:80)

t   t

using (3.17) and (3.54)

   f (wt, wt+1)     1
2  

(cid:107)   f (wt)        f (wt+1)(cid:107)2 =

t (cid:107)   j(wt)(cid:107)2 .
  2

1
2  

(3.57)

the proof is completed by plugging in (3.57) into (3.56).

corollary 3.15 if j has a lipschitz continuous gradient with modulus l,
and the stepsizes   t are chosen as

(cid:112)2     f (w   , w1)
(cid:114)

1   
t

then
2   f (w   , w1)

  

  t =

l
j(wt)     j(w   )     l
proof since    j is lipschitz continuous

min
1   t   t

(cid:80)
j(wt)     j(w   )        f (w   , w1) + 1

2  
t   t

min
1   t   t

(3.58)

.

1   
t

(cid:80)

t   2

t l2

.

3.2 unconstrained smooth convex minimization

plugging in (3.58) and using problem 3.15

j(wt)     j(w   )     l

min
1   t   t

   f (w   , w1)

2  

(cid:114)

(1 +(cid:80)
(cid:80)

t
1   
t

t

(cid:114)

1
t )

    l

111

   f (w   , w1)

2  

1   
t

.

3.2.5 conjugate gradient

let us revisit the problem of minimizing the quadratic objective function
(3.48). since    j(w) = aw    b, at the optimum    j(w) = 0 (see lemma 3.6)
and hence

aw = b.

(3.59)

in fact, the conjugate gradient (cg) algorithm was    rst developed as a
method to solve the above linear system.

as we already saw, updating w along the negative gradient direction may

lead to zigzagging. therefore cg uses the so-called conjugate directions.

t(cid:48) apt = 0 if t (cid:54)= t(cid:48).

de   nition 3.16 (conjugate directions) non zero vectors pt and pt(cid:48) are
said to be conjugate with respect to a symmetric positive de   nite matrix a
if p(cid:62)
conjugate directions {p0, . . . , pn   1} are linearly independent and form a
basis. to see this, suppose the pt   s are not linearly independent. then there
t   tpt = 0. the pt   s are conjugate
t(cid:48) apt(cid:48) = 0 for all t(cid:48).
t(cid:48) apt =   t(cid:48)p(cid:62)
directions, therefore p(cid:62)
since a is positive de   nite this implies that   t(cid:48) = 0 for all t(cid:48), a contradiction.
as it turns out, the conjugate directions can be generated iteratively as
follows: starting with any w0     rn de   ne p0 =    g0 = b     aw0, and set

exists non-zero coe   cients   t such that(cid:80)
t   tpt) =(cid:80)

t(cid:48) a((cid:80)

t   tp(cid:62)

  t =     g(cid:62)
t pt
p(cid:62)
t apt
wt+1 = wt +   tpt
gt+1 = awt+1     b

g(cid:62)
t+1apt
p(cid:62)
t apt

  t+1 =
pt+1 =    gt+1 +   t+1pt

(3.60a)

(3.60b)

(3.60c)

(3.60d)

(3.60e)

112

3 optimization

the following theorem asserts that the pt generated by the above procedure
are indeed conjugate directions.

theorem 3.17 suppose the t-th iterate generated by the conjugate gradient
method (3.60) is not the solution of (3.59), then the following properties
hold:

span{g0, g1, . . . , gt} = span{g0, ag0, . . . , atg0}.
span{p0, p1, . . . , pt} = span{g0, ag0, . . . , atg0}.

p(cid:62)
j gt = 0 for all j < t
p(cid:62)
j apt = 0 for all j < t.

(3.61)

(3.62)

(3.63)

(3.64)

proof the proof is by induction. the induction hypothesis holds trivially
at t = 0. assuming that (3.61) to (3.64) hold for some t, we prove that they
continue to hold for t + 1.

step 1: we    rst prove that (3.63) holds. using (3.60c), (3.60b) and (3.60a)

j gt+1 = p(cid:62)
p(cid:62)
= p(cid:62)
= p(cid:62)

j (awt+1     b)
(cid:18)
j (awt +   tpt     b)
awt     g(cid:62)
t pt
p(cid:62)
t apt
j gt     p(cid:62)
j apt
g(cid:62)
t pt.
p(cid:62)
t apt

j

= p(cid:62)

(cid:19)

apt     b

for j = t, both terms cancel out, while for j < t both terms vanish due to
the induction hypothesis.

step 2: next we prove that (3.61) holds. using (3.60c) and (3.60b)

gt+1 = awt+1     b = awt +   tapt     b = gt +   tapt.

by our induction hypothesis, gt     span{g0, ag0, . . . , atg0}, while apt    
span{ag0, a2g0, . . . , at+1g0}. combining the two we conclude that gt+1    
span{g0, ag0, . . . , at+1g0}. on the other hand, we already showed that gt+1
is orthogonal to {p0, p1, . . . , pt}. therefore, gt+1 /    span{p0, p1, . . . , pt}. thus
our induction assumption implies that gt+1 /    span{g0, ag0, . . . , atg0}. this
allows us to conclude that span{g0, g1, . . . , gt+1} = span{g0, ag0, . . . , at+1g0}.

3.2 unconstrained smooth convex minimization

113

step 3 we now prove (3.64) holds. using (3.60e)
t+1apj +   t+1p(cid:62)

t+1apj =    g(cid:62)
p(cid:62)

t apj.

by the de   nition of   t+1 (3.60d) the above expression vanishes for j = t. for
j < t, the    rst term is zero because apj     span{p0, p1, . . . , pj+1}, a subspace
orthogonal to gt+1 as already shown in step 1. the induction hypothesis
guarantees that the second term is zero.

step 4 clearly, (3.61) and (3.60e) imply (3.62). this concludes the proof.

a practical implementation of (3.60) requires two more observations:

first, using (3.60e) and (3.63)

   g(cid:62)

t pt = g(cid:62)

t gt       tg(cid:62)

t pt   1 = g(cid:62)

t gt.

therefore (3.60a) simpli   es to

  t =

g(cid:62)
t gt
p(cid:62)
t apt

.

(3.65)

second, using (3.60c) and (3.60b)

gt+1     gt = a(wt+1     wt) =   tapt.

but gt     span{p0, . . . , pt}, a subspace orthogonal to gt+1 by (3.63). therefore
g(cid:62)
t+1apt = 1
t+1gt+1). substituting this back into (3.60d) and using (3.65)
  t
yields

(g(cid:62)

  t+1 =

g(cid:62)
t+1gt+1
g(cid:62)
t gt

.

(3.66)

we summarize the cg algorithm in algorithm 3.3. unlike id119
whose convergence rates for minimizing the quadratic objective function
(3.48) depend upon the condition number of a, as the following theorem
shows, the cg iterates converge in at most n steps.

theorem 3.18 the cg iterates (3.60) converge to the minimizer of (3.48)
after at most n steps.

proof let w denote the minimizer of (3.48). since the pt   s form a basis

w     w0 =   0p0 + . . . +   n   1pn   1,

for some scalars   t. our proof strategy will be to show that the coe   cients

114

3 optimization

algorithm 3.3 conjugate gradient
1: input: initial point w0, residual norm tolerance  
2: set t = 0, g0 = aw0     b, and p0 =    g0
3: while (cid:107)awt     b(cid:107)       do

5:

4:

7:

6:

  t = g(cid:62)
t gt
p(cid:62)
t apt
wt+1 = wt +   tpt
gt+1 = gt +   tapt
  t+1 =
pt+1 =    gt+1 +   t+1pt
t = t + 1
9:
10: end while
11: return: wt

g(cid:62)
t+1gt+1
g(cid:62)
t gt

8:

  t coincide with   t de   ned in (3.60a). towards this end premultiply with
p(cid:62)
t a and use conjugacy to obtain

  t =

t a(w     w0)
p(cid:62)

p(cid:62)
t apt

.

(3.67)

on the other hand, following the iterative process (3.60b) from w0 until wt
yields

wt     w0 =   0p0 + . . . +   t   1pt   1.
again premultiplying with p(cid:62)
t a and using conjugacy
t a(wt     w0) = 0.
p(cid:62)

substituting (3.68) into (3.67) produces
t a(w     wt)
p(cid:62)

  t =

p(cid:62)
t apt

=     g(cid:62)
t pt
p(cid:62)
t apt

,

(3.68)

(3.69)

thus showing that   t =   t.

observe that the gt+1 computed via (3.60c) is nothing but the gradient of
j(wt+1). furthermore, consider the following one dimensional optimization
problem:

     r   t(  ) := j(wt +   pt).
min

di   erentiating   t with respect to   

  (cid:48)
t(  ) = p(cid:62)

t (awt +   apt     b) = p(cid:62)

t (gt +   apt).

3.2 unconstrained smooth convex minimization
the gradient vanishes if we set    =     g(cid:62)
t pt
, which recovers (3.60a). in other
p(cid:62)
t apt
words, every iteration of cg minimizes j(w) along a conjugate direction pt.
contrast this with id119 which minimizes j(w) along the negative
gradient direction gt at every iteration.

115

it is natural to ask if this idea of generating conjugate directions and
minimizing the objective function along these directions can be applied to
general convex functions. the main di   culty here is that theorems 3.17 and
3.18 do not hold. in spite of this, extensions of cg are e   ective even in this
setting. basically the update rules for gt and pt remain the same, but the
parameters   t and   t are computed di   erently. table 3.2 gives an overview
of di   erent extensions. see [nw99, lue84] for details.

table 3.2. non-quadratic modi   cations of conjugate id119

generic method compute hessian kt :=    2j(wt) and update   t

and   t with
  t =     g(cid:62)
t pt
p(cid:62)
t ktpt

and   t =     g(cid:62)
t+1ktpt
p(cid:62)
t ktpt

fletcher-reeves

polak-ribi`ere

hestenes-stiefel

g(cid:62)
t+1gt+1
g(cid:62)
t gt

.

set   t = argmin   j(wt +   pt) and   t =
.
set   t = argmin   j(wt +   pt), yt = gt+1     gt, and
  t = y(cid:62)
t gt+1
g(cid:62)
t gt
in practice, polak-ribi`ere tends to be better than
fletcher-reeves.
set   t = argmin   j(wt +   pt), yt = gt+1     gt, and
  t = y(cid:62)
t gt+1
y(cid:62)
t pt

.

3.2.6 higher order methods

recall the motivation for id119 as the minimizer of the quadratic
model

qt(w) := j(wt) + (cid:104)   j(wt), w     wt(cid:105) +

(w     wt)(cid:62)(w     wt),

1
2

the quadratic penalty in the above equation uniformly penalizes deviation
from wt in di   erent dimensions. when the function is ill-conditioned one
would intuitively want to penalize deviations in di   erent directions di   er-
ently. one way to achieve this is by using the hessian, which results in the

116

3 optimization

algorithm 3.4 newton   s method

1: input: initial point w0, gradient norm tolerance  
2: set t = 0
3: while (cid:107)   j(wt)(cid:107) >   do

4:

6:

5:

compute pt :=       2j(wt)   1   j(wt)
compute   t = argmin   j(wt +   pt) e.g., via algorithm 3.1.
wt+1 = wt +   tpt
t = t + 1
7:
8: end while
9: return: wt

following second order taylor approximation:

qt(w) := j(wt) + (cid:104)   j(wt), w     wt(cid:105) +

1
2

(w     wt)(cid:62)   2j(wt)(w     wt).

(3.70)

of course, this requires that j be twice di   erentiable. we will also assume
that j is strictly convex and hence its hessian is positive de   nite and in-
vertible. minimizing qt by taking gradients with respect to w and setting it
zero obtains

w     wt :=       2j(wt)   1   j(wt),

(3.71)

since we are only minimizing a model of the objective function, we perform
a line search along the descent direction (3.71) to compute the stepsize   t,
which yields the next iterate:

wt+1 = wt       t   2j(wt)   1   j(wt).

(3.72)

details can be found in algorithm 3.4.
suppose w    denotes the minimum of j(w). we say that an algorithm
exhibits quadratic convergence if the sequences of iterates {wk} generated
by the algorithm satis   es:

(cid:107)wk+1     w   (cid:107)     c (cid:107)wk     w   (cid:107)2

(3.73)

for some constant c > 0. we now show that newton   s method exhibits
quadratic convergence close to the optimum.

theorem 3.19 (quadratic convergence of newton   s method) suppose
j is twice di   erentiable, strongly convex, and the hessian of j is bounded
and lipschitz continuous with modulus m in a neighborhood of the so-

lution w   . furthermore, assume that (cid:13)(cid:13)   2j(w)   1(cid:13)(cid:13)     n . the iterations

(cid:90) 1

3.2 unconstrained smooth convex minimization

117

wt+1 = wt        2j(wt)   1   j(wt) converge quadratically to w   , the minimizer
of j.

proof first notice that
   j(wt)        j(w   ) =

   2j(wt + t(w        wt))(wt     w   )dt.

(3.74)
next using the fact that    2j(wt) is invertible and the gradient vanishes at
the optimum (   j(w   ) = 0), write
wt+1     w    = wt     w           2j(wt)   1   j(wt)

0

=    2j(wt)   1[   2j(wt)(wt     w   )     (   j(wt)        j(w   ))]. (3.75)

using (3.75), (3.74), and the lipschitz continuity of    2j

(cid:13)(cid:13)   j(wt)        j(w   )        2j(wt)(wt     w   )(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:90) 1
(cid:13)(cid:13)(cid:13)(cid:13)
(cid:90) 1
(cid:13)(cid:13)[   2j(wt + t(wt     w   ))        2j(wt)](cid:13)(cid:13)(cid:107)(wt     w   )(cid:107) dt

[   2j(wt + t(wt     w   ))        2j(wt)](wt     w   )dt

   

=

0

0

    (cid:107)wt     w   (cid:107)2

(cid:107)wt+1     w   (cid:107)     m
2

m t dt =

(cid:90) 1
(cid:13)(cid:13)   2j(wt)   1(cid:13)(cid:13)(cid:107)wt     w   (cid:107)2     n m

(cid:107)wt     w   (cid:107)2 .

m
2

0

2

finally use (3.75) and (3.76) to conclude that

(3.76)

(cid:107)wt     w   (cid:107)2.

newton   s method as we described it su   ers from two major problems.
first, it applies only to twice di   erentiable, strictly convex functions. sec-
ond, it involves computing and inverting of the n    n hessian matrix at
every iteration, thus making it computationally very expensive. although
newton   s method can be extended to deal with positive semi-de   nite hes-
sian matrices, the computational burden often makes it unsuitable for large
scale applications. in such cases one resorts to quasi-id77s.

3.2.6.1 quasi-id77s

unlike newton   s method, which computes the hessian of the objective func-
tion at every iteration, quasi-id77s never compute the hessian;
they approximate it from past gradients. since they do not require the ob-
jective function to be twice di   erentiable, quasi-id77s are much

118

3 optimization

fig. 3.9. the blue solid line depicts the one dimensional convex function j(w) =
w4 + 20w2 + w. the green dotted-dashed line represents the    rst order taylor
approximation to j(w), while the red dashed line represents the second order taylor
approximation, both evaluated at w = 2.

more widely applicable. they are widely regarded as the workhorses of
smooth nonlinear optimization due to their combination of computational ef-
   ciency and good asymptotic convergence. the most popular quasi-newton
algorithm is bfgs, named after its discoverers broyde, fletcher, goldfarb,
and shanno. in this section we will describe bfgs and its limited memory
counterpart lbfgs.
suppose we are given a smooth (not necessarily strictly) convex objective
function j : rn     r and a current iterate wt     rn. just like newton   s
method, bfgs forms a local quadratic model of the objective function, j:

qt(w) := j(wt) + (cid:104)   j(wt), w     wt(cid:105) +

(w     wt)(cid:62)ht(w     wt).

(3.77)

1
2

unlike newton   s method which uses the hessian to build its quadratic model
(3.70), bfgs uses the matrix ht (cid:31) 0, which is a positive-de   nite estimate
of the hessian. a quasi-newton direction of descent is found by minimizing
qt(w):

w     wt =    h   1

t    j(wt).

(3.78)

the stepsize   t > 0 is found by a line search obeying the wolfe conditions

64202464002000200400600800100012003.2 unconstrained smooth convex minimization

(3.42) and (3.43). the    nal update is given by

wt+1 = wt       th   1

t    j(wt).

119

(3.79)

given wt+1 we need to update our quadratic model (3.77) to
qt+1(w) := j(wt+1) + (cid:104)   j(wt+1), w     wt+1(cid:105) +

(w     wt+1)(cid:62)ht+1(w     wt+1).

1
2

(3.80)

when updating our model it is reasonable to expect that the gradient of
qt+1 should match the gradient of j at wt and wt+1. clearly,
   qt+1(w) =    j(wt+1) + ht+1(w     wt+1),

(3.81)
which implies that    qt+1(wt+1) =    j(wt+1), and hence our second con-
dition is automatically satis   ed. in order to satisfy our    rst condition, we
require

   qt+1(wt) =    j(wt+1) + ht+1(wt     wt+1) =    j(wt).

(3.82)

by rearranging, we obtain the so-called secant equation:

ht+1st = yt,

(3.83)
where st := wt+1     wt and yt :=    j(wt+1)      j(wt) denote the most recent
step along the optimization trajectory in parameter and gradient space,
respectively. since ht+1 is a positive de   nite matrix, pre-multiplying the
secant equation by st yields the curvature condition

s(cid:62)
t yt > 0.

(3.84)

if the curvature condition is satis   ed, then there are an in   nite number
of matrices ht+1 which satisfy the secant equation (the secant equation
represents n linear equations, but the symmetric matrix ht+1 has n(n+1)/2
degrees of freedom). to resolve this issue we choose the closest matrix to
ht which satis   es the secant equation. the key insight of the bfgs comes
from the observation that the descent direction computation (3.78) involves
the inverse matrix bt := h   1
. therefore, we choose a matrix bt+1 := h   1
such that it is close to bt and also satis   es the secant equation:

t+1

t

(cid:107)b     bt(cid:107)

b

min
s. t. b = b(cid:62) and byt = st.

(3.85)

(3.86)
if the matrix norm (cid:107)  (cid:107) is appropriately chosen [nw99], then it can be shown
that

bt+1 = (1     tsty(cid:62)

t )bt(1     tyts(cid:62)

t ) +   tsts(cid:62)
t ,

(3.87)

120

algorithm 3.5 lbfgs

3 optimization

6:

5:

4:

1: input: initial point w0, gradient norm tolerance   > 0
2: set t = 0 and b0 = i
3: while (cid:107)   j(wt)(cid:107) >   do
pt =    bt   j(wt)
find   t that obeys (3.42) and (3.43)
st =   tpt
wt+1 = wt + st
yt :=    j(wt+1)        j(wt)
if t = 0 : bt := s(cid:62)
t yt
y(cid:62)
t yt
  t = (s(cid:62)
t )bt(i       tyts(cid:62)
11: bt+1 = (i       tsty(cid:62)

t ) +   tsts(cid:62)

t yt)   1

10:

7:

8:

9:

i

t

t = t + 1
12:
13: end while
14: return: wt

where   t := (y(cid:62)
t st)   1. in other words, the matrix bt is modi   ed via an
incremental rank-two update, which is very e   cient to compute, to obtain
bt+1.

there exists an interesting connection between the bfgs update (3.87)
and the hestenes-stiefel variant of conjugate gradient. to see this assume
that an exact line search was used to compute wt+1, and therefore s(cid:62)
0. furthermore, assume that bt = 1, and use (3.87) to write
t    j(wt+1)
y(cid:62)

pt+1 =    bt+1   j(wt+1) =       j(wt+1) +

(3.88)

st,

t    j(wt+1) =

y(cid:62)
t st

which recovers the hestenes-stiefel update (see (3.60e) and table 3.2).

limited-memory bfgs (lbfgs) is a variant of bfgs designed for solv-
ing large-scale optimization problems where the o(d2) cost of storing and
updating bt would be prohibitively expensive. lbfgs approximates the
quasi-newton direction (3.78) directly from the last m pairs of st and yt via
a matrix-free approach. this reduces the cost to o(md) space and time per
iteration, with m freely chosen. details can be found in algorithm 3.5.

3.2.6.2 spectral gradient methods

although spectral gradient methods do not use the hessian explicitly, they
are motivated by arguments very reminiscent of the quasi-id77s.
recall the update rule (3.79) and secant equation (3.83). suppose we want

3.2 unconstrained smooth convex minimization

121

a very simple matrix which approximates the hessian. speci   cally, we want

ht+1 =   t+1i

(3.89)

where   t+1 is a scalar and i denotes the identity matrix. then the secant
equation (3.83) becomes

  t+1st = yt.

(3.90)

in general, the above equation cannot be solved. therefore we use the   t+1
which minimizes (cid:107)  t+1st     yt(cid:107)2 which yields the barzilai-borwein (bb) step-
size

  t+1 =

s(cid:62)
t yt
s(cid:62)
t st

.

(3.91)

as it turns out,   t+1 lies between the minimum and maximum eigenvalue of
the average hessian in the direction st, hence the name spectral gradient
method. the parameter update (3.79) is now given by

wt+1 = wt     1
  t

   j(wt).

(3.92)

a practical implementation uses safeguards to ensure that the stepsize   t+1
is neither too small nor too large. given 0 <   min <   max <     we compute

(cid:18)

(cid:18)

(cid:19)(cid:19)

s(cid:62)
t yt
s(cid:62)
t st

  t+1 = min

  max, max

  min,

.

(3.93)

one of the peculiar features of spectral gradient methods is their use
of a non-monotone line search. in all the algorithms we have seen so far,
the stepsize is chosen such that the objective function j decreases at every
iteration. in contrast, non-monotone line searches employ a parameter m    
1 and ensure that the objective function decreases in every m iterations. of
course, setting m = 1 results in the usual monotone line search. details can
be found in algorithm 3.6.

3.2.7 bundle methods

the methods we discussed above are applicable for minimizing smooth, con-
vex objective functions. some regularized risk minimization problems involve
a non-smooth objective function. in such cases, one needs to use bundle
methods. in order to lay the ground for bundle methods we    rst describe
their precursor the cutting plane method [kel60]. cutting plane method is
based on a simple observation: a convex function is bounded from below by

122

3 optimization

algorithm 3.6 spectral gradient method
1: input: w0, m     1,   max >   min > 0,        (0, 1), 1 >   2 >   1 > 0,

  0     [  min,   max], and   > 0

2: initialize: t = 0
3: while (cid:107)   j(wt)(cid:107) >   do

   = 1

4:
5: while true do
dt =     1
   j(wt)
6:
w+ = wt +   dt
   = (cid:104)dt,   j(wt)(cid:105)
if j(w+)     min0   j   min(t,m   1) j(xt   j) +        then

  t

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

wt+1 = w+
st = wt+1     wt
yt =    j(wt+1)        j(wt)
break
  tmp =     1
if   tmp >   1 and   tmp <   2   then

2   2  /(j(w+)     j(wt)         )

else

   =   tmp

else

   =   /2

end if

end if

22:

23:

end while
  t+1 = min(  max, max(  min, s(cid:62)
t yt
s(cid:62)
t st
t = t + 1
24:
25: end while
26: return: wt

))

its linearization (i.e.,    rst order taylor approximation). see figures 3.4 and
3.5 for geometric intuition, and recall (3.7) and (3.13):

j(w)     j(w(cid:48)) +(cid:10)w     w(cid:48), s(cid:48)(cid:11)

   w and s(cid:48)        j(w(cid:48)).

(3.94)

given subgradients s1, s2, . . . , st evaluated at locations w0, w1, . . . , wt   1, we
can construct a tighter (piecewise linear) lower bound for j as follows (also
see figure 3.10):

j(w)     j cp

t

(w) := max
1   i   t

{j(wi   1) + (cid:104)w     wi   1, si(cid:105)}.

(3.95)

3.2 unconstrained smooth convex minimization

given iterates {wi}t   1
the next iterate wt:

i=0, the cutting plane method minimizes j cp

t

wt := argmin

w

j cp
t

(w).

123

to obtain

(3.96)

this iteratively re   nes the piecewise linear lower bound j cp and allows us
to get close to the minimum of j (see figure 3.10 for an illustration).
if w    denotes the minimizer of j, then clearly each j(wi)     j(w   ) and
hence min0   i   t j(wi)     j(w   ). on the other hand, since j     j cp
it fol-
lows that j(w   )     j cp
(wt). in other words, j(w   ) is sandwiched between
min0   i   t j(wi) and j cp
(wt) (see figure 3.11 for an illustration). the cutting
plane method monitors the monotonically decreasing quantity

t

t

t

 t := min
0   i   t

j(wi)     j cp

t

(wt),

(3.97)

and terminates whenever  t falls below a prede   ned threshold  . this ensures
that the solution j(wt) is   optimum, that is, j(wt)     j(w   ) +  .

fig. 3.10. a convex function (blue solid curve) is bounded from below by its lin-
earizations (dashed lines). the gray area indicates the piecewise linear lower bound
obtained by using the linearizations. we depict a few iterations of the cutting plane
method. at each iteration the piecewise linear lower bound is minimized and a new
linearization is added at the minimizer (red rectangle). as can be seen, adding more
linearizations improves the lower bound.

although cutting plane method was shown to be convergent [kel60], it is

124

3 optimization

fig. 3.11. a convex function (blue solid curve) with four linearizations evaluated at
four di   erent locations (magenta circles). the approximation gap  3 at the end of
fourth iteration is indicated by the height of the cyan horizontal band i.e., di   erence
between lowest value of j(w) evaluated so far and the minimum of j cp
(w) (red
diamond).

4

well known (see e.g., [lnn95, bel05]) that it can be very slow when new
iterates move too far away from the previous ones (i.e., causing unstable
   zig-zag    behavior in the iterates). in fact, in the worst case the cutting
plane method might require exponentially many steps to converge to an  
optimum solution.

t

bundle methods stabilize cpm by augmenting the piecewise linear lower
(e.g., j cp
(w) in (3.95)) with a prox-function (i.e., proximity control func-
tion) which prevents overly large steps in the iterates [kiw90]. roughly
speaking, there are 3 popular types of bundle methods, namely, proximal
[kiw90], trust region [sz92], and level set [lnn95]. all three versions use
1

2 (cid:107)  (cid:107)2 as their prox-function, but di   er in the way they compute the new

iterate:

proximal: wt := argmin

trust region: wt := argmin

level set: wt := argmin

t

(w)},

(cid:107)w       wt   1(cid:107)2 + j cp
(w) | 1
2
(cid:107)w       wt   1(cid:107)2 | j cp

(cid:107)w       wt   1(cid:107)2       t},
(w)       t},

{   t
2
{j cp
{ 1
2

t

t

w

w

w

(3.98)

(3.99)

(3.100)

where   wt   1 is the current prox-center, and   t,   t, and   t are positive trade-
o    parameters of the stabilization. although (3.98) can be shown to be
equivalent to (3.99) for appropriately chosen   t and   t, tuning   t is rather
di   cult while a trust region approach can be used for automatically tuning

3.3 constrained optimization

125

  t. consequently the trust region algorithm bt of [sz92] is widely used in
practice.

3.3 constrained optimization

so far our focus was on unconstrained optimization problems. many ma-
chine learning problems involve constraints, and can often be written in the
following canonical form:

w

j(w)

min
s. t. ci(w)     0 for i     i
ei(w) = 0 for i     e

(3.101a)

(3.101b)

(3.101c)

where both ci and ei are convex functions. we say that w is feasible if and
only if it satis   es the constraints, that is, ci(w)     0 for i     i and ei(w) = 0
for i     e.
recall that w is the minimizer of an unconstrained problem if and only if
(cid:107)   j(w)(cid:107) = 0 (see lemma 3.6). unfortunately, when constraints are present
one cannot use this simple characterization of the solution. for instance, the
w at which (cid:107)   j(w)(cid:107) = 0 may not be a feasible point. to illustrate, consider
the following simple minimization problem (see figure 3.12):

1
2

w2

w

min
s. t. 1     w     2.

(3.102a)

(3.102b)

clearly, 1
2 w2 is minimized at w = 0, but because of the presence of the con-
straints, the minimum of (3.102) is attained at w = 1 where    j(w) = w is
equal to 1. therefore, we need other ways to detect convergence. in section
3.3.1 we discuss some general purpose algorithms based on the concept of or-
thogonal projection. in section 3.3.2 we will discuss lagrange duality, which
can be used to further characterize the solutions of constrained optimization
problems.

3.3.1 projection based methods

suppose we are interested in minimizing a smooth convex function of the
following form:

j(w),

min
w      

(3.103)

126

3 optimization

fig. 3.12. the unconstrained minimum of the quadratic function 1
2 w2 is attained
at w = 0 (red circle). but, if we enforce the constraints 1     w     2 (illustrated by
the shaded area) then the minimizer is attained at w = 1 (green diamond).

where     is a convex feasible region. for instance,     may be described by
convex functions ci and ei as in (3.101). the algorithms we describe in this
section are applicable when     is a relatively simple set onto which we can
compute an orthogonal projection. given a point w(cid:48) and a feasible region
   , the orthogonal projection p   (w(cid:48)) of w(cid:48) on     is de   ned as

(cid:13)(cid:13)w(cid:48)     w(cid:13)(cid:13)2 .

p   (w(cid:48)) := argmin
w      

(3.104)

geometrically speaking, p   (w(cid:48)) is the closest point to w(cid:48) in    . of course, if
w(cid:48)         then p   (w(cid:48)) = w(cid:48).
a w         such that

we are interested in    nding an approximate solution of (3.103), that is,

j(w)     min
w      

j(w) = j(w)     j         ,

(3.105)

for some pre-de   ned tolerance   > 0. of course, j    is unknown and hence the
gap j(w)     j    cannot be computed in practice. furthermore, as we showed
in section 3.3, for constrained optimization problems (cid:107)   j(w)(cid:107) does not
vanish at the optimal solution. therefore, we will use the following stopping

6420246w02468101214j(w)3.3 constrained optimization

127

algorithm 3.7 basic projection based method
1: input: initial point w0        , and projected gradient norm tolerance

  > 0

2: initialize: t = 0
3: while (cid:107)p   (wt        j(wt))     wt(cid:107) >   do

5:

4:

find direction of descent dt
wt+1 = p   (wt +   tdt)
t = t + 1
6:
7: end while
8: return: wt

criterion in our algorithms

(cid:107)p   (wt        j(wt))     wt(cid:107)      .

(3.106)
the intuition here is as follows: if wt        j(wt)         then p   (wt    
   j(wt)) = wt if, and only if,    j(wt) = 0, that is, wt is the global minimizer
of j(w). on the other hand, if wt       j(wt) /        but p   (wt       j(wt)) = wt,
then the constraints are preventing us from making any further progress
along the descent direction       j(wt) and hence we should stop.

the basic projection based method is described in algorithm 3.7. any
unconstrained optimization algorithm can be used to generate the direction
of descent dt. a line search is used to    nd the stepsize   t. the updated
parameter wt       tdt is projected onto     to obtain wt+1. if dt is chosen to
be the negative gradient direction       j(wt), then the resulting algorithm
is called the projected gradient method. one can show that the rates of
convergence of id119 with various line search schemes is also
preserved by projected id119.

3.3.2 lagrange duality

lagrange duality plays a central role in constrained id76.
the basic idea here is to augment the objective function (3.101) with a
weighted sum of the constraint functions by de   ning the lagrangian:

l(w,   ,   ) = j(w) +

  ici(w) +

  iei(w)

(3.107)

(cid:88)

i   i

(cid:88)

i   e

for   i     0 and   i     r. in the sequel, we will refer to    (respectively   ) as the
lagrange multipliers associated with the inequality (respectively equality)
constraints. furthermore, we will call    and    dual feasible if and only if

128

3 optimization

  i     0 and   i     r. the lagrangian satis   es the following fundamental
property, which makes it extremely useful for constrained optimization.

(cid:40)

(cid:88)

i   e

(cid:88)

i   i

theorem 3.20 the lagrangian (3.107) of (3.101) satis   es

max
     0,  

l(w,   ,   ) =

j(w) if w is feasible
    otherwise.

in particular, if j    denotes the optimal value of (3.101), then

j    = min

w

max
     0,  

l(w,   ,   ).

proof first assume that w is feasible, that is, ci(w)     0 for i     i and
ei(w) = 0 for i     e. since   i     0 we have

  ici(w) +

  iei(w)     0,

(3.108)

(cid:88)

i   i

with equality being attained by setting   i = 0 whenever ci(w) < 0. conse-
quently,

max
     0,  

l(w,   ,   ) = max
     0,  

j(w) +

  ici(w) +

  iei(w) = j(w)

(cid:88)

i   e

whenever w is feasible. on the other hand, if w is not feasible then either
ci(cid:48)(w) > 0 or ei(cid:48)(w) (cid:54)= 0 for some i(cid:48). in the    rst case simply let   i(cid:48)         to
see that max     0,   l(w,   ,   )        . similarly, when ei(cid:48)(w) (cid:54)= 0 let   i(cid:48)        
if ei(cid:48)(w) > 0 or   i(cid:48)            if ei(cid:48)(w) < 0 to arrive at the same conclusion.
if de   ne the lagrange dual function

(3.109)
for        0 and   , then one can prove the following property, which is often
called as weak duality.

d(  ,   ) = min

l(w,   ,   ),

w

theorem 3.21 (weak duality) the lagrange dual function (3.109) sat-
is   es

for all feasible w and        0 and   . in particular

d(  ,   )     j(w)

d    := max
     0,  

min

w

l(w,   ,   )     min

w

max
     0,  

l(w,   ,   ) = j   .

(3.110)

3.3 constrained optimization

proof as before, observe that whenever w is feasible
  iei(w)     0.

  ici(w) +

(cid:88)

i   i

therefore

d(  ,   ) = min

w

l(w,   ,   ) = min

w

j(w) +

  ici(w) +

(cid:88)

i   e

(cid:88)

i   i

129

  iei(w)     j(w)

(cid:88)

i   e

for all feasible w and        0 and   . in particular, one can choose w to be
the minimizer of (3.101) and        0 and    to be maximizers of d(  ,   ) to
obtain (3.110).

weak duality holds for any arbitrary function, not-necessarily convex. when
the objective function and constraints are convex, and certain technical con-
ditions, also known as slater   s conditions hold, then we can say more.

theorem 3.22 (strong duality) supposed the objective function f and
constraints ci for i     i and ei for i     e in (3.101) are convex and the
following constraint quali   cation holds:

there exists a w such that ci(w) < 0 for all i     i.

then the lagrange dual function (3.109) satis   es

d    := max
     0,  

min

w

l(w,   ,   ) = min

w

max
     0,  

l(w,   ,   ) = j   .

(3.111)

the proof of the above theorem is quite technical and can be found in
any standard reference (e.g., [bv04]). therefore we will omit the proof and
proceed to discuss various implications of strong duality. first note that

min

w

max
     0,  

l(w,   ,   ) = max
     0,  

min

w

l(w,   ,   ).

(3.112)

in other words, one can switch the order of minimization over w with max-
imization over    and   . this is called the saddle point property of convex
functions.
suppose strong duality holds. given any        0 and    such that d(  ,   ) >
       and a feasible w we can immediately write the duality gap

j(w)     j    = j(w)     d        j(w)     d(  ,   ),

where j    and d    were de   ned in (3.111). below we show that if w    is primal
optimal and (     ,      ) are dual optimal then j(w   )     d(     ,      ) = 0. this
provides a non-heuristic stopping criterion for constrained optimization: stop
when j(w)     d(  ,   )      , where   is a pre-speci   ed tolerance.

130

3 optimization
suppose the primal and dual optimal values are attained at w    and

(     ,      ) respectively, and consider the following line of argument:

j(w   ) = d(     ,      )
j(w) +

= min

w

    j(w   ) +

    j(w   ).

i   i

(cid:88)
(cid:88)
i   i
     
i ci(w   ) +

     
i ci(w) +

(cid:88)
     
(cid:88)
i ej(w)
i   e
     
i ei(w   )

i   e

(3.113a)

(3.113b)

(3.113c)

(3.113d)

to write (3.113a) we used strong duality, while (3.113c) obtains by setting
w = w    in (3.113c). finally, to obtain (3.113d) we used the fact that w    is
feasible and hence (3.108) holds. since (3.113) holds with equality, one can
conclude that the following complementary slackness condition:

(cid:88)

i   i

(cid:88)

i   e

     
i ci(w   ) +

     
i ei(w   ) = 0.

in other words,      
i = 0 whenever ci(w) < 0.
furthermore, since w    minimizes l(w,      ,      ) over w, it follows that its
gradient must vanish at w   , that is,

i ci(w   ) = 0 or equivalently      
(cid:88)

(cid:88)

i   ci(w   ) +
     

i    ei(w   ) = 0.
     

   j(w   ) +

i   i

i   e

putting everything together, we obtain

ci(w   )     0    i     i
ej(w   ) = 0    i     e

i     0
     
     
i ci(w   ) = 0
i    ei(w   ) = 0.
     

(3.114a)

(3.114b)

(3.114c)

(3.114d)

(3.114e)

   j(w   ) +

(cid:88)

i   i

i   ci(w   ) +
     

(cid:88)

i   e

the above conditions are called the kkt conditions. if the primal problem is
convex, then the kkt conditions are both necessary and su   cient. in other
words, if   w and (    ,     ) satisfy (3.114) then   w and (     ,     ) are primal and dual
optimal with zero duality gap. to see this note that the    rst two conditions
show that   w is feasible. since   i     0, l(w,   ,   ) is convex in w. finally the
last condition states that   w minimizes l(w,     ,     ). since     ici(   w) = 0 and

3.3 constrained optimization

ej(   w) = 0, we have

d(    ,     ) = min
w

l(w,     ,     )

n(cid:88)

i=1

= j(   w) +

= j(   w).

    ici(   w) +

131

m(cid:88)

j=1

    jej(   w)

3.3.3 linear and quadratic programs

so far we discussed general constrained optimization problems. many ma-
chine learning problems have special structure which can be exploited fur-
ther. we discuss the implication of duality for two such problems.

3.3.3.1 id135

an optimization problem with a linear objective function and (both equality
and inequality) linear constraints is said to be a linear program (lp). a
canonical linear program is of the following form:

c(cid:62)w

w

min
s. t. aw = b, w     0.

(3.115a)

(3.115b)

here w and c are n dimensional vectors, while b is a m dimensional vector,
and a is a m    n matrix with m < n.

suppose we are given a lp of the form:
c(cid:62)w

w

min
s. t. aw     b,

(3.116a)

(3.116b)

we can transform it into a canonical lp by introducing non-negative slack
variables

c(cid:62)w

min
w,  
s. t. aw        = b,        0.

(3.117a)

(3.117b)
next, we split w into its positive and negative parts w+ and w    respec-
i = max(0,   wi). using these new
tively by setting w+

i = max(0, wi) and w   

132

3 optimization

variables we rewrite (3.117) as

       c
      (cid:62)       w+
      
s. t. (cid:2) a    a    i (cid:3)       w+

w   
  

   c
0

min

w+,w   ,   

       = b,

       w+

w   
  

           0,

(3.118a)

(3.118b)

w   
  

thus yielding a canonical lp (3.115) in the variables w+, w    and   .

by introducing non-negative lagrange multipliers    and    one can write

the lagrangian of (3.115) as

l(w,   , s) = c(cid:62)w +   (cid:62)(aw     b)       (cid:62)w.

(3.119)

taking gradients with respect to the primal and dual variables and setting
them to zero obtains

a(cid:62)          = c
aw = b
  (cid:62)w = 0
w     0
       0.

(3.120a)

(3.120b)

(3.120c)

(3.120d)

(3.120e)

condition (3.120c) can be simpli   ed by noting that both w and    are con-
strained to be non-negative, therefore   (cid:62)w = 0 if, and only if,   iwi = 0 for
i = 1, . . . , n.

using (3.120a), (3.120c), and (3.120b) we can write

c(cid:62)w = (a(cid:62)         )(cid:62)w =   (cid:62)aw =   (cid:62)b.

substituting this into (3.115) and eliminating the primal variable w yields
the following dual lp

b(cid:62)  

max
  ,  
s.t. a(cid:62)          = c,        0.

(3.121a)

(3.121b)
as before, we let   + = max(  , 0) and       = max(0,     ) and convert the

3.3 constrained optimization

133

above lp into the following canonical lp

      

      (cid:62)         +

       b
s.t. (cid:2) a(cid:62)    a(cid:62)    i (cid:3)         +

   b
0

     
  

max
  ,  +,     

       = c,

         +

     
  

           0.

(3.122a)

(3.122b)

     
  

it can be easily veri   ed that the primal-dual problem is symmetric; by taking
the dual of the dual we recover the primal (problem 3.17). one important
thing to note however is that the primal (3.115) involves n variables and
n + m constraints, while the dual (3.122) involves 2m + n variables and
4m + 2n constraints.

3.3.3.2 quadratic programming

an optimization problem with a convex quadratic objective function and lin-
ear constraints is said to be a convex quadratic program (qp). the canonical
convex qp can be written as follows:

w

w(cid:62)gx + w(cid:62)d
1
min
2
i w = bi for i     e
s.t. a(cid:62)
i w     bi for i     i
a(cid:62)

(3.123a)

(3.123b)

(3.123c)
here g (cid:23) 0 is a n    n positive semi-de   nite matrix, e and i are    nite set of
indices, while d and ai are n dimensional vectors, and bi are scalars.

as a warm up let us consider the arguably simpler equality constrained
quadratic programs. in this case, we can stack the ai into a matrix a and
the bi into a vector b to write

w(cid:62)gw + w(cid:62)d

1
2

min

w
s.t. aw = b

(3.124a)

(3.124b)

by introducing non-negative lagrange multipliers    the lagrangian of the
above optimization problem can be written as

l(w,   ) =

1
2

w(cid:62)gw + w(cid:62)d +   (aw     b).

(3.125)

to    nd the saddle point of the lagrangian we take gradients with respect

134

3 optimization

to w and    and set them to zero. this obtains
gw + d + a(cid:62)   = 0
aw = b.

putting these two conditions together yields the following linear system of
equations

(cid:20) g a(cid:62)

(cid:21)(cid:20) w

(cid:21)

a 0

  

(cid:20)    d

(cid:21)

=

b

.

(3.126)

the matrix in the above equation is called the kkt matrix, and we can use
it to characterize the conditions under which (3.124) has a unique solution.
theorem 3.23 let z be a n   (n    m) matrix whose columns form a basis
for the null space of a, that is, az = 0. if a has full row rank, and the
reduced-hessian matrix z(cid:62)gz is positive de   nite, then there exists a unique
pair (w   ,      ) which solves (3.126). furthermore, w    also minimizes (3.124).
proof note that a unique (w   ,      ) exists whenever the kkt matrix is
non-singular. suppose this is not the case, then there exist non-zero vectors
a and b such that

(cid:20) g a(cid:62)
(cid:2) zu 0 (cid:3)(cid:20) g a(cid:62)

(cid:21)(cid:20) a
(cid:21)(cid:20) zu

a 0

b

a 0

0

(cid:21)

(cid:21)

= 0.

= u(cid:62)z(cid:62)gzu = 0.

since aa = 0 this implies that a lies in the null space of a and hence there
exists a u such that a = zu. therefore

positive de   niteness of z(cid:62)gz implies that u = 0 and hence a = 0. on the
other hand, the full row rank of a and a(cid:62)b = 0 implies that b = 0. in
summary, both a and b are zero, a contradiction.
let w (cid:54)= w    be any other feasible point and    w = w        w. since aw    =
aw = b we have that a   w = 0. hence, there exists a non-zero u such that
   w = zu. the objective function j(w) can be written as

j(w) =

(w           w)(cid:62)g(w           w) + (w           w)(cid:62)d

1
2

   w(cid:62)g   w     (gw    + d)(cid:62)   w.
2 u(cid:62)z(cid:62)gzu > 0 by positive de   niteness of
first note that 1
the reduced hessian. second, since w    solves (3.126) it follows that (gw    +

= j(w   ) +
1
2
2    w(cid:62)g   w = 1

3.4 stochastic optimization
135
d)(cid:62)   w =   (cid:62)a   w = 0. together these two observations imply that j(w) >
j(w   ).
if the technical conditions of the above theorem are met, then solving the
equality constrained qp (3.124) is equivalent to solving the linear system
(3.126). see [nw99] for a extensive discussion of algorithms that can be
used for this task.

next we turn our attention to the general qp (3.123) which also contains

inequality constraints. the lagrangian in this case can be written as

1
2

l(w,   ) =

w(cid:62)gw + w(cid:62)d +

i w     bi). (3.127)
let w    denote the minimizer of (3.123). if we de   ne the active set a(w   ) as

i w     bi) +

  i(a(cid:62)

  i(a(cid:62)

i   e

i   i

(cid:88)

(cid:111)

(cid:88)

(cid:110)

a(w   ) =

i s.t. i     i and a(cid:62)

i w    = bi

,

then the kkt conditions (3.114) for this problem can be written as

gw    + d +

(cid:88)

i   a(w   )

i w     bi < 0    i     i \ a(w   )
a(cid:62)
i w     bi = 0    i     e     a(w   )
a(cid:62)
(cid:88)

i     0    i     a(w   )
     
  iai = 0.

     
i ai +

i   e

(3.128a)

(3.128b)

(3.128c)

(3.128d)

conceptually the main di   culty in solving (3.123) is in identifying the active
i = 0 for all i     i \ a(w   ). most algorithms
set a(w   ). this is because      
for solving (3.123) can be viewed as di   erent ways to identify the active set.
see [nw99] for a detailed discussion.

3.4 stochastic optimization

recall that regularized risk minimization involves a data-driven optimization
problem in which the objective function involves the summation of loss terms
over a set of data to be modeled:

min

f

j(f ) :=      (f ) +

1
m

l(f (xi), yi).

m(cid:88)

i=1

classical optimization techniques must compute this sum in its entirety for
each evaluation of the objective, respectively its gradient. as available data
sets grow ever larger, such    batch    optimizers therefore become increasingly
ine   cient. they are also ill-suited for the incremental setting, where partial
data must be modeled as it arrives.

136

3 optimization

stochastic gradient-based methods, by contrast, work with gradient esti-
mates obtained from small subsamples (mini-batches) of training data. this
can greatly reduce computational requirements: on large, redundant data
sets, simple stochastic id119 routinely outperforms sophisticated
second-order batch methods by orders of magnitude.

the key idea here is that j(w) is replaced by an instantaneous estimate
jt which is computed from a mini-batch of size k comprising of a subset of
points (xt

i) with i = 1, . . . , k drawn from the dataset:

i, yt

jt(w) =      (w) +

1
k

l(w, xt

i, yt

i).

(3.129)

k(cid:88)

i=1

setting k = 1 obtains an algorithm which processes data points as they
arrive.

3.4.1 stochastic id119

perhaps the simplest stochastic optimization algorithm is stochastic gradi-
ent descent (sgd). the parameter update of sgd takes the form:

wt+1 = wt       t   jt(wt).

(3.130)

if jt is not di   erentiable, then one can choose an arbitrary subgradient from
   jt(wt) to compute the update. it has been shown that sgd asymptotically
converges to the true minimizer of j(w) if the stepsize   t decays as o(1/
t).
for instance, one could set

   

  t =

   + t

,

(3.131)

where    > 0 is a tuning parameter. see algorithm 3.8 for details.

3.4.1.1 practical considerations

one simple yet e   ective rule of thumb to tune    is to select a small subset
of data, try various values of    on this subset, and choose the    that most
reduces the objective function.

in some cases letting   t to decay as o(1/t) has been found to be more

e   ective:

  t =

  

   + t

.

(3.132)

the free parameter    > 0 can be tuned as described above. if    (w) is   -
strongly convex, then dividing the stepsize   t by      yields good practical
performance.

(cid:114)   

3.5 nonid76

137

algorithm 3.8 stochastic id119

1: input: maximum iterations t , batch size k, and   
2: set t = 0 and w0 = 0
3: while t < t do
4:

i) and compute    jt(wt)

i, yt

(cid:113)   

6:

5:

choose a subset of k data points (xt
compute stepsize   t =
wt+1 = wt       t   jt(wt)
t = t + 1
7:
8: end while
9: return: wt

   +t

3.5 nonid76

our focus in the previous sections was on convex objective functions. some-
times non-convex objective functions also arise in machine learning applica-
tions. these problems are signi   cantly harder and tools for minimizing such
objective functions are not as well developed. we brie   y describe one algo-
rithm which can be applied whenever we can write the objective function as
a di   erence of two convex functions.

3.5.1 concave-convex procedure

any function with a bounded hessian can be decomposed into the di   erence
of two (non-unique) convex functions, that is, one can write

j(w) = f (w)     g(w),

(3.133)

where f and g are convex functions. clearly, j is not convex, but there
exists a reasonably simple algorithm namely the concave-convex procedure
(ccp) for    nding a local minima of j. the basic idea is simple: in the
tth iteration replace g by its    rst order taylor expansion at wt, that is,
g(wt) + (cid:104)w     wt,   g(wt)(cid:105) and minimize

jt(w) = f (w)     g(wt)     (cid:104)w     wt,   g(wt)(cid:105) .

(3.134)

taking gradients and setting it to zero shows that jt is minimized by setting

   f (wt+1) =    g(wt).

(3.135)

the iterations of ccp on a toy minimization problem is illustrated in figure
3.13, while the complete algorithm listing can be found in algorithm 3.9.

138

3 optimization

fig. 3.13. given the function on the left we decompose it into the di   erence of two
convex functions depicted on the right panel. the ccp algorithm generates iterates
by matching points on the two convex curves which have the same tangent vectors.
as can be seen, the iterates approach the solution x = 2.0.

algorithm 3.9 concave-convex procedure

1: input: initial point w0, maximum iterations t , convex functions f ,g
2: set t = 0
3: while t < t do
4:

set wt+1 = argminw f (w)     g(wt)     (cid:104)w     wt,   g(wt)(cid:105)
t = t + 1
5:
6: end while
7: return: wt

theorem 3.24 let j be a function which can be decomposed into a di   er-
ence of two convex functions e.g., (3.133). the iterates generated by (3.135)
monotically decrease j. furthermore, the stationary point of the iterates is
a local minima of j.

proof since f and g are convex

f (wt)     f (wt+1) + (cid:104)wt     wt+1,   f (wt+1)(cid:105)
g(wt+1)     g(wt) + (cid:104)wt+1     wt,   g(wt)(cid:105) .

adding the two inequalities, rearranging, and using (3.135) shows that j(wt) =
f (wt)     g(wt)     f (wt+1)     g(wt+1) = j(wt+1), as claimed.
let w    be a stationary point of the iterates. then    f (w   ) =    g(w   ),
which in turn implies that w    is a local minima of j because    j(w   ) = 0.

there are a number of extensions to ccp. we mention only a few in the
passing. first, it can be shown that all instances of the em algorithm (sec-
tion ??) can be shown to be special cases of ccp. second, the rate of con-

1.01.52.02.53.03.54.080706050403020101.01.52.02.53.03.54.0500501001502003.6 some practical advice

139

vergence of ccp is related to the eigenvalues of the positive semi-de   nite
matrix    2(f + g). third, ccp can also be extended to solve constrained
problems of the form:

f0(w)     g0(w)

w

min
s.t. fi(w)     gi(w)     ci for i = 1, . . . , n.

where, as before, fi and gi for i = 0, 1, . . . , n are assumed convex. at every
iteration, we replace gi by its    rst order taylor approximation and solve the
following constrained convex problem:

f0(w)     g0(wt) + (cid:104)w     wt,   g0(wt)(cid:105)

min
s.t. fi(w)     gi(wt) + (cid:104)w     wt,   gi(wt)(cid:105)     ci for i = 1, . . . , n.

w

3.6 some practical advice

the range of optimization algorithms we presented in this chapter might be
somewhat intimidating for the beginner. some simple rules of thumb can
alleviate this anxiety

code reuse:
implementing an e   cient optimization algorithm correctly
is both time consuming and error prone. therefore, as far as possible use
existing libraries. a number of high class optimization libraries both com-
mercial and open source exist.

unconstrained problems: for unconstrained minimization of a smooth
convex function lbfgs (section 3.2.6.1 is the algorithm of choice. in many
practical situations the spectral gradient method (section 3.2.6.2) is also
very competitive. it also has the added advantage of being easy to imple-
ment. if the function to be minimized is non-smooth then bundle methods
(section 3.2.7) are to be preferred. amongst the di   erent formulations, the
bundle trust algorithm tends to be quite robust.

constrained problems: for constrained problems it is very important
to understand the nature of the constraints. simple equality (ax = b) and
box (l     x     u) constraints are easier to handle than general non-linear
constraints. if the objective function is smooth, the constraint set     is simple,
and orthogonal projections p    are easy to compute, then spectral projected
gradient (section 3.3.1) is the method of choice. if the optimization problem
is a qp or an lp then specialized solvers tend to be much faster than general
purpose solvers.

140

3 optimization

large scale problems:
if your parameter vector is high dimensional then
consider coordinate descent (section 3.2.2) especially if the one dimensional
line search along a coordinate can be carried out e   ciently. if the objective
function is made up of a summation of large number of terms, consider
stochastic id119 (section 3.4.1). although both these algorithms
do not guarantee a very accurate solution, practical experience shows that
for large scale machine learning problems this is rarely necessary.

duality: sometimes problems which are hard to optimize in the primal
may become simpler in the dual. for instance, if the objective function is
strongly convex but non-smooth, its fenchel conjugate is smooth with a
lipschitz continuous gradient.

problems
problem 3.1 (intersection of convex sets {1}) if c1 and c2 are con-
vex sets, then show that c1     c2 is also convex. extend your result to show

that(cid:84)n

i=1 ci are convex if ci are convex.

problem 3.2 (linear transform of convex sets {1}) given a set c    
rn and a linear transform a     rm  n, de   ne ac := {y = ax : x     c}. if
c is convex then show that ac is also convex.
problem 3.3 (convex combinations {1}) show that a subset of rn is
convex if and only if it contains all the convex combination of its elements.
problem 3.4 (convex hull {2}) show that the convex hull, conv(x) is
the smallest convex set which contains x.
problem 3.5 (epigraph of a convex function {2}) show that a func-
tion satis   es de   nition 3.3 if, and only if, its epigraph is convex.

problem 3.6 prove the jensen   s inequality (3.6).
problem 3.7 (strong convexity of the negative id178 {3}) show that
the negative id178 (3.15) is 1-strongly convex with respect to the (cid:107)  (cid:107)1 norm
on the simplex. hint: first show that   (t) := (t     1) log t     2 (t   1)2
t+1     0 for
all t     0. next substitute t = xi/yi to show that

(cid:88)

i

(xi     yi) log

    (cid:107)x     y(cid:107)2
1 .

xi
yi

3.6 some practical advice
141
problem 3.8 (strongly convex functions {2}) prove 3.16, 3.17, 3.18
and 3.19.
problem 3.9 (convex functions with lipschitz continuous gradient {2})
prove 3.22, 3.23, 3.24 and 3.25.
problem 3.10 (one dimensional projection {1}) if f : rd     r is
convex, then show that for an arbitrary x and p in rd the one dimensional
function   (  ) := f (x +   p) is also convex.
problem 3.11 (quasi-convex functions {2}) in section 3.1 we showed
that the below-sets of a convex function xc := {x| f (x)     c} are convex. give
a counter-example to show that the converse is not true, that is, there exist
non-convex functions whose below-sets are convex. this class of functions is
called quasi-convex.
problem 3.12 (gradient of the p-norm {1}) show that the gradient of
the p-norm (3.31) is given by (3.32).

problem 3.13 derive the fenchel conjugate of the following functions

(cid:40)

if x     c

0
    otherwise.

f (x) =

where c is a convex set

f (x) = ax + b

x(cid:62)ax where a is a positive de   nite matrix

1
2

f (x) =
f (x) =     log(x)
f (x) = exp(x)

f (x) = x log(x)

problem 3.14 (convergence of id119 {2}) suppose j has
a lipschitz continuous gradient with modulus l. then show that algorithm
3.2 with an inexact line search satisfying the wolfe conditions (3.42) and
(3.43) will return a solution wt with (cid:107)   j(wt)(cid:107)       in at most o(1/ 2) iter-
ations.

problem 3.15 show that

1 +(cid:80)t
(cid:80)t

t=1

t=1
1   
t

1
t

    1   
t

142
problem 3.16 (coordinate descent for quadratic programming {2})
derive a projection based method which uses coordinate descent to generate
directions of descent for solving the following box constrained qp:

3 optimization

1
2

w(cid:62)qw + c(cid:62)w

min
w   rn
s.t. l     w     u.

you may assume that q is positive de   nite and l and u are scalars.
problem 3.17 (dual of a lp {1}) show that the dual of the lp (3.122)
is (3.115). in other words, we recover the primal by computing the dual of
the dual.

4

online learning and boosting

so far the learning algorithms we considered assumed that all the training
data is available before building a model for predicting labels on unseen data
points. in many modern applications data is available only in a streaming
fashion, and one needs to predict labels on the    y. to describe a concrete
example, consider the task of spam    ltering. as emails arrive the learning
algorithm needs to classify them as spam or ham. tasks such as these are
tackled via online learning. online learning proceeds in rounds. at each
round a training example is revealed to the learning algorithm, which uses
its current model to predict the label. the true label is then revealed to
the learner which incurs a loss and updates its model based on the feedback
provided. this protocol is summarized in algorithm 4.1. the goal of online
learning is to minimize the total loss incurred. by an appropriate choice
of labels and id168s, this setting encompasses a large number of
tasks such as classi   cation, regression, and density estimation. in our spam
detection example, if an email is misclassi   ed the user can provide feedback
which is used to update the spam    lter, and the goal is to minimize the
number of misclassi   ed emails.

4.1 halving algorithm

the halving algorithm is conceptually simple, yet it illustrates many of the
concepts in online learning. suppose we have access to a set of n experts,
that is, functions fi which map from the input space x to the output space
y = {  1}. furthermore, assume that one of the experts is consistent, that
is, there exists a j     {1, . . . , n} such that fj(xt) = yt for t = 1, . . . , t . the
halving algorithm maintains a set ct of consistent experts at time t. initially
c0 = {1, . . . , n}, and it is updated recursively as

ct+1 = {i     ct s.t. fi(xt+1) = yt+1} .

(4.1)

the prediction on a new data point is computed via a majority vote amongst
the consistent experts:   yt = majority(ct).

lemma 4.1 the halving algorithm makes at most log2(n) mistakes.

143

144

4 online learning and boosting

algorithm 4.1 protocol of online learning

1: for t = 1, . . . , t do do
2: get training instance xt
predict label   yt
3:
4: get true label yt
5:
6: update model
7: end for

incur loss l(  yt, xt, yt)

proof let m denote the total number of mistakes. the halving algorithm
makes a mistake at iteration t if at least half the consistent experts ct predict
the wrong label. this in turn implies that

|ct+1|     |ct|

2

    |c0|
2m =

n
2m .

on the other hand, since one of the experts is consistent it follows that
1     |ct+1|. therefore, 2m     n. solving for m completes the proof.

4.2 weighted majority

we now turn to the scenario where none of the experts is consistent. there-
fore, the aim here is not to minimize the number mistakes but to minimize
regret.

in this chapter we will consider online methods for solving the following

optimization problem:

j(w) where j(w) =

min
w      

t(cid:88)

t=1

ft(w).

(4.2)

suppose we have access to a function    which is continuously di   erentiable
and strongly convex with modulus of strong convexity    > 0 (see section
3.1.4 for de   nition of strong convexity), then we can de   ne the bregman
divergence (3.29) corresponding to    as

     (w, w(cid:48)) =   (w)       (w(cid:48))    (cid:10)w     w(cid:48),     (w(cid:48))(cid:11) .

we can also generalize the orthogonal projection (3.104) by replacing the
square euclidean norm with the above bregman divergence:

p  ,   (w(cid:48)) = argmin
w      

     (w, w(cid:48)).

(4.3)

4.2 weighted majority

145

algorithm 4.2 stochastic (sub)id119
1: input: initial point x1, maximum iterations t
2: for t = 1, . . . , t do
3:

compute   wt+1 =          (     (wt)       tgt) with gt =    wft(wt)
set wt+1 = p  ,    (   wt+1)

4:
5: end for
6: return: wt +1

denote w    = p  ,   (w(cid:48)). just like the euclidean distance is non-expansive, the
bregman projection can also be shown to be non-expansive in the following
sense:

     (w, w(cid:48))          (w, w   ) +      (w   , w(cid:48))

for all w        . the diameter of     as measured by       is given by

diam  (   ) = max
w,w(cid:48)      

     (w, w(cid:48)).

(4.4)

(4.5)

for the rest of this chapter we will make the following standard assumptions:
    each ft is convex and revealed at time instance t.
        is a closed convex subset of rn with non-empty interior.
    the diameter diam  (   ) of     is bounded by f <    .
    the set of optimal solutions of (4.2) denoted by        is non-empty.
    the subgradient    wft(w) can be computed for every t and w        .
    the bregman projection (4.3) can be computed for every w(cid:48)     rn.
    the gradient      , and its inverse (     )   1 =          can be computed.

the method we employ to solve (4.2) is given in algorithm 4.2. before
analyzing the performance of the algorithm we would like to discuss three
special cases. first, euclidean distance squared which recovers projected
stochastic id119, second id178 which recovers exponentiated
id119, and third the p-norms for p > 2 which recovers the p-norm
id88. bugbug todo.

our key result is lemma 4.3 given below. it can be found in various guises
in di   erent places most notably lemma 2.1 and 2.2 in [?], theorem 4.1 and
eq. (4.21) and (4.15) in [?], in the proof of theorem 1 of [?], as well as lemma
3 of [?]. we prove a slightly general variant; we allow for projections with
an arbitrary bregman divergence and also take into account a generalized
version of strong convexity of ft. both these modi   cations will allow us to
deal with general settings within a uni   ed framework.

146

4 online learning and boosting

de   nition 4.2 we say that a convex function f is strongly convex with
respect to another convex function    with modulus    if

f (w)     f (w(cid:48))    (cid:10)w     w(cid:48),   (cid:11)            (w, w(cid:48)) for all           f (w(cid:48)).

(4.6)

2 (cid:107)  (cid:107)2.
the usual notion of strong convexity is recovered by setting   (  ) = 1
lemma 4.3 let ft be strongly convex with respect to    with modulus        0
for all t. for any w         the sequences generated by algorithm 4.2 satisfy
     (w, wt+1)          (w, wt)       t (cid:104)gt, wt     w(cid:105) +

(cid:107)gt(cid:107)2

  2
t
2  

    (1       t  )     (w, wt)       t(ft(wt)     ft(w)) +

  2
t
2  

(4.7)
(cid:107)gt(cid:107)2 . (4.8)

proof we prove the result in three steps. first we upper bound      (w, wt+1)
by      (w,   wt+1). this is a consequence of (4.4) and the non-negativity of the
bregman divergence which allows us to write

     (w, wt+1)          (w,   wt+1).

(4.9)

in the next step we use lemma 3.11 to write
     (w, wt) +      (wt,   wt+1)          (w,   wt+1) = (cid:104)     (   wt+1)          (wt), w     wt(cid:105) .
since          = (     )   1, the update in step 3 of algorithm 4.2 can equivalently
be written as      (   wt+1)          (wt) =      tgt. plugging this in the above
equation and rearranging

     (w,   wt+1) =      (w, wt)       t (cid:104)gt, wt     w(cid:105) +      (wt,   wt+1).

(4.10)

2   (cid:107)x(cid:107)2 +   

finally we upper bound      (wt,   wt+1). for this we need two observations:
first, (cid:104)x, y(cid:105)     1
2 (cid:107)y(cid:107)2 for all x, y     rn and    > 0. second, the   
2 (cid:107)wt       wt+1(cid:107)2.
strong convexity of    allows us to bound      (   wt+1, wt)       
using these two observations
     (wt,   wt+1) =   (wt)       (   wt+1)     (cid:104)     (   wt+1), wt       wt+1(cid:105)

=    (  (   wt+1)       (wt)     (cid:104)     (wt),   wt+1     wt(cid:105)) + (cid:104)  tgt, wt       wt+1(cid:105)
=         (   wt+1, wt) + (cid:104)  tgt, wt       wt+1(cid:105)
          
  
2
2
  2
t
2  

(cid:107)wt       wt+1(cid:107)2 +
(cid:107)gt(cid:107)2 .

(cid:107)wt       wt+1(cid:107)2

(cid:107)gt(cid:107)2 +

  2
t
2  

(4.11)

=

inequality (4.7) follows by putting together (4.9), (4.10), and (4.11), while
(4.8) follows by using (4.6) with f = ft and w(cid:48) = wt and substituting into

4.2 weighted majority

(4.7).

147

now we are ready to prove regret bounds.
lemma 4.4 let w               denote the best parameter chosen in hindsight,
and let (cid:107)gt(cid:107)     l for all t. then the regret of algorithm 4.2 can be bounded
via

ft(wt)     ft(w   )     f

    t   

+

l2
2  

  t.

(4.12)

(cid:19)

(cid:18) 1

  t

t(cid:88)

t=1

t(cid:88)

t=1

proof set w = w    and rearrange (4.8) to obtain
ft(wt)     ft(w   )     1
  t

((1         t)     (w   , wt)          (w   , wt+1)) +

(cid:107)gt(cid:107)2 .

  t
2  

since the diameter of     is bounded by f and       is non-negative

summing over t

t(cid:88)

t=1

t1 =

t=1

ft(wt)     ft(w   )     t(cid:88)
(cid:124)
(cid:18) 1
(cid:18) 1
(cid:18) 1

(cid:19)
(cid:19)
(cid:19)

t(cid:88)

      

      

  1

  1

      

f +

f

   

   

  1

t=2

((1       t  )     (w   , wt)          (w   , wt+1))

1
  t

(cid:123)(cid:122)

t1

     (w   , w1)     1
  t

     (w   , wt +1) +

     (w   , wt)

    1
  t   1

  t

      

t(cid:88)

     (w   , w1) +

     (w   , wt)

(cid:18) 1

  t

t=2

    1
  t   1

      

t(cid:88)

t=2

(cid:18) 1

  t

= f

(cid:19)

(cid:19)
(cid:19)

.

      

    1
  t   1

(cid:18) 1

  t

    t   

t(cid:88)
(cid:124)

t=1

+

(cid:125)
(cid:18) 1

(cid:107)gt(cid:107)2

  t
2  

(cid:123)(cid:122)

t2

.

(cid:125)
(cid:19)

on the other hand, since the subgradients are lipschitz continuous with
constant l it follows that

t(cid:88)

t=1

  t.

t2     l2
2  

putting together the bounds for t1 and t2 yields (4.12).

corollary 4.5 if    > 0 and we set   t = 1
ft(xt)     ft(x   )     l2
2    

t(cid:88)

  t then

t=1

(1 + log(t )),

t(cid:88)

t=1

(cid:18)

t(cid:88)

t=1

148
on the other hand, when    = 0, if we set   t = 1   

4 online learning and boosting

t(cid:88)

t=1

ft(xt)     ft(x   )    

f +

then

(cid:19)   

t .

t

l2
  

proof first consider    > 0 with   t = 1
consequently (4.12) specializes to

  t . in this case 1
  t

= t   , and

ft(wt)     ft(w   )     l2
2    

1
t

    l2
2    

(1 + log(t )).

when    = 0, and we set   t = 1   
   

ft(wt)     ft(w   )     f

t(cid:88)

t

t=1

and use problem 4.2 to rewrite (4.12) as

t +

l2
  

   
t +

    f

   
t .

l2
  

   
1

t

2

t(cid:88)

t=1

1

problems
problem 4.1 (generalized cauchy-schwartz {1}) show that (cid:104)x, y(cid:105)    
2   (cid:107)x(cid:107)2 +   

problem 4.2 (bounding sum of a series {1}) show that (cid:80)b

2 (cid:107)y(cid:107)2 for all x, y     rn and    > 0.

   

   
1

t

2

t=a

b     a + 1. hint: upper bound the sum by an integral.

   

5

conditional densities

a number of machine learning algorithms can be derived by using condi-
tional exponential families of distribution (section 2.3). assume that the
training set {(x1, y1), . . . , (xm, ym)} was drawn iid from some underlying
distribution. using bayes rule (1.15) one can write the likelihood

p(  |x, y )     p(  )p(y |x,   ) = p(  )

p(yi|xi,   ),

(5.1)

m(cid:89)

i=1

and hence the negative log-likelihood

    log p(  |x, y ) =     m(cid:88)

log p(yi|xi,   )     log p(  ) + const.

(5.2)

i=1

because we do not have any prior knowledge about the data, we choose a
zero mean unit variance isotropic normal distribution for p(  ). this yields

    log p(  |x, y ) =

1
2

log p(yi|xi,   ) + const.

(5.3)

finally, if we assume a conditional exponential family model for p(y|x,   ),
that is,

(cid:107)  (cid:107)2     m(cid:88)

i=1

p(y|x,   ) = exp ((cid:104)  (x, y),   (cid:105)     g(  |x)) ,

then

    log p(  |x, y ) =

(cid:107)  (cid:107)2 +

1
2

where

m(cid:88)
(cid:88)

i=1

y   y

g(  |x) = log

exp ((cid:104)  (x, y),   (cid:105)) ,

g(  |xi)     (cid:104)  (xi, yi),   (cid:105) + const.

(5.4)

(5.5)

(5.6)

is the log-partition function. clearly, (5.5) is a smooth convex objective
function, and algorithms for unconstrained minimization from chapter 3

149

150

5 conditional densities

can be used to obtain the maximum aposteriori (map) estimate for   . given
the optimal   , the class label at any given x can be predicted using

y    = argmax

p(y|x,   ).

y

(5.7)

in this chapter we will discuss a number of these algorithms that can be
derived by specializing the above setup. our discussion uni   es seemingly
disparate algorithms, which are often discussed separately in literature.

5.1 id28
we begin with the simplest case namely binary classi   cation1. the key ob-
servation here is that the labels y     {  1} and hence

g(  |x) = log (exp ((cid:104)  (x, +1),   (cid:105)) + exp ((cid:104)  (x,   1),   (cid:105))) .

(5.8)
de   ne     (x) :=   (x, +1)       (x,   1). plugging (5.8) into (5.4), using the
de   nition of      and rearranging
p(y = +1|x,   ) =

p(y =    1|x,   ) =

1 + exp

1 + exp

or more compactly

p(y|x,   ) =

1 + exp

since p(y|x,   ) is a logistic function, hence the name id28. the
classi   cation rule (5.7) in this case specializes as follows: predict +1 when-
ever p(y = +1|x,   )     p(y =    1|x,   ) otherwise predict    1. however

(5.9)

as our prediction func-
therefore one can equivalently use sign
tion. using (5.9) we can write the objective function of id28
as

p(y = +1|x,   )
p(y =    1|x,   )

log

(cid:16)

m(cid:88)

i=1

(cid:107)  (cid:107)2 +

1
2

log

1 + exp

1 the name id28 is a misnomer!

1

1

(cid:16)(cid:68)        (x),   
(cid:69)(cid:17) and
(cid:16)(cid:68)     (x),   
(cid:69)(cid:17) ,
(cid:69)(cid:17) .
(cid:16)(cid:68)   y     (x),   

1

,

=

(cid:69)
(cid:68)     (x),   
(cid:69)(cid:17)
(cid:16)(cid:68)     (x),   
(cid:16)(cid:68)   yi     (xi),   

(cid:69)(cid:17)(cid:17)

   2j(  ) = i     m(cid:88)

5.2 regression

151

to minimize the above objective function we    rst compute the gradient.

   j(  ) =    +

=    +

m(cid:88)
m(cid:88)

i=1

i=1

(cid:69)(cid:17)
(cid:16)(cid:68)   yi     (xi),   
(cid:16)(cid:68)   yi     (xi),   
(cid:69)(cid:17) (   yi     (xi))

exp

1 + exp

(p(yi|xi,   )     1)yi     (xi).

notice that the second term of the gradient vanishes whenever p(yi|xi,   ) =
1. therefore, one way to interpret id28 is to view it as a method
to maximize p(yi|xi,   ) for each point (xi, yi) in the training set. since the
objective function of id28 is twice di   erentiable one can also
compute its hessian

p(yi|xi,   )(1     p(yi|xi,   ))     (xi)     (xi)(cid:62),

i=1

where we used y2
(section 3.2.6) to obtain the optimal parameter   .

i = 1. the hessian can be used in the id77

5.2 regression

5.2.1 conditionally normal models

   xed variance

5.2.2 posterior distribution

integrating out vs. laplace approximation, e   cient estimation (sparse greedy)

5.2.3 heteroscedastic estimation

explain that we have two parameters. not too many details (do that as an
assignment).

5.3 multiclass classi   cation

5.3.1 conditionally multinomial models

joint feature map

152

5 conditional densities

5.4 what is a crf?
    motivation with learning a digit example
    general de   nition
    gaussian process + structure = crf

5.4.1 linear chain crfs
    graphical model
    applications
    optimization problem

5.4.2 higher order crfs
    2-d crfs and their applications in vision
    skip chain crfs
    hierarchical crfs (graph transducers, sutton et. al. jmlr etc)

5.4.3 kernelized crfs
    from feature maps to kernels
    the clique decomposition theorem
    the representer theorem
    optimization strategies for kernelized crfs

5.5 optimization strategies

5.5.1 getting started
    three things needed to optimize

    map estimate
    log-partition function
    gradient of log-partition function
    worked out example (linear chain?)

5.5.2 optimization algorithms

- optimization algorithms (lbfgs, sgd, eg (globerson et. al))

5.5.3 handling higher order crfs

- how things can be done for higher order crfs (brie   y)

5.6 id48

153

5.6 id48
    de   nition
    discuss that they are modeling joint distribution p(x, y)
    the way they predict is by marginalizing out x
    why they are wasteful and why crfs generally outperform them

5.7 further reading

what we did not talk about:
    details of id48 optimization
    crfs applied to predicting parse trees via matrix tree theorem (collins,
    crfs for graph matching problems
    crfs with gaussian distributions (yes they exist)

koo et al)

5.7.1 optimization

issues in optimization (blows up with number of classes). structure is not
there. can we do better?

problems

problem 5.1 poisson models

problem 5.2 bayes committee machine

problem 5.3 newton / cg approach

6

kernels and function spaces

kernels are measures of similarity. broadly speaking, machine learning al-
gorithms which rely only on the dot product between instances can be    ker-
nelized    by replacing all instances of (cid:104)x, x(cid:48)(cid:105) by a id81 k(x, x(cid:48)).
we saw examples of such algorithms in sections 1.3.3 and 1.3.4 and we will
see many more examples in chapter 7. arguably, the design of a good ker-
nel underlies the success of machine learning in many applications. in this
chapter we will lay the ground for the theoretical properties of kernels and
present a number of examples. algorithms which use these kernels can be
found in later chapters.

6.1 the basics
let x denote the space of inputs and k : x    x     r be a function which
satis   es

k(x, x(cid:48)) = (cid:104)  (x),   (x)(cid:105)

(6.1)

where    is a feature map which maps x into some dot product space h. in
other words, kernels correspond to dot products in some dot product space.
the main advantage of using a kernel as a similarity measure are threefold:
first, if the feature space is rich enough, then simple estimators such as
hyperplanes and half-spaces may be su   cient. for instance, to classify the
points in figure bugbug, we need a nonlinear decision boundary, but
once we map the points to a 3 dimensional space a hyperplane su   ces.
second, kernels allow us to construct machine learning algorithms in the
dot product space h without explicitly computing   (x). third, we need not
make any assumptions about the input space x other than for it to be a
set. as we will see later in this chapter, this allows us to compute similarity
between discrete objects such as strings, trees, and graphs. in the    rst half
of this chapter we will present some examples of kernels, and discuss some
theoretical properties of kernels in the second half.

155

156

6.1.1 examples

6 kernels and function spaces

6.1.1.1 linear kernel
linear kernels are perhaps the simplest of all kernels. we assume that x     rn
and de   ne

k(x, x(cid:48)) =(cid:10)x, x(cid:48)(cid:11) =

(cid:88)

xix(cid:48)
i.

i

if x and x(cid:48) are dense then computing the kernel takes o(n) time. on the
other hand, for sparse vectors this can be reduced to o(|nnz(x)    nnz(x(cid:48))|),
where nnz(  ) denotes the set of non-zero indices of a vector and |    | de-
notes the size of a set. linear kernels are a natural representation to use for
vectorial data. they are also widely used in id111 where documents
are represented by a vector containing the frequency of occurrence of words
(recall that we encountered this so-called bag of words representation in
chapter 1). instead of a simple bag of words, one can also map a text to the
set of pairs of words that co-occur in a sentence for a richer representation.

case   (x) = (cid:0)x2

(cid:1). although it is tedious to compute   (x)

6.1.1.2 polynomial kernel
given x     rn, we can compute a feature map    by taking all the d-th
order products (also called the monomials) of the entries of x. to illustrate
with a concrete example, let us consider x = (x1, x2) and d = 2, in which
and   (x(cid:48)) explicitly in order to compute k(x, x), there is a shortcut as the
following proposition shows.
proposition 6.1 let   (x) (resp.   (x(cid:48))) denote the vector whose entries
are all possible d-th degree ordered products of the entries of x (resp. x(cid:48)).
then

2, x1x2, x2x1

1, x2

(cid:88)

k(x, x(cid:48)) =(cid:10)  (x),   (x(cid:48))(cid:11) =(cid:0)(cid:10)x, x(cid:48)(cid:11)(cid:1)d .
(cid:88)
(cid:88)
=(cid:0)(cid:10)x, x(cid:48)(cid:11)(cid:1)d

xj1 . . . xjd    x(cid:48)

j1 . . . x(cid:48)

xjd    x(cid:48)

(cid:88)

xj1    x(cid:48)

j1 . . .

. . .

=

=

jd

jd

jd

jd

j1

j1

      (cid:88)

j

(6.2)

      d

xj    x(cid:48)

j

proof by direct computation

(cid:10)  (x),   (x(cid:48))(cid:11) =

6.1 the basics

157

the kernel (6.2) is called the polynomial kernel. an useful extension is the
inhomogeneous polynomial kernel

k(x, x(cid:48)) =(cid:0)(cid:10)x, x(cid:48)(cid:11) + c(cid:1)d ,

(6.3)

which computes all monomials up to degree d (problem 6.2).

6.1.1.3 radial basis function kernels

6.1.1.4 convolution kernels

the framework of convolution kernels is a general way to extend the notion
of kernels to structured objects such as strings, trees, and graphs. let x     x
be a discrete object which can be decomposed into p parts xp     xp in many
di   erent ways. as a concrete example consider the string x = abc which can
be split into two sets of substrings of size two namely {a, bc} and {ab, c}.
we denote the set of all such decompositions as r(x), and use it to build a
kernel on x as follows:

[k1 (cid:63) . . . (cid:63) kp ] (x, x(cid:48)) =

kp(  xp,   x(cid:48)
p).

(6.4)

(cid:88)

p(cid:89)

  x   r(x),  x(cid:48)   r(x(cid:48))

p=1

1, . . . ,   x(cid:48)

here, the sum is over all possible ways in which we can decompose x and
x(cid:48) into   x1, . . . ,   xp and   x(cid:48)
p respectively. if the cardinality of r(x) is
   nite, then it can be shown that (6.4) results in a valid kernel. although
convolution kernels provide the abstract framework, speci   c instantiations
of this idea lead to a rich set of kernels on discrete objects. we will now
discuss some of them in detail.

6.1.1.5 string kernels

the basic idea behind string kernels is simple: compare the strings by
means of the subsequences they contain. more the number of common sub-
sequences, the more similar two strings are. the subsequences need not have
equal weights. for instance, the weight of a subsequence may be given by the
inverse frequency of its occurrence. similarly, if the    rst and last characters
of a subsequence are rather far apart, then its contribution to the kernel
must be down-weighted.
formally, a string x is composed of characters from a    nite alphabet   
and |x| denotes its length. we say that s is a subsequence of x = x1x2 . . . x|x|
if s = xi1xi2 . . . xi|s| for some 1     i1 < i2 < . . . < i|s|     |x|. in particular, if
ii+1 = ii + 1 then s is a substring of x. for example, acb is not a subsequence
of adbc while abc is a subsequence and adc is a substring. assume that there
exists a function #(x, s) which returns the number of times a subsequence

158
6 kernels and function spaces
s occurs in x and a non-negative weighting function w(s)     0 which returns
the weight associated with s. then the basic string kernel can be written as

k(x, x(cid:48)) =

#(x, s) #(x(cid:48), s) w(s).

(6.5)

(cid:88)

s

di   erent string kernels are derived by specializing the above equation:

all substrings kernel:

if we restrict the summation in (6.5) to sub-
strings then [vs04] provide a su   x tree based algorithm which allows one
to compute for arbitrary w(s) the kernel k(x, x(cid:48)) in o(|x| + |x(cid:48)|) time and
memory.

k-spectrum kernel: the k-spectrum kernel is obtained by restricting
the summation in (6.5) to substrings of length k. a slightly general variant
considers all substrings of length up to k. here k is a tuning parameter
which is typically set to be a small number (e.g., 5). a simple trie based
algorithm can be used to compute the k-spectrum kernel in o((|x| + |x(cid:48)|)k)
time (problem 6.3).

inexact substring kernel: sometimes the input strings might have
measurement errors and therefore it is desirable to take into account inexact
matches. this is done by replacing #(x, s) in (6.5) by another function
#(x, s,  ) which reports the number of approximate matches of s in x. here
  denotes the number of mismatches allowed, typically a small number (e.g.,
3). by trading o    computational complexity with storage the kernel can be
computed e   ciently. see [lk03] for details.

mismatch kernel: instead of simply counting the number of occurrences
of a substring if we use a weighting scheme which down-weights the contribu-
tions of longer subsequences then this yields the so-called mismatch kernel.
given an index sequence i = (i1, . . . , ik) with 1     i1 < i2 < . . . < ik     |x|
we can associate the subsequence x(i) = xi1xi2 . . . xik with i. furthermore,
de   ne |i| = ik     i1 + 1. clearly, |i| > k if i is not contiguous. let        1 be
a decay factor. rede   ne

#(x, s) =

  |i|,

(6.6)

(cid:88)

s=x(i)

that is, we count all occurrences of s in x but now the weight associated with
a subsequence depends on its length. to illustrate, consider the subsequence
abc which occurs in the string abcebc twice, namely, abcebc and abcebc. the
   rst occurrence is counted with weight   3 while the second occurrence is
counted with the weight   6. as it turns out, this kernel can be computed
by a id145 algorithm (problem bugbug) in o(|x|    |x(cid:48)|)
time.

6.1 the basics

6.1.1.6 graph kernels

159

there are two di   erent notions of graph kernels. first, kernels on graphs
are used to compare nodes of a single graph. in contrast, kernels between
graphs focus on comparing two graphs. a random walk (or its continuous
time limit, di   usion) underlie both types of kernels. the basic intuition is
that two nodes are similar if there are a number of paths which connect
them while two graphs are similar if they share many common paths. to
describe these kernels formally we need to introduce some notation.
a graph g consists of an ordered set of n vertices v = {v1, v2, . . . , vn},
and a set of directed edges e     v   v . a vertex vi is said to be a neighbor
of another vertex vj if they are connected by an edge, i.e., if (vi, vj)     e;
this is also denoted vi     vj. the adjacency matrix of a graph is the n    n
matrix a with aij = 1 if vi     vj, and 0 otherwise. a walk of length k on g
is a sequence of indices i0, i1, . . . ik such that vir   1     vir for all 1     r     k.
the adjacency matrix has a normalized cousin, de   ned   a := d   1a, which
has the property that each of its rows sums to one, and it can therefore
serve as the transition matrix for a stochastic process. here, d is a diag-
j aij. a random walk on
g is a process generating sequences of vertices vi1, vi2, vi3, . . . according to
p(ik+1|i1, . . . ik) =   aik,ik+1. the tth power of   a thus describes t-length walks,
i.e., (   at)ij is the id203 of a transition from vertex vj to vertex vi via
a walk of length t (problem bugbug). if p0 is an initial id203 dis-
tribution over vertices, then the id203 distribution pt describing the
location of our random walker at time t is pt =   atp0. the jth component of
pt denotes the id203 of    nishing a t-length walk at vertex vj. a random
walk need not continue inde   nitely; to model this, we associate every node
vik in the graph with a stopping id203 qik . the overall id203 of
stopping after t steps is given by q(cid:62)pt.
given two graphs g(v, e) and g(cid:48)(v (cid:48), e(cid:48)), their direct product g   is a

onal matrix of node degrees, i.e., dii = di = (cid:80)

graph with vertex set

v   = {(vi, v(cid:48)

r) : vi     v, v(cid:48)

r     v (cid:48)},

(6.7)

and edge set

r), (vj, v(cid:48)

e   = {((vi, v(cid:48)

s)) : (vi, vj)     e     (v(cid:48)

r, v(cid:48)

s)     e(cid:48)}.

(6.8)
in other words, g   is a graph over pairs of vertices from g and g(cid:48), and
two vertices in g   are neighbors if and only if the corresponding vertices
in g and g(cid:48) are both neighbors; see figure 6.1 for an illustration. if a and
a(cid:48) are the respective adjacency matrices of g and g(cid:48), then the adjacency

160

6 kernels and function spaces

11   

21   

31   

2

1

3

g1

g2

1   

4   

2   

3   

34   

24   

14   

12   

22   

32   

33   

23   
g  

13   

fig. 6.1. two graphs (g1 & g2) and their direct product (g  ). each node of the
direct product graph is labeled with a pair of nodes (6.7); an edge exists in the
direct product if and only if the corresponding nodes are adjacent in both original
graphs (6.8). for instance, nodes 11(cid:48) and 32(cid:48) are adjacent because there is an edge
between nodes 1 and 3 in the    rst, and 1(cid:48) and 2(cid:48) in the second graph.

matrix of g   is a   = a    a(cid:48). similarly,   a   =   a      a(cid:48). performing a random
walk on the direct product graph is equivalent to performing a simultaneous
random walk on g and g(cid:48). if p and p(cid:48) denote initial id203 distributions
over the vertices of g and g(cid:48), then the corresponding initial id203
distribution on the direct product graph is p   := p     p(cid:48). likewise, if q and
q(cid:48) are stopping probabilities (that is, the id203 that a random walk
ends at a given vertex), then the stopping id203 on the direct product
graph is q   := q     q(cid:48).
to de   ne a kernel which computes the similarity between g and g(cid:48), one
natural idea is to simply sum up q(cid:62)     at  p   for all values of t. however, this
sum might not converge, leaving the kernel value unde   ned. to overcome
this problem, we introduce appropriately chosen non-negative coe   cients
  (t), and de   ne the kernel between g and g(cid:48) as

k(g, g(cid:48)) :=

  (t) q(cid:62)

     at  p  .

(6.9)

t=0

this idea can be extended to graphs whose nodes are associated with labels
by replacing the matrix   a   with a matrix of label similarities. for appro-
priate choices of   (t) the above sum converges and e   cient algorithms for
computing the kernel can be devised. see [?] for details.

as it turns out, the simple idea of performing a random walk on the prod-

   (cid:88)

6.2 kernels

161

uct graph can be extended to compute kernels on auto regressive moving
average (arma) models [vsv07]. similarly, it can also be used to de   ne
kernels between transducers. connections between the so-called rational ker-
nels on transducers and the graph kernels de   ned via (6.9) are made explicit
in [?].

6.2 kernels

6.2.1 feature maps

give examples, linear classi   er, nonlinear ones with r2-r3 map

6.2.2 the kernel trick

6.2.3 examples of kernels

gaussian, polynomial, linear, texts, graphs

- stress the fact that there is a di   erence between structure in the input

space and structure in the output space

6.3 algorithms

6.3.1 kernel id88

6.3.2 trivial classi   er

6.3.3 kernel principal component analysis

6.4 reproducing kernel hilbert spaces

as it turns out, this class of functions coincides with the class of positive
semi-de   nite functions. intuitively, the notion of a positive semi-de   nite
function is an extension of the familiar notion of a positive semi-de   nite
matrix (also see appendix bugbug):
de   nition 6.2 a real n    n symmetric matrix k satisfying

  i  jki,j     0

(6.10)

(cid:88)

i,j

for all   i,   j     r is called positive semi-de   nite. if equality in (6.10) occurs
only when   1, . . . ,   n = 0, then k is said to be positive de   nite.
de   nition 6.3 given a set of points x1, . . . , xn     x and a function k, the
matrix

ki,j = k(xi, xj)

(6.11)

162

6 kernels and function spaces

is called the gram matrix or the kernel matrix of k with respect to x1, . . . , xn.
de   nition 6.4 let x be a nonempty set, k : x    x     r be a function. if
k gives rise to a positive (semi-)de   nite gram matrix for all x1, . . . , xn     x
and n     n then k is said to be positive (semi-)de   nite.

clearly, every id81 k of the form (6.1) is positive semi-de   nite.
to see this simply write

(cid:42)(cid:88)

(cid:43)

(cid:88)

(cid:88)

(cid:88)

  i  jk(xi, xj) =

  i  j (cid:104)xi, xj(cid:105) =

  ixi,

  jxj

    0.

i,j

i,j

i

j

we now establish the converse, that is, we show that every positive semi-
de   nite id81 can be written as (6.1). towards this end, de   ne a
map    from x into the space of functions mapping x to r (denoted rx) via
  (x) = k(  , x). in other words,   (x) : x     r is a function which assigns the
value k(x(cid:48), x) to x(cid:48)     x. next construct a vector space by taking all possible
linear combinations of   (x)

n(cid:88)

i=1

n(cid:88)

i=1

f (  ) =

  i  (xi) =

  ik(  , xi),

(6.12)

where i     n,   i     r, and xi     x are arbitrary. this space can be endowed
with a natural dot product

(cid:104)f, g(cid:105) =

  i  jk(xi, x(cid:48)
j).

(6.13)

(cid:80)n(cid:48)
j=1   jf (x(cid:48)

j), independent of   i. similarly, for g, note that (cid:104)f, g(cid:105) =(cid:80)n

to see that the above dot product is well de   ned even though it contains
the expansion coe   cients (which need not be unique), note that (cid:104)f, g(cid:105) =
this time independent of   j. this also shows that (cid:104)f, g(cid:105) is bilinear. symme-
try follows because (cid:104)f, g(cid:105) = (cid:104)g, f(cid:105), while the positive semi-de   niteness of k
implies that

i=1   if (xi),

(cid:104)f, f(cid:105) =

  i  jk(xi, xj)     0.

applying (6.13) shows that for all functions (6.12) we have

i,j

in particular

(cid:104)f, k(  , x)(cid:105) = f (x).

(cid:10)k(  , x), k(  , x(cid:48))(cid:11) = k(x, x(cid:48)).

(6.14)

(6.15)

(6.16)

n(cid:88)

n(cid:48)(cid:88)

i=1

j=1

(cid:88)

6.4 reproducing kernel hilbert spaces

163

in view of these properties, k is called a reproducing kernel. by using (6.15)
and the following property of positive semi-de   nite functions (problem 6.1)

k(x, x(cid:48))2     k(x, x)    k(x(cid:48), x(cid:48))

(6.17)

we can now write

|f (x)|2 = |(cid:104)f, k(  , x)(cid:105)|     k(x, x)    (cid:104)f, f(cid:105) .

(6.18)
from the above inequality, f = 0 whenever (cid:104)f, f(cid:105) = 0, thus establishing
(cid:104)  ,  (cid:105) as a valid dot product. in fact, one can complete the space of functions
(6.12) in the norm corresponding to the dot product (6.13), and thus get a
hilbert space h, called the reproducing kernel hilbert space (rkhs).
an alternate way to de   ne a rkhs is as a hilbert space h on functions
from some input space x to r with the property that for any f     h and
x     x, the point evaluations f     f (x) are continuous (in particular, all
points values f (x) are well de   ned, which already distinguishes an rkhs
from many l2 hilbert spaces). given the point evaluation functional, one
can then construct the reproducing kernel using the riesz representation
theorem. the moore-aronszajn theorem states that, for every positive semi-
de   nite kernel on x    x, there exists a unique rkhs and vice versa.
we    nish this section by noting that (cid:104)  ,  (cid:105) is a positive semi-de   nite func-
tion in the vector space of functions (6.12). this follows directly from the
bilinearity of the dot product and (6.14) by which we can write for functions
f1, . . . , fp and coe   cients   1, . . . ,   p

(cid:88)

(cid:88)

(cid:42)(cid:88)

(cid:43)

(cid:88)

  i  j (cid:104)fi, fj(cid:105) =

  ifi,

  jfj

    0.

(6.19)

i

j

i

j

6.4.1 hilbert spaces

evaluation functionals, inner products

6.4.2 theoretical properties

mercer   s theorem, positive semide   niteness

6.4.3 id173

representer theorem, id173

164

6.5 banach spaces

6.5.1 properties

6.5.2 norms and convex sets

6 kernels and function spaces

- smoothest function (l2) - smallest coe   cients (l1) - structured priors
(cap formalism)

problems

problem 6.1 show that (6.17) holds for an arbitrary positive semi-de   nite
function k.

problem 6.2 show that the inhomogeneous polynomial kernel (6.3) is a
valid kernel and that it computes all monomials of degree up to d.
problem 6.3 (k-spectrum kernel {2}) given two strings x and x(cid:48) show
how one can compute the k-spectrum kernel (section 6.1.1.5) in o((|x| +
|x(cid:48)|)k) time. hint: you need to use a trie.

7

linear models

a hyperplane in a space h endowed with a dot product (cid:104)  ,  (cid:105) is described by
the set

{x     h|(cid:104)w, x(cid:105) + b = 0}

(7.1)
where w     h and b     r. such a hyperplane naturally divides h into two
half-spaces: {x     h|(cid:104)w, x(cid:105) + b     0} and {x     h|(cid:104)w, x(cid:105) + b < 0}, and
hence can be used as the decision boundary of a binary classi   er. in this
chapter we will study a number of algorithms which employ such linear
decision boundaries. although such models look restrictive at    rst glance,
when combined with kernels (chapter 6) they yield a large class of useful
algorithms.
all the algorithms we will study in this chapter maximize the margin.
given a set x = {x1, . . . , xm}, the margin is the distance of the closest point
in x to the hyperplane (7.1). elementary geometric arguments (problem 7.1)
show that the distance of a point xi to a hyperplane is given by |(cid:104)w, xi(cid:105) +
b|/(cid:107)w(cid:107), and hence the margin is simply

|(cid:104)w, xi(cid:105) + b|

(cid:107)w(cid:107)

min

i=1,...,m

.

(7.2)

note that the parameterization of the hyperplane (7.1) is not unique; if we
multiply both w and b by the same non-zero constant, then we obtain the
same hyperplane. one way to resolve this ambiguity is to set

|(cid:104)w, xi(cid:105) + b| = 1.

min

i=1,...m

in this case, the margin simply becomes 1/(cid:107)w(cid:107). we postpone justi   cation
of margin maximization for later and jump straight ahead to the description
of various algorithms.

7.1 support vector classi   cation

consider a binary classi   cation task, where we are given a training set
{(x1, y1), . . . , (xm, ym)} with xi     h and yi     {  1}. our aim is to    nd
a linear decision boundary parameterized by (w, b) such that (cid:104)w, xi(cid:105) + b     0

165

166

7 linear models

{x | (cid:104)w, x(cid:105) + b =    1}

{x | (cid:104)w, x(cid:105) + b = 1}

yi = +1

x2

yi =    1

w

x1

{x | (cid:104)w, x(cid:105) + b = 0}

(cid:104)w, x1(cid:105) + b = +1
(cid:104)w, x2(cid:105) + b =    1
(cid:104)w, x1     x2(cid:105) = 2

(cid:68) w(cid:107)w(cid:107) , x1     x2

(cid:69)

= 2(cid:107)w(cid:107)

fig. 7.1. a linearly separable toy binary classi   cation problem of separating the
diamonds from the circles. we normalize (w, b) to ensure that mini=1,...m |(cid:104)w, xi(cid:105) +
b| = 1. in this case, the margin is given by 1(cid:107)w(cid:107) as the calculation in the inset shows.

whenever yi = +1 and (cid:104)w, xi(cid:105)+b < 0 whenever yi =    1. furthermore, as dis-
cussed above, we    x the scaling of w by requiring mini=1,...m |(cid:104)w, xi(cid:105)+b| = 1.
a compact way to write our desiderata is to require yi((cid:104)w, xi(cid:105) + b)     1 for
all i (also see figure 7.1). the problem of maximizing the margin therefore
reduces to

or equivalently

max
w,b

s.t.

1
(cid:107)w(cid:107)
yi((cid:104)w, xi(cid:105) + b)     1 for all i,

min
w,b

s.t.

(cid:107)w(cid:107)2
1
2
yi((cid:104)w, xi(cid:105) + b)     1 for all i.

(7.3a)

(7.3b)

(7.4a)

(7.4b)

this is a constrained id76 problem with a quadratic objec-
tive function and linear constraints (see section 3.3). in deriving (7.4) we
implicitly assumed that the data is linearly separable, that is, there is a
hyperplane which correctly classi   es the training data. such a classi   er is
called a hard margin classi   er. if the data is not linearly separable, then
(7.4) does not have a solution. to deal with this situation we introduce

7.1 support vector classi   cation

167

non-negative slack variables   i to relax the constraints:

yi((cid:104)w, xi(cid:105) + b)     1       i.

given any w and b the constraints can now be satis   ed by making   i large
enough. this renders the whole optimization problem useless. therefore, one
has to penalize large   i. this is done via the following modi   ed optimization
problem:

m(cid:88)

min
w,b,  

s.t.

  i

c
m

(cid:107)w(cid:107)2 +
1
2
yi((cid:104)w, xi(cid:105) + b)     1       i for all i
  i     0,

i=1

where c > 0 is a penalty parameter. the resultant classi   er is said to be a
soft margin classi   er. by introducing non-negative lagrange multipliers   i
and   i one can write the lagrangian (see section 3.3)

  i(1       i     yi((cid:104)w, xi(cid:105) + b))     m(cid:88)

  i  i.

l(w, b,   ,   ,   ) =

1
2

(cid:107)w(cid:107)2 +

next take gradients with respect to w, b and    and set them to zero.

i=1

i=1

  i +

c
m

m(cid:88)
m(cid:88)
   wl = w     m(cid:88)
   bl =     m(cid:88)

i=1

  iyixi = 0

  iyi = 0

i=1

     il =

c
m

      i       i = 0.

(7.5a)

(7.5b)

(7.5c)

i=1

(7.6a)

(7.6b)

(7.6c)

substituting (7.6) into the lagrangian and simplifying yields the dual ob-
jective function:

(cid:88)

    1
2

m(cid:88)

yiyj  i  j (cid:104)xi, xj(cid:105) +

i,j

i=1

  i,

(7.7)

which needs to be maximized with respect to   . for notational convenience
we will minimize the negative of (7.7) below. next we turn our attention
to the dual constraints. recall that   i     0 and   i     0, which in conjunc-
tion with (7.6c) immediately yields 0       i     c
m . furthermore, by (7.6b)
i=1   iyi = 0. putting everything together, the dual optimization problem

(cid:80)m

168

boils down to

min

  

s.t.

(cid:88)
m(cid:88)

1
2

i,j

  iyi = 0

yiyj  i  j (cid:104)xi, xj(cid:105)     m(cid:88)

  i

i=1

7 linear models

(7.8a)

(7.8b)

i=1

0       i     c
m

.

(7.8c)
if we let h be a m    m matrix with entries hij = yiyj (cid:104)xi, xj(cid:105), while e,   ,
and y be m-dimensional vectors whose i-th components are one,   i, and yi
respectively, then the above dual can be compactly written as the following
quadratic program (qp) (section 3.3.3):

  (cid:62)h         (cid:62)e

1
2

  

min
s.t.   (cid:62)y = 0

0       i     c
m

.

(7.9a)

(7.9b)

(7.9c)

before turning our attention to algorithms for solving (7.9), a number of
observations are in order. first, note that computing h only requires com-
puting dot products between training examples. if we map the input data to
a reproducing kernel hilbert space (rkhs) via a feature map   , then we
can still compute the entries of h and solve for the optimal   . in this case,
hij = yiyj (cid:104)  (xi),   (xj)(cid:105) = yiyjk(xi, xj), where k is the kernel associated
with the rkhs. given the optimal   , one can easily recover the decision
boundary. this is a direct consequence of (7.6a), which allows us to write w
as a linear combination of the training data:

w =

  iyi  (xi),

m(cid:88)
m(cid:88)

i=1

and hence the decision boundary as

(cid:104)w, x(cid:105) + b =

  iyik(xi, x) + b.

(7.10)

i=1

by the kkt conditions (section 3.3) we have

  i(1       i     yi((cid:104)w, xi(cid:105) + b)) = 0 and   i  i = 0.

we now consider three cases for yi((cid:104)w, xi(cid:105) + b) and the implications of the
kkt conditions (see figure 7.2).

7.1 support vector classi   cation

169

{x | (cid:104)w, x(cid:105) + b =    1}

{x | (cid:104)w, x(cid:105) + b = 1}

fig. 7.2. the picture depicts the well classi   ed points (yi((cid:104)w, xi(cid:105) + b) > 1 in black,
the support vectors yi((cid:104)w, xi(cid:105) + b) = 1 in blue, and margin errors yi((cid:104)w, xi(cid:105) + b) < 1
in red.

yi((cid:104)w, xi(cid:105) + b) < 1: in this case,   i > 0, and hence the kkt conditions
m (see (7.6c)). such points

imply that   i = 0. consequently,   i = c
are said to be margin errors.

yi((cid:104)w, xi(cid:105) + b) > 1: in this case,   i = 0, (1     i   yi((cid:104)w, xi(cid:105)+b)) < 0, and by
the kkt conditions   i = 0. such points are said to be well classi   ed.
it is easy to see that the decision boundary (7.10) does not change
even if these points are removed from the training set.

yi((cid:104)w, xi(cid:105) + b) = 1: in this case   i = 0 and   i     0. since   i is non-negative
and satis   es (7.6c) it follows that 0       i     c
m . such points are said
to be on the margin. they are also sometimes called support vectors.

since the support vectors satisfy yi((cid:104)w, xi(cid:105) + b) = 1 and yi     {  1} it follows
that b = yi     (cid:104)w, xi(cid:105) for any support vector xi. however, in practice to
recover b we average

b = yi    (cid:88)

(cid:104)w, xi(cid:105) .

(7.11)

i

over all support vectors, that is, points xi for which 0 <   i < c
m . because
it uses support vectors, the overall algorithm is called c-support vector
classi   er or c-sv classi   er for short.

170

7 linear models

7.1.1 a regularized risk minimization viewpoint
a closer examination of (7.5) reveals that   i = 0 whenever yi((cid:104)w, xi(cid:105)+b) > 1.
on the other hand,   i = 1     yi((cid:104)w, xi(cid:105) + b) whenever yi((cid:104)w, xi(cid:105) + b) <
1. in short,   i = max(0, 1     yi((cid:104)w, xi(cid:105) + b)). using this observation one
can eliminate   i from (7.5), and write it as the following unconstrained
optimization problem:

min
w,b

(cid:107)w(cid:107)2 +

1
2

c
m

max(0, 1     yi((cid:104)w, xi(cid:105) + b)).

(7.12)

m(cid:88)

i=1

writing (7.5) as (7.12) is particularly revealing because it shows that a
support vector classi   er is nothing but a regularized risk minimizer. here
the regularizer is the square norm of the decision hyperplane 1
the id168 is the so-called binary hinge loss (figure 7.3):

2(cid:107)w(cid:107)2, and

l(w, x, y) = max(0, 1     y((cid:104)w, x(cid:105) + b)).

(7.13)

it is easy to verify that the binary hinge loss (7.13) is convex but non-
di   erentiable (see figure 7.3) which renders the overall objective function
(7.12) to be convex but non-smooth. there are two di   erent strategies to
minimize such an objective function. if minimizing (7.12) in the primal, one
can employ non-smooth convex optimizers such as bundle methods (section
3.2.7). this yields a d dimensional problem where d is the dimension of x.
on the other hand, since (7.12) is strongly convex because of the presence
of the 1
(see lemma 3.10). the dual problem is m dimensional and contains linear
constraints. this strategy is particularly attractive when the kernel trick is
used or whenever d (cid:29) m. in fact, the dual problem obtained via fenchel
duality is very related to the quadratic programming problem (7.9) obtained
via lagrange duality (problem 7.4).

2(cid:107)w(cid:107)2 term, its fenchel dual has a lipschitz continuous gradient

7.1.2 an exponential family interpretation

our motivating arguments for deriving the id166 algorithm have largely
been geometric. we now show that an equally elegant probabilistic interpre-
tation also exists. assuming that the training set {(x1, y1), . . . , (xm, ym)}
was drawn iid from some underlying distribution, and using the bayes rule
(1.15) one can write the likelihood

p(  |x, y )     p(  )p(y |x,   ) = p(  )

p(yi|xi,   ),

(7.14)

m(cid:89)

i=1

7.1 support vector classi   cation

171

loss

y((cid:104)w, x(cid:105) + b)

fig. 7.3. the binary hinge loss. note that the loss is convex but non-di   erentiable
at the kink point. furthermore, it increases linearly as the distance from the decision
hyperplane y((cid:104)w, x(cid:105) + b) decreases.

and hence the negative log-likelihood

    log p(  |x, y ) =     m(cid:88)

log p(yi|xi,   )     log p(  ) + const.

(7.15)

i=1

in the absence of any prior knowledge about the data, we choose a zero
mean unit variance isotropic normal distribution for p(  ). this yields

    log p(  |x, y ) =

1
2

log p(yi|xi,   ) + const.

(7.16)

(cid:107)  (cid:107)2     m(cid:88)

i=1

the maximum aposteriori (map) estimate for    is obtained by minimizing
(7.16) with respect to   . given the optimal   , we can predict the class label
at any given x via

y    = argmax

p(y|x,   ).

(7.17)
of course, our aim is not just to maximize p(yi|xi,   ) but also to ensure
that p(y|xi,   ) is small for all y (cid:54)= yi. this, for instance, can be achieved by
requiring

y

p(yi|xi,   )
p(y|xi,   )

      , for all y (cid:54)= yi and some        1.

(7.18)

as we saw in section 2.3 exponential families of distributions are rather    ex-
ible modeling tools. we could, for instance, model p(yi|xi,   ) as a conditional
exponential family distribution. recall the de   nition:

p(y|x,   ) = exp ((cid:104)  (x, y),   (cid:105)     g(  |x)) .

(7.19)

172

7 linear models

here   (x, y) is a joint feature map which depends on both the input data x
and the label y, while g(  |x) is the log-partition function. now (7.18) boils
down to

p(yi|xi,   )

maxy(cid:54)=yi p(y|xi,   )

= exp

  (xi, yi)     max
y(cid:54)=yi

  (xi, y),   

      .

(7.20)

(cid:18)(cid:28)

(cid:29)(cid:19)

if we choose    such that log    = 1, set   (x, y) = y
y     {  1} we can rewrite (7.20) as

2   (x), and observe that

2

(7.21)
by replacing     log p(yi|xi,   ) in (7.16) with the condition (7.21) we obtain
the following objective function:

  (xi),   

2

= yi (cid:104)  (xi),   (cid:105)     1.

(cid:68) yi

  (xi)    (cid:16)    yi

(cid:17)

(cid:69)

min

  
s.t.

(cid:107)  (cid:107)2
1
2
yi (cid:104)  (xi),   (cid:105)     1 for all i,

(7.22a)

(7.22b)

which recovers (7.4), but without the bias b. the prediction function is
recovered by noting that (7.17) specializes to

y    = argmax
y   {  1}

(cid:104)  (x, y),   (cid:105) = argmax
y   {  1}

y
2

(cid:104)  (x),   (cid:105) = sign((cid:104)  (x),   (cid:105)).

(7.23)

as before, we can replace (7.21) by a linear penalty for constraint viola-
tion in order to recover (7.5). the quantity log
maxy(cid:54)=yi p(y|xi,  ) is sometimes
called the log-odds ratio, and the above discussion shows that id166s can
be interpreted as maximizing the log-odds ratio in the exponential family.
this interpretation will be developed further when we consider extensions of
id166s to tackle multiclass, multilabel, and id170 problems.

p(yi|xi,  )

7.1.3 specialized algorithms for training id166s
the main task in training id166s boils down to solving (7.9). the m    m
matrix h is usually dense and cannot be stored in memory. decomposition
methods are designed to overcome these di   culties. the basic idea here
is to identify and update a small working set b by solving a small sub-
problem at every iteration. formally, let b     {1, . . . , m} be the working set
and   b be the corresponding sub-vector of   . de   ne   b = {1, . . . , m} \ b
and      b analogously. in order to update   b we need to solve the following

7.1 support vector classi   cation

sub-problem of (7.9) obtained by freezing      b:

b   (cid:62)

  b

1
2

min
  b

(cid:2)   (cid:62)
s.t. (cid:2)   (cid:62)
(cid:20) hbb hb   b

b   (cid:62)
0       i     c
m

  b

h   bb h   b   b

(cid:3)(cid:20) hbb hb   b
(cid:3) y = 0
(cid:21)

for all i     b.

(cid:21)(cid:20)   b

     b

(cid:21)

   (cid:2)   (cid:62)

b   (cid:62)

  b

(cid:3) e

173

(7.24a)

(7.24b)

(7.24c)

here,

h   bb h   b   b

is a permutation of the matrix h. by eliminating

constant terms and rearranging, one can simplify the above problem to

b(h   bb     b     e)

  (cid:62)
bhbb  b +   (cid:62)
1
min
2
  b
byb =      (cid:62)
s.t.   (cid:62)
0       i     c
m

  by   b
for all i     b.

(7.25a)

(7.25b)

(7.25c)

an extreme case of a decomposition method is the sequential minimal op-
timization (smo) algorithm of platt [pla99], which updates only two coef-
   cients per iteration. the advantage of this strategy as we will see below is
that the resultant sub-problem can be solved analytically. without loss of
and d = (     (cid:62)

generality let b = {i, j}, and de   ne s = yi/yj,(cid:2) ci

(cid:3) = (h   bb     b     e)(cid:62)

  by   b/yj). then (7.25) specializes to

cj

min
  i,  j

1
2

(hii  2

i + hjj  2

j + 2hij  j  i) + ci  i + cj  j

s.t. s  i +   j = d

0       i,   j     c
m

.

this qp in two variables has an analytic solution.

(7.26a)

(7.26b)

(7.26c)

lemma 7.1 (analytic solution of 2 variable qp) de   ne bounds

m

(cid:40)
(cid:40)min( c

d    c
max(0,
s
max(0, d
s )
m , d
s )
d    c
min( c
m ,
s

m

l =

h =

)

if s > 0

otherwise

if s > 0

)

otherwise,

(7.27)

(7.28)

174

and auxiliary variables

7 linear models

   = (hii + hjjs2     2shij) and
   = (cjs     ci     hijd + hjjds).

(7.29)

(7.30)

the optimal value of (7.26) can be computed analytically as follows: if    = 0
then

(cid:40)

  i =

l

if    < 0

h otherwise.

if    > 0, then   i = max(l, min(h,   /  )). in both cases,   j = (d     s  i).
proof eliminate the equality constraint by setting   j = (d     s  i). due to
the constraint 0       j     c
m it follows that s  i = d       j can be bounded
via d     c
m one can write
l       i     h where l and h are given by (7.27) and (7.28) respectively.
substituting   j = (d   s  i) into the objective function, dropping the terms
which do not depend on   i, and simplifying by substituting    and    yields
the following optimization problem in   i:

m     s  i     d. combining this with 0       i     c

1
2

i          i  
  2
min
  i
s.t. l       i     h.

first consider the case when    = 0. in this case,   i = l if    < 0 otherwise
  i = h. on other hand, if    > 0 then the unconstrained optimum of the
above optimization problem is given by   /  . the constrained optimum is
obtained by clipping appropriately: max(l, min(h,   /  )). this concludes
the proof.

to complete the description of smo we need a valid stopping criterion as
well as a scheme for selecting the working set at every iteration. in order
to derive a stopping criterion we will use the kkt gap, that is, the extent
to which the kkt conditions are violated. towards this end introduce non-
negative lagrange multipliers b     r,        rm and        rm and write the
lagrangian of (7.9).

l(  , b,   ,   ) =

  (cid:62)h         (cid:62)e + b  (cid:62)y       (cid:62)   +   (cid:62)(       c
1
2
m
2   (cid:62)h         (cid:62)e be the objective function and    j(  ) =
if we let j(  ) = 1
h       e its gradient, then taking gradient of the lagrangian with respect to
   and setting it to 0 shows that

(7.31)

e).

   j(  ) + by =          .

(7.32)

7.1 support vector classi   cation

175

furthermore, by the kkt conditions we have

  i  i = 0 and   i(

(7.33)
with   i     0 and   i     0. equations (7.32) and (7.33) can be compactly
rewritten as

      i) = 0,

c
m

   j(  )i + byi     0 if   i = 0
   j(  )i + byi     0 if   i =
c
m
   j(  )i + byi = 0 if 0 <   i <
since yi     {  1}, we can further rewrite (7.34) as
   yi   j(  )i     b for all i     iup
   yi   j(  )i     b for all i     idown,

c
m

.

(7.34a)

(7.34b)

(7.34c)

where the index sets iup and idown are de   ned as

iup = {i :   i <
idown = {i :   i <

c
m
c
m

, yi = 1 or   i > 0, yi =    1}
, yi =    1 or   i > 0, yi = 1}.

(7.35a)

(7.35b)

in summary, the kkt conditions imply that    is a solution of (7.9) if and
only if

m(  )     m (  )

where

m(  ) = max
i   iup

   yi   j(  )i and m (  ) = min
i   idown

   yi   j(  )i.

(7.36)

therefore, a natural stopping criterion is to stop when the kkt gap falls
below a desired tolerance  , that is,

m(  )     m (  ) +  .

(7.37)

finally, we turn our attention to the issue of working set selection. the
   rst order approximation to the objective function j(  ) can be written as

j(   + d)     j(  ) +    j(  )(cid:62)d.

we set d(cid:62) =(cid:2) d(cid:62)

since we are only interested in updating coe   cients in the working set b

b 0 (cid:3), in which case we can rewrite the above    rst order

176

approximation as

7 linear models

   j(  )(cid:62)

bdb     j(   + d)     j(  ).

from among all possible directions db we wish to choose one which decreases
the objective function the most while maintaining feasibility. this is best
expressed as the following optimization problem:

min
db
s.t.

bdb

   j(  )(cid:62)
y(cid:62)
bdb = 0
di     0 if   i = 0 and i     b
di     0 if   i =
and i     b
    1     di     1.

c
m

(7.38a)

(7.38b)

(7.38c)

(7.38d)

(7.38e)
here (7.38b) comes from y(cid:62)(   + d) = 0 and y(cid:62)   = 0, while (7.38c) and
(7.38d) comes from 0       i     c
m . finally, (7.38e) prevents the objective
function from diverging to       . if we specialize (7.38) to smo, we obtain

min
i,j

s.t.

   j(  )idi +    j(  )jdj

yidi + yjdj = 0
dk     0 if   k = 0 and k     {i, j}
dk     0 if   k =
and k     {i, j}
    1     dk     1 for k     {i, j}.

c
m

(7.39a)

(7.39b)

(7.39c)

(7.39d)

(7.39e)

at    rst glance, it seems that choosing the optimal i and j from the set
{1, . . . , m}  {1, . . . m} requires o(m2) e   ort. we now show that o(m) e   ort
su   ces.
de   ne new variables   dk = ykdk for k     {i, j}, and use the observation
yk     {  1} to rewrite the objective function as

(   yi   j(  )i + yj   j(  )j)   dj.

consider the case       j(  )iyi           j(  )jyj. because of the constraints
(7.39c) and (7.39d) if we choose i     iup and j     idown, then   dj =    1 and
  di = 1 is feasible and the objective function attains a negative value. for
all other choices of i and j (i, j     iup; i, j     idown; i     idown and j     iup)
the objective function value of 0 is attained by setting   di =   dj = 0. the
case       j(  )jyj           j(  )iyi is analogous. in summary, the optimization

7.2 extensions

problem (7.39) boils down to

177

min

i   iup,j   idown

yi   j(  )i     yj   j(  )j = min
i   iup

yi   j(  )i     max
j   idown

yj   j(  )j,

which clearly can be solved in o(m) time. comparison with (7.36) shows
that at every iteration of smo we choose to update coe   cients   i and   j
which maximally violate the kkt conditions.

7.2 extensions

7.2.1 the    trick

in the soft margin formulation the parameter c is a trade-o    between two
con   icting requirements namely maximizing the margin and minimizing the
training error. unfortunately, this parameter is rather unintuitive and hence
di   cult to tune. the   -id166 was proposed to address this issue. as theorem
7.3 below shows,    controls the number of support vectors and margin errors.
the primal problem for the   -id166 can be written as

m(cid:88)

min
w,b,  ,  

s.t.

  i

1
  m

(cid:107)w(cid:107)2        +
1
2
yi((cid:104)w, xi(cid:105) + b)              i for all i
  i     0, and        0.

i=1

as before, if we write the lagrangian by introducing non-negative lagrange
multipliers, take gradients with respect to the primal variables and set them
to zero, and substitute the result back into the lagrangian we obtain the
following dual:

yiyj  i  j (cid:104)xi, xj(cid:105)

min

  

s.t.

1
2

(cid:88)
m(cid:88)
m(cid:88)

i=1

i,j

  iyi = 0

  i     1

i=1

0       i     1
  m

.

(7.40a)

(7.40b)

(7.40c)

(7.41a)

(7.41b)

(7.41c)

(7.41d)

it turns out that the dual can be further simpli   ed via the following lemma.

178
7 linear models
lemma 7.2 let        [0, 1] and (7.41) be feasible. then there is at least one
i   i = 1. furthermore, if the    nal objective value

solution    which satis   es(cid:80)
of (7.41) is non-zero then all solutions satisfy(cid:80)
that(cid:80)

proof the feasible region of (7.41) is bounded, therefore if it is feasible
then there exists an optimal solution. let    denote this solution and assume

i   i > 1. in this case we can de   ne

i   i = 1.

1(cid:80)

     =

  ,

j   j

(cid:33)2

(cid:32)

1(cid:80)

j   j

and easily check that      is also feasible. as before, let h denote a m    m
matrix with hij = yiyj (cid:104)xi, xj(cid:105). since    is the optimal solution of (7.41) it
follows that

1
2

  (cid:62)h       1
2

    (cid:62)h      =

1
2

  (cid:62)h       1
2

  (cid:62)h  .

this implies that either 1
with the desired property or 1

2   (cid:62)h   = 0, in which case      is an optimal solution
2   (cid:62)h   (cid:54)= 0, in which case all optimal solutions

satisfy(cid:80)

i   i = 1.

in view of the above theorem one can equivalently replace (7.41) by the
following simpli   ed optimization problem with two equality constraints

yiyj  i  j (cid:104)xi, xj(cid:105)

min

  

s.t.

1
2

(cid:88)
m(cid:88)
m(cid:88)

i=1

i,j

  iyi = 0

  i = 1

i=1

0       i     1
  m

.

(7.42a)

(7.42b)

(7.42c)

(7.42d)

the following theorems, which we state without proof, explain the signif-
icance of    and the connection between   -id166 and the soft margin formu-
lation.

theorem 7.3 suppose we run   -id166 with kernel k on some data and
obtain    > 0. then

(i)    is an upper bound on the fraction of margin errors, that is points

for which yi ((cid:104)w, xi(cid:105) + bi) <   .

7.2 extensions

179

(ii)    is a lower bound on the fraction of support vectors, that is points

for which yi ((cid:104)w, xi(cid:105) + bi) =   .
(iii) suppose the data (x, y ) were generated iid from a distribution p(x, y)
such that neither p(x, y = +1) or p(x, y =    1) contain any discrete
components. moreover, assume that the kernel k is analytic and non-
constant. with id203 1, asympotically,    equals both the fraction
of support vectors and fraction of margin errors.

theorem 7.4 if (7.40) leads to a decision function with    > 0, then (7.5)
with c = 1

   leads to the same decision function.

7.2.2 squared hinge loss

in binary classi   cation, the actual loss which one would like to minimize is
the so-called 0-1 loss

l(w, x, y) =

if y((cid:104)w, x(cid:105) + b)     1
otherwise .

(7.43)

(cid:40)

0

1

this loss is di   cult to work with because it is non-convex (see figure 7.4). in

loss

y((cid:104)w, x(cid:105) + b)

fig. 7.4. the 0-1 loss which is non-convex and intractable is depicted in red. the
hinge loss is a convex upper bound to the 0-1 loss and shown in blue. the square
hinge loss is a di   erentiable convex upper bound to the 0-1 loss and is depicted in
green.

fact, it has been shown that    nding the optimal (w, b) pair which minimizes
the 0-1 loss on a training dataset of m labeled points is np hard [bdel03].
therefore various proxy functions such as the binary hinge loss (7.13) which
we discussed in section 7.1.1 are used. another popular proxy is the square

180

hinge loss:

7 linear models

l(w, x, y) = max(0, 1     y((cid:104)w, x(cid:105) + b))2.

(7.44)

besides being a proxy for the 0-1 loss, the squared hinge loss, unlike the
hinge loss, is also di   erentiable everywhere. this sometimes makes the opti-
mization in the primal easier. just like in the case of the hinge loss one can
derive the dual of the regularized risk minimization problem and show that
it is a quadratic programming problem (problem 7.5).

7.2.3 ramp loss

the ramp loss

l(w, x, y) = min(1     s, max(0, 1     y((cid:104)w, x(cid:105) + b)))

(7.45)
parameterized by s     0 is another proxy for the 0-1 loss (see figure 7.5).
although not convex, it can be expressed as the di   erence of two convex
functions

lconc(w, x, y) = max(0, 1     y((cid:104)w, x(cid:105) + b)) and
lcave(w, x, y) = max(0, s     y((cid:104)w, x(cid:105) + b)).

therefore the convex-concave procedure (ccp) we discussed in section

fig. 7.5. the ramp loss depicted here with s =    0.3 can be viewed as the sum
of a convex function namely the binary hinge loss (left) and a concave function
min(0, 1    y((cid:104)w, x(cid:105) + b)) (right). viewed alternatively, the ramp loss can be written
as the di   erence of two convex functions.

3.5.1 can be used to solve the resulting regularized risk minimization problem
with the ramp loss. towards this end write

(cid:107)w(cid:107)2 +

c
m

j(w) =

1
2

(cid:124)

m(cid:88)
(cid:123)(cid:122)

i=1

m(cid:88)

i=1

(cid:125)

    c
m

(cid:124)

(cid:123)(cid:122)

(cid:125)

lconc(w, xi, yi)

lcave(w, xi, yi)

.

(7.46)

jconc(w)

jcave(w)

m(cid:88)

i=1

   wj(w) =

c
m

7.3 support vector regression

181

recall that at every iteration of the ccp we replace jcave(w) by its    rst
order taylor approximation, computing which requires

this in turn can be computed as

   wlcave(w, xi, yi) =   iyixi with   i =

   wlcave(w, xi, yi).

(7.47)

(cid:40)   1

0

if s > y((cid:104)w, x(cid:105) + b)
otherwise.

(7.48)

m(cid:88)

i=1

(cid:33)

(cid:32)

m(cid:88)

i=1

c
m

ignoring constant terms, each iteration of the ccp algorithm involves solv-
ing the following minimization problem (also see (3.134))

j(w) =

(cid:107)w(cid:107)2 +

1
2

c
m

lconc(w, xi, yi)    

  iyixi

w.

(7.49)

let    denote a vector in rm with components   i. using the same notation
as in (7.9) we can write the following dual optimization problem which is
very closely related to the standard id166 dual (7.9) (see problem 7.6)

  (cid:62)h         (cid:62)e

1
2

  

min
s.t.   (cid:62)y = 0

    c
m

         i     c
m

(e       ).

(7.50a)

(7.50b)

(7.50c)

in fact, this problem can be solved by a smo solver with minor modi   ca-
tions. putting everything together yields algorithm 7.1.

algorithm 7.1 ccp for ramp loss

1: initialize   0 and   0
2: repeat
3:

solve (7.50) to    nd   t+1
compute   t+1 using (7.48)

4:
5: until   t+1 =   t

7.3 support vector regression

as opposed to classi   cation where the labels yi are binary valued, in re-
gression they are real valued. given a tolerance  , our aim here is to    nd a

182

7 linear models

loss

 

y     ((cid:104)w, x(cid:105) + b)

fig. 7.6. the   insensitive loss. all points which lie within the   tube shaded in
gray incur zero loss while points outside incur a linear loss.

hyperplane parameterized by (w, b) such that

|yi     ((cid:104)w, xi(cid:105) + b)|      .

(7.51)

in other words, we want to    nd a hyperplane such that all the training data
lies within an   tube around the hyperplane. we may not always be able to
   nd such a hyperplane, hence we relax the above condition by introducing
slack variables   +

i and write the corresponding primal problem as

i and      

m(cid:88)

min

w,b,  +,     

s.t.

i=1

c
m

i +      
(cid:107)w(cid:107)2 +
1
(  +
i )
2
yi     ((cid:104)w, xi(cid:105) + b)       +   +
((cid:104)w, xi(cid:105) + b)     yi       +      
i     0, and      
  +

i     0.

i

i

for all i

for all i

(7.52a)

(7.52b)

(7.52c)

(7.52d)

the lagrangian can be written by introducing non-negative lagrange mul-
tipliers   +

i and      
i :

i ,      

i ,   +

l(w, b,   +,      ,   +,      ,   +,      ) =

1
2

(cid:107)w(cid:107)2 +

m(cid:88)

i )     m(cid:88)

i +      
(  +

c
m
i (yi     ((cid:104)w, xi(cid:105) + b)             +)
  +

(  +

i=1

i=1

i   +

i +      

i      
i )

     
i (((cid:104)w, xi(cid:105) + b)     yi                ).

m(cid:88)
m(cid:88)

i=1

i=1

+

+

7.3 support vector regression

183

taking gradients with respect to the primal variables and setting them to
0, we obtain the following conditions:

m(cid:88)

i )xi

w =

m(cid:88)

(  +

i          
m(cid:88)

     

i

i=1

  +

i =

i =

i=1
c
m
c
m

i=1
  +
i +   +
     
i +      
(7.56)
    0 and substituting the above conditions into

(7.55)

i =

.

noting that   
the lagrangian yields the dual

{+,   }
i

{+,   }
,   
i

(cid:88)
m(cid:88)

i,j

min
  +,     

1
2

(  +

i          

i )(  +

j          

j )(cid:104)xi, xj(cid:105)

i )     m(cid:88)

i=1

yi(  +

i          
i )

s.t.

  +

+  

m(cid:88)

i=1

0       +
0          

i=1

i =

(  +

     

i +      
m(cid:88)
i     c
m
i     c
m

i=1

.

i

(7.53)

(7.54)

(7.57a)

(7.57b)

(7.57c)

(7.57d)

m(cid:88)

this is a quadratic programming problem with one equality constraint, and
hence a smo like decomposition method can be derived for    nding the
optimal coe   cients   + and       (problem 7.7).

as a consequence of (7.53), analogous to the classi   cation case, one can
map the data via a feature map    into an rkhs with kernel k and recover
the decision boundary f (x) = (cid:104)w,   (x)(cid:105) + b via

i=1

(  +

(  +

f (x) =

i          

i          

finally, the kkt conditions(cid:18) c

i )(cid:104)  (x)i,   (x)(cid:105) + b =
(cid:19)
     
  +
i = 0
i = 0 and
     
i (((cid:104)w, xi(cid:105) + b)     yi                ) = 0   +
i (yi     ((cid:104)w, xi(cid:105) + b)             +) = 0,

(cid:18) c

i )k(xi, x) + b. (7.58)

         

      +

(cid:19)

i=1

m

m

i

i

m(cid:88)

184

7 linear models

allow us to draw many useful conclusions:

    whenever |yi     ((cid:104)w, xi(cid:105) + b)| <  , this implies that   +

i =
     
i = 0. in other words, points which lie inside the   tube around the
hyperplane (cid:104)w, x(cid:105) + b do not contribute to the solution thus leading to
sparse expansions in terms of   .
    if ((cid:104)w, xi(cid:105)+b)   yi >   we have      

i > 0 and therefore      

i =      

i =   +

i = c

m . on the other
i = 0. the case yi     ((cid:104)w, xi(cid:105) + b) >   is symmetric
i > 0,   +

hand,   + = 0 and   +
and yields       = 0,   +

m , and      
    finally, if ((cid:104)w, xi(cid:105) + b)     yi =   we have      

i = c

i = 0.
i = 0 and 0          

m , while
i = 0. similarly, when yi     ((cid:104)w, xi(cid:105) + b) =   we obtain
i     c
i and      

i are never simultaneously non-zero.

m ,       = 0 and      

  + = 0 and   +
i = 0, 0       +
  +

note that   +

i     c

i = 0.

7.3.1 incorporating general id168s

using the same reasoning as in section 7.1.1 we can deduce from (7.52) that
the id168 of support vector regression is given by
l(w, x, y) = max(0,|y     (cid:104)w, x(cid:105)|      ).

(7.59)

it turns out that the support vector regression framework can be easily
extended to handle other, more general, convex id168s such as the
ones found in table 7.1. di   erent losses have di   erent properties and hence
lead to di   erent estimators. for instance, the square loss leads to penalized
least squares (ls) regression, while the laplace loss leads to the penalized
least absolute deviations (lad) estimator. huber   s loss on the other hand is
a combination of the penalized ls and lad estimators, and the pinball loss
with parameter        [0, 1] is used to estimate    -quantiles. setting    = 0.5
in the pinball loss leads to a scaled version of the laplace loss. if we de   ne
   = y   (cid:104)w, x(cid:105), then it is easily veri   ed that all these losses can all be written
as

               l+(        )

l   (           )
0

l(w, x, y) =

if    >  

if    <  
if        [    ,  ].

(7.60)

for all these di   erent id168s, the support vector regression formu-

7.3 support vector regression

lation can be written in a uni   ed fashion as follows

m(cid:88)

min

w,b,  +,     

s.t.

i ) + l   (     
i )

i=1

c
m

l+(  +

(cid:107)w(cid:107)2 +
1
2
yi     ((cid:104)w, xi(cid:105) + b)       +   +
((cid:104)w, xi(cid:105) + b)     yi       +      
i     0, and      
  +

i     0.

i

i

for all i

for all i

the dual in this case is given by
j          

i )(  +

(  +

min
  +,     

1
2

(cid:88)

i,j

i          
m(cid:88)

    c
m

m(cid:88)

i=1

  +

i =

m(cid:88)

i=1

i=1

{+,   }
0       
i
{+,   }
0       
i
{+,   }
  
i

    c
m

(cid:26)

s.t.

j )(cid:104)xi, xj(cid:105)
m(cid:88)

i )     m(cid:88)

(  +

i +      

t +(  +) + t    (     ) +  

i=1

i=1

     

i

     l{+,   }(  

{+,   }
i

)

(cid:27)

185

(7.61a)

(7.61b)

(7.61c)

(7.61d)

(7.62a)

yi(  +

i          
i )

(7.62b)

(7.62c)

(7.62d)

  {+,   } | c
m

     l{+,   }       

{+,   }
i

.

= inf

(7.62e)
here t +(  ) = l+(  )           l+(  ) and t    (  ) = l   (  )           l   (  ). we now show
how (7.62) can be specialized to the pinball loss. clearly, l+(  ) =       while
l   (     ) = (      1)  , and hence l   (  ) = (1      )  . therefore, t +(  ) = (      1)     
  (       1) = 0. similarly t    (  ) = 0. since      l+(  ) =    and      l   (  ) = (1        )
for all        0, it follows that the bounds on   {+,   } can be computed as
0       +
m (1        ). if we denote    =   +           and

m    and 0          

i     c

i     c

table 7.1. various id168s which can be used in support vector
regression. for brevity we denote y     (cid:104)w, x(cid:105) as    and write the loss
l(w, x, y) in terms of   .

 -insensitive loss max(0,|  |      )

laplace loss
square loss

huber   s robust loss

pinball loss

1

|  |
(cid:26) 1
2|  |2
(cid:26)     
2     2
|  |       

2

(       1)  

if |  |       
otherwise
if        0
otherwise.

186

7 linear models

observe that   = 0 for the pinball loss then (7.62) specializes as follows:

  i  j (cid:104)xi, xj(cid:105)     m(cid:88)

i=1

(cid:88)
m(cid:88)

1
2

i,j

min

  

s.t.

  i = 0

i=1
c
m

(       1)       i     c
m

  .

yi  i

(7.63a)

(7.63b)

(7.63c)

similar specializations of (7.62) for other id168s in table 7.1 can be
derived.

7.3.2 incorporating the    trick

one can also incorporate the    trick into support vector regression. the
primal problem obtained after incorporating the    trick can be written as

min

w,b,  +,     , 

s.t.

(cid:32)

m(cid:88)

(cid:33)

  +

1
  m

i +      
(cid:107)w(cid:107)2 +
1
(  +
i )
2
((cid:104)w, xi(cid:105) + b)     yi       +   +
yi     ((cid:104)w, xi(cid:105) + b)       +      
i     0,      
i     0, and       0.
  +

i=1

i

i

for all i

for all i

(7.64a)

(7.64b)

(7.64c)

(7.64d)

proceeding as before we obtain the following simpli   ed dual

(     

i       +

i )(     

j       +

j )(cid:104)xi, xj(cid:105)     m(cid:88)

i=1

min
  +,     

s.t.

1
2

(cid:88)
m(cid:88)
m(cid:88)

i=1

i,j

(     

i       +

i ) = 0

(     

i +   +

i ) = 1

i=1

0       +
0          

i     1
  m
i     1
  m

.

yi(     

i       +
i )

(7.65a)

(7.65b)

(7.65c)

(7.65d)

(7.65e)

7.4 novelty detection

the large margin approach can also be adapted to perform novelty detection
or quantile estimation. novelty detection is an unsupervised task where one

7.4 novelty detection

187

is interested in    agging a small fraction of the input x = {x1, . . . , xm} as
atypical or novel. it can be viewed as a special case of the quantile estimation
task, where we are interested in estimating a simple set c such that p r(x    
c)        for some        [0, 1]. one way to measure simplicity is to use the
volume of the set. formally, if |c| denotes the volume of a set, then the
quantile estimation task is to estimate

arginf{|c| s.t. p r(x     c)       }.

(7.66)

given the input data x one can compute the empirical density

(cid:40) 1

  p(x) =

m
0

if x     x
otherwise,

and estimate its (not necessarily unique)   -quantiles. unfortunately, such
estimates are very brittle and do not generalize well to unseen data. one
possible way to address this issue is to restrict c to be simple subsets such
as spheres or half spaces. in other words, we estimate simple sets which
contain    fraction of the dataset. for our purposes, we speci   cally work
with half-spaces de   ned by hyperplanes. while half-spaces may seem rather
restrictive remember that the kernel trick can be used to map data into
a high-dimensional space; half-spaces in the mapped space correspond to
non-linear decision boundaries in the input space. furthermore, instead of
explicitly identifying c we will learn an indicator function for c, that is, a
function f which takes on values    1 inside c and    1 elsewhere.
2(cid:107)w(cid:107)2 as a regularizer, the problem of estimating a hyperplane such

with 1

that a large fraction of the points in the input data x lie on one of its sides
can be written as:

m(cid:88)

min
w,  ,  

s.t.

1
  m

(cid:107)w(cid:107)2 +
  i       
1
2
(cid:104)w, xi(cid:105)              i for all i
  i     0.

i=1

(7.67a)

(7.67b)

(7.67c)

clearly, we want    to be as large as possible so that the volume of the half-
space (cid:104)w, x(cid:105)        is minimized. furthermore,        [0, 1] is a parameter which
is analogous to    we introduced for the   -id166 earlier. roughly speaking,
it denotes the fraction of input data for which (cid:104)w, xi(cid:105)       . an alternative
interpretation of (7.67) is to assume that we are separating the data set x
from the origin (see figure 7.7 for an illustration). therefore, this method
is also widely known as the one-class id166.

188

7 linear models

fig. 7.7. the novelty detection problem can be viewed as    nding a large margin
hyperplane which separates    fraction of the data points away from the origin.

the lagrangian of (7.67) can be written by introducing non-negative

lagrange multipliers   i, and   i:

l(w,   ,   ,   ,   ) =

(cid:107)w(cid:107)2 +

1
2

1
  m

  i        +

m(cid:88)

  i(         i     (cid:104)w, xi(cid:105))     m(cid:88)

  i  i.

i=1

i=1

i=1

by taking gradients with respect to the primal variables and setting them
to 0 we obtain

m(cid:88)

m(cid:88)

noting that   i,   i     0 and substituting the above conditions into the la-
grangian yields the dual

w =

  ixi

i=1
1
  m

      i     1
  m

  i =

m(cid:88)

i=1

  i = 1.

(cid:88)

i,j

m(cid:88)

i=1

min

  

1
2

  i  j (cid:104)xi, xj(cid:105)

s.t. 0       i     1
  m

  i = 1.

(7.68)

(7.69)

(7.70)

(7.71a)

(7.71b)

(7.71c)

7.5 margins and id203

189

this can easily be solved by a straightforward modi   cation of the smo
algorithm (see section 7.1.3 and problem 7.7). like in the previous sections,
an analysis of the kkt conditions shows that 0 <    if and only if (cid:104)w, xi(cid:105)       ;
such points are called support vectors. as before, we can replace (cid:104)xi, xj(cid:105) by
a kernel k(xi, xj) to transform half-spaces in the feature space to non-linear
shapes in the input space. the following theorem explains the signi   cance
of the parameter   .
theorem 7.5 assume that the solution of (7.71) satis   es    (cid:54)= 0, then the
following statements hold:

(i)    is an upper bound on the fraction of support vectors, that is points

for which (cid:104)w, xi(cid:105)       .

(ii) suppose the data x were generated independently from a distribution
p(x) which does not contain discrete components. moreover, assume
that the kernel k is analytic and non-constant. with id203 1,
asympotically,    equals the fraction of support vectors.

7.5 margins and id203

discuss the connection between probabilistic models and linear classi   ers.
issues of consistency, optimization, e   ciency, etc.

7.6 beyond binary classi   cation

in contrast to binary classi   cation where there are only two possible ways
to label a training sample, in some of the extensions we discuss below each
training sample may be associated with one or more of k possible labels.
therefore, we will use the decision function

y    = argmax
y   {1,...,k}

f (x, y) where f (x, y) = (cid:104)  (x, y), w(cid:105) .

(7.72)

recall that the joint feature map   (x, y) was introduced in section 7.1.2.
one way to interpret the above equation is to view f (x, y) as a compatibility
score between instance x and label y; we assign the label with the highest
compatibility score to x. there are a number of extensions of the binary
hinge loss (7.13) which can be used to estimate this score function. in all
these cases the objective function is written as

min

w

j(w) :=

(cid:107)w(cid:107)2 +

  
2

1
m

l(w, xi, yi).

(7.73)

m(cid:88)

i=1

7 linear models

190

(cid:80)m

2 (cid:107)w(cid:107)2 with the empirical
here    is a scalar which trades o    the regularizer 1
risk 1
i=1 l(w, xi, yi). plugging in di   erent id168s yields classi   ers
m
for di   erent settings. two strategies exist for    nding the optimal w. just
like in the binary id166 case, one can compute and maximize the dual of
(7.73). however, the number of dual variables becomes m|y|, where m is the
number of training points and |y| denotes the size of the label set. the second
strategy is to optimize (7.73) directly. however, the id168s we discuss
below are non-smooth, therefore non-smooth optimization algorithms such
as bundle methods (section 3.2.7) need to be used.

7.6.1 multiclass classi   cation

in multiclass classi   cation a training example is labeled with one of k pos-
sible labels, that is, y = {1, . . . , k}. we discuss two di   erent extensions of
the binary hinge loss to the multiclass setting. it can easily be veri   ed that
setting y = {  1} and   (x, y) = y
2   (x) recovers the binary hinge loss in both
cases.

7.6.1.1 additive multiclass hinge loss

a natural generalization of the binary hinge loss is to penalize all labels
which have been misclassi   ed. the loss can now be written as

max(cid:0)0, 1     ((cid:10)  (x, y)       (x, y(cid:48)), w(cid:11))(cid:1) .

l(w, x, y) =

(7.74)

(cid:88)

y(cid:48)(cid:54)=y

7.6.1.2 maximum multiclass hinge loss

another variant of (7.13) penalizes only the maximally violating label:

(cid:18)

(cid:19)
(1    (cid:10)  (x, y)       (x, y(cid:48)), w(cid:11))
(cid:10)  (x, y(cid:48)), w(cid:11) = 1 + max

y(cid:48)(cid:54)=y

.

(7.75)

f (x, y(cid:48)).

(7.76)

note that both (7.74) and (7.75) are zero whenever

f (x, y) = (cid:104)  (x, y), w(cid:105)     1 + max
y(cid:48)(cid:54)=y

l(w, x, y) := max

0, max
y(cid:48)(cid:54)=y

in other words, they both ensure an adequate margin of separation, in this
case 1, between the score of the true label f (x, y) and every other label
f (x, y(cid:48)). however, they di   er in the way they penalize violators, that is, la-
bels y(cid:48) (cid:54)= y for which f (x, y)     1 + f (x, y(cid:48)). in one case we linearly penalize
the violators and sum up their contributions while in the other case we lin-
early penalize only the maximum violator. in fact, (7.75) can be interpreted

7.6 beyond binary classi   cation

191

as the log odds ratio in the exponential family. towards this end choose   
such that log    = 1 and rewrite (7.20):

(cid:28)

(cid:29)

  (x, y)     max
y(cid:48)(cid:54)=y

  (x, y(cid:48)), w

    1.

p(y|x, w)

maxy(cid:48)(cid:54)=y p(y(cid:48)|x, w)

=

log

rearranging yields (7.76).

7.6.2 multilabel classi   cation

in multilabel classi   cation one or more of k possible labels are assigned to
a training example. just like in the multiclass case two di   erent losses can
be de   ned.

(cid:88)

(cid:18)

7.6.2.1 additive multilabel hinge loss
if we let yx     y denote the labels assigned to x, and generalize the hinge
loss to penalize all labels y(cid:48) /    yx which have been assigned higher score than
some y     yx, then the loss can be written as

max(cid:0)0, 1     ((cid:10)  (x, y)       (x, y(cid:48)), w(cid:11))(cid:1) .

(7.77)

l(w, x, y) =

y   yx and y(cid:48) /   yx

7.6.2.2 maximum multilabel hinge loss

another variant only penalizes the maximum violating pair. in this case the
loss can be written as

(cid:2)1    (cid:0)(cid:10)  (x, y)       (x, y(cid:48)), w(cid:11)(cid:1)(cid:3)(cid:19)

.

(7.78)

l(w, x, y) = max

0, max

y   yx,y(cid:48) /   yx

one can immediately verify that specializing the above losses to the mul-
ticlass case recovers (7.74) and (7.75) respectively, while the binary case
recovers (7.13). the above losses are zero only when

(cid:10)  (x, y(cid:48)), w(cid:11) = 1 + max

y(cid:48) /   yx

f (x, y(cid:48)).

min
y   yx

f (x, y) = min
y   yx

(cid:104)  (x, y), w(cid:105)     1 + max
y(cid:48) /   yx

this can be interpreted as follows: the losses ensure that all the labels
assigned to x have larger scores compared to labels not assigned to x with
the margin of separation of at least 1.

although the above id168s are compatible with multiple labels,
the prediction function argmaxy f (x, y) only takes into account the label
with the highest score. this is a signi   cant drawback of such models, which
can be overcome by using a multiclass approach instead. let |y| be the
size of the label set and z     r|y| denote a vector with   1 entries. we set

(cid:105)

(cid:68)

(cid:69)

(cid:104)

(cid:88)
(cid:104)

i

192

7 linear models

zy = +1 if the y     yx and zy =    1 otherwise, and use the multiclass loss
(7.75) on z. to predict we compute z    = argmaxz f (x, z) and assign to x
the labels corresponding to components of z    which are +1. since z can
take on 2|y| possible values, this approach is not feasible if |y| is large. to
tackle such problems, and to further reduce the computational complexity
we assume that the labels correlations are captured via a |y|    |y| positive
semi-de   nite matrix p , and   (x, y) can be written as   (x)     p y. here    
denotes the kronecker product. furthermore, we express the vector w as
a n    |y| matrix w , where n denotes the dimension of   (x). with these
assumptions (cid:104)  (x)     p (z     z(cid:48)), w(cid:105) can be rewritten as

  (x)(cid:62)w p, (z     z(cid:48))

=

  (x)(cid:62)w p

(zi     z(cid:48)
i),

i

and (7.78) specializes to

(cid:32)

l(w, x, z) := max

0,

(cid:32)
1    (cid:88)

i

min
i(cid:54)=zi
z(cid:48)

(cid:33)(cid:33)

(zi     z(cid:48)
i)

(cid:105)

i

  (x)(cid:62)w p

.

(7.79)

a analogous specialization of (7.77) can also be derived wherein the mini-
mum is replaced by a summation. since the minimum (or summation as the
case may be) is over |y| possible labels, computing the loss is tractable even
if the set of labels y is large.

7.6.3 ordinal regression and ranking

we can generalize our above discussion to consider slightly more general
ranking problems. denote by y the set of all directed acyclic graphs on n
nodes. the presence of an edge (i, j) in y     y indicates that i is preferred
to j. the goal is to    nd a function f (x, i) which imposes a total order on
{1, . . . , n} which is in close agreement with y. speci   cally, if the estimation
error is given by the number of subgraphs of y which are in disagreement
with the total order imposed by f , then the additive version of the loss can
be written as

l(w, x, y) =

(0, 1     (f (x, i)     f (x, j))) ,

(7.80)

max
(i,j)   g

(cid:88)

g   a(y)

where a(y) denotes the set of all possible subgraphs of y. the maximum
margin version, on the other hand, is given by

l(w, x, y) = max
g   a(y)

max
(i,j)   g

(0, 1     (f (x, i)     f (x, j))) .

(7.81)

7.7 large margin classi   ers with structure

193

in other words, we test for each subgraph g of y if the ranking imposed by g
is satis   ed by f . selecting speci   c types of directed acyclic graphs recovers
the multiclass and multilabel settings (problem 7.9).

7.7 large margin classi   ers with structure

7.7.1 margin

de   ne margin pictures

7.7.2 penalized margin

di   erent types of loss, rescaling

7.7.3 nonconvex losses

the max - max loss

7.8 applications

7.8.1 sequence annotation

7.8.2 matching

7.8.3 ranking

7.8.4 shortest path planning

7.8.5 image annotation

7.8.6 contingency table loss

7.9 optimization

7.9.1 column generation

subdi   erentials

7.9.2 bundle methods

7.9.3 overrelaxation in the dual

when we cannot do things exactly

194

7 linear models

7.10 crfs vs structured large margin models

7.10.1 id168

7.10.2 dual connections

7.10.3 optimization

problems
problem 7.1 (deriving the margin {1}) show that the distance of a
point xi to a hyperplane h = {x|(cid:104)w, x(cid:105) + b = 0} is given by |(cid:104)w, xi(cid:105) +
b|/(cid:107)w(cid:107).
problem 7.2 (id166 without bias {1}) a homogeneous hyperplane is one
which passes through the origin, that is,

h = {x|(cid:104)w, x(cid:105) = 0}.

(7.82)

if we devise a soft margin classi   er which uses the homogeneous hyperplane
as a decision boundary, then the corresponding primal optimization problem
can be written as follows:

m(cid:88)

min
w,  

s.t.

  i

(cid:107)w(cid:107)2 + c
1
2
yi (cid:104)w, xi(cid:105)     1       i for all i
  i     0,

i=1

(7.83a)

(7.83b)

(7.83c)

derive the dual of (7.83) and contrast it with (7.9). what changes to the
smo algorithm would you make to solve this dual?
problem 7.3 (deriving the simpli   ed   -id166 dual {2}) in lemma 7.2

we used (7.41) to show that the constraint (cid:80)
(cid:80)

i   i     1 can be replaced by
i   i = 1. show that an equivalent way to arrive at the same conclusion is
by arguing that the constraint        0 is redundant in the primal (7.40). hint:
observe that whenever    < 0 the objective function is always non-negative.
on the other hand, setting w =    = b =    = 0 yields an objective function
value of 0.
problem 7.4 (fenchel and lagrange duals {2}) we derived the la-
grange dual of (7.12) in section 7.1 and showed that it is (7.9). derive the
fenchel dual of (7.12) and relate it to (7.9). hint: see theorem 3.3.5 of
[bl00].

7.10 crfs vs structured large margin models
195
problem 7.5 (dual of the square hinge loss {1}) the analog of (7.5)
when working with the square hinge loss is the following

m(cid:88)

min
w,b,  

s.t.

  2
i

c
m

(cid:107)w(cid:107)2 +
1
2
yi((cid:104)w, xi(cid:105) + b)     1       i for all i
  i     0,

i=1

(7.84a)

(7.84b)

(7.84c)

derive the lagrange dual of the above optimization problem and show that
it a quadratic programming problem.
problem 7.6 (dual of the ramp loss {1}) derive the lagrange dual of
(7.49) and show that it the quadratic programming problem (7.50).
problem 7.7 (smo for various id166 formulations {2}) derive an smo
like decomposition algorithm for solving the dual of the following problems:
      -id166 (7.41).
    sv regression (7.57).
    sv novelty detection (7.71).
problem 7.8 (novelty detection with balls {2}) in section 7.4 we as-
sumed that we wanted to estimate a halfspace which contains a major frac-
tion of the input data. an alternative approach is to use balls, that is, we
estimate a ball of small radius in feature space which encloses a majority of
the input data. write the corresponding optimization problem and its dual.
show that if the kernel is translation invariant, that is, k(x, x(cid:48)) depends only
on (cid:107)x     x(cid:48)(cid:107) then the optimization problem with balls is equivalent to (7.71).
explain why this happens geometrically.
problem 7.9 (multiclass and multilabel loss from ranking loss {1})
show how the multiclass (resp. multilabel) losses (7.74) and (7.75) (resp.
(7.77) and (7.79)) can be derived as special cases of (7.80) and (7.81) re-
spectively.

problem 7.10 invariances (basic loss)

problem 7.11 polynomial transformations - sdp constraints

appendix 1

id202 and functional analysis

a1.1 johnson lindenstrauss lemma
lemma 1.1 (johnson lindenstrauss) let x be a set of n points in rd
represented as a n    d matrix a. given  ,    > 0 let

k     4 + 2  

(1.1)
be a positive integer. construct a d    k random matrix r with independent
standard normal random variables, that is, rij     n(0, 1), and let

 2/2      3/3

log n

e =

1   
k

ar.

(1.2)

de   ne f : rd     rk as the function which maps the rows of a to the rows
of e. with id203 at least 1     n     , for all u, v     x we have
(1      )(cid:107)u     v(cid:107)2     (cid:107)f (u)     f (v)(cid:107)2     (1 +  )(cid:107)u     v(cid:107)2 .

(1.3)

our proof presentation by and large follows [?]. we    rst show that
lemma 1.2 for any arbitrary vector        rd let qi denote the i-th compo-
nent of f (  ). then qi     n(0,(cid:107)  (cid:107)2 /k) and hence

e(cid:104)(cid:107)f (  )(cid:107)2(cid:105)

=

e(cid:2)q2

(cid:3) = (cid:107)  (cid:107)2 .

i

k(cid:88)

i=1

(1.4)

in other words, the expected length of vectors are preserved even after em-
bedding them in a k dimensional space. next we show that the lengths of
the embedded vectors are tightly concentrated around their mean.
lemma 1.3 for any   > 0 and any unit vector        rd we have

(cid:16)(cid:107)f (  )(cid:107)2 > 1 +  
(cid:17)
(cid:16)(cid:107)f (  )(cid:107)2 < 1      
(cid:17)

p r

p r

< exp

< exp

(cid:18)
(cid:18)

    k
2
    k
2

(cid:0) 2/2      3/3(cid:1)(cid:19)
(cid:0) 2/2      3/3(cid:1)(cid:19)

(1.5)

.

(1.6)

197

198
corollary 1.4 if we choose k as in (1.1) then for any        rd we have

1 id202 and functional analysis

(cid:16)

(1      )(cid:107)  (cid:107)2     (cid:107)f (  )(cid:107)2     (1 +  )(cid:107)  (cid:107)2(cid:17)     1     2

p r

n2+   .

(1.7)

proof follows immediately from lemma 1.3 by setting

(cid:18)

(cid:0) 2/2      3/3(cid:1)(cid:19)

2 exp

    k
2

    2

n2+   ,

2

and solving for k.

(cid:1) pairs of vectors u, v in x, and their corresponding distances

there are(cid:0)n
therefore, the id203 of not satisfying (1.3) is bounded by(cid:0)n

(cid:107)u     v(cid:107) are preserved within 1      factor as shown by the above lemma.
n2+   <
1/n   as claimed in the johnson lindenstrauss lemma. all that remains is
to prove lemma 1.2 and 1.3.
proof (lemma 1.2). since qi = 1   
j rij  j is a linear combination of stan-
dard normal random variables rij it follows that qi is normally distributed.
to compute the mean note that

(cid:80)

(cid:1)   

k

2

2

(cid:88)

j

e [qi] =

1   
k

  j e [rij] = 0.

since rij are independent zero mean unit variance random variables, e [rijril] =
1 if j = l and 0 otherwise. using this

e(cid:2)q2

i

(cid:3) =

e

1
k

       d(cid:88)

j=1

      2

rij  j

d(cid:88)

d(cid:88)

j=1

l=1

=

1
k

  j  l e [rijril] =

d(cid:88)

j=1

1
k

  2

j =

(cid:107)  (cid:107)2 .

1
k

proof (lemma 1.3). clearly, for all   

(cid:104)(cid:107)f (  )(cid:107)2 > 1 +  
(cid:105)

p r

(cid:104)

(cid:16)

  (cid:107)f (  )(cid:107)2(cid:17)

(cid:105)

> exp(  (1 +  ))

.

= p r

exp

a1.1 johnson lindenstrauss lemma
using markov   s inequality (p r[x     a]     e[x]/a) we obtain

(cid:104)

(cid:16)

  (cid:107)f (  )(cid:107)2(cid:17)

p r

exp

> exp(  (1 +  ))

199

(1.8)

(cid:105)    

i

.

=

=

=

exp

exp

i
exp(  (1 +  ))

exp(  (1 +  ))
i=1 q2
i
exp(  (1 +  ))

  (cid:107)f (  )(cid:107)2(cid:17)(cid:105)
(cid:16)
e(cid:104)
(cid:17)(cid:105)
(cid:16)
e(cid:104)
  (cid:80)k
(cid:1)(cid:105)
e(cid:104)(cid:81)k
i=1 exp(cid:0)  q2
(cid:32) e(cid:2)exp(cid:0)  q2
k (1 +  )(cid:1)(cid:33)k
(cid:1)(cid:3)
exp(cid:0)   
(cid:19)(cid:21)
1(cid:113)
       exp(cid:0)      
      k
k (1 +  )(cid:1)
(cid:105)    
(cid:113)
(cid:105)     (exp(    )(1 +  ))k/2 .

1     2  

1     2  

=

k

k

.

the last equality is because the qi   s are i.i.d. since    is a unit vector, from
the previous lemma qi     n(0, 1/k). therefore, kq2
is a   2 random variable
with moment generating function

i

(cid:20)

(cid:1)(cid:3) = e

(cid:18)   

k

exp

kq2
i

plugging this into (1.8)

i

e(cid:2)exp(cid:0)  q2
  (cid:107)f (  )(cid:107)2(cid:17)
(cid:16)
  (cid:107)f (  )(cid:107)2(cid:17)
(cid:16)

(cid:104)

(cid:104)

p r

exp

> exp (  (1 +  ))

.

setting    = k 

2(1+ ) in the above inequality and simplifying

p r

exp

> exp(  (1 +  ))

using the inequality

log(1 +  ) <        2/2 +  3/3

we can write

(cid:104)

(cid:16)

  (cid:107)f (  )(cid:107)2(cid:17)

p r

exp

> exp(  (1 +  ))

(cid:18)

(cid:105)     exp

    k
2

(cid:0) 2/2      3/3(cid:1)(cid:19)

.

this proves (1.5). to prove (1.6) we need to repeat the above steps and use
the inequality

log(1      ) <           2/2.

this is left as an exercise to the reader.

200

1 id202 and functional analysis

a1.2 spectral properties of matrices

a1.2.1 basics

a1.2.2 special matrices

unitary, hermitean, positive semide   nite

a1.2.3 normal forms

jacobi

a1.3 functional analysis

a1.3.1 norms and metrics

vector space, norm, triangle inequality

a1.3.2 banach spaces

normed vector space, evaluation functionals, examples, dual space

a1.3.3 hilbert spaces

symmetric inner product

a1.3.4 operators

spectrum, norm, bounded, unbounded operators

a1.4 fourier analysis

a1.4.1 basics

a1.4.2 operators

appendix 2

conjugate distributions

201

202

binomial     beta

  (x) = x

eh(n  ,n) =

  (n   + 1)  (n(1       ) + 1)

  (n + 2)

2 conjugate distributions

= b(n   + 1, n(1       ) + 1)

in traditional notation one represents the conjugate as

p(z;   ,   ) =
where    = n   + 1 and    = n(1     b  ) + 1.

z     1(1     z)     1

  (   +   )
  (  )  (  )

multinomial     dirichlet

  (x) = ex

eh(n  ,n) =

i=1   (n  i + 1)

  (n + d)

(cid:81)d
  ((cid:80)d
(cid:81)d

d(cid:89)

i=1

i=1   i)
i=1   (  i)

z  i   1

i

in traditional notation one represents the conjugate as

p(z;   ) =

where   i = n  i + 1

poisson     gamma

  (x) = x

eh(n  ,n) = n   n    (n  )

in traditional notation one represents the conjugate as

p(z;   ) =          (  )z     1e     x

where    = n   and    = n.

    multinomial / binomial
    gaussian
    laplace
    poisson
    dirichlet
    wishart
    student-t
    beta
    gamma

appendix 3

id168s

a3.1 id168s

a multitude of id168s are commonly used to derive seemingly di   er-
ent algorithms. this often blurs the similarities as well as subtle di   erences
between them, often for historic reasons: each new loss is typically accompa-
nied by at least one publication dedicated to it. in many cases, the loss is not
spelled out explicitly either but instead, it is only given by means of a con-
strained optimization problem. a case in point are the papers introducing
(binary) hinge loss [bm92, cv95] and structured loss [tgk04, tjha05].
likewise, a geometric description obscures the underlying id168, as
in novelty detection [spst+01].

in this section we give an expository yet unifying presentation of many
of those id168s. many of them are well known, while others, such
as multivariate ranking, hazard regression, or poisson regression are not
commonly used in machine learning. tables a3.1 and a3.1 contain a choice
subset of simple scalar and vectorial losses. our aim is to put the multitude
of id168s in an uni   ed framework, and to show how these losses
and their (sub)gradients can be computed e   ciently for use in our solver
framework.

note that not all losses, while convex, are continuously di   erentiable. in
this situation we give a subgradient. while this may not be optimal, the
convergence rates of our algorithm do not depend on which element of the
subdi   erential we provide: in all cases the    rst order taylor approximation
is a lower bound which is tight at the point of expansion.

in this setion, with little abuse of notation, vi is understood as the i-th
component of vector v when v is clearly not an element of a sequence or a
set.

a3.1.1 scalar id168s

it is well known [wah97] that the id76 problem

   subject to y (cid:104)w, x(cid:105)     1        and        0

min

  

(3.1)

203

204

3 id168s

m
u
l
t
i
v
a
r
i
a
t
e
r
e
g
r
e
s
s
i
o
n

12
(
f
   
y
)

(cid:62)
m

(
f
   
y
)
w
h
e
r
e

m
(cid:23)
0

m

(
f
   
y
)

[

t
j
h
a
0
5
]

s
c
a
l
e
d

s
o
f
t
-

m
a
r
g
i
n
m
u
l
t
i
c
l
a
s
s

[

c
s
0
3
]

s
o
f
t
-

m
a
r
g
i
n
m
u
l
t
i
c
l
a
s
s

[

t
g
k
0
4
]

s
o
f
t

m
a
x
m
u
l
t
i
c
l
a
s
s

[

c
d
l
s
9
9
]

l
o
g

m
a
x
y

(cid:48)

(cid:80)
  
(
y
,
y

y

(cid:48)

e
x
p
(
f
y

(cid:48)
)

   
f
y

(cid:48)

)
(
f
y

(cid:48)

   
f
y

+
   
(
y
,
y

(cid:48)

)
)

  
(cid:104)
(cid:80)
(
y
,
y

w
h
e
r
e

(cid:48)

y

   

)
(
e
y

   

i
s

t
h
e

   
e
y
(cid:105)
)

(cid:80)

y

(cid:48)

e
y

(cid:48)

e
x
p
(
f
(cid:48)y

)

/

y

(cid:48)

e
x
p
(
f
(cid:48)y

)

   
e
y

a
r
g
m
a
x

o
f

t
h
e

l
o
s
s

l
o
s
s

m
a
x
y

(cid:48)
(
f
y

(cid:48)

   
f
y

+
   
(
y
,
y

(cid:48)

)
)

d
e
r
i
v
a
t
i
v
e

w
h
e
r
e

y

e
y

   

   
e
y

   

i
s

t
h
e

a
r
g
m
a
x

o
f

t
h
e

l
o
s
s

l
o
g
i
s
t
i
c

[

c
s
s
0
0
]

n
o
v
e
l
t
y

[
s
p
s
t
+
0
1
]

p
o
i
s
s
o
n

r
e
g
r
e
s
s
i
o
n

[

c
r
e
9
3
]

h
u
b
e
r
   
s

r
o
b
u
s
t

l
o
s
s

[

m
s
r
+
9
7
]

 
-
i
n
s
e
n
s
i
t
i
v
e

[

v
g
s
9
7
]

l
e
a
s
t

a
b
s
o
l
u
t
e

d
e
v
i
a
t
i
o
n

q
u
a
n
t
i
l
e

r
e
g
r
e
s
s
i
o
n

[

k
o
e
0
5
]

l
e
a
s
t

m
e
a
n

s
q
u
a
r
e
s

[

w

i
l
9
8
]

|
f
   
y
|

12
(
f
   
y
)
2

m
a
x
(
0
,
  
   
f
)

l
o
g
(
1
+
e
x
p
(

   
y
f
)
)

m
a
x
(
0
,

|
f
   
y
|

   
 
)

m
a
x
(
  
(
f
   
y
)
,
(
1
   
  
)
(
y
   
f
)
)

12
(
f
   
y
)
2

e
x
p
(
f
)
   
y
f

i
f

|
f
   
y
|

   
1
,

e
l
s
e

|
f
   
y
|

   

12

e
x
p
(
f
)

   
y

f
   
y

i
f

|
f
   
y
|

   
1
,

0

i
f

|
f
   
y
|

   
 
,

e
l
s
e

  

i
f

f
>
y

a
n
d

  
   
1

s
i
g
n
(
f
   
y
)

o
t
h
e
r
w
i
s
e

e
l
s
e

s
i
g
n
(
f
   
y
)

f
   
y

s
i
g
n
(
f
   
y
)

   
y
/
(
1
+
e
x
p
(

   
y
f
)
)

0

i
f

f
   
  

a
n
d
   
1

o
t
h
e
r
w
i
s
e

v
e
c
t
o
r
i
a
l

l
o
s
s

f
u
n
c
t
i
o
n
s

a
n
d

t
h
e
i
r

d
e
r
i
v
a
t
i
v
e
s
,

d
e
p
e
n
d
n
g

i

o
n

t
h
e

v
e
c
t
o
r

f

:

=
w
x

a
n
d

o
n

y
.

a3.1 id168s
205
takes on the value max(0, 1     y (cid:104)w, x(cid:105)). the latter is a convex function in
w and x. likewise, we may rewrite the  -insensitive loss, huber   s robust
loss, the quantile regression loss, and the novelty detection loss in terms of
id168s rather than a constrained optimization problem. in all cases,
(cid:104)w, x(cid:105) will play a key role insofar as the loss is convex in terms of the scalar
quantity (cid:104)w, x(cid:105). a large number of id168s fall into this category,
as described in table a3.1. note that not all functions of this type are
continuously di   erentiable. in this case we adopt the convention that

   x max(f (x), g(x)) =

   xf (x)
   xg(x)

if f (x)     g(x)
otherwise .

(3.2)

since we are only interested in obtaining an arbitrary element of the subd-
i   erential this convention is consistent with our requirements.
let us discuss the issue of e   cient computation. for all scalar losses we
may write l(x, y, w) =   l((cid:104)w, x(cid:105) , y), as described in table a3.1. in this case a
simple application of the chain rule yields that    wl(x, y, w) =   l(cid:48)((cid:104)w, x(cid:105) , y)  x.
for instance, for squared loss we have

  l((cid:104)w, x(cid:105) , y) = 1

2 ((cid:104)w, x(cid:105)     y)2 and   l(cid:48)((cid:104)w, x(cid:105) , y) = (cid:104)w, x(cid:105)     y.

consequently, the derivative of the empirical risk term is given by

   wremp(w) =

1
m

  l(cid:48)((cid:104)w, xi(cid:105) , yi)    xi.

(3.3)

(cid:40)

m(cid:88)

i=1

this means that if we want to compute l and    wl on a large number of
observations xi, represented as matrix x, we can make use of fast linear
algebra routines to pre-compute the vectors

f = xw and g(cid:62)x where gi =   l(cid:48)(fi, yi).

(3.4)

this is possible for any of the id168s listed in table a3.1, and many
other similar losses. the advantage of this uni   ed representation is that im-
plementation of each individual loss can be done in very little time. the
computational infrastructure for computing xw and g(cid:62)x is shared. eval-
uating   l(fi, yi) and   l(cid:48)(fi, yi) for all i can be done in o(m) time and it is
not time-critical in comparison to the remaining operations. algorithm 3.1
describes the details.

an important but often neglected issue is worth mentioning. computing f
requires us to right multiply the matrix x with the vector w while computing
g requires the left multiplication of x with the vector g(cid:62). if x is stored in a
row major format then xw can be computed rather e   ciently while g(cid:62)x is

206

3 id168s

algorithm 3.1 scalarloss(w, x, y)

3: compute r =(cid:80)

1: input: weight vector w, feature matrix x, and labels y
2: compute f = xw
4: g     g(cid:62)x
5: return risk r and gradient g

  l(fi, yi) and g =   l(cid:48)(f, y)

i

expensive. this is particularly true if x cannot    t in main memory. converse
is the case when x is stored in column major format. similar problems are
encountered when x is a sparse matrix and stored in either compressed row
format or in compressed column format.

a3.1.2 structured loss

in recent years structured estimation has gained substantial popularity in
machine learning [tjha05, tgk04, bhs+07]. at its core it relies on two
types of convex id168s: logistic loss:

(cid:88)
exp(cid:0)(cid:10)w,   (x, y(cid:48))(cid:11)(cid:1)     (cid:104)w,   (x, y)(cid:105) ,
  (y, y(cid:48))(cid:10)w,   (x, y(cid:48))       (x, y)(cid:11) +    (y, y(cid:48)).

y(cid:48)   y

(3.5)

(3.6)

l(x, y, w) = log

and soft-margin loss:

l(x, y, w) = max
y(cid:48)   y

here   (x, y) is a joint feature map,    (y, y(cid:48))     0 describes the cost of mis-
classifying y by y(cid:48), and   (y, y(cid:48))     0 is a scaling term which indicates by how
much the large margin property should be enforced. for instance, [tgk04]
choose   (y, y(cid:48)) = 1. on the other hand [tjha05] suggest   (y, y(cid:48)) =    (y, y(cid:48)),
which reportedly yields better performance. finally, [mca07] recently sug-
gested generic functions   (y, y(cid:48)).

the logistic loss can also be interpreted as the negative log-likelihood of

a conditional exponential family model:

p(y|x; w) := exp((cid:104)w,   (x, y)(cid:105)     g(w|x)),

(3.7)
where the normalizing constant g(w|x), often called the log-partition func-
tion, reads

g(w|x) := log

exp(cid:0)(cid:10)w,   (x, y(cid:48))(cid:11)(cid:1) .

(cid:88)

y(cid:48)   y

(3.8)

a3.1 id168s

207

as a consequence of the hammersley-cli   ord theorem [jor08] every expo-
nential family distribution corresponds to a undirected graphical model. in
our case this implies that the labels y factorize according to an undirected
graphical model. a large number of problems have been addressed by this
setting, amongst them named entity tagging [lmp01], sequence alignment
[tjha05], segmentation [rss+07] and path planning [rbz06]. it is clearly
impossible to give examples of all settings in this section, nor would a brief
summary do this    eld any justice. we therefore refer the reader to the edited
volume [bhs+07] and the references therein.

if the underlying graphical model is tractable then e   cient id136 al-
gorithms based on id145 can be used to compute (3.5) and
(3.6). we discuss intractable id114 in section a3.1.2.1, and now
turn our attention to the derivatives of the above structured losses.

when it comes to computing derivatives of the logistic loss, (3.5), we have

   wl(x, y, w) =

      (x, y)

(3.9)

(cid:80)
(cid:80)
y(cid:48)   (x, y(cid:48)) exp(cid:104)w,   (x, y(cid:48))(cid:105)

y(cid:48) exp(cid:104)w,   (x, y(cid:48))(cid:105)

(cid:2)  (x, y(cid:48))(cid:3)       (x, y).

= ey(cid:48)   p(y(cid:48)|x)

(3.10)
where p(y|x) is the exponential family model (3.7). in the case of (3.6) we
denote by   y(x) the argmax of the rhs, that is

  (y, y(cid:48))(cid:10)w,   (x, y(cid:48))       (x, y)(cid:11) +    (y, y(cid:48)).

(3.11)

  y(x) := argmax

y(cid:48)

this allows us to compute the derivative of l(x, y, w) as

   wl(x, y, w) =   (y,   y(x)) [  (x,   y(x))       (x, y)] .

(3.12)

in the case where the loss is maximized for more than one distinct value   y(x)
we may average over the individual values, since any convex combination of
such terms lies in the subdi   erential.
note that (3.6) majorizes    (y, y   ), where y    := argmaxy(cid:48) (cid:104)w,   (x, y(cid:48))(cid:105)

[tjha05]. this can be seen via the following series of inequalities:

   (y, y   )       (y, y   )(cid:104)w,   (x, y   )       (x, y)(cid:105) +    (y, y   )     l(x, y, w).

the    rst inequality follows because   (y, y   )     0 and y    maximizes (cid:104)w,   (x, y(cid:48))(cid:105)
thus implying that   (y, y   )(cid:104)w,   (x, y   )       (x, y)(cid:105)     0. the second inequal-
ity follows by de   nition of the loss.

we conclude this section with a simple lemma which is at the heart of
several derivations of [joa05]. while the proof in the original paper is far
from trivial, it is straightforward in our setting:

208
3 id168s
lemma 3.1 denote by   (y, y(cid:48)) a loss and let   (xi, yi) be a feature map for
observations (xi, yi) with 1     i     m. moreover, denote by x, y the set of
all m patterns and labels respectively. finally let

m(cid:88)

  (x, y ) :=

  (xi, yi) and    (y, y (cid:48)) :=

  (yi, y(cid:48)
i).

(3.13)

i=1

i=1

m(cid:88)

then the following two losses are equivalent:

(cid:10)w,   (xi, y(cid:48))       (xi, yi)(cid:11) +   (yi, y(cid:48)) and max

(cid:10)w,   (x, y (cid:48))       (x, y )(cid:11) +    (y, y (cid:48)).

max

y(cid:48)

i=1

m(cid:88)

y (cid:48)

this is immediately obvious, since both feature map and loss decompose,
which allows us to perform maximization over y (cid:48) by maximizing each of its
m components. in doing so, we showed that aggregating all data and labels
into a single feature map and loss yields results identical to minimizing
the sum over all individual losses. this holds, in particular, for the sample
error loss of [joa05]. also note that this equivalence does not hold whenever
  (y, y(cid:48)) is not constant.

a3.1.2.1 intractable models

instance, for intractable id114, the computation of(cid:80)

we now discuss cases where computing l(x, y, w) itself is too expensive. for

y exp(cid:104)w,   (x, y)(cid:105)

cannot be computed e   ciently. [wj03] propose the use of a convex majoriza-
tion of the log-partition function in those cases. in our setting this means
that instead of dealing with
l(x, y, w) = g(w|x)     (cid:104)w,   (x, y)(cid:105) where g(w|x) := log

exp(cid:104)w,   (x, y)(cid:105)

(cid:88)

one uses a more easily computable convex upper bound on g via

sup

     marg(x)

(cid:104)w,   (cid:105) + hgauss(  |x).

(3.14)

(3.15)

y

here marg(x) is an outer bound on the conditional marginal polytope
associated with the map   (x, y). moreover, hgauss(  |x) is an upper bound
on the id178 by using a gaussian with identical variance. more re   ned
tree decompositions exist, too. the key bene   t of our approach is that the
solution    of the optimization problem (3.15) can immediately be used as a
gradient of the upper bound. this is computationally rather e   cient.

a3.1 id168s

209

mation problems of the form

likewise note that [tgk04] use relaxations when solving structured esti-

y(cid:48)   (y, y(cid:48))(cid:10)w,   (x, y(cid:48))       (x, y)(cid:11) +    (y, y(cid:48)),

l(x, y, w) = max

(3.16)
by enlarging the domain of maximization with respect to y(cid:48). for instance,
instead of an integer programming problem we might relax the setting to
a linear program which is much cheaper to solve. this, again, provides an
upper bound on the original id168.

in summary, we have demonstrated that convex relaxation strategies are
well applicable for bundle methods. in fact, the results of the corresponding
optimization procedures can be used directly for further optimization steps.

a3.1.3 scalar multivariate performance scores

we now discuss a series of structured id168s and how they can be
implemented e   ciently. for the sake of completeness, we give a concise rep-
resentation of previous work on multivariate performance scores and ranking
methods. all these id168s rely on having access to (cid:104)w, x(cid:105), which can
be computed e   ciently by using the same operations as in section a3.1.1.

a3.1.3.1 roc score

denote by f = xw the vector of function values on the training set. it is
well known that the area under the roc curve is given by

auc(x, y, w) =

1

m+m   

i((cid:104)w, xi(cid:105) < (cid:104)w, xj(cid:105)),

(3.17)

(cid:88)

yi<yj

where m+ and m    are the numbers of positive and negative observations
respectively, and i(  ) is indicator function. directly optimizing the cost 1    
auc(x, y, w) is di   cult as it is not continuous in w. by using max(0, 1 +
(cid:104)w, xi     xj(cid:105)) as the surrogate id168 for all pairs (i, j) for which yi < yj
we have the following convex multivariate empirical risk

remp(w) =

1

m+m   

max(0, 1 + (cid:104)w, xi     xj(cid:105)) =

1

m+m   

max(0, 1 + fi     fj).

(cid:88)

yi<yj

(cid:88)

yi<yj

(3.18)

obviously, we could compute remp(w) and its derivative by an o(m2) op-
eration. however [joa05] showed that both can be computed in o(m log m)
time using a sorting operation, which we now describe.

denote by c = f     1

2 y an auxiliary variable and let i and j be indices such

210

3 id168s

algorithm 3.2 rocscore(x, y, w)
1: input: feature matrix x, labels y, and weight vector w
2: initialization: s    = m    and s+ = 0 and l = 0m and c = xw     1
2 y
3:        {1, . . . , m} sorted in ascending order of c
4: for i = 1 to m do
if y  i =    1 then
5:

l  i     s+ and s        s        1
l  i        s    and s+     s+ + 1

else

6:

7:

8:

end if
9:
10: end for
11: rescale l     l/(m+m   ) and compute r = (cid:104)l, c(cid:105) and g = l(cid:62)x.
12: return risk r and subgradient g

that yi =    1 and yj = 1. it follows that ci     cj = 1 + fi     fj. the e   cient
algorithm is due to the observation that there are at most m distinct terms
ck, k = 1, . . . , m, each with di   erent frequency lk and sign, appear in (3.18).
these frequencies lk can be determined by    rst sorting c in ascending order
then scanning through the labels according to the sorted order of c and
keeping running statistics such as the number s    of negative labels yet to
encounter, and the number s+ of positive labels encountered. when visiting
yk, we know ck should appears s+ (or s   ) times with positive (or negative)
sign in (3.18) if yk =    1 (or yk = 1). algorithm 3.2 spells out explicitly how
to compute remp(w) and its subgradient.

a3.1.3.2 ordinal regression

essentially the same preference relationships need to hold for ordinal re-
gression. the only di   erence is that yi need not take on binary values any
more. instead, we may have an arbitrary number of di   erent values yi (e.g.,
1 corresponding to    strong reject    up to 10 corresponding to    strong accept   ,
when it comes to ranking papers for a conference). that is, we now have
yi     {1, . . . , n} rather than yi     {  1}. our goal is to    nd some w such that
(cid:104)w, xi     xj(cid:105) < 0 whenever yi < yj. whenever this relationship is not satis-
   ed, we incur a cost c(yi, yj) for preferring xi to xj. for examples, c(yi, yj)
could be constant i.e., c(yi, yj) = 1 [joa06] or linear i.e., c(yi, yj) = yj     yi.
denote by mi the number of xj for which yj = i. in this case, there are
i pairs (yi, yj) for which yi (cid:54)= yj; this implies that there
are m =   m /2 pairs (yi, yj) such that yi < yj. normalizing by the total

  m = m2    (cid:80)n

i=1 m2

a3.1 id168s

211

number of comparisons we may write the overall cost of the estimator as

(cid:88)

yi<yj

1
m

c(yi, yj)i((cid:104)w, xi(cid:105) > (cid:104)w, xj(cid:105)) where m =

1
2

(cid:35)

(cid:34)
m2     n(cid:88)

i

m2
i

. (3.19)

using the same convex majorization as above when we were maximizing the
roc score, we obtain an empirical risk of the form

remp(w) =

1
m

c(yi, yj) max(0, 1 + (cid:104)w, xi     xj(cid:105))

(3.20)

(cid:88)

yi<yj

now the goal is to    nd an e   cient algorithm for obtaining the number of
times when the individual losses are nonzero such as to compute both the
value and the gradient of remp(w). the complication arises from the fact
that observations xi with label yi may appear in either side of the inequality
depending on whether yj < yi or yj > yi. this problem can be solved as
follows: sort f = xw in ascending order and traverse it while keeping track
of how many items with a lower value yj are no more than 1 apart in terms
of their value of fi. this way we may compute the count statistics e   ciently.
algorithm 3.3 describes the details, generalizing the results of [joa06]. again,
its runtime is o(m log m), thus allowing for e   cient computation.

a3.1.3.3 preference relations

in general, our loss may be described by means of a set of preference relations
j (cid:23) i for arbitrary pairs (i, j)     {1, . . . m}2 associated with a cost c(i, j)
which is incurred whenever i is ranked above j. this set of preferences may
or may not form a partial or a total order on the domain of all observations.
in these cases e   cient computations along the lines of algorithm 3.3 exist.
in general, this is not the case and we need to rely on the fact that the set
p containing all preferences is su   ciently small that it can be enumerated
e   ciently. the risk is then given by

(cid:88)

(i,j)   p

1
|p|

c(i, j)i((cid:104)w, xi(cid:105) > (cid:104)w, xj(cid:105))

(3.21)

212

3 id168s

algorithm 3.3 ordinalregression(x, y, w, c)
1: input: feature matrix x, labels y, weight vector w, and score matrix c
2: initialization: l = 0n and ui = mi    i     [n] and r = 0 and g = 0m
3: compute f = xw and set c = [f     1

2 ]     r2m (concatenate the

2 , f + 1

4: compute m = (m2    (cid:80)n

vectors)

i=1 m2

i )/2

5: rescale c     c/m
6:        {1, . . . , 2m} sorted in ascending order of c
7: for i = 1 to 2m do
j =   i mod m
8:
if   i     m then

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

for k = 1 to yj     1 do
r     r     c(k, yj)ukcj
gj     gj     c(k, yj)uk

end for
lyj     lyj + 1

else

for k = yj + 1 to n do
r     r + c(yj, k)lkcj+m
gj     gj + c(yj, k)lk

end for
uyj     uyj     1

end if
21:
22: end for
23: g     g(cid:62)x
24: return: risk r and subgradient g

again, the same majorization argument as before allows us to write a convex
upper bound

remp(w) =

where    wremp(w) =

1
|p|

1
|p|

(cid:40)

c(i, j)

0
xi     xj

(cid:88)
(cid:88)

(i,j)   p

(i,j)   p

c(i, j) max (0, 1 + (cid:104)w, xi(cid:105)     (cid:104)w, xj(cid:105)) (3.22)

if (cid:104)w, xj     xi(cid:105)     1
otherwise

(3.23)

the implementation is straightforward, as given in algorithm 3.4.

a3.1 id168s

213

algorithm 3.4 preference(x, w, c, p )

1: input: feature matrix x, weight vector w, score matrix c, and prefer-

ence set p

2: initialization: r = 0 and g = 0m
3: compute f = xw
4: while (i, j)     p do
if fj     fi < 1 then

5:

6:

7:

r     r + c(i, j)(1 + fi     fj)
gi     gi + c(i, j) and gj     gj     c(i, j)

end if
8:
9: end while
10: g     g(cid:62)x
11: return risk r and subgradient g

a3.1.3.4 ranking

in webpage and document ranking we are often in a situation similar to that
described in section a3.1.3.2, however with the di   erence that we do not
only care about objects xi being ranked according to scores yi but moreover
that di   erent degrees of importance are placed on di   erent documents.

the information retrieval literature is full with a large number of di   er-
ent scoring functions. examples are criteria such as normalized discounted
cumulative gain (ndcg), mean reciprocal rank (mrr), precision@n, or
expected rank utility (eru). they are used to address the issue of evaluat-
ing rankers, search engines or recommender sytems [voo01, jk02, bhk98,
bh04]. for instance, in webpage ranking only the    rst k retrieved docu-
ments that matter, since users are unlikely to look beyond the    rst k, say
10, retrieved webpages in an internet search. [ls07] show that these scores
can be optimized directly by minimizing the following loss:

(cid:10)w, x  (i)     xi

(cid:11) + (cid:104)a     a(  ), b(y)(cid:105) .

(3.24)

l(x, y, w) = max

  

ci

here ci is a monotonically decreasing sequence, the documents are assumed
to be arranged in order of decreasing relevance,    is a permutation, the
vectors a and b(y) depend on the choice of a particular ranking measure, and
a(  ) denotes the permutation of a according to   . pre-computing f = xw
we may rewrite (3.24) as

(cid:105)     c(cid:62)f + a(cid:62)b(y)

(3.25)

l(f, y) = max

  

c(cid:62)f (  )     a(  )(cid:62)b(y)

(cid:88)

i

(cid:104)

214

3 id168s

algorithm 3.5 ranking(x, y, w)

1: input: feature matrix x, relevances y, and weight vector w
2: compute vectors a and b(y) according to some ranking measure
3: compute f = xw
4: compute elements of matrix cij = cifj     biaj
5:    = linearassignment(c)
6: r = c(cid:62)(f (  )     f ) + (a     a(  ))(cid:62)b
7: g = c(     1)     c and g     g(cid:62)x
8: return risk r and subgradient g

and consequently the derivative of l(x, y, w) with respect to w is given by
c(cid:62)f (  )     a(  )(cid:62)b(y).

   wl(x, y, w) = (c(       1)     c)(cid:62)x where      = argmax

  

(3.26)
here      1 denotes the inverse permutation, such that           1 = 1. finding the
permutation maximizing c(cid:62)f (  )    a(  )(cid:62)b(y) is a linear assignment problem
which can be easily solved by the hungarian marriage algorithm, that is,
the kuhn-munkres algorithm.

the original papers by [kuh55] and [mun57] implied an algorithm with
o(m3) cost in the number of terms. later, [kar80] suggested an algorithm
with expected quadratic time in the size of the assignment problem (ignor-
ing log-factors). finally, [ol93] propose a linear time algorithm for large
problems. since in our case the number of pages is fairly small (in the order
of 50 to 200 per query) the scaling behavior per query is not too important.
we used an existing implementation due to [jv87].

note also that training sets consist of a collection of ranking problems,
that is, we have several ranking problems of size 50 to 200. by means of
parallelization we are able to distribute the work onto a cluster of worksta-
tions, which is able to overcome the issue of the rather costly computation
per collection of queries. algorithm 3.5 spells out the steps in detail.

a3.1.3.5 contingency table scores
[joa05] observed that f   scores and related quantities dependent on a con-
tingency table can also be computed e   ciently by means of structured es-
timation. such scores depend in general on the number of true and false
positives and negatives alike. algorithm 3.6 shows how a corresponding em-
pirical risk and subgradient can be computed e   ciently. as with the pre-
vious losses, here again we use convex majorization to obtain a tractable
optimization problem.

a3.1 id168s

215
given a set of labels y and an estimate y(cid:48), the numbers of true positives
(t+), true negatives (t   ), false positives (f+), and false negatives (f   ) are
determined according to a contingency table as follows:

y(cid:48) > 0
y(cid:48) < 0

y > 0

y < 0

t+
f   

f+
t   

in the sequel, we denote by m+ = t+ + f    and m    = t    + f+ the numbers
of positives and negative labels in y, respectively. we note that f   score can
be computed based on the contingency table [joa05] as

(cid:105)

(cid:104)

f  (t+, t   ) =

(3.27)
if we want to use (cid:104)w, xi(cid:105) to estimate the label of observation xi, we may use
the following structured loss to    directly    optimize w.r.t. f   score [joa05]:

t+ + m        t    +   2m+

.

(1 +   2)t+

(y(cid:48)     y)(cid:62)f +    (t+, t   )

,

y(cid:48)

l(x, y, w) = max

(3.28)
where f = xw,    (t+, t   ) := 1     f  (t+, t   ), and (t+, t   ) is determined
by using y and y(cid:48). since     does not depend on the speci   c choice of (y, y(cid:48))
but rather just on which sets they disagree, l can be maximized as follows:
enumerating all possible m+m    contingency tables in a way such that given
a con   guration (t+, t   ), t+ (t   ) positive (negative) observations xi with
largest (lowest) value of (cid:104)w, xi(cid:105) are labeled as positive (negative). this is
e   ectively implemented as a nested loop hence run in o(m2) time. algorithm
3.6 describes the procedure in details.

a3.1.4 vector id168s

next we discuss    vector    id168s, i.e., functions where w is best de-
scribed as a matrix (denoted by w ) and the loss depends on w x. here, we
have feature vector x     rd, label y     rk, and weight matrix w     rd  k. we
also denote feature matrix x     rm  d as a matrix of m feature vectors xi,
and stack up the columns wi of w as a vector w.

some of the most relevant cases are multiclass classi   cation using both
the exponential families model and structured estimation, hierarchical mod-
els, i.e., ontologies, and multivariate regression. many of those cases are
summarized in table a3.1.

216

algorithm 3.6 f  (x, y, w)

3 id168s

1: input: feature matrix x, labels y, and weight vector w
2: compute f = xw
3:   +     {i : yi = 1} sorted in descending order of f
4:           {i : yi =    1} sorted in ascending order of f

5: let p0 = 0 and pi = 2(cid:80)m+
6: let n0 = 0 and ni = 2(cid:80)m   

, i = 1, . . . , m+
, i = 1, . . . , m   

k=i f  +
k=i f     

k

k

7: y(cid:48)        y and r           
8: for i = 0 to m+ do
9:

for j = 0 to m    do

rtmp =    (i, j)     pi + nj
if rtmp > r then

r     rtmp
t+     i and t        j

10:

11:

12:

13:

14:

end if
end for
    1, i = 1, . . . , t+
       1, i = 1, . . . , t   

15:
16: end for
17: y(cid:48)
  +
18: y(cid:48)
i
     
19: g     (y(cid:48)     y)(cid:62)x
20: return risk r and subgradient g

i

a3.1.4.1 unstructured setting
the simplest loss is multivariate regression, where l(x, y, w ) = 1
x(cid:62)w ). in this case it is clear that by pre-computing xw subsequent calcu-
lations of the loss and its gradient are signi   cantly accelerated.

2 (y   x(cid:62)w )(cid:62)m (y   

a second class of important losses is given by plain multiclass classi   cation
problems, e.g., recognizing digits of a postal code or categorizing high-level
document categories. in this case,   (x, y) is best represented by ey   x (using
a linear model). clearly we may view (cid:104)w,   (x, y)(cid:105) as an operation which
chooses a column indexed by y from xw , since all labels y correspond to
a di   erent weight vector wy. formally we set (cid:104)w,   (x, y)(cid:105) = [xw ]y. in this
case, structured estimation losses can be rewritten as

y(cid:48)   (y, y(cid:48))(cid:10)wy(cid:48)     wy, x(cid:11) +    (y, y(cid:48))

l(x, y, w ) = max

and    w l(x, y, w ) =   (y, y   )(ey        ey)     x.

(3.30)
here    and     are de   ned as in section a3.1.2 and y    denotes the value of y(cid:48)

(3.29)

a3.1 id168s

217

for which the rhs of (3.29) is maximized. this means that for unstructured
multiclass settings we may simply compute xw . since this needs to be per-
formed for all observations xi we may take advantage of fast id202
routines and compute f = xw for e   ciency. likewise note that comput-
ing the gradient over m observations is now a matrix-id127,
    eyi). then
too: denote by g the matrix of rows of gradients   (yi, y   
   w remp(x, y, w ) = g(cid:62)x. note that g is very sparse with at most two
nonzero entries per row, which makes the computation of g(cid:62)x essentially
as expensive as two matrix vector multiplications. whenever we have many
classes, this may yield signi   cant computational gains.

i )(ey   

i

log-likelihood scores of exponential families share similar expansions. we

have

l(x, y, w ) = log

   w l(x, y, w ) =

y(cid:48)

(cid:88)
exp(cid:10)w,   (x, y(cid:48))(cid:11)     (cid:104)w,   (x, y)(cid:105) = log
y(cid:48)(ey(cid:48)     x) exp(cid:10)wy(cid:48), x(cid:11)
(cid:80)
y(cid:48) exp(cid:10)wy(cid:48), x(cid:11)
(cid:80)

    ey     x.

(cid:88)

y(cid:48)

(3.31)

(3.32)

exp(cid:10)wy(cid:48), x(cid:11)     (cid:104)wy, x(cid:105)

the main di   erence to the soft-margin setting is that the gradients are
not sparse in the number of classes. this means that the computation of
gradients is slightly more costly.

a3.1.4.2 ontologies

fig. a3.1. two ontologies. left: a binary hierarchy with internal nodes {1, . . . , 7}
and labels {8, . . . 15}. right: a generic directed acyclic graph with internal nodes
{1, . . . , 6, 12} and labels {7, . . . , 11, 13, . . . , 15}. note that node 5 has two parents,
namely nodes 2 and 3. moreover, the labels need not be found at the same level of
the tree: nodes 14 and 15 are one level lower than the rest of the nodes.

assume that the labels we want to estimate can be found to belong to
a directed acyclic graph. for instance, this may be a gene-ontology graph

218

3 id168s

[abb+00] a patent hierarchy [ch04], or a genealogy. in these cases we have a
hierarchy of categories to which an element x may belong. figure a3.1 gives
two examples of such directed acyclic graphs (dag). the    rst example is
a binary tree, while the second contains nodes with di   erent numbers of
children (e.g., node 4 and 12), nodes at di   erent levels having children (e.g.,
nodes 5 and 12), and nodes which have more than one parent (e.g., node 5).
it is a well known fundamental property of trees that they have at most as
many internal nodes as they have leaf nodes.

it is now our goal to build a classi   er which is able to categorize observa-
tions according to which leaf node they belong to (each leaf node is assigned
a label y). denote by k + 1 the number of nodes in the dag including the
root node. in this case we may design a feature map   (y)     rk [ch04] by
associating with every label y the vector describing the path from the root
node to y, ignoring the root node itself. for instance, for the    rst dag in
figure a3.1 we have

  (8) = (1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0) and   (13) = (0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0)

whenever several paths are admissible, as in the right dag of figure a3.1
we average over all possible paths. for example, we have

  (10) = (0.5, 0.5, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0) and   (15) = (0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1).

also note that the lengths of the paths need not be the same (e.g., to
reach 15 it takes a longer path than to reach 13). likewise, it is natural to
assume that    (y, y(cid:48)), i.e., the cost for mislabeling y as y(cid:48) will depend on the
similarity of the path. in other words, it is likely that the cost for placing
x into the wrong sub-sub-category is less than getting the main category of
the object wrong.
to complete the setting, note that for   (x, y) =   (y)     x the cost of
computing all labels is k inner products, since the value of (cid:104)w,   (x, y)(cid:105) for a
particular y can be obtained by the sum of the contributions for the segments
of the path. this means that the values for all terms can be computed by
a simple breadth    rst traversal through the graph. as before, we may make
use of vectorization in our approach, since we may compute xw     rk to
obtain the contributions on all segments of the dag before performing the
graph traversal. since we have m patterns xi we may vectorize matters by
pre-computing xw .
also note that   (y)     (y(cid:48)) is nonzero only for those edges where the paths
for y and y(cid:48) di   er. hence we only change weights on those parts of the graph
where the categorization di   ers. algorithm 3.7 describes the subgradient and
loss computation for the soft-margin type of id168.

a3.1 id168s

219

rd  k

algorithm 3.7 ontology(x, y, w )
1: input: feature matrix x     rm  d, labels y, and weight matrix w    
2: initialization: g = 0     rm  k and r = 0
3: compute f = xw and let fi = xiw
4: for i = 1 to m do
5:

let di be the dag with edges annotated with the values of fi
traverse di to    nd node y    that maximize sum of fi values on the
path plus    (yi, y(cid:48))
7: gi =   (y   )       (yi)
r     r + zy        zyi

6:

8:
9: end for
10: g = g(cid:62)x
11: return risk r and subgradient g

the same reasoning applies to estimation when using an exponential fam-
ilies model. the only di   erence is that we need to compute a soft-max
over paths rather than exclusively choosing the best path over the ontol-
ogy. again, a breadth-   rst recursion su   ces: each of the leaves y of the
dag is associated with a id203 p(y|x). to obtain ey   p(y|x) [  (y)] all
we need to do is perform a bottom-up traversal of the dag summing over
all id203 weights on the path. wherever a node has more than one
parent, we distribute the id203 weight equally over its parents.

bibliography

[abb+00] m. ashburner, c. a. ball, j. a. blake, d. botstein, h. butler, j. m.
cherry, a. p. davis, k. dolinski, s. s. dwight, j. t. eppig, m. a. harris,
d. p. hill, l. issel-tarver, a. kasarskis, s. lewis, j. c. matese, j. e. richard-
son, m. ringwald, g. m. rubin, and g. sherlock, gene ontology: tool for the
uni   cation of biology. the gene ontology consortium, nat genet 25 (2000), 25   
29.

[agml90] s. f. altschul, w. gish, e. w. myers, and d. j. lipman, basic local
alignment search tool, journal of molecular biology 215 (1990), no. 3, 403   
410.

[bbl05] o. bousquet, s. boucheron, and g. lugosi, theory of classi   cation: a sur-

vey of recent advances, esaim: probab. stat. 9 (2005), 323    375.

[bcr84] c. berg, j. p. r. christensen, and p. ressel, harmonic analysis on semi-

groups, springer, new york, 1984.

[bdel03] s. ben-david, n. eiron, and p.m. long, on the di   culty of approximately

maximizing agreements, j. comput. system sci. 66 (2003), no. 3, 496   514.

[bel61] r. e. bellman, adaptive control processes, princeton university press,

princeton, nj, 1961.

[bel05] alexandre belloni, introduction to bundle methods, tech. report, operation

research center, m.i.t., 2005.

[ber85] j. o. berger, statistical decision theory and bayesian analysis, springer,

new york, 1985.

[bh04] j. basilico and t. hofmann, unifying collaborative and content-based    lter-
ing, proc. intl. conf. machine learning (new york, ny), acm press, 2004,
pp. 65   72.

[bhk98] j. s. breese, d. heckerman, and c. kardie, empirical analysis of predictive
algorithms for collaborative    ltering, proceedings of the 14th conference on
uncertainty in arti   cial intelligence, 1998, pp. 43   52.

[bhs+07] g. bakir, t. hofmann, b. sch  olkopf, a. smola, b. taskar, and s. v. n.
vishwanathan, predicting structured data, mit press, cambridge, mas-
sachusetts, 2007.

[bil68] patrick billingsley, convergence of id203 measures, john wiley and

sons, 1968.

[bis95] c. m. bishop, neural networks for pattern recognition, clarendon press,

oxford, 1995.

[bk07] r. m. bell and y. koren, lessons from the net   ix prize challenge, sigkdd

explorations 9 (2007), no. 2, 75   79.

[bkl06] a. beygelzimer, s. kakade, and j. langford, cover trees for nearest neigh-

bor, international conference on machine learning, 2006.

[bl00] j. m. borwein and a. s. lewis, convex analysis and nonlinear optimization:
theory and examples, cms books in mathematics, canadian mathematical
society, 2000.

221

222

3 bibliography

[bm92] k. p. bennett and o. l. mangasarian, robust id135 discrimi-
nation of two linearly inseparable sets, optim. methods softw. 1 (1992), 23   34.
[bnj03] d. blei, a. ng, and m. jordan, id44, journal of ma-

chine learning research 3 (2003), 993   1022.

[bt03] d.p. bertsekas and j.n. tsitsiklis, introduction to id203, athena sci-

enti   c, 2003.

[bv04] s. boyd and l. vandenberghe, id76, cambridge university

press, cambridge, england, 2004.

[cdls99] r. cowell, a. dawid, s. lauritzen, and d. spiegelhalter, probabilistic

networks and expert sytems, springer, new york, 1999.

[ch04] lijuan cai and t. hofmann, hierarchical document categorization with sup-
port vector machines, proceedings of the thirteenth acm conference on infor-
mation and knowledge management (new york, ny, usa), acm press, 2004,
pp. 78   87.

[cra46] h. cram  er, mathematical methods of statistics, princeton university press,

1946.

[cre93] n. a. c. cressie, statistics for spatial data, john wiley and sons, new york,

1993.

[cs03] k. crammer and y. singer, ultraconservative online algorithms for multi-

class problems, journal of machine learning research 3 (2003), 951   991.

[css00] m. collins, r. e. schapire, and y. singer, id28, adaboost
and bregman distances, proc. 13th annu. conference on comput. learning
theory, morgan kaufmann, san francisco, 2000, pp. 158   169.

[cv95] corinna cortes and v. vapnik, support vector networks, machine learning

20 (1995), no. 3, 273   297.

[dg03] s. dasgupta and a. gupta, an elementary proof of a theorem of johnson

and lindenstrauss, random struct. algorithms 22 (2003), no. 1, 60   65.

[dg08] j. dean and s. ghemawat, mapreduce: simpli   ed data processing on large

clusters, cacm 51 (2008), no. 1, 107   113.

[dgl96] l. devroye, l. gy  or   , and g. lugosi, a probabilistic theory of pattern

recognition, applications of mathematics, vol. 31, springer, new york, 1996.

[fel71] w. feller, an introduction to id203 theory and its applications, 2 ed.,

john wiley and sons, new york, 1971.

[fj95] a. frieze and m. jerrum, an analysis of a monte carlo algorithm for esti-

mating the permanent, combinatorica 15 (1995), no. 1, 67   83.

[fs99] y. freund and r. e. schapire, large margin classi   cation using the percep-

tron algorithm, machine learning 37 (1999), no. 3, 277   296.

[ft94] l. fahrmeir and g. tutz, multivariate statistical modelling based on gener-

alized linear models, springer, 1994.

[gim99] a. gionis, p. indyk, and r. motwani, similarity search in high dimensions
via hashing, proceedings of the 25th vldb conference (edinburgh, scotland)
(m. p. atkinson, m. e. orlowska, p. valduriez, s. b. zdonik, and m. l. brodie,
eds.), morgan kaufmann, 1999, pp. 518   529.

[gs04] t.l. gri   ths and m. steyvers, finding scienti   c topics, proceedings of the

national academy of sciences 101 (2004), 5228   5235.

[gw92] p. groeneboom and j. a. wellner, information bounds and nonparametric

id113, dmv, vol. 19, springer, 1992.

[hal92] p. hall, the bootstrap and edgeworth expansions, springer, new york, 1992.
[hay98] s. haykin, neural networks : a comprehensive foundation, macmillan, new

york, 1998, 2nd edition.

bibliography

223

[heb49] d. o. hebb, the organization of behavior, john wiley and sons, new york,

1949.

[hoe63] w. hoe   ding, id203 inequalities for sums of bounded random variables,

journal of the american statistical association 58 (1963), 13   30.

[hul93] j.b. hiriart-urruty and c. lemar  echal, convex analysis and minimization

algorithms, i and ii, vol. 305 and 306, springer-verlag, 1993.

[im98] p. indyk and r. motawani, approximate nearest neighbors: towards remov-
ing the curse of dimensionality, proceedings of the 30th symposium on theory
of computing, 1998, pp. 604   613.

[jk02] k. jarvelin and j. kekalainen, ir evaluation methods for retrieving highly
relevant documents, acm special interest group in information retrieval (si-
gir), new york: acm, 2002, pp. 41   48.

[joa05] t. joachims, a support vector method for multivariate performance mea-
sures, proc. intl. conf. machine learning (san francisco, california), morgan
kaufmann publishers, 2005, pp. 377   384.

[joa06]

, training linear id166s in linear time, proc. acm conf. knowledge

discovery and data mining (kdd), acm, 2006.

[jor08] m. i. jordan, an introduction to probabilistic id114, mit press,

2008, to appear.

[jv87] r. jonker and a. volgenant, a shortest augmenting path algorithm for dense
[kar80] r.m. karp, an algorithm to solve the m   n assignment problem in expected

and sparse linear assignment problems, computing 38 (1987), 325   340.

time o(mn log n), networks 10 (1980), no. 2, 143   152.

[kd05] s. s. keerthi and d. decoste, a modi   ed    nite id77 for fast

solution of large scale linear id166s, j. mach. learn. res. 6 (2005), 341   361.

[kel60] j. e. kelly, the cutting-plane method for solving convex programs, journal
of the society for industrial and applied mathematics 8 (1960), no. 4, 703   712.
[kiw90] krzysztof c. kiwiel, proximity control in bundle methods for convex non-
di   erentiable minimization, mathematical programming 46 (1990), 105   122.
[km00] paul komarek and andrew moore, a dynamic adaptation of ad-trees for
e   cient machine learning on large data sets, proc. intl. conf. machine learn-
ing, morgan kaufmann, san francisco, ca, 2000, pp. 495   502.

[koe05] r. koenker, quantile regression, cambridge university press, 2005.
[kuh55] h.w. kuhn, the hungarian method for the assignment problem, naval re-

search logistics quarterly 2 (1955), 83   97.

[lew98] d. d. lewis, naive (bayes) at forty: the independence assumption in in-
formation retrieval, proceedings of ecml-98, 10th european conference on
machine learning (chemnitz, de) (c. n  edellec and c. rouveirol, eds.), no.
1398, springer verlag, heidelberg, de, 1998, pp. 4   15.

[lk03] c. leslie and r. kuang, fast kernels for inexact string matching, proc.

annual conf. computational learning theory, 2003.

[lmp01] j. d. la   erty, a. mccallum, and f. pereira, conditional random    elds:
probabilistic modeling for segmenting and labeling sequence data, proceedings
of international conference on machine learning (san francisco, ca), vol. 18,
morgan kaufmann, 2001, pp. 282   289.

[lnn95] claude lemar  echal, arkadii nemirovskii, and yurii nesterov, new variants

of bundle methods, mathematical programming 69 (1995), 111   147.

[ls07] q. le and a.j. smola, direct optimization of ranking measures, j. mach.

learn. res. (2007), submitted.

[lt92] z. q. luo and p. tseng, on the convergence of coordinate descent method

224

3 bibliography

for convex di   erentiable minimization, journal of optimization theory and
applications 72 (1992), no. 1, 7   35.

[lue84] d. g. luenberger, linear and nonid135, second ed., addison-

wesley, reading, may 1984.

[mar61] m.e. maron, automatic indexing: an experimental inquiry, journal of the

association for computing machinery 8 (1961), 404   417.

[mca07] david mcallester, generalization bounds and consistency for structured
labeling, predicting structured data (cambridge, massachusetts), mit press,
2007.

[mcd89] c. mcdiarmid, on the method of bounded di   erences, survey in combina-

torics, cambridge university press, 1989, pp. 148   188.

[mit97] t. m. mitchell, machine learning, mcgraw-hill, new york, 1997.
[mn83] p. mccullagh and j. a. nelder, generalized linear models, chapman and

hall, london, 1983.

[msr+97] k.-r. m  uller, a. j. smola, g. r  atsch, b. sch  olkopf, j. kohlmorgen, and
v. vapnik, predicting time series with support vector machines, arti   cial neu-
ral networks icann   97 (berlin) (w. gerstner, a. germond, m. hasler, and
j.-d. nicoud, eds.), lecture notes in comput. sci., vol. 1327, springer-verlag,
1997, pp. 999   1004.

[mun57] j. munkres, algorithms for the assignment and transportation problems,

journal of siam 5 (1957), no. 1, 32   38.

[mya94] n. murata, s. yoshizawa, and s. amari, network information criterion    
determining the number of hidden units for arti   cial neural network models,
ieee transactions on neural networks 5 (1994), 865   872.

[nad65] e. a. nadaraya, on nonparametric estimates of density functions and re-
gression curves, theory of id203 and its applications 10 (1965), 186   190.
[nw99] j. nocedal and s. j. wright, numerical optimization, springer series in

operations research, springer, 1999.

[ol93] j.b. orlin and y. lee, quickmatch: a very fast algorithm for the assignment
problem, working paper 3547-93, sloan school of management, massachusetts
institute of technology, cambridge, ma, march 1993.

[pap62] a. papoulis, the fourier integral and its applications, mcgraw-hill, new

york, 1962.

[pla99] j. platt, fast training of support vector machines using sequential minimal
optimization, advances in kernel methods     support vector learning (cam-
bridge, ma) (b. sch  olkopf, c. j. c. burges, and a. j. smola, eds.), mit press,
1999, pp. 185   208.

[ptvf94] w. h. press, s. a. teukolsky, w. t. vetterling, and b. p. flannery,
numerical recipes in c. the art of scienti   c computation, cambridge university
press, cambridge, uk, 1994.

[rao73] c. r. rao, linear statistical id136 and its applications, john wiley and

sons, new york, 1973.

[rbz06] n. ratli   , j. bagnell, and m. zinkevich, maximum margin planning, inter-

national conference on machine learning, july 2006.

[ros58] f. rosenblatt, the id88: a probabilistic model for information storage
and organization in the brain, psychological review 65 (1958), no. 6, 386   408.
[rpb06] m. richardson, a. prakash, and e. brill, beyond id95: machine learn-
ing for static ranking, proceedings of the 15th international conference on
world wide web, www (l. carr, d. de roure, a. iyengar, c.a. goble,
and m. dahlin, eds.), acm, 2006, pp. 707   715.

bibliography

225

[rss+07] g. r  atsch, s. sonnenburg, j. srinivasan, h. witte, k.-r. m  uller, r. j.
sommer, and b. sch  olkopf, improving the caenorhabditis elegans genome an-
notation using machine learning, plos computational biology 3 (2007), no. 2,
e20 doi:10.1371/journal.pcbi.0030020.

[rud73] w. rudin, functional analysis, mcgraw-hill, new york, 1973.
[sil86] b. w. silverman, density estimation for statistical and data analysis, mono-
graphs on statistics and applied id203, chapman and hall, london, 1986.
[spst+01] b. sch  olkopf, j. platt, j. shawe-taylor, a. j. smola, and r. c.
williamson, estimating the support of a high-dimensional distribution, neu-
ral comput. 13 (2001), no. 7, 1443   1471.

[ss02] b. sch  olkopf and a. smola, learning with kernels, mit press, cambridge,

ma, 2002.

[sw86] g.r. shorack and j.a. wellner, empirical processes with applications to

statistics, wiley, new york, 1986.

[sz92] helga schramm and jochem zowe, a version of the bundle idea for minimiz-
ing a nonsmooth function: conceptual idea, convergence analysis, numerical
results, siam j. optimization 2 (1992), 121   152.

[tgk04] b. taskar, c. guestrin, and d. koller, max-margin markov networks,
advances in neural information processing systems 16 (cambridge, ma)
(s. thrun, l. saul, and b. sch  olkopf, eds.), mit press, 2004, pp. 25   32.

[tjha05] i. tsochantaridis, t. joachims, t. hofmann, and y. altun, large margin
methods for structured and interdependent output variables, j. mach. learn.
res. 6 (2005), 1453   1484.

[vap82] v. vapnik, estimation of dependences based on empirical data, springer,

berlin, 1982.

[vap95]
, the nature of statistical learning theory, springer, new york, 1995.
, statistical learning theory, john wiley and sons, new york, 1998.
[vap98]
[vdg00] s. van de geer, empirical processes in m-estimation, cambridge university

press, 2000.

[vdvw96] a. w. van der vaart and j. a. wellner, weak convergence and empirical

processes, springer, 1996.

[vgs97] v. vapnik, s. golowich, and a. j. smola, support vector method for func-
tion approximation, regression estimation, and signal processing, advances in
neural information processing systems 9 (cambridge, ma) (m. c. mozer,
m. i. jordan, and t. petsche, eds.), mit press, 1997, pp. 281   287.

[voo01] e. voorhees, overview of the trect 2001 id53 track,

trec, 2001.

[vs04] s. v. n. vishwanathan and a. j. smola, fast kernels for string and
tree matching, kernel methods in computational biology (cambridge, ma)
(b. sch  olkopf, k. tsuda, and j. p. vert, eds.), mit press, 2004, pp. 113   130.
[vsv07] s. v. n. vishwanathan, a. j. smola, and r. vidal, binet-cauchy kernels
on dynamical systems and its application to the analysis of dynamic scenes,
international journal of id161 73 (2007), no. 1, 95   119.

[wah97] g. wahba, support vector machines, reproducing kernel hilbert spaces and
the randomized gacv, tech. report 984, department of statistics, university
of wisconsin, madison, 1997.

[wat64] g. s. watson, smooth regression analysis, sankhya a 26 (1964), 359   372.
[wil98] c. k. i. williams, prediction with gaussian processes: from id75
to linear prediction and beyond, learning and id136 in id114
(m. i. jordan, ed.), kluwer academic, 1998, pp. 599   621.

226

3 bibliography

[wj03] m. j. wainwright and m. i. jordan, id114, exponential fami-
lies, and variational id136, tech. report 649, uc berkeley, department of
statistics, september 2003.

