implicit generative models:
dual vs. primal approaches

ilya tolstikhin

mpi for intelligent systems

ilya@tue.mpg.de

machine learning summer school 2017

t  ubingen, germany

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

the task:

(cid:73) there exists an unknown distribution px over the data space x

and we have an i.i.d. sample x1, . . . , xn from px .
(cid:73) find a model distribution pg over x similar to px .

we will work with latent variable models pg de   ned by 2 steps:
1. sample a code z from the latent space z;
2. map z to g(z)     x with a (random) transformation g : z     x .

(cid:90)

z

pg(x) :=

pg(x|z)pz(z)dx.

all techniques mentioned in this talk share two features:

(cid:73) while pg has no analytical expression, it is easy to sample from;
(cid:73) the objective allows for sgd training.

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

how to measure a similarity between px and pg?

(cid:73) f-divergences take any convex f : (0,   )     r with f (1) = 0.

(cid:90)

f

x

(cid:18) p(x)

(cid:19)

q(x)

df (p(cid:107)q) :=

q(x)dx

(cid:73) integral id203 metrics

take any class f of bounded real-valued functions on x .

(cid:12)(cid:12)ep [f (x)]     eq[f (y )](cid:12)(cid:12)

  f (p, q) := sup
f   f

(cid:73) optimal transport take any cost c(x, y) : x    x     r+.
e(x,y )     [c(x, y )],

wc(p, q) :=

inf

     p(x   p,y    q)

where p(x     p, y     q) is a set of all joint distributions of (x, y )
with marginals p and q respectively.

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

the goal: minimize df (px(cid:107)pg) with respect to pg
(cid:2)f   (cid:0)t (y )(cid:1)(cid:3)

variational (dual) representation of f -divergences:

ex   p [t (x)]     ey    q

df (p(cid:107)q) =

sup

t : x   dom(f   )

where f   (x) := supu x    u     f (u) is a convex conjugate of f .
solving inf pg df (px(cid:107)pg) is equivalent to
ex   px [t (x)]     ez   pz

(cid:2)f   (cid:0)t (g(z))(cid:1)(cid:3)

inf
g

sup
t

(*)

1. estimate expectations with samples:

n(cid:88)

i=1

    inf

g

1
n

sup
t

t (xi)     1
m

f   (cid:0)t (g(zj))(cid:1).

m(cid:88)

j=1

2. parametrize t = t   and g = g   using any    exible functions

(eg. deep nets) and run sgd on (*).

the goal: minimize df (px(cid:107)pg) with respect to pg
(cid:2)f   (cid:0)t (y )(cid:1)(cid:3)

variational (dual) representation of f -divergences:

ex   p [t (x)]     ey    q

df (p(cid:107)q) =

sup

t : x   dom(f   )

where f   (x) := supu x    u     f (u) is a convex conjugate of f .
solving inf pg df (px(cid:107)pg) is equivalent to
ex   px [t (x)]     ez   pz

(cid:2)f   (cid:0)t (g(z))(cid:1)(cid:3)

inf
g

sup
t

(*)

1. estimate expectations with samples:

n(cid:88)

i=1

    inf

g

1
n

sup
t

t (xi)     1
m

f   (cid:0)t (g(zj))(cid:1).

m(cid:88)

j=1

2. parametrize t = t   and g = g   using any    exible functions

(eg. deep nets) and run sgd on (*).

original id3

variational (dual) representation of f -divergences:
ex   p [t (x)]     ez   pz

df (px(cid:107)pg) =

sup

t : x   dom(f   )

where f   (x) := supu x    u     f (u) is a convex conjugate of f .

(cid:2)f   (cid:0)t (g(z))(cid:1)(cid:3)
2 + x log x and f   (t) =     log(cid:0)2     et(cid:1).

1. take f (x) =    (x + 1) log x+1

the domain of f    is (      , log 2);

2. take t = gf     t  , where gf (v) = log 2     log(1 + e   v);
3. parametrize g = g   and t   with deep nets
up to additive 2 log 2 term inf pg df (px(cid:107)pg) is equivalent to
1
   t  

(cid:0)g  (z)(cid:1)(cid:33)
ex   pd [log t  (x)] + ez   pz [log(cid:0)1     t  (g  (z))(cid:1)].

compare to the original gan objective

+ ez   pz log

1 + e   t  (x)

ex   px log

(cid:32)

sup
t  

1    

1 + e

inf
g  

1

inf
g  

sup
t  

theory vs. practice: do we know what gans do?

df (px(cid:107)pg) =

variational (dual) representation of f -divergences:
ex   p [t (x)]     ez   pz

(cid:2)f   (cid:0)t (g(z))(cid:1)(cid:3)
ex   pd [log t  (x)] + ez   pz [log(cid:0)1     t  (g  (z))(cid:1)].

where f   (x) := supu x    u     f (u) is a convex conjugate of f .

t : x   dom(f   )

sup

inf
g  

sup
t  

gans are not precisely solving inf pg js(px(cid:107)pg), because:
1. gans replace expectations with sample averages. uniform lows of

large numbers may not apply, as our function classes are huge;

2. instead of taking supremum over all possible witness functions t

gans optimize over classes of dnns;

3. in practice gans never optimize t      to the end    because of various

computational/numerical reasons.

a possible criticism of f -divergences:

(cid:73) when px and pg are supported on disjoint manifolds f -divergences

often max out.

(cid:73) this leads to numerical instabilities: no useful gradients for g.
(cid:73) consider pg(cid:48) and pg(cid:48)(cid:48) supported on manifolds m(cid:48) and m(cid:48)(cid:48).
suppose d(m(cid:48), mx ) < d(m(cid:48), mx ), where mx is the true
manifold. f -divergences will often give the same numbers.

possible solutions:

1. the smoothing: add a noise to both px and pg before comparing.

2. use other divergences, including ipms and the optimal transport.

minimizing mmd between px and pg

(cid:73) take any reproducing kernel k : x    x     r. let bk be a unit ball

of the corresponding rkhs hk.

(cid:73) maximum mean discrepancy is the following ipm:

  k(px , pg) := sup
t   bk

|epx [t (x)]     epg [t (y )]|

(mmd)

(cid:73) this optimization problem has a closed form analytical solution.

one can play the adversarial game using (mmd) instead of df (px(cid:107)pg):

(cid:73) no need to train the discriminator t ;
(cid:73) on the other hand, bk is a rather restricted class;
(cid:73) one can also train k adversarially, resulting in a stronger objective:

inf
pg

max

k

  k(px , pg).

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

minimizing the 1-wasserstein distance

1-wasserstein distance is de   ned by

w1(p, q) :=

inf

     p(x   p,y    q)

e(x,y )     [d(x, y )],

where p(x     p, y     q) is a set of all joint distributions of (x, y ) with
marginals p and q respectively and (x , d) is a metric space.
kantorovich-rubinstein duality:

w1(p, q) = sup
t   fl

|epx [t (x)]     epg [t (y )]|,

(kr)

where fl are all the bounded 1-lipschitz functions on (x , d).
wgan: in order to solve inf pg w1(px , pg) let   s play the adversarial
training card on (kr). parametrize t = t   using the weight clipping or
perform the gradient penalization.
unfortunately, (kr) holds only for the 1-wasserstein distance.

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

vae: maximizing the marginal log-likelihood

kl(px(cid:107)pg)     inf

pg

inf
pg

   epx [log pg(x)].

variational upper bound: for any conditional distribution q(z|x)
   epx [log pg(x)] = epx

    epx

(cid:2)kl(cid:0)q(z|x), pz
(cid:2)kl(cid:0)q(z|x), pg(z|x)(cid:1)(cid:3)
(cid:2)kl(cid:0)q(z|x), pz
(cid:2)kl(cid:0)q(z|x), pz

(cid:1)     eq(z|x)[log pg(x|z)](cid:3)
(cid:1)     eq(z|x)[log pg(x|z)](cid:3) .
(cid:1)     eq(z|x)[log pg(x|z)](cid:3)

in particular, if q is not restricted:
   epx [log pg(x)] = inf

epx

q

    epx

variational auto-encoders use the upper bound and

(cid:73) latent variable models with any pg(x|z), eg. n (x; g(z),   2    i)
(cid:73) set pz(z) = n (z; 0, i) and q(z|x) = n (z;   (x),   (x))
(cid:73) parametrize g = g  ,   , and    with deep nets. run sgd.

avb: reducing the gap in the upper bound

variational upper bound:
   epx [log pg(x)]     inf
q   q

epx

(cid:2)kl(cid:0)q(z|x), pz

(cid:1)     eq(z|x)[log pg(x|z)](cid:3)

adversarial id58 reduces the variational gap by

(cid:73) allowing for    exible encoders qe(z|x), de   ned implicitly by

random variables e(x,  ), where       p ;

(cid:73) replacing the kl divergence in the objective by the adversarial

approximation (any of the ones discussed above)

(cid:73) parametrize e with a deep net. run sgd.

downsides of vae and avb:

(cid:73) literature reports blurry samples. this is caused by the combination

of kl objective and the gaussian decoder.

(cid:73) importantly, pg(x|z) is trained only for encoded training points,
i.e. for z     q(z|x) and x     px . but we sample from z     pz.

unregularized auto-encoders

variational upper bound:
   epx [log pg(x)]     inf
q   q

epx

(cid:2)kl(cid:0)q(z|x), pz

(cid:1)     eq(z|x)[log pg(x|z)](cid:3)

(cid:73) the kl term in the upper bound may be viewed as a regularizer;
(cid:73) dropping it results in classical auto-encoders, where the

encoder-decoder pair tries to reconstruct all training images;
(cid:73) in this case training images x often end up being mapped to

di   erent spots chaotically scattered in the z space;

(cid:73) as a result, z captures no useful representations. sampling is hard.

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

minimizing the optimal transport

optimal transport for a cost function c(x, y) : x    x     r+ is
e(x,y )     [c(x, y )],

wc(px , pg) :=

inf

     p(x   px ,y    pg)

if pg(y |z = z) =   g(z) for all z     z, where g : z     x , we have

(cid:2)c(cid:0)x, g(z)(cid:1)(cid:3),

wc(px , pg) =

inf

q : qz =pz

epx

eq(z|x)

where qz is the marginal distribution of z when x     px , z     q(z|x).

minimizing the optimal transport

optimal transport for a cost function c(x, y) : x    x     r+ is
e(x,y )     [c(x, y )],

wc(px , pg) :=

inf

     p(x   px ,y    pg)

if pg(y |z = z) =   g(z) for all z     z, where g : z     x , we have

(cid:2)c(cid:0)x, g(z)(cid:1)(cid:3),

wc(px , pg) =

inf

q : qz =pz

epx

eq(z|x)

where qz is the marginal distribution of z when x     px , z     q(z|x).

relaxing the constraint

wc(px , pg) =

inf

q : qz =pz

(cid:2)c(cid:0)x, g(z)(cid:1)(cid:3),

epx

eq(z|x)

(cid:2)c(cid:0)x, g(z)(cid:1)(cid:3) +       d(qz, pz)

penalized optimal transport replaces the constraint with a penalty:

pot(px , pg) := inf
q

epx

eq(z|x)

and uses the adversarial training in the z space to approximate d.

(cid:73) for the 2-wasserstein distance c(x, y ) = (cid:107)x     y (cid:107)2

2 pot recovers

adversarial auto-encoders;

(cid:73) for the 1-wasserstein distance c(x, y ) = (cid:107)x     y (cid:107)2 pot and
wgan are solving the same problem from the primal and dual
forms respectively.

(cid:73) importantly, unlike vae, pot does not force q(z|x = x) to

intersect for di   erent x, which is known to lead to the blurriness.

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

(cid:73) gans approach the problem from a dual perspective.
(cid:73) they are known to produce very sharply looking images.

ez   pz [t    (g(z))]

max

g

(cid:73) but notoriously hard to train, unstable (although many would

disagree), and sometimes lead to mode collapses.

(cid:73) gans come without an encoder.

(gulrajani et al., 2017) aka improved wgan, 32x32 cifar-10

(radford et al., 2015) aka dcgan, 64x64 lsun

(cid:73) vaes approach the problem from its primal.
(cid:73) they enjoy a very stable training and often lead to diverse samples.

ez   q(z|x)[c(cid:0)x, g(z)(cid:3)

ex   px

max

g

(cid:73) but the samples look blurry
(cid:73) vaes come with encoders.

various papers are trying to combine a stability and recall of vaes with
the precision of gans:

(cid:73) choose an adversarially trained cost function c;
(cid:73) combine ae costs with the gan criteria;
(cid:73) . . .

(mescheder et al., 2017) aka avb, celeba

vae trained on cifar-10, z of 20 dim.

(bousquet et al., 2017) aka pot, cifar-10, same architecture

(bousquet et al., 2017) aka pot, cifar-10, test reconstruction

contents

1. unsupervised generative modelling and implicit models

2. distances on id203 measures

3. gan and f -gan: minimizing f -divergences (dual formulation)

4. wgan: minimizing the optimal transport (dual formulation)

5. vae: minimizing the kl-divergence (primal formulation)

6. pot: minimizing the optimal transport (primal formulation)

7. dual vs. primal: precision vs. recall? unifying vae and gan

most importantly:

we need an adequate way to evaluate the models

literature

1. nowozin, cseke, tomioka. f-gan: training generative neural

samplers using variational divergence minimization, 2016.

2. goodfellow et al. generative adversarial nets, 2014.

3. arjovsky, chintala, bottou. wasserstein gan, 2017.

4. arjovsky, bottou. towards principled methods for training

id3, 2017.

5. li et al. mmd gan: towards deeper understanding of moment

matching network, 2017.

6. dziugaite, roy, ghahramani. training generative neural networks

via maximum mean discrepancy optimization, 2015.

7. kingma, welling. auto-encoding id58, 2014.

8. makhzani et al. adversarial autoencoders, 2016.

9. mescheder, nowozin, geiger. adversarial id58: unifying
id5 and id3, 2017.

10. bousquet et al. from optimal transport to generative modeling: the

vegan cookbook, 2017.

