deep learning in the 
visual cortex
thomas serre
brown university

i.  fundamentals of primate vision
ii. computational mechanisms of rapid 

recognition and feedforward processing

iii. beyond feedforward processing: 

attentional mechanisms and cortical 
feedback

1

invariant recognition 
in natural images

2

id161 
successes

face detection

3

id161 
successes

face detection

3

mobileye system

already available on volvo s60 
and soon on most car 
manufacturers 

4

machine: millions of labeled 
examples for real-world applications

e.g., mobileye pedestrian detection system

5

what   s wrong with this 
picture?

6

    ~30,000 object categories 

(biederman, 1987)

    approach unlikely to scale up ... 

what   s wrong with this 
picture?

6

source: tenenbaum

humans: learning from 
very few examples

by age 6, a child knows 10-30k 
categories

7

bcs newsinvariant recognition 
in natural images

    subjects get the gist of the scene at 7 
images/s from unpredictable random 
sequence of images
- no time for eye movements
- no top-down / expectations

movie courtesy of jim dicarlo

8

potter 1971, 1975; see also biederman 1972; thorpe 1996

invariant recognition 
in natural images

    subjects get the gist of the scene at 7 
images/s from unpredictable random 
sequence of images
- no time for eye movements
- no top-down / expectations

movie courtesy of jim dicarlo

8

potter 1971, 1975; see also biederman 1972; thorpe 1996

invariant recognition 
in natural images

    subjects get the gist of the scene at 7 
images/s from unpredictable random 
sequence of images
- no time for eye movements
- no top-down / expectations

9

potter 1971, 1975; see also biederman 1972; thorpe 1996

invariant recognition 
in natural images

    subjects get the gist of the scene at 7 
images/s from unpredictable random 
sequence of images
- no time for eye movements
- no top-down / expectations

9

potter 1971, 1975; see also biederman 1972; thorpe 1996

invariant recognition 
in natural images

    subjects get the gist of the scene at 7 
images/s from unpredictable random 
sequence of images
- no time for eye movements
- no top-down / expectations

    coarse initial base representation
- enables rapid id164/recognition 

(   what is there?   )

- insuf   cient for object localization
- sensitive to presence of clutter

9

potter 1971, 1975; see also biederman 1972; thorpe 1996

two classes of 
models

marr    82; 
biederman    87 

10

two classes of 
models

marr    82; 
biederman    87 

10

two classes of 
models
i. fundamentals of primate vision

marr    82; 
biederman    87 
11

two classes of 
models
i. fundamentals of primate vision

 

i

 

d
n
a
n
o
i
t
i
n
g
o
c
e
r
 
d
p
a
r

g
n
s
s
e
c
o
r
p
d
r
a
w
r
o
f
d
e
e
f

 
.
i
i

 

i

marr    82; 
biederman    87 
11

two classes of 
models
i. fundamentals of primate vision

 

i

 

d
n
a
n
o
i
t
i
n
g
o
c
e
r
 
d
p
a
r

g
n
s
s
e
c
o
r
p
d
r
a
w
r
o
f
d
e
e
f

 
.
i
i

 

i

iii. attentional mechanisms 

and cortical feedback

marr    82; 
biederman    87 
11

streams of processing

12

ventral visual streamdorsal visual streamsimple eye vs. 
camera

13

source: unknown

simple eye vs. 
camera

    ~10m pixels

13

source: unknown

simple eye vs. 
camera

    ~10m pixels

    ~100m photoreceptors

- ~5m cones
- ~90m rods

13

source: unknown

eye stimulated by stuff in the world

source: webvision

14

eye stimulated by stuff in the world

photoreceptors

retinal output 
(ganglion cells)

modi   ed from http://thalamus.wustl.edu/course/eyeret.html

15

eye stimulated by stuff in the world

photoreceptors

retinal output 
(ganglion cells)

increase in    ring 
rate driven by 
external stimulus

modi   ed from http://thalamus.wustl.edu/course/eyeret.html

15

eye stimulated by stuff in the world

source: webvision

y = f   x wixi   

photoreceptors

retinal output 
(ganglion cells)

-

+

-

x1

x2

x3

w1 w2

w3

y

modi   ed from http://thalamus.wustl.edu/course/eyeret.html

16

eye stimulated by stuff in the world

source: webvision

y = f   x wixi   

afferent activity 
from neuron i

x1

x2

x3

w1 w2

w3

y

photoreceptors

retinal output 
(ganglion cells)

-

+

-

modi   ed from http://thalamus.wustl.edu/course/eyeret.html

16

eye stimulated by stuff in the world

source: webvision

y = f   x wixi   

synaptic 
ef   cacies

afferent activity 
from neuron i

-

+

-

x1

x2

x3

w1 w2

w3

y

photoreceptors

retinal output 
(ganglion cells)

modi   ed from http://thalamus.wustl.edu/course/eyeret.html

16

eye stimulated by stuff in the world

source: webvision

summation at 

the soma

y = f   x wixi   

synaptic 
ef   cacies

afferent activity 
from neuron i

-

+

-

x1

x2

x3

w1 w2

w3

y

photoreceptors

retinal output 
(ganglion cells)

modi   ed from http://thalamus.wustl.edu/course/eyeret.html

16

eye stimulated by stuff in the world

source: webvision

summation at 

the soma

y = f   x wixi   

synaptic 
ef   cacies

afferent activity 
from neuron i

-

+

-

x1

x2

x3

w1 w2

w3

y

photoreceptors

retinal output 
(ganglion cells)

modi   ed from http://thalamus.wustl.edu/course/eyeret.html

16

eye stimulated by stuff in the world

source: webvision

y = f   x wixi    / (pool)

photoreceptors

retinal output 
(ganglion cells)

-

+

-

x1

x2

x3

w1 w2

w3

y

pool

17

eye stimulated by stuff in the world

source: webvision

id172 via pool 
of neighboring units 

(e.g., tuned to 

different orientations)

y = f   x wixi    / (pool)

photoreceptors

retinal output 
(ganglion cells)

-

+

-

x1

x2

x3

w1 w2

w3

y

pool

17

rf organization in v1

simple cell

hubel & wiesel

18

hubel & wiesel    59    62    68

rf organization in v1

simple cell

hubel & wiesel

19

hubel & wiesel    59    62    68

rf organization in v1

complex cell

hubel & wiesel

20

hubel & wiesel    59    62    68

rf organization in v1

complex cell

hubel & wiesel

21

hubel & wiesel    59    62    68

rf organization in v1

hubel & wiesel

22

hubel & wiesel    59    62    68

hierarchical architecture: 
anatomy

rockland & pandya    79; 
maunsell & van essen    83; 
felleman & van essen    91

23

hierarchical architecture: 
anatomy

rockland & pandya    79; 
maunsell & van essen    83; 
felleman & van essen    91

23

source: thorpe & fabre-thorpe    01

hierarchical architecture:
latencies

nowak & bullier    97
schmolesky et al    98

24

gradual increase in complexity 

of preferred stimulus

hierarchical architecture: 
function

source: kobatake & tanaka    94; freiwald & tsao    10 
see also oram & perrett 1993; sheinberg & 
logothetis    96; gallant et al    96;  riesenhuber & 
poggio    99

25

gradual increase in complexity 

of preferred stimulus

hierarchical architecture: 
function

source: kobatake & tanaka    94; freiwald & tsao    10 
see also oram & perrett 1993; sheinberg & 
logothetis    96; gallant et al    96;  riesenhuber & 
poggio    99

25

parallel increase in invariance 
properties (position and scale) 

of neurons

hierarchical architecture: 
function

source: kobatake & tanaka 1994 
see also oram & perrett 1993; sheinberg & 
logothetis 1996; gallant et al 1996;  
riesenhuber & poggio 1999

26

columnar organization in v1

source: unknown

columnar organization

27

columnar organization in v1

dictionary of visual features of 
intermediate visual areas (v4/pit)

columnar organization

kobatake & tanaka 1994 
see also oram & perrett 1993; sheinberg & 
logothetis 1996; gallant et al 1996;  
riesenhuber & poggio 1999

28

invariant object category 

information can be decoded from 
small populations of cells in it

invariant image 
representation in the it

29

200 ms before stimulus onset (control). (c) categorization performance (n 0 64 sites, mean t sem) for different
data sources used as input to the classifier: multi-unit activity (mua) as shown in (b), single-unit activity (sua), and
local field potentials (lfp, supporting online material). (d) this confusion matrix describes the pattern of mistakes
made by the classifier (n 0 256 sites). each row indicates the actual category presented to the monkey (29), and
each column indicates the classifier predictions (in color code).

77 unique stimuli

decoding possible from around 100 ms 

% of
trials

 

 
l

o
g
e
t
a
c

a
u
t
c
a

fd
hf
mf
h
v
b
cd

0

9
0
0
2

 
,

8
1

.

 

 

 
y
r
a
u
r
b
e
f
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 
d
e
d
a
o
n
w
o
d

 

i

l

100

 

 

(

80

60

40

20

s
s
a
l
c

)
t
c
e
r
r
o
c
%

e
c
n
a
m
r
o
f
r
e
p
n
o
i
t
a
c
i
f
i

fig. 2.
invariance to
scale and position
changes. classification
performance (categori-
zation, n 0 64 sites,
chance 0 12.5%) when
the classifier was trained
on the responses to the
77 objects at a single
scale and position (de-
picted for one object by
      train      ) and perform-
ance was evaluated with
spatially shifted or scaled
versions of those ob-
jects (depicted for one
object by       test      ). the
classifier never       saw      
the shifted/scaled ver-
sions during training.
time interval 0 100 to
300 ms after stimulus
onset, bin size 0 50 ms.
the left-most column
shows the performance
for training and testing
on separate repetitions
of the objects at the
same standard position
and scale (as in fig. 1).
the second bar shows
the performance after
training on the stan-
dard position and scale
(scale 0 3.4-, center of
gaze) and testing on
the shifted and scaled images of the 77 objects. subsequent columns use different image scales
and positions for training and testing.

0
size:
position:

3.4o
center

3.4o
center

1.7o
center

6.8o
center

2o horz.

4o horz.

train

test

3.4o

3.4o

y
r
o
g
e
t
a
c
 
t
c
e
b
o
 
8

j

spikes over a short time interval (100- to 300-
ms interval divided into bins of 50 ms in this
case) (11, 23, 24, 28). this is notable consid-
ering the high trial-to-trial variability of cortical
neurons (27). the it population performance
is also robust to biological noise sources such
as neuronal death and failures in neurotrans-
mitter release efig. s1, (35)^. although fig. 1
(and most other decoding studies) assumes
precise knowledge about stimulus onset time,
this is not a limitation because we could also
accurately read out stimulus onset time from
the same it population efig. s5, (28)^.

a key computational difficulty of object
recognition is that it requires both selectivity
(different responses to distinct objects such
as one face versus another face) and in-
variance to image transformations (similar
responses to, e.g., rotations or translations of
the same face) (8, 12, 17). the main achieve-
ment of mammalian vision, and one reason
why it is still so much better than computer
vision algorithms, is the combination of high
selectivity and robust invariance. the results
in fig. 1 demonstrate selectivity;
the it
population can also support generalization
over objects within predefined categories,
suggesting that neuronal responses within a
category are similar (36). we also explored
the ability of the it population to generalize
recognition over changes in position and scale
by testing 71 additional sites with the original
77 images and four transformations in posi-
tion or scale. we could reliably classify (with
less than 10% reduction in performance) the
objects across these transformations even
though the classifier only bsaw[ each object
at one particular scale and position during
training (fig. 2). the bidentification[ per-
formance also robustly generalized across
position and scale (28). neurons also showed
scale and position invariance for novel objects
not seen before (fig. s6). the it population

invariant representation in it

representation is thus both selective and
invariant in a highly nontrivial manner. that
is, although neuronal population selectivity for
objects could be obtained from areas like v1,
this selectivity would not generalize over
changes in, e.g., position (supporting online
material).

we studied the temporal resolution of the
code by examining how classification per-

formance depended on the spike count bin
size in the interval from 100 to 300 ms after
stimulus onset (supporting online material).
we observed that bin sizes ranging from 12.5
through 50 ms yielded better performance than
larger bin sizes (fig. 3a). this does not imply
that downstream neurons are simply inte-
grating over 50-ms intervals or that no useful
object information is contained in smaller time

hung et al    05

30

864

4 november 2005 vol 310 science www.sciencemag.org

200 ms before stimulus onset (control). (c) categorization performance (n 0 64 sites, mean t sem) for different
data sources used as input to the classifier: multi-unit activity (mua) as shown in (b), single-unit activity (sua), and
local field potentials (lfp, supporting online material). (d) this confusion matrix describes the pattern of mistakes
made by the classifier (n 0 256 sites). each row indicates the actual category presented to the monkey (29), and
each column indicates the classifier predictions (in color code).

77 unique stimuli

decoding possible from around 100 ms 

% of
trials

 

 
l

o
g
e
t
a
c

a
u
t
c
a

fd
hf
mf
h
v
b
cd

0

9
0
0
2

 
,

8
1

.

 

 

 
y
r
a
u
r
b
e
f
n
o
g
r
o
g
a
m
e
c
n
e
c
s
.
w
w
w
m
o
r
f
 
d
e
d
a
o
n
w
o
d

 

i

l

100

 

 

(

80

60

40

20

s
s
a
l
c

)
t
c
e
r
r
o
c
%

e
c
n
a
m
r
o
f
r
e
p
n
o
i
t
a
c
i
f
i

fig. 2.
invariance to
scale and position
changes. classification
performance (categori-
zation, n 0 64 sites,
chance 0 12.5%) when
the classifier was trained
on the responses to the
77 objects at a single
scale and position (de-
picted for one object by
      train      ) and perform-
ance was evaluated with
spatially shifted or scaled
versions of those ob-
jects (depicted for one
object by       test      ). the
classifier never       saw      
the shifted/scaled ver-
sions during training.
time interval 0 100 to
300 ms after stimulus
onset, bin size 0 50 ms.
the left-most column
shows the performance
for training and testing
on separate repetitions
of the objects at the
same standard position
and scale (as in fig. 1).
the second bar shows
the performance after
training on the stan-
dard position and scale
(scale 0 3.4-, center of
gaze) and testing on
the shifted and scaled images of the 77 objects. subsequent columns use different image scales
and positions for training and testing.

0
size:
position:

3.4o
center

3.4o
center

1.7o
center

6.8o
center

2o horz.

4o horz.

train

test

3.4o

3.4o

y
r
o
g
e
t
a
c
 
t
c
e
b
o
 
8

j

spikes over a short time interval (100- to 300-
ms interval divided into bins of 50 ms in this
case) (11, 23, 24, 28). this is notable consid-
ering the high trial-to-trial variability of cortical
neurons (27). the it population performance
is also robust to biological noise sources such
as neuronal death and failures in neurotrans-
mitter release efig. s1, (35)^. although fig. 1
(and most other decoding studies) assumes
precise knowledge about stimulus onset time,
this is not a limitation because we could also
accurately read out stimulus onset time from
the same it population efig. s5, (28)^.

a key computational difficulty of object
recognition is that it requires both selectivity
(different responses to distinct objects such
as one face versus another face) and in-
variance to image transformations (similar
responses to, e.g., rotations or translations of
the same face) (8, 12, 17). the main achieve-
ment of mammalian vision, and one reason
why it is still so much better than computer
vision algorithms, is the combination of high
selectivity and robust invariance. the results
in fig. 1 demonstrate selectivity;
the it
population can also support generalization
over objects within predefined categories,
suggesting that neuronal responses within a
category are similar (36). we also explored
the ability of the it population to generalize
recognition over changes in position and scale
by testing 71 additional sites with the original
77 images and four transformations in posi-
tion or scale. we could reliably classify (with
less than 10% reduction in performance) the
objects across these transformations even
though the classifier only bsaw[ each object
at one particular scale and position during
training (fig. 2). the bidentification[ per-
formance also robustly generalized across
position and scale (28). neurons also showed
scale and position invariance for novel objects
not seen before (fig. s6). the it population

invariant representation in it

representation is thus both selective and
invariant in a highly nontrivial manner. that
is, although neuronal population selectivity for
objects could be obtained from areas like v1,
this selectivity would not generalize over
changes in, e.g., position (supporting online
material).

we studied the temporal resolution of the
code by examining how classification per-

formance depended on the spike count bin
size in the interval from 100 to 300 ms after
stimulus onset (supporting online material).
we observed that bin sizes ranging from 12.5
through 50 ms yielded better performance than
larger bin sizes (fig. 3a). this does not imply
that downstream neurons are simply inte-
grating over 50-ms intervals or that no useful
object information is contained in smaller time

hung et al    05

30

864

4 november 2005 vol 310 science www.sciencemag.org

button release and touch 
screen on targets 

- head-free monkeys
- multiple electrodes implanted 
along ventral stream (v4+pit)

human ss 
mit undergrads :-)

fam

new

a

)
t
c
e
r
r
o
c
 
%

(
 
y
c
a
r
u
c
c
a

100

90

80

70

60

50

feedforward processing

data collected by maxime cauchoix 
b
and denis fize (cnrs, france)

100

90

31

a

1
m

)

%

(
 
y
c
a
r
u
c
c
a

 

i

g
n
d
o
c
e
d

 

s
m
0
4
1

 

e
r
o

f

e
b

 
y
t
i
v
i
t
c
a

)

%

(
 
y
c
a
r
u
c
c
a

 

i

g
n
d
o
c
e
d

 

s
m
0
4
1

 

e
r
o

f

e
b

 
y
t
i
v
i
t
c
a

2
m

75
70
65
60
55
50
45
200

75
70
65
60
55
50
45
200

300

200

s
e
s
n
o
p
s
e
r
 
f

100

o
 
r
e
b
m
u
n

300

400

500

300

200

s
e
s
n
o
p
s
e
r
 
f

o

100

 
r
e
b
m
u
n

300
400
time (ms)

500

b

75
70
65
60
55
50
45
0

75
70
65
60
55
50
45
0

70

88

93

103

20

40

60

80

100

120

140

76

93

107

115

20

40

60
80
time (ms)

100

120

140

feedforward processing

data collected by maxime cauchoix 
and denis fize (cnrs, france)

32

    we know very little (i.e., stdp) about 
the learning rules at work in the visual 
cortex 

plasticity

very fast learning in it

learning and plasticity

li & dicarlo    08

33

fig. 2. task design and behavior. (a) a sample was followed by a delay and
a test stimulus. if the sample and test stimulus were the same category (a
match), monkeys were required to release a lever before the test disap-
peared. if they were not, there was another delay followed by a match. equal numbers of match and
nonmatch trials were randomly interleaved. (b) average performance of both monkeys. red and blue
bars indicate percentages of samples classi   ed as    dog    and    cat,    respectively.

    we know very little (i.e., stdp) about 
the learning rules at work in the visual 
cortex 

plasticity

v,

fig. 3. recording loca-
tions and single neu-
ron example. (a) re-
cording locations
in
both monkeys. a, an-
terior; p, posterior; d,
dorsal;
ventral.
there was no obvious
topography to task-
related neurons.
(b)
the average activity
of a single neuron in
response to stimuli at
the six morph blends.
the vertical lines cor-
respond (from left to
right) to sample onset,
offset, and test stimu-
lus onset. the inset
shows
the neuron   s
delay activity in re-
sponse
stimuli
along each of the nine
between-class morph
lines (see fig. 1). the
prototypes
(c1, c2,
c3, d1, d2, and d3)
are represented in the
outermost
columns;
each appears in three
morph lines. a color
scale indicates the ac-
tivity level.

to

neuron that exhibited greater activity in re-
sponse to dogs than to cats and responded
similarly to samples from the same category,
regardless of their degree of dogness or catness.
its activity was different in response to stimuli
near the category boundary, the cat-like dogs
(60:40 dog:cat) versus the dog-like cats (60:40
cat:dog) (22), but there was no difference in
activity elicited by these stimuli and by their
respective prototypes (the 100% cats or dogs)
(23). the inset in fig. 3b shows the neuron   s
activity in response to each of the 54 samples. it
exhibited overall greater activity in response to
dogs than to cats, but there were small differ-
ences within categories. just a few stimuli elic-
ited activity that was similar to that from the
other category. these stimuli were not consis-
tent across different neurons, however. across
the population of neurons, category activity ap-
peared at the start of neural responses to the
sample, about 100 ms after sample onset (24).
we examined all stimulus-selective neurons,
irrespective of whether they were category-se-
lective per se (25). for each neuron, we com-
puted the difference in activity between pairs of
samples at different positions along each be-
tween-category morph line (fig. 1a). in fig. 4,
a and b, each neuron   s average difference in
response to pairs of samples from the same
category (within-category difference, wcd) is
plotted against its difference in response to sam-
ples from different categories (between-catego-
ry difference, bcd). if neurons were not sensi-

li & dicarlo    08

supervised category learning in pfc

in activity during the sample and/or the
delay interval to cats versus dogs. similar
numbers of neurons preferred cats (sample

learning and plasticity

interval, 35/65; delay interval, 21/44) and
dogs (sample, 30/65; delay, 23/44).

figure 3b shows an example of a single

12 january 2001 vol 291 science www.sciencemag.org

34

fig. 2. task design and behavior. (a) a sample was followed by a delay and
a test stimulus. if the sample and test stimulus were the same category (a
match), monkeys were required to release a lever before the test disap-
peared. if they were not, there was another delay followed by a match. equal numbers of match and
nonmatch trials were randomly interleaved. (b) average performance of both monkeys. red and blue
bars indicate percentages of samples classi   ed as    dog    and    cat,    respectively.

    we know very little (i.e., stdp) about 
the learning rules at work in the visual 
cortex 

plasticity

v,

fig. 3. recording loca-
tions and single neu-
ron example. (a) re-
cording locations
in
both monkeys. a, an-
terior; p, posterior; d,
dorsal;
ventral.
there was no obvious
topography to task-
related neurons.
(b)
the average activity
of a single neuron in
response to stimuli at
the six morph blends.
the vertical lines cor-
respond (from left to
right) to sample onset,
offset, and test stimu-
lus onset. the inset
shows
the neuron   s
delay activity in re-
sponse
stimuli
along each of the nine
between-class morph
lines (see fig. 1). the
prototypes
(c1, c2,
c3, d1, d2, and d3)
are represented in the
outermost
columns;
each appears in three
morph lines. a color
scale indicates the ac-
tivity level.

to

neuron that exhibited greater activity in re-
sponse to dogs than to cats and responded
similarly to samples from the same category,
regardless of their degree of dogness or catness.
its activity was different in response to stimuli
near the category boundary, the cat-like dogs
(60:40 dog:cat) versus the dog-like cats (60:40
cat:dog) (22), but there was no difference in
activity elicited by these stimuli and by their
respective prototypes (the 100% cats or dogs)
(23). the inset in fig. 3b shows the neuron   s
activity in response to each of the 54 samples. it
exhibited overall greater activity in response to
dogs than to cats, but there were small differ-
ences within categories. just a few stimuli elic-
ited activity that was similar to that from the
other category. these stimuli were not consis-
tent across different neurons, however. across
the population of neurons, category activity ap-
peared at the start of neural responses to the
sample, about 100 ms after sample onset (24).
we examined all stimulus-selective neurons,
irrespective of whether they were category-se-
lective per se (25). for each neuron, we com-
puted the difference in activity between pairs of
samples at different positions along each be-
tween-category morph line (fig. 1a). in fig. 4,
a and b, each neuron   s average difference in
response to pairs of samples from the same
category (within-category difference, wcd) is
plotted against its difference in response to sam-
ples from different categories (between-catego-
ry difference, bcd). if neurons were not sensi-

li & dicarlo    08

supervised category learning in pfc

in activity during the sample and/or the
delay interval to cats versus dogs. similar
numbers of neurons preferred cats (sample

learning and plasticity

interval, 35/65; delay interval, 21/44) and
dogs (sample, 30/65; delay, 23/44).

figure 3b shows an example of a single

12 january 2001 vol 291 science www.sciencemag.org

34

matching object representations 
in man and monkey

figure 1. representational dissimilarity matrices for monkey and human it
for each pair of stimuli, each rdm (monkey, human) color codes the dissimilarity of the two response patterns elicited by the stimuli in it. the dissimilarity
measure is 1   r (pearson correlation across space). the color code re   ects percentiles (see color bar) computed separately for each rdm (for 1   r values
and their histograms, see figure 3a). the two rdms are the product of completely separate experiments and analysis pipelines (data not selected to match).
human data are from 316 bilateral inferior temporal voxels (1.95 3 1.95 3 2 mm3) with the greatest visual-object response in an independent data set. for control
analyses using different de   nitions of the it region of interest (size, laterality, exclusion of category-sensitive regions), see figures s9   s11. rdms were averaged
across two sessions for each of four subjects. monkey data are from 674 it single cells isolated in two monkeys (left it in one monkey, right in the other; kiani et al.,

kriegeskorte et al    08

35

source: felleman & vanessen    90

summary

hierarchies are ubiquitous:
anatomy, function & latencies

36

ventral visual streamsummary

two modes of processing: 
bottom-up vs. recurrent

37

ventral visual streamx

summary

two modes of processing: 
bottom-up vs. recurrent

37

ventral visual streamdeep learning in the 
visual cortex
thomas serre
brown university

i.  fundamentals of primate vision
ii. computational mechanisms of rapid 

recognition and feedforward processing

iii. beyond feedforward processing: 

attentional mechanisms and cortical 
feedback

38

vision is complex but the solution might be simple...

cavanagh    95

what we think 

we see

what we really 

see

39

