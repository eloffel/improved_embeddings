ipam summer school 2012 

tutorial on 

optimization methods for machine learning 

jorge nocedal   

northwestern university 

overview 

1.    we discuss some characteristics of optimization

 problems arising in deep learning, convex 

       id28 and inverse covariance estimation. 

2.   there are many tools at our disposal: first and second

 order methods; batch and stochastic algorithms;
 id173; primal and dual approaches; parallel
 computing 

3.   yet the state of affairs with neural nets is confusing to
 me: too many challenges are confronted at once: local
 vs local minima, nonlinearity, stochasticity,
 initializations, heuristics 

2 

open questions 

we need to isolate questions related to optimization, and study  
them in a controlled setting 

a key question is to understand the properties of stochastic vs 
 batch methods in the context of deep learning.  

after some clarity is obtained, we need to develop appropriate 
algorithms and work complexity bounds, both in a sequential  
and a parallel setting 

3 

organization 

i will discuss a few of optimization techniques and  
provide some insights into their strengths and weaknesses 

we will contrast the deterministic and stochastic settings. 
this motivates dynamic sample selection methods 

we emphasize the need for general purpose techniques, 
second order methods and scale invariance, vs heuristics 

4 

my background 

most of my practical experience in optimization methods  
for machine learning is for id103 (google) 

but i am aware of many tests done in a variety of machine  
learning applications due to my involvement in l-bfgs,  
newton-cg methods (hessian-free), dynamic sampling  
methods, etc 

i am interested in designing new optimization methods 
for machine applications, in particular for deep learning 

5 

intriguing statements 

   the best optimization algorithms are not the best 
 learning algorithms    

           bottou and bousquet  (2009) 

   when taking into account the estimation and optimization  
errors 

``stochastic methods best in spite of being worst optimization  
    methods    

6 

part i 

and 

problem characteristics 

second order methods 

7 

nonlinearity, ill conditioning 

neural nets are far more nonlinear than the functions 
minimized in many other applications           farabet 

the rate of convergence of an optimization algorithm 
is still important even though in practice one stops the 
iteration far from a minimizer    

8 

an objective function  (le,   ng) 

9 

general formulation 

min  j(w) =

m

1
m    (w;(zi, yi))
   
i =1

+  || w ||1

       (zi,yi)  training data
        zi          vector of features;    yi    labels
id168 (logistic, least squares, etc): 

   (w;(zi, yi))

(cid:0) 

ignore id173 term at first: unconstrained optimization    

min   f (x)

10 

                    the newton-cg method       (hessian free) 
problem:     min f (x)

          2 f (xk)p =       f (xk)             xk +1 = xk +  p  

-    this is a linear system of equations of size n 
-    apply the conjugate gradient (cg) method to compute an 
     approximate solution to this system 
-    cg is an iterative method endowed with very interesting 
     properties (optimal krylov method) 
-    hessian need not be computed explicitly 
-    continuum between 1st and 2nd order methods 

newton-cg: the convex case 

          2 f (xk)p =       f (xk)   
-    we show below that 
    any number of cg iterations yield a productive step 
-    not true of other iterative methods 
-    better direction and length 
   than 1st order methods 

the conjugate gradient method 

two equivalent problems 
1
2 xt ax     bt x                           (x) = ax     b
min    (x) =
solve    ax = b                                                                      r = ax     b

pk =    rk +   kpk   1
   k =

pk   1
t ark
pk   1
t apk   1

(cid:0) 

(cid:0) 

only product apk is needed
hessian-free
choose some initial point: x0 
initial direction:   p0 =    r0
for x0 = 0,   -r0 = b

interaction between cg and newton 

       we noted   -r = b  if x0 = 0

for the linear system 

          2 f (xk)p =       f (xk)           ax = b
r = ax     b                   b =         f (xk)
conclusion: if we terminate the cg algorithm after 1 iteration 
we obtain a steepest descent step 

newton-cg framework 

theorem (newton-cg with any number of cg steps)
suppose that f  is strictly convex. consider the  iteration
                         2 f (xk)p =       f (xk) + r            xk +1 = xk +  p
where    is chosen by a backtracking armijo line search.  
then     {xk}     x*

1                                               n 

steepest                            newton 
descent 

rates of convergence     scale invariance 

the rate of convergence can be: 
                linear       superlinear    quadratic 
depending on the accuracy of the cg solution 

       it inherits some of the scale invariance properties of the exact 
   id77: affine change of variables  

       x     dx  

newton-cg    the nonconvex case 

if hessian is not positive definite solve modified  system 

       [   2 f (x0) +  i] p =       f (x0)              > 0   

if    is large enough system is positive definite
let    1       2     ...       n  be the eigenvalues of    2 f (x0).
then the eigenvalues of     [   2 f (x0) +  i] are:
                                    1 +         2 +       ...       n +  
  
difficult to choose   .      trust region method learns   

the nonconvex case: alternatives 

replace    2 f (xk) by a positive definite approximation

       bp =       f (x0)              

option 1:  gauss-newton matrix j(xk)j(xk)t
option 2: stop cg early  -  negative curvature
option 3: trust region approach

for least squares gauss-newton matrix can be seen as an optimal 
inexact cg equivalent to solving in a    subspace inverse    

the nonconvex case: cg termination 

          2 f (xk)p =       f (x0)              
iterate until negative curvature 
is encountered:
      vt   f (xk)v < 0

xk 

negative 
curvature 

trust region 

       min    q(d) = dt   2 f (xk)d +    f (xk)t d + f (xk) 
              s.t.                  d         

history of newton-cg 

1.    proposed in the 1980s for unconstrained optimization and 
      systems of equations  (polyak 1960 (bounds)) 
2.   hessian-free option identified early on 
3.   trust region (1980s): robust technique for choosing  
  
4.    considered today a premier technique for large problems 
      (together with nonlinear cg and l-bfgs) 
5.    used in general nonid135: interior point, 
      active set, augmented lagrangian methods 
6.    application to stochastic problems (machine learning) 
      martens (2010),   byrd, chin, neveitt, nocedal (2011) 

newton-cg and global minimization 

1.    i know of no argument to suggest that newton-like methods 
      are better able at locating lower minima than 1st order methods 
2.    some researchers report success with    hessian-free methods    
       martens (2010). algorithms plagued with heuristics 
3.   trust region methods should be explored 
4.    properties of objective functions should be understood: do we 
      want to locate global minimizer? 
5.   more plausible for stochastic id119 

understanding newton   s method 

n

t     eigenvalue decomposition   

          2 f (xk) =   i
    vivi
i=1
1
n
          2 f (xk)   1 =
    vivi
  i
i=1
          2 f (xk)p =       f (xk)       p =       2 f (xk)   1   f (xk)

t        inverse

n

1
  i

i=1

       p =    

t   f (xk)       

    vi(vi
-    direction points along 
    eigenvectors corresponding to smallest eigenvalues 
-   - get direction and length 

inexact newton   s method 

 if we can compute the newton direction by gradually minimizing 
the quadratic, we might obtain efficiency and id173  
(steplength control) 

therefore we need to explicitly  
consider the minimization of a quadratic 
model of the objective function f 
       min    q(d) = dt   2 f (xk)d +    f (xk)t d + f (xk) 

enter the conjugate gradient method 

       min      (d) = dt   2 f (xk)d +    f (xk)t d + f (xk) 

                                               exact solution (convex case)
                      (d) =    2 f (xk)d +    f (xk) = 0             newton step 

 - the (linear) cg method is an iterative method for solving 
linear positive definite systems or minimizing the corresponding  
quadratic model. 
- key property: expanding subspace minimization 
tends to minimize first along the largest eigenvectors 

       a =    2 f (xk)            b =       f (xk) 

steepest descent 

coordinate relaxation 

coordinate relaxation does 
not terminate in n steps!  

the axis are no longer aligned  
with the coordinate directions 

conjugate directions always
 work 
(lead to solution in n steps) 

expanding subspace minimization 

each coordinate minimization determines 1  
    component of the solution. 
thus we minimize over an expanding subspace
 and we must have 

     (xk)t pi = 0     i = 0,1,...,k    1

or equivalently 

rk

t pi = 0     i = 0,1,...,k    1
(cid:0) 
steepest descent does not have this property. 
there are many conjugate direction methods. one of them is special   . 

(cid:0) 

monotonicity 

theorem: suppose that the cg method is started at zero. then 
the approximate solutions satisfy    d0        ...         di   

this provides id173 of the step 
-   another choice used in practice is to start the cg method with  
   the solution obtained at the previous iteration (monotonicity 
   lost) 

hessian sub sampling 

hessian sub-sampling for newton-cg 

s :5%, 10%                                          x

(cid:0) 

m =1                                                        m =168,000
function, gradient: large sample x                (batch) 
curvature information: small sample s 
       newton-like methods very robust with respect to choice  
      of hessian     

(cid:0) 

          2 f (xk)p =       f (xk)   

31 

stochastic optimization problem: (j(w) 

  j(w) =

m

1
m    (w;(zi, yi))
   
i =1

choose random sample of training points  x

1
| x |

        

i   x   

l(w;(zi,yi))

  j x(w) =
                                                                                              whose expectation is  j(w)
very small x :online, stochastic          
large  x : batch

32 

a sub-sampled hessian id77  

choose subsample   s     x

                       2 js(w) =

s   

   2   (w;zi, yi)

        

   2js(wk)dk =       jx (wk)      wk+1 = wk + dk

(cid:0) 

(cid:0) 

       coordinate size of subsample with number of cg steps 
       example: s=5%  and 10 cg steps          
      
       similar in cost to steepest descent     but much faster 
       experiments with logistic function: unit step acceptable 

 total step computation ~ 1 function evaluation 

33 

hessian-vector product without computing hessian 

given a function f : rn     r  and a direction d
goal:  compute      2 f (xk)d    exactly
define the function    (x;d) =    f (x)t d

(cid:0) 

   x  (x;d) =

      f (x)t d

   x

   
      

   
       =    2 f (x)t d

34 

example 
f (x) = exp(x1x2)
     (x;  ) =    f (x)t v = x2 exp(x1x2)

(

)v1 + x1exp(x1x2)

(

)v2

(cid:0) 

x  (x)  =
   2

(
(

   
   
   
   
   

2 exp(x1x2)
x2
exp(x1x2) + x1x2 exp(x1x2)

)v1 + exp(x1x2) + x1x2 exp(x1x2)
2 exp(x1x2)

(
)v1 + x1

(

)v2
)v2

   
   
   
   
   

cost of hessian-vector product comparable to cost of 
one gradient (factor of 3-5) 

35 

id28 

  jx (w) =

i   x   

l(w;(zi,yi))
   

(cid:0) 

      2jx (w)d =

i   x   

h(w;(zi,yi))

p(i)d      

       cost of hessian-vector product decreases linearly with |x| 
       hessian-vector product parallelizes, same as function 

(cid:0) 

36 

the algorithm 

function sample x  given (and fixed)

choose subsample  sk   (| sk |<<| x |)
solve
         2js(wk)dk =       jx (wk)      
by hessian - free cg method    
       wk +1 = wk +  kdk         (armijo)
resample  sk +1   (| sk +1 |<<| x |)

(cid:0) 

(cid:0) 

rather than one algorithm, this is general technique; 
can derive sub-sampled l-bfgs method  
                                         byrd, chin, neveitt, nocedal  (2011) 

37 

id103 problem  

characteristics: 
      
168,000 training points 
      
10,100 parameters (variables) 
       hessian subsample: 5% 
       solved on workstation 

n

nc

    exp
h =1

ln
    ( wijfhj)
i=0

nf
   
j=1

j(w) =

(cid:0) 

   
    wch

* jfhj

38 

function 

sub-sampled  
newton 

l-bfgs (m=5,20) 

classical newton-cg 

time 

39 

objective function  

10-50cg 

5cg 
n
j(w) =

    exp
h =1

nc
2cg 
ln
    ( wijfhj)
i=0

nf
   
j=1
1.    compute gradient (sum   ) 
2.    can code hessian-vector product 

(cid:0) 

varying cg limit 

   
    wch

* jfhj

l-bfgs 

40 

10%-1% 

l-bfgs 

50% 

100% 

varying hessian subsample s 

41 

summary of results 

id203 
10% 
12% 
13.5% 

speedup 
1.7 
2.0 
4.2 

preconditioning? 

how many cg iterations? 

for quadratic with gaussian noise, how to relate number of

 cg iterations to noise level? 

algorithmic solution. 

   2js(wk)dk =       jx(wk) + rk

x =    2jx(wk)dk +    jx(wk) 
rk
        =    2js(wk)dk +    jx(wk) +[   2jx(wk) +    2js(wk)]dk

iteration residual                          hessian error 

43 

implementation 

  after every matrix-vector product compute 

s{   2js(wk)pk}/ || pk ||

as an estimate to 
x =    2js(wk)dk +    jx(wk) +[   2jx(wk)        2js(wk)]dk
rk

iteration residual                          hessian error 

set cg stop test to balance errors, use sample variance 

44 

convergence  - scale invariance 

   2js(wk)dk =       jx (wk)      wk+1 = wk +  kdk

theorem: suppose id168 l(w) is strictly convex. 
 for any subsample size |s| and for any number of cg 
steps 

wk         w*

       newton-like method? 1st order method? 
       scale invariant:  
      

  k =1
          at almost all iterations   

x     ax

(cid:0) 

(cid:0) 

45 

(cid:0) 

(cid:0) 

