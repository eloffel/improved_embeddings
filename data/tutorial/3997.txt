deep learning &  
id171  
methods for vision 

cvpr 2012 tutorial: 9am-5:30pm 

 

rob fergus (nyu) 

kai yu (baidu) 

marc   aurelio ranzato (google) 

honglak lee (michigan) 

ruslan salakhutdinov (u. toronto) 

graham taylor (university of guelph) 

tutorial overview 

9.00am: 

 

introduction 

10.00am: 

coffee break 

10.30am: 

sparse coding 

 

 

 

rob fergus (nyu) 

kai yu (baidu) 

11.30am: 

neural networks 

marc   aurelio ranzato (google) 

12.30pm: 

lunch 

1.30pm: 
 
 

2.30pm: 
 

    

3.00pm: 

3.30pm: 
 
 

4.00pm: 

5.00pm: 

5.30pm: 

restricted boltzmann  honglak lee (michigan) 
machines  

deep boltzmann 
 
machines  

coffee break 

ruslan salakhutdinov (toronto) 

id21   ruslan salakhutdinov (toronto) 

motion & video  

graham taylor (guelph) 

summary / q & a 

all 

end 

overview 

    learning feature hierarchies for vision 

    mainly for recognition 

 

    many possible titles: 

    deep learning 

    id171 

    unsupervised id171 

 

    this talk:    basic concepts 

 

 
vision approaches 

 

 

    links to existing 

existing recognition approach 

image/video 

pixels 

hand-

designed 
feature 

extraction 

trainable 
classifier 

object 
class 

    features are not learned 

 

    trainable classifier is often generic (e.g. 

id166) 

slide: y.lecun 

motivation 

    features are key to recent progress in recognition 

 

    multitude of hand-designed features currently in use 

    sift, hog, lbp, mser, color-sift            . 

 

    where next? better classifiers? or keep building more 

features? 

felzenszwalb,  girshick,  

mcallester and ramanan, pami 

yan & huang  

(winner of pascal 2010 classification competition) 

what limits current performance? 

    ablation studies on deformable parts 

model  
     felzenszwalb, girshick, mcallester, ramanan, 

pami   10 

 

    replace each part with humans (amazon 

turk): 

 

 

 

 

parikh & zitnick, cvpr   10 

hand-crafted features 

    lp-    multiple kernel learning 
    gehler and nowozin, on feature 
combination for multiclass object 
classification, iccv   09 

    39 different kernels 
    phog, sift, v1s+, 

region cov.  etc.   

    mkl only gets  

 few % gain over  
 averaging features 

    features are  

mid-level representations 

    mid-level cues 

 

 

 

continuation 

parallelism 

junctions 

corners 

 

   tokens     from vision by d.marr: 

 

    object parts: 

 

 

 

    difficult to hand-engineer      what about learning them? 

 

 

why learn features? 

    better performance 

 

    other domains (unclear how to hand engineer): 

    kinect 

    video 

    multi spectral 

 

    feature computation time 

    dozens of features now regularly used 

    getting prohibitive for large datasets (10   s sec /image)  

 

why hierarchy? 

theoretical:  

      well-known depth-breadth tradeoff in circuits 
design [hastad 1987].  this suggests many 
functions can be much more ef   ciently 
represented with deeper architectures        [bengio 
& lecun 2007] 

 

biological:  

visual cortex is hierarchical 

 
]

e
p
r
o
h
t

[

hierarchies in vision 

    spatial pyramid matching 

    lazebnik et al. cvpr   06 

classifier 

 

    2 layer hierarchy 
    spatial pyramid 
descriptor pools 
vq   d sift 

 

spatial pyramid 

descriptor 

sift descriptors 

image 

hierarchies in vision 

    lampert et al. cvpr   09 

 

    learn attributes, then classes 

as combination of attributes 

 
 class 
labels 

attributes 

image  
features 

learning a hierarchy  
of feature extractors  

    each layer of hierarchy extracts features from 

output of previous layer 

    all the way from pixels     classifier 

    layers have the (nearly) same structure 

image/video 

pixels 

layer 1 

layer 2 

layer 3 

simple  
classifier 

    train all layers jointly 

 

multistage hubelwiesel architecture  

    stack multiple stages of simple cells / complex cells 

layers 

    higher stages compute more global, more invariant 

features 

    classification layer on top 

 

history: 

    neocognitron [fukushima 1971-1982] 

    convolutional nets [lecun 1988-2007]  

    hmax [poggio 2002-2006] 

    many others    

 

question: how do we find (or learn) the filters? 

slide: y.lecun 

classic approach to training 

    supervised 

    back-propagation 

    lots of labeled data 

    e.g. convolutional  

neural networks 

 

    problem:  

[lecun et al. 1998] 

    difficult to train deep models (vanishing 

gradients) 

    getting enough labels 

deep learning 

    unsupervised training 

 

    model distribution of input data 

 

    can use unlabeled data (unlimited) 

 

    refine with standard supervised 

techniques (e.g. backprop) 

 

single layer architecture  

input:  

image pixels / features 

filter 

normalize 
layer n 

pool 

details in the 
boxes matter 
(especially in a 
hierarchy) 

output:  

   features / classifier 

example id171 architectures 

pixels / 

feature
s 

filter with  
dictionary 
(patch/tiled/convoluti
onal) 

  + non-linearity  

id172 
between  
feature 
responses 

(group) 

max  

 

sparsit
y 

/  

softmax 

local contrast 
id172  
(subtractive & 

divisive) 

not an 
exact 
separation 

spatial/feature  

(sum or max)  

features 

sift descriptor 

image  
pixels 

apply 
gabor filters 

spatial pool  

(sum)  

normalize to 
unit length 

feature  
vector 

spatial pyramid matching 

sift 
feature
s 

filter with  
visual words 

max 

multi-scale 
spatial pool  

(sum)  

lazebnik,  
schmid,  
ponce  
[cvpr 2006] 

classifier 

filtering 

    patch 

     image as a set of patches 

 

filters 

#patches 

input 

 
s
r
e

t
l
i
f

#

filtering 

    convolutional 

    translation equivariance 

    tied filter weights  
(same at each position     few parameters) 

.
.
. 

input 

feature map 

translation equivariance 

    input translation     translation of features 

    fewer filters needed: no translated 

replications 

    but still need to cover orientation/frequency 

 

patch-based 

convolutional 

filtering 

    tiled 

    filters repeat every 

n 

    more filters than 

convolution for 
given # features 

 

input 

filters 

feature maps 

filtering 

    non-linearity 

    per-feature independent 

    tanh 

    sigmoid: 1/(1+exp(-x)) 

    rectified linear 

id172 

    contrast id172 

    see divisive id172 in neuroscience  

input 

filters 

id172 

    contrast id172 (across feature maps) 

    local mean = 0, local std. = 1,    local        7x7 

gaussian  

    equalizes the features maps 

 

 

feature maps 
 

feature maps 

after contrast id172 

id172 

    sparsity 

    constrain l0 or l1 norm of features 
    iterate with filtering operation (ista sparse 

coding) 

 

 

input  
patch 

filters 

features 

sparse coding 

id116 

role of id172  

    induces local competition between features  

to explain input 
        explaining away    in id114 
      just like top-down models 

      but more local mechanism 

 

    filtering alone cannot  

do this! 

 

 
example:  
convolutional sparse coding 

 

from zeiler et al. [cvpr   10/iccv   11] 

 

  

|.|1 

|.|1 

|.|1 
convolution 

|.|1 

filters 

pooling 

    spatial pooling 

    non-overlapping / overlapping regions 

    sum or max 

    boureau et al. icml   10 for theoretical analysis 

max 

sum 

role of pooling  

    spatial pooling 

    invariance to small 

transformations 

    larger receptive fields  

(see more of input) 
 

visualization technique from 
[le et al. nips   10]: 

 
videos from: 
http://ai.stanford.edu/~quocle/tid98web 

zeiler, taylor, fergus [iccv 2011] 

role of pooling  

 

    pooling across feature groups 

    additional form of inter-feature competition 

    gives and/or type behavior via (sum / max) 

    compositional models of zhu, yuille 

[zeiler et al.,    11] 

chen, zhu, lin, yuille, zhang [nips 
2007] 

unsupervised learning 

    only have class labels at top layer 

    intermediate layers have to be trained 

unsupervised 

 

    reconstruct input 

    1st layer: image 

    subsequent layers: features from layer 

beneath 

    need constraint to avoid learning identity 

 

auto-encoder 

output features 

e.g. 

decoder 

encoder 

feed-forward / 
bottom-up path 

input (image/ features) 

feed-back / 
generative / 
top-down 
path 

auto-encoder example 1 

    restricted id82 [hinton    02] 

decoder 
filters wt 
 
sigmoid 
function 
  (.) 

(binary) features z 

e.g. 

  (wtz) 

  (wx) 

(binary) input x 

encoder 
filters w 
 
sigmoid 
function 
  (.) 

auto-encoder example 2 

    predictive sparse decomposition  [ranzato et al.,    07] 

l1 

sparsit

y 

decoder 
filters d 
 
 

sparse features z 

e.g. 

dz 

  (wx) 

input patch x 

encoder 
filters w 
 
sigmoid 
function 
  (.) 

auto-encoder example 2 

    predictive sparse decomposition  [kavukcuoglu et al.,    09] 

sparse features z 

dz 

e.g. 

  (wx) 

input patch x 

encoder 
filters w 
 
sigmoid 
function 
  (.) 

l1 

sparsit

y 

decoder 
filters d 
 
 

training 

taxonomy of approaches 

    autoencoder (most deep learning 

methods) 
    rbms / dbms  

 

[lee / salakhutdinov] 

    denoising autoencoders  

[ranzato] 

    predictive sparse decomposition  [ranzato] 

    decoder-only 

    sparse coding  

 

[yu] 

    deconvolutional nets 

[yu]  

    encoder-only  

    neural nets (supervised)  

[ranzato] 

stacked auto-encoders 

class label 

decode

encode

r 

r 

features 

e.g. 

decode

encode

r 

r 

features 

decode

encode

r 

r 

input image 

[hinton & salakhutdinov  
science    06]  

at test time 

class label 

    remove decoders 
    use feed-forward 

path 
 

    gives 

standard(convolution
al) 
neural network 
 

    can fine-tune with 
[hinton & salakhutdinov  
science    06]  

backprop 

encode

r 

features 

e.g. 

encode

r 

features 

encode

r 

input image 

information flow in vision models 

class label 

top 

down  

bottom 

up 

input image 

    top-down (td) vs bottom-up (bu) 

 

    in vision typically: 

 bu appearance + td shape 
    example 1: mrf   s 

    example 2: parts & structure  

 

 

     models 

   

 

    context models 

    e.g. torralba et al. nips   05 

 

 

deep id82s 

both pathways 
use at train & 
test time 
 
 
 
 
td modulation 
of 
bu features 

class label 

decode

encode

r 

r 

features 

e.g. 

decode

encode

r 

r 

features 

decode

encode

r 

r 

input image 

salakhutdinov & hinton 
aistats   09 

see also: 
 
eslami et al. 
cvpr   12 
oral 
on monday 
 
 

why is top-down important? 

    example: occlusion 

    bu alone can   t separate sofa from cabinet 

    need td information to focus on relevant part of region 

multi-scale models 

    e.g. deformable parts model     

[felzenszwalb et al. pami   10], [zhu et al. cvpr   10] 

   
    note: shape part is hierarchical 

root 

 

parts 

sub- 
parts 

hog pyramid 

[felzenszwalb et al. pami   10] 

hierarchical model 

    most deep learning models are hierarchical 

input image/ features 

input image/ features 

[zeiler et al. iccv   11] 

multi-scale    vs    hierarchical 

root 

parts 

sub- 
parts 

feature pyramid 

input image/ features 

appearance term of each part 
is independent of others 

parts at one layer of hierarchy  
depend on others 

structure spectrum 

    learn everything 

    homogenous architecture 

    same for all modalities 

    only concession topology (2d vs 1d) 

 

 

 

how much learning? 

    build vision knowledge into structure 

    shape, occlusion etc. 

    stochastic grammars, parts and structure 

models 

structure spectrum 

learn 

    stochastic grammar models 

    set of production rules for objects 

    zhu & mumford, stochastic grammar of 

images, f&t 2006 

 

 

hand 
specify 

 
]
.
l

a

 
t

 

e
u
h
z

 
.

c
s

.

[

structure spectrum 

learn 

    r. girshick, p. felzenszwalb, d. 

mcallester,  id164 with 
grammar models, nips 2011 

    learn local appearance 

& shape 

hand 
specify 

structure spectrum 

learn 

    parts and structure models 

    defined connectivity graph 

    learn appearance / relative position 

 

 

hand 
specify 

[felzenszwalb & huttenlocher cvpr   00 ] 

[fischler and r. elschlager 1973 ] 

structure spectrum 

learn 

    fidler et al. eccv   10 

    fidler & leonardis cvpr   07 

 

    hierarchy 

of parts 
and structure 
models 

 

hand 
specify 

structure spectrum 

learn 

    leo zhu, yuanhao chen, alan yuille & 

collaborators 
    recursive composition, and/or graph 

    learn # units at layer 

 

 

hand 
specify 

structure spectrum 

learn 

    transforming auto-encoders 

     [hinton et al. icann   11]  

    deconvolutional networks  

    [zeiler et al. iccv   11] 

    explicit representation of what/where 

 

hand 
specify 

structure spectrum 

learn 

    neural nets / auto-encoders 

    dedicated  

pooling / lcn 
layers 

    no separation of  

what/where 

    modality  

independent  
(e.g. speech,  
images) 

 
]

   

2
1
l
m
c

i
 
,
.
l

a

 
t

 

e
e
l

[

 

 

hand 
specify 

structure spectrum 

learn 

    id82s 

    homogenous  

architecture 

    no separation of  

what/where 

    modality  

independent  
(e.g. speech, images) 

hand 
specify 

 

 

[salakhutdinov & hinton aistats   09] 

performance of deep learning 

    state-of-the-art on some (simpler) 

datasets 

 

    classification 

    ilsvrc 2010 (~1.4m images) 

    nec/uiuc winners (sparse coding)   

    full id163 (~16m images @ 2011) 

    le et al. icml   12  15.8%  (vs 9.3% weston et al.) 

    video  

    holywood 2 (action recognition): le et al. cvpr   11  53.3%  (vs 

50.9%) 

    detection 

    inria pedestrians: sermanet & lecun (6.6% vs 8.6% miss rate @ 

1fppi) 

    not yet state-of-the-art on more 

summary 

    unsupervised learning of feature 

hierarchies 
    detailed explanation in following talks 

 

    showing promise on vision benchmarks 

    success in other modalities (speech, text) 

 

    but few deep learning papers at cvpr! 

 

further resources 

    http://deeplearning.net/ 

    http://www.cs.toronto.edu/~hinton/csc2515

/deeprefs.html 

    http://www.cs.toronto.edu/~hinton/matlabf

orsciencepaper.html 

    nips 2011 workshop on deep learning 

and unsupervised id171 
    http://deeplearningworkshopnips2011.wordpress.com/ 

    torch5 http://torch5.sourceforge.net/ 

 

 

references 

   

   

   

   

   

   

[slide 5] 

p. felzenszwalb, r. girshick, d. mcallester, d. ramanan, id164 with 
discriminatively trained part based models,ieee transactions on pattern analysis 
and machine intelligence, vol. 32, no. 9, september 2010 

zheng song*, qiang chen*, zhongyang huang, yang hua, and shuicheng yan. con-
textualizing id164 and classification. in cvpr'11. (* indicates equal contri-
bution) [no. 1 performance in voc'10 classification task] 

[slide 6] 

finding the weakest link in person detectors, d. parikh, and c. l. zitnick, cvpr, 
2011. 

[slide 7] 

    gehler and nowozin, on feature combination for multiclass object classification, 

iccv   09 

   

[slide 8] 

   

   

   

http://www.amazon.com/vision-david-marr/dp/0716712849 

[slide 10]  

yoshua bengio and yann lecun: scaling learning algorithms towards ai, in bottou, l. 
and chapelle, o. and decoste, d. and weston, j. (eds), large-scale kernel 
machines, mit press, 2007 

references 

   

   

   

[slide 11] 

s. lazebnik, c. schmid, and j. ponce, beyond bags of features: spatial pyramid 
matching for recognizing natural scene categories, cvpr 2006 

[slide 12] 

    christoph h. lampert, hannes nickisch, stefan harmeling: "learning to detect 

unseen object classes by between-class attribute transfer", ieee id161 
and pattern recognition (cvpr), miami, fl, 2009  

   

   

   

   

[slide 14] riesenhuber, m. & poggio, t. (1999). id187 of object 
recognition in cortex. nature neuroscience 2: 1019-1025. 

http://www.scholarpedia.org/article/neocognitron 

k. fukushima: "neocognitron: a self-organizing neural network model for a 
mechanism of pattern recognition unaffected by shift in position", biological 
cybernetics, 36[4], pp. 193-202 (april 1980). 

y. lecun, l. bottou, y. bengio and p. haffner: gradient-based learning applied to 
document recognition, proceedings of the ieee, 86(11):2278-2324, november 1998 

 

 

references 

   

[slide 30] 

    y-lan boureau, jean ponce, and yann lecun, a theoretical analysis of 

feature pooling in vision algorithms, proc. international conference on 
machine learning (icml'10), 2010  

   

[slide 31] 

    q.v. le, j. ngiam, z. chen, d. chia, p. koh, a.y. ng , tiled convolutional 

neural networks. nips, 2010 

   

http://ai.stanford.edu/~quocle/tid98web 

    matthew d. zeiler, graham w. taylor, and rob fergus, adaptive 

deconvolutional networks for mid and high level id171, 
international conference on id161(november 6-13, 2011) 

   

[slide 32] 

    yuanhao chen, long zhu, chenxi lin, alan yuille, hongjiang zhang. rapid 

id136 on a novel and/or graph for id164, segmentation 
and parsing. nips 2007.  

 

 

references 

   

[slide 35] 

    p. smolensky, parallel distributed processing: volume 1: foundations, d. e. 

rumelhart, j. l. mcclelland, eds. (mit press, cambridge, 1986), pp. 194   
281. 

    g. e. hinton, neural comput. 14, 1711 (2002). 

   

[slide 36] 

    m. ranzato, y. boureau, y. lecun. "sparse id171 for deep 

belief networks". advances in neural information processing systems 20 
(nips 2007). 

   

[slide 39] 

    hinton, g. e. and salakhutdinov, r. r., reducing the dimensionality of data 

with neural networks. science, vol. 313. no. 5786, pp. 504 - 507, 28 july 
2006. 

   

[slide 41] 

    a. torralba, k. p. murphy and w. t. freeman, contextual models for object 

detection using boosted random fields, adv. in neural information 
processing systems 17 (nips), pp. 1401-1408, 2005. 

references 

   

[slide 42] 

    ruslan salakhutdinov and geoffrey hinton, deep id82s, 

12th international conference on artificial intelligence and statistics (2009). 

   

[slide 44] 

    p. felzenszwalb, r. girshick, d. mcallester, d. ramanan, id164 

with discriminatively trained part based models,ieee transactions on 
pattern analysis and machine intelligence, vol. 32, no. 9, september 2010 

   

   

long zhu, yuanhao chen, alan yuille, william freeman. latent hierarchical 
structural learning for id164. cvpr 2010.  

[slide 45] 

    matthew d. zeiler, graham w. taylor, and rob fergus, adaptive 

deconvolutional networks for mid and high level id171, 
international conference on id161(november 6-13, 2011) 

 

 

 

 

references 

   

[slide 48] 

    s.c. zhu and d. mumford, a stochastic grammar of images, foundations 

and trends in computer graphics and vision, vol.2, no.4, pp 259-362, 
2006.   

   

[slide 49] 

    r. girshick, p. felzenszwalb, d. mcallester, id164 with grammar 

models, nips 2011 

   

[slide 50] 

    p. felzenszwalb, d. huttenlocher, pictorial structures for object 

recognition, international journal of id161, vol. 61, no. 1, 
january 2005 

    m. fischler and r. elschlager. the representation and matching of pictoral 

structures. (1973) 

   

[slide 51] 

    s. fidler, m. boben, a. leonardis. a coarse-to-fine taxonomy of 

constellations for fast multi-class id164. eccv 2010.  

    s. fidler and a. leonardis. towards scalable representations of object 

references 

   

   

   

[slide 52] 

long zhu, chenxi lin, haoda huang, yuanhao chen, alan yuille. unsupervised 
structure learning: hierarchical recursive composition, suspicious coincidence and 
competitive exclusion. eccv 2008. 

[slide 53] 

    hinton, g. e., krizhevsky, a. and wang, s, transforming auto-encoders. icann-11: 

international conference on id158s, 2011 

    matthew d. zeiler, graham w. taylor, and rob fergus, adaptive deconvolutional 

networks for mid and high level id171, international conference on 
id161(november 6-13, 2011) 

   

[slide 54] 

    q.v. le, m.a. ranzato, r. monga, m. devin, k. chen, g.s. corrado, j. dean, a.y. 

ng., building high-level features using large scale unsupervised learning. icml, 
2012. 

   

[slide 55] 

    ruslan salakhutdinov and geoffrey hinton, deep id82s, 12th 

international conference on artificial intelligence and statistics (2009). 

 

 

references 

    [slide 56] 

    http://www.image-

net.org/challenges/lsvrc/2010/ 

    q.v. le, m.a. ranzato, r. monga, m. devin, k. 
chen, g.s. corrado, j. dean, a.y. ng., building 
high-level features using large scale 
unsupervised learning. icml, 2012. 

    q.v. le, w.y. zou, s.y. yeung, a.y. ng., 

learning hierarchical spatio-temporal features 
for action recognition with independent 
subspace analysis, cvpr 2011 

 

