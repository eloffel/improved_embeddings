advanced	
   hierarchical	
   	
   

models	
   

russ	
   salakhutdinov	
   

department of statistics and computer science!

university of toronto	
   

mo6va6on	
   

       learning	
   abstract	
   representa6ons	
   that	
   support	
   transfer	
   to	
   novel	
   

tasks,	
   lies	
   at	
   the	
   core	
   of	
   many	
   problems	
   in	
   computer	
   vision,	
   speech	
   
percep6on,	
   natural	
   language	
   processing,	
   and	
   machine	
   learning.	
   

      

in	
   many	
   machine	
   learning	
   applica6ons	
   performance	
   is	
   measured	
   
using	
   hundreds	
   or	
   thousands	
   of	
   training	
   examples.	
   

       for	
   human	
   learners,	
   a	
   single	
   example	
   of	
   a	
   novel	
   category	
   is	
   oaen	
   
su   cient	
   to	
   make	
   meaningful	
   generaliza6ons	
   to	
   novel	
   instances.	
   

goal:	
   transfer	
   higher-     order	
   knowledge	
   abstracted	
   from	
   
previously	
   learned	
   concept	
   to	
   infer	
   parameters	
   of	
   a	
   
novel	
   concept	
   from	
   few	
   examples.	
   

one-     shot	
   learning	
   
   segway    

   zarc    

how	
   can	
   we	
   learn	
   a	
   novel	
   concept	
      	
   a	
   high	
   dimensional	
   
sta6s6cal	
   object	
      	
   from	
   few	
   examples.	
   	
   	
   

(lake, salakhutdinov, gross, tenenbaum, cogsci 2011)!

tradi6onal	
   supervised	
   learning	
   

segway	
   

motorcycle	
   

test:	
   	
   
what	
   is	
   this?	
   

learning	
   to	
   transfer	
   

background	
   knowledge	
   

millions	
   of	
   unlabeled	
   images	
   	
   

learn	
   to	
   transfer	
   
knowledge	
   

some	
   labeled	
   images	
   

bicycle	
   

dolphin	
   

elephant	
   

tractor	
   

learn	
   novel	
   concept	
   
from	
   one	
   example	
   

test:	
   	
   
what	
   is	
   this?	
   

learning	
   to	
   transfer	
   

background	
   knowledge	
   

millions	
   of	
   unlabeled	
   images	
   	
   

learn	
   to	
   transfer	
   
knowledge	
   

key	
   problem	
   in	
   computer	
   vision,	
   	
   
speech	
   percep6on,	
   natural	
   language	
   
processing,	
   and	
   many	
   other	
   domains.	
   

some	
   labeled	
   images	
   

bicycle	
   

dolphin	
   

elephant	
   

tractor	
   

learn	
   novel	
   concept	
   
from	
   one	
   example	
   

test:	
   	
   
what	
   is	
   this?	
   

talk	
   roadmap	
   

dbm	
   

   animal   	
   

   vehicle   	
   

	
   horse	
   

	
   cow	
   

	
   car	
    	
   van	
    	
   truck	
   

part	
   2:	
   advanced	
   hierarchical	
   models	
   
      

introduc6on:	
   transfer	
   learning/	
   
one-     shot	
   learning.	
   	
   

       compound	
   hierarchical	
   deep	
   
models:	
   	
   
-    deep	
   boltzmann	
   machines.	
   
-    hierarchical	
   latent	
   dirichlet	
   

alloca6on	
   model.	
   

       applica6ons.	
   
       conclusions	
   

hierarchical	
   bayes	
   

level 3

{   0,   0}

animal

vehicle

...

hierarchical	
   bayesian	
   
models	
   

level 2

{  k,    k,   k}

level 1

{  c,    c}

cow

horse

sheep

truck

car

hierarchical	
   prior.	
   

id203 of observed 
data given parameters 

prior id203 of 
weight vector w 

posterior id203 of 
parameters given the 
training data d. 

      	
   fei-     fei,	
   fergus,	
   and	
   perona,	
   tpami	
   2006	
   
      	
   e.	
   bart,	
   i.	
   porteous,	
   p.	
   perona,	
   and	
   m.	
   welling,	
   cvpr	
   2007	
   
      	
   miller,	
   matsakis,	
   and	
   viola,	
   cvpr	
   2000	
   
      	
   sivic,	
   russell,	
   	
   zisserman,	
   freeman,	
   and	
   efros,	
   cvpr	
   2008	
   

hierarchical-     deep	
   models	
   
deep	
   nets	
   

part-     based	
   hierarchy	
   

hd	
   models:	
   compose	
   hierarchical	
   bayesian	
   
models	
   with	
   deep	
   networks,	
   two	
   in   uen6al	
   
approaches	
   from	
   unsupervised	
   learning	
   
deep	
   networks:	
   
      	
   learn	
   mul6ple	
   layers	
   of	
   nonlineari<es.	
   
      	
   trained	
   in	
   unsupervised	
   fashion	
   -     -     	
   
unsupervised	
   feature	
   learning	
      	
   no	
   need	
   to	
   
rely	
   on	
   human-     craaed	
   input	
   representa6ons.	
   
      	
   labeled	
   data	
   is	
   used	
   to	
   slightly	
   adjust	
   the	
   
model	
   for	
   a	
   speci   c	
   task.	
   

marr	
   and	
   nishihara	
   (1978)	
   

hierarchical	
   bayes	
   

category-     based	
   hierarchy	
   

hierarchical	
   bayes:	
   
      	
   explicitly	
   represent	
   category	
   hierarchies	
   for	
   
sharing	
   abstract	
   knowledge.	
   
      	
   explicitly	
   iden6fy	
   only	
   a	
   small	
   number	
   of	
   
parameters	
   that	
   are	
   relevant	
   to	
   the	
   new	
   
concept	
   being	
   learned.	
   

collins	
   &	
   quillian	
   (1969)	
   
(salakhutdinov, tenenbaum, torralba, nips 2011)!

mo6va6on	
   for	
   our	
   approach	
   

learning	
   to	
   transfer	
   knowledge:	
   	
   

hierarchical	
   

      	
   super-     category:	
      a	
   segway	
   looks	
   
like	
   a	
   funny	
   kind	
   of	
   vehicle   .	
   
      	
   higher-     level	
   features,	
   or	
   parts,	
   	
   shared	
   
with	
   other	
   classes:	
   

        	
   wheel,	
   handle,	
   post	
   

super-     class	
   

segway	
   

parts	
   

      	
   lower-     level	
   features:	
   

        	
   edges,	
   composi6on	
   of	
   edges 	
   	
   

edges	
   

deep	
   

hierarchical	
   genera6ve	
   model	
   

   animal   	
   

hierarchical	
   latent	
   dirichlet	
   
alloca6on	
   model	
   

   vehicle   	
   

k

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

dbm	
   model	
   

images	
   

lower-     level	
   generic	
   features:	
   	
   

      	
   edges,	
   combina6on	
   of	
   edges	
   

hierarchical	
   genera6ve	
   model	
   

   animal   	
   

hierarchical	
   latent	
   dirichlet	
   
alloca6on	
   model	
   

   vehicle   	
   

k

hierarchical	
   organiza<on	
   of	
   categories:	
   
      	
   express	
   priors	
   on	
   the	
   features	
   that	
   are	
   
typical	
   of	
   di   erent	
   kinds	
   of	
   concepts	
   
      	
   modular	
   data-     parameter	
   rela6ons	
   	
   	
   	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

dbm	
   model	
   

images	
   

higher-     level	
   class-     sensi<ve	
   features:	
   

      	
   capture	
   dis6nc6ve	
   perceptual	
   
structure	
   of	
   a	
   speci   c	
   concept	
   

lower-     level	
   generic	
   features:	
   	
   

      	
   edges,	
   combina6on	
   of	
   edges	
   

talk	
   roadmap	
   

dbm	
   

   animal   	
   

   vehicle   	
   

	
   horse	
   

	
   cow	
   

	
   car	
    	
   van	
    	
   truck	
   

part	
   2:	
   advanced	
   hierarchical	
   models	
   
      

introduc6on:	
   transfer	
   learning/	
   
one-     shot	
   learning.	
   	
   

       hierarchical	
   deep	
   models:	
   	
   
-    deep	
   boltzmann	
   machines.	
   
-    hierarchical	
   latent	
   dirichlet	
   

alloca6on	
   model.	
   

       applica6ons.	
   
       conclusions	
   

deep	
   boltzmann	
   machines	
   

internal	
   representa6ons	
   capture	
   
higher-     order	
   sta6s6cal	
   structure	
   

higher-     level	
   features:	
   
combina6on	
   of	
   edges	
   

low-     level	
   features:	
   
edges	
   

built	
   from	
   unlabeled	
   inputs.	
   	
   

image	
   

input:	
   pixels	
   

a	
   brief	
   review	
   

deep	
   boltzmann	
   machine	
   

model	
   parameters	
   

       dependencies	
   between	
   hidden	
   variables.	
   
       all	
   connec6ons	
   are	
   undirected.	
   
       bojom-     up	
   and	
   top-     down:	
   

h3

h2

h1

v

w3

w2

w1

input	
   

decomposi6on	
   

the	
   joint	
   id203	
   can	
   be	
   decomposed:	
   

h3

h2

h1

v

w3

w2

w1

h3

h2

h1

v

condi6onal	
   dbm	
   

prior	
   term	
   

replace	
   the	
   last	
   term	
   with	
   
more	
   structured	
   hierarchical	
   
prior.	
   

w3

w2

w1

dbm	
   

condi6onal	
   dbm	
   

stage-     wise	
   learning	
   

the	
   joint	
   id203	
   can	
   be	
   decomposed:	
   

dbms	
   approximate	
   intractable	
   posterior	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   fully	
   factorized	
   	
   
tractable	
   distribu6on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   the	
   varia6onal	
   lower-     bound	
   takes	
   form:	
   

condi6onal	
   dbm	
   

prior	
   term	
   

likelihood	
   term	
   

id178	
   func6onal	
   

fit	
   hierarchical	
   lda	
   prior	
   	
   

stage-     wise	
   learning	
   

the	
   joint	
   id203	
   can	
   be	
   decomposed:	
   

dbms	
   approximate	
   intractable	
   posterior	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   fully	
   factorized	
   	
   
tractable	
   distribu6on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   the	
   varia6onal	
   lower-     bound	
   takes	
   form:	
   

condi6onal	
   dbm	
   

prior	
   term	
   

likelihood	
   term	
   

id178	
   func6onal	
   

       learn	
   dbm.	
   
       using	
   varia6onal	
   id136,	
   infer	
   the	
   states	
   of	
   

the	
   top-     level	
   variables	
   and	
      t	
   an	
   lda	
   prior.	
   	
   

fit	
   hierarchical	
   lda	
   prior	
   	
   

talk	
   roadmap	
   

dbm	
   

   animal   	
   

   vehicle   	
   

	
   horse	
   

	
   cow	
   

	
   car	
    	
   van	
    	
   truck	
   

part	
   2:	
   advanced	
   hierarchical	
   models	
   
      

introduc6on:	
   transfer	
   learning/	
   
one-     shot	
   learning.	
   	
   

       compound	
   hierarchical	
   deep	
   
models:	
   	
   
-    deep	
   boltzmann	
   machines.	
   
-    hierarchical	
   latent	
   dirichlet	
   

alloca6on	
   model.	
   

       applica6ons.	
   
       conclusions.	
   

bag	
   of	
   words	
   representa6on	
   

slide	
   credit:	
   fei	
   fei	
   

analogy	
   to	
   documents	
   

sensory,	
   brain,	
   	
   

visual,	
   percep6on,	
   	
   

of	
   all	
   the	
   sensory	
   impressions	
   proceeding	
   to	
   
the	
   brain,	
   the	
   visual	
   experiences	
   are	
   the	
   
dominant	
   ones.	
   our	
   percep6on	
   of	
   the	
   world	
   
around	
   us	
   is	
   based	
   essen6ally	
   on	
   the	
   messages	
   
that	
   reach	
   the	
   brain	
   from	
   our	
   eyes.	
   for	
   a	
   long	
   
6me	
   it	
   was	
   thought	
   that	
   the	
   re6nal	
   image	
   was	
   
transmijed	
   point	
   by	
   point	
   to	
   visual	
   centers	
   in	
   
re6nal,	
   cerebral	
   cortex,	
   
the	
   brain;	
   the	
   cerebral	
   cortex	
   was	
   a	
   movie	
   
screen,	
   so	
   to	
   speak,	
   upon	
   which	
   the	
   image	
   in	
   
the	
   eye	
   was	
   projected.	
   through	
   the	
   discoveries	
   
of	
   hubel	
   and	
   wiesel	
   we	
   now	
   know	
   that	
   behind	
   
the	
   origin	
   of	
   the	
   visual	
   percep6on	
   in	
   the	
   brain	
   
there	
   is	
   a	
   considerably	
   more	
   complicated	
   
course	
   of	
   events.	
   by	
   following	
   the	
   visual	
   
impulses	
   along	
   their	
   path	
   to	
   the	
   various	
   cell	
   
layers	
   of	
   the	
   op6cal	
   cortex,	
   hubel	
   and	
   wiesel	
   
have	
   been	
   able	
   to	
   demonstrate	
   that	
   the	
   
message	
   about	
   the	
   image	
   falling	
   on	
   the	
   re1na	
   
undergoes	
   a	
   step-     wise	
   analysis	
   in	
   a	
   system	
   of	
   
nerve	
   cells	
   stored	
   in	
   columns	
   

eye,	
   cell,	
   op6cal	
   	
   
nerve,	
   image	
   
hubel,	
   wiesel	
   

china,	
   trade,	
   	
   

surplus,	
   commerce,	
   	
   
exports,	
   imports,	
   us,	
   	
   
yuan,	
   bank,	
   domes6c,	
   	
   

china	
   is	
   forecas6ng	
   a	
   trade	
   surplus	
   of	
   $90bn	
   
(  51bn)	
   to	
   $100bn	
   this	
   year,	
   a	
   threefold	
   increase	
   
on	
   2004's	
   $32bn.	
   the	
   commerce	
   ministry	
   said	
   
the	
   surplus	
   would	
   be	
   created	
   by	
   a	
   predicted	
   30%	
   
jump	
   in	
   exports	
   to	
   $750bn,	
   compared	
   with	
   a	
   18%	
   
rise	
   in	
   imports	
   to	
   $660bn.	
   the	
      gures	
   are	
   likely	
   to	
   
further	
   annoy	
   the	
   us,	
   which	
   has	
   long	
   argued	
   that	
   
china's	
   exports	
   are	
   unfairly	
   helped	
   by	
   a	
   
deliberately	
   undervalued	
   yuan.	
   	
   beijing	
   agrees	
   
foreign,	
   increase,	
   	
   
the	
   surplus	
   is	
   too	
   high,	
   but	
   says	
   the	
   yuan	
   is	
   only	
   
one	
   factor.	
   bank	
   of	
   china	
   governor	
   zhou	
   
xiaochuan	
   said	
   the	
   country	
   also	
   needed	
   to	
   do	
   
more	
   to	
   boost	
   domes6c	
   demand	
   so	
   more	
   goods	
   
stayed	
   within	
   the	
   country.	
   china	
   increased	
   the	
   
value	
   of	
   the	
   yuan	
   against	
   the	
   dollar	
   by	
   2.1%	
   in	
   
july	
   and	
   permijed	
   it	
   to	
   trade	
   within	
   a	
   narrow	
   
band,	
   but	
   the	
   us	
   wants	
   the	
   yuan	
   to	
   be	
   allowed	
   to	
   
trade	
   freely.	
   however,	
   beijing	
   has	
   made	
   it	
   clear	
   
that	
   it	
   will	
   take	
   its	
   6me	
   and	
   tread	
   carefully	
   before	
   
allowing	
   the	
   yuan	
   to	
   rise	
   further	
   in	
   value.	
   

trade,	
   value	
   

intui<on:	
   documents	
   contain	
   mul6ple	
   topics.	
   

slide	
   credit:	
   fei	
   fei	
   

latent	
   dirichlet	
   alloca6on	
   

text	
   
document	
   

discovered	
   
topics	
   

blei,	
   et	
   al.	
   2003	
   

latent	
   dirichlet	
   alloca6on	
   

	
   	
   	
   	
   	
   for	
   	
   

genera6ve	
   process:	
   	
   
draw	
   each	
   topic 	
   
	
   
for	
   each	
   document	
   d:	
   	
   
       draw	
   topic	
   propor6ons	
   
       for	
   each	
   word:	
   

       draw	
   topic	
   indicator	
   
       draw	
   word	
   	
   
	
   	
   

	
   

  	


  	


z 

w 

pr(topic	
   |	
   doc)	
   

  	


k 

n 

j 

pr(word	
   |	
   topic)	
   

latent	
   dirichlet	
   alloca6on	
   

	
   	
   	
   	
   	
   for	
   	
   

genera6ve	
   process:	
   	
   
	
   

draw	
   each	
   topic 	
   
for	
   each	
   document:	
   	
   
       draw	
   topic	
   propor6ons	
   
       for	
   each	
   word:	
   

       draw	
   topic	
   indicator	
   
       draw	
   word	
   	
   
	
   	
   

	
   

  

  

z 

w

pr(topic	
   |	
   doc)	
   

  

pr(word	
   |	
   topic)	
   

latent	
   dirichlet	
   alloca6on	
   

	
   	
   	
   	
   	
   for	
   	
   

genera6ve	
   process:	
   	
   
	
   

draw	
   each	
   topic 	
   
for	
   each	
   document:	
   	
   
       draw	
   topic	
   propor6ons	
   
       for	
   each	
   word:	
   

       draw	
   topic	
   indicator	
   
       draw	
   word	
   	
   
	
   	
   

	
   

remember:	
   compound	
   hd	
   model:	
   

words	
      	
   ac6va6ons	
   of	
   dbm   s	
   top-     level	
   units.	
   	
   
topics	
      	
   distribu6ons	
   over	
   top-     level	
   units,	
   or	
   
higher-     level	
   parts.	
   

  	


  	


z 

w 

pr(topic	
   |	
   doc)	
   

  	


k 

n 

j 

pr(word	
   |	
   topic)	
   

intui6on	
   

words	
      	
   ac6va6ons	
   of	
   dbm   s	
   top-     level	
   units.	
   	
   
topics	
      	
   distribu6ons	
   over	
   top-     level	
   units,	
   or	
   
higher-     level	
   parts.	
   	
   

  

  

z 

w

pr(topic	
   |	
   doc)	
   

  

pr(word	
   |	
   topic)	
   

dbm	
   generic	
   features:	
   	
   

words	
   

lda	
   high-     level	
   features:	
   

topics	
   

images	
   

documents	
   

each	
   topic	
   is	
   made	
   up	
   of	
   words.	
   

each	
   document	
   is	
   made	
   up	
   of	
   topics.	
   

hierarchical	
   lda	
   

modeling	
   super-     category	
   structure	
   

   animal   	
   

   vehicle   	
   

       draw	
   global	
   topic	
   

propor6ons:	
   
       draw	
   super-     class	
   speci   c	
   	
   

topic	
   propor6ons:	
   

       draw	
   class-     class	
   speci   c	
   	
   

topic	
   propor6ons:	
   

k
topics	
   

       draw	
   document	
   speci   c	
   

topic	
   propor6ons:	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

nonparametric	
   extension:	
   	
   	
   	
   	
   	
   

hierarchical	
   dirichlet	
   process	
   (hdp).	
   	
   

hierarchical	
   lda:	
   example	
   

global	
   topic	
   propor6ons:	
   

super-     class	
   speci   c	
   	
   topic	
   
propor6ons:	
   

fruits:	
   apples,	
   oranges,	
   pears	
   

aqua<c	
   animals:	
   dolphins,	
   sharks.	
   

class	
   speci   c	
   	
   topic	
   
propor6ons:	
   

apples:	
   

image	
   speci   c	
   	
   topic	
   
propor6ons:	
   

oranges:	
   

hierarchical	
   lda:	
   example	
   

global	
   topic	
   propor6ons:	
   

super-     class	
   speci   c	
   	
   topic	
   
propor6ons:	
   

fruits:	
   apples,	
   oranges,	
   pears	
   

aqua<c	
   animals:	
   dolphins,	
   sharks.	
   

class	
   speci   c	
   	
   topic	
   
propor6ons:	
   

apples:	
   

oranges:	
   

so	
   far	
   we	
   have	
   assumed	
   	
   

image	
   speci   c	
   	
   topic	
   
propor6ons:	
   

a	
      xed	
   hierarchy	
   

modeling	
   the	
   number	
   of	
   	
   

super-     categories	
   

place	
   chinese	
   restaurant	
   process	
   (crp)	
   prior	
   over	
   the	
   
number	
   of	
   super-     classes.	
   	
   	
   
	
   crp	
   de   nes	
   a	
   distribu6on	
   on	
   par66on	
   of	
   integers.	
   	
   
genera6ng	
   from	
   

	
   	
   	
   :	
   

	
   

customers	
   enter	
   a	
   restaurant	
   with	
   an	
   unbounded	
   number	
   of	
   tables,	
   
where	
   the	
   nth	
   customer	
   occupies	
   a	
   table	
   k	
   drawn	
   from:	
   

where	
   	
   	
   	
   	
   	
   	
   is	
   the	
   number	
   of	
   previous	
   customers	
   at	
   table	
   k	
   and	
   	
   	
   	
   	
   is	
   the	
   
concentra6on	
   parameter.	
   

customers	
      	
   integers,	
   tables	
      	
   clusters.	
   

modeling	
   the	
   hierarchy	
   

global	
   

super	
   class	
   1	
   

super	
   class	
   2	
   

class	
   1	
   

class	
   2	
   

class	
   3	
   

new	
   class	
   

modeling	
   the	
   hierarchy	
   

global	
   

super	
   class	
   1	
   

super	
   class	
   2	
   

class	
   1	
   

class	
   2	
   

new	
   class	
   

class	
   3	
   

modeling	
   the	
   hierarchy	
   

global	
   

super	
   class	
   1	
   

super	
   class	
   2	
   

class	
   1	
   

class	
   2	
   

new	
   class	
   

class	
   3	
   

new	
   class	
   

modeling	
   the	
   hierarchy	
   

global	
   

super	
   class	
   1	
   

super	
   class	
   2	
   

new	
   	
   
super	
   class	
   

class	
   2	
   

class	
   1	
   
new	
   class	
   
expected	
   number	
   of	
   clusters:	
   	
   
the	
   nested	
   crp,	
   ncrp,	
   extends	
   crp	
   to	
   nested	
   sequence	
   of	
   par66ons,	
   one	
   for	
   
each	
   level	
   of	
   the	
   tree	
   (blei	
   et.al.	
   nips	
   2003).	
   

new	
   class	
   

class	
   3	
   

new	
   class	
   

hierarchical	
   deep	
   model	
   

   animal   	
   

   vehicle   	
   

k

topics	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

hierarchical	
   deep	
   model	
   

   animal   	
   

tree	
   hierarchy	
   of	
   	
   
classes	
   is	
   learned	
   

   vehicle   	
   

	
   

	
   (nested	
   chinese	
   restaurant	
   process)	
   

prior:	
   a	
   nonparametric	
   prior	
   over	
   tree	
   
structures.	
   

k

topics	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

hierarchical	
   deep	
   model	
   

   animal   	
   

tree	
   hierarchy	
   of	
   	
   
classes	
   is	
   learned	
   

   vehicle   	
   

	
   

	
   (nested	
   chinese	
   restaurant	
   process)	
   

prior:	
   a	
   nonparametric	
   prior	
   over	
   tree	
   
structures.	
   

k

topics	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

	
   

	
   (hierarchical	
   dirichlet	
   process)	
   prior:	
   

a	
   nonparametric	
   prior	
   allowing	
   categories	
   to	
   
share	
   higher-     level	
   features,	
   or	
   parts.	
   

hierarchical	
   deep	
   model	
   

   animal   	
   

tree	
   hierarchy	
   of	
   	
   
classes	
   is	
   learned	
   

   vehicle   	
   

	
   

	
   (nested	
   chinese	
   restaurant	
   process)	
   

prior:	
   a	
   nonparametric	
   prior	
   over	
   tree	
   
structures	
   

k

topics	
   

	
   

	
   (hierarchical	
   dirichlet	
   process)	
   prior:	
   

a	
   nonparametric	
   prior	
   allowing	
   categories	
   to	
   
share	
   higher-     level	
   features,	
   or	
   parts.	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

condi<onal	
   deep	
   boltzmann	
   
machine.	
   

enforce	
   (approximate)	
   global	
   consistency	
   	
   
through	
   many	
   local	
   constraints.	
   	
   

hierarchical	
   deep	
   model	
   

   animal   	
   

tree	
   hierarchy	
   of	
   	
   
classes	
   is	
   learned	
   

   vehicle   	
   

	
   

	
   (nested	
   chinese	
   restaurant	
   process)	
   

unlike	
   standard	
   sta6s6cal	
   models,	
   	
   
prior:	
   a	
   nonparametric	
   prior	
   over	
   tree	
   
in	
   addi6on	
   to	
   inferring	
   parameters,	
   
structures	
   
we	
   also	
   infer	
   the	
   hierarchy	
   for	
   
sharing	
   those	
   parameters.	
   

	
   (hierarchical	
   dirichlet	
   process)	
   prior:	
   

a	
   nonparametric	
   prior	
   allowing	
   categories	
   to	
   
share	
   higher-     level	
   features,	
   or	
   parts.	
   

k

	
   

topics	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

condi<onal	
   deep	
   boltzmann	
   
machine.	
   

enforce	
   (approximate)	
   global	
   consistency	
   	
   
through	
   many	
   local	
   constraints.	
   	
   

cifar	
   object	
   recogni6on	
   

   animal   	
   

tree	
   hierarchy	
   of	
   	
   
classes	
   is	
   learned	
   

   vehicle   	
   

50,000	
   images	
   of	
   100	
   classes	
   

higher-     level	
   class	
   
sensi6ve	
   features	
   	
   

id136:	
   markov	
   chain	
   	
   
monte	
   carlo	
      	
   later!	
   

	
   horse	
   

	
   cow	
   

	
   car	
   

	
   van	
   

	
   truck	
   

4	
   million	
   unlabeled	
   images	
   

lower-     level	
   
generic	
   	
   features	
   	
   

32	
   x	
   32	
   pixels	
   x	
   3	
   	
   rgb	
   

learning	
   to	
   learn	
   

the	
   model	
   learns	
   how	
   to	
   share	
   the	
   knowledge	
   across	
   many	
   visual	
   
categories.	
   

   global   	
   

learned	
   super-     
class	
   hierarchy	
   

   aqua<c	
   
animal   	
   

   fruit   	
   

   human   	
   

dolphin	
   

turtle	
   

shark	
   

ray	
   

apple	
   

orange	
   

sun   ower	
   

girl	
   

baby	
    man	
   

woman	
   

basic	
   level	
   	
   
class	
   

   	
   

learned	
   higher-     level	
   
class-     sensi<ve	
   features	
   

   	
   

learned	
   low-     level	
   
generic	
   features	
   

learning	
   to	
   learn	
   

the	
   model	
   learns	
   how	
   to	
   share	
   the	
   knowledge	
   across	
   many	
   visual	
   
categories.	
   

crocodile	
   

spider	
   

   aqua<c	
   
animal   	
   
squirrel	
   

lizard	
   
kangaroo	
   

snake	
   

   global   	
   

   fruit   	
   

   human   	
   

learned	
   super-     
class	
   hierarchy	
   
castle	
   
skyscraper	
   

bridge	
   

road	
   

bus	
   

truck	
   
baby	
    man	
   

house	
   
train	
   
streetcar	
   

basic	
   level	
   	
   
tank	
   
class	
   

tractor	
   

leopard	
   
dolphin	
   
lion	
   

bear	
   
elephant	
   

caxle	
   

chimpanzee	
   

beaver	
   

fox	
   

turtle	
   
<ger	
   

shark	
   

ray	
   

apple	
   

orange	
   

sun   ower	
   

girl	
   

wolf	
   

shrew	
   

skunk	
   

oxer	
   
porcupine	
   

camel	
   

dolphin	
    ray	
   
whale	
   

turtle	
   

shark	
   

woman	
   

pine	
   

oak	
   
willow	
   tree	
   

   	
   

learned	
   higher-     level	
   
maple	
   tree	
   
boxle	
   
class-     sensi<ve	
   
bowl	
   
features	
   

cup	
   

can	
   

lamp	
   

mouse	
   

hamster	
   

rabbit	
   

   	
   

raccoon	
   

apple	
   

learned	
   low-     level	
   
pepper	
   
peer	
   
generic	
   features	
   
sun   ower	
   

orange	
   

possum	
   

man	
    boy	
   
girl	
   

woman	
   

man	
   

sharing	
   features	
   

   fruit   	
   

real	
   

reconst-     	
   
ruc6ons	
   

shape	
   

color	
   

learning	
   to	
   
learn	
   

	
   

l

e
p
p
a

	
   

e
g
n
a
r
o

	
   
r
e
w
o
   
n
u
s

	
   

e
t
a
r
 
n
o

i
t
c
e
e
d

t

i

n
h
p
o
d

l

apple	
   

orange	
   

sun   ower	
   
sun   ower	
   roc	
   curve	
   

5    3	
   	
   	
   1	
   ex   s	
   	
   

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

pixel-     
space	
   	
   
distance	
   

false alarm rate

learning	
   to	
   learn:	
   learning	
   a	
   hierarchy	
   for	
   sharing	
   parameters	
      	
   
	
   

	
    	
   	
   	
   	
   	
   	
   rapid	
   learning	
   of	
   a	
   novel	
   concept.	
   

object	
   recogni6on	
   	
   

area under roc curve for same/different  
    (1 new class vs. 99 distractor classes) 

1	
   

0.95	
   
0.9	
   

0.85	
   
0.8	
   
0.75	
   
0.7	
   

0.65	
   

gist	
   

lda	
   

(class	
   condi6onal)	
   

dbm	
   

	
   	
   	
   hdp-     dbm	
   
(no	
   super-     classes)	
   

hdp-     dbm	
   

50	
   
1	
    3	
    5	
    10	
   
#	
   examples	
   

[averaged	
   over	
   40	
   test	
   classes]	
   

our	
   model	
   outperforms	
   standard	
   computer	
   vision	
   	
   
features	
   (e.g.	
   gist).	
   

handwrijen	
   character	
   recogni6on	
   

   alphabet	
   1   	
   

   alphabet	
   2   	
   

learned	
   
higher-     level	
   	
   
features	
   
strokes	
   

	
   char	
   1	
    	
   char	
   2	
   

	
   char	
   3	
   	
    	
   char	
   4	
   

	
   char	
   5	
   

learned	
   lower-     
level	
   features	
   
edge
s	
   

25,000	
   	
   
characters	
   

handwrijen	
   character	
   recogni6on	
   

area under roc curve for same/different  
    (1 new class vs. 1000 distractor classes) 

1	
   

0.95	
   
0.9	
   

0.85	
   
0.8	
   
0.75	
   
0.7	
   

0.65	
   

lda	
   	
   

(class	
   condi6onal)	
   

dbm	
   

pixels	
   

hdp-     dbm	
   

(no	
   super-     classes)	
   

hdp-     dbm	
   

1	
    3	
    5	
   

10	
   
#	
   examples	
   

[averaged	
   over	
   40	
   test	
   classes]	
   

simula6ng	
   new	
   characters	
   

real	
   data	
   within	
   super	
   class	
   

global	
   

super	
   
class	
   1	
   

super	
   
class	
   2	
   

class	
   1	
    class	
   2	
    new	
   class	
   

simulated	
   new	
   characters	
   

simula6ng	
   new	
   characters	
   

real	
   data	
   within	
   super	
   class	
   

global	
   

super	
   
class	
   1	
   

super	
   
class	
   2	
   

class	
   1	
    class	
   2	
   

new	
   class	
   

simulated	
   new	
   characters	
   

simula6ng	
   new	
   characters	
   

real	
   data	
   within	
   super	
   class	
   

global	
   

super	
   
class	
   1	
   

super	
   
class	
   2	
   

class	
   1	
    class	
   2	
   

new	
   class	
   

simulated	
   new	
   characters	
   

simula6ng	
   new	
   characters	
   

real	
   data	
   within	
   super	
   class	
   

global	
   

super	
   
class	
   1	
   

super	
   
class	
   2	
   

class	
   1	
    class	
   2	
   

new	
   class	
   

simulated	
   new	
   characters	
   

simula6ng	
   new	
   characters	
   

real	
   data	
   within	
   super	
   class	
   

global	
   

super	
   
class	
   1	
   

super	
   
class	
   2	
   

class	
   1	
    class	
   2	
   

new	
   class	
   

simulated	
   new	
   characters	
   

simula6ng	
   new	
   characters	
   

real	
   data	
   within	
   super	
   class	
   

global	
   

super	
   
class	
   1	
   

super	
   
class	
   2	
   

class	
   1	
    class	
   2	
   

new	
   class	
   

simulated	
   new	
   characters	
   

simula6ng	
   new	
   characters	
   

real	
   data	
   within	
   super	
   class	
   

global	
   

super	
   
class	
   1	
   

super	
   
class	
   2	
   

class	
   1	
    class	
   2	
   

new	
   class	
   

simulated	
   new	
   characters	
   

learning from very few examples 

3	
   examples	
   of	
   
	
   	
   a	
   new	
   class	
   

condi6onal	
   samples	
   
in	
   the	
   same	
   class	
   

inferred	
   super-     class	
   

learning from very few examples 

learning from very few examples 

learning from very few examples 

learning from very few examples 

learning from very few examples 

learning from very few examples 

learning from very few examples 

learning from very few examples 

mo6on	
   capture	
   
drunken	
   walk	
   

walk	
   

sexy	
   walk	
   

mo6on	
   capture	
   
drunken	
   walk	
   

walk	
   

t

e
a
r
 

n
o

i
t
c
e
e
d

t

sexy	
   walk	
   

time	
   	
   

sexy	
   walk	
   roc	
   curve	
   
hdp-     dbm	
   

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

input	
   space	
   distance	
   
(no	
   hierarchy)	
   

false alarm rate

mo6on	
   capture	
   
drunken	
   walk	
   

walk	
   

the	
   same	
   model	
   can	
   be	
   applied	
   to	
   	
   
speech,	
   text,	
   video,	
   or	
   any	
   other	
   	
   
high-     dimensional	
   data.	
   

t

e
a
r
 

n
o

i
t
c
e
e
d

t

sexy	
   walk	
   

time	
   	
   

sexy	
   walk	
   roc	
   curve	
   
hdp-     dbm	
   

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

input	
   space	
   distance	
   
(no	
   hierarchy)	
   

false alarm rate

talk	
   roadmap	
   

dbm	
   

   animal   	
   

   vehicle   	
   

	
   horse	
   

	
   cow	
   

	
   car	
    	
   van	
    	
   truck	
   

part	
   2:	
   advanced	
   hierarchical	
   models	
   
      

introduc6on:	
   transfer	
   learning/	
   
one-     shot	
   learning.	
   	
   

       compound	
   hierarchical	
   deep	
   
models:	
   	
   
-    deep	
   boltzmann	
   machines.	
   
-    hierarchical	
   latent	
   dirichlet	
   

alloca6on	
   model.	
   

       applica6ons.	
   
       conclusions	
   

other	
   hierarchical	
   models	
   

at	
   a	
   minimum,	
   object	
   categoriza6on	
   requires	
   informa6on	
   about	
   	
   
      	
   category	
   mean	
   (prototype)	
   
      	
   variances	
   along	
   each	
   dimension	
   (similarity	
   metric)	
   

color	
   features	
   vary	
   strongly,	
   whereas	
   shape	
   features	
   vary	
   weakly.	
   	
   

a	
   single	
   example	
   provides	
   some	
   
informa6on	
   about	
   the	
   prototype,	
   but	
   not	
   
about	
   the	
   variances.	
   

learning	
   class-     speci   c	
   	
   

similarity	
   metrics	
   

dog
sheep

horse
novel category: cow

car van truck

(salakhutdinov, tenenbaum, & torralba, jmlr wc&p 2012)	
   

learning	
   class-     speci   c	
   	
   

similarity	
   metrics	
   

dog

sheep

horse

cow

car van truck

(salakhutdinov, tenenbaum, & torralba, jmlr wc&p 2012)	
   

learning	
   class-     speci   c	
   	
   

similarity	
   metrics	
   

animals

vehicle

dog
sheep

horse

cow

car van truck
in	
   order	
   to	
   transfer	
   appropriate	
   similarity	
   metric,	
   the	
   model	
   needs	
   to	
   
discover	
   how	
   to	
   group	
   related	
   categories	
   into	
   super-     categories.	
   

hierarchical	
   bayes	
   

level 2

{  k,    k,   k}

level 3

{   0,   0}

animal

vehicle

...

cow

horse

sheep

truck

car

level 1

{  c,    c}

      	
   probabilis6c	
   linear	
   model	
   with	
   gaussian	
   observa6on	
   noise:	
   

      	
   place	
   a	
   conjugate	
   normal-     gamma	
   prior	
   over	
   the	
   means	
   and	
   precision	
   
parameters:	
   

hierarchical	
   prior.	
   

as	
   before,	
   infer	
   the	
   hierarchy.	
   

image	
   retrieval	
   

msr	
   cambridge	
   
dataset	
   

aeroplanes
benches and chairs
bicycles/single
cars/front
cars/rear
cars/side
signs

buildings
chimneys
doors   
scenes/office
scenes/urban
windows

forks
knives
spoons

trees
birds
flowers
leaves
scenes/countryside

animals/cows
animals/sheep

clouds

retrieved	
   images	
   with	
   our	
   model	
   

query	
   image	
   

given	
   only	
   one	
   
examples	
   of	
   a	
   cpw	
   

nearest	
   neighbor	
   

unsupervised	
   category	
   discovery	
   

can	
   we	
   discover	
   when	
   the	
   model	
   has	
   encountered	
   novel	
   categories,	
   
and	
   how	
   can	
   we	
   break	
   up	
   new	
   instances	
   into	
   novel	
   categories?	
   

the	
   test	
   set	
   consists	
   of	
   many	
   unlabeled	
   examples	
   from	
   an	
   
unknown	
   number	
   of	
   basic-     level	
   classes.	
   	
   

existing categories

novel: 0.01
car: 0.99

novel: 0.02
plane: 0.97

novel: 0.02
bench: 0.92

existing categories

novel categories

novel categories
countryside: 0.53 building: 0.49

novel: 0.28

novel: 0.42 novel: 0.87
bird: 0.11

with	
   18	
   unlabeled	
   test	
   images	
   the	
   model	
   correctly	
   places	
   nine	
   familiar	
   images	
   
in	
   nine	
   di   erent	
   basic-     level	
   categories,	
   while	
   also	
   correctly	
   forming	
   three	
   
novel	
   categories	
   with	
   3	
   examples	
   each.	
   

20

40

60

80

100

120

140

20

40

60

80

100

120

140

160

180

200

object	
   detec6on	
   challenge	
   

consider	
   challenging	
   object	
   detec6on	
   task.	
   

bread	
   

by	
   looking	
   at	
   the	
   output	
   of	
   a	
   detector,	
   can	
   you	
   guess	
   which	
   
object	
   is	
   it	
   trying	
   to	
   detect?	
   

slide	
   credit:	
   antonio	
   torralba	
   

learning	
   from	
   few	
   examples	
   

sun	
   database	
   

car	
   

van	
   

truck	
   

bus	
   

classes	
   sorted	
   by	
   frequency	
   

rare	
   objects	
   are	
   similar	
   to	
   frequent	
   objects	
   

(salakhutdinov, torralba, & tenenbaum, cvpr  2011)	
   

learning	
   from	
   few	
   examples	
   

chair	
   

armchair	
   

swivel	
   chair	
   

deck	
   chair	
   

classes	
   sorted	
   by	
   frequency	
   

genera6ve	
   model	
   of	
   classi   er	
   

parameters	
   

many	
   state-     of-     the-     art	
   object	
   detec6on	
   systems	
   use	
   sophis6cated	
   models,	
   
based	
   on	
   mul6ple	
   parts	
   with	
   separate	
   appearance	
   and	
   shape	
   components.	
   

detect	
   objects	
   by	
   tes6ng	
   sub-     windows	
   and	
   
scoring	
   corresponding	
   test	
   patches	
   with	
   a	
   linear	
   
func6on.	
   

we	
   can	
   de   ne	
   hierarchical	
   prior	
   over	
   
parameters	
   of	
   discrimina<ve	
   model	
   and	
   
learn	
   the	
   hierarchy.	
   	
   

image	
   speci   c:	
   concatena6on	
   of	
   the	
   	
   
hog	
   feature	
   pyramid	
   at	
   mul6ple	
   scales.	
   
felzenszwalb,	
   mcallester	
   &	
   ramanan,	
   2008	
   

genera6ve	
   model	
   of	
   classi   er	
   

parameters	
   

by	
   learning	
   hierarchical	
   structure,	
   	
   
we	
   can	
   improve	
   the	
   current	
   	
   
state-     of-     the-     art.	
   	
   

sun	
   dataset:	
   32,855	
   examples	
   of	
   	
   	
   
200	
   categories	
   
hierarchical	
   model	
   

single	
   class	
   

hierarchical	
   
bayes	
   

185	
   ex	
    27	
   ex	
    12	
   ex	
   

truck	
   
single	
   	
   
classi   er	
   

hierarchical	
   
model	
   

dome	
   
single	
   	
   
classi   er	
   

hierarchical	
   
model	
   

genera6ve	
   model	
   of	
   matrix	
   

factoriza6ons	
   

image	
   bases	
   

rela6onal	
   data	
   

gene	
   expression	
   data	
   

karklin	
   and	
   lewicki	
   (2009)	
   

kemp	
   et.al.	
   (2006)	
   

meeds	
   et	
   al.	
   (2007)	
   

how	
   can	
   we	
   automa6cally	
   choose	
   the	
   right	
   structure	
   from	
   raw	
   data?	
   
context	
   free	
   grammar:	
   

us	
   senate	
   votes:	
   

evolu6on	
   of	
   structure	
   discovery	
   

grosse, salakhutdinov, freeman, and tenenbaum, uai 2012	
   

talk	
   roadmap	
   

dbm	
   

   animal   	
   

   vehicle   	
   

	
   horse	
   

	
   cow	
   

	
   car	
    	
   van	
    	
   truck	
   

part	
   2:	
   advanced	
   hierarchical	
   models	
   
      

introduc6on:	
   transfer	
   learning/	
   
one-     shot	
   learning.	
   	
   

       compound	
   hierarchical	
   deep	
   
models:	
   	
   
-    deep	
   boltzmann	
   machines.	
   
-    hierarchical	
   latent	
   dirichlet	
   

alloca6on	
   model.	
   

       applica6ons.	
   
       mcmc	
   techniques.	
   

id136	
   

problem:	
   when	
   dealing	
   with	
   complex	
   high-     	
   
dimensional	
   data:	
   the	
   id203	
   landscape	
   is	
   
highly	
   mul6modal.	
   

gibbs	
   sampler	
   

inability	
   to	
   e   ciently	
   explore	
   a	
   distribu6on	
   
with	
   many	
   isolated	
   modes.	
   

problem	
   for	
   both	
   directed	
   and	
   undirected	
   
graphical	
   models.	
   	
   

      	
   posterior	
   distribu6on:	
   
      	
   boltzmann	
   machine:	
   

tempered	
   transi6ons	
   

(radford neal, 1994)	
   

de   ne	
   a	
   sequence	
   of	
   intermediate	
   id203	
   distribu6ons	
   
where:	
   	
   
      	
   	
   	
   	
   	
   	
   	
   	
   
      	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   more	
   spread	
   out	
   and	
   easier	
   to	
   sample	
   from.	
   	
   

	
   	
   	
   	
   	
   	
   	
   is	
   the	
   original	
   complicated	
   distribu6on.	
   

	
   

one	
   way	
   is	
   to	
   de   ne:	
   

where	
   	
      inverse	
   temperatures   	
   
are	
   chosen	
   by	
   the	
   user.	
   	
   

tempered	
   transi6ons	
   

de   ne	
   reverse	
   transi6on	
   operator:	
   	
   

      	
   given	
   a	
   current	
   state,	
   apply	
   a	
   sequence	
   of	
   transi6on	
   operators:	
   

      	
   systema6cally	
      move   	
   the	
   sample	
   from	
   the	
   complicated	
   distribu6on	
   
to	
   the	
   easily	
   sampled	
   distribu6on	
   and	
   back.	
   
      	
   accept	
   a	
   new	
   state	
   	
   	
   	
   	
   	
   	
   	
   with	
   id203:	
   

learning	
   mrfs	
   using	
   tempered	
   

transi6ons	
   

training	
   data	
   

samples	
   with	
   	
   	
   	
   	
   	
   
tempered	
   transi6ons	
   

samples	
   without	
   

tempered	
   transi6ons	
   

plain	
   stochas6c	
   approxima6on	
   using	
   simple	
   gibbs	
   works	
   badly.	
   
a	
   large	
   frac6on	
   of	
   the	
   model   s	
   id203	
   mass	
   is	
   placed	
   on	
   images	
   of	
   
humans.	
   

(salakhutdinov, nips 2010)	
   

simulated	
   tempering	
   (st)	
   

simulated tempering (st)
how can we de   ne a meaningful partition?
simulated tempering is a single chain mcmc algorithm,
that samples from the joint distribution:
p(x, k)     wk exp(     ke(x)),

simulated tempering (st)
how can we de   ne a meaningful partition?
      	
   simulated	
   tempering:	
   sample	
   from	
   the	
   joint	
   distribu6on:	
   
simulated tempering is a single chain mcmc algorithm,
that samples from the joint distribution:
p(x, k)     wk exp(     ke(x)),

where wk are constants, 0 <   k <   k   1 < ... <   1 = 1
	
   where	
   wk	
   are	
   pre-     speci   ed	
   constants,	
   and	
   	
   
	
   represent	
   the	
   k	
      inverse	
   temperatures   .	
   	
   
are the k    inverse temperatures   .

where wk are constants, 0 <   k <   k   1 < ... <   1 = 1
are the k    inverse temperatures   .

k=3

k=2

k=1

k=3

k=2

k=1

11

11

simulated	
   tempering	
   (st)	
   

simulated tempering (st)
simulated tempering (st)
simulated tempering (st)
how can we de   ne a meaningful partition?
how can we de   ne a meaningful partition?
simulated tempering is a single chain mcmc algorithm,
p(x, k)     wk exp(     ke(x))
      	
   simulated	
   tempering:	
   sample	
   from	
   the	
   joint	
   distribu6on:	
   
simulated tempering is a single chain mcmc algorithm,
that samples from the joint distribution:
that samples from the joint distribution:
simulated tempering (st)
simulating from the joint p(x, k):
p(x, k)     wk exp(     ke(x)),
p(x, k)     wk exp(     ke(x)),
    given k, sample x with t that leaves p(x|k) invariant
p(x, k)     wk exp(     ke(x))
where wk are constants, 0 <   k <   k   1 < ... <   1 = 1
	
   where	
   wk	
   are	
   pre-     speci   ed	
   constants,	
   and	
   	
   
(e.g. gibbs).
	
   represent	
   the	
   k	
      inverse	
   temperatures   .	
   	
   
are the k    inverse temperatures   .
simulating from the joint p(x, k):
    given x, we sample k using metropolis update rule.
k=3
    given k, sample x with t that leaves p(x|k) invariant
k=3
      	
   the	
   main	
   problem	
   of	
   st:	
   
main problem of st:
(e.g. gibbs).
k=2
k=2
k=1
    given x, we sample k using metropolis update rule.
k=1
main problem of st:
wk needs to be proportional to 1/zk.

      	
   to	
   be	
   e   cient,	
   it	
   is	
   important	
   for	
   the	
   markov	
   chain	
   to	
   spend	
   roughly	
   equal	
   
amount	
   of	
   6me	
   at	
   each	
   temperature	
   level.	
   
      	
   hence	
   wk	
   needs	
   to	
   be	
   propor6onal	
   to	
   

where wk are constants, 0 <   k <   k   1 < ... <   1 = 1
are the k    inverse temperatures   .

p(k)    !x
p(k)    !x

wk exp(     ke(x)) = wkzk

wk exp(     ke(x)) = wkzk

wk needs to be proportional to 1/zk.

11

12

11

12

min!1,

adaptive simulated tempering
1: given kt, sample kt+1 from proposal distribution q(kt+1    kt).

adap6ve	
   simulated	
   tempering	
   (ast)	
   
adaptive st
      	
   par66oning	
   the	
   state	
   space	
   into	
   k	
   sets	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   each	
   corresponding	
   
partitioning the state space into k sets {k}     x , each
gkt
  
to	
   a	
   di   erent	
   temperature	
   value.	
   
corresponding to a di   erent temperature value.
gkt+1
      	
   if	
   the	
   move	
   into	
   a	
   di   erent	
   par66on	
   (temperature)	
   is	
   rejected:	
   
if the move into a di   erent partition (temperature) is
adaptive factor
rejected:
-   	
   the	
   adap6ve	
   weight	
   gk	
   for	
   the	
   current	
   par66on	
   k	
   will	
   increase.	
   
    the adaptive weight gk for the current partition k will
-   	
   this	
   will	
   (exponen6ally)	
   increase	
   the	
   id203	
   of	
   accep6ng	
   the	
   next	
   
i = gt
gt+1
increase
move	
   into	
   a	
   di   erent	
   temperature	
   level.	
   	
   
j     zi/zj as   t     0.

p(xt, kt+1)q(kt    kt+1)
p(xt, kt)q(kt+1    kt)
%
"
standard m-h update

i(1 +   ti(kt+1     {i})), i = 1, ..., k.

&
"#$%

2: update adaptive weights:

in fact: gt

#$

i/gt

    this will (exponentially) increase the id203 of
accepting the next move into a di   erent temperature
level.

g3

g2

k=3
k=2
k=1

g1

13

14
atchade	
   and	
   liu,	
   2004,	
   	
   
famong	
   liang,	
   2005	
   	
   

gkt
gkt+1

adap6ve	
   simulated	
   tempering	
   (ast)	
   
adaptive simulated tempering
adaptive simulated tempering
adaptive simulated tempering
      	
   given	
   kt,	
   sample	
   kt+1	
   from	
   proposal	
   distribu6on:	
   	
   	
   
1: given kt, sample kt+1 from proposal distribution q(kt+1    kt).
1: given kt, sample kt+1 from proposal distribution q(kt+1    kt).
	
   	
   accept	
   with	
   id203:	
   
adaptive simulated tempering
1: given kt, sample kt+1 from proposal distribution q(kt+1    kt).
min!1,
&
min!1,
&
p(xt, kt+1)q(kt    kt+1)
p(xt, kt+1)q(kt    kt+1)
gkt
min!1,
&
p(xt, kt+1)q(kt    kt+1)
  
gkt
  
1: given kt, sample kt+1 from proposal distribution q(kt+1    kt).
p(xt, kt)q(kt+1    kt)
p(xt, kt)q(kt+1    kt)
  
gkt+1
p(xt, kt)q(kt+1    kt)
"#$%
#$
%
"
min!1,
&
"#$%
%
#$
"
gkt+1
p(xt, kt+1)q(kt    kt+1)
"#$%
%
#$
"
gkt
adaptive factor
standard m-h update
adaptive factor
  
standard m-h update
adaptive factor
standard m-h update
p(xt, kt)q(kt+1    kt)
gkt+1
      	
   update	
   adap6ve	
   weights:	
   
"#$%
#$
%
"
2: update adaptive weights:
adaptive factor
standard m-h update
2: update adaptive weights:
2: update adaptive weights:
i = gt
i(1 +   ti(kt+1     {i})), i = 1, ..., k.
gt+1
2: update adaptive weights:
i = gt
i(1 +   ti(kt+1     {i})), i = 1, ..., k.
gt+1
i(1 +   ti(kt+1     {i})), i = 1, ..., k.
i = gt
gt+1
i(1 +   ti(kt+1     {i})), i = 1, ..., k.
i = gt
gt+1
j     zi/zj as   t     0.
in fact: gt
i/gt
      	
   it	
   can	
   be	
   veri   ed:	
   	
   
j     zi/zj as   t     0.
i/gt
j     zi/zj as   t     0.
in fact: gt
j     zi/zj as   t     0.
i/gt
in fact: gt
i/gt
k=3
g3
g3
k=3
g3
k=3
k=2
g2
k=2
g2
g2
k=2
k=1
k=1
g1
k=1

atchade	
   and	
   liu,	
   2004,	
   	
   
famong	
   liang,	
   2005	
   	
   

g2

g3

g1
g1

g1

k=3
k=2
k=1

in fact: gt

14

14

14

fast-     slow	
   ast	
   

      	
   when	
   using	
   ast	
   for	
   learning,	
   it	
   is	
   hard	
   to	
   balance	
   between:	
   	
   

-   	
   explora6on:	
   wai6ng	
   un6l	
   adap6ve	
   st	
   escapes	
   from	
   the	
   local	
   mode.	
   	
   
-   	
   exploita6on:	
   learning	
   model	
   parameters.	
   

fast-     slow	
   ast	
   

      	
   when	
   using	
   ast	
   for	
   learning,	
   it	
   is	
   hard	
   to	
   balance	
   between:	
   	
   

-   	
   explora6on:	
   wai6ng	
   un6l	
   adap6ve	
   st	
   escapes	
   from	
   the	
   local	
   mode.	
   	
   
-   	
   exploita6on:	
   learning	
   model	
   parameters.	
   

      	
   consider	
   two	
   chains,	
   sampling	
   from	
   the	
   same	
   target	
   distribu6on.	
   

coupled ast

k=3

k=2

k=1

fast chain

slow	
   chain	
   evolves	
   according	
   to	
   
the	
   standard	
   gibbs	
   updates.	
   

slow chain

fast	
   chain	
   uses	
   adap6ve	
   st.	
   

   slow    chain evolves according to simple gibbs updates.
   fast    chain uses adaptive st.

      	
   parameters	
   are	
   updated	
   based	
   on	
   the	
   slow	
   chain.	
   the	
   role	
   of	
   the	
   fast	
   
chain	
   is	
   to	
   explore	
   di   erent	
   modes.	
   

parameters are updated based on the slow chain. the
role of the fast chain is to facilitate mixing.

two parameters: learning rate and adaptation. related to fast pcd
of tieleman and hinton, 2009.

coupled ast

fast-     slow	
   ast	
   

k=3

k=2

k=1

fast chain

slow	
   chain	
   evolves	
   according	
   to	
   
the	
   standard	
   gibbs	
   updates.	
   

slow chain

fast	
   chain	
   uses	
   adap6ve	
   st.	
   

   slow    chain evolves according to simple gibbs updates.
   fast    chain uses adaptive st.

parameters are updated based on the slow chain. the
role of the fast chain is to facilitate mixing.

      	
   the	
   algorithm	
   is	
   only	
   twice	
   as	
   expensive	
   compared	
   to	
   the	
   standard	
   
stochas6c	
   approxima6on	
   algorithm.	
   	
   
      	
   parameters	
   are	
   updated	
   aaer	
   every	
   gibbs	
   update,	
   while	
   the	
   fast	
   chain	
   
runs	
   in	
   parallel,	
   adap6vely	
   mixing	
   between	
   di   erent	
   modes	
   of	
   the	
   
energy	
   landscape.	
   
      	
   unlike	
   fast	
   persistent	
   contras6ve	
   divergence	
   (pcd),	
   the	
   fast	
   chain	
   is	
   
likely	
   to	
   visit	
   spurious	
   modes	
   that	
   may	
   reside	
   far	
   away	
   from	
   the	
   data.	
   

two parameters: learning rate and adaptation. related to fast pcd
of tieleman and hinton, 2009.

15

1000	
   latents	
   

500	
   latents	
   

mnist	
   dataset	
   

adaptive st

gibbs	
   
gibbs

adaptive st

adap6ve	
   st	
   

k

 
l

 

e
v
e
l
e
r
u
t
a
r
e
p
m
e
t

20
18
16
14
12
10
8
6
4
2
0
0

200

400

number of gibbs updates

about	
   890,000	
   
parameters	
   
      	
   samples	
   from	
   the	
   two-     hidden-     layer	
   dbm	
   (1000-     500-     784)	
   produced	
   by	
   
the	
   gibbs	
   and	
   adap6ve	
   st	
   with	
   300	
   gibbs	
   steps	
   between	
   consecu6ve	
   
images	
   (by	
   column).	
   

left: samples from two-hidden-layer dbm produced by the gibbs
and adaptive st with 300 gibbs steps between consecutive images
(by column).

right: typical temperature trajectory. adaptive weights push
samples in roughly systematic manner through all 20 temperatures.

norb dataset

norb	
   dataset	
   

gibbs	
   
gibbs

adap6ve	
   st	
   
adaptive st

      	
   samples	
   from	
   two-     hidden-     layer	
   dbm:	
   4000-     4000-     (96x96),	
   produced	
   
by	
   the	
   gibbs	
   and	
   fast-     slow	
   adap6ve	
   st	
   with	
   500	
   gibbs	
   steps	
   between	
   
consecu6ve	
   images	
   (by	
   column).	
   about	
   3	
   million	
   parameters.	
   

samples from two-hidden-layer dbm produced by the gibbs and
adaptive st with 500 gibbs steps between consecutive images (by
column).

18

learning dbms

learning	
   dbms	
   

 

ast	
   

algorithm	
   

the	
   es6mates	
   of	
   the	
   average	
   test	
   log-     probabili6es	
   per	
   image	
   (in	
   nats)	
   
for	
   di   erent	
   learning	
   algorithms.	
   
(cid:239)80
(cid:239)85
(cid:239)90
(cid:239)95
(cid:239)100
(cid:239)105
(cid:239)110
(cid:239)115

mnist	
   
norb	
   
(+/-     	
   0.5)	
   
(+/-     	
   1.1)	
   
mnist norb
gibbs	
   
-     596.92	
   
-     87.23	
   
fast	
   pcd	
   
-     597.12	
   
-     86.72	
   
-596.92
-87.23
sap
-     595.54	
   
tempered	
   
-     85.41	
   
-597.12
-86.72
fpcd
transi6ons	
   
-595.54
trans-sap -85.41
-     591.18	
   
-     84.12	
   
fast-     slow	
   ast	
   
cast
-591.18
-84.12

cast
fpcd
sap
45
5
number of parameter updates ! 4000

datasets

models

gibbs	
   

30

35

40

50

10

15

20

25

 
0

d
o
o
h

i
l
e
k
i
l

(cid:239)
g
o

l
 

 

n
o
d
n
u
o
b
 
r
e
w
o
l

      	
   fast-     slow	
   ast	
   	
   tends	
   to	
   exhibit	
   a	
   more	
   stable	
   behavior	
   during	
   learning.	
   
the estimates of the variational lower bound on the average test
log-probabilities per image for di   erent learning algorithms.

recap	
   

       e   cient	
   learning	
   algorithms	
   for	
   hierarchical	
   genera6ve	
   models.	
   

deep	
   boltzmann	
   machine	
   

text	
   &	
   image	
   retrieval	
   /	
   	
   
object	
   recogni<on	
   

learning	
   to	
   	
   
learn	
   

filling	
   in	
   

object	
   detec<on	
   

mo<on	
   capture	
   

       deep	
   genera6ve	
   models	
   can	
   improve	
   current	
   state-     of-     the	
   art	
   in	
   

many	
   applica6on	
   domains:	
   
         object	
   recogni6on	
   and	
   detec6on,	
   text	
   and	
   image	
   retrieval,	
   handwrijen	
   

character	
   recogni6on,	
   mo6on	
   capture,	
   and	
   others.	
   

summary	
   

compose	
   hierarchical	
   bayesian	
   models	
   with	
   deep	
   networks	
   for	
   
transfer	
   learning	
   /	
   one-     shot	
   learning.	
   
deep	
   networks:	
   learning	
   part-     
based	
   hierarchy:	
   
      	
   mul6ple	
   layers	
   of	
   nonlineari<es.	
   
      	
   distributed	
   representa<ons.	
   
      	
   unsupervised	
   learning	
   of	
   generic	
   features	
   	
   -     -     	
   no	
   
need	
   to	
   rely	
   on	
   human-     craaed	
   input	
   representa6ons.	
   

hierarchical	
   bayes:	
   learning	
   
category	
   hierarchy:	
   
      	
   explicitly	
   learn	
   category	
   hierarchies	
   for	
   
sharing	
   abstract	
   knowledge.	
   
      	
   modular	
   data-     parameter	
   rela<ons.	
   
      	
   higher-     level	
   class	
   sensi<ve	
   features.	
   

thank	
   you	
   

