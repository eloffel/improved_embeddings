machine learning and ai  

via brain simulations  

andrew ng 

stanford university & google 

thanks to: 

stanford: 

    adam coates       quoc le       honglak lee     andrew saxe   andrew maas chris manning jiquan ngiam  richard socher     will zou  

google: 

            kai chen     greg corrado     jeff dean   matthieu devin  rajat monga marc   aurelio     paul tucker      kay le              
                                                                                                                                   ranzato 

andrew ng 

400 100,000 

this talk 

- make learning algorithms much better and easier to use. 
- make revolutionary advances in machine learning and ai.  

the idea of    deep learning.    using brain simulations, hope to:  
 
 
 
vision is not only mine; shared with many researchers:   

e.g., samy bengio, yoshua bengio, tom dean, jeff dean, nando de 

freitas, jeff hawkins, geoff hinton, quoc le, yann lecun, honglak 
lee, tommy poggio, ruslan salakhutdinov, josh tenenbaum, kai 
yu, jason weston,    .  

i believe this is our best shot at progress towards real ai.  

andrew ng 

what do we want computers to do with our data? 

images/video 

 

 

audio 

 

 

text 

label:    motorcycle    
suggest tags 
image search 
    

id103 
music classification 
speaker identification 
    

web search 
anti-spam 
machine translation 
     

andrew ng 

motorcycle 

motorcycle 

id161 is hard!  

motorcycle 

motorcycle 

motorcycle 

motorcycle 

motorcycle 

motorcycle 

motorcycle 

andrew ng 

what do we want computers to do with our data? 

images/video 

 

 

audio 

 

 

text 

label:    motorcycle    
suggest tags 
image search 
    

id103 
speaker identification 
music classification 
    

web search 
anti-spam 
machine translation 
     

machine learning performs well on many of these problems, but is a 
lot of work.  what is it about machine learning that makes it so hard 
to use? 

andrew ng 

machine learning for image classification 

   motorcycle    

this talk: develop ideas using images and audio.  
ideas apply to other problems (e.g., text) too. 

andrew ng 

why is this hard? 

you see this:  

but the camera sees this: 

andrew ng 

machine learning and feature representations 

pixel 1 

input 

pixel 2 

raw image 

motorbikes 
   non   -motorbikes 

 

2

 
l

e
x
p

i

learning 
algorithm 

pixel 1 

andrew ng 

machine learning and feature representations 

pixel 1 

input 

pixel 2 

raw image 

motorbikes 
   non   -motorbikes 

 

2

 
l

e
x
p

i

learning 
algorithm 

pixel 1 

andrew ng 

machine learning and feature representations 

pixel 1 

input 

pixel 2 

raw image 

motorbikes 
   non   -motorbikes 

 

2

 
l

e
x
p

i

learning 
algorithm 

pixel 1 

andrew ng 

what we want 

handlebars 

wheel 

feature 

representation 

learning 
algorithm 

input 

e.g., does it have handlebars?  wheels?  

raw image 

motorbikes 
   non   -motorbikes 

features 

 

2

 
l

e
x
p

i

l

 
s
e
e
h
w

pixel 1 

handlebars 

andrew ng 

computing features in id161 

but    we don   t have a handlebars detector. so, researchers try to hand-design features 
to capture various statistical properties of the image.  

0.1 

0.4 

0.7 

0.6 

0.1 

0.5 

0.1 

0.5 

0.2 

0.4 

0.4 

0.5 

0.6 

0.7 

0.3 

0.4 

find edges 

at four  

orientations 

sum up edge  

strength in 

each quadrant 

0.1 
0.7 
0.4 
0.6 
0.1 
0.4 
0.5 
0.5 
    

final  
feature  
vector 

andrew ng 

feature representations 

feature 

representation 

learning 
algorithm 

input 

andrew ng 

images/video 

audio 

text 

how is computer perception done? 

image 

vision features 

detection 

audio 

audio features 

speaker id 

 text 

text  features 

text classification, 
machine translation, 
information retrieval, 
.... 

andrew ng 

feature representations 

feature 

representation 

learning 
algorithm 

input 

andrew ng 

id161 features 

sift 

spin image 

hog 

rift 

textons 

gloh 

andrew ng 

audio features 

spectrogram 

mfcc 

flux 

zcr 

rolloff 

andrew ng 

nlp features 

id39 

id30 

parser features 

coming up with features is difficult, time-
consuming, requires expert knowledge.   
  
when working applications of learning, we 
spend a lot of time tuning the features.  

anaphora 

part of speech 

ontologies (id138) 

andrew ng 

feature representations 

input 

feature 

representation 

learning 
algorithm 

andrew ng 

the    one learning algorithm    hypothesis 

auditory cortex 

auditory cortex learns to see 
 

[roe et al., 1992] 

andrew ng 

the    one learning algorithm    hypothesis 

somatosensory cortex 

somatosensory cortex learns to see 

 

[metin & frost, 1989] 

andrew ng 

sensor representations in the brain 

seeing with your tongue 

human echolocation (sonar) 

haptic belt: direction sense 

implanting a 3rd eye 

[brainport; welsh & blasch, 1997; nagel et al., 2005; constantine-paton & law, 2009] 
andrew ng 

on two approaches to computer perception 

the adult visual system computes an incredibly complicated function of 

the input.   

we can try to directly implement most of this incredibly complicated 

function (hand-engineer features).  

can we learn this function instead?  

a trained learning algorithm (e.g., neural network, boosting, decision 
tree, id166,   ) is very complex.  but the learning algorithm itself is 
usually very simple.  the complexity of the trained algorithm comes 
from the data, not the algorithm.  

 

andrew ng 

learning input representations 

find a better way to represent images than pixels. 

 

andrew ng 

learning input representations 

find a better way to represent audio.  

 

andrew ng 

id171 problem 

    given a 14x14 image patch x, can represent 

it using 196 real numbers.  

 

 

255 
98 
93 
87 
89 
91 
48 
    

    problem: can we find a learn a better  

feature vector to represent this?  

andrew ng 

self-taught learning (unsupervised id171) 

unlabeled images 

    

testing: 
what is this?   

motorcycles 

not motorcycles 

andrew ng 

self-taught learning (unsupervised id171) 

unlabeled images 

    

testing: 
what is this?   

motorcycles 

not motorcycles 

andrew ng 

first stage of visual processing: v1 

v1 is the first stage of visual processing in the brain. 

neurons in v1 typically modeled as edge detectors:  

neuron #1 of visual cortex 

neuron #2 of visual cortex 

(model) 

(model) 

andrew ng 

id171 via sparse coding 

sparse coding (olshausen & field,1996). originally 

developed to explain early visual processing in  
the brain (edge detection). 

input: images x(1), x(2),    , x(m) (each in rn x n) 

learn: dictionary of bases f1, f2,    , fk (also rn x n), 

so that each input x can be approximately 
decomposed as:   

   

   

 

 

k 

 fj 

x        aj
j=1 
s.t. aj   s are mostly zero (   sparse   )  

andrew ng 

sparse coding illustration 

    natural images 

learned bases (f1 ,    , f64):     edges    

test example 

    0.8 *                   + 0.3 *                     + 0.5 * 

        0.8 *       f

     x  
[a1,    , a64] = [0, 0,    , 0, 0.8, 0,    , 0, 0.3, 0,    , 0, 0.5, 0]  

36         +  0.3 *        f42          

+ 0.5 *       f63 

(feature representation)  

more succinct, higher-level, 

representation. 

andrew ng 

501001502002503003504004505005010015020025030035040045050050100150200250300350400450500501001502002503003504004505005010015020025030035040045050050100150200250300350400450500more examples 

 

 

 

     0.6 *                  + 0.8 *                  + 0.4 * 

                                    f15                                 f

28                                                 

f

37  

represent as: [a15=0.6, a28=0.8, a37 = 0.4]. 

 

     1.3 *                  + 0.9 *                  + 0.3 * 

 

                                   f5                                   f

18                                               

f

29  

represent as: [a5=1.3, a18=0.9, a29 = 0.3]. 

 

    method    invents    edge detection.  

 

    automatically learns to represent an image in terms of the edges that 
 
appear in it.  gives a more succinct, higher-level representation than 
the raw pixels.  

 

    quantitatively similar to primary visual cortex (area v1) in brain.  

andrew ng 

sparse coding applied to audio 

image shows 20 basis functions learned from unlabeled audio.  

[evan smith & mike lewicki, 2006] 

andrew ng 

sparse coding applied to audio 

image shows 20 basis functions learned from unlabeled audio.  

[evan smith & mike lewicki, 2006] 

andrew ng 

sparse coding applied to touch data 

collect touch data using a glove, following distribution of grasps used by animals in the wild. 

grasps used by animals 

[macfarlane & graziano, 2009] 

example learned representations 

      biological data            

 

learning algorithm 

[andrew saxe]  
andrew ng 

sparse autoencoder sample basessparse rbm sample basesica sample basesid116 sample basessparse autoencoder sample basessparse rbm sample basesica sample basesid116 sample bases-1-0.500.510510152025experimental data distributionlog (excitatory/inhibitory area)number of neurons-1-0.500.510510152025model distributionlog (excitatory/inhibitory area)number of bases-1-0.500.5100.020.040.060.080.1log (excitatory/inhibitory area)id203pdf comparisons (p = 0.5872)  learning feature hierarchies 

higher layer 

(combinations of edges;  

  cf. v2) 

a1 

a2 

a3 

   sparse coding    
(edges; cf. v1)  

x1 

x2 

x3 

x4 

input image (pixels) 

[technical details: sparse autoencoder or sparse version of hinton   s dbn.] 

[lee, ranganath & ng, 2007] 
andrew ng 

learning feature hierarchies 

higher layer 

(model v3?) 

higher layer 

(model v2?) 

 
 
 
 
 

a1 

a2 

a3 

model v1 

x1 

x2 

x3 

x4 

input image 

[technical details: sparse autoencoder or sparse version of hinton   s dbn.] 

[lee, ranganath & ng, 2007] 
andrew ng 

hierarchical sparse coding (sparse dbn): trained on face images 

training set: aligned 
images of faces.  

object models 

object parts 
(combination  
of edges) 

edges 

pixels 

[honglak lee] 
andrew ng 

hierarchical sparse coding (sparse dbn) 

features learned from training on different object classes. 

faces 

cars 

elephants 

chairs 

[honglak lee] 
andrew ng 

machine learning 

applications 

andrew ng 

video activity recognition (hollywood 2 benchmark) 

method 
hessian + esurf [williems et al 2008] 
harris3d + hog/hof [laptev et al 2003, 2004] 
cuboids + hog/hof  [dollar et al 2005, laptev 2004] 
hessian + hog/hof [laptev 2004, williems et al 2008] 
dense + hog / hof [laptev 2004] 
cuboids + hog3d [klaser 2008, dollar et al 2005] 

unsupervised id171 (our method) 

accuracy 
38% 
45% 
46% 
46% 
47% 
46% 

52% 

unsupervised id171 significantly improves 

on the previous state-of-the-art.  

[le, zhou & ng, 2011] 
andrew ng 

sparse coding on audio (speech) 

spectrogram 

        0.9 *             + 0.7 *          + 0.2 * 

           x                               f36                         f

42                             

f

63  

andrew ng 

dictionary of bases fi learned for speech 

many bases seem to correspond to phonemes.  

[honglak lee] 
andrew ng 

hierarchical sparse coding (sparse dbn) for audio 

spectrogram 

[honglak lee] 
andrew ng 

hierarchical sparse coding (sparse dbn) for audio 

spectrogram 

[honglak lee] 
andrew ng 

hierarchical sparse coding (sparse dbn) for audio 

[honglak lee] 
andrew ng 

phoneme classification (timit benchmark) 

method 
clarkson and moreno (1999) 
gunawardana et al. (2005) 
sung et al. (2007) 
petrov et al. (2007) 
sha and saul (2006) 
yu et al. (2006) 

unsupervised id171 (our method) 

accuracy 
77.6% 
78.3% 
78.5% 
78.6% 
78.9% 
79.2% 

80.3% 

unsupervised id171 significantly improves 

on the previous state-of-the-art.  

[lee et al., 2009] 
andrew ng 

state-of-the-art 
unsupervised   
id171 

andrew ng 

images 

cifar object classification 

accuracy 

norb object classification 

accuracy 

prior art (ciresan et al., 2011)  

stanford id171 

80.5% 

82.0% 

prior art (scherer et al., 2010) 

stanford id171 

94.4% 

95.0% 

galaxy 
video 
hollywood2 classification 

prior art (laptev et al., 2004) 

stanford id171 

accuracy 

youtube 

48% 

53% 

prior art (liu et al., 2009) 

stanford id171 

kth 

accuracy 

ucf 

prior art (wang et al., 2010) 

stanford id171 

92.1% 

93.9% 

prior art (wang et al., 2010) 

stanford id171 

accuracy 

71.2% 

75.8% 

accuracy 

85.6% 

86.5% 

text/nlp 

paraphrase detection 

accuracy 

sentiment (mr/mpqa data) 

accuracy 

prior art (das & smith, 2009)  

stanford id171 

76.1% 

76.4% 

prior art (nakagawa et al., 2010)  

stanford id171 

77.3% 

77.7% 

multimodal (audio/video) 
avletters lip reading 

prior art (zhao et al., 2009) 

stanford id171 

accuracy 

58.9% 

65.8% 

other unsupervised id171 records:  
pedestrian detection (yann lecun) 
id103 (geoff hinton) 
pascal voc object classification (kai yu) 

andrew ng 

technical challenge: 

scaling up 

andrew ng 

supervised learning 

    choices of learning algorithm: 

    memory based 
    winnow 
    id88 
    na  ve bayes 
    id166 
       .  

    what matters the most?  

 
 
 
 
 
 
y
c
a
r
u
c
c
a

 
 
 
 
 

 

 
 

training set size (millions) 

[banko & brill, 2001] 

   it   s not who has the best algorithm that wins. 
it   s who has the most data.    

andrew ng 

scaling and classification accuracy (cifar-10) 

large numbers of features is critical. the specific learning algorithm is 
important, but ones that can scale to many features also have a big 
advantage.  

[adam coates] 
andrew ng 

attempts to scale up 

significant effort spent on algorithmic tricks to get algorithms to run faster. 

    efficient sparse coding.  [lecun, ng, yu]  

    efficient posterior id136 [bengio, hinton]  

    convolutional networks. [bengio, de freitas, lecun, lee, ng] 

    tiled networks. [hinton, ng] 

    randomized/fast parameter search. [dicarlo, ng]  

    massive data synthesis. [lecun, schmidhuber] 

    massive embedding models [bengio, collobert, hinton, weston] 

    fast decoder algorithms. [lecun, lee, ng, yu]  

    gpu, fpga and asic implementations. [dean, lecun, ng, olukotun]  

 

 

 

andrew ng 

images 

cifar object classification 

accuracy 

norb object classification 

accuracy 

prior art (ciresan et al., 2011)  

stanford id171 

80.5% 

82.0% 

prior art (scherer et al., 2010) 

stanford id171 

94.4% 

95.0% 

galaxy 
video 
hollywood2 classification 

prior art (laptev et al., 2004) 

stanford id171 

accuracy 

youtube 

48% 

53% 

prior art (liu et al., 2009) 

stanford id171 

kth 

accuracy 

ucf 

prior art (wang et al., 2010) 

stanford id171 

92.1% 

93.9% 

prior art (wang et al., 2010) 

stanford id171 

accuracy 

71.2% 

75.8% 

accuracy 

85.6% 

86.5% 

text/nlp 

paraphrase detection 

accuracy 

sentiment (mr/mpqa data) 

accuracy 

prior art (das & smith, 2009)  

stanford id171 

76.1% 

76.4% 

prior art (nakagawa et al., 2010)  

stanford id171 

77.3% 

77.7% 

multimodal (audio/video) 
avletters lip reading 

prior art (zhao et al., 2009) 

stanford id171 

accuracy 

58.9% 

65.8% 

other unsupervised id171 records:  
pedestrian detection (yann lecun) 
id103 (geoff hinton) 
pascal voc object classification (kai yu) 

andrew ng 

scaling up: discovering 

object classes 

[quoc v. le, marc'aurelio ranzato, rajat  monga, 
greg corrado, matthieu devin, kai chen, jeff dean] 
 

andrew ng 

training procedure 

what features can we learn if we train a massive model on a massive 

amount of data.  can we learn a    grandmother cell   ? 

    train on 10 million images (youtube) 
    1000 machines (16,000 cores) for 1 week.  
    1.15 billion parameters 
    test on novel images 

training set (youtube)                           test set (fitw + id163) 

andrew ng 

face neuron 

top stimuli from the test set 

optimal stimulus by numerical optimization 

andrew ng 

random distractors 

faces 

invariance properties 

 

e
s
n
o
p
s
e
r
 
e
r
u
t
a
e
f

 

e
s
n
o
p
s
e
r
 
e
r
u
t
a
e
f

+15 pixels 

horizontal shift 

o 

90 

3d rotation angle 

 

e
s
n
o
p
s
e
r
 
e
r
u
t
a
e
f

 

e
s
n
o
p
s
e
r
 
e
r
u
t
a
e
f

+15 pixels 

vertical shift 

scale factor 

1.6x 

andrew ng 

cat neuron 

top stimuli from the test set 

optimal stimulus by numerical optimization 

andrew ng 

cat face neuron 

random distractors 

cat faces 

visualization 

top stimuli from the test set 

optimal stimulus by numerical optimization 

pedestrian neuron 

random distractors 

pedestrians 

weaknesses & 

criticisms 

andrew ng 

weaknesses & criticisms 

    you   re learning everything.  it   s better to encode prior knowledge about 

structure of images (or audio, or text).  

  a: wasn   t there a similar machine learning vs. linguists debate in nlp ~20 

years ago   .   

    unsupervised id171 cannot currently do x, where x is:  

  go beyond gabor (1 layer) features.  

work on temporal data (video).  
learn hierarchical representations (id152). 
get state-of-the-art in activity recognition.  
get state-of-the-art on image classification. 
get state-of-the-art on id164. 
learn variable-size representations. 

 
    a: many of these were true, but not anymore (were not fundamental 

weaknesses).  there   s still work to be done though!  

    we don   t understand the learned features.  

  a: true. though many vision/audio/etc. features also suffer from this (e.g, 

concatenations/combinations of different features).  

 

 

andrew ng 

conclusion 

andrew ng 

unsupervised id171 summary 

unlabeled images 

car 

motorcycle 

    deep learning and self-taught learning: lets learn rather than 
manually design our features.  
    discover the fundamental computational principles that 
underlie perception?  
    sparse coding and deep versions very successful on vision 
and audio tasks.  other variants for learning recursive 
representations.  
    to get this to work for yourself, see online tutorial:  
   http://deeplearning.stanford.edu/wiki 

 

 
thanks to: 
 
  

stanford 

    adam coates       quoc le       honglak lee     andrew saxe   andrew maas chris manning jiquan ngiam  richard socher     will zou  

google 

            kai chen     greg corrado     jeff dean   matthieu devin  rajat monga marc   aurelio     paul tucker      kay le              
                                                                                                                                   ranzato 

andrew ng 

advanced topics + research philosophy 

andrew ng 

stanford university & google 

andrew ng 

learning recursive 

representations 

andrew ng 

feature representations of words 

imagine taking each word, and computing an n-dimensional feature vector for it.  
[distributional representations, or bengio et al., 2003, collobert & weston, 2008.]    
 
2-d embedding example below, but in practice use ~100-d embeddings.  
 

2 
4 

monday 
tuesday  2.1 
3.3 

x2 

5 

4 

3 

2 

1 

on  8 
5 

britain 

9 
2 

france  9.5 
1.5 

0 
0 
0 
0 
1 
0 
0 
0 

0 
1 
0 
0 
0 
0 
0 
0 

monday      britain 

   0        1      2     3     4      5     6     7     8      9     10 

x1 

                              on    monday,   britain    . 
 
  representation:  

9 
2 

8 
5 

2 
4 

andrew ng 

   generic    hierarchy on text doesn   t make sense 

node has to represent 
sentence fragment    cat 
sat on.     doesn   t make 
sense.  

9 
1 

5 
3 

7 
1 

8 
5 

9 
1 

4 
3 

the            cat           sat 

the            cat            on           the             mat. 

feature representation 
for words 

andrew ng 

what we want (illustration) 

s 

vp 

pp 

this node   s job is   
to represent  
   on the mat.    

np 

np 

9 
1 

5 
3 

7 
1 

8 
5 

9 
1 

4 
3 

the            cat           sat 

the            cat            on           the             mat. 

andrew ng 

what we want (illustration) 

5 
4 

s 

vp 

7 
3 

8 
3 

pp 

this node   s job is   
to represent  
   on the mat.    

5 
2 

np 

3 
3 

np 

9 
1 

5 
3 

7 
1 

8 
5 

9 
1 

4 
3 

the            cat           sat 

the            cat            on           the             mat. 

andrew ng 

what we want (illustration) 

the day after my birthday 

monday 
tuesday 

x2 

5 

4 

3 

2 

the country of my birth 

britain 

france 

1 
   0        1      2     3     4      5     6     7     8      9     10 

x1 

3 
5 

8 
3 

9 
3 

9 
2 

5 
2 

3 
3 

2 
8 

3 
2 

g 8 
5 

2 
4 

9 
2 

3 
2 

9 
2 

g 8 
5 

9 
2 

9 
9 

3 
2 

2 
2 

the      day     after     my    birthday,     

the  country     of       my       birth    

andrew ng 

learning recursive representations 

this node   s job is   
to represent  
   on the mat.    

8 
3 

3 
3 

8 
5 

9 
1 

4 
3 

the            cat            on           the             mat. 

andrew ng 

learning recursive representations 

this node   s job is   
to represent  
   on the mat.    

8 
3 

3 
3 

8 
5 

9 
1 

4 
3 

the            cat            on           the             mat. 

andrew ng 

learning recursive representations 

basic computational unit: neural network 
that inputs two candidate children   s 
representations, and outputs: 
    whether we should merge the two nodes. 
    the semantic representation if the two 
nodes are merged.   

   yes     

8 
3 

neural  
network 

this node   s job is   
to represent  
   on the mat.    

8 
3 

3 
3 

8 
5 

9 
1 

4 
3 

8 
5 

the            cat            on           the             mat. 
3 
3 

andrew ng 

parsing a sentence 

5 
2 

 yes 

 no 

0 
1 

 no 

0 
1 

 no 

0 
0 

3 
3 

 yes 

neural  
network 

neural  
network 

neural  
network 

neural  
network 

neural  
network 

9 
1 

5 
3 

7 
1 

8 
5 

9 
1 

4 
3 

the            cat           sat 

the            cat            on           the             mat. 

andrew ng 

parsing a sentence 

0 
1 

 no 

0 
1 

 no 

neural  
network 

neural  
network 

5 
2 

8 
3 

 yes 

neural  
network 

3 
3 

9 
1 

5 
3 

7 
1 

8 
5 

9 
1 

4 
3 

the            cat           sat 

the            cat            on           the             mat. 

andrew ng 

parsing a sentence 

5 
4 

7 
3 

8 
3 

5 
2 

3 
3 

9 
1 

5 
3 

7 
1 

8 
5 

9 
1 

4 
3 

the            cat           sat 

the            cat            on           the             mat. 

andrew ng 

finding similar sentences 

    each sentence has a feature vector representation.  
    pick a sentence (   center sentence   ) and list nearest neighbor sentences.  
    often either semantically or syntactically similar. (digits all mapped to 2.) 

similarities 

bad news 
 

center 
sentence 
both took 
further hits 
yesterday 

something said 

i had calls all 
night long from 
the states, he 
said 

 

nearest neighbor sentences (most similar feature 
vector) 
1. we 're in for a lot of turbulence ...  
2. bsn currently has 2.2 million common shares 

outstanding  

3. this is panic buying  
4. we have a couple or three tough weeks coming 
1. our intent is to promote the best alternative, he 

says  

2. we have sufficient cash flow to handle that, he 

said 

3. currently, average pay for machinists is 22.22 an 

hour, boeing said 

4. profit from trading for its own account dropped, the 

securities firm said 

gains and good 
news 
 
 
unknown words 
which are cities 

fujisawa gained 
22 to 2,222  
 

columbia , s.c 
 

1. mochida advanced 22 to 2,222  
2. commerzbank gained 2 to 222.2  
3. paris loved her at first sight  
4. profits improved across hess's businesses 
1. greenville , miss  
2. unk , md  

andrew ng 

finding similar sentences 

similarities 

declining to 
comment = not 
disclosing 
 
large changes in 
sales or revenue 

negation of 
different types 

people in bad 
situations 
 

center 
sentence 
hess declined to 
comment  
 

sales grew 
almost 2 % to 
222.2 million 
from 222.2 
million 

there's nothing 
unusual about 
business groups 
pushing for 
more 
government 
spending 
we were lucky 
 

nearest neighbor sentences (most similar feature 
vector) 
1. painewebber declined to comment  
2. phoenix declined to comment  
3. campeau declined to comment  
4. coastal wouldn't disclose the terms 
1. sales surged 22 % to 222.22 billion yen from 222.22 

billion 

2. revenue fell 2 % to 2.22 billion from 2.22 billion 
3. sales rose more than 2 % to 22.2 million from 22.2 

million 

4. volume was 222.2 million shares , more than triple 

recent levels 

1. we don't think at this point anything needs to be said 
2.
it therefore makes no sense for each market to adopt 
different circuit breakers 

3. you can't say the same with black and white  
4.
i don't think anyone left the place unk unk  

it was chaotic 

1.
2. we were wrong 
3. people had died 

andrew ng 

application: paraphrase detection 

    task: decide whether or not two sentences are paraphrases of each 

other.  (msr paraphrase corpus) 

 

method 

baseline 

rus et al., (2008) 

mihalcea et al., (2006) 

islam et al. (2007) 

qiu et al. (2006) 

fernando & stevenson (2008) (id138 based features) 

das et al. (2009) 

f1 

79.9 

80.5 

81.3 

81.3 

81.6 

82.4 

82.7 

wan et al (2006) (many features: pos, parsing, id7, etc.)  83.0 

stanford id171  

83.4 

andrew ng 

parsing sentences and parsing images 

a small crowd 
quietly enters the 
historic church. 
 

each node in the hierarchy has a    feature vector    representation.  

andrew ng 

nearest neighbor examples for image patches 

    each node (e.g., set of merged superpixels) in the hierarchy has a feature vector.  
    select a node (   center patch   ) and list nearest neighbor nodes.   
   

i.e., what image patches/superpixels get mapped to similar features?  

selected patch 

nearest neighbors 

andrew ng 

multi-class segmentation (stanford background dataset) 

method 
pixel crf (gould et al., iccv 2009) 
classifier on superpixel features 
region-based energy (gould et al., iccv 2009) 
local labelling (tighe & lazebnik, eccv 2010) 
superpixel mrf (tighe & lazebnik, eccv 2010) 
simultaneous mrf (tighe & lazebnik, eccv 2010) 

stanford  id171 (our method)  

accuracy 

74.3 
75.9 
76.4 
76.9 
77.5 
77.5 

78.1 

andrew ng 

multi-class segmentation msrc dataset: 21 classes 

methods 
textonboost (shotton et al., eccv 2006) 
framework over mean-shift patches (yang et al., cvpr 
2007) 
pixel crf (gould et al., iccv 2009) 
region-based energy (gould et al., ijcv 2008) 

stanford id171 (out method)  

accuracy 

72.2 
75.1 

75.3 
76.5 

76.7 

andrew ng 

analysis of feature 
learning algorithms 

    andrew coates   honglak lee 
            
 

andrew ng 

supervised learning 

    choices of learning algorithm: 

    memory based 
    winnow 
    id88 
    na  ve bayes 
    id166 
       .  

    what matters the most?  

 
 
 
 
 
 
y
c
a
r
u
c
c
a

 
 
 
 
 

 

 
 

training set size 

[banko & brill, 2001] 

   it   s not who has the best algorithm that wins. 
it   s who has the most data.    

andrew ng 

unsupervised id171 

    many choices in id171 algorithms; 
    sparse coding, rbm, autoencoder, etc.  
    pre-processing steps (whitening) 
    number of features learned  
    various hyperparameters.  

    what matters the most?  

 

 
 

andrew ng 

unsupervised id171 

most algorithms learn gabor-like edge detectors.  

sparse auto-encoder 

andrew ng 

unsupervised id171 

weights learned with and without whitening.  

with whitening 

without whitening 

with whitening 

without whitening 

 

 

 

sparse auto-encoder 

sparse rbm 

with whitening 

without whitening 

with whitening 

without whitening 

id116 

gaussian mixture model 

andrew ng 

scaling and classification accuracy (cifar-10) 

andrew ng 

results on cifar-10 and norb (old result) 

    id116 achieves state-of-the-art 

    scalable, fast and almost parameter-free, id116 does 

surprisingly well.   

cifar-10 test accuracy 

norb test accuracy (error) 

raw pixels 
rbm with back-propagation 
3-way factored rbm (3 layers) 
mean-covariance rbm (3 layers) 
improved local coordinate coding 
convolutional rbm 

sparse auto-encoder 
sparse rbm 
id116 (hard) 
id116 (triangle, 1600 features) 
id116 (triangle, 4000 features) 

37.3% 
64.8% 
65.3% 
71.0% 
74.5% 
78.9% 

73.4% 
72.4% 
68.6% 
77.9% 
79.6% 

convolutional neural networks 

93.4%  (6.6%) 

deep id82s 

92.8%  (7.2%) 

id50 

jarrett et al., 2009 

sparse auto-encoder 

sparse rbm 

id116 (hard) 

id116 (triangle) 

95.0%  (5.0%) 

94.4%  (5.6%) 

96.9%  (3.1%) 

96.2%  (3.8%) 

96.9%  (3.1%) 

97.0%  (3.0%) 

andrew ng 

tiled convolution 
neural networks 

        quoc le        jiquan ngiam 
           

andrew ng 

learning invariances 

    we want to learn invariant features.  

    convolutional networks uses weight tying to: 

    reduce number of weights that need to be learned.   

    allows scaling to larger images/models. 

    hard code translation invariance.  makes it harder to 

learn more complex types of invariances.   

    goal: preserve computational scaling advantage of 

convolutional nets, but learn more complex invariances.  

 

andrew ng 

fully connected topographic ica 

pooling units 

(sqrt) 

simple units 

(square) 

input 

doesn   t scale to large images. 

andrew ng 

fully connected topographic ica 

pooling units 

(sqrt) 

simple units 

(square) 

input 

orthogonalize 

doesn   t scale to large images. 

andrew ng 

local receptive fields 

pooling units 

(sqrt) 

simple units 

(square) 

input 

andrew ng 

convolution neural networks (weight tying) 

pooling units 

(sqrt) 

simple units 

(square) 

input 

andrew ng 

tiled networks (partial weight tying) 

pooling units 

(sqrt) 

tile size (k) = 2 

simple units 

(square) 

input 

local pooling can capture complex invariances (not just translation); 
but total number of parameters is small.  

andrew ng 

tiled networks (partial weight tying) 

pooling units 

(sqrt) 

tile size (k) = 2 

simple units 

(square) 

input 

andrew ng 

tiled networks (partial weight tying) 

pooling units 

(sqrt) 

tile size (k) = 2 

simple units 

(square) 

input 

number  

of maps (l)  

= 3 

andrew ng 

tiled networks (partial weight tying) 

pooling units 

(sqrt) 

tile size (k) = 2 

simple units 

(square) 

input 

number  

of maps (l)  

= 3 

local 
orthogonalization 

andrew ng 

norb and cifar-10 results 

algorithms 

norb accuracy 

deep tiled id98s [this work]  
id98s [huang & lecun, 2006] 
3d id50 [nair & hinton, 2009] 
deep id82s [salakhutdinov & hinton, 2009] 
tica [hyvarinen et al., 2001] 
id166s 

96.1% 
94.1% 
93.5% 
92.8% 
89.6% 
88.4% 

algorithms 

cifar-10 accuracy 

improved lcc [yu et al., 2010] 

deep tiled id98s [this work] 
lcc [yu et al., 2010] 
mcrbms [ranzato & hinton, 2010] 
best of all rbms [krizhevsky, 2009] 
tica [hyvarinen et al., 2001] 

74.5% 

73.1% 
72.3% 
71.0% 
64.8% 
56.1% 

andrew ng 

summary/big ideas 

andrew ng 

summary/big ideas 

    large scale brain simulations as revisiting of the big    ai 

dream.     

       deep learning    has had two big ideas: 

    learning multiple layers of representation 
    learning features from unlabeled data 

    has worked well so far in two regimes (confusing to 

outsiders):  
    lots of labeled data.    train the heck out of the network.     
    unsupervised id171/self-taught learning 

    scalability is important.  
    detailed tutorial: http://deeplearning.stanford.edu/wiki  

 

andrew ng 

end end 

end 

andrew ng 

