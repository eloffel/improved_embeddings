ipam summer school 2012 

tutorial on:  

deep learning 

geoffrey hinton 

canadian institute for advanced research 

& 

department of computer science 

university of toronto 

overview of the tutorial 

 
       a brief history of deep learning. 
       how to learn multi-layer generative models of unlabelled 
data by learning one layer of features at a time. 
       what is really going on when we stack rbms to form a 

deep belief net. 

       how to use generative models to make discriminative 

training methods work much better for classification and 
regression. 

       how to modify rbms to deal with real-valued input. 

introduction to deep learning 

part 1 

 

& 

deep belief nets 

a brief history of deep learning 

       the id26 algorithm for learning 

multiple layers of non-linear features was 
invented several times in the 1970   s and 1980   s 
(werbos, amari?, parker, lecun, rumelhart et. al.) 

       backprop clearly had great promise, but by the 
1990   s people in machine learning had largely 
given up on it because: 
       it did not seem to be able to make good use 
of multiple hidden layers (except in    time-
delay    and convolutional nets). 
      it did not work well in recurrent networks. 

how to learn many layers of features (~1985) 

back-propagate                
error signal to 
get derivatives 
for learning 

compare outputs with 
correct answer to get 
error signal 

outputs 

hidden 
layers 

input vector 

what is wrong with back-propagation? 

       it requires labeled training data. 

      almost all data is unlabeled.

  

       the learning time does not scale well 

      it is very slow in networks with multiple 
hidden layers. why? 

       it can get stuck in poor local optima. 

      these are often quite good, but for deep 
nets they are far from optimal. 

some major issues in deep learning 

       deep vs shallow 

      are deep nets really needed?   
      why? 

       supervised vs unsupervised 

      is almost all learning unsupervised? 
      is unsupervised learning more efficient? 
      was unsupervised learning just a quick fix 
because we had weak optimization methods 
and/or small datasets? 

 

more major issues in deep learning 

 

       priors and prejudice 

       how much prior knowledge should we try to wire into the 

weights or the activities of the networks? 

       smugglng knowledge into the networks without 

prejudicing the solution by manipulating the training set. 

       how do we map a task onto a neural network? 

       attention and recursion. 
       intelligent decomposition vs brute force scanning. 

overcoming the limitations of  back-propagation 

by using unsupervised learning 

 
       keep the efficiency and simplicity of using a 

gradient method for adjusting the weights, but use 
it for modeling the structure of the sensory input. 
      adjust the weights to maximize the id203 
that a generative model would have produced 
the sensory input.  
      learn p(image)  not  p(label | image) 

       if you want to do id161, first learn 
computer graphics 

       what kind of generative model should we learn? 

 belief nets 

       a belief net is a directed 

acyclic graph composed of 
stochastic variables. 

       we get to observe some of 
the variables and we would 
like to solve two problems: 

       the id136 problem: infer 
the states of the unobserved 
variables. 

       the learning problem: adjust 

the interactions between 
variables to make the 
network more likely to 
generate the observed data. 

stochastic 
hidden        
cause 

visible  
effect 

we will use nets composed of 
layers of stochastic binary variables 
with weighted connections.  later, 
we will generalize to other types of 
variable. 

stochastic binary units 

(bernoulli variables) 

       these have a state of 1 

or 0. 

       the id203 of 

turning on is determined 
by the weighted input 
from other units (plus a 
bias) 

1 

the image cannot be displayed. your computer may not have enough memory to open the image, or the image may have been 
corrupted. restart your computer, and then open the    le again. if the red x still appears, you may have to delete the image and then 
insert it again.

1=isp
(
)

0 

1

0 
ws
j

ji

b
i

   +
j

sp
(
i

)
1

=

=

1

+

exp(

b
         
i
j

ws
j

ji

)

 learning deep belief nets 

      

      

it is easy to generate an 
unbiased example at the 
leaf nodes, so we can see 
what kinds of data the 
network believes in.  
it is hard to infer the 
posterior distribution over 
all  possible configurations 
of hidden causes. 
it is hard to even get  a 
sample from the posterior. 
       so how can we learn deep 

      

belief nets that have 
millions of parameters? 

stochastic 
hidden        
cause 

visible  
effect 

the learning rule for sigmoid belief nets 

       learning is easy if we can 

get an unbiased sample 
from the posterior 
distribution over hidden 
states given the observed 
data. 

       for each unit, maximize 
the log id203 that its 
binary state in the sample 
from the posterior would be 
generated by the sampled 
binary states of its parents.  

js

j 
jiw

is

i 

p
i

   

sp
(
i

)
1

==

1

+

exp(

ws
j

)

ji

1
      
j
p
)
   
i

w
  

ji

=

  

s

(

s
i

j

learning 
rate 

explaining away (judea pearl) 

       even if two hidden causes are independent, they can 

become dependent when we observe an effect that they can 
both influence.  
       if we learn that there was an earthquake it reduces the 
id203 that the house jumped because of a truck. 

-10 

truck hits house 

-10 

earthquake 

20 

-20 

house jumps 

20 

posterior 
  
p(1,1)=.0001 
p(1,0)=.4999 
p(0,1)=.4999 
p(0,0)=.0001 

why it is usually very hard to learn     
sigmoid belief nets one layer at a time 

       to learn w, we need the posterior 
distribution in the first hidden layer. 
       problem 1: the posterior is typically 
complicated because of    explaining 
away   . 

       problem 2: the posterior depends 
on the prior as well as the likelihood.  
       so to learn w, we need to know 
the weights in higher layers, even 
if we are only approximating the 
posterior. all the weights interact. 

       problem 3: we need to integrate 
over all possible configurations of 
the higher variables to get the prior 
for first hidden layer. its hopeless! 

 

hidden variables 

hidden variables 

prior 
hidden variables 

  likelihood 

w 

          data 

some methods of learning  

deep belief nets 

       monte carlo methods can be used to sample 

from the posterior. 
      but its painfully slow for large, deep models. 

       in the 1990   s people developed variational 

methods for learning deep belief nets 
      these only get approximate samples from the 
posterior.  
      nevetheless, the learning is still guaranteed to 
improve a variational  bound on the log 
id203 of generating the observed data. 

a breakthrough that makes deep 

learning efficient 

       to learn deep nets efficiently, we need to learn one layer 

of features at a time. this does not work well if we 
assume that the latent variables are independent in the 
prior : 
       the latent variables are not independent in the 

posterior  so id136 is hard for non-linear models. 
        the learning tries to find independent causes using 

one hidden layer which is not usually possible. 

       we need a way of learning one layer at a time that takes 

into account  the fact that we will be learning more 
hidden layers later. 
       we solve this problem by using an undirected model. 

two types of generative neural network 

       if we connect binary stochastic neurons in a 

directed acyclic graph we get a sigmoid belief 
net (radford neal 1992). 

       if we connect binary stochastic neurons using 

symmetric connections we get a boltzmann 
machine (hinton & sejnowski, 1983). 
      if we restrict the connectivity in a special way, 
it is easy to learn a id82. 

restricted id82s 
(smolensky ,1986, called them    harmoniums   ) 

       we restrict the connectivity to make 

learning easier. 
       only one layer of hidden units. 
       we will deal with more layers later 

      

       no connections between hidden units. 
in an rbm, the hidden units are 
conditionally independent given the 
visible states.   
       so we can quickly get an unbiased 

sample from the posterior distribution 
when given a data-vector. 

       this is a big advantage over directed 

belief nets 

hidden 

j 

i 
visible 

the energy of a joint configuration 

(ignoring terms to do with biases) 

binary state of 
visible unit i 

binary state of 
hidden unit j 

v,he
(
)

      =
ji
,

i whv
j
ij

energy with configuration 
v on the visible units and 
h on the hidden units 

weight between 
units i and j 

   

hve
),(
   
w
   
ij

=

hv
i

j

weights       energies       probabilities 
 
       each possible joint configuration of the visible 

and hidden units has an energy 
       the energy is determined by the weights and 
biases (as in a hopfield net). 

       the energy of a joint configuration of the visible 

and hidden units determines its id203: 

hvp
,(

)

e      

hve
),(

       the id203 of a configuration over the visible 
units is found by summing the probabilities of all 
the joint configurations that contain it.  

using energies to define probabilities 
hve
),(

       the id203 of a joint 

configuration over both visible 
and hidden units depends on 
the energy of that joint 
configuration compared with 
the energy of all other joint 
configurations. 

e
   
       
e

hv
,'

'

hvp
),(

=

partition 
function 

hve
,'(

)'

       the id203 of a 

configuration of the visible 
units is the sum of the 
probabilities of all the joint 
configurations that contain it. 

vp
)(

=

hve
),(

   

hve
,'(

)'

   

e

h

   
   
e

hv
,'

'

a picture of the maximum likelihood learning 

algorithm for an rbm 

j 

j 

j 

j 

<

ihv

j

0>

i 

i 

i 

ihv
<

j

   >

i 

a fantasy 

t = 0                 t = 1                  t = 2                               t = infinity 
start with a training vector on the visible units. 
then alternate between updating all the hidden units in 
parallel and updating all the visible units in parallel. 

   

vp
log
)(
w
   
ij

<=

hv
i

j

0

<   >

hv
i

j

   >

a quick way to learn an rbm 

j 

j 

<

ihv

j

0>

ihv
<

j

1>

i 

i 

t = 0                 t = 1    
data 

reconstruction 

w
  
ij

=

(

  

hv
<
i

j

start with a training vector on the 
visible units. 
update all the hidden units in 
parallel 
update the all the visible units in 
parallel to get a    reconstruction   . 
update the hidden units again.  

0

<   >

hv
i

1
>

)

j

this is not following the gradient of the log likelihood. but it 
works well. it is approximately following the gradient of another 
objective function (carreira-perpinan & hinton, 2005). 

three ways to combine id203 density 
models (an underlying theme of the tutorial) 

       mixture:  take a weighted average of the distributions. 

       it can never be sharper than the individual distributions. 

it   s a very weak way to combine models. 

       product: multiply the distributions at each point and then 
renormalize (this is how an rbm combines the distributions defined 
by each hidden unit) 
       exponentially more powerful than a mixture. the 
id172 makes maximum likelihood learning 
difficult, but approximations allow us to learn anyway. 

       composition: use the values of the latent variables of one 
model as the data for the next model. 
       works well for learning multiple layers of representation, 

but only if the individual models are undirected. 

training a deep network 
(the main reason rbm   s are interesting) 

       first train a layer of features that receive input directly 

from the pixels. 

       then treat the activations of the trained features as if 

      

they were pixels and learn features of features in a 
second hidden layer. 
it can be proved that each time we add another layer of 
features we improve a variational lower bound on the log 
id203 of the training data. 
       the proof is slightly complicated.  
       but it is based on a neat equivalence between an 
rbm and a deep directed model (described later) 

the generative model after learning 3 layers 

       to generate data:  

1.    get an equilibrium sample 
from the top-level rbm by 
performing alternating gibbs 
sampling for a long time. 

2.    perform a top-down pass to 

get states for all the other 
layers. 

 
     so the lower level bottom-up 
connections  are not part of 
the generative model. they 
are just used for id136. 

        h3 

3w

         h2 

          h1 

      data 

2w

1w

why does greedy learning work?        
an aside: averaging factorial distributions         
       if you average some factorial distributions, you 

do not get a factorial distribution. 
      in an rbm, the posterior over the hidden units 
is factorial for each visible vector. 
      but the aggregated posterior over all training 
cases is not factorial (even if the data was 
generated by the rbm itself). 

why does greedy learning work? 

the weights, w,  in the bottom level rbm define 
p(v|h) and they also, indirectly, define p(h). 
so we can express the rbm model as 
vp
)(

hvphp
)(

|(

)

   =
h

if we leave p(v|h) alone and improve p(h), we will 
improve p(v).  
to improve p(h), we need it to be a better model of 
the aggregated posterior distribution over hidden 
vectors produced by applying w to the data. 

another view of why layer-by-layer    
learning works (hinton, osindero & teh 2006) 

       there is an unexpected equivalence between 

rbm   s and directed networks with many layers 
that all use the same weights. 
      this equivalence also gives insight into why 
contrastive divergence learning works. 

an infinite sigmoid belief net 
that is equivalent to an rbm 
       the distribution generated by this 
infinite directed net with replicated 
weights is the equilibrium distribution 
for a compatible pair of conditional 
distributions: p(v|h) and p(h|v) that 
are both defined by w 
       a top-down pass of the directed 
net is exactly equivalent to letting 
a restricted id82 
settle to equilibrium. 

       so this infinite directed net  

defines the same distribution as 
an rbm. 

etc. 

tw

         h2 

w

    v2 

tw

         h1 

w

tw

w

    v1 

         h0 

    v0 

id136 in a directed net 

with replicated weights 

       the variables in h0 are conditionally 

independent given v0. 
       id136 is trivial. we just 
multiply v0 by w transpose. 

       the model above h0 implements 

a complementary prior. 

       multiplying v0 by w transpose 

gives the product of the likelihood 
term and the prior term. 

id136 in the directed net is 
exactly equivalent to letting a 
restricted id82 settle 
to equilibrium starting at the data. 

      

etc. 

tw

         h2 

w

    v2 

tw

         h1 

w
      v1 

+ 

+ 
tw
            h0 
w
+ 

+ 

    v0 

       the learning rule for a sigmoid belief 

net is: 

w
     
ij

s

(

s
i

j

   

)  
s
i

       with replicated weights this becomes: 

s

0
j

(

0
s
i

   

1
s
i
1
s
i

)
+
0
s
(
j

   

1
s
j
1
s
j

)
(

+
1
s
i

2
s
i

)

   

...
+
ss
      
i
j

etc. 

tw

js
2

is
2

js
1

is
1

js
0

is
0

w

tw

w

tw

w

        h2 
tw

   v2 

w
        h1 
tw

   v1 

w

        h0 
tw

   v0 

learning a deep directed 

network 

etc. 

tw

       first learn with all the weights tied 

       this is exactly equivalent to 

learning an rbm 

       contrastive divergence learning 
is equivalent to ignoring the small 
derivatives contributed by the tied 
weights between deeper layers. 

         h0 

w

    v0 

         h2 

w

    v2 

tw

         h1 

w

tw

w

    v1 

         h0 

    v0 

       then freeze the first layer of weights 

in both directions and learn the 
remaining weights (still tied 
together). 
       this is equivalent to learning 

another rbm, using the 
aggregated posterior distribution 
of h0 as the data. 

    v1 

w

         h0 

etc. 

tw

         h2 

w

    v2 

tw

         h1 

    v1 

w

tw

         h0 

t

frozenw

frozenw

    v0 

how many layers should we use and how 

wide should they be?  

       there is no simple answer.  

       extensive experiments by yoshua bengio   s group 

(described later) suggest that several hidden layers is 
better than one.  

       results are fairly robust against changes in the size of a 

layer, but the top layer should be big. 

       deep belief nets give their creator a lot of freedom.  

       the best way to use that freedom depends on the task. 
       with enough narrow layers we can model any distribution 

over binary vectors (sutskever & hinton, 2007) 

what happens when the weights in higher layers 
become different from the weights in the first layer? 

       the higher layers no longer implement a complementary 

prior. 
       so performing id136 using the frozen weights in 
the first layer is no longer correct.  but its still pretty 
good. 

       using this incorrect id136 procedure gives a 

variational  lower bound on the log id203 of the 
data.  

       the higher layers learn a prior that is closer to the 
aggregated posterior distribution of the first hidden layer. 
       this improves the network   s model of the data. 

       hinton, osindero and teh (2006) prove that this 
improvement is always bigger than the loss in the variational 
bound caused by using less accurate id136. 

fine-tuning with a contrastive version of the 

   wake-sleep    algorithm 

    after learning many layers of features, we can fine-tune 

the features to improve generation. 

1.  do a stochastic bottom-up pass 

       adjust the top-down weights to be good at 

reconstructing the feature activities in the layer below. 

2.    do a few iterations of sampling in the top level rbm 

-- adjust the weights in the top-level rbm. 

3.    do a stochastic top-down pass 

       adjust the bottom-up weights to be good at 

reconstructing the feature activities in the layer above. 

removing structured noise by using 

top-down effects 

2000 units 

10 labels 

500 units  

500 units  

28 x 28 
pixel     
image  

       after an initial bottom up 
pass, gradually turn down 
the bottom up weights and 
turn up the top down 
weights. 

       look at what the first hidden 

layer would like to 
reconstruct. 

       it removes noise! 
       does it improve recognition 

of noisy images? 

removing structured noise by using 

top-down effects 

noisy 
image 

part 2: 

 

 using id26 for fine-tuning 

a generative model to be better at 

discrimination 

fine-tuning for discrimination 

       first learn one layer at a time greedily. 
       then treat this as    pre-training    that finds a 
good initial set of weights which can be fine-
tuned by  a local search procedure. 
      contrastive wake-sleep is one way of fine-
tuning the model to be better at generation. 
       id26 can be used to fine-tune the 

model for better discrimination. 
      this overcomes many of the limitations of 
standard id26. 

why id26 works better with 
greedy pre-training: the optimization view 

       greedily learning one layer at a time scales well 

to really big networks, especially if we have 
locality in each layer. 

       we do not start id26 until we already 
have sensible feature detectors that should 
already be very helpful for the discrimination task. 
      so the initial gradients are sensible and 
backprop only needs to perform a local search 
from a sensible starting point. 

why id26 works better with 
greedy pre-training: the overfitting view 

       most of the information in the final weights comes from 

information than the labels. 

modeling the distribution of input vectors.  
       the input vectors  generally contain a lot more 
       the precious information in the labels is only used for 
       the fine-tuning only modifies the features slightly to get 

the final fine-tuning.  

the category boundaries right. it does not need to 
discover features. 

       this type of id26 works well even if most of 
the training data is unlabeled.  
       the unlabeled data is still very useful for discovering 

good features. 

first, model the distribution of digit images 

the top two layers form a restricted 
id82 whose free energy 
landscape should model the low 
dimensional manifolds of the digits. 

2000 units 

the network learns a density model for 
unlabeled digit images. when we generate 
from the model we get things that look like 
real digits of all classes.  
but do the hidden features really help with 
digit discrimination?  
add 10 softmaxed units to the top and do 
id26. 

500 units  

500 units  

28 x 28 
pixel     
image  

results on permutation-invariant mnist task 

       very carefully trained backprop net with      1.6% 

one or two hidden layers (platt; hinton) 

       id166 (decoste & schoelkopf, 2002)                       1.4% 

       generative model of joint density of             1.25% 

images and labels (+ generative fine-tuning) 

       generative model of unlabelled digits          1.15% 

followed by gentle id26                 
(hinton & salakhutdinov, science 2006) 

unsupervised    pre-training    also helps for 

models that have more data and better priors 
       ranzato et. al. (nips 2006) used an additional 

600,000 distorted digits. 

       they also used convolutional multilayer neural 

networks that have some built-in, local 
translational invariance. 

back-propagation alone:                  0.49%  
 
unsupervised layer-by-layer 
pre-training followed by backprop:   0.39% (record) 

phone recognition with a dnn pre-trained as a 
deep belief net (mohamed, dahl & hinton 2009) 

183 labels 

not pre-trained 

128 units  

2000 binary hidden units  

2000 binary hidden units  

2000 binary hidden units  

2000 binary hidden units  

 

        after the standard 

post-processing using 
a bi-phone model this 
gets 23.0% phone 
error rate. 

       the best previous 

result on timit was 
24.4% and this 
required averaging 
several models. 

11 frames of 
39 mfcc   s 

we can do much better now 
using the spectrogram! 

learning dynamics of deep nets 

 the next 4 slides describe work by yoshua bengio   s group 

before fine-tuning 

after fine-tuning 

effect of unsupervised pre-training 

erhan et. al.    aistats   2009  

 

51 

effect of depth 

without pre-training 

w/o pre-training 

with pre-training 

52 

learning trajectories in function space  

(a 2-d visualization produced with id167) 

erhan et. al.    aistats   2009  

       each point is a 

model in function 
space 

       color = epoch 
       top: trajectories      
without pre-training. 
each trajectory 
converges to a 
different local min. 

       bottom: trajectories 

with pre-training.  

       no overlap! 

why unsupervised pre-training makes sense 

stuff 

image 

label 

if image-label pairs were 
generated this way, it 
would make sense to try 
to go straight from 
images to labels.   
for example,  do the 
pixels have even parity? 

stuff 

low 
bandwidth 

label 

high 
bandwidth 

image 

if image-label pairs are 
generated this way, it 
makes sense to first learn 
to recover the stuff that 
caused the image by 
inverting the high 
bandwidth pathway. 
 

modeling real-valued data 

       for images of digits it is possible to represent 

intermediate intensities as if they were probabilities by 
using    mean-field    logistic units. 
       we can treat intermediate values as the id203 

that the pixel is inked. 

       this will not work for real images. 

       in a real image, the intensity of a pixel is almost 

always, almost exactly the average of the neighboring 
pixels. 

       mean-field logistic units cannot represent precise 

intermediate values. 

replacing binary variables by  

integer-valued variables 

     (teh and hinton, 2001) 

       one way to model an integer-valued variable is 

to make n identical copies of a binary unit.  

       all copies have the same id203,                               

of being    on    :  p = logistic(x) 
      the total number of    on    copies is like the 
firing rate of a neuron. 
      it has a  binomial distribution with mean n p 
and variance n p(1-p) 

a better way to implement integer values 

       make many copies of a binary unit.  
       all copies have the same weights and the same 

adaptive bias, b, but they have different fixed offsets to 
the bias: 
b
b
,5.0

,5.3

,5.1

,5.2

....

b

b

   

   

   

   

 
 

   x

a fast approximation 

logistic
(

x

5.0

   

n

)

+

   

log(
1

+

x

e

)

n

   =

   

1
=

n

 
       contrastive divergence learning works well for the sum of 

      

binary units with offset biases. 
it also works for rectified linear units. these are much faster  
to compute than the sum of many logistic units with different 
biases. 
output = max(0,  x + noise(x))    many possible noise models 
 

how to train a bipartite network of rectified 

linear units 

       just use contrastive divergence to lower the energy of 
data and raise the energy of nearby configurations that 
the model prefers to the data. 

j 

j 

<

ihv

>

j

recon

i 

reconstruction 

ihv
<
 

data>

j

i 

data 

 

start with a training vector on the 
visible units. 
update all hidden units in parallel 
with sampling noise 
update the visible units in parallel 
to get a    reconstruction   . 
update the hidden units again  

w
  
ij

=

(

  

hv
<
i

>

j

data

<   

hv
i

>

j

recon

)

   3d object recognition: the norb dataset 

   stereo-pairs of grayscale images of toy objects. 

animals 

humans 

planes 

trucks 

cars 

normalized-
uniform 
version of 
norb 

- 6 lighting conditions, 162 viewpoints 
-   five object instances per class in the training set 
-    a different set of five instances per class in the test set 
- 24,300 training cases, 24,300 test cases 

simplifying the data 

       each training case is a stereo-pair of 96x96 images. 

       the object is centered. 
       the edges of the image are mainly blank. 
       the background is uniform and bright. 

       to make learning faster i  simplified the data: 

       throw away one image. 
       only use the middle 64x64 pixels of the other 

image. 

       downsample to 32x32 by averaging 4 pixels. 

simplifying the data even more so that it can 

be modeled by rectified linear units 
       the intensity histogram for each 32x32 image has a 

sharp peak for the bright background. 

       find this peak and call it zero. 
       call all intensities brighter than the background zero. 
       measure intensities downwards from the background 

intensity. 

 

0 

test set error rates on norb after greedy 
learning of one or two hidden layers using 

rectified linear units  

full norb (2 images of 96x96) 
       id28 on the raw pixels                20.5% 
       gaussian id166 (trained by leon bottou)           11.6% 
       convolutional neural net  (le cun   s group)        6.0% 
 (convolutional nets have knowledge of translations built in)                                            

reduced norb (1 image 32x32) 
       id28 on the raw pixels                 30.2% 
       id28 on first relu hidden layer     14.9%  
       id28 on second relu hid layer     10.2% 
see nair & hinton icml 2010 for better results 

the 
receptive 
fields of 
some 
rectified 
linear 
hidden 
units. 

a standard type of real-valued visible unit 

       we can model pixels as 

gaussian variables. 
alternating gibbs 
sampling is still easy, 
though learning needs to 
be much slower. 

 

     
e

 

hv
,e
(

)

=

2

)

v
b
(
   
i
i
2
2
  
i

   

vis

  

i

parabolic 
containment 
function 
   

hb
j

j

j

  

hid

   

b
v
   i
i
energy-gradient 
produced by the total 
input to a visible unit  

   

iv
  
i

   

ji
,

wh
j
ij

welling et. al. (2005) show how to extend rbm   s to the 
exponential family. see also bengio et. al. (2007) 

gaussian-binary rbm   s 

       lots of people have 
failed to get these to 
work and its extremely 
hard to learn tight 
variances for the 
visible units. 

       it took a long time to 
figure out why it is so 
hard to learn the visible 
variances. 

w
ij
  
i

  
i

w
ij

when sigma is much 
less than 1, the bottom-
up effects are too big 
and the top-down effects 
are too small. 

the solution: 

 

       use as many hidden units as it takes to provide 
big enough top-down effects using small weights 

       relu   s do this automatically. 

=

x
)

x
)(

ra

ar
(

      if a relu has a bias of zero, it exhibits scale 
equivariance:                                              
      this is a very nice property to have for 
images. 
      it is like the equivariance to translation 
exhibited by convolutional nets. 
r
(

shift

shift

r
((

x
))

x
))

(

=

 

an early example of gaussian-relu rbms 
       gaussian-relu rbm   s can be made to learn very 
nice convolutional filters on 32x32 color images 
(alex krizhevsky) 
       with an extra hidden layer of binary units and a few 
tricks, these gave record-breaking  discrimination on 
cifar-10. 

 

before fine-tuning 

after fine-tuning 

why are rectified linear units so 

powerful? 

       the non-linearity allows later layers to ignore 

irrelevant variations. 

       the linear part of a hidden relu unit can model 

covariance between pixel intensities much more 
efficiently than the binary hidden units in a binary 
gaussian rbm.  

       an rbm with gaussian visible units and relu 

hidden units is a competitor to a mixture of factor 
analysers as a density model. 
      but  mfa usually wins. 

