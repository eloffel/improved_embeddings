an algebraic perspective on deep learning

jason morton

penn state

july 19-20, 2012

ipam

supported by darpa fa8650-11-1-7145.

jason morton (penn state)

algebraic deep learning

7/19/2012

1 / 103

motivating questions

characterize representational power and learning performance.

(cid:73) what can dl models model? how well?
(cid:73) can we bound approximation errors, prove convergence, etc.?

seek a priori design insights

(cid:73) what model architecture for what data?
(cid:73) can we predict performance?
(cid:73) what tradeo   s should we make?
understand representations obtained

(cid:73) identi   ability
(cid:73) transferrability

i   ll describe an algebraic approach to these kinds of problems in
three parts

jason morton (penn state)

algebraic deep learning

7/19/2012

2 / 103

outline

1 algebraic geometry of tensor networks

tensors
tensor networks
algebraic geometry

2 algebraic description of id114

review of gm de   ninitions
algebraic and semialgebraic descriptions
restricted id82s

3

identi   ability, singular learning theory, other perspectives

identi   ability
singular learning theory

jason morton (penn state)

algebraic deep learning

7/19/2012

3 / 103

1 algebraic geometry of tensor networks

tensors
tensor networks
algebraic geometry

2 algebraic description of id114

review of gm de   ninitions
algebraic and semialgebraic descriptions
restricted id82s

3

identi   ability, singular learning theory, other perspectives

identi   ability
singular learning theory

jason morton (penn state)

algebraic deep learning

7/19/2012

4 / 103

matrices

i

t

j

a matrix m = (mij )

represents a linear transformation u     v
is a 2-way array
has an action by gl(u), gl(v ) on two sides

matrix decomposition is a workhorse of ml, much else
most data can be    attened into matrix format

jason morton (penn state)

algebraic deep learning

7/19/2012

5 / 103

 
 
 
 
tensors

i

j

t

k

a tensor t = (tijk)

represents a multilinear transformation u     v     w ,
w     u     v , u     w     v , etc.
is a multi-way array (here 3-way)
with a multilinear action on each    leg    or    side   

tensor decomposition is possible but more subtle
data arrives in tensor format, and something is lost by    attening

jason morton (penn state)

algebraic deep learning

7/19/2012

6 / 103

 
 
 
 
 
 
example of tensor decomposition
three possible generalizations of eigenvalue decomposition are the
same in the matrix case but not in the tensor case. for a p    p    p
tensor k ,

name

minimum r such that

tensor rank

border rank

k =(cid:80)r

not closed

i=1 ui     vi     wi

k = lim    0(s ), tensor rank(s ) = r
closed but hard to represent;
de   ning equations unknown.

multilinear rank k = a    c , c     rr  r  r , a     rp  r ,

closed and understood.
algebraic deep learning

jason morton (penn state)

7/19/2012

7 / 103

matrices vs tensors

generalization of matrix concepts to tensors is usually not
straightforward, but
   attenings are still matrices
e   ective computations in multiid202 generally reduce to
id202 (so far)

jason morton (penn state)

algebraic deep learning

7/19/2012

8 / 103

flatten

z

w

t

t

u

v w

z

u

v

view t     u   v    w    z as t : z       w         u   v

jason morton (penn state)

algebraic deep learning

7/19/2012

9 / 103

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
contract

(cid:96)

  m

i

s

  j

t

k

can express as:    atten, then multiply the matrices, then reshape

jason morton (penn state)

algebraic deep learning

7/19/2012

10 / 103

 
 
 
 
 
 
 
 
algebraic geometry of tensor networks

jason morton (penn state)

algebraic deep learning

7/19/2012

11 / 103

1 algebraic geometry of tensor networks

tensors
tensor networks
algebraic geometry

2 algebraic description of id114

review of gm de   ninitions
algebraic and semialgebraic descriptions
restricted id82s

3

identi   ability, singular learning theory, other perspectives

identi   ability
singular learning theory

jason morton (penn state)

algebraic deep learning

7/19/2012

12 / 103

what is a tensor network?

jason morton (penn state)

algebraic deep learning

7/19/2012

13 / 103

and why do they keep coming up?

when we reason about processes in space and time, involving
interaction and independence, causality and locality, we tend to
draw diagrams (networks) involving boxes, wires, arrows
attaching mathematical meaning to such a diagram

(cid:73) allows us to quantitatively model real systems and to compute
(cid:73) usually leads to de   ning a monoidal category

analyzing the monoidal category often means de   ning a functor
to the category of vector spaces and linear transformations

jason morton (penn state)

algebraic deep learning

7/19/2012

14 / 103

bayesiannetworks:directedfactorgraphmodelsconvertingabayesiannetwork(a)toadirectedfactorgraph(b).factorfistheconditionaldistributionpy|x,gispz|x,andhispw|z,y.xyzwxfgyzhwefghxyzw(a)(b)(c)(c)isastringdiagramforatypeofmonoidalcategory;mostoftherestofthetalkwillbede   ningthis.jasonmorton(pennstate)graphicalmodelsasmonoidalcategories1/4/20125/25tensor networks

category theory provides a succint and beautiful means to
formalize ideas about such diagrams and their interpetations in
applied mathematics
but i   ll focus on diagrams in the category of vector spaces and
linear transformations, often called tensor networks
fortunately, many of the same mathematical ideas work to
analyze these whether they occur in machine learning, statistics,
computational complexity, or quantum information
algebraic geometry and representation theory provide a powerful
set of tools to characterize and understand these objects as they
arise in . . .

jason morton (penn state)

algebraic deep learning

7/19/2012

15 / 103

tensor networks

ml and statistics

complexity theory

quantum information

and many-body systems

jason morton (penn state)

algebraic deep learning

7/19/2012

16 / 103

|0i|0i|0ihhhhh|000i+|111i   2bayesiannetworks:directedfactorgraphmodelsconvertingabayesiannetwork(a)toadirectedfactorgraph(b).factorfistheconditionaldistributionpy|x,gispz|x,andhispw|z,y.xyzwxfgyzhwefghxyzw(a)(b)(c)(c)isastringdiagramforatypeofmonoidalcategory;mostoftherestofthetalkwillbede   ningthis.jasonmorton(pennstate)graphicalmodelsasmonoidalcategories1/4/20125/25pfa   ancircuit/kernelcountingexample76540123nae76540123nae76540123nae76540123nae76540123561310987241112#ofsatisfyingassignments=hallpossibileassignments,allrestrictionsi=    pdet(x+y)4096-dimensionalspace(c2)   1212  12matrixjasonmorton(pennstate)pfa   ancircuits1/6/20125/5821=time(a)(b)=timefig.19.left(a)thecircuitrealization(internaltothetriangle)ofthefunctionfwofe.g.(23)whichoutputslogical-onegiveninput|x1x2x3i=|001i,|010iand|100iandlogical-zerootherwise.right(b)reversingtimeandsettingtheoutputto|1i(e.g.post-selection)givesanetworkrepresentingthew-state.thena    verealizationoffwisgiveninfigure21withanoptimizedco-algebraicconstructionshowninfigure21.fig.20.na    vectnsrealizationofthefamiliarw-state|001i+|010i+|100i.astandard(temporal)acyclicclassicalcircuitdecompositionintermsofthexor-algebrarealizesthefunctionfwofthreebits.thisfunctionisgivenarepresentationontensors.asillustrated,thenetworksinputispostselectedto|1itorealizethedesiredw-state.example22(networkrealizationof|  i=|01i+|10i+  k|11i).wewillnowdesignanetworktorealizethestate|01i+|10i+  k|11i.the   rststepistowritedownafunctionfssuchthatfs(0,1)=fs(1,0)=fs(1,1)=1(27)andfs(00)=0(inthepresentcase,fsisthelogicalor-gate).wepostselectthenetworkoutputon|1i,whichyieldsthestate|01i+|10i+|11i,seefigure23(a).thenextstepistorealizeadiagonaloperator,thatactsasidentityonallinputs,except|11iwhichgetssentto  k|11i.todothis,wedesignafunctionfdsuchthatfd(0,1)=fd(1,0)=fd(0,0)=0(28)andfd(1,1)=1(inthepresentcase,fdisthelogicaland-gate).thisdiagonal,takestheforminfigure23(b).the   nalstate|  i=|01i+|10i+  k|11iisrealizedbyconnectingbothnetworks,leadingtofigure23(c).vi.proofofthemaintheoremswearenowinapositiontostatethemaintheoremofthiswork.speci   cally,wehaveaconstructivemethodtorealizeanyquantumstateintermsofacategoricaltensornetwork.8westateandprovethetheoremforthecaseofqubit.thehigherdimensionalcaseofquditsfollowsfromknownresultsthatanyd-stateswitchingfunctioncanbeexpressedasapolynomialandrealizedasaconnectednetwork[47,86,87].thetheoremcanbestatedas8acorollaryofourexhaustivefactorizationofquantumstatesintotensornetworksisanewtypeofquantumnetworkuniversalityproof.toavoidconfusion,wepointoutthatpastuniversalityproofsinthegatemodelalreadyimplythatthelinearfragment(figure3)togetherwithlocalgatesisquantumuniversal.however,theknownuniversalityresultsclearlydonotprovideamethodtofactorastateintoatensornetwork!indeed,thedecompositionorfactorizationofastateintoatensornetworkisanentirelydi   erentproblemwhichweaddresshere.tensors and wiring diagrams

a multilinear operator

t : u     v     w
is a tensor

u

v

t

w

draw a wire for each vector space (variable)
box for each tensor (factor)
arrows denote primal/dual

jason morton (penn state)

algebraic deep learning

7/19/2012

17 / 103

 
 
 
 
 
 
composition

connect wires to compose/contract: h     g     f

/a or ida

/a

/b

f

/a

f

/b

g

/c

/d

h

juxtpose to tensor/run in parallel f     g

/a

/c

/b

/d

f

g

from these primitives (and duals, swaps, special maps) can build
complex networks
jason morton (penn state)

algebraic deep learning

7/19/2012

18 / 103

/
/
/
/
/
/
/
/
/
/
/
you could have invented id114

this graphical notation has arisen several times in several areas
(feynman diagrams, id114, circuits, representation
theory. . . )
this convergent evolution is no accident and re   ects an
underlying mathematical structure (monoidal categories)
using the resulting common generalizations to study graphical
models is promising; makes translating results from/to other
areas much easier

jason morton (penn state)

algebraic deep learning

7/19/2012

19 / 103

graphical language for monoidal categories

a 2-category with one object is a strict monoidal category; the
graphical language is poincar  e dual to the 2-cell diagram notation.

a

c

f 

   

g 

   

   

b

d

a

   

   

f 
bg 
c

(cid:28) f

g

(cid:28) f     g

(cid:28) f
g

(cid:28) g     f

see papers by joyal and street, selinger.

jason morton (penn state)

algebraic deep learning

7/19/2012

20 / 103

 
 
i
i
 
 
 
i
i
 
 
 
 
 
 
 
 
 
 
 
j
j
/
/
 
 
 
 
 
 
 
 
categories

a category c consists of a

class of objects ob(c) and set mor(a, b) of morphisms for each
ordered pair of objects,
an associative composition rule taking morphisms a f    b,
    c , to a morphism g     f : a    c
b
an identity ida     mor(a, a) such that idb    f = f = f     ida.

g

diagrammatically:

/a or ida

/a

/b

f

/a

f

/b

g

/c

/d

h

think of categories and variations as concrete and combinatorial.

jason morton (penn state)

algebraic deep learning

7/19/2012

21 / 103

/
/
/
/
/
/
/
monoidal categories
de   nition
a monoidal category (c,   ,   ,   ,   , i) is a category c with the
additional data of
(i) an abstract tensor product functor     : c    c     c,
(ii) a natural isomorphism called the associator

  abc : (a     b)     c     a     (b     c ),

  a : a     i, the left and right unitors,

(iii) a unit object i and natural isomorphisms   a : i     a     a and
such that if w , w(cid:48) are two words obtained from a1     a2            an by
inserting is and balanced parenthesis, then all isomorphisms
   : w     w(cid:48) composed of   s,   s, and   s and their inverses are equal.
thus we have a unique natural transformation w     w(cid:48).
a monoidal category is strict if   ,   , and    are equalities. monoidal
categories can be    stricti   ed,    so the   ,   ,    can often be ignored.

jason morton (penn state)

algebraic deep learning

7/19/2012

22 / 103

monoidal categories ctd.

de   nition
a monoidal category is braided if it is equipped with natural

isomorphisms ba   b : a     b        b     a subject to the hexagon axioms
and symmetric if ba   bbb   a = ida   b.
diagramatically:

=

braid isomorphism

symmetry relation

ba   b

jason morton (penn state)

algebraic deep learning

7/19/2012

23 / 103

from graphical model to string diagram
q: what does the graphical language of certain monoidal categories
with additional axioms look like?
a   : factor graph models.

x

z

y

w

x f
g

z

y

h

w

e

x

g

z

f

y

h

w

(a)

(b)

(c)

converting a id110 (a) to a directed factor graph (b) and
a string diagram (c). factor f is the conditional distribution py|x , g is
pz|x , and h is pw|z,y .

jason morton (penn state)

algebraic deep learning

7/19/2012

24 / 103

from graphical model to string diagram

   
???   
   
   
   

   

   
???   
   
   
   

   

   

converting an undirected factor graph to a string diagram.

jason morton (penn state)

algebraic deep learning

7/19/2012

25 / 103

example: na    ve bayes

???????
   
       
   
   
   

  a       a     ua

a

a a a

b c d

b c d

jason morton (penn state)

algebraic deep learning

7/19/2012

26 / 103

example: gluing two triangles

oooooo
   oooooo
oooooo
   oooooo
   

   

d

c

c

a

b

  a

  b

a

b

a

b

a

a

b

b

d

jason morton (penn state)

algebraic deep learning

7/19/2012

27 / 103

rbm: hadamard product of na    ve bayes

???????
   
       
????????
        
   
   
   
   

a

b c d

mc

b c d

mb

md

a

b c d

b c d

e

e

jason morton (penn state)

algebraic deep learning

7/19/2012

28 / 103

id114 as tensor networks

roughly

wires are variables (frobenius algebras)
boxes are factors at    xed parameters, or spaces of factors as
parameters vary
under suitable assumptions

(cid:73) global properties
(cid:73) (such as the set of equations cutting out the space of

representable id203 distributions)

(cid:73) can be computed by gluing local properties

how do we describe these spaces of representable id203
distributions?

jason morton (penn state)

algebraic deep learning

7/19/2012

29 / 103

algebraic geometry of tensor networks

jason morton (penn state)

algebraic deep learning

7/19/2012

30 / 103

1 algebraic geometry of tensor networks

tensors
tensor networks
algebraic geometry

2 algebraic description of id114

review of gm de   ninitions
algebraic and semialgebraic descriptions
restricted id82s

3

identi   ability, singular learning theory, other perspectives

identi   ability
singular learning theory

jason morton (penn state)

algebraic deep learning

7/19/2012

31 / 103

what is algebraic geometry?
study of solutions to systems of polynomial equations

3

2 x4     5x 3

consider the ring of multivariate polynomials f     c[x1, . . . , xn],
e.g. 3x 2
any polynomial f     c[x1, . . . , xn] has a zero locus
{v = (v1, . . . , vn)     cn : f (v ) = 0}.

this is the variety v (f ) cut out by f . for one polynomial, this
variety is a hypersurface.

jason morton (penn state)

algebraic deep learning

7/19/2012

32 / 103

introductiontoalgebraicgeometryletr[p]=r[p1,...,pm]bethesetofallpolynomialsinindeterminatesp1,...,pmwithrealcoef   cients.de   nitionletf   r[p1,...,pm].thevarietyde   nedbyfisthesetv(f):={a   rm|f(a)=0forallf   f}.v({p2   p21})=sethsullivant(ncsu)algebraicstatisticsjune9,20122/34what is algebraic geometry?

study of solutions to systems of polynomial equations

3

2 x4     5x 3

consider the ring of multivariate polynomials f     c[x1, . . . , xn],
e.g. 3x 2
any polynomial f     c[x1, . . . , xn] has a zero locus
{v = (v1, . . . , vn)     cn : f (v ) = 0}.

this is the variety v (f ) cut out by f . for one polynomial, this
variety is a hypersurface.
for example, the set of id203 distributions that can be
represented by an rbm with 4 visible and 2 hidden nodes is part
of a hypersurface.

(cid:73) its f has degree 110 and probably around 5 trillion monomials

[cueto-yu]

jason morton (penn state)

algebraic deep learning

7/19/2012

33 / 103

varieties de   ned by sets of polynomials

now suppose you have a set f of polynomials: a system of
polynomial equations
requiring them all to hold simultaneously means we keep only
the points where they all vanish:

(cid:73) the intersection of their hypersurfaces

the zero locus

{v = (v1, . . . , vn)     cn : f (v ) = 0 for all f     f}

of a set of polynomials f is the variety v (f).

jason morton (penn state)

algebraic deep learning

7/19/2012

34 / 103

sets of polynomials de   ned by varieties

on the other hand,

given a set s     cn, the vanishing ideal of s is

i (s) = {f     c[x1, . . . , xn] : f (a) = 0    a     s}.

hilbert   s basis theorem: such an ideal has a    nite generating set.

combining these operations:
a set s     cn has a zariski closure v (i (s)).

jason morton (penn state)

algebraic deep learning

7/19/2012

35 / 103

why applied algebraic geometry

   any su   ciently advanced    eld of mathematics can model any
problem.   

(cid:73) algebraic geometry is as old as it gets.

more formally:

(cid:73) ideal membership / computing gr  obner bases is

expspace-complete.

(cid:73) matiyasevich   s theorem: every recursively enumerable set is a

diophantine set (10th)

so any question with a computable answer can be phrased in
terms of algebraic geometry.

(cid:73) if done well, get geometric insight into the problem and deep

hammers.

jason morton (penn state)

algebraic deep learning

7/19/2012

36 / 103

implicitization

we would like to study e.g. the space of id203 distributions
representable by various deep learning models.
these models are not given as sets of polynomials f
they are given in terms of a parameterization described by a
graph: each box in the tensor network is allowed to be a certain
restricted set of tensors
so we need to study the map from parameter space to
id203 space, its    bers, and its image.
a complete description tells us what equations and what
inequalities a id203 distribution must satisfy in order to
come from, say, a dbn

jason morton (penn state)

algebraic deep learning

7/19/2012

37 / 103

implicitization

de   ne a polynomial map    from a parameter space        cn to
an ambient space cm

x = t
y = t 2

de   nes an image   (  )     cm. what equations de   ne, or cut
out this set? y     x 2 = 0 cuts out the image.
we took a zariski closure
the process of    nding de   ning equations of the image is called
implicitization
this is hard! but not impossible.

jason morton (penn state)

algebraic deep learning

7/19/2012

38 / 103

example: mixture of products

a

b

c

d

jason morton (penn state)

algebraic deep learning

7/19/2012

39 / 103

/ na    ve bayes / secant segre / tensor rank

   

   

   

   

   

    
    
???????
*****
       
     
   
   
   
   

p1

p1  p1  p1  p1 (cid:44)    p15
segre variety de   ned by
2    2 minors of    attenings
of 2    2    2    2 tensor
  2(p1  p1  p1  p1)

first secant of segre variety
3    3 minors of    attenings

dimension, equations de   ning such models?

jason morton (penn state)

algebraic deep learning

7/19/2012

40 / 103

computational algebraic geometry

there are computational tools for algebraic geometry, and many
advances mix computational experiments and theory.
gr  obner basis methods power general purpose software:
singular, macaulay 2, cocoa, (mathematica, maple)

(cid:73) symbolic term rewriting

polyhedral methods (e.g. polymake, gfan) for certain problems;
e.g. using work by cueto and yu, bray and m- reduce
implicitization to (very) large-scale id202: just    nd the
(1d) kernel of a 5 trillion    5 trillion matrix for rbm4,2.
computational algebraic geometry computations now routinely
burn millions of cpu-hours of cluster compute time (e.g.
implicitization, searching for e   ciently contractable networks).

jason morton (penn state)

algebraic deep learning

7/19/2012

41 / 103

numerical algebraic geometry

salmon problem: determine the ideal de   ning the fourth secant
variety of p3xp3xp3. set theoretic [friedland 2010], further
progress [bates, oeding 2010], [friedland, gross 2011].
numerical algebraic geometry: numerical methods for
approximating complex solutions of polynomial systems.

(cid:73) homotopy continuation (numerical path following).
(cid:73) can be used to    nd isolated solutions or points on each

positive-dimensional irreducible component.

(cid:73) can scale to thousands of variables for certain problems.
(cid:73) reliable, parallelized, adaptive multiprecision software is

available: bertini (bates, hauenstein, sommese, and wampler).

jason morton (penn state)

algebraic deep learning

7/19/2012

42 / 103

why are geometers interested?

applications (especially tensor networks in statistics and cs)
have revived classical viewpoints such as invariant theory.
re-climbing the hierarchy of languages and tools (italian school,
zariski-serre, grothendieck) as applied problems are uni   ed and
recast in more sophisticated language.
applied problems have also revealed gaps in our knowledge of
algebraic geometry and driven new theoretical developments and
computational tools

(cid:73) objects which are    large   : high-dimensional, many points, but

with many symmetries

(cid:73) these often stabilize in some sense for large n.

jason morton (penn state)

algebraic deep learning

7/19/2012

43 / 103

lectures 2 and 3

jason morton (penn state)

algebraic deep learning

7/19/2012

44 / 103

last time

last time, we talked about the

algebraic geometry of tensor networks

and how to learn something about this geometry

jason morton (penn state)

algebraic deep learning

7/19/2012

45 / 103

tensors

i

j

t

k

a tensor t = (tijk)

represents a multilinear transformation u     v     w ,
w     u     v , u     w     v , etc.
is a multi-way array (here 3-way)
with a multilinear action on each    leg    or    side   

tensor decomposition is possible but more subtle
data arrives in tensor format, and something is lost by    attening

jason morton (penn state)

algebraic deep learning

7/19/2012

46 / 103

 
 
 
 
 
 
one way things get harder with tensors

which tensor products cd1                cdn have    nitely many orbits
under gl(d1, c)              gl(dn, c)?
the answer for matrices is easy
kac (1980), parfenov (1998, 2001): up to c2     c3     c6, orbit
representatives and abutment graph

jason morton (penn state)

algebraic deep learning

7/19/2012

47 / 103

orbitsandtheirclosuresinthespacesck1            ckr91presentedinfig.1,wheretheindicesofverticesofthegraphcorrespondtotheindicesoforbitsappearingintheorem6.theintegersontheleft-handsidearethedimensionsoftheorbits.attheendof  2weprovetheorem11,whichassertsthatinallcasesunderconsiderationinourpapertheabutmentgraphsaresubgraphsoftheabutmentgraphforthecase(2,3,6).thisgraphispresentedinfig.2,wheretheindicesofverticescorrespondtotheindicesoforbitsintheorem8.theintegersontheleft-handsidearethedimensionsoftheorbitsintheirdependenceonn.forclaritytheresultsofthispaperarecollectedintable0.inthistable,foreachcase(2,m,n)weindicatethenumberoforbitsofgl2  glm  glnandthedegreeofthegeneratorforthealgebraofinvariantsofthecorrespondinggroupsl2  slm  sln;wealsoindicatethestatementsrelatingtotheorbitsandthegraphsofabuttings.table no.case(2,m,n)thenumberoforbitsofgl2  glm  glndegfassertionontheorbitsassertionontheabutmentgraph1(2,2,2)74lemma2theorem11,fig.22(2,2,3)96theorem8theorem11,fig.23(2,2,4)104theorem8theorem11,fig.24(2,2,n),n(cid:1)5100theorem8theorem11,fig.25(2,3,3)1812theorem6theorem11,figs.1,26(2,3,4)2412theorem8theorem11,fig.27(2,3,5)260theorem8theorem11,fig.28(2,3,6)276theorem8theorem11,fig.29(2,3,n),n(cid:1)7270theorem8theorem11,fig.2themainresultsofthepresentpaperwerepublished(withoutproofs)in[6].weusethisopportunitytopointoutthat[6]containstwodisappointingmistakes,oneofwhichisaconsequenceoftheother.namely:(1)intheorem2,theline(2,2,n),n(cid:1)4,hastenorbitswithrepresentatives1   9,19mustbereplacedbytheline(2,2,n),n(cid:1)4,hastenorbitswithrepresentatives1   7,11,13,19;(2)accordingly,the   gurewiththeabutmentgraphshouldcontainnoarrowfromvertex19tovertex9,butthereshouldbeanarrowfromthevertex19tovertex13instead.iwouldliketoexpressmydeepgratitudetomyresearchsupervisor  e.b.vinbergforsettingtheproblem,crucialadvice,andconstantattentiontothisresearch.another way

consider a matrix turned into a vector

[x1, x2, x3, . . . x(cid:96)]

can you compute its rank, svd, kernel, etc?

jason morton (penn state)

algebraic deep learning

7/19/2012

48 / 103

composition

connect wires to compose/contract: h     g     f

/a or ida

/a

/b

f

/a

f

/b

g

/c

/d

h

juxtpose to tensor/run in parallel f     g

/a

/c

/b

/d

f

g

from these primitives (and duals, swaps, special maps) can build
complex networks such as id114. . .

jason morton (penn state)

algebraic deep learning

7/19/2012

49 / 103

/
/
/
/
/
/
/
/
/
/
/
tensor networks

x

z

y

w

x f
g

z

y

h

w

e

x

g

z

f

y

h

w

jason morton (penn state)

algebraic deep learning

7/19/2012

50 / 103

algebraic geometry

study of solutions to systems of polynomial equations

ring of multivariate polynomials f     c[x1, . . . , xn], e.g.
3x 2
2 x4     5ix 3
the zero locus

3

{v = (v1, . . . , vn)     cn : f (v ) = 0 for all f     f}

of a set of polynomials f is the variety v (f).
given a set s     cn, the vanishing ideal of s is

i (s) = {f     c[x1, . . . , xn] : f (a) = 0    a     s}.

hilbert   s basis theorem: such an ideal has a    nite generating set.
a set s     cn has a zariski closure v (i (s)).

jason morton (penn state)

algebraic deep learning

7/19/2012

51 / 103

implicitization

de   ne a polynomial map    from a parameter space        cn to
an ambient space cm

x = t
y = t 2

de   nes an image   (  )     cm. what equations de   ne, or cut
out this set? y     x 2 = 0 cuts out the image.
we took a zariski closure
the process of    nding de   ning equations of the image is called
implicitization

jason morton (penn state)

algebraic deep learning

7/19/2012

52 / 103

algebraic geometry and tensor networks

to an algebraic geometer, a tensor network

appearing in machine learning (statistics, signal processing,
computational complexity, quantum computation, . . . )
describes a regular map    from the parameter space (choice of
tensors at the nodes) to an ambient space.
the image of    is an algebraic variety of representable
id203 distributions,
the    bers tell us about identi   ability, transferability, and
learning rate

jason morton (penn state)

algebraic deep learning

7/19/2012

53 / 103

1 algebraic geometry of tensor networks

tensors
tensor networks
algebraic geometry

2 algebraic description of id114

review of gm de   ninitions
algebraic and semialgebraic descriptions
restricted id82s

3

identi   ability, singular learning theory, other perspectives

identi   ability
singular learning theory

jason morton (penn state)

algebraic deep learning

7/19/2012

54 / 103

algebraic description of probabilistic models

   statistical models are algebraic varieties   

what distributions can be represented by a (graphical) model?
what is the geometry of the parameterization map?
implications for approximation and optimization (learning)
performance?

jason morton (penn state)

algebraic deep learning

7/19/2012

55 / 103

id187, undirected id114

model joint id203 distributions on n random variables
x1, . . . , xn with    nitely many states d1, . . . , dn.
de   ne dependence locally by a simplicial complex on
{1, 2, . . . , n}, parameterizing a family of id203 distributions
by potential functions or factors, one per maximal simplex.
in an undirected graphical model, the simplicial complex is the
clique complex of an undirected graph.

   d
//////
         
c
   
   a
b
ugm

   
???   
   
   
   

fg

   
//
     

fg

  
   

//
   

jason morton (penn state)

algebraic deep learning

7/19/2012

56 / 103

id187: id203 distribution

model joint id203 distributions on n random variables
x1, . . . , xn with    nitely many states d1, . . . , dn.
de   ne dependence locally by a simplicial complex on
{1, 2, . . . , n}, parameterizing a family of id203 distributions
by potential functions or factors, one per maximal simplex.
in an undirected graphical model, the simplicial complex is the
clique complex of an undirected graph.

de   nes a family of id203 distributions on discrete random
variables x1, . . . , xn, where xi has di states by (before marginalizing)

pm(x) =

1

z (cid:89)s   s

  s(xs)

where xs is the state vector restricted to the vertices in s, each   s is
a tensor corresponding to the factor associated to simplex s.

jason morton (penn state)

algebraic deep learning

7/19/2012

57 / 103

id187: undirected factor graph models

a hierarchical model de   nes a factor graph, which is a bipartite graph
   = (v     h, f , e ) of nodes, factors and edges.

each i     {1, . . . , n} = h     v is labeled by a random variable xi
and denoted by a open     or    lled     disc according to whether it
is hidden (latent) or visible respectively.
each maximal simplex s     s is denoted by a box fs
labeled by a
factor fs     f , and is connected by an edge to each variable disc
xi where i     s.

   d
//////
         
c
   
   a
b
ugm

   
???   
   
   
   

fg

   
//
     

//
   

  
   

fg

    hidden
    
???   
   
   
   
fg

jason morton (penn state)

algebraic deep learning

7/19/2012

58 / 103

 
 
implicitization and id114

the hammersley-cli   ord theorem is a theorem about
implicitizing undirected id114
they delayed publication for years trying to address the
nonnegative case
this was completed in [geiger, meek, sturmfels 2006] by
studying the algebraic geometry of these models (   algebraic
statistics   )

jason morton (penn state)

algebraic deep learning

7/19/2012

59 / 103

id110s: directed factor graph models

a (discrete) id110 m = (h, v , d, g ) is based on a
directed acyclic graph g .

vertices partitioned [n] = h     v into hidden and visible
variables; each variable i     [n] has a number di of states.
the parameterization de   nes for each variable x a conditional
id203 distribution (a singly stochastic matrix) p(xi|xpa(i))
where pa(i) is the set of vertices which are parents of i.
then

pm(v ) =(cid:88)xh (cid:89)i   [n]

p(xi|xpa(i))

with p(xi|xpa(i))     0,(cid:80)i p(xi|xpa(i)) = 1 and no global

id172 is needed because of the local id172.

jason morton (penn state)

algebraic deep learning

7/19/2012

60 / 103

id110s: directed factor graph models

every id110 can be written as a directed factor
graph model, but not conversely [frey 2003].
algebraic geometry of id110s [garcia, stillman,
sturmfels 2005]

jason morton (penn state)

algebraic deep learning

7/19/2012

61 / 103

   unhidden    binary deep belief network
consider a binary dbn with layer widths n0, . . . , n(cid:96). an    unhidden   
binary dbn de   nes joint id203 distributions of the form

p(h0, h1, . . . , h(cid:96)) = p(h(cid:96)   1, h(cid:96))

(cid:96)   2(cid:89)k=0

p(hk|hk+1) ,

nk(cid:89)j=1

p(hk

p(hk|hk+1) =
j |hk+1)     exp(cid:32)hk

p(hk

j |hk+1) ,

j bk

j + hk
j

w k+1

j,i hk+1

i (cid:33) ,

nk+1(cid:88)i=1

j )j     {0, 1}nk is the state of the units in the kth layer,

hk = (hk
j,i     r is the connection weight between the units j and i
w k
from the (k     1)th and kth layer respectively, and
j     r is the bias weight of the jth unit in the kth layer.
bk

jason morton (penn state)

algebraic deep learning

7/19/2012

62 / 103

binary dbn

now the dbn model dbn(n0, n1, . . . , n(cid:96)) is the set of marginal
distributions

p(h0) =

(cid:88)h1   {0,1}n1 ,...,h(cid:96)   {0,1}n(cid:96)

p(v , h1, . . . , h(cid:96)),

h0     v     {0, 1}n0

(1)

of joint id203 distributions of that form.
the dbn has

d = (

(cid:96)(cid:88)k=1

nk   1nk) + (

(cid:96)(cid:88)k=0

nk) parameters.

so this is its expected dimension if there is no collapse or waste
of parameters. which tuples (n0, . . . , n(cid:96)) have the expected
dimension?

jason morton (penn state)

algebraic deep learning

7/19/2012

63 / 103

where we are

the dbn contains many of the known models, hence
don   t have a complete semialgebraic description of dbn, dbm.
have partial information about representational power
have algebraic, semialgebraic descriptions of submodels: na    ve
bayes, id48, trees, rbm, etc.

(cid:73) in some cases (especially small number of states), this is done
(cid:73) in others, just have coarse information (dimension, relative

power)

translating that understanding to something prescriptive is
ongoing
let   s look at some of these submodels

jason morton (penn state)

algebraic deep learning

7/19/2012

64 / 103

na    ve bayes / secant segre / tensor rank

a

b

c

d

jason morton (penn state)

algebraic deep learning

7/19/2012

65 / 103

na    ve bayes / secant segre / tensor rank
look at one hiden node in such a network, binary variables

   

   

   

   

   

    
    
???????
*****
       
     
   
   
   
   

p1

p1  p1  p1  p1 (cid:44)    p15
segre variety de   ned by
2    2 minors of    attenings
of 2    2    2    2 tensor
  2(p1  p1  p1  p1)

first secant of segre variety
3    3 minors of    attenings

dimension, equations de   ning such models?

jason morton (penn state)

algebraic deep learning

7/19/2012

66 / 103

expected dimension of secant varieties

the expected dimension of   k(p1)n is

min(kn + k     1, 2n     1)

(e.g. by a parameter count)
but (especially for small n) things can collide and we can get a
defect, where the dimension is less than expected
dimension is among the    rst questions one can ask about a
variety

(cid:73) is there hope for identi   ability?
(cid:73) are there wasted parameters/ positive-dimensional    bers?
(cid:73) how big a model is needed to be able to represent all

distributions?

jason morton (penn state)

algebraic deep learning

7/19/2012

67 / 103

dimension of secant varieties

recently [catalisano, geramita, gimigliano 2011] showed
  k(p1)n has the expected dimension

min(kn + k     1, 2n     1)

except   3(p1)4 where it is 13 not 14.
progress in palatini 1909, . . . , alexander hirschowitz 1995,
2000, cgg 2002,03,05, abo ottaviani peterson 2006, draisma
2008, others.
classically studied, revived by applications to statistics, quantum
information, and complexity; shift to higher secants, solution.
so a generic tensor of (c2)   n can be written as a sum of (cid:100) 2n
n+1(cid:101)
decomposable tensors, no fewer.

jason morton (penn state)

algebraic deep learning

7/19/2012

68 / 103

representation theory of secant varieties

raicu (2011) proved the ideal-theoretic gss [garcia stillman
sturmfels 05] conjecture

equations de   ning   2(pk1              pkn)
using representation theory of ideal of   2(pk1              pkn) as a
glk1           glkn-module
(progress in [landsberg manivel 04, landsberg weyman 07,
allman rhodes 08]).

jason morton (penn state)

algebraic deep learning

7/19/2012

69 / 103

secantvarietiesofsegre   veronesevarieties15de   nition3.14.givenapartition  =(  1,      ,  t)   r,ann-partition     nrandablockm   ud  ,weassociatetotheelementc    m   c    ud  then-tableaut=(t1,      ,tn)=t1            tnofshape  ,obtainedasfollows.supposethattheblockmhastheset  ijinitsi-throwandj-thcolumn.thenwesetequaltoitheentriesintheboxesoftjindexedbyelementsof  ij(recallfromsection2.3thattheboxesofatableauareindexedcanonically:froid113fttorightandtoptobottom).notethateachtableautjhasentries1,      ,t,withiappearingexactly  i  djtimes.notealsothatinordertoconstructthen-tableautwehavemadeachoiceoftheorderingoftherowsofm:interchangingrowsiandi0when  i=  i0shouldyieldthesameelementm   ud  ,thereforeweidentifythecorrespondingn-tableauxthatdi   erbyinterchangingtheentriesequaltoiandi0.example3.15.weletn=2,d=(2,1),r=4,  =(2,2)asinexample3.2,andconsiderthe2-partition  =(  1,  2),with  1=(5,3),  2=(2,1,1).wehavec    1,612,344,527,8312233144   1342c    2,347,831,614,5231144322   3421let   swritedowntheactionofthemap    onthetableauxpicturedabove          12233144   1342      =11122122   1221+12211122   1122+12222111   1212.wecollectinthefollowinglemmathebasicrelationsthatn-tableauxsatisfy.lemma3.16.fixannpartition     nr,andlettbeann-tableauofshape  .thefollowingrelationshold:(1)if  isapermutationoftheentriesoftthatpreservesthesetofentriesineachcolumnoft,then  (t)=sgn(  )  t.inparticular,ifthasrepeatedentriesinacolumn,thent=0.equations of the nai  ve bayes model (secant
varieties of segre varieties)

good news/bad news

good news: we know them for small number of states
bad news: but we don   t know them for large numbers of states.
good news: in some cases, just minors of    attenings
bad news: but not in general.
good news: many models (trees, rbm, dbn) are built by
gluing na    ve bayes models together, so we have some
information and the good news above propagates
bad news: but so does the bad news.

jason morton (penn state)

algebraic deep learning

7/19/2012

70 / 103

algebraic description of id48

a simpli   ed (circular) version. fix parameter matrices a1, . . . , ad .
then up to a global rescaling,

p = (cid:88)i1,...,in

tr(ai1        ain)ei1i2      in

jason morton (penn state)

algebraic deep learning

7/19/2012

71 / 103

algebraic description of id48

a simpli   ed (circular) version. fix parameter matrices a1, . . . , ad .
then up to a global rescaling,

p = (cid:88)i1,...,in

tr(ai1        ain)ei1i2      in

what are the polynomial relations that hold among the coe   cients

pi1,...in = tr(ai1        ain)?

that is, the ideal i = {f : f (pi1,...in) = 0} of polynomials f in the
coe   cients such that f (pi1,...in) = 0.
series of papers [bray and m- 2006], [sch  onhuth 2008, 2011], [critch
2012] provide characterizations, membership tests, identi   ability, etc.

jason morton (penn state)

algebraic deep learning

7/19/2012

72 / 103

(phylogenetic) trees

jason morton (penn state)

algebraic deep learning

7/19/2012

73 / 103

trees

jason morton (penn state)

algebraic deep learning

7/19/2012

74 / 103

trees and the general markov model

studied in a long series of papers by authors including
sturmfels-sullivant, casanellas, draisma-kuttler,
allman-rhodes, many others

(cid:73) many techniques were developed    rst on this reasonably

tractable class

ideas include changes of coordinates (fourier transform,
cumulant coordinates), gluing constructions, hard work
now have complete algebraic description of many special classes
complete semi-algebraic description of gmm for small number
of states [allman et al. 2012]
this is a submodel of the dbn.

jason morton (penn state)

algebraic deep learning

7/19/2012

75 / 103

restricted id82s

jason morton (penn state)

algebraic deep learning

7/19/2012

76 / 103

pre-rbm: graphical model on a bipartite graph

binary
state
vectors

h

v

   

   
   
   

   

m variables

(cid:122)
(cid:125)(cid:124)
(cid:123)
oooooooooooooooooo
jjjjjjjjjjjjjj
/////////
/////////
???????????
???????????
oooooooooooooooooo
tttttttttttttt
           
           
         
         
(cid:125)
(cid:123)(cid:122)
(cid:124)

   
   
   
   
   
   
n variables

   
   
   

   

   
   
   

c

w

b

real

parameters

unnormalized potential is built from node and edge parameters

  (v , h) = exp(h(cid:62)w v + b(cid:62)v + c(cid:62)h).

the id203 distribution on the binary random variables is

p(v , h) =

1
z     (v , h),

z =(cid:88)v ,h

  (v , h).

jason morton (penn state)

algebraic deep learning

7/19/2012

77 / 103

restricted id82s

binary
state
vectors

h

v

   

m hidden variables
   
   

(cid:125)(cid:124)
(cid:123)
(cid:122)
oooooooooooooooooo
jjjjjjjjjjjjjj
/////////
/////////
???????????
???????????
oooooooooooooooooo
tttttttttttttt
           
           
         
         
(cid:124)
(cid:125)
(cid:123)(cid:122)

   
   
   
   
   
   
n visible variables

   
   
   

   
   
   

   
   
   

c

w

b

real

parameters

unnormalized fully-observed potential is

  (v , h) = exp(h(cid:62)w v + b(cid:62)v + c(cid:62)h).

the id203 distribution on the visible random variables is

p(v ) =

1

z    (cid:88)h   {0,1}k

  (v , h),

z =(cid:88)v ,h

  (v , h).

jason morton (penn state)

algebraic deep learning

7/19/2012

78 / 103

restricted id82s

binary
state
vectors

h

v

   

m hidden variables
   
   

(cid:122)
(cid:125)(cid:124)
(cid:123)
oooooooooooooooooo
jjjjjjjjjjjjjj
/////////
/////////
???????????
???????????
oooooooooooooooooo
tttttttttttttt
           
           
         
         
(cid:125)
(cid:123)(cid:122)
(cid:124)

   
   
   
   
   
   
n observed variables

   
   
   

   
   
   

   
   
   

c

w

b

real

parameters

the restricted id82 (rbm) is the undirected
graphical model for binary random variables thus speci   ed.
denote by m m
b     rn, c     rk, w     rm  n vary.
m m
n is a subset of the id203 simplex    2n   1.

n the set of joint distributions as

jason morton (penn state)

algebraic deep learning

7/19/2012

79 / 103

hadamard products of varieties
given two projective varieties x and y in p(cid:96), their hadamard
product x   y is the closure of the image of

x    y (cid:57)(cid:57)(cid:75) p(cid:96) , (x, y ) (cid:55)    (x0y0 : x1y1 : . . . : x(cid:96)y(cid:96)).

we also de   ne hadamard powers x [m] = x     x [m   1].
if m is a subset of the simplex    (cid:96)   1 then m [m] is also de   ned by
componentwise multiplication followed by rescaling so that the
coordinates sum to one. this is compatible with taking zariski
closure: m [m] = m

[m]

lemma
rbm variety and rbm model factor as

v m
n = (v 1

n )[m]

and m m

n = (m 1

n )[m].

jason morton (penn state)

algebraic deep learning

7/19/2012

80 / 103

rbm as hadamard product of na    ve bayes

???????
   
       
????????
        
   
   
   
   

a

b c d

mc

b c d

mb

md

a

b c d

b c d

e

e

jason morton (penn state)

algebraic deep learning

7/19/2012

81 / 103

representational power of rbms

conjecture

the restricted id82 has the expected dimension.
that is, m m
min{nm + n + m, 2n     1} in    2n   1.

n is a semialgebraic set of dimension

jason morton (penn state)

algebraic deep learning

7/19/2012

82 / 103

expected dimension

mm

n

figure: the expected dimension of the model rbmn,m, and of mn,m+1,
min{2n     1, nm + n + m}. there is a barely noticeable irregularity on the
left side of the image, where the dimension of rbmn,m equals the
dimension of the ambient id203 simplex pn for all large enough m.

jason morton (penn state)

algebraic deep learning

7/19/2012

83 / 103

  501001502001501005000.511.522.533.54x 104representational power of rbms

we can show many special cases and the following general result:

theorem (cueto m- sturmfels)

the restricted id82 has the expected dimension

nm + n + m when m < 2n   (cid:100)log2(n+1)(cid:101)
min{nm + n + m, 2n     1} when m = 2n   (cid:100)log2(n+1)(cid:101) and
2n     1 when m     2n   (cid:98)log2(n+1)(cid:99).
covers most cases of restricted id82s in practice,
as those generally satisfy m     2n   (cid:100)log2(n+1)(cid:101).
proof uses tropical geometry, coding theory

jason morton (penn state)

algebraic deep learning

7/19/2012

84 / 103

tropical rbm model

tropical geometry is the    polyhedral shadow    of algebraic
geometry.
the process of passing from ordinary arithmetic to the max-plus
algebra is known as tropicalization.
the tropicalization    of our rbm parameterization is the map
   : rnm+n+m     tp2n   1 = r2n/r(1, 1, . . . , 1) whose 2n
coordinates are the tropical polynomials
qv = max(cid:8)h(cid:62)wv + b(cid:62)v + c(cid:62)h : h     {0, 1}m(cid:9)

this yields a piecewise-linear concave function rnm+n+m     r on
the space of model parameters (w , b, c).

its image tm m
n

is called the tropical rbm model.

jason morton (penn state)

algebraic deep learning

7/19/2012

85 / 103

tropical rbm variety

the tropical hypersurface t (f ) is the union of all codimension
one cones in the normal fan of the newton polytope of f .
is the intersection in tp2n   1 of
the tropical rbm variety tv m
n
all the tropical hypersurfaces t (f ) where f runs over all
polynomials that vanish on v m

n (or on m m

n ).

understand tropical variety, use:

dim(tm m

n )     dim(tv m

n ) =
n )     min{nm + n + m, 2n     1}

n ) = dim(v m

dim(m m

and coding theory to obtain the result.

jason morton (penn state)

algebraic deep learning

7/19/2012

86 / 103

relative representational power

another way to study the representational power of rbms and
dbns is to compare them with other models
when does one potential model contain another?

jason morton (penn state)

algebraic deep learning

7/19/2012

87 / 103

when does a mixture of products contain a
product of mixtures?

product of mixtures (rbm)

mixture of products

{0, 1}{0, 1}{0, 1}{0, 1}

{0, 1, . . . , k     1}

rbm6,4

m6,k

problem
given some n, m     n, what is the smallest k     n for which the
k-mixture of product distributions on n binary variables contains the
rbm model with n visible and m hidden units?

jason morton (penn state)

algebraic deep learning

7/19/2012

88 / 103

exponentially more e   cient
from yoshua   s    rst talk

jason morton (penn state)

algebraic deep learning

7/19/2012

89 / 103

#2 the need for distributed representations mul%-     	
   id91	
   id91	
   18	
   learning	
   a	
   set	
   of	
   features	
   that	
   are	
   not	
   mutually	
   exclusive	
   can	
   be	
   exponen%ally	
   more	
   sta%s%cally	
   e   cient	
   than	
   nearest-     neighbor-     like	
   or	
   id91-     like	
   models	
   when does a mixture of products contain a
product of mixtures?

the number of parameters of the smallest mixture of products
containing the rbm

such results aid our understanding of

(cid:73) grows exponentially in the number of parameters of the rbm
(cid:73) for any    xed ratio of hidden vs. visible units 0 < m/n <   
(cid:73) how models complement each other,
(cid:73) why techniques such as deep learning can be expected to

succeed, and

(cid:73) when model selection can be based on theory.

jason morton (penn state)

algebraic deep learning

7/19/2012

90 / 103

not very often

figure: plot of the smallest k for which mn,k contains rbmn,m. fixing
dimension (grey line), the rbms which are hardest to represent as
mixtures are those where m/n     1.

jason morton (penn state)

algebraic deep learning

7/19/2012

91 / 103

mnlog2(k)=m34n   log2(k)   n   1341dim(rbm)=const.modes and strong modes

de   nition
call x     {0, 1}n a mode of p     pn if p(x) > p(  x) for all   x with

dh(  x, x) = 1, and a strong mode if p(x) >(cid:80)  x:dh (  x,x)=1 p(  x).

one way to make precise that the rbm can represent more
complicated distributions than a mixture model of similar size is
to study this bumpiness in hamming space.
on the one hand the sets of strong modes c    {0, 1}n realizable
by a mixture model mn,k are exactly the binary codes of
minimum hamming distance two and cardinality at most k.
on the other hand. . .

jason morton (penn state)

algebraic deep learning

7/19/2012

92 / 103

rbms and linear threshold codes

de   nition
a subset c     {0, 1}n is an (n, m)-linear threshold code (ltc) i   
there exist n linear threshold functions fi : {0, 1}m     {0, 1}, i     [n]
with

{(f1(x), f2(x), . . . , fn(x))     {0, 1}n : x     {0, 1}m} = c.

if the functions fi can be chosen self-dual (hyperplanes are central),
then c is called homogeneous.

jason morton (penn state)

algebraic deep learning

7/19/2012

93 / 103

11553377226644strong modes and linear threshold codes

on the other hand,
for codes c     {0, 1}n,|c| = 2m of minimum distance two,
when c is a homogeneous linear threshold code (ltc)
then rbmn,m can represent a distribution with strong modes c.
and, if rbmn,m can represent a distribution with strong modes
c, then c is a ltc.

jason morton (penn state)

algebraic deep learning

7/19/2012

94 / 103

combining these results gives our answer to

problem
given some n, m     n, what is the smallest k     n for which the
k-mixture of product distributions on n binary variables contains the
rbm model with n visible and m hidden units?

namely, if 4(cid:100)m/3(cid:101)    n, then mn,k     rbmn,m if and only if
k     2m;
and if 4(cid:100)m/3(cid:101) > n, then mn,k     rbmn,m only if
k     min{2l +m   l, 2n   1}, where l is max{l     n : 4(cid:100)l/3(cid:101)    n}.
thus an exponentially larger mixture model, with an
exponentially larger number of parameters, is required to
represent distributions that can be represented by the rbm.

there   s another way to see that the rbm has points of rank 2m
when 2m     n

jason morton (penn state)

algebraic deep learning

7/19/2012

95 / 103

not very often

figure: plot of the smallest k for which mn,k contains rbmn,m. fixing
dimension (grey line), the rbms which are hardest to represent as
mixtures are those where m/n     1.

jason morton (penn state)

algebraic deep learning

7/19/2012

96 / 103

mnlog2(k)=m34n   log2(k)   n   1341dim(rbm)=const.1 algebraic geometry of tensor networks

tensors
tensor networks
algebraic geometry

2 algebraic description of id114

review of gm de   ninitions
algebraic and semialgebraic descriptions
restricted id82s

3

identi   ability, singular learning theory, other perspectives

identi   ability
singular learning theory

jason morton (penn state)

algebraic deep learning

7/19/2012

97 / 103

identi   ability: uniqueness of parameter estimates

a parameterization of a set of id203 distributions is
identi   able if it is injective.
a parameterization of a set of id203 distributions is
generically identi   able if it is injective except on a proper
algebraic subvariety of parameter space.
identi   ability questions can be answered with algebraic geometry
(e.g. many recent results in phylogenetics)
a weaker question: what conditions guarantee generic
identi   ability up to known symmetries?
a still weaker question: is the dimension of the space of
representable distributions (states) equal to the expected
dimension (number of parameters)? or are parameters wasted?

jason morton (penn state)

algebraic deep learning

7/19/2012

98 / 103

uniqueness up to known symmetries and normal
forms

identify internal symmetries (here sl2)
reparameterize to choose a normal form

jason morton (penn state)

algebraic deep learning

7/19/2012

99 / 103

singular learning theory
a model is more than its implicitization; the parameterization map is
critically important to learning performance and quality.

how fast and how well can a model learn?

when a statistical model is regular, we can use central limit
theorems to    gure out their behavior for large data.
but most hidden variable models are not regular (identi   able w/
positive de   nite fisher information matrix) but singular.
singular learning theory [watanabe 2009] o   ers one avenue for
progress in this situation based on algebraic geometry.
asymptotics, generalization error, etc are governed by the real
log canonical threshold.
resolve the model singularities and develop new limit theorems.

jason morton (penn state)

algebraic deep learning

7/19/2012

100 / 103

comparing architectures

/.
()

        

        

        
/.
()

        
        
        
        
        
        
        
        
        
        
        
        
        
        
        

        
-,
*+

        

        

-,
*+

rbm

        
/.
        
        
        
        
-,
()
*+
        
/.
        
        
        
        
-,
()
*+
        
        
        
        
        
        
/.
-,
()
*+
        
        
        
        
        
        
        
/.
-,
()
*+

treelike

(9,18): 189 parameters

(9,8,6,6): 185 parameters

/.
()

/.
        
        
()
        
        
        
        
        
/.
        
        
()

        
        
        

        
-,
        
*+
        
        
        
        
        
        
        
-,
*+

-,
*+

bulge

/.
()
/.
()
/.
()

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

        
        
        

-,
*+
-,
*+
-,
*+

column

(9,11,6): 191 parameters

(9,9,9): 189 parameters

jason morton (penn state)

algebraic deep learning

7/19/2012

101 / 103

 
 
o
o
o
o
 
 
 
 
o
o
 
 
o
o
 
 
o
o
 
 
o
o
 
 
o
o
 
 
o
o
optimal architectures for learning

the real log canonical threshold   q of a parameterization at true
distribution q = p(x|  ) determines bayes generalization error,
gn(q) = eq[kl(q||p   n)]     sq =   q
n ) [watanabe 2009].

n + o( 1

(cid:73) expected kl-divergence
(cid:73) from the true model to the predicted distribution p   
(cid:73) after seeing n observations and updating to the posterior.
e.g. what do we have to believe about which qs appear in
nature for the deep model to be better,   rbm(q) >   col(q)?
techniques for calculating    are rapidly evolving; known for
simple binary id114 such as trees [zwiernik 2011].

jason morton (penn state)

algebraic deep learning

7/19/2012

102 / 103

advertisement

modern applications of representation theory

(cid:73) an ima pi graduate summer school
(cid:73) at the university of chicago
(cid:73) summer 2014

    12 lectures on tensor networks

jason morton (penn state)

algebraic deep learning

7/19/2012

103 / 103

