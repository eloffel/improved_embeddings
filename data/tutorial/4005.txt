density estimation

computing, and avoiding, partition functions

roadmap:
    motivation: density estimation
    understanding annealing/tempering
    nade

iain murray
school of informatics, university of edinburgh

includes work with ruslan salakhutdinov and hugo larochelle

probabilistic model h

predict new images: p (x|h)

high density can be boring

p (x|h)

image reconstruction

observation model: p (y | x)

underlying image:

p (x| y)     p (y | x) p (x)

(e.g., zoran and weiss, 2011; lucas theis   s work)

roadmap

    unsupervised learning and p (x|h)
    evaluating p (x|h)

salakhutdinov and murray (2008)

murray and salakhutdinov (2009)

wallach, murray, salakhutdinov & mimno (2009)

    nade:    density estimation put    rst   

larochelle and murray (2011)

restricted id82s

p (x, h|   ) =

1
z(  )

exp(cid:104)(cid:80)ij wijxihj +(cid:80)i bx

j hj(cid:105)
i xi +(cid:80)j bh

p (x|   ) =(cid:88)h

p (x, h|   ) =

1

z(  )(cid:88)h
(cid:124)

exp [       ]

(cid:123)(cid:122)

(cid:125)

f (x;   ), tractable

z a normalizer

p(x) =

f (x)
z

x     uniform

x     model

xannealing / tempering

e.g. p (x;   )     p    (x)     (x)(1     )

   = 0

   = 0.01

   = 0.1

   = 0.25

   = 0.5

   = 1

1/   =    temperature   

annealed importance sampling

z

p    (xk)

p(x) =

k(cid:89)k=1(cid:101)tk(xk   1; xk),

k(cid:89)k=1
standard importance sampling of p(x) = p   (x)
z

q(x) =   (x0)

tk(xk; xk   1)

with q(x)

x0   p0(x)p(x):x0x1x2xk   1xk  t1  t2  tkxk   pk+1(x)q(x):x0x1x2xk   1xkt1t2tkannealed importance sampling

z    

1
s

s(cid:88)s=1

p   (x)
q(x)

q   

   p

 10   100  500 1000 10000252253254255256257258259number of ais runslog z  large variance20 sec3.3 min17 min33 min5.5 hrsestimated logztrue logzparallel tempering

standard mcmc: transitions + swap proposals on joint:

p (x) =(cid:89)  

p (x;   )

    larger system
    information from low    di   uses up by slow random walk

p(x)p  1(x)p  2(x)p  3(x)t1t  1t  2t  3t1t  1t  2t  3t1t  1t  2t  3t1t  1t  2t  3t1t  1t  2t  3tempered transitions

drive temperature up. . .

. . . and back down

proposal: swap order,    nal point   x0 putatively     p (x)
acceptance id203:

min(cid:34)1,

p  1(  x0)
p (  x0)       

p  k(  xk   1)
p  k   1(  x0)

p  k   1(  xk   1)
p  k(  xk   1)       

p (  x0)

p  1(  x0)(cid:35)

  x0   p(x)p(x):  x0  x1  x2  xk   1  xk  xk   1  x2  x1  x0  t  1  t  2  t  k  t  k  t  2  t  1summary on z

whirlwind tour of annealing / tempering

must be able to get anywhere in distribution

methods to use generally for hardest problems.

an experiment

take 60,000 binarized mnist digits, like these:

    train an rbm using cd (and then    nd z)
    train a mixture of multivariate bernoullis with em

compare samples and test-set log-probs

a comparison

samples from:
    mixture of bernoullis,    143 nats/test digit
    rbm,    106 nats/test digit

which is which?

a better    tted rbm

rbm samples

training set examples

test log-prob now 20 nats better (   86 nats/digit)

dependent latent variables

   deep belief net   

lateral connections

directed model

p (x) =

1

z (cid:88)h

p    (x, h), not available

chib-style estimates

bayes rule:

p (h| x) =

p (h, x)

p (x)

for any special state h   :

p (x) =

p (h   , x)
p (h    | x)     estimate

murray and salakhutdinov (2009)

variational approach

p    (x, h)

1
z

log p (x) = log(cid:88)h
    (cid:88)h

q(h) log p    (x, h)     log z + h[q(h)]

h(2)h(1)xresults mnist

510152025303540   87   86.5   86   85.5   85number of markov chain stepsestimated test log   id203estimate of variational lower boundais estimatorour proposed estimatorresults natural scenes

510152025303540   585   580   575   570   565number of markov chain stepsestimated test log   id203estimate of variational lower boundais estimatorour proposed estimatorp (x|h) taught me

    rbm: state-of-the-art for binary dists

    deep nets only very slightly better on mnist

    some gaussian rbms are really bad...
...and going deep won   t help

    most topic model p (x|h) ests wrong

roadmap

    unsupervised learning and p (x|h)
    evaluating p (x|h)

salakhutdinov and murray (2008)

murray and salakhutdinov (2009)

wallach, murray, salakhutdinov & mimno (2009)

    nade:    density estimation put    rst   

larochelle and murray (2011)

decompose into scalars

p (x) = p (x1) p (x2 | x1) p (x3 | x1, x2) . . .

=(cid:89)k

p (xk | x<k)

fully visible id110s   good at estimating       (tractable)   not as good a model as rbmsw  xxckx1x2x3x4general graphical modelfully visible sigmoid belief net (fvsbn)p(x)p(x)=   kp(xk|x<k)   xkp(xk=1|x<k)fvsbn: fully visible sigmoid belief net

id28 for conditionals

fvsbns beat mixtures, but not rbms

fully visible id110s   good at estimating       (tractable)   not as good a model as rbmsw  xxckx1x2x3x4general graphical modelfully visible sigmoid belief net (fvsbn)p(x)p(x)=   kp(xk|x<k)   xkp(xk=1|x<k)approximate rbm

p (xk | x<k) from mcmc

or mean    eld

(requires    tted rbm. creates new model.)

one mean field step

  xk =   (cid:0)bx
k + wk,.h(k)(cid:1)
h(k) =   (cid:0)bh + w (cid:62).,<kx<k(cid:1)

neural autoregressive distribution estimator (nade)   we turn this into an ef   cient autoencoder by:1.by using only 1 up-down iteration2.untying the up and down weights3.by    tting                to the datah(k)=sigm(b+w  ,<kx<k)   xk=sigm   ck+vk,  h(k)   wvx   xhhhh(1)(2)(3)(4)p(xk|x<k)nade neural autoregressive distribution estimator

fit as new model

p (x|h) =(cid:81)k   p(xk)

tractable, o(dh)

neural autoregressive distribution estimator (nade)   we turn this into an ef   cient autoencoder by:1.by using only 1 up-down iteration2.untying the up and down weights3.by    tting                to the datah(k)=sigm(b+w  ,<kx<k)   xk=sigm   ck+vk,  h(k)   wvx   xhhhh(1)(2)(3)(4)p(xk|x<k)nade results

experimentsmanuscriptunderreviewbyaistats2011table1:distributionestimationresults.tonormalizetheresults,theaveragetestlog-likelihood(all)foreachmodelonagivendatasetwassubtractedbytheallofmob(whichisgiveninthelastrowunder   id172   ).95%con   denceintervalsarealsogiven.thebestresultaswellasanyotherresultwithanoverlappingcon   denceintervalisshowninbold.modeladultconnect-4dnamushroomsnips-0-12ocr-lettersrcv1webmob0.000.000.000.000.000.000.000.00  0.10  0.04  0.53  0.10  1.12  0.32  0.11  0.23rbm4.180.751.29-0.6912.65-2.49-1.290.78  0.06  0.02  0.48  0.09  1.07  0.30  0.11  0.20rbm4.15-1.721.45-0.6911.250.99-0.040.02mult.  0.06  0.03  0.40  0.05  1.06  0.29  0.11  0.21rbforest4.120.591.390.0412.613.780.56-0.15  0.06  0.02  0.49  0.07  1.07  0.28  0.11  0.21fvsbn7.2711.0214.554.1913.141.26-2.240.81  0.04  0.01  0.50  0.05  0.98  0.23  0.11  0.20nade7.2511.4213.384.6516.9413.340.931.77  0.05  0.01  0.57  0.04  1.11  0.21  0.11  0.20id172-20.44-23.41-98.19-14.46-290.02-40.56-47.59-30.16tomeasurethesensitivityofnadetotheorderingoftheobservationswetrainedadozenseparatemodelsforthemushrooms,dnaandnips-0-12datasetsusingdi   erentrandomshu   ings.wethencomputedthestandarddeviationofthetwelveassociatedtestlog-likelihoodaverages,foreachofthedatasets.standarddeviationsof0.045,0.050and0.150wereobservedonmushrooms,dnaandnips-0-12respectively,whichisquitereasonablewhencomparedtotheintrinsicuncertaintyassociatedwithusinga   nitetestset(seethecon   denceintervalsoftable1).hence,itdoesnotseemnecessarytooptimizetheorderingoftheobservationvariables.onecouldtrytoreducethevarianceofthelearnedso-lutionbytraininganensembleofseveralnademodelsondi   erentobservationorderings,whilesharingtheweightmatrixwacrossthosemodelsbutusingdi   er-entoutputmatricesv.whilewehaven   textensivelyexperimentedwiththisvariant,wehavefoundsuchsharingtoproducebetter   lterswhenusedonthebinarizedmnistdataset(seenextsection).6.2nadevs.anintractablerbmwhilenadewasinspiredbytherbm,doesitsper-formancecomeclosetothatoftherbminitsmosttypicalregime,i.e.withhundredsofhiddenunits?inotherwords,wastractabilitygainedwithalossinperformance?toanswerthesequestions,wetrainednadeonabinarizedversionofthemnistdataset.thisver-sionwasusedbysalakhutdinovandmurray(2008)totrainrbmswithdi   erentversionsofcontrastivedi-vergenceandevaluatethemasdistributionestimators.sincethepartitionfunctioncannotbecomputedex-actly,itwasapproximatedusingannealedimportancesampling.thismethodestimatesthemeanofsomeunboundedpositiveweightsbyanempiricalmeanofsamples.itisn   tpossibletomeaningfullyupper-boundthepartitionfunctionfromtheseresults:thetruetestlog-likelihoodaveragescouldbemuchsmallerthanthevaluesanderrorbarsreportedbysalakhutdinovandmurray(2008),althoughtheirapproximationswereshowntobeaccurateinatractablecase.rbmswith500hiddenunitswerereportedtoob-tain   125.53,   105.50and   86.34inaveragetestlog-likelihoodwhentrainedusingcontrastivedivergencewith1,3and25stepsofgibbssampling,respectively.incomparison,nadewith500hiddenunits,alearn-ingrateof0.0005andadecreaseconstantof0obtained   88.86.thisisalmostasgoodasthebestrbmclaimandmuchbetterthanrbmstrainedwithjustafewstepsofgibbssampling.again,italsoimprovesovermixturesofbernoulliswhich,with10,100and500com-ponentsobtain   168.95,   142.63and   137.64averagetestlog-likelihoodsrespectively(takenfromsalakhut-dinovandmurray(2008)).finally,fvsbntrainedbystochasticgradientdescentachieves   97.45andimprovesonthemixturemodelsbutnotonnade.itthenappearsthattractabilitywasgainedatalmostnocostintermsofperformance.wearealsocon   dentthatevenbetterperformancecouldhavebeenachievedwithabetteroptimizationmethodthanstochasticgra-dientdescent.indeed,thelog-likelihoodonthetraining   little variation when changing input ordering:  dna = +/- 0.05   mushrooms = +/- 0.045   nips-0-12 = +/- 0.15nade results

   on a binarized version of mnist:experimentsmodellog. like.mob-137.64  rbm (cd1)-125.53  rbm (cd3)-105.50    rbm (cd25)-86.34fvsbn-97.45nade-88.86manuscriptunderreviewbyaistats2011figure2:(left):samplesfromnadetrainedonabinaryversionofmnist.(middle):probabilitiesfromwhicheachpixelwassampled.(right):visualizationofsomeoftherowsofw.this   gureisbetterseenonacomputerscreen.setitto0,toobtain:0=   kl(q(vi,v>i,h|v<i)||p(vi,v>i,h|v<i))     k(i)0=   ck   wk,    (i)+log     k(i)1     k(i)     k(i)1     k(i)=exp(ck+wk,    (i))  k(i)=exp(ck+wk,    (i))1+exp(ck+wk,    (i))  k(i)=sigm      ck+   j   iwkj  j(i)+   j<iwkjvj      whereinthelaststepwehavereplacedthema-trix/vectormultiplicationwk,    (i)byitsexplicitsum-mationformandhaveusedthefactthat  j(i)=vjforj<i.similarly,wesetthederivativewithrespectto  j(i)forj   ito0andobtain:0=   kl(q(vi,v>i,h|v<i)||p(vi,v>i,h|v<i))     j(i)0=   bj     (i)   w  ,j+log     j(i)1     j(i)     j(i)1     j(i)=exp(bj+  (i)   w  ,j)  j(i)=exp(bj+  (i)   w  ,j)1+exp(bj+  (i)   w  ,j)  j(i)=sigm   bj+   kwkj  k(i)   wethenrecoverthemean-   eldupdatesofequa-tions7and8.referencesbengio,y.,&bengio,s.(2000).modelinghigh-dimensionaldiscretedatawithmulti-layerneuralnetworks.advancesinneuralinformationprocess-ingsystems12(nips   99)(pp.400   406).mitpress.bengio,y.,lamblin,p.,popovici,d.,&larochelle,h.(2007).greedylayer-wisetrainingofdeepnetworks.advancesinneuralinformationprocessingsystems19(nips   06)(pp.153   160).mitpress.chen,x.r.,krishnaiah,p.r.,&liang,w.w.(1989).estimationofmultivariatebinarydensityusingor-thogonalfunctions.journalofmultivariateanalysis,31,178   186.freund,y.,&haussler,d.(1992).afastandexactlearningruleforarestrictedclassofboltzmannma-chines.advancesinneuralinformationprocessingsystems4(nips   91)(pp.912   919).denver,co:morgankaufmann,sanmateo.frey,b.j.(1998).graphicalmodelsformachinelearn-inganddigitalcommunication.mitpress.frey,b.j.,hinton,g.e.,&dayan,p.(1996).doesthewake-sleepalgorithid113arngooddensityestimators?advancesinneuralinformationprocessingsystems8(nips   95)(pp.661   670).mitpress,cambridge,ma.hinton,g.e.(2002).trainingproductsofexpertsbyminimizingcontrastivedivergence.neuralcomputa-tion,14,1771   1800.hinton,g.e.,osindero,s.,&teh,y.(2006).afastlearningalgorithmfordeepbeliefnets.neuralcomputation,18,1527   1554.larochelle,h.,&bengio,y.(2008).classi   cationusingdiscriminativerestrictedboltzmannmachines.pro-ceedingsofthe25thannualinternationalconferencesamplesintractable{         *****: taken from salakhutdinov and murray (2008)talking points

when should we learn p (x|h)?
monte carlo methods

autoregressive models

(see also lucas theis)

a longer talk on nade:
http://videolectures.net/aistats2011_larochelle_neural/

appendix slides

markov chain estimation

stationary condition for markov chain:

p (h   |x) =(cid:88)h
   (cid:34) 1

s

t (h       h)p (h|x)
t (h       h(s)), h(s)     p(h)(cid:35) =   p
s(cid:88)s=1

p(h) draws a sequence from an equilibrium markov chain:

h(1)   p(h|v)h(1)h(2)h(3)h(4)h(5)ttttbias in answer

p (x) =

p (h   , x)
p (h   |x)

=

p (h   , x)

e[  p]     e(cid:20)p (h   , x)

  p

(cid:21)

idea: bias markov chain by starting at h   

s=1 t (h       h(s)) will often overestimate p (h   |x)

1

s(cid:80)s

q(h) =

t (h(1)    h   )
p (h(1)|x) p(h)

h   h(1)h(2)h(3)h(4)h(5)tttttnew estimator

we actually need a slightly more complicated q:

q(h) =

1
s

s(cid:88)s=1 (cid:101)t (h(s)    h   )
p (h(s)|x) p(h)
q(h)(cid:34)1(cid:44) 1
s(cid:88)s=1

e

s

t (h       h(s))(cid:35) =

1

p (h   |x)

  p (x) =

p (x, h   )
  p (h   |x)

unbiased     stochastic lower bound on log p (x)

h   h(1)h(2)h(3)h(4)h(5)etetetett