scattering invariant deep networks

for classification

   

 st  phane mallat

ihes 

ecole polytechnique

wednesday, july 18, 2012

      image classification

caltech 101:
anchor

joshua tree

beaver

lotus

water lily

    considerable variability in each class. 
    a euclidean norm does not measure signal   similarities  .

wednesday, july 18, 2012

     metric for classification

    classification requires finding a metric to compare signals, with:

- small distances d(f, g) within a class
- large distances d(f, g) across classes.

    if one finds a representation            such that

  (f)

d(f, g) = !  (f)       (g)! (kernel metric)

then the classification may be linearized (id166, pca,...). 

    is there an appropriate kernel metric, which     ?  

wednesday, july 18, 2012

convolutional networks and applications in vision

yann lecun, koray kavukcuoglu and cl  ement farabet

    a view of convolution networks

computer science department, courant institute of mathematical sciences, new york university

{yann,koray,cfarabet}@cs.nyu.edu

y. lecun et. al.

abstract    intelligent tasks, such as visual perception, auditory
perception, and language understanding require the construction
of good internal representations of the world (or    features   ),
which must be invariant to irrelevant variations of the input
while, preserving relevant information. a major question for
machine learning is how to learn such good features auto-
matically. convolutional networks (convnets) are a biologically-
inspired trainable architecture that can learn invariant features.
each stage in a convnets is composed of a    lter bank, some
non-linearities, and feature pooling layers. with multiple stages,
a convnet can learn multi-level hierarchies of features. while
convnets have been successfully deployed in many commercial
applications from ocr to video surveillance, they require large
amounts of labeled training samples. we describe new unsu-
pervised learning algorithms, and new non-linear stages that
allow convnets to be trained with very few labeled samples.
applications to visual object recognition and vision navigation

one of the key questions of vision science (natural and
arti   cial) is how to produce good internal representations of
the visual world. what sort of internal representation would
allow an arti   cial vision system to detect and classify objects

x

  (x)

fig. 1. a typical convnet architecture with two feature stages

id166

fig. 2. an example of feature extraction stage of the type f    rabs   n    pa.
an input image (or a feature map) is passed through a    lter bank, followed
by abs(gi. tanh()), local subtractive and divisive contrast id172, and
spatial pooling/sub-sampling.

    deep convolution networks are very efficient image and audio 

extracted at all locations on the input. each stage is composed
of three layers: a    lter bank layer, a non-linearity layer, and a
feature pooling layer. a typical convnet is composed of one,
two or three such 3-layer stages, followed by a classi   cation
module. each layer type is now described for the case of image
recognition.
filter bank layer - f : the input is a 3d array with n1 2d
feature maps of size n2   n3. each component is denoted xijk,
and each feature map is denoted xi. the output is also a 3d
array, y composed of m1 feature maps of size m2    m3. a
trainable    lter (kernel) kij in the    lter bank has size l1    l2

participants in the recent darpa-sponsored lagr program
on vision-based navigation for off-road robots used convnets
for long-range obstacle detection [22], [23]. in [22], the system
is pre-trained off-line using a combination of unsupervised
learning (as described in section ii) and supervised learning.
it is then adapted on-line, as the robot runs, using labels
provided by a short-range stereovision system (see videos at
http://www.cs.nyu.edu/ yann/research/lagr). inter-
esting new applications include image restoration [24] and
image segmentation, particularly for biological images [25].
the big advantage over mrfs is the ability to take a large
context window into account. stunning results were obtained

the resolution [5], [6]. in some recent versions of convnets,
the pooling also pools similar feature at the same location, in
addition to the same feature at nearby locations [7].

supervised training is performed using a form of stochastic
id119 to minimize the discrepancy between the

classifiers: why ?  

wednesday, july 18, 2012

   representation for classification

    what principles to construct such representations ?

    deep convolution networks:

    why convolutions ? 
    which filters ? 
    why multistage and how deep ?
    why pooling ? how to pool ?
    why non-linear, which non-linearities ?
    why normalizing ?
    what is the role of sparsity ?

    what are the underlying useful mathematics ?

wednesday, july 18, 2012

        texture discrimination

    textures define high-dimensional image classes.
    realizations of stationary processes      but typically not gaussian, 
not markovian and not characterized by second order moments.

x

same power spectrum

same power spectrum

wednesday, july 18, 2012

       audio textures
j. mcdermott textures

    natural sounds (1s)   original            gaussian model

    hammer
    insect
    water
    applause

wednesday, july 18, 2012

     the best image classifier

wednesday, july 18, 2012

 psychophysics of vision

hypercolumns in v1:
  directional wavelets

complex cells
    non-linear
    large receptive fields
    some forms of invariance

simple cells gabor linear modelsfigure 2: adapted from (hubel and wiesel, 1962).

[wolf et al.] 

  (x) =   (x)ei  x

wednesday, july 18, 2012

   what   pathway towards v4:
    more specialized invariance
      grand mother cells  

following hubel and wiesel, we say that the simple cells are tuned to a particular preferred
feature. this tuning is accomplished by weighting the lgn inputs in such a way that a simple
cell    res when the inputs arranged to build the preferred feature are co-activated. in contrast,
the complex cells    inputs are weighted such that the activation of any of their inputs can drive
the cell by itself. so the complex cells are said to pool the response of several simple cells. as a
visual signal passes from lgn to v1 its representation increases in selectivity, patterns without
edges (such as su   ciently small circular dots of light) are no longer represented. then as the
signal passes from simple cells to complex cells the representation gains in invariance. complex
cells downstream from simple cells that respond only when their preferred feature appears in a
small window of space now represent stimuli presented over a larger region.

    audio psychophysics

reduction of
processing rate

 cochlea:
dilated wavelet filters

    wavelets appear at early stages of vision and audition.
why ?

wednesday, july 18, 2012

0

  

 low-level signal representation

    low-level signal processing: 

    compression/id205 for storage and transmission
    inverse problems from partial and degraded measurements

    a key idea: find sparse accurate representations with few 

parameters. 

    mathematical tools: fourier transform, wavelet bases, adaptive 
dictionary representations, variational formulations...                   
a relatively well understood framework.

    classification problems: discriminate not reconstruct.
    different problems where sparsity yields instabilities.

wednesday, july 18, 2012

  analysis versus synthesis
   how to construct a sparse representation ?

   what about stability ?

wednesday, july 18, 2012

      image classification

caltech 101:
anchor

joshua tree

beaver

lotus

water lily

    considerable variability in each class. 
    reduce variability means constructing invariants.

wednesday, july 18, 2012

     signal classification
    very high dimensional space n     106.
    few training samples per class p ! n.
    signals do not belong to a low-dimensional manifold.
classi   er
supervised learning

unsupervised learning

space dimension

106        

lower dimensional manifold

  
- sift
- mfcc

!x     x!!

x x!

  (x!)
!  (x)       (x!)!

  (x)

reduce variability due to
translations, transposition (audio)
rotations, scaling (images)
action of groups

wednesday, july 18, 2012

  3 linear
+ ranking
- id166
- pca

  2

- gmm
- bag of features
- eigenmaps
- dictionary learning
- id91
reduce structural
variability

     stable translation invariants

    invariance to translations xc(t) = x(t     c)

   c     r ,   (xc) =   (x) .

    metric stability with deformations x   (t) = x(t       (t))

small deformations of x =    small modi   cations of   (x)

      , "  (x   )       (x)"     c sup

t

|     (t)|"x" .

    preserve information

wednesday, july 18, 2012

deformation size

        overview

    part 1: invariance and deformation stability 
    fourier failure
    wavelet stability to deformations
    scattering invariants and deep convolution networks
    mathematical properties of deep scattering networks
    classification of images 

    part 2: inverse, textures and multiple invariants
    inverse scattering by phase retrieval and sparsity
    scattering models of stationary processes
    texture classification
    invariants over multiple groups: transposition, rotation, scaling

wednesday, july 18, 2012

fourier & correlation invariance

    fourier transform   x(  ) =! x(t) e   i  t dt
    translation invariance: if xc(t) = x(t     c) then

|!xc(  )| = |  x(  )|

    for the auto-correlation cx(u) =! x(t) x(t     u) dt

cx(u) = cxc(u) .

wednesday, july 18, 2012

   fourier & correlation instabilities

    instabilites to small deformations x   (t) = x(t       (t)) :

is big at high frequencies

||  x   (  )|    |   x(  )||
    "|  x  |2     |  x|2" = "cx       cx" is big .

  

  x(  )

  (t) =   t
  x   (  )

unstable

stable

t

wednesday, july 18, 2012

         wavelet transform

    dilated wavelets:     (t) = 2   jq   (2   jqt) with    = 2   jq .

    !(t)

    (t)

|     (  )|2

|       (  )|2

|       !(  )|2

quadrature pairs

0

  

  !

  

q-constant band-pass    lters       

    wavelet transform: w x(t) =!x !   (t) , x !     (t)"  
|       (  )|2 = 1 then w is unitary :
    if
!x !     !2 = !x!2 .

|     (  )|2 +!  
!w x!2 = !x !   !2 +!  

wednesday, july 18, 2012

         wavelet transform

    for images, dilated and rotated wavelets:
    (t) = 2j   (2jrt) with    = 2jr

  2

|       (  )|2

  1

[wolf et al.] 

    wavelet transform: w x(t) =!x !   (t) , x !     (t)"  
|       (  )|2 = 1 then w is unitary :
    if

|     (  )|2 +!  

wednesday, july 18, 2012

         wavelet stabilization

window fourier

  

!|x !     (t)|"  

wavelet time-frequency
  
      

!|x !     | !   (t)"  

time/space averaging
  

  (t     u)

t

wednesday, july 18, 2012

t

  (t     u)

t

locally invariant to translations
and stable to deformations

mfsc (audio)
sift (images)

but loss of information.

         wavelet stabilization

window fourier

  

!|x !     (t)|"  

wavelet time-frequency

!|x !     | !   (t)"  

time/space averaging
  

  

locally invariant to translations
and stable to deformations

mfsc (audio)
sift (images)

but loss of information.

wednesday, july 18, 2012

         wavelet stabilization

!|x !     (t)|"  

wavelet time-frequency

!|x !     | !   (t)"  

time/space averaging

370ms window

non-linearity is needed to
have a non-zero invariant
a modulus is    optimal   

wednesday, july 18, 2012

locally invariant to translations
and stable to deformations

mfsc (audio)
sift (images)

but loss of information.

      stable translation invariance

x !     (t)

: translation covariant, not invariant, and

t

! x !     (t) dt = 0
    translation invariant representation: ! m(x !     )(t) dt
    di   eomorphism stability: m commutes with di   eomorphisms.
    l2 stability: !m h! = !h! and !m g     m h!     !g     h!
    m(h)(t) = |h(t)| =!|hr(t)|2 + |hi(t)|2
|x !     1(t)|
    a modulus computes a
lower frequency envelop
    stable invariant:! |x !     (t)| dt = !x !     !1 .

wednesday, july 18, 2012

    recovering lost information
|x !     1(t)|
x !     1(t)

    a modulus computes a
lower frequency envelop

    the averaging |x !     1| !    removes high frequencies:

    (  )

!|x !     1|(  )

      2(  )

must recover high frequencies:
stable modulation spectrum

  

    wavelet transform:
    translation invariance by time averaging the amplitude:

{|x !     1| !     2}  2

     1 ,   2 ,

|| x "     1| "     2| "   

: stable to deformations

wednesday, july 18, 2012

     windowed scattering

for any path p = (  1,   2, ...,   m) of order m

s[p]x(t) = ||x !     1| !     2| ...| !     m| !   (t)

a window of size n yields o(qm logm n) coe   cients of order m.

first two orders:

t  orsten dau model

1 channel
x !     1

|x !     1|
q log n channels

fig. 2. transfer functions of the modulation    lters. in the range 0   10 hz
the functions have a constant bandwidth of 5 hz. between 10 and 1000 hz
a logarithmic scaling with a constant q value of 2 is applied. only the range
from 0 to 200 hz is plotted.

wednesday, july 18, 2012

fig. 1. block diagram of the psycho-acoustical model for describing
modulation-detection data with an optimal detector as decision device. the
signals are preprocessed, subjected to adaptation,    ltered by a modulation

||x !     1| !     2|
(q log n)2 channels

ters. in the range 0   10 hz a constant bandwidth of 5 hz is
assumed. the lowest modulation    lter represents a low-pass
   lter with a cutoff frequency of 2.5 hz. from 10 hz up to
1000 hz a logarithmic scaling with a constant q value of 2 is
assumed.2 the spacing in the modulation-frequency domain
resembles the spacing of critical bands in the audio-
frequency domain. within the model only the    hilbert   
envelope of the modulation    lter outputs for center frequen-
cies above 10 hz is further examined, introducing a nonlin-
earity in the processing of amplitude modulation.3 for    lters
with a lower center frequency it is not reasonable to extract
the hilbert envelope from the signal, because the distinction

     deep convolution network

y. lecun et. al.

    iteration on u x = {x !    , |x !     |}   , contracting.

x !    ,

x !  

x

|x !     1|

= |x !     1| !   

s[  1]x
||x !     1| !     2|

s[  1,   2]x

|||x !     1| !     2| !     3|

    output at all layers:
mfsc and sift are 1st layer outputs: s[  1]x

{s[p]x}p   p .

wednesday, july 18, 2012

      amplitude modulations

    amplitude modulations such as tremolos or attacks:
  (t     n/  1)

x(t) = h !e (t) . a(t) with e(t) =!n

  h(  ): formant,

  1 : pitch, a(t): amplitude modulation

    pitch harmonics: if   1 = k   1 then

s[  1]x(t) = |x "     1| "   (t) = |  h(  1)| a "   (t)

    amplitude modulation spectrum:

s[  1,   2]x(t) = ||f "     1| "     2| "   (t) = |  h(  1)| |  a(  2)|

wednesday, july 18, 2012

        amplitude modulation

log(  1)

)

1
!
(
g
o

l

log(  1)

512ms window

first   order windowed scattering (small scale)

|x !     1|(t)

t

first   order windowed scattering (large scale)

s[  1]x(t) = |x "     1| "   (t)

n  
  h(  1)

1977 hz

s[  1,   2]x(t) = ||x "     1| "     2| "   (t) for   1 = 1977

second   order windowed scattering (large scale) band #75

t

t

18 hz

t

)

1
!
(
g
o

l

log(  2)

)

2
!
(
g
o

l

wednesday, july 18, 2012

      frequency modulated  sounds

    frequence modulations such as vibratos:

x(t) = h !!e(t) with !e(t) ="n

  h(  ): formant,

  1: pitch,

  (t       cos   2t     n/  1) .

  2: vibrato frequency.

    pitch harmonics: if   1 = k   1 then

s[  1]x(t) = |  h(  1)|

    vibrato harmonics: if   2 = l   2 then
s[  1,   2]x(t) = cl |  h(  1)|  2l   2l

2

wednesday, july 18, 2012

        frequency modulation

log(  1)

)

1
!
(
g
o

l

log(  1)

512ms window

first   order windowed scattering (small scale)

|x !     1|(t)

t

first   order windowed scattering (large scale)

s[  1]x(t) = |x "     1| "   (t)

n  
  h(  1)

1977 hz

s[  1,   2]x(t) = ||x "     1| "     2| "   (t) for   1 = 1977

second   order windowed scattering (large scale) band #75

t

t

18 hz

t

)

1
!
(
g
o

l

log(  2)

)

2
!
(
g
o

l

wednesday, july 18, 2012

           interferences

x(t) =!m

am cos(  mt)

  x(  )

0

      (  )

  

|x !     (t)|2 = e2

   + !m!!=m

interferences :

cm,m! cos(  m       m!)t

music chord :

c major

minor 3rd
  3       2
  2       1
major 3rd

perfect 5th
  3       1

wednesday, july 18, 2012

!|x !     1|(  )
      2(  )

0

  

        arpeggio

first   order windowed scattering (small scale)

|x !     1|(t)

t

first   order windowed scattering (large scale)

s[  1]x(t) = |x "     1| "   (t)

s[  1,   2]x(t) = ||x "     1| "     2| "   (t) for   1 = 1977

second   order windowed scattering (large scale) band #72

t

t

t

2000 hz

131 hz

log(  1)

)

1
!
(
g
o

l

log(  1)

1 s

)

1
!
(
g
o

l

log(  2)

)

2
!
(
g
o

l

wednesday, july 18, 2012

    sounds with same spectrum 

x: stationary process

j. mcdermott

first   order windowed scattering (small scale)

|x !     1|(x)

fourier
spectrum
  

log(  1)

)

1
!
(
g
o

l

log(  1)

2s window

)

1
!
(
g
o

l

log(  2)

)

2
!
(
g
o

l

wednesday, july 18, 2012

t

first   order windowed scattering (large scale)

s[  1]x(t) = |x "     1| "   (t)

s[  1,   2]x(t) = ||x "     1| "     2| "   (t) for   1 = log(1122)

second   order windowed scattering (large scale) band #51

t

t

t

t

t

    image wavelet scattering

fourier

  f

  1

  1

  2

  2

wavelet scattering

|f !     1| !   

||f !     1| !     2| !   

sift

window size = image size

images

f

wednesday, july 18, 2012

             textures with same spectrum
x: stationary process
textures

wavelet scattering

fourier

power spectrum

x

|x !     1| !   

||x !     1| !     2| !   

  1

  1

  2

  2

wednesday, july 18, 2012

window size = image size

     deep convolution network

y. lecun et. al.

    iteration on u x = {x !    , |x !     |}   , contracting.

x !    ,

x !  

x

|x !     1|

= |x !     1| !   

s[  1]x
||x !     1| !     2|

s[  1,   2]x

|||x !     1| !     2| !     3|

    output at all layers:

{s[p]x}p   p .

wednesday, july 18, 2012

      scattering  properties

for any path p = (  1,   2, ...,   m) of order m

s[p]x(t) = ||x !     1| !     2| ...| !     m| !   (t)

!sx!2 =!p   p

!s[p]x!2

theorem: for appropriate wavelets, a scattering is

contracting !sx     sy!     !x     y!
preserves energy !sx!2 = !x!2
stable to deformations !sx     sx  !     c sup
when    goes to 1, sx converges to sx(p)     l2(p   )
which is translation invariant.

t

|     (t)|!x!

wednesday, july 18, 2012

proof: the modulus pushes the energy towards low frequencies

x

     energy conservation
!u x! = !w x! = !x!

x !  

|x !     1|

|||x !     1| !     2| !     3|

!u[p]x!     !sx! = !x!
    fast decay across layers of
    reduced number of paths with non-negligible output.
    computational complexity: o(n log n).

wednesday, july 18, 2012

   frequency to paths mapping

p = (  1, ...,   m)
sx(p(  ))|

x(t)

20

40

60

1

0.5

1
0
0
1
0.5

  x(  )

      

2

1

0.5

1
      
4
0.5

0

0
1

x   (t) = x(t       (t)) with   (t) =   t .

0.5
0

40

60

2

4

2

x(t)

40

60

  x(  )

2

20

20

0
1

0

0
1

0.5
0
0
1
0
0.5
1

0

20

40

60

20

x   (t)

60

40

20

40

20

20

40

40

60

60

60

2

2

0

  x   (  )

0
0.5
0
1
0
0.5
1
0
0
0.5
1
!|  x|    |   x  |!
0
0
2
0.5
!x!!  !!   

2

0

0

2

4

0.5

0
1

4

0.5

0
1

4

4

= 13
4

0.5

0

4

2

0.5
0

0

0
1

2

0
0.5
!sx     sx  !p   
0
0
2
!x!!  !!   

0

2

2

2

2

      
4

4

4

4

4

4

= 1.4

4

4

0.04

0.02

0.04
0
0
0.02
0.02

0
0
0
0.02
   0.02
0
0
0.02
   0.02
0
0
0.02
   0.02
0
0
0.02
   0.02
0
0
0.02

   0.02
0
0

   0.02
0
wednesday, july 18, 2012

       affine space classification

joan bruna
    each class xk is represented by a scattering centroid e(sxk)
and a space vk of principal variance directions (pca).

a   ne space model ak = e(sxk) + vk.

x1

x2

e(sx1)

s

x

wednesday, july 18, 2012

e(sx2)

a2

x

x

sx

a1

    affine space learning

    estimation of affine approximation spaces with pca 

    estimation of the mean                and the covariance        from 
transformed labeled examples            in each class 

e(sxk)

  k

sxn

    the best approximation space             of dimension d is 
generated by the d eigenvectors of        of largest eigenvalues. it 
carries the principal deformation directions of each class.

vk

  k

    the dimension d is optimized by cross-validation.

wednesday, july 18, 2012

 digit classification: mnist

wednesday, july 18, 2012

 digit classification: mnist

wavelet scattering

x

|x !     1| !   (2j n)

||x !     1| !     2| !   (2j n)

2j = 8 : window size

cross-validated

wednesday, july 18, 2012

 digit classification: mnist

classi   cation errors

training size conv. net.

300
5000
20000
60000

7.2%
1.5%
0.8%
0.5%

scattering

4.4%
1.0%
0.6%
0.4%

lecun et. al.

wednesday, july 18, 2012

        overview

    part 1: invariance and deformation stability 
    fourier failure
    wavelet stability
    scattering transform invariants and deep convolution networks
    mathematical properties of deep networks
    classification of images 

    part 2: inverse, textures and multiple invariants
    inverse scattering by phase retrieval and sparsity
    scattering models of stationary processes
    texture classification
    invariants over multiple groups: transposition, rotation, scaling

wednesday, july 18, 2012

         wavelet transform

    dilated wavelets:     (t) = 2   jq   (2   jqt) with    = 2   jq .

    !(t)

    (t)

|     (  )|2

|       (  )|2

|       !(  )|2

0

  

  !

  

q-constant band-pass    lters       

    wavelet transform: w x(t) =!x !   (t) , x !     (t)"  
    if

|       |2 = 1 then w is unitary.

|  |2 +!  
!w x!2 = !x !   !2 +!  

!x !     !2 = !x!2 .

wednesday, july 18, 2012

         wavelet transform

    for images, dilated and rotated wavelets:
    (t) = 2j   (2jrt) with    = 2jr

  2

|       (  )|2

  1

[wolf et al.] 

    wavelet transform: w x(t) =!x !   (t) , x !     (t)"  
    if

|       |2 = 1 then w is unitary.

|  |2 +!  

wednesday, july 18, 2012

     deep convolution network

y. lecun et. al.

    iteration on u x = {x !    , |x !     |}   , contracting.

x !    ,

x !  

x

|x !     1|

= |x !     1| !   

s[  1]x
||x !     1| !     2|

s[  1,   2]x

|||x !     1| !     2| !     3|

    output at all layers:
mfsc and sift are 1st layer outputs: s[  1]x

{s[p]x}p   p .

wednesday, july 18, 2012

 reconstruction, phase retrieval

ir  ne waldspurger

theorem for appropriate wavelets

u x =!x !   (t) , |x !     (t)|"  

is invertible and the inverse is continuous.

250

200

150

100

50

0

0

250

200

150

100

50

0

0

wednesday, july 18, 2012

x(t)

50

100

150

200

250

300

350

400

450

500

50

100

150

200

250

300

350

400

450

500

x !   (t)

|x !     (t)|

u

u   1

400
200
0
40
20
0
40
20
0
40
20
0
20
10
0
20
10
0

0

0

0

0

0

0

100

200

300

400

500

100

200

300

400

500

100

200

300

400

500

100

200

300

400

500

100

200

300

400

500

100

200

300

400

500

      scattering inversion: sparsity

inverse scattering:
more stable phase recovery if {|x !     (t)|}   are sparse
because fewer phase to compute

progressive inversions of u
x !  

x

more precise if sparse

s[  1]x

|x !     (t)|

t

s[  1,   2]x

= ||x !     1| !     2| !   
not exactly invertible because the last layer is missing.
smaller information loss if sparse: sparse deconvolution.

not invertible because the last layer is missing.

not invertible because the last layer is missing.

    scattering invariants discriminate signals that are sparse

wednesday, july 18, 2012

      audio reconstruction

joakim anden

original audio signal x

reconstruction from sx for a window of 3 s with n samples

q = 8

from order 1 s[  1]x : q log n coe   cients

from order 2 s[  1,   2]x : (q log n)2/2 coe   cients

wednesday, july 18, 2012

         sparsity for learning

    need a sparse analysis representation:
!!x(t),     (t     u)# = x "     (u)"  ,u

but we do not know how to learn them...

    we know how to learn sparse analysis representations:

         

(unstable)

x    !  
!x    !  

by    nding d = {    }   which minimizes:
         ! +   !   |    |

    learn by synthesis and classify with analysis operators:

{!x,     "}  

: stable

(autoencoders)

wednesday, july 18, 2012

        texture discrimination

    textures define high-dimensional image classes.
    realizations of stationary processes      but typically not gaussian, 
not markovian and not characterized by second order moments.

x

same power spectrum

same power spectrum

wednesday, july 18, 2012

 scattering stationary processes

    if x(t) is stationary then

u[p]x = |      |x !     1| !       | !     m|

is stationary

    expected scattering: sx(p) = e(u[p]x)
depends on normalized moments of order 2m of x.

    a windowed scattering

s[p]x(t) = u[p]x !   (t)

is an unbiased estimator of sx(p) = e(u[p]x).

wednesday, july 18, 2012

     maximum id178 distribution

joan bruna

    given sx(p) = e(u[p]x) for p     p
the maximum id178 distribution is (boltzman theorem):

where   p are lagrange multipliers and z is de   ned by

p(x) =

  p u[p]x#

1
z

exp!"p   p
! p(x) dx = 1 .

    metropolis-hasting algorithm samples the distribution,
but computationaly very expansive.
    faster iterative algorithm with sparsity condition on l0 norm.

wednesday, july 18, 2012

     synthesis from second order

j. mcdermott textures

joan bruna
joakim anden
    estimation of x(x) from log2 n second order coe   cients:

q = 1

- original jackhammer
- synthesized

- original water
- synthesized

- original applause
- synthesized

wednesday, july 18, 2012

      image reconstruction

original

reconstructed

wednesday, july 18, 2012

     scattering white noises

constant fourier power spectrum:   rx(  ) =   2.
sx(p(  ))2 : radon measure

x(x)

bernoulli

2 x 105
1.5

200

400

600

800

1000

1

0.5

0
0
2 x 106
1.5

1

0.5

10

5

0
0
4

2

0

   2

0.5

1

1.5

2

2.5

     
3

sx(  1)

sx(  1,   2)

sx(  1,   2,   3)

sx(  1,   2,   3,   4)

   
  
3

sx(p(  ))2 d   .

   4
0
600
gaussian white

200

400

wednesday, july 18, 2012

800

1000

0
0

0.5

1

1.5

2

2.5

!   rx(  )|     2j (  )|2 d   =! 2j+1

2j

 gain control and id172

    invariant information is in transfer functions:

s[  1, ...,   m   1,   m]x(t)

s[  1, ...,   m]x(t)

: tuned gain control

computed by cascading a normalized propagator

u x =!x !    ,

|x !     |

x !   " :

surround suppression

wednesday, july 18, 2012

    multifractal scattering

    multifractal scaling:
sx(  1)            1

1

process

white gaussian

fractional brownian noise bh(t)

mandelbrot cascade

nasdaq:aapl
dirac measure

poisson pp density   

wednesday, july 18, 2012

sx(  1,   2)
sx(  1)     (  2      1

1 )     2

  1
   1/2
h
  1
2/3
0

  2
   1/2
   1/2
0
   0.15

0

0 if    <  
0 if   1 +   2 <   
   1/2 if              1/2 if   1 +   2       

       affine space classification

joan bruna
    each class xk is represented by a scattering centroid e(sxk)
and a space vk of principal variance directions (pca).

a   ne space model ak = e(sxk) + vk.

x1

x2

e(sx1)

s

x

wednesday, july 18, 2012

e(sx2)

a2

x

x

sx

a1

     classification of textures

curet database

61 classes

rotations and 
illumination 
variations.

wednesday, july 18, 2012

     classification of textures

x

||x !     1|| !   

||x !     1| !     2| !   

window size = image size

cross-validated

wednesday, july 18, 2012

     classification of textures

curet database

61 classes

rotations and 
illumination 
variations.

texte

classi   cation errors

training fourier markov
field
per class
2.46%

spectr.
2.15%

46

scattering

0.2 %

wednesday, july 18, 2012

varma & zisserman

      audio genre classification

joakim anden
gtzan: music genre classi   cation (jazz, rock, classic,...)
10 classes and 30 seconds tracks.

classi   cation errors
error (%)

feature set

mfcc

delta-mfcc

scattering, m=1

scattering, m=2

32
23
28

16

wednesday, july 18, 2012

        same or different ?

aren jensen

encyclopaedias

wednesday, july 18, 2012

frequency transposition invariance

same words by di   erent people
change of pich     frequency scaling:             
    log frequency translation: log        log    + log   
    translation invariance in (t, log   ) with deformation stability

wavelet modulus: |x !     1(t)|

log   1

t

wednesday, july 18, 2012

 transposition invariant scattering

log   1

t

separable wavelets in t and log   1

    2,    2(t, log   1) =     2(t)         2(log   1)

separable 2d wavelet transform of: y(t, log   1) = |x "     1(t)|

y !     2,    2(t, log   1)

invariance by wavelet amplitude averaging in (t, log   1):

|y !     2,    2| !   (t, log   1)

invariant scattering:

| |y !     2,    2|... !     m,    m| !   (t, log   1)

wednesday, july 18, 2012

classification with transp. invariants
joakim anden
gtzan: music genre classi   cation (jazz, rock, classic,...)
10 classes and 30 seconds tracks.

classi   cation errors
error (%)

feature set

scattering, m=2

scat.+ transp. inv., m=2

16

13

wednesday, july 18, 2012

  rotation and affine invariance

    scatterings along translation, rotation and a   ne groups:
sx

x

translat.
invar.

rotation
invar.

a   ne
invar.

laurent sifre

uiuc database:

wednesday, july 18, 2012

  rotation and affine invariance

    scatterings along translation, rotation and a   ne groups:
sx

x

translat.
invar.

rotation
invar.

a   ne
invar.

laurent sifre

wavelet transform along
positions, rotations and scales
in    v1 hypercolumns   

wednesday, july 18, 2012

translation and rotation invariance
laurent sifre

x

|x !     1|

wednesday, july 18, 2012

  multiple scattering invariants

x

wavelet convolutions

along space

modulus pooling

wavelet convolutions

along space, rotations, scales

modulus pooling

linear averaging

along space, rotations, scales

s[p]x

wednesday, july 18, 2012

  rotation and affine invariance

    scatterings along translation, rotation and a   ne groups:
sx

x

translat.
invar.

rotation
invar.

a   ne
invar.

laurent sifre

uiuc database:

classi   cation errors

training translation transl + rotation a   ne
1%

15 %

3%

20

wednesday, july 18, 2012

       unsupervised learning

    need to learn informative, stable invariants.
    over general manifolds as opposed to groups
    the final linear averaging providing adapted invariants can be 

learned by supervised classifiers (id166).

    problem: unsupervised learning of the dictionary and of the   

non-linear pooling. 

    sparsity is important to build informative invariants:              

auto-encoders with group sparsity.

    why does it work ? still a mathematical mystery.

wednesday, july 18, 2012

         conclusion

    an interpretation of convolution networks for groups:
    filters must be wavelets
    stable pooling: complex modulus + averaging 
    multilayers: recover lost information and refine invariants
    sparsity is needed to preserve information in invariants
    normalisation: to   decorrelate   outputs 
    learning: needed but not for first layers.                                                 

    unsupervised deep learning: still not understood mathematically

    papers and softwares:     www.cmap.polytechnique.fr/scattering 

wednesday, july 18, 2012

