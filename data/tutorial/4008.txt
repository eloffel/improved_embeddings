      learning hierarchies
      learning hierarchies
      of invariant features
      of invariant features

     courant institute of mathematical sciences 
     courant institute of mathematical sciences 

 yann lecun
 yann lecun

and 
and 

    center for neural science, 
    center for neural science, 

      new york university
      new york university

yann lecun

challenges for machine learning, vision, signal processing, ai, neuroscience
challenges for machine learning, vision, signal processing, ai, neuroscience

how can learning build a perceptual system?

how do we learn representations of the perceptual world?

in ml/cv/asr/mir: how do we learn features (not just classifiers)?

with good representations, we can learn categories from just a few 
examples.

ml has neglected the question of learning representations, relying 
instead on domain expertise to engineer features and kernels.

deep learning addresses the problem of learning representations

goal 1: biologically-plausible methods for deep learning

goal 2: representation learning for computer perception

yann lecun

architecture of    mainstream    image and 
architecture of    mainstream    image and 
audio recognition systems
audio recognition systems

traditional way: handcrafted features + classifier

(hand  crafted)

feature  extraction

   simple     trainable  

classifier

modern mainstream approaches to image and id103

mid-level features, often trained unsupervised

low  level
features
(fixed)
mfcc
sift
hog

mid  level
features

(unsupervised)
mix  of  gaussians

k  means

sparse  coding

classifier

(supervised)
sometimes
structured

yann lecun

the mammalian visual cortex is hierarchical
the mammalian visual cortex is hierarchical

the ventral (recognition) pathway in the visual cortex has multiple stages
retina - lgn - v1 - v2 - v4 - pit - ait ....

yann lecun

[gallant  &  van  essen]  

[picture  from  simon  thorpe]

vision occupies a big chunk of our brains
vision occupies a big chunk of our brains

1/3 of the macaque brain

yann lecun

[from  van  essen]

the primate's visual system is deep (lgn->v1->v2->v4->it)
the primate's visual system is deep (lgn->v1->v2->v4->it)

the recognition of everyday objects is a very fast process.
the recognition of common objects is essentially    feed forward.    
but not all of vision is feed forward.
much of the visual system (all of it?) is the result of learning
how much prior structure is there?
if the visual system is deep  (around 10 layers) and learned
what is the learning algorithm of the visual cortex?
what learning algorithm can train neural nets as 
   deep    as the visual system (10 layers?).
unsupervised vs supervised learning
what is the id168?
what is the organizing principle?
broader question (hinton): what is the learning 
algorithm of the neo-cortex?

yann lecun

trainable feature hierarchies
trainable feature hierarchies

why can't we make all the modules trainable?

proposed way: hierarchy of trained features

trainable
feature

transform

trainable
feature

transform

trainable
classifier/
predictor

learned  internal  representation

yann lecun

do we really need deep architectures?
do we really need deep architectures?

we can approximate any function as close as we want with shallow 
architecture. why would we need deep ones?

kernel machines and 2-layer neural net are    universal   .
deep learning machines

deep machines are more efficient for representing certain classes of 
functions, particularly those involved in visual recognition
they can represent more complex functions with less 
   hardware    
we need an efficient parameterization of the class of functions that 
are useful for    ai    tasks.

yann lecun

why are deep architectures more efficient?
why are deep architectures more efficient?
[bengio  &  lecun  2007     scaling  learning  algorithms  towards  ai   ]
a deep architecture trades space for time (or breadth for depth)
more layers (more sequential computation), 
but less hardware (less parallel computation).
depth-breadth tradoff
example1: n-bit parity
requires n-1 xor gates in a tree of depth log(n).
requires an exponential number of gates of we restrict ourselves to 
2 layers (dnf formula with exponential number of minterms).
example2:  circuit for addition of 2 n-bit binary numbers
requires o(n) gates, and o(n) layers using n one-bit adders with 
ripple carry propagation.
requires lots of gates (some polynomial in n) if we restrict 
ourselves to two layers (e.g. disjunctive normal form).
bad news: almost all boolean functions have a dnf formula with 
an exponential number of minterms o(2^n).....

yann lecun

strategies (a parody of [hinton 2007])
strategies (a parody of [hinton 2007])

defeatism: since no good parameterization of the    ai-set    is available, 
let's parameterize a much smaller set for each specific task through 
careful engineering (preprocessing, kernel....).
denial: kernel machines can approximate anything we want, and the vc-
bounds guarantee generalization. why would we need anything else?
unfortunately, kernel machines with common kernels can only 
represent a tiny subset of functions efficiently
optimism: let's look for learning models that can be applied to the largest 
possible subset of the ai-set, while requiring the smallest amount of task-
specific knowledge for each task.
there is a parameterization of the ai-set with neurons.
is there an efficient parameterization of the ai-set with computer 
technology?
until very recently, much of the ml community oscillated between 
defeatism and denial.

yann lecun

deep supervised learning is hard
deep supervised learning is hard

the loss surface is non-convex, ill-conditioned, has saddle points, has 
flat spots.....
for large networks, it will be horrible! (not really, actually)
back-prop doesn't work well with networks that are tall and skinny.
lots of layers with few hidden units.
back-prop works fine with short and fat networks
but over-parameterization becomes a problem without 
id173
short and fat nets with fixed first layers aren't very different 
from id166s.
for reasons that are not well understood theoretically, back-prop works 
well when  they are highly structured
e.g. convolutional networks.

yann lecun

fixed preprocessing (features, kernels, basis functions)
fixed preprocessing (features, kernels, basis functions)

map the inputs into a (higher dimensional) 
   feature    space
with more dimensions, the task is more 
likely to be linearly separable.
problem: how should we pick the features so 
that the task becomes linearly separable in the 
feature space?
classical approach 1: we use our prior 
knowledge about the problem to hand-
craft an appropriate feature set.
 classical approach 2: we use a 
   standard    set of basis functions (rbfs....)

yann lecun

the simplest form of unsupervised feature extraction: 
the simplest form of unsupervised feature extraction: 
kernels machines: 
kernels machines: 

simplest approach: the kernel method (thanks to 
grace wahba's representer theorem)
make each basis function a    bump    function (a 
template matcher).
place one bump around each training sample.
compute a linear combination of the bumps.
in the    bump space   , we get one separate 
dimension for each training sample, so if the 
bumps are narrow enough, we can learn any 
mapping on the training set.
to generalize on unseen samples, we adjust the 
bump widths and we regularize the weights. 
regularize, so only the    useful    bumps are used
we get a support vector machine.
problem: an id166 is a glorified template matcher 
which is only as good as its kernel. 

yann lecun

trainable front-end, structured architectures
trainable front-end, structured architectures

the solutions: 
invariance: do not use a fixed front-end, make it trainable, so it 
can learn to extract invariant representations
structure: do not use simple linearly-parameterized classifiers, use 
architectures whose id136 process involves multiple non-linear 
decisions, as well as search and    reasoning   .
we need total flexibility in the design of the architecture of the machine:
so that we can tailor the architecture to the task
so that we can build our prior knowledge about the task into the 
architecture
multi-module architectures.

yann lecun

   backprop and
   backprop and

    training multi-module
    training multi-module

    architectures
    architectures

[bottou  &  gallinari  nips  1991]
[lecun,  bottou,  bengio,  haffner,  1998]

yann lecun

multi-module architectures and backprop
multi-module architectures and backprop

for supervised learning
we allow the function f(w,x) to 
be non-linearly parameterized in 
w.
this allows us to play with a 
large repertoire of functions with 
rich class boundaries.
we assume that f(w,x) is 
differentiable almost everywhere 
with respect to w.

yann lecun

multimodule systems: cascade
multimodule systems: cascade

complex learning machines can be 
built by assembling modules into 
networks
 simple example: sequential/layered 
feed-forward architecture (cascade)
forward propagation:

yann lecun

multimodule systems: implementation
multimodule systems: implementation

each module is an object

contains trainable parameters
inputs are arguments
output is returned, but also 
stored internally
example: 2 modules m1, m2

torch7 (by hand)

hid  =  m1:forward(in)
out  =  m2:forward(hid)

torch7 (using the nn.sequential class)

model  =  nn.sequential()
model:add(m1)
model:add(m2)
out  =  model:forward(in)  

yann lecun

yann lecun

yann lecun

yann lecun

yann lecun

yann lecun

yann lecun

multimodule systems: implementation
multimodule systems: implementation

id26 through a module
contains trainable parameters
inputs are arguments
gradient with respect to input is 
returned. 
arguments are input and gradient 
with respect to output

torch7 (by hand)

hidg  =  m2:backward(hid,outg)
ing  =  m1:backward(in,hidg)

torch7 (using the nn.sequential class)

ing  =  model:backward(in,outg)  

yann lecun

yann lecun

yann lecun

yann lecun

yann lecun

yann lecun

assembling a simple neural net
assembling a simple neural net

model  =  nn.sequential()
model:add(nn.reshape(ninputs))
model:add(nn.linear(ninputs,nhiddens))
model:add(nn.tanh())
model:add(nn.linear(nhiddens,noutputs))
model:add(nn.tanh())

      add  cost  module
criterion  =  nn.msecriterion()

      get  the  weight  and  grads  in  vectors    
parameters,gradparameters  =  
    model:getparameters()
  

yann lecun

assembling a simple neural net
assembling a simple neural net

we create a function feval(w) that, for the current input, will compute 
the cost and the gradient of the cost with respect to the parameters.
this function (actually a closure) can be passed to an optimizer

      create  closure  to  evaluate  f(param)  and  df/dparam
local  feval  =  function(w)
    parameters:copy(w)
          reset  gradients
    gradparameters:zero()
    local  output  =  model:forward(input)
    local  f  =  criterion:forward(output,  target)
    recordstuff(output,target,f)
          estimate  df/dw
    local  df_dout  =  criterion:backward(output,  target)
    model:backward(input,  df_dout)
    return  f,gradparameters
end
yann lecun

any architecture works
any architecture works

any connection is permissible
networks with loops must be 
   unfolded in time   .
any module is permissible
as long as it is continuous and 
differentiable almost everywhere 
with respect to the parameters, and 
with respect to non-terminal inputs.

yann lecun

deep supervised learning is hard
deep supervised learning is hard
example: what is the id168 for the simplest 2-layer neural net ever
function: 1-1-1 neural net. map 0.5 to 0.5 and -0.5 to -0.5 
(identity function) with quadratic cost: 

yann lecun

      convolutional nets
      convolutional nets

[lecun,  bottou,  bengio,  haffner  1998]

yann lecun

feature transform = 
feature transform = 
id172     filter bank     non-linearity     pooling
id172     filter bank     non-linearity     pooling

norm

filter
bank  

non  
linear

feature
pooling  

norm

filter
bank  

non  
linear

feature
pooling  

classifier

stacking multiple stages of  

[id172 

 filter bank 

   

   

 non-linearity 

   

 pooling].

id172: variations on whitening

subtractive: average removal, high pass filtering
divisive: local contrast id172, variance id172
filter bank: dimension expansion, projection on overcomplete basis
non-linearity: sparsification, saturation, lateral inhibition....
component-wise shrinkage or tanh, winner-takes-all

pooling: aggregation over space or feature type, subsampling
average : 1
 

x i ; max : max

x i ; l p : p    x i

p ; prob : 1

k    

i

i

b log (   

i

eb x i)

yann lecun

feature transform = 
feature transform = 
id172     filter bank     non-linearity     pooling
id172     filter bank     non-linearity     pooling

norm

filter
bank  

non  
linear

feature
pooling  

norm

filter
bank  

non  
linear

feature
pooling  

classifier

filter bank     non-linearity = non-linear embedding in high dimension
feature pooling = contraction, id84, smoothing
learning the filter banks at every stage
creating a hierarchy of features
basic elements are inspired by models of the visual (and auditory) cortex

simple cell + complex cell model of [hubel and wiesel 1962]
many    traditional    feature extraction methods are based on this
sift, gist, hog, convolutional networks.....

 [fukushima 1974-1982], [lecun 1988-now],  [poggio 2005-now], [ng 
2006-now], many others....

yann lecun

basic convolutional network architecture
basic convolutional network architecture

   simple  cells   

   complex  cells   

multiple  
convolutions

pooling  
subsampling

retinotopic  feature  maps

[lecun  et  al.  89]
[lecun  et  al.  98]

yann lecun

convolutional network architecture
convolutional network architecture

yann lecun

convolutional network (convnet)
convolutional network (convnet)

layer  1
64x75x75

layer  2
64@14x14

input
83x83

layer  3
256@6x6

layer  4
256@1x1 output

101

9x9
convolution
(64  kernels)

10x10  pooling,
5x5  subsampling

9x9
convolution
(4096  kernels)

6x6  pooling
4x4  subsamp

  

non-linearity: shrinkage function, tanh
pooling: l2, average, max, average   tanh  
training: supervised (1988-2006), unsupervised+supervised (2006-now)

yann lecun

convolutional network (vintage 1990) 
convolutional network (vintage 1990) 
filters     tanh     average-tanh     filters     tanh     average-tanh     filters     tanh

yann lecun

   mainstream    object recognition pipeline 2006-2010: similar to convnets
   mainstream    object recognition pipeline 2006-2010: similar to convnets

filter
bank  

non  

linearity

feature
pooling  

filter
bank  

non  

linearity

feature
pooling  

classifier

oriented
  edges

winner
takes
all

sift

histogram
(sum)

k  means
or  
sparse  coding

pyramid
histogram
(sum)

id166  or
another  
simple
classifier

fixed low-level features + unsupervised mid-level features + simple classifier
example (on caltech 101 dataset): 

sift + vector quantization + pyramid pooling + id166: >65% 

[lazebnik et al. cvpr 2006]

sift + local sparse coding macrofeat. + pyr/ pooling + id166: >77%

[boureau et al. iccv 2011]

yann lecun

other applications with state-of-the-art performance
other applications with state-of-the-art performance

traffic sign recognition (gtsrb)

house number recognition (google) 

german traffic sign reco bench 
97.2% accuracy

street view house numbers
94.8% accuracy

yann lecun

convnet architecture with multi-stage features
convnet architecture with multi-stage features

feature maps from all stages are pooled/subsampled and sent to the 
final classification layers

pooled low-level features: good for textures and local motifs
high-level features: good for    gestalt    and global shape

yann lecun

[sermanet,  chintala,  lecun  arxiv:1204.3968,  2012]

prediction of epilepsy seizures from intra-cranial eeg
prediction of epilepsy seizures from intra-cranial eeg

piotr  mirowski,  deepak  mahdevan  (nyu  neurology),  yann  lecun

yann lecun

epilepsy prediction
epilepsy prediction

temporal  convolutional  net

32

outputs

32

integration  of
all  channels  
and  all  features
across  several  
time  samples

4

   

   

   

   

   

       

   
   

   

   

   

10

8

integration  of

all  channels  and  all  features
across  several  time  samples

   

   

   

   

inputs

384

time,  in  samples

feature  extraction
over  short  time

windows

for  individual

channels

(we  look  for

10  sorts

of  features)

s
l
e
n
n
a
h
c
  
g
e
e

32

   

64

yann lecun

industrial applications of convnets
industrial applications of convnets

at&t/lucent/ncr
check reading, ocr, handwriting recognition (deployed 1996)
nec
intelligent vending machines and advertizing posters, cancer 
cell detection, automotive applications
google
face and license plate removal from streetview images
microsoft
handwriting recognition, speech detection
orange
face detection, hci, cell phone-based applications
startups, other companies...

yann lecun

       fast scene parsing
       fast scene parsing

[farabet,  couprie,  najman,  lecun,    icml  2012]

yann lecun

labeling every pixel with the object it belongs to
labeling every pixel with the object it belongs to

would help identify obstacles, targets, landing sites, dangerous areas
would help line up depth map with edge maps

yann lecun

[farabet  et  al.  icml  2012]

scene parsing/labeling: convnet architecture
scene parsing/labeling: convnet architecture

each output sees a large input context:

46x46 window at full rez; 92x92 at    rez; 184x184 at    rez
[7x7conv]->[2x2pool]->[7x7conv]->[2x2pool]->[7x7conv]->
trained supervised on fully-labeled images

categories

laplacian
pyramid

level  1  
features

level  2
features

upsampled
level  2  features

yann lecun

scene parsing/labeling: system architecture
scene parsing/labeling: system architecture

dense
feature maps

convnet

multi-scale
pyramid
(band-pass filtered)

original image

yann lecun

method 1: majority over super-pixel regions
method 1: majority over super-pixel regions

s
u
p
e
r
  
p
i
x
e
l
  

b
o
u
n
d
a
r
y

  

h
y
p
e
t
h
e
s
e
s

m
u
l
t
i
  
s
c
a
l
e
  
c
o
n
v
n
e
t

majority

vote
over

superpixels

superpixel  boundaries

categories  aligned
with  region
boundaries

   soft     categories  scores

c
o
n
v
o
l
u
t
i
o
n
a
l
  
c
l
a
s
s
i
f
i
e
r

[farabet  et  al.  ieee  t.  pami  2012]

features  from
convolutional  net
(d=768  per  pixel)

input  image

yann lecun

method 2: optimal cover of purity tree
method 2: optimal cover of purity tree

2-layer
neural
net

distribution of
categories within
each segment

spanning tree
from pixel 
similarity graph

yann lecun

[farabet  et  al.  icml  2012]

scene parsing/labeling: performance
scene parsing/labeling: performance

stanford background dataset [gould 1009]: 8 categories

sift flow dataset [liu 2009]: 33 categories

barcelona dataset[tighe 2010]: 
170 categories.

yann lecun

[farabet  et  al.  ieee  t.  pami  2012]

scene parsing/labeling: results
scene parsing/labeling: results

samples from the sift-flow dataset (liu)

yann lecun

[farabet  et  al.  2012]

scene parsing/labeling: sift flow dataset (33 categories)
scene parsing/labeling: sift flow dataset (33 categories)

samples from the sift-flow dataset (liu)

yann lecun

[farabet  et  al.  icml  2012]

scene parsing/labeling: sift flow dataset (33 categories)
scene parsing/labeling: sift flow dataset (33 categories)

yann lecun

[farabet  et  al.  icml  2012]

scene parsing/labeling
scene parsing/labeling

yann lecun

[farabet  et  al.  icml  2012]

scene parsing/labeling
scene parsing/labeling

yann lecun

[farabet  et  al.  icml  2012]

scene parsing/labeling
scene parsing/labeling

yann lecun

[farabet  et  al.  2012]

scene parsing/labeling
scene parsing/labeling

yann lecun

[farabet  et  al.  2012]

scene parsing/labeling
scene parsing/labeling

no post-processing
frame-by-frame
convnet runs at 50ms/frame on virtex-6 fpga hardware

but communicating the features over ethernet limits system perf.

yann lecun

scene parsing/labeling: temporal consistency
scene parsing/labeling: temporal consistency

majority vote on spatio-temporal super-pixels
reset every second

yann lecun

    a general view of 
    a general view of 

  unsupervised 
  unsupervised 

learning
learning

yann lecun

learning features with unsupervised pre-training
learning features with unsupervised pre-training

supervised learning requires lots of labeled samples
most available data is unlabeled
models need to be large to    understand    the task
but large models have many parameters and require many labeled samples
unsupervised learning can be used to pre-train the system before 
supervised refinement
unsupervised pre-training    consumes    degrees of freedom while placing 
the system in a favorable region of parameter space.
supervised refinement merely find the closest local minimum within the 
attractor found by unsupervised pre-training.
unsupervised id171 through sparse/overcomplete auto-encoders
with high-dimensional and sparse representations, the data manifold is 
   flattened    (any collection of points is flatter in higher dimension)

yann lecun

unsupervised learning: capturing dependencies between variables
unsupervised learning: capturing dependencies between variables

energy function: viewed as a negative log id203 density 

probabilistic view:
produce a id203 density 
function that:
has high value in regions of 
high sample density
has low value everywhere else 
(integral = 1).
energy-based view:
produce an energy function 
e(y,w) that:
has low value in regions of high 
sample density
has high(er) value everywhere 
else

p(y|w)  

e(y,w)

yann lecun

y

y

unsupervised learning: capturing dependencies between variables
unsupervised learning: capturing dependencies between variables

energy function viewed as a negative log density
example: y = x^2 

y

x

yann lecun

energy <-> id203
energy <-> id203

yann lecun

p(y|w)

e(y,w)

y

y

training an energy-based model
training an energy-based model

make the energy around training samples low
make the energy everywhere else higher

e(y)

yann lecun

)y
y
(
p

y

training an energy-based model to approximate a density
training an energy-based model to approximate a density

maximizing  p(y|w)  on  training  samples

make  this  big

make  this  small

minimizing    log  p(y,w)  on  training  samples

make  this  small

make  this  big

yann lecun

p(y)

e(y)

y

y

training an energy-based model with id119
training an energy-based model with id119
gradient of the  negative log-likelihood loss for one sample y:

id119:

e(y)

pushes down on the
energy of the samples

pulls up on the
energy of low-energy y's

y

y

yann lecun

how do we push up on the energy of everything else?
how do we push up on the energy of everything else?

solution 1: contrastive divergence [hinton 2000]
move away from a training sample a bit
push up on that
solution 2: score matching
on the training samples: minimize the gradient of the energy, and 
maximize the trace of its hessian.
solution 3: denoising or contracting auto-encoder (not energy-based)
train the id136 dynamics to map noisy samples to clean 
samples, or regularize the mapping so as not to vary as one moves 
away from the data manifold.
solution 4: main insight! [ranzato, ..., lecun ai-stat 2007]
restrict the information content of the code (features) z
if the code z can only take a few different configurations, only a 
correspondingly small number of ys can be perfectly reconstructed 
idea: impose a sparsity prior on z
this is reminiscent of sparse coding [olshausen & field 1997]

yann lecun

restricted id82s
restricted id82s
[hinton  &  salakhutdinov  2005]
y and z are binary
enc and dec are linear
distance is negative dot product

y

distance

decoder
(basis  fns)

z

encoder
(predictor)

distance

yann lecun

the main insight [ranzato et al. aistats 2007]
the main insight [ranzato et al. aistats 2007]

if the information content of the feature vector is limited (e.g. by 
imposing sparsity constraints), the energy must be large in most of the 
space.
pulling down on the energy of the training samples will 
necessarily make a groove
the volume of the space over which the energy is low is limited by the 
id178 of the feature vector
input vectors are reconstructed from feature vectors. 
if few feature configurations are possible, few input vectors can 
be reconstructed properly 

yann lecun

why limit the information content of the code?
why limit the information content of the code?

training  sample

input  vector  which  is  not  a  training  sample
feature  vector

input  space

feature  
space

yann lecun

why limit the information content of the code?
why limit the information content of the code?

training  sample

input  vector  which  is  not  a  training  sample
feature  vector

training  based  on  minimizing  the  reconstruction  error  over  
the  training  set
input  space

feature  
space

yann lecun

why limit the information content of the code?
why limit the information content of the code?

training  sample

input  vector  which  is  not  a  training  sample
feature  vector

bad:  machine  does  not  learn  structure  from  training  data!!  
it  just  copies  the  data.
input  space

feature  
space

yann lecun

why limit the information content of the code?
why limit the information content of the code?

training  sample

input  vector  which  is  not  a  training  sample
feature  vector

idea:  reduce  number  of  available  codes.

input  space

feature  
space

yann lecun

why limit the information content of the code?
why limit the information content of the code?

training  sample

input  vector  which  is  not  a  training  sample
feature  vector

idea:  reduce  number  of  available  codes.

input  space

feature  
space

yann lecun

why limit the information content of the code?
why limit the information content of the code?

training  sample

input  vector  which  is  not  a  training  sample
feature  vector

idea:  reduce  number  of  available  codes.

input  space

feature  
space

yann lecun

sparsity penalty to restrict the code
sparsity penalty to restrict the code

we are going to impose a sparsity penalty on the code to restrict its 
information content.
we will allow the code to have higher dimension than the input
categories are more easily separable in high-dim sparse feature spaces
this is a trick that id166 use: they have one dimension per sample
sparse features are optimal when an active feature costs more than an 
inactive one (zero).
e.g. neurons that spike consume more energy
the brain is about 2% active on average.

yann lecun

  2  dimensional  toy  dataset

mixture  of  3  cauchy  distrib.

  visualizing  energy  surface
(black  =  low,  white  =  high)    

[ranzato  's  phd  thesis  2009]

pca  

(1  code  unit)

w ' y
wz
   y    wz   2
f    y    
dimens.

autoencoder
(3  code  units)

       w ey    
w d z
   y    wz   2
f    y       log    
part.  func.

sparse  coding
(3  code  units)

   
wz
   y    wz   2         z   
f    y    
sparsity

k  means
(3    code  units)
   
wz
   y    wz   2
f    y    
1  of  n  code

encoder
decoder
energy
loss
pull  up

energies surfaces for various id168s
energies surfaces for various id168s

pca

energy loss

neg-log-likel.

margin loss

sparse cod.

kmeans

5

5

2-1-2

146

2-3-2

12

2-3-2

1

2-3-2

6

3

6
2-100-1-100-2

227

2-20-2
cd1

24

2-20-2

5

20

1

2-3-2

23

2-20-2

yann lecun

  2  dimensional  toy  dataset

spiral

  visualizing  energy  surface
(black  =  low,  white  =  high)    

pca  

(1  code  unit)

w ' y
wz
   y    wz   2
f    y    
dimens.

autoencoder
(1  code  unit)

       w ey    
w d z
   y    wz   2
f    y    
dimens.

sparse  coding
(20  code  units)
      w e z    
w d z
   y    wz   2
f    y    
sparsity

k  means

(20    code  units)
   
wz
   y    wz   2
f    y    
1  of  n  code

encoder
decoder
energy
loss
pull  up

unsupervised id171 as density estimation
unsupervised id171 as density estimation

contrast function or energy function: 

e(y,w) = minz e(y,z,w)
y: input
z:    features   , representation, latent variables
w: parameters of the model (to be learned)
maximum a posteriori approximation for z

density function p(y|w) 
learn w so as to maximize the likelihood of 
the training data under the model

e(y,z)

input  y

parameters  w

  features    z

yann lecun

example: a toy problem. the spiral
example: a toy problem. the spiral

 dataset

10,000 random points along a 
spiral in a 2d plane
the spiral fits in a square with 
opposite corners (-1,1), (1,-1)
the spiral is designed so that no 
function can predict a single 
value of     from 

 goal

learn an energy surface with low 
energies along the spiral and high 
energy everywhere else

yann lecun

pca (principal component analysis)
pca (principal component analysis)

  can  be  seen  as  encoder  decoder  architecture  that  minimizes  mean  
square  reconstruction  error  (energy  loss)
  optimal  code  is  constrained  to  be  equal  to  the  value  predicted  by  
encoder
  flat  surfaces  are  avoided  by  using  a  code  of  low  dimension
enc    y    =wy
dec   z    =w t y ,where w       nxm
c e   y , z    =   wy    z   2
c d    y , z    =   w t z    y   2
  for  large  value  of              energy  
reduces  to

   

e    y ,w    =   w t wy    y   2

yann lecun

id116
id116

  in  this  architecture  the  code  z  is  a  binary  vector  of  size  n  (n  being  
the  number  of  prototypes)
  for  any  sample  y,  only  one  component  is  active  (equal  to  1)  and  
all  the  others  are  inactive  (equal  to  0).  
  this  is  a  one  of  n  sparse  binary  code
  the  energy  is:

e    y , z    =   i z i   y    w i   2
w iis theith prototype

  id136  involves  finding  z  
that  minimizes  the  energy

yann lecun

auto-encoder neural net (2-100-1-100-2 architecture)
auto-encoder neural net (2-100-1-100-2 architecture)

  a  neural  network  autoencoder  that  learns  a  low  dimensional  
representation.  
  architecture  used

input  layer:  2  units
first  hidden  layer:  100  units
second  hidden  layer  (the  code):  1  unit
third  hidden  layer:  100  units
output  layer:  2  units

  similar  to  pca  but  non  linear
  energy  is

e    y    =   dec   enc    y          y   2

yann lecun

wide auto-encoder neural net with energy loss
wide auto-encoder neural net with energy loss

  the  energy    loss  simply  pulls  down  on  the  energy  of  the  training  samples  (no  
contrastive  term).
  because  the  dimension  of  the  code  is  larger  than  the  input,  nothing  prevents  the  
architecture  from  learning  the  identity  function,  which  gives  a  very  flat  energy  
surface  (a  collapse):  everything  is  perfectly  reconstructed.
  simplest  example:  a  multi  layer  neural  network  with  identical  input  and  output  
layers  and  a  large  hidden  layer.
  architecture  used

input  layer:  2  units
hidden  layer  (the  code):  20  units
output  layer:  2  units

  energy  loss  leads  to  a  collapse
  tried  a  number  of  loss  functions

yann lecun

wide auto-encoder with negative-log-likelihood loss
wide auto-encoder with negative-log-likelihood loss

  negative  log  likelihood  loss
pull  down  on  the  energy  of  
training  (observed)  samples
pull  up  on  the  energies  of  all  the  
other  (unobserved)  samples
approximate  the  log  of  partition  
function  through  dense  sampling.
energy  surface  is  very     stiff     
because  of  small  number  of  
parameters.
hence  the  energy  surface  is  not  
perfect.

yann lecun

wide auto-encoder with energy-based contrastive loss
wide auto-encoder with energy-based contrastive loss

  linear  linear  contrastive  loss

avoid  the  cost  associated  with  
minimizing  negative  log  likelihood
idea  is  to  pull  up  on  unobserved  points  
in  the  vicinity  of  training  samples
we  use  langevin  dynamics  to  
generate  such  points

   y        y       

    e        y    
    y       
the  loss  function  is

l   y ,w    =    e    y ,w       max   0,m   e        y ,w       

yann lecun

wide auto-encoder with sparse code
wide auto-encoder with sparse code

  sparse  codes

limiting  the  information  
content  of  the  code  prevents  flat  
energy  surfaces,  without  the  
need  to  explicitly  push  up  the  
bad  points
idea  is  to  make  the  high  
dimensional  code  sparse  by  
forcing  each  variable  to  be  zero  
most  of  the  time

yann lecun

    unsupervised 
    unsupervised 

     id171:
     id171:
     variations on the 
     variations on the 

    sparse auto-encoder theme
    sparse auto-encoder theme

yann lecun

sparse coding & sparse modeling
sparse coding & sparse modeling
[olshausen  &  field  1997]

sparse linear reconstruction
energy  = reconstruction_error + code_prediction_error + code_sparsity

e (y i , z )=   y i   w d z   2+       j

   z j   

   y i       y   2

w d z

input y

       j .

features  

z

   z j   

id136 is slow

y       z =argmin z e (y , z )

yann lecun

how to speed up id136 in a generative model?
how to speed up id136 in a generative model?

factor graph with an asymmetric factor
id136 z     y is easy

run z through deterministic decoder, and sample y

id136 y     z is hard, particularly if decoder function is many-to-one

map: minimize sum of two factors with respect to z
z* =  argmin_z  distance[decoder(z), y] + factorb(z)

generative  model
factor  a

distance

decoder

factor  b

input

y

yann lecun

z

latent
variable

idea: train a    simple    function to approximate the solution
idea: train a    simple    function to approximate the solution

[kavukcuoglu,  ranzato,  lecun,  rejected  by  every  conference,  2008  2009]
train a    simple    feed-forward function to predict the result of a complex 
optimization on the data points of interest
generative  model
factor  a

distance

decoder

factor  b

input

y

fast  feed  forward  model

z

latent
variable

factor  a'

encoder

distance

1. find optimal zi for all yi; 2. train encoder to predict zi from yi

yann lecun

predictive sparse decomposition (psd): sparse auto-encoder
predictive sparse decomposition (psd): sparse auto-encoder
[kavukcuoglu,  ranzato,  lecun,  2008  

  arxiv:1010.3467],

   

prediction the optimal code with a trained encoder
energy  = reconstruction_error + code_prediction_error + code_sparsity

e    y i , z    =   y i   w d z   2      z    ge   w e ,y i      2          j
ge(w e , y i)=shrinkage(w ey i)

   z j   

   y i       y   2

w d z

ge   w e ,y i   

   z        z   2

       j .

features  

z

   z j   

input y

yann lecun

soft shrinkage non-linearity
soft shrinkage non-linearity

yann lecun

psd: basis functions on mnist
psd: basis functions on mnist

basis functions (and encoder matrix) are digit parts

yann lecun

predictive sparse decomposition (psd): training
predictive sparse decomposition (psd): training

training on 
natural images 
patches. 
12x12
256 basis 
functions

yann lecun

learned features on natural patches: v1-like receptive fields
learned features on natural patches: v1-like receptive fields

yann lecun

better idea: give the    right    structure to the encoder
better idea: give the    right    structure to the encoder

[gregor  &  lecun,  icml  2010],  [bronstein  et  al.  icml  2012]
ista/fista: iterative algorithm that converges to optimal sparse code

input y

w e

+

sh()

z

s

yann lecun

lista: train we and s matrices to give a good approximation quickly
lista: train we and s matrices to give a good approximation quickly

think of the fista flow graph as a recurrent neural net where we and s are 
trainable parameters

input y

w e

+

z

sh()

s

time-unfold the flow graph for k iterations
learn the we and s matrices with    backprop-through-time   
get the best approximate solution within k iterations

y

w e

yann lecun

+

sh       

s

+

sh       

s

z

learning ista (lista) vs ista/fista
learning ista (lista) vs ista/fista

yann lecun

one stage: filter     shrinkage     l2 pooling     contrast norm
one stage: filter     shrinkage     l2 pooling     contrast norm

c
o
n
t
r
a
s
t
  
n
o
r
m
a
l
i
z
a
t
i
o
n

s
u
b
t
r
a
c
t
i
v
e
+
d

i
v
i
s
i
v
e
  

c
o
n
v
o
l

u
t
i
o
n
s

  

s
h
r
i

n
k
a
g
e

  

l
2
p
o
o
l
i

n
g
&

  

  
s
u
b
  
s
a
m
p

l
i

n
g

this  is  one  stage  of  the  convnet

yann lecun

local contrast id172
local contrast id172

performed on the state of every layer, including 
the input
subtractive local contrast id172

subtracts from every value in a feature a 
gaussian-weighted average of its 
neighbors (high-pass filter)

divisive local contrast id172

divides every value in a layer by the 
standard deviation of its neighbors over 
space and over all feature maps

subtractive + divisive lcn performs a kind of 
approximate whitening.

yann lecun

using psd to train a hierarchy of features
using psd to train a hierarchy of features

phase 1: train first layer using psd

   y i       y   2

w d z

y

z

   z j   

       j .

ge   w e ,y i   

   z        z   2

yann lecun

features  

using psd to train a hierarchy of features
using psd to train a hierarchy of features

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor

y

ge   w e ,y i   

   z j   

yann lecun

features  

using psd to train a hierarchy of features
using psd to train a hierarchy of features

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd

   y i       y   2

w d z

       j .

   z j   

y

z

   z j   

ge   w e ,y i   

   z        z   2

features  

y

ge   w e ,y i   

yann lecun

using psd to train a hierarchy of features
using psd to train a hierarchy of features

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd
phase 4: use encoder + absolute value as 2nd feature extractor

y

ge   w e ,y i   

   z j   

   z j   

ge   w e ,y i   

features  

yann lecun

using psd to train a hierarchy of features
using psd to train a hierarchy of features

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd
phase 4: use encoder + absolute value as 2nd feature extractor
phase 5: train a supervised classifier on top
phase 6 (optional): train the entire system with supervised back-propagation

y

ge   w e ,y i   

yann lecun

   z j   

   z j   

classifier

ge   w e ,y i   

features  

using psd features for object recognition
using psd features for object recognition

64 filters on 9x9 patches trained with psd 
with linear-sigmoid-diagonal encoder

yann lecun

multistage hubel-wiesel architecture: filters
multistage hubel-wiesel architecture: filters

after psd

after supervised refinement

stage 1

stage2

yann lecun

results on caltech101 with sigmoid non-linearity
results on caltech101 with sigmoid non-linearity

      like  hmax  model

yann lecun

using a few more tricks...
using a few more tricks...
pyramid pooling on last layer: 1% improvement over regular pooling
shrinkage non-linearity + lateral inhibition: 1.6% improvement over tanh
discriminative term in sparse coding: 2.8% improvement 

yann lecun

results on caltech101: purely supervised 
results on caltech101: purely supervised 
with soft-shrink, l2 pooling, contrast id172
with soft-shrink, l2 pooling, contrast id172

supervised learning with soft-shrinkage non-linearity, l2 complex cells, and 
sparsity penalty on the complex cell outputs: 71%

yann lecun

what does local contrast id172 do?
what does local contrast id172 do?

original

reconstuction
with  lcn

reconstruction
without  lcn

yann lecun

why do random filters work?
why do random filters work?

random
filters
for
simple
cells

trained
filters
for
simple
cells

yann lecun

optimal
stimuli
for  each
complex  
cell

small norb dataset
small norb dataset

two-stage system: error rate versus number of labeled training samples 

no  id172
no  id172

random  filters

unsup  filters

sup  filters

unsup+sup  filters

yann lecun

 convolutional sparse coding
 convolutional sparse coding

convolutional psd
convolutional psd

[kavukcuoglu,  sermanet,  boureau,  mathieu,  lecun.  nips  2010]:  convolutional  psd

[zeiler,  krishnan,  taylor,  fergus,  cvpr  2010]:  deconvolutional  network
[lee,  gross,  ranganath,  ng,    icml  2009]:  convolutional  boltzmann  machine
[norouzi,  ranjbar,  mori,  cvpr  2009]:    convolutional  boltzmann  machine
[chen,  sapiro,  dunson,  carin,  preprint  2010]:  deconvolutional  network  with  
automatic  adjustment  of  code  dimension.

yann lecun

convolutional training
convolutional training

problem: 

with patch-level training, the learning algorithm must reconstruct 
the entire patch with a single feature vector
but when the filters are used convolutionally, neighboring feature 
vectors will be highly redundant

patch  level  training  produces
lots  of  filters  that  are  shifted
versions  of  each  other.

yann lecun

convolutional sparse coding
convolutional sparse coding
replace the dot products with dictionary element by convolutions.

input y is a full image
each code component zk is a feature map (an image)
each dictionary element is a convolution kernel

regular sparse coding

convolutional s.c.

y

=

.

   

k

wk

*

zk

   deconvolutional networks    [zeiler, taylor, fergus cvpr 2010]

yann lecun

convolutional psd: encoder with a soft sh() function 
convolutional psd: encoder with a soft sh() function 

convolutional formulation

extend sparse coding from patch to image

patch based learning

convolutional learning

yann lecun

pedestrian detection, face detection
pedestrian detection, face detection

yann lecun

[osadchy,miller  lecun  jmlr  2007],[kavukcuoglu  et  al.  nips  2010]

convnet architecture with multi-stage features
convnet architecture with multi-stage features

feature maps from all stages are pooled/subsampled and sent to the 
final classification layers

pooled low-level features: good for textures and local motifs
high-level features: good for    gestalt    and global shape

filter+tanh
64 feat maps

av pooling
2x2

filter+tanh

input
78x126xrgb

filter+tanh
22 feat maps

l2 pooling
3x3

yann lecun

[sermanet,  chintala,  lecun  arxiv:1204.3968,  2012]

pedestrian detection (inria dataset)
pedestrian detection (inria dataset)

yann lecun

[kavukcuoglu  et  al.  nips  2010]

convolutional psd pre-training for pedestrian detection 
convolutional psd pre-training for pedestrian detection 

convpsd pre-training improves the accuracy of pedestrian detection over 
purely supervised training from random initial conditions.

yann lecun

convolutional psd pre-training for pedestrian detection 
convolutional psd pre-training for pedestrian detection 

trained on inria. tested on inria, daiid113r, tud-brussles, eth
same testing protocol as in [dollar et al. t.pami 2011]

yann lecun

results on    near scale    images (>80 pixels tall, no occlusions)
results on    near scale    images (>80 pixels tall, no occlusions)

inria
p=288

tudbrussels
p=508

daiid113r
p=21790

eth
p=804

yann lecun

results on    reasonable    images (>50 pixels tall, few occlusions)
results on    reasonable    images (>50 pixels tall, few occlusions)

inria
p=288

tudbrussels
p=508

daiid113r
p=21790

eth
p=804

yann lecun

        musical genre 
        musical genre 

       recognition
       recognition

       same architecture, different data
       same architecture, different data

[henaff  et  al.  ismir  2011]

yann lecun

convolutional psd features on time-frequency signals
convolutional psd features on time-frequency signals

input:    constant q transform    over 46.4ms windows (1024 samples)

96 filters, with frequencies spaced every quarter tone (4 octaves)

architecture:

input: sequence of contrast-normalized cqt vectors
1: psd features, 512 trained filters
 rectification
2: shrinkage function 
3: pooling over 5 seconds
4: linear id166 classifier
5: pooling of id166 categories over 30 seconds

   

gtzan dataset

1000 clips, 30 second each
10 genres: blues, classical, country, disco, hiphop, jazz, metal, pop, 
reggae and rock. 

results

84% correct classification
(state of the art is at 92% with many features)

yann lecun

architecture: contrast norm     filters     shrink     max pooling
architecture: contrast norm     filters     shrink     max pooling

c
o
n
t
r
a
s
t
  
n
o
r
m
a
l
i
z
a
t
i
o
n

s
u
b
t
r
a
c
t
i
v
e
+
d

i
v
i
s
i
v
e
  

f

i
l
t
e
r
s

  

s
h
r
i

n
k
a
g
e

  

m
a
x
p
o
o
l
i

n
g

  
(
5
s
)

i

l
n
e
a
r
  
c
l
a
s
s
i
f
i
e
r

single  stage  convolutional  network
training  of  filters:  psd  (unsupervised)

yann lecun

constant q transform over 46.4 ms     contrast id172
constant q transform over 46.4 ms     contrast id172

subtractive+divisive  contrast  id172

yann lecun

convolutional psd features on time-frequency signals
convolutional psd features on time-frequency signals

octave-wide features                                     full 4-octave features

minor 3rd

perfect 4th

perfect 5th

quartal chord

major triad

transient

yann lecun

psd features on 
psd features on 
constant-q transform
constant-q transform

octave-wide features 

encoder basis functions 

decoder basis functions

yann lecun

time-frequency
time-frequency
features
features

octave-wide features on 
8 successive acoustic 
vectors 

almost no temporal 
structure in the 
filters!

yann lecun

accuracy on gtzan dataset (small, old, etc...)
accuracy on gtzan dataset (small, old, etc...)

accuracy: 83.4%. state of the art: 84.3%
very fast

yann lecun

        learning 
        learning 

       mid-level features
       mid-level features

[boureau  et  al.,  cvpr  2010,  icml  2010,  cvpr  2011]

yann lecun

convnets and    conventional    vision architectures are similar
convnets and    conventional    vision architectures are similar

filter
bank  

non  

linearity

feature
pooling  

filter
bank  

non  

linearity

feature
pooling  

classifier

oriented
  edges

wta histogram

(sum)

k  means

sift

pyramid
histogram
(sum)

id166  with
histogram
intersection
kernel

can't we use the same tricks as convnets to train the second stage of a 
   conventional vision architecture?
stage 1: sift
stage 2: sparse coding over neighborhoods + pooling

yann lecun

using dl/convnet ideas in    conventional    recognition 
using dl/convnet ideas in    conventional    recognition 
systems
systems

adapting insights from convnets:

[boureau  et  al.  cvpr  2010]
jointly encoding spatial neighborhoods instead of single points: 
increase spatial receptive fields for higher-level features

use max pooling instead of average pooling
train supervised dictionary for sparse coding

this yields state-of-the-art results:

75.7% on caltech-101 (+/-1.1%): record for single system
85.6% on 15-scenes (+/- 0.2): record!

yann lecun

the competition: sift + sparse-coding + pmk-id166 
the competition: sift + sparse-coding + pmk-id166 
replacing id116 with sparse coding

[yang 2008] [boureau, bach, ponce, lecun 2010]

yann lecun

sparse coding within clusters 
sparse coding within clusters 

splitting the sparse coding into clusters

only similar things get pooled together
[boureau,  et  al.  cvpr  2011]

yann lecun

   learning 
   learning 

   invariant features
   invariant features

   (learning complex cells)
   (learning complex cells)

[kavukcuoglu,  ranzato,  fergus,  lecun,  cvpr  2009]
[gregor  &  lecun  2010]

yann lecun

learning invariant features with l2 group sparsity
learning invariant features with l2 group sparsity

unsupervised psd ignores the spatial pooling step.
could we devise a similar method that learns the pooling layer as well?
idea [hyvarinen & hoyer 2001]: group sparsity on pools of features
minimum number of pools must be non-zero
number of features that are on within a pool doesn't matter
pools tend to regroup similar features

e (y ,z )=   y    w d z   2+    z     ge(w e ,y )   2+    

j        

k    p j

2
z k

   y i       y   2

w d z

input y

z

            k    p j    zk
2   

ge   w e ,y i   

   z        z   2

l2  norm  within  
each  pool

yann lecun

       j .

features  

learning invariant features with l2 group sparsity
learning invariant features with l2 group sparsity
idea: features are pooled in group. 

sparsity: sum over groups of l2 norm of activity in group.

[cardoso 1998, hyv  rinen hoyer 2001]: multidim ica, subspace ica 

square, decoder only (cardoso), encoder only (hyv  rinen), 

[welling, hinton, osindero nips 2002]: pooled product of experts (poe)
overcomplete, encoder only, log student-t penalty on l2 pooling
[kavukcuoglu, ranzato, fergus lecun, cvpr 2009]: invariant psd (ipsd)

overcomplete, encoder-decoder (like psd), l2 pooling

[le et al. nips 2011]: reconstruction ica (rica)

same as [kavukcuoglu 2010] with linear encoder and tied decoder 

[gregor & lecun arxiv:1006:0448,  2010] [le et al. icml 2012]

locally-connect non shared (tiled) encoder-decoder

input

y

yann lecun

encoder  only  (poe,  ica),

decoder  only  or

encoder  decoder  (ipsd,  rica)

simple  
features  

l2  norm  within  
each  pool

z

            k    p j    zk
2   

       j .

invariant
features  

pooling similar features using group sparsity
pooling similar features using group sparsity

a sparse-overcomplete version of  hyvarinen's subspace ica
decoder ensures reconstruction (unlike ica which requires orthonogonal matrix)
1. apply filters on a patch (with suitable non-linearity)
2. arrange filter outputs on a 2d plane
3. square filter outputs
4. minimize sqrt of sum of blocks of squared filter outputs

[kavukcuoglu,  ranzato,  fergu,  lecun,  cvpr  2009]
[jenatton,  obozinski,  bach  aistats  2010]  [le  et  al.  nips2011]

yann lecun

groups are local in a 2d topographic map
groups are local in a 2d topographic map

the filters arrange 
themselves spontaneously so 
that similar filters enter the 
same pool.
the pooling units can be seen 
as complex cells
outputs of pooling units are 
invariant to local 
transformations of the input
for some it's translations, 
for others rotations, or 
other transformations.

yann lecun

pinwheels?
pinwheels?

does that look 
pinwheely to 
you?

yann lecun

image-level training, local filters but no weight sharing
image-level training, local filters but no weight sharing

training on 115x115 images. kernels are 15x15 (not shared across space!)

decoder

reconstructed  input

[gregor & lecun 2010]
local receptive fields
no shared weights
4x overcomplete
l2 pooling
group sparsity over pools

(inferred)  code

predicted  code

input

encoder

yann lecun

image-level training, local filters but no weight sharing
image-level training, local filters but no weight sharing

topographic maps of 
continuously-varying 
features
local overlapping pools are 
invariant complex cells

[gregor & lecun 
arxiv:1006.0448] 
(double tanh encoder)

[le et al. icml'12] 
(linear encoder)

yann lecun

image-level training, local filters but no weight sharing
image-level training, local filters but no weight sharing

training on 115x115 images. kernels are 15x15 (not shared across space!)

yann lecun

k  obermayer  and  gg  blasdel,  journal  of  
neuroscience,  vol  13,  4114  4129  (monkey)

119x119  image  input

100x100  code

20x20  receptive  field  size

sigma=5

michael  c.  crair,  et.  al.  the  journal  of  neurophysiology  
vol.  77  no.  6  june  1997,  pp.  3381  3385  (cat)

image-level training, local filters but no weight sharing
image-level training, local filters but no weight sharing

color indicates orientation (by fitting gabors)

yann lecun

theory of repeated [filter bank     l2 pooling     average pooling]
theory of repeated [filter bank     l2 pooling     average pooling]

st  phane mallat's    scattering transform   : theory of convnet-like architectures
[mallat & bruna cvpr 2011] classification with scattering operators
[mallat & bruna, arxiv:1203.1513 2012] invariant scattering convolution 
networks
[mallat cpam 2012] group invariant scattering

yann lecun

       sparse coding using
       sparse coding using
        lateral inhibition
        lateral inhibition

[gregor,  szlam,  lecun,  nips  2011]

yann lecun

invariant features lateral inhibition
invariant features lateral inhibition

replace the l1 sparsity term by a lateral inhibition matrix
easy way to impose some structure on the sparsity 

yann lecun

[gregor,  szlam,  lecun  nips  2011]

invariant features via lateral inhibition: structured sparsity
invariant features via lateral inhibition: structured sparsity

 each edge in the tree indicates a zero in the s matrix (no mutual inhibition)

sij is larger if two neurons are far away in the tree

yann lecun

yann lecun

yann lecun

yann lecun

invariant features via lateral inhibition: topographic maps
invariant features via lateral inhibition: topographic maps

non-zero values in s form a ring in a 2d topology

input patches are high-pass filtered

yann lecun

invariant features via lateral inhibition: topographic maps
invariant features via lateral inhibition: topographic maps

non-zero values in s form a ring in a 2d topology

left: no high-pass filtering of input
right: patch-level mean removal

yann lecun

invariant features via lateral excitation: topographic maps 
invariant features via lateral excitation: topographic maps 

short-range lateral excitation + l1 sparsity

yann lecun

 
 

     learning what/where 
     learning what/where 

features with
features with

 temporal constancy
 temporal constancy

[gregor  &  lecun  arxiv:1006.0448,  2010]

yann lecun

invariant features through temporal constancy 
invariant features through temporal constancy 

object is cross-product of object type and instantiation parameters
mapping units [hinton 1981], capsules [hinton 2011]

object  type

[karol  gregor  et  al.]

object  size

yann lecun

small

medium

large

what-where auto-encoder architecture
what-where auto-encoder architecture

decoder

st

st  1

st  2

predicted

input

w1

w1

w1

w2

t

c1

t

c1

t  1

c1

t  1

c1

f       w 1

f       w 1

t  2

c1

t  2

c1
f       w 1

encoder

st

st  1

yann lecun

t

c2

t

c2

f

inferred  

code

predicted

code

   w 2
   w 2
st  2

   w 2

input

low-level filters connected to each complex cell
low-level filters connected to each complex cell

c1
(where)

c2
(what)

yann lecun

generating from the network
generating from the network

input

yann lecun

learning  features  and  pose  
learning  features  and  pose  
embedding  with  drlim
embedding  with  drlim

dimensionality  reduction  by  learning  
dimensionality  reduction  by  learning  

and  invariant  mapping.
and  invariant  mapping.

contrastive  loss  function  for  metric  learning
contrastive  loss  function  for  metric  learning

[goldberger,  roweis,  hinton,  salakhutdinov,  nips  2004]:  nca
[chopra,  hadsell,  lecun  cvpr  2005]
[hadsell,  chopra,  lecun,  cvpr  2006]:  drlim
[taylor,  fergus,  lecun,  bregler,  eccv  2010]
[taylor,  fergus,  williams,  spiro,  bregler  nips  2010]
[taylor,  spiro,  bregler,  fergus,  cvpr  2011]

yann lecun

drlim: metric learning
drlim: metric learning

id84 by learning an invariant mapping

step 1: construct neighborhood graph.
step 2: choose a parameterized family of functions.
step 3: optimize the parameters such that:
outputs for similar samples are pulled closer.
outputs for dissimilar samples are pushed away.

g w

yann lecun

joint  work  with  sumit  chopra:  hadsell  et  al.  cvpr  06;  chopra  et  al.,  cvpr  05

drlim: contrative id168
drlim: contrative id168

id84 by learning an invariant mapping

step 1: construct neighborhood graph.
step 2: choose a parameterized family of functions.
step 3: optimize the parameters such that:
outputs for similar samples are pulled closer.
outputs for dissimilar samples are pushed away.

id168 for inputs x1 and x2  with binary label y and 
dw = ||gw(x1) - gw(x2)||2: 

yann lecun

joint  work  with  sumit  chopra:  hadsell  et  al.  cvpr  06;  chopra  et  al.,  cvpr  05

drlim: contrative id168
drlim: contrative id168

id84 by learning an invariant mapping

step 1: construct neighborhood graph.
step 2: choose a parameterized family of functions.
step 3: optimize the parameters such that:
outputs for similar samples are pulled closer.
outputs for dissimilar samples are pushed away.

g w

yann lecun

joint  work  with  sumit  chopra:  hadsell  et  al.  cvpr  06;  chopra  et  al.,  cvpr  05

siamese architecture
siamese architecture

ew

   gw   x1      gw   x2      2

gw   x   

w

gw   x   

x1
  

x2

siamese  architecture  [broid113y,  sackinger,  shah,  lecun  1994]

yann lecun

siamese architecture and id168
siamese architecture and id168

id168:
outputs 
corresponding to 
input samples 
that are 
neighbors in the 
neigborhood 
graph should be 
nearby
outputs for input 
samples that are 
not neighbors 
should be far 
away from each 
other

yann lecun

make  this  small

make  this  large

dw

dw

   g w     x1      g w    x2      

   g w     x1      g w    x2      

gw     x1   

gw     x2   

gw     x1   

gw     x2   

x1

x 2

x1

x 2

similar  images  (neighbors  
in  the  neighborhood  graph)

dissimilar  images  
(non  neighbors  in  the  
neighborhood  graph)

id168
id168

lsimilar= 1
2

2
dw

ldissimilar= 1
2

{max   0 ,m    d w   }2

margin
m

dw

dw

   g w     x1      g w    x2      

   g w     x1      g w    x2      

gw     x1   

gw     x2   

gw     x1   

gw     x2   

x1

x 2

x1

x 2

id168:
pay quadratically 
for making outputs 
of neighbors far 
apart
pay quadratically 
for making outputs 
of non-neigbors 
smaller than a 
margin m

yann lecun

a  digit  manifold  with  invariance  to  shifts
a  digit  manifold  with  invariance  to  shifts

  training  set:  3000     4     and  
3000     9     from  mnist.  
each  digit  is  shifted  
horizontally  by    6,    3,  3,  
and  6    pixels
neighborhood  graph:  5  
nearest  neighbors  in  
euclidean  distance,  and  
shifted  versions  of  self  and  
nearest  neighbors
output  dimension:  2
test  set  (shown)  1000     4     
and  1000     9   

yann lecun

norb dataset: pose manifold with lighting invariance
norb dataset: pose manifold with lighting invariance

objective: learn a mapping into 3-space that is invariant to lighting.
dataset: 972 images of a single object. 

18 azimuths, 
9 elevations, 
6 lighting conditions
neighborhood graph: 
nearest neighbors in azimuth.
nearest neighbors in elevation.
independent of lighting condition.

architecture: input dimension: 48x48. output dimension: 3.                     
            a 2-layer fully connected neural network.

yann lecun

norb dataset     automatic discovery of the 
norb dataset     automatic discovery of the 
viewpoint manifold with invariance to lighting
viewpoint manifold with invariance to lighting

yann lecun

different  views  of  a  single  manifold  are  shown

drlim: learning a face manifold for face verification
drlim: learning a face manifold for face verification

dissimilar

similar

dissimilar

similar

yann lecun

face  verification  datasets:  at&t/orl
face  verification  datasets:  at&t/orl

     the  at&t/orl  dataset  
     total  subjects:  40.  images  per  subject:  10.    total  images:  400.
     images  had  a  moderate  degree  of  variation  in  pose,  lighting,  expression  and  head  position.
     images  from  35  subjects  were  used  for  training.  images  from  5  remaining  subjects  for  testing.
     training  set  was  taken  from:  3500  genuine  and  119000  impostor  pairs.
     test  set  was  taken  from:  500  genuine  and  2000  impostor  pairs.
     http://www.uk.research.att.com/facedatabase.html

at&t/orl  
dataset

internal  state  for  genuine  and  impostor  pairs
internal  state  for  genuine  and  impostor  pairs

classification  examples
classification  examples

example:  correctly  classified  genuine  pairs

energy:  0.3159

energy:  0.0043

example:  correctly  classified  impostor  pairs

energy:  0.0046

energy:  20.1259

energy:  32.7897

energy:  5.7186

example:  mis  classified  
pairs

energy:  10.3209

energy:  2.8243

drlim for invariant id171
drlim for invariant id171

[raia  hadsell's  phd  thesis,  2008]

co-location patch data
multiple tourist photos
3d reconstruction
groundtruth matches 

input  64x64

layer  1
6x60x60

layer  2
6x20x20

layer  3
21x15x15

layer  4
21x5x5

layer  5
55x1x1

output
25x1x1

c
o
n
v
o
u

l

t
i

o
n
s

p
o
o

l
i

n
g

c
o
n
v
o
u

l

t
i

o
n
s

p
o
o

l
i

n
g

c
o
n
v
o
u

l

t
i

o
n
s

f
u

l
l
  
c
o
n
n
e
c
t

yann lecun

data  from:  winder  and  brown,  cvpr  07

drlim for invariant id171
drlim for invariant id171

error on test set at 
95% detection rate: 
          0.93%

yann lecun

         hardware
         hardware

         implementations
         implementations

yann lecun

higher end: fpga with neuflow architecture
higher end: fpga with neuflow architecture

now running on picocomputing 8x10cm high-performance fpga board

virtex 6 lx240t: 680 mac units,  20 neuflow tiles

full scene labeling at 20 frames/sec (50ms/frame) at 320x240

yann lecun

new  board  with  virtex  6

newflow: architecture
newflow: architecture

grid  of  passive  processing  tiles  (pts)

[x20 on a virtex6 lx240t]
yann lecun

mem

dma

multi  port  memory  
controller  (dma)

[x12 on a v6 lx240t]

cpu

risc  cpu,  to  
reconfigure  tiles  and  
data  paths,  at  runtime

global  network  on  chip  to  
allow  fast  reconfiguration

newflow: processing tile architecture
newflow: processing tile architecture

term  by:  term  
streaming    operators  
(mul,div,add,su
b,max)

[x8,2 per tile]

configurable  router,  
to  stream  data  in  
and  out  of  the  tile,  to  
neighbors  or  dma  
ports

[x20]

configurable  piece  wise  
linear    or  quadratic  
mapper

[x4]

full  1/2d    parallel  convolver  
with  100  mac  units

[x4]

yann lecun

configurable  bank  of  
fifos  ,  for  stream  
buffering,  up  to  10kb  
per  pt

[x8]

[virtex6 lx240t]

newflow asic: 2.5x5 mm, 45nm, 0.6watts, 160gops 
newflow asic: 2.5x5 mm, 45nm, 0.6watts, 160gops 

collaboration nyu-purdue (eugenio culurciello's group)
suitable for vision-enabled embedded and mobile devices
status: waiting for first samples end of 2012

yann lecun

pham,  jelaca,  farabet,  martini,  lecun,  culurciello  2012]  

newflow: performance
newflow: performance

intel
intel

i7  4  cores
i7  4  cores

neuflow
neuflow
virtex4
virtex4

neuflow
neuflow
virtex  6
virtex  6

nvidia  
nvidia  
gt335m
gt335m

neuflow
neuflow
ibm  45nm
ibm  45nm

nvidia
nvidia
gtx480
gtx480

peak

gop/sec
actual
gop/sec

fps

power  
(w)

40

12
14
50

40

37
46
10

160

147
182
10

182

54
67
30

embed?
(gop/s/w) 0.24

3.7

14.7

1.8

160

147
182
0.6

245

1350

294
374
220

1.34

yann lecun

 
 

       software platforms:
       software platforms:

 torch7
 torch7

http://www.torch.ch
http://www.torch.ch

[collobert,  kavukcuoglu,  farabet  2011]

yann lecun

torch7
torch7

ml/vision development environment

collaboration between idiap, nyu, and nec labs
uses the lua interpreter/compiler as a front-end
very simple and efficient scripting language
ultra simple and natural syntax
it's like scheme underneath, but with a    normal    syntax.
lua is widely used in the video game and web industries

torch7 extend lua with numerical, ml, vision libraries

powerful vector/matrix/tensor engine (developed by us)
interfaces to numerical libraries, opencv, etc
back-ends for sse, openmp, cuda, arm-neon,...
qt-based gui and graphics
open source: http://www.torch.ch/
first released in early 2012

yann lecun

torch7
torch7

ml/vision development environment

as simple as matlab, but faster and 
better designed as a language.
2d convolution
x = torch.rand(100,100)
k = torch.rand(10,10)
res1 = torch.conv2(x,k)

matrix and vector operations
> m = torch.tensor(2,2):fill(2)
> n = torch.tensor(2,4):fill(3)
> x = torch.tensor(2):fill(4)
> y = torch.tensor(2):fill(5)
> = x*y -- dot product
40
> = m*x --- matrix-vector
 16
 16
[torch.tensor of dimension 2]

training a neural net
mlp = nn.sequential()
mlp:add( nn.linear(10, 25) ) -- 10 input, 25 hidden units
mlp:add( nn.tanh() ) -- some hyperbolic tangent transfer function
mlp:add( nn.linear(25, 1) ) -- 1 output
criterion = nn.msecriterion() -- mean squared error criterion
trainer = nn.stochasticgradient(mlp, criterion)
trainer:train(dataset) -- train using some examples

yann lecun

acknowledgements
acknowledgements

y-lan boureau

kevin jarrett

koray kavukcuoglu

camille couprie

karol gregor

cl  ment farabet

arthur szlam

marc'aurelio ranzato

rob fergus

pierre sermanet

laurent najman (esiee)

yann lecun

eugenio culurciello 
(purdue)

   the end
   the end

yann lecun

