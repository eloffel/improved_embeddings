neural networks: 
representation 
non-linear 
hypotheses 

machine learning 

non-linear classification 

x2 

x1 

andrew ng 

id161: car detection 

cars 

not a car 

testing: 
 
 
what is this?   

andrew ng 

pixel 1 

learning 
algorithm 

pixel 2 

raw image 

pixel 2 

cars 
   non   -cars 

pixel 1 

andrew ng 

pixel 1 

learning 
algorithm 

pixel 2 

raw image 

pixel 2 

cars 
   non   -cars 

pixel 1 

andrew ng 

pixel 1 

raw image 

pixel 2 

learning 
algorithm 

pixel 2 

50 x 50 pixel images    2500 pixels 
                        (7500 if rgb) 

pixel 1 intensity 

pixel 2 intensity 

pixel 2500 intensity 

cars 
   non   -cars 

pixel 1 

quadratic features (               ):    3 million
 

                  features 

 

andrew ng 

neural networks: 
representation 
model 
representation i 

machine learning 

neural networks 

origins: algorithms that try to mimic the brain. 
was very widely used in 80s and early 90s; popularity 
diminished in late 90s. 
recent resurgence: state-of-the-art technique for many 
applications 

neuron in the brain 

andrew ng 

neurons in the brain 

[credit: us national institutes of health, national institute on aging] 

andrew ng 

neuron model: logistic unit 

sigmoid (logistic) activation function. 

andrew ng 

neural network 

   activation    of unit    in layer  

matrix of weights controlling 
function mapping from layer    to 
layer 

if network has      units in layer    ,           units in layer           , then 
will be of dimension                               . 

andrew ng 

neural networks: 
representation 
model 
representation ii 

machine learning 

forward propagation: vectorized implementation 

add              . 

andrew ng 

neural network learning its own features 

layer 1 

layer 2 

layer 3 

andrew ng 

other network architectures 

layer 1 

layer 2 

layer 3 

layer 4 

andrew ng 

neural networks: 
representation 

examples and 
intuitions i 

machine learning 

non-linear classification example: xor/xnor 
    ,      are binary (0 or 1). 

x2 

x2 

x1 

x1 

andrew ng 

simple example: and 

1.0 

0 
0 
1 
1 

0 
1 
0 
1 

andrew ng 

example: or function 

-10 

20 

20 

0 
0 
1 
1 

0 
1 
0 
1 

andrew ng 

neural networks: 
representation 

examples and 
intuitions ii 

machine learning 

negation: 

0 
1 

andrew ng 

putting it together:  

-30 

20 

20 

10 

-20 

-20 

-10 

20 

20 

0 
0 
1 
1 

0 
1 
0 
1 

andrew ng 

neural network intuition 

layer 1 

layer 2 

layer 3 

layer 4 

andrew ng 

handwritten digit classification 

[courtesy of yann lecun] 

andrew ng 

handwritten digit classification 

[courtesy of yann lecun] 

andrew ng 

andrew ng 

neural networks: 
representation 

multi-class 
classification 

machine learning 

andrew ng 

multiple output units: one-vs-all. 

pedestrian 

car 

motorcycle 

truck 

want                      ,                         ,                        ,   etc. 

when pedestrian         when car 

when motorcycle 

andrew ng 

multiple output units: one-vs-all. 

want                      ,                         ,                        ,   etc. 

when pedestrian         when car 

when motorcycle 

training set:  

       one of          , 

 ,             , 

 

pedestrian    car  motorcycle   truck 

andrew ng 

andrew ng 

neural networks: 
learning 

cost function 

machine learning 

andrew ng 

neural network (classification) 

layer 1 

layer 2 

layer 3 

layer 4 

binary classification 
  
 
  1 output unit 
 

total no. of layers in network 

no. of units (not counting bias unit) in 
layer  

multi-class classification (k classes) 

 

 

e.g.           ,             ,                 , 

pedestrian  car  motorcycle   truck 

  k output units 
 

andrew ng 

cost function 

id28: 
 
 
 
neural network: 

andrew ng 

andrew ng 

neural networks: 
learning 
gradient 
descent 

andrew ng 

machine learning 

have some function 

want  

outline: 

    start with some 

    keep changing              to reduce                     

until we hopefully end up at a minimum 

andrew ng 

j(   0,   1) 

   0 

   1 

andrew ng 

j(   0,   1) 

   0 

   1 

andrew ng 

id119 algorithm 

andrew ng 

andrew ng 

neural networks: 
learning 
id26 
algorithm 

machine learning 

andrew ng 

gradient computation 

need code to compute: 
-
-

  
  

andrew ng 

id26 algorithm 
training set 

set                    (for all          ). 
for 

set 
perform forward propagation to compute         for       
using       , compute 
compute  

andrew ng 

andrew ng 

neural networks: 
learning 
random 
initialization 

machine learning 

andrew ng 

initial value of 

for id119 and advanced optimization 
method, need initial value for     . 
opttheta = fminunc(@costfunction,   
 

initialtheta, options) 

 

consider id119 
 
set  

         ? 
initialtheta = zeros(n,1) 

 

 

 

andrew ng 

zero initialization 

after each update, parameters corresponding to inputs going into each of 
two hidden units are identical.  

andrew ng 

random initialization: symmetry breaking 

initialize each            to a random value in  
(i.e.                            ) 
 
e.g. 

 

- init_epsilon; 

theta1 =  rand(10,11)*(2*init_epsilon)  
 
 
theta2 =  rand(1,11)*(2*init_epsilon)  
 

- init_epsilon; 

 

andrew ng 

andrew ng 

neural networks: 
learning 
putting it 
together 

andrew ng 

machine learning 

training a neural network 
pick a network architecture (connectivity pattern between neurons) 

no. of input units: dimension of features 
no. output units: number of classes 
reasonable default: 1 hidden layer, or if >1 hidden layer, have same no. of hidden 
units in every layer (usually the more the better) 

andrew ng 

training a neural network 
1. randomly initialize weights 
2.
3.
4.

implement forward propagation to get               for any   
implement code to compute cost function 
implement backprop to compute partial derivatives 

for i = 1:m 

perform forward propagation and id26 using 
example 
(get activations        and delta terms       for 

           ). 

andrew ng 

training a neural network 
5. use gradient checking to compare                   computed using 

id26 vs. using  numerical estimate of gradient          
of          . 
then disable gradient checking code. 

6. use id119 or advanced optimization method with 

id26 to try to  minimize          as a function of 
parameters 

andrew ng 

andrew ng 

andrew ng 

neural networks: 
learning 

id26 
example: autonomous 
driving 

andrew ng 

machine learning 

[courtesy of dean pomerleau] 

andrew ng 

[courtesy of dean pomerleau] 

andrew ng 

andrew ng 

neural networks: 
id171 

autoencoder 

machine learning 

andrew ng 

autoencoders (and sparsity) 

andrew ng 

sparse autoencoders 

andrew ng 

unsupervised id171/self-taught learning 

andrew ng 

unsupervised pre-training + fine-tuning 

andrew ng 

andrew ng 

neural networks: 
id171 
deep 
learning 

andrew ng 

machine learning 

unsupervised pre-training + fine-tuning 

andrew ng 

andrew ng 

andrew ng 

