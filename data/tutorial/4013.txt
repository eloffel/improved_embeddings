ipam summer school 2012 

tutorial on:  

deep learning 

geoffrey hinton 

canadian institute for advanced research 

& 

department of computer science 

university of toronto 

part 3: 

some applications of deep learning 

       id103 

 

       deep learning is now being deployed in the latest 

id103 systems. 

       object recognition 

       deep learning is breaking records on really tough 

object recognition tasks. 

      

      

id162 
       deep learning finds efficient codes for images. 
 predicting the next character in a string 
       deep learning reads wikipedia and discovers the 

meaning of life. 

using deep neural nets for  

id103 

       each phoneme is modeled by one or more  

three-state hidden markov model.  

       the sound-wave is pre-processed to yield 
frames of acoustic coefficients every 10ms. 

       a deep neural net looks at a window of frames 

and outputs the probabilities of the various 
possible id48 states for the central frame. 

       a decoder then uses these probabilities to find 

the best string of words.  

 
 

phone recognition on the timit benchmark 

(mohamed, dahl, & hinton, 2011) 

 

183 id48-state  labels 

not pre-trained 
2000 binary hidden units  

2000 binary hidden units  

2000 binary hidden units  

2000 binary hidden units  

15 frames of 40 filterbank 
outputs + deltas & delta deltas 

        after standard post-

processing using a bi-phone 
model, a deep net with 8 
layers gets 20.7% error rate. 
       the best previous speaker- 
independent result on timit 
was 24.4% and this required 
averaging several models. 
       a deep net that uses 3-way 

factors to model the 
covariance structure of the 
filterbank outputs gets 20.5% 
without using deltas (dahl 
et.al. 2010) 

what happened next 

       this method of using deep nets for speech 

recognition was ported to msr by george dahl 
and abdel-rahman mohamed and to google by 
navdeep jaitly. 

       it forms the basis of a speech recognizer 

recently deployed by microsoft. 

       google has also been developing this 

technology (see next slide).  

word error rates from msr, ibm, and 

the google speech group 

word error rates from msr, ibm, and 

the google speech group 

experiment on id163 1000 classes 

(1.3 million high-resolution training images) 

       the 2010 competition winner got 47% error for 
its first choice and 25% error for top 5 choices. 
       the current record is 45% error for first choice. 
      this uses methods developed by the winners 
of the 2011 competition. 

       our chief critic, jitendra malik, has said that this 

competition is a good test of whether deep 
neural networks really do work well for object 
recognition. 

a convolutional net for id163 

       alex krizhevsky developed a very deep 

convolutional neural net. 
      7 hidden layers not counting max pooling. 
      early layers are convolutional, last two layers 
are globally connected. 
      alex trains on random 224x224 patches from 
256x256 images to get more data. 
      uses rectified linear units in every layer. 
      uses competitive id172 to suppress 
hidden activities. 

the performance of alex   s net 

       at test time, use the central patch and the 4 corner 

patches and their reflections. 
      by combining the 10 predicted distributions alex 
gets an error rate which is about the same as 
the state-of-the-art.  

       if he uses    dropout    to regularize the weights in the 

globally connected layers (which contain most of 
the parameters) he gets 39% error for first choice 
and 19% for top 5 choices. 
      this is a big improvement over the current state 
of the art (45% for top choice & 25% for top 5). 

some examples from an earlier version of the net 

it can deal with a wide range of objects 

it makes some really cool errors 

using a deep autoencoder as a hash-function 

for finding approximate matches 

hash 
function 

   supermarket search    

another view of semantic hashing 

       fast retrieval methods typically work by 

intersecting stored lists that are associated with 
cues extracted from the query. 

       computers have special hardware that can 

intersect  32 very long lists in one instruction. 
      each bit in a 32-bit binary code specifies a list 
of half the addresses in the memory. 

       semantic hashing uses machine learning to map 

the retrieval problem onto the type of list 
intersection the computer is good at. 

semantic hashing for id162 

       currently,  id162 is typically done by 

using the captions. why not use the images too? 
      pixels are not like words: individual pixels do 
not tell us much about the content. 
      extracting object classes from images is hard. 

       maybe we should extract a real-valued vector 

that has information about the content? 
      matching real-valued vectors in a big 
database is slow and requires a lot of storage 
       short binary codes are easy to store and match 

a two-stage method 

       first, use semantic hashing with 30-bit binary 

codes to get a long    shortlist    of  promising 
images. 

       then use 256-bit binary codes to do a serial 

search for good matches. 
      this only requires a few words of storage per 
image and the serial search can be done 
using fast bit-operations. 

       but how good are the 256-bit binary codes? 

      do they find images that we think are similar? 

krizhevsky   s deep autoencoder 

the encoder 
has about 
67,000,000 
parameters.  
 
 it takes a few 
gtx 285 gpu 
days to train on 
two million 
images.  

256-bit binary code 

there is no 
theory to justify 
this architecture 

512 

1024 

2048 

4096 

8192 

1024 

1024 

1024 

autoencoder 

euclidean distance in 
pixel intensity space 

autoencoder 

euclidean distance in 
pixel intensity space 

autoencoder 

euclidean distance in 
pixel intensity space 

an obvious extension 

       use a multimedia auto-encoder that  represents 

captions and images in a single code. 
      the captions should help it extract more 
meaningful image features  such as   
   contains an animal    or    indoor image    

       rbm   s already work much better than standard 
lda topic models for modeling bags of words. 
      so the multimedia  auto-encoder should be       
+ a win (for images)                                            
+ a win (for captions)                                       
+ a win (for the interaction during training)  

a less obvious extension 

       semantic hashing gives incredibly fast retrieval 

but its hard to go much beyond 32 bits. 

       we can afford to use semantic hashing several 
times with variations of the query and merge the 
shortlists 
      its easy to enumerate the hamming ball 
around a query image address in ascending 
address order, so merging is linear time. 

       apply many transformations to the query image 

to get transformation independent  retrieval. 
      image translations are an obvious candidate. 

a better starting point for semantic 

hashing 

       the last hidden layer of a deep net for object 
recognition has a lot of information about the 
prominent objects in the image. 
      so use the last hidden layer as the input to an 
autoencoder. 

images that give similar activity vectors in the last hidden layer 

another failure of back-propagation 

   the most exciting version is back-

propagation through time. this should be 
able to learn distributed sequential 
   programs    to predict the next input vector. 
      it doesn   t work properly. 
      the gradient either explodes or dies. 
 

the equivalence between layered, 
feedforward nets and recurrent nets 
w1          w2 

time=3 

w1                   w2 

w3 w4 

w1                   w2 

w3 w4 

w1                   w2 

w3 w4 

w3          w4 

assume that there is a 
time delay of 1 in using 
each connection. 
the recurrent net is 
just a layered net that 
keeps reusing the 
same weights. 

time=2 

time=1 

time=0 

two ways to use curvature information 

to improve optimization 

       quasi-newton: exact minimization on a very 

crude quadratic approximation to the curvature. 

       hessian-free: partial minimization on a much 
better quadratic approximation to the curvature 
      put a huge amount of work into coming up 
with a good gauss-newton approximation to 
the curvature.  
      (see icml 2010 paper by james martens) 

what is a word 

       a word operates on a mental state to produce a 

new mental state. 

       if we want to model the mental state as a big 

vector, a word needs to define a transition matrix. 
      how can each of 100,000 words define a really 
big transition matrix without requiring a huge 
number of parameters? 

       maybe we should think of a word as a sequence of 

characters and allow each character to define a 
transition matrix? 

advantages of working with characters 
       the web is composed of character strings. 
       any learning method powerful enough to 

understand the world by reading the web ought to 
find it trivial to learn which strings make words 
(this turns out to be true). 

       pre-processing text to get words is a big hassle 
      what about morphemes (prefixes, suffixes etc) 
      what about subtle effects like    sn    words? 
      what about new york?   
      what about finnish 

..                           
       ymmartamattomyydellansakaan 

..                           

..                           

..                           

..                           

..                           

..                           
..                           

using 3-way factors to allow a character to create a 

whole transition matrix 

fu

fv

f

2000	
   
hidden	
   
units	
   

2000	
   
hidden	
   
units	
   

cfw
c 

t
f

each factor, f, 
defines a rank one 
f vu
matrix ,  

character:	
   
1-     of-     86	
   
each character, c, determines 
a gain        for each of these 
rank one matrices 

cfw

predicted	
   distribu8on	
   	
   
for	
   next	
   character.	
   	
   
	
   
it	
   is	
   a	
   lot	
   easier	
   to	
   
predict	
   86	
   characters	
   
than	
   100,000	
   words.	
   

training the character model 

       ilya sutskever used 5 million strings of 100 

characters taken from wikipedia. for each string 
he starts predicting at the 11th character. 

       it takes a month on a gpu board to get a really 

good model. it needs very big mini-batches. 

       ilya   s best model is about equal to the state of 
the art for character prediction, but works in a 
very different way from the best other models. 
      it can balance quotes and brackets over long 
distances. markov models cannot do this. 

how to use the model to continue a 

character string 

       given an initial string, we could generate 
whatever character the net thinks is most 
probable. 
      this degenerates into    the united states of 
the united states of the united states of        
       its better to get the net to produce a id203 

distribution for the next character and then  
sample from this distribution. 
      we then tell the net that the character we 
sampled was the real next character and ask 
it to predict the one after that, and so on. 

some text generated by the model 

in 1974 northern denver had been 
overshadowed by cnl, and several irish 
intelligence agencies in the mediterranean 
region. however, on the victoria, kings 
hebrew stated that charles decided to 
escape during an alliance. the mansion 
house was completed in 1882, the second in 
its bridge are omitted, while closing is the 
proton reticulum composed below it aims, 
such that it is the blurring of appearing on any 
well-paid type of box printer. 

 
he was elected president during the revolutionary 
war and forgave opus paul at rome. the regime 
of his crew of england, is now arab women's icons 
in  and the demons that use something between 
the characters    sisters in lower coil trains were 
always operated on the line of the ephemerable 
street, respectively, the graphic or other facility for 
deformation of a given proportion of large 
segments at rtus). the b every chord was a 
"strongly cold internal palette pour even the white 
blade." 
 
 

some completions produced by the model 

       sheila thrunges                               (most frequent) 

       shiela, thrungelini del rey                       (first try) 

       the meaning of life is literary recognition.  (6th try) 

what does it know? 

       it knows a huge number of words and a lot 
about proper names, dates, and numbers. 
       it is good at balancing quotes and brackets. 

      it can count brackets: none, one, many 

       it knows a lot about syntax but its very hard to 
pin down exactly what form this knowledge has. 
      its syntactic knowledge is not modular. 

       it knows a lot of semantic associations 
      e.g. it knows plato is associated with 
wittgenstein and cabbage is associated with 
vegetable. 

completing a sentence 
using the neural network 

 (after a lot more training) 

       t                                 the tradition of 
the meaning of life is 
the ancient human reproduction: it is 
less favorable to the good boy for 
when to remove her bigger.                       

 

part 4: 

 

a computational principle that explains 

sex, the brain, and sparse coding 

theme of this lecture 

       an interesting discovery in machine learning 

provides an explanation for two puzzling 
phenomena in biology. 

       the two phenomena appear to have nothing to 

do with one another, but actually there is one 
principal that explains both of them. 

       also, we can make neural nets work much better 

and explain why sparse coding improves 
classification. 

a problem with sexual reproduction 

       fitness depends on genes working well together. 

but sexual reproduction breaks up sets of co-
adapted genes.  
      this is a puzzle in the theory of evolution. 
       a recent paper by livnat, papadimitriou and 

feldman (pnas 2008) claims that breaking up 
complex co-adaptations is actually a good thing 
even though it may be bad in the short term. 
      it may help optimization in the long run. 
      it may make organisms more robust to changes 
in the environment. we show this is a big effect. 

a problem with neural communication 

       cortical neurons do signal processing by sending 
discrete spikes of activity with apparently random 
timing. 
      why don   t they communicate precise analog 
values by using the precise times of spikes. 
      surely analog values are more useful for signal 
processing? 

how spike timing could be used to compute 

scalar products 

       we want to take the scalar product of a vector of 
activations (coded as spike times) with a vector 
of  parameters (coded as synaptic weights). 

       the answer must then be converted back to a 

spike time.   

a picture of how an integrate-and-fire 
neuron could compute a scalar product 

 

     

 
 
 
e
g
r
a
h
c
 
f

o

 

n
o

i
t
c
e
n

j

i
 
f

o

 

e

t

a
r

end of time 
window 

w1 
t1 

w2 
t2 

w3 

t3 

real time       

       at the end of the time 

window, the total 
injected charge is the 
scalar product of the 
time advances and 
the weights. 

       but how do we 

convert this back into 
a spike time?  

 

t
     
 
e
g
r
a
h
c

wt'

converting accumulated charge into a spike 

time 

wt'

       at the end of the time window, 

add an additional input that 
injects charge at the rate: 

1

      

iw

i

       the total rate of injection of 
charge is then 1 and so the 
additional time taken to reach a 
threshold of      is: 

wt'   t
       so in the next window, the 

t

advance of the outgoing spike 
represents the scalar product 

wt'

time       

t

end of first 
time window 

does the brain really make use of spike 

times to communicate real numbers? 

      

in the hippocampus of a rat, the location of the rat is 
represented by place cells.  
       the precise time at which a place cell fires probably 

indicates where the rat is within that place field. 

       but there is not much evidence that spike times are used 

this way in sensory cortex. this leaves two main 
possibilities: 
       evolution failed to discover an obvious trick. 
       our ideas about signal processing are hopelessly 

wrong. 

how could 1 bit possibly be better? 

       maybe the kind of signal processing problem that  
the brain solves is very different from the kind of 
signal processing problem engineers solve. 
      engineers want to fit one model to data that 
they understand (e.g. a linear dynamical 
system). 
      the brain is confronted by a buzzing, blooming 
confusion. it needs to fit many different models 
and use the wisdom of crowds. 

why engineers need to understand the 

principles underlying neural 

communication 

       if we spread a really big neural net over many 

cores, we have to communicate the states of the 
neurons.  

       it would be really helpful if communicating 1 bit  
was actually better than sending a real number. 

an apparent change of topic: 

is there anything we cannot do with 

very big, deep neural networks? 

       it appears to be hard to do massive model 

averaging: 
      each net takes a long time to learn. 
      at test time we don   t want to run lots of 
different large neural nets. 

averaging many models 

       to win a machine learning competition (e.g. netflix) 
you need to use many different types of model and 
then combine them to make predictions at test time. 

       id90 are not very powerful models, but 

they are easy to fit to data and very fast at test time.  
      averaging many id90 works really well. 
its called id79s. kinect uses this. 
      we make the individual trees different by giving 
them different training sets. that   s called id112 

two ways to average models 

       mixture: we can combine models by taking the 

arithmetic means of their output probabilities: 

model a:    .3   .2   .5 
model b:    .1   .8   .1 
combined  .2   .5   .3 

       product: we can combine models by taking the 

geometric means of their output probabilities: 

model a:    .3    .2    .5 
model b:    .1    .8    .1 
combined  .03  .16  .05   /sum 

dropout: an efficient way to average 

many large neural nets. 

       consider a neural net with 

one hidden layer. 

       each time we present a 

training example, we 
randomly omit each hidden 
unit with id203 0.5. 

       so we are randomly 
sampling from 2^h 
different architectures. 
      all architectures share 
weights. 

dropout as a form of model averaging 

       we sample from 2^h models. so only a few of 
the models ever get trained, and they only get 
one training example. 
      this is as extreme as id112 can get. 

       the sharing of the weights means that every 

model is very strongly regularized. 
      it   s a much better regularizer than l2 or l1 
penalties that pull the weights towards zero. 

but what do we do at test time? 

       we could sample many different architectures 

and take the geometric mean of their output 
distributions. 

       it better to use all of the hidden units, but to 

halve their outgoing weights. 
      this exactly computes the geometric mean of 
the predictions of all 2^h models. 

what if we have more hidden layers? 

       use dropout of 0.5 in every layer. 

       at test time, use the    mean net    that has all the 

outgoing weights halved. 

       this is not exactly the same as averaging all the 

separate dropped out models, but it   s a pretty 
good approximation, and its fast. 

what about the input layer? 

       it helps to use dropout there too, but with a 
higher id203 of keeping an input unit. 
      this trick is already used by the    denoising 
autoencoders    developed in yoshua bengio   s 
group. 

a familiar example of dropout 

       do id28, but for each training case, 

dropout all but one of the inputs. 

       at test time, use all of the inputs. 

      its better to divide the learned weights by the 
number of features, but if we just want the 
best class its unnecessary. 
       this is called    na  ve bayes   . 
       why keep just one input? 

how well does dropout work? 

       if your deep neural net is significantly overfitting, 

it will reduce the number of errors by a lot. 
      any net that uses    early stopping    can do 
better by using dropout (at the cost of taking 
quite a lot longer to train).  

       if your deep neural net is not overfitting you 

should be using a bigger one. 

initial experiments on permutation 
invariant mnist (nitish srivastava) 

       mnist is a standard machine learning 

benchmark. 

       it has 60,000 training images of hand-written 

digits and 10,000 test images. 

       there are many ways of improving performance: 

      put in prior knowledge of geometry 
      add extra training data by transforming the 
images. 

       without using these tricks, the record for neural 

nets was 160 errors on the test set. 

using weight constraints 

       in neural nets, it is standard to use an l2 penalty 

on the weights (called weight-decay). 
      this improves generalization by keeping the 
weights small. 

       it is generally better to constrain the length of the 

incoming weight vector of each hidden unit. 
      if the weight vector becomes longer than 
allowed, the weights are renormalized by 
division. 

       weight constraints make it possible to use a very 

big initial learning rate that then decays. 

experiments on timit 

(nitish srivastava) 

       first pre-train a deep neural network one layer at 

a time on unlabeled windows of acoustic 
coefficients. 

       then fine-tune to discriminate between the 

classes using a small learning rate. 

       standard fine-tuning:  22.7% error on test set 
       dropout fine-tuning:    19.7% error on test set 

      this is a record for speaker-independent 
methods. 

experiment on timit 
(nitish srivastava) 

experiment on document classification 
(nitish srivastava & ruslan salakhutdinov) 
       represent a document by a vector of counts of 

the 2000 most frequent non-stop words. 

       use a subset of the documents from 50 non-

overlapping classes. 

       predict the class from the word count vector 

using a net with two hidden layers. 

experiment on cifar-10 

 

       benchmark task for object recognition: 

      10 classes, 32x32 color images downsampled 
from web and carefully hand-labeled. 
      50,000 training cases, 10,000 test cases. 

      a big convolutional net gets 18% error. 
      with dropout in the last layer it gets 16% error 

another way to think about dropout 

       if a hidden unit knows which other hidden units 

are present, it can co-adapt to them on the 
training data.  
      but complex co-adaptations are likely to go 
wrong on new test data. 
      big, complex conspiracies are not robust. 

       if a hidden unit has to work well with 

combinatorially many sets of co-workers, it is 
more likely to do something that is individually 
useful, but also marginally useful given what its 
co-workers typically achieve.  

a simple example for a linear model 

       training data: 

1,     1,    0     0            6 
1,     1,    1     1            4 
 
-5   +11   +4   -6      co-adapted weights 
 
+3,  +3,   -1,   -1      less co-adapted weights 

  

 

 

 

  

noise versus model averaging 

       here are two apparently different ways to 

improve generalization: 
      method 1: regularize by adding noise to the 
weights or neural activities (equivalent to 
weight penalties). 
      method 2: average the predictions of many 
different models. 

       dropout is an example of both. so these 

methods are not as different as they appear. 

sparse coding 

      

if we use a large    dictionary    of hidden units and code an image by 
only activating a few hidden units, we get an efficient code. 
       this is often done by using an l1 penalty on the hidden activities 

and then iterating to drive many of the activities to zero. 

       one big advantage of sparse coding is that the sparse codes are 

usually good for predicting class labels. 
       why is this? is it because each active hidden unit is more 

informative? 

       we now know that randomly setting most of the activities to zero 

allows us to learn very good codes. 

       so maybe sparse coding produces good codes for classification 

precisely because its unstable: a hidden unit does not know 
which other units will be present. 

another advantage of using dropout 

       dropout forces neurons to be robust to the loss 

       this should make    id107    work 

of co-workers. 

better. 

       two nets produce an offspring by randomly 
picking each hidden unit to come from one or 
other parent (we need to know the 
correspondence of hidden units). 
      the offspring already works quite well and 
after some training it will work about as well 
as the parents. 

a simple way to run on a cluster 

(suggested by inman harvey) 

       every so often, a    mother    network advertises for 

a mate. after considering various possible 
mates, the mother network produces an 
offspring. 

       the mother network is then suspended and that 

core is used to run the child network. 

       after a while we compare the child with the 

mother and decide which one to kill. 

       this algorithm requires very little interaction.  

      it could happily use a million cores. 
      nets need to change gender frequently. 

now for 

something not  

completely 

different 

an alternative to dropout 

       in dropout, each neuron computes an activity, p, 
using the logistic function. then it sends p to the 
next layer with a id203 of 0.5. 

       this has exactly the same expected value as 

sending 0.5 with id203 p.  
      that is exactly what a stochastic binary 
neuron does (if we call 0.5 one spike) 
      so what happens if we use stochastic binary 
neurons in the forward pass but do the 
backward pass as if we had done a    normal    
forward pass? 

the effect of only sending one bit 

       the deep neural network learns slower and gets 

more errors on the training data. 
       but it generalizes much better. 
      its about the same win as dropout, but we have 
not properly compared them yet. 

       dropout variance          =     p  /4 
       stochastic bit variance =     p(1-p)/4 

2 

      stochastic bits have more variance for small p. 
      this is the poisson limit and resembles neurons 

an amusing piece of history 

       in 2005 we discovered that deep nets can be pre-
trained effectively on unlabeled data by learning a 
stack of    restricted id82s    (see my 
2007 youtube techtalk for details). 

       the pre-training uses stochastic binary units. after 
pre-training we cheat and use id26 by 
pretending that they are deterministic units that 
send the real-valued outputs of logistics. 
      we would get less overfitting if we stayed with 
stochastic binary neurons in the forward pass.  

some explanations for why cortical 
neurons don   t send analog values 

       there is no efficient way for them to do it. 

      but some neurons use the precise times of 
spikes very effectively.  

       evolution just didn   t figure it out. 

      evolution had hundreds of millions of years. if 
neurons wanted to send analog values, evolution 
would have found a way.  

       its better to send stochastic spikes because they 

act as a great regularizer.  
      this helps the brain to use a lot of neurons 
without overfitting (10^14 parameters,10^9 seconds)  

another look at restricted boltzmann 

machines 

       when pre-training a deep net, we can use one-

step contrastive divergence as a shortcut to 
make maximum likelihood training faster. 
      but, for pre-training, cd actually works better 
than proper maximum likelihood training of the 
rbm. 

       so maybe there are better ways to understand 

what cd training is achieving. 
      maybe its fitting a type of auto-encoder! 

autoencoders vs rbms trained with ml 

       an autoencoder tries to make the reconstruction match 

the data. 

       an rbm trained with ml tries to make the distribution of 

the reconstructions match the distribution of the data. 
       so the rbm is happy to sometimes reconstruct a as 

b provided it also sometimes reconstructs b as a.  

       consider a pixel that is pure noise. 

       the autoencoder will use its hidden state to code the 

value of the noise so that it can reconstruct it. 

       an rbm trained with ml will totally ignore that pixel 
because the bias of the pixel can get the distribution 
correct.    

a picture of cd training 

one term changes the 
generative weight. the 
other term changes the 
recognition weight. 

s

0
j

(

0
s
i

   

1
s
i
1
s
i

)
+
0
s
(
j

j 

j 

<

ihv

j

0>

ihv
<

j

1>

i 

i 

t = 0                 t = 1    
data 

reconstruction 

   

1
s
j
1
s
j

)
(

+
1
s
i

2
s
i

)

   

...
+
ss
      
j
i

how cd learning back-propagates 

through stochastic neurons  
h

h +   h

  hj
out

v

v +   v

  hj
in

assume that         is small. 
 
to first order,         is the derivative of the reconstruction  
error w.r.t. the inputs to the hidden units. 

  v
  h

an improved version of contrastive 

divergence learning for density modeling 
       the main worry with cd is that there will be deep 
minima of the energy function far away from the 
data.  
       to find these we need to run the markov chain for 
       but we cannot afford to run the chain for too long 

a long time (maybe thousands of steps).  

for each update of the weights. 

       maybe we can run the same markov chain over 
many weight updates? (neal, 1992) 
       if the learning rate is very small, this should be 
equivalent to running the chain for many steps 
and then doing a bigger weight update. 

persistent cd 

(tijmen teileman, icml 2008 & 2009) 

       use minibatches of 100 cases to estimate the 
first term in the gradient. use a single batch of 
100 fantasies to estimate the second term in the 
gradient.  

  
       after each weight update, generate the new 

fantasies from the previous fantasies by using 
one alternating gibbs update. 
      so the fantasies can get far from the data. 

a puzzle 

       why does persistent cd work so well with only 

100 negative examples to characterize the 
whole partition function? 

      for all interesting problems the partition 
function is highly multi-modal. 

      how does it manage to find all the modes 
without starting at the data?  

the learning causes very fast mixing 

  
       the learning interacts with the markov chain. 
 
       persisitent contrastive divergence cannot be 

analysed by viewing the learning as an outer loop. 
      wherever the fantasies outnumber the 
positive data, the free-energy surface is 
raised. this makes the fantasies rush around 
hyperactively. 

how persistent cd moves between the 

modes of the model   s distribution 

       if a mode has more 

fantasy particles than data, 
the free-energy surface is 
raised until the fantasy 
particles escape. 
      this can overcome  
free-energy barriers that 
would be too high for the 
markov chain to jump. 
       the free-energy surface is 

being changed to help 
mixing in addition to 
defining the model. 

fast pcd (tieleman & hinton 2009) 

       to settle on a good set of weights, it helps to 
turn down the learning rate towards the end of 
learning. 

       but with a small learning rate, we dont get the 

fast mixing of the fantasy particles. 

       in addition to the    real    weights that define the 
model, we could have temporary weights that 
learn fast and decay fast.  

       the fast weights provide an additive overlay that 
achieves fast mixing even when the real weights 
are hardly changing. 

training a multilayer id82 

       for a full id82 (i.e. with connections 

between hidden units), we cannot use variational 
learning because one of the terms has the wrong sign. 
= < sisj >data     < sisj >model

   log p(data)

   wij

       variational learning maximizes the sum over all training 

cases of: 

              log p(data)     kl(q||p)   

approximate 
posterior 

true 
posterior 

how to train a id82 

       for the data-dependent expectations, assume 

the energy landscape is unimodal and use a 
mean-field approximation to the posterior. 

       for the model   s expectations uses pcd. 

       ruslan salakhutdinov showed that this worked, 

but it works much better if the weights of the 
hidden units are initialized sensibly. 

how to pre-train a deep id82 

(salakhutdinov & hinton, neural computation, 2012) 

       in a dbn, each rbm replaces the prior over the 
previous hidden layer (that is implicitly defined by 
the lower rbm) by a better prior. 

       suppose we just replace half of the prior defined 
by the lower rbm by half of a better prior defined 
by the higher rbm. 
      the new prior is then the geometric mean of the 
priors defined by the two rbms 
      the geometric mean is a better prior than the 
old one due to the convexity of kl divergence. 

combining two rbms to make a dbm 

2h
2w

1w
v

1h

1h

'2h
2w

1w
'v

each of these two rbms is a 
product of two identical experts 

2h

1h

2w

1w

v

readings on deep belief nets 

a reading list (that is still being updated) can be 

found at  

 
www.cs.toronto.edu/~hinton/deeprefs.html 

