deep  learning,
deep  learning,
graphical  models,
graphical  models,

  energy  based  models,
  energy  based  models,
  structured  prediction
  structured  prediction

  yann  lecun,  
  yann  lecun,  

        the  courant  institute  of  mathematical  sciences
        the  courant  institute  of  mathematical  sciences

new  york  university
new  york  university

http://yann.lecun.com
http://www.cs.nyu.edu/~yann

yann  lecun

end  to  end  learning.
end  to  end  learning.

making  every  single  module  in  the  
system  trainable.
every  module  is  trained  simultaneously  
so  as  to  optimize  a  global  loss  function.  

yann  lecun

using  graphs  instead  of  vectors.
using  graphs  instead  of  vectors.

whereas  traditional  learning  
machines  manipulate  fixed  size  
vectors,  graph  transformer  
networks  manipulate  graphs.

yann  lecun

energy  based  model
energy  based  model

highly  popular  methods  in  the  machine  learning  and  natural  language  
processing  communities  have  their  roots  in  speech  and  handwriting  
recognition
structured id88, id49, and related learning 
models for    id170    are descendants of discriminative 
learning methods for id103 and word-level handwriting 
recognition methods from the early 90's
a  tutorial  and  energy  based  learning:
[lecun & al., 2006]
discriminative  training  for     structured  output     models
the whole literature on discriminative id103 [1987-]
the whole literature on neural-net/id48 hybrids for speech [bottou 
1991, bengio 1993, haffner 1993, bourlard 1994]
graph transformer networks [lecun & al. proc ieee 1998]
structured id88 [collins 2001]
id49 [lafferty & al 2001]
max margin markov nets [altun & al 2003, taskar & al 2003]

yann  lecun

energy  based  model  for  decision  making
energy  based  model  for  decision  making

  model:  measures  the  compatibility  
between  an  observed  variable  x  and  
a  variable  to  be  predicted  y  through  
an  energy  function  e(y,x).  

  id136:  search  for  the  y  that  
minimizes  the  energy  within  a  set
  if  the  set  has  low  cardinality,  we  can  
use  exhaustive  search.  

yann  lecun

complex  tasks:  id136  is  non  trivial
complex  tasks:  id136  is  non  trivial

when  the  
cardinality  or  
dimension  of  y  is  
large,  exhaustive  
search  is  
impractical.
we  need  to  use    
   smart     id136  
procedures:  min  
sum,  viterbi,  min  
cut,  belief  
propagation,  
gradient  decent.....

yann  lecun

converting  energies  to  probabilities
converting  energies  to  probabilities
energies  are  uncalibrated
the energies of two separately-trained systems cannot be combined
the energies are uncalibrated (measured in arbitrary untis)
how  do  we  calibrate  energies?
we turn them into probabilities (positive numbers that sum to 1).
simplest way: gibbs distribution
other ways can be reduced to gibbs by a suitable redefinition of the 
energy.

partition  function  

inverse  temperature

yann  lecun

handwriting  recognition
handwriting  recognition
sequence  labeling
sequence  labeling
integrated  segmentation  and  
recognition  of  sequences.
each  segmentation  and  
recognition  hypothesis  is  a  path  
in  a  graph
id136  =  finding  the  shortest  
path  in  the  interpretation  graph.
un  normalized  hierarchical  
id48s  a.k.a.  graph  
transformer  networks
[lecun, bottou, bengio, 
haffner, proc ieee 1998]

yann  lecun

latent  variable  models
latent  variable  models

the  energy  includes     hidden     variables  z  whose  value  is  never  given  to  us

yann  lecun

what  can  the  latent  variables  represent?
what  can  the  latent  variables  represent?
variables  that  would  make  the  task  easier  if  they  were  known:
face recognition: the gender of the person, the orientation of 
the face.
object recognition: the pose parameters of the object 
(location, orientation, scale), the lighting conditions.
parts of speech tagging: the segmentation of the sentence 
into syntactic units, the parse tree.
id103: the segmentation of the sentence into 
phonemes or phones.
handwriting recognition: the segmentation of the line into 
characters.
object recognition/scene parsing: the segmentation of the 
image into components (objects, parts,...)
in  general,  we  will  search  for  the  value  of  the  latent  variable  that  
allows  us  to  get  an  answer  (y)  of  smallest  energy.

yann  lecun

probabilistic  latent  variable  models
probabilistic  latent  variable  models

marginalizing  over  latent  variables  instead  of  minimizing.

equivalent  to  traditional  energy  based  id136  with  a  redefined  
energy  function:

reduces  to  traditional  minimization  when  beta  >infinity

yann  lecun

training  an  ebm
training  an  ebm

training  an  ebm  consists  in  shaping  the  energy  function  so  that  the  
energies  of  the  correct  answer  is  lower  than  the  energies  of  all  other  
answers.
training sample: x = image of an animal, y =    animal    

e   animal , x       e   y , x         y   animal

yann  lecun

architecture  and  loss  function
architecture  and  loss  function

family  of  energy  functions

training  set

loss  functional  /  loss  function
measures the quality of an energy function on training 
set
training
form  of  the  loss  functional  
invariant under permutations and repetitions of the samples

per  sample

loss

desired
answer

energy  surface
for  a  given  xi
as  y  varies

regularizer

yann  lecun

designing  a  loss  functional
designing  a  loss  functional

push  down  on  the  energy  of  the  correct  answer
pull  up  on  the  energies  of  the  incorrect  answers,  particularly  if  they  
are  smaller  than  the  correct  one

yann  lecun

architecture  +  id136  algo  +  loss  function  =  model
architecture  +  id136  algo  +  loss  function  =  model

e(w,y,x)

w

x

y

  1.  design  an  architecture:  a  particular  form  for  e(w,y,x).  
  2.  pick  an  id136  algorithm  for  y:  map  or  conditional  
distribution,  belief  prop,  min  cut,  variational  methods,  
gradient  descent,  mcmc,  hmc.....
  3.  pick  a  loss  function:  in  such  a  way  that  minimizing  it  
with  respect  to  w  over  a  training  set  will  make  the  id136  
algorithm  find  the  correct  y  for  a  given  x.
  4.  pick  an  optimization  method.

  problem:  what  loss  functions  will  make  the  machine  approach  
the  desired  behavior?    

yann  lecun

examples  of  loss  functions:  energy  loss
examples  of  loss  functions:  energy  loss

energy  loss
simply pushes down on the energy of the correct answer

w orks!!!

yann  lecun

collapses!!!

negative  log  likelihood  loss
negative  log  likelihood  loss

conditional  id203  of  the  samples  (assuming  independence)

gibbs  distribution:

we  get  the  nll  loss  by  dividing  by  p  and  beta:

reduces  to  the  id88  loss  when  beta  >infinity

yann  lecun

negative  log  likelihood  loss
negative  log  likelihood  loss

pushes  down  on  the  energy  of  the  correct  answer
pulls  up  on  the  energies  of  all  answers  in  proportion  to  their  id203

yann  lecun

negative  log  likelihood  loss
negative  log  likelihood  loss

a  probabilistic  model  is  an  ebm  in  which:
the energy can be integrated over y (the variable to be predicted)
the id168 is the negative log-likelihood
negative  log  likelihood  loss  has  been  used  for  a  long  time  in  many  
communities  for  discriminative  learning  with  structured  outputs
id103: many papers going back to the early 90's 
[bengio 92], [bourlard 94]. they call    maximum mutual 
information   
handwriting recognition [bengio lecun 94], [lecun et al. 98]
bio-informatics [haussler]
id49 [lafferty et al. 2001]
lots more...... 
in all the above cases, it was used with non-linearly parameterized 
energies. 

yann  lecun

a  simpler  loss  functions:id88  loss
a  simpler  loss  functions:id88  loss

id88  loss  [lecun  et  al.  1998],  [collins  2002]
pushes down on the energy of the correct answer
pulls up on the energy of the machine's answer
always positive. zero when answer is correct
no    margin   : technically does not prevent the energy surface from 
being almost flat.
works pretty well in practice, particularly if the energy 
parameterization does not allow flat surfaces.
this is often called    discriminative viterbi training    in the 
speech and handwriting literature

yann  lecun

id88  loss  for  binary  classification
id88  loss  for  binary  classification

energy:  

id136:  

loss:  

learning  rule:  

if  gw(x)  is  linear  in  w:  

yann  lecun

a  better  loss  function:  generalized  margin  losses
a  better  loss  function:  generalized  margin  losses

first,  we  need  to  define  the  most  offending  incorrect  answer

most  offending  incorrect  answer:  discrete  case

most  offending  incorrect  answer:  continuous  case

yann  lecun

examples  of  generalized  margin  losses
examples  of  generalized  margin  losses

hinge  loss  
[altun et al. 2003], [taskar et al. 2003]
with the linearly-parameterized binary 
classifier architecture, we get linear id166s

log  loss  
   soft hinge    loss
with the linearly-parameterized binary 
classifier architecture, we get linear 
id28

yann  lecun

examples  of  margin  losses:  square  square  loss
examples  of  margin  losses:  square  square  loss

square  square  loss  
[lecun-huang 2005]
appropriate for positive energy 
functions

learning  y  =  x^2

no  collapse!!!

yann  lecun

other  margin  like  losses
other  margin  like  losses

lvq2  loss  [kohonen,  oja],  driancourt  bottou  1991]

minimum  classification  error  loss  [juang,  chou,  lee  1997]

square  exponential  loss  [osadchy,  miller,  lecun  2004]

yann  lecun

what  make  a     good     loss  function
what  make  a     good     loss  function

good  and  bad  loss  functions

slightly  more  general  form:

l (w , x i ,y i)=    y h ( e (w ,y i , x i)   e (w , y , x i)+c (y i , y))

yann  lecun

advantages/disadvantages  of  various  losses
advantages/disadvantages  of  various  losses

loss  functions  differ  in  how  they  pick  the  point(s)  whose  energy  is  
pulled  up,  and  how  much  they  pull  them  up
losses  with  a  log  partition  function  in  the  contrastive  term  pull  up  all  
the  bad  answers  simultaneously.  
this may be good if the gradient of the contrastive term can be 
computed efficiently
this may be bad if it cannot, in which case we might as well 
use a loss with a single point in the contrastive term
variational  methods  pull  up  many  points,  but  not  as  many  as  with  the  
full  log  partition  function.
efficiency  of  a  loss/architecture:  how  many  energies  are  pulled  up  for  
a  given  amount  of  computation?  
the theory for this is to be developed

yann  lecun

face  detection  and  pose  estimation  with  a  convolutional  ebm
face  detection  and  pose  estimation  with  a  convolutional  ebm

   

  training:  52,850,    32x32  
grey  level  images  of  faces,  
52,850  selected  non  faces.
  each  training  image  was  used  
5  times  with  random  variation  
in  scale,  in  plane  rotation,  
brightness  and  contrast.
2nd  phase:  half  of  the  initial  
negative  set  was  replaced  by  
false  positives  of  the  initial  
version  of  the  detector  .  

e    w , z , x    

(  energy)

   g w

    x       f    z       

gw

    x    

convolutional  

network

f    z    
analytical  

mapping  onto  
face  manifold

  

w(param)

small  e*(w,x):  face
large  e*(w,x):  no  face

x  

(image)

z  

(pose)  

[osadchy,  miller,  lecun,  nips  2004]

yann  lecun

face  manifold
face  manifold

||g(x)  min_z  f(z)|||

face  manifold  

parameterized  by  pose  

low  dimensional  space

g(x)

  

f(z)

apply

mapping:  g 

image  x

energy  based  contrastive  loss  function
energy  based  contrastive  loss  function

attract  the  network  output  gw(x)  to  the  
location  of  the  desired  pose  f(z)  on  the  manifold
(

)  +

)

(

=

=

(

)

repel  the  network  output  gw(x)  away  
from  the  face/pose  manifold

convolutional  network  architecture
convolutional  network  architecture
[lecun  et  al.  1988,  1989,  1998,  2005]

hierarchy  of  local  filters  (convolution  kernels),  
sigmoid  pointwise  non  linearities,  and  spatial  subsampling
all  the  filter  coefficients  are  learned  with  gradient  descent  (back  prop)  

yann  lecun

building  a  detector/recognizer:  replicated  conv.  nets
building  a  detector/recognizer:  replicated  conv.  nets

output:  3x3

window
32x32

input:40x40  

  traditional  detectors/classifiers  must  be  applied  to  every  
location  on  a  large  input  image,  at  multiple  scales.
  convolutional  nets  can  replicated  over  large  images  very  
cheaply.
  the  network  is  applied  to  multiple  scales  spaced  by  sqrt(2)
non  maximum  suppression  with  exclusion  window

yann  lecun

building  a  detector/recognizer:  
building  a  detector/recognizer:  
replicated  convolutional  nets
replicated  convolutional  nets

  computational  cost  for  replicated  convolutional  net:

96x96    >  4.6  million  multiply  accumulate  operations
120x120    >  8.3  million  multiply  accumulate  operations
240x240    >  47.5  million  multiply  accumulate  operations  
480x480    >  232  million  multiply  accumulate  operations
  computational  cost  for  a  non  convolutional  detector  of  the  
same  size,  applied  every  12  pixels:

96x96    >  4.6  million  multiply  accumulate  operations
120x120    >  42.0  million  multiply  accumulate  operations
240x240    >  788.0  million  multiply  accumulate  operations  
480x480    >  5,083  million  multiply  accumulate  operations

96x96  window

12  pixel  shift

84x84  overlap

face  detection:  results
face  detection:  results

data set->
false positives per image->

our detector

tilted

profile

mit+cmu

4.42

26.9

0.47

90%

97%

67%

3.36

83%

0.5

83%

1.28

88%

jones & viola (tilted)

90%

95%

x

jones & viola (profile)

x

70%

83%

rowley et al

89%

96%

schneiderman & kanade

86%

93%

x

x

x

x

yann  lecun

face  detection  and  pose  estimation:  results
face  detection  and  pose  estimation:  results

yann  lecun

the  oldest  example  of  structured  prediction  
the  oldest  example  of  structured  prediction  
trainable  automatic  speech  recognition  system  with  a  
convolutional  net  (tdnn)  and  dynamic  time  warping  (dtw)
the  feature  extractor  
and  the  structured  
classifier  are  trained  
simultanesously  in  an  
integrated  fashion.
with  the  lvq2  loss  :
driancourt and 
bottou's speech 
recognizer (1991)
with  nll:  
bengio's speech 
recognizer (1992)
haffner's speech 
recognizer (1993)

yann  lecun

energy  based  factor  graphs:  energy  =  sum  of     factors   
energy  based  factor  graphs:  energy  =  sum  of     factors   

sequence  labeling
output is a sequence 
y1,y2,y3,y4......
nlp parsing, mt, 
speech/handwriting 
recognition, biological 
sequence analysis
the factors ensure  
grammatical consistency 
they give low energy to 
consistent sub-sequences of 
output symbols
the graph is generally simple 
(chain or tree)
id136 is easy (dynamic 
programming, min-sum) 

yann  lecun

+

y1

y2

y3

y4

x  

energy  based  factor  graphs
energy  based  factor  graphs
when  the  energy  is  a  sum  of  partial  energy  functions  (or  when  the  
id203  is  a  product  of  factors):
efficient id136 algorithms can be used for id136 (without the 
id172 step).

+

e1(x,z1)

e2(z1,z2)

e3(z2,z3)

e4(z3,y)

x

z1

z2

z3

y

yann  lecun

the  energy  is  a  sum  of     factor     functions  

factor  graph

efficient  id136:  energy  based  factor  graphs
efficient  id136:  energy  based  factor  graphs
example:  
z1, z2, y1 are binary
z2 is ternary
a na  ve exhaustive 
id136 would require 
2x2x2x3=24 energy 
evaluations (= 96 factor 
evaluations)
but: ea only has 2 possible 
input configurations, eb 
and ec have 4, and ed 6.
hence, we can precompute 
the 16 factor values, and 
put them on the arcs in a 
trellis.
a path in the trellis is a 
config of variable
the cost of the path is the 
energy of the config

equivalent  trellis

yann  lecun

energy  based  belief  prop
energy  based  belief  prop

the  previous  picture  shows  a  chain  graph  of  factors  with  2  
inputs.
the  extension  of  this  procedure  to  trees,  with  factors  that  can  
have  more  than  2  inputs  the     min  sum     algorithm  (a  non  
probabilistic  form  of  belief  propagation)
basically,  it  is  the  sum  product  algorithm  with  a  different  semi  
ring  algebra  (min  instead  of  sum,  sum  instead  of  product),  and  
no  id172  step.
[kschischang, frey, loeliger, 2001][mckay's book]

yann  lecun

simple  energy  based  factor  graphs  with     shallow     factors
simple  energy  based  factor  graphs  with     shallow     factors
linearly  parameterized  factors

with  the  nll  loss  :
lafferty's 
conditional 
random field
with  hinge  loss:  
taskar and 
altun/hofmann's 
max margin 
markov nets  and 
latent id166
with  id88  loss
collins's structured 
id88 model 

yann  lecun

example  :  the  conditional  random  field  architecture
example  :  the  conditional  random  field  architecture
a  crf  is  an  energy  based    factor  graph  in  which:  
the factors are linear in the parameters (shallow factors)
the factors take neighboring output variables as inputs
the factors are often all identical

yann  lecun

example  :  the  conditional  random  field  architecture
example  :  the  conditional  random  field  architecture
applications:    
x is a sentence, y is a sequence of parts of speech tags (there is 
one yi for each possible group of words).
x is an image, y is a set of labels for each window in the image 
(vegetation, building, sky....).

yann  lecun

deep/non  linear  factors  for  speech  and  handwriting
deep/non  linear  factors  for  speech  and  handwriting
trainable  speech/handwriting  recognition  systems  that  integrate  neural  nets  (or  
other     deep     classifiers)  with  dynamic  time  warping,  hidden  markov  models,  or  
other  graph  based  hypothesis  representations

training  the  feature  
extractor  as  part  of  the  
whole  process.
with  the  lvq2  loss  :
driancourt and 
bottou's speech 
recognizer (1991)
with  nll:  
bengio's speech 
recognizer (1992)
haffner's speech 
recognizer (1993)

with  minimum  empirical  error  loss
ljolje and rabiner (1990)
with  nll:  
bengio (1992), haffner (1993), bourlard 
(1994)
with  mce
juang et al. (1997)
late  id172  scheme  (un  normalized  
id48)
bottou pointed out the label bias 
problem (1991)
denker and burges proposed a solution 
(1995)

yann  lecun

deep  factors  &  implicit  
deep  factors  &  implicit  
graphs:  gtn
graphs:  gtn

handwriting  recognition  with  
graph  transformer  networks
un  normalized  hierarchical  
id48s
trained with id88 loss 
[lecun, bottou, bengio, 
haffner 1998]
trained with nll loss 
[bengio, lecun 1994], 
[lecun, bottou, bengio, 
haffner 1998]
answer  =  sequence  of  symbols
latent  variable  =  segmentation

yann  lecun

graph  
graph  
transformer  
transformer  
networks
networks

variables:
x: input image
z: path in the 
interpretation 
graph/segmentation
y: sequence of labels on a 
path
loss  function:  computing  the  
energy  of  the  desired  answer:  

yann  lecun

graph  
graph  
transformer  
transformer  
networks
networks

variables:
x: input image
z: path in the 
interpretation 
graph/segmentation
y: sequence of labels on a 
path
loss  function:  computing  the  
constrastive  term:  

yann  lecun

graph  
graph  
transformer  
transformer  
networks
networks

example:  id88  loss
  loss  =  energy  of  desired  
answer       energy  of  best  
answer.
(no margin)

yann  lecun

global  training  helps
global  training  helps

pen  based  handwriting  recognition  
(for  tablet  computer)  
[bengio&lecun 1995]

yann  lecun

  graph  
  graph  
composition,
composition,
transducers.
transducers.

the  composition  of  two  
graphs  can  be  computed,  
the  same  way  the  dot  
product  between  two  
vectors  can  be  computed.
general  theory:  semi  ring  
algebra  on  weighted  finite  
state  transducers  and  
acceptors.

yann  lecun

check  reader
check  reader

graph  transformer  network  
trained  to  read  check  amounts.
trained  globally  with  
negative  log  likelihood  loss.
50%  percent  corrent,  49%  
reject,  1%  error  (detectable  
later  in  the  process.
fielded  in  1996,  used  in  many  
banks  in  the  us  and  europe.
processes  an  estimated  10%  of  
all  the  checks  written  in  the  
us.

yann  lecun

deep  factors  /  deep  graph:  asr  with  tdnn/id48
deep  factors  /  deep  graph:  asr  with  tdnn/id48
discriminative  automatic  speech  recognition  system  with  id48  and  
various  acoustic  models
training the acoustic model (feature extractor)  and a 
(normalized) id48 in an integrated fashion.
with  minimum  empirical  error  loss
ljolje and rabiner (1990)
with  nll:  
bengio (1992)
haffner (1993)
bourlard (1994)
with  mce
juang et al. (1997)
late  id172  scheme  (un  normalized  id48)
bottou pointed out the label bias problem (1991)
denker and burges proposed a solution (1995)

yann  lecun

feed  forward,  causal,  and  bi  directional  models  
feed  forward,  causal,  and  bi  directional  models  
ebfg  are  all     undirected   ,  but  the  architecture  determines  the  
complexity  of  the  id136  in  certain  directions

distance

complicated
function

x

y
feed  forward
predicting y 
from x is easy
predicting x 
from y is hard

yann  lecun

y
x
   causal   
predicting y 
from x is 
hard
predicting x 
from y is easy

y

x
bi  directional
x->y and y->x are 
both hard if the two 
factors don't agree.
they are both easy if 
the factors agree

