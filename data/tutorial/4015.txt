id171 for comparing examples
ipam graduate summer school on deep learning

graham taylor

school of engineering
university of guelph

papers and software available at: http://www.uoguelph.ca/~gwtaylor

thursday, july 12, 2012

overview: this talk

13 jul 2012 /
learning similarity / g taylor 

2

thursday, july 12, 2012

overview: this talk

    learning to compare examples
- it   s a big    eld!
- we will focus on methods inspired by deep learning

and representation learning

13 jul 2012 /
learning similarity / g taylor 

2

thursday, july 12, 2012

overview: this talk

    learning to compare examples
- it   s a big    eld!
- we will focus on methods inspired by deep learning

and representation learning

    applications:    nding similar documents, human pose estimation, pose-

sensitive retrieval
... and a dutch progressive-electro band
called c-mon & kypski

d
e
e

{s

s
n
o
i
t
a
t
i

m

i

13 jul 2012 /
learning similarity / g taylor 

2

thursday, july 12, 2012

outline

unsupervised
lsa, semantic hashing

supervised
nca, nonlinear nca, drlim

weakly supervised
applications to pose-sensitive retrieval

13 jul 2012 /
learning similarity / g taylor 

3

thursday, july 12, 2012

108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
    
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149

high-dimensional to low-dimensional space. finally we introduce a related but different objective
for our model based on drlim.
3.1 neighbourhood components analysis

address space

semantic
hashing
function

semantically
similar
documents

nca (both linear and nonlinear) and drlim do not presuppose the existence of a meaningful and
computable distance metric in the input space. they only require that neighbourhood relationships
be de   ned between training samples. this is well-suited for learning a metric for non-parametric
classi   cation (e.g. knn) on high-dimensional data. if the original data does not contain discrete
class labels, but real-valued labels (e.g. pose information for images of people) one alternative is to
de   ne neighbourhoods based on the distance in the real-valued label space and proceed as usual.
however, if classi   cation is not our ultimate goal, we may wish to exploit the    soft    nature of the
labels and use an alternative objective (i.e. one that does not optimize knn performance).
suppose we are given a set of n labeled training cases {xi, yi}, i = 1, 2, . . . , n, where xi     rd,
and yi     rl. for each training vector, xi, the id203 that point i selects one of its neighbours j
is de   ned in the transformed feature space [12]:
exp(   d2
xi
ij)

document 

(1)

pij =

dij = ||zi     zj||2

where we use a euclidean distance metric dij and zi = f (xi|  ) is the mapping (parametrized
by   ) from input space to feature space. for nca this is typically linear, but it can be extended
to be nonlinear through back-propagation (for example in [32] it is a multi-layer neural network).
nca assumes that the labels, yi are discrete yi     1, 2, . . . , c rather than real-valued and seeks to
maximize the expected number of correctly classi   ed points on the training data which minimizes:

   k   =i exp(   d2

,

ik)

lnca =    

n   i=1    j:yi=yj

pij.

(2)

the parameters are found by minimizing lnca with respect to   , back-propagating in the case of
a multi-layer parametrization. instead of seeking to optimize knn classi   cation performance, we
can use the nca regression (ncar) objective [18]:

lncar =

pij||yi     yj||2
2.

(3)

n   i=1   j

intuitively, if i and j are neighbours in feature space, then they should also lie close together in label
space. while we use the euclidean distance in label space, our approach generalizes to other metrics
which may be more appropriate for a different domain.
keller et al. [18] consider the linear case of ncar, where    is a weight matrix and y is a scalar
representing bellman error to map states with similar bellman errors close together. similar to
nca, we can extend this objective to the nonlinear, multi-layer case. we simply need to compute
the derivative of lncar with respect to the output of the mapping, zi, and backpropagate through
the remaining layers of the network. the gradient can be computed ef   ciently as:

   lncar

=   j

(zi     zj)   pij   y2

ij       i    + pji   y2

ij       j       .

(4)

learning similarity

    pixel distance     semantic similarity
    computing distances in pixel space is also computationally expensive
    learning parametric embeddings that are invariant to certain input variability
    today: focus on representations that capture human pose 

13 jul 2012 /
learning similarity / g taylor 

4

thursday, july 12, 2012

the unsupervised approach

    learn (possibly deep) representations completely unsupervised
- compute distances between top-level representations
- representations are usually low-dimensional
    classical methods: latent semantic analysis (based on svd), plsa, lda
    but directed models don   t seem like a natural    t
- fast id136 is important for information retrieval
    use undirected models in which exact id136 is fast
- single layer approach by generalizing rbms: welling et al. 2005
- multi-layer approach: salakhutdinov and hinton 2007    semantic hashing   

13 jul 2012 /
learning similarity / g taylor 

5

thursday, july 12, 2012

semantic hashing

    visible layer represents word-count vector of a document
-    constrained poisson model   
    learn constrained poisson     binary    rst layer
    learn one or more binary rbms in a    greedy    fashion
    unroll to a deep autoencoder and       ne-tune    w/ backprop
- during    ne-tuning add gaussian noise to code layer
- this forces the codes to be close to binary

h

w

v

binary

constrained
poisson

latent topic features

n*w

w

softmax

13 jul 2012 /
learning similarity / g taylor 

6

thursday, july 12, 2012

observed distribution
over words

reconstructed distribution
over words

(figures from r. salakhutdinov and g. hinton)

extremely fast retrieval

    documents are mapped to 20-d binary codes
    can retrieve similar documents stored at nearby 

addresses with no search

    binary lsa signi   cantly reduces performance
- not surprising: it has not been optimized to 

make binary codes perform well

    one weakness: documents with similar 

addresses have similar content but the converse 
is not necessarily true
- can we use external information (e.g. labels) to 

pull together codes of similar documents?

address space

semantic
hashing
function

document 

european community 
monetary/economic

semantically
similar
documents

disasters and 
accidents

13 jul 2012 /
learning similarity / g taylor 

7

thursday, july 12, 2012

energy markets

government 
borrowing

accounts/earnings

(figures from russ salakhutdinov)

cross validation for metric learning?

    consider id92 classi   cation as an example.
    q: what is the right distance metric for knn classi   cation?

motivation: knn classification

a: the one that optimizes test error!

    what is the right distance for knn classi   cation?
- the one that optimizes test error!
    think about approximating this by training error, de   ned 

by leave-one-out cross-validation
    let   s try to approximate this by

    two problems:
- loo error is a highly discontinuous function of the 

the one which optimizes training error,
de   ned using leave-one-out cross validation.

- we still need to choose k
    look for a smoother (or at least continuous) cost function

distance metric

?

    so if i gave you a    nite set of distance metrics to chose between

(and i told you k), you could pick the best one.

    obvious next question: if i gave you a continuously

parameterized family of metrics to search through, could you
   nd the one which maximizes loo classi   cation performance?

13 jul 2012 /
learning similarity / g taylor 

8

(slide from sam roweis)

    and what about k...?

thursday, july 12, 2012

stochastic nearest neighbour

    instead picking from a    xed set of     nearest neighbours, select a single 

k

neighbour stochastically

    let each point     select other points    as its neighbour with id203     
pij

j

i

based on the softmax of the distance     :
dij

stochastic neighbour selection

exp(   d2
ij)
    idea: instead of picking a    xed number

pij =

where:

k of nearest neighbours, and voting
their classes, select a single neighbour
dij = ||zi     zj||2
stochastically, and look at the expected
votes for each class.
zi = f (xi|  )

   k   =i exp(   d2

ik)

xk

xipij

xj

    imagine that each point i selects other points j as its neighbour
with a id203 pij based on the softmax of the distance dij:

13 jul 2012 /
learning similarity / g taylor 

9

(figure from sam roweis)

thursday, july 12, 2012

e   dij

nca: loss

    maximize the expected number of 

points correctly classi   ed under this 
scheme

    this is much smoother than the actual 

leave-one-out cross-validation error!

    in fact, it is differentiable w.r.t. 

parameters of mapping
- can use sgd or other gradient-based 

optimizer

    and there is no explicit parameter

k

lnca =    

n   i=1    j:yi=yj

pij

minimize loss w.r.t.      
  

13 jul 2012 /
learning similarity / g taylor 

10

thursday, july 12, 2012

nca: embeddings

pca

lda

nca

concentric rings

(d=3)

wine
(d=13)

faces
(d=560)

usps digits

(d=256)

f (x|   = a) = ax

13 jul 2012 /
learning similarity / g taylor 

11

thursday, july 12, 2012

(figures from goldberger et al.)

nca: mnist

mnist
(d=784)

13 jul 2012 /
learning similarity / g taylor 

12

thursday, july 12, 2012

nonlinear nca

    the original nca paper (goldberger et al. 2004) points out that

need not be a linear mapping

f (xi|  )

    salakhutdinov and hinton (2007) pre-train with an rbm, then    ne-tune with the 

nca objective

    can combine the nca objective with an autoencoder objective to regularize:

c =   lnca + (1       )lae

    can take advantage of unlabeled data!

13 jul 2012 /
learning similarity / g taylor 

13

thursday, july 12, 2012

learning nonlinear nca

pre-training

mixed-objective    ne-tuning

30

w
2000

4

2000
w

500

3

500

500

w

2

500

w

1

top
rbm

rbm

rbm

rbm

13 jul 2012 /
learning similarity / g taylor 

14

thursday, july 12, 2012

(1       )lae

8

5

  lnca

500

500

w (cid:10)(cid:161)

t
1

w (cid:10)(cid:161)

t
2

7

w (cid:10)(cid:161)

t
3

6

2000

t
4

w (cid:10)(cid:161)
30

decoder

8

w (cid:10)(cid:161)

t
1

500

500

w (cid:10)(cid:161)

t
2

7

w (cid:10)(cid:161)

t
3

6

2000

t
4

w (cid:10)(cid:161)
30

5

w (cid:10)(cid:161)

4

2000

w (cid:10)(cid:161)

3

500

w (cid:10)(cid:161)

2

500

4

3

2

w (cid:10)(cid:161)

4

2000

w (cid:10)(cid:161)

3

500

w (cid:10)(cid:161)

2

500

4

3

2

w (cid:10)(cid:161)

1

1

w (cid:10)(cid:161)

1

1

encoder

(figures from r. salakhutdinov and g. hinton)

limitations of nca

    despite very nice embeddings (see right) nca has a 
quadratic id172 term (must consider all pairs)
- mini-batch training (approximate) 
- objectives that don   t require id172

    what about continuous labels?
- (goldberger et al. 2004) describe a    soft    form of 

nca that can use continuous labels

noninear nca (mnist)

1

7

4

2

6

0

9

8

3

5

linear nca (mnist)

13 jul 2012 /
learning similarity / g taylor 

15

thursday, july 12, 2012

(figures from r. salakhutdinov and g. hinton)

learning embeddings with a siamese network

d(  ,  ) = small

f (  |  )

f (  |  )

13 jul 2012 /
learning similarity / g taylor 

16

thursday, july 12, 2012

learning embeddings with a siamese network

d(  ,  ) = small

identical
pathways

f (  |  )

f (  |  )

13 jul 2012 /
learning similarity / g taylor 

16

thursday, july 12, 2012

learning embeddings with a siamese network

d(  ,  ) = small

d(  ,  ) = big

identical
pathways

f (  |  )

f (  |  )

f (  |  )

f (  |  )

web

13 jul 2012 /
learning similarity / g taylor 

16

thursday, july 12, 2012

not a new idea

(broid113y, guyon, lecun, sackinger, and shah 1994)

    architecture proposed for signature veri   cation
- didn   t really get the distance function right
- learning unstable
- small (by today   s standards) training set
    1d convolution (tdnn)
    developed independently elsewhere:
- baldi and chauvin, 1992:    ngerprint 

veri   cation

- becker and hinton, 1992 - discovering depth 

in random-dot stereograms

13 jul 2012 /
learning similarity / g taylor 

17

thursday, july 12, 2012

the embedding: convolutional networks

    stacking multiple stages of filter bank + non-linearity + pooling
    shared with other approaches (sift, gist, hog)
    main difference: learn the    lter banks at every layer

filter 
bank

non-

linearity

feature 
pooling

filter 
bank

non-

linearity

feature 
pooling

...

? ? ? ?

classi   er

13 jul 2012 /
learning similarity / g taylor 

18

thursday, july 12, 2012

input:

layer 1:

128  128

16  120  120

output:
32  1  1

layer 4:
32  4  4

layer 3:
32  16  16

layer 2:
16  24  24

3.1 neighbourhood components analysis

nca (both linear and nonlinear) and drlim do not presuppose the existence of a meaningful and
computable distance metric in the input space. they only require that neighbourhood relationships
be de   ned between training samples. this is well-suited for learning a metric for non-parametric
classi   cation (e.g. knn) on high-dimensional data. if the original data does not contain discrete
class labels, but real-valued labels (e.g. pose information for images of people) one alternative is to
de   ne neighbourhoods based on the distance in the real-valued label space and proceed as usual.
however, if classi   cation is not our ultimate goal, we may wish to exploit the    soft    nature of the
labels and use an alternative objective (i.e. one that does not optimize knn performance).
suppose we are given a set of n labeled training cases {xi, yi}, i = 1, 2, . . . , n, where xi     rd,
and yi     rl. for each training vector, xi, the id203 that point i selects one of its neighbours j
is de   ned in the transformed feature space [12]:
exp(   d2
xi
ij)

110
111
112
113
114
115
embedding with a siamese convolutional net
116
117
118
119
120
121
122
123
124
125
126
127
    
128
129
130
131
    
132
133
134
135
136
137
138
139

where we use a euclidean distance metric dij and zi = f (xi|  ) is the mapping (parametrized
by   ) from input space to feature space. for nca this is typically linear, but it can be extended
x j
to be nonlinear through back-propagation (for example in [32] it is a multi-layer neural network).
nca assumes that the labels, yi are discrete yi     1, 2, . . . , c rather than real-valued and seeks to
maximize the expected number of correctly classi   ed points on the training data which minimizes:

the parameters are found by minimizing lnca with respect to   , back-propagating in the case of
19
a multi-layer parametrization. instead of seeking to optimize knn classi   cation performance, we
can use the nca regression (ncar) objective [18]:

lnca =    
what   s the objective function?
-needs to pull together semantically similar pairs
-needs to push apart semantically dissimilar pairs

   k   =i exp(   d2

n   i=1    j:yi=yj

dij = ||zi     zj||2

pij||yi     yj||2
2.

distance in low-
dimensional space

13 jul 2012 /
learning similarity / g taylor 

lncar =

convolutions,
tanh(), abs()

convolutions,
tanh(), abs()

pij =

average 
pooling

average 
pooling

pij.

connected

ik)

fully 

,

n   i=1   j

image
pairs

thursday, july 12, 2012

(hadsell, chopra and lecun 2006)

id84 by learning
an invariant mapping (drlim)

sij

is a binary indicator

l = sijls(xi, xj) + (1     sij)ld(xi, xj)

similarity loss

dissimilarity loss

ls(xi, xj) =

1
2

(dij)2

ld(xi, xj) =

1
2

[max(0,       dij)]2

3.5

3

2.5

s
s
o
l

2

1.5

1

0.5

0
0

    the similarity loss    pushes together    similar points
    the dissimilarity loss    pulls apart    dissimilar points
- but only if their distance is within some margin, 
  

margin

  

0.5

1

mn
d
ij

1.5

2

2.5

13 jul 2012 /
learning similarity / g taylor 

20

thursday, july 12, 2012

spring analogy

    solid dots are points that are similar to 

the point in the centre

    hollow dots are points that are 

dissimilar to the point in the centre

    forces acting on the points are shown 

in blue
- the length of the arrow represents the 

strength of the force

    radius represents the margin, 

  

13 jul 2012 /
learning similarity / g taylor 

21

thursday, july 12, 2012

(figures from hadsell et al.)

thursday, july 12, 2012

(figures from hadsell et al.)

existing paradigm: pairwise similarity

13 jul 2012 /
learning similarity / g taylor 

23

thursday, july 12, 2012

existing paradigm: pairwise similarity

    nca, drlim: binary notion of similarity typically de   ned by class membership 

or explicitly constructed neighbourhood graph

13 jul 2012 /
learning similarity / g taylor 

23

thursday, july 12, 2012

existing paradigm: pairwise similarity

    nca, drlim: binary notion of similarity typically de   ned by class membership 

or explicitly constructed neighbourhood graph

    de   ning pairwise similarity is dif   cult and inconsistent across observers

13 jul 2012 /
learning similarity / g taylor 

23

thursday, july 12, 2012

existing paradigm: pairwise similarity

    nca, drlim: binary notion of similarity typically de   ned by class membership 

or explicitly constructed neighbourhood graph

    de   ning pairwise similarity is dif   cult and inconsistent across observers

    despite crowd-sourcing platforms (e.g. amazon mechanical turk) gathering 

semantically similar pairs of images is expensive

web

13 jul 2012 /
learning similarity / g taylor 

23

thursday, july 12, 2012

hands by hand

    one solution is to turn to synthetic 

data (e.g. shakhnarovich et al. 
2003, jain et al. 2008) 

    dif   cult to generalize to real (e.g. 

   youtube    settings)

    another solution: ask people to 

label heads and hands (spiro et al. 
2010) or superimpose articulated 
skeletons (bourdev et al. 2009)

13 jul 2012 /
learning similarity / g taylor 

24

thursday, july 12, 2012

(spiro, taylor, williams and bregler acvhl 2010)

hands by hand

    one solution is to turn to synthetic 

data (e.g. shakhnarovich et al. 
2003, jain et al. 2008) 

    dif   cult to generalize to real (e.g. 

   youtube    settings)

    another solution: ask people to 

label heads and hands (spiro et al. 
2010) or superimpose articulated 
skeletons (bourdev et al. 2009)

13 jul 2012 /
learning similarity / g taylor 

24

thursday, july 12, 2012

(spiro, taylor, williams and bregler acvhl 2010)

nonparametric pose estimation

(taylor, spiro, williams, fergus and bregler nips 2010)

13 jul 2012 /
learning similarity / g taylor 

25

thursday, july 12, 2012

nonparametric pose estimation

database

(taylor, spiro, williams, fergus and bregler nips 2010)

13 jul 2012 /
learning similarity / g taylor 

25

thursday, july 12, 2012

nonparametric pose estimation

database

(taylor, spiro, williams, fergus and bregler nips 2010)

13 jul 2012 /
learning similarity / g taylor 

25

thursday, july 12, 2012

nonparametric pose estimation

database

(taylor, spiro, williams, fergus and bregler nips 2010)

    if we have a database of 

images labeled with 2d or 3d 
pose information - we can do 
non-parametric pose estimation

13 jul 2012 /
learning similarity / g taylor 

25

thursday, july 12, 2012

(taylor, spiro, williams, fergus and bregler nips 2010)

nonparametric pose estimation

    if we have a database of 

images labeled with 2d or 3d 
pose information - we can do 
non-parametric pose estimation

query

database

find 

nearest 
neighbor

copy 
pose

13 jul 2012 /
learning similarity / g taylor 

25

thursday, july 12, 2012

nonparametric pose estimation

(taylor, spiro, williams, fergus and bregler nips 2010)

query

database

    if we have a database of 

images labeled with 2d or 3d 
pose information - we can do 
non-parametric pose estimation
    nearest neighbor lookup must 

be quick (e.g. performed in a 
low-dimensional space)

find 

nearest 
neighbor

copy 
pose

13 jul 2012 /
learning similarity / g taylor 

25

thursday, july 12, 2012

nonparametric pose estimation

(taylor, spiro, williams, fergus and bregler nips 2010)

query

database

find 

nearest 
neighbor

copy 
pose

    if we have a database of 

images labeled with 2d or 3d 
pose information - we can do 
non-parametric pose estimation
    nearest neighbor lookup must 

be quick (e.g. performed in a 
low-dimensional space)

    it also must be informative of 
pose and invariant to clothing, 
lighting, scale, and other 
appearance changes

13 jul 2012 /
learning similarity / g taylor 

25

thursday, july 12, 2012

nca regression

lncar =

n   i=1   j

minimize loss w.r.t.      
  

pij||yi     yj||2

2

xi

yi = yj

yi = [48.2, 46.3, . . . , 63.3]t

pay a high cost for    neighbours    in 
feature space that are far away in 
pose space

xj

13 jul 2012 /
learning similarity / g taylor 

26

yi = [54.4, 45.8, . . . , 64.1]t

thursday, july 12, 2012

snowbird dataset

    we digitally recorded all contributing and invited speakers at the 2010 

snowbird workshop

    after each session of talks, blocks of 150 frames were distributed as human 

intelligence tasks (hits) on amazon mechanical turk

    split speakers into 39k training examples, 37k test examples (no overlap in 

identity)

13 jul 2012 /
learning similarity / g taylor 

27

thursday, july 12, 2012

comparison of approaches

pixel distance

gist

linear nca regression (ncar)

convolutional ncar (c-ncar)

drlim regression (drlimr)

convolutional drlimr (c-drlimr)

13 jul 2012 /
learning similarity / g taylor 

28

thursday, july 12, 2012

not practical

   global representation of image
   still not practical
   applied to pre-computed gist
   fit by conjugate gradient
   convolutions applied to pixels
   tanh(),abs(),average 
downsampling
   similar to ncar but adds an 
explicit contrastive loss
   similar to c-ncar but adds an 
explicit contrastive loss

comparison of approaches

pixel distance

gist

linear nca regression (ncar)

convolutional ncar (c-ncar)

drlim regression (drlimr)

convolutional drlimr (c-drlimr)

13 jul 2012 /
learning similarity / g taylor 

28

thursday, july 12, 2012

    

    

not practical

xi

x j

   global representation of image
   still not practical
   applied to pre-computed gist
   fit by conjugate gradient
   convolutions applied to pixels
   tanh(),abs(),average 
downsampling
   similar to ncar but adds an 
explicit contrastive loss
   similar to c-ncar but adds an 
explicit contrastive loss

comparison of approaches

pixel distance

gist

linear nca regression (ncar)

convolutional ncar (c-ncar)

drlim regression (drlimr)

convolutional drlimr (c-drlimr)

13 jul 2012 /
learning similarity / g taylor 

28

thursday, july 12, 2012

not practical

   global representation of image
   still not practical
   applied to pre-computed gist
   fit by conjugate gradient
   convolutions applied to pixels
   tanh(),abs(),average 
downsampling
   similar to ncar but adds an 
explicit contrastive loss
   similar to c-ncar but adds an 
explicit contrastive loss

labeling pose

    both pixel-based matching 

and gist focus on scene 
content, lighting

    our method learns invariance 

to background, focuses on 
pose

    though trained on hands 
relative to head, seems to 
capture something more 
substantial about body pose

13 jul 2012 /
learning similarity / g taylor 

29

thursday, july 12, 2012

./012

3/145067894:#;

3/145067894:,;

<=>0?14506789

@abc

d=e0fg

!"#$%&

!"'$()

!"#*$%%

!"+$+#

!"+$+#

!"#$*,

!"&$(-

!",&$--

!"&*$)-

!"#($#)

!",$-+

!")$%)

!"-$+#

!"+$%,

!"+$%,

!"&$)'

!",$&+

!"#$+-

!"+$%'

!"#*$+(

!")$#(

!",$)%

!"'$%)

!"%$*(

!"($(%

!"&$*'

!",$&#

!"($#(

!"##$+%

!"##$+%

!"&$&*

!",$'*

!",*$*'

!",%$))

!",,$-(

!"'$**

!",$),

!",#$#%

!",*$*%

!"#'$-'

results

embedding

none
none
pca
pca
ncar
ncar
s-drlim

boost-ssc

pse(b)
pse(o)

input code size err-sy err-re
pixels
25.12
25.30
gist
24.85
gist
25.74
gist
gist
24.93
23.15
lcn
+gist
25.19
gist
22.65
lcn
+gist
16.41
lcn
19.61
lcn

16384
512
128
32
32
32
32
32
32
32

32.86
47.41
47.17
48.99
34.21
32.90
37.80
34.80
28.95
25.40

13 jul 2012 /
learning similarity / g taylor 

30

thursday, july 12, 2012

25.4 pixel error

16.4 pixel error

can we get away with not asking people to provide explicit 

labels of body parts?

13 jul 2012 /
learning similarity / g taylor 

31

thursday, july 12, 2012

learning invariance through imitation

    a new paradigm for learning 
invariant mappings: imitation

    people have a remarkable 

ability to mimic image content

    exploit the abundance of 

webcams to quickly crowd-
source a massive dataset of 
people in similar pose
    active crowd-sourcing

13 jul 2012 /
learning similarity / g taylor 

32

thursday, july 12, 2012

a new take on temporal coherence

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

a new take on temporal coherence

    how do we select the images 

people are asked to imitate?

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

a new take on temporal coherence

    how do we select the images 

people are asked to imitate?
    temporal coherence in video 
can increase the number of 
pairwise similarities and add 
graded similarity

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

a new take on temporal coherence

    how do we select the images 

people are asked to imitate?
    temporal coherence in video 
can increase the number of 
pairwise similarities and add 
graded similarity

d
d
e
e
e
e
s

{s

...

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

a new take on temporal coherence

    how do we select the images 

people are asked to imitate?
    temporal coherence in video 
can increase the number of 
pairwise similarities and add 
graded similarity

d
d
e
e
e
e
s

{s

s
s
n
n
o
o
i
t
i
a
t
t
a
i
m
t
i
m

i

i

...

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

a new take on temporal coherence

    how do we select the images 

people are asked to imitate?
    temporal coherence in video 
can increase the number of 
pairwise similarities and add 
graded similarity

d
d
e
e
e
e
s

{s

s
s
n
n
o
o
i
t
i
a
t
t
a
i
m
t
i
m

i

i

...

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

a new take on temporal coherence

    how do we select the images 

people are asked to imitate?
    temporal coherence in video 
can increase the number of 
pairwise similarities and add 
graded similarity

d
d
e
e
e
e
s

{s

s
s
n
n
o
o
i
t
i
a
t
t
a
i
m
t
i
m

i

i

...

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

a new take on temporal coherence

    how do we select the images 

people are asked to imitate?
    temporal coherence in video 
can increase the number of 
pairwise similarities and add 
graded similarity

    video is used only as a source 

of seed images

    our model learns only from 
user-contributed imitations

d
d
e
e
e
e
s

{s

s
s
n
n
o
o
i
t
i
a
t
t
a
i
m
t
i
m

i

i

...

13 jul 2012 /
learning similarity / g taylor 

33

thursday, july 12, 2012

formalizing the problem

yi
    each image,      , has an associated seed label,     
    we seek to learn a mapping:

xi

z = f (x|  )

such that if       and       come from nearby seed images, then

can be linear or nonlinear

zi = f (xi|  )

xi

xj
dij = ||zi     zj||2

                                               will be small.

x1

x2

x3

x4

13 jul 2012 /
learning similarity / g taylor 

34

thursday, july 12, 2012

y1 = y2 = y3 = y4

id84 by learning
an invariant mapping (drlim)

l = sijls(xi, xj) + (1     sij)ld(xi, xj)

similarity loss

dissimilarity loss

ls(xi, xj) =

ld(xi, xj) =

1
2

1
2

(dij)2

[max(0,       dij)]2

3.5

3

2.5

2

1.5

1

0.5

s
s
o
l

margin   

13 jul 2012 /
learning similarity / g taylor 

35

0
0

0.5

1

1.5

2

2.5

mn
d
ij

thursday, july 12, 2012

id84 by learning
an invariant mapping (drlim)

l = sijls(xi, xj) +   (sij, 0)ld(xi, xj)

sij no longer binary

similarity loss

dissimilarity loss

ls(xi, xj) =

ld(xi, xj) =

1
2

1
2

(dij)2

[max(0,       dij)]2

3.5

3

2.5

2

1.5

1

0.5

s
s
o
l

margin   

13 jul 2012 /
learning similarity / g taylor 

35

0
0

0.5

1

1.5

2

2.5

mn
d
ij

thursday, july 12, 2012

from discrete labels to similarity

    in addition to the learned mapping,  , we require a mapping from discrete 

f

seed identity to a real-valued similarity score

    simplest example:

sij = (1 + |yi     yj|)   1

13 jul 2012 /
learning similarity / g taylor 

36

thursday, july 12, 2012

skip convnet intro

from discrete labels to similarity

    in addition to the learned mapping,  , we require a mapping from discrete 

f

seed identity to a real-valued similarity score

sij = (1 + |yi     yj|)   1

    simplest example:

s = 1

y = 1

13 jul 2012 /
learning similarity / g taylor 

36

thursday, july 12, 2012

skip convnet intro

from discrete labels to similarity

    in addition to the learned mapping,  , we require a mapping from discrete 

f

seed identity to a real-valued similarity score

sij = (1 + |yi     yj|)   1

    simplest example:

s = 1

s = 1/2

y = 1

y = 2

13 jul 2012 /
learning similarity / g taylor 

36

thursday, july 12, 2012

skip convnet intro

from discrete labels to similarity

    in addition to the learned mapping,  , we require a mapping from discrete 

f

seed identity to a real-valued similarity score

    simplest example:

sij = (1 + |yi     yj|)   1

s = 1

s = 1/2

y = 1

s = 1/3

y = 2

13 jul 2012 /
learning similarity / g taylor 

36

thursday, july 12, 2012

y = 3

skip convnet intro

input:

layer 1:

128  128

16  120  120

output:
32  1  1

layer 4:
32  4  4

layer 3:
32  16  16

layer 2:
16  24  24

3.1 neighbourhood components analysis

nca (both linear and nonlinear) and drlim do not presuppose the existence of a meaningful and
computable distance metric in the input space. they only require that neighbourhood relationships
be de   ned between training samples. this is well-suited for learning a metric for non-parametric
classi   cation (e.g. knn) on high-dimensional data. if the original data does not contain discrete
class labels, but real-valued labels (e.g. pose information for images of people) one alternative is to
de   ne neighbourhoods based on the distance in the real-valued label space and proceed as usual.
however, if classi   cation is not our ultimate goal, we may wish to exploit the    soft    nature of the
labels and use an alternative objective (i.e. one that does not optimize knn performance).
suppose we are given a set of n labeled training cases {xi, yi}, i = 1, 2, . . . , n, where xi     rd,
and yi     rl. for each training vector, xi, the id203 that point i selects one of its neighbours j
is de   ned in the transformed feature space [12]:
exp(   d2
xi
ij)

110
111
112
113
114
115
embedding with a siamese convolutional net
116
117
118
119
120
121
122
123
124
125
126
127
    
128
129
130
131
    
132
133
134
135
136
137
138
139

where we use a euclidean distance metric dij and zi = f (xi|  ) is the mapping (parametrized
by   ) from input space to feature space. for nca this is typically linear, but it can be extended
x j
to be nonlinear through back-propagation (for example in [32] it is a multi-layer neural network).
nca assumes that the labels, yi are discrete yi     1, 2, . . . , c rather than real-valued and seeks to
maximize the expected number of correctly classi   ed points on the training data which minimizes:

the parameters are found by minimizing lnca with respect to   , back-propagating in the case of
37
a multi-layer parametrization. instead of seeking to optimize knn classi   cation performance, we
can use the nca regression (ncar) objective [18]:

   k   =i exp(   d2

n   i=1    j:yi=yj

dij = ||zi     zj||2

pij||yi     yj||2
2.

distance in low-
dimensional space

13 jul 2012 /
learning similarity / g taylor 

lnca =    

skip toy experiments

lncar =

convolutions,
tanh(), abs()

convolutions,
tanh(), abs()

pij =

average 
pooling

average 
pooling

pij.

connected

ik)

fully 

,

n   i=1   j

image
pairs

thursday, july 12, 2012

input:

layer 1:

128  128

16  120  120

output:
32  1  1

layer 4:
32  4  4

layer 3:
32  16  16

layer 2:
16  24  24

3.1 neighbourhood components analysis

nca (both linear and nonlinear) and drlim do not presuppose the existence of a meaningful and
computable distance metric in the input space. they only require that neighbourhood relationships
be de   ned between training samples. this is well-suited for learning a metric for non-parametric
classi   cation (e.g. knn) on high-dimensional data. if the original data does not contain discrete
class labels, but real-valued labels (e.g. pose information for images of people) one alternative is to
de   ne neighbourhoods based on the distance in the real-valued label space and proceed as usual.
however, if classi   cation is not our ultimate goal, we may wish to exploit the    soft    nature of the
labels and use an alternative objective (i.e. one that does not optimize knn performance).
suppose we are given a set of n labeled training cases {xi, yi}, i = 1, 2, . . . , n, where xi     rd,
and yi     rl. for each training vector, xi, the id203 that point i selects one of its neighbours j
is de   ned in the transformed feature space [12]:
exp(   d2
xi
ij)

110
111
112
113
114
115
embedding with a siamese convolutional net
116
117
118
119
120
121
122
123
124
125
126
127
    
128
129
130
131
    
132
133
134
135
136
137
138
139

where we use a euclidean distance metric dij and zi = f (xi|  ) is the mapping (parametrized
by   ) from input space to feature space. for nca this is typically linear, but it can be extended
x j
to be nonlinear through back-propagation (for example in [32] it is a multi-layer neural network).
nca assumes that the labels, yi are discrete yi     1, 2, . . . , c rather than real-valued and seeks to
maximize the expected number of correctly classi   ed points on the training data which minimizes:

the parameters are found by minimizing lnca with respect to   , back-propagating in the case of
37
a multi-layer parametrization. instead of seeking to optimize knn classi   cation performance, we
can use the nca regression (ncar) objective [18]:

   k   =i exp(   d2

n   i=1    j:yi=yj

dij = ||zi     zj||2

pij||yi     yj||2
2.

13 jul 2012 /
learning similarity / g taylor 

lnca =    

skip toy experiments

lncar =

convolutions,
tanh(), abs()

convolutions,
tanh(), abs()

pij =

average 
pooling

average 
pooling

pij.

connected

ik)

fully 

,

n   i=1   j

image
pairs

thursday, july 12, 2012

one frame of fame

    no need to collect data ourselves: we 

leverage an existing project in an 
unintended way

    one frame of fame is a music video by 

the dutch band c-mon & kypski

    the band aims to replace selected frames 

with audience imitations

    to date, the band has >35k contributions
    only manual intervention is in determining 

scene cuts

13 jul 2012 /
learning similarity / g taylor 

38

thursday, july 12, 2012

one frame of fame

    no need to collect data ourselves: we 

leverage an existing project in an 
unintended way

    one frame of fame is a music video by 

the dutch band c-mon & kypski

    the band aims to replace selected frames 

with audience imitations

    to date, the band has >35k contributions
    only manual intervention is in determining 

scene cuts

13 jul 2012 /
learning similarity / g taylor 

38

thursday, july 12, 2012

one frame of fame dataset

s
n
o
i
t
a
t
i

m

i
 
f
o
 
r
e
b
m
u
n

40

35

30

25

20

15

10

5

0

 
0

train
test

 

y
c
n
e
u
q
e
r
f

1000

2000

3000

4000

5000

6000

seed frame

40

35

30

25

20

15

10

5

0
 
0

13 jul 2012 /
learning similarity / g taylor 

39

thursday, july 12, 2012

 

train
test

100

200

300

number of imitations

400

500

id162

    the most common evaluation metric used by the ir community is discounted 

cumulative gain (dcg)

    typically used to measure search engine performance
    user submits query, presented with a ranked list of results

dcg@k =

2gj     1
log(j + 1)

k   j=1

    we only consider the    rst k results
    in our experiments, we let gj = (1 + |yi     yj|)   1

query
(test set)

ranked list 
(training set)

13 jul 2012 /
learning similarity / g taylor 

40

thursday, july 12, 2012

methods for similarity score

    simple:

ij = (1 + |yi     yj|)   1
sm

    block:

sm
ij = 1 if

|yi     yj| <= w

13 jul 2012 /
learning similarity / g taylor 

41

thursday, july 12, 2012

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

10

20

30

40

50

60

10

20

30

40

50

60

retrieval performance: quantitative

    both pixel-based matching, 

and pca perform horribly: 
pixels: 0.021, pca-32: 0.026

    standard drlim does not 
consider graded similarity

    performance of using a    xed-

size window of constant af   nity 
falls between drlim and soft 
methods

    similar performance observed 

for k=1, k=5, k=20 nn

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

 

0
0

2

13 jul 2012 /
learning similarity / g taylor 

42

thursday, july 12, 2012

k=10

 

gist+linear
c   drlim
pse (block, w=10)
pse

8

10
5
x 10

4

6

# weight updates

retrieval performance: qualitative

query
(test set)

nearest neighbours

thursday, july 12, 2012

more qualitative results

query
(test set)

nearest neighbours

thursday, july 12, 2012

even more qualitative results

query
(test set)

nearest neighbours

thursday, july 12, 2012

48

face detection using our learned embedding

    users on amazon mechanical turk 

were asked to provide facial 
bounding boxes for the training set
    we reduced our training (and test) 

set to the subset of    valid    
annotations - for example, some 
images do not contain faces and 
therefore were not assigned 
bounding boxes

13 jul 2012 /
learning similarity / g taylor 

46

thursday, july 12, 2012

nearest neighbor face detection

query image (from test set)

proposed bounding box

13 jul 2012 /
learning similarity / g taylor 

47

thursday, july 12, 2012

find nearest neighbors via learned pose-sensitive embedding

apply median bounding box of neighbors

scoring bounding boxes

    use intersection over union score (pascal voc): iou > 0.5?
    red - our guess; green - ground truth

intersection

union

iou=0.6271

13 jul 2012 /
learning similarity / g taylor 

48

thursday, july 12, 2012

outperforming pittpatt

    pittpatt is a commercial face 

detector

t

e
a
r
 

n
o

i
t
c
e
e
d

t

    opencv - vj is a commonly used 
implementation of boosting (viola-
jones) - known to not work that well

    detection is iou > 0.5

13 jul 2012 /
learning similarity / g taylor 

49

thursday, july 12, 2012

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35
 
0

 

our method
pittpatt (best)
pittpatt (most confident)
opencv   vj

20

40

60

80

100

nearest neighbours (k)

pittpatt failures (pse succeeds)

13 jul 2012 /
learning similarity / g taylor 

50

thursday, july 12, 2012

pittpatt failures (2)

13 jul 2012 /
learning similarity / g taylor 

51

thursday, july 12, 2012

pittpatt produces incorrect detection

pse

pittpatt

pse

pittpatt

13 jul 2012 /
learning similarity / g taylor 

52

thursday, july 12, 2012

unsupervised
learn similarity structure completely from unlabeled data.
dif   cult to ensure that similar examples map to similar codes.

summary

108
109
110
111
112
113
114
115
116
117
118
119
120
121
supervised
122
123
use labels or neighbourhood graph to inform map.
124
often, this information is not available!
125
126
127
    
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149

weakly supervised
use of temporal coherence to guide learning.

13 jul 2012 /
learning similarity / g taylor 

53

high-dimensional to low-dimensional space. finally we introduce a related but different objective
for our model based on drlim.
3.1 neighbourhood components analysis

address space

semantic
hashing
function

semantically
similar
documents

nca (both linear and nonlinear) and drlim do not presuppose the existence of a meaningful and
computable distance metric in the input space. they only require that neighbourhood relationships
be de   ned between training samples. this is well-suited for learning a metric for non-parametric
classi   cation (e.g. knn) on high-dimensional data. if the original data does not contain discrete
class labels, but real-valued labels (e.g. pose information for images of people) one alternative is to
de   ne neighbourhoods based on the distance in the real-valued label space and proceed as usual.
however, if classi   cation is not our ultimate goal, we may wish to exploit the    soft    nature of the
labels and use an alternative objective (i.e. one that does not optimize knn performance).
suppose we are given a set of n labeled training cases {xi, yi}, i = 1, 2, . . . , n, where xi     rd,
and yi     rl. for each training vector, xi, the id203 that point i selects one of its neighbours j
is de   ned in the transformed feature space [12]:
exp(   d2
xi
ij)

document 

(1)

pij =

dij = ||zi     zj||2

where we use a euclidean distance metric dij and zi = f (xi|  ) is the mapping (parametrized
by   ) from input space to feature space. for nca this is typically linear, but it can be extended
to be nonlinear through back-propagation (for example in [32] it is a multi-layer neural network).
nca assumes that the labels, yi are discrete yi     1, 2, . . . , c rather than real-valued and seeks to
maximize the expected number of correctly classi   ed points on the training data which minimizes:

   k   =i exp(   d2

,

ik)

lnca =    

n   i=1    j:yi=yj

pij.

(2)

the parameters are found by minimizing lnca with respect to   , back-propagating in the case of
a multi-layer parametrization. instead of seeking to optimize knn classi   cation performance, we
can use the nca regression (ncar) objective [18]:

lncar =

pij||yi     yj||2
2.

(3)

n   i=1   j

intuitively, if i and j are neighbours in feature space, then they should also lie close together in label
space. while we use the euclidean distance in label space, our approach generalizes to other metrics
which may be more appropriate for a different domain.
keller et al. [18] consider the linear case of ncar, where    is a weight matrix and y is a scalar
representing bellman error to map states with similar bellman errors close together. similar to
nca, we can extend this objective to the nonlinear, multi-layer case. we simply need to compute
the derivative of lncar with respect to the output of the mapping, zi, and backpropagate through
the remaining layers of the network. the gradient can be computed ef   ciently as:

   lncar

=   j

(zi     zj)   pij   y2

ij       i    + pji   y2

ij       j       .

(4)

thursday, july 12, 2012

the university of guelph is not in belgium!

montreal

new york

guelph

toronto

thursday, july 12, 2012

