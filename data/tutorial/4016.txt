does the brain do 
 inverse graphics? 

geoffrey hinton, alex krizhevsky, navdeep jaitly, 

tijmen tieleman & yichuan tang 

 

department of computer science 

university of toronto 

 
 

the representation used by the neural 

nets that work best for recognition 

 (yann lecun) 

       convolutional neural nets use multiple 

layers of  feature detectors that have local 
receptive fields and shared weights. 

       the feature extraction layers are interleaved 

with sub-sampling layers that throw away 
information about precise position in order 
to achieve some translation invariance. 

combining the outputs of replicated features 

       get a small amount of translational 

invariance at each level by averaging some 
neighboring replicated detectors to give a 
single output to the next level. 
      this reduces the number of inputs to the 
next layer of feature extraction, thus 
allowing us to have many more different 
types of feature at the next layer. 
      taking the maximum activation of some 
neighboring detectors works slightly better. 

why convolutional neural 

networks are doomed 

       convolutional nets are doomed for two reasons: 

       sub-sampling loses the precise spatial relationships 

between  higher-level parts such as a nose and a 
mouth. the precise spatial relationships are needed 
for identity recognition 
       but overlapping the sub-sampling pools mitigates 
this. 

       they cannot extrapolate their understanding of 

geometric relationships to radically new viewpoints. 

 

equivariance vs invariance 

       sub-sampling tries to make the neural activities 

invariant for small changes in viewpoint. 
      this is a silly goal, motivated by the fact that 
the final label needs to be viewpoint-invariant. 

       its better to aim for equivariance: changes in 
viewpoint lead to corresponding changes in 
neural activities.  
      in the perceptual system, its the weights that 
code viewpoint-invariant knowledge, not the 
neural activities. 

what is the right representation of images? 
       id161 is inverse computer graphics, so the 

higher levels of a vision system should look like the 
representations used in graphics. 

       graphics programs use id187 in which 

spatial structure is modeled by matrices that represent 
the transformation from a coordinate frame embedded in 
the whole to a coordinate frame embedded in each part. 
       these matrices are totally viewpoint invariant. 
       this representation makes it easy to compute the 
relationship between a part and the retina from the 
relationship between a whole and the retina. 
       its just a matrix multiply! 

some psychological evidence that our visual 
systems impose coordinate frames in order 

to represent shapes (after irvin rock) 

the cube task 

       you can all imagine a wire-frame cube. 

       now imagine the cube standing on one corner 
so that body-diagonal from that corner, through 
the center of the cube to the opposite corner is 
vertical. 

       where are the other corners of the cube? 

an arrangement of 6 rods 

a different percept of the 6 rods 

alternative representations 
       the very same arrangement of rods can be 

represented in quite different ways. 
      its not like the necker cube where the alternative 
percepts disagree on depth. 

       the alternative percepts do not disagree, but they 

make different facts obvious. 
      in the zig-zag representation it is obvious that 
there is one pair of parallel edges.  
      in the crown representation there are no obvious 
pairs of parallel edges because the edges do not 
align with the intrinsic frame of any of the parts. 

a structural description of the    crown    

formed by the six rods 

a structural description of the    zig-zag    

a mental image of the crown 

a mental image 
specifies how 
each node is 
related to the 
viewer.  

this makes 
it easier to 
   see    new 
relationships 

a task that forces you to form a mental 

image 

       imagine making the following journey: 

      you go a mile east. 
      then you go a  mile north. 
      then you go a mile east again. 

       what is the  direction back to your staring point? 

analog operations on structural 

descriptions 

       we can imagine the    petals    of the crown all folding 

upwards and inwards. 
       what is happening in our heads when we imagine this 

continuous transformation? 

       why are the easily imagined transformations quite 

different for the two different structural descriptions? 
       one particular continuous transformation called    mental 
rotation    was intensively studied by roger shepard and 
other psychologists in the 1970   s  
       mental rotation really is continuous: when we are 

halfway through performing a mental rotation we have 
a mental representation at the intermediate orientation.   

mental rotation:  

more evidence for coordinate frames 

we perform mental rotation to decide if the tilted r has the 
correct handedness, not to recognize that it is an r.  
 
but why do we need to do mental rotation to decide 
handedness? 
 

what mental rotation achieves 

      

it is very difficult to compute the handedness of a 
coordinate transformation by looking at the individual 
elements of the matrix. 
       the handedness is the sign of the determinant of the 

matrix relating the object to the viewer.  

       this is a high-order parity problem. 

       to avoid this computation, we rotate to upright preserving 
handedness, then we look to see which way it faces when 
it is in its familiar orientation. 
if we had individual neurons that represented a whole 
pose,  we would not have a problem with identifying 
handedness, because these neurons would have a 
handedness. 

      

two layers in a hierarchy of parts 

       a higher level visual entity is present if several lower level 
visual entities can agree on their predictions for its pose.  

jp

jt
 face  

tt
i
ij

   

tt
h
hj

ijt

hjt

hp

pose of mouth                   
i. e. relationship to camera 

ht
nose 

ip

it
mouth 

a crucial property of the pose vectors 

       they allow spatial transformations to be 

modeled by linear operations. 
      this makes it easy to learn a hierarchy of 
visual entities. 
      it makes it easy to generalize across 
viewpoints.  

       the invariant geometric properties of a shape 

are in the weights, not in the activities. 
      the activities are equivariant. 

extrapolating shape recognition to very 
different sizes, orientations and positions 

that were not in the training data. 

       current wisdom (e.g. andrew ng   s talk)  

      learn different models for very different 
viewpoints. 
      get a lot more training data to cover all 
significantly different viewpoints. 

       the only reasonable alternative? 

      massive extrapolation requires linear models. 
      so capture the knowledge of the geometry of 
the shape in a linear model.  

a toy example of what we could do if we could 

reliably extract the poses of parts of objects 
       consider images composed of five ellipses. 
       the shape is determined entirely by the 

spatial relations between the ellipses 
because all parts have the same shape. 

       can we sets of spatial relationships from data 

that contains several different shapes? 
       can we generalize far beyond the range of 

variation in the training examples?  

       can we learn without being told which 
ellipse corresponds to which part of a 
shape? 

examples of two shapes (yichuan tang) 

examples of training data 

examples of training data 

it can extrapolate! 

generated data after training 

generated data after training 

learning one shape using factor analysis 
       the pose of the whole can predict the poses of all the 
parts. this generative model is easy to fit using the em 
algorithm for factor analysis.  

jt

 face  
hjt

= factor loadings 

ht
nose 

ijt

it
mouth 

learning the right assignments of the 

ellipses to the parts of a shape. 

       if we do not know how to assign the ellipses to 

the parts of a known shape we can try many 
different assignments and pick the one that 
gives the best reconstruction. 
      then we assume that is the correct 
assignment and use that assignment for 
learning. 

       this quickly leads to models that have strong 

opinions about the correct assignment. 

       we could eliminate most of th search by using a 

feed-forward neural net to predict the 
assignments. 

fitting a mixture of shapes 

       we can use a mixture of factor analysers (mfa) to 

fit images that contain many different shapes so 
long as each image only contains one example of 
one shape. 

recognizing 
deformed 
shapes with 

mfa 

a big problem 

       how do we get from pixels to the first level 
parts that output explicit pose parameters? 
      we do not have any labeled data. 
      this    de-rendering    stage has to be very 
non-linear.  

a picture of three capsules 

i 

x 

y 

i 

x 

y 

i 

x 

y 

intensity of the 
capsule   s visual 
entity. 

input 
image 

extracting pose information by using a 

domain specific decoder  

(navdeep jaitly & tijmen tieleman) 
       the idea is to define a simple decoder that 

produces an image by adding together 
contributions from each capsule. 

       each capsule learns a fixed    template    that gets 

intensity-scaled and translated differently for 
reconstruction each image. 

       the encoder must learn to extract the 

appropriate intensity and translation for each 
capsule from the input image.  

output 
image 

decoder: add together intensity-scaled and 
translated contributions from each capsule. 

learned 
template 

i 

x 

y 

i 

x 

y 

i 

x 

y 

train encoder using 
id26 

the templates learned by the 

autoencoder 

each template is 
multiplied by a case-
specific intensity and 
translated by a case-
specific 
 
then it is added to the 
output image.  

x   
y

   ,

 

n
o

i
t
c
u
r
t
s
n
o
c
e
r

 
r
o
r
r
e

 
 
.

n
o
c
e
r

 

e
g
a
m

i

 

1

 

l

e
u
s
p
a
c

 

2

 

l

e
u
s
p
a
c

 

3

 

l

e
u
s
p
a
c

 

4

 

l

e
u
s
p
a
c

modeling the capsule outputs with a 

factor analyser 

10 factors 

yxiyxi
111

2

2

yxi
33
3

yxi
44

4

.......

2

we learn a mixture of 10 factor analysers on 
unlabeled data. what do the means look like? 

means discovered by a mixture of factor analysers 

mean  - 2     of each factor 

  

mean  + 2     of each factor 

  

directly on 
pixels 

on outputs of 
capsules 

another simple way to 

learn the lowest level parts 

       use pairs of images that are related by a known 

coordinate transformation 
      e.g. a small translation of the image.  

       we often have non-visual access to image 

transformations 
      e.g. when we make an eye-movement. 

       cats learn to see much more easily if they control 

the image transformations (held & hein) 

learning the lowest level capsules 

       we are given a pair of images related by a known translation. 

step 1: compute the capsule outputs for the first image. 

        each capsule uses its own set of    recognition    hidden    

units to extract the x and y coordinates of the visual entity    
it represents (and also the id203 of existence)  

step 2: apply the transformation to the outputs of each capsule 
   - just add   x  to each x output and   y to each y output 
step 3: predict the transformed image from the transformed 

outputs of the capsules 
       each capsule uses its own set of    generative    hidden       

units to compute its contribution to the prediction. 

actual 
output  

target 
output 

gate 

+  x  +  y 
x 

y 

p

+  x  +  y 
x 

y 

p

+  x  +  y 
x 

y 

p

id203 that 
the capsule   s 
visual entity is 
present 

input 
image 

why it has to work 

       when the net is trained with back-propagation, 
the only way it can get the transformations right 
is by using x and y in a way that is consistent 
with the way we are using   x  and   y. 

       this allows us to force the capsules to extract 
the coordinates of visual entities without having 
to decide what the entities are or where they are.  

output 
image 

gate 

x 

p

x 

p

x 

p

id203 
that the 
feature is 
present 

input 
image 

dealing with the three   dimensional world 

       use stereo images to allow the encoder to extract 

3-d poses. 
      using capsules, 3-d would not be much harder 
than 2-d if we started with 3-d pixels. 
      the loss of the depth coordinate is a separate 
problem from the complexity of 3-d geometry. 

       at least capsules stand a chance of dealing with 

the 3-d geometry properly. 
      convolutional nets in 3-d are a nightmare. 

 

dealing with 3-d viewpoint (alex krizhevsky) 
input stereo pair      output stereo pair       correct output 

it also works on test data 

the  end 

       the work on transforming autoencoders  is in a 
paper called    transforming autoencoders    on my 
webpage: 

     http://www.cs.toronto.edu/~hinton/absps/transauto6.pdf 

       the work on    dropout    described in my previous 

lecture is in a paper on arxiv: 

     http://arxiv.org/abs/1207.0580 

relationship to slow feature analysis 

       slow feature analysis uses a very primitive 

model of the dynamics. 
      it assumes that the underlying state does not 
change much.  
      this is a much weaker way of extracting 
features than using precise knowledge of the 
dynamics. 

       also, slow feature analysis only produces dumb 
features that do not output explicit instantiation 
parameters.  

relationship to a kalman filter 

       a linear dynamical system can predict the next 

observation vector. 
       but only when there is a linear relationship between 

the underlying dynamics and the observations. 

       the extended kalman filter assumes linearity about the 

current operating point. it   s a fudge. 

       transforming autoencoders use non-linear recognition 

units to map the observation space to the space in which 
the dynamics is linear. then they use non-linear 
generation units to map the prediction back to 
observation space  
       this is a much better approach than the extended 

kalman filter, especially when the dynamics is known. 

