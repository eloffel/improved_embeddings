learning representations of sequences
ipam graduate summer school on deep learning

graham taylor

school of engineering
university of guelph

papers and software available at: http://www.uoguelph.ca/~gwtaylor

thursday, july 12, 2012

overview: this talk

13 jul 2012 /
learning representations of sequences / g taylor 

2

thursday, july 12, 2012

overview: this talk

    learning representations of temporal data:
- existing methods and challenges faced
- recent methods inspired by deep learning

and representation learning

x (input)

x
w

n

nx

x
w

n

nx

y (output)

ny

y
w

n

y
w

n

ny

np

pk
(cid:68)

np

nz

 pk

pooling
layer

nz

zk
m,n

 zk

feature
layer

13 jul 2012 /
learning representations of sequences / g taylor 

2

thursday, july 12, 2012

overview: this talk

    learning representations of temporal data:
- existing methods and challenges faced
- recent methods inspired by deep learning

and representation learning

x (input)

x
w

n

nx

x
w

n

nx

y (output)

ny

y
w

n

y
w

n

ny

np

pk
(cid:68)

np

nz

 pk

pooling
layer

nz

zk
m,n

 zk

feature
layer

    applications: in particular, modeling human pose and activity
- highly structured data: e.g. motion capture
- weakly structured data: e.g. video

13 jul 2012 /
learning representations of sequences / g taylor 

2

thursday, july 12, 2012

t

n
e
n
o
p
m
o
c

w

j

1
2
3
4
5
6
7
8
9
10

100

200

300

400

500

600

700

outline

13 jul 2012 /
learning representations of sequences / g taylor 

3

thursday, july 12, 2012

outline

learning representations from sequences
existing methods, challenges

yt   2

yt   1

yt

13 jul 2012 /
learning representations of sequences / g taylor 

3

thursday, july 12, 2012

outline

learning representations from sequences
existing methods, challenges

yt   2

yt   1

yt

composable, distributed-state models for sequences
conditional restricted id82s and their variants

13 jul 2012 /
learning representations of sequences / g taylor 

3

thursday, july 12, 2012

outline

learning representations from sequences
existing methods, challenges

yt   2

yt   1

yt

composable, distributed-state models for sequences
conditional restricted id82s and their variants

using learned representations to analyze video
a brief and (incomplete survey of deep learning for activity recognition

x (input)

x
w

n

nx

x
w

n

nx

y (output)

ny

y
w

n

y
w

n

ny

np

pk
(cid:68)

np

nz

 pk

pooling
layer

nz

zk
m,n

 zk

feature
layer

13 jul 2012 /
learning representations of sequences / g taylor 

3

thursday, july 12, 2012

time series data

    time is an integral part of many human behaviours (motion, reasoning)
    in building statistical models, time is sometimes ignored, often problematic
    models that do incorporate dynamics fail to account for the fact that data is 

often high-dimensional, nonlinear, and contains long-range dependencies

13 jul 2012 /
learning representations of sequences / g taylor 

4

thursday, july 12, 2012

graphic: david mccandless, informationisbeautiful.net

intensity (no of stories)2003200120022000200420052008200920062007time series data

    time is an integral part of many human behaviours (motion, reasoning)
    in building statistical models, time is sometimes ignored, often problematic
    models that do incorporate dynamics fail to account for the fact that data is 

often high-dimensional, nonlinear, and contains long-range dependencies

today we will discuss a number of models that have been developed to 
address these challenges.

13 jul 2012 /
learning representations of sequences / g taylor 

4

thursday, july 12, 2012

graphic: david mccandless, informationisbeautiful.net

intensity (no of stories)2003200120022000200420052008200920062007vector autoregressive models

vt = b +

m   m=1

amvt   m + et

    have dominated statistical time-series analysis for approx. 50 years
    can be    t easily by least-squares regression
    can fail even for simple nonlinearities present in the system
- but many data sets can be modeled well by a linear system
    well understood; many extensions exist

13 jul 2012 /
learning representations of sequences / g taylor 

5

thursday, july 12, 2012

markov (   id165   ) models

vt   2

vt   1

vt

    fully observable
    sequential observations may have nonlinear dependence
    derived by assuming sequences have markov property:

    this leads to joint:

p(vt|{vt   1
1 }) = p({vn
1 })
    number of parameters exponential in     !
n

1 }) = p(vt|{vt   1
t   t=n +1

p({vt

t   n})
p(vt|{vt   1

t   n})

13 jul 2012 /
learning representations of sequences / g taylor 

6

thursday, july 12, 2012

exponential increase in parameters
|  | = qn +1

here, q = 3

p(a|a)

p(b|a)

p(c|a)

p(a|b)

p(b|b)

p(c|b)

p(a|c)

p(b|c)

p(c|c)

1st order markov

(n = 1)

13 jul 2012 /
learning representations of sequences / g taylor 

7

thursday, july 12, 2012

exponential increase in parameters
|  | = qn +1

p(a|{a,c}) p(b|{a,c}) p(c|{a,c})

here, q = 3

p(a|{a,b}) p(b|{a,b}) p(c|{a,b})

p(a|{b,a}) p(b|{b,a}) p(c|{b,c})

p(a|a)

p(b|a)

p(c|a)

p(a|{a,a}) p(b|{a,a}) p(c|{a,a})

p(a|{b,a}) p(b|{b,a}) p(c|{b,b})

p(a|{c,a}) p(b|{c,a}) p(c|{c,c})

p(a|b)

p(b|b)

p(c|b)

p(a|{b,a}) p(b|{b,a}) p(c|{b,a})

p(a|{c,a}) p(b|{c,a}) p(c|{c,b})

p(a|c)

p(b|c)

p(c|c)

p(a|{c,a}) p(b|{c,a}) p(c|{c,a})

1st order markov

(n = 1)

2nd order markov (n = 2)

13 jul 2012 /
learning representations of sequences / g taylor 

7

thursday, july 12, 2012

id48 (id48)

ht   2

ht   1

vt   2

vt   1

ht

vt

13 jul 2012 /
learning representations of sequences / g taylor 

8

thursday, july 12, 2012

id48 (id48)

introduces a hidden 
state that controls the 
dependence of the 
current observation 
on the past

ht   2

ht   1

vt   2

vt   1

ht

vt

13 jul 2012 /
learning representations of sequences / g taylor 

8

thursday, july 12, 2012

id48 (id48)

introduces a hidden 
state that controls the 
dependence of the 
current observation 
on the past

ht   2

ht   1

vt   2

vt   1

ht

vt

    successful in speech & id38, biology
    de   ned by 3 sets of parameters:
- initial state parameters, 
  
- transition matrix, 
- emission distribution,
    factored joint distribution:

p(vt|ht)

a

p({ht},{vt}) = p(h1)p(v1|h1)

13 jul 2012 /
learning representations of sequences / g taylor 

8

thursday, july 12, 2012

t   t=2

p(ht|ht   1)p(vt|ht)

id136 and learning

13 jul 2012 /
learning representations of sequences / g taylor 

9

thursday, july 12, 2012

h1

v1

h1

v1

h1

v1

h2

. . .

ht

v2

h2

v2

vt

. . .

ht

vt

h2

. . .

ht

v2

vt

id136 and learning

    typically three tasks we want to 

perform in an id48:

13 jul 2012 /
learning representations of sequences / g taylor 

9

thursday, july 12, 2012

h1

v1

h1

v1

h1

v1

h2

. . .

ht

v2

h2

v2

vt

. . .

ht

vt

h2

. . .

ht

v2

vt

id136 and learning

    typically three tasks we want to 

perform in an id48:
- likelihood estimation

13 jul 2012 /
learning representations of sequences / g taylor 

9

thursday, july 12, 2012

p({v1, . . . , vt}|  )

h2

   {h1,...,ht }

. . .

ht

v2

h2

v2

vt

. . .

ht

vt

h2

. . .

ht

v2

vt

h1

v1

h1

v1

h1

v1

id136 and learning

    typically three tasks we want to 

perform in an id48:
- likelihood estimation
- id136

13 jul 2012 /
learning representations of sequences / g taylor 

9

thursday, july 12, 2012

p({v1, . . . , vt}|  )

h2

   {h1,...,ht }

. . .

ht

v2

h2

v2

vt

. . .

ht

vt

h2

. . .

ht

v2

vt

h1

v1

h1

v1

h1

v1

???id136 and learning

    typically three tasks we want to 

perform in an id48:
- likelihood estimation
- id136
- learning

13 jul 2012 /
learning representations of sequences / g taylor 

9

thursday, july 12, 2012

p({v1, . . . , vt}|  )

h2

   {h1,...,ht }

. . .

ht

v2

h2

v2

vt

. . .

ht

vt

h2

. . .

ht

v2

vt

h1

v1

h1

v1

h1

v1

???id136 and learning

    typically three tasks we want to 

perform in an id48:
- likelihood estimation
- id136
- learning
    all are exact and tractable due to 

the simple structure of the id48

13 jul 2012 /
learning representations of sequences / g taylor 

9

thursday, july 12, 2012

p({v1, . . . , vt}|  )

h2

   {h1,...,ht }

. . .

ht

v2

h2

v2

vt

. . .

ht

vt

h2

. . .

ht

v2

vt

h1

v1

h1

v1

h1

v1

???limitations of id48s

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure
    id48 cannot model such data ef   ciently: a single, discrete 

k-state multinomial must represent the history of the time series

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure
    id48 cannot model such data ef   ciently: a single, discrete 

k-state multinomial must represent the history of the time series

    to model     bits of information, they need       hidden states

k

2k

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure
    id48 cannot model such data ef   ciently: a single, discrete 

k-state multinomial must represent the history of the time series

    to model     bits of information, they need       hidden states

k

2k

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure
    id48 cannot model such data ef   ciently: a single, discrete 

k-state multinomial must represent the history of the time series

    to model     bits of information, they need       hidden states

k

2k

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure
    id48 cannot model such data ef   ciently: a single, discrete 

k-state multinomial must represent the history of the time series

2k
    to model     bits of information, they need       hidden states
    we seek models with distributed hidden state:

k

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure
    id48 cannot model such data ef   ciently: a single, discrete 

k-state multinomial must represent the history of the time series

k

2k
    to model     bits of information, they need       hidden states
    we seek models with distributed hidden state:
- capacity linear in the number of components

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

limitations of id48s

    many high-dimensional data sets contain rich componential structure
    id48 cannot model such data ef   ciently: a single, discrete 

k-state multinomial must represent the history of the time series

k

2k
    to model     bits of information, they need       hidden states
    we seek models with distributed hidden state:
- capacity linear in the number of components

13 jul 2012 /
learning representations of sequences / g taylor 

10

thursday, july 12, 2012

linear dynamical systems

ht   2

ht   1

vt   2

vt   1

ht

vt

13 jul 2012 /
learning representations of sequences / g taylor 

11

thursday, july 12, 2012

linear dynamical systems

graphical model is the 
same as id48 but 
with real-valued state 
vectors

ht   2

ht   1

vt   2

vt   1

ht

vt

13 jul 2012 /
learning representations of sequences / g taylor 

11

thursday, july 12, 2012

linear dynamical systems

graphical model is the 
same as id48 but 
with real-valued state 
vectors

ht   2

ht   1

vt   2

vt   1

ht

vt

    characterized by linear-gaussian dynamics and observations:
p(vt|ht) = n (vt; cht, r)

p(ht|ht   1) = n (ht; aht   1, q)

    id136 is performed using kalman smoothing (belief propagation)
    learning can be done by em
    dynamics, observations may also depend on an observed input (control)

13 jul 2012 /
learning representations of sequences / g taylor 

11

thursday, july 12, 2012

latent representations for real-world data

data for many real-world problems (e.g. vision, motion capture) is high-
dimensional, containing complex non-linear relationships between components

13 jul 2012 /
learning representations of sequences / g taylor 

12

thursday, july 12, 2012

latent representations for real-world data

data for many real-world problems (e.g. vision, motion capture) is high-
dimensional, containing complex non-linear relationships between components

id48
pro: complex, nonlinear emission model
con: single     -state multinomial represents entire history

k

13 jul 2012 /
learning representations of sequences / g taylor 

12

thursday, july 12, 2012

latent representations for real-world data

data for many real-world problems (e.g. vision, motion capture) is high-
dimensional, containing complex non-linear relationships between components

id48
pro: complex, nonlinear emission model
con: single     -state multinomial represents entire history

k

linear dynamical systems
pro: state can convey much more information
con: emission model constrained to be linear

13 jul 2012 /
learning representations of sequences / g taylor 

12

thursday, july 12, 2012

learning distributed representations

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

    can we learn, in a similar way, representations of 

temporal data?

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

    can we learn, in a similar way, representations of 

temporal data?

    simple idea: spatial representation of time:

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

    can we learn, in a similar way, representations of 

temporal data?

    simple idea: spatial representation of time:

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

spectrogram: http://soundsofstanford.wordpress.com

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

    can we learn, in a similar way, representations of 

temporal data?

    simple idea: spatial representation of time:

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

spectrogram: http://soundsofstanford.wordpress.com

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

    can we learn, in a similar way, representations of 

temporal data?

    simple idea: spatial representation of time:

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

spectrogram: http://soundsofstanford.wordpress.com

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

    can we learn, in a similar way, representations of 

temporal data?

    simple idea: spatial representation of time:
- need a buffer; not biologically plausible
- cannot process inputs of differing length
- cannot distinguish between absolute and relative 

position

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

spectrogram: http://soundsofstanford.wordpress.com

learning distributed representations

    simple networks are capable of discovering useful 

and interesting internal representations of static 
data (e.g. many of the talks so far!)

    can we learn, in a similar way, representations of 

temporal data?

    simple idea: spatial representation of time:
- need a buffer; not biologically plausible
- cannot process inputs of differing length
- cannot distinguish between absolute and relative 

position

    this motivates an implicit representation of time in 
connectionist models where time is represented by 
its effect on processing

13 jul 2012 /
learning representations of sequences / g taylor 

13

thursday, july 12, 2012

spectrogram: http://soundsofstanford.wordpress.com

a rich history

13 jul 2012 /
learning representations of sequences / g taylor 

14

thursday, july 12, 2012

a rich history

elman networks
time-delayed    context    units, truncated bptt.
(elman, 1990), (jordan, 1986)

13 jul 2012 /
learning representations of sequences / g taylor 

14

thursday, july 12, 2012

a rich history

elman networks
time-delayed    context    units, truncated bptt.
(elman, 1990), (jordan, 1986)

mean-   eld id82s through time
id136 is approximate, learning less ef   cient than id48s.
(williams and hinton, 1990)

13 jul 2012 /
learning representations of sequences / g taylor 

14

thursday, july 12, 2012

a rich history

elman networks
time-delayed    context    units, truncated bptt.
(elman, 1990), (jordan, 1986)

mean-   eld id82s through time
id136 is approximate, learning less ef   cient than id48s.
(williams and hinton, 1990)

spiking id82s
hidden-state dynamics and smoothness constraints on observed data.
(hinton and brown, 2000)

13 jul 2012 /
learning representations of sequences / g taylor 

14

thursday, july 12, 2012

recurrent neural networks

  yt   1

ht   1

vt   1

  yt

ht

vt

  yt+1

ht+1

vt+1

= w hvvt + w hhht   1 + bh
= w yhht + by

xt
hj,t = f (xj,t)
st
  yk,t = g(sk,t)

13 jul 2012 /
learning representations of sequences / g taylor 

15

thursday, july 12, 2012

(figure from martens and sutskever)

recurrent neural networks

  yt   1

ht   1

vt   1

  yt

ht

vt

  yt+1

ht+1

vt+1

    neural network replicated in time

= w hvvt + w hhht   1 + bh
= w yhht + by

xt
hj,t = f (xj,t)
st
  yk,t = g(sk,t)

13 jul 2012 /
learning representations of sequences / g taylor 

15

thursday, july 12, 2012

(figure from martens and sutskever)

recurrent neural networks

  yt   1

ht   1

vt   1

  yt

ht

vt

  yt+1

ht+1

vt+1

    neural network replicated in time
    at each step, receives input vector, updates its internal representation via 

nonlinear id180, and makes a prediction:

= w hvvt + w hhht   1 + bh
= w yhht + by

xt
hj,t = f (xj,t)
st
  yk,t = g(sk,t)

13 jul 2012 /
learning representations of sequences / g taylor 

15

thursday, july 12, 2012

(figure from martens and sutskever)

training recurrent neural networks

13 jul 2012 /
learning representations of sequences / g taylor 

16

thursday, july 12, 2012

training recurrent neural networks

    possibly high-dimensional, distributed, internal representation and nonlinear 

dynamics allow model, in theory, model complex time series

13 jul 2012 /
learning representations of sequences / g taylor 

16

thursday, july 12, 2012

training recurrent neural networks

    possibly high-dimensional, distributed, internal representation and nonlinear 

dynamics allow model, in theory, model complex time series

    exact gradients can be computed exactly via id26 through time

13 jul 2012 /
learning representations of sequences / g taylor 

16

thursday, july 12, 2012

training recurrent neural networks

    possibly high-dimensional, distributed, internal representation and nonlinear 

dynamics allow model, in theory, model complex time series

    exact gradients can be computed exactly via id26 through time
    it is an interesting and powerful model. what   s the catch?
- training id56s via id119 fails on simple problems
- attributed to    vanishing    or    exploding    gradients
- much work in the 1990   s focused on identifying and addressing these 

issues: none of these methods were widely adopted

13 jul 2012 /
learning representations of sequences / g taylor 

16

thursday, july 12, 2012

training recurrent neural networks

    possibly high-dimensional, distributed, internal representation and nonlinear 

dynamics allow model, in theory, model complex time series

    exact gradients can be computed exactly via id26 through time
    it is an interesting and powerful model. what   s the catch?
- training id56s via id119 fails on simple problems
- attributed to    vanishing    or    exploding    gradients
- much work in the 1990   s focused on identifying and addressing these 

issues: none of these methods were widely adopted

13 jul 2012 /
learning representations of sequences / g taylor 

16

thursday, july 12, 2012

(figure adapted from james martens)

training recurrent neural networks

    possibly high-dimensional, distributed, internal representation and nonlinear 

dynamics allow model, in theory, model complex time series

    exact gradients can be computed exactly via id26 through time
    it is an interesting and powerful model. what   s the catch?
- training id56s via id119 fails on simple problems
- attributed to    vanishing    or    exploding    gradients
- much work in the 1990   s focused on identifying and addressing these 

issues: none of these methods were widely adopted

    best-known attempts to resolve the problem of id56 training:
- long short-term memory (lstm) (hochreiter and schmidhuber 1997)
- echo-state network (esn) (jaeger and haas 2004)

13 jul 2012 /
learning representations of sequences / g taylor 

16

thursday, july 12, 2012

failure of id119

two hypotheses for why id119 fails for nn:

18 may 2012 /
learning representations of sequences / g taylor 

17

thursday, july 12, 2012

failure of id119

two hypotheses for why id119 fails for nn:
    increased frequency and severity of bad local minima

18 may 2012 /
learning representations of sequences / g taylor 

17

thursday, july 12, 2012

(figures from james martens)

failure of id119

two hypotheses for why id119 fails for nn:
    increased frequency and severity of bad local minima
    pathological curvature, like the type seen in the 

rosenbrock function:

f (x, y) = (1     x)2 + 100(y     x2)2

18 may 2012 /
learning representations of sequences / g taylor 

17

thursday, july 12, 2012

(figures from james martens)

second order methods

    model the objective function by the local approximation:

f (   + p)     q  (p)     f (  ) +    f (  )t p +

1
2

pt bp

where      is the search direction and      is a matrix which quanti   es curvature

p

b

    in newton   s method,     is the hessian matrix, 
    by taking the curvature information into account, newton   s method    rescales    

h

b

the gradient so it is a much more sensible direction to follow

    not feasible for high-dimensional problems!

18 may 2012 /
learning representations of sequences / g taylor 

18

thursday, july 12, 2012

(figure from james martens)

hessian-free optimization

based on exploiting two simple ideas (and some additional    tricks   ):

18 may 2012 /
learning representations of sequences / g taylor 

19

thursday, july 12, 2012

hessian-free optimization

based on exploiting two simple ideas (and some additional    tricks   ):
    for an n-dimensional vector    , the hessian-vector product         can easily be 

hd

d

computed using    nite differences at the cost of a single extra gradient evaluation
- in practice, the r-operator (perlmutter 1994) is used instead of    nite differences

18 may 2012 /
learning representations of sequences / g taylor 

19

thursday, july 12, 2012

hessian-free optimization

based on exploiting two simple ideas (and some additional    tricks   ):
    for an n-dimensional vector    , the hessian-vector product         can easily be 

hd

d

computed using    nite differences at the cost of a single extra gradient evaluation
- in practice, the r-operator (perlmutter 1994) is used instead of    nite differences

    there is a very effective algorithm for optimizing quadratic objectives which 

requires only hessian-vector products: linear conjugate-gradient (cg)

18 may 2012 /
learning representations of sequences / g taylor 

19

thursday, july 12, 2012

hessian-free optimization

based on exploiting two simple ideas (and some additional    tricks   ):
    for an n-dimensional vector    , the hessian-vector product         can easily be 

hd

d

computed using    nite differences at the cost of a single extra gradient evaluation
- in practice, the r-operator (perlmutter 1994) is used instead of    nite differences

    there is a very effective algorithm for optimizing quadratic objectives which 

requires only hessian-vector products: linear conjugate-gradient (cg)

this method was shown to effectively train id56s in the pathological 
long-term dependency problems they were previously not able to solve 
(martens and sutskever 2011)

18 may 2012 /
learning representations of sequences / g taylor 

19

thursday, july 12, 2012

hessian-free optimization

based on exploiting two simple ideas (and some additional    tricks   ):
    for an n-dimensional vector    , the hessian-vector product         can easily be 

hd

d

computed using    nite differences at the cost of a single extra gradient evaluation
- in practice, the r-operator (perlmutter 1994) is used instead of    nite differences

    there is a very effective algorithm for optimizing quadratic objectives which 

requires only hessian-vector products: linear conjugate-gradient (cg)

this method was shown to effectively train id56s in the pathological 
long-term dependency problems they were previously not able to solve 
(martens and sutskever 2011)

id56 demo code (using theano): http://github.com/gwtaylor/theano-id56

18 may 2012 /
learning representations of sequences / g taylor 

19

thursday, july 12, 2012

generative models with distributed state

13 jul 2012 /
learning representations of sequences / g taylor 

20

thursday, july 12, 2012

generative models with distributed state

    many sequences are high-dimensional and have complex structure
- music, human motion, weather/climate data
- id56s simply predict the expected value at the next time step
- they can   t capture multi-modality

13 jul 2012 /
learning representations of sequences / g taylor 

20

thursday, july 12, 2012

generative models with distributed state

    many sequences are high-dimensional and have complex structure
- music, human motion, weather/climate data
- id56s simply predict the expected value at the next time step
- they can   t capture multi-modality
    generative models (like restricted id82s) can capture 

complex distributions

13 jul 2012 /
learning representations of sequences / g taylor 

20

thursday, july 12, 2012

generative models with distributed state

    many sequences are high-dimensional and have complex structure
- music, human motion, weather/climate data
- id56s simply predict the expected value at the next time step
- they can   t capture multi-modality
    generative models (like restricted id82s) can capture 

complex distributions

    use binary hidden state and gain the best of id48 & lds:
- the nonlinear dynamics and observation model of the id48 without the 

limited hidden state

- the ef   cient, expressive state of the lds without the linear-gaussian 

restriction on dynamics and observations

13 jul 2012 /
learning representations of sequences / g taylor 

20

thursday, july 12, 2012

distributed binary hidden state

    using distributed binary representations 

for hidden state in directed models of time 
series makes id136 dif   cult. but we 
can:
- use a restricted id82 

(rbm) for the interactions between 
hidden and visible variables. a factorial 
posterior makes id136 and sampling 
easy.

- treat the visible variables in the previous 

time slice as additional    xed inputs

hidden variables (factors) at time t

visible variables (observations) at time t

one typically uses binary logistic 
units for both visibles and hiddens

13 jul 2012 /
learning representations of sequences / g taylor 

21

thursday, july 12, 2012

p(hj = 1|v) =   (bj +   i
p(vi = 1|h) =   (bi +   j

viwij)

hjwij)

modeling observations with an rbm

    so the distributed binary latent (hidden) state of an rbm lets us:
- model complex, nonlinear dynamics
- easily and exactly infer the latent binary state given the observations 
    but rbms treat data as static (i.i.d.)

hidden variables (factors) at time t

visible variables (joint angles) at time t

13 jul 2012 /
learning representations of sequences / g taylor 

22

thursday, july 12, 2012

modeling observations with an rbm

    so the distributed binary latent (hidden) state of an rbm lets us:
- model complex, nonlinear dynamics
- easily and exactly infer the latent binary state given the observations 
    but rbms treat data as static (i.i.d.)

hidden variables (factors) at time t

visible variables (joint angles) at time t

13 jul 2012 /
learning representations of sequences / g taylor 

22

thursday, july 12, 2012

modeling observations with an rbm

    so the distributed binary latent (hidden) state of an rbm lets us:
- model complex, nonlinear dynamics
- easily and exactly infer the latent binary state given the observations 
    but rbms treat data as static (i.i.d.)

hidden variables (factors) at time t

visible variables (joint angles) at time t

13 jul 2012 /
learning representations of sequences / g taylor 

22

thursday, july 12, 2012

conditional restricted id82s
(taylor, hinton and roweis nips 2006, jmlr 2011)

13 jul 2012 /
learning representations of sequences / g taylor 

23

thursday, july 12, 2012

conditional restricted id82s
(taylor, hinton and roweis nips 2006, jmlr 2011)

    start with a restricted id82 (rbm)

hidden layer

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

23

thursday, july 12, 2012

conditional restricted id82s
(taylor, hinton and roweis nips 2006, jmlr 2011)

    start with a restricted id82 (rbm)

    add two types of directed connections:

hidden layer

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

23

thursday, july 12, 2012

conditional restricted id82s
(taylor, hinton and roweis nips 2006, jmlr 2011)

    start with a restricted id82 (rbm)

    add two types of directed connections:
- autoregressive connections model short-term, linear structure

hidden layer

visible layer

recent history

13 jul 2012 /
learning representations of sequences / g taylor 

23

thursday, july 12, 2012

conditional restricted id82s
(taylor, hinton and roweis nips 2006, jmlr 2011)

    start with a restricted id82 (rbm)

    add two types of directed connections:
- autoregressive connections model short-term, linear structure
- history can also in   uence dynamics through hidden layer

hidden layer

visible layer

recent history

13 jul 2012 /
learning representations of sequences / g taylor 

23

thursday, july 12, 2012

conditional restricted id82s
(taylor, hinton and roweis nips 2006, jmlr 2011)

    start with a restricted id82 (rbm)

    add two types of directed connections:
- autoregressive connections model short-term, linear structure
- history can also in   uence dynamics through hidden layer

    conditioning does not change id136 nor learning

hidden layer

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

23

thursday, july 12, 2012

recent history

contrastive divergence learning in a crbm

fixed

j

fixed

j

< vihj >data

< vihj >recon

fixed

i

iter = 0 
(data)

fixed

i

iter =1 

(reconstruction)

    when updating visible and hidden units, we implement directed connections 

by treating data from previous time steps as a dynamically changing bias

    id136 and learning do not change

13 jul 2012 /
learning representations of sequences / g taylor 

24

thursday, july 12, 2012

stacking: the conditional deep belief network

13 jul 2012 /
learning representations of sequences / g taylor 

25

thursday, july 12, 2012

stacking: the conditional deep belief network

    learn a crbm

hidden layer

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

25

thursday, july 12, 2012

stacking: the conditional deep belief network

    learn a crbm

    now, treat the sequence of hidden units as    fully 

observed    data and train a second crbm

hidden layer

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

25

thursday, july 12, 2012

stacking: the conditional deep belief network

    learn a crbm

    now, treat the sequence of hidden units as    fully 

observed    data and train a second crbm

    the composition of crbms is a conditional deep 

belief net

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

25

thursday, july 12, 2012

stacking: the conditional deep belief network

    learn a crbm

    now, treat the sequence of hidden units as    fully 

observed    data and train a second crbm

    the composition of crbms is a conditional deep 

belief net

    it can be    ne-tuned generatively or discriminatively

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

25

thursday, july 12, 2012

motion synthesis with a 2-layer cdbn

l

h1
t

0

h0

t   2

h0

t   1

    model is trained on ~8000 frames 

of 60fps data (49 dimensions)

    10 styles of walking: cat, chicken, 
dinosaur, drunk, gangly, graceful, 
normal, old-man, sexy and strong
    600 binary hidden units per layer
    < 1 hour training on a modern 

workstation

13 jul 2012 /
learning representations of sequences / g taylor 

26

thursday, july 12, 2012

motion synthesis with a 2-layer cdbn

l

h1
t

0

h0

t   2

h0

t   1

    model is trained on ~8000 frames 

of 60fps data (49 dimensions)

    10 styles of walking: cat, chicken, 
dinosaur, drunk, gangly, graceful, 
normal, old-man, sexy and strong
    600 binary hidden units per layer
    < 1 hour training on a modern 

workstation

13 jul 2012 /
learning representations of sequences / g taylor 

26

thursday, july 12, 2012

modeling context

13 jul 2012 /
learning representations of sequences / g taylor 

27

thursday, july 12, 2012

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

modeling context

    a single model was trained on 10    styled    

walks from cmu subject 137

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

27

thursday, july 12, 2012

modeling context

    a single model was trained on 10    styled    

walks from cmu subject 137

    the model can generate each style based 

on initialization

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

27

thursday, july 12, 2012

modeling context

    a single model was trained on 10    styled    

walks from cmu subject 137

    the model can generate each style based 

on initialization

    we cannot prevent nor control 

transitioning

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

27

thursday, july 12, 2012

modeling context

    a single model was trained on 10    styled    

walks from cmu subject 137

    the model can generate each style based 

on initialization

    we cannot prevent nor control 

transitioning

    how to blend styles?

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

27

thursday, july 12, 2012

modeling context

    a single model was trained on 10    styled    

walks from cmu subject 137

    the model can generate each style based 

on initialization

    we cannot prevent nor control 

transitioning

    how to blend styles?
    style or person labels can be provided as 

part of the input to the top layer

labels

m

l

h1
t

hidden layer

h0

t   2

h0

t   1

hidden layer

0

visible layer

13 jul 2012 /
learning representations of sequences / g taylor 

27

thursday, july 12, 2012

how to make context influence dynamics?

!

!

18 may 2012 /
learning representations of sequences / g taylor 

28

thursday, july 12, 2012

!!"#$!!!!!!!!!!"#%!!!!!!!!!!"!

multiplicative interactions

    let latent variables act like gates, that dynamically 

change the connections between other variables

    this amounts to letting variables multiply 

connections between other variables: three-way 
multiplicative interactions

zk

    recently used in the context of learning 

correspondence between images (memisevic & 
hinton 2007, 2010) but long history before that

hj

vi

13 jul 2012 /
learning representations of sequences / g taylor 

29

thursday, july 12, 2012

multiplicative interactions

    let latent variables act like gates, that dynamically 

change the connections between other variables

    this amounts to letting variables multiply 

connections between other variables: three-way 
multiplicative interactions

zk

    recently used in the context of learning 

correspondence between images (memisevic & 
hinton 2007, 2010) but long history before that

hj

vi

roland memisevic has a nice tutorial and review paper on the subject: 
http://www.cs.toronto.edu/~rfm/multiview-feature-learning-cvpr/

13 jul 2012 /
learning representations of sequences / g taylor 

29

thursday, july 12, 2012

gated restricted id82s (grbm)
two views: memisevic & hinton (2007)

hj

latent variables

hj

zk

vi

zk

vi

output

input

output

input

13 jul 2012 /
learning representations of sequences / g taylor 

30

thursday, july 12, 2012

inferring optical flow: image    analogies   

    toy images (memisevic & hinton 2006)
    no structure in these images, only how 

they change

    can infer optical    ow from a pair of 

images and apply it to a random image

13 jul 2012 /
learning representations of sequences / g taylor 

31

input

figure 2: columns (left to right): input images; output images; inferred    ow   elds;
random target images; inferred transformation applied to target images. for the trans-
formations (last column) gray values represent the id203 that a pixel is    on    ac-
cording to the model, ranging from black for 0 to white for 1.

output

new input

apply trans

inferred
flow    eld

thursday, july 12, 2012

8

back to motion style

    introduce a set of latent    context    variables 

whose value is known at training time

    in our example, these represent    motion style    

but could also represent height, weight, gender, 
etc.

zk

    the contextual variables gate every existing 

pairwise connection in our model

hj

vi

13 jul 2012 /
learning representations of sequences / g taylor 

32

thursday, july 12, 2012

learning and id136

    learning and id136 remain almost the same 

as in the standard crbm

    we can think of the context or style variables as 

   blending in    a whole    sub-network   

    this allows us to share parameters across 

styles but selectively adapt dynamics

zk

hj

vi

13 jul 2012 /
learning representations of sequences / g taylor 

33

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

13 jul 2012 /
learning representations of sequences / g taylor 

34

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

34

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

j

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

34

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

34

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

34

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

34

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

34

thursday, july 12, 2012

overparameterization

    note: weight matrix           has been replaced by 

w v,h

a tensor              ! (likewise for other weights)

w v,h,z

    the number of parameters is             - per 

o(n 3)

group of weights         

    more, if we want sparse, overcomplete hiddens 

    however, there is a simple yet powerful solution!

13 jul 2012 /
learning representations of sequences / g taylor 

35

thursday, july 12, 2012

hidden layer

style

j

l

features

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

factoring

w z
lf

w vh
ijl

w h
jf

w v
if

hidden layer

j

style features

l

    
w vh
ijl

output layer
(e.g. data at time t)

w vh

ijl =   f

w v

if w h

jf w z
lf

13 jul 2012 /
learning representations of sequences / g taylor 

36

(figure adapted from roland memisevic)

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

j

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

hidden layer

style

j

l

features

    

    

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

supervised modeling of style

(taylor, hinton and roweis icml 2009, jmlr 2011)

hidden layer

j

    

hidden layer

style

j

l

features

    

factors

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

input layer
(e.g. data at time t-1:t-n)

output layer
(e.g. data at time t)

13 jul 2012 /
learning representations of sequences / g taylor 

37

thursday, july 12, 2012

parameter sharing

   

   

   

   

   

   

   

   

13 jul 2012 /
learning representations of sequences / g taylor 

38

thursday, july 12, 2012

motion synthesis: 
factored 3rd-order crbm

j

l

    

    same 10-styles dataset
    600 binary hidden units
    3  200 deterministic factors
    100 real-valued style features
    < 1 hour training on a modern 

workstation

    synthesis is real-time

13 jul 2012 /
learning representations of sequences / g taylor 

39

thursday, july 12, 2012

summary

motion synthesis: 
factored 3rd-order crbm

j

l

    

    same 10-styles dataset
    600 binary hidden units
    3  200 deterministic factors
    100 real-valued style features
    < 1 hour training on a modern 

workstation

    synthesis is real-time

13 jul 2012 /
learning representations of sequences / g taylor 

39

thursday, july 12, 2012

summary

)
e
c
a
p
s
 

a

t

a
d

 

d
e
z

i
l

a
m
r
o
n
(
 
r
o
r
r
e

 

n
o

i

i
t
c
d
e
r
p
s
m
r

 

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

 
0

 

crbm   600 (201,795)
fcrbm   600   60 (110,445)
style   fcrbm   600   60 (110,795)
unfac   style   crbm   100 (218,445)
autoregressive   6 (12,195)

5

10

15

20

predict   ahead interval

quantitative evaluation

    not computationally tractable to 

compute likelihoods

    annealed importance sampling will not 

work in conditional models (open 
problem)

    can evaluate predictive power (even 

though it has been trained generatively)

    can also evaluate in denoising tasks

13 jul 2012 /
learning representations of sequences / g taylor 

40

thursday, july 12, 2012

3d convnets for activity recognition
shuiwang ji, wei xu, ming yang, and kai yu (icml 2010)
    one approach: treat video frames as still images (lecun et al. 2005)
    alternatively, perform 3d convolution so that discriminative features across 

space and time are captured

3d convolutional neural networks for human action recognition
3d convolutional neural networks for human action recognition

3d convolutional neural networks for human action recognition

(a) 2d convolution 
(a) 2d convolution 

(a) 2d convolution 

 
 l

 a
 
 r
 l
 a
 o
 r
p
 o
 
 m
p
 
 e
 m
 e

t

t

 
 l

 a
 r
 o
p
 
 m
 e

t

 
 l

 a
 r
 o
p
 
 m
 e

t

figure 2. extraction of multiple features from contiguous
figure 2. extraction of multiple features from contiguous
frames. multiple 3d convolutions can be applied to con-
frames. multiple 3d convolutions can be applied to con-
tiguous frames to extract multiple features. as in figure 1,
tiguous frames to extract multiple features. as in figure 1,
the sets of connections are color-coded so that the shared
the sets of connections are color-coded so that the shared
weights are in the same color. note that all the 6 sets of
weights are in the same color. note that all the 6 sets of
connections do not share weights, resulting in two di   erent
connections do not share weights, resulting in two di   erent
feature maps on the right.
feature maps on the right.

figure 2. extraction of multiple features from contiguous
frames. multiple 3d convolutions can be applied to con-
tiguous frames to extract multiple features. as in figure 1,
the sets of connections are color-coded so that the shared
weights are in the same color. note that all the 6 sets of
connections do not share weights, resulting in two di   erent
feature maps on the right.

multiple convolutions applied to contiguous frames 
to extract multiple features

 
 l

 a
 
 l
 r
 a
 o
 r
p
 o
 
 m
p
 
 e
 m
 e

t

t

(b) 3d convolution 
(b) 3d convolution 
41

13 jul 2012 /
figure 1. comparison of 2d (a) and 3d (b) convolutions.
(b) 3d convolution 
figure 1. comparison of 2d (a) and 3d (b) convolutions.
in (b) the size of the convolution kernel in the temporal
learning representations of sequences / g taylor 
in (b) the size of the convolution kernel in the temporal
dimension is 3, and the sets of connections are color-coded
dimension is 3, and the sets of connections are color-coded
so that the shared weights are in the same color. in 3d
so that the shared weights are in the same color. in 3d
convolution, the same 3d kernel is applied to overlapping
convolution, the same 3d kernel is applied to overlapping
3d cubes in the input video to extract motion features.
3d cubes in the input video to extract motion features.

figure 1. comparison of 2d (a) and 3d (b) convolutions.
in (b) the size of the convolution kernel in the temporal
dimension is 3, and the sets of connections are color-coded
so that the shared weights are in the same color. in 3d
convolution, the same 3d kernel is applied to overlapping
3d cubes in the input video to extract motion features.

previous layer, thereby capturing motion information.
previous layer, thereby capturing motion information.
formally, the value at position (x, y, z) on the jth fea-

images from ji et al. 2010

set of lower-level feature maps. similar to the case
set of lower-level feature maps. similar to the case
of 2d convolution, this can be achieved by applying
of 2d convolution, this can be achieved by applying
multiple 3d convolutions with distinct kernels to the
multiple 3d convolutions with distinct kernels to the
same location in the previous layer (figure 2).
same location in the previous layer (figure 2).

set of lower-level feature maps. similar to the case
of 2d convolution, this can be achieved by applying
multiple 3d convolutions with distinct kernels to the
same location in the previous layer (figure 2).

2.2. a 3d id98 architecture
2.2. a 3d id98 architecture
based on the 3d convolution described above, a variety
2.2. a 3d id98 architecture
based on the 3d convolution described above, a variety
of id98 architectures can be devised. in the following,
of id98 architectures can be devised. in the following,
we describe a 3d id98 architecture that we have devel-
we describe a 3d id98 architecture that we have devel-
oped for human action recognition on the trecvid
oped for human action recognition on the trecvid
data set. in this architecture shown in figure 3, we
data set. in this architecture shown in figure 3, we
consider 7 frames of size 60  40 centered on the current

based on the 3d convolution described above, a variety
of id98 architectures can be devised. in the following,
we describe a 3d id98 architecture that we have devel-
oped for human action recognition on the trecvid

thursday, july 12, 2012

3d id98 architecture

3d convolutional neural networks for human action recognition

hardwired 

7x7x3 3d 
convolution 

2x2 

subsampling 

7x6x3 3d 
convolution 

3x3 

subsampling 

7x4 

convolution 

full 

connnection 

input: 

7@60x40 

h1: 

33@60x40 

c2: 

23*2@54x34 

s3: 

23*2@27x17 

c6: 

128@1x1 

c4: 

13*6@21x12 

s5: 

13*6@7x4 

image from ji et al. 2010

figure 3. a 3d id98 architecture for human action recognition. this architecture consists of 1 hardwired layer, 3 convo-
lution layers, 2 subsampling layers, and 1 full connection layer. detailed descriptions are given in the text.

2 different 3d    lters 
applied to each of 5 
blocks independently

subsample 
spatially

3 different 3d    lters 
applied to each of 5 
channels in 2 blocks

two fully-
connected 
layers

action units

hardwired to extract: 
1)grayscale
2)grad-x
3)grad-y
4)   ow-x
5)   ow-y

we then apply 3d convolutions with a kernel size of
7    7    3 (7    7 in the spatial dimension and 3 in the
temporal dimension) on each of the 5 channels sepa-
rately. to increase the number of feature maps, two
sets of di   erent convolutions are applied at each loca-
tion, resulting in 2 sets of feature maps in the c2 layer
each consisting of 23 feature maps. this layer con-
tains 1,480 trainable parameters.
in the subsequent
subsampling layer s3, we apply 2    2 subsampling on
each of the feature maps in the c2 layer, which leads
to the same number of feature maps with reduced spa-
tial resolution. the number of trainable parameters in
this layer is 92. the next convolution layer c4 is ob-
tained by applying 3d convolution with a kernel size

13 jul 2012 /
learning representations of sequences / g taylor 

42

the 7 input frames have been converted into a 128d
feature vector capturing the motion information in the
input frames. the output layer consists of the same
number of units as the number of actions, and each
unit is fully connected to each of the 128 units in
the c6 layer.
in this design we essentially apply a
linear classi   er on the 128d feature vector for action
classi   cation. for an action recognition problem with
3 classes, the number of trainable parameters at the
output layer is 384. the total number of trainable
parameters in this 3d id98 model is 295,458, and all
of them are initialized randomly and trained by on-
line error back-propagation algorithm as described in
(lecun et al., 1998). we have designed and evalu-

thursday, july 12, 2012

3d convnet: discussion

    good performance on trecvid surveillance data (celltoear, objectput, 

pointing)

    good performance on kth actions (box, handwave, handclap, jog, run, 

walk)

    still a fair amount of engineering: person detection (trecvid), foreground 

extraction (kth), hard-coded    rst layer

3d convolutional neural networks for human action recognition

figure 4. sample human detection and tracking results from camera numbers 1, 2, 3, and 5, respectively from left to right.

image from ji et al. 2010

43

13 jul 2012 /
learning representations of sequences / g taylor 

against-all linear id166 is learned for each action class.
speci   cally, we extract dense sift descriptors (lowe,
2004) from raw gray images or motion edge history
images (mehi) (yang et al., 2009). local features on
raw gray images preserve the appearance information,
while mehi concerns with the shape and motion pat-
terns. these sift descriptors are calculated every 6
pixels from 7   7 and 16   16 local image patches in the
same cubes as in the 3d id98 model. then they are
softly quantized using a 512-word codebook to build

thursday, july 12, 2012

performed by 25 subjects. to follow the setup in the
hmax model, we use a 9-frame cube as input and ex-
tract foreground as in (jhuang et al., 2007). to reduce
the memory requirement, the resolutions of the input
frames are reduced to 80    60 in our experiments as
compared to 160    120 used in (jhuang et al., 2007).
we use a similar 3d id98 architecture as in figure
3 with the sizes of kernels and the number of feature
maps in each layer modi   ed to consider the 80    60    9
inputs.
in particular, the three convolutional layers

learning features for video understanding

    most work on unsupervised feature extraction 

has concentrated on static images

transformation 
feature maps

    we propose a model that extracts motion-

sensitive features from pairs of images

    existing attempts (e.g. memisevic & hinton 
2007, cadieu & olshausen 2009) ignore the 
pictorial structure of the input

    thus limited to modeling small image patches

13 jul 2012 /
learning representations of sequences / g taylor 

44

thursday, july 12, 2012

image pair

gated restricted id82s (grbm)
two views: memisevic & hinton (2007)

hj

latent variables

hj

zk

vi

zk

vi

output

input

output

input

13 jul 2012 /
learning representations of sequences / g taylor 

45

thursday, july 12, 2012

convolutional grbm
graham taylor, rob fergus, yann lecun, and chris bregler (eccv 2010)

    like the grbm, captures third-order interactions

    shares weights at all locations in an image

    as in a standard rbm, exact id136 is ef   cient

    pk
pooling
layer

np

pk
(cid:68)

nz

np

    zk
feature
layer

zk
m,n

nz

    id136 and reconstruction are performed 

x (input)

through convolution operations

nx

x
w

n

x
w

n

nx

y (output)

ny

y
w

n

y
w

n

ny

13 jul 2012 /
learning representations of sequences / g taylor 

46

thursday, july 12, 2012

more complex example of    analogies   

input

output

13 jul 2012 /
learning representations of sequences / g taylor 

47

thursday, july 12, 2012

more complex example of    analogies   

feature maps

input

output

13 jul 2012 /
learning representations of sequences / g taylor 

47

thursday, july 12, 2012

more complex example of    analogies   

feature maps

input

output

13 jul 2012 /
learning representations of sequences / g taylor 

47

thursday, july 12, 2012

more complex example of    analogies   

feature maps

?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

input

output

input

output

13 jul 2012 /
learning representations of sequences / g taylor 

47

thursday, july 12, 2012

more complex example of    analogies   

feature maps

ground

truth

?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?

input

output

input

output

novel input

transformation 

(model)

13 jul 2012 /
learning representations of sequences / g taylor 

47

thursday, july 12, 2012

human activity: kth actions dataset

k
z

)
 
 
 
 
 
(
 
e
r
u
t
a
e
f

    we learn 32 feature maps
    6 are shown here
    kth contains 25 subjects 

performing 6 actions under 4 
conditions 

    only preprocessing is local 

contrast id172
   motion sensitive features (1,3)
   edge features (4)
   segmentation operator (6)

time

13 jul 2012 /
learning representations of sequences / g taylor 

48

hand clapping (above); walking (below)

thursday, july 12, 2012

activity recognition: kth

prior art

acc
(%)

convolutional
architectures

hog3d+km+id166

85.3

convgrbm+3d-convnet+logistic reg.

hog/hof+km+id166

86.1

convgrbm+3d convnet+mlp

hog+km+id166

hof+km+id166

79.0

3d convnet+3d convnet+logistic reg.

88.0

3d convnet+3d convnet+mlp

acc.
(%)

88.9

90.0

79.4

79.5

    compared to methods that do not use explicit interest point detection
    state of the art: 92.1% (laptev et al. 2008) 93.9% (le et al. 2011)
    other reported result on 3d convnets uses a different evaluation scheme

13 jul 2012 /
learning representations of sequences / g taylor 

49

thursday, july 12, 2012

activity recognition: hollywood 2

    12 classes of human action extracted from 69 movies (20 hours)
    much more realistic and challenging than kth (changing scenes, zoom, etc.)
    performance is evaluated by mean average precision over classes

method

average prec.

prior art (wang et al. survey 2009):
prior art (wang et al. survey 2009):

hog3d+km+id166

hog/hof+km+id166

hog+km+id166

hof+km+id166

our method:
our method:

grbm+sc+id166

45.3

47.4

39.4

45.5

46.8

13 jul 2012 /
learning representations of sequences / g taylor 

50

thursday, july 12, 2012

summary

13 jul 2012 /
learning representations of sequences / g taylor 

51

thursday, july 12, 2012

summary

    learning distributed representations of 

sequences

13 jul 2012 /
learning representations of sequences / g taylor 

51

thursday, july 12, 2012

summary

    learning distributed representations of 

sequences

    for high-dimensional, multi-modal data: 

crbm, fcrbm

j

l

    

13 jul 2012 /
learning representations of sequences / g taylor 

51

thursday, july 12, 2012

summary

    learning distributed representations of 

sequences

    for high-dimensional, multi-modal data: 

crbm, fcrbm

j

l

    

    activity recognition: 2 methods

13 jul 2012 /
learning representations of sequences / g taylor 

51

x (input)

x
n
w

nx

x
w

n

nx

y (output)

ny

y
n
w

y
n
w

ny

np

pk
(cid:68)

np

nz

 pk

pooling
layer

nz

zk
m,n

 zk

feature
layer

thursday, july 12, 2012

the university of guelph is not in belgium!

montreal

new york

guelph

toronto

thursday, july 12, 2012

