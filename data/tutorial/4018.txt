some relevant topics in optimization

stephen wright

university of wisconsin-madison

ipam, july 2012

stephen wright (uw-madison)

optimization

ipam, july 2012

1 / 118

1

introduction/overview

2 gradient methods

3 stochastic gradient methods for convex minimization

4 sparse and regularized optimization

5 decomposition methods

6 augmented lagrangian methods and splitting

stephen wright (uw-madison)

optimization

ipam, july 2012

2 / 118

introduction

learning from data leads naturally to optimization formulations. typical
ingredients of a learning problem include

collection of    training    data, from which we want to learn to make
id136s about future data.

parametrized model, whose parameters can in principle be determined
from training data + prior knowledge.

objective that captures prediction errors on the training data and
deviation from prior knowledge or desirable structure.

other typical properties of learning problems are huge underlying data set,
and requirement for solutions with only low-medium accuracy.

formulation as an optimization problem can be di   cult and controversial.
however there are several important paradigms in which the issue is well
settled. (e.g. support vector machines, id28,
recommender systems.)

stephen wright (uw-madison)

optimization

ipam, july 2012

3 / 118

optimization formulations

there is a wide variety of optimization formulations for machine learning
problems. but several common issues and structures arise in many cases.

imposing structure. can include id173 functions in the
objective or constraints.

(cid:107)x(cid:107)1 to induce sparsity in the vector x;
nuclear norm (cid:107)x(cid:107)    (sum of singular values) to induce low rank in x .

objective: can be derived from bayesian statistics + maximum
likelihood criterion. can incorporate prior knowledge.

partially separable: f (x) =(cid:80)

objectives f have distinctive properties in several applications:

e   e fe(xe), where each xe is a subvector

of x, and each term fe corresponds to a single item of data.
sometimes possible to compute subvectors of the gradient    f at
proportionately lower cost than the full gradient.

these two properties are often combined: in partially separable f ,
subvector xe is often small.

stephen wright (uw-madison)

optimization

ipam, july 2012

4 / 118

examples: partially separable structure

1. id166 with hinge loss:

f (w ) = c

n(cid:88)

i=1

max(1     yi (w t xi ), 0) +

(cid:107)w(cid:107)2,

1
2

2. matrix completion. given k    n marix m with entries (u, v )     e

where variable vector w contains feature weights, xi are feature
vectors, yi =   1 are labels, and c > 0 is a parameter.
speci   ed, seek l (k    r ) and r (n    r ) such that m     lr t .
f +   v(cid:107)rv  (cid:107)2

v       muv )2 +   u(cid:107)lu  (cid:107)2

(cid:88)

(lu  r t

(cid:110)

(cid:111)

f

.

min
l,r

(u,v )   e

stephen wright (uw-madison)

optimization

ipam, july 2012

5 / 118

examples: partially separable structure

3. regularized id28 (2 classes):

n(cid:88)

i=1

f (w ) =     1
n

log(1 + exp(yi w t xi )) +   (cid:107)w(cid:107)1.

4. id28 (m classes): yij = 1 if data point i is in class j;

yij = 0 otherwise. w[j] is the subvector of w for class j.

n(cid:88)

       m(cid:88)

m(cid:88)
[j]xi )     log(

yij (w t

i=1

j=1

j=1

f (w ) =     1
n

       +

m(cid:88)

j=1

(cid:107)w[j](cid:107)2
2.

exp(w t

[j]xi ))

stephen wright (uw-madison)

optimization

ipam, july 2012

6 / 118

examples:    partial gradient    structure

1. dual, nonlinear id166:

min

  

1
2

  t k        1t    s.t. 0            c 1, y t    = 0,

where kij = yi yj k(xi , xj ), with k(  ,  ) a id81. subvectors of
the gradient k        1 can be updated and maintained economically.

2. id28 (again): gradient of log-likelihood function is
x t u, where ui =    yi (1 + exp(yi w t xi )), i = 1, 2, . . . , n.

1
n

if w is sparse, it may be cheap to evaluate u, which is dense. then,
evaluation of partial gradient [   f (x)]g may be cheap.

partitioning of x may also arise naturally from problem structure, parallel
implementation, or administrative reasons (e.g. decentralized control).

(block) coordinate descent methods that exploit this property have been
successful. (more tomorrow.)

stephen wright (uw-madison)

optimization

ipam, july 2012

7 / 118

batch vs incremental

considering the partially separable form

(cid:88)

e   e

f (x) =

fe(xe),

the size |e| of the training set can be very large. practical considerations,
and di   ering requirements for solution accuracy lead to a fundamental
divide in algorithmic strategy.

incremental: select a single e at random, evaluate    fe(xe), and
take a step in this direction. (note that e (   fe(xe)) = |e|   1   f (x).)
stochastic approximation (sa).
batch: select a subset of data   e     e , and minimize the function

e      e fe(xe). sample-average approximation (saa).

  f (x) =(cid:80)

minibatch is a kind of compromise: aggregate the e into small groups,
consisting of 10 or 100 individual terms, and apply incremental algorithms
to the rede   ned summation. (gives lower-variance gradient estimates.)

stephen wright (uw-madison)

optimization

ipam, july 2012

8 / 118

background: optimization and machine learning

a long history of connections. examples:

back-propagation for neural networks was recognized in the 80s or
earlier as an incremental gradient method.

support vector machine formulated as a linear and quadratic program
in the late 1980s. duality allowed formulation of nonlinear id166 as a
convex qp. from late 1990s, many specialized optimization methods
were applied: interior-point, coordinate descent / decomposition,
cutting-plane, stochastic gradient.

stochastic gradient. originally robbins-munro (1951). optimizers in
russia developed algorithms from 1980 onwards. rediscovered by
machine learning community around 2004 (bottou, lecun). parallel
and independent work in ml and optimization communities until
2009. intense research continues.

connections are now stronger than ever, with much collaborative and
crossover activity.

stephen wright (uw-madison)

optimization

ipam, july 2012

9 / 118

gradient methods

min f (x), with smooth convex f . usually assume

  i (cid:22)    2f (x) (cid:22) li

for all x,

with 0            l. (l is thus a lipschitz constant on the gradient    f .)
   > 0     strongly convex. have

f (y )     f (x)        f (x)t (y     x)     1
2

  (cid:107)y     x(cid:107)2.
(mostly assume (cid:107)    (cid:107) := (cid:107)    (cid:107)2.) de   ne conditioning    := l/  .
sometimes discuss convex quadratic f :

f (x) =

1
2

x t ax, where   i (cid:22) a (cid:22) li .

stephen wright (uw-madison)

optimization

ipam, july 2012

10 / 118

what   s the setup?

assume in this part of talk that we can evaluate f and    f at each iterate
xi . but we are interested in extending to broader class of problems:

nonsmooth f ;

f not available;

only an estimate of the gradient (or subgradient) is available;
impose a constraint x         for some simple set     (e.g. ball, box,
simplex);

a nonsmooth id173 term may be added to the objective f .

focus on algorithms that can be adapted to these circumstances.

stephen wright (uw-madison)

optimization

ipam, july 2012

11 / 118

steepest descent

xk+1 = xk       k   f (xk ),

for some   k > 0.

di   erent ways to identify an appropriate   k .

1 hard: interpolating scheme with safeguarding to identify an

approximate minimizing   k .

2 easy: backtracking.     , 1

2     , 1

4     , 1

8     , ... until a su   cient decrease in

f is obtained.

3 trivial: don   t test for function decrease. use rules based on l and   .

traditional analysis for 1 and 2: usually yields global convergence at
unspeci   ed rate. the    greedy    strategy of getting good decrease from the
current search direction is appealing, and may lead to better practical
results.

analysis for 3: focuses on convergence rate, and leads to accelerated
multistep methods.

stephen wright (uw-madison)

optimization

ipam, july 2012

12 / 118

line search

works for nonconvex f also.

seek   k that satis   es wolfe conditions:    su   cient decrease    in f :

f (xk       k   f (xk ))     f (xk )     c1  k(cid:107)   f (xk )(cid:107)2,

(0 < c1 (cid:28) 1)

while not being too small (signi   cant increase in the directional derivative):

   f (xk+1)t   f (xk )        c2(cid:107)   f (xk )(cid:107)2,

(c1 < c2 < 1).

can show that for convex f , accumulation points   x of {xk} are stationary:
   f (  x) = 0. (optimal, when f is convex.)
can do a one-dimensional line search for   k , taking minima of quadratic
or cubics that interpolate the function and gradient information at the last
two values tried. use brackets to ensure steady convergence. often    nd a
suitable    within 3 attempts.

(see e.g. ch. 3 of nocedal & wright, 2006)

stephen wright (uw-madison)

optimization

ipam, july 2012

13 / 118

backtracking

try   k =     ,     /2,     /4,     /8, ... until the su   cient decrease condition is
satis   ed.

(no need to check the second wolfe condition, as the value of   k thus
identi   ed is    within striking distance    of a value that   s too large     so it is
not too short.)

these methods are widely used in many applications, but they don   t work
on nonsmooth problems when subgradients replace gradients, or when f is
not available.

stephen wright (uw-madison)

optimization

ipam, july 2012

14 / 118

constant (short) steplength

(cid:16)

(cid:17)(cid:107)   f (xk )(cid:107)2

2.

by elementary use of taylor   s theorem, obtain
1       k
2

f (xk+1)     f (xk )       k

l

for   k     1/l, have

thus

f (xk+1)     f (xk )     1
2l

(cid:107)   f (xk )(cid:107)2
2.

(cid:107)   f (xk )(cid:107)2     2l[f (xk )     f (xk+1)].

by summing from k = 0 to k = n, and telescoping the sum, we have

n(cid:88)

(cid:107)   f (xk )(cid:107)2     2l[f (x0)     f (xn+1)].

k=1

(it follows that    f (xk )     0 if f is bounded below.)

stephen wright (uw-madison)

optimization

ipam, july 2012

15 / 118

rate analysis

another elementary use of taylor   s theorem shows that

(cid:107)xk+1     x   (cid:107)2     (cid:107)xk     x   (cid:107)2       k (2/l       k )(cid:107)   f (xk )(cid:107)2,

so that {(cid:107)xk     x   (cid:107)} is decreasing.
de   ne for convenience:    k := f (xk )     f (x   ).
by convexity, have
   k        f (xk )t (xk     x   )     (cid:107)   f (xk )(cid:107)(cid:107)xk     x   (cid:107)     (cid:107)   f (xk )(cid:107)(cid:107)x0     x   (cid:107).
from previous page (subtracting f (x   ) from both sides of the inequality),
and using the inequality above, we have

   k+1        k     (1/2l)(cid:107)   f (xk )(cid:107)2        k    

1

2l(cid:107)x0     x   (cid:107)2    2
k .

stephen wright (uw-madison)

optimization

ipam, july 2012

16 / 118

weakly convex: 1/k sublinear; strongly convex: linear

take reciprocal of both sides and manipulate (using (1      )   1     1 +  ):

1

   k+1

    1
   k

+

1

2l(cid:107)x0     x   (cid:107)2     1

   0

+

k + 1

2l(cid:107)x0     x   (cid:107)2 ,

which yields

f (xk+1)     f (x   )     2l(cid:107)x0     x(cid:107)2

k + 1

.

the classic 1/k convergence rate!
by assuming    > 0, can set   k     2/(   + l) and get a linear (geometric)
rate: much better than sublinear, in the long run

(cid:18) l       

l +   

(cid:19)2k (cid:107)x0     x   (cid:107)2 =

(cid:18)

1     2

   + 1

(cid:19)2k (cid:107)x0     x   (cid:107)2.

(cid:107)xk     x   (cid:107)2    

stephen wright (uw-madison)

optimization

ipam, july 2012

17 / 118

since by taylor   s theorem we have

   k = f (xk )     f (x   )     (l/2)(cid:107)xk     x   (cid:107)2,

it follows immediately that

f (xk )     f (x   )     l
2

(cid:18)

1     2

   + 1

(cid:19)2k (cid:107)x0     x   (cid:107)2.

note: a geometric / linear rate is generally much better than any
sublinear (1/k or 1/k 2) rate.

stephen wright (uw-madison)

optimization

ipam, july 2012

18 / 118

the 1/k 2 speed limit

nesterov (2004) gives a simple example of a smooth function for which no
method that generates iterates of the form xk+1 = xk       k   f (xk ) can
converge at a rate faster than 1/k 2, at least for its    rst n/2 iterations.
note that xk+1     x0 + span(   f (x0),   f (x1), . . . ,   f (xk )).

                     

a =

2    1
   1
0    1

0
0
2    1
0
2    1
. . .
. . .

0

. . .

. . . 0
. . . 0
. . . 0

. . .
. . .
0
. . .
0    1 2

                      ,

                     

                     

1
0
0
...
0

e1 =

and set f (x) = (1/2)x t ax     et
1 x. the solution has x   (i) = 1     i/(n + 1).
if we start at x0 = 0, each    f (xk ) has nonzeros only in its    rst k entries.
hence, xk+1(i) = 0 for i = k + 1, k + 2, . . . , n. can show

f (xk )     f         3l(cid:107)x0     x   (cid:107)2
32(k + 1)2 .

stephen wright (uw-madison)

optimization

ipam, july 2012

19 / 118

exact minimizing   k: faster rate?
take   k to be the exact minimizer of f along       f (xk ). does this yield a
better rate of linear convergence?
consider the convex quadratic f (x) = (1/2)x t ax. (thus x    = 0 and
f (x   ) = 0.) here    is the condition number of a.
we have    f (xk ) = axk . exact minimizing   k :

  k =

x t
k a2xk
x t
k a3xk

which is in the interval

(cid:104) 1

(cid:105)

l , 1

  

. thus

= arg min

  

(xk       axk )t a(xk       axk ),

1
2

f (xk+1)     f (xk )     1
2

(x t

k a2xk )2

(x t

k axk )(x t

k a3xk )

,

so, de   ning zk := axk , we have
f (xk+1)     f (x   )
f (xk )     f (x   )

    1    

(cid:107)zk(cid:107)4
k a   1zk )(z t
(z t

k azk )

.

stephen wright (uw-madison)

optimization

ipam, july 2012

20 / 118

use kantorovich inequality:

(z t az)(z t a   1z)     (l +   )2

(cid:107)z(cid:107)4.

thus

and so

f (xk+1)     f (x   )
f (xk )     f (x   )

(cid:18)

    1     4l  

(l +   )2 =

1     2

   + 1

f (xk )     f (x   )    

1     2

   + 1

[f (x0)     f (x   )].

4l  

(cid:18)

(cid:19)2k

(cid:19)2

,

no improvement in the linear rate over constant steplength.

stephen wright (uw-madison)

optimization

ipam, july 2012

21 / 118

the slow linear rate is typical!

not just a pessimistic bound!

stephen wright (uw-madison)

optimization

ipam, july 2012

22 / 118

multistep methods: heavy-ball

enhance the search direction by including a contribution from the previous
step.

consider    rst constant step lengths:

xk+1 = xk          f (xk ) +   (xk     xk   1)

analyze by de   ning a composite iterate vector:

wk :=

(cid:21)

xk   1     x   

(cid:20) xk     x   
(cid:20)        2f (x   ) + (1 +   )i      i

.

i

0

b :=

(cid:21)

.

thus

wk+1 = bwk + o((cid:107)wk(cid:107)),

stephen wright (uw-madison)

optimization

ipam, july 2012

23 / 118

b has same eigenvalues as

(cid:20)        + (1 +   )i      i

i

0

(cid:21)

,

   = diag(  1,   2, . . . ,   n),

where   i are the eigenvalues of    2f (x   ). choose   ,    to explicitly
minimize the max eigenvalue of b, obtain

   =

4
l

1
(1 + 1/

   

  )2 ,

   =

1    

2   
   + 1

leads to linear convergence for (cid:107)xk     x   (cid:107) with rate approximately

(cid:19)2

.

(cid:18)
(cid:19)

(cid:18)

1    

2   
   + 1

.

stephen wright (uw-madison)

optimization

ipam, july 2012

24 / 118

summary: linear convergence, strictly convex f

steepest descent: linear rate approx (1     2/  );
   
heavy-ball: linear rate approx (1     2/

  ).

big di   erence! to reduce (cid:107)xk     x   (cid:107) by a factor  , need k large enough that

(cid:19)k           k       
(cid:19)k           k    

(cid:18)

(cid:18)

1     2
  
1     2   
  
   

| log  |
2
   

| log  |

  
2

(steepest descent)

(heavy-ball)

a factor of

   di   erence. e.g. if    = 100, need 10 times fewer steps.

stephen wright (uw-madison)

optimization

ipam, july 2012

25 / 118

conjugate gradient

basic step is

xk+1 = xk +   k pk ,

pk =       f (xk ) +   k pk   1.

we can identify it with heavy-ball by setting   k =   k   k /  k   1. however,
cg can be implemented in a way that doesn   t require knowledge (or
estimation) of l and   .

choose   k to (approximately) miminize f along pk ;
choose   k by a variety of formulae (fletcher-reeves, polak-ribiere,
etc), all of which are equivalent if f is convex quadratic. e.g.

  k = (cid:107)   f (xk )(cid:107)2/(cid:107)   f (xk   1)(cid:107)2.

stephen wright (uw-madison)

optimization

ipam, july 2012

26 / 118

cg, cont   d.

nonlinear cg: variants include fletcher-reeves, polak-ribiere, hestenes.
restarting periodically with pk =       f (xk ) is a useful feature (e.g. every n
iterations, or when pk is not a descent direction).

for f quadratic, convergence analysis is based on eigenvalues of a and
chebyshev polynomials, min-max arguments. get

finite termination in as many iterations as there are distinct
eigenvalues;
   
asymptotic linear convergence with rate approx 1     2/
heavy-ball.)

  . (like

see e.g. chap. 5 of nocedal & wright (2006) and refs therein.

stephen wright (uw-madison)

optimization

ipam, july 2012

27 / 118

accelerated first-order methods

accelerate the rate to 1/k 2 for weakly convex, while retaining the linear
rate (related to

  ) for strongly convex case.

   

nesterov (1983, 2004) describes a method that requires   .
0: choose x0,   0     (0, 1); set y0     x0./
k: xk+1     yk     1

l   f (yk ); (*short-step gradient*)
k+1 = (1       k+1)  2
solve for   k+1     (0, 1):   2
set   k =   k (1       k )/(  2
k +   k+1);
set yk+1     xk+1 +   k (xk+1     xk ).
still works for weakly convex (   =    ).

k +   k+1/  ;

stephen wright (uw-madison)

optimization

ipam, july 2012

28 / 118

stephen wright (uw-madison)

optimization

ipam, july 2012

29 / 118

kxk+1xkyk+1xk+2yk+2yconvergence results: nesterov

   
if   0     1/

  , have

f (xk )     f (x   )     c1 min

(cid:32)(cid:18)

1     1   
  

(cid:19)k

(cid:33)

,

,

   
(

4l

l + c2k)2

where constants c1 and c2 depend on x0,   0, l.

linear convergence at    heavy-ball    rate in strongly convex case, otherwise
1/k 2.

   
in the special case of   0 = 1/
  k     1   
  

,

  , this scheme yields

  k     1    

2   
   + 1

.

stephen wright (uw-madison)

optimization

ipam, july 2012

30 / 118

fista

(beck & teboulle 2007). similar to the above, but with a fairly short and
elementary analysis (though still not very intuitive).

(cid:16)

0: choose x0; set y1 = x0, t1 = 1;
k: xk     yk     1

(cid:113)
l   f (yk );
tk+1     1
yk+1     xk + tk   1

1 + 4t2
k
(xk     xk   1).

(cid:17)

1 +

2

;

tk+1

for (weakly) convex f , converges with f (xk )     f (x   )     1/k 2.
when l is not known, increase an estimate of l until it   s big enough.

beck & teboulle (2010) does the convergence analysis in 2-3 pages:
elementary, technical.

stephen wright (uw-madison)

optimization

ipam, july 2012

31 / 118

a non-monotone gradient method: barzilai-borwein

(barzilai & borwein 1988) bb is a gradient method, but with an unusual
choice of   k . allows f to increase (sometimes dramatically) on some steps.

xk+1 = xk       k   f (xk ),

  k := arg min
  

(cid:107)sk       zk(cid:107)2,

where

sk := xk     xk   1,

zk :=    f (xk )        f (xk   1).

explicitly, we have

  k =

s t
k zk
z t
k zk

.

note that for convex quadratic f = (1/2)x t ax, we have

  k =

s t
k ask
s t
k a2sk

    [l   1,      1].

hence, can view bb as a kind of quasi-id77, with the hessian
approximated by      1
k i .

stephen wright (uw-madison)

optimization

ipam, july 2012

32 / 118

comparison: bb vs greedy steepest descent

stephen wright (uw-madison)

optimization

ipam, july 2012

33 / 118

many bb variants

k sk /s t

k zk /z t

k zk in place of   k = s t

can use   k = s t
alternate between these two formulae;
calculate   k as above and hold it constant for 2, 3, or 5 successive
steps;
take   k to be the exact steepest descent step from the previous
iteration.

k zk ;

nonmonotonicity appears essential to performance. some variants get
global convergence by requiring a su   cient decrease in f over the worst of
the last 10 iterates.

the original 1988 analysis in bb   s paper is nonstandard and illuminating
(just for a 2-variable quadratic).

in fact, most analyses of bb and related methods are nonstandard, and
consider only special cases. the precursor of such analyses is akaike
(1959). more recently, see ascher, dai, fletcher, hager and others.

stephen wright (uw-madison)

optimization

ipam, july 2012

34 / 118

primal-dual averaging

(see nesterov 2009) basic step:

k(cid:88)

i=0

     
k

xk+1 = arg min

x

1

k + 1

[f (xi ) +    f (xi )t (x     xi )] +

(cid:107)x     x0(cid:107)2

     
k

  g t
k x +

(cid:107)x     x0(cid:107)2,

= arg min

where   gk :=(cid:80)k

x

i=0    f (xi )/(k + 1)     the averaged gradient.

the last term is always centered at the    rst iterate x0.
gradient information is averaged over all steps, with equal weights.

   is constant - results can be sensitive to this value.
the approach still works for convex nondi   erentiable f , where    f (xi )
is replaced by a vector from the subgradient    f (xi ).

stephen wright (uw-madison)

optimization

ipam, july 2012

35 / 118

convergence properties

nesterov proves convergence for averaged iterates:

k(cid:88)

  xk+1 =

1

xi .

k + 1

i=0

provided the iterates and the solution x    lie within some ball of radius d
around x0, we have

f (  xk+1)     f (x   )     c   
k

,

where c depends on d, a uniform bound on (cid:107)   f (x)(cid:107), and    (coe   cient
of stabilizing term).
note: there   s averaging in both primal (xi ) and dual (   f (xi )) spaces.
generalizes easily and robustly to the case in which only estimated
gradients or subgradients are available.

(averaging smooths the errors in the individual gradient estimates.)

stephen wright (uw-madison)

optimization

ipam, july 2012

36 / 118

extending to the constrained case: x        

how do these methods change when we require x        , with     closed and
convex?

some algorithms and theory stay much the same, provided we can involve
    explicity in the subproblems.

example: primal-dual averaging for minx       f (x).

where   gk :=(cid:80)k

easy to solve.

xk+1 = arg min
x      

  g t
k x +

(cid:107)x     x0(cid:107)2,

     
k

i=0    f (xi )/(k + 1). when     is a box, this subproblem is

stephen wright (uw-madison)

optimization

ipam, july 2012

37 / 118

example: nesterov   s constant step scheme for minx       f (x). requires
just only calculation to be changed from the unconstrained version.
0: choose x0,   0     (0, 1); set y0     x0, q     1/   =   /l.
k: xk+1     arg miny      

l   f (yk )](cid:107)2
2;
solve for   k+1     (0, 1):   2
k+1 = (1       k+1)  2
k + q  k+1;
set   k =   k (1       k )/(  2
k +   k+1);
set yk+1     xk+1 +   k (xk+1     xk ).

2(cid:107)y     [yk     1

1

convergence theory is unchanged.

stephen wright (uw-madison)

optimization

ipam, july 2012

38 / 118

regularized optimization (more later)

fista can be applied with minimal changes to the regularized problem

min

x

f (x) +      (x),

where f is convex and smooth,    convex and    simple    but usually
nonsmooth, and    is a positive parameter.

simply replace the gradient step by

(cid:20)

(cid:13)(cid:13)(cid:13)(cid:13)x    

(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

xk = arg min

x

l
2

yk     1
l

   f (yk )

+      (x).

(this is the    shrinkage    step; when        0 or    = (cid:107)    (cid:107)1, can be solved
cheaply.)

more on this later.

stephen wright (uw-madison)

optimization

ipam, july 2012

39 / 118

further reading

1 y. nesterov, introductory lectures on id76: a basic course,

kluwer academic publishers, 2004.

2 a. beck and m. teboulle,    gradient-based methods with application to signal

recovery problems,    in press, 2010. (see teboulle   s web site).

3 b. t. polyak, introduction to optimization, optimization software inc, 1987.

4 j. barzilai and j. m. borwein,    two-point step size gradient methods,    ima

journal of numerical analysis, 8, pp. 141-148, 1988.

5 y. nesterov,    primal-dual subgradient methods for convex programs,   

mathematical programming, series b, 120, pp. 221-259, 2009.

6 j. nocedal and s. wright, numerical optimization, 2nd ed., springer, 2006.

stephen wright (uw-madison)

optimization

ipam, july 2012

40 / 118

stochastic gradient methods

still deal with (weakly or strongly) convex f . but change the rules:

allow f nonsmooth.

can   t get function values f (x).

at any feasible x, have access only to an unbiased estimate of an
element of the subgradient    f .

common settings are:

f (x) = e  f (x,   ),

where    is a random vector with distribution p over a set   . also the
special case:

m(cid:88)

f (x) =

fi (x),

i=1
where each fi is convex and nonsmooth.

stephen wright (uw-madison)

optimization

ipam, july 2012

41 / 118

applications

this setting is useful for machine learning formulations. given data
xi     rn and labels yi =   1, i = 1, 2, . . . , m,    nd w that minimizes

m(cid:88)

     (w ) +

(cid:96)(w ; xi , yi ),

i=1

where    is a regularizer,    > 0 is a parameter, and (cid:96) is a loss. for linear
classi   ers/regressors, have the speci   c form (cid:96)(w t xi , yi ).
example: id166 with hinge loss (cid:96)(w t xi , yi ) = max(1     yi (w t xi ), 0) and
   = (cid:107)    (cid:107)1 or    = (cid:107)    (cid:107)2
2.
example: id28: (cid:96)(w t xi , yi ) = log(1 + exp(yi w t xi )). in
regularized version may have   (w ) = (cid:107)w(cid:107)1.

stephen wright (uw-madison)

optimization

ipam, july 2012

42 / 118

subgradients

for each x in domain of f , g is a subgradient of f at x if

f (z)     f (x) + g t (z     x),

for all z     domf .

right-hand side is a supporting hyperplane.
the set of subgradients is called the subdi   erential, denoted by    f (x).
when f is di   erentiable at x, have    f (x) = {   f (x)}.
we have strong convexity with modulus    > 0 if

f (z)     f (x)+g t (z   x)+

1
2

  (cid:107)z   x(cid:107)2,

for all x, z     domf with g        f (x).

generalizes the assumption    2f (x) (cid:23)   i made earlier for smooth
functions.

stephen wright (uw-madison)

optimization

ipam, july 2012

43 / 118

stephen wright (uw-madison)

optimization

ipam, july 2012

44 / 118

xsupporting hyperplanesf   classical    stochastic approximation

denote by g (x,   ) ths subgradient estimate generated at x. for
unbiasedness need e  g (x,   )        f (x).
basic sa scheme: at iteration k, choose   k i.i.d. according to distribution
p, choose some   k > 0, and set

xk+1 = xk       k g (xk ,   k ).

note that xk+1 depends on all random variables up to iteration k, i.e.
  [k] := {  1,   2, . . . ,   k}.
when f is strongly convex, the analysis of convergence of e ((cid:107)xk     x   (cid:107)2) is
fairly elementary - see nemirovski et al (2009).

stephen wright (uw-madison)

optimization

ipam, july 2012

45 / 118

rate: 1/k

de   ne ak = 1
e ((cid:107)g (x,   )(cid:107)2)     m 2 for all x of interest. thus

2 e ((cid:107)xk     x   (cid:107)2). assume there is m > 0 such that

(cid:107)xk+1     x   (cid:107)2

2

1
2

=

(cid:107)xk       k g (xk ,   k )     x   (cid:107)2
(cid:107)xk     x   (cid:107)2
taking expectations, get

1
2
1
2

=

2       k (xk     x   )t g (xk ,   k ) +

k(cid:107)g (xk ,   k )(cid:107)2.
  2

1
2

ak+1     ak       k e [(xk     x   )t g (xk ,   k )] +

1
2

  2
k m 2.

for middle term, have

e [(xk     x   )t g (xk ,   k )] = e  [k   1]e  k [(xk     x   )t g (xk ,   k )|  [k   1]]

= e  [k   1](xk     x   )t gk ,

stephen wright (uw-madison)

optimization

ipam, july 2012

46 / 118

... where

gk := e  k [g (xk ,   k )|  [k   1]]        f (xk ).

by strong convexity, have

(xk     x   )t gk     f (xk )     f (x   ) +

  (cid:107)xk     x   (cid:107)2       (cid:107)xk     x   (cid:107)2.
hence by taking expectations, we get e [(xk     x   )t gk ]     2  ak . then,
substituting above, we obtain

1
2

ak+1     (1     2    k )ak +

1
2

  2
k m 2

when

a neat inductive argument (below) reveals the 1/k rate:

ak     q
2k

,

for q := max

(cid:107)x1     x   (cid:107)2,

m 2
  2

(cid:19)

.

  k     1
k  

,

(cid:18)

stephen wright (uw-madison)

optimization

ipam, july 2012

47 / 118

proof: clearly true for k = 1. otherwise:

ak+1     (1     2    k )ak +

(cid:19)
(cid:19) q

ak +

  2
k m 2

1
2
m 2
2k 2  2

+

q
2k 2

2k

q

2(k + 1)

(cid:18)
(cid:18)

   

   

=

=

   

1     2
k
1     2
k
(k     1)
2k 2 q
k 2     1
k 2
q

2(k + 1)

,

as claimed.

stephen wright (uw-madison)

optimization

ipam, july 2012

48 / 118

but... what if we don   t know   ? or if    = 0?

the choice   k = 1/(k  ) requires strong convexity, with knowledge of the
modulus   . an underestimate of    can greatly degrade the performance of
the method (see example in nemirovski et al. 2009).

   
now describe a robust stochastic approximation approach, which has a
k (in function value convergence), and works for weakly convex
rate 1/
nonsmooth functions and is not sensitive to choice of parameters in the
step length.

this is the approach that generalizes to mirror descent.

stephen wright (uw-madison)

optimization

ipam, july 2012

49 / 118

robust sa

at iteration k:

set xk+1 = xk       k g (xk ,   k ) as before;
set

(cid:80)k
(cid:80)k

  xk =

i=1   i xi
i=1   i

.

for any    > 0 (not critical), choose step lengths to be

  k =

   
  

.

k

m

then f (  xk ) converges to f (x   ) in expectation with rate approximately
(log k)/k 1/2. the choice of    is not critical.

stephen wright (uw-madison)

optimization

ipam, july 2012

50 / 118

analysis of robust sa

the analysis is again elementary. as above (using i instead of k), have:

  i e [(xi     x   )t gi ]     ai     ai+1 +

1
2

  2
i m 2.

by convexity of f , and gi        f (xi ):

f (x   )     f (xi ) + g t

i (x        xi ),

thus

  i e [f (xi )     f (x   )]     ai     ai+1 +

1
2

i m 2,
  2

so by summing iterates i = 1, 2, . . . , k, telescoping, and using ak+1 > 0:

k(cid:88)

i=1

  i e [f (xi )     f (x   )]     a1 +

m 2

1
2

  2
i .

k(cid:88)

i=1

stephen wright (uw-madison)

optimization

ipam, july 2012

51 / 118

thus dividing by(cid:80)
(cid:34)(cid:80)k
(cid:80)k

e

i=1   i :

i=1   i f (xi )

i=1   i

(cid:35)

    f (x   )

2 m 2(cid:80)k
(cid:80)k
    a1 + 1

i=1   i

i=1   2
i

.

by convexity, we have

f (  xk )    

so obtain the fundamental bound:

(cid:80)k
(cid:80)k

i=1   i f (xi )

,

i=1   i

2 m 2(cid:80)k
(cid:80)k
e [f (  xk )     f (x   )]     a1 + 1

i=1   i

i=1   2
i

.

stephen wright (uw-madison)

optimization

ipam, july 2012

52 / 118

   
by substituting   i =   
m

i

, we obtain

2   2(cid:80)k
(cid:80)k
e [f (  xk )     f (x   )]     a1 + 1
(cid:104) a1

  
m

  
m

i=1

k

    a1 +   2 log(k + 1)

   

1
i

i=1
1   

i

= m

+    log(k + 1)

  

(cid:105)

k   1/2.

that   s it!

other variants: constant stepsizes   k for a    xed    budget    of iterations;
periodic restarting; averaging just over the recent iterates. all can be
analyzed with the basic bound above.

stephen wright (uw-madison)

optimization

ipam, july 2012

53 / 118

constant step size

we can also get rates of approximately 1/k for the strongly convex case,
without performing iterate averaging and without requiring an accurate
estimate of   . the tricks are to (a) de   ne the desired threshold for ak in
advance and (b) use a constant step size
recall the bound from a few slides back, and set   k       :

ak+1     (1     2    )ak +

  2m 2.

1
2

de   ne the    limiting value          by

a    = (1     2    )a    +

  2m 2.

1
2

take the di   erence of the two expressions above:

(ak+1     a   )     (1     2    )(ak     a   )

from which it follows that {ak} decreases monotonically to a   , and

(ak     a   )     (1     2    )k (a0     a   ).

stephen wright (uw-madison)

optimization

ipam, july 2012

54 / 118

constant step size, continued

rearrange the expression for a    to obtain

a    =

  m 2
4  

.

from the previous slide, we thus have

ak     (1     2    )k (a0     a   ) + a   

    (1     2    )k a0 +

  m 2
4  

.

given threshold   > 0, we aim to    nd    and k such that ak       for all
k     k . we ensure that both terms on the right-hand side of the
expression above are less than  /2. the right values are:

   :=

2   
m 2 ,

k :=

m 2
4   2 log

(cid:16) a0

(cid:17)

2 

.

stephen wright (uw-madison)

optimization

ipam, july 2012

55 / 118

constant step size, continued

clearly the choice of    guarantees that the second term is less than  /2.

for the    rst term, we obtain k from an elementary argument:

(1     2    )k a0      /2

    k log(1     2    )         log(2a0/ )
   
k(   2    )         log(2a0/ )
   

k     1
2    

log(2a0/ ),

since log(1 + x)     x

from which the result follows, by substituting for    in the right-hand side.

if    is underestimated by a factor of   , we undervalue    by the same
factor, and k increases by 1/  . (easy modi   cation of the analysis above.)

underestimating    gives a mild performance penalty.

stephen wright (uw-madison)

optimization

ipam, july 2012

56 / 118

constant step size: summary

pro: avoid averaging, 1/k sublinear convergence, insensitive to
underestimates of   .

con: need to estimate probably unknown quantities: besides   , we need
m (to get   ) and a0 (to get k ).
we use constant size size in the parallel sg approach hogwild!, to be
described later.

stephen wright (uw-madison)

optimization

ipam, july 2012

57 / 118

mirror descent

the step from xk to xk+1 can be viewed as the solution of a subproblem:

xk+1 = arg min

z

g (xk ,   k )t (z     xk ) +

(cid:107)z     xk(cid:107)2
2,

1
2  k

a linear estimate of f plus a prox-term. this provides a route to handling
constrained problems, regularized problems, alternative prox-functions.
for the constrained problem minx       f (x), simply add the restriction z        
to the subproblem above. in some cases (e.g. when     is a box), the
subproblem is still easy to solve.
we may use other prox-functions in place of (1/2)(cid:107)z     x(cid:107)2
2 above. such
alternatives may be particularly well suited to particular constraint sets    .

mirror descent is the term used for such generalizations of the sa
approaches above.

stephen wright (uw-madison)

optimization

ipam, july 2012

58 / 118

mirror descent cont   d
given constraint set    , choose a norm (cid:107)    (cid:107) (not necessarily euclidean).
de   ne the distance-generating function    to be a strongly convex function
on     with modulus 1 with respect to (cid:107)    (cid:107), that is,

(  (cid:48)(x)       (cid:48)(z))t (x     z)     (cid:107)x     z(cid:107)2,

for all x, z        ,

where   (cid:48)(  ) denotes an element of the subdi   erential.
now de   ne the prox-function v (x, z) as follows:

v (x, z) =   (z)       (x)       (cid:48)(x)t (z     x).

this is also known as the bregman distance. we can use it in the
subproblem in place of 1

2(cid:107)    (cid:107)2:

xk+1 = arg min
z      

g (xk ,   k )t (z     xk ) +

1
  k

v (z, xk ).

stephen wright (uw-madison)

optimization

ipam, july 2012

59 / 118

bregman distance is the deviation from linearity:

stephen wright (uw-madison)

optimization

ipam, july 2012

60 / 118

  xzv(x,z)bregman distances: examples
for any    , we can use   (x) := (1/2)(cid:107)x       x(cid:107)2
v (x, z) = (1/2)(cid:107)x     z(cid:107)2
2.

for the simplex     = {x     rn : x     0, (cid:80)n
n(cid:88)

the 1-norm (cid:107)    (cid:107)1, choose    to be the id178 function

  (x) =

xi log xi ,

2, leading to prox-function

i=1 xi = 1}, we can use instead

leading to bregman distance

v (x, z) =

i=1

n(cid:88)

i=1

zi log(zi /xi ).

these are the two most useful cases.

convergence results for sa can be generalized to mirror descent.

stephen wright (uw-madison)

optimization

ipam, july 2012

61 / 118

incremental gradient

(see e.g. bertsekas (2011) and references therein.) finite sums:

m(cid:88)

f (x) =

fi (x).

step k typically requires choice of one index ik     {1, 2, . . . , m} and
evaluation of    fik (xk ). components ik are selected sometimes randomly or
cyclically. (latter option does not exist in the setting f (x) := e  f (x;   ).)

i=1

there are incremental versions of the heavy-ball method:
xk+1 = xk       k   fik (xk ) +   (xk     xk   1).

approach like dual averaging: assume a cyclic choice of ik , and
approximate    f (xk ) by the average of    fi (x) over the last m iterates:

xk+1 = xk       k
m

   fik   l+1(xk   l+1).

m(cid:88)

l=1

stephen wright (uw-madison)

optimization

ipam, july 2012

62 / 118

achievable accuracy

consider the basic incremental method:

xk+1 = xk       k   fik (xk ).

how close can f (xk ) come to f (x   )     deterministically (not just in
expectation).
bertsekas (2011) obtains results for constant steps   k       .

cyclic choice of ik :

random choice of ik :

lim inf

k       f (xk )     f (x   ) +     m2c 2.
k       f (xk )     f (x   ) +     mc 2.

lim inf

where    is close to 1 and c is a bound on the lipschitz constants for    fi .
(bertsekas actually proves these results in the more general context of
regularized optimization - see below.)

stephen wright (uw-madison)

optimization

ipam, july 2012

63 / 118

applications to id166

sa techniques have an obvious application to linear id166 classi   cation. in
fact, they were proposed in this context and analyzed independently by
researchers in the ml community for some years.

codes: sgd (bottou), pegasos (shalev-schwartz et al, 2007).

tutorial: stochastic optimization for machine learning, tutorial by n.
srebro and a. tewari, icml 2010 for many more details on the
connections between stochastic optimization and machine learning.

related work: zinkevich (icml, 2003) on online convex programming.
aiming to approximate the minimize the average of a sequence of convex
functions, presented sequentially. no i.i.d. assumption, regret-based
analysis. take steplengths of size o(k   1/2) in gradient    fk (xk ) of latest
convex function. average regret is o(k   1/2).

stephen wright (uw-madison)

optimization

ipam, july 2012

64 / 118

parallel stochastic approximation

several approaches tried for parallel stochastic approximation.

dual averaging: average gradient estimates evaluated in parallel on
di   erent cores. requires message passing / synchronization (dekel et
al, 2011; duchi et al, 2010).
round-robin: cores evaluate    fi in parallel and update centrally
stored x in round-robin fashion. requires synchronization (langford
et al, 2009).
asynchronous: hogwild!: each core grabs the centrally-stored x
and evaluates    fe(xe) for some random e, then writes the updates
back into x (niu, r  e, recht, wright, nips, 2011).

hogwild!: each processor runs independently:

1 sample e from e ;

2 read current state of x;

for v in e do xv     xv       [   fe(xe)]v ;

3

stephen wright (uw-madison)

optimization

ipam, july 2012

65 / 118

hogwild! convergence

updates can be old by the time they are applied, but we assume a
bound    on their age.

nui et al (2011) analyze the case in which the update is applied to
just one v     e, but can be extended easily to update the full edge e,
provided this is done atomically.
processors can overwrite each other   s work, but sparsity of    fe helps
    updates to not interfere too much.

analysis of niu et al (2011) recently simpli   ed and generalized by
richtarik (2012).

in addition to l,   , m, d0 de   ned above, also de   ne quantities that
capture the size and interconnectivity of the subvectors xe.

  e = |{e(cid:48) : e(cid:48)     e (cid:54)=    }|: number of indices e(cid:48) such that xe and xe(cid:48)
have common components;

e   e   e/|e|2: average rate of overlapping subvectors.

   =(cid:80)

stephen wright (uw-madison)

optimization

ipam, july 2012

66 / 118

hogwild! convergence

(richtarik 2012) (for full atomic update of index e) given       (0, d0/l),
we have

e (f (xj )     f (x   ))      ,

min
0   j   k

for

and k     k , where

  k    

   

(1 + 2     )lm 2|e|2

k =

(1 + 2     )lm 2|e|2

  2 

log

(cid:18) 2ld0

 

(cid:19)

    1

.

broadly, recovers the sublinear 1/k convergence rate seen in regular sgd,
with the delay    and overlap measure    both appearing linearly.

stephen wright (uw-madison)

optimization

ipam, july 2012

67 / 118

hogwild! performance

hogwild! compared with averaged gradient (aig) and round-robin (rr).
experiments run on a 12-core machine. (10 cores used for gradient
evaluations, 2 cores for data shu   ing.)

stephen wright (uw-madison)

optimization

ipam, july 2012

68 / 118

hogwild! performance

stephen wright (uw-madison)

optimization

ipam, july 2012

69 / 118

extensions

to improve scalability, could restrict write access.

break x into blocks; assign one block per processor; allow a processor
to update only components in its block;

share blocks by periodically writing to a central repository, or
gossipping between processors.

analysis in progress.

le et al (2012) (featured recently in the ny times) implemented an
algorithm like this on 16,000 cores.

another useful tool for splitting problems and coordinating information
between processors is the alternating direction method of mulitipliers
(admm).

stephen wright (uw-madison)

optimization

ipam, july 2012

70 / 118

further reading

1 a. nemirovski, a. juditsky, g. lan, and a. shapiro,    robust stochastic
approximation approach to stochastic programming,    siam journal on
optimization, 19, pp. 1574-1609, 2009.

2 d. p. bertsekas,    incremental gradient, subgradient, and proximal methods for

id76: a survey,    chapter 4 in optimization and machine learning,
s. nowozin, s. sra, and s. j. wright (2011).

3 a. juditsky and a. nemirovski,     first-order methods for nonsmooth convex

large-scale optimization. i and ii    methods,    chapters 5 and 6 in optimization
and machine learning (2011).

4 o. l. mangasarian and m. solodov,    serial and parallel id26

convergencevia nonmonotone perturbed minimization,    optimization methods and
software 4 (1994), pp. 103   116.

5 d. blatt, a. o. hero, and h. gauchman,    a convergent incremental gradient

method with constant step size,    siam journal on optimization 18 (2008), pp.
29   51.

6 niu, f., recht, b., r  e, c., and wright, s. j.,    hogwild!: a lock-free approach

to parallelizing stochastic id119,    nips 24, 2011.

stephen wright (uw-madison)

optimization

ipam, july 2012

71 / 118

