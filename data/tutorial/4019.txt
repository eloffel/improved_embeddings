sparse and regularized optimization

in many applications, we seek not an exact minimizer of the underlying
objective, but rather an approximate minimizer that satis   es certain
desirable properties:

sparsity (few nonzeros);
speci   c nonzero patterns (e.g. tree structure);
low-rank (if a matrix);
low    total-variation   ;
generalizability. (vapnik:    ...tradeo    between the quality of the
approximation of the given data and the complexity of the
approximating function.   )

a common way to obtain structured solutions is to modify the objective f
by adding a regularizer      (x), for some parameter    > 0.

min

x

f (x) +      (x),

where    induces the desired structure in x.

stephen wright (uw-madison)

optimization

ipam, july 2012

72 / 110

applications i

lasso for variable selection. originally stated as

min

x

1
2

(cid:107)ax     b(cid:107)2

2 such that (cid:107)x(cid:107)1     t ,

for parameter t > 0. equivalent to an    (cid:96)2-(cid:96)1    formulation:

min

x

1
2

(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1,

for some    > 0.

group lasso to select disjoint groups of variables:
(cid:107)x[g ](cid:107)2,

(cid:107)ax     b(cid:107)2

min

2 +

g   g
with each [g ] a subset of indices {1, 2, . . . , n}.

x

1
2

(cid:88)

easy to shrink with disjoint groups.
still easy when (cid:107)    (cid:107)2 is replaced by (cid:107)    (cid:107)   .

stephen wright (uw-madison)

optimization

ipam, july 2012

73 / 110

applications ii

overlapping groups. there are sometimes complex relationships
between the variables, and we want the set of variables selected (the
nonzeros) to respect these relationships. can sometimes design groups
regularizers that induce this structure. examples:

each group is the set of descendants of a node in a directed graph;
when coe   cients form a tree (e.g. wavelet representations), each
group could be (a) the set of ancestors of a node; (b) parent-child
pairs; (c) all subtrees.

there   s much recent work on shrink-based algorithms by f. bach and the
willow and sierra project teams.

for hierarchical groups ([g ]     [h] (cid:54)=     implies either [g ]     [h] or
[h]     [g ]), shrink operator can be computed e   ciently by ordering
groups appropriately.
for non-hierarchical groups with (cid:107)    (cid:107)   , use a network    ow technique
to shrink (mairal et al, nips, 2010).

stephen wright (uw-madison)

optimization

ipam, july 2012

74 / 110

applications iii

nonconvex regularizers. nonconvex element-wise penalties have
become popular for variable selection in statistics.

scad (smoothed clipped absolute deviation) (fan and li, 2001)
mcp (zhang, 2010).

properties: unbiased estimates, sparse estimates, solution path continuous
in regulariation parameter    .

code: sparsenet (mazumder, friedman, hastie, 2011): coordinate desc.

stephen wright (uw-madison)

optimization

ipam, july 2012

75 / 110

applications iv

compressed sensing. sparse signal recovery from noisy measurements.
given matrix a (with more columns than rows) and observation vector y ,
seek a sparse x (i.e. few nonzeros) such that ax     y . solve

min

x

1
2

(cid:107)ax     b(cid:107)2

2 +   (cid:107)x(cid:107)1.

under    restricted isometry    properties on a (   tall    column
submatrices are nearly orthonormal), (cid:107)x(cid:107)1 is a good surrogate for
card(x).

assume that a is not stored explicitly, but matrix-vector
multiplications are available. hence can compute f and    f .

stephen wright (uw-madison)

optimization

ipam, july 2012

76 / 110

applications v: classi   cation and regression

support vector machines. see above. can use (cid:107)w(cid:107)1 in the linear id166,
to get a sparse weight vector.

(cid:96)1-regularized id28. see above.

stephen wright (uw-madison)

optimization

ipam, july 2012

77 / 110

applications vi: matrix completion

seek a matrix x     rm  n with low rank that matches certain observations,
possibly noisy.

min
x

1
2

(cid:107)a(x )     b(cid:107)2

2 +      (x ),

where a(x ) is a linear mapping of the components of x (e.g.
element-wise observations).

can have    as the nuclear norm     see discussion above for solution of
subproblems via svd.

alternatively: x is the sum of sparse matrix and a low-rank matrix. the
element-wise 1-norm (cid:107)x(cid:107)1 is useful in inducing sparsity.

stephen wright (uw-madison)

optimization

ipam, july 2012

78 / 110

basics of shrinking

regularizer    is often nonsmooth but    simple.    often the problem is easy
to solve when f is replaced by a quadratic with diagonal hessian:

equivalently,

min

z

g t (z     x) +

(cid:107)z     x(cid:107)2

2 +      (z).

1
2  

min

z

1
2  

(cid:107)z     (x       g )(cid:107)2

2 +      (z).

de   ne the shrink operator as the arg min:

s   (y ,   ) := arg min

z

(cid:107)z     y(cid:107)2

2 +      (z).

1
2  

typical algorithm:

xk+1 = s   (xk       k gk ,   k ),

with for example gk =    f (xk ).

stephen wright (uw-madison)

optimization

ipam, july 2012

79 / 110

interesting regularizers and their shrinks: i

cases for which the subproblem is simple:

  (z) = (cid:107)z(cid:107)1. thus s   (y ,   ) = sign(y ) max(|y|         , 0). when y
complex, have

max(|y|          , 0)

max(|y|          , 0) +      

y .

  (z) =(cid:80)

s   (y ,   ) =

g   g (cid:107)z[g ](cid:107)2 or   (z) =(cid:80)

are non-overlapping subvectors of z. here

g   g (cid:107)z[g ](cid:107)   , where z[g ], g     g

s   (y ,   )[g ] =

max(|y[g ]|          , 0)

max(|y[g ]|          , 0) +      

y[g ].

  (x) = i   (x): indicator function for a closed convex set    . then
s   (y ,   ) is the projection of y onto    .

stephen wright (uw-madison)

optimization

ipam, july 2012

80 / 110

interesting regularizers and their shrinks: ii

z is a matrix and   (z ) = (cid:107)z(cid:107)    is the nuclear norm of z : the sum of
singular values. threshold operator is

s   (y ,   ) := arg min

z

(cid:107)z     y(cid:107)2

f +   (cid:107)z(cid:107)   

1
2  

with solution obtained from the svd y = u  v t with u, v
orthonormal and    = diag(  i )i=1,2,...,m. setting
     = diag(max(  i          , 0)i=1,2,...,m), the solution is

s   (y ,   ) = u     v t .

stephen wright (uw-madison)

optimization

ipam, july 2012

81 / 110

basic prox-linear algorithm

(fukushima and mine, 1981) for solving minx f (x) +      (x).

0: choose x0
k: choose   k > 0 and set

xk+1 = s   (xk       k   f (xk );   k )

= arg min

z

   f (xk )t (z     xk ) +

(cid:107)z     xk(cid:107)2

2 +      (z).

1
2  k

this approach goes by many names, including    forward-backward
splitting,       shrinking / thresholding.   

straightforward, but can be fast when the id173 is strong (i.e.
solution is    highly constrained   ) and the reduced problem is well
conditioned.
can show convergence for steps   k     (0, 2/l), where l is the bound on
   2f . (like a short-step gradient method.)

stephen wright (uw-madison)

optimization

ipam, july 2012

82 / 110

enhancements

alternatively, since   k plays the role of a steplength, can adjust it to get
better performance and guaranteed convergence.

   backtracking:    decrease   k until su   cient decrease condition holds.
use barzilai-borwein strategies to get nonmonotonic methods. by
enforcing su   cient decrease every 10 iterations (say), still get global
convergence.

the approach can be accelerated using optimal gradient techniques. see
earlier discussion of fista, where we solve the shrinking problem with
  k = 1/l in place of a step along       f with this steplength.
note that these methods reduce ultimately to gradient methods on a
reduced space: the optimal manifold de   ned by the regularizer   .
acceleration or higher-order information can help improve performance.

stephen wright (uw-madison)

optimization

ipam, july 2012

83 / 110

continuation in   

performance of basic shrinking methods is quite sensitive to    .
typically higher        stronger id173     optimal manifold has lower
dimension. hence, it   s easier to identify the optimal manifold, and basic
shrinking methods can sometimes do so quickly.

for smaller    , a simple    continuation    strategy can help:

0: given target value   f , choose initial   0 >   f , starting point   x and

factor        (0, 1).

k: find approx solution x(  k ) of minx f (x) +      (x), starting from   x;

if   k =   f then stop;
set   k+1     max(  f ,     k ) and   x     x(  k );

recent report by xiao and zhang (2012) analyzes this strategy.

(solution x(   ) is often desired on a range of    values anyway.)

stephen wright (uw-madison)

optimization

ipam, july 2012

84 / 110

stochastic gradient + id173: dual averaging
solve the regularized problem, but have only estimates of    f (xk ).
we can combine dual averaging, stochastic gradient, and shrinking: see
xiao (2010) who extends nesterov (2009).

min

x

     (x) := e  f (x;   ) +      (x)

  gk =(cid:80)k

at iteration k choose   k randomly and i.i.d from the    distribution, and
choose gk        f (xk ;   k ). use these to de   ne the averaged subgradient

i=1 gi /(k + 1), and solve the subproblem
     
k

  g t
k x +      (x) +

xk+1 = arg min

x

(cid:107)x     x0(cid:107)2.

same as earlier, but with regularizer    included explicitly.

can prove convergence results for averaged iterates   xk : roughly

e      (  xk )          

       c   
k

,

where expectation is over the random number stream   0,   1, . . . ,   k   1.

stephen wright (uw-madison)

optimization

ipam, july 2012

85 / 110

stochastic gradient + id173: fobos

an obvious extension of the prox-linear approach to the stochastic
gradient setting: replace    f by an approximation e.g. gk        f (xk ;   k ):

xk+1 = s   (xk       k gk ;   k ).

(duchi and singer, 2009, p.9-10).

comid: generalization to mirror descent.

also ssg (lin et al, 2011), and truncated gradient (langford et al, 2009).

stephen wright (uw-madison)

optimization

ipam, july 2012

86 / 110

identifying optimal manifolds

identi   cation of the manifold of the regularizer    on which x    lies can
improve algorithm performance, by focusing attention on a reduced space.
we can thus evaluate partial gradients and hessians, restricted to just this
space.

for nonsmooth regularizer   , the optimal manifold is a smooth surface
passing through x    along which the restriction of    is smooth.
example: for   (x) = (cid:107)x(cid:107)1, have manifold consisting of z with

                   0

    0
= 0

zi

if x   
if x   
if x   

i > 0
i < 0
i = 0.

if we know the optimal nonzero components, we know the manifold. we
could restrict the search to just this set of nonzeros.

stephen wright (uw-madison)

optimization

ipam, july 2012

87 / 110

identi   cation in stochastic gradient / dual averaging

in the stochastic setting, dual averaging has identi   cation properties most
like shrink algorithms with full gradient.

in the strongly convex setting, as the non-averaged iterates xk (mostly)
converge to x   , the gradient estimate   gk (mostly) converges to    f (x   ).
eventually, the subsequence of iterates that lie on the optimal manifold
becomes dense. (lee and wright, 2012)

this property is not shared by algorithms that

average the primal iterates;
use only the latest gradient estimate gk in the step computation (that
is, they don   t average the dual iterates.

in particular, under (cid:96)1 id173, algorithms with these features don   t
usually produce sparse solutions.

algorithmic implication: once the manifold settles down, can switch to a
di   erent algorithm, better suited to a small space.

stephen wright (uw-madison)

optimization

ipam, july 2012

88 / 110

further reading

1 optimization for machine learning, edited volume with 18 chapters, mit press,

nips workshop series, 2011.

2 f. bach, r. jenatton, j. mairal, g. obozinski,    id76 with

sparsity-inducing norms,    in [1], 2011.

3 m. fukushima and h. mine.    a generalized proximal point algorithm for certain

non-convex minimization problems.    international journal of systems science, 12,
pp. 989   1000, 1981.

4 p. l. combettes and v. r. wajs.    signal recovery by proximal forward-backward

splitting.    multiscale modeling and simulation, 4, pp. 1168   1200, 2005.

5 s. j. wright, r. d. nowak, and m. a. t. figueiredo.    sparse reconstruction by

separable approximation.    ieee transactions on signal processing, 57, pp.
2479   2493, 2009.

6 l. xiao.    dual averaging methods for regularized stochastic learning and online
optimization.    journal of machine learning research, 11, pp 2543   2596, 2010.
7 l. xiao and t. zhang,    a proximal-gradient homotopy methods for the sparse

least-squares problem,    technical report, march 2012. arxiv:1203:2003v1.

8 a. lewis and s. wright,    a proximal method for composite minimization,    2008.
9 s. lee and s. j. wright,    manifold identi   cation for dual averaging in regularized

stochastic online learning,    jmlr, 2012.

stephen wright (uw-madison)

optimization

ipam, july 2012

89 / 110

v. decomposition / coordinate relaxation

for min f (x), at iteration k, choose a subset gk     {1, 2, . . . , n} and take
a step dk only in these components. i.e.    x dk (i) = 0 for i /    gk .
gives more manageable subproblem sizes, in practice.

can

take a reduced gradient step in the gk components;
take multiple    inner iterations   
actually solve the reduced subproblem in the space de   ned by gk .

stephen wright (uw-madison)

optimization

ipam, july 2012

90 / 110

constraints and regularizers complicate things

for minx       f (x), need to put enough components into gk to stay feasible,
as well as make progress.
example: min f (x1, x2) with x1 + x2 = 1. relaxation with gk = {1} or
gk = {2} won   t work.
for separable regularizer (e.g. group lasso) with

(cid:88)

i   g

  (x) =

  i (x[i]),

need to ensure that gk is a union of the some index subsets [g ]. i.e. the
relaxation components must be consonant with the partitioning.

stephen wright (uw-madison)

optimization

ipam, july 2012

91 / 110

decomposition and dual id166

decomposition has long been popular for solving the dual (qp)
formulation of id166, since the number of variables (= number of training
examples) is sometimes very large.

smo: each gk has two components.
libid166: smo approach (still |gk| = 2), with di   erent heuristic for
choosing gk .
laid166: again |gk| = 2, with focus on online setting.
id166-light: small |gk| (default 10).
gpdt: larger |gk| (default 400) with gradient projection solver as
inner loop.

stephen wright (uw-madison)

optimization

ipam, july 2012

92 / 110

choice of gk and convergence results
some methods (e.g. tseng and yun, 2010) require gk to be chosen so
that the improvement in subproblem objective obtained over the subset gk
is at least a    xed fraction of the improvement available over the whole
space. undesirable, since to check it, usually need to evaluate the full
gradient    f (xk ).
alternative is a generalized gauss-seidel requirement, where each
coordinate is    touched    at least once every t iterations:
gk     gk+1     . . .     gk+t   1 = {1, 2, . . . , n}.

can show global convergence (e.g. tseng and yun, 2009; wright, 2010).

there are also results on

global linear convergence rates
optimal manifold identi   cation
fast local convergence for an algorithm that takes reduced steps on
the estimated optimal manifold.

all are deterministic analyses.

stephen wright (uw-madison)

optimization

ipam, july 2012

93 / 110

naive stochastic coordinate descent

analysis tools of stochastic gradient may be useful.
for minx f (x), take steps of the form xk+1 = xk       k gk , where

(cid:40)

gk (i) =

[   f (xk )]i
0

if i     gk
otherwise,

with suitable random selection of gk can ensure that gk (appropriately
scaled) is an unbiased estimate of    f (xk ). hence can apply sgd
techniqes discussed earlier, to choose   k and obtain convergence.

stephen wright (uw-madison)

optimization

ipam, july 2012

94 / 110

stochastic coordinate descent

(richtarik and takac, 2012; see also nesterov, 2012)

x
describe an approach that

min

  (x) := f (x) +   (x).

partitions the components of x into subvectors, consonant with the
regularizer   ;
makes a random selection of one partition to update at each iteration;
exploits knowledge of the partial lipschitz constant for each partition
in choosing the step.
allows parallel implementation.

essentially, picks one partition and does the basic short-step, prox-linear
method for that component, shrunk with the regularizer for that
component.

richtarik and takac call it rcdc: randomized coordinate descent for
composite functions.

stephen wright (uw-madison)

optimization

ipam, july 2012

95 / 110

rcdc details
partition components {1, 2, . . . , n} into m blocks with block [i] with
corresponding columns from the n    n identity matrix denoted by ui .
denote by li the partial lipschitz constant on partition [i]:

(cid:107)   [i]f (x + ui t)        [i]f (x)(cid:107)     li(cid:107)t(cid:107).

separate the regularizer    as above:

m(cid:88)
(cid:32) m(cid:88)

i=1

  (x) =

  i (x[i]).

(cid:33)1/2

(cid:107)x(cid:107)w :=

wi(cid:107)x[i](cid:107)2

for overall structure, de   ne a weighted norm, for weights w1, w2, . . . , wm:

i=1

weighted measure of level set size:

rw (x) := max

y

x      x    {(cid:107)y     x   (cid:107)w :   (y )       (x)}.

max

stephen wright (uw-madison)

optimization

ipam, july 2012

96 / 110

rcdc algorithm

the key subproblem, for a given partition i:

p(x, i) : min

d

d t   [i]f (x) +

li
2

(cid:107)d(cid:107)2 +   i (x[i] + d).

fix probabilities of choosing each partition: pi , i = 1, 2, . . . , m.

iteration k:

choose partition ik     {1, 2, . . . , m} with id203 pi ;
solve p(xk , ik ) to obtain step dk,i ;
set xk+1 = xk + uik dk,i .

for convex f and   , and uniform weights pi = 1/m, can prove high
id203 convergence of f to within a speci   ed threshold of f (x   ) in 1/k
iterations.

stephen wright (uw-madison)

optimization

ipam, july 2012

97 / 110

rcdc analysis

the basic analysis requires three steps.
i. given random varaible sequence   0,   1,   2,       with

(cid:18)

(cid:19)

e [  k+1 |   k ]    

1     1
c2

  k , whenever   k      ,

where   > 0 is a speci   ed threshold and c2     (0, 1) is some constant, we
have for k     k with

k := c2 log

  0
   

,

that p(  k      )     1       .
ii. expected improvement in    at step k is the average of the m possible
partition-wise improvements. for each partition, because of the short-step
strategy, the actual improvement is at least equal to the improvement
predicted by the subproblem p(xk , i).

stephen wright (uw-madison)

optimization

ipam, july 2012

98 / 110

rcdc analysis, continued

e [  (xk+1)| xk ]       (xk ) +

(cid:2)d t

m(cid:88)
+   [i]((xk )[i] + dk,i )       i ((xk )[i])(cid:3)

k,i   [i]f (xk ) +

(cid:107)dk,i(cid:107)2

1
m

li
2

i=1

=   (xk ) + j(xk ).

(each term in the sum j(xk ) is the improvement predicted if partition i is
selected at iteration k.)

iii. the predicted optimality gap after step k is a substantial improvement
over the gap   (xk )           at iteration k.

  (x) + j(x)               max

, 1       (x)          
2(cid:107)x     x   (cid:107)2

l

(  (x)          ).

(cid:18) 1

2

(cid:19)

stephen wright (uw-madison)

optimization

ipam, july 2012

99 / 110

rcdc analysis, continued

put these pieces together, de   ning

  k :=   (xk )          ,

and assuming that the de   ned threshold   has

  < min(r2

l(x0),   (x0)          )

and de   ning

k :=

we have for k     k that

2nr2
l(x0)
 

log

  (x0)          

   

,

p(  (xk )                )     1       .

stephen wright (uw-madison)

optimization

ipam, july 2012

100 / 110

rcdc, strongly convex case

if    is strongly convex with respect to the weighted norm (cid:107)    (cid:107)l, we have
expected convergence at a linear rate. require
  (x)       (y ) + (x     y )t     (y ) +
  
2
where       denotes any subgradient of   . de   ning
if        2
otherwise

1       /4
1/  

(cid:107)x     y(cid:107)2
l,

(cid:40)

     :=

,

we have

e [  (xk )          ]    

(cid:18)
1     1         

(cid:19)k

n

(  (x0)          ).

stephen wright (uw-madison)

optimization

ipam, july 2012

101 / 110

further reading

1 p. tseng and s. yun,    a coordinate id119 method for linearly
constrained smooth optimization and support vector machines training.   
computational optimization and applications, 47, pp. 179   206, 2010.

2 p. tseng and s. yun,    a coordinate id119 method for nonsmooth

separable minimization.    mathematical programming, series b, 117. pp.
387   423, 2009.

3 p. richtarik and m. takac,    iteration complexity of randomized block-coordinate

id119 methods for minimizing a composite function,    technical report,
revised, july 2012.

4 s. j. wright,    accelerated block-coordinate relaxation for regularized

optimization.    siam j. optimization 22 (2012), pp. 159   186.

5 y. nesterov,    e   ciency of coordinate descent methods on huge-scale optimization

problems.    siam j. optimization 22 (2012), pp. 341   362.

stephen wright (uw-madison)

optimization

ipam, july 2012

102 / 110

augmented lagrangian methods and splitting

consider linearly constrained problem:

min f (x) s.t. ax = b.

augmented lagrangian is

l(x,   ;   ) := f (x) +   t (ax     b) +

(cid:107)ax     b(cid:107)2
2,

  
2

where    > 0. basic augmented lagrangian / method of multipliers is

l(x,   k   1;   k );
xk = arg min
  k =   k   1 +   k (axk     b);

x

(choose   k+1).

extends in a fairly straightforward way to inequality constraints, nonlinear
constraints.

stephen wright (uw-madison)

optimization

ipam, july 2012

103 / 110

quick history of augmented lagrangian

dates from 1969: hestenes, powell.

developments in 1970s, early 1980s by rockafellar, bertsekas, and
others.

lancelot code for nonid135: conn, gould, toint,
around 1990.

largely lost favor as an approach for general nonid135
during the next 15 years.

recent revival in the context of sparse optimization and its many
applications, in conjunction with splitting / coordinate descent.

stephen wright (uw-madison)

optimization

ipam, july 2012

104 / 110

separable objectives: admm

alternating directions method of multipliers (admm) arises when the
objective in the basic linearly constrained problem is separable:

f (x) + h(z) subject to ax + bz = c,

min
(x,z)

for which

l(x, z,   ;   ) := f (x) + h(z) +   t (ax + bz     c) +

(cid:107)ax     bz     c(cid:107)2
2.
standard augmented lagrangian would minimize l(x, z,   ;   ) over (x, z)
jointly     but these are coupled through the quadratic term, so the
advantage of separability is lost.

  
2

instead, minimize over x and z separately and sequentially:

x

xk = arg min

l(x, zk   1,   k   1;   k );
l(xk , z,   k   1;   k );
zk = arg min
  k =   k   1 +   k (axk + bzk     c).

z

stephen wright (uw-madison)

optimization

ipam, july 2012

105 / 110

admm

basically, does a round of block-coordinate descent in (x, z).

the minimizations over x and z add only a quadratic term to f and
h, respectictly. this does not alter the cost much.

can perform these minimizations inexactly.

convergence is often slow, but su   cient for many applications.

many recent applications to compressed sensing, image processing,
matrix completion, sparse principal components analysis.

admm has a rich collection of antecendents. for an excellent recent
survey, including a diverse collection of machine learning applications, see
(boyd et al, 2011).

stephen wright (uw-madison)

optimization

ipam, july 2012

106 / 110

admm for consensus optimization

given

min

m(cid:88)

i=1

fi (x),

form m copies of the x, with the original x as a    master    variable:

min

x,x 1,x 2,...,x m

fi (x i ) subject to x i = x, i = 1, 2, . . . , m.

m(cid:88)

i=1

apply admm, with z = (x 1, x 2, . . . , x m), get

x i
k = arg min
x i

fi (x i ) + (  i

(cid:18)

m(cid:88)

i=1

xk =

1
m

x i
k +

  i
k =   i

k   1 +   k (x i

(cid:19)
k   1)t (x i     xk   1) +
1
  i
k   1
  k
k     xk ),    i

,

(cid:107)x i     xk   1(cid:107)2

2,    i ,

  k
2

obvious parallel possibilities in the x i updates. synchronize for x update.

stephen wright (uw-madison)

optimization

ipam, july 2012

107 / 110

admm for awkward intersections

the feasible set is sometimes an intersection of two or more convex sets
that are easy to handle separately (e.g. projections are easily computable),
but whose intersection is more di   cult to work with.

example: optimization over the cone of doubly nonnegative matrices:

f (x ) s.t. x (cid:23) 0, x     0.

min
x

general form:

min f (x) s.t. x        i ,

i = 1, 2, . . . , m

just consensus optimization, with indicator functions for the sets.
k   1(cid:107)2
2,

k   1)t (x     x i

xk = arg min

(cid:107)x     x i

f (x) +

(  i

x

m(cid:88)

i=1

  k
k   1) +
2
2,    i
(cid:107)xk     x i(cid:107)2

k   1)t (xk     x i ) +

(  i

x i
k = arg min
xi      i
k   1 +   k (xk     x i

  i
k =   i

k ),    i .

  k
2

stephen wright (uw-madison)

optimization

ipam, july 2012

108 / 110

admm and prox-linear

given

reformulate as the equality constrained problem:

min

x

f (x) +      (x),

min
x,z

f (x) +      (z) subject to x = z.

admm form:

xk := min

x

f (x) +      (zk   1) + (  k )t (x     zk   1) +
f (xk ) +      (z) + (  k )t (xk     z) +
  k
2

(cid:107)zk   1     x(cid:107)2
2,

  k
2

(cid:107)z     xk(cid:107)2
2,

zk := min

z

  k+1 :=   k +   k (xk     zk ).

minimization over z is the shrink operator     often inexpensive.
minimization over x can be performed approximately using an
algorithm suited to the form of f .

stephen wright (uw-madison)

optimization

ipam, july 2012

109 / 110

further reading

s. boyd, n. parikh, e. chu, b. peleato, and j. eckstein,    distributed optimization
and statistical learning via the alternating direction methods of multipliers,   
foundations and trends in machine learning, 3, pp. 1-122, 2011.

s. boyd,    alternating direction method of multipliers,    talk at nips workshop
on optimization and machine learning, december 2011:
videolectures.net/nipsworkshops2011 boyd multipliers/

j. eckstein and d. p. bertsekas,    on the douglas-rachford splitting method and
the proximal point algorithm for maximal monotone operators,    mathematical
programming, 55, pp. 293-318, 1992.

stephen wright (uw-madison)

optimization

ipam, july 2012

110 / 110

