a tutorial on sparse modeling.

outline:

1. why?

2. what?

3. how.

4. no really, why?

sparse modeling is a component in many state of the art signal processing
and machine learning tasks.

    image processing (denoising, inpainting, superresolution): [yu, mal-

lat, sapiro], [mairal, elad, sapiro].

    object recognition:

[yang, yu, gong, huang], [boureau, la roux,

bach, ponce, lecun].

    general supervised learning:

[mairal, bach, ponce, sapiro, zisser-

man].

    building graphs for large scale semi-supervised learning: [liu, wang,

kumar, chang].

sparse modeling and id105s

given a d    n matrix x of n points in rd.

    want to factor x     w z, where w is d    k, and z is k    n .

    w is a dictionary, z are the coe   cients.

    we need to choose an appropriate notion of    close    and conditions

on z to force the decomposition to be parsimonious

    if we restrict the size of k < min(d, n ), and    close    is operator or

frobenious norm, we get pca.

    if we restrict zij     {0, 1}, and pi zij = 1 (i.e. zj is really sparse!), we

get id116

    everything in between (including the endpoints): dictionary learning.

e.g.

arg min

z   rk  n,w    s(d   1)  k

n

xj=1

||w zj     xj||2, ||zj||0     q,

    or the z coordinate convexi   cation:

arg min

z   rk  n,w    s(d   1)  k xj=1

||w zj     xj||2 +   ||zj||1.

structured sparsity/group sparsity

coe   cents lie in speci   ed groups; constraints on or penalties for non-
sparse group activations rather than non-sparse elementwise activations

    a simple    manifold    model: non-overlapping groups and 1-sparse

group activations.

    if the groups overlap, can encourage trees, grids, etc.

manifold learning

    manifold=locally well approximated by a   ne spaces (for a true man-

ifold, the tangent spaces).

    it may be impractical to work with the tangent planes at every point

in x.

    if the    curvature    of x is not excessive, it may be possible to    nd
a set of l good q dimensional    secant    planes so that every point is
close to its secant plane.

choosing the q   planes that minimize the average distance from each
point in x to its plane is minimizing

such that

||w z     x||2
f ,

q

q

q

w = d( (cid:16)

z}|{

w1 w2

z}|{

z}|{

... wl (cid:17), z =

|x1|

0

z}|{
   
qn z1
qn
qn
qn

                              

0
...
0

|x2|

z}|{

0

z2
0
...
0

0

0

z3
...
0

        

        

        
. . .
0

|xl|

z}|{

0

0

0
0
zl

,

   

                              

where xi is the set of points whose nearest plane is the span of wi

thus we can interpret approximating the data set by l q-planes as a
   structured    sparse dictionary design problem with k = lq.

in fact, all the previous models are    manifold    models. for each of the
previous models:

    the analysis map from x to z with w    xed is a piecewise a   ne.

    the reconstruction map y = w z is linear.

for example: for the map

z    = z   (x, w ) = arg min

z

||w z     x||2 +   ||z||1,

    under mild regularity conditions on w , the solution z    is unique, and

has explicit solution once its sign is    xed:

z   |    = (w t

   w   )   1(w t

   x         ),

where    = sign(z), and     is the set of nonzero entries in   .

sparse coding vs. compressive sensing

compressive sensing: arg min

z   rk

||w z     x||2 +   ||z||1.

here, z is the data, and x is the code. encoding is trivial (multiplication
by w ), decoding requires an optimization. w is universal.

sparse coding: arg min

z   rk

||w z     x||2 +   ||z||1.

here, z is the code, and x is the data. decoding is trivial (multiplication
by w ), encoding requires an optimization. w is adapted.

greedy methods for the forward l0 problem with w    xed

||w z     x||2,

min

z

||z||0     q,

where the d    k matrix w is the dictionary, the k    1 z is the code, and
x is an d    1 data vector.

    matching pursuit, orthogonal matching pursuit, order recursive match-

ing pursuit

    cosamp [needell and tropp].

(o)mp:

1. initialize: coe   cients z = 0, residual r = x, active set     =    .

2. j = arg maxi |w t

i r|

3.     =    s j

4. for mp zj = w t
   r

for omp z = (cid:16)w t

   w   (cid:17)   1

w t

   x

5. r = x     w z. goto 2 until q iterations.

note that with a bit of bookkeeping, it is only necessary to multiply w
against x once, instead of q times. this at a cost of an extra o(k2)
storage for the gram matrix q of w . we can also keep a running update
of q   1
using a cholesky factorization, and the submatrix
q    = w t w    of q. critical for many id136s with a    xed dictionary.

    = (cid:16)w t

   w   (cid:17)   1

1. initialize: t = s = w t x, active set     =    .

2. j = arg maxi |ti|

3.     =    s j, update q   1
4. t = s        q   q   1

    s   

   

5. goto 2 until q iterations.

when to use what method?

    ormp>omp>>mp, in terms of accuracy. exactly opposite in terms

of runtime.

    don   t use mp unless you have to (need every cpu cycle, or in convo-

lutional problems).

    if problem is large, and only being done once, solution is not very

sparse, and dictionary is well conditioned, use cosamp.

in general, greedy methods good when you expect/will enforce extreme
sparsity. computation time is roughly on the order of one multiplication
of the data by the dictionary, assuming you have stored the q.

methods for the forward relaxed problem with w    xed:

too many methods to discuss. will focus on two good ones.

arg min

z

||w z     x||2 +   ||z||1

lars [efron, hastie, johnstone, tibshirani] uses the explicit solution
once the active set is    xed to generate a path in solution space param-
eterized by the regularity. as before, can store w t w and keep running
updates of all variables in compact form for large speedup.

1. set     = arg max |w t

j x|,    = |w t

   x|,

2. choose the next smallest    such that with

z|    = (w t

   w   )   1(w t

   x            ), z   c = 0,

(a)     i        c such that |w t

i (w z     x)| =   ; in this case,     =    s i.

(b)     i         such that zi = 0; in this case,     =         i.

3. update   

ista: iterated shrinkage thresholding algorithm or proximal gradient
descent:

1. initialize: z = 0.

2. y = z       w t (w z     x)

(gradient step with respect to the smooth part).

3. z = arg minp ||p     y||2 +     |p|1

= shrink(y,     )
= (|y|         )+ sign(y)

(optimize the nonsmooth part with a penalty for straying too far from
smooth update).

4. goto 2 until stopping criteria.

as before, we can precompute things and make the algorithm a little
faster. set q = w t w , b = w t x.

1. initialize: z = 0, t1 = 1.

2. xk = shrink((i     q)z     b,     )

3. tk + 1

4. repeat until stopping criteria.

notice: linear map, followed by o   set, followed by nonlinearity. repeat.

using a clever (magic) momentum term convergence can be greatly sped
up! [nesterov 1983, beck and teboulle 2009]

1. yk = shrink((i     q)zk     b,     )

2. tk+1 = (cid:16)1 +q1 + t2
3. zk+1 = yk + tk   1
tk+1

k+1(cid:17) /2

(yk     yk   1)

when to use what method?

    for many small, very sparse problems use lars (almost as fast as

omp there).

    if problem is large, and only being done once, solution is not very
sparse, and dictionary is well conditioned, use nesterov accelerated
proximal id119.

note: an introduction to methods for basis pursuit could easily be a
weeklong a   air.

learning the w

general good practice: some version of stochastic id119.

    gradient w.r.t. w :

   w = (w z     x)xt .

can sometimes do better with averaging type sgd. e.g.
ponce, sapiro].

[mairal, bach,

batch: alternate between updating the codes and updating the    lters,
as in k-svd [aharon el. al]:

1. initialize w .

2. solve for z as above.

3. for each wj,

       nd all x where wj is activated

    for each such xp,    nd ep by removing the contribution of wj (that

is ep = xp     wjzjp).

    update wj     pca(ep)

what do we know theoretically?:

about the compressed sensing problem, lots!

    if w is su   ciently regular (e.g.

incoherent), and z is su   ciently
sparse, both greedy methods and l1 relaxations are guaranteed to
recover the true z

    mutual coherence:   (w ) = maxi6=j(| < wj, wi > |)

    problem: dictionaries we train will often be coherent.

what do we know theoretically about dictionary learning (that is,

when does it work?):

very little!

dictionary identi   cation:

    if enough data is sampled i.i.d. from distribution built from an inco-
herent dictionary, then w.h.p. the    true solution    is a local minimum
for the dictionary learning problem [gribonval and schnass], [geng
and wright].

    these works are for the constrained problem

min |z|1 s.t. w z = x.

generalization bounds [maurer and pontil]:

de   ne

z(x, w ) = arg min
|z|1=1

||w z     x||2

suppose the n point set x     rd is generated i.i.d. from   , and w    is the
minimizer of

||w   z(x, w   )     x||2,

xx   x

and   w is the minimizer of

and b is the value of that expression at the minimizer. then

e

  (x)||   w z(x,   w )     x||2,

  (x)||w   z(x, w   )     x||2 < b + o   

e

   ksln m

m

m    
+slog(1/  
   

with id203   . (see also [vainsencher, mannor, bruckstein])

    but all of these discuss our ability to succesfully use the model. they
do not give much insight to when the model makes sense and should
be used.

    can we look at a set of data points, extract some geometric statistics,
and then decide sparse modeling is a reasonable approach for that
data? and estimate the correct method and parameters for maximum
generalization?

    even in simple cases?

let r(x, k, q) be the minimal error ||w   z        x||2
x in the pure sparse coding model.

fro for a given k, q, and

    question 1: what is the worst possible reconstruction error for a

data set with n points? in equations, the problem is to describe

f (k, q, n) =

max

x   sd   1,|x|=n

r(x, k, q).

here x is constrained to the unit sphere to avoid a trivial answer via
scaling, and |x| is the number of elements in x.

    question 2: suppose that we know x is actually close to a given set
of q   -planes in rd, that is, there exist orthogonal matrices p1, ..., pm
of size d    q   

xj

min

i

||xj     pip t

i xj||2       .

then describe

f (k, q, n) =

min

x   sd   1,|x|=n

r(x, k, q).

also: how to get from a representation of x via the p to a represen-
tation via w and z?

    question 3: more generally, what kinds of geometries (if not locally
approximated by planes) allow for good representations via the various
sparse coding models? in other words, given a data set, how can we
decide which (if any) of the models are appropriate?

    not completely trivial/nontrivial even for pca, depending on the

(kind of) noise in the data

    or even: how can we decide on the parameters of the model if

we know the correct one?!

    just deciding q is a serious issue (even in the pca case, with

certain kinds of noise)....

    what about the relationship between sparse modeling and pooling?

