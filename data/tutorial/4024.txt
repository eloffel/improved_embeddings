learning	
   hierarchical	
   
genera.ve	
   models	
   

russ	
   salakhutdinov	
   

department of statistics and computer science!

university of toronto	
   

machine	
   learning   s	
   successes	
   

      	
   computer	
   vision:	
   

-   	
   image	
   inpain.ng/denoising,	
   segmenta.on	
   
-   	
   object	
   recogni.on/detec.on,	
   scene	
   understanding	
   

      	
   informa.on	
   retrieval	
   /	
   nlp:	
   

-   	
   text,	
   audio,	
   and	
   image	
   retrieval	
   	
   
-   	
   parsing,	
   machine	
   transla.on,	
   text	
   analysis	
   

      	
   speech	
   processing:	
   

-   	
   speech	
   recogni.on,	
   voice	
   iden.   ca.on	
   	
   

      	
   robo.cs:	
   

-   	
   autonomous	
   car	
   driving,	
   planning,	
   control	
   

      	
   computa.onal	
   biology	
   
      	
   cogni.ve	
   science.	
   

mining	
   for	
   structure	
   

massive	
   increase	
   in	
   both	
   computa.onal	
   power	
   and	
   the	
   amount	
   of	
   
data	
   available	
   from	
   web,	
   video	
   cameras,	
   laboratory	
   measurements.	
   

images	
   &	
   video	
   

text	
   &	
   language	
   	
   

speech	
   &	
   audio	
   

deep	
   genera.ve	
   models	
   that	
   	
   	
   
support	
   id136s	
   and	
   discover	
   
climate	
   change	
   
structure	
   at	
   mul.ple	
   levels.	
   

rela.onal	
   data/	
   	
   
social	
   network	
   

product	
   	
   
recommenda.on	
   

gene	
   expression	
   

geological	
   data	
   

mostly	
   unlabeled	
   

      	
   develop	
   sta.s.cal	
   models	
   that	
   can	
   discover	
   underlying	
   structure,	
   cause,	
   or	
   
sta.s.cal	
   correla.on	
   from	
   data	
   in	
   unsupervised	
   or	
   semi-     supervised	
   way.	
   	
   
      	
   mul.ple	
   applica.on	
   domains.	
   

mining	
   for	
   structure	
   

massive	
   increase	
   in	
   both	
   computa.onal	
   power	
   and	
   the	
   amount	
   of	
   
data	
   available	
   from	
   web,	
   video	
   cameras,	
   laboratory	
   measurements.	
   

images	
   &	
   video	
   

text	
   &	
   language	
   	
   

speech	
   &	
   audio	
   

gene	
   expression	
   

product	
   	
   
recommenda.on	
   

rela.onal	
   data/	
   	
   
social	
   network	
   

climate	
   change	
   

geological	
   data	
   

mostly	
   unlabeled	
   

      	
   develop	
   sta.s.cal	
   models	
   that	
   can	
   discover	
   underlying	
   structure,	
   cause,	
   or	
   
sta.s.cal	
   correla.on	
   from	
   data	
   in	
   unsupervised	
   or	
   semi-     supervised	
   way.	
   	
   
      	
   mul.ple	
   applica.on	
   domains.	
   

deep	
   genera.ve	
   model	
   

deep	
   boltzmann	
   machine	
   

gaussian-     bernoulli	
   markov	
   
random	
   field	
   

12,000	
   latent	
   	
   
variables	
   

model	
   p(image)	
   

planes	
   

96	
   by	
   96	
   
images	
   

stereo	
   pair	
   

24,000	
   training	
   images	
   

(salakhutdinov, 2008; salakhutdinov & hinton, ai & statistics 2009)!

deep	
   genera.ve	
   model	
   

sanskrit	
   

model	
   p(image)	
   

25,000	
   characters	
   from	
   50	
   
alphabets	
   around	
   the	
   world.	
   

      	
   3,000	
   hidden	
   variables	
   
      	
   784	
   	
   observed	
   variables	
   
	
   	
   	
   (28	
   by	
   28	
   images)	
   
      	
   over	
   2	
   million	
   parameters	
   

bernoulli	
   markov	
   random	
   field	
   

deep	
   genera.ve	
   model	
   

condi.onal	
   
simula.on	
   

p(image|par.al	
   image)	
   

bernoulli	
   markov	
   random	
   field	
   

deep	
   genera.ve	
   model	
   

condi.onal	
   
simula.on	
   

why	
   so	
   di   cult?	
   

28	
   

28	
   

possible	
   images!	
   

p(image|par.al	
   image)	
   

bernoulli	
   markov	
   random	
   field	
   

deep	
   genera.ve	
   model	
   
reuters	
   dataset:	
   804,414	
   	
   
newswire	
   stories:	
   unsupervised	
   

model	
   p(document)	
   

interbank markets

european community 
monetary/economic  

energy markets

leading          
economic         
indicators       

disasters and 
accidents     

legal/judicial

bag	
   of	
   words	
   

accounts/
earnings 

government 
borrowings 

(hinton & salakhutdinov, science 2006)!

talk	
   roadmap	
   

part	
   1:	
   deep	
   networks	
   	
   

      
introduc.on,	
   graphical	
   models.	
   
       restricted	
   boltzmann	
   machines:	
   

learning	
   low-     level	
   features.	
   

       deep	
   belief	
   networks:	
   learning	
   

part-     based	
   hierarchies.	
   

part	
   2:	
   deep	
   boltzmann	
   machines.	
   

      
id136	
   and	
   learning	
   	
   
       advanced	
   deep	
   models	
   

id136	
   problem	
   

      	
   given	
   a	
   dataset	
   
      	
   bayes	
   rule:	
   

likelihood	
   func.on	
   	
   

prior	
   id203	
   of	
   parameters	
   

posterior	
   distribu.on	
   over	
   
parameters	
   

      	
   compu.ng	
   posterior	
   distribu.on	
   is	
   known	
   as	
   id136	
   problem.	
   
however,	
   

      	
   this	
   integral	
   can	
   be	
   very	
   high-     dimensional	
   and	
   di   cult	
   to	
   compute.	
   

predic.on	
   

likelihood	
   func.on	
   	
   

prior	
   id203	
   of	
   parameters	
   

posterior	
   distribu.on	
   over	
   
parameters	
   

      	
   predic0on:	
   given	
   data,	
   compu.ng	
   condi.onal	
   id203	
   of	
   a	
   
	
   	
   new	
   data	
   point	
   x*	
   requires	
   compu.ng	
   the	
   following	
   integral:	
   

	
   which	
   is	
   some.mes	
   called	
   predic0ve	
   distribu0on.	
   	
   
      	
   compu.ng	
   predic.ve	
   distribu.on	
   requires	
   posterior.	
   	
   

computa.onal	
   challenges	
   

      	
   compu.ng	
   marginal	
   likelihoods	
   ojen	
   requires	
   compu.ng	
   very	
   
	
   	
   high-     dimensional	
   integrals.	
   
      	
   compu.ng	
   posterior	
   distribu.ons	
   (and	
   hence	
   predic.ve	
   	
   
	
   	
   distribu.ons)	
   is	
   ojen	
   analy.cally	
   intractable.	
   	
   

      	
   next:	
   graphical	
   models.	
   

graphical	
   models	
   

graphical	
   models:	
   powerful	
   framework	
   for	
   represen.ng	
   dependency	
   
structure	
   between	
   random	
   variables.	
   

a

b

c

      	
   the	
   joint	
   id203	
   distribu.on	
   over	
   a	
   set	
   of	
   
random	
   variables.	
   
      	
   the	
   graph	
   contains	
   a	
   set	
   of	
   nodes	
   (ver.ces)	
   that	
   
represent	
   random	
   variables,	
   and	
   a	
   set	
   of	
   links	
   
(edges)	
   that	
   represent	
   dependencies	
   between	
   
those	
   random	
   variables.	
   	
   

      	
   the	
   joint	
   distribu.on	
   over	
   all	
   random	
   variables	
   decomposes	
   into	
   a	
   product	
   
of	
   factors,	
   where	
   each	
   factor	
   depends	
   on	
   a	
   subset	
   of	
   the	
   variables.	
   	
   	
   
two	
   type	
   of	
   graphical	
   models:	
   	
   

      	
   directed	
   (bayesian	
   networks)	
   	
   
      	
   undirected	
   (markov	
   random	
      elds,	
   boltzmann	
   machines)	
   	
   

hybrid	
   graphical	
   models	
   that	
   combine	
   directed	
   and	
   undirected	
   models,	
   such	
   
as	
   deep	
   belief	
   networks,	
   hierarchical-     deep	
   models.	
   

directed	
   graphical	
   models	
   

directed	
   graphs	
   are	
   useful	
   for	
   expressing	
   causal	
   rela.onships	
   between	
   
random	
   variables.	
   	
   

      	
   the	
   joint	
   distribu.on	
   de   ned	
   by	
   the	
   graph	
   is	
   given	
   
by	
   the	
   product	
   of	
   a	
   condi0onal	
   distribu0on	
   for	
   each	
   
node	
   condi0oned	
   on	
   its	
   parents.	
   

x1

x2

x4

x3

x5

x6

x7

      	
   for	
   example,	
   the	
   joint	
   distribu.on	
   over	
   x1,..,x7	
   
factorizes:	
   	
   	
   

directed	
   acyclic	
   graphs,	
   or	
   dags.	
   

directed	
   graphical	
   models	
   

example:	
   genera.ve	
   model	
   of	
   an	
   image:	
   

      	
   object	
   iden.ty	
   (discrete	
   variable)	
   and	
   the	
   
posi.on	
   and	
   orienta.on	
   (con.nuous	
   variables)	
   
have	
   independent	
   prior	
   probabili0es.	
   	
   
      	
   the	
   image	
   has	
   a	
   id203	
   distribu.on	
   that	
   
depends	
   on	
   the	
   object	
   iden.ty,	
   posi.on,	
   and	
   
orienta.on	
   (likelihood	
   func0on).	
   	
   

the	
   joint	
   distribu.on:	
   

id136:	
   compu.ng	
   posterior:	
   

likelihood	
   

prior	
   

marginal	
   likelihood:	
   ojen	
   di   cult	
   to	
   compute	
   

popular	
   models	
   

latent	
   dirichlet	
   alloca0on	
   

probabilis0c	
   matrix	
   factoriza0on	
   

  

  

z 

w

pr(topic	
   |	
   doc)	
   

  

pr(word	
   |	
   topic)	
   

      	
   one	
   of	
   the	
   popular	
   models	
   for	
   
modeling	
   word	
   count	
   vectors.	
   	
   
we	
   will	
   see	
   this	
   model	
   later.	
   	
   

      	
   one	
   of	
   the	
   popular	
   models	
   for	
   
collabora.ve	
      ltering	
   applica.ons.	
   
part	
   of	
   the	
   winning	
   solu.on	
   in	
   the	
   
neklix	
   contest.	
   	
   

bayesian	
   matrix	
   factoriza.on	
   

      	
   let	
   us	
      rst	
   look	
   at	
   some	
   examples.	
   	
   

      	
   we	
   have	
   n	
   users,	
   m	
   movies,	
   and	
   integer	
   ra.ng	
   values	
   from	
   1	
   to	
   k.	
   
      	
   let	
   rij	
   be	
   the	
   ra.ng	
   of	
   user	
   i	
   for	
   movie	
   j,	
   and	
   u	
   2	
   rd	
      n,	
   and	
   v	
   2	
   rd   m	
   be	
   
latent	
   user	
   and	
   movie	
   feature	
   matrices:	
   !

      	
   our	
   goal	
   is	
   to	
   predict	
   missing	
   values	
   (missing	
   ra.ngs).	
   !

(salakhutdinov & mnih, icml 2008)!

bayesian	
   matrix	
   factoriza.on	
   

      	
   we	
   can	
   de   ne	
   a	
   probabilis.c	
   bilinear	
   model	
   with	
   gaussian	
   observa.on	
   noise:	
   

      	
   we	
   can	
   place	
   gaussian	
   priors	
   over	
   latent	
   variables:	
   

      	
   hierarchical	
   prior:	
   introduce	
   gaussian-     wishart	
   priors	
   over	
   the	
   user	
   and	
   movie	
   
hyper-     parameters:!

bayesian	
   matrix	
   factoriza.on	
   

predic.ve	
   distribu.on	
   
      	
   consider	
   predic.ng	
   a	
   ra.ng	
   rij*	
   for	
   user	
   i	
   and	
   query	
   movie	
   j:	
   	
   	
   

      	
   exact	
   evalua.on	
   of	
   this	
   predic.ve	
   distribu.on	
   is	
   analy.cally	
   intractable.	
   	
   
      	
   posterior	
   distribu.on	
   over	
   parameters	
   and	
   hyper-     parameters	
   is	
   complicated	
   
and	
   does	
   not	
   have	
   a	
   closed-     form	
   expression.	
   	
   
      	
   need	
   to	
   approximate.	
   	
   

      	
   one	
   op.on	
   would	
   be	
   to	
   approximate	
   the	
   posterior	
   using	
   factorized	
   
distribu.on	
   and	
   use	
   varia.onal	
   framework.	
   	
   
      	
   alterna.ve	
   would	
   be	
   to	
   resort	
   to	
   monte	
   carlo	
   methods.	
   	
   

markov	
   random	
   fields	
   

a

c

d

b

      	
   each	
   poten.al	
   func.on	
   is	
   a	
   mapping	
   from	
   joint	
   
con   gura.ons	
   of	
   random	
   variables	
   in	
   a	
   clique	
   to	
   non-     
nega.ve	
   real	
   numbers.	
   
      	
   the	
   choice	
   of	
   poten.al	
   func.ons	
   is	
   not	
   restricted	
   to	
   
having	
   speci   c	
   probabilis.c	
   interpreta.ons.	
   

poten.al	
   func.ons	
   are	
   ojen	
   represented	
   as	
   exponen.als:	
   

where	
   e(x)	
   is	
   called	
   an	
   energy	
   func.on.	
   	
   
      	
   suppose	
   x	
   is	
   a	
   binary	
   random	
   vector	
   with 	
   
      	
   if	
   x	
   is	
   100-     dimensional,	
   we	
   need	
   to	
   sum	
   over

boltzmann	
   distribu.on	
   

	
   
	
   

	
   	
   	
   	
   	
   	
   	
   .	
   	
   

	
   	
   	
   	
   	
   
	
   terms!	
   	
   

compu0ng	
   z	
   is	
   oken	
   very	
   hard.	
   this	
   represents	
   a	
   major	
   limita0on	
   of	
   undirected	
   models.	
   

markov	
   random	
   fields	
   

a

c

d

b

      	
   each	
   poten.al	
   func.on	
   is	
   a	
   mapping	
   from	
   joint	
   
con   gura.ons	
   of	
   random	
   variables	
   in	
   a	
   clique	
   to	
   non-     
nega.ve	
   real	
   numbers.	
   
      	
   the	
   choice	
   of	
   poten.al	
   func.ons	
   is	
   not	
   restricted	
   to	
   
having	
   speci   c	
   probabilis.c	
   interpreta.ons.	
   

poten.al	
   func.ons	
   are	
   ojen	
   represented	
   as	
   exponen.als:	
   

where	
   e(x)	
   is	
   called	
   an	
   energy	
   func.on.	
   	
   

boltzmann	
   distribu.on	
   

compare	
   to	
   compu0ng	
   posterior:	
   

where	
   

maximum	
   likelihood	
   learning	
   

consider	
   binary	
   pairwise	
   mrf:	
   

given	
   a	
   set	
   of	
   i.i.d.	
   training	
   examples	
   	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
model	
   parameters	
   	
   	
   	
   	
   .	
   

	
   

	
   	
   ,	
   	
   we	
   want	
   to	
   learn	
   	
   
	
   

	
   	
   

maximize	
   log-     likelihood	
   objec.ve:	
   

deriva.ve	
   of	
   the	
   log-     likelihood:	
   	
   

di   cult	
   to	
   compute:	
   exponen.ally	
   many	
   	
   
con   gura.ons	
   

mrfs	
   with	
   latent	
   variables	
   

for	
   many	
   interes.ng	
   real-     world	
   problems,	
   we	
   need	
   to	
   introduce	
   hidden	
   
or	
   latent	
   variables.	
   	
   

      	
   our	
   random	
   variables	
   will	
   contain	
   both	
   visible	
   
and	
   hidden	
   variables	
   x=(v,h).	
   	
   
      	
   id203	
   of	
   observed	
   input	
   is	
   given	
   by	
   
marginalizing	
   out	
   the	
   states	
   of	
   hidden	
   variables:	
   

h3

h2

h1

v

w3

w2

w1

      	
   in	
   general	
   compu.ng	
   both	
   par..on	
   func.on	
   
and	
   summa.on	
   over	
   hiddens	
   will	
   be	
   
intractable,	
   except	
   for	
   special	
   cases.	
   	
   
      	
   parameter	
   learning	
   becomes	
   a	
   very	
   
challenging	
   task.	
   

deep	
   networks	
   have	
   to	
   deal	
   with	
   this	
   intractability.	
   	
   

id136	
   problem	
   

      	
   for	
   most	
   situa.ons,	
   we	
   will	
   be	
   interested	
   in	
   evalua.ng	
   expecta.ons	
   (for	
   
example	
   in	
   order	
   to	
   make	
   predic.ons):	
   

where	
   the	
   integral	
   will	
   be	
   replaced	
   with	
   
summa.on	
   in	
   case	
   of	
   discrete	
   variables.	
   	
   	
   

      	
   we	
   will	
   ojen	
   use	
   the	
   following	
   nota.on:	
   	
   
      	
   we	
   can	
   evaluate	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   pointwise	
   but	
   cannot	
   evaluate	
   	
   	
   

-    posterior	
   distribu.on:	
   

-    markov	
   random	
   fields:	
   

varia.onal	
   id136	
   

      	
   approximate	
   intractable	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   simpler,	
   tractable	
   	
   
	
   	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   	
   	
   

id178	
   func.onal	
   	
   	
   

varia.onal	
   lower	
   bound	
   

where	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   a	
   kullback-     leibler	
   divergence	
      	
   a	
   non-     symmetric	
   measure	
   
of	
   the	
   di   erence	
   between	
   two	
   distribu.ons	
   q	
   and	
   p:	
   	
   	
   	
   

varia.onal	
   id136	
   

      	
   approximate	
   intractable	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   simpler,	
   tractable	
   
distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   	
   	
   
      	
   varia.onal	
   lower-     bound:	
   

      	
   the	
   goal	
   of	
   varia.onal	
   id136	
   is	
   to	
   maximize	
   the	
   varia.onal	
   lower-     bound	
   
with	
   respect	
   to	
   approximate	
   q	
   distribu.on,	
   i.e	
   minimize	
   the	
   kl	
   term.	
   	
   	
   

mean-     field	
   approxima.on	
   
      	
   we	
   can	
   choose	
   a	
   fully	
   factorized	
   distribu.on:	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   
	
   	
   this	
   is	
   known	
   as	
   a	
   mean-        eld	
   approxima0on.	
   	
   	
   
      	
   the	
   varia.onal	
   lower-     bound	
   takes	
   form:	
   

      	
   suppose	
   that	
   we	
   keep	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
      xed	
   and	
   maximize	
   the	
   bound	
   w.r.t.	
   all	
   
possible	
   forms	
   for	
   the	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   	
   	
   

mean-     field	
   approxima.on	
   

the	
   original	
   distribu.on	
   (yellow),	
   along	
   
with	
   laplace	
   (red),	
   and	
   varia.onal	
   
(green)	
   approxima.ons.	
   	
   

      	
   by	
   maximizing	
   the	
   bound,	
   we	
   obtain	
   a	
   general	
   form:	
   

      	
   itera.ve	
   procedure:	
   ini.alize	
   all	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   and	
   then	
   iterate	
   through	
   the	
   factors	
   
replacing	
   each	
   in	
   turn	
   with	
   a	
   revised	
   es.mate.	
   	
   	
   
      	
   convergence	
   is	
   guaranteed	
   (see	
   bishop,	
   chapter	
   10).	
   	
   	
   

talk	
   roadmap	
   

part	
   1:	
   deep	
   networks	
   	
   

      
introduc.on,	
   graphical	
   models.	
   
       restricted	
   boltzmann	
   machines:	
   

learning	
   low-     level	
   features.	
   

       deep	
   belief	
   networks:	
   learning	
   

part-     based	
   hierarchies.	
   

part	
   2:	
   deep	
   boltzmann	
   machines.	
   

      
id136	
   and	
   learning	
   	
   
       advanced	
   deep	
   models	
   

restricted	
   boltzmann	
   machines	
   

	
   	
   hidden	
   variables	
   

bipar.te	
   	
   
structure	
   

stochas.c	
   binary	
   visible	
   variables	
   
are	
   connected	
   to	
   stochas.c	
   binary	
   hidden	
   
variables

	
   	
   	
   	
   	
   	
   	
   	
   .	
   	
   

	
   

the	
   energy	
   of	
   the	
   joint	
   con   gura.on:	
   	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

model	
   parameters.	
   

id203	
   of	
   the	
   joint	
   con   gura.on	
   is	
   given	
   by	
   the	
   boltzmann	
   distribu.on:	
   

par..on	
   func.on	
   

poten.al	
   func.ons	
   

markov	
   random	
      elds,	
   boltzmann	
   machines,	
   log-     linear	
   models.	
   

restricted	
   boltzmann	
   machines	
   

	
   	
   hidden	
   variables	
   

bipar.te	
   	
   
structure	
   

product	
   of	
   experts	
   formula.on.	
   

the	
   joint	
   distribu.on	
   is	
   given	
   by:	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

where	
   the	
   undirected	
   edges	
   in	
   the	
   graphical	
   model	
   represent	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   .	
   

marginalizing	
   over	
   the	
   states	
   of	
   hidden	
   variables:	
   

markov	
   random	
      elds,	
   boltzmann	
   machines,	
   log-     linear	
   models.	
   

product	
   of	
   experts	
   

restricted	
   boltzmann	
   machines	
   

	
   	
   hidden	
   variables	
   

bipar.te	
   	
   
structure	
   

restricted:	
   	
   	
   no	
   interac.on	
   between	
   

	
   

	
   

	
   	
   hidden	
   variables	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

inferring	
   the	
   distribu.on	
   over	
   the	
   
hidden	
   variables	
   is	
   easy:	
   

similarly:	
   

factorizes:	
   easy	
   to	
   compute	
   

markov	
   random	
      elds,	
   boltzmann	
   machines,	
   log-     linear	
   models.	
   

learning	
   features	
   

observed	
   	
   data	
   	
   

subset	
   of	
   25,000	
   characters	
   

learned	
   w:	
   	
      edges   	
   
subset	
   of	
   1000	
   features	
   

new	
   image:	
   

=	
   

represent:	
   

as	
   

logis.c	
   func.on:	
   suitable	
   for	
   
modeling	
   binary	
   images	
   

most	
   hidden	
   	
   
variables	
   are	
   o   	
   

   .	
   

model	
   learning	
   

	
   	
   hidden	
   variables	
   

given	
   a	
   set	
   of	
   i.i.d.	
   training	
   examples	
   	
   

	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   we	
   want	
   to	
   learn	
   	
   

model	
   parameters

	
   

	
   

	
   	
   	
   	
   	
   	
   .	
   	
   	
   	
   

maximize	
   (penalized)	
   log-     likelihood	
   objec.ve:	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

deriva.ve	
   of	
   the	
   log-     likelihood:	
   

regulariza.on	
   	
   

di   cult	
   to	
   compute:	
   exponen.ally	
   many	
   	
   
con   gura.ons	
   

model	
   learning	
   

	
   	
   hidden	
   variables	
   

given	
   a	
   set	
   of	
   i.i.d.	
   training	
   examples	
   	
   

	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   we	
   want	
   to	
   learn	
   	
   

model	
   parameters

	
   

	
   

	
   	
   	
   	
   	
   	
   .	
   	
   	
   	
   

maximize	
   (penalized)	
   log-     likelihood	
   objec.ve:	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

deriva.ve	
   of	
   the	
   log-     likelihood:	
   

approximate	
   maximum	
   likelihood	
   learning:	
   	
   
contras.ve	
   divergence	
   (hinton	
   2000)
mcmc-     id113	
   es.mator	
   (geyer	
   1991)	
   	
   
	
   
tempered	
   mcmc	
   
	
   
	
   
(salakhutdinov,	
   nips	
   2009)

	
   
	
   

	
   

	
   	
   	
   	
   	
   pseudo	
   likelihood	
   (besag	
   1977)
	
   	
   	
   	
   	
   composite	
   likelihoods	
   (lindsay,	
   1988;	
   varin	
   2008)	
   	
   
	
   	
   	
   	
   	
   adap.ve	
   mcmc	
   	
   
	
   	
   	
   	
   	
   (salakhutdinov,	
   icml	
   2010)	
   

	
   	
   

contras.ve	
   divergence	
   

run	
   markov	
   chain	
   for	
   a	
   few	
   steps	
   (e.g.	
   one	
   step):	
   

   	
   

data	
   

reconstructed	
   data	
   

update	
   model	
   parameters:	
   

hinton, neural computation 2002!

rbms	
   for	
   images	
   

gaussian-     bernoulli	
   rbm:	
   	
   

de   ne	
   energy	
   func.ons	
   for	
   	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   various	
   data	
   modali.es:	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

gaussian	
   

bernoulli	
   

(salakhutdinov & hinton, nips 2007)!

rbms	
   for	
   images	
   

gaussian-     bernoulli	
   rbm:	
   	
   

image	
   	
   	
   	
   	
   	
   visible	
   variables	
   

	
   where	
   

interpreta.on:	
   mixture	
   of	
   exponen.al	
   
number	
   of	
   gaussians	
   

	
   is	
   an	
   implicit	
   prior,	
   and	
   	
   

gaussian	
   

rbms	
   for	
   images	
   and	
   text	
   

images:	
   gaussian-     bernoulli	
   rbm	
   

4	
   million	
   unlabelled	
   images	
   

learned	
   features	
   (out	
   of	
   10,000)	
   

text:	
   mul.nomial-     bernoulli	
   rbm	
   

reuters	
   dataset:	
   
804,414	
   unlabeled	
   
newswire	
   stories	
   
bag-     of-     words	
   	
   

learned	
   features:	
   ``topics      	
   

russian	
   
russia	
   
moscow	
   
yeltsin	
   
soviet	
   

clinton	
   
house	
   
president	
   
bill	
   
congress	
   

computer	
   
system	
   
product	
   
sojware	
   
develop	
   

trade	
   
country	
   
import	
   
world	
   
economy	
   

stock	
   
wall	
   
street	
   
point	
   
dow	
   

(salakhutdinov & hinton sigir 2007, nips 2010)!

speech	
   

learned	
      rst-     layer	
   bases	
   

lee	
   et.al.,	
   nips	
   2009	
   

comparison	
   of	
   bases	
   to	
   phonemes	
   

   oy   	
   

   el   	
   

   s   	
   

	
   

e
m
e
n
o
h
p

	
   
s
e
s
a
b
	
   
r
e
y
a

l
	
   
t
s
r
i

f

slide	
   credit:	
   honglak	
   lee	
   

collabora.ve	
   filtering	
   

bernoulli	
   hidden:	
   user	
   preferences	
   

h

w1

v

mul.nomial	
   visible:	
   user	
   ra.ngs	
   

neklix	
   dataset:	
   	
   
480,189	
   users	
   	
   
17,770	
   movies	
   	
   
over	
   100	
   million	
   ra.ngs	
   

learned	
   features:	
   ``genre      	
   

fahrenheit	
   9/11	
   
bowling	
   for	
   columbine	
   
the	
   people	
   vs.	
   larry	
   flynt	
   
canadian	
   bacon	
   
la	
   dolce	
   vita	
   

independence	
   day	
   
the	
   day	
   ajer	
   tomorrow	
   
con	
   air	
   
men	
   in	
   black	
   ii	
   
men	
   in	
   black	
   

friday	
   the	
   13th	
   
the	
   texas	
   chainsaw	
   massacre	
   
children	
   of	
   the	
   corn	
   
child's	
   play	
   
the	
   return	
   of	
   michael	
   myers	
   

scary	
   movie	
   
naked	
   gun	
   	
   
hot	
   shots!	
   
american	
   pie	
   	
   
police	
   academy	
   

state-     of-     the-     art	
   performance	
   	
   
on	
   the	
   neklix	
   dataset.	
   	
   
relates	
   to	
   probabilis0c	
   matrix	
   factoriza0on	
   

(salakhutdinov & mnih icml 2007)!

mul.ple	
   applica.on	
   domains	
   

       natural	
   images	
   
       text/documents	
   
       collabora.ve	
   filtering	
   /	
   matrix	
   factoriza.on	
   	
   
       video	
   (langford,	
   salakhutdinov	
   and	
   zhang,	
   icml	
   2009)	
   	
   
       mo.on	
   capture	
   (taylor	
   et.al.	
   nips	
   2007)	
   
       speech	
   percep.on	
   (dahl	
   et.	
   al.	
   nips	
   2010,	
   lee	
   et.al.	
   nips	
   2010)	
   

same	
   learning	
   algorithm	
   -     -     	
   	
   	
   	
   

	
   	
   	
   
	
   mul.ple	
   input	
   domains. 

	
   

	
   

	
   

	
   

	
   

limita.ons	
   on	
   the	
   types	
   of	
   structure	
   that	
   can	
   be	
   
represented	
   by	
   a	
   single	
   layer	
   of	
   low-     level	
   features!	
   

talk	
   roadmap	
   

part	
   1:	
   deep	
   networks	
   	
   

      
introduc.on,	
   graphical	
   models.	
   
       restricted	
   boltzmann	
   machines:	
   

learning	
   low-     level	
   features.	
   

       deep	
   belief	
   networks:	
   learning	
   

part-     based	
   hierarchies.	
   

part	
   2:	
   deep	
   boltzmann	
   machines.	
   

      
id136	
   and	
   learning	
   	
   
       advanced	
   deep	
   models	
   

deep	
   belief	
   network	
   	
   

low-     level	
   features:	
   
edges	
   

built	
   from	
   unlabeled	
   inputs.	
   	
   

image	
   

(hinton et.al. neural computation 2006)!

input:	
   pixels	
   

deep	
   belief	
   network	
   

internal	
   representa.ons	
   capture	
   
higher-     order	
   sta.s.cal	
   structure	
   

higher-     level	
   features:	
   
combina.on	
   of	
   edges	
   

low-     level	
   features:	
   
edges	
   

built	
   from	
   unlabeled	
   inputs.	
   	
   

image	
   

(hinton et.al. neural computation 2006)!

input:	
   pixels	
   

deep	
   belief	
   network	
   

deep	
   belief	
   network	
   

the	
   joint	
   id203	
   

distribu.on	
   factorizes:	
   

w3

rbm	
   

w2

w1

sigmoid	
   	
   
belief	
   	
   
network	
   

sigmoid	
   belief	
   	
   
network	
   

rbm	
   

h3

h2

h1

v

deep	
   belief	
   network	
   

deep	
   belief	
   network	
   

the	
   joint	
   id203	
   

distribu.on	
   factorizes:	
   

h3

h2

h1

v

w3

rbm	
   

w2

w1

sigmoid	
   	
   
belief	
   	
   
network	
   

layerwise	
   pretraining:	
   
       learn	
   and	
   freeze	
   1st	
   layer	
   rbm	
   
       treat	
   inferred	
   values	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

as	
   the	
   data	
   for	
   training	
   2nd-     
layer	
   rbm.	
   

       learn	
   and	
   freeze	
   2nd	
   layer	
   

rbm.	
   

       proceed	
   to	
   the	
   next	
   layer.	
   

unsupervised	
   feature	
   learning.	
   

layerwise	
   pretraining	
   

deep	
   belief	
   network	
   

h3

h2

h1

v

w3

w2

w1

e   cient	
   layer-     wise	
   pretraining	
   
algorithm.	
   

varia.onal	
   lower	
   bound	
   

likelihood	
   term	
   

id178	
   func.onal	
   

similar	
   arguments	
   for	
   pretraining	
   a	
   
deep	
   boltzmann	
   machine	
   

replace	
   with	
   a	
   	
   
second	
   layer	
   rbm	
   

layerwise	
   pretraining	
   

deep	
   belief	
   network	
   

h3

h2

h1

v

w3

w2

w1

e   cient	
   layer-     wise	
   pretraining	
   
algorithm.	
   

varia.onal	
   lower	
   bound	
   

layerwise	
   pretraining	
   	
   
improves	
   	
   varia.onal	
   	
   
lower	
   bound	
   

likelihood	
   term	
   

id178	
   func.onal	
   

similar	
   arguments	
   for	
   pretraining	
   a	
   
deep	
   boltzmann	
   machine	
   

replace	
   with	
   a	
   	
   
second	
   layer	
   rbm	
   

dbns	
   for	
   classi   ca.on	
   

softmax output

10

t
w
4
2000
t
w
3
500
t
w
2
500
t
w
1

10

2000

500

500

w +(cid:161)

t
4

w +(cid:161)

t
3

w +(cid:161)

t
2

w +(cid:161)

t
1

4

3

2

1

2000
w
3
500

500

500

500

w
2

w

1

rbm

rbm

rbm

pretraining

unrolling

fine(cid:239)tuning

      	
   ajer	
   layer-     by-     layer	
   unsupervised	
   pretraining,	
   discrimina.ve	
      ne-     tuning	
   	
   
by	
   backpropaga.on	
   achieves	
   an	
   error	
   rate	
   of	
   1.2%	
   on	
   mnist.	
   id166   s	
   get	
   
1.4%	
   and	
   randomly	
   ini.alized	
   backprop	
   gets	
   1.6%.	
   	
   
      	
   clearly	
   unsupervised	
   learning	
   helps	
   generaliza.on.	
   it	
   ensures	
   that	
   most	
   of	
   
the	
   informa.on	
   in	
   the	
   weights	
   comes	
   from	
   modeling	
   the	
   input	
   data.	
   

(hinton and salakhutdinov, science 2006)!

dbns	
   for	
   regression	
   

predic.ng	
   the	
   orienta.on	
   of	
   a	
   face	
   patch	
   

training	
   data:	
   1000	
   face	
   patches	
   of	
   	
   
30	
   training	
   people.	
   
regression	
   task:	
   predict	
   orienta.on	
   of	
   a	
   new	
   face.	
   

test	
   data:	
   1000	
   face	
   patches	
   of	
   	
   
10	
   new	
   people.	
   	
   

gaussian	
   processes	
   with	
   spherical	
   gaussian	
   kernel	
   achieves	
   a	
   rmse	
   	
   	
   
(root	
   mean	
   squared	
   error)	
   of	
   16.33	
   degree.	
   	
   

(salakhutdinov and hinton, nips 2007)!

dbns	
   for	
   regression	
   

addi0onal	
   unlabeled	
   training	
   data:	
   12000	
   face	
   patches	
   from	
   30	
   
training	
   people.	
   
      	
   pretrain	
   a	
   stack	
   of	
   rbms:	
   784-     1000-     1000-     1000.	
   
      	
   features	
   were	
   extracted	
   with	
   no	
   idea	
   of	
   the	
      nal	
   task.	
   

the	
   same	
   gp	
   on	
   the	
   top-     level	
   features: 	
   
	
   
gp	
   with	
      ne-     tuned	
   covariance	
   gaussian	
   kernel: 	
   
standard	
   gp	
   without	
   using	
   dbns:
	
   

	
   

	
   

	
   

	
   

	
   
	
   
	
   

	
   rmse:	
   11.22	
   
	
   rmse:	
   6.42	
   
	
   rmse:	
   16.33	
   

deep	
   autoencoders	
   

)4c
%&'

%&'

%&'

6$

!

($$

*

($$

!
"$$$

6

"$$$
!
#$$$

#

#$$$
!

"

b-=4>-,

)
"

)
#

)
6

)
*
?4>-@5/a-,
*

!
#$$$
!
"$$$
!

($$

!

6$

!

($$

!
"$$$
!
#$$$
!

6

#

"

#$$$

"$$$

! !(cid:161)

)
"

! !(cid:161)

)
#

! !(cid:161)

)
6

;

:

9

($$

!

6$

)
*

!(cid:161)

(

! !(cid:161)

*

($$

! !(cid:161)

6

"$$$

! !(cid:161)

#

#$$$

*

6

#

! !(cid:161)

"

"

%&'

<1=4>-,

+,-.,/01012

31,455012

701-(cid:239).81012

informa.on	
   retrieval	
   

interbank markets

european community 
monetary/economic  

2-     d	
   lsa	
   space	
   

energy markets

leading          
economic         
indicators       

disasters and 
accidents     

legal/judicial

accounts/
earnings 

government 
borrowings 

      	
   the	
   reuters	
   corpus	
   volume	
   ii	
   contains	
   804,414	
   newswire	
   stories	
   
(randomly	
   split	
   into	
   402,207	
   training	
   and	
   402,207	
   test).	
   
      	
      bag-     of-     words   	
   representa.on:	
   each	
   ar.cle	
   is	
   represented	
   as	
   a	
   vector	
   containing	
   
the	
   counts	
   of	
   the	
   most	
   frequently	
   used	
   2000	
   words	
   in	
   the	
   training	
   set.	
   

(hinton and salakhutdinov, science 2006)!

informa.on	
   retrieval	
   

reuters	
   dataset	
   

deep generative model
latent sematic analysis
id44

 

reuters	
   dataset:	
   804,414	
   	
   
newswire	
   stories.	
   
deep	
   genera.ve	
   model	
   signi   cantly	
   
outperforms	
   lsa	
   and	
   lda	
   topic	
   models	
   

0.1       0.4       1.6       6.4        25       100 

recall (%) 

)

%

(
 
n
o
i
s
i
c
e
r
p

50

40

30

20

10

 

seman.c	
   hashing	
   

address space

european community 
monetary/economic

disasters and 
accidents

semantically
similar
documents

semantic
hashing
function

document 

energy markets

government 
borrowing

accounts/earnings

      	
   learn	
   to	
   map	
   documents	
   into	
   seman0c	
   20-     d	
   binary	
   codes.	
   
      	
   retrieve	
   similar	
   documents	
   stored	
   at	
   the	
   nearby	
   addresses	
   with	
   no	
   
search	
   at	
   all.	
   

(salakhutdinov and hinton, sigir 2007)!

searching	
   large	
   image	
   database	
   

using	
   binary	
   codes	
   

      	
   map	
   images	
   into	
   binary	
   codes	
   for	
   fast	
   retrieval.	
   

      	
   small	
   codes,	
   torralba,	
   fergus,	
   weiss,	
   cvpr	
   2008	
   
      	
   spectral	
   hashing,	
   y.	
   weiss,	
   a.	
   torralba,	
   r.	
   fergus,	
   nips	
   2008	
   
      	
   kulis	
   and	
   darrell,	
   nips	
   2009,	
   gong	
   and	
   lazebnik,	
   cvpr	
   20111	
   
      	
   norouzi	
   and	
   fleet,	
   icml	
   2011,	
   	
   

learning	
   similarity	
   measures	
   

1

7

4

2

6

0

learning similarity metric 

9

8

a

y

30

w
2000
w

d[y  ,y  ]
b

a

4

3

w

2

w

1

3

5

500

500

a
x

30

w
2000
w

b

y

4

3

w

2

w

1

500

500

b
x

      	
   learn	
   a	
   nonlinear	
   transforma.on	
   of	
   the	
   input	
   space.	
   
      	
   op.mize	
   to	
   make	
   knn	
   perform	
   well	
   in	
   the	
   low-     dimensional	
   feature	
   
space	
   

(salakhutdinov and hinton, ai and statistics 2007)!

compare	
   to	
   other	
   approaches	
   

learning similarity metric 

1

7

4

2

6

0

9

8

a

y

30

w
2000
w

d[y  ,y  ]
b

a

4

3

w

2

w

1

3

5

500

500

a
x

b

y

30

w
2000
w

500

w

500

w

4

3

2

1

b
x

pca	
   

neighborhood	
   component	
   
analysis	
   

linear	
   discriminant	
   	
   
analysis	
   

talk	
   roadmap	
   

part	
   1:	
   deep	
   networks	
   	
   

      
introduc.on,	
   graphical	
   models.	
   
       restricted	
   boltzmann	
   machines:	
   

learning	
   low-     level	
   features.	
   

       deep	
   belief	
   networks:	
   learning	
   

part-     based	
   hierarchies.	
   

part	
   2:	
   deep	
   boltzmann	
   machines.	
   

      
id136	
   and	
   learning	
   	
   
       advanced	
   deep	
   models	
   

dbns	
   vs.	
   dbms	
   

deep belief network!
h3

deep id82!
h3

h2

h1

v

w3

w2

w1

h2

h1

v

w3

w2

w1

dbns	
   are	
   hybrid	
   models:	
   	
   

      	
   id136	
   in	
   dbns	
   is	
   problema.c	
   due	
   to	
   explaining	
   away.	
   
      	
   only	
   greedy	
   pretrainig,	
   no	
   joint	
   op0miza0on	
   over	
   all	
   layers.	
   	
   
      	
   approximate	
   id136	
   is	
   feed-     forward:	
   no	
   bozom-     up	
   and	
   top-     down.	
   	
   	
   	
   

introduce	
   a	
   new	
   class	
   of	
   models	
   called	
   deep	
   boltzmann	
   machines.	
   

mathema.cal	
   formula.on	
   

deep	
   boltzmann	
   machine	
   

model	
   parameters	
   

h3

h2

h1

v

       dependencies	
   between	
   hidden	
   variables.	
   
       all	
   connec.ons	
   are	
   undirected.	
   
       bozom-     up	
   and	
   top-     down:	
   

w3

w2

w1

bozom-     up	
   

top-     down	
   

input	
   

unlike	
   many	
   exis.ng	
   feed-     forward	
   models:	
   convnet	
   (lecun),	
   
hmax	
   (poggio	
   et.al.),	
   deep	
   belief	
   nets	
   (hinton	
   et.al.)	
   

mathema.cal	
   formula.on	
   

deep	
   boltzmann	
   machine	
   

       condi.onal	
   distribu.ons:	
   

h3

h2

h1

v

w3

w2

w1

input	
   

       note	
   that	
   exact	
   computa.on	
   of	
   	
   	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   is	
   intractable.	
   	
   

mathema.cal	
   formula.on	
   

deep	
   boltzmann	
   machine	
   

neural	
   network	
   	
   

output	
   

deep	
   belief	
   network	
   

h3

h2

h1

v

h3

w3

h2

w2

h1

w1

v

h3
w3
h2
w2
h1
w1
v

w3

w2

w1

input	
   

unlike	
   many	
   exis.ng	
   feed-     forward	
   models:	
   convnet	
   (lecun),	
   
hmax	
   (poggio),	
   deep	
   belief	
   nets	
   (hinton)	
   

mathema.cal	
   formula.on	
   

deep	
   boltzmann	
   machine	
   

neural	
   network	
   	
   

output	
   

deep	
   belief	
   network	
   

h3

h2

h1

v

h3

w3

h2

w2

h1

w1

v

h3
w3
h2
w2
h1
w1
v

w3

i

w2

n
f
e
r
e
n
c
e

	
   

w1

input	
   

unlike	
   many	
   exis.ng	
   feed-     forward	
   models:	
   convnet	
   (lecun),	
   
hmax	
   (poggio),	
   deep	
   belief	
   nets	
   (hinton)	
   

mathema.cal	
   formula.on	
   

deep	
   boltzmann	
   machine	
   

model	
   parameters	
   

h3

h2

h1

v

w3

w2

w1

       dependencies	
   between	
   hidden	
   variables.	
   

maximum	
   likelihood	
   learning:	
   

problem:	
   both	
   expecta.ons	
   are	
   

intractable!	
   

learning	
   rule	
   for	
   undirected	
   graphical	
   models:	
   	
   
mrfs,	
   crfs,	
   factor	
   graphs.	
   	
   

previous	
   work	
   

many	
   approaches	
   for	
   learning	
   boltzmann	
   machines	
   have	
   been	
   
proposed	
   over	
   the	
   last	
   20	
   years:	
   
      	
   hinton	
   and	
   sejnowski	
   (1983),	
   
      	
   peterson	
   and	
   anderson	
   (1987)	
   
      	
   galland	
   (1991)	
   	
   
      	
   kappen	
   and	
   rodriguez	
   (1998)	
   
      	
   lawrence,	
   bishop,	
   and	
   jordan	
   (1998)	
   
      	
   tanaka	
   (1998)	
   	
   
      	
   welling	
   and	
   hinton	
   (2002)	
   	
   
      	
   zhu	
   and	
   liu	
   (2002)	
   
      	
   welling	
   and	
   teh	
   (2003)	
   
      	
   yasuda	
   and	
   tanaka	
   (2009)	
   	
   	
   

real-     world	
   applica.ons	
      	
   thousands	
   	
   
of	
   hidden	
   and	
   observed	
   variables	
   
with	
   millions	
   of	
   parameters.	
   

many	
   of	
   the	
   previous	
   approaches	
   were	
   not	
   successful	
   for	
   learning	
   
general	
   boltzmann	
   machines	
   with	
   hidden	
   variables.	
   

algorithms	
   based	
   on	
   contras.ve	
   divergence,	
   score	
   matching,	
   pseudo-     
likelihood,	
   composite	
   likelihood,	
   mcmc-     id113,	
   piecewise	
   learning,	
   cannot	
   
handle	
   mul.ple	
   layers	
   of	
   hidden	
   variables.	
   	
   	
   

new	
   learning	
   algorithm	
   

posterior	
   id136	
   

condi.onal	
   

approximate	
   
condi.onal	
   

simulate	
   from	
   the	
   model	
   
uncondi.onal	
   

approximate	
   the	
   
joint	
   distribu.on	
   

(salakhutdinov, 2008; nips 2009)!

new	
   learning	
   algorithm	
   

posterior	
   id136	
   

condi.onal	
   

approximate	
   
condi.onal	
   

simulate	
   from	
   the	
   model	
   
uncondi.onal	
   

approximate	
   the	
   
joint	
   distribu.on	
   

data-     dependent	
   

data-     independent	
   

density	
   

match	
   	
   

new	
   learning	
   algorithm	
   

posterior	
   id136	
   

condi.onal	
   

approximate	
   
condi.onal	
   

mean-     field	
   

simulate	
   from	
   the	
   model	
   
uncondi.onal	
   

approximate	
   the	
   
joint	
   distribu.on	
   

markov	
   chain	
   
monte	
   carlo	
   

data-     dependent	
   

data-     independent	
   

match	
   	
   

key	
   idea	
   of	
   our	
   approach:	
   
data-     dependent:	
   	
   	
   	
   	
   varia0onal	
   id136,	
   mean-        eld	
   theory	
   
data-     independent:	
   	
   stochas0c	
   approxima0on,	
   mcmc	
   based	
   

sampling	
   from	
   dbms	
   

sampling	
   from	
   two-     hidden	
   layer	
   dbm	
   by	
   running	
   a	
   markov	
   chain:	
   

randomly	
   
ini.alize	
   

   	
   

sample	
   

stochas.c	
   approxima.on	
   

time	
   	
   t=1	
   

t=2	
   

t=3	
   

h2

h1

v

update	
   

h2

h1

v

update	
   

h2

h1

v

update	
   	
   	
   	
   	
   	
   and	
   	
   	
   	
   	
   	
   sequen.ally,	
   	
   where	
   

       generate

	
   

	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   by	
   simula.ng	
   from	
   a	
   markov	
   chain	
   

that	
   leaves	
   	
   	
   	
   	
   	
   	
   	
   	
   invariant	
   (e.g.	
   gibbs	
   or	
   m-     h	
   sampler)	
   

       update 	
   	
   	
   	
   	
   	
   by	
   replacing	
   intractable	
   	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   with	
   a	
   point	
   

es.mate	
   	
   

in	
   prac.ce	
   we	
   simulate	
   several	
   markov	
   chains	
   in	
   parallel.	
   

robbins	
   and	
   monro,	
   ann.	
   math.	
   stats,	
   1957	
   
	
   l.	
   younes,	
   	
   id203	
   theory	
   1989	
   

stochas.c	
   approxima.on	
   

update	
   rule	
   decomposes:	
   

true	
   gradient	
   

noise	
   term	
   

almost	
   sure	
   convergence	
   guarantees	
   as	
   learning	
   rate	
   	
   	
   

problem:	
   high-     dimensional	
   data:	
   
the	
   energy	
   landscape	
   is	
   highly	
   	
   
mul.modal	
   

markov	
   chain	
   
monte	
   carlo	
   

key	
   insight:	
   the	
   transi.on	
   operator	
   can	
   be	
   	
   
any	
   valid	
   transi.on	
   operator	
      	
   tempered	
   	
   
transi.ons,	
   parallel/simulated	
   tempering.	
   

connec.ons	
   to	
   the	
   theory	
   of	
   stochas.c	
   approxima.on	
   and	
   adap.ve	
   mcmc.	
   

varia.onal	
   id136	
   

approximate	
   intractable	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   simpler,	
   tractable	
   
distribu.on 	
   

	
   	
   	
   	
   	
   :	
   

posterior	
   id136	
   

mean-     field	
   

varia.onal	
   lower	
   bound	
   

minimize	
   kl	
   between	
   approxima.ng	
   and	
   true	
   
distribu.ons	
   with	
   respect	
   to	
   varia.onal	
   parameters	
   	
   	
   	
   	
   .	
   	
   

(salakhutdinov, 2008; salakhutdinov & larochelle, ai & statistics 2010)!

varia.onal	
   id136	
   

approximate	
   intractable	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   simpler,	
   tractable	
   
distribu.on 	
   

	
   	
   	
   	
   	
   :	
   

posterior	
   id136	
   

mean-     field	
   

varia.onal	
   lower	
   bound	
   

mean-     field:	
   choose	
   a	
   fully	
   factorized	
   distribu.on:	
   

with	
   

varia0onal	
   id136:	
   maximize	
   the	
   lower	
   bound	
   w.r.t.	
   
varia.onal	
   parameters	
   	
   	
   	
   	
   .	
   	
   

nonlinear	
      xed-     	
   
point	
   equa.ons:	
   

varia.onal	
   id136	
   

approximate	
   intractable	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   simpler,	
   tractable	
   
distribu.on 	
   

	
   	
   	
   	
   	
   :	
   

posterior	
   id136	
   

mean-     field	
   

varia.onal	
   lower	
   bound	
   

uncondi.onal	
   simula.on	
   

1.	
   varia0onal	
   id136:	
   maximize	
   the	
   lower	
   	
   
bound	
   w.r.t.	
   varia.onal	
   parameters	
   	
   	
   	
   	
   	
   

markov	
   chain	
   
monte	
   carlo	
   

2.	
   mcmc:	
   apply	
   stochas.c	
   approxima.on	
   	
   
to	
   update	
   model	
   parameters	
   	
   	
   	
   	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

almost	
   sure	
   convergence	
   guarantees	
   to	
   an	
   asympto.cally	
   
stable	
   point.	
   

varia.onal	
   id136	
   

approximate	
   intractable	
   distribu.on	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   with	
   simpler,	
   tractable	
   
distribu.on 	
   

	
   	
   	
   	
   	
   :	
   

posterior	
   id136	
   

mean-     field	
   

varia.onal	
   lower	
   bound	
   

uncondi.onal	
   simula.on	
   

markov	
   chain	
   
monte	
   carlo	
   

1.	
   varia0onal	
   id136:	
   maximize	
   the	
   lower	
   	
   
bound	
   w.r.t.	
   varia.onal	
   parameters	
   	
   	
   	
   	
   	
   

fast	
   id136	
   
learning	
   can	
   scale	
   to	
   
millions	
   of	
   examples	
   

2.	
   mcmc:	
   apply	
   stochas.c	
   approxima.on	
   	
   
to	
   update	
   model	
   parameters	
   	
   	
   	
   	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

almost	
   sure	
   convergence	
   guarantees	
   to	
   an	
   asympto.cally	
   
stable	
   point.	
   

good	
   genera.ve	
   model?	
   

handwrizen	
   characters	
   

good	
   genera.ve	
   model?	
   

handwrizen	
   characters	
   

good	
   genera.ve	
   model?	
   

handwrizen	
   characters	
   

simulated	
   

real	
   data	
   

good	
   genera.ve	
   model?	
   

handwrizen	
   characters	
   

real	
   data	
   

simulated	
   

good	
   genera.ve	
   model?	
   

handwrizen	
   characters	
   

good	
   genera.ve	
   model?	
   

mnist	
   handwrizen	
   digit	
   dataset	
   

handwri.ng	
   recogni.on	
   

mnist	
   dataset	
   

60,000	
   examples	
   of	
   10	
   digits	
   
learning	
   algorithm	
   
logis.c	
   regression	
   
k-     nn	
   	
   
neural	
   net	
   (plaz	
   2005)	
   
id166	
   (decoste	
   et.al.	
   2002)	
   
deep	
   autoencoder	
   
(bengio	
   et.	
   al.	
   2007)	
   	
   
deep	
   belief	
   net	
   
(hinton	
   et.	
   al.	
   2006)	
   	
   
dbm	
   	
   

error	
   
12.0%	
   
3.09%	
   
1.53%	
   
1.40%	
   
1.40%	
   

0.95%	
   

1.20%	
   

op.cal	
   character	
   recogni.on	
   

42,152	
   examples	
   of	
   26	
   english	
   lezers	
   	
   

learning	
   algorithm	
   
logis.c	
   regression	
   
k-     nn	
   	
   
neural	
   net	
   
id166	
   (larochelle	
   et.al.	
   2009)	
   
deep	
   autoencoder	
   
(bengio	
   et.	
   al.	
   2007)	
   	
   
deep	
   belief	
   net	
   
(larochelle	
   et.	
   al.	
   2009)	
   	
   
dbm	
   

error	
   
22.14%	
   
18.92%	
   
14.62%	
   
9.70%	
   
10.05%	
   

9.68%	
   

8.40%	
   

permuta.on-     invariant	
   version.	
   

genera.ve	
   model	
   of	
   3-     d	
   objects	
   

24,000	
   examples,	
   5	
   object	
   categories,	
   5	
   di   erent	
   objects	
   within	
   each	
   	
   
category,	
   6	
   lightning	
   condi.ons,	
   9	
   eleva.ons,	
   18	
   azimuths.	
   	
   	
   

3-     d	
   object	
   recogni.on	
   

pazern	
   comple.on	
   

learning	
   algorithm	
   
logis.c	
   regression	
   
k-     nn	
   (lecun	
   2004)	
   
id166	
   (bengio	
   &	
   lecun	
   	
   2007)	
   
deep	
   belief	
   net	
   (nair	
   &	
   
hinton	
   	
   2009)	
   	
   
dbm	
   

error	
   
22.5%	
   
18.92%	
   
11.6%	
   
9.0%	
   

7.2%	
   

permuta.on-     invariant	
   version.	
   

spoken	
   query	
   detec.on	
   

       630	
   speaker	
   timit	
   corpus:	
   3,696	
   training	
   and	
   944	
   test	
   uzerances.	
   
       10	
   query	
   keywords	
   were	
   randomly	
   selected	
   and	
   10	
   examples	
   of	
   

each	
   keyword	
   were	
   extracted	
   from	
   the	
   training	
   set.	
   	
   

       goal:	
   for	
   each	
   keyword,	
   rank	
   all	
   944	
   uzerances	
   based	
   on	
   the	
   

uzerance   s	
   id203	
   of	
   containing	
   that	
   keyword.	
   	
   	
   

       performance	
   measure:	
   the	
   average	
   equal	
   error	
   rate	
   (eer).	
   	
   

learning	
   algorithm	
   
gmm	
   unsupervised	
   	
   
dbm	
   unsupervised	
   	
   
dbm	
   (1%	
   labels)	
   
dbm	
   (30%	
   labels)	
   
dbm	
   (100%	
   labels)	
   

avg	
   eer	
   
16.4%	
   
14.7%	
   
13.3%	
   
10.5%	
   
9.7%	
   

13.5

13

12.5

12

11.5

11

10.5

10

r
e
e

 
.

g
v
a

9.5

0

0.1

0.2

0.3

0.5

0.4
0.6
training ratio

0.7

0.8

0.9

1

(yaodong	
   zhang	
   et.al.	
   	
   icassp	
   2012)!

robust	
   boltzmann	
   machines	
   

      	
   build	
   more	
   complex	
   models	
   that	
   can	
   deal	
   with	
   occlusions	
   or	
   structured	
   
noise.	
   	
   
binary	
   rbm	
   modeling	
   

gaussian	
   rbm,	
   modeling	
   

clean	
   faces	
   

occlusions	
   

i

n
f
e
r
r
e
d

	
   

binary	
   pixel-     wise	
   

mask	
   

gaussian	
   noise	
   

observed	
   

relates	
   to	
   le	
   roux,	
   heess,	
   shozon,	
   and	
   winn,	
   
neural	
   computa.on,	
   2011	
   
eslami,	
   heess,	
   winn,	
   cvpr	
   2012	
   

tang,	
   salakhutdinov,	
   and	
   hinton,	
   cvpr	
   2012	
   

robust	
   boltzmann	
   machines	
   

internal	
   states	
   of	
   
robm	
   during	
   
learning.	
   	
   

id136	
   on	
   the	
   test	
   
subjects	
   

comparing	
   to	
   other	
   
denoising	
   algorithms	
   

deep	
   lamber.an	
   network	
   

consider	
   more	
   complex	
   models:	
   undirected	
   +	
   directed	
   models.	
   

deep	
   lamber.an	
   net	
   	
   

deep	
   
undirected	
   

i

n
f
e
r
r
e
d

	
   

directed	
   

observed	
   

combines	
   the	
   elegant	
   proper.es	
   of	
   the	
   lamber.an	
   model	
   with	
   the	
   gaussian	
   
rbms	
   (and	
   deep	
   belief	
   nets,	
   deep	
   boltzmann	
   machines).	
   

tang,	
   salakhutdinov,	
   and	
   hinton,	
   icml	
   2012	
   

lamber.an	
   re   ectance	
   model	
   

      	
   a	
   simple	
   model	
   of	
   the	
   image	
   forma.on	
   process.	
   
      	
   albedo	
   is	
   the	
   di   use	
   re   ec.vity	
   of	
   a	
   surface,	
   material	
   dependent,	
   
illumina.on	
   independent	
   
      	
   images	
   with	
   di   erent	
   illumina.on	
   can	
   be	
   generated	
   by	
   varying	
   light	
   
direc.ons	
   

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:2)(cid:9)(cid:10)(cid:11)(cid:5)(cid:12)(cid:5)(cid:13)(cid:7)(cid:2)(cid:9)(cid:13)(cid:5)(cid:10)(cid:14)(cid:15)(cid:16)(cid:5)(cid:17)

(cid:18)(cid:8)(cid:5)(cid:19)(cid:5)(cid:6)

deep	
   lamber.an	
   networks	
   

model	
   details:	
   

deep	
   lamber.an	
   net	
   	
   

image	
   
albedo	
   

surface	
   	
   
normals	
   

light	
   	
   
source	
   

i

n
f
e
r
r
e
d

	
   

observed	
   

id136:	
   gibbs	
   sampler.	
   
learning:	
   stochas.c	
   approxima.on	
   

yale	
   b	
   extended	
   face	
   dataset	
   

      	
   38	
   subjects,	
      	
   45	
   images	
   of	
   varying	
   illumina.ons	
   per	
   subject,	
   
divided	
   into	
   4	
   subsets	
   of	
   increasing	
   illumina.on	
   varia.ons	
   	
   	
   
      	
   28	
   subjects	
   for	
   training,	
   10	
   (original	
   yale	
   database)	
   for	
   tes.ng	
   	
   
      	
   toronto	
   face	
   database	
   is	
   used	
   to	
   pretrain	
   the	
      albedo	
   dbn   	
   

deep	
   lamber.an	
   networks	
   

yale	
   b	
   extended	
   database	
   

one	
   test	
   image	
   

two	
   test	
   images	
   

face	
   religh.ng	
   

deep	
   lamber.an	
   networks	
   

recogni.on	
   as	
   func.on	
   of	
   the	
   number	
   of	
   training	
   images	
   for	
   10	
   test	
   
subjects.	
   	
   

one-     shot	
   
recogni.on	
   

generic	
   objects	
   
      	
   amsterdam	
   library	
   of	
   images	
   (geusebroek	
   et	
   al.	
   2005).	
   
      	
   for	
   each	
   object,	
   10	
   images	
   taken	
   for	
   training,	
   5	
   for	
   tes.ng.	
   

mul.-     modal	
   input	
   

learning	
   systems	
   that	
   combine	
   mul.ple	
   input	
   domains	
   

images	
   

video	
   

speech	
   &	
   	
   
audio	
   

text	
   &	
   language	
   	
   

laser	
   scans	
   

time	
   series	
   	
   
data	
   

develop	
   learning	
   systems	
   that	
   come	
   	
   
closer	
   to	
   displaying	
   human	
   like	
   intelligence.	
   

mul.-     modal	
   input	
   

learning	
   systems	
   that	
   combine	
   mul.ple	
   input	
   domains	
   

video	
   

text	
   

speech	
   

more	
   robust	
   percep.on.	
   	
   

ngiam	
   et.al.,	
   icml	
   2011	
   used	
   deep	
   autoencoders	
   (video	
   +	
   speech)	
   

      	
   guillaumin,	
   verbeek,	
   and	
   schmid,	
   cvpr	
   2011	
   
      	
   huiskes,	
   thomee,	
   and	
   lew,	
   mul.media	
   informa.on	
   retrieval,	
   2010	
   	
   
      	
   xing,	
   yan,	
   and	
   hauptmann,	
   uai	
   2005.	
   	
   

training	
   data	
   

pentax,	
   k10d,	
   
kangarooisland	
   
southaustralia,	
   sa	
   
australia	
   
australiansealion	
   300mm	
   

sandbanks,	
   lake,	
   
lakeontario,	
   sunset,	
   
walking,	
   beach,	
   purple,	
   
sky,	
   water,	
   clouds,	
   
overtheexcellence	
   	
   

<no	
   text>	
   

camera,	
   jahdakine,	
   
lightpain.ng,	
   
re   ec.on	
   
doublepaneglass	
   
wowiekazowie	
   	
   

top20buzer   ies	
   

mickikrimmel,	
   
mickipedia,	
   headshot	
   

samples	
   from	
   the	
   mir	
   flickr	
   dataset	
   -     	
   crea.ve	
   commons	
   license	
   

mul.-     modal	
   input	
   

improve	
   classi   ca.on	
   	
   

      

pentax,	
   k10d,	
   kangarooisland	
   
southaustralia,	
   sa	
   australia	
   
australiansealion	
   300mm	
   

sea	
   /	
   not	
   sea	
   

       fill	
   in	
   missing	
   modali.es	
   

beach,	
   sea,	
   surf,	
   
strand,	
   shore,	
   
wave,	
   seascape,	
   
sand,	
   ocean,	
   waves	
   

       retrieve	
   data	
   from	
   one	
   modality	
   when	
   queried	
   using	
   data	
   from	
   

another	
   modality	
   
beach,	
   sea,	
   surf,	
   
strand,	
   shore,	
   
wave,	
   seascape,	
   
sand,	
   ocean,	
   waves	
   

srivastava	
   and	
   salakhutdinov,	
   2012	
   

mul.-     modal	
   deep	
   boltzmann	
   

machine	
   

gaussian	
   rbm	
   

dense	
   image	
   features	
   

image	
   

text	
   

replicated	
   sojmax	
   
sparse	
   word	
   counts	
   

srivastava	
   and	
   salakhutdinov,	
   2012	
   

mul.-     modal	
   dbm	
   

      	
   flickr	
   data	
   -     	
   1	
   million	
   images	
   along	
   with	
   text	
   tags,	
   25k	
   annotated	
   

recogni.on	
   results	
   

      	
   mul.modal	
   inputs	
   (images	
   +	
   text),	
   38	
   classes.	
   	
   	
   

learning	
   algorithm	
   
image-     text	
   id166	
   
image-     text	
   lda	
   
mul.modal	
   dbm	
   

mean	
   average	
   precision	
   

0.475	
   
0.492	
   
0.587	
   

      	
   unimodal	
   inputs	
   (images	
   only).	
   	
   

learning	
   algorithm	
   
image-     id166	
   
image-     lda	
   
image	
   dbn	
   

mean	
   average	
   precision	
   

0.375	
   
0.315	
   
0.452	
   

retrieval	
   results	
   

pazern	
   comple.on	
   

given	
   a	
   test	
   image,	
   we	
   generate	
   associated	
   text	
      	
   achieve	
   far	
   bezer	
   
classi   ca.on	
   results.	
   	
   	
   

pazern	
   comple.on	
   

model	
   selec.on	
   

how	
   to	
   choose	
   the	
   number	
   of	
   layers	
   and	
   the	
   number	
   of	
   hidden	
   units?	
   
more	
   generally,	
   how	
   can	
   we	
   choose	
   between	
   models?	
   

dbm	
   samples	
   

mixture	
   of	
   bernoulli   s	
   

goal:	
   	
   compare	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   on	
   the	
   valida.on	
   set:	
   	
   

need	
   an	
   es.mate	
   of	
   par..on	
   func.on	
   	
   

model	
   selec.on	
   

mcmc-     based	
   algorithm	
   based	
   on	
   annealed	
   importance	
   sampling	
   to	
   
es.mate	
   par..on	
   func.on	
   of	
   a	
   dbm	
   model.	
   	
   	
   

annealing,	
   or	
   tempering:	
   	
   	
    	
   

	
   

	
      temperature   	
   

(salakhutdinov & murray, icml 2008, salakhutdinov  2008)	
   

model	
   selec.on	
   

dbm	
   samples	
   

mixture	
   of	
   bernoulli   s	
   

mob,	
   test	
   log-     id203:
dbm,	
   test	
   log-     id203:

	
   
	
   

	
   -     137.64	
   nats/digit	
   	
   
	
   	
   	
   -     85.97	
   nats/digit	
   	
   

di   erence	
   of	
   over	
   50	
   nats	
   is	
   striking!	
   

model	
   selec.on	
   

dbm	
   samples	
   

mixture	
   of	
   bernoulli   s	
   

mob,	
   test	
   log-     id203:
dbm,	
   test	
   log-     id203:

we	
   will	
   come	
   back	
   to	
   advanced	
   

	
   -     137.64	
   nats/digit	
   	
   
	
   	
   	
   -     85.97	
   nats/digit	
   	
   
mcmc	
   techniques	
   later.	
   

	
   
	
   

di   erence	
   of	
   over	
   50	
   nats	
   is	
   striking!	
   

learning	
   part-     based	
   hierarchy	
   

object	
   parts.	
   

combina.on	
   of	
   edges.	
   

trained	
   from	
   mul.ple	
   classes	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   
(cars,	
   faces,	
   motorbikes,	
   airplanes).	
   

lee	
   et.al.,	
   icml	
   2009	
   

learning	
   hierarchical	
   representa.ons	
   
deep	
   boltzmann	
   machines:	
   	
   

learning	
   hierarchical	
   structure	
   	
   
in	
   features:	
   edges,	
   combina.on	
   	
   
of	
   edges.	
   	
   
      	
   performs	
   well	
   in	
   many	
   applica.on	
   domains	
   
      	
   combines	
   bozom	
   and	
   top-     down	
   
      	
   fast	
   id136:	
   frac.on	
   of	
   a	
   second	
   
      	
   learning	
   scales	
   to	
   millions	
   of	
   examples	
   

many	
   examples,	
   few	
   categories	
   

next:	
   few	
   examples,	
   many	
   categories	
      	
   one-     shot	
   learning	
   

one-     shot	
   learning	
   
   segway    

   zarc    

how	
   can	
   we	
   learn	
   a	
   novel	
   concept	
      	
   a	
   high	
   dimensional	
   
sta.s.cal	
   object	
      	
   from	
   few	
   examples.	
   	
   	
   

(lake, salakhutdinov, gross, tenenbaum, cogsci 2011)!

tradi.onal	
   supervised	
   learning	
   

segway	
   

motorcycle	
   

test:	
   	
   
what	
   is	
   this?	
   

learning	
   to	
   transfer	
   

background	
   knowledge	
   

millions	
   of	
   unlabeled	
   images	
   	
   

learn	
   to	
   transfer	
   
knowledge	
   

some	
   labeled	
   images	
   

bicycle	
   

dolphin	
   

elephant	
   

tractor	
   

learn	
   novel	
   concept	
   
from	
   one	
   example	
   

test:	
   	
   
what	
   is	
   this?	
   

learning	
   to	
   transfer	
   

background	
   knowledge	
   

millions	
   of	
   unlabeled	
   images	
   	
   

learn	
   to	
   transfer	
   
knowledge	
   

key	
   problem	
   in	
   computer	
   vision,	
   	
   
speech	
   percep.on,	
   natural	
   language	
   
processing,	
   and	
   many	
   other	
   domains.	
   

some	
   labeled	
   images	
   

bicycle	
   

dolphin	
   

elephant	
   

tractor	
   

learn	
   novel	
   concept	
   
from	
   one	
   example	
   

test:	
   	
   
what	
   is	
   this?	
   

thank	
   you	
   

code	
   for	
   learning	
   rbms,	
   dbns,	
   and	
   dbms	
   is	
   available	
   at:	
   
hzp://www.utstat.toronto.edu/~rsalakhu/code.html	
   	
   

