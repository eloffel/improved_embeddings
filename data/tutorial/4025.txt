large-scale deep learning

ipam summer school

marc'aurelio ranzato  -
ranzato@google.com
www.cs.toronto.edu/~ranzato    

ucla, 24 july 2012

two approches to deep learning

deep neural nets:
- usually more efficient (at training & test time)
- typically more unconstrained (partition function has to be              
  replaced by other constraints, e.g. sparsity).
- more flexible
- ideal for end-to-end learning of complex systems

deep probabilistic models:
- typically intractable
- easier to compose
- easier to interpret (e.g., you can generate samples from them)
- better deal with uncertainty 

2
ranzato

example: auto-encoder

neural net:

code

z =       w e

reconstruction

t x    be   
   x =w d z    bd

probabilistic model (gaussian rbm):

e [ z   x ]=       w t x    be   
e [ x    z ]=w z    bd

vincent    a connection between score matching and denoising autoencoders     neural comp. 2011
3
swersky et al.    on autoencoders and score matching for ebm     icml 2011
ranzato

gated mrf

c
hk
p

c

xq

m

x p

c

f

p    hk

c=1   v   = f     p k   c ' v    2   

+

- relation to simple-complex cell model

4

gated mrf & subspace ica

subspace ica:

p   hk

c=1   v   = f     pk   c ' v   2   

hyvarinen et al.    emergence of   features by...independent feature subspaces     neural comp 2000

probabilistic model (gated mrf):

p   hk

c=1   v   =          pk    c ' v   2   

welling et al.    learning sparse topographic representations with pot     nips 2002
ranzato et al.    factored 3-way rbms for modeling natural images     aistats 2010

5

gated mrf & ipsd

ipsd:

feature=      p k   c ' v   2   

kavukcuoglu et al.    learning invariant features through topographic filter maps     cvpr 2009

probabilistic model (gated mrf):

p   hk

c=1   v   =          pk    c ' v   2   

welling et al.    learning sparse topographic representations with pot     nips 2002
ranzato et al.    factored 3-way rbms for modeling natural images     aistats 2010

6

probabilities: yes/no?

deep neural nets:
- usually more efficient (at training & test time)
- typically more unconstrained (partition function has to be              
  replaced by other constraints, e.g. sparsity).
- more flexible
- ideal for end-to-end learning of complex systems

deep probabilistic models:
- typically intractable
- easier to compose
- easier to interpret (e.g., you can generate samples from them)
- better deal with uncertainty 

7

today we are going to focus on

deep neural nets

since they are more easily scalable

8

pooling

by    pooling    (e.g., max or average) filter
responses at different locations we gain
robustness to the exact spatial location
of features.

9
ranzato

pooling

over the years, some new modules have proven to be very 
effective when plugged into conv-nets:
- l2 pooling

layer i

layer i   1

n    x , y    

   x , y   

hi   1, x , y=          j , k       n    x , y     hi , j , k

2

jarrett et al.    what is the best multi-stage architecture for object 
recognition?    iccv 2009

10
ranzato

pooling

over the years, some new modules have proven to be very 
effective when plugged into conv-nets:
- l2 pooling

layer i

layer i   1

n    x , y    

   x , y   

hi   1, x , y=          j , k       n    x , y     hi , j , k

2

p    hk

c=1   v   = f     p k   c ' v    2   

like a gated mrf!

+

jarrett et al.    what is the best multi-stage architecture for object 
recognition?    iccv 2009

11
ranzato

pooling & lcn

over the years, some new modules have proven to be very 
effective when plugged into conv-nets:
- l2 pooling

layer i

layer i   1

n    x , y    

   x , y   

hi   1, x , y=          j , k       n    x , y     hi , j , k

2

- local contrast id172

layer i

layer i   1

   x , y   

hi   1, x , y=

n    x , y    

hi , x , y   mi , n    x , y   

    i , n     x , y   

jarrett et al.    what is the best multi-stage architecture for object 
recognition?    iccv 2009

12
ranzato

l2 pooling

-1
 0
 0
 0
 0
hi

l2 pooling

5

       i=1

2
         i

 +1
hi   1

kavukguoglu et al.    learning invariant features ...    cvpr 2009

13
ranzato

l2 pooling

 0
 0
 0
 0
+1
hi

l2 pooling

5

       i=1

2
         i

 +1
hi   1

kavukguoglu et al.    learning invariant features ...    cvpr 2009

14
ranzato

l2 pooling helps learning 

representations more robust to local distortions!

15
ranzato

local contrast id172

hi   1, x , y=

hi , x , y   mi , n    x , y   

    i , n     x , y   

0

 1

 0
 -1

 0

0

 0.5

 0
 -0.5

 0

lcn

16
ranzato

local contrast id172

hi   1, x , y=

hi , x , y   mi , n    x , y   

    i , n     x , y   

1

 11

 1
 -9

 1

0

 0.5

 0
 -0.5

 0

lcn

17
ranzato

l2 pooling & lcn

l2 pooling & local contrast id172
help learning more invariant representations!

18
ranzato

conv nets: typical architecture

one stage (zoom)

filtering

pooling

lcn

whole system

input 
image

1st stage

2nd stage

3rd stage

class
labels

linear
layer

19
ranzato

conv nets: training

since convolutions and sub-sampling are differentiable, we can use 
standard back-propagation.

algorithm:

given a small mini-batch
- fprop
- bprop
- parameter update

20
ranzato

conv nets: examples

- object category recognition   
   boureau et al.    ask the locals: multi-way local pooling for image       
   recognition    iccv 2011
- segmentation
   turaga et al.    maximin learning of image segmentation    nips 2009
- ocr   
  ciresan et al.    mcdnn for image classification    cvpr 2012
- pedestrian detection
   kavukcuoglu et al.    learning convolutional feature hierarchies for    
   visual recognition    nips 2010
- robotics
   sermanet et al.    mapping and planning..with long range perception     
    iros 2008

21
ranzato

limitations & solutions

- requires lots of labeled data to train
+ unsupervised learning

- difficult optimization
+ layer-wise training

- scalability
+ distributed training

22
ranzato

limitations & solutions

- requires lots of labeled data to train
+ unsupervised learning

- difficult optimization
+ layer-wise training

- scalability
+ distributed training

23
ranzato

tera-scale deep learning @ google

observation #1: more features always improve                          
                                performance unless data is scarce.

observation #2: deep learning methods have higher capacity     
                                 and have the potential to model data better.

q #1: given lots of data and lots of machines, can we scale up      
             deep learning methods?
q #2: will deep learning methods perform much better?

24
ranzato

the challenge

a large scale problem has: 
    lots of training samples (>10m)
    lots of classes (>10k) and 
    lots of input dimensions (>10k).

    best optimizer in practice is on-line sgd which is naturally           
   sequential, hard to parallelize.
    layers cannot be trained independently and in parallel, hard to      
   distribute
    model can have lots of parameters that may clog the network,      
   hard to distribute across machines 

25
ranzato

our solution

2nd layer

1st layer

input

le et al.    building high-level features using large-scale unsupervised learning    icml 2012

26
ranzato

our solution

1st 

machine

2nd

machine

3rd 

machine

2nd layer

1st layer

input

27
ranzato

our solution

1st 

machine

2nd

machine

3rd 

machine

model 

parallelism

28
ranzato

distributed deep nets

deep net

model 

parallelism

input #1

le et al.    building high-level features using large-scale unsupervised learning    icml 2012

29
ranzato

distributed deep nets

model 

parallelism

+

data

parallelism

input #3

input #2

input #1

le et al.    building high-level features using large-scale unsupervised learning    icml 2012

30
ranzato

asynchronous sgd

parameter server

1st replica

2nd replica

3rd replica

31
ranzato

asynchronous sgd

parameter server

    l
      1

1st replica

2nd replica

3rd replica

32
ranzato

asynchronous sgd

parameter server

   1

1st replica

2nd replica

3rd replica

33
ranzato

asynchronous sgd

parameter server
(update parameters)

1st replica

2nd replica

3rd replica

34
ranzato

asynchronous sgd

parameter server

    l
      2

1st replica

2nd replica

3rd replica

35
ranzato

asynchronous sgd

parameter server

   2

1st replica

2nd replica

3rd replica

36
ranzato

asynchronous sgd

parameter server
(update parameters)

1st replica

2nd replica

3rd replica

37
ranzato

highly distributed asynchronous sgd

sharded 

parameter server

1st replica

2nd replica

3rd replica

38
ranzato

problem: each parameter needs its own learning rate!

39
ranzato

highly distributed asynchronous sgd

sharded 

parameter server

1st replica

2nd replica

3rd replica

40
ranzato

adagrad

sharded 

parameter server

i        t    1
   t

i=
   t

i       t
      

t

      k =0

i     l
i
       t

   

2
    l
i    
      k

    similar to diagonal approx. of       
   hessian
    takes care of different scaling    
   of learning rates
    handles annealing automatically

duchi et al.    adaptive subgradient methods for online learning and stochastic optimization    
jmlr 2011

41
ranzato

42

from  quiroga et al.    invariant visual representation by single 
neurons in the human brain    nature 2005
   here we report on a remarkable subset of mtl 
neurons that are selectively activated by strikingly 
different pictures of given individuals, landmarks or 
objects and in some cases even by letter strings with 
their names.   
halle berry neuron

43

unsupervised learning with 1b parameters

data: 10m youtube (unlabeled) frames of size 200x200.

le et al.    building high-level features using large-scale unsupervised learning    icml 2012

44
ranzato

unsupervised learning with 1b parameters

deep net:
    3 stages
    each stage consists of local filtering, l2 pooling, lcn 

- 18x18 filters
- 8 filters at each location
- l2 pooling and lcn over 5x5 neighborhoods

    training jointly the three layers by:

- reconstructing the input of each layer
- sparsity on the code

le et al.    building high-level features using large-scale unsupervised learning    icml 2012

45
ranzato

unsupervised learning with 1b parameters

deep net:
    3 stages
    each stage consists of local filtering, l2 pooling, lcn 

- 18x18 filters
- 8 filters at each location
- l2 pooling and lcn over 5x5 neighborhoods

    training jointly the three layers by:

- reconstructing the input of each layer
- sparsity on the code

1b parameters!!!

le et al.    building high-level features using large-scale unsupervised learning    icml 2012

46
ranzato

unsupervised learning with 1b parameters

one stage (zoom)

filtering

l2

pooling

lcn

whole system

input 
image

1st stage

2nd stage

3rd stage

47
ranzato

unsupervised learning with 1b parameters

l=reconstructionerrorlayer    1       

reconstructionerrorlayer    2       

reconstructionerrorlayer    3       
   sparsitylayer    3   

48
ranzato

validating unsupervised learning

the network has seen lots of objects during training, but 
without any label. 
q.: how can we validate unsupervised learning?
q.: did the network form any high-level representation? 
       e.g., does it have any neuron responding for faces?

    build validation set with 50% faces, 50% random images
- study properties of neurons

49
ranzato

validating unsupervised learning

1st stage

2nd stage

3rd stage

n
e
u
r
o
n
 
r
e
s
p
o
n
s
e
s

50
ranzato

top images for best face neuron

51
ranzato

best input for face neuron

52
ranzato

face / no face

53
ranzato

invariance properties

54
ranzato

invariance properties

55
ranzato

comparison: face neuron

accuracy %

method
random guess                                                                       65
best training sample                                                             74
1 layer net                                                                            71
deep net before training                                                      67
deep net after training                                                        81
deep net after training without lcn                                    78

56
ranzato

comparison: face neuron

method
id116 on 40x40, k=30000                                               72
3-layer autoencoder                                                            72
deep net after training                                                        81

accuracy %

57
ranzato

pedestrian neuron

58
ranzato

top images for pedestrian neuron

59
ranzato

pedestrian neuron

60
ranzato

cat neuron

61
ranzato

top images for cat neuron

62
ranzato

cat neuron

63
ranzato

unsupervised + supervised (id163)

input 
image

y

1st stage

2nd stage

3rd stage

   y

cost

64
ranzato

object recognition on id163

id163 v.2011 (16m images, 20k categories)

method
weston & bengio 2011
linear classifier on deep features
deep net (from random)
deep net (from unsup.)

accuracy %

9.3
13.1
13.6
15.8

65
ranzato

top inputs after supervision

66
ranzato

top inputs after supervision

67
ranzato

references on convnets & alike

tutorials & background material
    yoshua bengio, learning deep architectures for ai, foundations and trends in 
machine learning, 2(1), pp.1-127, 2009.
    lecun, chopra, hadsell, ranzato, huang: a tutorial on energy-based learning, in 
bakir, g. and hofman, t. and sch  lkopf, b. and smola, a. and taskar, b. (eds), 
predicting structured data, mit press, 2006
convolutional nets
    lecun, bottou, bengio and haffner: gradient-based learning applied to 
document recognition, proceedings of the ieee, 86(11):2278-2324, november 
1998
    jarrett, kavukcuoglu, ranzato, lecun: what is the best multi-stage 
architecture for object recognition?, proc. international conference on 
id161 (iccv'09), ieee, 2009
- kavukcuoglu, sermanet, boureau, gregor, mathieu, lecun: learning convolutional 
feature hierachies for visual recognition, advances in neural information 
processing systems (nips 2010), 23, 2010

68
ranzato

unsupervised learning
    ica with reconstruction cost for efficient overcomplete id171. le, 
karpenko, ngiam, ng. in nips*2011
    rifai, vincent, muller, glorot, bengio, contracting auto-encoders: explicit 
invariance during feature extraction, in: proceedings of the twenty-eight 
international conference on machine learning (icml'11), 2011
- vincent, larochelle, lajoie, bengio, manzagol, stacked denoising autoencoders: 
learning useful representations in a deep network with a local denoising 
criterion, journal of machine learning research, 11:3371--3408, 2010.
- gregor, szlam, lecun: structured sparse coding via lateral inhibition, 
advances in neural information processing systems (nips 2011), 24, 2011
- kavukcuoglu, ranzato, lecun. "fast id136 in sparse coding algorithms with 
applications to object recognition". arxiv 1010.3467 2008
- hinton, krizhevsky, wang, transforming auto-encoders, icann, 2011
id201
    multimodal deep learning, ngiam, khosla, kim, nam, lee, ng. in proceedings of 
the twenty-eighth international conference on machine learning, 2011.
69
ranzato

locally connected nets
    gregor, lecun    emergence of complex-like cells in a temporal product network 
with local receptive fields    arxiv. 2009
    ranzato, mnih, hinton    generating more realistic images using gated mrf's    
nips 2010
    le, ngiam, chen, chia, koh, ng    tiled convolutional neural networks    nips 2010
distributed learning
    le, ranzato, monga, devin, corrado, chen, dean, ng. "building high-level 
features using large scale unsupervised learning". international conference of 
machine learning (icml 2012), edinburgh, 2012.
papers on scene parsing
    farabet, couprie, najman, lecun,    scene parsing with multiscale feature 
    farabet, couprie, najman, lecun,    scene parsing with multiscale feature 
learning, purity trees, and optimal covers   , in proc. of the international 
learning, purity trees, and optimal covers   , in proc. of the international 
conference on machine learning (icml'12), edinburgh, scotland, 2012.
conference on machine learning (icml'12), edinburgh, scotland, 2012.
- socher, lin, ng, manning,    parsing natural scenes and natural language with 
id56s   . international conference of machine learning 
(icml 2011) 2011.

70
ranzato

papers on object recognition
- boureau, le roux, bach, ponce, lecun: ask the locals: multi-way local pooling for 
image recognition, proc. international conference on id161 2011
- sermanet, lecun: traffic sign recognition with multi-scale convolutional 
networks, proceedings of international joint conference on neural networks 
(ijid98'11)
- ciresan, meier, gambardella, schmidhuber. convolutional neural network 
committees for handwritten character classification. 11th international 
conference on document analysis and recognition (icdar 2011), beijing, china.
- ciresan, meier, masci, gambardella, schmidhuber. flexible, high performance 
convolutional neural networks for image classification. international joint 
conference on artificial intelligence ijcai-2011.
papers on action recognition
    learning hierarchical spatio-temporal features for action recognition with 
independent subspace analysis, le, zou, yeung, ng. in id161 and 
pattern recognition (cvpr), 2011
papers on segmentation
    turaga, briggman, helmstaedter, denk, seung maximin learning of image 
segmentation. nips, 2009. 

71
ranzato

papers on vision for robotics
    hadsell, sermanet, scoffier, erkan, kavackuoglu, muller, lecun: learning long-
range vision for autonomous off-road driving, journal of field robotics, 
26(2):120-144, february 2009,
deep convex nets & deconv-nets
    deng, yu.    deep convex network: a scalable architecture for speech pattern 
classification.    interspeech, 2011.
- zeiler, taylor, fergus "adaptive deconvolutional networks for mid and high 
level id171." iccv. 2011
papers on biological inspired vision

    serre, wolf, bileschi, riesenhuber, poggio. robust object recognition with 
cortex-like mechanisms, ieee transactions on pattern analysis and machine 
intelligence, 29, 3, 411-426, 2007.
- pinto, doukhan, dicarlo, cox "a high-throughput screening approach to 
discovering good forms of biologically inspired visual representation." {plos} 
computational biology. 2009

72
ranzato

papers on embedded convnets for real-time vision applications
    farabet, martini, corda, akselrod, culurciello, lecun: neuflow: a runtime 
reconfigurable dataflow processor for vision, workshop on embedded computer 
vision, cvpr 2011,
papers on image denoising using neural nets
    burger, schuler, harmeling: image denoisng: can plain neural networks 
compete with bm3d?, id161 and pattern recognition, cvpr 2012,
convnets & invariance from a mathematical perspective
    bruna, mallat: invariance scattering convolutional network, pami 2012,
convnets to learn embeddings
    hadsell, chopra, lecun: id84 by learning an invariant 
mapping, cvpr 2006,

73
ranzato

software & links

deep learning website 
    http://deeplearning.net/
    http://deeplearning.net/
c++ code for convnets 
    http://eblearn.sourceforge.net/
    http://eblearn.sourceforge.net/
matlab code for r-ica unsupervised algorithm 
- http://ai.stanford.edu/~quocle/rica_release.zip
python-based learning library 
- http://deeplearning.net/software/theano/
lush learning library which includes convnets
- http://lush.sourceforge.net/
- http://lush.sourceforge.net/
torch7: learning library that supports neural net training
http://www.torch.ch

74
ranzato

acknowledgements

quoc le, andrew ng

jeff dean, kai chen, greg corrado, matthieu devin, 
mark mao, rajat monga, paul tucker, samy bengio

yann lecun, pierre sermanet, clement farabet

75
ranzato

