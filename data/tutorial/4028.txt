deep learning in the 
visual cortex
thomas serre
brown university

i.  fundamentals of primate vision
ii. computational mechanisms of rapid 

recognition and feedforward processing

iii. beyond feedforward processing: 

attentional mechanisms and cortical 
feedback

feedforward processing

ventral visual stream    coarse initial base representation
- enables rapid id164/recognition 

(   what is there?   )

- insuf   cient for object localization
- sensitive to presence of clutter

feedforward processing

the    crowding    problem

isolated objects

see also dayan    08; pelli & tillman    08;  and many others

   |lfeelef   |lfeefefthe    crowding    problem

isolated objects

see also dayan    08; pelli & tillman    08;  and many others

   |lfeelef   |lfeeeffthe    crowding    problem

isolated objects

clutter

see also dayan    08; pelli & tillman    08;  and many others

   |lfeelef   |lfeeefff   |lfeelefthe    crowding    problem

prediction: recognition in clutter 
requires attentional mechanisms 
via cortical feedback 

isolated objects

clutter

spatial attention

see also dayan    08; pelli & tillman    08;  and many others

   |lfeelef   |lfeeefff   |lfeeleff   |lfeeeflthe    crowding    problem

prediction: recognition in clutter 
requires attentional mechanisms 
via cortical feedback 

isolated objects

clutter

spatial attention

x xx

see also dayan    08; pelli & tillman    08;  and many others

   |lfeelef   |lfeeefff   |lfeeleff   |lfeeefl1. monkey electrophysiology
read-out of inferior temporal cortex population 
activity: spatial attention eliminates clutter
with zhang, meyers, bichot, poggio & desimone

2. computational model of integrated 
attention and recognition
what and where: a bayesian attention theory of 
attention
with chikkerur, tan & poggio

1. monkey electrophysiology
read-out of inferior temporal cortex population 
activity: spatial attention eliminates clutter
with zhang, meyers, bichot, poggio & desimone

2. computational model of integrated 
attention and recognition
what and where: a bayesian attention theory of 
attention
with chikkerur, tan & poggio

the    readout    approach

neuron 1
neuron 2
neuron 3
neuron n

prediction

pattern classi   er

zhang meyers bichot serre poggio desimone pnas   11

the experiment

zhang meyers bichot serre poggio desimone unpublished data

the experiment

train readout classi   er 

on isolated object

test generalization in 

clutter

++

++

zhang* meyers* bichot serre poggio desimone in sub

the experiment

train readout classi   er 

on isolated object

test generalization in 

clutter

isolated

++

++

zhang* meyers* bichot serre poggio desimone in sub

the experiment

train readout classi   er 

on isolated object

test generalization in 

clutter

isolated

++

++

zhang* meyers* bichot serre poggio desimone in sub

the experiment

train readout classi   er 

on isolated object

test generalization in 

clutter

++

++

isolated

attended

zhang* meyers* bichot serre poggio desimone in sub

the experiment

train readout classi   er 

on isolated object

test generalization in 

clutter

++

++

isolated

attended

non-attended

zhang* meyers* bichot serre poggio desimone in sub

effects of attention on decoding accuracy:
decoding category
decoding object identity

zhang* meyers* bichot serre poggio desimone in sub

effects of attention on decoding accuracy:
decoding category
decoding object identity

++

zhang* meyers* bichot serre poggio desimone in sub

effects of attention on decoding accuracy:
decoding category
decoding object identity

++

zhang* meyers* bichot serre poggio desimone in sub

effects of attention on decoding accuracy:
decoding category
decoding object identity

++

zhang* meyers* bichot serre poggio desimone in sub

effects of attention on decoding accuracy:
decoding category
decoding object identity

++

zhang* meyers* bichot serre poggio desimone in sub

effects of attention on decoding accuracy:
decoding category
decoding object identity

++

zhang* meyers* bichot serre poggio desimone in sub

effects of attention on decoding accuracy:
decoding category
decoding target location

++

zhang* meyers* bichot serre poggio desimone in sub

changes in the salience of distractor stimuli 
dominate over attention related enhancements

aligned to the time when one of the 

distractors underwent a change

zhang* meyers* bichot serre poggio desimone in sub

summary

summary

    consistent with feedforward id187: in the absence of attention 
information about the identity of individual objects (and position) in clutter is 
greatly reduced relative to when objects are shown in isolation

summary

    consistent with feedforward id187: in the absence of attention 
information about the identity of individual objects (and position) in clutter is 
greatly reduced relative to when objects are shown in isolation

    attention seems to restore the pattern of neural activity toward the vector 

representing the isolated object

summary

    consistent with feedforward id187: in the absence of attention 
information about the identity of individual objects (and position) in clutter is 
greatly reduced relative to when objects are shown in isolation

    attention seems to restore the pattern of neural activity toward the vector 

representing the isolated object

    in spite of this nearly exclusive representation of the attended object, an 

increase in the salience of non-attended objects overrode these attentional 
enhancements

summary

    consistent with feedforward id187: in the absence of attention 
information about the identity of individual objects (and position) in clutter is 
greatly reduced relative to when objects are shown in isolation

    attention seems to restore the pattern of neural activity toward the vector 

representing the isolated object

    in spite of this nearly exclusive representation of the attended object, an 

increase in the salience of non-attended objects overrode these attentional 
enhancements

    results provide computational level explanation for how attention operates on 

neural representations to solve the problem of invariant recognition in clutter

1. monkey electrophysiology
read-out of inferior temporal cortex population 
activity: spatial attention eliminates clutter
with zhang, meyers, bichot, poggio & desimone

2. computational model of integrated 
attention and recognition
what and where: a bayesian attention theory of 
attention
with chikkerur, tan & poggio

fig. 19 experiment 2.3. learned templates using gabor wavelets of
lengths 17, 25, 33, 39, 49 respectively

fig. 18 experiment 2.2. max2 scores at resolutions 1 to 10

figure 21 displays the superposed templates of the 5
scales, at the detected location and resolution of the testing

figure 22(a) displays the max2 scores over the 15 res-
olutions. (b) displays the combined sum2 map at the opti-
mal resolution. the combined sum2 map is the sum of the

computationally, applying a larger gabor    lter to an im-

fig. 20 experiment 2.3. testing image. for each template, we run the
id136 algorithm over 15 resolutions, from 110    140 to 341    434

perception as bayesian 
id136

negative experience in experiment 2. our method can
sometimes be distracted by cluttered edges or strong edges
in the background. one may need to incorporate local ap-
pearance variables such as textures and smoothness into the
model.

image 

measurements

mumford    92; knill & richards    96; dayan & zemel 
   99; rao    02    04; kersten & yuille    03; kersten et al 

   04;  lee & mumford    03; dean    05; george & 
hawkins    05    09; yu & dayan    05; hinton    07; 
epshtein et al    08;  murray & kreutz-delgado    07

ifig. 19 experiment 2.3. learned templates using gabor wavelets of
lengths 17, 25, 33, 39, 49 respectively

fig. 18 experiment 2.2. max2 scores at resolutions 1 to 10

figure 21 displays the superposed templates of the 5
scales, at the detected location and resolution of the testing

figure 22(a) displays the max2 scores over the 15 res-
olutions. (b) displays the combined sum2 map at the opti-
mal resolution. the combined sum2 map is the sum of the

computationally, applying a larger gabor    lter to an im-

fig. 20 experiment 2.3. testing image. for each template, we run the
id136 algorithm over 15 resolutions, from 110    140 to 341    434

perception as bayesian 
id136

negative experience in experiment 2. our method can
sometimes be distracted by cluttered edges or strong edges
in the background. one may need to incorporate local ap-
pearance variables such as textures and smoothness into the
model.

image 

measurements

mumford    92; knill & richards    96; dayan & zemel 
   99; rao    02    04; kersten & yuille    03; kersten et al 

   04;  lee & mumford    03; dean    05; george & 
hawkins    05    09; yu & dayan    05; hinton    07; 
epshtein et al    08;  murray & kreutz-delgado    07

ifig. 19 experiment 2.3. learned templates using gabor wavelets of
lengths 17, 25, 33, 39, 49 respectively

fig. 18 experiment 2.2. max2 scores at resolutions 1 to 10

figure 21 displays the superposed templates of the 5
scales, at the detected location and resolution of the testing

figure 22(a) displays the max2 scores over the 15 res-
olutions. (b) displays the combined sum2 map at the opti-
mal resolution. the combined sum2 map is the sum of the

computationally, applying a larger gabor    lter to an im-

fig. 20 experiment 2.3. testing image. for each template, we run the
id136 algorithm over 15 resolutions, from 110    140 to 341    434

perception as bayesian 
id136

negative experience in experiment 2. our method can
sometimes be distracted by cluttered edges or strong edges
in the background. one may need to incorporate local ap-
pearance variables such as textures and smoothness into the
model.

visual scene 
description

image 

measurements

mumford    92; knill & richards    96; dayan & zemel 
   99; rao    02    04; kersten & yuille    03; kersten et al 

   04;  lee & mumford    03; dean    05; george & 
hawkins    05    09; yu & dayan    05; hinton    07; 
epshtein et al    08;  murray & kreutz-delgado    07

sip (s|i) / p (i|s)p (s)

fig. 19 experiment 2.3. learned templates using gabor wavelets of
lengths 17, 25, 33, 39, 49 respectively

fig. 18 experiment 2.2. max2 scores at resolutions 1 to 10

figure 21 displays the superposed templates of the 5
scales, at the detected location and resolution of the testing

figure 22(a) displays the max2 scores over the 15 res-
olutions. (b) displays the combined sum2 map at the opti-
mal resolution. the combined sum2 map is the sum of the

computationally, applying a larger gabor    lter to an im-

fig. 20 experiment 2.3. testing image. for each template, we run the
id136 algorithm over 15 resolutions, from 110    140 to 341    434

perception as bayesian 
id136

negative experience in experiment 2. our method can
sometimes be distracted by cluttered edges or strong edges
in the background. one may need to incorporate local ap-
pearance variables such as textures and smoothness into the
model.

visual scene 
description

image 

measurements

mumford    92; knill & richards    96; dayan & zemel 
   99; rao    02    04; kersten & yuille    03; kersten et al 

   04;  lee & mumford    03; dean    05; george & 
hawkins    05    09; yu & dayan    05; hinton    07; 
epshtein et al    08;  murray & kreutz-delgado    07

sihypothesis #1

to recognize and localize objects in a 
scene,  the visual system selects 
objects one at a time

p (o, l, i)

visual scene 
description

image 

measurements

chikkerur serre tan & poggio    10;

o,lihypothesis #2

two independent streams of 
processing for object and location

explicit tuning for position 

(shape implicit)

p (o, l, i) = p (o)p (l)p (i|l, o)
object

pfc

location

image 

measurements

explicit tuning for shape 

(position implicit)

chikkerur serre tan & poggio    10;

ventral    what    streamdorsal    where    streamoilhypothesis #3

objects encoded by collections of 
generic features (cond. ind. given an 
object and its location)

explicit tuning for position 

(shape implicit)

pfc

explicit tuning for shape 

(position implicit)

chikkerur serre tan & poggio    10;

fi

xi

...

ventral    what    streamdorsal    where    streamperception as 
bayesian id136

location

chikkerur serre tan & poggio    10;

object

position and 
scale tolerant 

features

(it)

retinotopic 
features

(v4)

n

image 

measurements

fixiilxiifioloperception as 
bayesian id136

p(l|i)

saliency map

chikkerur serre tan & poggio    10;

fixiiloperception as 
bayesian id136

    goal of visual perception is to 

estimate posterior probabilities of 
visual features, objects and their 
locations in an image

    attention corresponds to 

conditioning on high-level latent 
variables representing particular 
objects or locations (as well as on 
sensory input), and doing 
id136 over the other latent 
variables

    here we used belief propagation 

to solve the id136 problem

object

position and 
scale tolerant 

features

(it)

location

retinotopic 
features

(v4)

n

image 

measurements

biologically-plausible implementations of 
belief propagation: zemel et al.    98; beck & 
pouget    07; deneve    08; george    08; litvak & 
ullman    09; rao    04; steimer et al.    09

lxiifiodorsal / 
where

ventral / 
what

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

n

bayesian id136 and 
attention

special case of the id172 model 
of attention by reynolds & heeger    09

fioxiillxiifiodorsal / 
where

ventral / 
what

feedforward input

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

bayesian id136 and 
attention

n

feedforward 
(bottom-up) 

sweep

special case of the id172 model 
of attention by reynolds & heeger    09

fioxiillxiifiodorsal / 
where

ventral / 
what

feedforward input

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

suppressive drive

bayesian id136 and 
attention

n

feedforward 
(bottom-up) 

sweep

special case of the id172 model 
of attention by reynolds & heeger    09

fioxiillxiifioxkdorsal / 
where

ventral / 
what

top-down spatial 

attention

top-down feature-
based attention

feedforward input

attentional modulation

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

suppressive drive

bayesian id136 and 
attention

n

feedforward 
(bottom-up) 

sweep

special case of the id172 model 
of attention by reynolds & heeger    09

fioxiillxiifioxk0.5

p(f)

0

1

p(f|i)

0.5

p(l)

0

1

p(l|i)
p(f)

0.5

0.5

0

1

0.5

0

1

0.5

0

0
feedforward input
1
0.5

0.5

1

p(f|i)

p (x i|i) =

0

p(l)

p(l|i)

dorsal / 
where

ventral / 
what

0.5

0

1

0.5

0

1

0.5

0

1

0.5

0

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

suppressive drive

0

bayesian id136 and 
attention

feedforward 
(bottom-up) 

sweep

lxiifioxk(parallel) 

feature-based 

attention

dorsal / 
where

ventral / 
what

visual search

feedforward input

attentional modulation

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

suppressive drive

n

bayesian id136 and 
attention

lxiifiodorsal / 
where

ventral / 
what

it

v4

fi

xi

feedforward input

attentional modulation

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

suppressive drive

n

bayesian id136 and 
attention

consistent with data from v4 by 
bichot et al    05

lxiifiodorsal / 
where

ventral / 
what

it

v4

fi

xi

feedforward input

attentional modulation

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

suppressive drive

n

bayesian id136 and 
attention

consistent with data from v4 by 
bichot et al    05

lxiifiop (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

visual search

experimental data

s. chikkerur et al. / vision research xxx (2010) xxx   xxx

model data

9

a

b

c

n

fig. 5. (a) effect of feature attention on neuron response (replotted from bichot et al., 2005). (b) the time course of the neuron response is sampled at 150 ms. (c) the model
predicts multiplicative modulation of the response of xi units under attention.

a

d

b

bayesian id136 and 
attention

c

e

f

consistent with data from v4 by 
bichot et al    05

lxiifio(serial) spatial 

attention

dorsal / 
where

ventral / 
what

spatial cueing

feedforward input

attentional modulation

p (x i|i) =

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

suppressive drive

n

bayesian id136 and 
attention

lxiifiop (x i|i) =

mcadams and maunsell    99

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

model

w| attention

p (l = x)   1
p (l = x) = 1/|l|

multiplicative scaling of tuning 
curves by spatial attention

p (x i|i) =

trujillo and treue    02

p (i|x i) pf i,l p (x i|f i, l)p (l)p (f i)
px inp (i|x i)pf i,l p (x i|f i, l)p (l)p (f i)o

mc adams and maunsell    99

contrast vs. response 
gain

predicted by reynolds & heeger    09

learning to localize cars and 
pedestrians in street scenes

n

xiifiollearning to localize cars and 
pedestrians in street scenes

learning object priors

n

xiifiollearning to localize cars and 
pedestrians in street scenes

learning object priors

n

contextxiifiollearning to localize cars and 
pedestrians in street scenes
learning location priors 
(global contextual cues)

learning object priors

n

(see torralba et al)

xiifiolcontextthe experiment

    dataset:

- 100 street-scenes images 

with cars & pedestrians and 
20 without

    experiment

- 8 participants asked to 

count the number of cars/
pedestrians

- blocks/randomized 

presentations

- each image presented twice
    eye movements recorded using 

an infra-red eye tracker

    eye movements as proxy for 

attention

predicting eye movements during 
searches for cars and pedestrians

p (l|i)

saliency map

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

n

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

n

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements during 
searches for cars and pedestrians

overall model accounts 
for 92% of inter-subject 

agreement!

1st three    xations

1

0.75

0.5

cars

pedestrians

uniform priors (bottom-up)
feature priors 
feature + contextual (spatial) priors
humans

*similar (independent) results by ehinger hidalgo torralba & oliva (in press)

xiifiolcontextpredicting eye movements 
during free viewing

method

bruce and tsotos    06

itti et al    01
proposed

roc area
72.8%
72.7%
77.9%

n

human eye data from bruce & tsotsos

xiifiolsummary

summary

    attention as part of the id136 process that solves the visual recognition 

problem of    what is where    

summary

    attention as part of the id136 process that solves the visual recognition 

problem of    what is where    

    main goal of the visual system is to infer the identity and the position of 

objects in visual scenes: 
- spatial attention emerges as a strategy to reduce the uncertainty in shape information 

while feature-based attention reduces the uncertainty in spatial information

- featural and spatial attention represent two distinct modes of a computational process 

solving the problem of recognizing and localizing objects, especially in dif   cult 
recognition tasks such as in cluttered natural scenes

summary

    attention as part of the id136 process that solves the visual recognition 

problem of    what is where    

    main goal of the visual system is to infer the identity and the position of 

objects in visual scenes: 
- spatial attention emerges as a strategy to reduce the uncertainty in shape information 

while feature-based attention reduces the uncertainty in spatial information

- featural and spatial attention represent two distinct modes of a computational process 

solving the problem of recognizing and localizing objects, especially in dif   cult 
recognition tasks such as in cluttered natural scenes

    model agnostic about the speci   c algorithm for the id136 process (i.e., no 

claim made about the brain computing probabilities explicitly) 

two modes of vision

two modes of vision

    rapid bottom-up / feedforward processing during    rst 100-150ms of visual 

processing:
- coarse/initial base representation
- enables rapid id164/recognition (   what is there?   )

two modes of vision

    rapid bottom-up / feedforward processing during    rst 100-150ms of visual 

processing:
- coarse/initial base representation
- enables rapid id164/recognition (   what is there?   )

    top-down / re-entrant attentional processing 

- enables recognition in clutter
- enables object localization

collaborators (mit): 
    narcisse bichot
    sharat chikkerur
    bob desimone
    ethan meyers
    tomaso poggio
    cheston tan 
    ying zhang

acknowledgments

robert j. and nancy d. carney 
fund for scienti   c innovation

