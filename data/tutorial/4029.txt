multiview id171

roland memisevic

frankfurt, montreal

tutorial at ipam 2012

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

1 / 163

outline

1

introduction

id171
correspondence in id161
multiview id171

2 learning relational features

encoding relations
learning

3 factorization, eigen-spaces and complex cells

factorization
eigen-spaces, energy models, complex cells

4 applications and extensions
applications and extensions
conclusions

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

2 / 163

outline

1

introduction

id171
correspondence in id161
multiview id171

2 learning relational features

encoding relations
learning

3 factorization, eigen-spaces and complex cells

factorization
eigen-spaces, energy models, complex cells

4 applications and extensions
applications and extensions
conclusions

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

3 / 163

what this is about

extend id171 to model relations.
   mapping units   ,    bi-linear models   ,    energy-models   ,    complex
cells   ,    spatio-temporal features   ,    covariance features   ,    bi-linear
classi   cation   ,    quadrature features   ,    gated id82   ,
   mcrbm   , ...
id171 beyond object recognition

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

4 / 163

what this is about

extend id171 to model relations.
   mapping units   ,    bi-linear models   ,    energy-models   ,    complex
cells   ,    spatio-temporal features   ,    covariance features   ,    bi-linear
classi   cation   ,    quadrature features   ,    gated id82   ,
   mcrbm   , ...
id171 beyond object recognition

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

4 / 163

local features for recognition

object recognition started to work very well.
the main reason is the use of local features.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

5 / 163

local features for recognition

object recognition started to work very well.
the main reason is the use of local features.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

5 / 163

bag-of-features

bag-of-features

1 find interest points.
2 crop patches around interest points.
3 represent each patch with a sparse local descriptor (   features   ).
4 add all local descriptors to obtain a global descriptor for the

image.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

6 / 163

bag-of-features

bag-of-features

1 find interest points.
2 crop patches around interest points.
3 represent each patch with a sparse local descriptor (   features   ).
4 add all local descriptors to obtain a global descriptor for the

image.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

6 / 163

bag-of-features

f1

fn

bag-of-features

1 find interest points.
2 crop patches around interest points.
3 represent each patch with a sparse local descriptor (   features   ).
4 add all local descriptors to obtain a global descriptor for the

image.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

6 / 163

bag-of-features

f 1
1

f 1
n

f m
1

f m
n

bag-of-features

1 find interest points.
2 crop patches around interest points.
3 represent each patch with a sparse local descriptor (   features   ).
4 add all local descriptors to obtain a global descriptor for the

image.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

6 / 163

bag-of-features

f 1
1

f 1
n

f m
1

f m
n

bag-of-features

1 find interest points.
2 crop patches around interest points.
3 represent each patch with a sparse local descriptor (   features   ).
4 add all local descriptors to obtain a global descriptor for the

image.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

6 / 163

convolutional

convolutional

1 crop patches along a regular grid (dense or not).
2 represent each patch with a local descriptor.
3 concatenate all descriptors in a very large vector.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

7 / 163

convolutional

convolutional

1 crop patches along a regular grid (dense or not).
2 represent each patch with a local descriptor.
3 concatenate all descriptors in a very large vector.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

7 / 163

convolutional

convolutional

1 crop patches along a regular grid (dense or not).
2 represent each patch with a local descriptor.
3 concatenate all descriptors in a very large vector.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

7 / 163

classi   cation

f2

?

cathedral

high-rise

f1

after computing representations, use id28, id166,
nn, ...
there are various extensions, like fancy pooling, etc.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

8 / 163

extracting local features

f1

fn

how to extract local features.
engineer them. sift, hog, lbp, etc.
learn them from image data     deep learning

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

9 / 163

extracting local features

f1

fn

how to extract local features.
engineer them. sift, hog, lbp, etc.
learn them from image data     deep learning

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

9 / 163

extracting local features

f1

fn

how to extract local features.
engineer them. sift, hog, lbp, etc.
learn them from image data     deep learning

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

9 / 163

id171

z(y) = sigmoid(w ty)
y(z) = w z

z
zk

wjk

yj

y

id171

(cid:107)y       y(cid:0)z(cid:0)y  (cid:1)(cid:1)(cid:107)2

(cid:88)

  

w = arg min

w

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

10 / 163

id171 models

z
zk

wjk

yj

p(yj|z) = sigmoid(cid:0)(cid:88)
p(zk|y) = sigmoid(cid:0)(cid:88)

k

(cid:1)
(cid:1)

wjkzk

wjkyj

j

restricted id82 (rbm)

y

z exp(cid:0)(cid:80)

(cid:1)

p(y, z) = 1
learning: maximum likelihood/contrastive divergence.

jk wjkyjzk

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

11 / 163

id171 models

  y

  yj

wkj

zk

ajk

yj

y

(cid:1)

ajkyj

zk = sigmoid(cid:0)(cid:88)
(cid:88)

j

yj =

wjkzk

k

autoencoder

add id136 parameters.
learning: minimize reconstruction error.
add a sparsity penalty or corrupt inputs during training (vincent et
al., 2008).

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

11 / 163

id171 models

z

zk

wjk

yj

y

(cid:88)

k

yj =

wjkzk

independent components analysis (ica)

learning: make responses sparse, while keeping    lters sensible

(cid:107)w ty(cid:107)1

min
w
s.t. w tw = i

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

12 / 163

id171 works

(cifar)

(norb)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

13 / 163

manifold perspective

y

g(y)

f(z)

z

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

14 / 163

outline

1

introduction

id171
correspondence in id161
multiview id171

2 learning relational features

encoding relations
learning

3 factorization, eigen-spaces and complex cells

factorization
eigen-spaces, energy models, complex cells

4 applications and extensions
applications and extensions
conclusions

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

15 / 163

beyond object recognition

can we do more with id171 than recognize things?

brains can do much more than recognize objects.
many vision tasks go beyond object recognition.
in surprisingly many of them, the relationship between images
carries the relevant information.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

16 / 163

beyond object recognition

can we do more with id171 than recognize things?

brains can do much more than recognize objects.
many vision tasks go beyond object recognition.
in surprisingly many of them, the relationship between images
carries the relevant information.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

16 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

correspondences in id161

correspondence is one of the most ubiquitous problems in
id161.

some correspondence tasks in vision

tracking
stereo
geometry
optical flow
invariant recognition
odometry
action recognition
contours, within-image structure

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

17 / 163

heider and simmel

adding frames is not just about adding proportionally more
information.
the relationships between frames contain additional information,
that is not present in any single frame.
see heider and simmel, 1944: any single frame shows a bunch
of geometric    gures. the motions reveal the story.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

18 / 163

random dot stereograms

you can see objects even when images contain no features.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

19 / 163

outline

1

introduction

id171
correspondence in id161
multiview id171

2 learning relational features

encoding relations
learning

3 factorization, eigen-spaces and complex cells

factorization
eigen-spaces, energy models, complex cells

4 applications and extensions
applications and extensions
conclusions

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

20 / 163

learning features to model correspondences

if correspondences matter in vision, can we learn them?

z

x

y

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

21 / 163

?learning features to model correspondences

we can, if we let latent
variables act like gates, that
dynamically change the
connections between fellow
variables.

xi

zk

yj

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

22 / 163

learning features to model correspondences

learning and id136
(slightly) different from
learning without.
we can set things up, such
that id136 is almost
unchanged. yet, the meaning
of the latent variables will be
entirely different.

xi

zk

yj

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

23 / 163

learning features to model correspondences

multiplicative interactions allow
hidden variables to blend in a
whole    sub   -network.
this leads to a qualitatively
quite different behaviour from
the common, bi-partite feature
learning models.

xi

zk

yj

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

24 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

multiplicative interactions

brief history of gating

   mapping units    (hinton; 1981),    dynamic mappings    (v.d.
malsburg; 1981)
binocular+motion energy models (adelson, bergen; 1985),
(ozhawa, deangelis, freeman; 1990), (fleet et al., 1994)
higher-order neural nets,    sigma-pi-units   
routing circuits (olshausen; 1994)
bi-linear models (tenenbaum, freeman; 2000), (grimes, rao;
2005), (olshausen; 2007)
subspace som (kohonen, 1996)
isa, topographic ica (hyvarinen, hoyer; 2000), (karklin, lewicki;
2003): higher-order within image structure
(2006    ) gbm, mcrbm, gae, convisa, applications...

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

25 / 163

mapping units 1981

(hinton, 1981)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

26 / 163

mapping units 1981

(hinton, 1981)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

26 / 163

example application: action recognition

(hollywood 2)

(marsza  ek et al., 2009)

convolutional gbm (taylor et al., 2010)
hierarchical isa (le, et al., 2011)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

27 / 163

mocap

(taylor, hinton; 2009), (taylor,
et al.; 2010)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

28 / 163

gated mrfs

(ranzato et al., 2010)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

29 / 163

analogy making

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

30 / 163

invariance

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

31 / 163

aperture feature similaritiesimage similarities001122outline

1

introduction

id171
correspondence in id161
multiview id171

2 learning relational features

encoding relations
learning

3 factorization, eigen-spaces and complex cells

factorization
eigen-spaces, energy models, complex cells

4 applications and extensions
applications and extensions
conclusions

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

32 / 163

outline

1

introduction

id171
correspondence in id161
multiview id171

2 learning relational features

encoding relations
learning

3 factorization, eigen-spaces and complex cells

factorization
eigen-spaces, energy models, complex cells

4 applications and extensions
applications and extensions
conclusions

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

33 / 163

sparse coding of images pairs?

z

x

y

how to extend sparse coding to model relations?
sparse coding on the concatenation?

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

34 / 163

?sparse coding of images pairs?

z

x

y

how to extend sparse coding to model relations?
sparse coding on the concatenation?

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

34 / 163

?sparse coding on the concatenation ?

a case study: translations of binary, one-d images.
suppose images are random and can change in one of three
ways:

example image x:

possible image y:

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

35 / 163

sparse coding on the concatenation ?

a hidden variable that collects evidence for a shift to the right.
what if the images are random or noisy?
can we pool over more than one pixel?

zk

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

36 / 163

sparse coding on the concatenation ?

a hidden variable that collects evidence for a shift to the right.
what if the images are random or noisy?
can we pool over more than one pixel?

zk

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

36 / 163

sparse coding on the concatenation ?

a hidden variable that collects evidence for a shift to the right.
what if the images are random or noisy?
can we pool over more than one pixel?

zk

?

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

36 / 163

sparse coding on the concatenation ?

obviously not, because now the hidden unit would get equally
happy if it would see the non-shift (second pixel from the left).
the problem: hidden variables act like or-gates, that accumulate
evidence.

zk

?

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

37 / 163

cross-products

intuitively, what we need instead are logical ands, which can
represent coincidences (eg. zetzsche et al., 2003, 2005).
this amounts to using the outer product l := outer(x, y):

we can unroll this matrix, and let this be the data:

zk

wijk

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

38 / 163

cross-products

each component lij of the outer-product matrix will constitute
evidence for exactly one type of shift.
hiddens pool over products of pixels.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

39 / 163

cross-products

each component lij of the outer-product matrix will constitute
evidence for exactly one type of shift.
hiddens pool over products of pixels.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

39 / 163

cross-products

each component lij of the outer-product matrix will constitute
evidence for exactly one type of shift.
hiddens pool over products of pixels.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

39 / 163

a different view: families of manifolds

y

x

id171 reveals the (local) manifold structure in data.
when y is a transformed version of x, we can still think of y as
being con   ned to a manifold, but it will be a conditional manifold.
idea: learn a model for y, but let parameters be a function of x.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

40 / 163

conditional id136

xi

z

zk

yj

x

y

inferring z

if we use a linear function, wjk(x) =(cid:80)

(cid:88)

(cid:0)(cid:88)

i wijkxi, we get

(cid:1)yj =

(cid:88)

wijkxiyj

zk =

wjkyj =

wijkxi

(cid:88)

j

j

i

ij

id136 via bilinear function of the inputs.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

41 / 163

conditional id136

z

zk

yj

y

xi

x

inferring y

to infer y:

yj =

(cid:88)

k

wjkzk =

(cid:88)

(cid:0)(cid:88)

k

i

(cid:1)zk =

(cid:88)

ik

wijkxi

wijkxizk

id136 via bilinear function of x, z.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

41 / 163

input-modulated    lters

z

zk

yj

y

xi

x

this is id171 with input-dependent weights.
input pixels can vote for features in the output image.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

42 / 163

a different visualization

z

zk

xi

x

yj

y

a hidden can blend in one slice w    k of the parameter tensor.
a slice does id75 in    pixel space   .
so for binary hiddens, this is a mixture of 2k image warps.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

43 / 163

outline

1

introduction

id171
correspondence in id161
multiview id171

2 learning relational features

encoding relations
learning

3 factorization, eigen-spaces and complex cells

factorization
eigen-spaces, energy models, complex cells

4 applications and extensions
applications and extensions
conclusions

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

44 / 163

learning is predictive coding

z

zk

yj

y

xi

x

predictive sparse coding

the cost for a training pair (x, y) is:

(cid:88)

(cid:0)yj    (cid:88)

wijkxizk)2

j

ik

training as usual: infer z, update w . (tenenbaum, freeman;
2000), (grimes, rao; 2005), (olshausen; 2007), (memisevic,
hinton; 2007)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

45 / 163

example: gated id82

z

zk

yj

y

xi

x

three-way rbm (memisevic, hinton; 2007)

(cid:88)

wijkxiyjzk

e(x, y, z) =

z(x) exp(cid:0)e(x, y, z)(cid:1), z(x) =(cid:80)

ijk

y,z exp(cid:0)e(x, y, z)(cid:1)

p(y, z|x) = 1

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

46 / 163

example: gated id82

z

zk

yj

y

xi

x

three-way rbm (memisevic, hinton; 2007)

(cid:88)

wijkxiyjzk

e(x, y, z) =

z(x) exp(cid:0)e(x, y, z)(cid:1), z(x) =(cid:80)

ijk

y,z exp(cid:0)e(x, y, z)(cid:1)

p(y, z|x) = 1

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

46 / 163

example: gated id82

z

zk

yj

y

xi

x

three-way rbm (memisevic, hinton; 2007)

p(zk|x, y) = sigmoid(

p(yj|x, z) = sigmoid(

wijkxiyj)

wijkxizk)

(cid:88)
(cid:88)

ij

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

46 / 163

ik

example: gated auto-encoder

  yj

  y

xi

x

zk

z

yj

y

gated autoencoders

turn encoder and decoder weights into functions of x.
learning the same as in a standard auto-encoder for y.
the model is still a dag, so back-prop works exactly like in a
standard autoencoder. (memisevic, 2011)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

47 / 163

toy example: conditionally trained    hidden
   ow-   elds   

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

48 / 163

toy example: conditionally trained    hidden
   ow-   elds   , inhibitory connections

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

49 / 163

toy example: learning optical    ow

x

y

z

xtest

y(xtest, z)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

50 / 163

   combinatorial    ow   elds   

x

y

z

xtest

y(xtest, z)

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

51 / 163

joint training

z

zk

xi

x

yj

y

conditional training makes it hard to answer questions like:
   how likely are the given images transforms of one another?   
to answer questions like these, we require a joint image model,
p(x, y|z), given mapping units.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

52 / 163

joint training

z

zk

xi

x

yj

y

wijkxiyjzk

(cid:88)
exp(cid:0)e(x, y, z)(cid:1)
exp(cid:0)e(x, y, z)(cid:1)

1
z

ijk

e(x, y, z) =

p(x, y, z) =

(cid:88)

x,y,z

z =

use three-way sampling in a gated id82 (susskind
et al., 2011).
can apply this to matching tasks (more later).

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

53 / 163

joint training

z

zk

xi

x

yj

y

wijkxiyjzk

(cid:88)
exp(cid:0)e(x, y, z)(cid:1)
exp(cid:0)e(x, y, z)(cid:1)

1
z

ijk

e(x, y, z) =

p(x, y, z) =

(cid:88)

x,y,z

z =

use three-way sampling in a gated id82 (susskind
et al., 2011).
can apply this to matching tasks (more later).

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

53 / 163

joint training

  yj

  y

xi

x

zk

z

yj

y

for the autoencoder we can use a simple hack:
add up two conditional costs:

(cid:0)yj    (cid:80)

ik wijkxizk)2+(cid:80)

i

(cid:0)xi    (cid:80)

(cid:80)

j

jk wijkyjzk)2

force parameters to transform in both directions.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

54 / 163

joint training

  xi

  x

yj

y

zk

z

xi

x

for the autoencoder we can use a simple hack:
add up two conditional costs:

(cid:0)yj    (cid:80)

ik wijkxizk)2+(cid:80)

i

(cid:0)xi    (cid:80)

(cid:80)

j

jk wijkyjzk)2

force parameters to transform in both directions.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

54 / 163

pool over products

take-home message

to gather evidence for a transformation,

let hidden units compute the sum over products of input components.

roland memisevic (frankfurt, montreal)

multiview id171

tutorial at ipam 2012

55 / 163

