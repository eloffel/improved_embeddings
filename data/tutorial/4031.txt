an informal mathematical tour of 

id171

nando de freitas

ben marlin, kevin swersky, bo chen, marc   aurelio ranzato

misha denil, hugo larochelle,  loris bazzani

how we get supervisory signals in 
   unsupervised    id171?

1. reconstruction
2. contrastive learning 
3. association/correlation  (time, translation, etc.)

questions and choices we face
1. deterministic or probabilistic models? e.g. energy based 

models and autoencoders vs rbms, sparse coding, 
hierarchical dirichlet processes,    

2. what inductive principles are appropriate and when? 

maximum likelihood, bayes, max-margin, score matching, 
ratio matching, pseudo-likelihood, contrastive divergence, 
method of moments, indirect id136,    

3. static, discrete models or continuous-time dynamical 

systems?

4. traditional ai (logic, search, constraints) or the ml approach?

how much data and model complexity?
how much data and model complexity?

[murphy, 2012]

re-visiting logic, computational complexity and 

id124 (2-sat)

consider the cnf expression s = c1     . . .     cm, where each clausec i is a
disjunction of literals xi,1     . . .     xi,ki de   ned on propositional variables. when
each clause has two parents at most, the problem is known as 2-sat.

p

b

f

e

h

c1

c2

c3

c4

s

  p     b
p arrot =    bird
  b     f
bird =    f lies
  f     e
f lies =    escapes
f lies =    hasw ings   f     h

p

b

f

e

h

c1

c2

c3

c4

  p     b
p arrot =    bird
  b     f
bird =    f lies
  f     e
f lies =    escapes
f lies =    hasw ings   f     h

s

1. verification: does (p=1,b=1,f=1,e=1,h=1), i.e. (11111), satisfy this 2-sat problem?

2. verification: does (10111) satisfy it?

3. maximization: what is the maximum number of clauses that can be satisfied?

4. what is the number of possible assignments to (pbfeh)?

5. counting: how many assignments satisfy this 2-sat example?

p

b

f

e

h

c1

c2

c3

c4

s

counting: how many assignments satisfy this 2-sat example
approximate answer: use the monte carlo method.

i. sample p, b, f, e and h by flipping a coin for each variable n times.
ii. for each sample of (pbfeh), check for satisfiability.
iii. the id203 of satisfiability, p(s=1), is approximated as the 

number of satisfying samples divided by n.

iv. the expected number number of satisfiable samples n = p(s=1) 25.  

from max-2-sat to energy models

p

b

f

e

h

c1

c2

c3

c4

  p     b       
  b     f       
  f     e       
  f     h       

introduce the notation
p = x1, b = x2, f = x3, e = x4, h = x5.
the total energy of the system is:

s

assume some clauses are harder to 
satisfy than others. introduce a 
weight (  ) to measure this.
to obtain the energy of the system of 
binary variables, use x for a negated 
propositional variable and 1-x 
otherwise. then sum over all clauses.

b

f

e

h

p

ising 
model

e =   1p +   2b + (  3 +   4)f       1p b       2bf       3f e       4f h
let p = x1, b = x2, f = x3, e = x4 and h = x5

the energy can be written as:

e =    

5xi=1

bixi    

5xi=1xj>i

xiwijxj

in our case:

b

f

p

e

h

from max-2-sat to energy to id203

let us look at the energy of a few con   gurations, assuming all the   i = 1.
in this case the energy is simply:

e(x1, x2, . . . , x5) = x1 + x2 + 2x3     x1x2     x2x3     x3x4     x3x5

what is the lowest energy? when is it attained?
what is the maximum energy?
what should the most probable con   guration be?

boltzmann distribution

ising models and the 2nd law of thermodynamics

    the ising model describes many physical phenomena

       the ising model can be reinterpreted as a statistical model for the 
motion of atoms. a coarse model is to make space-time a lattice and 
imagine that each position either contains an atom or it doesn   t.   
wikipedia ising model page.

       the original motivation for the model was the phenomenon of 
magnetism.   

    second law of thermodynamics and stability

on information and energy     maxwell   s demon
in this thought experiment,    an imaginary container is divided into two parts by 
an insulated wall, with a door that can be opened and closed by what came to 
be called    maxwell   s demon   . the hypothetical demon is only able to let the 
   hot    molecules of gas flow through to a favored side of the chamber, causing 
that side to appear to spontaneously heat up while the other side cools down.   

    does this violate the 2nd law?

    what is the relation of information and energy?

nature seems to be very powerful at solving 
hard computational problems. 

can we recruit nature to do computing?

nature has limits!

but it is important that we follow nature   s 
guidelines for design!

the space of 1000 by 1000 black and white 
images is 

we might not be able to recruit nature to solve 
np hard problems, but we can use it to speed-up 
problems that we currently struggle with: e.g. 
cryptography and neural computation

geordie rose, d-wave systems

quantum annealing for deep learning

lateral connections

[denil & df, 2011]

hopfield networks and dynamics

hopfield networks and dynamics

hopfield networks and dynamics

stochastic approximation

stochastic approximation

id113 - definition

    the idea of id113 (id113) is to    nd the
parameters    that maximize the id203 of the data d given these
parameters:

     := arg max

log p(d|  )

  

    id113 assumes that the data has been generated by a distribution p(d|  0)

for some true parameter   0.

id113 - properties

    for independent and identically distributed (i.i.d.) data from p(x|  0),

the id113 minimizes the id181:

     = arg max

p(xi|  )

= arg max

log p(xi|  )

  

  

nyi=1
nxi=1
nxi=1
nxi=1
   z log

1
n

1
n

  

  

= arg max

= arg max

       arg min

nxi=1

log p(xi|  0)

log p(xi|  )     1

n

log

p(xi|  )
p(xi|  0)

p(xi|  0)
p(xi|  )

p(x|  0)dx

id113 - properties

    under smoothness and identi   ability assumptions,

the id113 is consistent:

    

p

      0

plim(    ) =   0

p (|           0| >   )   0

lim
n      

or equivalently,

or equivalently,

for every   .

id113 - properties

    the id113 is asymptotically normal. that is, as n        , we have:

           0 =    n (0, i   1)
where i is the fisher information matrix.

    it is asymptotically optimal or e   cient. that is, asymptotically, it has
the lowest variance among all well behaved estimators. in particular it
attains a lower bound on the clt variance known as the cramer-rao
lower bound.

    but what about issues like robustness and computation? is id113 always

the right option?

bias and variance

    note that the estimator is a function of the data:      = g(d).
    its bias is:

bias(    ) = ep(d|  0)(    )       0 =            0

    its variance is:

v(    ) = ep(d|  0)(             )2

    its mean squared error is:

mse = ep(d|  0)(           0) = (           0)2 + ep(d|  0)(             )2

id113 for the univariate gaussian distribution
    in the case of iid data sampled from a univariate gaussian, the log-

likelihood is given by

`(  ,   2) =

nxi=1

log n (xi|  ,   2) =    

1
2  2

nxi=1

(xi       )2    

n
2

ln   2    

n
2

ln(2  )

    to    nd the maximum of this function, we set the partial derivatives to 0

and solve. (we should check that the second derivative is positive.)

1
n

     =

nxi=1
nxi=1
    the quantities pi xi, pi x2

    2 =

1
n

xi = x

(xi     x)2 =   1

n

i!     (x)2

x2

nxi=1

i and n are called the su   cient statistics
of the data, since they capture all the relevant information needed for
estimating the parameter.

id113 for the bernoulli distribution
    we toss a coin n times and record the sequence of heads and tails, d =
(x1, x2, . . . , xn ). how do we estimate the id203 of heads from this?

    the log-likelihood is given by

`(  ) =

log ber(xi|  ) =

nxi=1

nxi=1

log    xi(1       )1   xi   = n1 log    + n2 log(1       )

where n1 = pi xi is the number of heads and n2 = pi(1     xi) is the

number of tails (these are the su   cient statistics).

    to    nd the id113, we    nd the maximum of this expression as follows:

=

d`
d  
     =

n1
      
n1

n2
1       

n1 + n2

= 0

where n1 + n2 = n .

id113 for binary id28

    recall that binary id28 has the form

p(yi|xi, w) = ber(yi|sigm(wt xi))

where sigm(  ) = 1/(1 + e     ) and yi     {0, 1}. let   i = sigm(wt xi).

    then the negative log-likelihood of all the data is given by

j(w) =    

=    

nxi=1
nxi=1

log[  i(yi=1)

i

   (1       i)i(yi=0)]

[yi log   i + (1     yi) log(1       i)]

this is also called the cross-id178 error function.

id113 for binary id28

    the gradient and hessian of j(w) are given by:

g(w) =

h =

d
dw

d
dw

j(w) =xi
g(w)t =xi

(  i     yi)xi = xt (       y)

(   w  i)xt

i =xi

  i(1       i)xixt

i = xt diag(  i(1       i))x

    one can show that h is positive de   nite; hence the nll is convex and

has a unique global minimum.

    to    nd this minimum, we will however have to learn a few things about

optimization.

revision: gradient and hessian

    let x be an n-dimensional vector, and f (x) a scalar-valued function. the

gradient vector of f with respect to x is the following vector:

   xf (x) =

derivative of a scalar product

                  

   f (x)
   x1
   f (x)
   x2

...

   f (x)
   xn

                  

   x at x = a

derivative of a quadratic form

   x xt ax = (a + at )x

if a is symmetric, this becomes    xxt ax = 2ax.

    the hessian matrix of a scalar valued function with respect to x, written

   2
xf (x) or simply as h, is the n    n matrix of partial derivatives,

   2
xf (x) =

                     

   2f (x)

   x2
1

   2f (x)
   x2   x1

...

   2f (x)
   x1   x2
   2f (x)

   x2
2

...

   2f (x)
   xn   x1

   2f (x)
   xn   x2

      
      
. . .
      

   2f (x)
   x1   xn
   2f (x)
   x2   xn

...

   2f (x)

   x2
n

                     

obviously this is a symmetric matrix, since

   2f (x)
   xi   xj

=

   2f (x)
   xj   xi

.

    we can think of the hessian as the gradient of the gradient, which can be

written as

h =    x(   t

x f (x))

    for the quadratic form f (x) = xt ax, the gradient vector is (a + at )x,

so the hessian is a + at . if a is symmetric, this is 2a.

problems with id113

    computation can be expensive.
    it su   ers from over   tting.
    it provides a point-estimate (best guess) of the parameters. i does not

give any measure of uncertainty in this guess.

    it migh not be the best option when the data generating mechanism is
outside the model class. even if it    nds the closest solution in terms of
kl divergence, other distance metrics might be more appropriate.

id113 is statistically consistent 
and efficient but is not computationally tractable for many 
models of interest like rbm   s, mrf   s, crf   s due to the 
partition function.

consistent 

efficient

tractable

many more tractable estimators have been proposed: 
pseudo/composite-likelihood [besag, 1975], ratio matching 
[hyv  rinen, 2007], generalized score matching [lyu, 2009], 
non-local contrastive objectives [vickrey, 2010], minimum 
id203 flow [sohl-dickstein, 2010],    

consistent 

efficient

tractable

example: restricted id82s

k hidden units

h1

h2

h3

h4

x1

x2

x3

d visible units

a restricted boltzmann 
machine (rbm) is a 
id82 with a 
bipartite graph structure.  

typically one layer of nodes 
are fully observed variables 
(the visible layer), while the 
other consists of latent 
variables (the hidden layer).

example: restricted id82s

the joint id203 of the visible and hidden variables 
is defined through a bilinear energy function.

example: restricted id82s

the bipartite graph structure gives the rbm a special 
property: the visible variables are conditionally 
independent given the hidden variables and vice versa. 

example: restricted id82s

the marginal id203 of the visible vector is 
obtained by summing out over all joint states of the 
hidden variables.

example: restricted id82s

this construction eliminates the latent, hidden 
variables, leaving a distribution defined in terms of the 
visible variables.

however, computing the normalizing constant 
(partition function) still has exponential complexity in 
d.

stochastic maximum likelihood

exact maximum likelihood learning is intractable in an 
rbm. stochastic ml estimation can instead be applied, 
usually using a simple block gibbs sampler.

contrastive divergence

the contrastive divergence principle results in a 
gradient that looks identical to stochastic maximum 
likelihood. the difference is that cd samples from the t-
step gibbs distribution.

pseudo-likelihood

the principle of maximum pseudo-likelihood is based 
on optimizing a product of one-dimensional conditional 
densities under a log loss.

ratio matching

the ratio matching principle is very similar to pseudo-
likelihood, but is based on minimizing a squared 
difference between one dimensional conditional 
distributions.

generalized score matching

the generalized score matching principle is similar to 
ratio matching, except that the difference between 
inverse one dimensional conditional distributions is 
minimized.

gradient comparison

experiments: log likelihood

[marlin, swersky, chen & df, 2010]

experiments: classification error

[marlin, swersky, chen & df, 2010]

experiments: de-noising

[marlin, swersky, chen & df, 2010]

theoretical results

consistency: pl and rm are both asymptotically 
consistent when the model is well-specified.
efficiency: neither pl nor rm is strictly more 
efficient than the other. we have a partial 
understanding of the relationship between their 
asymptotic covariance matrices.

[marlin & df, 2011]

discrete mrfs: efficiency simulations

pl 

better

rm 

better

bound width

max-margin learning

we have seen several examples of the ranking loss 
in this summer school, e.g. semantic learning, nlp:

[bordes, glorot, weston & bengio, 2012]

max-margin learning

the principle of contrasting is very useful for 
unsupervised id171.

but we must always choose a    negative set    with 
respect to which we contrast the    positive set    
(the given data)

can we optimize this process of choosing the 
negative set?

let us again use an rbm as a running example

max-margin learning

make the data more probable than anything else

[marlin 2010]

max-margin learning

we write the constraint in terms of energy, 
introduce a margin     and slack variables    :

max-margin learning

this objective has an exponential number of 
constraints. we can obtain a linear number of 
constraints by contrasting against the 
configuration of lowest energy

max-margin learning

a common trick is to rewrite the objective using a 
hinge loss:

so we get back to the ranking objective that jason 
weston and yoshua bengio introduced, but 
contrasting against a different    dataset   .

is this discussion of statistical 
estimators of any relevance in 

the continuous case?

discriminative autoencoders (neural 
nets devoid of id203) seem to do 

a reasonable job already

unification and design

probabilistic ebms

example: gaussian-bernoulli rbms

deterministic autoencoders

vanilla 1-layer autoencoder

score matching

denoising autoencoders

[vincent, 2011]

recipe

example 1: gaussian-bernoulli rbms

[swersky, ranzato, buchman, marlin & df 2011]

example 2: mpot model

example 2: mpot model

[swersky, ranzato, buchman, marlin & df 2011]

intuition

learned covariance filters

id116 for id171

[adam coates, honglak lee & andrew ng 2009]

learned bases (centroids)

[adam coates, honglak lee & andrew ng 2009]

mapping image to feature vector

[adam coates, honglak lee & andrew ng 2009]

id116 for id171

[adam coates, honglak lee & andrew ng 2009]

on threshold features and sparse coding

[denil & df 2011]

fast scalable feature methods

[denil & df 2011]

fast scalable feature methods

[denil & df 2011]

spatio-temporal id171

t=0:4

temporal pooling rbm

t=0:2

t=2:4

spatial pooling rbm

temporal pooling rbm

spatial pooling rbm

t=0

t=1

t=2

t=3

t=4

[chen & df 2010]

spatio-temporal id171

[chen & df 2010]

imagining so as to plan where to 

look and search

observed gaze sequence

model predictions

[bo chen & ndf 2010]

9

[denil, bazzani, larochelle & df, 2012]

hidden  activations

h

weights

foveated  image

multi-fixation rbm 

class  label

[2]
h

h
t+1

a
t+1

h
t

action

a
t

grid/place cells: the black lines  show how 
a rat explored a large box in a fairly haphazard 
manner. the red dots are the neuron response  
obtained with an electrode inserted in the rat   s 
subcortex .

[hafting et al 2005]

multi-target tracking
multi-target tracking

bayes net depiction

y

nasty integral

importance sampling for 
importance sampling for 
optimal filtering / tracking
optimal filtering / tracking

one can compute the 
one can compute the 

integrals recursively in time
integrals recursively in time

given the 
samples

from

id143ing (sis)
id143ing (sis)

id143ing (sis)
id143ing (sis)

id143ing (sir)
id143ing (sir)

id143ing code
id143ing code

simple example
simple example

particle methods more generally 
particle methods more generally 
the goal is to approximate a target distribution over a 
sequence of states                                            that is 
growing with    time    as well as the partition function.

we do this using sequential importance sampling (m&u, 49)

e.g. for filtering, we use:

using clever proposals: e.g. boosting, rfs
using clever proposals: e.g. boosting, rfs

[okuma, taleghani, df, little & lowe 2004]

use deep architectures as 

the observation models
x  encodes  location,  scale,  speed,  rotation,  
discrete  states,     

xt

x
t+1

t
h

t+1
h

goal: estimate  belief  state  b          =  p(x        |h              )
1:t+1

t+1

t+1

sequential decision making and optimization

state

xt

belief

reward

action

bt

rt

a t

ht

policy

x
t+1

bt+1

r
t+1

a
t+1

ht+1

class  label
[2]h
xt

state

ht

belief

reward

action

bt

rt

a t

ht+1

x
t+1

bt+1

r
t+1

a
t+1

policy

pseudo-code

[denil, bazzani, larochelle & df, 2012]

multi-target: localization & invariance

optimizing the reward
function, which is only
known point-wise. no 
need for derivatives. 

control with bayesian optimization

digits experiment:

face experiment:

115

