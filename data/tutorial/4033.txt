cs229 lecture notes

andrew ng

supervised learning

let   s start by talking about a few examples of supervised learning problems.
suppose we have a dataset giving the living areas and prices of 47 houses
from portland, oregon:

living area (feet2) price (1000$s)

2104
1600
2400
1416
3000

...

400
330
369
232
540
...

we can plot this data:

housing prices

1000

900

800

700

600

500

400

300

200

100

)
0
0
0
1
$

 

n
i
(
 

e
c
i
r
p

0

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

square feet

given data like this, how can we learn to predict the prices of other houses

in portland, as a function of the size of their living areas?

1

cs229 fall 2018

2

to establish notation for future use, we   ll use x(i) to denote the    input   
variables (living area in this example), also called input features, and y(i)
to denote the    output    or target variable that we are trying to predict
(price). a pair (x(i), y(i)) is called a training example, and the dataset
that we   ll be using to learn   a list of m training examples {(x(i), y(i)); i =
1, . . . , m}   is called a training set. note that the superscript    (i)    in the
notation is simply an index into the training set, and has nothing to do with
exponentiation. we will also use x denote the space of input values, and y
the space of output values. in this example, x = y = r.
to describe the supervised learning problem slightly more formally, our
goal is, given a training set, to learn a function h : x 7    y so that h(x) is a
   good    predictor for the corresponding value of y. for historical reasons, this
function h is called a hypothesis. seen pictorially, the process is therefore
like this:

training 
    set

learning 
algorithm

x

(living area of
 house.)

h

predicted y
(predicted price)
of house)

when the target variable that we   re trying to predict is continuous, such
as in our housing example, we call the learning problem a regression prob-
lem. when y can take on only a small number of discrete values (such as
if, given the living area, we wanted to predict if a dwelling is a house or an
apartment, say), we call it a classi   cation problem.

3

part i
id75

to make our housing example more interesting, let   s consider a slightly richer
dataset in which we also know the number of bedrooms in each house:

living area (feet2) #bedrooms price (1000$s)

2104
1600
2400
1416
3000

...

3
3
3
2
4
...

400
330
369
232
540
...

here, the x   s are two-dimensional vectors in r2. for instance, x(i)
1

is the
living area of the i-th house in the training set, and x(i)
is its number of
2
bedrooms. (in general, when designing a learning problem, it will be up to
you to decide what features to choose, so if you are out in portland gathering
housing data, you might also decide to include other features such as whether
each house has a    replace, the number of bathrooms, and so on. we   ll say
more about feature selection later, but for now let   s take the features as
given.)

to perform supervised learning, we must decide how we   re going to rep-
resent functions/hypotheses h in a computer. as an initial choice, let   s say
we decide to approximate y as a linear function of x:

h  (x) =   0 +   1x1 +   2x2

here, the   i   s are the parameters (also called weights) parameterizing the
space of linear functions mapping from x to y. when there is no risk of
confusion, we will drop the    subscript in h  (x), and write it more simply as
h(x). to simplify our notation, we also introduce the convention of letting
x0 = 1 (this is the intercept term), so that

h(x) =

n

xi=0

  ixi =   t x,

where on the right-hand side above we are viewing    and x both as vectors,
and here n is the number of input variables (not counting x0).

4

now, given a training set, how do we pick, or learn, the parameters   ?
one reasonable method seems to be to make h(x) close to y, at least for
the training examples we have. to formalize this, we will de   ne a function
that measures, for each value of the      s, how close the h(x(i))   s are to the
corresponding y(i)   s. we de   ne the cost function:

j(  ) =

1
2

m

xi=1

(h  (x(i))     y(i))2.

if you   ve seen id75 before, you may recognize this as the familiar
least-squares cost function that gives rise to the ordinary least squares
regression model. whether or not you have seen it previously, let   s keep
going, and we   ll eventually show this to be a special case of a much broader
family of algorithms.

1 lms algorithm

we want to choose    so as to minimize j(  ). to do so, let   s use a search
algorithm that starts with some    initial guess    for   , and that repeatedly
changes    to make j(  ) smaller, until hopefully we converge to a value of
   that minimizes j(  ). speci   cally, let   s consider the id119
algorithm, which starts with some initial   , and repeatedly performs the
update:

  j :=   j       

   
     j

j(  ).

(this update is simultaneously performed for all values of j = 0, . . . , n.)
here,    is called the learning rate. this is a very natural algorithm that
repeatedly takes a step in the direction of steepest decrease of j.

in order to implement this algorithm, we have to work out what is the
partial derivative term on the right hand side. let   s    rst work it out for the
case of if we have only one training example (x, y), so that we can neglect
the sum in the de   nition of j. we have:

   
     j

j(  ) =

1
   
     j
2
1
2

= 2   

(h  (x)     y)2
(h  (x)     y)   
   

= (h  (x)     y)   
= (h  (x)     y) xj

   
     j

(h  (x)     y)
  ixi     y!
     j   n
xi=0

for a single training example, this gives the update rule:1

5

  j :=   j +   (cid:0)y(i)     h  (x(i))(cid:1) x(i)

j .

the rule is called the lms update rule (lms stands for    least mean squares   ),
and is also known as the widrow-ho    learning rule. this rule has several
properties that seem natural and intuitive. for instance, the magnitude of
the update is proportional to the error term (y(i)     h  (x(i))); thus, for in-
stance, if we are encountering a training example on which our prediction
nearly matches the actual value of y(i), then we    nd that there is little need
to change the parameters; in contrast, a larger change to the parameters will
be made if our prediction h  (x(i)) has a large error (i.e., if it is very far from
y(i)).

we   d derived the lms rule for when there was only a single training
example. there are two ways to modify this method for a training set of
more than one example. the    rst is replace it with the following algorithm:

repeat until convergence {

  j :=   j +   pm

i=1(cid:0)y(i)     h  (x(i))(cid:1) x(i)

j

(for every j).

}

the reader can easily verify that the quantity in the summation in the update
rule above is just    j(  )/     j (for the original de   nition of j). so, this is
simply id119 on the original cost function j. this method looks
at every example in the entire training set on every step, and is called batch
id119. note that, while id119 can be susceptible
to local minima in general, the optimization problem we have posed here
for id75 has only one global, and no other local, optima; thus
id119 always converges (assuming the learning rate    is not too
large) to the global minimum.
indeed, j is a convex quadratic function.
here is an example of id119 as it is run to minimize a quadratic
function.

1we use the notation    a := b    to denote an operation (in a computer program) in
which we set the value of a variable a to be equal to the value of b. in other words, this
operation overwrites a with the value of b. in contrast, we will write    a = b    when we are
asserting a statement of fact, that the value of a is equal to the value of b.

6

50

45

40

35

30

25

20

15

10

5

5

10

15

20

25

30

35

40

45

50

the ellipses shown above are the contours of a quadratic function. also
shown is the trajectory taken by id119, which was initialized at
(48,30). the x   s in the    gure (joined by straight lines) mark the successive
values of    that id119 went through.

when we run batch id119 to    t    on our previous dataset,
to learn to predict housing price as a function of living area, we obtain
  0 = 71.27,   1 = 0.1345. if we plot h  (x) as a function of x (area), along
with the training data, we obtain the following    gure:

housing prices

1000

900

800

700

600

500

400

300

200

100

)
0
0
0
1
$
 
n
i
(
 
e
c
i
r
p

0

500

1000

1500

2000

2500

3000

3500

4000

4500

5000

square feet

if the number of bedrooms were included as one of the input features as well,
we get   0 = 89.60,   1 = 0.1392,   2 =    8.738.
the above results were obtained with batch id119. there is
an alternative to batch id119 that also works very well. consider
the following algorithm:

loop {

for i=1 to m, {
  j :=   j +   (cid:0)y(i)     h  (x(i))(cid:1) x(i)

j

}

7

(for every j).

}

in this algorithm, we repeatedly run through the training set, and each time
we encounter a training example, we update the parameters according to
the gradient of the error with respect to that single training example only.
this algorithm is called stochastic id119 (also incremental
id119). whereas batch id119 has to scan through
the entire training set before taking a single step   a costly operation if m is
large   stochastic id119 can start making progress right away, and
continues to make progress with each example it looks at. often, stochastic
id119 gets       close    to the minimum much faster than batch gra-
dient descent. (note however that it may never    converge    to the minimum,
and the parameters    will keep oscillating around the minimum of j(  ); but
in practice most of the values near the minimum will be reasonably good
approximations to the true minimum.2) for these reasons, particularly when
the training set is large, stochastic id119 is often preferred over
batch id119.

2 the normal equations

id119 gives one way of minimizing j. let   s discuss a second way
of doing so, this time performing the minimization explicitly and without
resorting to an iterative algorithm. in this method, we will minimize j by
explicitly taking its derivatives with respect to the   j   s, and setting them to
zero. to enable us to do this without having to write reams of algebra and
pages full of matrices of derivatives, let   s introduce some notation for doing
calculus with matrices.

2while it is more common to run stochastic id119 as we have described it
and with a    xed learning rate   , by slowly letting the learning rate    decrease to zero as
the algorithm runs, it is also possible to ensure that the parameters will converge to the
global minimum rather than merely oscillate around the minimum.

2.1 matrix derivatives
for a function f : rm  n 7    r mapping from m-by-n matrices to the real
numbers, we de   ne the derivative of f with respect to a to be:

8

   f

   f

...

   a11

   a1n

   af (a) =   
      

      
. . .
      
thus, the gradient    af (a) is itself an m-by-n matrix, whose (i, j)-element
is    f /   aij. for example, suppose a =(cid:20) a11 a12
a21 a22 (cid:21) is a 2-by-2 matrix, and
the function f : r2  2 7    r is given by

   
      

   am1

   amn

   f

   f

...

f (a) =

a11 + 5a2

12 + a21a22.

3
2

here, aij denotes the (i, j) entry of the matrix a. we then have

   af (a) =(cid:20)

3
2
a22

10a12

a21 (cid:21) .

we also introduce the trace operator, written    tr.    for an n-by-n
(square) matrix a, the trace of a is de   ned to be the sum of its diagonal
entries:

n

tra =

aii

xi=1

if a is a real number (i.e., a 1-by-1 matrix), then tr a = a. (if you haven   t
seen this    operator notation    before, you should think of the trace of a as
tr(a), or as application of the    trace    function to the matrix a. it   s more
commonly written without the parentheses, however.)

the trace operator has the property that for two matrices a and b such
that ab is square, we have that trab = trba. (check this yourself!) as
corollaries of this, we also have, e.g.,

trabc = trcab = trbca,

trabcd = trdabc = trcdab = trbcda.

the following properties of the trace operator are also easily veri   ed. here,
a and b are square matrices, and a is a real number:

tra = trat

tr(a + b) = tra + trb

tr aa = atra

we now state without proof some facts of matrix derivatives (we won   t
need some of these until later this quarter). equation (4) applies only to
non-singular square matrices a, where |a| denotes the determinant of a. we
have:

9

   atrab = bt
   at f (a) = (   af (a))t

   atrabat c = cab + c t abt

   a|a| = |a|(a   1)t .

(1)
(2)
(3)
(4)

to make our matrix notation more concrete, let us now explain in detail the
meaning of the    rst of these equations. suppose we have some    xed matrix
b     rn  m. we can then de   ne a function f : rm  n 7    r according to
f (a) = trab. note that this de   nition makes sense, because if a     rm  n,
then ab is a square matrix, and we can apply the trace operator to it; thus,
f does indeed map from rm  n to r. we can then apply our de   nition of
matrix derivatives to    nd    af (a), which will itself by an m-by-n matrix.
equation (1) above states that the (i, j) entry of this matrix will be given by
the (i, j)-entry of bt , or equivalently, by bji.

the proofs of equations (1-3) are reasonably simple, and are left as an
exercise to the reader. equations (4) can be derived using the adjoint repre-
sentation of the inverse of a matrix.3

2.2 least squares revisited

armed with the tools of matrix derivatives, let us now proceed to    nd in
closed-form the value of    that minimizes j(  ). we begin by re-writing j in
matrix-vectorial notation.

given a training set, de   ne the design matrix x to be the m-by-n
matrix (actually m-by-n + 1, if we include the intercept term) that contains

3if we de   ne a    to be the matrix whose (i, j) element is (   1)i+j times the determinant
of the square matrix resulting from deleting row i and column j from a, then it can be
proved that a   1 = (a   )t /|a|. (you can check that this is consistent with the standard
way of    nding a   1 when a is a 2-by-2 matrix. if you want to see a proof of this more
general result, see an intermediate or advanced id202 text, such as charles curtis,
1991, id202, springer.) this shows that a    = |a|(a   1)t . also, the determinant
of a matrix can be written |a| =pj aija   
ij. since (a   )ij does not depend on aij (as can
be seen from its de   nition), this implies that (   /   aij)|a| = a   
ij. putting all this together
shows the result.

the training examples    input values in its rows:

10

.

also, let ~y be the m-dimensional vector containing all the target values from
the training set:

x =   
            
~y =   
            

    (x(1))t    
    (x(2))t    

...

    (x(m))t    

   
            

.

y(1)
y(2)
...
y(m)

   
            
   
             
      
(x(m))t   
h  (x(1))     y(1)

(x(1))t   

...

...

h  (x(m))     y(m)

x       ~y =    
      
=    
      

y(1)
...
y(m)

   
      

.

   
      

now, since h  (x(i)) = (x(i))t   , we can easily verify that

thus, using the fact that for a vector z, we have that zt z =pi z2

m

i :

1
2

(x       ~y)t (x       ~y) =

(h  (x(i))     y(i))2

1
2

xi=1

= j(  )

finally, to minimize j, let   s    nd its derivatives with respect to   . combining
equations (2) and (3), we    nd that

   at trabat c = bt at c t + bat c

(5)

hence,

11

     j(  ) =      
1

1
2

(x       ~y)t (x       ~y)

=

=

=

1

1

2      (cid:0)  t x t x         t x t ~y     ~yt x   + ~yt ~y(cid:1)
2      tr(cid:0)  t x t x         t x t ~y     ~yt x   + ~yt ~y(cid:1)
2      (cid:0)tr   t x t x       2tr ~yt x  (cid:1)
2(cid:0)x t x   + x t x       2x t ~y(cid:1)

1

=
= x t x       x t ~y

in the third step, we used the fact that the trace of a real number is just the
real number; the fourth step used the fact that tra = trat , and the    fth
step used equation (5) with at =   , b = bt = x t x, and c = i, and
equation (1). to minimize j, we set its derivatives to zero, and obtain the
normal equations:

x t x   = x t ~y

thus, the value of    that minimizes j(  ) is given in closed form by the
equation

   = (x t x)   1x t ~y.

3 probabilistic interpretation

when faced with a regression problem, why might id75, and
speci   cally why might the least-squares cost function j, be a reasonable
choice? in this section, we will give a set of probabilistic assumptions, under
which least-squares regression is derived as a very natural algorithm.

let us assume that the target variables and the inputs are related via the

equation

y(i) =   t x(i) +   (i),

where   (i) is an error term that captures either unmodeled e   ects (such as
if there are some features very pertinent to predicting housing price, but
that we   d left out of the regression), or random noise. let us further assume
that the   (i) are distributed iid (independently and identically distributed)
according to a gaussian distribution (also called a normal distribution) with

12

mean zero and some variance   2. we can write this assumption as      (i)    
n (0,   2).    i.e., the density of   (i) is given by
exp(cid:18)   

2  2 (cid:19) .

   2    

p(  (i)) =

(  (i))2

1

this implies that

p(y(i)|x(i);   ) =

1

   2    

exp(cid:18)   

(y(i)       t x(i))2

2  2

(cid:19) .

the notation    p(y(i)|x(i);   )    indicates that this is the distribution of y(i)
given x(i) and parameterized by   . note that we should not condition on   
(   p(y(i)|x(i),   )   ), since    is not a random variable. we can also write the
distribution of y(i) as y(i) | x(i);        n (  t x(i),   2).
given x (the design matrix, which contains all the x(i)   s) and   , what
is the distribution of the y(i)   s? the id203 of the data is given by
p(~y|x;   ). this quantity is typically viewed a function of ~y (and perhaps x),
for a    xed value of   . when we wish to explicitly view this as a function of
  , we will instead call it the likelihood function:

l(  ) = l(  ; x, ~y) = p(~y|x;   ).

note that by the independence assumption on the   (i)   s (and hence also the
y(i)   s given the x(i)   s), this can also be written

l(  ) =

=

m

m

yi=1
yi=1

p(y(i) | x(i);   )
exp(cid:18)   

   2    

1

(y(i)       t x(i))2

2  2

(cid:19) .

now, given this probabilistic model relating the y(i)   s and the x(i)   s, what
is a reasonable way of choosing our best guess of the parameters   ? the
principal of maximum likelihood says that we should choose    so as to
make the data as high id203 as possible. i.e., we should choose    to
maximize l(  ).

instead of maximizing l(  ), we can also maximize any strictly increasing
function of l(  ). in particular, the derivations will be a bit simpler if we

instead maximize the log likelihood    (  ):

13

   (  ) = log l(  )

m

= log

m

yi=1
xi=1

log

1

   2    
1

   2    
1

=

= m log

   2        

exp(cid:18)   
exp(cid:18)   
1
  2   

1
2

2  2

(y(i)       t x(i))2

(cid:19)
(y(i)       t x(i))2
(cid:19)
(y(i)       t x(i))2.

2  2

m

xi=1

hence, maximizing    (  ) gives the same answer as minimizing

1
2

m

xi=1

(y(i)       t x(i))2,

which we recognize to be j(  ), our original least-squares cost function.

to summarize: under the previous probabilistic assumptions on the data,
least-squares regression corresponds to    nding the maximum likelihood esti-
mate of   . this is thus one set of assumptions under which least-squares re-
gression can be justi   ed as a very natural method that   s just doing maximum
likelihood estimation. (note however that the probabilistic assumptions are
by no means necessary for least-squares to be a perfectly good and rational
procedure, and there may   and indeed there are   other natural assumptions
that can also be used to justify it.)

note also that, in our previous discussion, our    nal choice of    did not
depend on what was   2, and indeed we   d have arrived at the same result
even if   2 were unknown. we will use this fact again later, when we talk
about the exponential family and generalized linear models.

4 locally weighted id75

consider the problem of predicting y from x     r. the leftmost    gure below
shows the result of    tting a y =   0 +   1x to a dataset. we see that the data
doesn   t really lie on straight line, and so the    t is not very good.

14

y

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

y

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

y

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1

2

3

4

5

6

7

x

1

2

3

4

5

6

7

x

1

2

3

4

5

6

7

x

   tting a 5-th order polynomial y =p5

instead, if we had added an extra feature x2, and    t y =   0 +   1x +   2x2,
then we obtain a slightly better    t to the data. (see middle    gure) naively, it
might seem that the more features we add, the better. however, there is also
a danger in adding too many features: the rightmost    gure is the result of
j=0   jxj. we see that even though the
   tted curve passes through the data perfectly, we would not expect this to
be a very good predictor of, say, housing prices (y) for di   erent living areas
(x). without formally de   ning what these terms mean, we   ll say the    gure
on the left shows an instance of under   tting   in which the data clearly
shows structure not captured by the model   and the    gure on the right is
an example of over   tting. (later in this class, when we talk about learning
theory we   ll formalize some of these notions, and also de   ne more carefully
just what it means for a hypothesis to be good or bad.)

as discussed previously, and as shown in the example above, the choice of
features is important to ensuring good performance of a learning algorithm.
(when we talk about model selection, we   ll also see algorithms for automat-
ically choosing a good set of features.) in this section, let us talk brie   y talk
about the locally weighted id75 (lwr) algorithm which, assum-
ing there is su   cient training data, makes the choice of features less critical.
this treatment will be brief, since you   ll get a chance to explore some of the
properties of the lwr algorithm yourself in the homework.

in the original id75 algorithm, to make a prediction at a query

point x (i.e., to evaluate h(x)), we would:

1. fit    to minimize pi(y(i)       t x(i))2.

2. output   t x.

in contrast, the locally weighted id75 algorithm does the fol-

lowing:

1. fit    to minimize pi w(i)(y(i)       t x(i))2.

2. output   t x.

15

here, the w(i)   s are non-negative valued weights. intuitively, if w(i) is large
for a particular value of i, then in picking   , we   ll try hard to make (y(i)    
  t x(i))2 small. if w(i) is small, then the (y(i)       t x(i))2 error term will be
pretty much ignored in the    t.

a fairly standard choice for the weights is4

w(i) = exp(cid:18)   

(x(i)     x)2

2   2

(cid:19)

note that the weights depend on the particular point x at which we   re trying
to evaluate x. moreover, if |x(i)     x| is small, then w(i) is close to 1; and
if |x(i)     x| is large, then w(i) is small. hence,    is chosen giving a much
higher    weight    to the (errors on) training examples close to the query point
x. (note also that while the formula for the weights takes a form that is
cosmetically similar to the density of a gaussian distribution, the w(i)   s do
not directly have anything to do with gaussians, and in particular the w(i)
are not random variables, normally distributed or otherwise.) the parameter
   controls how quickly the weight of a training example falls o    with distance
of its x(i) from the query point x;    is called the bandwidth parameter, and
is also something that you   ll get to experiment with in your homework.

locally weighted id75 is the    rst example we   re seeing of a
non-parametric algorithm. the (unweighted) id75 algorithm
that we saw earlier is known as a parametric learning algorithm, because
it has a    xed,    nite number of parameters (the   i   s), which are    t to the
data. once we   ve    t the   i   s and stored them away, we no longer need to
keep the training data around to make future predictions. in contrast, to
make predictions using locally weighted id75, we need to keep
the entire training set around. the term    non-parametric    (roughly) refers
to the fact that the amount of stu    we need to keep in order to represent the
hypothesis h grows linearly with the size of the training set.

4if x is vector-valued, this is generalized to be w(i) = exp(   (x(i)    x)t (x(i)    x)/(2   2)),

or w(i) = exp(   (x(i)     x)t      1(x(i)     x)/2), for an appropriate choice of    or   .

16

part ii
classi   cation and logistic
regression

let   s now talk about the classi   cation problem. this is just like the regression
problem, except that the values y we now want to predict take on only
a small number of discrete values. for now, we will focus on the binary
classi   cation problem in which y can take on only two values, 0 and 1.
(most of what we say here will also generalize to the multiple-class case.)
for instance, if we are trying to build a spam classi   er for email, then x(i)
may be some features of a piece of email, and y may be 1 if it is a piece
of spam mail, and 0 otherwise. 0 is also called the negative class, and 1
the positive class, and they are sometimes also denoted by the symbols    -   
and    +.    given x(i), the corresponding y(i) is also called the label for the
training example.

5 id28

we could approach the classi   cation problem ignoring the fact that y is
discrete-valued, and use our old id75 algorithm to try to predict
y given x. however, it is easy to construct examples where this method
performs very poorly. intuitively, it also doesn   t make sense for h  (x) to take
values larger than 1 or smaller than 0 when we know that y     {0, 1}.

to    x this, let   s change the form for our hypotheses h  (x). we will choose

where

h  (x) = g(  t x) =

1

1 + e     t x

,

g(z) =

1

1 + e   z

is called the logistic function or the sigmoid function. here is a plot
showing g(z):

17

1

0.9

0.8

0.7

0.6

)
z
(
g

0.5

0.4

0.3

0.2

0.1

0
   5

   4

   3

   2

   1

0
z

1

2

3

4

5

j=1   jxj.

notice that g(z) tends towards 1 as z        , and g(z) tends towards 0 as
z           . moreover, g(z), and hence also h(x), is always bounded between
0 and 1. as before, we are keeping the convention of letting x0 = 1, so that
  t x =   0 +pn

for now, let   s take the choice of g as given. other functions that smoothly
increase from 0 to 1 can also be used, but for a couple of reasons that we   ll see
later (when we talk about glms, and when we talk about generative learning
algorithms), the choice of the logistic function is a fairly natural one. before
moving on, here   s a useful property of the derivative of the sigmoid function,
which we write as g   :

g   (z) =

d
dz

1

1 + e   z
1

1

=

(1 + e   z)2 (cid:0)e   z(cid:1)
(1 + e   z)   (cid:18)1    
= g(z)(1     g(z)).

=

1

(1 + e   z)(cid:19)

so, given the id28 model, how do we    t    for it? following
how we saw least squares regression could be derived as the maximum like-
lihood estimator under a set of assumptions, let   s endow our classi   cation
model with a set of probabilistic assumptions, and then    t the parameters
via maximum likelihood.

18

let us assume that

p (y = 1 | x;   ) = h  (x)
p (y = 0 | x;   ) = 1     h  (x)

note that this can be written more compactly as

p(y | x;   ) = (h  (x))y (1     h  (x))1   y

assuming that the m training examples were generated independently, we
can then write down the likelihood of the parameters as

l(  ) = p(~y | x;   )

m

=

=

m

yi=1
p(y(i) | x(i);   )
yi=1(cid:0)h  (x(i))(cid:1)y(i)

(cid:0)1     h  (x(i))(cid:1)1   y(i)

as before, it will be easier to maximize the log likelihood:

   (  ) = log l(  )

=

m

xi=1

y(i) log h(x(i)) + (1     y(i)) log(1     h(x(i)))

how do we maximize the likelihood? similar to our derivation in the case
of id75, we can use gradient ascent. written in vectorial notation,
our updates will therefore be given by    :=    +           (  ). (note the positive
rather than negative sign in the update formula, since we   re maximizing,
rather than minimizing, a function now.) let   s start by working with just
one training example (x, y), and take derivatives to derive the stochastic
gradient ascent rule:

   
     j

1

1

1

g(  t x)     (1     y)

   (  ) = (cid:18)y
= (cid:18)y
= (cid:0)y(1     g(  t x))     (1     y)g(  t x)(cid:1) xj
= (y     h  (x)) xj

1     g(  t x)(cid:19)    
1     g(  t x)(cid:19) g(  t x)(1     g(  t x))

g(  t x)     (1     y)

g(  t x)

     j

1

   
     j

  t x

above, we used the fact that g   (z) = g(z)(1     g(z)). this therefore gives us
the stochastic gradient ascent rule

19

  j :=   j +   (cid:0)y(i)     h  (x(i))(cid:1) x(i)

j

if we compare this to the lms update rule, we see that it looks identical; but
this is not the same algorithm, because h  (x(i)) is now de   ned as a non-linear
function of   t x(i). nonetheless, it   s a little surprising that we end up with
the same update rule for a rather di   erent algorithm and learning problem.
is this coincidence, or is there a deeper reason behind this? we   ll answer this
when we get to glm models. (see also the extra credit problem on q3 of
problem set 1.)

6 digression: the id88 learning algo-

rithm

we now digress to talk brie   y about an algorithm that   s of some historical
interest, and that we will also return to later when we talk about learning
theory. consider modifying the id28 method to    force    it to
output values that are either 0 or 1 or exactly. to do so, it seems natural to
change the de   nition of g to be the threshold function:

g(z) =(cid:26) 1 if z     0

0 if z < 0

if we then let h  (x) = g(  t x) as before but using this modi   ed de   nition of
g, and if we use the update rule

then we have the id88 learning algorithm.

  j :=   j +   (cid:0)y(i)     h  (x(i))(cid:1) x(i)

j .

in the 1960s, this    id88    was argued to be a rough model for how
individual neurons in the brain work. given how simple the algorithm is, it
will also provide a starting point for our analysis when we talk about learning
theory later in this class. note however that even though the id88 may
be cosmetically similar to the other algorithms we talked about, it is actually
a very di   erent type of algorithm than id28 and least squares
id75; in particular, it is di   cult to endow the id88   s predic-
tions with meaningful probabilistic interpretations, or derive the id88
as a id113 algorithm.

20

7 another algorithm for maximizing    (  )

returning to id28 with g(z) being the sigmoid function, let   s
now talk about a di   erent algorithm for maximizing    (  ).

to get us started, let   s consider newton   s method for    nding a zero of a
function. speci   cally, suppose we have some function f : r 7    r, and we
wish to    nd a value of    so that f (  ) = 0. here,        r is a real number.
newton   s method performs the following update:

   :=       

f (  )
f    (  )

.

this method has a natural interpretation in which we can think of it as
approximating the function f via a linear function that is tangent to f at
the current guess   , solving for where that linear function equals to zero, and
letting the next guess for    be where that linear function is zero.

here   s a picture of the newton   s method in action:

60

50

40

30

20

10

0

)
x
(
f

60

50

40

30

20

10

0

)
x
(
f

60

50

40

30

20

10

0

)
x
(
f

   10

1

1.5

2

2.5

3
x

3.5

4

4.5

5

   10

1

1.5

2

2.5

3
x

3.5

4

4.5

5

   10

1

1.5

2

2.5

3
x

3.5

4

4.5

5

in the leftmost    gure, we see the function f plotted along with the line
y = 0. we   re trying to    nd    so that f (  ) = 0; the value of    that achieves this
is about 1.3. suppose we initialized the algorithm with    = 4.5. newton   s
method then    ts a straight line tangent to f at    = 4.5, and solves for the
where that line evaluates to 0. (middle    gure.) this give us the next guess
for   , which is about 2.8. the rightmost    gure shows the result of running
one more iteration, which the updates    to about 1.8. after a few more
iterations, we rapidly approach    = 1.3.

newton   s method gives a way of getting to f (  ) = 0. what if we want to
use it to maximize some function    ? the maxima of     correspond to points
where its    rst derivative       (  ) is zero. so, by letting f (  ) =       (  ), we can use
the same algorithm to maximize    , and we obtain update rule:

   :=       

      (  )
         (  )

.

(something to think about: how would this change if we wanted to use
newton   s method to minimize rather than maximize a function?)

21

lastly, in our id28 setting,    is vector-valued, so we need to
generalize newton   s method to this setting. the generalization of newton   s
method to this multidimensional setting (also called the newton-raphson
method) is given by

   :=        h    1        (  ).

here,         (  ) is, as usual, the vector of partial derivatives of    (  ) with respect
to the   i   s; and h is an n-by-n matrix (actually, n + 1-by-n + 1, assuming
that we include the intercept term) called the hessian, whose entries are
given by

hij =

   2   (  )
     i     j

.

newton   s method typically enjoys faster convergence than (batch) gra-
dient descent, and requires many fewer iterations to get very close to the
minimum. one iteration of newton   s can, however, be more expensive than
one iteration of id119, since it requires    nding and inverting an
n-by-n hessian; but so long as n is not too large, it is usually much faster
overall. when newton   s method is applied to maximize the logistic regres-
sion log likelihood function    (  ), the resulting method is also called fisher
scoring.

22

part iii
generalized linear models5

so far, we   ve seen a regression example, and a classi   cation example. in the
regression example, we had y|x;        n (  ,   2), and in the classi   cation one,
y|x;        bernoulli(  ), for some appropriate de   nitions of    and    as functions
of x and   .
in this section, we will show that both of these methods are
special cases of a broader family of models, called generalized linear models
(glms). we will also show how other models in the glm family can be
derived and applied to other classi   cation and regression problems.

8 the exponential family

to work our way up to glms, we will begin by de   ning exponential family
distributions. we say that a class of distributions is in the exponential family
if it can be written in the form

p(y;   ) = b(y) exp(  t t (y)     a(  ))

(6)

here,    is called the natural parameter (also called the canonical param-
eter) of the distribution; t (y) is the su   cient statistic (for the distribu-
tions we consider, it will often be the case that t (y) = y); and a(  ) is the log
partition function. the quantity e   a(  ) essentially plays the role of a nor-
malization constant, that makes sure the distribution p(y;   ) sums/integrates
over y to 1.

a    xed choice of t , a and b de   nes a family (or set) of distributions that
is parameterized by   ; as we vary   , we then get di   erent distributions within
this family.

we now show that the bernoulli and the gaussian distributions are ex-
amples of exponential family distributions. the bernoulli distribution with
mean   , written bernoulli(  ), speci   es a distribution over y     {0, 1}, so that
p(y = 1;   ) =   ; p(y = 0;   ) = 1       . as we vary   , we obtain bernoulli
distributions with di   erent means. we now show that this class of bernoulli
distributions, ones obtained by varying   , is in the exponential family; i.e.,
that there is a choice of t , a and b so that equation (6) becomes exactly the
class of bernoulli distributions.

5the presentation of the material in this section takes inspiration from michael i.
jordan, learning in id114 (unpublished book draft), and also mccullagh and
nelder, generalized linear models (2nd ed.).

we write the bernoulli distribution as:

23

p(y;   ) =   y(1       )1   y

= exp(y log    + (1     y) log(1       ))
= exp(cid:18)(cid:18)log(cid:18)   

1       (cid:19)(cid:19) y + log(1       )(cid:19) .

thus, the natural parameter is given by    = log(  /(1       )). interestingly, if
we invert this de   nition for    by solving for    in terms of   , we obtain    =
1/(1 + e     ). this is the familiar sigmoid function! this will come up again
when we derive id28 as a glm. to complete the formulation
of the bernoulli distribution as an exponential family distribution, we also
have

t (y) = y
a(  ) =     log(1       )
= log(1 + e  )

b(y) = 1

this shows that the bernoulli distribution can be written in the form of
equation (6), using an appropriate choice of t , a and b.

let   s now move on to consider the gaussian distribution. recall that,
when deriving id75, the value of   2 had no e   ect on our    nal
choice of    and h  (x). thus, we can choose an arbitrary value for   2 without
changing anything. to simplify the derivation below, let   s set   2 = 1.6 we
then have:

p(y;   ) =

=

1
   2  
1
   2  

exp(cid:18)   
exp(cid:18)   

1
2
1
2

(y       )2(cid:19)
y2(cid:19)    exp(cid:18)  y    

1
2

  2(cid:19)

6if we leave   2 as a variable, the gaussian distribution can also be shown to be in the
exponential family, where        r2 is now a 2-dimension vector that depends on both    and
  . for the purposes of glms, however, the   2 parameter can also be treated by considering
a more general de   nition of the exponential family: p(y;   ,    ) = b(a,    ) exp((  t t (y)    
a(  ))/c(   )). here,    is called the dispersion parameter, and for the gaussian, c(   ) =   2;
but given our simpli   cation above, we won   t need the more general de   nition for the
examples we will consider here.

thus, we see that the gaussian is in the exponential family, with

24

   =   
t (y) = y
a(  ) =   2/2
=   2/2

b(y) = (1/   2  ) exp(   y2/2).

there   re many other distributions that are members of the exponen-
tial family: the multinomial (which we   ll see later), the poisson (for mod-
elling count-data; also see the problem set); the gamma and the exponen-
tial (for modelling continuous, non-negative random variables, such as time-
intervals); the beta and the dirichlet (for distributions over probabilities);
and many more.
in the next section, we will describe a general    recipe   
for constructing models in which y (given x and   ) comes from any of these
distributions.

9 constructing glms

suppose you would like to build a model to estimate the number y of cus-
tomers arriving in your store (or number of page-views on your website) in
any given hour, based on certain features x such as store promotions, recent
advertising, weather, day-of-week, etc. we know that the poisson distribu-
tion usually gives a good model for numbers of visitors. knowing this, how
can we come up with a model for our problem? fortunately, the poisson is an
exponential family distribution, so we can apply a generalized linear model
(glm). in this section, we will we will describe a method for constructing
glm models for problems such as these.

more generally, consider a classi   cation or regression problem where we
would like to predict the value of some random variable y as a function of
x. to derive a glm for this problem, we will make the following three
assumptions about the conditional distribution of y given x and about our
model:

1. y | x;        exponentialfamily(  ). i.e., given x and   , the distribution of

y follows some exponential family distribution, with parameter   .

2. given x, our goal is to predict the expected value of t (y) given x.
in most of our examples, we will have t (y) = y, so this means we
would like the prediction h(x) output by our learned hypothesis h to

25

satisfy h(x) = e[y|x].
(note that this assumption is satis   ed in the
choices for h  (x) for both id28 and id75. for
instance, in id28, we had h  (x) = p(y = 1|x;   ) = 0    p(y =
0|x;   ) + 1    p(y = 1|x;   ) = e[y|x;   ].)

3. the natural parameter    and the inputs x are related linearly:    =   t x.

(or, if    is vector-valued, then   i =   t

i x.)

the third of these assumptions might seem the least well justi   ed of
the above, and it might be better thought of as a    design choice    in our
recipe for designing glms, rather than as an assumption per se. these
three assumptions/design choices will allow us to derive a very elegant class
of learning algorithms, namely glms, that have many desirable properties
such as ease of learning. furthermore, the resulting models are often very
e   ective for modelling di   erent types of distributions over y; for example, we
will shortly show that both id28 and ordinary least squares can
both be derived as glms.

9.1 ordinary least squares

to show that ordinary least squares is a special case of the glm family
of models, consider the setting where the target variable y (also called the
response variable in glm terminology) is continuous, and we model the
conditional distribution of y given x as a gaussian n (  ,   2). (here,    may
depend x.) so, we let the exponentialf amily(  ) distribution above be
the gaussian distribution. as we saw previously, in the formulation of the
gaussian as an exponential family distribution, we had    =   . so, we have

h  (x) = e[y|x;   ]

=   
=   
=   t x.

the    rst equality follows from assumption 2, above; the second equality
follows from the fact that y|x;        n (  ,   2), and so its expected value is given
by   ; the third equality follows from assumption 1 (and our earlier derivation
showing that    =    in the formulation of the gaussian as an exponential
family distribution); and the last equality follows from assumption 3.

26

9.2 id28

we now consider id28. here we are interested in binary classi   -
cation, so y     {0, 1}. given that y is binary-valued, it therefore seems natural
to choose the bernoulli family of distributions to model the conditional dis-
tribution of y given x. in our formulation of the bernoulli distribution as
an exponential family distribution, we had    = 1/(1 + e     ). furthermore,
note that if y|x;        bernoulli(  ), then e[y|x;   ] =   . so, following a similar
derivation as the one for ordinary least squares, we get:

h  (x) = e[y|x;   ]

=   
= 1/(1 + e     )
= 1/(1 + e     t x)

so, this gives us hypothesis functions of the form h  (x) = 1/(1 + e     t x). if
you are previously wondering how we came up with the form of the logistic
function 1/(1 + e   z), this gives one answer: once we assume that y condi-
tioned on x is bernoulli, it arises as a consequence of the de   nition of glms
and exponential family distributions.

to introduce a little more terminology, the function g giving the distri-
bution   s mean as a function of the natural parameter (g(  ) = e[t (y);   ])
is called the canonical response function. its inverse, g   1, is called the
canonical link function. thus, the canonical response function for the
gaussian family is just the identify function; and the canonical response
function for the bernoulli is the logistic function.7

9.3 softmax regression

let   s look at one more example of a glm. consider a classi   cation problem
in which the response variable y can take on any one of k values, so y    
{1, 2, . . . , k}. for example, rather than classifying email into the two classes
spam or not-spam   which would have been a binary classi   cation problem   
we might want to classify it into three classes, such as spam, personal mail,
and work-related mail. the response variable is still discrete, but can now
take on more than two values. we will thus model it as distributed according
to a multinomial distribution.

7many texts use g to denote the link function, and g   1 to denote the response function;
but the notation we   re using here, inherited from the early machine learning literature,
will be more consistent with the notation used in the rest of the class.

let   s derive a glm for modelling this type of multinomial data. to do
so, we will begin by expressing the multinomial as an exponential family
distribution.

27

to parameterize a multinomial over k possible outcomes, one could use
k parameters   1, . . . ,   k specifying the id203 of each of the outcomes.
however, these parameters would be redundant, or more formally, they would
not be independent (since knowing any k     1 of the   i   s uniquely determines
the last one, as they must satisfy pk
i=1   i = 1). so, we will instead pa-
rameterize the multinomial with only k     1 parameters,   1, . . . ,   k   1, where
  i = p(y = i;   ), and p(y = k;   ) = 1    pk   1
i=1   i. for notational convenience,
we will also let   k = 1    pk   1
i=1   i, but we should keep in mind that this is
de   ne t (y)     rk   1 as follows:

to express the multinomial as an exponential family distribution, we will

not a parameter, and that it is fully speci   ed by   1, . . . ,   k   1.

t (1) =

, t (2) =

, t (3) =

,        , t (k   1) =

, t (k) =

   

                  

1
0
0
...
0

   

                  

   

                  

0
1
0
...
0

   

                  

   

                  

0
0
1
...
0

   

                  

   

                  

0
0
0
...
1

   

                  

   

                  

0
0
0
...
0

,

   

                  

unlike our previous examples, here we do not have t (y) = y; also, t (y) is
now a k     1 dimensional vector, rather than a real number. we will write
(t (y))i to denote the i-th element of the vector t (y).
we introduce one more very useful piece of notation. an indicator func-
tion 1{  } takes on a value of 1 if its argument is true, and 0 otherwise
(1{true} = 1, 1{false} = 0). for example, 1{2 = 3} = 0, and 1{3 =
5     2} = 1. so, we can also write the relationship between t (y) and y as
(t (y))i = 1{y = i}. (before you continue reading, please make sure you un-
derstand why this is true!) further, we have that e[(t (y))i] = p (y = i) =   i.
we are now ready to show that the multinomial is a member of the

28

k

k

1

1

i=1 1{y=i}

  1{y=2}
2
  1{y=2}
2
  (t (y))2
2

exponential family. we have:
p(y;   ) =   1{y=1}
=   1{y=1}
=   (t (y))1
= exp((t (y))1 log(  1) + (t (y))2 log(  2) +

         1{y=k}
         1   pk   1
         1   pk   1
       +(cid:16)1    pk   1
= exp((t (y))1 log(  1/  k) + (t (y))2 log(  2/  k) +
       + (t (y))k   1 log(  k   1/  k) + log(  k))
= b(y) exp(  t t (y)     a(  ))

i=1 (t (y))i(cid:17) log(  k))

i=1 (t (y))i

1

k

where

   =    
            

log(  1/  k)
log(  2/  k)

...

log(  k   1/  k)

,

   
            

a(  ) =     log(  k)
b(y) = 1.

this completes our formulation of the multinomial as an exponential family
distribution.

the link function is given (for i = 1, . . . , k) by

  i = log

  i
  k

.

for convenience, we have also de   ned   k = log(  k/  k) = 0. to invert the
link function and derive the response function, we therefore have that

e  i =

  i
  k
  ke  i =   i
k
k

(7)

e  i =

  i = 1

xi=1

  k

xi=1
this implies that   k = 1/pk

quation (7) to give the response function
e  i
j=1 e  j

  i =

pk

i=1 e  i, which can be substituted back into e-

29

this function mapping from the      s to the      s is called the softmax function.
to complete our model, we use assumption 3, given earlier, that the   i   s
are linearly related to the x   s. so, have   i =   t
i x (for i = 1, . . . , k     1),
where   1, . . . ,   k   1     rn+1 are the parameters of our model. for notational
convenience, we can also de   ne   k = 0, so that   k =   t
k x = 0, as given
previously. hence, our model assumes that the conditional distribution of y
given x is given by

=

p(y = i|x;   ) =   i
pk
pk

=

e  i
j=1 e  j
e  t
i x
j=1 e  t
j x

(8)

this model, which applies to classi   cation problems where y     {1, . . . , k}, is
called softmax regression. it is a generalization of id28.

our hypothesis will output

h  (x) = e[t (y)|x;   ]

x;      
            

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

= e   
            

=    
            
   
                     

=

1{y = 1}
1{y = 2}

...

1{y = k     1}
   
  1
  2
            
...
  k   1

pk

pk

j x)

j x)

exp(  t
1 x)
j=1 exp(  t
exp(  t
2 x)
j=1 exp(  t

...
exp(  t
k   1x)
j=1 exp(  t

j x)

pk

.

   

                     

in other words, our hypothesis will output the estimated id203 that
p(y = i|x;   ), for every value of i = 1, . . . , k. (even though h  (x) as de   ned
above is only k     1 dimensional, clearly p(y = k|x;   ) can be obtained as
1    pk   1

i=1   i.)

lastly, let   s discuss parameter    tting. similar to our original derivation
of ordinary least squares and id28, if we have a training set of
m examples {(x(i), y(i)); i = 1, . . . , m} and would like to learn the parameters
  i of this model, we would begin by writing down the log-likelihood

30

   (  ) =

=

m

m

xi=1
xi=1

log p(y(i)|x(i);   )

log

k

l x(i)
j=1 e  t

yl=1  e  t
pk

j x(i)!1{y(i)=l}

to obtain the second line above, we used the de   nition for p(y|x;   ) given
in equation (8). we can now obtain the maximum likelihood estimate of
the parameters by maximizing    (  ) in terms of   , using a method such as
gradient ascent or newton   s method.

