cs229 lecture notes

andrew ng

part v
support vector machines

this set of notes presents the support vector machine (id166) learning al-
gorithm. id166s are among the best (and many believe are indeed the best)
   o   -the-shelf    supervised learning algorithms. to tell the id166 story, we   ll
need to    rst talk about margins and the idea of separating data with a large
   gap.    next, we   ll talk about the optimal margin classi   er, which will lead
us into a digression on lagrange duality. we   ll also see kernels, which give
a way to apply id166s e   ciently in very high dimensional (such as in   nite-
dimensional) feature spaces, and    nally, we   ll close o    the story with the
smo algorithm, which gives an e   cient implementation of id166s.

1 margins: intuition

we   ll start our story on id166s by talking about margins. this section will
give the intuitions about margins and about the    con   dence    of our predic-
tions; these ideas will be made formal in section 3.

consider id28, where the id203 p(y = 1|x;   ) is mod-
eled by h  (x) = g(  t x). we would then predict    1    on an input x if and
only if h  (x)     0.5, or equivalently, if and only if   t x     0. consider a
positive training example (y = 1). the larger   t x is, the larger also is
h  (x) = p(y = 1|x; w, b), and thus also the higher our degree of    con   dence   
that the label is 1. thus, informally we can think of our prediction as being
a very con   dent one that y = 1 if   t x     0. similarly, we think of logistic
regression as making a very con   dent prediction of y = 0, if   t x     0. given
a training set, again informally it seems that we   d have found a good    t to
the training data if we can    nd    so that   t x(i)     0 whenever y(i) = 1, and

1

2

  t x(i)     0 whenever y(i) = 0, since this would re   ect a very con   dent (and
correct) set of classi   cations for all the training examples. this seems to be
a nice goal to aim for, and we   ll soon formalize this idea using the notion of
functional margins.

for a di   erent type of intuition, consider the following    gure, in which x   s
represent positive training examples, o   s denote negative training examples,
a decision boundary (this is the line given by the equation   t x = 0, and
is also called the separating hyperplane) is also shown, and three points
have also been labeled a, b and c.

a

  

b

  

c

  

notice that the point a is very far from the decision boundary. if we are
asked to make a prediction for the value of y at a, it seems we should be
quite con   dent that y = 1 there. conversely, the point c is very close to
the decision boundary, and while it   s on the side of the decision boundary
on which we would predict y = 1, it seems likely that just a small change to
the decision boundary could easily have caused out prediction to be y = 0.
hence, we   re much more con   dent about our prediction at a than at c. the
point b lies in-between these two cases, and more broadly, we see that if
a point is far from the separating hyperplane, then we may be signi   cantly
more con   dent in our predictions. again, informally we think it   d be nice if,
given a training set, we manage to    nd a decision boundary that allows us
to make all correct and con   dent (meaning far from the decision boundary)
predictions on the training examples. we   ll formalize this later using the
notion of geometric margins.

3

2 notation

to make our discussion of id166s easier, we   ll    rst need to introduce a new
notation for talking about classi   cation. we will be considering a linear
classi   er for a binary classi   cation problem with labels y and features x.
from now, we   ll use y     {   1, 1} (instead of {0, 1}) to denote the class labels.
also, rather than parameterizing our linear classi   er with the vector   , we
will use parameters w, b, and write our classi   er as

hw,b(x) = g(wt x + b).

here, g(z) = 1 if z     0, and g(z) =    1 otherwise. this    w, b    notation
allows us to explicitly treat the intercept term b separately from the other
parameters. (we also drop the convention we had previously of letting x0 = 1
be an extra coordinate in the input feature vector.) thus, b takes the role of
what was previously   0, and w takes the role of [  1 . . .   n]t .

note also that, from our de   nition of g above, our classi   er will directly
predict either 1 or    1 (cf. the id88 algorithm), without    rst going
through the intermediate step of estimating the id203 of y being 1
(which was what id28 did).

3 functional and geometric margins

let   s formalize the notions of the functional and geometric margins. given a
training example (x(i), y(i)), we de   ne the functional margin of (w, b) with
respect to the training example

    (i) = y(i)(wt x + b).

note that if y(i) = 1, then for the functional margin to be large (i.e., for
our prediction to be con   dent and correct), we need wt x + b to be a large
positive number. conversely, if y(i) =    1, then for the functional margin
to be large, we need wt x + b to be a large negative number. moreover, if
y(i)(wt x + b) > 0, then our prediction on this example is correct. (check
this yourself.) hence, a large functional margin represents a con   dent and a
correct prediction.

for a linear classi   er with the choice of g given above (taking values in
{   1, 1}), there   s one property of the functional margin that makes it not a
very good measure of con   dence, however. given our choice of g, we note that
if we replace w with 2w and b with 2b, then since g(wt x + b) = g(2wt x + 2b),

4

this would not change hw,b(x) at all. i.e., g, and hence also hw,b(x), depends
only on the sign, but not on the magnitude, of wt x + b. however, replacing
(w, b) with (2w, 2b) also results in multiplying our functional margin by a
factor of 2. thus, it seems that by exploiting our freedom to scale w and b,
we can make the functional margin arbitrarily large without really changing
anything meaningful. intuitively, it might therefore make sense to impose
some sort of id172 condition such as that ||w||2 = 1; i.e., we might
replace (w, b) with (w/||w||2, b/||w||2), and instead consider the functional
margin of (w/||w||2, b/||w||2). we   ll come back to this later.
given a training set s = {(x(i), y(i)); i = 1, . . . , m}, we also de   ne the
function margin of (w, b) with respect to s as the smallest of the functional
margins of the individual training examples. denoted by     , this can therefore
be written:

     = min

i=1,...,m

    (i).

next, let   s talk about geometric margins. consider the picture below:

w

a

  

(i)

b

the decision boundary corresponding to (w, b) is shown, along with the
vector w. note that w is orthogonal (at 90   ) to the separating hyperplane.
(you should convince yourself that this must be the case.) consider the
point at a, which represents the input x(i) of some training example with
label y(i) = 1. its distance to the decision boundary,   (i), is given by the line
segment ab.

how can we    nd the value of   (i)? well, w/||w|| is a unit-length vector
pointing in the same direction as w. since a represents x(i), we therefore

5

   nd that the point b is given by x(i)       (i)    w/||w||. but this point lies on
the decision boundary, and all points x on the decision boundary satisfy the
equation wt x + b = 0. hence,

wt (cid:18)x(i)       (i) w

||w||(cid:19) + b = 0.

solving for   (i) yields

  (i) =

wt x(i) + b

||w||

=(cid:18) w

||w||(cid:19)t

x(i) +

.

b
||w||

this was worked out for the case of a positive training example at a in the
   gure, where being on the    positive    side of the decision boundary is good.
more generally, we de   ne the geometric margin of (w, b) with respect to a
training example (x(i), y(i)) to be

  (i) = y(i) (cid:18) w

||w||(cid:19)t

x(i) +

b

||w||! .

note that if ||w|| = 1, then the functional margin equals the geometric
margin   this thus gives us a way of relating these two di   erent notions of
margin. also, the geometric margin is invariant to rescaling of the parame-
ters; i.e., if we replace w with 2w and b with 2b, then the geometric margin
does not change. this will in fact come in handy later. speci   cally, because
of this invariance to the scaling of the parameters, when trying to    t w and b
to training data, we can impose an arbitrary scaling constraint on w without
changing anything important; for instance, we can demand that ||w|| = 1, or
|w1| = 5, or |w1 + b| + |w2| = 2, and any of these can be satis   ed simply by
rescaling w and b.
finally, given a training set s = {(x(i), y(i)); i = 1, . . . , m}, we also de   ne
the geometric margin of (w, b) with respect to s to be the smallest of the
geometric margins on the individual training examples:

   = min

i=1,...,m

  (i).

4 the optimal margin classi   er

given a training set, it seems from our previous discussion that a natural
desideratum is to try to    nd a decision boundary that maximizes the (ge-
ometric) margin, since this would re   ect a very con   dent set of predictions

6

on the training set and a good       t    to the training data. speci   cally, this
will result in a classi   er that separates the positive and the negative training
examples with a    gap    (geometric margin).

for now, we will assume that we are given a training set that is linearly
separable; i.e., that it is possible to separate the positive and negative ex-
amples using some separating hyperplane. how will we    nd the one that
achieves the maximum geometric margin? we can pose the following opti-
mization problem:

max  ,w,b   

s.t. y(i)(wt x(i) + b)       ,

||w|| = 1.

i = 1, . . . , m

i.e., we want to maximize   , subject to each training example having func-
tional margin at least   . the ||w|| = 1 constraint moreover ensures that the
functional margin equals to the geometric margin, so we are also guaranteed
that all the geometric margins are at least   . thus, solving this problem will
result in (w, b) with the largest possible geometric margin with respect to the
training set.

if we could solve the optimization problem above, we   d be done. but the
   ||w|| = 1    constraint is a nasty (non-convex) one, and this problem certainly
isn   t in any format that we can plug into standard optimization software to
solve. so, let   s try transforming the problem into a nicer one. consider:

max    ,w,b

    
||w||

s.t. y(i)(wt x(i) + b)         ,

i = 1, . . . , m

here, we   re going to maximize     /||w||, subject to the functional margins all
being at least     . since the geometric and functional margins are related by
   =     /||w|, this will give us the answer we want. moreover, we   ve gotten rid
of the constraint ||w|| = 1 that we didn   t like. the downside is that we now
    
have a nasty (again, non-convex) objective
||w|| function; and, we still don   t
have any o   -the-shelf software that can solve this form of an optimization
problem.

let   s keep going. recall our earlier discussion that we can add an arbi-
trary scaling constraint on w and b without changing anything. this is the
key idea we   ll use now. we will introduce the scaling constraint that the
functional margin of w, b with respect to the training set must be 1:

     = 1.

7

since multiplying w and b by some constant results in the functional margin
being multiplied by that same constant, this is indeed a scaling constraint,
and can be satis   ed by rescaling w, b. plugging this into our problem above,
and noting that maximizing     /||w|| = 1/||w|| is the same thing as minimizing
||w||2, we now have the following optimization problem:

minw,b

1
2||w||2

s.t. y(i)(wt x(i) + b)     1,

i = 1, . . . , m

we   ve now transformed the problem into a form that can be e   ciently
solved. the above is an optimization problem with a convex quadratic ob-
jective and only linear constraints. its solution gives us the optimal mar-
gin classi   er. this optimization problem can be solved using commercial
quadratic programming (qp) code.1

while we could call the problem solved here, what we will instead do is
make a digression to talk about lagrange duality. this will lead us to our
optimization problem   s dual form, which will play a key role in allowing us to
use kernels to get optimal margin classi   ers to work e   ciently in very high
dimensional spaces. the dual form will also allow us to derive an e   cient
algorithm for solving the above optimization problem that will typically do
much better than generic qp software.

5 lagrange duality

let   s temporarily put aside id166s and maximum margin classi   ers, and talk
about solving constrained optimization problems.

consider a problem of the following form:

minw

f (w)

s.t. hi(w) = 0,

i = 1, . . . , l.

some of you may recall how the method of lagrange multipliers can be used
to solve it. (don   t worry if you haven   t seen it before.) in this method, we
de   ne the lagrangian to be

l(w,   ) = f (w) +

  ihi(w)

l

xi=1

1you may be familiar with id135, which solves optimization problems
that have linear objectives and linear constraints. qp software is also widely available,
which allows convex quadratic objectives and linear constraints.

8

here, the   i   s are called the lagrange multipliers. we would then    nd
and set l   s partial derivatives to zero:
= 0;

= 0,

   l
   wi

   l
     i

and solve for w and   .

in this section, we will generalize this to constrained optimization prob-
lems in which we may have inequality as well as equality constraints. due to
time constraints, we won   t really be able to do the theory of lagrange duality
justice in this class,2 but we will give the main ideas and results, which we
will then apply to our optimal margin classi   er   s optimization problem.

consider the following, which we   ll call the primal optimization problem:

minw

f (w)

s.t. gi(w)     0,
hi(w) = 0,

i = 1, . . . , k
i = 1, . . . , l.

to solve it, we start by de   ning the generalized lagrangian

l(w,   ,   ) = f (w) +

  igi(w) +

k

xi=1

  ihi(w).

l

xi=1

here, the   i   s and   i   s are the lagrange multipliers. consider the quantity

  p(w) = max

  ,   :   i   0l(w,   ,   ).

here, the    p    subscript stands for    primal.    let some w be given.
if w
violates any of the primal constraints (i.e., if either gi(w) > 0 or hi(w) 6= 0
for some i), then you should be able to verify that

  p(w) = max

  ,   :   i   0

f (w) +

=    .

  igi(w) +

k

xi=1

  ihi(w)

l

xi=1

(1)

(2)

conversely, if the constraints are indeed satis   ed for a particular value of w,
then   p(w) = f (w). hence,

  p(w) =(cid:26) f (w)

    otherwise.

if w satis   es primal constraints

2readers interested in learning more about this topic are encouraged to read, e.g., r.

t. rockarfeller (1970), convex analysis, princeton university press.

9

thus,   p takes the same value as the objective in our problem for all val-
ues of w that satis   es the primal constraints, and is positive in   nity if the
constraints are violated. hence, if we consider the minimization problem

min

w

  p(w) = min

w

  ,   :   i   0l(w,   ,   ),
max

we see that it is the same problem (i.e., and has the same solutions as) our
original, primal problem. for later use, we also de   ne the optimal value of
the objective to be p    = minw   p(w); we call this the value of the primal
problem.

now, let   s look at a slightly di   erent problem. we de   ne

  d(  ,   ) = min

w l(w,   ,   ).

here, the    d    subscript stands for    dual.    note also that whereas in the
de   nition of   p we were optimizing (maximizing) with respect to   ,   , here
we are minimizing with respect to w.

we can now pose the dual optimization problem:

max

  ,   :   i   0

  d(  ,   ) = max

  ,   :   i   0

min
w l(w,   ,   ).

this is exactly the same as our primal problem shown above, except that the
order of the    max    and the    min    are now exchanged. we also de   ne the
optimal value of the dual problem   s objective to be d    = max  ,   :   i   0   d(w).
how are the primal and the id78 related? it can easily be shown

that

d    = max

  ,   :   i   0

min
w l(w,   ,   )     min

w

  ,   :   i   0l(w,   ,   ) = p   .
max

(you should convince yourself of this; this follows from the    max min    of a
function always being less than or equal to the    min max.   ) however, under
certain conditions, we will have

d    = p   ,

so that we can solve the dual problem in lieu of the primal problem. let   s
see what these conditions are.

suppose f and the gi   s are convex,3 and the hi   s are a   ne.4 suppose
further that the constraints gi are (strictly) feasible; this means that there
exists some w so that gi(w) < 0 for all i.

3when f has a hessian, then it is convex if and only if the hessian is positive semi-
de   nite. for instance, f (w) = wt w is convex; similarly, all linear (and a   ne) functions
are also convex. (a function f can also be convex without being di   erentiable, but we
won   t need those more general de   nitions of convexity here.)

4i.e., there exists ai, bi, so that hi(w) = at

i w + bi.    a   ne    means the same thing as

linear, except that we also allow the extra intercept term bi.

10

under our above assumptions, there must exist w   ,      ,       so that w    is the
solution to the primal problem,      ,       are the solution to the dual problem,
and moreover p    = d    = l(w   ,      ,      ). moreover, w   ,       and       satisfy the
karush-kuhn-tucker (kkt) conditions, which are as follows:

   
   wil(w   ,      ,      ) = 0,
   
     il(w   ,      ,      ) = 0,
i gi(w   ) = 0,
     
gi(w   )     0,
          0,

i = 1, . . . , n

i = 1, . . . , l

i = 1, . . . , k
i = 1, . . . , k
i = 1, . . . , k

(3)

(4)

(5)
(6)
(7)

moreover, if some w   ,      ,       satisfy the kkt conditions, then it is also a
solution to the primal and id78.

we draw attention to equation (5), which is called the kkt dual com-
i > 0, then gi(w   ) =
plementarity condition. speci   cally, it implies that if      
0. (i.e., the    gi(w)     0    constraint is active, meaning it holds with equality
rather than with inequality.) later on, this will be key for showing that the
id166 has only a small number of    support vectors   ; the kkt dual comple-
mentarity condition will also give us our convergence test when we talk about
the smo algorithm.

6 optimal margin classi   ers

previously, we posed the following (primal) optimization problem for    nding
the optimal margin classi   er:

minw,b

1
2||w||2

s.t. y(i)(wt x(i) + b)     1,

i = 1, . . . , m

we can write the constraints as

gi(w) =    y(i)(wt x(i) + b) + 1     0.

we have one such constraint for each training example. note that from the
kkt dual complementarity condition, we will have   i > 0 only for the train-
ing examples that have functional margin exactly equal to one (i.e., the ones

corresponding to constraints that hold with equality, gi(w) = 0). consid-
er the    gure below, in which a maximum margin separating hyperplane is
shown by the solid line.

11

the points with the smallest margins are exactly the ones closest to the
decision boundary; here, these are the three points (one negative and two pos-
itive examples) that lie on the dashed lines parallel to the decision boundary.
thus, only three of the   i   s   namely, the ones corresponding to these three
training examples   will be non-zero at the optimal solution to our optimiza-
tion problem. these three points are called the support vectors in this
problem. the fact that the number of support vectors can be much smaller
than the size the training set will be useful later.

let   s move on. looking ahead, as we develop the dual form of the prob-
lem, one key idea to watch out for is that we   ll try to write our algorithm
in terms of only the inner product hx(i), x(j)i (think of this as (x(i))t x(j))
between points in the input feature space. the fact that we can express our
algorithm in terms of these inner products will be key when we apply the
kernel trick.

when we construct the lagrangian for our optimization problem we have:

l(w, b,   ) =

1
2||w||2    

m

xi=1

  i(cid:2)y(i)(wt x(i) + b)     1(cid:3) .

(8)

note that there   re only      i    but no      i    lagrange multipliers, since the
problem has only inequality constraints.

let   s    nd the dual form of the problem. to do so, we need to    rst
minimize l(w, b,   ) with respect to w and b (for    xed   ), to get   d, which

12

we   ll do by setting the derivatives of l with respect to w and b to zero. we
have:

m

this implies that

   wl(w, b,   ) = w    

  iy(i)x(i) = 0

xi=1

m

as for the derivative with respect to b, we obtain

w =

  iy(i)x(i).

xi=1

   
   bl(w, b,   ) =

m

xi=1

  iy(i) = 0.

(9)

(10)

if we take the de   nition of w in equation (9) and plug that back into the

lagrangian (equation 8), and simplify, we get

l(w, b,   ) =

m

xi=1

  i    

1
2

m

xi,j=1

y(i)y(j)  i  j(x(i))t x(j)     b

  iy(i).

m

xi=1

but from equation (10), the last term must be zero, so we obtain

l(w, b,   ) =

m

xi=1

  i    

1
2

m

xi,j=1

y(i)y(j)  i  j(x(i))t x(j).

recall that we got to the equation above by minimizing l with respect to w
and b. putting this together with the constraints   i     0 (that we always had)
and the constraint (10), we obtain the following dual optimization problem:

max   w (  ) =

s.t.   i     0,

m

  i    

1
2

m

xi=1

m

xi,j=1

i = 1, . . . , m

y(i)y(j)  i  jhx(i), x(j)i.

  iy(i) = 0,

xi=1

you should also be able to verify that the conditions required for p    =
d    and the kkt conditions (equations 3   7) to hold are indeed satis   ed in
our optimization problem. hence, we can solve the dual in lieu of solving
the primal problem. speci   cally, in the dual problem above, we have a
maximization problem in which the parameters are the   i   s. we   ll talk later

13

about the speci   c algorithm that we   re going to use to solve the dual problem,
but if we are indeed able to solve it (i.e.,    nd the      s that maximize w (  )
subject to the constraints), then we can use equation (9) to go back and    nd
the optimal w   s as a function of the      s. having found w   , by considering
the primal problem, it is also straightforward to    nd the optimal value for
the intercept term b as

b    =    

maxi:y(i)=   1 w   t x(i) + mini:y(i)=1 w   t x(i)

2

.

(11)

(check for yourself that this is correct.)

before moving on, let   s also take a more careful look at equation (9),
which gives the optimal value of w in terms of (the optimal value of)   .
suppose we   ve    t our model   s parameters to a training set, and now wish to
make a prediction at a new point input x. we would then calculate wt x + b,
and predict y = 1 if and only if this quantity is bigger than zero. but
using (9), this quantity can also be written:

  iy(i)x(i)!t
wt x + b =   m
xi=1
xi=1
  iy(i)hx(i), xi + b.

=

m

x + b

(12)

(13)

hence, if we   ve found the   i   s, in order to make a prediction, we have to
calculate a quantity that depends only on the inner product between x and
the points in the training set. moreover, we saw earlier that the   i   s will all
be zero except for the support vectors. thus, many of the terms in the sum
above will be zero, and we really need to    nd only the inner products between
x and the support vectors (of which there is often only a small number) in
order calculate (13) and make our prediction.

by examining the dual form of the optimization problem, we gained sig-
ni   cant insight into the structure of the problem, and were also able to write
the entire algorithm in terms of only inner products between input feature
vectors. in the next section, we will exploit this property to apply the ker-
nels to our classi   cation problem. the resulting algorithm, support vector
machines, will be able to e   ciently learn in very high dimensional spaces.

7 kernels

back in our discussion of id75, we had a problem in which the
input x was the living area of a house, and we considered performing regres-

14

sion using the features x, x2 and x3 (say) to obtain a cubic function. to
distinguish between these two sets of variables, we   ll call the    original    input
value the input attributes of a problem (in this case, x, the living area).
when that is mapped to some new set of quantities that are then passed to
the learning algorithm, we   ll call those new quantities the input features.
(unfortunately, di   erent authors use di   erent terms to describe these two
things, but we   ll try to use this terminology consistently in these notes.) we
will also let    denote the feature mapping, which maps from the attributes
to the features. for instance, in our example, we had

  (x) =   
   

x
x2

x3    
    .

rather than applying id166s using the original input attributes x, we may
instead want to learn using some features   (x). to do so, we simply need to
go over our previous algorithm, and replace x everywhere in it with   (x).

since the algorithm can be written entirely in terms of the inner prod-
ucts hx, zi, this means that we would replace all those inner products with
h  (x),   (z)i. speci   cally, given a feature mapping   , we de   ne the corre-
sponding kernel to be

k(x, z) =   (x)t   (z).

then, everywhere we previously had hx, zi in our algorithm, we could simply
replace it with k(x, z), and our algorithm would now be learning using the
features   .

now, given   , we could easily compute k(x, z) by    nding   (x) and   (z)
and taking their inner product. but what   s more interesting is that often,
k(x, z) may be very inexpensive to calculate, even though   (x) itself may
be very expensive to calculate (perhaps because it is an extremely high di-
mensional vector). in such settings, by using in our algorithm an e   cient
way to calculate k(x, z), we can get id166s to learn in the high dimensional
feature space space given by   , but without ever having to explicitly    nd or
represent vectors   (x).

let   s see an example. suppose x, z     rn, and consider

k(x, z) = (xt z)2.

15

xjzj!

we can also write this as

n

n

xizi!  n
k(x, z) =   n
xj=1
xi=1
xi=1
xj=1
xi,j=1

(xixj)(zizj)

xixjzizj

=

=

n

thus, we see that k(x, z) =   (x)t   (z), where the feature mapping    is given
(shown here for the case of n = 3) by

  (x) =

.

x1x1
x1x2
x1x3
x2x1
x2x2
x2x3
x3x1
x3x2
x3x3

   

                                       

   

                                       

note that whereas calculating the high-dimensional   (x) requires o(n2) time,
   nding k(x, z) takes only o(n) time   linear in the dimension of the input
attributes.

for a related kernel, also consider

k(x, z) = (xt z + c)2

n

=

(xixj)(zizj) +

xi,j=1

n

(   2cxi)(   2czi) + c2.
xi=1

(check this yourself.) this corresponds to the feature mapping (again shown

for n = 3)

16

  (x) =

,

x1x1
x1x2
x1x3
x2x1
x2x2
x2x3
x3x1
x3x2

x3x3   2cx1   2cx2   2cx3

c

   

                                                               

   

                                                               

and the parameter c controls the relative weighting between the xi (   rst
order) and the xixj (second order) terms.

more broadly, the kernel k(x, z) = (xt z + c)d corresponds to a feature

mapping to an (cid:0)n+d

d (cid:1) feature space, corresponding of all monomials of the

form xi1xi2 . . . xik that are up to order d. however, despite working in this
o(nd)-dimensional space, computing k(x, z) still takes only o(n) time, and
hence we never need to explicitly represent feature vectors in this very high
dimensional feature space.

now, let   s talk about a slightly di   erent view of kernels. intuitively, (and
there are things wrong with this intuition, but nevermind), if   (x) and   (z)
are close together, then we might expect k(x, z) =   (x)t   (z) to be large.
conversely, if   (x) and   (z) are far apart   say nearly orthogonal to each
other   then k(x, z) =   (x)t   (z) will be small. so, we can think of k(x, z)
as some measurement of how similar are   (x) and   (z), or of how similar are
x and z.

given this intuition, suppose that for some learning problem that you   re
working on, you   ve come up with some function k(x, z) that you think might
be a reasonable measure of how similar x and z are. for instance, perhaps
you chose

k(x, z) = exp(cid:18)   ||x     z||2
2  2 (cid:19) .

this is a reasonable measure of x and z   s similarity, and is close to 1 when
x and z are close, and near 0 when x and z are far apart. can we use this
de   nition of k as the kernel in an id166? in this particular example, the
answer is yes. (this kernel is called the gaussian kernel, and corresponds

17

to an in   nite dimensional feature mapping   .) but more broadly, given some
function k, how can we tell if it   s a valid kernel; i.e., can we tell if there is
some feature mapping    so that k(x, z) =   (x)t   (z) for all x, z?

suppose for now that k is indeed a valid kernel corresponding to some
feature mapping   . now, consider some    nite set of m points (not necessarily
the training set) {x(1), . . . , x(m)}, and let a square, m-by-m matrix k be
de   ned so that its (i, j)-entry is given by kij = k(x(i), x(j)). this matrix
is called the kernel matrix. note that we   ve overloaded the notation and
used k to denote both the id81 k(x, z) and the kernel matrix k,
due to their obvious close relationship.

now, if k is a valid kernel, then kij = k(x(i), x(j)) =   (x(i))t   (x(j)) =
  (x(j))t   (x(i)) = k(x(j), x(i)) = kji, and hence k must be symmetric. more-
over, letting   k(x) denote the k-th coordinate of the vector   (x), we    nd that
for any vector z, we have

zikijzj

zt kz = xi xj
= xi xj
= xi xj
zixk
= xk xi xj
= xk  xi

    0.

zi  (x(i))t   (x(j))zj

  k(x(i))  k(x(j))zj

zi  k(x(i))  k(x(j))zj

zi  k(x(i))!2

the second-to-last step above used the same trick as you saw in problem
set 1 q1. since z was arbitrary, this shows that k is positive semi-de   nite
(k     0).
hence, we   ve shown that if k is a valid kernel (i.e., if it corresponds to
some feature mapping   ), then the corresponding kernel matrix k     rm  m
is symmetric positive semide   nite. more generally, this turns out to be not
only a necessary, but also a su   cient, condition for k to be a valid kernel
(also called a mercer kernel). the following result is due to mercer.5

5many texts present mercer   s theorem in a slightly more complicated form involving
l2 functions, but when the input attributes take values in rn, the version given here is
equivalent.

18

theorem (mercer). let k : rn    rn 7    r be given. then for k
to be a valid (mercer) kernel, it is necessary and su   cient that for any
{x(1), . . . , x(m)}, (m <    ), the corresponding kernel matrix is symmetric
positive semi-de   nite.

given a function k, apart from trying to    nd a feature mapping    that
corresponds to it, this theorem therefore gives another way of testing if it is
a valid kernel. you   ll also have a chance to play with these ideas more in
problem set 2.

in class, we also brie   y talked about a couple of other examples of ker-
nels. for instance, consider the digit recognition problem, in which given an
image (16x16 pixels) of a handwritten digit (0-9), we have to    gure out which
digit it was. using either a simple polynomial kernel k(x, z) = (xt z)d or
the gaussian kernel, id166s were able to obtain extremely good performance
on this problem. this was particularly surprising since the input attributes
x were just 256-dimensional vectors of the image pixel intensity values, and
the system had no prior knowledge about vision, or even about which pixels
are adjacent to which other ones. another example that we brie   y talked
about in lecture was that if the objects x that we are trying to classify are
strings (say, x is a list of amino acids, which strung together form a protein),
then it seems hard to construct a reasonable,    small    set of features for
most learning algorithms, especially if di   erent strings have di   erent length-
s. however, consider letting   (x) be a feature vector that counts the number
of occurrences of each length-k substring in x. if we   re considering strings
of english letters, then there are 26k such strings. hence,   (x) is a 26k di-
mensional vector; even for moderate values of k, this is probably too big for
us to e   ciently work with. (e.g., 264     460000.) however, using (dynam-
ic programming-ish) string matching algorithms, it is possible to e   ciently
compute k(x, z) =   (x)t   (z), so that we can now implicitly work in this
26k-dimensional feature space, but without ever explicitly computing feature
vectors in this space.

the application of kernels to support vector machines should already
be clear and so we won   t dwell too much longer on it here. keep in mind
however that the idea of kernels has signi   cantly broader applicability than
id166s. speci   cally, if you have any learning algorithm that you can write
in terms of only inner products hx, zi between input attribute vectors, then
by replacing this with k(x, z) where k is a kernel, you can    magically   
allow your algorithm to work e   ciently in the high dimensional feature space
corresponding to k. for instance, this kernel trick can be applied with the
id88 to derive a kernel id88 algorithm. many of the algorithms

19

that we   ll see later in this class will also be amenable to this method, which
has come to be known as the    kernel trick.   

8 id173 and the non-separable case

the derivation of the id166 as presented so far assumed that the data is
linearly separable. while mapping data to a high dimensional feature space
via    does generally increase the likelihood that the data is separable, we
can   t guarantee that it always will be so. also, in some cases it is not clear
that    nding a separating hyperplane is exactly what we   d want to do, since
that might be susceptible to outliers. for instance, the left    gure below
shows an optimal margin classi   er, and when a single outlier is added in the
upper-left region (right    gure), it causes the decision boundary to make a
dramatic swing, and the resulting classi   er has a much smaller margin.

to make the algorithm work for non-linearly separable datasets as well
as be less sensitive to outliers, we reformulate our optimization (using    1
id173) as follows:

min  ,w,b

1
2||w||2 + c

  i

m

xi=1

s.t. y(i)(wt x(i) + b)     1       i,

i = 1, . . . , m.

  i     0,

i = 1, . . . , m

thus, examples are now permitted to have (functional) margin less than 1,
and if an example has functional margin 1       i (with    > 0), we would pay
a cost of the objective function being increased by c  i. the parameter c
controls the relative weighting between the twin goals of making the ||w||2
small (which we saw earlier makes the margin large) and of ensuring that
most examples have functional margin at least 1.

20

as before, we can form the lagrangian:

l(w, b,   ,   , r) =

1
2

wt w + c

m

xi=1

  i    

m

xi=1

  i(cid:2)y(i)(xt w + b)     1 +   i(cid:3)   

ri  i.

m

xi=1

here, the   i   s and ri   s are our lagrange multipliers (constrained to be     0).
we won   t go through the derivation of the dual again in detail, but after
setting the derivatives with respect to w and b to zero as before, substituting
them back in, and simplifying, we obtain the following dual form of the
problem:

m

max   w (  ) =

xi=1
s.t. 0       i     c,

m

m

1
2

  i    
i = 1, . . . , m

xi,j=1

y(i)y(j)  i  jhx(i), x(j)i

  iy(i) = 0,

xi=1

as before, we also have that w can be expressed in terms of the   i   s
as given in equation (9), so that after solving the dual problem, we can
continue to use equation (13) to make our predictions. note that, somewhat
surprisingly, in adding    1 id173, the only change to the dual problem
is that what was originally a constraint that 0       i has now become 0    
  i     c. the calculation for b    also has to be modi   ed (equation 11 is no
longer valid); see the comments in the next section/platt   s paper.
also, the kkt dual-complementarity conditions (which in the next sec-
tion will be useful for testing for the convergence of the smo algorithm)
are:

  i = 0     y(i)(wt x(i) + b)     1
  i = c     y(i)(wt x(i) + b)     1
0 <   i < c     y(i)(wt x(i) + b) = 1.

(14)
(15)
(16)

now, all that remains is to give an algorithm for actually solving the dual

problem, which we will do in the next section.

9 the smo algorithm

the smo (sequential minimal optimization) algorithm, due to john platt,
gives an e   cient way of solving the dual problem arising from the derivation

21

of the id166. partly to motivate the smo algorithm, and partly because it   s
interesting in its own right, let   s    rst take another digression to talk about
the coordinate ascent algorithm.

9.1 coordinate ascent

consider trying to solve the unconstrained optimization problem

max

  

w (  1,   2, . . . ,   m).

here, we think of w as just some function of the parameters   i   s, and for now
ignore any relationship between this problem and id166s. we   ve already seen
two optimization algorithms, gradient ascent and newton   s method. the
new algorithm we   re going to consider here is called coordinate ascent:

loop until convergence: {

for i = 1, . . . , m, {

  i := arg max     i w (  1, . . . ,   i   1,     i,   i+1, . . . ,   m).

}

}
thus, in the innermost loop of this algorithm, we will hold all the vari-
ables except for some   i    xed, and reoptimize w with respect to just the
parameter   i. in the version of this method presented here, the inner-loop
reoptimizes the variables in order   1,   2, . . . ,   m,   1,   2, . . .. (a more sophis-
ticated version might choose other orderings; for instance, we may choose
the next variable to update according to which one we expect to allow us to
make the largest increase in w (  ).)

when the function w happens to be of such a form that the    arg max   
in the inner loop can be performed e   ciently, then coordinate ascent can be
a fairly e   cient algorithm. here   s a picture of coordinate ascent in action:

22

2.5

2

1.5

1

0.5

0

   0.5

   1

   1.5

   2

   2

   1.5

   1

   0.5

0

0.5

1

1.5

2

2.5

the ellipses in the    gure are the contours of a quadratic function that
we want to optimize. coordinate ascent was initialized at (2,   2), and also
plotted in the    gure is the path that it took on its way to the global maximum.
notice that on each step, coordinate ascent takes a step that   s parallel to one
of the axes, since only one variable is being optimized at a time.

9.2 smo

we close o    the discussion of id166s by sketching the derivation of the smo
algorithm. some details will be left to the homework, and for others you
may refer to the paper excerpt handed out in class.

here   s the (dual) optimization problem that we want to solve:

m

max   w (  ) =

xi=1
s.t. 0       i     c,

m

m

1
2

  i    
i = 1, . . . , m

xi,j=1

y(i)y(j)  i  jhx(i), x(j)i.

  iy(i) = 0.

xi=1

(17)

(18)

(19)

let   s say we have set of   i   s that satisfy the constraints (18-19). now,
suppose we want to hold   2, . . . ,   m    xed, and take a coordinate ascent step
and reoptimize the objective with respect to   1. can we make any progress?
the answer is no, because the constraint (19) ensures that

  1y(1) =    

  iy(i).

m

xi=2

or, by multiplying both sides by y(1), we equivalently have

m

23

  1 =    y(1)

  iy(i).

xi=2

(this step used the fact that y(1)     {   1, 1}, and hence (y(1))2 = 1.) hence,
  1 is exactly determined by the other   i   s, and if we were to hold   2, . . . ,   m
   xed, then we can   t make any change to   1 without violating the constrain-
t (19) in the optimization problem.

thus, if we want to update some subject of the   i   s, we must update at
least two of them simultaneously in order to keep satisfying the constraints.
this motivates the smo algorithm, which simply does the following:

repeat till convergence {
1. select some pair   i and   j to update next (using a heuristic that
tries to pick the two that will allow us to make the biggest progress
towards the global maximum).

2. reoptimize w (  ) with respect to   i and   j, while holding all the

other   k   s (k 6= i, j)    xed.

}
to test for convergence of this algorithm, we can check whether the kkt
conditions (equations 14-16) are satis   ed to within some tol. here, tol is
the convergence tolerance parameter, and is typically set to around 0.01 to
0.001. (see the paper and pseudocode for details.)

the key reason that smo is an e   cient algorithm is that the update to
  i,   j can be computed very e   ciently. let   s now brie   y sketch the main
ideas for deriving the e   cient update.

let   s say we currently have some setting of the   i   s that satisfy the con-
straints (18-19), and suppose we   ve decided to hold   3, . . . ,   m    xed, and
want to reoptimize w (  1,   2, . . . ,   m) with respect to   1 and   2 (subject to
the constraints). from (19), we require that

m

  1y(1) +   2y(2) =    

  iy(i).

xi=3

since the right hand side is    xed (as we   ve    xed   3, . . .   m), we can just let
it be denoted by some constant   :

we can thus picture the constraints on   1 and   2 as follows:

  1y(1) +   2y(2) =   .

(20)

24

c

h
  2

l

(1)
  1
y

+

  2

(2)
y =  

  1

c

from the constraints (18), we know that   1 and   2 must lie within the box
[0, c]   [0, c] shown. also plotted is the line   1y(1) +   2y(2) =   , on which we
know   1 and   2 must lie. note also that, from these constraints, we know
l       2     h; otherwise, (  1,   2) can   t simultaneously satisfy both the box
and the straight line constraint. in this example, l = 0. but depending on
what the line   1y(1) +   2y(2) =    looks like, this won   t always necessarily be
the case; but more generally, there will be some lower-bound l and some
upper-bound h on the permissible values for   2 that will ensure that   1,   2
lie within the box [0, c]    [0, c].

using equation (20), we can also write   1 as a function of   2:

  1 = (         2y(2))y(1).

(check this derivation yourself; we again used the fact that y(1)     {   1, 1} so
that (y(1))2 = 1.) hence, the objective w (  ) can be written

w (  1,   2, . . . ,   m) = w ((         2y(2))y(1),   2, . . . ,   m).

treating   3, . . . ,   m as constants, you should be able to verify that this is
just some quadratic function in   2. i.e., this can also be expressed in the
form a  2
2 + b  2 + c for some appropriate a, b, and c. if we ignore the    box   
constraints (18) (or, equivalently, that l       2     h), then we can easily
maximize this quadratic function by setting its derivative to zero and solving.
we   ll let   new,unclipped
denote the resulting value of   2. you should also be
able to convince yourself that if we had instead wanted to maximize w with
respect to   2 but subject to the box constraint, then we can    nd the resulting
value optimal simply by taking   new,unclipped
and    clipping    it to lie in the

2

2

25

    h

h
  new,unclipped
2
l

2

if   new,unclipped
> h
if l       new,unclipped
if   new,unclipped
< l

2

2

[l, h] interval, to get

  new

2

=       
   

finally, having found the   new
the optimal value of   new

.

2

1

, we can use equation (20) to go back and    nd

there   re a couple more details that are quite easy but that we   ll leave you
to read about yourself in platt   s paper: one is the choice of the heuristics
used to select the next   i,   j to update; the other is how to update b as the
smo algorithm is run.

