cs229 lecture notes

andrew ng

part vi
learning theory

1 bias/variance tradeo   

when talking about id75, we discussed the problem of whether
to    t a    simple    model such as the linear    y =   0 +  1x,    or a more    complex   
model such as the polynomial    y =   0 +   1x+           5x5.    we saw the following
example:

y

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

y

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1

2

3

4

5

6

7

x

y

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1

2

3

4

5

6

7

x

1

2

3

4

5

6

7

x

fitting a 5th order polynomial to the data (rightmost    gure) did not
result in a good model. speci   cally, even though the 5th order polynomial
did a very good job predicting y (say, prices of houses) from x (say, living
area) for the examples in the training set, we do not expect the model shown
to be a good one for predicting the prices of houses not in the training set. in
other words, what   s has been learned from the training set does not generalize
well to other houses. the generalization error (which will be made formal
shortly) of a hypothesis is its expected error on examples not necessarily in
the training set.

both the models in the leftmost and the rightmost    gures above have
large generalization error. however, the problems that the two models su   er
if the relationship between y and x is not linear,
from are very di   erent.

1

2

then even if we were    tting a linear model to a very large amount of training
data, the linear model would still fail to accurately capture the structure
in the data. informally, we de   ne the bias of a model to be the expected
generalization error even if we were to    t it to a very (say, in   nitely) large
training set. thus, for the problem above, the linear model su   ers from large
bias, and may under   t (i.e., fail to capture structure exhibited by) the data.
apart from bias, there   s a second component to the generalization error,
consisting of the variance of a model    tting procedure. speci   cally, when
   tting a 5th order polynomial as in the rightmost    gure, there is a large risk
that we   re    tting patterns in the data that happened to be present in our
small,    nite training set, but that do not re   ect the wider pattern of the
relationship between x and y. this could be, say, because in the training set
we just happened by chance to get a slightly more-expensive-than-average
house here, and a slightly less-expensive-than-average house there, and so
on. by    tting these    spurious    patterns in the training set, we might again
obtain a model with large generalization error. in this case, we say the model
has large variance.1

often, there is a tradeo    between bias and variance. if our model is too
   simple    and has very few parameters, then it may have large bias (but small
variance); if it is too    complex    and has very many parameters, then it may
su   er from large variance (but have smaller bias).
in the example above,
   tting a quadratic function does better than either of the extremes of a    rst
or a    fth order polynomial.

2 preliminaries

in this set of notes, we begin our foray into learning theory. apart from
being interesting and enlightening in its own right, this discussion will also
help us hone our intuitions and derive rules of thumb about how to best
apply learning algorithms in di   erent settings. we will also seek to answer
a few questions: first, can we make formal the bias/variance tradeo    that
was just discussed? the will also eventually lead us to talk about model
selection methods, which can, for instance, automatically decide what order
polynomial to    t to a training set. second, in machine learning it   s really

1in these notes, we will not try to formalize the de   nitions of bias and variance beyond
this discussion. while bias and variance are straightforward to de   ne formally for, e.g.,
id75, there have been several proposals for the de   nitions of bias and variance
for classi   cation, and there is as yet no agreement on what is the    right    and/or the most
useful formalism.

3

generalization error that we care about, but most learning algorithms    t their
models to the training set. why should doing well on the training set tell us
anything about generalization error? speci   cally, can we relate error on the
training set to generalization error? third and    nally, are there conditions
under which we can actually prove that learning algorithms will work well?

we start with two simple but very useful lemmas.

lemma. (the union bound). let a1, a2, . . . , ak be k di   erent events (that
may not be independent). then

p (a1                  ak)     p (a1) + . . . + p (ak).

in id203 theory, the union bound is usually stated as an axiom
(and thus we won   t try to prove it), but it also makes intuitive sense: the
id203 of any one of k events happening is at most the sums of the
probabilities of the k di   erent events.

lemma. (hoe   ding inequality) let z1, . . . , zm be m independent and iden-
tically distributed (iid) random variables drawn from a bernoulli(  ) distri-
i=1 zi

bution. i.e., p (zi = 1) =   , and p (zi = 0) = 1       . let      = (1/m)pm

be the mean of these random variables, and let any    > 0 be    xed. then

p (|           | >   )     2 exp(   2  2m)

this lemma (which in learning theory is also called the cherno    bound)
says that if we take        the average of m bernoulli(  ) random variables   to
be our estimate of   , then the id203 of our being far from the true value
is small, so long as m is large. another way of saying this is that if you have
a biased coin whose chance of landing on heads is   , then if you toss it m
times and calculate the fraction of times that it came up heads, that will be
a good estimate of    with high id203 (if m is large).

using just these two lemmas, we will be able to prove some of the deepest

and most important results in learning theory.

to simplify our exposition, let   s restrict our attention to binary classi   ca-
tion in which the labels are y     {0, 1}. everything we   ll say here generalizes
to other, including regression and multi-class classi   cation, problems.
we assume we are given a training set s = {(x(i), y(i)); i = 1, . . . , m}
of size m, where the training examples (x(i), y(i)) are drawn iid from some
id203 distribution d. for a hypothesis h, we de   ne the training error
(also called the empirical risk or empirical error in learning theory) to
be

m

    (h) =

1
m

1{h(x(i)) 6= y(i)}.

xi=1

4

this is just the fraction of training examples that h misclassi   es. when we
want to make explicit the dependence of     (h) on the training set s, we may
also write this a     s(h). we also de   ne the generalization error to be

  (h) = p(x,y)   d(h(x) 6= y).

i.e. this is the id203 that, if we now draw a new example (x, y) from
the distribution d, h will misclassify it.
note that we have assumed that the training data was drawn from the
same distribution d with which we   re going to evaluate our hypotheses (in
the de   nition of generalization error). this is sometimes also referred to as
one of the pac assumptions.2

consider the setting of linear classi   cation, and let h  (x) = 1{  t x     0}.
what   s a reasonable way of    tting the parameters   ? one approach is to try
to minimize the training error, and pick

     = arg min

  

    (h  ).

we call this process empirical risk minimization (erm), and the resulting
hypothesis output by the learning algorithm is   h = h    . we think of erm
as the most    basic    learning algorithm, and it will be this algorithm that we
focus on in these notes. (algorithms such as id28 can also be
viewed as approximations to empirical risk minimization.)

in our study of learning theory, it will be useful to abstract away from
the speci   c parameterization of hypotheses and from issues such as whether
we   re using a linear classi   er. we de   ne the hypothesis class h used by a
learning algorithm to be the set of all classi   ers considered by it. for linear
classi   cation, h = {h   : h  (x) = 1{  t x     0},        rn+1} is thus the set of
all classi   ers over x (the domain of the inputs) where the decision boundary
is linear. more broadly, if we were studying, say, neural networks, then we
could let h be the set of all classi   ers representable by some neural network
architecture.
empirical risk minimization can now be thought of as a minimization over
the class of functions h, in which the learning algorithm picks the hypothesis:

  h = arg min

h   h

    (h)

2pac stands for    probably approximately correct,    which is a framework and set of
assumptions under which numerous results on learning theory were proved. of these, the
assumption of training and testing on the same distribution, and the assumption of the
independently drawn training examples, were the most important.

5

3 the case of    nite h

let   s start by considering a learning problem in which we have a    nite hy-
pothesis class h = {h1, . . . , hk} consisting of k hypotheses. thus, h is just a
set of k functions mapping from x to {0, 1}, and empirical risk minimization
selects   h to be whichever of these k functions has the smallest training error.
we would like to give guarantees on the generalization error of   h. our
strategy for doing so will be in two parts: first, we will show that     (h) is a
reliable estimate of   (h) for all h. second, we will show that this implies an
upper-bound on the generalization error of   h.

take any one,    xed, hi     h. consider a bernoulli random variable z
whose distribution is de   ned as follows. we   re going to sample (x, y)     d.
i.e., we   re going to draw one example,
then, we set z = 1{hi(x) 6= y}.
and let z indicate whether hi misclassi   es it. similarly, we also de   ne zj =
1{hi(x(j)) 6= y(j)}. since our training set was drawn iid from d, z and the
zj   s have the same distribution.

we see that the misclassi   cation id203 on a randomly drawn example   

that is,   (h)   is exactly the expected value of z (and zj). moreover, the
training error can be written

    (hi) =

1
m

m

xj=1

zj.

thus,     (hi) is exactly the mean of the m random variables zj that are drawn
iid from a bernoulli distribution with mean   (hi). hence, we can apply the
hoe   ding inequality, and obtain

p (|  (hi)         (hi)| >   )     2 exp(   2  2m).

this shows that, for our particular hi, training error will be close to
generalization error with high id203, assuming m is large. but we
don   t just want to guarantee that   (hi) will be close to     (hi) (with high
id203) for just only one particular hi. we want to prove that this will
be true for simultaneously for all h     h. to do so, let ai denote the event
that |  (hi)         (hi)| >   . we   ve already show that, for any particular ai, it
holds true that p (ai)     2 exp(   2  2m). thus, using the union bound, we

have that

p (    h     h.|  (hi)         (hi)| >   ) = p (a1                  ak)

k

6

   

p (ai)

k

xi=1
xi=1

2 exp(   2  2m)

   
= 2k exp(   2  2m)

if we subtract both sides from 1, we    nd that

p (      h     h.|  (hi)         (hi)| >   ) = p (   h     h.|  (hi)         (hi)|       )

    1     2k exp(   2  2m)

(the          symbol means    not.   ) so, with id203 at least 1   2k exp(   2  2m),
we have that   (h) will be within    of     (h) for all h     h. this is called a uni-
form convergence result, because this is a bound that holds simultaneously
for all (as opposed to just one) h     h.
in the discussion above, what we did was, for particular values of m and
  , give a bound on the id203 that for some h     h, |  (h)         (h)| >   .
there are three quantities of interest here: m,   , and the id203 of error;
we can bound either one in terms of the other two.

for instance, we can ask the following question: given    and some    > 0,
how large must m be before we can guarantee that with id203 at least
1       , training error will be within    of generalization error? by setting
   = 2k exp(   2  2m) and solving for m, [you should convince yourself this is
the right thing to do!], we    nd that if

m    

1
2  2 log

2k
  

,

then with id203 at least 1       , we have that |  (h)         (h)|        for all
h     h. (equivalently, this shows that the id203 that |  (h)         (h)| >   
for some h     h is at most   .) this bound tells us how many training
examples we need in order make a guarantee. the training set size m that
a certain method or algorithm requires in order to achieve a certain level of
performance is also called the algorithm   s sample complexity.

the key property of the bound above is that the number of training
examples needed to make this guarantee is only logarithmic in k, the number
of hypotheses in h. this will be important later.

7

similarly, we can also hold m and       xed and solve for    in the previous
equation, and show [again, convince yourself that this is right!] that with
id203 1       , we have that for all h     h,
|    (h)       (h)|    r 1

2k
  

2m

log

.

now, let   s assume that uniform convergence holds, i.e., that |  (h)       (h)|    
   for all h     h. what can we prove about the generalization of our learning
algorithm that picked   h = arg minh   h     (h)?
de   ne h    = arg minh   h   (h) to be the best possible hypothesis in h. note
that h    is the best that we could possibly do given that we are using h, so
it makes sense to compare our performance to that of h   . we have:

  (  h)         (  h) +   
        (h   ) +   
      (h   ) + 2  

the    rst line used the fact that |  (  h)        (  h)|        (by our uniform convergence
assumption). the second used the fact that   h was chosen to minimize     (h),
and hence     (  h)         (h) for all h, and in particular     (  h)         (h   ). the third
line used the uniform convergence assumption again, to show that     (h   )    
  (h   ) +   . so, what we   ve shown is the following: if uniform convergence
occurs, then the generalization error of   h is at most 2   worse than the best
possible hypothesis in h!

let   s put all this together into a theorem.

theorem. let |h| = k, and let any m,    be    xed. then with id203 at
least 1       , we have that

  (  h)    (cid:18)min

h   h

  (h)(cid:19) + 2r 1

2m

log

2k
  

.

this is proved by letting    equal the       term, using our previous argu-
ment that uniform convergence occurs with id203 at least 1       , and
then noting that uniform convergence implies   (h) is at most 2   higher than
  (h   ) = minh   h   (h) (as we showed previously).

this also quanti   es what we were saying previously saying about the
bias/variance tradeo    in model selection. speci   cally, suppose we have some
hypothesis class h, and are considering switching to some much larger hy-
pothesis class h        h.
if we switch to h   , then the    rst term minh   (h)

8

can only decrease (since we   d then be taking a min over a larger set of func-
tions). hence, by learning using a larger hypothesis class, our    bias    can

only decrease. however, if k increases, then the second 2      term would also

increase. this increase corresponds to our    variance    increasing when we use
a larger hypothesis class.

by holding    and       xed and solving for m like we did before, we can

also obtain the following sample complexity bound:
corollary. let |h| = k, and let any   ,    be    xed. then for   (  h)    
minh   h   (h) + 2   to hold with id203 at least 1       , it su   ces that

2k
  

m    

1
2  2 log
= o(cid:18) 1
4 the case of in   nite h

  2 log

k

  (cid:19) ,

we have proved some useful theorems for the case of    nite hypothesis classes.
but many hypothesis classes, including any parameterized by real numbers
(as in linear classi   cation) actually contain an in   nite number of functions.
can we prove similar results for this setting?

let   s start by going through something that is not the    right    argument.
better and more general arguments exist, but this will be useful for honing
our intuitions about the domain.

suppose we have an h that is parameterized by d real numbers. since we
are using a computer to represent real numbers, and ieee double-precision
   oating point (double   s in c) uses 64 bits to represent a    oating point num-
ber, this means that our learning algorithm, assuming we   re using double-
precision    oating point, is parameterized by 64d bits. thus, our hypothesis
class really consists of at most k = 264d di   erent hypotheses. from the corol-
lary at the end of the previous section, we therefore    nd that, to guarantee
  (  h)       (h   ) + 2  , with to hold with id203 at least 1       , it su   ces
that m     o(cid:16) 1
  (cid:17) = o  ,  (d). (the   ,    subscripts are

to indicate that the last big-o is hiding constants that may depend on    and
  .) thus, the number of training examples needed is at most linear in the
parameters of the model.

   (cid:17) = o(cid:16) d

   2 log 264d

   2 log 1

the fact that we relied on 64-bit    oating point makes this argument not
entirely satisfying, but the conclusion is nonetheless roughly correct: if what
we   re going to do is try to minimize training error, then in order to learn

9

   well    using a hypothesis class that has d parameters, generally we   re going
to need on the order of a linear number of training examples in d.

(at this point, it   s worth noting that these results were proved for an al-
gorithm that uses empirical risk minimization. thus, while the linear depen-
dence of sample complexity on d does generally hold for most discriminative
learning algorithms that try to minimize training error or some approxima-
tion to training error, these conclusions do not always apply as readily to
discriminative learning algorithms. giving good theoretical guarantees on
many non-erm learning algorithms is still an area of active research.)

0) + (u2

0     v2

1)x1 +          (u2

the other part of our previous argument that   s slightly unsatisfying is
that it relies on the parameterization of h. intuitively, this doesn   t seem like
it should matter: we had written the class of linear classi   ers as h  (x) =
1{  0 +   1x1 +            nxn     0}, with n + 1 parameters   0, . . . ,   n. but it could
also be written hu,v(x) = 1{(u2
1     v2
n)xn     0}
with 2n + 2 parameters ui, vi. yet, both of these are just de   ning the same
h: the set of linear classi   ers in n dimensions.

to derive a more satisfying argument, let   s de   ne a few more things.
given a set s = {x(i), . . . , x(d)} (no relation to the training set) of points
x(i)     x , we say that h shatters s if h can realize any labeling on s.
i.e., if for any set of labels {y(1), . . . , y(d)}, there exists some h     h so that
h(x(i)) = y(i) for all i = 1, . . . d.
given a hypothesis class h, we then de   ne its vapnik-chervonenkis
dimension, written vc(h), to be the size of the largest set that is shattered
by h. (if h can shatter arbitrarily large sets, then vc(h) =    .)

for instance, consider the following set of three points:

n     v2

  

  

x

2

  

x1

can the set h of linear classi   ers in two dimensions (h(x) = 1{  0 +  1x1 +
  2x2     0}) can shatter the set above? the answer is yes. speci   cally, we

see that, for any of the eight possible labelings of these points, we can    nd a
linear classi   er that obtains    zero training error    on them:

10

x

2

x

2

x1

x1

x

2

x

2

x1

x1

x

2

x

2

x1

x1

x

2

x

2

x1

x1

moreover, it is possible to show that there is no set of 4 points that this
hypothesis class can shatter. thus, the largest set that h can shatter is of
size 3, and hence vc(h) = 3.
note that the vc dimension of h here is 3 even though there may be
sets of size 3 that it cannot shatter. for instance, if we had a set of three
points lying in a straight line (left    gure), then there is no way to    nd a linear
separator for the labeling of the three points shown below (right    gure):

  

x

2

  

  

x

2

x1

x1

in order words, under the de   nition of the vc dimension, in order to
prove that vc(h) is at least d, we need to show only that there   s at least
one set of size d that h can shatter.
the following theorem, due to vapnik, can then be shown. (this is, many
would argue, the most important theorem in all of learning theory.)

11

theorem. let h be given, and let d = vc(h). then with id203 at
least 1       , we have that for all h     h,
|  (h)         (h)|     o r d

  ! .

m
d

1
m

log

log

m

+

1

thus, with id203 at least 1       , we also have that:
1

  (  h)       (h   ) + o r d

m

log

+

log

m
d

1
m

  ! .

in other words, if a hypothesis class has    nite vc dimension, then uniform
convergence occurs as m becomes large. as before, this allows us to give a
bound on   (h) in terms of   (h   ). we also have the following corollary:
corollary. for |  (h)         (h)|        to hold for all h     h (and hence   (  h)    
  (h   ) + 2  ) with id203 at least 1       , it su   ces that m = o  ,  (d).

in other words, the number of training examples needed to learn    well   
using h is linear in the vc dimension of h. it turns out that, for    most   
hypothesis classes, the vc dimension (assuming a    reasonable    parameter-
ization) is also roughly linear in the number of parameters. putting these
together, we conclude that (for an algorithm that tries to minimize training
error) the number of training examples needed is usually roughly linear in
the number of parameters of h.

