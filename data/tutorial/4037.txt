cs229 lecture notes

andrew ng

part vii
id173 and model
selection

suppose we are trying to select among several di   erent models for a learning
problem. for instance, we might be using a polynomial regression model
h  (x) = g(  0 +   1x +   2x2 +          +   kxk), and wish to decide if k should be
0, 1, . . . , or 10. how can we automatically select a model that represents
a good tradeo    between the twin evils of bias and variance1? alternatively,
suppose we want to automatically choose the bandwidth parameter    for
locally weighted regression, or the parameter c for our    1-regularized id166.
how can we do that?

for the sake of concreteness, in these notes we assume we have some
   nite set of models m = {m1, . . . , md} that we   re trying to select among.
for instance, in our    rst example above, the model mi would be an i-th
order polynomial regression model. (the generalization to in   nite m is not
hard.2) alternatively, if we are trying to decide between using an id166, a
neural network or id28, then m may contain these models.

1given that we said in the previous set of notes that bias and variance are two very
di   erent beasts, some readers may be wondering if we should be calling them    twin    evils
here. perhaps it   d be better to think of them as non-identical twins. the phrase    the
fraternal twin evils of bias and variance    doesn   t have the same ring to it, though.

2if we are trying to choose from an in   nite set of models, say corresponding to the
possible values of the bandwidth        r+, we may discretize    and consider only a    nite
number of possible values for it. more generally, most of the algorithms described here
can all be viewed as performing optimization search in the space of models, and we can
perform this search over in   nite model classes as well.

1

2

1 cross validation

let   s suppose we are, as usual, given a training set s. given what we know
about empirical risk minimization, here   s what might initially seem like a
algorithm, resulting from using empirical risk minimization for model selec-
tion:

1. train each model mi on s, to get some hypothesis hi.

2. pick the hypotheses with the smallest training error.

this algorithm does not work. consider choosing the order of a poly-
nomial. the higher the order of the polynomial, the better it will    t the
training set s, and thus the lower the training error. hence, this method will
always select a high-variance, high-degree polynomial model, which we saw
previously is often poor choice.

here   s an algorithm that works better. in hold-out cross validation

(also called simple cross validation), we do the following:

1. randomly split s into strain (say, 70% of the data) and scv (the remain-

ing 30%). here, scv is called the hold-out cross validation set.

2. train each model mi on strain only, to get some hypothesis hi.

3. select and output the hypothesis hi that had the smallest error     scv(hi)
on the hold out cross validation set. (recall,     scv(h) denotes the empir-
ical error of h on the set of examples in scv.)

by testing on a set of examples scv that the models were not trained on,
we obtain a better estimate of each hypothesis hi   s true generalization error,
and can then pick the one with the smallest estimated generalization error.
usually, somewhere between 1/4     1/3 of the data is used in the hold out
cross validation set, and 30% is a typical choice.

optionally, step 3 in the algorithm may also be replaced with selecting
the model mi according to arg mini     scv(hi), and then retraining mi on the
entire training set s. (this is often a good idea, with one exception being
learning algorithms that are be very sensitive to perturbations of the initial
conditions and/or data. for these methods, mi doing well on strain does not
necessarily mean it will also do well on scv, and it might be better to forgo
this retraining step.)

the disadvantage of using hold out cross validation is that it    wastes   
about 30% of the data. even if we were to take the optional step of retraining

3

the model on the entire training set, it   s still as if we   re trying to    nd a good
model for a learning problem in which we had 0.7m training examples, rather
than m training examples, since we   re testing models that were trained on
only 0.7m examples each time. while this is    ne if data is abundant and/or
cheap, in learning problems in which data is scarce (consider a problem with
m = 20, say), we   d like to do something better.

here is a method, called k-fold cross validation, that holds out less

data each time:

1. randomly split s into k disjoint subsets of m/k training examples each.

let   s call these subsets s1, . . . , sk.

2. for each model mi, we evaluate it as follows:

for j = 1, . . . , k

train the model mi on s1                  sj   1     sj+1              sk (i.e., train
on all the data except sj) to get some hypothesis hij.
test the hypothesis hij on sj, to get     sj (hij).

the estimated generalization error of model mi is then calculated
as the average of the     sj (hij)   s (averaged over j).

3. pick the model mi with the lowest estimated generalization error, and
retrain that model on the entire training set s. the resulting hypothesis
is then output as our    nal answer.

a typical choice for the number of folds to use here would be k = 10.
while the fraction of data held out each time is now 1/k   much smaller
than before   this procedure may also be more computationally expensive
than hold-out cross validation, since we now need train to each model k
times.

while k = 10 is a commonly used choice, in problems in which data is
really scarce, sometimes we will use the extreme choice of k = m in order
to leave out as little data as possible each time. in this setting, we would
repeatedly train on all but one of the training examples in s, and test on that
held-out example. the resulting m = k errors are then averaged together to
obtain our estimate of the generalization error of a model. this method has
its own name; since we   re holding out one training example at a time, this
method is called leave-one-out cross validation.

finally, even though we have described the di   erent versions of cross vali-
dation as methods for selecting a model, they can also be used more simply to
evaluate a single model or algorithm. for example, if you have implemented

4

some learning algorithm and want to estimate how well it performs for your
application (or if you have invented a novel learning algorithm and want to
report in a technical paper how well it performs on various test sets), cross
validation would give a reasonable way of doing so.

2 feature selection

one special and important case of model selection is called feature selection.
to motivate this, imagine that you have a supervised learning problem where
the number of features n is very large (perhaps n     m), but you suspect that
there is only a small number of features that are    relevant    to the learning
task. even if you use a simple linear classi   er (such as the id88) over
the n input features, the vc dimension of your hypothesis class would still be
o(n), and thus over   tting would be a potential problem unless the training
set is fairly large.

in such a setting, you can apply a feature selection algorithm to reduce the
number of features. given n features, there are 2n possible feature subsets
(since each of the n features can either be included or excluded from the
subset), and thus feature selection can be posed as a model selection problem
over 2n possible models. for large values of n, it   s usually too expensive to
explicitly enumerate over and compare all 2n models, and so typically some
heuristic search procedure is used to    nd a good feature subset. the following
search procedure is called forward search:

1. initialize f =    .

2. repeat {

(a) for i = 1, . . . , n if i 6    f , let fi = f     {i}, and use some ver-
sion of cross validation to evaluate features fi.
(i.e., train your
learning algorithm using only the features in fi, and estimate its
generalization error.)

(b) set f to be the best feature subset found on step (a).

}

3. select and output the best feature subset that was evaluated during the

entire search procedure.

5

the outer loop of the algorithm can be terminated either when f =
{1, . . . , n} is the set of all features, or when |f | exceeds some pre-set thresh-
old (corresponding to the maximum number of features that you want the
algorithm to consider using).

this algorithm described above one instantiation of wrapper model
feature selection, since it is a procedure that    wraps    around your learning
algorithm, and repeatedly makes calls to the learning algorithm to evaluate
how well it does using di   erent feature subsets. aside from forward search,
other search procedures can also be used. for example, backward search
starts o    with f = {1, . . . , n} as the set of all features, and repeatedly deletes
features one at a time (evaluating single-feature deletions in a similar manner
to how forward search evaluates single-feature additions) until f =    .

wrapper feature selection algorithms often work quite well, but can be
computationally expensive given how that they need to make many calls to
the learning algorithm. indeed, complete forward search (terminating when
f = {1, . . . , n}) would take about o(n2) calls to the learning algorithm.

filter feature selection methods give heuristic, but computationally
much cheaper, ways of choosing a feature subset. the idea here is to compute
some simple score s(i) that measures how informative each feature xi is about
the class labels y. then, we simply pick the k features with the largest scores
s(i).

one possible choice of the score would be de   ne s(i) to be (the absolute
value of) the correlation between xi and y, as measured on the training data.
this would result in our choosing the features that are the most strongly
correlated with the class labels. in practice, it is more common (particularly
for discrete-valued features xi) to choose s(i) to be the mutual information
mi(xi, y) between xi and y:

mi(xi, y) = xxi   {0,1} xy   {0,1}

p(xi, y) log

p(xi, y)
p(xi)p(y)

.

(the equation above assumes that xi and y are binary-valued; more generally
the summations would be over the domains of the variables.) the probabil-
ities above p(xi, y), p(xi) and p(y) can all be estimated according to their
empirical distributions on the training set.

to gain intuition about what this score does, note that the mutual infor-

mation can also be expressed as a kullback-leibler (kl) divergence:

mi(xi, y) = kl (p(xi, y)||p(xi)p(y))

you   ll get to play more with kl-divergence in problem set #3, but infor-
mally, this gives a measure of how di   erent the id203 distributions

6

p(xi, y) and p(xi)p(y) are.
if xi and y are independent random variables,
then we would have p(xi, y) = p(xi)p(y), and the kl-divergence between the
two distributions will be zero. this is consistent with the idea if xi and y
are independent, then xi is clearly very    non-informative    about y, and thus
the score s(i) should be small. conversely, if xi is very    informative    about
y, then their mutual information mi(xi, y) would be large.

one    nal detail: now that you   ve ranked the features according to their
scores s(i), how do you decide how many features k to choose? well, one
standard way to do so is to use cross validation to select among the possible
values of k. for example, when applying naive bayes to text classi   cation   
a problem where n, the vocabulary size, is usually very large   using this
method to select a feature subset often results in increased classi   er accuracy.

3 bayesian statistics and id173

in this section, we will talk about one more tool in our arsenal for our battle
against over   tting.

at the beginning of the quarter, we talked about parameter    tting using

maximum likelihood (ml), and chose our parameters according to

  ml = arg max

  

m

yi=1

p(y(i)|x(i);   ).

throughout our subsequent discussions, we viewed    as an unknown param-
eter of the world. this view of the    as being constant-valued but unknown
is taken in frequentist statistics. in the frequentist this view of the world,   
is not random   it just happens to be unknown   and it   s our job to come up
with statistical procedures (such as maximum likelihood) to try to estimate
this parameter.

an alternative way to approach our parameter estimation problems is to
take the bayesian view of the world, and think of    as being a random
variable whose value is unknown.
in this approach, we would specify a
prior distribution p(  ) on    that expresses our    prior beliefs    about the
parameters. given a training set s = {(x(i), y(i))}m
i=1, when we are asked to
make a prediction on a new value of x, we can then compute the posterior

7

(1)

distribution on the parameters

p(  |s) =

p(s|  )p(  )

p(s)

=

(cid:0)qm
r   (qm

i=1 p(y(i)|x(i),   )(cid:1) p(  )
i=1 p(y(i)|x(i),   )p(  )) d  

in the equation above, p(y(i)|x(i),   ) comes from whatever model you   re using
for your learning problem. for example, if you are using bayesian logistic re-
gression, then you might choose p(y(i)|x(i),   ) = h  (x(i))y(i)(1   h  (x(i)))(1   y(i)),
where h  (x(i)) = 1/(1 + exp(     t x(i))).3

when we are given a new test example x and asked to make it prediction
on it, we can compute our posterior distribution on the class label using the
posterior distribution on   :

p(y|x, s) = z  

p(y|x,   )p(  |s)d  

(2)

in the equation above, p(  |s) comes from equation (1). thus, for example,
if the goal is to the predict the expected value of y given x, then we would
output4

e[y|x, s] = zy

yp(y|x, s)dy

the procedure that we   ve outlined here can be thought of as doing    fully
bayesian    prediction, where our prediction is computed by taking an average
with respect to the posterior p(  |s) over   . unfortunately, in general it is
computationally very di   cult to compute this posterior distribution. this is
because it requires taking integrals over the (usually high-dimensional)    as
in equation (1), and this typically cannot be done in closed-form.

thus, in practice we will instead approximate the posterior distribution
for   . one common approximation is to replace our posterior distribution for
   (as in equation 2) with a single point estimate. the map (maximum
a posteriori) estimate for    is given by

  map = arg max

  

m

yi=1

p(y(i)|x(i),   )p(  ).

(3)

3since we are now viewing    as a random variable, it is okay to condition on it value,

and write    p(y|x,   )    instead of    p(y|x;   ).   

4the integral below would be replaced by a summation if y is discrete-valued.

8

note that this is the same formulas as for the ml (maximum likelihood)
estimate for   , except for the prior p(  ) term at the end.

in practical applications, a common choice for the prior p(  ) is to assume
that        n (0,    2i). using this choice of prior, the    tted parameters   map
will have smaller norm than that selected by maximum likelihood.
(see
problem set #3.) in practice, this causes the bayesian map estimate to be
less susceptible to over   tting than the ml estimate of the parameters. for
example, bayesian id28 turns out to be an e   ective algorithm for
text classi   cation, even though in text classi   cation we usually have n     m.

