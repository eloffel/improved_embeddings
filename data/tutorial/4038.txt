cs229 lecture notes

andrew ng

1 the id88 and large margin classi(cid:12)ers

in this (cid:12)nal set of notes on learning theory, we will introduce a di(cid:11)erent
model of machine learning. speci(cid:12)cally, we have so far been considering
batch learning settings in which we are (cid:12)rst given a training set to learn
with, and our hypothesis h is then evaluated on separate test data. in this set
of notes, we will consider the online learning setting in which the algorithm
has to make predictions continuously even while it   s learning.

in this setting, the learning algorithm is given a sequence of examples
(x(1); y(1)); (x(2); y(2)); : : : (x(m); y(m)) in order. speci(cid:12)cally, the algorithm (cid:12)rst
sees x(1) and is asked to predict what it thinks y(1) is. after making its pre-
diction, the true value of y(1) is revealed to the algorithm (and the algorithm
may use this information to perform some learning). the algorithm is then
shown x(2) and again asked to make a prediction, after which y(2) is revealed,
and it may again perform some more learning. this proceeds until we reach
(x(m); y(m)).
in the online learning setting, we are interested in the total
number of errors made by the algorithm during this process. thus, it models
applications in which the algorithm has to make predictions even while it   s
still learning.

we will give a bound on the online learning error of the id88 algo-
rithm. to make our subsequent derivations easier, we will use the notational
convention of denoting the class labels by y =2 f(cid:0)1; 1g.
its predictions according to

recall that the id88 algorithm has parameters (cid:18) 2 rn+1, and makes

h(cid:18)(x) = g((cid:18)t x)

(1)

where

g(z) = (cid:26) 1

if z (cid:21) 0
(cid:0)1 if z < 0:

1

cs229 winter 2003

2

also, given a training example (x; y), the id88 learning rule updates
the parameters as follows.
if h(cid:18)(x) = y, then it makes no change to the
parameters. otherwise, it performs the update1

(cid:18) := (cid:18) + yx:

the following theorem gives a bound on the online learning error of the
id88 algorithm, when it is run as an online algorithm that performs
an update each time it gets an example wrong. note that the bound below
on the number of errors does not have an explicit dependence on the number
of examples m in the sequence, or on the dimension n of the inputs (!).

theorem (block, 1962, and noviko(cid:11), 1962). let a sequence of exam-
ples (x(1); y(1)); (x(2); y(2)); : : : (x(m); y(m)) be given. suppose that jjx(i)jj (cid:20) d
for all i, and further that there exists a unit-length vector u (jjujj2 = 1) such
that y(i) (cid:1) (ut x(i)) (cid:21) (cid:13) for all examples in the sequence (i.e., ut x(i) (cid:21) (cid:13) if
y(i) = 1, and ut x(i) (cid:20) (cid:0)(cid:13) if y(i) = (cid:0)1, so that u separates the data with a
margin of at least (cid:13)). then the total number of mistakes that the id88
algorithm makes on this sequence is at most (d=(cid:13))2.

proof. the id88 updates its weights only on those examples on which
it makes a mistake. let (cid:18)(k) be the weights that were being used when it made
its k-th mistake. so, (cid:18)(1) = ~0 (since the weights are initialized to zero), and
if the k-th mistake was on the example (x(i); y(i)), then g((x(i))t (cid:18)(k)) 6= y(i),
which implies that
(2)

(x(i))t (cid:18)(k)y(i) (cid:20) 0:

also, from the id88 learning rule, we would have that (cid:18)(k+1) = (cid:18)(k) +
y(i)x(i).

we then have

((cid:18)(k+1))t u = ((cid:18)(k))t u + y(i)(x(i))t u

(cid:21) ((cid:18)(k))t u + (cid:13)

by a straightforward inductive argument, implies that

((cid:18)(k+1))t u (cid:21) k(cid:13):

(3)

1this looks slightly di(cid:11)erent from the update rule we had written down earlier in the
quarter because here we have changed the labels to be y 2 f(cid:0)1; 1g. also, the learning rate
parameter (cid:11) was dropped. the only e(cid:11)ect of the learning rate is to scale all the parameters
(cid:18) by some (cid:12)xed constant, which does not a(cid:11)ect the behavior of the id88.

cs229 winter 2003

also, we have that

jj(cid:18)(k+1)jj2 = jj(cid:18)(k) + y(i)x(i)jj2

= jj(cid:18)(k)jj2 + jjx(i)jj2 + 2y(i)(x(i))t (cid:18)(i)
(cid:20) jj(cid:18)(k)jj2 + jjx(i)jj2
(cid:20) jj(cid:18)(k)jj2 + d2

3

(4)

the third step above used equation (2). moreover, again by applying a
straightfoward inductive argument, we see that (4) implies

jj(cid:18)(k+1)jj2 (cid:20) kd2:
putting together (3) and (4) we (cid:12)nd that

(5)

pkd (cid:21) jj(cid:18)(k+1)jj
(cid:21) ((cid:18)(k+1))t u
(cid:21) k(cid:13):

the second inequality above follows from the fact that u is a unit-length
vector (and z t u = jjzjj (cid:1) jjujj cos (cid:30) (cid:20) jjzjj (cid:1) jjujj, where (cid:30) is the angle between
z and u). our result implies that k (cid:20) (d=(cid:13))2. hence, if the id88 made
a k-th mistake, then k (cid:20) (d=(cid:13))2.

(cid:3)

