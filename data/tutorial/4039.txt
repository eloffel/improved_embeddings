cs229 lecture notes

andrew ng

the id116 id91 algorithm

in the id91 problem, we are given a training set {x(1), . . . , x(m)}, and
want to group the data into a few cohesive    clusters.    here, x(i)     rn
as usual; but no labels y(i) are given. so, this is an unsupervised learning
problem.

the id116 id91 algorithm is as follows:

1. initialize cluster centroids   1,   2, . . . ,   k     rn randomly.

2. repeat until convergence: {

for every i, set

for each j, set

}

c(i) := arg min

j

||x(i)       j||2.

  j := pm
i=1 1{c(i) = j}x(i)
pm
i=1 1{c(i) = j}

.

in the algorithm above, k (a parameter of the algorithm) is the number
of clusters we want to    nd; and the cluster centroids   j represent our current
guesses for the positions of the centers of the clusters. to initialize the cluster
centroids (in step 1 of the algorithm above), we could choose k training
examples randomly, and set the cluster centroids to be equal to the values of
these k examples. (other initialization methods are also possible.)

the inner-loop of the algorithm repeatedly carries out two steps: (i)
   assigning    each training example x(i) to the closest cluster centroid   j, and
(ii) moving each cluster centroid   j to the mean of the points assigned to it.
figure 1 shows an illustration of running id116.

1

2

(a)

(b)

(c)

(d)

(e)

(f)

figure 1: id116 algorithm. training examples are shown as dots, and
cluster centroids are shown as crosses. (a) original dataset. (b) random ini-
tial cluster centroids (in this instance, not chosen to be equal to two training
examples). (c-f) illustration of running two iterations of id116. in each
iteration, we assign each training example to the closest cluster centroid
(shown by    painting    the training examples the same color as the cluster
centroid to which is assigned); then we move each cluster centroid to the
mean of the points assigned to it. (best viewed in color.) images courtesy
michael jordan.

is the id116 algorithm guaranteed to converge? yes it is, in a certain

sense. in particular, let us de   ne the distortion function to be:

j(c,   ) =

m

x

i=1

||x(i)       c(i)||2

thus, j measures the sum of squared distances between each training exam-
ple x(i) and the cluster centroid   c(i) to which it has been assigned. it can
be shown that id116 is exactly coordinate descent on j. speci   cally, the
inner-loop of id116 repeatedly minimizes j with respect to c while holding
      xed, and then minimizes j with respect to    while holding c    xed. thus,
j must monotonically decrease, and the value of j must converge. (usu-
ally, this implies that c and    will converge too. in theory, it is possible for

3

id116 to oscillate between a few di   erent id91s   i.e., a few di   erent
values for c and/or      that have exactly the same value of j, but this almost
never happens in practice.)

the distortion function j is a non-convex function, and so coordinate
descent on j is not guaranteed to converge to the global minimum. in other
words, id116 can be susceptible to local optima. very often id116 will
work    ne and come up with very good id91s despite this. but if you
are worried about getting stuck in bad local minima, one common thing to
do is run id116 many times (using di   erent random initial values for the
cluster centroids   j). then, out of all the di   erent id91s found, pick
the one that gives the lowest distortion j(c,   ).

