cs229 lecture notes

andrew ng

mixtures of gaussians and the em algorithm

in this set of notes, we discuss the em (expectation-maximization) for den-
sity estimation.

suppose that we are given a training set {x(1), . . . , x(m)} as usual. since
we are in the unsupervised learning setting, these points do not come with
any labels.

we wish to model the data by specifying a joint distribution p(x(i), z(i)) =
p(x(i)|z(i))p(z(i)). here, z(i)     multinomial(  ) (where   j     0, pk
j=1   j = 1,
and the parameter   j gives p(z(i) = j),), and x(i)|z(i) = j     n (  j,   j). we
let k denote the number of values that the z(i)   s can take on. thus, our
model posits that each x(i) was generated by randomly choosing z(i) from
{1, . . . , k}, and then x(i) was drawn from one of k gaussians depending on
z(i). this is called the mixture of gaussians model. also, note that the
z(i)   s are latent random variables, meaning that they   re hidden/unobserved.
this is what will make our estimation problem di   cult.

the parameters of our model are thus   ,    and   . to estimate them, we

can write down the likelihood of our data:

   (  ,   ,   ) =

=

m

x

i=1

m

x

i=1

log p(x(i);   ,   ,   )

log

k

x
z(i)=1

p(x(i)|z(i);   ,   )p(z(i);   ).

however, if we set to zero the derivatives of this formula with respect to
the parameters and try to solve, we   ll    nd that it is not possible to    nd the
maximum likelihood estimates of the parameters in closed form. (try this
yourself at home.)

the random variables z(i) indicate which of the k gaussians each x(i)
had come from. note that if we knew what the z(i)   s were, the maximum

1

2

likelihood problem would have been easy. speci   cally, we could then write
down the likelihood as

   (  ,   ,   ) =

m

x

i=1

log p(x(i)|z(i);   ,   ) + log p(z(i);   ).

maximizing this with respect to   ,    and    gives the parameters:

  j =

m

x

1{z(i) = j},

1
m

i=1
  j = pm
i=1 1{z(i) = j}x(i)
pm
i=1 1{z(i) = j}
  j = pm
i=1 1{z(i) = j}(x(i)       j)(x(i)       j)t

,

.

pm

i=1 1{z(i) = j}

indeed, we see that if the z(i)   s were known, then maximum likelihood
estimation becomes nearly identical to what we had when estimating the
parameters of the gaussian discriminant analysis model, except that here
the z(i)   s playing the role of the class labels.1

however, in our density estimation problem, the z(i)   s are not known.

what can we do?

the em algorithm is an iterative algorithm that has two main steps.
applied to our problem, in the e-step, it tries to    guess    the values of the
z(i)   s. in the m-step, it updates the parameters of our model based on our
guesses. since in the m-step we are pretending that the guesses in the    rst
part were correct, the maximization becomes easy. here   s the algorithm:

repeat until convergence: {

(e-step) for each i, j, set

w(i)

j

:= p(z(i) = j|x(i);   ,   ,   )

1there are other minor di   erences in the formulas here from what we   d obtained in
ps1 with gaussian discriminant analysis,    rst because we   ve generalized the z(i)   s to be
multinomial rather than bernoulli, and second because here we are using a di   erent   j
for each gaussian.

3

,

(m-step) update the parameters:

  j

:=

1
m

m

x

w(i)
j ,

i=1

  j

  j

}

i=1 w(i)
:= pm
j x(i)
i=1 w(i)
pm
i=1 w(i)
:= pm

j

j (x(i)       j)(x(i)       j)t

pm

i=1 w(i)

j

in the e-step, we calculate the posterior id203 of our parameters
the z(i)   s, given the x(i) and using the current setting of our parameters. i.e.,
using bayes rule, we obtain:

p(z(i) = j|x(i);   ,   ,   ) =

p(x(i)|z(i) = j;   ,   )p(z(i) = j;   )
l=1 p(x(i)|z(i) = l;   ,   )p(z(i) = l;   )

pk

here, p(x(i)|z(i) = j;   ,   ) is given by evaluating the density of a gaussian
with mean   j and covariance   j at x(i); p(z(i) = j;   ) is given by   j, and so
on. the values w(i)
j calculated in the e-step represent our    soft    guesses2 for
the values of z(i).

also, you should contrast the updates in the m-step with the formulas we
had when the z(i)   s were known exactly. they are identical, except that in-
stead of the indicator functions    1{z(i) = j}    indicating from which gaussian
each datapoint had come, we now instead have the w(i)

   s.

j

the em-algorithm is also reminiscent of the id116 id91 algo-
rithm, except that instead of the    hard    cluster assignments c(i), we instead
have the    soft    assignments w(i)
j . similar to id116, it is also susceptible
to local optima, so reinitializing at several di   erent initial parameters may
be a good idea.

it   s clear that the em algorithm has a very natural interpretation of
repeatedly trying to guess the unknown z(i)   s; but how did it come about,
and can we make any guarantees about it, such as regarding its convergence?
in the next set of notes, we will describe a more general view of em, one

2the term    soft    refers to our guesses being probabilities and taking values in [0, 1]; in
contrast, a    hard    guess is one that represents a single best guess (such as taking values
in {0, 1} or {1, . . . , k}).

that will allow us to easily apply it to other estimation problems in which
there are also latent variables, and which will allow us to give a convergence
guarantee.

4

