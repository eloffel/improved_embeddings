cs229 lecture notes

andrew ng

part x
factor analysis

when we have data x(i)     rn that comes from a mixture of several gaussians,
the em algorithm can be applied to    t a mixture model. in this setting, we
usually imagine problems where we have su   cient data to be able to discern
the multiple-gaussian structure in the data. for instance, this would be the
case if our training set size m was signi   cantly larger than the dimension n
of the data.

now, consider a setting in which n     m. in such a problem, it might be
di   cult to model the data even with a single gaussian, much less a mixture of
gaussian. speci   cally, since the m data points span only a low-dimensional
subspace of rn, if we model the data as gaussian, and estimate the mean
and covariance using the usual maximum likelihood estimators,

   =

   =

1
m

1
m

m

m

xi=1
xi=1

x(i)

(x(i)       )(x(i)       )t ,

we would    nd that the matrix    is singular. this means that      1 does not
exist, and 1/|  |1/2 = 1/0. but both of these terms are needed in computing
the usual density of a multivariate gaussian distribution. another way of
stating this di   culty is that maximum likelihood estimates of the parameters
result in a gaussian that places all of its id203 in the a   ne space
spanned by the data,1 and this corresponds to a singular covariance matrix.

1this is the set of points x satisfying x =pm

i=1   ix(i), for some   i   s so thatpm

i=1   1 =

1.

1

2

more generally, unless m exceeds n by some reasonable amount, the max-
imum likelihood estimates of the mean and covariance may be quite poor.
nonetheless, we would still like to be able to    t a reasonable gaussian model
to the data, and perhaps capture some interesting covariance structure in
the data. how can we do this?

in the next section, we begin by reviewing two possible restrictions on
  , ones that allow us to    t    with small amounts of data but neither of
which will give a satisfactory solution to our problem. we next discuss some
properties of gaussians that will be needed later; speci   cally, how to    nd
marginal and conditonal distributions of gaussians. finally, we present the
factor analysis model, and em for it.

1 restrictions of   

if we do not have su   cient data to    t a full covariance matrix, we may
place some restrictions on the space of matrices    that we will consider. for
instance, we may choose to    t a covariance matrix    that is diagonal. in this
setting, the reader may easily verify that the maximum likelihood estimate
of the covariance matrix is given by the diagonal matrix    satisfying

  jj =

1
m

m

xi=1

(x(i)

j       j)2.

thus,   jj is just the empirical estimate of the variance of the j-th coordinate
of the data.

recall that the contours of a gaussian density are ellipses. a diagonal
   corresponds to a gaussian where the major axes of these ellipses are axis-
aligned.

sometimes, we may place a further restriction on the covariance matrix
that not only must it be diagonal, but its diagonal entries must all be equal.
in this setting, we have    =   2i, where   2 is the parameter under our control.
the maximum likelihood estimate of   2 can be found to be:

  2 =

1
mn

n

m

xj=1

xi=1

(x(i)

j       j)2.

this model corresponds to using gaussians whose densities have contours
that are circles (in 2 dimensions; or spheres/hyperspheres in higher dimen-
sions).

3

if we were    tting a full, unconstrained, covariance matrix    to data, it
was necessary that m     n + 1 in order for the maximum likelihood estimate
of    not to be singular. under either of the two restrictions above, we may
obtain non-singular    when m     2.

however, restricting    to be diagonal also means modeling the di   erent
coordinates xi, xj of the data as being uncorrelated and independent. often,
it would be nice to be able to capture some interesting correlation structure
in the data. if we were to use either of the restrictions on    described above,
we would therefore fail to do so. in this set of notes, we will describe the
factor analysis model, which uses more parameters than the diagonal    and
captures some correlations in the data, but also without having to    t a full
covariance matrix.

2 marginals and conditionals of gaussians

before describing factor analysis, we digress to talk about how to    nd condi-
tional and marginal distributions of random variables with a joint multivari-
ate gaussian distribution.

suppose we have a vector-valued random variable

x =(cid:20) x1
x2 (cid:21) ,

where x1     rr, x2     rs, and x     rr+s. suppose x     n (  ,   ), where

   =(cid:20)   1

  21   22 (cid:21) .
  2 (cid:21) ,    =(cid:20)   11   12

here,   1     rr,   2     rs,   11     rr  r,   12     rr  s, and so on. note that since
covariance matrices are symmetric,   12 =   t

21.

under our assumptions, x1 and x2 are jointly multivariate gaussian.
what is the marginal distribution of x1? it is not hard to see that e[x1] =   1,
and that cov(x1) = e[(x1       1)(x1       1)] =   11. to see that the latter is
true, note that by de   nition of the joint covariance of x1 and x2, we have

that

cov(x) =   

4

= e[(x       )(x       )t ]

  21   22 (cid:21)
= (cid:20)   11   12
= e"(cid:18) x1       1
x2       2 (cid:19)(cid:18) x1       1
= e(cid:20) (x1       1)(x1       1)t

(x2       2)(x1       1)t

x2       2 (cid:19)t#

(x1       1)(x2       2)t

(x2       2)(x2       2)t (cid:21) .

matching the upper-left subblocks in the matrices in the second and the last
lines above gives the result.

since marginal distributions of gaussians are themselves gaussian, we
therefore have that the marginal distribution of x1 is given by x1     n (  1,   11).
also, we can ask, what is the conditional distribution of x1 given x2? by
referring to the de   nition of the multivariate gaussian distribution, it can
be shown that x1|x2     n (  1|2,   1|2), where

  1|2 =   1 +   12     1
  1|2 =   11       12     1

22 (x2       2),
22   21.

(1)
(2)

when working with the factor analysis model in the next section, these
formulas for    nding conditional and marginal distributions of gaussians will
be very useful.

3 the factor analysis model

in the factor analysis model, we posit a joint distribution on (x, z) as follows,
where z     rk is a latent random variable:

z     n (0, i)

x|z     n (   +   z,   ).

here, the parameters of our model are the vector        rn, the matrix
       rn  k, and the diagonal matrix        rn  n. the value of k is usually
chosen to be smaller than n.

5

thus, we imagine that each datapoint x(i) is generated by sampling a k
dimension multivariate gaussian z(i). then, it is mapped to a k-dimensional
a   ne space of rn by computing    +   z(i). lastly, x(i) is generated by adding
covariance    noise to    +   z(i).

equivalently (convince yourself that this is the case), we can therefore

also de   ne the factor analysis model according to

z     n (0, i)
       n (0,   )
x =    +   z +   .

where    and z are independent.

let   s work out exactly what distribution our model de   nes. our random

variables z and x have a joint gaussian distribution

(cid:20) z
x (cid:21)     n (  zx,   ).

we will now    nd   zx and   .

we know that e[z] = 0, from the fact that z     n (0, i). also, we have

that

e[x] = e[   +   z +   ]

=    +   e[z] + e[  ]
=   .

putting these together, we obtain

  zx =(cid:20) ~0
   (cid:21)

next, to    nd,   , we need to calculate   zz = e[(z     e[z])(z     e[z])t ] (the
upper-left block of   ),   zx = e[(z     e[z])(x     e[x])t ] (upper-right block),
and   xx = e[(x     e[x])(x     e[x])t ] (lower-right block).

now, since z     n (0, i), we easily    nd that   zz = cov(z) = i. also,

e[(z     e[z])(x     e[x])t ] = e[z(   +   z +          )t ]

= e[zzt ]  t + e[z  t ]
=   t .

in the last step, we used the fact that e[zzt ] = cov(z) (since z has zero
mean), and e[z  t ] = e[z]e[  t ] = 0 (since z and    are independent, and

6

hence the expectation of their product is the product of their expectations).
similarly, we can    nd   xx as follows:

e[(x     e[x])(x     e[x])t ] = e[(   +   z +          )(   +   z +          )t ]

= e[  zzt   t +   zt   t +   z  t +     t ]
=   e[zzt ]  t + e[    t ]
=     t +   .

putting everything together, we therefore have that

x (cid:21)     n (cid:18)(cid:20) ~0
(cid:20) z

   (cid:21) ,(cid:20) i

       t +    (cid:21)(cid:19) .

  t

(3)

hence, we also see that the marginal distribution of x is given by x    
n (  ,     t +   ). thus, given a training set {x(i); i = 1, . . . , m}, we can write
down the log likelihood of the parameters:

   (  ,   ,   ) = log

m

yi=1

1

(2  )n/2|    t +   |1/2 exp(cid:18)   

1
2

(x(i)       )t (    t +   )   1(x(i)       )(cid:19) .

to perform id113, we would like to maximize this
quantity with respect to the parameters. but maximizing this formula ex-
plicitly is hard (try it yourself), and we are aware of no algorithm that does
so in closed-form. so, we will instead use to the em algorithm. in the next
section, we derive em for factor analysis.

4 em for factor analysis

the derivation for the e-step is easy. we need to compute qi(z(i)) =
p(z(i)|x(i);   ,   ,   ). by substituting the distribution given in equation (3)
into the formulas (1-2) used for    nding the conditional distribution of a
gaussian, we    nd that z(i)|x(i);   ,   ,        n (  z(i)|x(i),   z(i)|x(i)), where

  z(i)|x(i) =   t (    t +   )   1(x(i)       ),
  z(i)|x(i) = i       t (    t +   )   1  .

so, using these de   nitions for   z(i)|x(i) and   z(i)|x(i), we have

qi(z(i)) =

1

(2  )k/2|  z(i)|x(i)|1/2 exp(cid:18)   

1
2

(z(i)       z(i)|x(i))t      1

z(i)|x(i)(z(i)       z(i)|x(i))(cid:19) .

7

(4)

let   s now work out the m-step. here, we need to maximize

m

xi=1 zz(i)

qi(z(i)) log

p(x(i), z(i);   ,   ,   )

qi(z(i))

dz(i)

with respect to the parameters   ,   ,   . we will work out only the optimiza-
tion with respect to   , and leave the derivations of the updates for    and   
as an exercise to the reader.

we can simplify equation (4) as follows:
m

m

=

xi=1 zz(i)
qi(z(i))(cid:2)log p(x(i)|z(i);   ,   ,   ) + log p(z(i))     log qi(z(i))(cid:3) dz(i) (5)
xi=1
ez(i)   qi(cid:2)log p(x(i)|z(i);   ,   ,   ) + log p(z(i))     log qi(z(i))(cid:3)

here, the    z(i)     qi    subscript indicates that the expectation is with respect
to z(i) drawn from qi.
in the subsequent development, we will omit this
subscript when there is no risk of ambiguity. dropping terms that do not
depend on the parameters, we    nd that we need to maximize:

(6)

m

xi=1

=

=

m

e(cid:2)log p(x(i)|z(i);   ,   ,   )(cid:3)
xi=1
xi=1

e(cid:20)log
e(cid:20)   

log |  |    

n
2

1
2

m

1

(2  )n/2|  |1/2 exp(cid:18)   

1
2

log(2  )    

(x(i)              z(i))t      1(x(i)              z(i))(cid:19)(cid:21)
(x(i)              z(i))t      1(x(i)              z(i))(cid:21)

1
2

let   s maximize this with respect to   . only the last term above depends
on   . taking derivatives, and using the facts that tr a = a (for a     r),
trab = trba, and    atrabat c = cab + c t ab, we get:

(x(i)              z(i))t      1(x(i)              z(i))(cid:21)

  t      1  z(i) + trz(i)t

  t      1(x(i)       )(cid:21)
+ tr  t      1(x(i)       )z(i)t(cid:21)

  t      1  z(i)z(i)t

     

m

m

2

1
2

z(i)t

   e(cid:20) 1
xi=1
     e(cid:20)   tr
xi=1
     e(cid:20)   tr
xi=1
eh        1  z(i)z(i)t
xi=1

1
2

m

m

=

=

=

+      1(x(i)       )z(i)ti

8

setting this to zero and simplifying, we get:

m

xi=1

  ez(i)   qihz(i)z(i)ti =

m

xi=1

(x(i)       )ez(i)   qihz(i)ti .

hence, solving for   , we obtain

   =  m
xi=1

(x(i)       )ez(i)   qihz(i)ti!  m
xi=1

ez(i)   qihz(i)z(i)ti!   1

.

(7)

it is interesting to note the close relationship between this equation and the
normal equation that we   d derived for least squares regression,

     t = (yt x)(x t x)   1.   

the analogy is that here, the x   s are a linear function of the z   s (plus noise).
given the    guesses    for z that the e-step has found, we will now try to
estimate the unknown linearity    relating the x   s and z   s.
it is therefore
no surprise that we obtain something similar to the normal equation. there
is, however, one important di   erence between this and an algorithm that
performs least squares using just the    best guesses    of the z   s; we will see
this di   erence shortly.

to complete our m-step update, let   s work out the values of the expecta-
tions in equation (7). from our de   nition of qi being gaussian with mean
  z(i)|x(i) and covariance   z(i)|x(i), we easily    nd

ez(i)   qihz(i)ti =   t
ez(i)   qihz(i)z(i)ti =   z(i)|x(i)  t

z(i)|x(i)

z(i)|x(i) +   z(i)|x(i).

the latter comes from the fact that, for a random variable y , cov(y ) =
e[y y t ]     e[y ]e[y ]t , and hence e[y y t ] = e[y ]e[y ]t + cov(y ). substitut-
ing this back into equation (7), we get the m-step update for   :

   =  m
xi=1

(x(i)       )  t

z(i)|x(i)!  m
xi=1

  z(i)|x(i)  t

z(i)|x(i) +   z(i)|x(i)!   1

.

(8)

it is important to note the presence of the   z(i)|x(i) on the right hand side of
this equation. this is the covariance in the posterior distribution p(z(i)|x(i))
of z(i) give x(i), and the m-step must take into account this uncertainty

9

about z(i) in the posterior. a common mistake in deriving em is to assume
that in the e-step, we need to calculate only expectation e[z] of the latent
random variable z, and then plug that into the optimization in the m-step
everywhere z occurs. while this worked for simple problems such as the
mixture of gaussians, in our derivation for factor analysis, we needed e[zzt ]
as well e[z]; and as we saw, e[zzt ] and e[z]e[z]t di   er by the quantity   z|x.
thus, the m-step update must take into account the covariance of z in the
posterior distribution p(z(i)|x(i)).

lastly, we can also    nd the m-step optimizations for the parameters   

and   . it is not hard to show that the    rst is given by

   =

x(i).

1
m

m

xi=1

since this doesn   t change as the parameters are varied (i.e., unlike the update
for   , the right hand side does not depend on qi(z(i)) = p(z(i)|x(i);   ,   ,   ),
which in turn depends on the parameters), this can be calculated just once
and needs not be further updated as the algorithm is run. similarly, the
diagonal    can be found by calculating

   =

1
m

m

xi=1

x(i)x(i)t

   x(i)  t

z(i)|x(i)  t        z(i)|x(i)x(i)t

+  (  z(i)|x(i)  t

z(i)|x(i)+  z(i)|x(i))  t ,

and setting   ii =   ii (i.e., letting    be the diagonal matrix containing only
the diagonal entries of   ).

