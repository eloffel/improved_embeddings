cs229 lecture notes

andrew ng

part xii
independent components
analysis

our next topic is independent components analysis (ica). similar to pca,
this will    nd a new basis in which to represent our data. however, the goal
is very di   erent.

as a motivating example, consider the    cocktail party problem.    here, n
speakers are speaking simultaneously at a party, and any microphone placed
in the room records only an overlapping combination of the n speakers    voices.
but let   s say we have n di   erent microphones placed in the room, and be-
cause each microphone is a di   erent distance from each of the speakers, it
records a di   erent combination of the speakers    voices. using these micro-
phone recordings, can we separate out the original n speakers    speech signals?
to formalize this problem, we imagine that there is some data s     rn

that is generated via n independent sources. what we observe is

x = as,

where a is an unknown square matrix called the mixing matrix. repeated
observations gives us a dataset {x(i); i = 1, . . . , m}, and our goal is to recover
the sources s(i) that had generated our data (x(i) = as(i)).

in our cocktail party problem, s(i) is an n-dimensional vector, and s(i)

is
the sound that speaker j was uttering at time i. also, x(i) in an n-dimensional
vector, and x(i)
is the acoustic reading recorded by microphone j at time i.
let w = a   1 be the unmixing matrix. our goal is to    nd w , so
that given our microphone recordings x(i), we can recover the sources by
computing s(i) = w x(i). for notational convenience, we also let wt
i denote

j

j

1

the i-th row of w , so that

2

.

w =   
      

    wt
1    
...
    wt
n    

   
      

thus, wi     rn, and the j-th source can be recovered by computing s(i)
wt

j =

j x(i).

1

ica ambiguities

to what degree can w = a   1 be recovered? if we have no prior knowledge
about the sources and the mixing matrix, it is not hard to see that there are
some inherent ambiguities in a that are impossible to recover, given only the
x(i)   s.

speci   cally, let p be any n-by-n permutation matrix. this means that
each row and each column of p has exactly one    1.    here   re some examples
of permutation matrices:

p =   
   

0 1 0
1 0 0
0 0 1

   
    ; p =(cid:20) 0 1

0 1 (cid:21) .
1 0 (cid:21) ; p =(cid:20) 1 0

if z is a vector, then p z is another vector that   s contains a permuted version
of z   s coordinates. given only the x(i)   s, there will be no way to distinguish
between w and p w . speci   cally, the permutation of the original sources is
ambiguous, which should be no surprise. fortunately, this does not matter
for most applications.

further, there is no way to recover the correct scaling of the wi   s. for in-
stance, if a were replaced with 2a, and every s(i) were replaced with (0.5)s(i),
then our observed x(i) = 2a    (0.5)s(i) would still be the same. more broadly,
if a single column of a were scaled by a factor of   , and the corresponding
source were scaled by a factor of 1/  , then there is again no way, given only
the x(i)   s to determine that this had happened. thus, we cannot recover the
   correct    scaling of the sources. however, for the applications that we are
concerned with   including the cocktail party problem   this ambiguity also
does not matter. speci   cally, scaling a speaker   s speech signal s(i)
j by some
positive factor    a   ects only the volume of that speaker   s speech. also, sign
changes do not matter, and s(i)
sound identical when played on a
speaker. thus, if the wi found by an algorithm is scaled by any non-zero real

j and    s(i)

j

3

number, the corresponding recovered source si = wt
i x will be scaled by the
same factor; but this usually does not matter. (these comments also apply
to ica for the brain/meg data that we talked about in class.)

are these the only sources of ambiguity in ica? it turns out that they
are, so long as the sources si are non-gaussian. to see what the di   culty is
with gaussian data, consider an example in which n = 2, and s     n (0, i).
here, i is the 2x2 identity matrix. note that the contours of the density of
the standard normal distribution n (0, i) are circles centered on the origin,
and the density is rotationally symmetric.

now, suppose we observe some x = as, where a is our mixing matrix.
the distribution of x will also be gaussian, with zero mean and covariance
e[xxt ] = e[asst at ] = aat . now, let r be an arbitrary orthogonal (less
formally, a rotation/re   ection) matrix, so that rrt = rt r = i, and let
a    = ar. then if the data had been mixed according to a    instead of
a, we would have instead observed x    = a   s. the distribution of x    is
also gaussian, with zero mean and covariance e[x   (x   )t ] = e[a   sst (a   )t ] =
e[arsst (ar)t ] = arrt at = aat . hence, whether the mixing matrix
is a or a   , we would observe data from a n (0, aat ) distribution. thus,
there is no way to tell if the sources were mixed using a and a   . so, there
is an arbitrary rotational component in the mixing matrix that cannot be
determined from the data, and we cannot recover the original sources.

our argument above was based on the fact that the multivariate standard
normal distribution is rotationally symmetric. despite the bleak picture that
this paints for ica on gaussian data, it turns out that, so long as the data is
not gaussian, it is possible, given enough data, to recover the n independent
sources.

2 densities and linear transformations

before moving on to derive the ica algorithm proper, we    rst digress brie   y
to talk about the e   ect of linear transformations on densities.

suppose we have a random variable s drawn according to some density
ps(s). for simplicity, let us say for now that s     r is a real number. now, let
the random variable x be de   ned according to x = as (here, x     r, a     r).
let px be the density of x. what is px?

let w = a   1. to calculate the    id203    of a particular value of
x, it is tempting to compute s = w x, then evaluate ps at that point, and
conclude that    px(x) = ps(w x).    however, this is incorrect. for example,
let s     uniform[0, 1], so that s   s density is ps(s) = 1{0     s     1}. now, let

4

a = 2, so that x = 2s. clearly, x is distributed uniformly in the interval
[0, 2]. thus, its density is given by px(x) = (0.5)1{0     x     2}. this does
not equal ps(w x), where w = 0.5 = a   1. instead, the correct formula is
px(x) = ps(w x)|w |.

more generally, if s is a vector-valued distribution with density ps, and

x = as for a square, invertible matrix a, then the density of x is given by

px(x) = ps(w x)    |w |,

where w = a   1.
remark. if you   ve seen the result that a maps [0, 1]n to a set of volume |a|,
then here   s another way to remember the formula for px given above, that also
generalizes our previous 1-dimensional example. speci   cally, let a     rn  n be
given, and let w = a   1 as usual. also let c1 = [0, 1]n be the n-dimensional
hypercube, and de   ne c2 = {as : s     c1}     rn to be the image of c1
under the mapping given by a. then it is a standard result in id202
(and, indeed, one of the ways of de   ning determinants) that the volume of
c2 is given by |a|. now, suppose s is uniformly distributed in [0, 1]n, so its
density is ps(s) = 1{s     c1}. then clearly x will be uniformly distributed
in c2. its density is therefore found to be px(x) = 1{x     c2}/vol(c2) (since
it must integrate over c2 to 1). but using the fact that the determinant
of the inverse of a matrix is just the inverse of the determinant, we have
1/vol(c2) = 1/|a| = |a   1| = |w |. thus, px(x) = 1{x     c2}|w | = 1{w x    
c1}|w | = ps(w x)|w |.

3

ica algorithm

we are now ready to derive an ica algorithm. the algorithm we describe
is due to bell and sejnowski, and the interpretation we give will be of their
algorithm as a method for id113. (this is di   erent
from their original interpretation, which involved a complicated idea called
the infomax principal, that is no longer necessary in the derivation given the
modern understanding of ica.)

we suppose that the distribution of each source si is given by a density

ps, and that the joint distribution of the sources s is given by

n

p(s) =

ps(si).

yi=1

note that by modeling the joint distribution as a product of the marginal,
we capture the assumption that the sources are independent. using our

formulas from the previous section, this implies the following density on
x = as = w    1s:

n

5

p(x) =

ps(wt

i x)    |w |.

yi=1

all that remains is to specify a density for the individual sources ps.

recall that, given a real-valued random variable z, its cumulative distri-
       pz(z)dz.
also, the density of z can be found from the cdf by taking its derivative:
pz(z) = f    (z).

bution function (cdf) f is de   ned by f (z0) = p (z     z0) = r z0

thus, to specify a density for the si   s, all we need to do is to specify some
cdf for it. a cdf has to be a monotonic function that increases from zero
to one. following our previous discussion, we cannot choose the cdf to be
the cdf of the gaussian, as ica doesn   t work on gaussian data. what we   ll
choose instead for the cdf, as a reasonable    default    function that slowly
increases from 0 to 1, is the sigmoid function g(s) = 1/(1 + e   s). hence,
ps(s) = g    (s).1

the square matrix w is the parameter in our model. given a training

set {x(i); i = 1, . . . , m}, the log likelihood is given by

   (w ) =

m

xi=1   n
xj=1

log g    (wt

j x(i)) + log |w |! .

we would like to maximize this in terms w . by taking derivatives and using
the fact (from the    rst set of notes) that    w |w | = |w |(w    1)t , we easily
derive a stochastic gradient ascent learning rule. for a training example x(i),
the update rule is:

   
w := w +      
            
            

1     2g(wt
1     2g(wt

1 x(i))
2 x(i))

...

1     2g(wt

n x(i))

x(i)t

   
            

+ (w t )   1   
            

,

1if you have prior knowledge that the sources    densities take a certain form, then it
is a good idea to substitute that in here. but in the absence of such knowledge, the
sigmoid function can be thought of as a reasonable default that seems to work well for
many problems. also, the presentation here assumes that either the data x(i) has been
preprocessed to have zero mean, or that it can naturally be expected to have zero mean
(such as acoustic signals). this is necessary because our assumption that ps(s) = g    (s)
implies e[s] = 0 (the derivative of the logistic function is a symmetric function, and
hence gives a density corresponding to a random variable with zero mean), which implies
e[x] = e[as] = 0.

6

where    is the learning rate.

after the algorithm converges, we then compute s(i) = w x(i) to recover

the original sources.

remark. when writing down the likelihood of the data, we implicity as-
sumed that the x(i)   s were independent of each other (for di   erent values
of i; note this issue is di   erent from whether the di   erent coordinates of
x(i) are independent), so that the likelihood of the training set was given by

qi p(x(i); w ). this assumption is clearly incorrect for speech data and other

time series where the x(i)   s are dependent, but it can be shown that having
correlated training examples will not hurt the performance of the algorithm
if we have su   cient data. but, for problems where successive training ex-
amples are correlated, when implementing stochastic gradient ascent, it also
sometimes helps accelerate convergence if we visit training examples in a ran-
domly permuted order. (i.e., run stochastic gradient ascent on a randomly
shu   ed copy of the training set.)

