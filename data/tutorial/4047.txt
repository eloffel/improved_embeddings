   #[1]yerevann

[2]yerevann blog on neural networks

interpreting neurons in an lstm network

   27 jun 2017

   by [3]tigran galstyan and [4]hrant khachatrian.

   a few months ago, we showed how effectively an lstm network can perform
   text [5]id68.

   for humans, id68 is a relatively easy and interpretable
   task, so it   s a good task for interpreting what the network is doing,
   and whether it is similar to how humans approach the same task.

   in this post we   ll try to understand: what do individual neurons of the
   network actually learn? how are they used to make decisions?

contents

     * [6]id68
     * [7]network architecture
     * [8]analyzing the neurons
          + [9]how does    t    become         ?
          + [10]what did this neuron learn?
     * [11]visualizing lstm cells
     * [12]concluding remarks

id68

   about half of the billions of internet users speak languages written in
   non-latin alphabets, like russian, arabic, chinese, greek and armenian.
   very often, they haphazardly use the latin alphabet to write those
   languages.

               : privet, privyet, priwjet,    
                  : kayf halk, keyf 7alek,    
                  : barev dzez, barew dzez,    

   so a growing share of user-generated text content is in these
      latinized    or    romanized    formats that are difficult to parse, search
   or even identify. id68 is the task of automatically
   converting this content into the native canonical format.

   aydpes aveli sirun e.:                                        :

   what makes this problem non-trivial?
    1. different users romanize in different ways, as we saw above. for
       example, v or w could be armenian   .
    2. multiple letters can be romanized to the same latin letter. for
       example, r could be armenian    or   .
    3. a single letter can be romanized to a combination of multiple latin
       letters. for example, ch could be cyrillic    or armenian   , but c
       and h by themselves are for other letters.
    4. english words and translingual latin tokens like urls occur in
       non-latin text. for example, the letters in youtube.com or msft
       should not be changed.

   humans are great at resolving these ambiguities. we showed that lstms
   can also learn to resolve all these ambiguities, at least for armenian.
   for example, our model correctly transliterated es sirum em deep
   learning into                        deep learning and not                                
                   .

network architecture

   we took lots of armenian text from wikipedia and used [13]probabilistic
   rules to obtain romanized text. the rules are chosen in a way that they
   cover most of the romanization rules people use for armenian.

   we encode latin characters as one-hot vectors and apply character level
   bidirectional lstm. at each time-step the network tries to guess the
   next character of the original armenian sentence. sometimes a single
   armenian character is represented by multiple latin letters, so it is
   very helpful to align the romanized and original texts before giving
   them to lstm (otherwise we should use sequence-to-sequence networks,
   which are harder to train). fortunately we can do the alignment,
   because the romanized version was generated by ourselves. for example,
   dzi should be transliterated into     , where dz corresponds to    and i
   to   . so we add a placeholder character in the armenian version:     
   becomes   _  , so that now z should be transliterated into _. after the
   id136 we just remove _s from the output string.

   our network consists of two lstms (228 cells) going forward and
   backward on the latin sequence. the outputs of the lstms are
   concatenated at each step (concat layer), then a dense layer with 228
   neurons is applied on top of it (hidden layer), and another dense layer
   (output layer) with softmax activations is used to get the output
   probabilities. we also concatenate the input vector to the hidden
   layer, so it has 300 neurons. this is a more simplified version of the
   network described in our [14]previous post on this topic (the main
   difference is that we don   t use the second layer of bilstm).

analyzing the neurons

   we tried to answer the following questions:
     * how does the network handle interesting cases with several possible
       outcomes (e.g. r =>    vs    etc.)?
     * what are the problems particular neurons are helping solve?

how does    t    become         ?

   first, we fixed one particular character for the input and one for the
   output. for example we are interested in how t becomes    (we know t can
   become   ,    or   ). we now that it usually happens when t appears in a
   bigram ts, which should be converted to   _.

   for every neuron, we draw the histograms of its activations in cases
   where the correct output is   , and where the correct output is not   .
   for most of the neurons these two histograms are pretty similar, but
   there are cases like this:
   input = t, output =    input = t, output !=   

   these histograms show that by looking at the activation of this
   particular neuron we can guess with high accuracy whether the output
   for t is   . to quantify the difference between the two histograms we
   used [15]hellinger distance (we take the minimum and maximum values of
   neuron activations, split the range into 1000 bins and apply discrete
   hellinger distance formula on two histograms). we calculated this
   distance for all neurons and visualized the most interesting ones in a
   single image:

   t=>  

   the color of a neuron indicates the distance between its two histograms
   (darker colors correspond to larger distances). the width of a line
   between two neurons indicate the mean of the value that the neuron on
   the lower end of the connection contributes to the neuron on the higher
   end. orange and green lines correspond to positive and negative
   signals, respectively.

   the neurons at the top of the image are from the output layer, the
   neurons below the output layer are from the hidden layer (top 12
   neurons in terms of the distance between histograms). concat layer
   comes under the hidden layer. the neurons of the concat layer are split
   into two parts: the left half of the neurons are the outputs of the
   lstm that goes forward on the input sequence and the right half
   contains the neurons from the lstm that goes backwards. from each lstm
   we display top 10 neurons in terms of the distance between histograms.

   in the case of t =>   , it is obvious that all top 12 neurons of the
   hidden layer pass positive signals to    and    (another armenian
   character that is often romanized as ts), and pass negative signals to
     ,    and others.

   t=>   - concat layer

   we can also see that the outputs of the right-to-left lstm are darker,
   which implies that these neurons    have more knowledge    about whether to
   predict   . on the other hand, the lines between those neurons and the
   hidden layer are thicker, which means that they have more contribution
   in activating the top 12 neurons in the hidden layer. this is a very
   natural result, because we know that t usually becomes    when the next
   symbol is s, and only the right-to-left lstm is aware of the next
   character.

   we did the same analysis for the neurons and gates inside the lstms.
   the results are visualized as six rows of neurons at the bottom of the
   image. in particular, it is interesting to note that the most
      confident    neurons are the so called cell inputs. recall that cell
   inputs, as well as all the gates, depend on the input at the current
   step and the hidden state of the previous step (which is the hidden
   state at the next character as we talk about the right-to-left lstm),
   so all of them are    aware    of the next s, but for some reason cell
   inputs are more confident than others.

   in the cases where s should be transliterated into _ (the placeholder),
   the useful information is more likely to come from the lstm that goes
   forward, as s becomes _ mainly in case of ts =>   _. we see that in the
   next plot:

   s=>placeholder

what did this neuron learn?

   in the second part of our analysis we tried to figure out in which
   ambiguous cases each of the neurons is most helpful. we took the set of
   latin characters that can be transliterated into more than one armenian
   letters. then we removed the cases where one of the possible outcomes
   appears less than 300 times in our 5000 sample sentences, because our
   distance metric didn   t seem to work well with few samples. and we
   analyzed every fixed neuron for every possible input-output pair.

   for example, here is the analysis of the neuron #70 of the output layer
   of the left-to-right lstm. we have seen in the previous visualization
   that it helps determining whether s should be transliterated into _. we
   see that the top input-output pairs for this neuron are the following:
   hellinger distance latin character armenian character
   0.9482             s               _
   0.8285             h                 
   0.8091             h               _
   0.6125             o                 

   so this neuron is most helpful when predicting _ from s (as we already
   knew), but it also helps to determine whether latin h should be
   transliterated as armenian    or the placeholder _ (e.g. armenian    is
   usually romanized as ch, so h sometimes becomes _).

   we visualize hellinger distances of the histograms of neuron
   activations when the input is h and the output is _, and see that the
   neuron #70 is among the top 10 neurons of the left-to-right lstm for
   the h=>_ pair.

   h=>placeholder

visualizing lstm cells

   inspired by [16]this paper by andrej karpathy, justin johnson and
   fei-fei li, we tried to find neurons or lstm cells specialised in some
   language specific patterns in the sequences. in particular, we tried to
   find the neurons that react most to the suffix            (romanized as
   tyun).

   tyun

   the first row of this visualization is the output sequence. rows below
   show the activations of the most interesting neurons:
    1. cell #6 in the lstm that goes backwards,
    2. cell #147 in the lstm that goes forward,
    3. 37th neuron in the hidden layer,
    4. 78th neuron in the concat layer.

   tyun

   we can see that cell #6 is active on tyuns and is not active on the
   other parts of the sequence. cell #144 of the forward lstm behaves the
   opposite way, it is active on everything except tyuns.

   we know that t in the suffix tyun should always become    in armenian,
   so we thought that if a neuron is active on tyuns, it may help in
   determining whether the latin t should be transliterated as    or   . so
   we visualized the most important neurons for the pair t =>   .

   t->  

   indeed, cell #147 in the forward lstm is among the top 10.

concluding remarks

   interpretability of neural networks remains an important challenge in
   machine learning. id98s and lstms perform well for many learning tasks,
   but there are very few tools to understand the inner workings of these
   systems. id68 is a pretty good problem for analyzing the
   impact of particular neurons.

   our experiments showed that too many neurons are involved in the
      decision making    even for the simplest cases, but it is possible to
   identify a subset of neurons that have more influence than the rest. on
   the other hand, most neurons are involved in multiple decision making
   processes depending on the context. this is expected, since nothing in
   the id168s we use when training neural nets forces the neurons
   to be independent and interpretable. recently, there have been [17]some
   attempts to apply information-theoretic id173 terms in order
   to obtain more interpretability. it would be interesting to test those
   ideas in the context of id68.

   we would like to thank adam mathias bittlingmayer and zara alaverdyan
   for helpful comments and discussions.
   recurrent neural networks
   natural language processing
   armenian
   interpretability

related posts

     * [18]challenges of reproducing r-net neural network using keras 25
       aug 2017
     * [19]announcing yerevann non-profit foundation 17 oct 2016
     * [20]sentence representations and id53 (slides) 21 sep
       2016

   please enable javascript to view the [21]comments powered by disqus.

references

   1. https://yerevann.github.io/atom.xml
   2. https://yerevann.github.io/
   3. https://github.com/tigrangalstyan
   4. https://github.com/hrant-khachatrian
   5. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/
   6. https://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/#id68
   7. https://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/#network-architecture
   8. https://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/#analyzing-the-neurons
   9. https://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/#how-does-t-become-  
  10. https://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/#what-did-this-neuron-learn
  11. https://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/#visualizing-lstm-cells
  12. https://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/#concluding-remarks
  13. https://github.com/yerevann/translit-id56/blob/master/languages/hy-am/id68.json
  14. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#network-architecture
  15. https://en.wikipedia.org/wiki/hellinger_distance
  16. https://arxiv.org/abs/1506.02078
  17. https://arxiv.org/abs/1606.03657
  18. https://yerevann.github.io/2017/08/25/challenges-of-reproducing-r-net-neural-network-using-keras/
  19. https://yerevann.github.io/2016/10/17/announcing-yerevann-non-profit-foundation/
  20. https://yerevann.github.io/2016/09/21/presentation-sentence-representations-and-question-answering/
  21. https://disqus.com/?ref_noscript
