   #[1]intuitions behind the world atom feed

   [2]intuitions behind the world

     * [3]intuitions behind the world
     *
     * [4]about
     * [5]research
     * [6]follow on twitter
     * [7]subscribe

      intuitions behind the world table of contents

     *
     * about
     * research
     * follow on twitter
     * subscribe

   (button)

   -->

understand pytorch code in 10 minutes

   so pytorch is the new popular framework for deep learners and many new
   papers release code in pytorch that one might want to inspect. here is
   my understanding of it narrowed down to the most basics to help read
   pytorch code. this is based on justin johnson   s [8]great tutorial. if
   you want to learn more or have more than 10 minutes for a pytorch
   starter go read that!

   pytorch consists of 4 main packages:
    1. torch: a general purpose array library similar to numpy that can do
       computations on gpu when the tensor type is cast to
       (torch.cuda.tensorfloat)
    2. torch.autograd: a package for building a computational graph and
       automatically obtaining gradients
    3. torch.nn: a neural net library with common layers and cost
       functions
    4. torch.optim: an optimization package with common optimization
       algorithms like sgd,adam, etc
    5. torch.jit: a just-in-time (jit) compiler that at runtime takes your
       pytorch models and rewrites them to run at production-efficiency.
       the jit compiler can also export your model to run in a c++-only
       runtime based on caffe2 bits.

0. import stuff

   you can import pytorch stuff like this:
import torch # arrays on gpu
import torch.autograd as autograd #build a computational graph
import torch.nn as nn ## neural net library
import torch.nn.functional as f ## most non-linearities are here
import torch.optim as optim # optimization package


1. the torch array replaces numpy ndarray ->provides id202 on gpu
support

   pytorch provides a multi-dimensional array like numpy array that can be
   processed on gpu when it   s data type is cast as
   (torch.cuda.tensorfloat). this array and it   s associated functions are
   general scientific computing tool.

   confer [9]torch for numpy users for how it relates to numpy.
# 2 matrices of size 2x3 into a 3d tensor 2x2x3
d = [ [[1., 2.,3.], [4.,5.,6.]], [[7.,8.,9.], [11.,12.,13.]] ]
d = torch.tensor(d) # array from python list
print "shape of the tensor:", d.size()

# the first index is the depth
z = d[0] + d[1]
print "adding up the two matrices of the 3d tensor:",z

shape of the tensor: torch.size([2, 2, 3])
adding up the two matrices of the 3d tensor:
  8  10  12
 15  17  19
[torch.floattensor of size 2x3]

# a heavily used operation is reshaping of tensors using .view()
print d.view(2,-1) #-1 makes torch infer the second dim

  1   2   3   4   5   6
  7   8   9  11  12  13
[torch.floattensor of size 2x6]

2. torch.autograd can make a computational graph -> auto-compute gradients

   the second feature is the autograd package that provides the ability to
   define a computational graph so that we can automatically compute
   gradients. in the computational graph, a node is an array and an edge
   is an operation on the array. to make a computational graph, we make a
   node by wrapping an array inside the torch.aurograd.variable()
   function. all operations that we do on this node from then on will be
   defined as edges in the computational graph. the edges of the graph
   also result in new nodes in the computational graph. each node in the
   graph has a .data property which is a multi-dimensional array and a
   .grad property which is it   s gradient with respect to some scalar value
   (.grad is also a .variable() itself). after defining the computational
   graph, we can calculate gradients of the loss with respect to all nodes
   in the graph with a single command i.e. loss.backward().
     * convert a tensor to a node in the computational graph using
       torch.autograd.variable()
          + access its value using x.data
          + access its gradient using x.grad
     * do operations on the .variable() to make edges of the graph

# d is a tensor not a node, to create a node based on it:
x = autograd.variable(d, requires_grad=true)
print "the node's data is the tensor:", x.data.size()
print "the node's gradient is empty at creation:", x.grad # the grad is empty ri
ght now

the node's data is the tensor: torch.size([2, 2, 3])
the node's gradient is empty at creation: none

# do operation on the node to make a computational graph
y = x + 1
z = x + y
s = z.sum()
print s.creator

<torch.autograd._functions.reduce.sum object at 0x7f1e59988790>

# calculate gradients
s.backward()
print "the variable now has gradients:",x.grad

the variable now has gradients: variable containing:
(0 ,.,.) =
  2  2  2
  2  2  2

(1 ,.,.) =
  2  2  2
  2  2  2
[torch.floattensor of size 2x2x3]

3. torch.nn contains various nn layers (linear mappings of rows of a tensor)
+ (nonlinearities ) -> helps build a neural net computational graph without
the hassle of manipulating tensors and parameters manually

   the third feature is a high-level neural networks library torch.nn that
   abstracts away all parameter handling in layers of neural networks to
   make it easy to define a nn in a few commands (e.g. torch.nn.conv).
   this package also comes with a set of popular id168s (e.g.
   torch.nn.mseloss). we start with defining a model container, for
   example, a model with a sequence of layers using torch.nn.sequential
   and then list the layers that we desire in a sequence. the library
   handles every thing else; we can access the parameter nodes variable()
   using model.parameters()
# linear transformation of a 2x5 matrix into a 2x3 matrix
linear_map = nn.linear(5,3)
print "using randomly initialized params:", linear_map.parameters

using randomly initialized params: <bound method linear.parameters of linear (5
-> 3)>

# data has 2 examples with 5 features and 3 target
data = torch.randn(2,5) # training
y = autograd.variable(torch.randn(2,3)) # target
# make a node
x = autograd.variable(data, requires_grad=true)
# apply transformation to a node creates a computational graph
a = linear_map(x)
z = f.relu(a)
o = f.softmax(z)
print "output of softmax as a id203 distribution:", o.data.view(1,-1)

# id168
loss_func = nn.mseloss() #instantiate id168
l=loss_func(z,y) # calculatemse loss between output and target
print "loss:", l

output of softmax as a id203 distribution:
 0.2092  0.1979  0.5929  0.4343  0.3038  0.2619
[torch.floattensor of size 1x6]

loss: variable containing:
 2.9838
[torch.floattensor of size 1]

   we can also define custom layers by sub-classing torch.nn.module and
   implementing a forward() function that accepts a variable() as input
   and produces a variable() as output. we can even make a dynamic network
   by defining a layer that morphs in time!
     * when defining a custom layer, 2 functions need to be implemented:
          + __init__ function has to always be inherited first, then
            define parameters of the layer here as the class variables
            i.e. self.x
          + forward funtion is where we pass an input through the layer,
            perform operations on inputs using parameters and return the
            output. the input needs to be an autograd.variable() so that
            pytorch can build the computational graph of the layer.

class log_reg_classifier(nn.module):
    def __init__(self, in_size,out_size):
        super(log_reg_classifier,self).__init__() #always call parent's init
        self.linear = nn.linear(in_size, out_size) #layer parameters

    def forward(self,vect):
        return f.log_softmax(self.linear(vect)) #

4. torch.optim can do optimization -> we build a nn computational graph using
torch.nn, compute gradients using torch.autograd, and then feed them into
torch.optim to update network parameters

   the forth feature is an optimization package torch.optim that works in
   tandem with the nn library. this library contains sophisticated
   optimizers like adam, rmsprop, etc. we define an optimizer and pass
   network parameters and learning rate to it (i.e.
   opt=torch.optim.adam(model.parameters(), lr=learning_rate)) and then we
   just call opt.step() to do a one-step update on our parameters.
optimizer = optim.sgd(linear_map.parameters(), lr = 1e-2) # instantiate optimize
r with model params + learning rate

# epoch loop: we run following until convergence
optimizer.zero_grad() # make gradients zero
l.backward(retain_variables = true)
optimizer.step()
print l

variable containing:
 2.9838
[torch.floattensor of size 1]

   building a neural net is easy. here is an example showing how things
   work together:
# define model
model = log_reg_classifier(10,2)

# define id168
loss_func = nn.mseloss()

# define optimizer
optimizer = optim.sgd(model.parameters(),lr=1e-1)

# send data through model in minibatches for 10 epochs
for epoch in range(10):
    for minibatch, target in data:
        model.zero_grad() # pytorch accumulates gradients, making them zero for
each minibatch

        #forward pass
        out = model(autograd.variable(minibatch))

        #backward pass
        l = loss_func(out,target) #calculate loss
        l.backward() # calculate gradients
        optimizer.step() # make an update step

5. torch.jit can compile python code -> useful for production of models

   regardless of whether you use tracing or @script, the result is a
   python-free representation of your model, which can be used to optimize
   the model or to export the model from python for use in production
   environments.
     * the pytorch tracer, torch.jit.trace, is a function that records all
       the native pytorch operations performed in a code region, along
       with the data dependencies between them. in fact, pytorch has had a
       tracer since 0.3, which has been used for exporting models through
       onnx. what changes now, is that you no longer necessarily need to
       take the trace and run it elsewhere - pytorch can re-execute it for
       you, using a carefully designed high-performance c++ runtime. as we
       develop pytorch 1.0 this runtime will integrate all the
       optimizations and hardware integrations that caffe2 provides.
     * tracing mode is a great way to minimize the impact on your code,
       but we   re also very excited about the models that fundamentally
       make use of control flow such as id56s. our solution to this is a
       scripting mode. in this case you write out a regular python
       function, except that you can no longer use certain more
       complicated language features. once you isolated the desired
       functionality, you let us know that you   d like the function to get
       compiled by decorating it with an @script decorator. this
       annotation will transform your python function directly into our
       high-performance c++ runtime. this lets us recover all the pytorch
       operations along with loops and conditionals. they will be embedded
       into our internal representation of this function, and will be
       accounted for every time this function is run.

from torch.jit import script

@script
def id56_loop(x):
    hidden = none
    for x_t in x.split(1):
        x, hidden = model(x, hidden)
    return x
     __________________________________________________________________

   hamidreza saghir

written by hamidreza saghir

   machine learning researcher - phd candidate at university of toronto
   [10]share on twitter [11]share on facebook [12]share on google+

   updated june 26, 2017
     __________________________________________________________________

   please enable javascript to view the [13]comments powered by disqus.

     *
     * [14]about
     * [15]research
     * [16]follow on twitter
     * [17]subscribe

      2019 [18]intuitions behind the world powered by [19]jekyll +
   [20]skinny bones.

references

   visible links
   1. https://hsaghir.github.io/atom.xml
   2. https://hsaghir.github.io/
   3. https://hsaghir.github.io/
   4. https://hsaghir.github.io/about/
   5. https://hsaghir.github.io/research/
   6. https://twitter.com/hrsaghir
   7. http://eepurl.com/cu6l3h
   8. https://github.com/jcjohnson/pytorch-examples
   9. https://github.com/torch/torch7/wiki/torch-for-numpy-users
  10. https://twitter.com/intent/tweet?text=understand pytorch code in 10 minutes&url=https://hsaghir.github.io/data_science/pytorch_starter/&via=hrsaghir
  11. https://www.facebook.com/sharer/sharer.php?u=https://hsaghir.github.io/data_science/pytorch_starter/
  12. https://plus.google.com/share?url=https://hsaghir.github.io/data_science/pytorch_starter/
  13. http://disqus.com/?ref_noscript
  14. https://hsaghir.github.io/about/
  15. https://hsaghir.github.io/research/
  16. https://twitter.com/hrsaghir
  17. http://eepurl.com/cu6l3h
  18. https://hsaghir.github.io/
  19. http://jekyllrb.com/
  20. http://mmistakes.github.io/skinny-bones-jekyll/

   hidden links:
  22. https://hsaghir.github.io/
  23. https://hsaghir.github.io/
  24. https://hsaghir.github.io/about/
  25. https://hsaghir.github.io/research/
  26. https://hsaghir.github.iohttps://twitter.com/hrsaghir
  27. https://hsaghir.github.iohttp://eepurl.com/cu6l3h
  28. https://hsaghir.github.io/
