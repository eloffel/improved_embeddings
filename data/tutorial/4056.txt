[1]convnetjs deep id24 demo

description

   this demo follows the description of the deep id24 algorithm
   described in [2]playing atari with deep id23, a paper
   from nips 2013 deep learning workshop from deepmind. the paper is a
   nice demo of a fairly standard (model-free) id23
   algorithm (id24) learning to play atari games.

   in this demo, instead of atari games, we'll start out with something
   more simple: a 2d agent that has 9 eyes pointing in different angles
   ahead and every eye senses 3 values along its direction (up to a
   certain maximum visibility distance): distance to a wall, distance to a
   green thing, or distance to a red thing. the agent navigates by using
   one of 5 actions that turn it different angles. the red things are
   apples and the agent gets reward for eating them. the green things are
   poison and the agent gets negative reward for eating them. the training
   takes a few tens of minutes with current parameter settings.

   over time, the agent learns to avoid states that lead to states with
   low rewards, and picks actions that lead to better states instead.

q-learner full specification and options

   the textfield below gets eval()'d to produce the q-learner for this
   demo. this allows you to fiddle with various parameters and settings
   and also shows how you can use the api for your own purposes. all of
   these settings are optional but are listed to give an idea of
   possibilities. feel free to change things around and hit reload!
   documentation for all options is the paper linked to above, and there
   are also comments for every option in the source code javascript file.

   var num_inputs = 27; // 9 eyes, each sees 3 numbers (wall, g
   var num_actions = 5; // 5 possible angles agent can turn____
   var temporal_window = 1; // amount of temporal memory. 0 = a
   var network_size = num_inputs*temporal_window + num_actions*
   ____________________________________________________________
   // the value function network computes a value of taking any
   // given an input state. here we specify one explicitly the 
   // but user could also equivalently instead use opt.hidden_l
   // to just insert simple relu hidden layers.________________
   var layer_defs = [];________________________________________
   layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth
   layer_defs.push({type:'fc', num_neurons: 50, activation:'rel
   layer_defs.push({type:'fc', num_neurons: 50, activation:'rel
   layer_defs.push({type:'regression', num_neurons:num_actions}
   ____________________________________________________________
   // options for the temporal difference learner that trains t
   // by backpropping the temporal difference learning rule.___
   var tdtrainer_options = {learning_rate:0.001, momentum:0.0, 
   ____________________________________________________________
   var opt = {};_______________________________________________
   opt.temporal_window = temporal_window;______________________
   opt.experience_size = 30000;________________________________
   opt.start_learn_threshold = 1000;___________________________
   opt.gamma = 0.7;____________________________________________
   opt.learning_steps_total = 200000;__________________________
   opt.learning_steps_burnin = 3000;___________________________
   opt.epsilon_min = 0.05;_____________________________________
   opt.epsilon_test_time = 0.05;_______________________________
   opt.layer_defs = layer_defs;________________________________
   opt.tdtrainer_options = tdtrainer_options;__________________
   ____________________________________________________________
   var brain = new deepqlearn.brain(num_inputs, num_actions, op
      _________________________________________________________
   (button) reload

q-learner api

   it's very simple to use deeqlearn.brain: initialize your network:
   var brain = new deepqlearn.brain(num_inputs, num_actions);

   and to train it proceed in loops as follows:
   var action = brain.forward(array_with_num_inputs_numbers);
   // action is a number in [0, num_actions) telling index of the action the age
nt chooses
   // here, apply the action on environment and observe some reward. finally, co
mmunicate it:
   brain.backward(reward); // <-- learning magic happens here

   that's it! let the agent learn over time (it will take
   opt.learning_steps_total), and it will only get better and better at
   accumulating reward as it learns. note that the agent will still take
   random actions with id203 opt.epsilon_min even once it's fully
   trained. to completely disable this randomness, or change it, you can
   disable the learning and set epsilon_test_time to 0:
   brain.epsilon_test_time = 0.0; // don't make any random choices, ever
   brain.learning = false;
   var action = brain.forward(array_with_num_inputs_numbers); // get optimal act
ion from learned policy

state visualizations

   left: current input state (quite a useless thing to look at). right:
   average reward over time (this should go up as agent becomes better on
   average at collecting rewards)
   (takes ~10 minutes to train with current settings. if you're impatient,
   scroll down and load an example pre-trained network from pre-filled
   json)

controls

   (button) go very fast (button) go fast (button) go normal speed
   (button) go slow
   (button) start learning (button) stop learning

i/o

   you can save and load a network from json here. note that the textfield
   is prefilled with a pretrained network that works reasonable well, if
   you're impatient to let yours train enough. just hit the load button!
   (button) save network to json (button) load network from json
      {"layers":[{"out_depth":59,"out_sx":1,"out_sy":1,"layer_t
      _________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

references

   1. http://cs.stanford.edu/people/karpathy/convnetjs/
   2. http://arxiv.org/pdf/1312.5602v1.pdf
