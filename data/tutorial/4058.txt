[1]convnetjs trainer demo on mnist

description

   this demo lets you evaluate multiple trainers against each other on
   mnist. by default i've set up a little benchmark that puts sgd/sgd with
   momentum/adagrad/adadelta/nesterov against each other. for reference
   math and explanations on these refer to matthew zeiler's [2]adadelta
   paper (windowgrad is idea #1 in the paper). in my own experience,
   adagrad/adadelta are "safer" because they don't depend so strongly on
   setting of learning rates (with adadelta being slightly better), but
   well-tuned sgd+momentum almost always converges faster and at better
   final values.

   report questions/bugs/suggestions to [3]@karpathy.

     __________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   re-run

loss vs. number of examples seen

testing accuracy vs. number of examples seen

training accuracy vs. number of examples seen

references

   1. http://cs.stanford.edu/people/karpathy/convnetjs/
   2. http://www.matthewzeiler.com/pubs/googletr2012/googletr2012.pdf
   3. https://twitter.com/karpathy
