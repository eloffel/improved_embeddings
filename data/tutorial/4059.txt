   this app works best with javascript enabled.

   [1]

spacy

   [usage___]
     * [2]usage
     * [3]models
     * [4]api
     * [5]universe
     *

   ____________________
    get started
       [get started     spacy 101_________..]
          + get started
          + [6]installation
          + [7]models & languages
          + [8]facts & figures
          + [9]spacy 101
               o [10]what's spacy?
               o [11]features
               o [12]linguistic annotations
               o [13]pipelines
               o [14]vocab
               o [15]serialization
               o [16]training
               o [17]language data
               o [18]lightning tour
               o [19]architecture
               o [20]community & faq
          + [21]new in v2.1
          + [22]new in v2.0
          + guides
          + [23]linguistic features
          + [24]rule-based matching
          + [25]processing pipelines
          + [26]vectors & similarity
          + [27]training models
          + [28]saving & loading
          + [29]adding languages
          + [30]visualizers
          + in-depth
          + [31]code examples

[32]spacy 101: everything you need to know

   the most important concepts, explained in simple terms

   whether you   re new to spacy, or just want to brush up on some nlp
   basics and implementation details     this page should have you covered.
   each section will explain one of spacy   s features in simple terms and
   with examples or illustrations. some sections will also reappear across
   the usage guides as a quick introduction.

help us improve the docs

   did you spot a mistake or come across explanations that are unclear? we
   always appreciate improvement [33]suggestions or [34]pull requests. you
   can find a    suggest edits    link at the bottom of each page that points
   you to the source.

[35]what   s spacy?

   spacy is a free, open-source library for advanced natural language
   processing (nlp) in python.

   if you   re working with a lot of text, you   ll eventually want to know
   more about it. for example, what   s it about? what do the words mean in
   context? who is doing what to whom? what companies and products are
   mentioned? which texts are similar to each other?

   spacy is designed specifically for production use and helps you build
   applications that process and    understand    large volumes of text. it
   can be used to build information extraction or natural language
   understanding systems, or to pre-process text for deep learning.

table of contents

     * [36]features
     * [37]linguistic annotations
     * [38]id121
     * [39]pos tags and dependencies
     * [40]named entities
     * [41]word vectors and similarity
     * [42]pipelines
     * [43]vocab, hashes and lexemes
     * [44]serialization
     * [45]training
     * [46]language data
     * [47]lightning tour
     * [48]architecture
     * [49]community & faq

[50]what spacy isn   t

     * spacy is not a platform or    an api   . unlike a platform, spacy does
       not provide a software as a service, or a web application. it   s an
       open-source library designed to help you build nlp applications,
       not a consumable service.
     * spacy is not an out-of-the-box chat bot engine. while spacy can be
       used to power conversational applications, it   s not designed
       specifically for id70, and only provides the underlying text
       processing capabilities.
     * spacy is not research software. it   s built on the latest research,
       but it   s designed to get things done. this leads to fairly
       different design decisions than [51]nltk or [52]corenlp, which were
       created as platforms for teaching and research. the main difference
       is that spacy is integrated and opinionated. spacy tries to avoid
       asking the user to choose between multiple algorithms that deliver
       equivalent functionality. keeping the menu small lets spacy deliver
       generally better performance and developer experience.m
     * spacy is not a company. it   s an open-source library. our company
       publishing spacy and other software is called [53]explosion ai.

download the spacy cheat sheet!

   [54]spacy cheatsheet

   for the launch of our [55]   advanced nlp with spacy    course on datacamp
   we created the first official spacy cheat sheet! a handy two-page
   reference to the most important concepts and features, from loading
   models and accessing linguistic annotations, to custom pipeline
   components and rule-based matching.

   [56]download

[57]features

   in the documentation, you   ll come across mentions of spacy   s features
   and capabilities. some of them refer to linguistic concepts, while
   others are related to more general machine learning functionality.
   name description
   id121 segmenting text into words, punctuations marks etc.
   part-of-speech (pos) tagging assigning word types to tokens, like verb
   or noun.
   id33 assigning syntactic dependency labels, describing
   the relations between individual tokens, like subject or object.
   lemmatization assigning the base forms of words. for example, the lemma
   of    was    is    be   , and the lemma of    rats    is    rat   .
   sentence boundary detection (sbd) finding and segmenting individual
   sentences.
   id39 (ner) labelling named    real-world    objects,
   like persons, companies or locations.
   similarity comparing words, text spans and documents and how similar
   they are to each other.
   text classification assigning categories or labels to a whole document,
   or parts of a document.
   rule-based matching finding sequences of tokens based on their texts
   and linguistic annotations, similar to id157.
   training updating and improving a statistical model   s predictions.
   serialization saving objects to files or byte strings.

[58]statistical models

   while some of spacy   s features work independently, others require
   [59]statistical models to be loaded, which enable spacy to predict
   linguistic annotations     for example, whether a word is a verb or a
   noun. spacy currently offers statistical models for a variety of
   languages, which can be installed as individual python modules. models
   can differ in size, speed, memory usage, accuracy and the data they
   include. the model you choose always depends on your use case and the
   texts you   re working with. for a general-purpose use case, the small,
   default models are always a good start. they typically include the
   following components:
     * binary weights for the part-of-speech tagger, dependency parser and
       named entity recognizer to predict those annotations in context.
     * lexical entries in the vocabulary, i.e. words and their
       context-independent attributes like the shape or spelling.
     * word vectors, i.e. multi-dimensional meaning representations of
       words that let you determine how similar they are to each other.
     * configuration options, like the language and processing pipeline
       settings, to put spacy in the correct state when you load in the
       model.

[60]linguistic annotations

   spacy provides a variety of linguistic annotations to give you insights
   into a text   s grammatical structure. this includes the word types, like
   the parts of speech, and how the words are related to each other. for
   example, if you   re analyzing text, it makes a huge difference whether a
   noun is the subject of a sentence, or the object     or whether    google   
   is used as a verb, or refers to the website or company in a specific
   context.

loading models

python -m spacy download en_core_web_sm

>>> import spacy
>>> nlp = spacy.load("en_core_web_sm")

   once you   ve [61]downloaded and installed a model, you can load it via
   [62]spacy.load(). this will return a language object containing all
   components and data needed to process text. we usually call it nlp.
   calling the nlp object on a string of text will return a processed doc:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"apple is looking at buying u.k. startup for $1 billion")
for token in doc:
    print(token.text, token.pos_, token.dep_)

   even though a doc is processed     e.g. split into individual words and
   annotated     it still holds all information of the original text, like
   whitespace characters. you can always get the offset of a token into
   the original string, or reconstruct the original by joining the tokens
   and their trailing whitespace. this way, you   ll never lose any
   information when processing text with spacy.

[63]id121

   during processing, spacy first tokenizes the text, i.e. segments it
   into words, punctuation and so on. this is done by applying rules
   specific to each language. for example, punctuation at the end of a
   sentence should be split off     whereas    u.k.    should remain one token.
   each doc consists of individual tokens, and we can iterate over them:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"apple is looking at buying u.k. startup for $1 billion")
for token in doc:
    print(token.text)

     0   1     2    3    4     5      6     7  8 9   10
   apple is looking at buying u.k. startup for $ 1 billion

   first, the raw text is split on whitespace characters, similar to
   text.split(' '). then, the tokenizer processes the text from left to
   right. on each substring, it performs two checks:
    1. does the substring match a tokenizer exception rule? for example,
          don   t    does not contain whitespace, but should be split into two
       tokens,    do    and    n   t   , while    u.k.    should always remain one
       token.
    2. can a prefix, suffix or infix be split off? for example punctuation
       like commas, periods, hyphens or quotes.

   if there   s a match, the rule is applied and the tokenizer continues its
   loop, starting with the newly split substrings. this way, spacy can
   split complex, nested tokens like combinations of abbreviations and
   multiple punctuation marks.
     * tokenizer exception: special-case rule to split a string into
       several tokens or prevent a token from being split when punctuation
       rules are applied.
     * prefix: character(s) at the beginning, e.g. $, (,    ,   .
     * suffix: character(s) at the end, e.g. km, ),    , !.
     * infix: character(s) in between, e.g. -, --, /,    .

   [64]example of the id121 process

   while punctuation rules are usually pretty general, tokenizer
   exceptions strongly depend on the specifics of the individual language.
   this is why each [65]available language has its own subclass like
   english or german, that loads in lists of hard-coded data and exception
   rules.

     id121 rules

   to learn more about how spacy   s id121 rules work in detail, how
   to customize and replace the default tokenizer and how to add
   language-specific data, see the usage guides on [66]adding languages
   and [67]customizing the tokenizer.

[68]part-of-speech tags and dependencies needs model

   after id121, spacy can parse and tag a given doc. this is where
   the statistical model comes in, which enables spacy to make a
   prediction of which tag or label most likely applies in this context. a
   model consists of binary data and is produced by showing a system
   enough examples for it to make predictions that generalize across the
   language     for example, a word following    the    in english is most
   likely a noun.

   linguistic annotations are available as [69]token attributes. like many
   nlp libraries, spacy encodes all strings to hash values to reduce
   memory usage and improve efficiency. so to get the readable string
   representation of an attribute, we need to add an underscore _ to its
   name:
import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'apple is looking at buying u.k. startup for $1 billion')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)

     * text: the original word text.
     * lemma: the base form of the word.
     * pos: the simple part-of-speech tag.
     * tag: the detailed part-of-speech tag.
     * dep: syntactic dependency, i.e. the relation between tokens.
     * shape: the word shape     capitalization, punctuation, digits.
     * is alpha: is the token an alpha character?
     * is stop: is the token part of a stop list, i.e. the most common
       words of the language?

    text    lemma   pos  tag   dep    shape alpha stop
   apple   apple   propn nnp nsubj    xxxxx true  false
   is      be      verb  vbz aux      xx    true  true
   looking look    verb  vbg root     xxxx  true  false
   at      at      adp   in  prep     xx    true  true
   buying  buy     verb  vbg pcomp    xxxx  true  false
   u.k.    u.k.    propn nnp compound x.x.  false false
   startup startup noun  nn  dobj     xxxx  true  false
   for     for     adp   in  prep     xxx   true  true
   $       $       sym   $   quantmod $     false false
   1       1       num   cd  compound d     false false
   billion billion num   cd  probj    xxxx  true  false

tip: understanding tags and labels

   most of the tags and labels look pretty abstract, and they vary between
   languages. spacy.explain will show you a short description     for
   example, spacy.explain("vbz") returns    verb, 3rd person singular
   present   .

   using spacy   s built-in [70]displacy visualizer, here   s what our example
   sentence and its dependencies look like:

   iframe: [71]data:text/html,<html><head><meta
   charset="utf-8"></head><body>%3csvg%0a%20%20%20%20xmlns%3d%22http%3a%2f
   %2fwww.w3.org%2f2000%2fsvg%22%0a%20%20%20%20xmlns%3axlink%3d%22http%3a%
   2f%2fwww.w3.org%2f1999%2fxlink%22%0a%20%20%20%20id%3d%22e109581593f245c
   e9c4ac12f78e0c74e-0%22%0a%20%20%20%20class%3d%22displacy%22%0a%20%20%20
   %20width%3d%221975%22%0a%20%20%20%20height%3d%22399.5%22%0a%20%20%20%20
   style%3d%22max-width%3a%20none%3b%20height%3a%20399.5px%3b%20color%3a%2
   0%23000000%3b%20background%3a%20%23ffffff%3b%20font-family%3a%20arial%2
   2%0a%3e%0a%20%20%20%20%3ctext%20class%3d%22displacy-token%22%20fill%3d%
   22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%
   20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d
   %22currentcolor%22%20x%3d%2250%22%3eapple%3c%2ftspan%3e%0a%20%20%20%20%
   20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fil
   l%3d%22currentcolor%22%20x%3d%2250%22%3epropn%3c%2ftspan%3e%0a%20%20%20
   %20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displacy-token%2
   2%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309
   .5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-word%
   22%20fill%3d%22currentcolor%22%20x%3d%22225%22%3eis%3c%2ftspan%3e%0a%20
   %20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222e
   m%22%20fill%3d%22currentcolor%22%20x%3d%22225%22%3everb%3c%2ftspan%3e%0
   a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displa
   cy-token%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20
   y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22disp
   lacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22400%22%3elooking%3c%2
   ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%
   22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%22400%22%3everb%
   3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20cl
   ass%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20text-anchor%3d
   %22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20
   class%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22575%22
   %3eat%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22dis
   placy-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%22575%
   22%3eadp%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3c
   text%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20text-
   anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3
   ctspan%20class%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d
   %22750%22%3ebuying%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20c
   lass%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%
   20x%3d%22750%22%3everb%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%
   20%20%20%20%3ctext%20class%3d%22displacy-token%22%20fill%3d%22currentco
   lor%22%20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20
   %20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d%22currentc
   olor%22%20x%3d%22925%22%3eu.k.%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20
   %3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22curr
   entcolor%22%20x%3d%22925%22%3epropn%3c%2ftspan%3e%0a%20%20%20%20%3c%2ft
   ext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displacy-token%22%20fill%3
   d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0
   a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-word%22%20fill%
   3d%22currentcolor%22%20x%3d%221100%22%3estartup%3c%2ftspan%3e%0a%20%20%
   20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22
   %20fill%3d%22currentcolor%22%20x%3d%221100%22%3enoun%3c%2ftspan%3e%0a%2
   0%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d%22displacy-
   token%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22middle%22%20y%3
   d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displac
   y-word%22%20fill%3d%22currentcolor%22%20x%3d%221275%22%3efor%3c%2ftspan
   %3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displacy-tag%22%20d
   y%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%221275%22%3eadp%3c%2ft
   span%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctext%20class%3d
   %22displacy-token%22%20fill%3d%22currentcolor%22%20text-anchor%3d%22mid
   dle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%
   3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%221450%22%3e%2
   4%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d%22displac
   y-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%221450%22%
   3esym%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%20%20%3ctex
   t%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%20text-anc
   hor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%20%20%3cts
   pan%20class%3d%22displacy-word%22%20fill%3d%22currentcolor%22%20x%3d%22
   1625%22%3e1%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3ctspan%20class%3d
   %22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22currentcolor%22%20x%3d%
   221625%22%3enum%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%3e%0a%0a%20%20%2
   0%20%3ctext%20class%3d%22displacy-token%22%20fill%3d%22currentcolor%22%
   20text-anchor%3d%22middle%22%20y%3d%22309.5%22%3e%0a%20%20%20%20%20%20%
   20%20%3ctspan%20class%3d%22displacy-word%22%20fill%3d%22currentcolor%22
   %20x%3d%221800%22%3ebillion%3c%2ftspan%3e%0a%20%20%20%20%20%20%20%20%3c
   tspan%20class%3d%22displacy-tag%22%20dy%3d%222em%22%20fill%3d%22current
   color%22%20x%3d%221800%22%3enum%3c%2ftspan%3e%0a%20%20%20%20%3c%2ftext%
   3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%
   20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22
   displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e10
   9581593f245ce9c4ac12f78e0c74e-0-0%22%0a%20%20%20%20%20%20%20%20%20%20%2
   0%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d
   %22m70%2c264.5%20c70%2c89.5%20395.0%2c89.5%20395.0%2c264.5%22%0a%20%20%
   20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%20%
   20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f
   %3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22
   font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%
   20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0c7
   4e-0-0%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22
   displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20sta
   rtoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%
   20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%2
   0%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20nsubj%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%2
   0%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%20class%3d%22displacy-
   arrowhead%22%20d%3d%22m70%2c266.5%20l62%2c254.5%2078%2c254.5%22%20fill%
   3d%22currentcolor%22%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%2
   0%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpa
   th%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%0a%
   20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4ac1
   2f78e0c74e-0-1%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width%3d
   %222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m245%2c264.5%20c
   245%2c177.0%20390.0%2c177.0%20390.0%2c264.5%22%0a%20%20%20%20%20%20%20%
   20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20s
   troke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20
   %20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%20
   0.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%
   20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20x
   link%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-1%22%0a%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label%
   22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%225
   0%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22cur
   rentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-anc
   hor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20%20%20%20%20aux%0a%20%20%20%20%20%20%20%20%
   20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a
   %20%20%20%20%20%20%20%20%3cpath%20class%3d%22displacy-arrowhead%22%20d%
   3d%22m245%2c266.5%20l237%2c254.5%20253%2c254.5%22%20fill%3d%22currentco
   lor%22%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%
   3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20
   %20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%20
   %20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-2
   %22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20d%3d%22m420%2c264.5%20c420%2c177.0%20
   565.0%2c177.0%20565.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20
   fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22cur
   rentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%2
   0%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20let
   ter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctex
   tpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d
   %22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-2%22%0a%20%20%20%20%20%2
   0%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label%22%0a%20%20%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%20%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0
   a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22middl
   e%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%
   20%20%20%20%20%20%20%20%20prep%0a%20%20%20%20%20%20%20%20%20%20%20%20%3
   c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%
   20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22dis
   placy-arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m565.0%
   2c266.5%20l573.0%2c254.5%20557.0%2c254.5%22%0a%20%20%20%20%20%20%20%20%
   20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0
   a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arr
   ow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%2
   0%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%2
   0%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-3%22%0a%20%20%20%2
   0%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%
   20%20%20%20%20%20d%3d%22m595%2c264.5%20c595%2c177.0%20740.0%2c177.0%207
   40.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22
   %0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22currentcolor%22%0a%2
   0%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%
   221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%20
   1px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e1095
   81593f245ce9c4ac12f78e0c74e-0-3%22%0a%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20class%3d%22displacy-label%22%0a%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20
   %20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%
   20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22middle%22%0a%20%20%20%
   20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20
   %20%20%20pcomp%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%
   0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpa
   th%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arrowhead%
   22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m740.0%2c266.5%20l748.0
   %2c254.5%20732.0%2c254.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%
   3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%3c
   %2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%2
   0%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%
   3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arro
   w-e109581593f245ce9c4ac12f78e0c74e-0-4%22%0a%20%20%20%20%20%20%20%20%20
   %20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0d%3d%22m945%2c264.5%20c945%2c177.0%201090.0%2c177.0%201090.0%2c264.5%2
   2%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%20%20%2
   0%20%20%20%20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%20%20%
   20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20
   style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4
   ac12f78e0c74e-0-4%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20
   class%3d%22displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%20%20
   %20%20%20%20%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20%20%20
   %20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20compo
   und%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20
   %20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%20class%
   3d%22displacy-arrowhead%22%20d%3d%22m945%2c266.5%20l937%2c254.5%20953%2
   c254.5%22%20fill%3d%22currentcolor%22%20%2f%3e%0a%20%20%20%20%3c%2fg%3e
   %0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20
   %20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22di
   splacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e1095
   81593f245ce9c4ac12f78e0c74e-0-5%22%0a%20%20%20%20%20%20%20%20%20%20%20%
   20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%2
   2m770%2c264.5%20c770%2c89.5%201095.0%2c89.5%201095.0%2c264.5%22%0a%20%2
   0%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%2
   0%20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%
   2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%
   22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%2
   0%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0
   c74e-0-5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%
   22displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20s
   tartoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%2
   0%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20
   %20%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20
   %20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20dobj%0a%20%20%
   20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%
   20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%
   20%20%20%20%20%20class%3d%22displacy-arrowhead%22%0a%20%20%20%20%20%20%
   20%20%20%20%20%20d%3d%22m1095.0%2c266.5%20l1103.0%2c254.5%201087.0%2c25
   4.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%
   0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%2
   0%20%3cg%20class%3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3
   cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%
   0a%20%20%20%20%20%20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4
   ac12f78e0c74e-0-6%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width
   %3d%222px%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m770%2c264.5%
   20c770%2c2.0%201275.0%2c2.0%201275.0%2c264.5%22%0a%20%20%20%20%20%20%20
   %20%20%20%20%20fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20
   stroke%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%2
   0%20%20%20%20%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%2
   00.8em%3b%20letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20
   %20%20%20%3ctextpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20
   xlink%3ahref%3d%22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-6%22%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label
   %22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%22
   50%25%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22cu
   rrentcolor%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-an
   chor%3d%22middle%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20prep%0a%20%20%20%20%20%20%20%2
   0%20%20%20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%
   0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0class%3d%22displacy-arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%2
   0d%3d%22m1275.0%2c266.5%20l1283.0%2c254.5%201267.0%2c254.5%22%0a%20%20%
   20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%2
   0%20%20%20%2f%3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class
   %3d%22displacy-arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%2
   0%20%20%20%20%20%20%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-
   7%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%
   20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m1470%2c264.5%20c1470%2c89.5%
   201795.0%2c89.5%201795.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20
   %20fill%3d%22none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22
   currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%2
   0%20%3ctext%20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20
   letter-spacing%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c
   textpath%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref
   %3d%22%23arrow-e109581593f245ce9c4ac12f78e0c74e-0-7%22%0a%20%20%20%20%2
   0%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-label%22%0a%20%20
   %20%20%20%20%20%20%20%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%2
   2%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22mi
   ddle%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%
   20%20%20%20%20%20%20%20%20%20quantmod%0a%20%20%20%20%20%20%20%20%20%20%
   20%20%3c%2ftextpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20
   %20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3
   d%22displacy-arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22
   m1470%2c266.5%20l1462%2c254.5%201478%2c254.5%22%0a%20%20%20%20%20%20%20
   %20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%
   3e%0a%20%20%20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy
   -arrow%22%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%
   20%20%20%20%20class%3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%
   20%20%20id%3d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-8%22%0a%20%20%
   20%20%20%20%20%20%20%20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20
   %20%20%20%20%20%20%20d%3d%22m1645%2c264.5%20c1645%2c177.0%201790.0%2c17
   7.0%201790.0%2c264.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%2
   2none%22%0a%20%20%20%20%20%20%20%20%20%20%20%20stroke%3d%22currentcolor
   %22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%
   20dy%3d%221.25em%22%20style%3d%22font-size%3a%200.8em%3b%20letter-spaci
   ng%3a%201px%22%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%
   20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arr
   ow-e109581593f245ce9c4ac12f78e0c74e-0-8%22%0a%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20%20class%3d%22displacy-label%22%0a%20%20%20%20%20%20
   %20%20%20%20%20%20%20%20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%
   20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%2
   0%20%20%20%20%20%20%20%20%20%20%20%20%20text-anchor%3d%22middle%22%0a%2
   0%20%20%20%20%20%20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20%20compound%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ft
   extpath%3e%0a%20%20%20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20
   %20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy
   -arrowhead%22%0a%20%20%20%20%20%20%20%20%20%20%20%20d%3d%22m1645%2c266.
   5%20l1637%2c254.5%201653%2c254.5%22%0a%20%20%20%20%20%20%20%20%20%20%20
   %20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%
   20%20%3c%2fg%3e%0a%0a%20%20%20%20%3cg%20class%3d%22displacy-arrow%22%3e
   %0a%20%20%20%20%20%20%20%20%3cpath%0a%20%20%20%20%20%20%20%20%20%20%20%
   20class%3d%22displacy-arc%22%0a%20%20%20%20%20%20%20%20%20%20%20%20id%3
   d%22arrow-e109581593f245ce9c4ac12f78e0c74e-0-9%22%0a%20%20%20%20%20%20%
   20%20%20%20%20%20stroke-width%3d%222px%22%0a%20%20%20%20%20%20%20%20%20
   %20%20%20d%3d%22m1295%2c264.5%20c1295%2c2.0%201800.0%2c2.0%201800.0%2c2
   64.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22none%22%0a%20%2
   0%20%20%20%20%20%20%20%20%20%20stroke%3d%22currentcolor%22%0a%20%20%20%
   20%20%20%20%20%2f%3e%0a%20%20%20%20%20%20%20%20%3ctext%20dy%3d%221.25em
   %22%20style%3d%22font-size%3a%200.8em%3b%20letter-spacing%3a%201px%22%3
   e%0a%20%20%20%20%20%20%20%20%20%20%20%20%3ctextpath%0a%20%20%20%20%20%2
   0%20%20%20%20%20%20%20%20%20%20xlink%3ahref%3d%22%23arrow-e109581593f24
   5ce9c4ac12f78e0c74e-0-9%22%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20
   %20%20class%3d%22displacy-label%22%0a%20%20%20%20%20%20%20%20%20%20%20%
   20%20%20%20%20startoffset%3d%2250%25%22%0a%20%20%20%20%20%20%20%20%20%2
   0%20%20%20%20%20%20fill%3d%22currentcolor%22%0a%20%20%20%20%20%20%20%20
   %20%20%20%20%20%20%20%20text-anchor%3d%22middle%22%0a%20%20%20%20%20%20
   %20%20%20%20%20%20%3e%0a%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%2
   0pobj%0a%20%20%20%20%20%20%20%20%20%20%20%20%3c%2ftextpath%3e%0a%20%20%
   20%20%20%20%20%20%3c%2ftext%3e%0a%20%20%20%20%20%20%20%20%3cpath%0a%20%
   20%20%20%20%20%20%20%20%20%20%20class%3d%22displacy-arrowhead%22%0a%20%
   20%20%20%20%20%20%20%20%20%20%20d%3d%22m1800.0%2c266.5%20l1808.0%2c254.
   5%201792.0%2c254.5%22%0a%20%20%20%20%20%20%20%20%20%20%20%20fill%3d%22c
   urrentcolor%22%0a%20%20%20%20%20%20%20%20%2f%3e%0a%20%20%20%20%3c%2fg%3
   e%0a%3c%2fsvg%3e%0a</body></html>

     part-of-speech tagging and morphology

   to learn more about part-of-speech tagging and rule-based morphology,
   and how to navigate and use the parse tree effectively, see the usage
   guides on [72]part-of-speech tagging and [73]using the dependency
   parse.

[74]named entities needs model

   a named entity is a    real-world object    that   s assigned a name     for
   example, a person, a country, a product or a book title. spacy can
   recognize[75]various types of named entities in a document, by asking
   the model for a prediction. because models are statistical and strongly
   depend on the examples they were trained on, this doesn   t always work
   perfectly and might need some tuning later, depending on your use case.

   named entities are available as the ents property of a doc:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"apple is looking at buying u.k. startup for $1 billion")

for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

     * text: the original entity text.
     * start: index of start of entity in the doc.
     * end: index of end of entity in the doc.
     * label: entity label, i.e. type.

    text    start end label                     description
 apple        0    5  org   companies, agencies, institutions.
 u.k.        27   31  gpe   geopolitical entity, i.e. countries, cities, states.
 $1 billion  44   54  money monetary values, including unit.

   using spacy   s built-in [76]displacy visualizer, here   s what our example
   sentence and its named entities look like:

   iframe: [77]data:text/html,<html><head><meta
   charset="utf-8"></head><body>%3cdiv%20class%3d%22entities%22%20style%3d
   %22line-height%3a%202.5%3b%20font-family%3a%20-apple-system%2c%20blinkm
   acsystemfont%2c%20'segoe%20ui'%2c%20helvetica%2c%20arial%2c%20sans-seri
   f%2c%20'apple%20color%20emoji'%2c%20'segoe%20ui%20emoji'%2c%20'segoe%20
   ui%20symbol'%3b%20font-size%3a%2018px%22%3ebut%20%0a%3cmark%20class%3d%
   22entity%22%20style%3d%22background%3a%20%237aecec%3b%20padding%3a%200.
   45em%200.6em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20b
   order-radius%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-web
   kit-box-decoration-break%3a%20clone%22%3egoogle%20%0a%3cspan%20style%3d
   %22font-size%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%
   201%3b%20border-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3
   b%20vertical-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg%3c
   %2fspan%3e%3c%2fmark%3eis%20starting%20from%20behind.%20the%20company%2
   0made%20a%20late%20push%20into%20hardware%2c%0aand%20%0a%3cmark%20class
   %3d%22entity%22%20style%3d%22background%3a%20%237aecec%3b%20padding%3a%
   200.45em%200.6em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b
   %20border-radius%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20
   -webkit-box-decoration-break%3a%20clone%22%3eapple%20%0a%3cspan%20style
   %3d%22font-size%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%
   3a%201%3b%20border-radius%3a%200.35em%3b%20text-transform%3a%20uppercas
   e%3b%20vertical-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg
   %3c%2fspan%3e%3c%2fmark%3e%e2%80%99s%20%0a%3cmark%20class%3d%22entity%2
   2%20style%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6
   em%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radi
   us%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-de
   coration-break%3a%20clone%22%3esiri%20%0a%3cspan%20style%3d%22font-size
   %3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20bor
   der-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical
   -align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%
   3e%3c%2fmark%3e%2c%20available%20on%20%0a%3cmark%20class%3d%22entity%22
   %20style%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6e
   m%3b%20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radiu
   s%3a%200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-dec
   oration-break%3a%20clone%22%3eiphones%20%0a%3cspan%20style%3d%22font-si
   ze%3a%200.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20b
   order-radius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertic
   al-align%3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspa
   n%3e%3c%2fmark%3e%2c%20and%20%0a%3cmark%20class%3d%22entity%22%20style%
   3d%22background%3a%20%237aecec%3b%20padding%3a%200.45em%200.6em%3b%20ma
   rgin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.
   35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-decoration-b
   reak%3a%20clone%22%3eamazon%20%0a%3cspan%20style%3d%22font-size%3a%200.
   8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20border-radi
   us%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical-align%3
   a%20middle%3b%20margin-left%3a%200.5rem%22%3eorg%3c%2fspan%3e%3c%2fmark
   %3e%e2%80%99s%20%0a%3cmark%20class%3d%22entity%22%20style%3d%22backgrou
   nd%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%20margin%3a%200%2
   00.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b%20box
   -decoration-break%3a%20clone%3b%20-webkit-box-decoration-break%3a%20clo
   ne%22%3ealexa%20%0a%3cspan%20style%3d%22font-size%3a%200.8em%3b%20font-
   weight%3a%20bold%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em
   %3b%20text-transform%3a%20uppercase%3b%20vertical-align%3a%20middle%3b%
   20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%2fmark%3esoftwar
   e%2c%20which%20runs%20on%20its%20%0a%3cmark%20class%3d%22entity%22%20st
   yle%3d%22background%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%
   20margin%3a%200%200.25em%3b%20line-height%3a%201%3b%20border-radius%3a%
   200.35em%3b%20box-decoration-break%3a%20clone%3b%20-webkit-box-decorati
   on-break%3a%20clone%22%3eecho%20%0a%3cspan%20style%3d%22font-size%3a%20
   0.8em%3b%20font-weight%3a%20bold%3b%20line-height%3a%201%3b%20border-ra
   dius%3a%200.35em%3b%20text-transform%3a%20uppercase%3b%20vertical-align
   %3a%20middle%3b%20margin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%
   2fmark%3eand%20%0a%3cmark%20class%3d%22entity%22%20style%3d%22backgroun
   d%3a%20%23bfeeb7%3b%20padding%3a%200.45em%200.6em%3b%20margin%3a%200%20
   0.25em%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b%20box-
   decoration-break%3a%20clone%3b%20-webkit-box-decoration-break%3a%20clon
   e%22%3edot%20%0a%3cspan%20style%3d%22font-size%3a%200.8em%3b%20font-wei
   ght%3a%20bold%3b%20line-height%3a%201%3b%20border-radius%3a%200.35em%3b
   %20text-transform%3a%20uppercase%3b%20vertical-align%3a%20middle%3b%20m
   argin-left%3a%200.5rem%22%3eproduct%3c%2fspan%3e%3c%2fmark%3edevices%2c
   %20have%20clear%20leads%20in%20consumer%20adoption.%3c%2fdiv%3e%0a</bod
   y></html>

     id39

   to learn more about entity recognition in spacy, how to add your own
   entities to a document and how to train and update the entity
   predictions of a model, see the usage guides on [78]named entity
   recognition and [79]training the named entity recognizer.

[80]word vectors and similarity needs model

   similarity is determined by comparing word vectors or    word
   embeddings   , multi-dimensional meaning representations of a word. word
   vectors can be generated using an algorithm like [81]id97 and
   usually look like this:

banana.vector
array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,
       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,
      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,
       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,
      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,
       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,
       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,
       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,
       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,
       # ... and so on ...
       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,
      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,
      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)

important note

   to make them compact and fast, spacy   s small [82]models (all packages
   that end in sm) don   t ship with word vectors, and only include
   context-sensitive tensors. this means you can still use the
   similarity() methods to compare documents, spans and tokens     but the
   result won   t be as good, and individual tokens won   t have any vectors
   assigned. so in order to use real word vectors, you need to download a
   larger model:
- python -m spacy download en_core_web_sm
+ python -m spacy download en_core_web_lg

   models that come with built-in word vectors make them available as the
   [83]token.vector attribute. [84]doc.vector and [85]span.vector will
   default to an average of their token vectors. you can also check if a
   token has a vector assigned, and get the l2 norm, which can be used to
   normalize vectors.
import spacy

nlp = spacy.load('en_core_web_md')
tokens = nlp(u'dog cat banana afskfsd')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)

     * text: the original token text.
     * has vector: does the token have a vector representation?
     * vector norm: the l2 norm of the token   s vector (the square root of
       the sum of the values squared)
     * oov: out-of-vocabulary

   the words    dog   ,    cat    and    banana    are all pretty common in english,
   so they   re part of the model   s vocabulary, and come with a vector. the
   word    afskfsd    on the other hand is a lot less common and
   out-of-vocabulary     so its vector representation consists of 300
   dimensions of 0, which means it   s practically nonexistent. if your
   application will benefit from a large vocabulary with more vectors, you
   should consider using one of the larger models or loading in a full
   vector package, for example, [86]en_vectors_web_lg, which includes over
   1 million unique vectors.

   spacy is able to compare two objects, and make a prediction of how
   similar they are. predicting similarity is useful for building
   id126s or flagging duplicates. for example, you can
   suggest a user content that   s similar to what they   re currently looking
   at, or label a support ticket as a duplicate if it   s very similar to an
   already existing one.

   each doc, span and token comes with a [87].similarity() method that
   lets you compare it with another object, and determine the similarity.
   of course similarity is always subjective     whether    dog    and    cat    are
   similar really depends on how you   re looking at it. spacy   s similarity
   model usually assumes a pretty general-purpose definition of
   similarity.
import spacy

nlp = spacy.load('en_core_web_md')  # make sure to use larger model!
tokens = nlp(u'dog cat banana')

for token1 in tokens:
    for token2 in tokens:
        print(token1.text, token2.text, token1.similarity(token2))

   in this case, the model   s predictions are pretty on point. a dog is
   very similar to a cat, whereas a banana is not very similar to either
   of them. identical tokens are obviously 100% similar to each other
   (just not always exactly 1.0, because of vector math and floating point
   imprecisions).

     word vectors

   to learn more about word vectors, how to customize them and how to load
   your own vectors into spacy, see the usage guide on [88]using word
   vectors and semantic similarities.

[89]pipelines

   when you call nlp on a text, spacy first tokenizes the text to produce
   a doc object. the doc is then processed in several different steps    
   this is also referred to as the processing pipeline. the pipeline used
   by the [90]default models consists of a tagger, a parser and an entity
   recognizer. each pipeline component returns the processed doc, which is
   then passed on to the next component.
   [91]the processing pipeline
     * name: id of the pipeline component.
     * component: spacy   s implementation of the component.
     * creates: objects, attributes and properties modified and set by the
       component.

   name component creates description
   tokenizer [92]tokenizer doc segment text into tokens.
   tagger [93]tagger doc[i].tag assign part-of-speech tags.
   parser [94]dependencyparser doc[i].head, doc[i].dep, doc.sents,
   doc.noun_chunks assign dependency labels.
   ner [95]entityrecognizer doc.ents, doc[i].ent_iob, doc[i].ent_type
   detect and label named entities.
   textcat [96]textcategorizer doc.cats assign document labels.
       [97]custom components doc._.xxx, token._.xxx, span._.xxx assign
   custom attributes, methods or properties.

   the processing pipeline always depends on the statistical model and its
   capabilities. for example, a pipeline can only include an entity
   recognizer component if the model includes data to make predictions of
   entity labels. this is why each model will specify the pipeline to use
   in its meta data, as a simple list containing the component names:
"pipeline": ["tagger", "parser", "ner"]

(button) does the order of pipeline components matter?[98]  

   in spacy v2.x, the statistical components like the tagger or parser are
   independent and don   t share any data between themselves. for example,
   the named entity recognizer doesn   t use any features set by the tagger
   and parser, and so on. this means that you can swap them, or remove
   single components from the pipeline without affecting the others.

   however, custom components may depend on annotations set by other
   components. for example, a custom lemmatizer may need the
   part-of-speech tags assigned, so it   ll only work if it   s added after
   the tagger. the parser will respect pre-defined sentence boundaries, so
   if a previous component in the pipeline sets them, its dependency
   predictions may be different. similarly, it matters if you add the
   [99]entityruler before or after the statistical entity recognizer: if
   it   s added before, the entity recognizer will take the existing
   entities into account when making predictions.
     __________________________________________________________________

     processing pipelines

   to learn more about how processing pipelines work in detail, how to
   enable and disable their components, and how to create your own, see
   the usage guide on [100]language processing pipelines.

[101]vocab, hashes and lexemes

   whenever possible, spacy tries to store data in a vocabulary, the
   [102]vocab, that will be shared by multiple documents. to save memory,
   spacy also encodes all strings to hash values     in this case for
   example,    coffee    has the hash 3197928453018144401. entity labels like
      org    and part-of-speech tags like    verb    are also encoded. internally,
   spacy only    speaks    in hash values.
     * token: a word, punctuation mark etc. in context, including its
       attributes, tags and dependencies.
     * lexeme: a    word type    with no context. includes the word shape and
       flags, e.g. if it   s lowercase, a digit or punctuation.
     * doc: a processed container of tokens in context.
     * vocab: the collection of lexemes.
     * stringstore: the dictionary mapping hash values to strings, for
       example 3197928453018144401        coffee   .

   [103]doc, vocab, lexeme and stringstore

   if you process lots of documents containing the word    coffee    in all
   kinds of different contexts, storing the exact string    coffee    every
   time would take up way too much space. so instead, spacy hashes the
   string and stores it in the [104]stringstore. you can think of the
   stringstore as a lookup table that works in both directions     you can
   look up a string to get its hash, or a hash to get its string:
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"i love coffee")
print(doc.vocab.strings[u"coffee"])  # 3197928453018144401
print(doc.vocab.strings[3197928453018144401])  # 'coffee'

what does    l    at the end of a hash mean?

   if you return a hash value in the python 2 interpreter, it   ll show up
   as 3197928453018144401l. the l just means    long integer        it   s not
   actually a part of the hash value.

   now that all strings are encoded, the entries in the vocabulary don   t
   need to include the word text themselves. instead, they can look it up
   in the stringstore via its hash value. each entry in the vocabulary,
   also called [105]lexeme, contains the context-independent information
   about a word. for example, no matter if    love    is used as a verb or a
   noun in some context, its spelling and whether it consists of
   alphabetic characters won   t ever change. its hash value will also
   always be the same.
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"i love coffee")
for word in doc:
    lexeme = doc.vocab[word.text]
    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix
_,
            lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)

     * text: the original text of the lexeme.
     * orth: the hash value of the lexeme.
     * shape: the abstract word shape of the lexeme.
     * prefix: by default, the first letter of the word string.
     * suffix: by default, the last three letters of the word string.
     * is alpha: does the lexeme consist of alphabetic characters?
     * is digit: does the lexeme consist of digits?

    text         orth         shape prefix suffix is_alpha is_digit
   i      4690420944186131903 x     i      i      true     false
   love   3702023516439754181 xxxx  l      ove    true     false
   coffee 3197928453018144401 xxxx  c      fee    true     false

   the mapping of words to hashes doesn   t depend on any state. to make
   sure each value is unique, spacy uses a [106]hash function to calculate
   the hash based on the word string. this also means that the hash for
      coffee    will always be the same, no matter which model you   re using or
   how you   ve configured spacy.

   however, hashes cannot be reversed and there   s no way to resolve
   3197928453018144401 back to    coffee   . all spacy can do is look it up in
   the vocabulary. that   s why you always need to make sure all objects you
   create have access to the same vocabulary. if they don   t, spacy might
   not be able to find the strings it needs.
import spacy
from spacy.tokens import doc
from spacy.vocab import vocab

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"i love coffee")  # original doc
print(doc.vocab.strings[u"coffee"])  # 3197928453018144401
print(doc.vocab.strings[3197928453018144401])  # 'coffee'     

empty_doc = doc(vocab())  # new doc with empty vocab
# empty_doc.vocab.strings[3197928453018144401] will raise an error :(

empty_doc.vocab.strings.add(u"coffee")  # add "coffee" and generate hash
print(empty_doc.vocab.strings[3197928453018144401])  # 'coffee'     

new_doc = doc(doc.vocab)  # create new doc with first doc's vocab
print(new_doc.vocab.strings[3197928453018144401])  # 'coffee'     

   if the vocabulary doesn   t contain a string for 3197928453018144401,
   spacy will raise an error. you can re-add    coffee    manually, but this
   only works if you actually know that the document contains that word.
   to prevent this problem, spacy will also export the vocab when you save
   a doc or nlp object. this will give you the object and its encoded
   annotations, plus the    key    to decode it.

[107]serialization

   if you   ve been modifying the pipeline, vocabulary, vectors and
   entities, or made updates to the model, you   ll eventually want to save
   your progress     for example, everything that   s in your nlp object. this
   means you   ll have to translate its contents and structure into a format
   that can be saved, like a file or a byte string. this process is called
   serialization. spacy comes with built-in serialization methods and
   supports the [108]pickle protocol.

what   s pickle?

   pickle is python   s built-in object persistence system. it lets you
   transfer arbitrary python objects between processes. this is usually
   used to load an object to and from disk, but it   s also used for
   distributed computing, e.g. with [109]pyspark or [110]dask. when you
   unpickle an object, you   re agreeing to execute whatever code it
   contains. it   s like calling eval() on a string     so don   t unpickle
   objects from untrusted sources.

   all container classes, i.e. [111]language (nlp), [112]doc, [113]vocab
   and [114]stringstore have the following methods available:
     method   returns        example
   to_bytes   bytes   data = nlp.to_bytes()
   from_bytes object  nlp.from_bytes(data)
   to_disk    -       nlp.to_disk("/path")
   from_disk  object  nlp.from_disk("/path")

     saving and loading

   to learn more about how to save and load your own models, see the usage
   guide on [115]saving and loading.

[116]training

   spacy   s models are statistical and every    decision    they make     for
   example, which part-of-speech tag to assign, or whether a word is a
   named entity     is a prediction. this prediction is based on the
   examples the model has seen during training. to train a model, you
   first need training data     examples of text, and the labels you want
   the model to predict. this could be a part-of-speech tag, a named
   entity or any other information.

   the model is then shown the unlabelled text and will make a prediction.
   because we know the correct answer, we can give the model feedback on
   its prediction in the form of an error gradient of the id168
   that calculates the difference between the training example and the
   expected output. the greater the difference, the more significant the
   gradient and the updates to our model.
     * training data: examples and their annotations.
     * text: the input text the model should predict a label for.
     * label: the label the model should predict.
     * gradient: gradient of the id168 calculating the difference
       between input and expected output.

   [117]the training process

   when training a model, we don   t just want it to memorize our examples    
   we want it to come up with theory that can be generalized across other
   examples. after all, we don   t just want the model to learn that this
   one instance of    amazon    right here is a company     we want it to learn
   that    amazon   , in contexts like this, is most likely a company. that   s
   why the training data should always be representative of the data we
   want to process. a model trained on wikipedia, where sentences in the
   first person are extremely rare, will likely perform badly on twitter.
   similarly, a model trained on romantic novels will likely perform badly
   on legal text.

   this also means that in order to know how the model is performing, and
   whether it   s learning the right things, you don   t only need training
   data     you   ll also need evaluation data. if you only test the model
   with the data it was trained on, you   ll have no idea how well it   s
   generalizing. if you want to train a model from scratch, you usually
   need at least a few hundred examples for both training and evaluation.
   to update an existing model, you can already achieve decent results
   with very few examples     as long as they   re representative.

     training statistical models

   to learn more about training and updating models, how to create
   training data and how to improve spacy   s id39
   models, see the usage guides on [118]training.

[119]language data

   every language is different     and usually full of exceptions and
   special cases, especially amongst the most common words. some of these
   exceptions are shared across languages, while others are entirely
   specific     usually so specific that they need to be hard-coded. the
   [120]lang module contains all language-specific data, organized in
   simple python files. this makes the data easy to update and extend.

   the shared language data in the directory root includes rules that can
   be generalized across languages     for example, rules for basic
   punctuation, emoji, emoticons, single-letter abbreviations and norms
   for equivalent tokens with different spellings, like " and    . this
   helps the models make more accurate predictions. the individual
   language data in a submodule contains rules that are only relevant to a
   particular language. it also takes care of putting together all
   components and creating the language subclass     for example, english or
   german.
from spacy.lang.en import english
from spacy.lang.de import german

nlp_en = english()  # includes english data
nlp_de = german()  # includes german data

   [121]language data architecture
   name description
   stop words
   [122]stop_words.py list of most common words of a language that are
   often useful to filter out, for example    and    or    i   . matching tokens
   will return true for is_stop.
   tokenizer exceptions
   [123]tokenizer_exceptions.py special-case rules for the tokenizer, for
   example, contractions like    can   t    and abbreviations with punctuation,
   like    u.k.   .
   norm exceptions
   [124]norm_exceptions.py special-case rules for normalizing tokens to
   improve the model   s predictions, for example on american vs. british
   spelling.
   punctuation rules
   [125]punctuation.py id157 for splitting tokens, e.g. on
   punctuation or special characters like emoji. includes rules for
   prefixes, suffixes and infixes.
   character classes
   [126]char_classes.py character classes to be used in regular
   expressions, for example, latin characters, quotes, hyphens or icons.
   lexical attributes
   [127]lex_attrs.py custom functions for setting lexical attributes on
   tokens, e.g. like_num, which includes language-specific words like
      ten    or    hundred   .
   syntax iterators
   [128]syntax_iterators.py functions that compute views of a doc object
   based on its syntax. at the moment, only used for [129]noun chunks.
   lemmatizer
   [130]lemmatizer.py lemmatization rules or a lookup-based lemmatization
   table to assign base forms, for example    be    for    was   .
   tag map
   [131]tag_map.py dictionary mapping strings in your tag set to
   [132]universal dependencies tags.
   morph rules
   [133]morph_rules.py exception rules for morphological analysis of
   irregular words like personal pronouns.

     language data

   to learn more about the individual components of the language data and
   how to add a new language to spacy in preparation for training a
   language model, see the usage guide on [134]adding languages.

[135]lightning tour

   the following examples and code snippets give you an overview of
   spacy   s functionality and its usage.

[136]install models and process text

python -m spacy download en_core_web_sm
python -m spacy download de_core_news_sm

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"hello, world. here are two sentences.")
print([t.text for t in doc])

nlp_de = spacy.load("de_core_news_sm")
doc_de = nlp_de(u"ich bin ein berliner.")
print([t.text for t in doc_de])

   api: [137]spacy.load() usage:[138]models, [139]spacy 101

[140]get tokens, noun chunks & sentences needs model

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"peach emoji is where it has always been. peach is the superior "
          u"emoji. it's outranking eggplant      ")
print(doc[0].text)          # 'peach'
print(doc[1].text)          # 'emoji'
print(doc[-1].text)         # '    '
print(doc[17:19].text)      # 'outranking eggplant'

noun_chunks = list(doc.noun_chunks)
print(noun_chunks[0].text)  # 'peach emoji'

sentences = list(doc.sents)
assert len(sentences) == 3
print(sentences[1].text)    # 'peach is the superior emoji.'

   api: [141]doc, [142]token usage:[143]spacy 101

[144]get part-of-speech tags and flags needs model

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"apple is looking at buying u.k. startup for $1 billion")
apple = doc[0]
print("fine-grained pos tag", apple.pos_, apple.pos)
print("coarse-grained pos tag", apple.tag_, apple.tag)
print("word shape", apple.shape_, apple.shape)
print("alphanumeric characters?", apple.is_alpha)
print("punctuation mark?", apple.is_punct)

billion = doc[10]
print("digit?", billion.is_digit)
print("like a number?", billion.like_num)
print("like an email address?", billion.like_email)

   api: [145]token usage:[146]part-of-speech tagging

[147]use hash values for any string

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"i love coffee")

coffee_hash = nlp.vocab.strings[u"coffee"]  # 3197928453018144401
coffee_text = nlp.vocab.strings[coffee_hash]  # 'coffee'
print(coffee_hash, coffee_text)
print(doc[2].orth, coffee_hash)  # 3197928453018144401
print(doc[2].text, coffee_text)  # 'coffee'

beer_hash = doc.vocab.strings.add(u"beer")  # 3073001599257881079
beer_text = doc.vocab.strings[beer_hash]  # 'beer'
print(beer_hash, beer_text)

unicorn_hash = doc.vocab.strings.add(u"     ")  # 18234233413267120783
unicorn_text = doc.vocab.strings[unicorn_hash]  # '     '
print(unicorn_hash, unicorn_text)

   api: [148]stringstore usage:[149]vocab, hashes and lexemes 101

[150]recognize and update named entities needs model

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"san francisco considers banning sidewalk delivery robots")
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

from spacy.tokens import span

doc = nlp(u"fb is hiring a new vp of global policy")
doc.ents = [span(doc, 0, 1, label=doc.vocab.strings[u"org"])]
for ent in doc.ents:
    print(ent.text, ent.start_char, ent.end_char, ent.label_)

   usage: [151]id39

[152]train and update neural network models

import spacy
import random

nlp = spacy.load("en_core_web_sm")
train_data = [(u"uber blew through $1 million", {"entities": [(0, 4, "org")]})]

other_pipes = [pipe for pipe in nlp.pipe_names if pipe != "ner"]
with nlp.disable_pipes(*other_pipes):
    optimizer = nlp.begin_training()
    for i in range(10):
        random.shuffle(train_data)
        for text, annotations in train_data:
            nlp.update([text], [annotations], sgd=optimizer)
nlp.to_disk("/model")

   api: [153]language.update usage:[154]training spacy   s statistical
   models

[155]visualize a dependency parse and named entities in your browser
v2.0needs model

output

   [156]displacy visualization
from spacy import displacy

doc_dep = nlp(u"this is a sentence.")
displacy.serve(doc_dep, style="dep")

doc_ent = nlp(u"when sebastian thrun started working on self-driving cars at goo
gle "
              u"in 2007, few people outside of the company took him seriously.")
displacy.serve(doc_ent, style="ent")

   api: [157]displacy usage:[158]visualizers

[159]get word vectors and similarity needs model

import spacy

nlp = spacy.load("en_core_web_md")
doc = nlp(u"apple and banana are similar. pasta and hippo aren't.")

apple = doc[0]
banana = doc[2]
pasta = doc[6]
hippo = doc[8]

print("apple <-> banana", apple.similarity(banana))
print("pasta <-> hippo", pasta.similarity(hippo))
print(apple.has_vector, banana.has_vector, pasta.has_vector, hippo.has_vector)

   for the best results, you should run this example using the
   [160]en_vectors_web_lg model (currently not available in the live
   demo).

   usage: [161]word vectors and similarity

[162]simple and efficient serialization

import spacy
from spacy.tokens import doc
from spacy.vocab import vocab

nlp = spacy.load("en_core_web_sm")
customer_feedback = open("customer_feedback_627.txt").read()
doc = nlp(customer_feedback)
doc.to_disk("/tmp/customer_feedback_627.bin")

new_doc = doc(vocab()).from_disk("/tmp/customer_feedback_627.bin")

   api: [163]language, [164]doc usage:[165]saving and loading models

[166]match text with token rules

import spacy
from spacy.matcher import matcher

nlp = spacy.load("en_core_web_sm")
matcher = matcher(nlp.vocab)

def set_sentiment(matcher, doc, i, matches):
    doc.sentiment += 0.1

pattern1 = [{"orth": "google"}, {"orth": "i"}, {"orth": "/"}, {"orth": "o"}]
pattern2 = [[{"orth": emoji, "op": "+"}] for emoji in ["    ", "    ", "    ", "    "]]
matcher.add("googleio", none, pattern1)  # match "google i/o" or "google i/o"
matcher.add("happy", set_sentiment, *pattern2)  # match one or more happy emoji

doc = nlp(u"a text about google i/o         ")
matches = matcher(doc)

for match_id, start, end in matches:
    string_id = nlp.vocab.strings[match_id]
    span = doc[start:end]
    print(string_id, span.text)
print("sentiment", doc.sentiment)

   api: [167]matcher usage:[168]rule-based matching

[169]minibatched stream processing

texts = [u"one document.", u"...", u"lots of documents"]
# .pipe streams input, and produces streaming output
iter_texts = (texts[i % 3] for i in range(100000000))
for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50)):
    assert doc.is_parsed
    if i == 100:
        break

[170]get syntactic dependencies needs model

import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"when sebastian thrun started working on self-driving cars at google
"
          u"in 2007, few people outside of the company took him seriously.")

dep_labels = []
for token in doc:
    while token.head != token:
        dep_labels.append(token.dep_)
        token = token.head
print(dep_labels)

   api: [171]token usage:[172]using the dependency parse

[173]export to numpy arrays

import spacy
from spacy.attrs import orth, like_url

nlp = spacy.load("en_core_web_sm")
doc = nlp(u"check out https://spacy.io")
for token in doc:
    print(token.text, token.orth, token.like_url)

attr_ids = [orth, like_url]
doc_array = doc.to_array(attr_ids)
print(doc_array.shape)
print(len(doc), len(attr_ids))

assert doc[0].orth == doc_array[0, 0]
assert doc[1].orth == doc_array[1, 0]
assert doc[0].like_url == doc_array[0, 1]

assert list(doc_array[:, 1]) == [t.like_url for t in doc]
print(list(doc_array[:, 1]))

[174]calculate inline markup on original string

import spacy

def put_spans_around_tokens(doc):
    """here, we're building a custom "syntax highlighter" for
    part-of-speech tags and dependencies. we put each token in a
    span element, with the appropriate classes computed. all whitespace is
    preserved, outside of the spans. (of course, html will only display
    multiple whitespace if enabled     but the point is, no information is lost
    and you can calculate what you need, e.g. <br />, <p> etc.)
    """
    output = []
    html = '<span class="{classes}">{word}</span>{space}'
    for token in doc:
        if token.is_space:
            output.append(token.text)
        else:
            classes = "pos-{} dep-{}".format(token.pos_, token.dep_)
            output.append(html.format(classes=classes, word=token.text, space=to
ken.whitespace_))
    string = "".join(output)
    string = string.replace("\n", "")
    string = string.replace("\t", "    ")
    return "<pre>{}</pre>".format(string)


nlp = spacy.load("en_core_web_sm")
doc = nlp(u"this is a test.\n\nhello   world.")
html = put_spans_around_tokens(doc)
print(html)

[175]architecture

   the central data structures in spacy are the doc and the vocab. the doc
   object owns the sequence of tokens and all their annotations. the vocab
   object owns a set of look-up tables that make common information
   available across documents. by centralizing strings, word vectors and
   lexical attributes, we avoid storing multiple copies of this data. this
   saves memory, and ensures there   s a single source of truth.

   text annotations are also designed to allow a single source of truth:
   the doc object owns the data, and span and token are views that point
   into it. the doc object is constructed by the tokenizer, and then
   modified in place by the components of the pipeline. the language
   object coordinates these components. it takes raw text and sends it
   through the pipeline, returning an annotated document. it also
   orchestrates training and serialization.
   [176]library architecture

[177]container objects

   name description
   [178]doc a container for accessing linguistic annotations.
   [179]span a slice from a doc object.
   [180]token an individual token     i.e. a word, punctuation symbol,
   whitespace, etc.
   [181]lexeme an entry in the vocabulary. it   s a word type with no
   context, as opposed to a word token. it therefore has no part-of-speech
   tag, dependency parse etc.

[182]processing pipeline

   name description
   [183]language a text-processing pipeline. usually you   ll load this once
   per process as nlp and pass the instance around your application.
   [184]tokenizer segment text, and create doc objects with the discovered
   segment boundaries.
   [185]lemmatizer determine the base forms of words.
   morphology assign linguistic features like lemmas, noun case, verb
   tense etc. based on the word and its part-of-speech tag.
   [186]tagger annotate part-of-speech tags on doc objects.
   [187]dependencyparser annotate syntactic dependencies on doc objects.
   [188]entityrecognizer annotate named entities, e.g. persons or
   products, on doc objects.
   [189]textcategorizer assign categories or labels to doc objects.
   [190]matcher match sequences of tokens, based on pattern rules, similar
   to id157.
   [191]phrasematcher match sequences of tokens based on phrases.
   [192]entityruler add entity spans to the doc using token-based rules or
   exact phrase matches.
   [193]sentencizer implement custom sentence boundary detection logic
   that doesn   t require the dependency parse.
   [194]other functions automatically apply something to the doc, e.g. to
   merge spans of tokens.

[195]other classes

   name description
   [196]vocab a lookup table for the vocabulary that allows you to access
   lexeme objects.
   [197]stringstore map strings to and from hash values.
   [198]vectors container class for vector data keyed by string.
   [199]goldparse collection for training annotations.
   [200]goldcorpus an annotated corpus, using the json file format.
   manages annotations for tagging, id33 and ner.

[201]community & faq

   we   re very happy to see the spacy community grow and include a mix of
   people from all kinds of different backgrounds     computational
   linguistics, data science, deep learning, research and more. if you   d
   like to get involved, below are some answers to the most important
   questions and resources for further reading.

[202]help, my code isn   t working!

   bugs suck, and we   re doing our best to continuously improve the tests
   and fix bugs as soon as possible. before you submit an issue, do a
   quick search and check if the problem has already been reported. if
   you   re having installation or loading problems, make sure to also check
   out the [203]troubleshooting guide. help with spacy is available via
   the following platforms:

how do i know if something is a bug?

   of course, it   s always hard to know for sure, so don   t worry     we   re
   not going to be mad if a bug report turns out to be a typo in your
   code. as a simple rule, any c-level error without a python traceback,
   like a segmentation fault or memory error, is always a spacy bug.

   because models are statistical, their performance will never be
   perfect. however, if you come across patterns that might indicate an
   underlying issue, please do file a report. similarly, we also care
   about behaviors that contradict our docs.
     * [204]stack overflow: usage questions and everything related to
       problems with your specific code. the stack overflow community is
       much larger than ours, so if your problem can be solved by others,
       you   ll receive help much quicker.
     * [205]gitter chat: general discussion about spacy, meeting other
       community members and exchanging tips, tricks and best practices.
     * [206]github issue tracker: bug reports and improvement suggestions,
       i.e. everything that   s likely spacy   s fault. this also includes
       problems with the models beyond statistical imprecisions, like
       patterns that point to a bug.

important note

   please understand that we won   t be able to provide individual support
   via email. we also believe that help is much more valuable if it   s
   shared publicly, so that more people can benefit from it. if you come
   across an issue and you think you might be able to help, consider
   posting a quick update with your solution. no matter how simple, it can
   easily save someone a lot of time and headache     and the next time you
   need help, they might repay the favor.

[207]how can i contribute to spacy?

   you don   t have to be an nlp expert or python pro to contribute, and
   we   re happy to help you get started. if you   re new to spacy, a good
   place to start is the [208]help wanted (easy) label on github, which we
   use to tag bugs and feature requests that are easy and self-contained.
   we also appreciate contributions to the docs     whether it   s fixing a
   typo, improving an example or adding additional explanations. you   ll
   find a    suggest edits    link at the bottom of each page that points you
   to the source.

   another way of getting involved is to help us improve the [209]language
   data     especially if you happen to speak one of the languages currently
   in [210]alpha support. even adding simple tokenizer exceptions, stop
   words or lemmatizer data can make a big difference. it will also make
   it easier for us to provide a statistical model for the language in the
   future. submitting a test that documents a bug or performance issue, or
   covers functionality that   s especially important for your application
   is also very helpful. this way, you   ll also make sure we never
   accidentally introduce regressions to the parts of the library that you
   care about the most.

   for more details on the types of contributions we   re looking for, the
   code conventions and other useful tips, make sure to check out the
   [211]contributing guidelines.

code of conduct

   spacy adheres to the [212]contributor covenant code of conduct. by
   participating, you are expected to uphold this code.

[213]i   ve built something cool with spacy     how can i get the word out?

   first, congrats     we   d love to check it out! when you share your
   project on twitter, don   t forget to tag [214]@spacy_io so we don   t miss
   it. if you think your project would be a good fit for the [215]spacy
   universe, feel free to submit it! tutorials are also incredibly
   valuable to other users and a great way to get exposure. so we strongly
   encourage writing up your experiences, or sharing your code and some
   tips and tricks on your blog. since our website is open-source, you can
   add your project or tutorial by making a pull request on github.

   if you would like to use the spacy logo on your site, please get in
   touch and ask us first. however, if you want to show support and tell
   others that your project is using spacy, you can grab one of our spacy
   badges here:
   [built%20with-spacy-09a3d5.svg]
[![built with spacy](https://img.shields.io/badge/built%20with-spacy-09a3d5.svg)
](https://spacy.io)

   [made%20with%20   %20and-spacy-09a3d5.svg]
[![built with spacy](https://img.shields.io/badge/made%20with%20   %20and-spacy-09
a3d5.svg)](https://spacy.io)

   [216]suggest edits
     * spacy
     * [217]usage
     * [218]models
     * [219]api
     * [220]universe

     * support
     * [221]issue tracker
     * [222]stack overflow
     * [223]reddit user group
     * [224]gitter chat

     * connect
     * [225]twitter
     * [226]github
     * [227]blog

     * stay in the loop!
     * receive updates about new releases, tutorials and more.
     * ____________________
       ____________________ (button) sign up

      2016-2019 [228]explosion ai[229]legal / imprint

references

   visible links
   1. https://spacy.io/
   2. https://spacy.io/usage
   3. https://spacy.io/models
   4. https://spacy.io/api
   5. https://spacy.io/universe
   6. https://spacy.io/usage
   7. https://spacy.io/usage/models
   8. https://spacy.io/usage/facts-figures
   9. https://spacy.io/usage/spacy-101
  10. https://spacy.io/usage/spacy-101/#whats-spacy
  11. https://spacy.io/usage/spacy-101/#features
  12. https://spacy.io/usage/spacy-101/#annotations
  13. https://spacy.io/usage/spacy-101/#pipelines
  14. https://spacy.io/usage/spacy-101/#vocab
  15. https://spacy.io/usage/spacy-101/#serialization
  16. https://spacy.io/usage/spacy-101/#training
  17. https://spacy.io/usage/spacy-101/#language-data
  18. https://spacy.io/usage/spacy-101/#lightning-tour
  19. https://spacy.io/usage/spacy-101/#architecture
  20. https://spacy.io/usage/spacy-101/#community-faq
  21. https://spacy.io/usage/v2-1
  22. https://spacy.io/usage/v2
  23. https://spacy.io/usage/linguistic-features
  24. https://spacy.io/usage/rule-based-matching
  25. https://spacy.io/usage/processing-pipelines
  26. https://spacy.io/usage/vectors-similarity
  27. https://spacy.io/usage/training
  28. https://spacy.io/usage/saving-loading
  29. https://spacy.io/usage/adding-languages
  30. https://spacy.io/usage/visualizers
  31. https://spacy.io/usage/examples
  32. https://spacy.io/usage/spacy-101/#_title
  33. https://github.com/explosion/spacy/issues
  34. https://github.com/explosion/spacy/pulls
  35. https://spacy.io/usage/spacy-101/#whats-spacy
  36. https://spacy.io/usage/spacy-101/#features
  37. https://spacy.io/usage/spacy-101/#annotations
  38. https://spacy.io/usage/spacy-101/#annotations-token
  39. https://spacy.io/usage/spacy-101/#annotations-pos-deps
  40. https://spacy.io/usage/spacy-101/#annotations-ner
  41. https://spacy.io/usage/spacy-101/#vectors-similarity
  42. https://spacy.io/usage/spacy-101/#pipelines
  43. https://spacy.io/usage/spacy-101/#vocab
  44. https://spacy.io/usage/spacy-101/#serialization
  45. https://spacy.io/usage/spacy-101/#training
  46. https://spacy.io/usage/spacy-101/#language-data
  47. https://spacy.io/usage/spacy-101/#lightning-tour
  48. https://spacy.io/usage/spacy-101/#architecture
  49. https://spacy.io/usage/spacy-101/#community
  50. https://spacy.io/usage/spacy-101/#what-spacy-isnt
  51. https://github.com/nltk/nltk
  52. https://stanfordnlp.github.io/corenlp/
  53. https://explosion.ai/
  54. http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06
  55. https://www.datacamp.com/courses/advanced-nlp-with-spacy
  56. http://datacamp-community-prod.s3.amazonaws.com/29aa28bf-570a-4965-8f54-d6a541ae4e06
  57. https://spacy.io/usage/spacy-101/#features
  58. https://spacy.io/usage/spacy-101/#statistical-models
  59. https://spacy.io/models
  60. https://spacy.io/usage/spacy-101/#annotations
  61. https://spacy.io/usage/models
  62. https://spacy.io/api/top-level#spacy.load
  63. https://spacy.io/usage/spacy-101/#annotations-token
  64. https://spacy.io/id121-57e618bd79d933c4ccd308b5739062d6.svg
  65. https://spacy.io/usage/models#languages
  66. https://spacy.io/usage/adding-languages
  67. https://spacy.io/usage/linguistic-features#id121
  68. https://spacy.io/usage/spacy-101/#annotations-pos-deps
  69. https://spacy.io/api/token#attributes
  70. https://spacy.io/usage/visualizers
  71. 
  72. https://spacy.io/usage/linguistic-features#pos-tagging
  73. https://spacy.io/usage/linguistic-features#dependency-parse
  74. https://spacy.io/usage/spacy-101/#annotations-ner
  75. https://spacy.io/api/annotation#named-entities
  76. https://spacy.io/usage/visualizers
  77. 
  78. https://spacy.io/usage/linguistic-features#named-entities
  79. https://spacy.io/usage/training#ner
  80. https://spacy.io/usage/spacy-101/#vectors-similarity
  81. https://en.wikipedia.org/wiki/id97
  82. https://spacy.io/models
  83. https://spacy.io/api/token#vector
  84. https://spacy.io/api/doc#vector
  85. https://spacy.io/api/span#vector
  86. https://spacy.io/models/en#en_vectors_web_lg
  87. https://spacy.io/api/token#similarity
  88. https://spacy.io/usage/vectors-similarity
  89. https://spacy.io/usage/spacy-101/#pipelines
  90. https://spacy.io/models
  91. https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg
  92. https://spacy.io/api/tokenizer
  93. https://spacy.io/api/tagger
  94. https://spacy.io/api/dependencyparser
  95. https://spacy.io/api/entityrecognizer
  96. https://spacy.io/api/textcategorizer
  97. https://spacy.io/usage/processing-pipelines#custom-components
  98. https://spacy.io/usage/spacy-101/#pipeline-components-order
  99. https://spacy.io/api/entityruler
 100. https://spacy.io/usage/processing-pipelines
 101. https://spacy.io/usage/spacy-101/#vocab
 102. https://spacy.io/api/vocab
 103. https://spacy.io/vocab_stringstore-1d1c9ccd7a1cf4d168bfe4ca791e6eed.svg
 104. https://spacy.io/api/stringstore
 105. https://spacy.io/api/lexeme
 106. https://en.wikipedia.org/wiki/hash_function
 107. https://spacy.io/usage/spacy-101/#serialization
 108. http://www.diveintopython3.net/serializing.html#dump
 109. https://spark.apache.org/docs/0.9.0/python-programming-guide.html
 110. http://dask.pydata.org/en/latest/
 111. https://spacy.io/api/language
 112. https://spacy.io/api/doc
 113. https://spacy.io/api/vocab
 114. https://spacy.io/api/stringstore
 115. https://spacy.io/usage/saving-loading#models
 116. https://spacy.io/usage/spacy-101/#training
 117. https://spacy.io/training-73950e71e6b59678754a87d6cf1481f9.svg
 118. https://spacy.io/usage/training
 119. https://spacy.io/usage/spacy-101/#language-data
 120. https://github.com/explosion/spacy/tree/master/spacy/lang
 121. https://spacy.io/language_data-ef63e6a58b7ec47c073fb59857a76e5f.svg
 122. https://github.com/explosion/spacy/tree/master/spacy/lang/en/stop_words.py
 123. https://github.com/explosion/spacy/tree/master/spacy/lang/de/tokenizer_exceptions.py
 124. https://github.com/explosion/spacy/tree/master/spacy/lang/norm_exceptions.py
 125. https://github.com/explosion/spacy/tree/master/spacy/lang/punctuation.py
 126. https://github.com/explosion/spacy/tree/master/spacy/lang/char_classes.py
 127. https://github.com/explosion/spacy/tree/master/spacy/lang/en/lex_attrs.py
 128. https://github.com/explosion/spacy/tree/master/spacy/lang/en/syntax_iterators.py
 129. https://spacy.io/usage/linguistic-features#noun-chunks
 130. https://github.com/explosion/spacy/tree/master/spacy/lang/de/lemmatizer.py
 131. https://github.com/explosion/spacy/tree/master/spacy/lang/en/tag_map.py
 132. http://universaldependencies.org/u/pos/all.html
 133. https://github.com/explosion/spacy/tree/master/spacy/lang/en/morph_rules.py
 134. https://spacy.io/usage/adding-languages
 135. https://spacy.io/usage/spacy-101/#lightning-tour
 136. https://spacy.io/usage/spacy-101/#lightning-tour-models
 137. https://spacy.io/api/top-level#spacy.load
 138. https://spacy.io/usage/models
 139. https://spacy.io/usage/spacy-101
 140. https://spacy.io/usage/spacy-101/#lightning-tour-tokens-sentences
 141. https://spacy.io/api/doc
 142. https://spacy.io/api/token
 143. https://spacy.io/usage/spacy-101
 144. https://spacy.io/usage/spacy-101/#lightning-tour-pos-tags
 145. https://spacy.io/api/token
 146. https://spacy.io/usage/linguistic-features#pos-tagging
 147. https://spacy.io/usage/spacy-101/#lightning-tour-hashes
 148. https://spacy.io/api/stringstore
 149. https://spacy.io/usage/spacy-101#vocab
 150. https://spacy.io/usage/spacy-101/#lightning-tour-entities
 151. https://spacy.io/usage/linguistic-features#named-entities
 152. https://spacy.io/usage/spacy-101/#lightning-tour-training"
 153. https://spacy.io/api/language#update
 154. https://spacy.io/usage/training
 155. https://spacy.io/usage/spacy-101/#lightning-tour-displacy
 156. https://spacy.io/displacy-small-76bdf4d2a9e027d99071a4490530af68.svg
 157. https://spacy.io/api/top-level#displacy
 158. https://spacy.io/usage/visualizers
 159. https://spacy.io/usage/spacy-101/#lightning-tour-word-vectors
 160. https://spacy.io/models/en#en_vectors_web_lg
 161. https://spacy.io/usage/vectors-similarity
 162. https://spacy.io/usage/spacy-101/#lightning-tour-serialization
 163. https://spacy.io/api/language
 164. https://spacy.io/api/doc
 165. https://spacy.io/usage/saving-loading#models
 166. https://spacy.io/usage/spacy-101/#lightning-tour-rule-matcher
 167. https://spacy.io/api/matcher
 168. https://spacy.io/usage/rule-based-matching
 169. https://spacy.io/usage/spacy-101/#lightning-tour-minibatched
 170. https://spacy.io/usage/spacy-101/#lightning-tour-dependencies
 171. https://spacy.io/api/token
 172. https://spacy.io/usage/linguistic-features#dependency-parse
 173. https://spacy.io/usage/spacy-101/#lightning-tour-numpy-arrays
 174. https://spacy.io/usage/spacy-101/#lightning-tour-inline
 175. https://spacy.io/usage/spacy-101/#architecture
 176. https://spacy.io/architecture-bcdfffe5c0b9f221a2f6607f96ca0e4a.svg
 177. https://spacy.io/usage/spacy-101/#architecture-containers
 178. https://spacy.io/api/doc
 179. https://spacy.io/api/span
 180. https://spacy.io/api/token
 181. https://spacy.io/api/lexeme
 182. https://spacy.io/usage/spacy-101/#architecture-pipeline
 183. https://spacy.io/api/language
 184. https://spacy.io/api/tokenizer
 185. https://spacy.io/api/lemmatizer
 186. https://spacy.io/api/tagger
 187. https://spacy.io/api/dependencyparser
 188. https://spacy.io/api/entityrecognizer
 189. https://spacy.io/api/textcategorizer
 190. https://spacy.io/api/matcher
 191. https://spacy.io/api/phrasematcher
 192. https://spacy.io/api/entityruler
 193. https://spacy.io/api/sentencizer
 194. https://spacy.io/api/pipeline-functions
 195. https://spacy.io/usage/spacy-101/#architecture-other
 196. https://spacy.io/api/vocab
 197. https://spacy.io/api/stringstore
 198. https://spacy.io/api/vectors
 199. https://spacy.io/api/goldparse
 200. https://spacy.io/api/goldcorpus
 201. https://spacy.io/usage/spacy-101/#community-faq
 202. https://spacy.io/usage/spacy-101/#faq-help-code
 203. https://spacy.io/usage/#troubleshooting
 204. https://stackoverflow.com/questions/tagged/spacy
 205. https://gitter.im/explosion/spacy
 206. https://github.com/explosion/spacy/issues
 207. https://spacy.io/usage/spacy-101/#faq-contributing
 208. https://github.com/explosion/spacy/issues?q=is:issue+is:open+label:"help+wanted+(easy)"
 209. https://spacy.io/usage/adding-languages#language-data
 210. https://spacy.io/usage/models#languages
 211. https://github.com/explosion/spacy/tree/master/contributing.md
 212. http://contributor-covenant.org/version/1/4/
 213. https://spacy.io/usage/spacy-101/#faq-project-with-spacy
 214. https://twitter.com/spacy_io
 215. https://spacy.io/universe
 216. https://github.com/explosion/spacy/tree/master/website/docs/usage/spacy-101.md
 217. https://spacy.io/usage
 218. https://spacy.io/models
 219. https://spacy.io/api
 220. https://spacy.io/universe
 221. https://github.com/explosion/spacy/issues
 222. http://stackoverflow.com/questions/tagged/spacy
 223. https://www.reddit.com/r/spacynlp/
 224. https://gitter.im/explosion/spacy
 225. https://twitter.com/spacy_io
 226. https://github.com/explosion/spacy
 227. https://explosion.ai/blog
 228. https://explosion.ai/
 229. https://explosion.ai/legal

   hidden links:
 231. https://github.com/explosion/spacy
 232. https://explosion.ai/
