
   [1]

the stanford natural language processing group

the stanford nlp group

     * [2]people
     * [3]publications
     * [4]research blog
     * [5]software
     * [6]teaching
     * [7]local

[8]research blog

wikitablequestions: a complex real-world question understanding dataset

   [9]ice pasupat       02/11/2016
     __________________________________________________________________

   task: learn to produce an answer y to a given question x according to a
   given table t

   natural language question understanding has been one of the most
   important challenges in artificial intelligence. indeed, eminent ai
   benchmarks such as the turing test require an ai system to understand
   natural language questions, with various topics and complexity, and
   then respond appropriately. during the past few years, we have
   witnessed rapid progress in id53 technology, with virtual
   assistants like siri, google now, and cortana answering daily life
   questions, and ibm watson winning over humans in jeopardy!. however,
   even the best id53 systems today still face two main
   challenges that have to solved simultaneously:
     * question complexity (depth). many questions the systems encounter
       are simple lookup questions (e.g., "where is chichen itza?" or
       "who's the manager of man utd?"). the answers can be found by
       searching the [10]surface [11]forms. but occasionally users will
       want to ask questions that require multiple, non-trivial steps to
       answer (e.g., "what the cheapest bus to chichen itza leaving
       tomorrow?" or "how many times did manchester united reach the final
       round of premier league while ferguson was the manager?"). these
       questions require deeper understanding and cannot be answered just
       by retrieval.
     * domain size (breadth). many systems are trained or engineered to
       work very well in a few specific domains such as managing calendar
       schedules or finding restaurants. developing a system to handle
       questions in any topic from local weather to global military
       conflicts, however, is much more difficult.

   while most systems understand questions containing either depth or
   breadth alone (e.g., by handling complex questions in a few domains and
   fall back to web search on the rest), they often struggle on ones that
   require both. to this end, we have decided to create a new dataset,
   wikitablequestions, that address both challenges at the same time.

task and dataset

   in the wikitablequestions dataset, each question comes with a table
   from wikipedia. given the question and the table, the task is to answer
   the question based on the table. the dataset contains 2108 tables from
   a large variety of topics (more breadth) and 22033 questions with
   different complexity (more depth). tables in the test set do not appear
   in the training set, so a system must be able to generalize to unseen
   tables.

   the dataset can be accessed from the [12]project page or on
   [13]codalab. the training set can also be browsed [14]online.

   we now give some examples that demonstrate the challenges of the
   dataset. consider the following [15]table:

   table from https://en.wikipedia.org/wiki/piotr_k%c4%99dzia

   the question is "in what city did piotr's last 1st place finish occur?"
   in order to answer the question, one might perform the following steps:

   steps for finding the answer

   with this example, we can observe several challenges:
     * schema mapping. one fundamental challenge when working with messy
       real-world data is handling diverse and possibly unseen data
       schemas. in this case, the system must know that the word "place"
       refers to the "position" column while the word "city" refers to the
       "venue" column, even if the same table schema has not been observed
       before during training.
     * compositionality. natural language can express complex ideas thanks
       to the principle of compositionality: the ability to compose
       smaller phrases into bigger ones. small phrases could correspond to
       different operations (e.g., locating the last item), which can be
       composed to get the final answer.
     * variety of operations. to fully utilize a rich data source, it is
       essential to be able to perform different operations such as
       filtering data ("1st place", "in 1990"), pinpointing data ("the
       longest", "the first"), computing statistics ("total", "average",
       "how many"), and comparing quantities ("difference between", "at
       least 10"). the wikitablequestions dataset contains questions with
       a large variety of operations, some of which can be observed in
       other questions for the [16]table above:

     * what was piotr's total number of 3rd place finishes?
     * which competition did this competitor compete in next after the
       world indoor championships in 2008?
     * how long did it take piotr to run the medley relay in 2001?
     * which 4x400 was faster, 2005 or 2003?
     * how many times has this competitor placed 5th or better in
       competition?

     * common sense reasoning. finally, one of the most challenging aspect
       of natural language is that the meaning of some phrases must be
       inferred using the context and common sense. for instance, the word
       "better" in the last example (... placed 5th or better    ) means
       "position     5", but in "scored 5 or better" it means "score     5".

   here are some other examples (cherry-picked from the first 50 examples)
   that show the variety of operations and topics of our dataset:

     * [17]how many people stayed at least 3 years in office?
     * [18]which players played the same position as ardo kreek?
     * [19]in how many games did the winning team score more than 4
       points?
     * [20]what's the number of parishes founded in the 1800s?
     * [21]in 1996 the sc house of representatives had a republican
       majority. how many years had passed since the last time this
       happened?
     * [22]how many consecutive friendly competitions did chalupny score
       in?

comparison to existing qa datasets

   most qa datasets address only either breadth (domain size) or depth
   (question complexity). early id29 datasets such as
   [23]geoquery and [24]atis contain complex sentences (high depth) in a
   focused domain (low breadth). here are some examples from geoquery,
   which contains questions on a us geography database:

     * how many states border texas?
     * what states border texas and have a major river?
     * what is the total population of the states that border texas?
     * what states border states that border states that border states
       that border texas?

   more recently, facebook released the [25]babi dataset featuring 20
   types of automatically generated questions with different complexity on
   simulated worlds. here is an example:

     john picked up the apple.
     john went to the office.
     john went to the kitchen.
     john dropped the apple.
     question: where was the apple before the kitchen?

   in contrast, many qa datasets contain questions spanning a variety of
   topics (high breadth), but the questions are much simpler or
   retrieval-based (low depth). for example, [26]webquestions dataset
   contains factoid questions that can be answered using a structured
   knowledge base. here are some examples:

     * what is the name of justin bieber brother?
     * what character did natalie portman play in star wars?
     * where donald trump went to college?
     * what countries around the world speak french?

   other knowledge base qa datasets include [27]free917 (also on freebase)
   and [28]qald (on both knowledge bases and unstructured data).

   qa datasets that focus on information retrieval and answer selection
   (such as [29]trec, [30]wikiqa, [31]qanta quiz bowl, and many
   [32]jeopardy! questions) are also of this kind: while some questions in
   these datasets look complex, the answers can be mostly inferred by
   working with the surface form. here is an example from qanta quiz bowl
   dataset:

     with the assistence of his chief minister, the duc de sully, he
     lowered taxes on peasantry, promoted economic recovery, and
     instituted a tax on the paulette. victor at ivry and arquet, he was
     excluded from succession by the treaty of nemours, but won a great
     victory at coutras. his excommunication was lifted by clement viii,
     but that pope later claimed to be crucified when this monarch
     promulgated the edict of nantes. for 10 points, name this french
     king, the first bourbon who admitted that "paris is worth a mass"
     when he converted following the war of the three henrys.

   finally, there are several datasets that address both breadth and depth
   but in a different angle. for example, [33]qald hybrid qa requires the
   system to combine information from multiple data sources, and in
   [34]ai2 science exam questions and [35]todai robot university entrance
   questions, the system has to perform common sense reasoning and logical
   id136 on a large volume of knowledge to derive the answers.

state of the art on wikitablequestions dataset

   in our paper, we present a id29 system which learns to
   construct formal queries ("logical forms") that can be executed on the
   tables to get the answers.

   semantic parse and logical form of the utterance "in what city did
   piotr&squot;s last 1st place finish occur?"

   the system learns a statistical model that builds logical forms in a
   hierarchical fashion (more depth) using parts can be freely constructed
   from any table schema (more breadth). the system achieve a test
   accuracy of 37.1%, which is higher than the previous id29
   system and an information retrieval baseline.

   we encourage everyone to play with the dataset, develop systems to
   tackle the challenges, and advance the field of natural language
   understanding! for suggestions and comments on the dataset, please
   contact the author [36]ice pasupat.
     __________________________________________________________________

stanford nlp group

   gates computer science building
   353 serra mall
   stanford, ca 94305-9020
   [37]directions and parking

affiliated groups

     * [38]stanford ai lab
     * [39]stanford infolab
     * [40]csli

connect

     * [41]stack overflow
     * [42]github
     * [43]twitter

local links

   [44]nlp lunch    [45]nlp reading group
   [46]nlp seminar    [47]calendar
   [48]javanlp ([49]javadocs)    [50]machines
   [51]ai speakers    [52]q&a

references

   visible links
   1. https://nlp.stanford.edu/
   2. https://nlp.stanford.edu/people/
   3. https://nlp.stanford.edu/pubs/
   4. https://nlp.stanford.edu/blog/
   5. https://nlp.stanford.edu/software/
   6. https://nlp.stanford.edu/teaching/
   7. https://nlp.stanford.edu/new_local/
   8. https://nlp.stanford.edu/blog/
   9. http://cs.stanford.edu/~ppasupat/
  10. https://en.wikipedia.org/wiki/chichen_itza#location
  11. http://www.manutd.com/en/players-and-staff/managers/
  12. http://nlp.stanford.edu/software/sempre/wikitable/
  13. https://worksheets.codalab.org/bundles/0x38a4fe7a21bc4992a5d3725df48c0b28/
  14. http://nlp.stanford.edu/software/sempre/wikitable/viewer/
  15. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#204-622
  16. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#204-622
  17. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#203-705
  18. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#203-116
  19. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#204-475
  20. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#203-36
  21. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#203-95
  22. http://nlp.stanford.edu/software/sempre/wikitable/viewer/#204-920
  23. http://www.cs.utexas.edu/users/ml/nldata/geoquery.html
  24. https://catalog.ldc.upenn.edu/ldc94s19
  25. https://research.facebook.com/researchers/1543934539189348
  26. http://www-nlp.stanford.edu/software/sempre/
  27. http://knight.cis.temple.edu/~yates/open-sem-parsing/index.html
  28. http://qald.sebastianwalter.org/
  29. http://trec.nist.gov/data/qamain.html
  30. http://research.microsoft.com/apps/pubs/?id=252176
  31. https://cs.umd.edu/~miyyer/qblearn/
  32. http://qr.ae/rovwak
  33. http://qald.sebastianwalter.org/index.php?x=challenge&q=6
  34. http://allenai.org/aristo/
  35. http://21robot.org/
  36. http://cs.stanford.edu/~ppasupat/
  37. http://forum.stanford.edu/visitors/directions/gates.php
  38. http://ai.stanford.edu/
  39. http://infolab.stanford.edu/
  40. https://www-csli.stanford.edu/
  41. http://stackoverflow.com/tags/stanford-nlp
  42. https://github.com/stanfordnlp/corenlp
  43. https://twitter.com/stanfordnlp
  44. https://nlp.stanford.edu/local/nlp_lunch.shtml
  45. http://nlp.stanford.edu/read/
  46. http://nlp.stanford.edu/seminar/
  47. https://nlp.stanford.edu/local/calendar.shtml
  48. https://nlp.stanford.edu/javanlp/
  49. https://nlp.stanford.edu/nlp/javadoc/javanlp/
  50. https://nlp.stanford.edu/local/machines.shtml
  51. http://ai.stanford.edu/portfolio-view/distinguished-speaker-series
  52. https://nlp.stanford.edu/local/qa/

   hidden links:
  54. https://nlp.stanford.edu/blog/wikitablequestions-a-complex-real-world-question-understanding-dataset/
