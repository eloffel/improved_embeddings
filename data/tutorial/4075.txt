   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

introduction to id24

   [16]go to the profile of aneek das
   [17]aneek das (button) blockedunblock (button) followfollowing
   mar 13, 2017

   imagine yourself in a treasure hunt in a maze . the game is as
   follows :

   you start at a given position, the starting state . from any state you
   can go left, right, up or down or stay in the same place provided you
   don   t cross the premises of the maze. each action will take you to a
   cell of the grid (a different state). now, there is a treasure chest at
   one of the states (the goal state). also, the maze has a pit of snakes
   in certain positions/states. your goal is to travel from the starting
   state to the goal state by following a path that doesn   t have snakes in
   it.
   [1*xl9txwgczpqwasy26a6sig.png]
   grid outline of the maze

   when you place an agent in the grid(we will refer to it as our
   environment) it will first explore. it doesn   t know what snakes are ,
   neither does it know what or where the treasure is. so, to give it the
   idea of snakes and the treasure chest we will give some rewards to it
   after it takes each action. for every snake pit it steps onto we will
   give it a reward of -10. for the treasure we will give it a reward of
   +10. now we want our agent to complete the task as fast as possible (to
   take the shortest route). for this, we will give rest of the states a
   reward of -1. then we will tell it to maximise the score. now as the
   agent explores , it learns that snakes are harmful for it, the treasure
   is good for it and it has to get the treasure as fast as possible. the
      -    path in the figure shows the shortest path with maximum reward.

id24 attempts to learn the value of being in a given state, and taking
a specific action there.

   what we will do is develop a table. where the rows will be the states
   and the columns are the actions it can take. so, we have a 16x5 (80
   possible state-action) pairs where each state is one cell of the
   maze-grid.

   we start by initializing the table to be uniform (all zeros), and then
   as we observe the rewards we obtain for various actions, we update the
   table accordingly. we will update the table using the bellman
   equation .
   [1*2rjuubuk5arfagjau4jiwa.png]
   eqn.1

      s    represents the current state .    a    represents the action the agent
   takes from the current state.     s        represents the state resulting
   from the action.    r    is the reward you get for taking the action and
            is the discount factor. so, the q value for the state    s    taking
   the action    a    is the sum of the instant reward and the discounted
   future reward (value of the resulting state). the discount factor         
   determines how much importance you want to give to future rewards. say,
   you go to a state which is further away from the goal state, but from
   that state the chances of encountering a state with snakes is less, so,
   here the future reward is more even though the instant reward is less.

   we will refer to each iteration(attempt made by the agent) as an
   episode. for each episode, the agent will try to achieve the goal state
   and for every transition it will keep on updating the values of the q
   table.

   lets see how to calculate the q table :

   for this purpose we will take a smaller maze-grid for ease.
   [1*wb_f7jzxp5rierp8sat2mw.png]

   the initial q-table would look like ( states along the rows and actions
   along the columns ) :
   [1*90fidykqmduvckhs4pyuxg.png]
   q matrix

   u         up, d         down, l         left, r         right

   the reward table would look like :
   [1*kl-5aykojm0d5dyk6h1y9a.png]
   r matrix

   here, e represents null, i.e., there can be no such transitions.

   algorithm:
    1. initialise q-matrix by all zeros. set value for         . fill rewards
       matrix.
    2. for each episode. select a random starting state (here we will
       restrict our starting state to state-1).
    3. select one among all possible actions for the current state (s).
    4. travel to the next state (s   ) as a result of that action (a).
    5. for all possible actions from the state (s   ) select the one with
       the highest q value.
    6. update q-table using eqn.1 .
    7. set the next state as the current state.
    8. if goal state reached then end.

   example : lets say we start with state 1 . we can go either d or r.
   say, we chose d . then we will reach 3 (the snake pit). so, then we can
   go either u or r . so, taking    = 0.8, we have :

   q(1,d) = r(1,d) +   *[max(q(3,u) & q(3,r))]

   q(1,d) = -10 + 0.8*0 = -10

   here, max(q(3,u) & q(3,r)) = 0 as q matrix not yet updated. -10 is for
   stepping on the snakes. so, new q-table looks like :
   [1*7tcb3zftbm6xidad0nue2w.png]

   now, 3 is the starting state. from 3, lets say we go r. so, we go on
   4 . from 4, we can go u or l .

   q(3,r) = r(3,r) + 0.8*[max(q(4,u) & q(4,l))]

   q(3,r) = 10 + 0.8*0 = 10
   [1*akzyfu-mrl4hxliwrlcuiw.png]

   so, now we have reached the goal state 4. so, we terminate and more
   passes to let our agent understand the value of each state and action.
   keep making passes until the values remain constant. this means that
   your agent has tried out all possible state-action pairs.

   implementation in python :
   [1*c-jnu4ckjhpydcicitkuca.png]

   output for the last q_matrix :
   [1*c-7yzflzx8kllguwykukjg.png]

   in the next article i will cover the usage of neural networks for
   id24 and the short-comings of using the tabular approach. also,
   we will be working on games from open-ai gym for testing. until then,
   bye.

     * [18]machine learning
     * [19]artificial intelligence
     * [20]id23
     * [21]id24
     * [22]neural networks

   (button)
   (button)
   (button) 474 claps
   (button) (button) (button) 11 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of aneek das

[24]aneek das

     (button) follow
   [25]towards data science

[26]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 474
     * (button)
     *
     *

   [27]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [28]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/88d1c4f2b49c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-id24-88d1c4f2b49c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/introduction-to-id24-88d1c4f2b49c&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_xz5orultnb5u---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@aneekdas?source=post_header_lockup
  17. https://towardsdatascience.com/@aneekdas
  18. https://towardsdatascience.com/tagged/machine-learning?source=post
  19. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  20. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  21. https://towardsdatascience.com/tagged/id24?source=post
  22. https://towardsdatascience.com/tagged/neural-networks?source=post
  23. https://towardsdatascience.com/@aneekdas?source=footer_card
  24. https://towardsdatascience.com/@aneekdas
  25. https://towardsdatascience.com/?source=footer_card
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/
  28. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  30. https://medium.com/p/88d1c4f2b49c/share/twitter
  31. https://medium.com/p/88d1c4f2b49c/share/facebook
  32. https://medium.com/p/88d1c4f2b49c/share/twitter
  33. https://medium.com/p/88d1c4f2b49c/share/facebook
