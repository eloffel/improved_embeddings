   #[1]github [2]recent commits to lstm-human-activity-recognition:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]142
     * [35]star [36]2,090
     * [37]fork [38]606

[39]guillaume-chevalier/[40]lstm-human-activity-recognition

   [41]code [42]issues 6 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   human activity recognition example using tensorflow on smartphone
   sensors dataset and an lstm id56 (deep learning algo). classifying the
   type of movement amongst six activity categories - guillaume chevalier
   [47]https://www.neuraxio.com/
   [48]machine-learning [49]deep-learning [50]lstm
   [51]human-activity-recognition [52]neural-network [53]id56
   [54]recurrent-neural-networks [55]tensorflow [56]activity-recognition
     * [57]16 commits
     * [58]1 branch
     * [59]0 releases
     * [60]fetching contributors
     * [61]mit

    1. [62]jupyter notebook 99.6%
    2. [63]python 0.4%

   (button) jupyter notebook python
   branch: master (button) new pull request
   [64]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/g
   [65]download zip

downloading...

   want to be notified of new releases in
   guillaume-chevalier/lstm-human-activity-recognition?
   [66]sign in [67]sign up

launching github desktop...

   if nothing happens, [68]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [69]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [70]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [71]download the github extension for visual studio
   and try again.

   (button) go back
   [72]@guillaume-chevalier
   [73]guillaume-chevalier [74]edited reported best accuracy: it's better
   than i thought.
   latest commit [75]3a20702 apr 3, 2019
   [76]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [77]lstm_files [78]updated to python3 and tensorflow 1.0.0 may 24, 2017
   [79]data
   [80].gitignore
   [81]license [82]added mit license and details about accelerometer
   accuracy nov 28, 2016
   [83]lstm.ipynb
   [84]readme.md

readme.md

[85]lstms for human activity recognition

   human activity recognition (har) using smartphones dataset and an lstm
   id56. classifying the type of movement amongst six categories:
     * walking,
     * walking_upstairs,
     * walking_downstairs,
     * sitting,
     * standing,
     * laying.

   compared to a classical approach, using a recurrent neural networks
   (id56) with long short-term memory cells (lstms) require no or almost no
   feature engineering. data can be fed directly into the neural network
   who acts like a black box, modeling the problem correctly. [86]other
   research on the activity recognition dataset can use a big amount of
   feature engineering, which is rather a signal processing approach
   combined with classical data science techniques. the approach here is
   rather very simple in terms of how much was the data preprocessed.

   let's use google's neat deep learning library, tensorflow,
   demonstrating the usage of an lstm, a type of id158
   that can process sequential data / time series.

video dataset overview

   follow this link to see a video of the 6 activities recorded in the
   experiment with one of the participants:

                         [87]video of the experiment
   [88][watch video]

details about the input data

   i will be using an lstm on the data to learn (as a cellphone attached
   on the waist) to recognise the type of activity that the user is doing.
   the dataset's description goes like this:

     the sensor signals (accelerometer and gyroscope) were pre-processed
     by applying noise filters and then sampled in fixed-width sliding
     windows of 2.56 sec and 50% overlap (128 readings/window). the
     sensor acceleration signal, which has gravitational and body motion
     components, was separated using a butterworth low-pass filter into
     body acceleration and gravity. the gravitational force is assumed to
     have only low frequency components, therefore a filter with 0.3 hz
     cutoff frequency was used.

   that said, i will use the almost raw data: only the gravity effect has
   been filtered out of the accelerometer as a preprocessing step for
   another 3d feature as an input to help learning. if you'd ever want to
   extract the gravity by yourself, you could fork my code on using a
   [89]butterworth low-pass filter (lpf) in python and edit it to have the
   right cutoff frequency of 0.3 hz which is a good frequency for activity
   recognition from body sensors.

what is an id56?

   as explained in [90]this article, an id56 takes many input vectors to
   process them and output other vectors. it can be roughly pictured like
   in the image below, imagining each rectangle has a vectorial depth and
   other special hidden quirks in the image below. in our case, the "many
   to one" architecture is used: we accept time series of [91]feature
   vectors (one vector per [92]time step) to convert them to a id203
   vector at the output for classification. note that a "one to one"
   architecture would be a standard feedforward neural network.

     [93][687474703a2f2f6b617270617468792e6769746875622e696f2f61737365747
     32f726e6e2f64696167732e6a706567]
     [94]http://karpathy.github.io/2015/05/21/id56-effectiveness/

what is an lstm?

   an lstm is an improved id56. it is more complex, but easier to train,
   avoiding what is called the vanishing gradient problem. i recommend
   [95]this article for you to learn more on lstms.

results

   scroll on! nice visuals awaits.
# all includes

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import tensorflow as tf  # version 1.0.0 (some previous versions are used in pas
t commits)
from sklearn import metrics

import os

# useful constants

# those are separate normalised input features for the neural network
input_signal_types = [
    "body_acc_x_",
    "body_acc_y_",
    "body_acc_z_",
    "body_gyro_x_",
    "body_gyro_y_",
    "body_gyro_z_",
    "total_acc_x_",
    "total_acc_y_",
    "total_acc_z_"
]

# output classes to learn how to classify
labels = [
    "walking",
    "walking_upstairs",
    "walking_downstairs",
    "sitting",
    "standing",
    "laying"
]

let's start by downloading the data:

# note: linux bash commands start with a "!" inside those "ipython notebook" cel
ls

data_path = "data/"

!pwd && ls
os.chdir(data_path)
!pwd && ls

!python download_dataset.py

!pwd && ls
os.chdir("..")
!pwd && ls

dataset_path = data_path + "uci har dataset/"
print("\n" + "dataset is now located at: " + dataset_path)

/home/ubuntu/pynb/lstm-human-activity-recognition
data     lstm_files  lstm_old.ipynb  readme.md
license  lstm.ipynb  lstm.py         screenlog.0
/home/ubuntu/pynb/lstm-human-activity-recognition/data
download_dataset.py  source.txt

downloading...
--2017-05-24 01:49:53--  https://archive.ics.uci.edu/ml/machine-learning-databas
es/00240/uci%20har%20dataset.zip
resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249
connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... c
onnected.
http request sent, awaiting response... 200 ok
length: 60999314 (58m) [application/zip]
saving to:    uci har dataset.zip   

100%[======================================>] 60,999,314  1.69mb/s   in 38s

2017-05-24 01:50:31 (1.55 mb/s) -    uci har dataset.zip    saved [60999314/60999314
]

downloading done.

extracting...
extracting successfully done to /home/ubuntu/pynb/lstm-human-activity-recognitio
n/data/uci har dataset.
/home/ubuntu/pynb/lstm-human-activity-recognition/data
download_dataset.py  __macosx  source.txt  uci har dataset  uci har dataset.zip
/home/ubuntu/pynb/lstm-human-activity-recognition
data     lstm_files  lstm_old.ipynb  readme.md
license  lstm.ipynb  lstm.py         screenlog.0

dataset is now located at: data/uci har dataset/

preparing dataset:

train = "train/"
test = "test/"


# load "x" (the neural network's training and testing inputs)

def load_x(x_signals_paths):
    x_signals = []

    for signal_type_path in x_signals_paths:
        file = open(signal_type_path, 'r')
        # read dataset from disk, dealing with text files' syntax
        x_signals.append(
            [np.array(serie, dtype=np.float32) for serie in [
                row.replace('  ', ' ').strip().split(' ') for row in file
            ]]
        )
        file.close()

    return np.transpose(np.array(x_signals), (1, 2, 0))

x_train_signals_paths = [
    dataset_path + train + "inertial signals/" + signal + "train.txt" for signal
 in input_signal_types
]
x_test_signals_paths = [
    dataset_path + test + "inertial signals/" + signal + "test.txt" for signal i
n input_signal_types
]

x_train = load_x(x_train_signals_paths)
x_test = load_x(x_test_signals_paths)


# load "y" (the neural network's training and testing outputs)

def load_y(y_path):
    file = open(y_path, 'r')
    # read dataset from disk, dealing with text file's syntax
    y_ = np.array(
        [elem for elem in [
            row.replace('  ', ' ').strip().split(' ') for row in file
        ]],
        dtype=np.int32
    )
    file.close()

    # substract 1 to each output class for friendly 0-based indexing
    return y_ - 1

y_train_path = dataset_path + train + "y_train.txt"
y_test_path = dataset_path + test + "y_test.txt"

y_train = load_y(y_train_path)
y_test = load_y(y_test_path)

additionnal parameters:

   here are some core parameter definitions for the training.

   for example, the whole neural network's structure could be summarised
   by enumerating those parameters and the fact that two lstm are used one
   on top of another (stacked) output-to-input as hidden layers through
   time steps.
# input data

training_data_count = len(x_train)  # 7352 training series (with 50% overlap bet
ween each serie)
test_data_count = len(x_test)  # 2947 testing series
n_steps = len(x_train[0])  # 128 timesteps per series
n_input = len(x_train[0][0])  # 9 input parameters per timestep


# lstm neural network's internal structure

n_hidden = 32 # hidden layer num of features
n_classes = 6 # total classes (should go up, or should go down)


# training

learning_rate = 0.0025
lambda_loss_amount = 0.0015
training_iters = training_data_count * 300  # loop 300 times on the dataset
batch_size = 1500
display_iter = 30000  # to show test set accuracy during training


# some debugging info

print("some useful info to get an insight on dataset's shape and normalisation:"
)
print("(x shape, y shape, every x's mean, every x's standard deviation)")
print(x_test.shape, y_test.shape, np.mean(x_test), np.std(x_test))
print("the dataset is therefore properly normalised, as expected, but not yet on
e-hot encoded.")

some useful info to get an insight on dataset's shape and normalisation:
(x shape, y shape, every x's mean, every x's standard deviation)
(2947, 128, 9) (2947, 1) 0.0991399 0.395671
the dataset is therefore properly normalised, as expected, but not yet one-hot e
ncoded.

utility functions for training:

def lstm_id56(_x, _weights, _biases):
    # function returns a tensorflow lstm (id56) id158 from gi
ven parameters.
    # moreover, two lstm cells are stacked which adds deepness to the neural net
work.
    # note, some code of this notebook is inspired from an slightly different
    # id56 architecture used on another dataset, some of the credits goes to
    # "aymericdamien" under the mit license.

    # (note: this step could be greatly optimised by shaping the dataset once
    # input shape: (batch_size, n_steps, n_input)
    _x = tf.transpose(_x, [1, 0, 2])  # permute n_steps and batch_size
    # reshape to prepare input to hidden activation
    _x = tf.reshape(_x, [-1, n_input])
    # new shape: (n_steps*batch_size, n_input)

    # relu activation, thanks to yu zhao for adding this improvement here:
    _x = tf.nn.relu(tf.matmul(_x, _weights['hidden']) + _biases['hidden'])
    # split data because id56 cell needs a list of inputs for the id56 inner loop
    _x = tf.split(_x, n_steps, 0)
    # new shape: n_steps * (batch_size, n_hidden)

    # define two stacked lstm cells (two recurrent layers deep) with tensorflow
    lstm_cell_1 = tf.contrib.id56.basiclstmcell(n_hidden, forget_bias=1.0, state_
is_tuple=true)
    lstm_cell_2 = tf.contrib.id56.basiclstmcell(n_hidden, forget_bias=1.0, state_
is_tuple=true)
    lstm_cells = tf.contrib.id56.multiid56cell([lstm_cell_1, lstm_cell_2], state_i
s_tuple=true)
    # get lstm cell output
    outputs, states = tf.contrib.id56.static_id56(lstm_cells, _x, dtype=tf.float32
)

    # get last time step's output feature for a "many-to-one" style classifier,
    # as in the image describing id56s at the top of this page
    lstm_last_output = outputs[-1]

    # linear activation
    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']


def extract_batch_size(_train, step, batch_size):
    # function to fetch a "batch_size" amount of data from "(x|y)_train" data.

    shape = list(_train.shape)
    shape[0] = batch_size
    batch_s = np.empty(shape)

    for i in range(batch_size):
        # loop index
        index = ((step-1)*batch_size + i) % len(_train)
        batch_s[i] = _train[index]

    return batch_s


def one_hot(y_, n_classes=n_classes):
    # function to encode neural one-hot output labels from number indexes
    # e.g.:
    # one_hot(y_=[[5], [0], [3]], n_classes=6):
    #     return [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]

    y_ = y_.reshape(len(y_))
    return np.eye(n_classes)[np.array(y_, dtype=np.int32)]  # returns floats

let's get serious and build the neural network:

# graph input/output
x = tf.placeholder(tf.float32, [none, n_steps, n_input])
y = tf.placeholder(tf.float32, [none, n_classes])

# graph weights
weights = {
    'hidden': tf.variable(tf.random_normal([n_input, n_hidden])), # hidden layer
 weights
    'out': tf.variable(tf.random_normal([n_hidden, n_classes], mean=1.0))
}
biases = {
    'hidden': tf.variable(tf.random_normal([n_hidden])),
    'out': tf.variable(tf.random_normal([n_classes]))
}

pred = lstm_id56(x, weights, biases)

# loss, optimizer and evaluation
l2 = lambda_loss_amount * sum(
    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()
) # l2 loss prevents this overkill neural network to overfit the data
cost = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(labels=y, logits=p
red)) + l2 # softmax loss
optimizer = tf.train.adamoptimizer(learning_rate=learning_rate).minimize(cost) #
 adam optimizer

correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

hooray, now train the neural network:

# to keep track of training's performance
test_losses = []
test_accuracies = []
train_losses = []
train_accuracies = []

# launch the graph
sess = tf.interactivesession(config=tf.configproto(log_device_placement=true))
init = tf.global_variables_initializer()
sess.run(init)

# perform training steps with "batch_size" amount of example data at each loop
step = 1
while step * batch_size <= training_iters:
    batch_xs =         extract_batch_size(x_train, step, batch_size)
    batch_ys = one_hot(extract_batch_size(y_train, step, batch_size))

    # fit training using batch data
    _, loss, acc = sess.run(
        [optimizer, cost, accuracy],
        feed_dict={
            x: batch_xs,
            y: batch_ys
        }
    )
    train_losses.append(loss)
    train_accuracies.append(acc)

    # evaluate network only at some steps for faster training:
    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_siz
e > training_iters):

        # to not spam console, show training accuracy/loss in this "if"
        print("training iter #" + str(step*batch_size) + \
              ":   batch loss = " + "{:.6f}".format(loss) + \
              ", accuracy = {}".format(acc))

        # evaluation on the test set (no learning made here - just evaluation fo
r diagnosis)
        loss, acc = sess.run(
            [cost, accuracy],
            feed_dict={
                x: x_test,
                y: one_hot(y_test)
            }
        )
        test_losses.append(loss)
        test_accuracies.append(acc)
        print("performance on test set: " + \
              "batch loss = {}".format(loss) + \
              ", accuracy = {}".format(acc))

    step += 1

print("optimization finished!")

# accuracy for test data

one_hot_predictions, accuracy, final_loss = sess.run(
    [pred, accuracy, cost],
    feed_dict={
        x: x_test,
        y: one_hot(y_test)
    }
)

test_losses.append(final_loss)
test_accuracies.append(accuracy)

print("final result: " + \
      "batch loss = {}".format(final_loss) + \
      ", accuracy = {}".format(accuracy))

warning:tensorflow:from <ipython-input-19-3339689e51f6>:9: initialize_all_variab
les (from tensorflow.python.ops.variables) is deprecated and will be removed aft
er 2017-03-02.
instructions for updating:
use `tf.global_variables_initializer` instead.
training iter #1500:   batch loss = 5.416760, accuracy = 0.15266665816307068
performance on test set: batch loss = 4.880829811096191, accuracy = 0.0563284717
5002098
training iter #30000:   batch loss = 3.031930, accuracy = 0.607333242893219
performance on test set: batch loss = 3.0515167713165283, accuracy = 0.606718659
4009399
training iter #60000:   batch loss = 2.672764, accuracy = 0.7386666536331177
performance on test set: batch loss = 2.780435085296631, accuracy = 0.7027485370
635986
training iter #90000:   batch loss = 2.378301, accuracy = 0.8366667032241821
performance on test set: batch loss = 2.6019773483276367, accuracy = 0.761791586
8759155
training iter #120000:   batch loss = 2.127290, accuracy = 0.9066667556762695
performance on test set: batch loss = 2.3625404834747314, accuracy = 0.811672866
3444519
training iter #150000:   batch loss = 1.929805, accuracy = 0.9380000233650208
performance on test set: batch loss = 2.306251049041748, accuracy = 0.8276212215
423584
training iter #180000:   batch loss = 1.971904, accuracy = 0.9153333902359009
performance on test set: batch loss = 2.0835530757904053, accuracy = 0.877163112
1635437
training iter #210000:   batch loss = 1.860249, accuracy = 0.8613333702087402
performance on test set: batch loss = 1.9994492530822754, accuracy = 0.878859758
3770752
training iter #240000:   batch loss = 1.626292, accuracy = 0.9380000233650208
performance on test set: batch loss = 1.879166603088379, accuracy = 0.8944689035
415649
training iter #270000:   batch loss = 1.582758, accuracy = 0.9386667013168335
performance on test set: batch loss = 2.0341007709503174, accuracy = 0.836104393
0053711
training iter #300000:   batch loss = 1.620352, accuracy = 0.9306666851043701
performance on test set: batch loss = 1.8185184001922607, accuracy = 0.863929331
3026428
training iter #330000:   batch loss = 1.474394, accuracy = 0.9693333506584167
performance on test set: batch loss = 1.7638503313064575, accuracy = 0.874787867
0692444
training iter #360000:   batch loss = 1.406998, accuracy = 0.9420000314712524
performance on test set: batch loss = 1.5946787595748901, accuracy = 0.902273416
519165
training iter #390000:   batch loss = 1.362515, accuracy = 0.940000057220459
performance on test set: batch loss = 1.5285792350769043, accuracy = 0.904648721
2181091
training iter #420000:   batch loss = 1.252860, accuracy = 0.9566667079925537
performance on test set: batch loss = 1.4635565280914307, accuracy = 0.910756587
9821777
training iter #450000:   batch loss = 1.190078, accuracy = 0.9553333520889282
...
performance on test set: batch loss = 0.42567864060401917, accuracy = 0.93247365
95153809
training iter #2070000:   batch loss = 0.342763, accuracy = 0.9326667189598083
performance on test set: batch loss = 0.4292983412742615, accuracy = 0.927383661
2701416
training iter #2100000:   batch loss = 0.259442, accuracy = 0.9873334169387817
performance on test set: batch loss = 0.44131210446357727, accuracy = 0.92738366
12701416
training iter #2130000:   batch loss = 0.284630, accuracy = 0.9593333601951599
performance on test set: batch loss = 0.46982717514038086, accuracy = 0.90939927
10113525
training iter #2160000:   batch loss = 0.299012, accuracy = 0.9686667323112488
performance on test set: batch loss = 0.48389002680778503, accuracy = 0.91381055
11665344
training iter #2190000:   batch loss = 0.287106, accuracy = 0.9700000286102295
performance on test set: batch loss = 0.4670214056968689, accuracy = 0.921615123
7487793
optimization finished!
final result: batch loss = 0.45611169934272766, accuracy = 0.9165252447128296

training is good, but having visual insight is even better:

   okay, let's plot this simply in the notebook for now.
# (inline plots: )
%matplotlib inline

font = {
    'family' : 'bitstream vera sans',
    'weight' : 'bold',
    'size'   : 18
}
matplotlib.rc('font', **font)

width = 12
height = 12
plt.figure(figsize=(width, height))

indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size,
batch_size))
plt.plot(indep_train_axis, np.array(train_losses),     "b--", label="train losse
s")
plt.plot(indep_train_axis, np.array(train_accuracies), "g--", label="train accur
acies")

indep_test_axis = np.append(
    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]
),
    [training_iters]
)
plt.plot(indep_test_axis, np.array(test_losses),     "b-", label="test losses")
plt.plot(indep_test_axis, np.array(test_accuracies), "g-", label="test accuracie
s")

plt.title("training session's progress over iterations")
plt.legend(loc='upper right', shadow=true)
plt.ylabel('training progress (loss or accuracy values)')
plt.xlabel('training iteration')

plt.show()

   [96]lstm training testing comparison curve

and finally, the multi-class confusion matrix and metrics!

# results

predictions = one_hot_predictions.argmax(1)

print("testing accuracy: {}%".format(100*accuracy))

print("")
print("precision: {}%".format(100*metrics.precision_score(y_test, predictions, a
verage="weighted")))
print("recall: {}%".format(100*metrics.recall_score(y_test, predictions, average
="weighted")))
print("f1_score: {}%".format(100*metrics.f1_score(y_test, predictions, average="
weighted")))

print("")
print("confusion matrix:")
confusion_matrix = metrics.confusion_matrix(y_test, predictions)
print(confusion_matrix)
normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.su
m(confusion_matrix)*100

print("")
print("confusion matrix (normalised to % of total test data):")
print(normalised_confusion_matrix)
print("note: training and testing data is not equally distributed amongst classe
s, ")
print("so it is normal that more than a 6th of the data is correctly classifier
in the last category.")

# plot results:
width = 12
height = 12
plt.figure(figsize=(width, height))
plt.imshow(
    normalised_confusion_matrix,
    interpolation='nearest',
    cmap=plt.cm.rainbow
)
plt.title("confusion matrix \n(normalised to % of total test data)")
plt.colorbar()
tick_marks = np.arange(n_classes)
plt.xticks(tick_marks, labels, rotation=90)
plt.yticks(tick_marks, labels)
plt.tight_layout()
plt.ylabel('true label')
plt.xlabel('predicted label')
plt.show()

testing accuracy: 91.65252447128296%

precision: 91.76286479743305%
recall: 91.65252799457076%
f1_score: 91.6437546304815%

confusion matrix:
[[466   2  26   0   2   0]
 [  5 441  25   0   0   0]
 [  1   0 419   0   0   0]
 [  1   1   0 396  87   6]
 [  2   1   0  87 442   0]
 [  0   0   0   0   0 537]]

confusion matrix (normalised to % of total test data):
[[ 15.81269073   0.06786563   0.88225317   0.           0.06786563   0.        ]
 [  0.16966406  14.96437073   0.84832031   0.           0.           0.        ]
 [  0.03393281   0.          14.21784878   0.           0.           0.        ]
 [  0.03393281   0.03393281   0.          13.43739319   2.95215464
    0.20359688]
 [  0.06786563   0.03393281   0.           2.95215464  14.99830341   0.        ]
 [  0.           0.           0.           0.           0.          18.22192001]
]
note: training and testing data is not equally distributed amongst classes,
so it is normal that more than a 6th of the data is correctly classifier in the
last category.

   [97]confusion matrix
sess.close()

conclusion

   outstandingly, the final accuracy is of 91%! and it can peak to values
   such as 93.25%, at some moments of luck during the training, depending
   on how the neural network's weights got initialized at the start of the
   training, randomly.

   this means that the neural networks is almost always able to correctly
   identify the movement type! remember, the phone is attached on the
   waist and each series to classify has just a 128 sample window of two
   internal sensors (a.k.a. 2.56 seconds at 50 fps), so it amazes me how
   those predictions are extremely accurate given this small window of
   context and raw data. i've validated and re-validated that there is no
   important bug, and the community used and tried this code a lot. (note:
   be sure to report something in the issue tab if you find bugs,
   otherwise [98]quora, [99]stackoverflow, and other [100]stackexchange
   sites are the places for asking questions.)

   i specially did not expect such good results for guessing between the
   labels "sitting" and "standing". those are seemingly almost the same
   thing from the point of view of a device placed at waist level
   according to how the dataset was originally gathered. thought, it is
   still possible to see a little cluster on the matrix between those
   classes, which drifts away just a bit from the identity. this is great.

   it is also possible to see that there was a slight difficulty in doing
   the difference between "walking", "walking_upstairs" and
   "walking_downstairs". obviously, those activities are quite similar in
   terms of movements.

   i also tried my code without the gyroscope, using only the 3d
   accelerometer's 6 features (and not changing the training
   hyperparameters), and got an accuracy of 87%. in general, gyroscopes
   consumes more power than accelerometers, so it is preferable to turn
   them off.

improvements

   in [101]another open-source repository of mine, the accuracy is pushed
   up to nearly 94% using a special deep lstm architecture which combines
   the concepts of bidirectional id56s, residual connections, and stacked
   cells. this architecture is also tested on another similar activity
   dataset. it resembles the nice architecture used in "[102]google   s
   id4 system: bridging the gap between human and
   machine translation", without an attention mechanism, and with just the
   encoder part - as a "many to one" architecture instead of a "many to
   many" to be adapted to the human activity recognition (har) problem. i
   also worked more on the problem and came up with the [103]laid56,
   however it's complicated for just a little gain. thus the current,
   original activity recognition project is simply better to use for its
   outstanding simplicity.

   if you want to learn more about deep learning, i have also built a list
   of the learning ressources for deep learning which have revealed to be
   the most useful to me [104]here.

references

   the [105]dataset can be found on the uci machine learning repository:

     davide anguita, alessandro ghio, luca oneto, xavier parra and jorge
     l. reyes-ortiz. a public domain dataset for human activity
     recognition using smartphones. 21th european symposium on artificial
     neural networks, computational intelligence and machine learning,
     esann 2013. bruges, belgium 24-26 april 2013.

   the id56 image for "many-to-one" is taken from karpathy's post:

     andrej karpathy, the unreasonable effectiveness of recurrent neural
     networks, 2015,
     [106]http://karpathy.github.io/2015/05/21/id56-effectiveness/

citation

   copyright (c) 2016 guillaume chevalier. to cite my code, you can point
   to the url of the github repository, for example:

     guillaume chevalier, lstms for human activity recognition, 2016,
     [107]https://github.com/guillaume-chevalier/lstm-human-activity-reco
     gnition

   my code is available for free and even for private usage for anyone
   under the [108]mit license, however i ask to cite for using the code.

extra links

connect with me

     * [109]linkedin
     * [110]twitter
     * [111]github
     * [112]quora
     * [113]youtube
     * [114]dev/consulting

liked this project? did it help you? leave a [115]star, [116]fork and share
the love!

   this activity recognition project has been seen in:
     * [117]hacker news 1st page
     * [118]awesome tensorflow
     * [119]tensorflow world
     * and more.
     __________________________________________________________________

# let's convert this notebook to a readme automatically for the github project's
 title page:
!jupyter nbconvert --to markdown lstm.ipynb
!mv lstm.md readme.md

[nbconvertapp] converting notebook lstm.ipynb to markdown
[nbconvertapp] support files will be in lstm_files/
[nbconvertapp] making directory lstm_files
[nbconvertapp] making directory lstm_files
[nbconvertapp] writing 38654 bytes to lstm.md

     *    2019 github, inc.
     * [120]terms
     * [121]privacy
     * [122]security
     * [123]status
     * [124]help

     * [125]contact github
     * [126]pricing
     * [127]api
     * [128]training
     * [129]blog
     * [130]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [131]reload to refresh your
   session. you signed out in another tab or window. [132]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/commits/master.atom
   3. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/guillaume-chevalier/lstm-human-activity-recognition
  32. https://github.com/join
  33. https://github.com/login?return_to=/guillaume-chevalier/lstm-human-activity-recognition
  34. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/watchers
  35. https://github.com/login?return_to=/guillaume-chevalier/lstm-human-activity-recognition
  36. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/stargazers
  37. https://github.com/login?return_to=/guillaume-chevalier/lstm-human-activity-recognition
  38. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/network/members
  39. https://github.com/guillaume-chevalier
  40. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
  41. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
  42. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/issues
  43. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/pulls
  44. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/projects
  45. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/pulse
  46. https://github.com/join?source=prompt-code
  47. https://www.neuraxio.com/
  48. https://github.com/topics/machine-learning
  49. https://github.com/topics/deep-learning
  50. https://github.com/topics/lstm
  51. https://github.com/topics/human-activity-recognition
  52. https://github.com/topics/neural-network
  53. https://github.com/topics/id56
  54. https://github.com/topics/recurrent-neural-networks
  55. https://github.com/topics/tensorflow
  56. https://github.com/topics/activity-recognition
  57. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/commits/master
  58. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/branches
  59. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/releases
  60. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/graphs/contributors
  61. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/license
  62. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/search?l=jupyter-notebook
  63. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/search?l=python
  64. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/find/master
  65. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/archive/master.zip
  66. https://github.com/login?return_to=https://github.com/guillaume-chevalier/lstm-human-activity-recognition
  67. https://github.com/join?return_to=/guillaume-chevalier/lstm-human-activity-recognition
  68. https://desktop.github.com/
  69. https://desktop.github.com/
  70. https://developer.apple.com/xcode/
  71. https://visualstudio.github.com/
  72. https://github.com/guillaume-chevalier
  73. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/commits?author=guillaume-chevalier
  74. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/commit/3a20702cd6740362c9007d465311524b48a271e6
  75. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/commit/3a20702cd6740362c9007d465311524b48a271e6
  76. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/tree/3a20702cd6740362c9007d465311524b48a271e6
  77. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/tree/master/lstm_files
  78. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/commit/53132c8708990bef2ee46c05d7d5054718eea73e
  79. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/tree/master/data
  80. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/.gitignore
  81. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/license
  82. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/commit/6952ba57903107c6780a4b4c776146fa304e5cca
  83. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/lstm.ipynb
  84. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/readme.md
  85. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
  86. https://archive.ics.uci.edu/ml/machine-learning-databases/00240/uci har dataset.names
  87. http://www.youtube.com/watch?feature=player_embedded&v=xoen9w05_4a
  88. https://youtu.be/xoen9w05_4a
  89. https://github.com/guillaume-chevalier/filtering-stft-and-laplace-transform
  90. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  91. https://www.quora.com/what-do-samples-features-time-steps-mean-in-lstm/answer/guillaume-chevalier-2
  92. https://www.quora.com/what-do-samples-features-time-steps-mean-in-lstm/answer/guillaume-chevalier-2
  93. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  94. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  95. http://colah.github.io/posts/2015-08-understanding-lstms/
  96. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/lstm_files/lstm_16_0.png
  97. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/lstm_files/lstm_18_1.png
  98. https://www.quora.com/
  99. https://stackoverflow.com/questions/tagged/tensorflow?sort=votes&pagesize=50
 100. https://stackexchange.com/sites#science
 101. https://github.com/guillaume-chevalier/har-stacked-residual-bidir-lstms
 102. https://arxiv.org/pdf/1609.08144.pdf
 103. https://github.com/guillaume-chevalier/linear-attention-recurrent-neural-network
 104. https://github.com/guillaume-chevalier/awesome-deep-learning-resources
 105. https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones
 106. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 107. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
 108. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/blob/master/license
 109. https://ca.linkedin.com/in/chevalierg
 110. https://twitter.com/guillaume_che
 111. https://github.com/guillaume-chevalier/
 112. https://www.quora.com/profile/guillaume-chevalier-2
 113. https://www.youtube.com/c/guillaumechevalier
 114. http://www.neuraxio.com/en/
 115. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/stargazers
 116. https://github.com/guillaume-chevalier/lstm-human-activity-recognition/network/members
 117. https://news.ycombinator.com/item?id=13049143
 118. https://github.com/jtoy/awesome-tensorflow#tutorials
 119. https://github.com/astorfi/tensorflow-world#some-useful-tutorials
 120. https://github.com/site/terms
 121. https://github.com/site/privacy
 122. https://github.com/security
 123. https://githubstatus.com/
 124. https://help.github.com/
 125. https://github.com/contact
 126. https://github.com/pricing
 127. https://developer.github.com/
 128. https://training.github.com/
 129. https://github.blog/
 130. https://github.com/about
 131. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
 132. https://github.com/guillaume-chevalier/lstm-human-activity-recognition

   hidden links:
 134. https://github.com/
 135. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
 136. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
 137. https://github.com/guillaume-chevalier/lstm-human-activity-recognition
 138. https://help.github.com/articles/which-remote-url-should-i-use
 139. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#-lstms-for-human-activity-recognition
 140. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#video-dataset-overview
 141. https://youtu.be/xoen9w05_4a
 142. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#details-about-the-input-data
 143. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#what-is-an-id56
 144. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#what-is-an-lstm
 145. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#results
 146. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#lets-start-by-downloading-the-data
 147. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#preparing-dataset
 148. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#additionnal-parameters
 149. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#utility-functions-for-training
 150. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#lets-get-serious-and-build-the-neural-network
 151. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#hooray-now-train-the-neural-network
 152. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#training-is-good-but-having-visual-insight-is-even-better
 153. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#and-finally-the-multi-class-confusion-matrix-and-metrics
 154. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#conclusion
 155. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#improvements
 156. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#references
 157. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#citation
 158. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#extra-links
 159. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#connect-with-me
 160. https://github.com/guillaume-chevalier/lstm-human-activity-recognition#liked-this-project-did-it-help-you-leave-a-star-fork-and-share-the-love
 161. https://github.com/
