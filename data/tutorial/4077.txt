   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]emergent // future
     * [9]machine learning trends
     * [10]algorithm economy
     * [11]why deep learning matters
     * [12]algorithmia
     __________________________________________________________________

simple id23 with tensorflow part 0: id24 with tables
and neural networks

   [13]go to the profile of arthur juliani
   [14]arthur juliani (button) blockedunblock (button) followfollowing
   aug 25, 2016
   [1*1bzwu1-rotcdbzsungkpfw.jpeg]
   we   ll be learning how to solve the openai frozenlake environment. our
   version is a little less photo-realistic.

   for this tutorial in my id23 series, we are going to
   be exploring a family of rl algorithms called id24 algorithms.
   these are a little different than the policy-based algorithms that will
   be looked at in the the following tutorials (parts 1   3). instead of
   starting with a complex and unwieldy deep neural network, we will begin
   by implementing a simple lookup-table version of the algorithm, and
   then show how to implement a neural-network equivalent using
   tensorflow. given that we are going back to basics, it may be best to
   think of this as part-0 of the series. it will hopefully give an
   intuition into what is really happening in id24 that we can then
   build on going forward when we eventually combine the policy gradient
   and id24 approaches to build state-of-the-art rl agents (if you
   are more interested in policy networks, or already have a grasp on
   id24, feel free to start the tutorial series [15]here instead).

   unlike id189, which attempt to learn functions which
   directly map an observation to an action, id24 attempts to learn
   the value of being in a given state, and taking a specific action
   there. while both approaches ultimately allow us to take intelligent
   actions given a situation, the means of getting to that action differ
   significantly. you may have heard about [16]deepq-networks which can
   play atari games. these are really just larger and more complex
   implementations of the id24 algorithm we are going to discuss
   here.

tabular approaches for tabular environments

   [1*mcjdzr-wfmmks0rpqxsmkw.png]
   the rules of the frozenlake environment.

   for this tutorial we are going to be attempting to solve the
   [17]frozenlake environment from the [18]openai gym. for those
   unfamiliar, the openai gym provides an easy way for people to
   experiment with their learning agents in an array of provided toy
   games. the frozenlake environment consists of a 4x4 grid of blocks,
   each one either being the start block, the goal block, a safe frozen
   block, or a dangerous hole. the objective is to have an agent learn to
   navigate from the start to the goal without moving onto a hole. at any
   given time the agent can choose to move either up, down, left, or
   right. the catch is that there is a wind which occasionally blows the
   agent onto a space they didn   t choose. as such, perfect performance
   every time is impossible, but learning to avoid the holes and reach the
   goal are certainly still doable. the reward at every step is 0, except
   for entering the goal, which provides a reward of 1. thus, we will need
   an algorithm that learns long-term expected rewards. this is exactly
   what id24 is designed to provide.

   in it   s simplest implementation, id24 is a table of values for
   every state (row) and action (column) possible in the environment.
   within each cell of the table, we learn a value for how good it is to
   take a given action within a given state. in the case of the frozenlake
   environment, we have 16 possible states (one for each block), and 4
   possible actions (the four directions of movement), giving us a 16x4
   table of q-values. we start by initializing the table to be uniform
   (all zeros), and then as we observe the rewards we obtain for various
   actions, we update the table accordingly.

   we make updates to our q-table using something called the [19]bellman
   equation, which states that the expected long-term reward for a given
   action is equal to the immediate reward from the current action
   combined with the expected reward from the best future action taken at
   the following state. in this way, we reuse our own q-table when
   estimating how to update our table for future actions! in equation
   form, the rule looks like this:

     eq 1. q(s,a) = r +   (max(q(s   ,a   ))

   this says that the q-value for a given state (s) and action (a) should
   represent the current reward (r) plus the maximum discounted (  ) future
   reward expected according to our own table for the next state (s   ) we
   would end up in. the discount variable allows us to decide how
   important the possible future rewards are compared to the present
   reward. by updating in this way, the table slowly begins to obtain
   accurate measures of the expected future reward for a given action in a
   given state. below is a python walkthrough of the q-table algorithm
   implemented in the frozenlake environment:

   iframe: [20]/media/64fa930782a8b7dc8b1cbeea6d8122f4?postid=d195264329d0

   (thanks to [21]praneet d for finding the optimal hyperparameters for
   this approach)

id24 with neural networks

   now, you may be thinking: tables are great, but they don   t really
   scale, do they? while it is easy to have a 16x4 table for a simple grid
   world, the number of possible states in any modern game or real-world
   environment is nearly infinitely larger. for most interesting problems,
   tables simply don   t work. we instead need some way to take a
   description of our state, and produce q-values for actions without a
   table: that is where neural networks come in. by acting as a function
   approximator, we can take any number of possible states that can be
   represented as a vector and learn to map them to q-values.

   in the case of the frozenlake example, we will be using a one-layer
   network which takes the state encoded in a one-hot vector (1x16), and
   produces a vector of 4 q-values, one for each action. such a simple
   network acts kind of like a glorified table, with the network weights
   serving as the old cells. the key difference is that we can easily
   expand the tensorflow network with added layers, id180,
   and different input types, whereas all that is impossible with a
   regular table. the method of updating is a little different as well.
   instead of directly updating our table, with a network we will be using
   id26 and a id168. our id168 will be
   sum-of-squares loss, where the difference between the current predicted
   q-values, and the    target    value is computed and the gradients passed
   through the network. in this case, our q-target for the chosen action
   is the equivalent to the q-value computed in equation 1 above.

     eq2. loss =    (q-target - q)  

   below is the tensorflow walkthrough of implementing our simple
   q-network:

   iframe: [22]/media/fbb452cad17fbab1e0c7185d6805ddcb?postid=d195264329d0

   while the network learns to solve the frozenlake problem, it turns out
   it doesn   t do so quite as efficiently as the q-table. while neural
   networks allow for greater flexibility, they do so at the cost of
   stability when it comes to id24. there are a number of possible
   extensions to our simple q-network which allow for greater performance
   and more robust learning. two tricks in particular are referred to as
   experience replay and freezing target networks. those improvements and
   other tweaks were the key to getting atari-playing deep q-networks, and
   we will be exploring those additions in the future. for more info on
   the theory behind id24, see this [23]great post by tambet
   matiisen. i hope this tutorial has been helpful for those curious about
   how to implement simple id24 algorithms!
     __________________________________________________________________

   if this post has been valuable to you, please consider [24]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[25]arthur juliani, or on twitter
   [26]@awjliani.
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. part 0         id24 agents
    2. [27]part 1         two-armed bandit
    3. [28]part 1.5         contextual bandits
    4. [29]part 2         policy-based agents
    5. [30]part 3         model-based rl
    6. [31]part 4         deep q-networks and beyond
    7. [32]part 5         visualizing an agent   s thoughts and actions
    8. [33]part 6         partial observability and deep recurrent q-networks
    9. [34]part 7         action-selection strategies for exploration
   10. [35]part 8         asynchronous actor-critic agents (a3c)

     * [36]machine learning
     * [37]artificial intelligence
     * [38]neural networks
     * [39]deep learning
     * [40]robotics

   (button)
   (button)
   (button) 13.2k claps
   (button) (button) (button) 104 (button) (button)

     (button) blockedunblock (button) followfollowing
   [41]go to the profile of arthur juliani

[42]arthur juliani

   deep learning [43]@unity3d

     (button) follow
   [44]emergent // future

[45]emergent // future

   exploring frontier technology through the lens of artificial
   intelligence, data science, and the shape of things to come

     * (button)
       (button) 13.2k
     * (button)
     *
     *

   [46]emergent // future
   never miss a story from emergent // future, when you sign up for
   medium. [47]learn more
   never miss a story from emergent // future
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d195264329d0
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0&source=--------------------------nav_reg&operation=register
   8. https://medium.com/emergent-future?source=logo-lo_hn3ezkqe4svl---d7fff85024c6
   9. https://medium.com/emergent-future/machine-learning-trends-and-the-future-of-artificial-intelligence-2016-15c15cd6c129
  10. https://medium.com/emergent-future/how-the-algorithm-economy-and-containers-are-changing-the-way-we-build-and-deploy-apps-today-4ecdbb59318d
  11. https://medium.com/emergent-future/why-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4
  12. https://algorithmia.com/
  13. https://medium.com/@awjuliani?source=post_header_lockup
  14. https://medium.com/@awjuliani
  15. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  16. http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html
  17. https://gym.openai.com/envs/frozenlake-v0
  18. https://gym.openai.com/
  19. https://en.wikipedia.org/wiki/bellman_equation
  20. https://medium.com/media/64fa930782a8b7dc8b1cbeea6d8122f4?postid=d195264329d0
  21. https://github.com/praneetdutta
  22. https://medium.com/media/fbb452cad17fbab1e0c7185d6805ddcb?postid=d195264329d0
  23. http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
  24. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  25. https://medium.com/@awjuliani
  26. https://twitter.com/awjuliani
  27. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  28. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c
  29. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  30. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  31. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df
  32. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.kdgfgy7k8
  33. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  34. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  35. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  36. https://medium.com/tag/machine-learning?source=post
  37. https://medium.com/tag/artificial-intelligence?source=post
  38. https://medium.com/tag/neural-networks?source=post
  39. https://medium.com/tag/deep-learning?source=post
  40. https://medium.com/tag/robotics?source=post
  41. https://medium.com/@awjuliani?source=footer_card
  42. https://medium.com/@awjuliani
  43. http://twitter.com/unity3d
  44. https://medium.com/emergent-future?source=footer_card
  45. https://medium.com/emergent-future?source=footer_card
  46. https://medium.com/emergent-future
  47. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  49. https://medium.com/p/d195264329d0/share/twitter
  50. https://medium.com/p/d195264329d0/share/facebook
  51. https://medium.com/p/d195264329d0/share/twitter
  52. https://medium.com/p/d195264329d0/share/facebook
