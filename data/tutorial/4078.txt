   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

simple id23 in tensorflow: part 1 - two-armed bandit

   [9]go to the profile of arthur juliani
   [10]arthur juliani (button) blockedunblock (button) followfollowing
   jun 14, 2016
   [1*r9iokw7uuvo_yizzab4ruw.jpeg]

introduction

   id23 provides the capacity for us not only to teach
   an artificial agent how to act, but to allow it to learn through it   s
   own interactions with an environment. by combining the complex
   representations that deep neural networks can learn with the
   goal-driven learning of an rl agent, computers have accomplished some
   amazing feats, like [11]beating humans at over a dozen atari games, and
   [12]defeating the go world champion.

   learning how to build these agents requires a bit of a change in
   thinking for anyone used to working in a supervised learning setting
   though. gone is the ability to simply get the algorithm to pair certain
   stimuli with certain responses. instead rl algorithms must enable the
   agent to learn the correct pairings itself through the use of
   observations, rewards, and actions. since there is no longer a    true   
   correct action for an agent to take in any given circumstance that we
   can just tell it, things get a little tricky. in this post and those to
   follow, i will be walking through the creation and training of
   id23 agents. the agent and task will begin simple, so
   that the concepts are clear, and then work up to more complex task and
   environments.

two-armed bandit

   the simplest id23 problem is the n-armed bandit.
   essentially, there are n-many slot machines, each with a different
   fixed payout id203. the goal is to discover the machine with the
   best payout, and maximize the returned reward by always choosing it. we
   are going to make it even simpler, by only having two possible slot
   machines to choose between. in fact, this problem is so simple that it
   is more of a precursor to real rl problems than one itself. let me
   explain. typical aspects of a task that make it an rl problem are the
   following:
     * different actions yield different rewards. for example, when
       looking for treasure in a maze, going left may lead to the
       treasure, whereas going right may lead to a pit of snakes.
     * rewards are delayed over time. this just means that even if going
       left in the above example is the right things to do, we may not
       know it till later in the maze.
     * reward for an action is conditional on the state of the
       environment. continuing the maze example, going left may be ideal
       at a certain fork in the path, but not at others.

   the n-armed bandit is a nice starting place because we don   t have to
   worry about aspects #2 and 3. all we need to focus on is learning which
   rewards we get for each of the possible actions, and ensuring we chose
   the optimal ones. in the context of rl lingo, this is called learning a
   policy. we are going to be using a method called policy gradients,
   where our simple neural network learns a policy for picking actions by
   adjusting it   s weights through id119 using feedback from the
   environment. there is another approach to id23 where
   agents learn value functions. in those approaches, instead of learning
   the optimal action in a given state, the agent learns to predict how
   good a given state or action will be for the agent to be in. both
   approaches allow agents to learn good behavior, but the policy gradient
   approach is a little more direct.

policy gradient

   the simplest way to think of a policy gradient network is one which
   produces explicit outputs. in the case of our bandit, we don   t need to
   condition these outputs on any state. as such, our network will consist
   of just a set of weights, with each corresponding to each of the
   possible arms to pull in the bandit, and will represent how good our
   agent thinks it is to pull each arm. if we initialize these weights to
   1, then our agent will be somewhat optimistic about each arm   s
   potential reward.

   to update our network, we will simply try an arm with an e-greedy
   policy (see [13]part 7 for more on action-selection strategies). this
   means that most of the time our agent will choose the action that
   corresponds to the largest expected value, but occasionally, with e
   id203, it will choose randomly. in this way, the agent can try
   out each of the different arms to continue to learn more about them.
   once our agent has taken an action, it then receives a reward of either
   1 or -1. with this reward, we can then make an update to our network
   using the policy loss equation:

   loss = -log(  )*a

   a is advantage, and is an essential aspect of all reinforcement
   learning algorithms. intuitively it corresponds to how much better an
   action was than some baseline. in future algorithms, we will develop
   more complex baselines to compare our rewards to, but for now we will
   assume that the baseline is 0, and it can be thought of as simply the
   reward we received for each action.

      is the policy. in this case, it corresponds to the chosen action   s
   weight.

   intuitively, this id168 allows us to increase the weight for
   actions that yielded a positive reward, and decrease them for actions
   that yielded a negative reward. in this way the agent will be more or
   less likely to pick that action in the future. by taking actions,
   getting rewards, and updating our network in this circular manner, we
   will quickly converge to an agent that can solve our bandit problem!
   don   t take my word for it though. try it out yourself.

   iframe: [14]/media/2ce231ab4378c4a22c17225e5fcae8d8?postid=fd544fab149

   (update 09/10/2016): i rewrote the ipython walkthrough for this
   tutorial today. the loss equation used previously was less intuitive
   than i would have liked. i have replaced it with a more standard and
   interpretable version that will definitely be more useful for those
   interested in applying id189 to more complex
   problems.)

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[15]arthur juliani, or on twitter
   [16]@awjliani.

   if this post has been valuable to you, please consider [17]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. [18]part 0         id24 agents
    2. part 1         two-armed bandit
    3. [19]part 1.5         contextual bandits
    4. [20]part 2         policy-based agents
    5. [21]part 3         model-based rl
    6. [22]part 4         deep q-networks and beyond
    7. [23]part 5         visualizing an agent   s thoughts and actions
    8. [24]part 6         partial observability and deep recurrent q-networks
    9. [25]part 7         action-selection strategies for exploration
   10. [26]part 8         asynchronous actor-critic agents (a3c)

     * [27]machine learning
     * [28]artificial intelligence
     * [29]deep learning
     * [30]technology
     * [31]robotics

   (button)
   (button)
   (button) 2.9k claps
   (button) (button) (button) 25 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of arthur juliani

[33]arthur juliani

   deep learning [34]@unity3d

     * (button)
       (button) 2.9k
     * (button)
     *
     *

   [35]go to the profile of arthur juliani
   never miss a story from arthur juliani, when you sign up for medium.
   [36]learn more
   never miss a story from arthur juliani
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/fd544fab149
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@awjuliani?source=post_header_lockup
  10. https://medium.com/@awjuliani
  11. https://deepmind.com/id25
  12. https://deepmind.com/alpha-go
  13. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.gl3i8atjl
  14. https://medium.com/media/2ce231ab4378c4a22c17225e5fcae8d8?postid=fd544fab149
  15. https://medium.com/@awjuliani
  16. https://twitter.com/awjuliani
  17. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  18. https://medium.com/p/d195264329d0
  19. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c
  20. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  21. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  22. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df
  23. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.kdgfgy7k8
  24. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  25. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  26. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  27. https://medium.com/tag/machine-learning?source=post
  28. https://medium.com/tag/artificial-intelligence?source=post
  29. https://medium.com/tag/deep-learning?source=post
  30. https://medium.com/tag/technology?source=post
  31. https://medium.com/tag/robotics?source=post
  32. https://medium.com/@awjuliani?source=footer_card
  33. https://medium.com/@awjuliani
  34. http://twitter.com/unity3d
  35. https://medium.com/@awjuliani
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/fd544fab149/share/twitter
  39. https://medium.com/p/fd544fab149/share/facebook
  40. https://medium.com/p/fd544fab149/share/twitter
  41. https://medium.com/p/fd544fab149/share/facebook
