   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]emergent // future
     * [9]machine learning trends
     * [10]algorithm economy
     * [11]why deep learning matters
     * [12]algorithmia
     __________________________________________________________________

simple id23 with tensorflow part 1.5: contextual bandits

   [13]go to the profile of arthur juliani
   [14]arthur juliani (button) blockedunblock (button) followfollowing
   sep 27, 2016
   [1*_xuyp-sah47eyatzul56ww.jpeg]

   (note: this post is designed as an additional tutorial to act as a
   bridge between [15]parts 1 & [16]2.)

   in [17]part 1 of my simple rl series, we introduced the field of
   id23, and i demonstrated how to build an agent which
   can solve the multi-armed bandit problem. in that situation, there are
   no environmental states, and the agent must simply learn to choose
   which action is best to take. without a given state state, the best
   action at any moment is also the best action always. [18]part 2
   establishes the full id23 problem in which there are
   environmental states, new states depend on previous actions, and
   rewards can be delayed over time.

   there is actually a set of problems in-between the stateless situation
   and the full rl problem. i want to provide an example of such a
   problem, and show how to solve it. my hope is that those entirely new
   to rl can benefit from being introduced to each element of the full
   formulation step by step. specifically, in this post i want to show how
   to solve problems in which there are states, but they aren   t determined
   by the previous states or actions. additionally, we won   t be
   considering delayed rewards. all of that comes in [19]part 2. this
   simplified way of posing the rl problem is referred to as the
   contextual bandit.
   [1*3nzibtrann6uvltplxwaga.png]
   above: multi-armed bandit problem, where only action effect reward.
   middle: contextual bandit problem, where state and action effect
   reward. bottom: full rl problem, where action effects state, and
   rewards may be delayed in time.

contextual bandit

   in the original multi-armed bandit problem discussed in part 1, there
   is only a single bandit, which can be thought of as like a
   slot-machine. the range of actions available to the agent consist of
   pulling one of multiple arms of the bandit. by doing so, a reward of +1
   or -1 is received at different rates. the problem is considered solved
   if the agent learns to always choose the arm that most often provides a
   positive reward. in such a case, we can design an agent that completely
   ignores the state of the environment, since for all intents and
   purposes, there is only ever a single, unchanging state.

   contextual bandits introduce the concept of the state. the state
   consists of a description of the environment that the agent can use to
   take more informed actions. in our problem, instead of a single bandit,
   there can now be multiple bandits. the state of the environment tells
   us which bandit we are dealing with, and the goal of the agent is to
   learn the best action not just for a single bandit, but for any number
   of them. since each bandit will have different reward probabilities for
   each arm, our agent will need to learn to condition its action on the
   state of the environment. unless it does this, it won   t achieve the
   maximum reward possible over time. in order to accomplish this, we will
   be building a single-layer neural network in tensorflow that takes a
   state and produces an action. by using a policy-gradient update method,
   we can have the network learn to take actions that maximize its reward.
   below is the ipython notebook walking through the tutorial.

   iframe: [20]/media/1fe1714d993ade853ffcfb8801d9798b?postid=bff01d1aad9c

   hopefully you   ve found this tutorial helpful in giving an intuition of
   how id23 agents can learn to solve problems of
   varying complexity and interactivity. if you   ve mastered this problem,
   you are ready to explore the full problem where time and actions matter
   in part 2 and beyond of this series.
     __________________________________________________________________

   if this post has been valuable to you, please consider [21]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   more from my simple id23 with tensorflow series:
    1. [22]part 0         id24 agents
    2. [23]part 1         two-armed bandit
    3. part 1.5         contextual bandits
    4. [24]part 2         policy-based agents
    5. [25]part 3         model-based rl
    6. [26]part 4         deep q-networks and beyond
    7. [27]part 5         visualizing an agent   s thoughts and actions
    8. [28]part 6         partial observability and deep recurrent q-networks
    9. [29]part 7         action-selection strategies for exploration
   10. [30]part 8         asynchronous actor-critic agents (a3c)
     __________________________________________________________________

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[31]arthur juliani, or on twitter
   [32]@awjliani.

     * [33]machine learning
     * [34]artificial intelligence
     * [35]deep learning
     * [36]neural networks
     * [37]robotics

   (button)
   (button)
   (button) 1.5k claps
   (button) (button) (button) 17 (button) (button)

     (button) blockedunblock (button) followfollowing
   [38]go to the profile of arthur juliani

[39]arthur juliani

   deep learning [40]@unity3d

     (button) follow
   [41]emergent // future

[42]emergent // future

   exploring frontier technology through the lens of artificial
   intelligence, data science, and the shape of things to come

     * (button)
       (button) 1.5k
     * (button)
     *
     *

   [43]emergent // future
   never miss a story from emergent // future, when you sign up for
   medium. [44]learn more
   never miss a story from emergent // future
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bff01d1aad9c
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c&source=--------------------------nav_reg&operation=register
   8. https://medium.com/emergent-future?source=logo-lo_nnvhlda7uak6---d7fff85024c6
   9. https://medium.com/emergent-future/machine-learning-trends-and-the-future-of-artificial-intelligence-2016-15c15cd6c129
  10. https://medium.com/emergent-future/how-the-algorithm-economy-and-containers-are-changing-the-way-we-build-and-deploy-apps-today-4ecdbb59318d
  11. https://medium.com/emergent-future/why-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4
  12. https://algorithmia.com/
  13. https://medium.com/@awjuliani?source=post_header_lockup
  14. https://medium.com/@awjuliani
  15. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  16. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  17. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  18. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  19. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  20. https://medium.com/media/1fe1714d993ade853ffcfb8801d9798b?postid=bff01d1aad9c
  21. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  22. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  23. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  24. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  25. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  26. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.i2zpbmre8
  27. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a
  28. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  29. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  30. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  31. https://medium.com/@awjuliani
  32. https://twitter.com/awjuliani
  33. https://medium.com/tag/machine-learning?source=post
  34. https://medium.com/tag/artificial-intelligence?source=post
  35. https://medium.com/tag/deep-learning?source=post
  36. https://medium.com/tag/neural-networks?source=post
  37. https://medium.com/tag/robotics?source=post
  38. https://medium.com/@awjuliani?source=footer_card
  39. https://medium.com/@awjuliani
  40. http://twitter.com/unity3d
  41. https://medium.com/emergent-future?source=footer_card
  42. https://medium.com/emergent-future?source=footer_card
  43. https://medium.com/emergent-future
  44. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  46. https://medium.com/p/bff01d1aad9c/share/twitter
  47. https://medium.com/p/bff01d1aad9c/share/facebook
  48. https://medium.com/p/bff01d1aad9c/share/twitter
  49. https://medium.com/p/bff01d1aad9c/share/facebook
