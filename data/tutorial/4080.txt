   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

simple id23 with tensorflow: part 2 - policy-based agents

   [9]go to the profile of arthur juliani
   [10]arthur juliani (button) blockedunblock (button) followfollowing
   jun 24, 2016
   [1*g_whtiry9fglw3it6hffha.gif]

   after a weeklong break, i am back again with part 2 of my reinforcement
   learning tutorial series. in [11]part 1, i had shown how to put
   together a basic agent that learns to choose the more rewarding of two
   possible options. in this post, i am going to describe how we get from
   that simple agent to one that is capable of taking in an observation of
   the world, and taking actions which provide the optimal reward not just
   in the present, but over the long run. with these additions, we will
   have a full reinforcement agent.

   environments which pose the full problem to an agent are referred to as
   id100 (mdps). these environments not only provide
   rewards and state transitions given actions, but those rewards are also
   condition on the state of the environment and the action the agent
   takes within that state. these dynamics are also temporal, and can be
   delayed over time.

   to be a little more formal, we can define a markov decision process as
   follows. an mdp consists of a set of all possible states s from which
   our agent at any time will experience s. a set of all possible actions
   a from which our agent at any time will take action a. given a state
   action pair (s, a), the transition id203 to a new state s    is
   defined by t(s, a), and the reward r is given by r(s, a). as such, at
   any time in an mdp, an agent is given a state s, takes action a, and
   receives new state s    and reward r.

   while it may seem relatively simple, we can pose almost any task we
   could think of as an mdp. for example, imagine opening a door. the
   state is the vision of the door that we have, as well as the position
   of our body and door in the world. the actions are our every movement
   our body could make. the reward in this case is the door successfully
   opening. certain actions, like walking toward the door are essential to
   solving the problem, but aren   t themselves reward-giving, since only
   actually opening the door will provide the reward. in this way, an
   agent needs to learn to assign value to actions the lead eventually to
   the reward, hence the introduction of temporal dynamics.

cart-pole task

   in order to accomplish this, we are going to need a challenge that is
   more difficult for the agent than the two-armed bandit. to meet provide
   this challenge we are going to utilize the openai gym, a collection of
   id23 environments. we will be using one of the
   classic tasks, the cart-pole. to learn more about the openai gym, and
   this specific task, check out their tutorial [12]here. essentially, we
   are going to have our agent learn how to balance a pole for as long as
   possible without it falling. unlike the two-armed bandit, this task
   requires:
     * observations         the agent needs to know where pole currently is, and
       the angle at which it is balancing. to accomplish this, our neural
       network will take an observation and use it when producing the
       id203 of an action.
     * delayed reward         keeping the pole in the air as long as possible
       means moving in ways that will be advantageous for both the present
       and the future. to accomplish this we will adjust the reward value
       for each observation-action pair using a function that weighs
       actions over time.

   to take reward over time into account, the form of policy gradient we
   used in the previous tutorials will need a few adjustments. the first
   of which is that we now need to update our agent with more than one
   experience at a time. to accomplish this, we will collect experiences
   in a buffer, and then occasionally use them to update the agent all at
   once. these sequences of experience are sometimes referred to as
   rollouts, or experience traces. we can   t just apply these rollouts by
   themselves however, we will need to ensure that the rewards are
   properly adjusted by a discount factor

   intuitively this allows each action to be a little bit responsible for
   not only the immediate reward, but all the rewards that followed. we
   now use this modified reward as an estimation of the advantage in our
   loss equation. with those changes, we are ready to solve cartpole!

   let   s get to it!
   [13]awjuliani/deeprl-agents
   deeprl-agents - a set of deep id23 agents implemented
   in tensorflow.github.com

   and with that we have a fully-functional id23 agent.
   our agent is still far from the state of the art though. while we are
   using a neural network for the policy, the network still isn   t as deep
   or complex as the most advanced networks. in the next post i will be
   showing how to use deep neural networks to create agents able to learn
   more complex relationships with the environment in order to play a more
   exciting game than pole balancing. in doing so, i will be diving into
   the kinds of representations that a network learns for more complex
   environments.
     __________________________________________________________________

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[14]arthur juliani, or on twitter
   [15]@awjliani.

   if this post has been valuable to you, please consider [16]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. [17]part 0         id24 agents
    2. [18]part 1         two-armed bandit
    3. [19]part 1.5         contextual bandits
    4. part 2         policy-based agents
    5. [20]part 3         model-based rl
    6. [21]part 4         deep q-networks and beyond
    7. [22]part 5         visualizing an agent   s thoughts and actions
    8. [23]part 6         partial observability and deep recurrent q-networks
    9. [24]part 7         action-selection strategies for exploration
   10. [25]part 8         asynchronous actor-critic agents (a3c)

     * [26]machine learning
     * [27]artificial intelligence
     * [28]deep learning
     * [29]technology
     * [30]robotics

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 47 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of arthur juliani

[32]arthur juliani

   deep learning [33]@unity3d

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [34]go to the profile of arthur juliani
   never miss a story from arthur juliani, when you sign up for medium.
   [35]learn more
   never miss a story from arthur juliani
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/ded33892c724
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@awjuliani?source=post_header_lockup
  10. https://medium.com/@awjuliani
  11. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  12. https://gym.openai.com/docs
  13. https://github.com/awjuliani/deeprl-agents/blob/master/vanilla-policy.ipynb
  14. https://medium.com/@awjuliani
  15. https://twitter.com/awjuliani
  16. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  17. https://medium.com/p/d195264329d0
  18. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  19. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c
  20. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  21. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df
  22. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.kdgfgy7k8
  23. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  24. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  25. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  26. https://medium.com/tag/machine-learning?source=post
  27. https://medium.com/tag/artificial-intelligence?source=post
  28. https://medium.com/tag/deep-learning?source=post
  29. https://medium.com/tag/technology?source=post
  30. https://medium.com/tag/robotics?source=post
  31. https://medium.com/@awjuliani?source=footer_card
  32. https://medium.com/@awjuliani
  33. http://twitter.com/unity3d
  34. https://medium.com/@awjuliani
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://github.com/awjuliani/deeprl-agents/blob/master/vanilla-policy.ipynb
  38. https://medium.com/p/ded33892c724/share/twitter
  39. https://medium.com/p/ded33892c724/share/facebook
  40. https://medium.com/p/ded33892c724/share/twitter
  41. https://medium.com/p/ded33892c724/share/facebook
