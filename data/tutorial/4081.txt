   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

simple id23 with tensorflow: part 3 - model-based rl

it has been a while since my last post in this series, where i showed how to
design a policy-gradient reinforcement agent that could solve the cartpole
task. in this tutorial, i would like to re-examine the cartpole problem, but
this time introduce the concept of a model of the environment that the agent
can use to improve it   s performance.

   [9]go to the profile of arthur juliani
   [10]arthur juliani (button) blockedunblock (button) followfollowing
   jul 26, 2016
   [1*abboxupjetnbfpifaxm7yg.png]

   (if you haven   t read them already, here are links to the [11]first and
   [12]second tutorials in this series. each tutorial builds on the last,
   so if you are new to id23, i suggest reading through
   them chronologically.)

   what is a model and why would we want to use one? in this case, a model
   is going to be a neural network that attempts to learn the dynamics of
   the real environment. for example, in the cartpole we would like a
   model to be able to predict the next position of the cart given the
   previous position and an action. by learning an accurate model, we can
   train our agent using the model rather than requiring to use the real
   environment every time. while this may seem less useful when the real
   environment is itself a simulation, like in our cartpole task, it can
   have huge advantages when attempting to learn policies for acting in
   the physical world.

   unlike in computer simulations, physical environments take time to
   navigate, and the physical rules of the world prevent things like easy
   environment resets from being feasible. instead, we can save time and
   energy by building a model of the environment. with such a model, an
   agent can    imagine    what it might be like to move around the real
   environment, and we can train a policy on this imagined environment in
   addition to the real one. if we were given a good enough model of an
   environment, an agent could be trained entirely on that model, and even
   perform well when placed into a real environment for the first time.

   how are we going to accomplish this in tensorflow? as i mentioned
   above, we are going to be using a neural network that will learn the
   transition dynamics between a previous observation and action, and the
   expected new observation, reward, and done state. our training
   procedure will involve switching between training our model using the
   real environment, and training our agent   s policy using the model
   environment. by using this approach we will be able to learn a policy
   that allows our agent to solve the cartpole task without actually ever
   training the policy on the real environment! read the ipython notebook
   below for the details on how this is done.

   iframe: [13]/media/fbceb46187b482b112ffe5ca9be52565?postid=9a6fe0cce99

   since there are now two network involved, there are plenty of
   hyper-parameters to adjust in order to improve performance or
   efficiency. i encourage you to play with them in order to discover
   better means of combining the the models. in part 4 i will be exploring
   how to utilize convolutional networks to learn representations of more
   complex environments, such as atari games.
     __________________________________________________________________

   if this post has been valuable to you, please consider [14]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[15]arthur juliani, or on twitter
   [16]@awjliani.
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. [17]part 0         id24 agents
    2. [18]part 1         two-armed bandit
    3. [19]part 1.5         contextual bandits
    4. [20]part 2         policy-based agents
    5. part 3         model-based rl
    6. [21]part 4         deep q-networks and beyond
    7. [22]part 5         visualizing an agent   s thoughts and actions
    8. [23]part 6         partial observability and deep recurrent q-networks
    9. [24]part 7         action-selection strategies for exploration
   10. [25]part 8         asynchronous actor-critic agents (a3c)
     __________________________________________________________________

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[26]arthur juliani, or on twitter
   [27]@awjliani.

     * [28]machine learning
     * [29]artificial intelligence
     * [30]deep learning
     * [31]computer science
     * [32]neural networks

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of arthur juliani

[34]arthur juliani

   deep learning [35]@unity3d

     * (button)
       (button) 1k
     * (button)
     *
     *

   [36]go to the profile of arthur juliani
   never miss a story from arthur juliani, when you sign up for medium.
   [37]learn more
   never miss a story from arthur juliani
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/9a6fe0cce99
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@awjuliani?source=post_header_lockup
  10. https://medium.com/@awjuliani
  11. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  12. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  13. https://medium.com/media/fbceb46187b482b112ffe5ca9be52565?postid=9a6fe0cce99
  14. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  15. https://medium.com/@awjuliani
  16. https://twitter.com/awjuliani
  17. https://medium.com/p/d195264329d0
  18. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  19. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c
  20. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  21. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df
  22. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.kdgfgy7k8
  23. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  24. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  25. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  26. https://medium.com/@awjuliani
  27. https://twitter.com/awjuliani
  28. https://medium.com/tag/machine-learning?source=post
  29. https://medium.com/tag/artificial-intelligence?source=post
  30. https://medium.com/tag/deep-learning?source=post
  31. https://medium.com/tag/computer-science?source=post
  32. https://medium.com/tag/neural-networks?source=post
  33. https://medium.com/@awjuliani?source=footer_card
  34. https://medium.com/@awjuliani
  35. http://twitter.com/unity3d
  36. https://medium.com/@awjuliani
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/9a6fe0cce99/share/twitter
  40. https://medium.com/p/9a6fe0cce99/share/facebook
  41. https://medium.com/p/9a6fe0cce99/share/twitter
  42. https://medium.com/p/9a6fe0cce99/share/facebook
