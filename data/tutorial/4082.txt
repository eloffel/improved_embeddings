   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

simple id23 with tensorflow part 4: deep q-networks
and beyond

   [9]go to the profile of arthur juliani
   [10]arthur juliani (button) blockedunblock (button) followfollowing
   sep 2, 2016
   [1*k8hteljd8qvcplwhumaid18.jpeg]
   a smart game agent will learn to avoid dangerous holes in the ground.

   welcome to the latest installment of my id23 series.
   in this tutorial we will be walking through the creation of a deep
   q-network. it will be built upon the simple one layer q-network we
   created in [11]part 0, so i would recommend reading that first if you
   are new to id23. while our ordinary q-network was
   able to barely perform as well as the q-table in a simple game
   environment, deep q-networks are much more capable. in order to
   transform an ordinary q-network into a id25 we will be making the
   following improvements:
    1. going from a single-layer network to a multi-layer convolutional
       network.
    2. implementing experience replay, which will allow our network to
       train itself using stored memories from it   s experience.
    3. utilizing a second    target    network, which we will use to compute
       target q-values during our updates.

   it was these three innovations that allowed [12]the google deepmind
   team to achieve superhuman performance on dozens of atari games using
   their id25 agent. we will be walking through each individual
   improvement, and showing how to implement it. we won   t stop there
   though. the pace of deep learning research is extremely fast, and the
   id25 of 2014 is no longer the most advanced agent around anymore. i will
   discuss two simple additional improvements to the id25 architecture,
   double id25 and dueling id25, that allow for improved performance,
   stability, and faster training time. in the end we will have a network
   that can tackle a number of challenging atari games, and we will
   demonstrate how to train the id25 to learn a basic navigation task.

getting from q-network to deep q-network

   [1*t54ngd-b_ckcp3n6hyxlvg.png]

addition 1: convolutional layers

   since our agent is going to be learning to play video games, it has to
   be able to make sense of the game   s screen output in a way that is at
   least similar to how humans or other intelligent animals are able to.
   instead of considering each pixel independently, convolutional layers
   allow us to consider regions of an image, and maintain spatial
   relationships between the objects on the screen as we send information
   up to higher levels of the network. in this way, they act similarly to
   human receptive fields. indeed there is a body of research showing that
   convolutional neural network learn representations that are [13]similar
   to those of the primate visual cortex. as such, they are ideal for the
   first few elements within our network.

   in tensorflow, we can utilize the tf.contrib.layers.convolution2d
   function to easily create a convolutional layer. we write for function
   as follows:

     convolution_layer =
     tf.contrib.layers.convolution2d(inputs,num_outputs,kernel_size,strid
     e,padding)

   here num_outs refers to how many filters we would like to apply to the
   previous layer. kernel_size refers to how large a window we would like
   to slide over the previous layer. stride refers to how many pixels we
   want to skip as we slide the window across the layer. finally, padding
   refers to whether we want our window to slide over just the bottom
   layer (   valid   ) or add padding around it (   same   ) in order to ensure
   that the convolutional layer has the same dimensions as the previous
   layer. for more information, see the [14]tensorflow documentation.

addition 2: experience replay

   the second major addition to make id25s work is experience replay. the
   basic idea is that by storing an agent   s experiences, and then randomly
   drawing batches of them to train the network, we can more robustly
   learn to perform well in the task. by keeping the experiences we draw
   random, we prevent the network from only learning about what it is
   immediately doing in the environment, and allow it to learn from a more
   varied array of past experiences. each of these experiences are stored
   as a tuple of <state,action,reward,next state>. the experience replay
   buffer stores a fixed number of recent memories, and as new ones come
   in, old ones are removed. when the time comes to train, we simply draw
   a uniform batch of random memories from the buffer, and train our
   network with them. for our id25, we will build a simple class that
   handles storing and retrieving memories.

addition 3: separate target network

   the third major addition to the id25 that makes it unique is the
   utilization of a second network during the training procedure. this
   second network is used to generate the target-q values that will be
   used to compute the loss for every action during training. why not use
   just use one network for both estimations? the issue is that at every
   step of training, the q-network   s values shift, and if we are using a
   constantly shifting set of values to adjust our network values, then
   the value estimations can easily spiral out of control. the network can
   become destabilized by falling into feedback loops between the target
   and estimated q-values. in order to mitigate that risk, the target
   network   s weights are fixed, and only periodically or slowly updated to
   the primary q-networks values. in this way training can proceed in a
   more stable manner.

   instead of updating the target network periodically and all at once, we
   will be updating it frequently, but slowly. this technique was
   introduced in [15]another deepmind paper earlier this year, where they
   found that it stabilized the training process.

going beyond id25

   with the additions above, we have everything we need to replicate the
   dwn of 2014. but the world moves fast, and a number of improvements
   above and beyond the id25 architecture [16]described by deepmind, have
   allowed for even greater performance and stability. before training
   your new id25 on your favorite atari game, i would suggest checking the
   newer additions out. i will provide a description and some code for two
   of them: double id25, and dueling id25. both are simple to implement, and
   by combining both techniques, we can achieve better performance with
   faster training times.

double id25

   the main intuition behind [17]double id25 is that the regular id25 often
   overestimates the q-values of the potential actions to take in a given
   state. while this would be fine if all actions were always
   overestimates equally, there was reason to believe this wasn   t the
   case. you can easily imagine that if certain suboptimal actions
   regularly were given higher q-values than optimal actions, the agent
   would have a hard time ever learning the ideal policy. in order to
   correct for this, the authors of did25 paper propose a simple trick:
   instead of taking the max over q-values when computing the target-q
   value for our training step, we use our primary network to chose an
   action, and our target network to generate the target q-value for that
   action. by decoupling the action choice from the target q-value
   generation, we are able to substantially reduce the overestimation, and
   train faster and more reliably. below is the new did25 equation for
   updating the target value.

     q-target = r +   q(s   ,argmax(q(s   ,a,  ),     ))

dueling id25

   [1*n_t9i7meejaowlduh1i7cw.png]
   above: regular id25 with a single stream for q-values. below: dueling
   id25 where the value and advantage are calculated separately and then
   combined only at the final layer into a q value.

   in order to explain the reasoning behind the architecture changes that
   [18]dueling id25 makes, we need to first explain some a few additional
   id23 terms. the q-values that we have been discussing
   so far correspond to how good it is to take a certain action given a
   certain state. this can be written as q(s,a). this action given state
   can actually be decomposed into two more fundamental notions of value.
   the first is the value function v(s), which says simple how good it is
   to be in any given state. the second is the advantage function a(a),
   which tells how much better taking a certain action would be compared
   to the others. we can then think of q as being the combination of v and
   a. more formally:

     q(s,a) =v(s) + a(a)

   the goal of dueling id25 is to have a network that separately computes
   the advantage and value functions, and combines them back into a single
   q-function only at the final layer. it may seem somewhat pointless to
   do this at first glance. why decompose a function that we will just put
   back together? the key to realizing the benefit is to appreciate that
   our id23 agent may not need to care about both value
   and advantage at any given time. for example: imagine sitting outside
   in a park watching the sunset. it is beautiful, and highly rewarding to
   be sitting there. no action needs to be taken, and it doesn   t really
   make sense to think of the value of sitting there as being conditioned
   on anything beyond the environmental state you are in. we can achieve
   more robust estimates of state value by decoupling it from the
   necessity of being attached to specific actions.

putting it all together

   [1*r2nbdo9njjvy_z0mqo5usq.png]
   simple block-world environment. the goal is to move the blue block to
   the green block while avoiding the red block.

   now that we have learned all the tricks to get the most out of our id25,
   let   s actually try it on a game environment! while the id25 we have
   described above could learn atari games with enough training, getting
   the network to perform well on those games takes at least a day of
   training on a powerful machine. for educational purposes, i have built
   a simple game environment which our id25 learns to master in a couple
   hours on a moderately powerful machine (i am using a gtx970). in the
   environment the agent controls a blue square, and the goal is to
   navigate to the green squares (reward +1) while avoiding the red
   squares (reward -1). at the start of each episode all squares are
   randomly placed within a 5x5 grid-world. the agent has 50 steps to
   achieve as large a reward as possible. because they are randomly
   positioned, the agent needs to do more than simply learn a fixed path,
   as was the case in the frozenlake environment from [19]tutorial 0.
   instead the agent must learn a notion of spatial relationships between
   the blocks. and indeed, it is able to do just that!
   [20]awjuliani/deeprl-agents
   deeprl-agents - a set of deep id23 agents implemented
   in tensorflow.github.com

   the game environment outputs 84x84x3 color images, and uses function
   calls as similar to the openai gym as possible. in doing so, it should
   be easy to modify this code to work on any of the openai atari games. i
   encourage those with the time and computing resources necessary to try
   getting the agent to perform well in an atari game. the hyperparameters
   may need some tuning, but it is definitely possible. good luck!
     __________________________________________________________________

   if this post has been valuable to you, please consider [21]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[22]arthur juliani, or on twitter
   [23]@awjliani.
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. [24]part 0         id24 agents
    2. [25]part 1         two-armed bandit
    3. [26]part 1.5         contextual bandits
    4. [27]part 2         policy-based agents
    5. [28]part 3         model-based rl
    6. part 4         deep q-networks and beyond
    7. [29]part 5         visualizing an agent   s thoughts and actions
    8. [30]part 6         partial observability and deep recurrent q-networks
    9. [31]part 7         action-selection strategies for exploration
   10. [32]part 8         asynchronous actor-critic agents (a3c)

     * [33]machine learning
     * [34]artificial intelligence
     * [35]deep learning
     * [36]computer science
     * [37]robotics

   (button)
   (button)
   (button) 2.4k claps
   (button) (button) (button) 54 (button) (button)

     (button) blockedunblock (button) followfollowing
   [38]go to the profile of arthur juliani

[39]arthur juliani

   deep learning [40]@unity3d

     * (button)
       (button) 2.4k
     * (button)
     *
     *

   [41]go to the profile of arthur juliani
   never miss a story from arthur juliani, when you sign up for medium.
   [42]learn more
   never miss a story from arthur juliani
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8438a3e2b8df
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@awjuliani?source=post_header_lockup
  10. https://medium.com/@awjuliani
  11. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0#.a5pppei4l
  12. http://www.davidqiu.com:8888/research/nature14236.pdf
  13. http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003963
  14. https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#convolution2d
  15. https://arxiv.org/pdf/1509.02971.pdf
  16. http://www.davidqiu.com:8888/research/nature14236.pdf
  17. http://www.aaai.org/ocs/index.php/aaai/aaai16/paper/download/12389/11847
  18. http://arxiv.org/pdf/1511.06581.pdf
  19. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  20. https://github.com/awjuliani/deeprl-agents/blob/master/double-dueling-id25.ipynb
  21. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  22. https://medium.com/@awjuliani
  23. https://twitter.com/awjuliani
  24. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  25. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  26. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c
  27. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  28. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  29. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.kdgfgy7k8
  30. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  31. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  32. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  33. https://medium.com/tag/machine-learning?source=post
  34. https://medium.com/tag/artificial-intelligence?source=post
  35. https://medium.com/tag/deep-learning?source=post
  36. https://medium.com/tag/computer-science?source=post
  37. https://medium.com/tag/robotics?source=post
  38. https://medium.com/@awjuliani?source=footer_card
  39. https://medium.com/@awjuliani
  40. http://twitter.com/unity3d
  41. https://medium.com/@awjuliani
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://github.com/awjuliani/deeprl-agents/blob/master/double-dueling-id25.ipynb
  45. https://medium.com/p/8438a3e2b8df/share/twitter
  46. https://medium.com/p/8438a3e2b8df/share/facebook
  47. https://medium.com/p/8438a3e2b8df/share/twitter
  48. https://medium.com/p/8438a3e2b8df/share/facebook
