   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

simple id23 with tensorflow part 5: visualizing an agent   s
thoughts and actions

   [9]go to the profile of arthur juliani
   [10]arthur juliani (button) blockedunblock (button) followfollowing
   sep 9, 2016
   [1*byjuaytlyzuxvegddclwga.jpeg]

   in this post of my id23 series, i want to further
   explore the kinds of representations that our neural agent learns
   during the training process. while getting a high score, or
   accomplishing a specified task is what we often want our neural agents
   to be capable of, it is just as important to understand how, and even
   more critically, why that agent is behaving in a certain way. in order
   to make the learning process a little more transparent, i built a d3.js
   powered web interface that presents various kinds of information about
   our agent as it learns. i call it the id23 control
   center. in this post i will use it to provide a little more insight
   into the how and why of an rl agent.

the control center interface

   [1*5funksdcbeivst5uhspvtg.png]
   the id23 control center interface.

   the control center was designed with the purpose of allowing the user
   to track the performance of an agent in realtime as it learns to
   perform a task. on the left of the interface, the episode length and
   reward over time are tracked and updated dynamically. the right
   displays an animated gif of a sample training episode, along with the
   advantage and value functions being computed by the agent at every step
   of the episode.

   the interface is currently specific to the neural network and task that
   i described in [11]my last post. it is a double-dueling-id25, and the
   environment is a simple gridworld. the rules of that gridworld are as
   follows: the agent controls the blue square and can move either up,
   down, left, or right. the goal is to get to the green square as quickly
   as possible, while avoiding the red square. the green square provides
   +1 reward, the red square -1 reward, and each step is -0.1 reward. at
   the beginning of each episode, the three squares are randomly placed on
   the grid.

   iframe: [12]/media/583c3de27949a283ed0cb6b28b8500b9?postid=4f27b134bb2a

   display of a neural agent   s actions and thoughts during an episode of
   training.

   the dd-id25 neural agent processes two separate streams of information
   as it experiences the gridworld: an advantage stream and a value
   stream. the advantage stream represent how good the network thinks it
   is to take each action, given the current state it is in. the value
   stream represents how good the network thinks it is to be in a given
   state, regardless of possible action. with the control center, we can
   watch as the network learns to correctly predict the value of its state
   and actions over time. as the training process proceeds, it goes from
   seemingly random values to accurately interpreting certain actions as
   the most advantageous. we can think of this visualization as providing
   a portal into the    thought process    of our agent. does it know that it
   is in a good position when it is in a good position? does it know that
   going down was a good thing to do when it went down? this can give us
   the insights needed to understand why our agent might not be performing
   ideally as we train it under different circumstances in different
   environments.

getting inside our agent   s head

   not only can we use the interface to explore how the agent does during
   training, we can also use it for testing and debugging our fully
   trained agents. for example, after training our agent to solve the
   simple 3x3 gridworld described above, we can provide it with some
   special test scenarios it had never encountered during the training
   process to evaluate whether it really is representing experience as we
   would expect it to.

   below is an example of the agent performing a modified version of the
   task with only green squares. as you can see, as the agent gets closer
   to the green squares the value estimate increases just as we would
   expect. it also has high estimates of the advantage for taking actions
   that get it closer to the green goals.

   iframe: [13]/media/5189cd59b7cb3817919c965d3367be60?postid=4f27b134bb2a

   for the next test we can invert the situation, giving the agent a world
   in which there were only two red squares. it didn   t like this very
   much. as you can see below, the agent attempts to stay away from either
   square, resulting in behavior where it goes back and forth for a long
   period of time. notice how the value estimate decreases as the agent
   approaches the red squares.

   iframe: [14]/media/f197fbeeb965c56e3dd083fae02c1a5c?postid=4f27b134bb2a

   finally, i provided the agent with a bit of an existential challenge.
   instead of augmenting the kind of goals present, i removed them all. in
   this scenario, the blue square is by itself in the environment, with no
   other objects. without a goal to move towards, the agent moves around
   seemingly at random, and the value estimates are likewise seemingly
   meaningless. what would [15]camus say?

   iframe: [16]/media/12b377832afea18cecbdd15875d9e9db?postid=4f27b134bb2a

   taken together, these three experiments provide us with evidence that
   our agent is indeed responding to the environment as we would
   intuitively expect. these kinds of checks are essential to make when
   designing any id23 agent. if we aren   t careful about
   the expectations we built into the agent itself and the reward
   structure of the environment, we can easily end up with situations
   where the agent doesn   t properly learn the task, or at least doesn   t
   learn it as we   d expect. in the gridworld for example, taking a step
   results in a -0.1 reward. this wasn   t always the case though.
   originally there was no such penalty, and the agent would learn to move
   to the green square, but do so after an average of about 50 steps! it
   had no    reason    to hurry, to the goal, so it didn   t. by penalizing each
   step even a small amount, the agent is able to quickly learn the
   intuitive behavior of moving directly to the green goal. this reminds
   us of just how subconscious our own reward structures as humans often
   are. while we may explicitly only think of the green as being rewarding
   and the red as being punishing, we are subconsciously constraining our
   actions by a desire to finish quickly. when designing rl agents, we
   need ensure that we are making their reward structures as rich as ours.

using the control center

   if you want to play with a working version of the control center
   without training an agent yourself, just [17]follow this link
   (currently requires google chrome). the agent   s performance you will
   see was pretrained on the gridworld task for 40,000 episodes. you can
   click the timeline on the left to look at an example episode from any
   point in training. the earlier episodes clearly show the agent failing
   to properly interpret the task, but by the end of training the agent
   almost always goes straight to the goal.

   the control center is a piece of software i plan to continue to develop
   as i work more with various id23 algorithms. it is
   currently hard-coded to certain specifics of the gridworld and dd-id25
   described in [18]part 4, but if you are interested in using the
   interface for your own projects, feel free to [19]fork it on github,
   and adjust/adapt it to your particular needs as you see fit. hopefully
   it can provide new insights into the internal life of your learning
   algorithms too!
     __________________________________________________________________

   if this post has been valuable to you, please consider [20]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[21]arthur juliani, or on twitter
   [22]@awjliani.
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. [23]part 0         id24 agents
    2. [24]part 1         two-armed bandit
    3. [25]part 1.5         contextual bandits
    4. [26]part 2         policy-based agents
    5. [27]part 3         model-based rl
    6. [28]part 4         deep q-networks and beyond
    7. part 5         visualizing an agent   s thoughts and actions
    8. [29]part 6         partial observability and deep recurrent q-networks
    9. [30]part 7         action-selection strategies for exploration
   10. [31]part 8         asynchronous actor-critic agents (a3c)

     * [32]machine learning
     * [33]artificial intelligence
     * [34]deep learning
     * [35]robotics
     * [36]computer science

   (button)
   (button)
   (button) 426 claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [37]go to the profile of arthur juliani

[38]arthur juliani

   deep learning [39]@unity3d

     * (button)
       (button) 426
     * (button)
     *
     *

   [40]go to the profile of arthur juliani
   never miss a story from arthur juliani, when you sign up for medium.
   [41]learn more
   never miss a story from arthur juliani
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4f27b134bb2a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@awjuliani?source=post_header_lockup
  10. https://medium.com/@awjuliani
  11. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.nw6seawjg
  12. https://medium.com/media/583c3de27949a283ed0cb6b28b8500b9?postid=4f27b134bb2a
  13. https://medium.com/media/5189cd59b7cb3817919c965d3367be60?postid=4f27b134bb2a
  14. https://medium.com/media/f197fbeeb965c56e3dd083fae02c1a5c?postid=4f27b134bb2a
  15. https://en.wikipedia.org/wiki/albert_camus
  16. https://medium.com/media/12b377832afea18cecbdd15875d9e9db?postid=4f27b134bb2a
  17. http://awjuliani.github.io/center/
  18. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.i2zpbmre8
  19. https://github.com/awjuliani/rl-cc
  20. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  21. https://medium.com/@awjuliani
  22. https://twitter.com/awjuliani
  23. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  24. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  25. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c
  26. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  27. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  28. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.i2zpbmre8
  29. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk
  30. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  31. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  32. https://medium.com/tag/machine-learning?source=post
  33. https://medium.com/tag/artificial-intelligence?source=post
  34. https://medium.com/tag/deep-learning?source=post
  35. https://medium.com/tag/robotics?source=post
  36. https://medium.com/tag/computer-science?source=post
  37. https://medium.com/@awjuliani?source=footer_card
  38. https://medium.com/@awjuliani
  39. http://twitter.com/unity3d
  40. https://medium.com/@awjuliani
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://medium.com/p/4f27b134bb2a/share/twitter
  44. https://medium.com/p/4f27b134bb2a/share/facebook
  45. https://medium.com/p/4f27b134bb2a/share/twitter
  46. https://medium.com/p/4f27b134bb2a/share/facebook
