   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]emergent // future
     * [9]machine learning trends
     * [10]algorithm economy
     * [11]why deep learning matters
     * [12]algorithmia
     __________________________________________________________________

simple id23 with tensorflow part 6: partial observability
and deep recurrent q-networks

   [13]go to the profile of arthur juliani
   [14]arthur juliani (button) blockedunblock (button) followfollowing
   oct 7, 2016
   [1*jegpswpqfp3jzvwqj8uvya.jpeg]

   in this installment of my simple rl series, i want to introduce the
   concept of partial observability and demonstrate how to design neural
   agents which can successfully deal with it. as always, in addition to a
   theoretical discussion of the problem, i have included a tensorflow
   implementation of a neural agent which can solve this class of
   problems.

   for us humans, having access to a limited and changing world is a
   universal aspect of our shared experience. despite our partial access
   to the world, we are able to solve all sorts of challenging problems in
   the course of going about our daily lives. in contrast, the neural
   agents we have discussed so far in this tutorial series are ill
   equipped to handle partial observability.

the problem of partial observability

   when we think about the kinds of environments used until this point to
   train our networks, the agent has had access to all the information
   about the environment it might need in order to take an optimal action.
   take for example the gridworld used in [15]tutorials 4 [16]& 5 of this
   series:
   [1*jr960o88ve2vwgtvhryvma.gif]
   a fully observable mdp. the goal of the game is to move the blue block
   to as many green blocks as possible in 50 steps while avoiding red
   blocks. when the blue block moves to a green or red block, that other
   block is moved to a new random place in the environment. green blocks
   provide +1 reward, while red blocks provide -1 reward.

   because the entire world is visible at any moment (and nothing moves
   aside from the agent), a single frame of this environment gives the
   agent all it needs to know in order to maximize its reward.
   environments which follow a structure where a given state conveys
   everything the agent needs to act optimally are called [17]markov
   decision processes (mdps).

   while mdps provide a nice formalism, almost all real world problems
   fail to meet this standard. take for example your field of view at this
   very moment. can you see what is behind you? this limited perspective
   on the visual world is almost always the default for humans and other
   animals. even if we were to have 360 degree vision, we may still not
   know what is on the other side of a wall just beyond us. information
   outside our view is often essential to making decisions regarding the
   world.

   in addition to being spatially limited, information available at a
   given moment is also often temporally limited. when looking at a photo
   of a ball being thrown between two people, the lack of motion may make
   us unable to determine the direction and speed of the ball. in games
   like pong, not only the position of the ball, but also it   s direction
   and speed are essential to making the correct decisions.
   [1*vyrk-gzrb-hu99oejjkmmw.jpeg]
   is the ball going from the child to the baseball player, or the other
   way around?

   environments which present themselves in a limited way to the agent are
   referred to as partially observable id100 (pomdps).
   while they are trickier to solve than their fully observable
   counterparts, understanding them is essential to solving most realistic
   tasks.

making sense of a limited, changing world

   how can we build a neural agent which still functions well in a
   partially observable world? the key is to give the agent a capacity for
   temporal integration of observations. the intuition behind this is
   simple: if information at a single moment isn   t enough to make a good
   decision, then enough varying information over time probably is.
   revisiting the photo example of the thrown ball a single image of a
   ball in motion tells us nothing about its movements, but two images in
   sequence allows us to discern the direction of movement. a longer
   sequence might even allow us to make sense of the speed of the ball.
   the same principle can be applied to problems where there is a limited
   field of view. if you can   t see behind you, by turning around you can
   integrate the forward and backward views over time and get a complete
   picture of the world with which to act upon.

   within the context of id23, there are a number of
   possible ways to accomplish this temporal integration. the solution
   taken by [18]deepmind in their original paper on deep q-networks was to
   stack the frames from the atari simulator. instead of feeding the
   network a single frame at a time, they used an external frame buffer
   which kept the last four frames of the game in memory and fed this to
   the neural network. this approach worked relatively well for the simple
   games they employed, but it isn   t ideal for a number of reasons. the
   first is that it isn   t necessarily biologically plausible. when light
   hits our retinas, it does it at a single moment. there is no way for
   light to be stored up and passed all at once to an eye. secondly, by
   using blocks of 4 frames as their state, the experience buffer used
   needed to be much larger to accommodate the larger stored states. this
   makes the training process require a larger amount of potentially
   unnecessary memory. lastly, we may simply need to keep things in mind
   that happened much earlier than would be feasible to capture with
   stacking frames. sometimes an event hundreds of frames earlier might be
   essential to deciding what to do at the current moment. we need a way
   for our agent to keep events in mind more robustly.
   [1*kvg4opnzx2pflx9qymihiw.png]
   the approach used by [19]deepmind for dealing with partial
   observability.

recurrent neural networks

   all of these issues can be solved by moving the temporal integration
   into the agent itself. this is accomplished by utilizing a recurrent
   block in our neural agent. you may have heard of recurrent neural
   networks, and their capacity to learn temporal dependencies. this has
   been used popularly for the purpose of text generation, where groups
   have trained id56s to reproduce everything from [20]barack obama
   speeches to [21]freeform poetry. andrej karpathy has a [22]great post
   outlining id56s and their capacities, which i highly recommend. thanks
   to the high-level nature of tensorflow, we are free to treat the id56 as
   somewhat of a black-box that we simply plug into our existing deep
   q-network.

   by utilizing a recurrent block in our network, we can pass the agent
   single frames of the environment, and the network will be able to
   change its output depending on the temporal pattern of observations it
   receives. it does this by maintaining a hidden state that it computes
   at every time-step. the recurrent block can feed the hidden state back
   into itself, thus acting as an augmentation which tells the network
   what has come before. the class of agents which utilize this recurrent
   network are referred to as [23]deep recurrent q-networks (drqn).
   [1*ax8qjjp-gdxa4dc3gp9lcq.png]
   the approach taken drqn which allows for neural agent to learn temporal
   patterns.

implementing in tensorflow

   in order to implement a deep recurrent q-network (drqn) architecture in
   tensorflow, we need to make a few modifications to our id25 described in
   [24]part 4 (see below for full implementation, or follow link
   [25]here).
     * the first change is to the agent itself. we will insert a lstm
       recurrent cell between the output of the last convolutional layer
       and the input into the split between the value and advantage
       streams. we can do this by utilizing the tf.nn.dynamic_id56 function
       and defining a tf.nn.id56_cell.lstmcell which is feed to the id56
       node. we also need to slightly alter the training process in order
       to send an empty hidden state to our recurrent cell at the
       beginning of each sequence.
     * the second main change needed will be to adjust the way our
       experience buffer stores memories. since we want to train our
       network to understand temporal dependencies, we can   t use random
       batches of experience. instead we need to be able to draw traces of
       experience of a given length. in this implementation, our
       experience buffer will store entire episodes, and randomly draw
       traces of 8 steps from a random batch of episodes. by doing this we
       both retain our random sampling as well as ensure each trace of
       experiences actually follows from one another.
     * finally, we will be utilizing a technique [26]developed by a group
       at carnegie mellon who recently used drqn to train a neural network
       to play the first person shooter game doom. instead of sending all
       the gradients backwards when training their agent, they sent only
       the last half of the gradients for a given trace. we can do this by
       simply masking the loss for the first half of each trace in a
       batch. i suggest reading their paper for a more thorough
       explanation of this, but they found it improved performance by only
       sending more meaningful information through the network. in fact,
       it worked so well that the agent was able (with a few other tricks)
       to play doom at a convincing level.

   iframe: [27]/media/82927370481670be29aa42090b0d6fdc?postid=68463e9aeefc

   a drqn playing doom. for more information, see
   [28]https://arxiv.org/abs/1609.05521

the limited gridworld

   for this tutorial however i   d like to work with something a little less
   flashy, though hopefully more informative. recall our gridworld, where
   everything was visible to the agent at any moment. by simply limiting
   the agent   s view of the environment, we can turn our mdp into a pomdp.
   in this new version of the gridworld, the agent can only see a single
   block around it in any direction, whereas the environment itself
   contains 9x9 blocks. additional changes are as follows: each episode is
   fixed at 50 steps, there are four green and two red squares, and when
   the agent moves to a red or green square, a new one is randomly placed
   in the environment to replace it. what are the consequences of this?
   [1*0tfwowwu8zpqbw26vqxasa.gif]
   in the limited gridworld, the agent can only see 3x3 area around it,
   whereas the environment has 9x9 blocks total (see image at beginning of
   article).

   if we attempt to use our id25 as described in parts 4 and 5 of this
   series, we find that it performs relatively poorly, never achieving
   more than an average of 2.3 cumulative reward after 10,000 training
   episodes.
   [1*2luwhi7sxocxd3nq7xa9nw.png]
   average reward over time for id25 agent in limited gridworld.

   the problem is that the agent has no way of remembering where it has
   been or what it has seen. if two areas look the same, then the agent
   can do nothing but react in exactly the same way to them, even if they
   are in different parts of the environment. now let   s look at how our
   drqn does in the same limited environment over time.
   [1*rihuslrkkdyeichezyysjg.png]
   average reward over time for drqn agent in limited gridworld.

   by allowing for a temporal integration of information, the agent learns
   a sense of spatial location that is able to augment its observation at
   any moment, and allow the agent to receive a larger reward each
   episode. below is the ipython notebook where this drqn agent is
   implemented. feel free to replicate the results yourself, and play with
   the hyperparameters. different settings for many of them may provide
   greater performance for your particular task.
   [29]awjuliani/deeprl-agents
   deeprl-agents - a set of deep id23 agents implemented
   in tensorflow.github.com

   with this code you have everything you need to train a drqn that can go
   out into the messy world and solve problems with partial observability!
     __________________________________________________________________

   if this post has been valuable to you, please consider [30]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[31]arthur juliani, or on twitter
   [32]@awjliani.
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. [33]part 0         id24 agents
    2. [34]part 1         two-armed bandit
    3. [35]part 1.5         contextual bandits
    4. [36]part 2         policy-based agents
    5. [37]part 3         model-based rl
    6. [38]part 4         deep q-networks and beyond
    7. [39]part 5         visualizing an agent   s thoughts and actions
    8. part 6         partial observability and deep recurrent q-networks
    9. [40]part 7         action-selection strategies for exploration
   10. [41]part 8         asynchronous actor-critic agents (a3c)

     * [42]machine learning
     * [43]artificial intelligence
     * [44]neural networks
     * [45]robotics
     * [46]deep learning

   (button)
   (button)
   (button) 1.6k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [47]go to the profile of arthur juliani

[48]arthur juliani

   deep learning [49]@unity3d

     (button) follow
   [50]emergent // future

[51]emergent // future

   exploring frontier technology through the lens of artificial
   intelligence, data science, and the shape of things to come

     * (button)
       (button) 1.6k
     * (button)
     *
     *

   [52]emergent // future
   never miss a story from emergent // future, when you sign up for
   medium. [53]learn more
   never miss a story from emergent // future
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/68463e9aeefc
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc&source=--------------------------nav_reg&operation=register
   8. https://medium.com/emergent-future?source=logo-lo_j36lc27mqeky---d7fff85024c6
   9. https://medium.com/emergent-future/machine-learning-trends-and-the-future-of-artificial-intelligence-2016-15c15cd6c129
  10. https://medium.com/emergent-future/how-the-algorithm-economy-and-containers-are-changing-the-way-we-build-and-deploy-apps-today-4ecdbb59318d
  11. https://medium.com/emergent-future/why-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4
  12. https://algorithmia.com/
  13. https://medium.com/@awjuliani?source=post_header_lockup
  14. https://medium.com/@awjuliani
  15. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.79papzwoa
  16. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.n7lxg8qw0
  17. https://en.wikipedia.org/wiki/markov_decision_process
  18. http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html
  19. http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html
  20. https://medium.com/@samim/obama-id56-machine-generated-political-speeches-c8abd18a2ea0#.edu7q0lqw
  21. http://neuralnetpoetry.blogspot.com/
  22. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  23. https://arxiv.org/abs/1507.06527
  24. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.79papzwoa
  25. https://github.com/awjuliani/deeprl-agents/blob/master/deep-recurrent-q-network.ipynb
  26. https://arxiv.org/abs/1609.05521
  27. https://medium.com/media/82927370481670be29aa42090b0d6fdc?postid=68463e9aeefc
  28. https://arxiv.org/abs/1609.05521
  29. https://github.com/awjuliani/deeprl-agents/blob/master/deep-recurrent-q-network.ipynb
  30. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  31. https://medium.com/@awjuliani
  32. https://twitter.com/awjuliani
  33. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  34. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  35. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c#.uzs1axw0s
  36. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  37. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  38. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.i2zpbmre8
  39. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a
  40. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf
  41. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  42. https://medium.com/tag/machine-learning?source=post
  43. https://medium.com/tag/artificial-intelligence?source=post
  44. https://medium.com/tag/neural-networks?source=post
  45. https://medium.com/tag/robotics?source=post
  46. https://medium.com/tag/deep-learning?source=post
  47. https://medium.com/@awjuliani?source=footer_card
  48. https://medium.com/@awjuliani
  49. http://twitter.com/unity3d
  50. https://medium.com/emergent-future?source=footer_card
  51. https://medium.com/emergent-future?source=footer_card
  52. https://medium.com/emergent-future
  53. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  55. https://github.com/awjuliani/deeprl-agents/blob/master/deep-recurrent-q-network.ipynb
  56. https://medium.com/p/68463e9aeefc/share/twitter
  57. https://medium.com/p/68463e9aeefc/share/facebook
  58. https://medium.com/p/68463e9aeefc/share/twitter
  59. https://medium.com/p/68463e9aeefc/share/facebook
