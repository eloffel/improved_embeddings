   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]emergent // future
     * [9]machine learning trends
     * [10]algorithm economy
     * [11]why deep learning matters
     * [12]algorithmia
     __________________________________________________________________

simple id23 with tensorflow part 7: action-selection
strategies for exploration

   [13]go to the profile of arthur juliani
   [14]arthur juliani (button) blockedunblock (button) followfollowing
   nov 14, 2016
   [1*k7krwrfvyg2pupebxcvbog.jpeg]

   in this entry of my rl series i would like to focus on the role that
   exploration plays in an agent   s behavior. i will go over a few of the
   commonly used approaches to exploration which focus on
   action-selection, and show their comparative strengths and weaknesses,
   as well as demonstrate how to implement each using tensorflow. the
   methods are discussed here in the context of a q-network, but can be
   applied to policy networks as well. to make things more intuitive, i
   also built an interactive visualization to provide a better sense of
   how each exploration strategy works (it uses simulated q-values, so
   there is no actual neural network running in the browser         though
   [15]such things [16]do exist!). since i can   t embed it in medium, i
   have linked to it [17]here, and below. i highly recommend playing with
   it as you read through the post. let   s get started!
   [18]id23 exploration
   interactive visualization of action selection strategies:
   awjuliani.github.io

why explore?

   the first question one may ask is: why do we need exploration at all?
   the problem can be framed as one of obtaining representative training
   data. in order for an agent to learn how to deal optimally with all
   possible states in an environment, it must be exposed to as many of
   those states as possible. unlike in traditional supervised learning
   settings however, the agent in a id23 problem only
   has access to the environment through its own actions. as a result,
   there emerges a chicken and egg problem: an agent needs the right
   experiences to learn a good policy, but it also needs a good policy to
   obtain those experiences.

   from this problem has emerged an entire subfield within reinforcement
   learning that has attempted to develop techniques for meaningfully
   balancing the exploration and exploitation tradeoff. ideally, such an
   approach should encourage exploring an environment until the point that
   it has learned enough about it to make informed decisions about optimal
   actions. there are a number of frequently used approaches to
   encouraging exploration. in this post i want to go over some of the
   basic approaches related to the selection of actions. in a later post
   in this series i will cover more advanced methods which encourage
   exploration through the use of intrinsic motivation.

greedy approach

   [1*yvh2ue_lupguzgnojguzqa.png]
   each value corresponds to the q-value for a given action at a random
   state in an environment. the height of the light blue bar corresponds
   to the id203 of choosing a given action. the dark blue bar
   corresponds to a chosen action. to try an interactive version,
   go [19]here.

   explanation: all id23 algorithms seek to maximize
   reward over time. a naive approach to ensuring the optimal action is
   taken at any given time is to simply choose the action which the agent
   expects to provide the greatest reward. this is referred to as a greedy
   method. taking the action which the agent estimates to be the best at
   the current moment is an example of exploitation: the agent is
   exploiting its current knowledge about the reward structure of the
   environment to act. this approach can be thought of as providing little
   to no exploratory potential.

   shortcomings: the problem with a greedy approach is that it almost
   universally arrives at a suboptimal solution. imagine a simple
   two-armed bandit problem (for an introduction to multi-armed bandits,
   see [20]part 1 of this series). if we suppose one arm gives a reward of
   1 and the other arm gives a reward of 2, then if the agent   s parameters
   are such that it chooses the former arm first, then regardless of how
   complex a neural network we utilize, under a greedy approach it will
   never learn that the latter action is more optimal.

   implementation:

   iframe: [21]/media/e41f703479e09e371760cc6209fea9cf?postid=d3a97b7cceaf

random approach

   [1*oi0nq0a3zyszu5_slyhufg.png]
   each value corresponds to the q-value for a given action at a random
   state in an environment. the height of the light blue bars correspond
   to the id203 of choosing a given action. the dark blue bar
   corresponds to a chosen action. to try an interactive version,
   go [22]here.

   explanation: the opposite approach to greedy selection is to simply
   always take a random action.

   shortcomings: only in circumstances where a random policy is optimal
   would this approach be ideal. however it can be useful as an initial
   means of sampling from the state space in order to fill an experience
   buffer when using id25.

   implementation:

   iframe: [23]/media/79381c63a8635bbd82a6bb58747f06cf?postid=d3a97b7cceaf

  -greedy approach

   [1*d2clnghy5pi9oy1nhfmxsq.png]
   each value corresponds to the q-value for a given action at a random
   state in an environment. the height of the light blue bars correspond
   to the id203 of choosing a given action. the dark blue bar
   corresponds to a chosen action. to try an interactive version,
   go [24]here.

   explanation: a simple combination of the greedy and random approaches
   yields one of the most used exploration strategies:   -greedy. in this
   approach the agent chooses what it believes to be the optimal action
   most of the time, but occasionally acts randomly. this way the agent
   takes actions which it may not estimate to be ideal, but may provide
   new information to the agent. the    in   -greedy is an adjustable
   parameter which determines the id203 of taking a random, rather
   than principled, action. due to its simplicity and surprising power,
   this approach has become the defacto technique for most recent
   id23 algorithms, including id25 and its variants.

   adjusting during training: at the start of the training process the e
   value is often initialized to a large id203, to encourage
   exploration in the face of knowing little about the environment. the
   value is then annealed down to a small constant (often 0.1), as the
   agent is assumed to learn most of what it needs about the environment.

   shortcomings: despite the prevalence of usage that it enjoys, this
   method is far from optimal, since it takes into account only whether
   actions are most rewarding or not.

   implementation:

   iframe: [25]/media/f4c5a050c7813b9fb51b38fa6af9d127?postid=d3a97b7cceaf

boltzmann approach

   [1*tzv1wpnmlnw_irheyepxhq.png]
   each value corresponds to the q-value for a given action at a random
   state in an environment. the height of the light blue bars correspond
   to the id203 of choosing a given action. the dark blue bar
   corresponds to a chosen action. to try an interactive version,
   go [26]here.

   explanation: in exploration, we would ideally like to exploit all the
   information present in the estimated q-values produced by our network.
   boltzmann exploration does just this. instead of always taking the
   optimal action, or taking a random action, this approach involves
   choosing an action with weighted probabilities. to accomplish this we
   use a softmax over the networks estimates of value for each action. in
   this case the action which the agent estimates to be optimal is most
   likely (but is not guaranteed) to be chosen. the biggest advantage over
   e-greedy is that information about likely value of the other actions
   can also be taken into consideration. if there are 4 actions available
   to an agent, in e-greedy the 3 actions estimated to be non-optimal are
   all considered equally, but in boltzmann exploration they are weighed
   by their relative value. this way the agent can ignore actions which it
   estimates to be largely sub-optimal and give more attention to
   potentially promising, but not necessarily ideal actions.

   adjusting during training: in practice we utilize an additional
   temperature parameter (  ) which is annealed over time. this parameter
   controls the spread of the softmax distribution, such that all actions
   are considered equally at the start of training, and actions are
   sparsely distributed by the end of training.
   [1*176nhun1ccphofndvmgyhq.png]
   the boltzmann softmax equation.

   shortcomings: the underlying assumption made in boltzmann exploration
   is that the softmax over network outputs provides a measure of the
   agent   s confidence in each action. if action 2 is 0.7 and action 1 is
   0.2 the tempting interpretation is that the agent believes that action
   2 is 70% likely to be optimal, whereas action 1 is 20% likely to be
   optimal. in reality this isn   t the case. instead what the agent is
   estimating is a measure of how optimal the agent thinks the action is,
   not how certain it is about that optimality. while this measure can be
   a useful proxy, it is not exactly what would best aid exploration. what
   we really want to understand is the agent   s uncertainty about the value
   of different actions.

   implementation:

   iframe: [27]/media/25ab1b482cdba0f16caf7a3321bbdc25?postid=d3a97b7cceaf

bayesian approaches (w/ dropout)

   iframe: [28]/media/4344e241a0c4fdd2205af2f119329a87?postid=d3a97b7cceaf

   each value corresponds to the q-value for a given action at a random
   state in an environment. the height of the light blue bars correspond
   to the id203 of choosing a given action. the dark blue bar
   corresponds to a chosen action. additionally each change in value
   corresponds to a new sampling from a bayesian neural network using
   dropout. to try an interactive version, go [29]here.

   explanation: what if an agent could exploit its own uncertainty about
   its actions? this is exactly the ability that a class of neural network
   models referred to as bayesian neural networks (bnns) provide. unlike
   traditional neural network which act deterministically, bnns act
   probabilistically. this means that instead of having a single set of
   fixed weights, a bnn maintains a id203 distribution over possible
   weights. in a id23 setting, the distribution over
   weight values allows us to obtain distributions over actions as well.
   the variance of this distribution provides us an estimate of the
   agent   s uncertainty about each action.

   in practice however it is impractical to maintain a distribution over
   all weights. instead we can utilize dropout to simulate a probabilistic
   network. dropout is a technique where network activations are randomly
   set to zero during the training process in order to act as a
   regularizer. by repeatedly sampling from a network with dropout, we are
   able to obtain a measure of uncertainty for each action. when taking a
   single sample from a network with dropout, we are doing something that
   approximates sampling from a bnn. for more on the implications of using
   dropout for bnns, i highly recommend yarin gal   s [30]phd thesis on the
   topic.

   shortcomings: in order to get true uncertainty estimates, multiple
   samples are required, thus increasing computational complexity. in my
   own experiments however i have found it sufficient to sample only once,
   and use the noisy estimates provided by the network. in order to reduce
   the noise in the estimate, the dropout keep id203 is simply
   annealed over time from 0.1 to 1.0.

   implementation:

   iframe: [31]/media/c9548c0eed1da1a2ab4e34dc773b6cbb?postid=d3a97b7cceaf

comparison & full code

   [1*n25drtxskwmbmma9nezueg.png]

   i compared each of the approaches using a id25 trained on the cartpole
   environment available in the openai gym. the bayesian dropout and
   boltzmann methods proved most helpful, at least in my experiment. i
   encourage those interested to play around with the hyperparameters, as
   i am sure better performance can be gained from doing so. indeed,
   different approaches may be best depending on what hyperparameters are
   used. below is the full implementation of each method in tensorflow:

   iframe: [32]/media/0d8aa599a5ad6efac947d8823090c210?postid=d3a97b7cceaf

advanced approaches

   all of the methods discussed above deal with the selection of actions.
   there is another approach to exploration that deals with the nature of
   the reward signal itself. these approaches fall under the umbrella of
   intrinsic motivation, and there has been a lot of great work in this
   area. in a future post i will be exploring these approaches in more
   depth, but for those interested, here is a small selection of notable
   recent papers on the topic:
     * [33]variational information maximizing exploration
     * [34]incentivizing exploration in id23 with deep
       predictive models
     * [35]unifying count-based exploration and intrinsic motivation
     * [36]hierarchical deep id23: integrating temporal
       abstraction and intrinsic motivation
     __________________________________________________________________

   i hope that this survey of approaches has been helpful for those
   interested in learning how to improve exploration in their rl agents!
     __________________________________________________________________

   if this post has been valuable to you, please consider [37]donating to
   help support future tutorials, articles, and implementations. any
   contribution is greatly appreciated!

   if you   d like to follow my work on deep learning, ai, and cognitive
   science, follow me on medium @[38]arthur juliani, or on twitter
   [39]@awjliani.
     __________________________________________________________________

   more from my simple id23 with tensorflow series:
    1. [40]part 0         id24 agents
    2. [41]part 1         two-armed bandit
    3. [42]part 1.5         contextual bandits
    4. [43]part 2         policy-based agents
    5. [44]part 3         model-based rl
    6. [45]part 4         deep q-networks and beyond
    7. [46]part 5         visualizing an agent   s thoughts and actions
    8. [47]part 6         partial observability and deep recurrent q-networks
    9. part 7         action-selection strategies for exploration
   10. [48]part 8         asynchronous actor-critic agents (a3c)

     * [49]machine learning
     * [50]artificial intelligence
     * [51]deep learning
     * [52]neural networks
     * [53]robotics

   (button)
   (button)
   (button) 1.2k claps
   (button) (button) (button) 13 (button) (button)

     (button) blockedunblock (button) followfollowing
   [54]go to the profile of arthur juliani

[55]arthur juliani

   deep learning [56]@unity3d

     (button) follow
   [57]emergent // future

[58]emergent // future

   exploring frontier technology through the lens of artificial
   intelligence, data science, and the shape of things to come

     * (button)
       (button) 1.2k
     * (button)
     *
     *

   [59]emergent // future
   never miss a story from emergent // future, when you sign up for
   medium. [60]learn more
   never miss a story from emergent // future
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d3a97b7cceaf
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf&source=--------------------------nav_reg&operation=register
   8. https://medium.com/emergent-future?source=logo-lo_5dnkz3f8qxau---d7fff85024c6
   9. https://medium.com/emergent-future/machine-learning-trends-and-the-future-of-artificial-intelligence-2016-15c15cd6c129
  10. https://medium.com/emergent-future/how-the-algorithm-economy-and-containers-are-changing-the-way-we-build-and-deploy-apps-today-4ecdbb59318d
  11. https://medium.com/emergent-future/why-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4
  12. https://algorithmia.com/
  13. https://medium.com/@awjuliani?source=post_header_lockup
  14. https://medium.com/@awjuliani
  15. http://cs.stanford.edu/people/karpathy/convnetjs/
  16. http://playground.tensorflow.org/#activation=tanh&batchsize=10&dataset=circle&regdataset=reg-plane&learningrate=0.03&id173rate=0&noise=0&networkshape=4,2&seed=0.41200&showtestdata=false&discretize=false&perctraindata=50&x=true&y=true&xtimesy=false&xsquared=false&ysquared=false&cosx=false&sinx=false&cosy=false&siny=false&collectstats=false&problem=classification&initzero=false&hidetext=false
  17. http://awjuliani.github.io/exploration/index.html
  18. http://awjuliani.github.io/exploration/index.html
  19. http://awjuliani.github.io/exploration/index.html
  20. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149#.rq4f18od7
  21. https://medium.com/media/e41f703479e09e371760cc6209fea9cf?postid=d3a97b7cceaf
  22. http://awjuliani.github.io/exploration/index.html
  23. https://medium.com/media/79381c63a8635bbd82a6bb58747f06cf?postid=d3a97b7cceaf
  24. http://awjuliani.github.io/exploration/index.html
  25. https://medium.com/media/f4c5a050c7813b9fb51b38fa6af9d127?postid=d3a97b7cceaf
  26. http://awjuliani.github.io/exploration/index.html
  27. https://medium.com/media/25ab1b482cdba0f16caf7a3321bbdc25?postid=d3a97b7cceaf
  28. https://medium.com/media/4344e241a0c4fdd2205af2f119329a87?postid=d3a97b7cceaf
  29. http://awjuliani.github.io/exploration/index.html
  30. http://mlg.eng.cam.ac.uk/yarin/blog_2248.html
  31. https://medium.com/media/c9548c0eed1da1a2ab4e34dc773b6cbb?postid=d3a97b7cceaf
  32. https://medium.com/media/0d8aa599a5ad6efac947d8823090c210?postid=d3a97b7cceaf
  33. https://arxiv.org/abs/1605.09674
  34. https://arxiv.org/abs/1507.00814
  35. https://arxiv.org/abs/1606.01868
  36. https://arxiv.org/abs/1604.06057
  37. https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=v2r22dv4xsr5y&lc=us&item_name=arthur juliani's deep learning tutorials&currency_code=usd&bn=pp-donationsbf:btn_donatecc_lg.gif:nonhosted
  38. https://medium.com/@awjuliani
  39. https://twitter.com/awjuliani
  40. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-id24-with-tables-and-neural-networks-d195264329d0
  41. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149
  42. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c#.uzs1axw0s
  43. https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724
  44. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99
  45. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.i2zpbmre8
  46. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a
  47. https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.9djtshpqo
  48. https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw
  49. https://medium.com/tag/machine-learning?source=post
  50. https://medium.com/tag/artificial-intelligence?source=post
  51. https://medium.com/tag/deep-learning?source=post
  52. https://medium.com/tag/neural-networks?source=post
  53. https://medium.com/tag/robotics?source=post
  54. https://medium.com/@awjuliani?source=footer_card
  55. https://medium.com/@awjuliani
  56. http://twitter.com/unity3d
  57. https://medium.com/emergent-future?source=footer_card
  58. https://medium.com/emergent-future?source=footer_card
  59. https://medium.com/emergent-future
  60. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  62. http://awjuliani.github.io/exploration/index.html
  63. https://medium.com/p/d3a97b7cceaf/share/twitter
  64. https://medium.com/p/d3a97b7cceaf/share/facebook
  65. https://medium.com/p/d3a97b7cceaf/share/twitter
  66. https://medium.com/p/d3a97b7cceaf/share/facebook
