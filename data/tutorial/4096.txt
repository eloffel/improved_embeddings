    #[1]sigmoidal    feed [2]sigmoidal    comments feed [3]sigmoidal   
   natural language processing algorithms (nlp ai) comments feed
   [4]alternate [5]alternate

   [6]machine learning consulting and data science from sigmoidal.io
   machine learning consulting and data science from sigmoidal.io
   0

   [7]machine learning consulting and data science from sigmoidal.io
   machine learning consulting and data science from sigmoidal.io
     * blog

machine learning blog
       [8]see all entries
          + [9]natural language processing algorithms (nlp ai)
          + [10]artificial intelligence and machine learning for
            healthcare
          + [11]machine learning terms every manager should know
          + [12]machine learning for trading     topic overview
          + [13]deep learning for id161     going beyond image
            classification and regression
     * [14]company
     * use cases

ai business
use cases
          + [15]finance
            investment opportunities discovery
          + [16]trading
            ai choosing the most profitable portfolio
          + [17]ai in pharma
            finding drug side-effect reviews in social media
     * [18]contact us

   [19]schedule a call

natural language processing algorithms (nlp ai)


nlp ai     before deep learning era

   back in the days before the era     when a neural network was more of a
   scary, enigmatic mathematical curiosity than a powerful tool     there
   were surprisingly many relatively successful applications of classical
   mining algorithms in the natural language processing algorithms (nlp)
   domain. it seemed that problems like [20]spam filtering or [21]part of
   speech tagging could be solved using rather straightforward and
   interpretable models.

   but not every problem can be solved this way. simple models fail to
   adequately capture linguistic subtleties like context, idioms, or irony
   (though humans often fail at that one too). algorithms based on overall
   summarization (e.g. [22]bag-of-words) turned out to be not powerful
   enough to capture sequential nature of text , whereas [23]id165s
   struggled to model general context and suffered severely from a
   [24]curse of dimensionality. even [25]id48-based models had trouble
   overcoming these issues due to their their memorylessness.

first breakthrough     id97

   one of the main challenges in language analysis is the method of
   transforming text into numerical input, which makes modeling feasible.
   it is not a problem in id161 tasks due to the fact that in an
   image, each pixel is represented by three numbers depicting the
   saturations of three base colors. for many years, researchers tried
   numerous algorithms for finding so called embeddings, which refer, in
   general, to representing text as vectors. at first, most of these
   methods were based on counting words or short sequences of words
   (id165s).

   the initial approach to tackle this problem is one-hot encoding, where
   each word from the vocabulary is represented as a unique binary vector
   with only one nonzero entry. a simple generalization is to encode
   id165s (sequence of n consecutive words) instead of single words. the
   major disadvantage to this method is very high dimensionality, each
   vector has a size of the vocabulary (or even bigger in case of id165s)
   which makes modeling difficult. another drawback to this approach is
   lack of semantic information. this means that all vectors representing
   single words are equidistant. in this embedding, space synonyms are
   just as far from each other as completely unrelated words. using this
   kind of word representations unnecessarily makes tasks much more
   difficult as it forces your model to memorize particular words instead
   of trying to capture the semantics.
   id97 figure 1: id97 representations of words projected onto a
   two-dimensional space.

   the first major leap forward for natural language processing algorithm
   came in 2013 with the introduction of id97     a neural network based
   model used exclusively for producing embeddings. imagine starting from
   a sequence of words, removing the middle one, and having a model
   predict it only by looking at context words (i.e. continuous bag of
   words, [26]cbow). the alternative version of that model is asking to
   predict the context given the middle word ([27]skip-gram). this idea is
   counterintuitive because such model might be used in information
   retrieval tasks (a certain word is missing and the problem is to
   predict it using its context), but that   s rarely the case. instead, it
   turns out that if you initialize your embeddings randomly and then use
   them as learnable parameters in training cbow or a skip-gram model, you
   obtain a vector representation of each word that can be used for any
   task. those powerful representations emerge during training, because
   the model is forced to recognize words that appear in the same context.
   this way you avoid memorizing particular words, but rather convey
   semantic meaning of the word explained not by a word itself, but by its
   context.

   in 2014 stanford   s research group challenged id97 with a strong
   competitor: [28]glove. they proposed a different approach, arguing that
   the best way to encode semantic meaning of words in vectors is through
   global word-word co-occurrence matrix as opposed to local
   co-occurrences as in id97. as you can see in figure 2 the ratio of
   co-occurrence probabilities is able to discriminate words when compared
   to the context word. it is around 1 when both target words co-occur
   very often or very rarely with the context word. only when the context
   word co-occurs with one of the target words is the ratio either very
   small or very big. this is the intuition behind glove. the exact
   algorithm involves representing words as vectors in a way that their
   difference, multiplied by a context word, is equal to the ratio of the
   co-occurrence probabilities.

   figure 2: ratios of co-occurence probabilities semantically
   discriminating words     motivation behid glove


further improvements

   even though the new powerful id97 representation boosted the
   performance of many classical algorithms, there was still a need for a
   solution capable of capturing sequential dependencies in a text (both
   long- and short-term). the first concept for this problem was so-called
   vanilla recurrent neural networks (id56s). vanilla id56s take advantage
   of the temporal nature of text data by feeding words to the network
   sequentially while using the information about previous words stored in
   a hidden-state.
   id56 unrolled figure 3: a recurrent neural network. image courtesy of an
   excellent colah   s.

   these networks proved very effective in handling local temporal
   dependencies, but performed quite poorly when presented with long
   sequences. this failure was caused by the fact that after each time
   step, the content of the hidden-state was overwritten by the output of
   the network. to address this issue, computer scientists and researchers
   designed a new id56 architecture called long-short term memory
   ([29]lstm). lstm deals with the problem by introducing an extra unit in
   the network called a memory cell, a mechanism that is responsible for
   storing long term dependencies and several gates responsible for
   control of the information flow in the unit. how this works is at each
   time step, the forget gate generates a fraction which depicts an amount
   of memory cell content to forget. next, the input gate determines how
   much of the input will be added to the content of the memory cell.
   finally, the output gate decides how much of the memory cell content to
   generate as the whole unit   s output. all the gates act like regular
   neural network layers with learnable parameters, which means that over
   time, the network adapts and is better at deciding what kind of input
   is relevant for the task and what information can and should be
   forgotten.

   lstms have actually been around since late 1990s, but they are quite
   expensive computationally and memory wise, so it is only recently,
   thanks to remarkable advances in hardware, that it became feasible to
   train id137 in reasonable time. nowadays, there exist many
   variations of lstm such as [30]mlstm, which introduces multiplicative
   dependency on the input or [31]gru which, thanks to an intelligent
   simplification of the memory cell update mechanism, significantly
   decreased the number of trainable parameters.

   after a short while it became clear that these models significantly
   outperform classic approaches, but researchers were hungry for more.
   they started to study the astounding success of convolutional neural
   networks in id161 and wondered whether those concepts could
   be incorporated into nlp. it quickly turned out that a simple
   replacement of 2d filters (processing a small segment of the image,
   e.g. regions of 3  3 pixels) with 1d filters (processing a small part of
   the sentence, e.g. 5 consecutive words) made it possible. similarly to
   2d id98s, these models learn more and more abstract features as the
   network gets deeper with the first layer processing raw input and all
   subsequent layers processing outputs of its predecessor. of course, a
   single id27 (embedding space is usually around 300
   dimensions) carries much more information than a single pixel, which
   means that it not necessary to use such deep networks as in the case of
   images. you may think of it as the embedding doing the job supposed to
   be done by first few layers, so they can be skipped. those intuitions
   proved correct in experiments on various tasks. 1d id98s were much
   lighter and more accurate than id56s and could be trained even an order
   of magnitude faster due to an easier parallelization.

          convolutional neural networks, were first used to solve computer
     vision problems and remain state-of-the-art in that space. learn
     more about [32]their applications and capabilities here.

   despite incredible contributions made by id98s, the networks still
   suffered from several drawbacks. in a classic setup, a convolutional
   network consists of several convolutional layers which are responsible
   for creating so-called feature maps and a module transforming it into
   predictions. feature maps are essentially high level features extracted
   from text (or image) preserving the location where it emerged in the
   text (or image). the prediction module performs aggregating operations
   on feature maps and either ignores the location of the feature (fully
   convolutional networks) or more commonly: learns where particular
   features appear most often (fully connected modules). the problem with
   these approaches arises for example in the id53 task,
   where the model is supposed to produce the answer given the text and a
   question. in this case, it is difficult and often unnecessary to store
   all information carried by the text in a single text, as is done by
   classic prediction modules. instead, we would like to focus on a
   particle part of text where the most crucial information is stored for
   a particular question. this problem is addressed by [33]attention
   mechanism, which weighs parts of the text depending on what may be
   relevant based on the input. this approach has also been found useful
   for classic applications like text classification or translation. will
   attention transform the nlp field? llya sutskever, co-founder and
   research director of [34]openai stated in [35]an interview:

        i am very excited by the recently introduced id12, due
     to their simplicity and due to the fact that they work so well.
     although these models are new, i have no doubt that they are here to
     stay, and that they will play a very important role in the future of
     deep learning.   

     llya sutskever, openai

typical nlp problems

   there are a variety of language tasks that, while simple and
   second-nature to humans, are very difficult for a machine. the
   confusion is mostly due to linguistic nuances like irony and idioms.
   let   s take a look at some of the areas of nlp that researchers are
   trying to tackle (roughly in order of their complexity):

   the most common and possibly easiest one is id31. this
   is, essentially, determining the attitude or emotional reaction of a
   speaker/writer toward a particular topic (or in general). possible
   sentiments are positive, neutral, and negative. check out [36]this
   great article about using deep convolutional neural networks for
   gauging sentiment in tweets. another interesting experiment showed that
   a deep recurrent net could the learn sentiment [37]by accident.
   sentiment prediction figure 4: activation of a neuron from a net used
   to generate next character of text. it is clear that it learned the
   sentiment even though it was trained in an entirely unsupervised
   environment.

   a natural generalization of the previous case is document
   classification, where instead of assigning one of three possible flags
   to each article, we solve an ordinary classification problem. according
   to a comprehensive [38]comparison of algorithms, it is safe to say that
   deep learning is the way to go fortext classification.

natural language processing algorithms for machine translation

   now, we move on to the real deal: machine translation. machine
   translation has posed a serious challenge for quite some time. it is
   important to understand that this an entirely different task than the
   two previous ones we   ve discussed. for this task, we require a model to
   predict a sequence of words, instead of a label. machine translation
   makes clear what the fuss is all about with deep learning, as it has
   been an incredible breakthrough when it comes to sequential data. in
   this [39]blog post you can read more about how     yep, you guessed it    
   recurrent neural networks tackle translation, and [40]in this one you
   can learn about how they achieve state-of-the-art results.

   say you need an automatic text summarization model, and you want it to
   extract only the most important parts of a text while preserving all of
   the meaning. this requires an algorithm that can understand the entire
   text while focusing on the specific parts that carry most of the
   meaning. this problem is neatly solved by previously
   mentioned [41]attention mechanisms, which can be introduced as modules
   inside an [42]end-to-end solution.

   lastly, there is id53, which comes as close to artificial
   intelligence as you can get. for this task, not only does the model
   need to understand a question, but it is also required to have a full
   understanding of a text of interest and know exactly where to look to
   produce an answer. for a detailed explanation of a id53
   solution (using deep learning, of course), check out [43]this article.
   attention mechanism id56 figure 5: beautiful [44]visualization of an
   attention mechanism in a recurrent neural network trained to translate
   english to french.

   since deep learning offers vector representations for various kinds of
   data (e.g., text and images), you can build models to specialize in
   different domains. this is how researchers came up with [45]visual
   id53. the task is    trivial   : just answer a question about
   an image. sounds like a job for a 7-year-old, right? nonetheless, deep
   models are the first to produce any reasonable results without human
   supervision. results and a description of such a model are in [46]this
   paper.

                    starving for applications? get your hands dirty and
     [47]implement your chatbot using lstms.

id86

   you may have noticed that all of the above tasks share a common
   denominator. for id31 an article is always positive,
   negative or neutral. in document classification each example belongs to
   one class. this means that these problems belong to a family of
   problems called supervised learning. where the model is presented with
   an example and a correct value associated with it. things get tricky
   when you want your model to generate text.

   andrej karpathy provides a comprehensive review of how id56s tackle this
   problem in his excellent [48]blog post. he shows examples of deep
   learning used to generate new shakespeare novels or how to produce
   source code that seems to be written by a human, but actually doesn   t
   do anything. these are great examples that show how powerful such a
   model can be, but there are also real life business applications of
   these algorithms. imagine you want to target clients with ads and you
   don   t want them to be generic by copying and pasting the same message
   to everyone. there is definitely no time for writing thousands of
   different versions of it, so an ad generating tool may come in handy.

   id56s seem to perform reasonably well at producing text at a character
   level, which means that the network predicts consecutive letters (also
   spaces, punctuation and so on) without actually being aware of a
   concept of word. however, it turned out that those models really
   struggled with sound generation. that is because to produce a word you
   need only few letters, but when producing sound in high quality, with
   even 16khz sampling, there are hundreds or maybe even thousands points
   that form a spoken word. again, researchers turned to id98s and again
   with great success. mathematicians at deepmind developed a very
   sophisticated convolutional generative [49]wavenet model, which deals
   with a very large receptive field (length of the actual raw input)
   problem by using a so-called attrous convolutions, which increase the
   receptive field exponentially with each layer. this is currently the
   state-of-the-art model significantly outperforming all other available
   baselines, but is very expensive to use, i.e. it takes 90 seconds to
   generate 1 second of raw audio. this means that there is still a lot of
   room for improvement, but we   re definitely on the right track.

recap

   so, now you know. deep learning appeared in nlp relatively recently due
   to computational issues, and we needed to learn much more about neural
   networks to understand their capabilities. but once we did, it changed
   the game forever.
   tags: [50]natural language processing
   [51]share on facebook [52]share on twitter
     __________________________________________________________________

0 comments

leave a reply [53]cancel

   you must be [54]logged in to post a comment.

more recent stories

   11/28/2017 [55]machine learning terms every manager should know
   [56]read more

   10/10/2017 [57]deep learning chatbot     analysis and implementation
   [58]read more

   [59]trading chart
   11/03/2017 [60]machine learning for trading     topic overview [61]read
   more

   sigmoidal llc
   733 3rd avenue, 15th floor
   new york, ny, 10017
          [62](347) 779 2050
            [63][email protected]
     *
     *
     *
     *

   [software-5.jpg]

   so glad to see you sticking around!

   want to be the first one to receive the new stuff?

   enter your email address below and we'll send you the goodies straight
   to your inbox.
   ____________________
   (button) subscribe

   thank you for subscribing

   this means the world to us!

   spamming is not included! pinky promise.

   iframe: [64]https://www.googletagmanager.com/ns.html?id=gtm-ngj2l8w

references

   visible links
   1. https://sigmoidal.io/feed/
   2. https://sigmoidal.io/comments/feed/
   3. https://sigmoidal.io/boosting-your-solutions-with-nlp/feed/
   4. https://sigmoidal.io/wp-json/oembed/1.0/embed?url=https://sigmoidal.io/boosting-your-solutions-with-nlp/
   5. https://sigmoidal.io/wp-json/oembed/1.0/embed?url=https://sigmoidal.io/boosting-your-solutions-with-nlp/&format=xml
   6. https://sigmoidal.io/
   7. https://sigmoidal.io/
   8. https://sigmoidal.io/blog
   9. https://sigmoidal.io/boosting-your-solutions-with-nlp/
  10. https://sigmoidal.io/artificial-intelligence-and-machine-learning-for-healthcare/
  11. https://sigmoidal.io/machine-learning-terminology-explained-top-8-must-know-concepts/
  12. https://sigmoidal.io/machine-learning-for-trading/
  13. https://sigmoidal.io/dl-computer-vision-beyond-classification/
  14. https://sigmoidal.io/company/
  15. https://sigmoidal.io/use-case-investment-opportunities-discovery/
  16. https://sigmoidal.io/use-case-portfolio-analysis-ai-choosing-the-most-profitable-portfolio/
  17. https://sigmoidal.io/artificial-intelligence-pharma/
  18. https://sigmoidal.io/contact-us/
  19. https://sigmoidal.io/contact-us
  20. http://www.cs.ubbcluj.ro/~gabis/docdiplome/bayesian/000539771r.pdf
  21. http://www.aclweb.org/anthology/n09-2054
  22. https://en.wikipedia.org/wiki/bag-of-words_model
  23. https://en.wikipedia.org/wiki/id165
  24. https://en.wikipedia.org/wiki/curse_of_dimensionality
  25. https://en.wikipedia.org/wiki/hidden_markov_model
  26. http://iksinc.wordpress.com/tag/continuous-bag-of-words-cbow/
  27. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  28. https://nlp.stanford.edu/projects/glove/
  29. https://en.wikipedia.org/wiki/long_short-term_memory
  30. http://arxiv.org/abs/1609.07959
  31. http://arxiv.org/abs/1406.1078
  32. https://sigmoidal.io/dl-computer-vision-beyond-classification/
  33. https://arxiv.org/abs/1706.03762
  34. http://openai.com/
  35. https://www.re-work.co/blog/deep-learning-ilya-sutskever-google-openai
  36. http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8244338
  37. http://blog.openai.com/unsupervised-sentiment-neuron/
  38. http://arxiv.org/pdf/1703.01898.pdf
  39. http://medium.com/@ageitgey/--is-fun-part-5-language-translation-with---and-the-magic-of-sequences-2ace0acca0aa
  40. http://arxiv.org/pdf/1705.00861.pdf
  41. http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
  42. http://arxiv.org/pdf/1602.06023.pdf
  43. https://einstein.ai/research/state-of-the-art-deep-learning-model-for-question-answering
  44. http://distill.pub/2016/augmented-id56s/
  45. https://arxiv.org/abs/1705.03865
  46. http://arxiv.org/pdf/1705.06824.pdf
  47. https://sigmoidal.io/chatbots-for-b2c-and-deep-learning/
  48. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  49. https://deepmind.com/blog/wavenet-generative-model-raw-audio/
  50. https://sigmoidal.io/tag/natural-language-processing/
  51. https://www.facebook.com/sharer/sharer.php?u=https://sigmoidal.io/boosting-your-solutions-with-nlp/
  52. https://twitter.com/share?url=https://sigmoidal.io/boosting-your-solutions-with-nlp/
  53. https://sigmoidal.io/boosting-your-solutions-with-nlp/#respond
  54. https://sigmoidal.io/wp-login.php?redirect_to=https://sigmoidal.io/boosting-your-solutions-with-nlp/
  55. https://sigmoidal.io/machine-learning-terminology-explained-top-8-must-know-concepts/
  56. https://sigmoidal.io/machine-learning-terminology-explained-top-8-must-know-concepts/
  57. https://sigmoidal.io/chatbots-for-b2c-and-deep-learning/
  58. https://sigmoidal.io/chatbots-for-b2c-and-deep-learning/
  59. https://sigmoidal.io/machine-learning-for-trading/
  60. https://sigmoidal.io/machine-learning-for-trading/
  61. https://sigmoidal.io/machine-learning-for-trading/
  62. tel:+1-347-779-2050
  63. https://sigmoidal.io/cdn-cgi/l/email-protection#4b23220b38222c2624222f2a27652224
  64. https://www.googletagmanager.com/ns.html?id=gtm-ngj2l8w

   hidden links:
  66. https://sigmoidal.io/boosting-your-solutions-with-nlp/
  67. https://sigmoidal.io/
  68. https://sigmoidal.io/boosting-your-solutions-with-nlp/
  69. https://sigmoidal.io/boosting-your-solutions-with-nlp/
  70. https://sigmoidal.io/machine-learning-terminology-explained-top-8-must-know-concepts/
  71. https://sigmoidal.io/chatbots-for-b2c-and-deep-learning/
  72. https://www.linkedin.com/company/17914183/
  73. https://web.facebook.com/sigmoidal
  74. https://www.youtube.com/channel/uckhgompnzyvxg2g5txnknxw/videos
  75. https://twitter.com/sigmoidal_io
  76. https://sigmoidal.io/boosting-your-solutions-with-nlp/#start
  77. https://sigmoidal.io/boosting-your-solutions-with-nlp/
