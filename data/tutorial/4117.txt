   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]huggingface
     * [9]artificial intelligence
     * [10]natural language processing
     * [11]ios applications
     * [12]get the app
     __________________________________________________________________

state-of-the-art neural coreference resolution for chatbots

   [13]go to the profile of thomas wolf
   [14]thomas wolf (button) blockedunblock (button) followfollowing
   jul 7, 2017

     tl;dr, links: [15]online demo at [16]https://huggingface.co/coref,
     [17]github repo for neuralcoref:
     [18]https://github.com/huggingface/neuralcoref

   at hugging face      we work on the most amazing and challenging subset of
   natural language: millennial language. full of uncertainties     ,
   implicit references     , emojis     , jokes      and constantly creating novel
   expressions   

   to navigate these stormy waters     , we have developed a number of
   specific nlp tools based on the latest research in the field. one of
   these tools is a [19]coreference system we use to keep track of short
   term references already at the front end of our ai      brains.

   we couldn   t find a tool we could easily integrate in our conversational
   agent so we decided to develop it internally and [20]open-source it.

   you can also [21]try the coreference system for yourself on the demo we
   setup!

     but, what is coreference?

   let   s have a look at bob      who is talking with his ai friend alice     
   [1*qu9osjpiwqzeg_yh3b1mla.png]

   there are several implicit references in the last message from bob     
     *    she    refers to the same entity as    my sister   : bob   s sister.
     *    he    refers to the same entity as    a friend called john   : bob   s
       sister   s friend.

   the process of linking together mentions that relates to real world
   entities is called coreference resolution [22][1].
   [1*yestczcj4zvlioclwmvnsq.png]
   hugging face coreference system in operation. [23]try it for yourself!

   humans naturally associate these references together         but for an ai     
   brain, it is much more difficult! and when we say it is hard, we mean
   it,

     coreference resolution is the basis of the [24]winograd schema
     challenge, a test of machine intelligence     build to [25]defeat the
     ais who   ve beaten the turing test!     

     so how do we solve this problem without creating a [26]strong-ai?
     __________________________________________________________________

   coreference is a rather old nlp research topic [27][1]. it[28]* has
   seen a revival of interest in the past two years as several research
   groups [29][2] applied cutting-edge deep-learning and
   reinforcement-learning techniques to it. it was published earlier this
   year that coreference resolution may be instrumental in improving the
   performances of nlp neural architectures like id56 and lstm (see
      [30]linguistic knowledge as memory for recurrent neural networks    by
   b. dhingra, z. yang, w. w. cohen, and r. salakhutdinov).

   a typical coreference resolution algorithm goes like this:
     * we extract a series of mentions    words that are potentially
       referring to real world entities   

   [1*8xt3pkiddmj-o3lz5rqrgq.png]
   in our previous dialogue, we can identify six mentions
     * for each mentions and each pair of mentions, we compute a set of
       features (more on that soon).
     * then, we find the most likely antecedent for each mention (if there
       is one) based on this set of features. this last step is called
       pairwise ranking [31][3].

   traditionally the set of features was hand-engineered from linguistic
   features and it could be huge. some high quality systems use 120+
   features [32][4]!

   here comes the nice thing about modern nlp techniques like word vectors
   and neural nets. they allow us to automatically learn a lot of these
   hand-engineered features and reduce the set of hand-designed features
   by an almost an order of magnitude while keeping a good or even better
   accuracy.

   let   s see that in action on a simple example.

   her is a feminine pronoun and should have a higher id203 to refer
   to my sister      than to my brother     .

   we could encode that by hand, listing the gender and other properties
   of every word in our vocabulary         but we can also assign a vector to
   every word the vocabulary and let our model learn vectors that are
   adapted for our task on a training dataset.

   so we trained our id27s on [33]a large coreference annotated
   dataset without supplying any information regarding word gender. and
   here is what we got.
   [1*wfvjhr8kbtsz2g99tnvlgw.png]
   left: initial id27s (pca of pre-trained id97)         right:
   trained id27s (pca)

   on the left, the original id97 words vectors don   t specifically
   care about gender association [34][5]. on the right side, after
   training, our word vectors shows feminine and masculine nouns nicely
   separated along the principal components of the vectors even though we
   didn   t supply any information regarding word gender (gender has become
   a direction of main variance of our trained word vectors).
   [1*cv82f8nm38u6326gziioug.png]

   obviously the quality of our trained embeddings depends a lot on the
   training dataset. our embeddings are trained on the [35]ontonotes 5.0
   dataset (the largest coreference annotated corpus).

   but clearly this dataset has its flaws. in particular, as often in nlp
   datasets, it is build mainly from news and web articles hence with a
   more formal language than the usual chatbot user.

   we can see on our pca projection that pairs of words like dad/mum or
   brother/sister seem less clearly separated than a pair of more formal
   words like woman/man (along the first components of the pca).

   and, in fact, you can construct sentences for which our coreference
   system will [36]work nicely on some pairs of mention but [37]fail on
   another pair of mentions which are less formal. we give some hacks to
   circumvent that in a minute but the cleanest way is of course to gather
   a labeled data set that is more representative of your production data
   (like the over 10m messages already exchanged by our users with their
   huggingface ais).
     __________________________________________________________________

   so is that all? if we can learn every linguistic features relevant to
   our words, then how can coreference resolution be more difficult than
   the turing test?

   well, in the [38]winograd schema challenge, your ai has to solve
   questions like:

     the trophy would not fit in the brown suitcase because it was too
     big. what was too big? the trophy or the suitcase?

   [1*p_zwcbamors45onlfgta2a.png]
   terry winograd ([39]flickr/lisa padilla)

   our carefully gender-tuned-id27s will not be enough to help
   us solve these questions because we actually have to take the context
   of our mentions into account and maybe even external common knowledge!

   in our model, we add some simple context information by averaging word
   vectors surrounding each mention but there are many way you can add
   some context information [40][2]. the good thing is, because we   re
   building an entertaining ai, we don   t need 100% accuracy for it to work
   for users. and a high accuracy can be very hard to reach: the best team
   competing for the [41]winograd schema challenge last year reached only
   58% of success!

   our model then goes very roughly as follow: we take id27s for
   several words inside and around each mention, average them if needed,
   add some simple integer features (length of the mention, speaker
   information, location of the mentions   ) to obtain a features
   representation for each mention and it surroundings. then we plug these
   representations into two neural nets. a first neural net gives us a
   score for each pair of a mention and a possible antecedent while a
   second neural net gives us a score for a mention having no antecedent
   (sometimes a mention is the first reference to an entity in a text). we
   can then simply compare all these scores together and take the highest
   score to determine whether a mention has an antecedent and which one it
   should be.
   [1*xcs9sydotumkivktvht9bw.png]
   rough sketch of the coreference scoring model.

   the neural model is trained on a non-probabilistic slack-rescaled
   max-margin objective. it means that the system computes a score for
   each pair of mentions and a score for each individual mention but these
   scores are not probabilities, just scores in arbitrary unit (the
   highest the better).

   this scoring system is an adaptation of the very nice work published
   last year by kevin clark and christopher manning (see [42]   deep
   id23 for mention-ranking coreference models    by kevin
   clark and christopher d. manning, emnlp 2016, [43]improving coreference
   resolution by learning entity-level distributed representations by
   kevin clark and christopher d. manning, acl 2016, and the references
   therein). the full details and even more are given in theses
   publications which you should definitely read if you are interested in
   this model.

   the scoring model of clark and manning is [44]open-source and a full
   implementation (with mention detection and features computation) has
   been integrated in [45]stanford   s corenlp.

   we initially used this implementation in our system. however, we found
   that several important features were missing for our needs:
    1. easy integration in our current nlp processing pipeline. corenlp is
       an extensive tool but it is also a large monolithic java bloc that
       is hard to integrate in a high throughput distributed system like
       ours.
    2. capability to evolve with our users language and take into account
       user-specific informations. new word vectors have to be dynamically
       learned to use the fact that    kendall jenner    is a woman and a
       model even though she is not mentioned in our training dataset.
    3. taking care of the speakers in a conversation. the quality of the
       coreference resolution depend for a large part of the speaker
       information associated to each mention.

   here is how we solved that:
    1. our current pipeline is based on a set of deep-learning python
       tools and the high speed parsing is done by [46]spacy. we are big
       fans of spacy ultra-fast parser and of the work of matthew and ines
       at [47]explosion.ai. the coreference system with mentions
       detection, features extraction and neural net computation is thus
       implemented on top of spacy and numpy (in the future we could
       easily switch to [48]thinc when thinc   s api is stabilized).
    2. we make use of [49]recent work on id27s to compute
       embeddings for unknown words on the fly from definitions or
       information that you can provide (it   s very simple in fact: you can
       compute a id27 for    kendall jenner    simply by averaging
       the vectors for    woman    and    model    for example).
    3. we input and take care of the various speakers in the conversation
       when computing the features and resolving the coreferences.

   you can [50]try the coreference system for yourself!

   you can also [51]fork the code and use it in your projects. hope you
   liked it and let us know how you use it     
     __________________________________________________________________

   *. [52]^ coreference pun!
    1. [53]^ good introductions of the subject can be found in [54]the
       great nlp class of chris manning and in    speech and language
       processing: an introduction to natural language processing,
       computational linguistics, and id103    by daniel
       jurafsky & james h. martin, 2nd edition 2007 (chapter 21 in
       particular). linguistic is a fascinatingly huge domain so you may
       also have heard of [55]anaphors and cataphors. [56]coreferences,
       anaphors and cataphors describes different relationships that
       sometimes overlap but not always. in particular coreferences are
       mentions that all relates to the same object of the outside world.
    2. [57]^ see in particular: [58]   learning global features for
       coreference resolution    by sam wiseman, alexander m. rush, and
       stuart m. shieber, naacl 2016, [59]   deep id23 for
       mention-ranking coreference models    by kevin clark and christopher
       d. manning, emnlp 2016, and    [60]latent structures for coreference
       resolution    by sebastian martschat and michael strube, acl 2015.
    3. [61]^ there are other ways you can link entities (entity-mention
       models, , for e.g. by constructing cluster of mentions and
       considering global features (coreference is a id91 operation)
       but we won   t talk about them here. you should read the
       [62]references above of [63]the great nlp class of chris manning,
       or refer to    speech and language processing: an introduction to
       natural language processing, computational linguistics, and speech
       recognition    by daniel jurafsky & james h. martin, for example, if
       you want to know more about that.
    4. [64]^ see [65]modeling the lifespan of discourse entities with
       application to coreference resolution by marneffe et al. (2015) and
       [66]search space pruning: a simple solution for better coreference
       resolvers by nafise sadat moosavi and michael strube (2016).
    5. [67]^ well they do somehow but they also have to encode many other
       semantic/syntaxique features of the words         many of these being
       more important than gender    to predict the surrounding words (the
       objective in id97 training)

   thanks to [68]cl  ment delangue and [69]julien chaumond.
     * [70]artificial intelligence
     * [71]nlp
     * [72]natural language
     * [73]chatbots

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [74]go to the profile of thomas wolf

[75]thomas wolf

   natural language processing, deep learning and computational
   linguistics     science lead [76]@huggingface | thomwolf.io

     (button) follow
   [77]huggingface

[78]huggingface

   stories @ hugging face

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [79]huggingface
   never miss a story from huggingface, when you sign up for medium.
   [80]learn more
   never miss a story from huggingface
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/3302365dcf30
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30&source=--------------------------nav_reg&operation=register
   8. https://medium.com/huggingface?source=logo-lo_9viz1rx3wmbg---ba0dbdd23ac6
   9. https://medium.com/huggingface/tagged/artificial-intelligence
  10. https://medium.com/huggingface/tagged/nlp
  11. https://medium.com/huggingface/tagged/ios
  12. https://huggingface.co/
  13. https://medium.com/@thomwolf?source=post_header_lockup
  14. https://medium.com/@thomwolf
  15. https://huggingface.co/coref/
  16. https://huggingface.co/coref
  17. https://github.com/huggingface/neuralcoref
  18. https://github.com/huggingface/neuralcoref
  19. https://huggingface.co/coref/?text=my sister has a friend called john. really, tell me more about him ? she think he is so funny      !
  20. https://github.com/huggingface/neuralcoref
  21. https://huggingface.co/coref/
  22. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#96fa
  23. https://huggingface.co/coref/?text=my sister has a friend called john. really, tell me more about him ? she think he is so funny      !
  24. https://en.wikipedia.org/wiki/winograd_schema_challenge
  25. https://motherboard.vice.com/en_us/article/wnjxxm/this-alternative-to-the-turing-test-aims-to-find-common-sense-in-ai
  26. https://en.wikipedia.org/wiki/ai-complete
  27. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#96fa
  28. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#d0ca
  29. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#06e5
  30. https://arxiv.org/abs/1703.02620
  31. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#7f43
  32. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#ce28
  33. https://www.gabormelli.com/rkb/ontonotes_corpus
  34. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#0bcf
  35. https://www.gabormelli.com/rkb/ontonotes_corpus
  36. https://huggingface.co/coref/?text=my father and my mother are working hard. she is always nice but he is sometimes rude.
  37. https://huggingface.co/coref/?text=my dad and my mum are working hard. she is always nice but he is sometimes rude.
  38. https://en.wikipedia.org/wiki/winograd_schema_challenge
  39. https://www.flickr.com/photos/lisap/379993291/in/photolist-4f45jv-d83ev-ar1zyf-ar1a8c-ar1zwp-ar1aap-b2hqb-ar1a4t-7zq8vn-h4vgqr-7lsewp-ekwg6y-ekquyr-9hndsi-ekwg2n-t8i5x-t8ioz-t8huj-t8ix7-avu4tb-4vqpbz-zzylt-68tjoe-68ue8x-2f7jam
  40. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#06e5
  41. https://en.wikipedia.org/wiki/winograd_schema_challenge
  42. http://cs.stanford.edu/people/kevclark/resources/clark-manning-emnlp2016-deep.pdf
  43. http://cs.stanford.edu/people/kevclark/resources/clark-manning-acl16-improving.pdf
  44. https://github.com/clarkkev/deep-coref
  45. https://stanfordnlp.github.io/corenlp/
  46. https://spacy.io/
  47. https://explosion.ai/
  48. https://github.com/explosion/thinc
  49. https://arxiv.org/abs/1706.00286
  50. https://huggingface.co/coref/
  51. https://github.com/huggingface/neuralcoref
  52. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#116e
  53. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#c1eb
  54. http://web.stanford.edu/class/cs224n/
  55. https://en.wikipedia.org/wiki/anaphora_(linguistics)
  56. https://en.wikipedia.org/wiki/coreference
  57. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#116e
  58. http://nlp.seas.harvard.edu/papers/corefmain.pdf
  59. http://cs.stanford.edu/people/kevclark/resources/clark-manning-emnlp2016-deep.pdf
  60. http://www.aclweb.org/anthology/q/q15/q15-1029.pdf
  61. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#b027
  62. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#4821
  63. http://web.stanford.edu/class/cs224n/
  64. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#7c50
  65. https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43407.pdf
  66. http://www.aclweb.org/anthology/n16-1115.pdf
  67. https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30#b9bf
  68. https://medium.com/@clementdelangue?source=post_page
  69. https://medium.com/@julien_c?source=post_page
  70. https://medium.com/tag/artificial-intelligence?source=post
  71. https://medium.com/tag/nlp?source=post
  72. https://medium.com/tag/natural-language?source=post
  73. https://medium.com/tag/chatbots?source=post
  74. https://medium.com/@thomwolf?source=footer_card
  75. https://medium.com/@thomwolf
  76. http://twitter.com/huggingface
  77. https://medium.com/huggingface?source=footer_card
  78. https://medium.com/huggingface?source=footer_card
  79. https://medium.com/huggingface
  80. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  82. https://medium.com/p/3302365dcf30/share/twitter
  83. https://medium.com/p/3302365dcf30/share/facebook
  84. https://medium.com/p/3302365dcf30/share/twitter
  85. https://medium.com/p/3302365dcf30/share/facebook
