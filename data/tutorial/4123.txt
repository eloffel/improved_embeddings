   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

                       id123 with tensorflow

   using neural networks to explore natural language.

   by [27]steven hewitt

   july 17, 2017

   on the left: a normal fully connected network. on the right: the same
   network during training, with p = 0.5. on the left: a normal fully
   connected network. on the right: the same network during training, with
   p = 0.5. (source: steven hewitt, used with permission)

   attention readers: you can access all of the code on [28]github and
   view the ipython notebook [29]here.

   id123 is a simple exercise in logic that attempts to
   discern whether one sentence can be inferred from another. a computer
   program that takes on the task of id123 attempts to
   categorize an ordered pair of sentences into one of three categories.
   the first category, called    positive entailment,    occurs when you can
   use the first sentence to prove that a second sentence is true. the
   second category,    negative entailment,    is the inverse of positive
   entailment. this occurs when the first sentence can be used to disprove
   the second sentence. finally, if the two sentences have no correlation,
   they are considered to have a    neutral entailment.   

   id123 is useful as a component in much larger
   applications. for example, question-answering systems may use textual
   entailment to verify an answer from stored information. textual
   entailment may also enhance document summarization by filtering out
   sentences that don   t include new information. other natural language
   processing (nlp) systems find similar uses for entailment.

   this article will guide you through how to build a simple and
   fast-to-train neural network to perform id123 using
   [30]tensorflow.

before we get started

   in addition to installing [31]tensorflow version 1.0, make sure you   ve
   installed each of the following:
     * [32]jupyter
     * [33]numpy
     * [34]matplotlib

   to get a better sense of progress during network training, you're also
   welcome to install [35]tqdm, but it's not required. please
   [36]access[37] the code [38]and jupyter notebook[39] for this article
   on github. we   ll be using [40]stanford   s snli data set for our
   training, but we   ll download and extract the data we need using code
   from the jupyter notebook, so you don   t need to download it manually.
   if this is your first time working with tensorflow, i   d encourage you
   to check out aaron schumacher   s article, [41]   [42]hello,
   tensorflow[43].   

   we   ll start by doing all necessary imports, and we   ll let our jupyter
   notebook know it should display graphs and images in the notebook
   itself.
%matplotlib inline

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import urllib
import sys
import os
import zipfile

   the files we're about to use may take five minutes or more to download,
   so if you're following along by running the program in the
   corresponding notebook, feel free to start running the next few cells.
   in the meantime, let   s explore id123 in further detail.

examples of id123

   in this section, we   ll walk through a few examples of textual
   entailment to illustrate what we mean by positive, negative, and
   neutral entailment. to begin, we   ll look at positive entailment   when
   you read, for example, that    maurita and jade both were at the scene of
   the car crash,    you can infer that    multiple people saw the accident.   
   in this example sentence pair, we can prove the second sentence (also
   known as a    hypothesis   ) from the first sentence (also called the
      text   ), meaning that this represents a positive entailment. given that
   maurita and jade were both there to view the crash, multiple people
   must have seen it. note:    car crash    and    accident    have similar
   meanings, but they aren   t the same word. in fact, entailment doesn   t
   always mean that the sentences share words, as can be seen in this
   sentence pair, which only shares the word    the.   

   let   s consider another sentence pair. how, if at all, does the sentence
      two dogs played in the park with the old man    entail    there was only
   one canine in the park that day   ? if there are two dogs, there must be
   at least two canines. since the second sentence contradicts that idea,
   this is negative entailment.

   finally, to illustrate neutral entailment, we consider, how, if at all,
   the sentence    i played baseball with the kids    entails    the kids love
   ice cream.    playing baseball and loving ice cream have absolutely
   nothing to do with each other. i could play baseball with ice cream
   lovers, and i could play baseball with ice cream haters (both are
   equally possible). thus, the first sentence says nothing about the
   truth or falsehood of the second   implying neutral entailment.
   types of entailment figure 1. types of entailment. credit: steven
   hewitt.

representing words as numbers using word vectorization

   unfortunately for neural networks, they primarily work with numeric
   values. to get around this, we need to represent our words as numbers
   in some way. ideally, these numbers mean something; for example, we
   could use the character codes of the letters in a word, but that
   doesn   t tell us anything about the meaning of it (which would mean that
   tensorflow would have to do a lot of work to tell that    dog    and
      canine    are close to the same concept). turning similar meanings into
   something a neural network can understand happens by a process called
   word vectorization.

   one common way to create word vectorizations is to have each word
   represent a single point in a very high-dimensional space. words with
   similar representations should be relatively close together in this
   space. for example, each color has a representation that is usually
   very similar to other colors; demonstrations of this can be found in
   [44]the tensorflow tutorial on word vectorization.

working with stanford   s glove word vectorization + snli data set

   for our purposes, we won   t need to create a new representation of words
   as numbers. there already exist quite a few fantastic general-purpose
   vector representations of words as well as ways to train even more
   specialized material if the general-purpose data isn   t enough.

   the [45]associated notebook for this article is designed to work with
   the [46]pre-trained data for stanford   s glove word vectorization. we   ll
   be using the six-billion-token wikipedia 2014 + gigaword 5 vectors,
   since it   s the smallest and easiest to download. we   ll download the
   file programmatically, but keep in mind that it may take a while to run
   (it   s a fairly large file).

   at the same time, we'll also be picking up our data set for textual
   entailment: [47]stanford's snli data set. we'll be using the
   development set in the interest of speed (it has only 10,000 sentence
   pairs), but if you're interested in getting better results and have
   time to spare for training, you can try using the full data set
   instead.
glove_zip_file = "glove.6b.zip"
glove_vectors_file = "glove.6b.50d.txt"

snli_zip_file = "snli_1.0.zip"
snli_dev_file = "snli_1.0_dev.txt"
snli_full_dataset_file = "snli_1.0_train.txt"

from six.moves.url.lib.request import urlretrieve

#large file - 862 mb
if (not os.path.isfile(glove_zip_file) and
    not os.path.isfile(glove_vectors_file)):
    urlretrieve ("http://nlp.stanford.edu/data/glove.6b.zip",
                 glove_zip_file)

#medium-sized file - 94.6 mb
if (not os.path.isfile(snli_zip_file) and
    not os.path.isfile(snli_dev_file)):
    urlretrieve ("https://nlp.stanford.edu/projects/snli/snli_1.0.zip",
                 snli_zip_file)

def unzip_single_file(zip_file_name, output_file_name):
    """
        if the outfile is already created, don't recreate
        if the outfile does not exist, create it from the zipfile
    """
    if not os.path.isfile(output_file_name):
        with open(output_file_name, 'wb') as out_file:
            with zipfile.zipfile(zip_file_name) as zipped:
                for info in zipped.infolist():
                    if output_file_name in info.filename:
                        with zipped.open(info) as requested_file:
                            out_file.write(requested_file.read())
                            return

unzip_single_file(glove_zip_file, glove_vectors_file)
unzip_single_file(snli_zip_file, snli_dev_file)
# unzip_single_file(snli_zip_file, snli_full_dataset_file)

   now that we have our glove vectors downloaded, we can load them into
   memory, deserializing the space separated format into a python
   dictionary:
glove_wordmap = {}
with open(glove_vectors_file, "r") as glove:
    for line in glove:
        name, vector = tuple(line.split(" ", 1))
        glove_wordmap[name] = np.fromstring(vector, sep=" ")

   once we have our words, we need our input to contain entire sentences
   and process it through a neural network. let's start with making the
   sequence:
def sentence2sequence(sentence):
    """

    - turns an input sentence into an (n,d) matrix,
        where n is the number of tokens in the sentence
        and d is the number of dimensions each word vector has.

      tensorflow doesn't need to be used here, as simply
      turning the sentence into a sequence based off our
      mapping does not need the computational power that
      tensorflow provides. normal python suffices for this task.
    """
    tokens = sentence.lower().split(" ")
    rows = []
    words = []
    #greedy search for tokens
    for token in tokens:
        i = len(token)
        while len(token) > 0 and i > 0:
            word = token[:i]
            if word in glove_wordmap:
                rows.append(glove_wordmap[word])
                words.append(word)
                token = token[i:]
                i = len(token)
            else:
                i = i-1
    return rows, words

what the computer sees when it looks at a sentence

   to better visualize the word vectorization process, and to see what the
   computer sees when it looks at a sentence, we can represent the vectors
   as images. feel free to use [48]the notebook to play around with
   visualizing your own sentences. each row represents a single word, and
   the columns represent individual dimensions of the vectorized word. the
   vectorizations are trained in terms of relationships to other words, so
   what the representations actually mean is ambiguous. the computer can
   understand this vector language, and that   s the most important part to
   us. generally speaking, two vectors that contain similar colors in the
   same positions represent words that are similar in meaning.
def visualize(sentence):
    rows, words = sentence2sequence(sentence)
    mat = np.vstack(rows)

    fig = plt.figure()
    ax = fig.add_subplot(111)
    shown = ax.matshow(mat, aspect="auto")
    ax.yaxis.set_major_locator(ticker.multiplelocator(1))
    fig.colorbar(shown)

    ax.set_yticklabels([""]+words)
    plt.show()

visualize("the quick brown fox jumped over the lazy dog.")
visualize("the pretty flowers shone in the sunlight.")

   sentences in vectorized form, visualized figure 2. sentences in
   vectorized form, visualized. credit: steven hewitt.

   unlike images, sentences are inherently sequential and can   t be
   constrained by size, so instead of fully connected forward-feeding
   networks that take in one input value and simply run until it produces
   a single output, we need a new type of network. we need...recurrence.

vanilla recurrent networks

   recurrent neural networks (id56s) are a sequence-learning tool for
   neural networks. this type of neural network has only one layer   s worth
   of hidden inputs, which is re-used for each input from the sequence,
   along with a    memory    that   s passed ahead to the next input   s
   calculations. these are calculated using id127, where
   the matrix indices are trained weights, just like they are in a fully
   connected layer.

   the same calculations are repeated for each input in the sequence,
   meaning that a single    layer    of a recurrent neural network can be
   unrolled into many layers. in fact, there will be as many layers as
   there are inputs in the sequence. this allows the network to process a
   very complex sentence. tensorflow includes its own implementation of a
   vanilla id56 cell, basicid56cell, which can be added to your tensorflow
   graph as follows:
id56_size = 64
id56 = tf.contrib.id56.basicid56cell(id56_size)

the vanishing gradient problem

   in theory, the network would be able to remember things from one of the
   first layers, much earlier in the sentence   even at the end of the
   sentence. the main problem with this form of recurrence is that, in
   practice, earlier data is completely drowned out by newer inputs and
   information that doesn   t end up being nearly as important. recurrent
   neural networks, or at least a neural network with standard hidden
   units, often fail to hold on to information for long periods of time.
   this failure is known as the vanishing gradient problem.

   the simplest way to visualize this is by example. in the simplest case,
   input and    memory    are roughly equally weighted. the first input into
   the data will affect approximately half of the first output (the other
   half being the starting    memory   ), a quarter of the second output, then
   an eighth of the third output, and so on.

   this means we can   t use vanilla recurrent networks, at least not if we
   want to keep track of both sentences in this pair. the solution is to
   use a different type of recurrent network layer. perhaps the simplest
   of these is the long short-term memory layer, also known as an lstm.

utilizing lstm

   in an lstm, instead of the input (xt) always being used the same way
   every time in the calculation of current memory, the network makes a
   decision on how much the current values can affect the memory by an
      input gate    (it), and makes another decision on what memory (ct) is
   forgotten by an appropriately named    forget gate    (ft), and finally
   makes a third decision on what parts of memory are sent to the next
   timestep (ht) by an    output gate    (ot).
   hidden units within an lstm layer figure 3. a diagram showing the
   hidden units within an lstm layer. credit: steven hewitt (adapted from
   [49]this similar image, distributed under [50]cc by-sa 4.0).

   the combination of these three gates creates a choice: a single lstm
   node can either keep information in long-term memory or keep it in
   short-term memory, but it can   t do both at the same time. short-term
   memory lstms usually train to have relatively open input gates that let
   a lot of information in and forget many things often, while long-term
   memory lstms have tight input gates that only allow very small, very
   specific pieces of information in. this tightness means that it doesn   t
   lose its information easily, allowing for longer retention time.

   in general, lstms are very cryptic. different lstm nodes in the same
   network may have vastly different gates that rely upon one another,
   such as perhaps having a short-term gate remember the word    not    in the
   sentence    john did not go to the store,    so that when the word    go   
   appears, a long-term gate could remember    not go    instead of    go.    of
   course, this is a contrived example, and, in practice, these
   relationships are very complex to the point of being indecipherable.

defining the constants for our network

   since we aren   t going to use a vanilla id56 layer in our network, let's
   clear out the graph and add an lstm layer, which tensorflow also
   includes by default. since this is going to be the first part of our
   actual network, let's also define all the constants we'll need for the
   network, which we'll talk about as they come up:
#constants setup
max_hypothesis_length, max_evidence_length = 30, 30
batch_size, vector_size, hidden_size = 128, 50, 64

lstm_size = hidden_size

weight_decay = 0.0001

learning_rate = 1

input_p, output_p = 0.5, 0.5

training_iterations_count = 100000

display_step = 10

def score_setup(row):
    convert_dict = {
      'entailment': 0,
      'neutral': 1,
      'contradiction': 2
    }
    score = np.zeros((3,))
    for x in range(1,6):
        tag = row["label"+str(x)]
        if tag in convert_dict: score[convert_dict[tag]] += 1
    return score / (1.0*np.sum(score))

def fit_to_size(matrix, shape):
    res = np.zeros(shape)
    slices = [slice(0,min(dim,shape[e])) for e, dim in enumerate(matrix.shape)]
    res[slices] = matrix[slices]
    return res

def split_data_into_scores():
    import csv
    with open("snli_1.0_dev.txt","r") as data:
        train = csv.dictreader(data, delimiter='\t')
        evi_sentences = []
        hyp_sentences = []
        labels = []
        scores = []
        for row in train:
            hyp_sentences.append(np.vstack(
                    sentence2sequence(row["sentence1"].lower())[0]))
            evi_sentences.append(np.vstack(
                    sentence2sequence(row["sentence2"].lower())[0]))
            labels.append(row["gold_label"])
            scores.append(score_setup(row))

        hyp_sentences = np.stack([fit_to_size(x, (max_hypothesis_length, vector_
size))
                          for x in hyp_sentences])
        evi_sentences = np.stack([fit_to_size(x, (max_evidence_length, vector_si
ze))
                          for x in evi_sentences])

        return (hyp_sentences, evi_sentences), labels, np.array(scores)

data_feature_list, correct_values, correct_scores = split_data_into_scores()

l_h, l_e = max_hypothesis_length, max_evidence_length
n, d, h = batch_size, vector_size, hidden_size
l_seq = l_h + l_e

   we'll also reset the graph to not include the id56 cell we added
   earlier, since we won't be using that for this network:
tf.reset_default_graph()

   with both those out of the way, we can define our lstm using tensorflow
   as follows:
lstm = tf.contrib.id56.basiclstmcell(lstm_size)

implementing dropout, for id173

   if we simply used lstm layers and nothing more, the network might read
   a lot of meaning into common, but inconsequential, words like    a,   
      the,    and    and.    the network might incorrectly believe that it has
   found negative entailment if one sentence uses the phrase    an animal   
   and the other uses    the animal,    even if those phrases refer to the
   same object.

   to solve this, we need to regulate to see if individual words end up
   being important to the meaning as a whole, and we do that by a process
   called    dropout.    dropout is a id173 pattern in neural network
   design that revolves around dropping randomly selected hidden and
   visible units. as the size of a neural network increases, so does the
   number of parameters used to calculate the final result, each of which
   contributes to overfitting if trained all at once. in order to
   regularize for this, a portion of the units contained within the
   network are selected randomly and zeroed out temporarily during
   training, and their outputs are scaled appropriately during actual use.

   dropout on    standard    (i.e., fully connected) layers is also useful
   because it effectively trains multiple smaller networks, and then
   combines them during testing time. one of the constants in machine
   learning is that combining multiple models nearly always makes for a
   better method than a single model on its own, and dropout serves to
   turn a single neural network into multiple smaller neural networks that
   share some nodes, and thus some parameters, with the others.

   a dropout layer has one hyperparameter known as p, which is simply the
   id203 that each unit is kept in the network for that iteration of
   training. the units that are kept provide their outputs to the next
   layer, and the units that are not kept provide nothing. what follows is
   an example showing the difference between a fully connected network
   without dropout and a fully connected network with dropout during one
   iteration of training:
   network during training figure 4. on the left: a normal fully connected
   network. on the right: the same network during training, with p = 0.5.
   credit: steven hewitt.

tensorflow   s dropoutwrapper for recurrent layers

   unfortunately, dropout does not play particularly nicely with lstm
   layers' internal gates. the loss of certain pieces of crucial memory
   means that complicated relationships required for id85
   have a harder time forming with dropout, so for our lstm layer, we   ll
   skip using dropout on internal gates, instead using it on everything
   else. thankfully, this is the default implementation of
   [51]tensorflow   s dropoutwrapper for recurrent layers:
lstm_drop =  tf.contrib.id56.dropoutwrapper(lstm, input_p, output_p)

completing our model

   with all the explanations out of the way, we can finish up our model.
   the first step is tokenizing and using our glove dictionary to turn the
   two input sentences into a single sequence of vectors. since we can   t
   effectively use dropout on information that gets passed within an lstm,
   we   ll use dropout on features from words and on final output
   instead   effectively using dropout on the first and last layers from the
   unrolled lstm network portions.

   you may notice that we use a bi-directional id56, with two different
   lstm units. this form of recurrent network runs both forward and
   backward through the input data, which allows the network to review
   both the hypothesis and the evidence both independently and in relation
   to each other.

   the final output from the lstms will be passed into a set of fully
   connected layers, and then from that, we   ll get a single real-valued
   score that indicates how strong each of the kinds of entailment are,
   which we use to select our final result and determine our confidence in
   that result.
# n: the number of elements in each of our batches,
#   which we use to train subsets of data for efficiency's sake.
# l_h: the maximum length of a hypothesis, or the second sentence.  this is
#   used because training an id56 is extraordinarily difficult without
#   rolling it out to a fixed length.
# l_e: the maximum length of evidence, the first sentence.  this is used
#   because training an id56 is extraordinarily difficult without
#   rolling it out to a fixed length.
# d: the size of our used glove or other vectors.
hyp = tf.placeholder(tf.float32, [n, l_h, d], 'hypothesis')
evi = tf.placeholder(tf.float32, [n, l_e, d], 'evidence')
y = tf.placeholder(tf.float32, [n, 3], 'label')
# hyp: where the hypotheses will be stored during training.
# evi: where the evidences will be stored during training.
# y: where correct scores will be stored during training.

# lstm_size: the size of the gates in the lstm,
#    as in the first lstm layer's initialization.
lstm_back = tf.contrib.id56.basiclstmcell(lstm_size)
# lstm_back:  the lstm used for looking backwards
#   through the sentences, similar to lstm.

# input_p: the id203 that inputs to the lstm will be retained at each
#   iteration of dropout.
# output_p: the id203 that outputs from the lstm will be retained at
#   each iteration of dropout.
lstm_drop_back = tf.contrib.id56.dropoutwrapper(lstm_back, input_p, output_p)
# lstm_drop_back:  a dropout wrapper for lstm_back, like lstm_drop.


fc_initializer = tf.random_normal_initializer(stddev=0.1)
# fc_initializer: initial values for the fully connected layer's weights.
# hidden_size: the size of the outputs from each lstm layer.
#   multiplied by 2 to account for the two lstms.
fc_weight = tf.get_variable('fc_weight', [2*hidden_size, 3],
                            initializer = fc_initializer)
# fc_weight: storage for the fully connected layer's weights.
fc_bias = tf.get_variable('bias', [3])
# fc_bias: storage for the fully connected layer's bias.

# tf.graphkeys.id173_losses:  a key to a collection in the graph
#   designated for losses due to id173.
#   in this case, this portion of loss is id173 on the weights
#   for the fully connected layer.
tf.add_to_collection(tf.graphkeys.id173_losses,
                     tf.nn.l2_loss(fc_weight))

x = tf.concat([hyp, evi], 1) # n, (lh+le), d
# permuting batch_size and n_steps
x = tf.transpose(x, [1, 0, 2]) # (le+lh), n, d
# reshaping to (n_steps*batch_size, n_input)
x = tf.reshape(x, [-1, vector_size]) # (le+lh)*n, d
# split to get a list of 'n_steps' tensors of shape (batch_size, n_input)
x = tf.split(x, l_seq,)

# x: the inputs to the bidirectional_id56


# tf.contrib.id56.static_bidirectional_id56: runs the input through
#   two recurrent networks, one that runs the inputs forward and one
#   that runs the inputs in reversed order, combining the outputs.
id56_outputs, _, _ = tf.contrib.id56.static_bidirectional_id56(lstm, lstm_back,
                                                            x, dtype=tf.float32)
# id56_outputs: the list of lstm outputs, as a list.
#   what we want is the latest output, id56_outputs[-1]

classification_scores = tf.matmul(id56_outputs[-1], fc_weight) + fc_bias
# the scores are relative certainties for how likely the output matches
#   a certain entailment:
#     0: positive entailment
#     1: neutral entailment
#     2: negative entailment

showing tensorflow how to calculate accuracy

   in order to test the accuracy and begin to add in optimization
   constraints, we need to show tensorflow how to calculate the
   accuracy   or the percentage of correctly predicted labels.

   we also need to determine a loss, to show how poorly the network is
   doing. since we have both classification scores and optimal scores, the
   choice here is using a variation on softmax loss from tensorflow:
   tf.nn.softmax_cross_id178_with_logits. we add in id173
   losses to help with overfitting and then prepare an optimizer to learn
   how to reduce the loss.
with tf.variable_scope('accuracy'):
    predicts = tf.cast(tf.argmax(classification_scores, 1), 'int32')
    y_label = tf.cast(tf.argmax(y, 1), 'int32')
    corrects = tf.equal(predicts, y_label)
    num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))
    accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))

with tf.variable_scope("loss"):
    cross_id178 = tf.nn.softmax_cross_id178_with_logits(
        logits = classification_scores, labels = y)
    loss = tf.reduce_mean(cross_id178)
    total_loss = loss + weight_decay * tf.add_n(
        tf.get_collection(tf.graphkeys.id173_losses))

optimizer = tf.train.gradientdescentoptimizer(learning_rate)

opt_op = optimizer.minimize(total_loss)

let   s train the network

   finally, we can train the network! if you installed tqdm, you can use
   it to keep track of progress as the network trains.
# initialize variables
init = tf.global_variables_initializer()

# use tqdm if installed
tqdm_installed = false
try:
    from tqdm import tqdm
    tqdm_installed = true
except:
    pass

# launch the tensorflow session
sess = tf.session()
sess.run(init)

# training_iterations_count: the number of data pieces to train on in total
# batch_size: the number of data pieces per batch
training_iterations = range(0,training_iterations_count,batch_size)
if tqdm_installed:
    # add a progress bar if tqdm is installed
    training_iterations = tqdm(training_iterations)

for i in training_iterations:

    # select indices for a random data subset
    batch = np.random.randint(data_feature_list[0].shape[0], size=batch_size)

    # use the selected subset indices to initialize the graph's
    #   placeholder values
    hyps, evis, ys = (data_feature_list[0][batch,:],
                      data_feature_list[1][batch,:],
                      correct_scores[batch])

    # run the optimization with these initialized values
    sess.run([opt_op], feed_dict={hyp: hyps, evi: evis, y: ys})
    # display_step: how often the accuracy and loss should
    #   be tested and displayed.
    if (i/batch_size) % display_step == 0:
        # calculate batch accuracy
        acc = sess.run(accuracy, feed_dict={hyp: hyps, evi: evis, y: ys})
        # calculate batch loss
        tmp_loss = sess.run(loss, feed_dict={hyp: hyps, evi: evis, y: ys})
        # display results
        print("iter " + str(i/batch_size) + ", minibatch loss= " + \
              "{:.6f}".format(tmp_loss) + ", training accuracy= " + \
              "{:.5f}".format(acc))

   your network is now trained! you should see accuracies around 50-55%,
   which can be improved by careful modification of hyperparameters and
   increasing the data set size to include the entire training set.
   usually, this will correspond with an increase in training time.

   feel free to modify the following code in [52]the notebook by inserting
   your own sentences:
evidences = ["maurita and jade both were at the scene of the car crash."]

hypotheses = ["multiple people saw the accident."]

sentence1 = [fit_to_size(np.vstack(sentence2sequence(evidence)[0]),
                         (30, 50)) for evidence in evidences]

sentence2 = [fit_to_size(np.vstack(sentence2sequence(hypothesis)[0]),
                         (30,50)) for hypothesis in hypotheses]

prediction = sess.run(classification_scores, feed_dict={hyp: (sentence1 * n),
                                                        evi: (sentence2 * n),
                                                        y: [[0,0,0]]*n})
print(["positive", "neutral", "negative"][np.argmax(prediction[0])]+
      " entailment")

   finally, once we're done playing with our model, we'll close the
   session to free up system resources.
sess.close()

interested in developing more results?

   the design focus of this network was creating a simple system that was
   easy and quick to train. in order to get more accurate results, you may
   want to consider:
     * adding [53]more layers of lstms.
     * using alternative types of id56 layers, such as [54]gated recurrent
       units (grus). tensorflow also includes an [55]implementation of
       grus.
     * adding more hidden units. if you do this, increase id173
       and dropout strengths to account for the fact that there are more
       parameters in the network.
     * experimentation with other kinds of networks entirely!

   this post is a collaboration between o'reilly and [56]tensorflow.
   [57]see our statement of editorial independence.
   article image: on the left: a normal fully connected network. on the
   right: the same network during training, with p = 0.5. (source: steven
   hewitt, used with permission).
   tags: [58]all about tensorflow

   share
    1. [59]tweet
    2.
    3.
     __________________________________________________________________

   [60]steven hewitt

[61]steven hewitt

   steven hewitt is currently a graduate student in the computer science
   division of the eecs department at the university of california,
   berkeley. his academic interests include ai, natural language
   processing, education, and robotics. his research endeavors currently
   include teaching programs to understand code patterns and display them
   in a way relevant to humans, id27 methods, and
   question-answering systems. when he   s not attending classes or coding,
   you may find him composing music or generating fractal flame art.
   [62]more
     __________________________________________________________________

   [63]bots landscape.

   [64]ai

[65]infographic: the bot platform ecosystem

   by [66]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [67]vertical forest, milan.

   [68]ai

[69]evolve ai

   by [70]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [71]welcome sign at o'reilly ai conference 2016

   [72]ai

[73]highlights from the o'reilly ai conference in new york 2016

   by [74]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [75]close up of uber's self driving car in pittsburgh.

   [76]ai

[77]how ai is propelling driverless cars, the future of surface transport

   by [78]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [79]our company
     * [80]teach/speak/write
     * [81]careers
     * [82]customer service
     * [83]contact us

site map

     * [84]ideas
     * [85]learning
     * [86]topics
     * [87]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [88]terms of service     [89]privacy policy     [90]editorial independence

   animal

   iframe: [91]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/c2c1c03c-0ac1-496b-87b1-3d12c2065f06
  28. https://github.com/steven-hewitt/entailment-with-tensorflow
  29. https://github.com/steven-hewitt/entailment-with-tensorflow
  30. https://www.tensorflow.org/
  31. https://www.tensorflow.org/
  32. http://jupyter.org/
  33. http://www.numpy.org/
  34. http://matplotlib.org/
  35. https://pypi.python.org/pypi/tqdm
  36. https://github.com/steven-hewitt/entailment-with-tensorflow
  37. https://github.com/steven-hewitt/entailment-with-tensorflow
  38. https://github.com/steven-hewitt/entailment-with-tensorflow
  39. https://github.com/steven-hewitt/entailment-with-tensorflow
  40. http://nlp.stanford.edu/projects/snli/
  41. https://www.oreilly.com/learning/hello-tensorflow
  42. https://www.oreilly.com/learning/hello-tensorflow
  43. https://www.oreilly.com/learning/hello-tensorflow
  44. https://www.tensorflow.org/tutorials/id97
  45. https://github.com/steven-hewitt/entailment-with-tensorflow
  46. http://nlp.stanford.edu/projects/glove/
  47. http://nlp.stanford.edu/projects/snli/
  48. https://github.com/steven-hewitt/entailment-with-tensorflow
  49. https://en.wikipedia.org/wiki/long_short-term_memory#/media/file:long_short_term_memory.png
  50. https://creativecommons.org/licenses/by-sa/4.0/
  51. https://www.tensorflow.org/api_docs/python/tf/contrib/id56/dropoutwrapper
  52. https://github.com/steven-hewitt/entailment-with-tensorflow
  53. https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms
  54. https://arxiv.org/pdf/1412.3555.pdf
  55. https://www.tensorflow.org/api_docs/python/tf/contrib/id56/grucell
  56. https://www.tensorflow.org/
  57. http://www.oreilly.com/about/editorial_independence.html
  58. https://www.oreilly.com/tags/all-about-tensorflow
  59. https://twitter.com/share
  60. https://www.oreilly.com/people/c2c1c03c-0ac1-496b-87b1-3d12c2065f06
  61. https://www.oreilly.com/people/c2c1c03c-0ac1-496b-87b1-3d12c2065f06
  62. https://www.oreilly.com/people/c2c1c03c-0ac1-496b-87b1-3d12c2065f06
  63. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  64. https://www.oreilly.com/topics/ai
  65. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  66. https://www.oreilly.com/people/b1d73-jon-bruner
  67. https://www.oreilly.com/ideas/evolve-ai
  68. https://www.oreilly.com/topics/ai
  69. https://www.oreilly.com/ideas/evolve-ai
  70. https://www.oreilly.com/people/14d38-naveen-rao
  71. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  72. https://www.oreilly.com/topics/ai
  73. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  74. https://www.oreilly.com/people/0d2c1-mac-slocum
  75. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  76. https://www.oreilly.com/topics/ai
  77. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  78. https://www.oreilly.com/people/c7521-shahin-farshchi
  79. http://oreilly.com/about/
  80. http://oreilly.com/work-with-us.html
  81. http://oreilly.com/careers/
  82. http://shop.oreilly.com/category/customer-service.do
  83. http://shop.oreilly.com/category/customer-service.do
  84. https://www.oreilly.com/ideas
  85. https://www.oreilly.com/topics/oreilly-learning
  86. https://www.oreilly.com/topics
  87. https://www.oreilly.com/all
  88. http://oreilly.com/terms/
  89. http://oreilly.com/privacy.html
  90. http://www.oreilly.com/about/editorial_independence.html
  91. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  93. https://www.oreilly.com/
  94. https://www.facebook.com/oreilly/
  95. https://twitter.com/oreillymedia
  96. https://www.youtube.com/user/oreillymedia
  97. https://plus.google.com/+oreillymedia
  98. https://www.linkedin.com/company/oreilly-media
  99. https://www.oreilly.com/
