   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]data science

                     ideas on interpreting machine learning

   mix-and-match approaches for visualizing data and interpreting machine
   learning models and results.

   by [27]patrick hall[28]wen phan[29]srisatish ambati

   march 15, 2017

   inputs activating different neurons in a neural network. inputs
   activating different neurons in a neural network. (source: image
   courtesy of patrick hall and the h2o.ai team, used with permission)

   you   ve probably heard by now that machine learning algorithms can use
   big data to predict whether a donor will give to a charity, whether an
   infant in a nicu will develop sepsis, whether a customer will respond
   to an ad, and on and on. machine learning can even [30]drive cars and
   [31]predict elections. ... err, wait. can it? i believe it can, but
   these recent high-profile hiccups should leave everyone who works with
   data (big or not) and machine learning algorithms asking themselves
   some very hard questions: do i understand my data? do i understand the
   model and answers my machine learning algorithm is giving me? and do i
   trust these answers? unfortunately, the complexity that bestows the
   extraordinary predictive abilities on machine learning algorithms also
   makes the answers the algorithms produce hard to understand, and maybe
   even hard to trust.

   although it is possible to enforce monotonicity constraints (a
   relationship that only changes in one direction) between independent
   variables and a machine-learned response function, machine learning
   algorithms tend to create nonlinear, non-monotonic, non-polynomial, and
   even non-continuous functions that approximate the relationship between
   independent and dependent variables in a data set. (this relationship
   might also be referred to as the conditional distribution of the
   dependent variables, given the values of the independent variables.)
   these functions can then make very specific predictions about the
   values of dependent variables for new data   whether a donor will give to
   a charity, an infant in a nicu will develop sepsis, a customer will
   respond to an ad, etc. conversely, traditional linear models tend to
   create linear, monotonic, and continuous functions to approximate the
   very same relationships. even though they   re not always the most
   accurate predictors, the elegant simplicity of linear models makes the
   results they generate easy to interpret.

   while understanding and trusting models and results is a general
   requirement for good (data) science, model interpretability is a
   serious legal mandate in the regulated verticals of banking, insurance,
   and other industries. business analysts, doctors, and industry
   researchers simply must understand and trust their models and modeling
   results. for this reason, linear models were the go-to applied
   predictive modeling tool for decades, even though it usually meant
   giving up a couple points on the accuracy scale. today, many
   organizations and individuals are embracing machine learning algorithms
   for predictive modeling tasks, but difficulties in interpretation still
   present a barrier for the widespread, practical use of machine learning
   algorithms.

   in this article, i present several approaches beyond the usual error
   measures and assessment plots for visualizing data and interpreting
   machine learning models and results. users are encouraged to mix and
   match these techniques to best fit their own needs.

   wherever possible,    interpretability    of each technique in this article
   is deconstructed into more basic components   complexity, scope,
   understanding, and trust   which i will first outline below.

complexity of response function to be explained

   linear, monotonic functions: functions created by id75
   algorithms are probably the most interpretable class of models. these
   models will be referred to here as    linear and monotonic,    meaning that
   for a change in any given independent variable (or sometimes
   combination or function of an independent variable), the response
   function changes at a defined rate, in only one direction, and at a
   magnitude represented by a readily available coefficient. monotonicity
   also enables intuitive and even automatic reasoning about predictions.
   for instance, if a lender rejects your credit card application, they
   can tell you why because their id203-of-default model often
   assumes your credit score, your account balances, and the length of
   your credit history are monotonically related to your ability to pay
   your credit card bill. when these explanations are created
   automatically, they are typically called    reason codes.    of course,
   linear and monotonic response functions enable the calculation of
   relative variable importance measures, too. linear and monotonic
   functions have several uses in machine learning interpretability. part
   1 and part 2 below discuss the many ways linear, monotonic functions
   can be used to make machine learning interpretable.

   nonlinear, monotonic functions: although most machine learned response
   functions are nonlinear, some can be constrained to be monotonic with
   respect to any given independent variable. while there is no single
   coefficient that represents the change in the response function induced
   by a change in a single independent variable, nonlinear and monotonic
   functions do always change in one direction as a single input variable
   changes. nonlinear, monotonic response functions usually allow for the
   generation of both reason codes and relative variable importance
   measures. nonlinear, monotonic response functions are highly
   interpretable and often suitable for use in [32]regulated applications.

   (of course, there are linear, non-monotonic machine-learned response
   functions that can, for instance, be created by the [33]multi-variate
   adaptive regression splines approach. these functions are not
   highlighted here because they tend to be less accurate predictors than
   purely nonlinear, non-monotonic functions, while also lacking the high
   interpretability of their monotonic counterparts.)

   nonlinear, non-monotonic functions: most machine learning algorithms
   create nonlinear, non-monotonic response functions. this class of
   functions is the most difficult to interpret, as they can change in a
   positive and negative direction and at a varying rate for any change in
   an independent variable. typically, the only standard interpretability
   measure these functions provide are relative variable importance
   measures. you may need to use a combination of additional techniques
   presented below to interpret these extremely complex models.

scope of interpretability

   global interpretability: some of the presented techniques facilitate
   global interpretations of machine learning algorithms, their results,
   or the machine-learned relationship between the inputs and the
   dependent variable(s) (e.g., the model of the conditional
   distribution). global interpretations help us understand the entire
   conditional distribution modeled by the trained response function, but
   global interpretations can be approximate or based on average values.

   local interpretability: local interpretations promote understanding of
   small regions of the conditional distribution, such as clusters of
   input records and their corresponding predictions, or deciles of
   predictions and their corresponding input rows. because small sections
   of the conditional distribution are more likely to be linear,
   monotonic, or otherwise well-behaved, local explanations can be more
   accurate than global explanations.

understanding and trust

   machine learning algorithms and the functions they create during
   training are sophisticated, intricate, and opaque. humans who would
   like to use these models have basic, emotional needs to understand and
   trust them because we rely on them for our livelihoods or because we
   need them to make important decisions for us. for some users, technical
   descriptions of algorithms in textbooks and journals provide enough
   insight to fully understand machine learning models. for these users,
   cross-validation, error measures, and assessment plots probably also
   provide enough information to trust a model. unfortunately, for many
   applied practitioners, the usual definitions and assessments don   t
   often inspire full trust and understanding in machine learning models
   and their results. the techniques presented here go beyond the standard
   practices to engender greater understanding and trust. these techniques
   enhance understanding by providing specific insights into the
   mechanisms of the algorithms and the functions they create, or by
   providing detailed information about the answers they provide. the
   techniques below enhance trust by enabling users to observe or ensure
   the stability and dependability of machine learning algorithms, the
   functions they create, or the answers they generate.

   another important way to classify model interpretability techniques is
   whether they are    model-agnostic,    meaning they can be applied to
   different types of machine learning algorithms, or    model-specific,   
   meaning techniques that are only applicable for a single type or class
   of algorithm. the techniques herein are described in an approachable,
   high-level manner that is generally model-agnostic. model-specific
   techniques will be called out for the reader when appropriate.

   the techniques are presented in three parts. part 1 includes approaches
   for seeing and understanding your data in the context of training and
   interpreting machine learning algorithms, part 2 introduces techniques
   for combining linear models and machine learning algorithms for
   situations where interpretability is of paramount importance, and part
   3 describes approaches for understanding and validating the most
   complex types of predictive models.

part 1: seeing your data

   most real data sets are hard to see because they have many variables
   and many rows. like most sighted people, i rely on my visual sense
   quite heavily for understanding information. for me, seeing data is
   basically tantamount to understanding data. however, i can only
   understand two or three visual dimensions, preferably two. and,
   something called [34]change blindness frustrates human attempts to
   reason analytically given information split across different pages or
   screens. so, if a data set has more than two or three variables or more
   rows than can fit on a single page or screen, it   s realistically going
   to be hard to understand what   s going on in it without resulting to
   more advanced techniques than scrolling through countless rows of data.

   of course, there are many, many ways to visualize data sets. most of
   the techniques highlighted below help illustrate all of a data set in
   just two dimensions, not just univariate or bivariate slices of a data
   set (meaning one or two variables at a time). this is important in
   machine learning because most machine learning algorithms automatically
   model high-degree interactions between variables (meaning the effect of
   combining many   i.e., more than two or three   variables together).
   traditional univariate and bivariate tables and plots are still
   important and you should use them, but they are more relevant in the
   context of traditional linear models and slightly less helpful in
   understanding nonlinear models that can pick up on arbitrarily
   high-degree interactions between independent variables.

glyphs

   ideas on interpreting machine learning glyphs representing operating
   systems and web browser (agent) types figure 1. glyphs representing
   operating systems and web browser (agent) types. image courtesy of
   [35]ivy wang and the h2o.ai team.

   glyphs are visual symbols used to represent data. the color, texture,
   or alignment of a glyph can be used to represent different values or
   attributes of data. in figure 1, colored circles are defined to
   represent different types of operating systems and web browsers. when
   arranged in a certain way, these glyphs can be used to represent rows
   of a data set.
   interpreting machine learning models glyphs arranged to represent many
   rows of a data set figure 2. glyphs arranged to represent many rows of
   a data set. image courtesy of ivy wang and the h2o.ai team.

   figure 2 gives an example of how glyphs can be used to represent rows
   of a data set. each grouping of four glyphs can be either a row of data
   or an aggregated group of rows in a data set. the highlighted
   windows/internet explorer combination is very common in the data set
   (represented by blue, teal, and green circles) and so is the os x and
   safari combination (represented by two grey circles). it   s quite
   possible these two combinations are two compact and disjoint clusters
   of data. we can also see that, in general, operating system versions
   tend to be older than browser versions and that using windows and
   safari is correlated with using newer operating system and browser
   versions, whereas linux users and bots are correlated with older
   operating system and browser versions. the red dots that represent
   queries from bots standout visually (unless you are red-green
   colorblind). using bright colors or unique alignments for events of
   interest or outliers is a good method for making important or unusual
   data attributes clear in a glyph representation.

correlation graphs

   correlation graph representing loans made by a large financial firm
   figure 3. a correlation graph representing loans made by a large
   financial firm. figure courtesy of patrick hall and the h2o.ai team.

   a correlation graph is a two-dimensional representation of the
   relationships (correlation) in a data set. while many details regarding
   the display of a correlation graph are optional and could be improved
   beyond those chosen for figure 3, correlation graphs are a very
   powerful tool for seeing and understanding relationships (correlation)
   between variables in a data set. even data sets with tens of thousands
   of variables can be displayed in two dimensions using this technique.

   in figure 3, the nodes of the graph are the variables in a loan data
   set and the edge weights (thickness) between the nodes are defined by
   the absolute values of their pairwise pearson correlation. for visual
   simplicity, absolute weights below a certain threshold are not
   displayed. the node size is determined by a node   s number of
   connections (node degree), node color is determined by a graph
   community calculation, and node position is defined by a graph force
   field algorithm. the correlation graph allows us to see groups of
   correlated variables, identify irrelevant variables, and discover or
   verify important relationships that machine learning models should
   incorporate, all in two dimensions.

   in a supervised model built for the data represented in figure 3,
   assuming one of the represented variables was an appropriate target, we
   would expect variable selection techniques to pick one or two variables
   from the light green, blue, and purple groups, we would expect
   variables with thick connections to the target to be important
   variables in the model, and we would expect a model to learn that
   unconnected variables like channel_r are not very important. figure 3
   also illustrates common sense relationships such as that between
   first_time_homebuyer_flag_n and original_interest_rate that should be
   reflected in a trustworthy model.

2-d projections

   two-dimensional projections of the famous 784-dimensional mnist data
   set figure 4. two-dimensional projections of the famous 784-dimensional
   mnist data set using (left) principal components analysis (pca) and
   (right) a stacked denoising autoencoder. image courtesy of patrick hall
   and the h2o.ai team.

   there are many techniques for projecting the rows of a data set from a
   usually high-dimensional original space into a more visually
   understandable lower-dimensional space, ideally two or three
   dimensions. popular techniques include:
     * [36]principal component analysis (pca)
     * [37]multidimensional scaling (mds)
     * [38]t-distributed stochastic neighbor embedding (id167)
     * [39]autoencoder networks

   each of these techniques has strengths and weaknesses, but the key idea
   they all share is to represent the rows of a data set in a meaningful
   low-dimensional space. data sets containing images, text, or even
   business data with many variables can be difficult to visualize as a
   whole. these projection techniques enable high-dimensional data sets to
   be projected into a representative low-dimensional space and visualized
   using the trusty old scatter plot technique. a high-quality projection
   visualized in a scatter plot should exhibit key structural elements of
   a data set, such as clusters, hierarchy, sparsity, and outliers.

   in figure 4, the famous [40]mnist data set is projected from its
   original 784 dimensions onto two dimensions using two different
   techniques: pca and autoencoder networks. the quick and dirty pca
   projection can separate digits labeled as 0 from digits labeled as 1
   very well. these two-digit classes are projected into fairly compact
   clusters, but the other digit classes are generally overlapping. in the
   more sophisticated (but also more computationally expensive)
   autoencoder projection, all the digit classes appear as clusters, with
   visually similar digits appearing close to one another in the reduced
   two-dimensional space. the autoencoder projection is capturing the
   presumed clustered structure of the original high-dimensional space and
   the relative locations of those clusters. interestingly, both plots can
   pick up on a few outlying digits.

   projections can add an extra and specific degree of trust if they are
   used to confirm machine learning modeling results. for instance, if
   known hierarchies, classes, or clusters exist in training or test data
   sets and these structures are visible in 2-d projections, it is
   possible to confirm that a machine learning model is labeling these
   structures correctly. a secondary check is to confirm that similar
   attributes of structures are projected relatively near one another and
   different attributes of structures are projected relatively far from
   one another. consider a model used to classify or cluster marketing
   segments. it is reasonable to expect a machine learning model to label
   older, richer customers differently than younger, less affluent
   customers   and moreover, to expect that these different groups should be
   relatively disjointed and compact in a projection, and relatively far
   from one another. such results should also be stable under minor
   perturbations of the training or test data, and projections from
   perturbed versus non-perturbed samples can be used to check for
   stability or for potential patterns of change over time.

partial dependence plots

   partial dependence plots from a gradient boosted tree ensemble model
   figure 5. one-dimensional partial dependence plots from a gradient
   boosted tree ensemble model of the well-known california housing data
   set. image courtesy patrick hall and the h2o.ai team.

   partial dependence plots show us the way machine-learned response
   functions change based on the values of one or two independent
   variables of interest, while averaging out the effects of all other
   independent variables. partial dependence plots with two independent
   variables are particularly useful for visualizing complex types of
   variable interactions between the independent variables of interest.
   partial dependence plots can be used to verify monotonicity of response
   functions under monotonicity constraints, and they can be used to see
   the nonlinearity, non-monotonicity, and two-way interactions in very
   complex models. in fact, the way partial dependence plots enhance
   understanding is exactly by showing the nonlinearity, non-monotonicity,
   and two-way interactions between independent variables and a dependent
   variable in complex models. they can also enhance trust when displayed
   relationships conform to domain knowledge expectations, when the plots
   remain stable or change in expected ways over time, or when displayed
   relationships remain stable under minor perturbations of the input
   data.

   partial dependence plots are global in terms of the rows of a data set,
   but local in terms of the independent variables. they are used almost
   exclusively to show the relationship between one or two independent
   variables and the dependent variable over the domain of the independent
   variable(s). [41]individual conditional expectation (ice) plots, a
   newer and less well-known adaptation of partial dependence plots, can
   be used to create more localized explanations using the same ideas as
   partial dependence plots. ice plots are particularly useful when there
   are strong relationships between many input variables.

residual analysis

   example residual analysis application figure 6. screenshot from an
   example residual analysis application. image courtesy of [42]micah
   stubbs and the h2o.ai team.

   residuals refer to the difference between the recorded value of a
   dependent variable and the predicted value of a dependent variable for
   every row in a data set. generally, the residuals of a well-fit model
   should be randomly distributed because good models will account for
   most phenomena in a data set, except for random error. plotting the
   residual values against the predicted values is a time-honored model
   assessment technique and a great way to see all your modeling results
   in two dimensions. if strong patterns are visible in plotted residuals,
   this is a dead giveaway that there are problems with your data, your
   model, or both. vice versa, if models are producing randomly
   distributed residuals, this a strong indication of a well-fit,
   dependable, trustworthy model, especially if other fit statistics
   (i.e., r^2, auc, etc.) are in the appropriate ranges.

   in figure 6, the callouts point to a strong linear pattern in the
   residuals. the plot shows the traditional residual plot and residuals
   plotted by certain independent variables. breaking out the residual
   plot by independent variables can expose more granular information
   about residuals and assist in reasoning through the cause of non-random
   patterns. figure 6 also points to outliers, which residual plots can
   help to identify. as many machine learning algorithms seek to minimize
   squared residuals, observations with high residual values will have a
   strong impact on most models, and human analysis of the validity of
   these outliers can have a big impact on model accuracy.

   now that several visualization techniques have been presented, they can
   be tied back to the overarching concepts scope, complexity,
   understanding and trust by asking a few simple questions. these
   questions will be asked of techniques presented in later sections as
   well.

   do visualizations provide global or local interpretability?

   both. most forms of visualizations can be used to see a courser view of
   the entire data set, or they can provide granular views of local
   portions of the data set. ideally, advanced visualization tool kits
   enable users to pan, zoom, and drill-down easily. otherwise, users can
   plot different parts of the data set at different scales themselves.

   what complexity of functions can visualizations help interpret?

   visualizations can help explain functions of all complexities.

   how do visualizations enhance understanding?

   for most people, visual representations of structures (clusters,
   hierarchy, sparsity, outliers) and relationships (correlation) in a
   data set are easier to understand than scrolling through plain rows of
   data and looking at each variable's values.

   how do visualizations enhance trust?

   seeing structures and relationships in a data set usually makes those
   structures and relationships easier to understand. an accurate machine
   learning model should create answers that are representative of the
   structures and relationships in a data set. understanding the
   structures and relationships in a data set is a first step to knowing
   if a model   s answers are trustworthy.

   in certain cases, visualizations can display the results of sensitivity
   analysis, which can also enhance trust in machine learning results. in
   general, visualizations themselves can sometimes be thought of as a
   type of sensitivity analysis when they are used to display data or
   models as they change over time, or as data are intentionally changed
   to test stability or important corner cases for your application.

part 2: using machine learning in regulated industry

   for analysts and data scientists working in regulated industries, the
   potential boost in predictive accuracy provided by machine learning
   algorithms may not outweigh their current realities of internal
   documentation needs and external regulatory responsibilities. for these
   practitioners, traditional linear modeling techniques may be the only
   option for predictive modeling. however, the forces of innovation and
   competition don   t stop because you work under a regulatory regime. data
   scientists and analysts in the regulated verticals of banking,
   insurance, and other similar industries face a unique conundrum. they
   must find ways to make more and more accurate predictions, but keep
   their models and modeling processes transparent and interpretable.

   the techniques presented in this section are newer types of linear
   models or models that use machine learning to augment traditional,
   linear modeling methods. linear model interpretation techniques are
   highly sophisticated, typically model specific, and the inferential
   features and capabilities of linear models are rarely found in other
   classes of models. these techniques are meant for practitioners who
   just can   t use machine learning algorithms to build predictive models
   because of interpretability concerns. these models produce linear,
   monotonic response functions (or at least monotonic ones) with globally
   interpretable results like those of traditional linear models, but
   often with a boost in predictive accuracy provided by machine learning
   algorithms.

ols regression alternatives

penalized regression

   shrunken feasible regions for l1/lasso penalized regression parameters
   (left) and l2/ridge penalized regression parameters (right) figure 7.
   shrunken feasible regions for l1/lasso penalized regression parameters
   (left) and l2/ridge penalized regression parameters (right). image
   courtesy patrick hall, tomas nykodym, and the h2o.ai team.

   [43]ordinary least squares (ols) regression is about 200 years old.
   maybe it   s time to move on? as an alternative, penalized regression
   techniques can be a gentle introduction to machine learning.
   contemporary penalized regression techniques usually combine
   [44]l1/lasso penalties for variable selection purposes and
   [45]tikhonov/l2/ridge penalties for robustness in a technique known as
   [46]elastic net. they also make fewer assumptions about data than ols
   regression. instead of solving the classic normal equation or using
   statistical tests for variable selection, penalized regression
   minimizes constrained objective functions to find the best set of
   regression parameters for a given data set. typically, this is a set of
   parameters that model a linear relationship but also satisfy certain
   penalties for assigning correlated or meaningless variables to large
   regression coefficients. you can learn all about penalized regression
   in [47]elements of statistical learning, but for our purposes here,
   it   s just important to know when you might want to try penalized
   regression.

   penalized regression has been applied widely across many research
   disciplines, but it is a great fit for business data with many columns,
   even data sets with more columns than rows, and for data sets with a
   lot of correlated variables. l1/lasso penalties drive unnecessary
   regression parameters to zero, selecting a small, representative subset
   of regression parameters for the regression model while avoiding
   potential multiple comparison problems that arise in forward, backward,
   and stepwise variable selection. tikhonov/l2/ridge penalties help
   preserve parameter estimate stability, even when many correlated
   variables exist in a wide data set or important predictor variables are
   correlated. it   s also important to know penalized regression techniques
   don   t always create confidence intervals, t-statistics, or p-values for
   regression parameters. these types of measures are typically only
   available through iterative methods or id64 that can require
   extra computing time.

generalized additive models (gams)

   spline functions figure 8. spline functions for several variables
   created by a generalized additive model. image courtesy patrick hall
   and the h2o.ai team.

   [48]generalized additive models (gams) enable you to hand-tune a
   tradeoff between increased accuracy and decreased interpretability by
   fitting standard regression coefficients to certain variables and
   nonlinear spline functions to other variables. also, most
   implementations of gams generate convenient plots of the fitted
   splines. depending on your regulatory or internal documentation
   requirements, you may be able to use the splines directly in predictive
   models for increased accuracy. if not, you may be able to eyeball the
   fitted spline and switch it out for a more interpretable polynomial,
   log, trigonometric or other simple function of the predictor variable
   that may also increase predictive accuracy. you can learn more about
   gams in elements of statistical learning, too.

quantile regression

   quantile regression in two dimensions figure 9. an illustration of
   quantile regression in two dimensions. figure courtesy of patrick hall
   and the h2o.ai team.

   [49]quantile regression allows you to fit a traditional, interpretable,
   linear model to different percentiles of your training data, allowing
   you to find different sets of variables with different parameters for
   modeling different behaviors across a customer market or portfolio of
   accounts. it probably makes sense to model low-value customers with
   different variables and different parameter values from those of
   high-value customers, and quantile regression provides a statistical
   framework for doing so.

   do alternative regression techniques provide global or local
   interpretability?

   alternative regression techniques often produce globally interpretable
   linear, monotonic functions that can be interpreted using coefficient
   values or other traditional regression measures and statistics.

   what are the complexity of alternative regression functions?

   alternative regression functions are generally linear, monotonic
   functions. however, gam approaches can create quite complex nonlinear
   functions.

   how do alternative regression techniques enhance understanding?

   it   s quite possible that the lessened assumption burden, the ability to
   select variables without potentially problematic multiple statistical
   significance tests, the ability to incorporate important but correlated
   predictors, the ability to fit nonlinear phenomena, or the ability to
   fit different quantiles of the data's conditional distribution (and not
   just the mean of the conditional distribution) could lead to more
   accurate understanding of modeled phenomena.

   how do alternative regression techniques enhance trust?

   basically, these techniques are trusted linear models, but used in new
   and different ways. trust could be increased further if these
   techniques lead to more accurate results for your application.

build toward machine learning model benchmarks

   assessment plots that compare linear models with interactions to
   machine learning algorithms figure 10. assessment plots that compare
   linear models with interactions to machine learning algorithms. figure
   courtesy of patrick hall and the h2o.ai team.

   two of the main differences between machine learning algorithms and
   traditional linear models are that machine learning algorithms
   incorporate many implicit, high-degree variable interactions into their
   predictions and that machine learning algorithms create nonlinear,
   non-polynomial, non-monotonic, and even non-continuous response
   functions.

   if a machine learning algorithm is seriously outperforming a
   traditional linear model, fit a decision tree to your inputs and target
   and generate a plot of the tree. the variables that are under or over
   one another in each split typically have strong interactions. try
   adding some of these interactions into the linear model, including
   high-degree interactions that occur over several levels of the tree. if
   a machine learning algorithm is vastly outperforming a traditional,
   linear model, also try breaking it into several piecewise linear
   models. gams or partial dependence plots are ways to see how
   machine-learned response functions treat a variable across its domain
   and can give insight into where and how piecewise models could be used.
   multivariate adaptive regression splines is a statistical technique
   that can automatically discover and fit different linear functions to
   different parts of a complex, nonlinear conditional distribution. you
   can try multivariate adaptive regression splines to fit piecewise
   models directly.

   does building toward machine learning model benchmarks provide global
   or local interpretability?

   if linearity and monotonicity are maintained, this process will result
   in globally interpretable linear, monotonic functions. if piecewise
   functions are used, building toward machine learning model benchmarks
   could provide local interpretability, but potentially at the expense of
   global interpretability.

   what complexity of function does building toward machine learning model
   benchmarks create?

   with caution, testing, and restraint, building toward machine learning
   benchmarks can preserve the linearity and monotonicity of traditional
   linear models. however, adding many interactions or piecewise
   components will result in extremely complex response functions.

   how does building toward machine learning model benchmarks enhance
   understanding?

   this process simply uses traditional, understandable models in a new
   way. building toward machine learning model benchmarks could lead to
   greater understanding if more data exploration or techniques such as
   gams, partial dependence plots, or multivariate adaptive regression
   splines lead to deeper understanding of interactions and nonlinear
   phenomena in a data set.

   how does building toward machine learning model benchmarks enhance
   trust?

   this process simply uses traditional, trusted models in a new way.
   building toward machine learning model benchmarks could lead to
   increased trust in models if additional data exploration or techniques
   such as gams, partial dependence plots, or multivariate adaptive
   regression splines create linear models that represent the phenomenon
   of interest in the data set more accurately.

machine learning in traditional analytics processes

   potential uses for machine learning in traditional analytical processes
   figure 11. diagrams of several potential uses for machine learning in
   traditional analytical processes. figure courtesy of [50]vinod iyengar
   and the h2o.ai team.

   instead of using machine learning predictions directly for analytical
   decisions, traditional analytical lifecycle processes (such as data
   preparation and model deployment) can be augmented with machine
   learning techniques leading to potentially more accurate predictions
   from regulator-approved linear, monotonic models. figure 11 outlines
   three possible scenarios in which analytical processes can be augmented
   with machine learning:
     * introduce complex predictors into traditional, linear models:
       introducing interactions, polynomials, or simple functional
       transformations into linear models is a standard practice. machine
       learning algorithms can be used to create different types of
       nonlinear and non-polynomial predictors that can also represent
       high-degree interactions between independent variables. there are
       many options for creating these predictors. examples include the
       nonlinear features extracted by autoencoder networks or the optimal
       bins represented by the terminal node labels of a decision tree.

     * use multiple gated linear models: very often, segmenting data into
       smaller groups based on important data attributes or time and
       building linear models for each segment can lead to more accurate
       results. it is not uncommon for organizations to use several
       deployed linear models to handle different market segments or
       different times of year. deciding how to manually fuse the
       predictions of these different models can be a tedious task for
       analysts and data scientists. however, if data is collected about
       past model performance, this process can be automated by allowing a
       gate model to decide which linear model an observation should be
       delegated to for a decision.

     * predict linear model degradation: in most cases, models are trained
       on static snapshots of data and then validated on later snapshots
       of similar data. though an accepted practice, this process leads to
       model degradation when the real-world phenomena represented in the
       training and validation data start to change. such degradation
       could occur when competitors enter or leave a market, when
       macroeconomic factors change, or when consumer fads change, and for
       many other common reasons. if data is collected about market and
       economic factors and about past model performance, another model
       can be used to predict when traditional deployed models need to be
       retrained or replaced. like changing an expensive mechanical
       component before it requires maintenance, models can be retrained
       or replaced before their predictive power lessens. (i   ve
       [51]written previously about the topic of test error in applied
       machine learning and ways that machine learning should be used in
       the real world.)

   of course, there are many other opportunities for incorporating machine
   learning into the lifecycle of a traditional model. you may have better
   ideas or implementations in place already!

   does incorporation of machine learning into traditional analytical
   processes provide global or local interpretability?

   it generally attempts to retain the global interpretability of
   traditional linear models. however, adding features extracted by
   machine learning algorithms into a linear model can reduce global
   interpretability.

   what complexity of function does incorporating machine learning into
   traditional analytical processes create?

   the goal is to continue using linear, monotonic response functions, but
   in more efficient and automated ways.

   how does the incorporation of machine learning into traditional
   analytical processes enhance understanding?

   incorporating machine learning models into traditional analytical
   processes aims to use linear, understandable models more efficiently
   and accurately. understanding can be enhanced further if the process of
   adding nonlinear features to a linear model, using gated models, or
   forecasting model degradation leads to deeper knowledge of driving
   phenomena that create nonlinearity, trends, or changes in your data.

   how does the incorporation of machine learning into traditional
   analytical processes enhance trust?

   it can help make our understandable models more accurate, and if
   augmentation does lead to increased accuracy, this is an indication
   that the pertinent phenomena in the data have been modeled in a more
   trustworthy, dependable fashion.

small, interpretable ensembles

   diagram of a small, stacked ensemble figure 12. a diagram of a small,
   stacked ensemble. figure courtesy of vinod iyengar and the h2o.ai team.

   many organizations are so adept at traditional linear modeling
   techniques that they simply cannot squeeze much more accuracy out of
   any single model. one potential way to increase accuracy without losing
   too much interpretability is to combine the predictions of a small
   number of well-understood models. the predictions can simply be
   averaged, manually weighted, or combined in more mathematically
   sophisticated ways. for instance, predictions from the best overall
   model for a certain purpose can be combined with another model for the
   same purpose that excels at rare id37. an analyst or data
   scientist could do experiments to determine the best weighting for the
   predictions of each model in a simple ensemble, and partial dependency
   plots could be used to ensure that the model inputs and predictions
   still behave monotonically with respect to one another.

   if you prefer or require a more rigorous way to combine model
   predictions, then [52]super learners are a great option. super learners
   are a specific implementation of [53]stacked generalization introduced
   by wolpert in the early 1990s. stacked generalization uses a combiner
   model to decide the weighting for the constituent predictions in the
   ensemble. overfitting is a serious concern when stacking models. super
   learners prescribe an approach for cross-validation and add constraints
   on the prediction weights in the ensemble to limit overfitting and
   increase interpretability. figure 12 is an illustration of
   cross-validated predictions from two id90 and a linear
   regression being combined by another decision tree in a stacked
   ensemble.

   do small, interpretable ensembles provide global or local
   interpretability?

   they provide increased accuracy, but may decrease overall global
   interpretability. they do not affect the interpretability of each
   individual constituent model, but the resulting ensemble model may be
   more difficult to interpret.

   what complexity of function do small, interpretable ensembles create?

   they can create very complex response functions. to ensure
   interpretability is preserved, use the lowest possible number of
   individual constituent models, use simple, linear combinations of
   constituent models, and use partial dependence plots to check that
   linear or monotonic relationships have been preserved.

   how do small, interpretable ensembles enhance understanding?

   they enhance understanding if the process of combining interpretable
   models leads to greater awareness and familiarity with phenomena in
   your data that positively impacts generalization and predictions on
   future data.

   how do small, interpretable ensembles enhance trust?

   they allow us to boost the accuracy of traditional trustworthy models
   without sacrificing too much interpretability. increased accuracy is an
   indication that the pertinent phenomena in the data have been modeled
   in a more trustworthy, dependable fashion. trust can be further
   enhanced by small, interpretable ensembles when models complement each
   other in ways that conform to human expectations and domain knowledge.

monotonicity constraints

   monotonic data and model constraints for neural networks figure 13. an
   illustration of monotonic data and model constraints for neural
   networks. figure courtesy of vinod iyengar and the h2o.ai team.

   monotonicity constraints can turn difficult-to-interpret nonlinear,
   non-monotonic models into highly interpretable, and possibly
   regulator-approved, nonlinear, monotonic models. monotonicity is very
   important for at least two reasons:
    1. monotonicity is often expected by regulators: no matter what a
       training data sample says, regulators may still want to see
       monotonic behavior. consider savings account balances in credit
       scoring. a high savings account balance should be an indication of
       creditworthiness, whereas a low savings account balance should be
       an indicator of potential default risk. if a certain batch of
       training data contains many examples of individuals with high
       savings account balances defaulting on loans or individuals with
       low savings account balances paying off loans, of course a
       machine-learned response function trained on this data would be
       non-monotonic with respect to savings account balance. this type of
       predictive function could be unsatisfactory to regulators because
       it defies decades of accumulated domain expertise and thus
       decreases trust in the model or sample data.
    2. monotonicity enables consistent reason code generation: consistent
       reason code generation is generally considered a gold standard of
       model interpretability. if monotonicity is guaranteed by a credit
       scoring model, reasoning about credit applications is
       straightforward and automatic. if someone's savings account balance
       is low, their credit worthiness is also low. once monotonicity is
       assured, reasons for credit decisions can then be reliably ranked
       using the max-points-lost method. the max-points-lost method places
       an individual on the monotonic, machine-learned response surface
       and measures their distance from the maximum point on the surface
       (i.e., the ideal, most creditworthy possible customer). the axis
       (e.g., independent variable) on which an individual is the farthest
       from the ideal customer is the most important negative reason code
       for a credit decision. the axis (e.g., independent variable) on
       which an individual is the closest to the ideal customer is the
       least important negative reason code for a credit decision, and
       other independent variables are ranked as reason codes between
       these two given the position of the individual in relation to the
       ideal customer. monotonicity simply ensures clear, logical
       reasoning using the max-points-lost method: under a monotonic
       model, an individual who was granted a loan could never have a
       lower savings account balance than an individual who was denied a
       loan.

   monotonicity can arise from constraints on input data, constraints on
   generated models, or from both. figure 13 represents a process where
   carefully chosen and processed non-negative, monotonic independent
   variables are used in conjunction with a single hidden layer neural
   network training algorithm that is constrained to produce only positive
   parameters. this training combination generates a nonlinear, monotonic
   response function from which reason codes can be calculated, and by
   analyzing model parameter values, high-degree interactions can be
   identified. finding and creating such non-negative, monotonic
   independent variables can be a tedious, time-consuming, trial-and-error
   task. luckily, neural network and tree-based response functions can
   usually be constrained to be monotonic with respect to any given
   independent variable without burdensome id174
   requirements. [54]monotonic neural networks often entail custom
   architecture and constraints on the values of the generated model
   parameters. for [55]tree-based models, monotonicity constraints are
   usually enforced by a uniform splitting strategy, where splits of a
   variable in one direction always increase the average value of the
   dependent variable in the resultant child node, and splits of the
   variable in the other direction always decrease the average value of
   the dependent variable in resultant child node. as implementations of
   monotonicity constraints vary for different types of models in
   practice, they are a model-specific interpretation technique.

   do monotonicity constraints provide global or local interpretability?

   monotonicity constraints create globally interpretable response
   functions.

   what complexity of function do monotonicity constraints create?

   they create nonlinear, monotonic response functions.

   how do monotonicity constraints enhance understanding?

   they enable automatic generation of reason codes and for certain cases
   (i.e., single hidden-layer neural networks and single id90)
   important, high-degree variable interactions can also be automatically
   determined.

   how do monotonicity constraints enhance trust?

   trust is increased when monotonic relationships, reason codes, and
   detected interactions are parsimonious with domain expertise or
   reasonable expectations. sensitivity analysis can also increase trust
   in the model if results remain stable when input data undergoes minor
   perturbations and if the model changes in dependable, predictable ways
   over time.

part 3: understanding complex machine learning models

   the techniques presented in this part can create interpretations for
   nonlinear, non-monotonic response functions. they can be used alongside
   techniques discussed in parts 1 and 2 to increase the interpretability
   of all model types. practitioners might need to use more than one of
   the interpretability techniques described below to derive satisfactory
   explanations for their most complex models.

surrogate models

   illustration of surrogate models figure 14. an illustration of
   surrogate models for explaining a complex neural network. figure
   courtesy of patrick hall and the h2o.ai team.

   a surrogate model is a simple model that is used to explain a complex
   model. surrogate models are usually created by training a linear
   regression or decision tree on the original inputs and predictions of a
   complex model. coefficients, variable importance, trends, and
   interactions displayed in the surrogate model are then assumed to be
   indicative of the internal mechanisms of the complex model. there are
   few, possibly no, theoretical guarantees that the simple surrogate
   model is highly representative of the more complex model.

   what is the scope of interpretability for surrogate models?

   generally, surrogate models are global. the globally interpretable
   attributes of a simple model are used to explain global attributes of a
   more complex model. however, there is nothing to preclude fitting
   surrogate models to more local regions of a complex model's conditional
   distribution, such as clusters of input records and their corresponding
   predictions, or deciles of predictions and their corresponding input
   rows. because small sections of the conditional distribution are more
   likely to be linear, monotonic, or otherwise well-behaved, local
   surrogate models can be more accurate than global surrogate models.
   [56]local interpretable model-agnostic explanations (discussed in the
   next section) is a formalized approach for local surrogate models. of
   course, global and local surrogate models can be used together to
   foster both global and local interpretability.

   what complexity of functions can surrogate models help explain?

   surrogate models can help explain machine learning models of any
   complexity, but they are probably most helpful for nonlinear,
   non-monotonic models.

   how do surrogate models enhance understanding?

   surrogate models enhance understanding because they provide insight
   into the internal mechanisms of complex models.

   how do surrogate models enhance trust?

   surrogate models enhance trust when their coefficients, variable
   importance, trends, and interactions are in line with human domain
   knowledge and reasonable expectations of modeled phenomena. surrogate
   models can increase trust when used in conjunction with sensitivity
   analysis to test that explanations remain stable and in line with human
   domain knowledge, and reasonable expectations when data is lightly and
   purposefully perturbed, when interesting scenarios are simulated, or as
   data changes over time.

local interpretable model-agnostic explanations (lime)

   the lime process figure 15. an illustration of the lime process in
   which a weighted linear model is used to explain a single prediction
   from a complex neural network. figure courtesy of [57]marco tulio
   ribeiro; [58]image used with permission.

   [59]lime is a prescribed method for building local surrogate models
   around single observations. it is meant to shed light on how decisions
   are made for specific observations. lime requires that a set of
   explainable records be found, simulated, or created. this set of
   well-understood records is used to explain how machine learning
   algorithms make decisions for other, less well-understood records. an
   implementation of lime may proceed as follows. first, the set of
   explainable records are scored using the complex model. then, to
   interpret a decision about another record, the explanatory records are
   weighted by their closeness to that record, and an l1 regularized
   linear model is trained on this weighted explanatory set. the
   parameters of the linear model then help explain the prediction for the
   selected record.

   lime was originally described in the context of explaining image or
   text classification decisions. it could certainly also be applied to
   business or customer data, for instance by explaining customers at
   every decile of predicted probabilities for default or churn, or by
   explaining representative customers from well-known market segments.
   multiple implementations of lime are available. two i see the most
   often are from the [60]original authors of lime and from the [61]eli5
   package, which implements several handy machine learning
   interpretability tools.

   what is the scope of interpretability for lime?

   lime is a technique for local interpretability.

   what complexity of functions can lime help explain?

   lime can help explain machine learning models of any complexity, but it
   is probably most appropriate for nonlinear, non-monotonic models.

   how does lime enhance understanding?

   lime provides insight into the important variables and their linear
   trends around specific, important observations, even for extremely
   complex response functions.

   how does lime enhance trust?

   lime increases trust when the important variables and their linear
   trends around specific records conform to human domain knowledge and
   reasonable expectations of modeled phenomena. lime can also enhance
   trust when used in conjunction with maximum activation analysis to see
   that a model treats obviously different records using different
   internal mechanisms and obviously similar records using similar
   internal mechanisms. lime can even be used as a type of sensitivity
   analysis to determine whether explanations remain stable and in line
   with human domain knowledge and expectations when data is intentionally
   and subtly perturbed, when pertinent scenarios are simulated, or as
   data changes over time.

maximum activation analysis

   inputs activating different neurons in a neural network figure 16.
   illustration of different inputs activating different neurons in a
   neural network. image courtesy of patrick hall and the h2o.ai team.

   in maximum activation analysis, examples are found or simulated that
   maximally activate certain neurons, layers, or filters in a neural
   network or certain trees in decision tree ensembles. for the purposes
   of maximum activation analysis, low residuals for a certain tree are
   analogous to high-magnitude neuron output in a neural network. like
   monotonicity constraints in part 2, maximum activation analysis is a
   general idea that is likely a model-specific interpretation technique
   in-practice.

   maximum activation analysis elucidates internal mechanisms of complex
   models by determining the parts of the response function that specific
   observations or groups of similar observations excite to the highest
   degree, either by high-magnitude output from neurons or by low residual
   output from trees. maximum activation analysis can also find
   interactions when different types of observations consistently activate
   the same internal structures of a model.

   figure 16 is an idealized illustration of a good customer causing
   high-magnitude outputs from a different set of neurons than a
   fraudulent customer. here the red highlighting indicates the three
   maximally activated neurons for each type of input. the maximally
   activated neurons are different for these two very different inputs,
   indicating that the internal structure of the model treats input
   classes differently. if this pattern were to be consistent for many
   different examples of good customers and fraudulent customers, it would
   also indicate that internal model mechanisms are stable and dependable.

   what is the scope of interpretability for maximum activation analysis?

   maximum activation analysis is local in scope because it illustrates
   how certain observations or groups of observations are treated by
   discernible aspects of a complex response function.

   what complexity of functions can maximum activation analysis help
   explain?

   maximum activation analysis can help explain machine learning models of
   any complexity, but it is probably best suited for nonlinear,
   non-monotonic models.

   how does maximum activation analysis enhance understanding?

   maximum activation analysis increases understanding because it exposes
   the internal structures of complex models.

   how does maximum activation analysis enhance trust?

   lime, discussed above, helps explain the predictions of a model in
   local regions of the conditional distribution. maximum activation
   analysis helps enhance trust in localized, internal mechanisms of a
   model. the two make a great pair for creating detailed, local
   explanations of complex response functions. maximum activation analysis
   enhances trust when a complex model handles obviously different records
   using different internal mechanisms and obviously similar records using
   similar internal mechanisms. it enhances trust when these internal
   mechanisms are consistently and repeatedly different for many input
   examples. it enhances trust when found interactions match human domain
   knowledge or expectations, and maximum activation analysis also
   enhances trust when used as a type of sensitivity analysis. it can be
   used to investigate if internal treatment of observations remains
   stable when data is lightly perturbed, when interesting situations are
   simulated, or as data changes over time.

sensitivity analysis

   a variable   s distribution changing over time figure 17. a
   representation of a variable   s distribution changing over time. figure
   courtesy of patrick hall and the h2o.ai team.

   sensitivity analysis investigates whether model behavior and outputs
   remain stable when data is intentionally perturbed or other changes are
   simulated in data. beyond traditional assessment practices, sensitivity
   analysis of machine learning model predictions is perhaps the most
   important validation technique for machine learning models. machine
   learning models can make drastically differing predictions from minor
   changes in input variable values. in practice, many linear model
   validation techniques focus on the numerical instability of regression
   parameters due to correlation between input variables or between input
   variables and the dependent variable. it may be prudent for those
   switching from linear modeling techniques to machine learning
   techniques to focus less on numerical instability of model parameters
   and to focus more on the potential instability of model predictions.

   sensitivity analysis can also test model behavior and outputs when
   interesting situations or known corner cases are simulated. different
   techniques, including those presented in this article and many others,
   can be used to conduct sensitivity analysis. output distributions,
   error measurements, plots, and interpretation techniques can be used to
   explore the way models behave in important scenarios, how they change
   over time, or if models remain stable when data is subtly and
   intentionally corrupted.

   what is the scope of interpretability for sensitivity analysis?

   sensitivity analysis is a global interpretation technique when global
   interpretation techniques are used, such as using a single, global
   surrogate model to ensure major interactions remain stable when data is
   lightly and purposely corrupted.

   sensitivity analysis is a local interpretation technique when local
   interpretation techniques are used, for instance using lime to
   determine if the important variables in a credit allocation decision
   remain stable for a given customer segment under macroeconomic stress
   testing.

   what complexity of functions can sensitivity analysis help explain?

   sensitivity analysis can help explain the predictions of any type of
   response function, but is probably most appropriate for nonlinear
   response functions and response functions that model high degree
   variable interactions. for both cases, small changes in input variable
   values can result in large changes in a predicted response.

   how does sensitivity analysis enhance understanding?

   sensitivity analysis enhances understanding because it shows a model's
   likely behavior and output in important situations, and how a model's
   behavior and output may change over time.

   how does sensitivity analysis enhance trust?

   sensitivity analysis enhances trust when a model's behavior and outputs
   remain stable when data is subtly and intentionally corrupted. it also
   increases trust if models adhere to human domain knowledge and
   expectations when interesting situations are simulated, or as data
   changes over time.

variable importance measures

   for nonlinear, non-monotonic response functions, variable importance
   measures are often the only commonly available quantitative measure of
   the machine-learned relationships between independent variables and the
   dependent variable in a model. variable importance measures rarely give
   insight into even the average direction that a variable affects a
   response function. they simply state the magnitude of a variable's
   relationship with the response as compared to other variables used in
   the model.

global variable importance measures

   variable importance in a decision tree ensemble model figure 18. an
   illustration of variable importance in a decision tree ensemble model.
   figure courtesy of patrick hall and the h2o.ai team.

   variable importance measures are typically seen in tree-based models
   but are sometimes also reported for other models. as illustrated in
   figure 18, a simple heuristic rule for variable importance in a
   decision tree is related to the depth and frequency at which a variable
   is split on in a tree, where variables used higher in the tree and more
   frequently in the tree are more important. for a single decision tree,
   a variable's importance is quantitatively determined by the cumulative
   change in the splitting criterion for every node in which that variable
   was chosen as the best splitting candidate. for a gradient boosted tree
   ensemble, variable importance is calculated as it is for a single tree
   but aggregated for the ensemble. for a id79, variable
   importance is also calculated as it is for a single tree and
   aggregated, but an additional measure of variable importance is
   provided by the change in out-of-bag accuracy caused by shuffling the
   independent variable of interest, where larger decreases in accuracy
   are taken as larger indications of importance. (shuffling is seen as
   zeroing-out the effect of the given independent variable in the model,
   because other variables are not shuffled.) for neural networks,
   variable importance measures are typically associated with the
   aggregated, absolute magnitude of model parameters for a given variable
   of interest. global variable importance techniques are typically model
   specific, and practitioners should be aware that unsophisticated
   measures of variable importance can be biased toward larger scale
   variables or variables with a high number of categories.

leave-one-covariate-out (loco)

   the loco approach figure 19. an illustration of the loco approach.
   figure courtesy of patrick hall and the h2o.ai team.

   a recent [62]preprint article put forward a local, model-agnostic
   generalization of mean accuracy decrease variable importance measures
   called leave-one-covariate-out, or loco. loco was originally described
   in the context of regression models, but the general idea of loco is
   model agnostic and can be implemented in various ways. a general
   implementation of loco may proceed as follows. loco creates local
   interpretations for each row in a training or unlabeled score set by
   scoring the row of data once and then again for each input variable
   (e.g., covariate) in the row. in each additional scoring run, one input
   variable is set to missing, zero, its mean value, or another
   appropriate value for leaving it out of the prediction. the input
   variable with the largest absolute impact on the prediction for that
   row is taken to be the most important variable for that row's
   prediction. variables can also be ranked by their impact on the
   prediction on a per-row basis. loco also creates global variable
   importance measures by estimating the mean change in accuracy for each
   variable over an entire data set and can even provide confidence
   intervals for these global estimates of variable importance.

   what is the scope of interpretability for variable importance measures?

   variable importance measures are usually global in scope; however, the
   loco approach offers local variable importance measures for each row in
   a training set or in new data.

   what complexity of functions can variable importance measures help
   explain?

   variable importance measures are most useful for nonlinear,
   non-monotonic response functions but can be applied to many types of
   machine-learned response functions.

   how do variable importance measures enhance understanding?

   variable importance measures increase understanding because they tell
   us the most influential variables in a model and their relative rank.

   how do variable importance measures enhance trust?

   variable importance measures increase trust if they are in line with
   human domain knowledge and reasonable expectations. they also increase
   trust if they remain stable when data is lightly and intentionally
   perturbed, and if they change in acceptable ways as data changes over
   time or when pertinent scenarios are simulated.

treeinterpreter

   single decision tree figure 20. a single decision tree with a
   highlighted decision path. figure courtesy of [63]@crossid178;
   [64]image used with permission.

   several [65]average tree interpretation approaches have been proposed
   over the years, but the simple, open source package known as
   [66]treeinterpreter has become popular in recent months.
   treeinterpreter decomposes decision tree and id79 predictions
   into bias (overall training data average) and component terms from each
   independent variable used in the model. tree interpreter is model
   specific to algorithms based on id90. figure 20 portrays the
   decomposition of the decision path into bias and individual
   contributions for a decision tree. the red branches in figure 20
   illustrate the decision path for a certain record of data through this
   single decision tree created from the results of treeinterpreter.
   (treeinterpreter simply outputs a list of the bias and individual
   contributions for a variable in a given model or the contributions the
   input variables in a single record make to a single prediction.) the
   [67]eli5 package also has an implementation of treeinterpreter.

   what is the scope of interpretability for treeinterpreter?

   treeinterpreter is global in scope when it represents average
   contributions of independent variables to an overall decision tree or
   id79 model prediction. it is local in scope when used to
   explain certain predictions.

   what complexity of functions can treeinterpreter help explain?

   treeinterpreter is meant to explain the usually nonlinear,
   non-monotonic response functions created by decision tree and random
   forest algorithms.

   how does treeinterpreter enhance understanding?

   treeinterpreter increases understanding by displaying average, ranked
   contributions of independent variables to the predictions of decision
   tree and id79 models.

   how does treeinterpreter enhance trust?

   treeinterpreter enhances trust when displayed variable contributions
   conform to human domain knowledge or reasonable expectations.
   treeinterpreter also enhances trust if displayed explanations remain
   stable when data is subtly and intentionally corrupted, and if
   explanations change in appropriate ways as data changes over time or
   when interesting scenarios are simulated.

conclusion

   over the last few months, as my friends and colleagues heard that i was
   compiling this article, the pace at which they emailed, texted,
   tweeted, and slacked me regarding new work in this field has only
   accelerated. currently, i probably see two new libraries, algorithms,
   or papers a day, and i can   t possibly keep up with adding them into
   this overview article. the fact is, this article will always be a
   finite document and has to stop somewhere. so, it stops here for now. i
   believe the article has put forward a useful ontology for understanding
   machine learning interpretability techniques moving into the future by
   categorizing them based on four criteria: their scope (local versus
   global), the complexity of response function they can explain, their
   application domain (model specific versus model agnostic), and how they
   can enhance trust and understanding. i also believe the article has
   covered the main categories of techniques, especially from an applied
   and commercial usage perspective. if i had more time and could keep
   adding to the article indefinitely, i think two topics i would
   prioritize for readers today would be [68]rulefit and the multiple
   advances in making deep learning more interpretable, one example of
   many being [69]learning deep k-nearest neighbor representations. these
   are two things i   m certainly looking forward to learning more about
   myself.

   with so many new developments occurring so quickly, it   s truly an
   exciting time to be working on this subject. i hope you find the
   information in this article helpful. for those of us working on the
   subject, i hope you are as inspired as i am about the future of making
   machine learning and artificial intelligence more useful and
   transparent to users and consumers of these revolutionary technologies.

   related resources:
     * [70]learning path: machine learning
     * [71]practical machine learning with h2o
     * [72]hands-on machine learning with scikit-learn and tensorflow
     * [73]thoughtful machine learning with python

   [74]machine learning: a quick and simple definition. get a basic
   overview of machine learning and then go deeper with recommended
   resources.
   article image: inputs activating different neurons in a neural network.
   (source: image courtesy of patrick hall and the h2o.ai team, used with
   permission).

   share
    1. [75]tweet
    2.
    3.
     __________________________________________________________________

   [76]patrick hall

[77]patrick hall

   patrick hall is senior director for data science products at h2o.ai
   where he focuses mainly on model interpretability and model management.
   patrick is also currently an adjunct professor in the department of
   decision sciences at george washington university, where he teaches
   graduate classes in data mining and machine learning. prior to joining
   h2o.ai, patrick held global customer facing roles and research and
   development roles at sas institute.
   [78]more
   [79]wen phan

[80]wen phan

   wen phan is a senior solutions architect at h2o.ai. wen works with
   customers and organizations to architect systems, smarter applications,
   and data products to make better decisions, achieve positive outcomes,
   and transform the way they do business. internally, wen uses his
   hard-earned field experiences, customer feedback, and market trends to
   drive product innovation and development. wen holds a b.s. in
   electrical engineering and m.s. in analytics and decision sciences.
   personally, he enjoys spinning hip-hop records, dining out, and
   spendin...
   [81]more
   [82]sri ambati

[83]srisatish ambati

   sri is co-founder and ceo of h2o (@h2oai), the builders of h2o. h2o
   democratizes big data science and makes hadoop do math for better
   predictions. before h2o, sri spent time scaling r over big data with
   researchers at purdue and stanford. sri also co-founded platfora and
   was the director of engineering at datastax. he was a partner and
   performance engineer at the java multi-core startup, azul systems,
   tinkering with the entire ecosystem of enterprise apps at scale. sri
   has also pursued theoretical neuroscience at berkeley, and has worked
   on nos...
   [84]more
     __________________________________________________________________

   video
   [85]logstash ui

   [86]data science

[87]revealing the uncommonly common with elasticsearch

   by [88]mark harwood

   this webcast looks at how elasticsearch is taking search engine
   technology and branching it out to provide insightful analysis of large
   datasets.

   [89]marina bay and the skyline of the central business district of
   singapore at dusk, by william cho.

   [90]data science

[91]how intelligent data platforms are powering smart cities

   by [92]ben lorica

   smart cities and smart nations run on data.

   [93]a woman posed with a hollerith pantograph: the keyboard is for the
   1920 population card, and a 1940 census form.

   [94]data science

[95]beyond algorithms: optimizing the search experience

   by [96]daniel tunkelang

   making search smarter through better human-computer interaction.

   runnable code
   [97]pandas table

   [98]data science

[99]introducing pandas objects

   by [100]jake vanderplas

   python data science handbook: early release

about us

     * [101]our company
     * [102]teach/speak/write
     * [103]careers
     * [104]customer service
     * [105]contact us

site map

     * [106]ideas
     * [107]learning
     * [108]topics
     * [109]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [110]terms of service     [111]privacy policy     [112]editorial
   independence

   animal

   iframe: [113]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/data-science
  27. https://www.oreilly.com/people/54c3c-patrick-hall
  28. https://www.oreilly.com/people/wen-phan
  29. https://www.oreilly.com/people/ba92c-srisatish-ambati
  30. http://www.nytimes.com/2016/12/21/technology/san-francisco-california-uber-driverless-car-.html
  31. http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html
  32. https://www.youtube.com/watch?v=sitmy5oen_a
  33. https://en.wikipedia.org/wiki/multivariate_adaptive_regression_splines
  34. https://en.wikipedia.org/wiki/change_blindness
  35. https://twitter.com/ivy_wang
  36. https://en.wikipedia.org/wiki/principal_component_analysis
  37. https://en.wikipedia.org/wiki/multidimensional_scaling
  38. https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding
  39. https://en.wikipedia.org/wiki/autoencoder
  40. http://yann.lecun.com/exdb/mnist/
  41. https://arxiv.org/abs/1309.6392
  42. https://twitter.com/micahstubbs
  43. https://en.wikipedia.org/wiki/least_squares
  44. https://en.wikipedia.org/wiki/lasso_(statistics)
  45. https://en.wikipedia.org/wiki/tikhonov_id173
  46. https://en.wikipedia.org/wiki/elastic_net_id173
  47. http://statweb.stanford.edu/~tibs/elemstatlearn/printings/eslii_print10.pdf
  48. https://en.wikipedia.org/wiki/generalized_additive_model
  49. https://en.wikipedia.org/wiki/quantile_regression
  50. https://twitter.com/vinodiyengar
  51. https://www.oreilly.com/ideas/the-preoccupation-with-test-error-in-applied-machine-learning
  52. http://biostats.bepress.com/ucbbiostat/paper222/
  53. http://dl.acm.org/citation.cfm?id=148453
  54. https://papers.nips.cc/paper/1358-monotonic-networks.pdf
  55. https://github.com/dmlc/xgboost/issues/1514
  56. https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime
  57. https://www.oreilly.com/people/marco-tulio-ribeiro
  58. https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime
  59. https://arxiv.org/pdf/1606.05386.pdf
  60. https://github.com/marcotcr/lime
  61. http://eli5.readthedocs.io/en/latest/index.html
  62. https://arxiv.org/pdf/1604.04173v1.pdf
  63. https://twitter.com/crossid178
  64. http://blog.datadive.net/interpreting-random-forests/
  65. http://link.springer.com/article/10.1134/s0032946011030069
  66. https://github.com/andosa/treeinterpreter
  67. http://eli5.readthedocs.io/en/latest/index.html
  68. http://statweb.stanford.edu/~jhf/r_rulefit.html
  69. https://arxiv.org/pdf/1702.08833.pdf
  70. https://www.safaribooksonline.com/library/view/learning-path-machine/9781491958155/?utm_source=newsite&utm_medium=content&utm_campaign=lgen&utm_content=patrick-hall-post-interpreting-ml-related-resources-link
  71. https://www.safaribooksonline.com/library/view/practical-machine-learning/9781491964590/?utm_source=newsite&utm_medium=content&utm_campaign=lgen&utm_content=patrick-hall-post-interpreting-ml-related-resources-link
  72. https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/?utm_source=newsite&utm_medium=content&utm_campaign=lgen&utm_content=patrick-hall-post-interpreting-ml-related-resources-link
  73. https://www.safaribooksonline.com/library/view/thoughtful-machine-learning/9781491924129/?utm_source=newsite&utm_medium=content&utm_campaign=lgen&utm_content=patrick-hall-post-interpreting-ml-related-resources-link
  74. https://www.oreilly.com/ideas/machine-learning-a-quick-and-simple-definition
  75. https://twitter.com/share
  76. https://www.oreilly.com/people/54c3c-patrick-hall
  77. https://www.oreilly.com/people/54c3c-patrick-hall
  78. https://www.oreilly.com/people/54c3c-patrick-hall
  79. https://www.oreilly.com/people/wen-phan
  80. https://www.oreilly.com/people/wen-phan
  81. https://www.oreilly.com/people/wen-phan
  82. https://www.oreilly.com/people/ba92c-srisatish-ambati
  83. https://www.oreilly.com/people/ba92c-srisatish-ambati
  84. https://www.oreilly.com/people/ba92c-srisatish-ambati
  85. https://www.oreilly.com/learning/revealing-the-uncommonly-common-with-elasticsearch
  86. https://www.oreilly.com/topics/data-science
  87. https://www.oreilly.com/learning/revealing-the-uncommonly-common-with-elasticsearch
  88. https://www.oreilly.com/people/d9f55-mark-harwood
  89. https://www.oreilly.com/ideas/how-intelligent-data-platforms-are-powering-smart-cities
  90. https://www.oreilly.com/topics/data-science
  91. https://www.oreilly.com/ideas/how-intelligent-data-platforms-are-powering-smart-cities
  92. https://www.oreilly.com/people/4e7ad-ben-lorica
  93. https://www.oreilly.com/ideas/beyond-algorithms-optimizing-the-search-experience
  94. https://www.oreilly.com/topics/data-science
  95. https://www.oreilly.com/ideas/beyond-algorithms-optimizing-the-search-experience
  96. https://www.oreilly.com/people/e9a1c-daniel-tunkelang
  97. https://www.oreilly.com/learning/introducing-pandas-objects
  98. https://www.oreilly.com/topics/data-science
  99. https://www.oreilly.com/learning/introducing-pandas-objects
 100. https://www.oreilly.com/people/89c9c-jake-vanderplas
 101. http://oreilly.com/about/
 102. http://oreilly.com/work-with-us.html
 103. http://oreilly.com/careers/
 104. http://shop.oreilly.com/category/customer-service.do
 105. http://shop.oreilly.com/category/customer-service.do
 106. https://www.oreilly.com/ideas
 107. https://www.oreilly.com/topics/oreilly-learning
 108. https://www.oreilly.com/topics
 109. https://www.oreilly.com/all
 110. http://oreilly.com/terms/
 111. http://oreilly.com/privacy.html
 112. http://www.oreilly.com/about/editorial_independence.html
 113. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
 115. https://www.oreilly.com/
 116. https://www.facebook.com/oreilly/
 117. https://twitter.com/oreillymedia
 118. https://www.youtube.com/user/oreillymedia
 119. https://plus.google.com/+oreillymedia
 120. https://www.linkedin.com/company/oreilly-media
 121. https://www.oreilly.com/
