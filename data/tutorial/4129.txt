   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

                               hello, tensorflow!

   building and training your first tensorflow graph from the ground up.

   by [27]aaron schumacher

   june 20, 2016

   braided river. braided river. (source: [28]national park service,
   alaska region on flickr)

   this post is also available as [29]an oriole tutorial.

   the [30]tensorflow project is bigger than you might realize. the fact
   that it's a library for deep learning, and its connection to google,
   has helped tensorflow attract a lot of attention. but beyond the hype,
   there are unique elements to the project that are worthy of closer
   inspection:
     * the core library is suited to a broad family of machine learning
       techniques, not    just    deep learning.
     * id202 and other internals are prominently exposed.
     * in addition to the core machine learning functionality, tensorflow
       also includes its own logging system, its own interactive log
       visualizer, and even its own heavily engineered serving
       architecture.
     * the execution model for tensorflow differs from python's
       scikit-learn, or most tools in r.

   cool stuff, but   especially for someone hoping to explore machine
   learning for the first time   tensorflow can be a lot to take in.

   how does tensorflow work? let's break it down so we can see and
   understand every moving part. we'll explore the data flow [31]graph
   that defines the computations your data will undergo, how to train
   models with [32]id119 using tensorflow, and how
   [33]tensorboard can visualize your tensorflow work. the examples here
   won't solve industrial machine learning problems, but they'll help you
   understand the components underlying everything built with tensorflow,
   including whatever you build next!

names and execution in python and tensorflow

   the way tensorflow manages computation is not totally different from
   the way python usually does. with both, it's important to remember, to
   paraphrase [34]hadley wickham, that an object has no name (see figure
   1). in order to see the similarities (and differences) between how
   python and tensorflow work, let   s look at how they refer to objects and
   handle evaluation.
   names    have    objects, rather than the reverse figure 1. names    have   
   objects, rather than the reverse. image courtesy of hadley wickham,
   used with permission.

   the variable names in python code aren't what they represent; they're
   just pointing at objects. so, when you say in python that foo = [] and
   bar = foo, it isn't just that foo equals bar; foo is bar, in the sense
   that they both point at the same list object.
>>> foo = []
>>> bar = foo
>>> foo == bar
## true
>>> foo is bar
## true

   you can also see that id(foo) and id(bar) are the same. this identity,
   especially with [35]mutable data structures like lists, can lead to
   surprising bugs when it's misunderstood.

   internally, python manages all your objects and keeps track of your
   variable names and which objects they refer to. the tensorflow graph
   represents another layer of this kind of management; as we   ll see,
   python names will refer to objects that connect to more granular and
   managed tensorflow graph operations.

   when you enter a python expression, for example at an interactive
   interpreter or read evaluate print loop (repl), whatever is read is
   almost always evaluated right away. python is eager to do what you tell
   it. so, if i tell python to foo.append(bar), it appends right away,
   even if i never use foo again.

   a lazier alternative would be to just remember that i said
   foo.append(bar), and if i ever evaluate foo at some point in the
   future, python could do the append then. this would be closer to how
   tensorflow behaves, where defining relationships is entirely separate
   from evaluating what the results are.

   tensorflow separates the definition of computations from their
   execution even further by having them happen in separate places: a
   graph defines the operations, but the operations only happen within a
   session. graphs and sessions are created independently. a graph is like
   a blueprint, and a session is like a construction site.

   back to our plain python example, recall that foo and bar refer to the
   same list. by appending bar into foo, we've put a list inside itself.
   you could think of this structure as a graph with one node, pointing to
   itself. nesting lists is one way to represent a graph structure like a
   tensorflow computation graph.
>>> foo.append(bar)
>>> foo
## [[...]]

   real tensorflow graphs will be more interesting than this!

the simplest tensorflow graph

   to start getting our hands dirty, let   s create the simplest tensorflow
   graph we can, from the ground up. tensorflow is admirably easier to
   [36]install than some other frameworks. the examples here work with
   either python 2.7 or 3.3+, and the tensorflow version used is 0.8.
>>> import tensorflow as tf

   at this point tensorflow has already started managing a lot of state
   for us. there's already an implicit default graph, for example.
   [37]internally, the default graph lives in the _default_graph_stack,
   but we don't have access to that directly. we use
   tf.get_default_graph().
>>> graph = tf.get_default_graph()

   the nodes of the tensorflow graph are called    operations,    or    ops.    we
   can see what operations are in the graph with graph.get_operations().
>>> graph.get_operations()
## []

   currently, there isn't anything in the graph. we   ll need to put
   everything we want tensorflow to compute into that graph. let's start
   with a simple constant input value of one.
>>> input_value = tf.constant(1.0)

   that constant now lives as a node, an operation, in the graph. the
   python variable name input_value refers indirectly to that operation,
   but we can also find the operation in the default graph.
>>> operations = graph.get_operations()
>>> operations
## [<tensorflow.python.framework.ops.operation at 0x1185005d0>]
>>> operations[0].node_def
## name: "const"
## op: "const"
## attr {
##   key: "dtype"
##   value {
##     type: dt_float
##   }
## }
## attr {
##   key: "value"
##   value {
##     tensor {
##       dtype: dt_float
##       tensor_shape {
##       }
##       float_val: 1.0
##     }
##   }
## }

   tensorflow uses protocol buffers internally. ([38]protocol buffers are
   sort of like a google-strength [39]json.) printing the node_def for the
   constant operation above shows what's in tensorflow's protocol buffer
   representation for the number one.

   people new to tensorflow sometimes wonder why there's all this fuss
   about making    tensorflow versions    of things. why can't we just use a
   normal python variable without also defining a tensorflow object?
   [40]one of the tensorflow tutorials has an explanation:

     to do efficient numerical computing in python, we typically use
     libraries like numpy that do expensive operations such as matrix
     multiplication outside python, using highly efficient code
     implemented in another language. unfortunately, there can still be a
     lot of overhead from switching back to python every operation. this
     overhead is especially bad if you want to run computations on gpus
     or in a distributed manner, where there can be a high cost to
     transferring data.

     tensorflow also does its heavy lifting outside python, but it takes
     things a step further to avoid this overhead. instead of running a
     single expensive operation independently from python, tensorflow
     lets us describe a graph of interacting operations that run entirely
     outside python. this approach is similar to that used in theano or
     torch.

   tensorflow can do a lot of great things, but it can only work with
   what's been explicitly given to it. this is true even for a single
   constant.

   if we inspect our input_value, we see it is a constant 32-bit float
   tensor of no dimension: just one number.
>>> input_value
## <tf.tensor 'const:0' shape=() dtype=float32>

   note that this doesn't tell us what that number is. to evaluate
   input_value and get a numerical value out, we need to create a
      session    where graph operations can be evaluated and then explicitly
   ask to evaluate or    run    input_value. (the session picks up the default
   graph by default.)
>>> sess = tf.session()
>>> sess.run(input_value)
## 1.0

   it may feel a little strange to    run    a constant. but it isn't so
   different from evaluating an expression as usual in python; it's just
   that tensorflow is managing its own space of things   the computational
   graph   and it has its own method of evaluation.

the simplest tensorflow neuron

   now that we have a session with a simple graph, let's build a neuron
   with just one parameter, or weight. often, even simple neurons also
   have a bias term and a non-identity activation function, but we'll
   leave these out.

   the neuron's weight isn't going to be constant; we expect it to change
   in order to learn based on the    true    input and output we use for
   training. the weight will be a tensorflow [41]variable. we'll give that
   variable a starting value of 0.8.
>>> weight = tf.variable(0.8)

   you might expect that adding a variable would add one operation to the
   graph, but in fact that one line adds four operations. we can check all
   the operation names:
>>> for op in graph.get_operations(): print(op.name)
## const
## variable/initial_value
## variable
## variable/assign
## variable/read

   we won't want to follow every operation individually for long, but it
   will be nice to see at least one that feels like a real computation.
>>> output_value = weight * input_value

   now there are six operations in the graph, and the last one is that
   multiplication.
>>> op = graph.get_operations()[-1]
>>> op.name
## 'mul'
>>> for op_input in op.inputs: print(op_input)
## tensor("variable/read:0", shape=(), dtype=float32)
## tensor("const:0", shape=(), dtype=float32)

   this shows how the multiplication operation tracks where its inputs
   come from: they come from other operations in the graph. to understand
   a whole graph, following references this way quickly becomes tedious
   for humans. [42]tensorboard graph visualization is designed to help.

   how do we find out what the product is? we have to    run    the
   output_value operation. but that operation depends on a variable:
   weight. we told tensorflow that the initial value of weight should be
   0.8, but the value hasn't yet been set in the current session. the
   tf.initialize_all_variables() function generates an operation which
   will initialize all our variables (in this case just one) and then we
   can run that operation.
>>> init = tf.initialize_all_variables()
>>> sess.run(init)

   the result of tf.initialize_all_variables() will include initializers
   for all the variables currently in the graph, so if you add more
   variables you'll want to use tf.initialize_all_variables() again; a
   stale init wouldn't include the new variables.

   now we're ready to run the output_value operation.
>>> sess.run(output_value)
## 0.80000001

   recall that's 0.8 * 1.0 with 32-bit floats, and 32-bit floats [43]have
   a hard time with 0.8; 0.80000001 is as close as they can get.

see your graph in tensorboard

   up to this point, the graph has been simple, but it would already be
   nice to see it represented in a diagram. we'll use tensorboard to
   generate that diagram. tensorboard reads the name field that is stored
   inside each operation (quite distinct from python variable names). we
   can use these tensorflow names and switch to more conventional python
   variable names. using tf.mul here is equivalent to our earlier use of
   just * for multiplication, but it lets us set the name for the
   operation.
>>> x = tf.constant(1.0, name='input')
>>> w = tf.variable(0.8, name='weight')
>>> y = tf.mul(w, x, name='output')

   tensorboard works by looking at a directory of output created from
   tensorflow sessions. we can write this output with a summarywriter, and
   if we do nothing aside from creating one with a graph, it will just
   write out that graph.

   the first argument when creating the summarywriter is an output
   directory name, which will be created if it doesn't exist.
>>> summary_writer = tf.train.summarywriter('log_simple_graph', sess.graph)

   now, at the command line, we can start up tensorboard.
$ tensorboard --logdir=log_simple_graph

   tensorboard runs as a local web app, on port 6006. (   6006    is    goog   
   upside-down.) if you go in a browser to localhost:6006/#graphs you
   should see a diagram of the graph you created in tensorflow, which
   looks something like figure 2.
   a tensorboard visualization of the simplest tensorflow neuron figure 2.
   a tensorboard visualization of the simplest tensorflow neuron.

making the neuron learn

   now that we   ve built our neuron, how does it learn? we set up an input
   value of 1.0. let's say the correct output value is zero. that is, we
   have a very simple    training set    of just one example with one feature,
   which has the value one, and one label, which is zero. we want the
   neuron to learn the function taking one to zero.

   currently, the system takes the input one and returns 0.8, which is not
   correct. we need a way to measure how wrong the system is. we'll call
   that measure of wrongness the    loss    and give our system the goal of
   minimizing the loss. if the loss can be negative, then minimizing it
   could be silly, so let's make the loss the square of the difference
   between the current output and the desired output.
>>> y_ = tf.constant(0.0)
>>> loss = (y - y_)**2

   so far, nothing in the graph does any learning. for that, we need an
   optimizer. we'll use a id119 optimizer so that we can update
   the weight based on the derivative of the loss. the optimizer takes a
   learning rate to moderate the size of the updates, which we'll set at
   0.025.
>>> optim = tf.train.gradientdescentoptimizer(learning_rate=0.025)

   the optimizer is remarkably clever. it can automatically work out and
   apply the appropriate gradients through a whole network, carrying out
   the backward step for learning.

   let's see what the gradient looks like for our simple example.
>>> grads_and_vars = optim.compute_gradients(loss)
>>> sess.run(tf.initialize_all_variables())
>>> sess.run(grads_and_vars[1][0])
## 1.6

   why is the value of the gradient 1.6? our loss is error squared, and
   the derivative of that is two times the error. currently the system
   says 0.8 instead of 0, so the error is 0.8, and two times 0.8 is 1.6.
   it's working!

   for more complex systems, it will be very nice indeed that tensorflow
   calculates and then applies these gradients for us automatically.

   let's apply the gradient, finishing the id26.
>>> sess.run(optim.apply_gradients(grads_and_vars))
>>> sess.run(w)
## 0.75999999  # about 0.76

   the weight decreased by 0.04 because the optimizer subtracted the
   gradient times the learning rate, 1.6 * 0.025, pushing the weight in
   the right direction.

   instead of hand-holding the optimizer like this, we can make one
   operation that calculates and applies the gradients: the train_step.
>>> train_step = tf.train.gradientdescentoptimizer(0.025).minimize(loss)
>>> for i in range(100):
>>>     sess.run(train_step)
>>>
>>> sess.run(y)
## 0.0044996012

   running the training step many times, the weight and the output value
   are now very close to zero. the neuron has learned!

training diagnostics in tensorboard

   we may be interested in what's happening during training. say we want
   to follow what our system is predicting at every training step. we
   could print from inside the training loop.
>>> sess.run(tf.initialize_all_variables())
>>> for i in range(100):
>>>     print('before step {}, y is {}'.format(i, sess.run(y)))
>>>     sess.run(train_step)
>>>
## before step 0, y is 0.800000011921
## before step 1, y is 0.759999990463
## ...
## before step 98, y is 0.00524811353534
## before step 99, y is 0.00498570781201

   this works, but there are some problems. it's hard to understand a list
   of numbers. a plot would be better. and even with only one value to
   monitor, there's too much output to read. we're likely to want to
   monitor many things. it would be nice to record everything in some
   organized way.

   luckily, the same system that we used earlier to visualize the graph
   also has just the mechanisms we need.

   we instrument the computation graph by adding operations that summarize
   its state. here, we'll create an operation that reports the current
   value of y, the neuron's current output.
>>> summary_y = tf.scalar_summary('output', y)

   when you run a summary operation, it returns a string of protocol
   buffer text that can be written to a log directory with a
   summarywriter.
>>> summary_writer = tf.train.summarywriter('log_simple_stats')
>>> sess.run(tf.initialize_all_variables())
>>> for i in range(100):
>>>     summary_str = sess.run(summary_y)
>>>     summary_writer.add_summary(summary_str, i)
>>>     sess.run(train_step)
>>>

   now after running tensorboard --logdir=log_simple_stats, you get an
   interactive plot at localhost:6006/#events (figure 3).
   a tensorboard visualization of a neuron   s output against training
   iteration number figure 3. a tensorboard visualization of a neuron   s
   output against training iteration number.

flowing onward

   here's a final version of the code. it's fairly minimal, with every
   part showing useful (and understandable) tensorflow functionality.
import tensorflow as tf

x = tf.constant(1.0, name='input')
w = tf.variable(0.8, name='weight')
y = tf.mul(w, x, name='output')
y_ = tf.constant(0.0, name='correct_value')
loss = tf.pow(y - y_, 2, name='loss')
train_step = tf.train.gradientdescentoptimizer(0.025).minimize(loss)

for value in [x, w, y, y_, loss]:
    tf.scalar_summary(value.op.name, value)

summaries = tf.merge_all_summaries()

sess = tf.session()
summary_writer = tf.train.summarywriter('log_simple_stats', sess.graph)

sess.run(tf.initialize_all_variables())
for i in range(100):
    summary_writer.add_summary(sess.run(summaries), i)
    sess.run(train_step)

   the example we just ran through is even simpler than the ones that
   inspired it in michael nielsen's [44]neural networks and deep learning.
   for myself, seeing details like these helps with understanding and
   building more complex systems that use and extend from simple building
   blocks. part of the beauty of tensorflow is how flexibly you can build
   complex systems from simpler components.

   if you want to continue experimenting with tensorflow, it might be fun
   to start making more interesting neurons, perhaps with different
   [45]id180. you could train with more interesting data.
   you could add more neurons. you could add more layers. you could dive
   into more complex [46]pre-built models, or spend more time with
   tensorflow's own [47]tutorials and [48]how-to guides. go for it!

   thanks to paco nathan, ben lorica, and marie beaugureau for nudging
   this post toward existence. thanks to henry chen, dennis leung, and
   paul gulley at deep learning analytics, and the dc machine learning
   journal club for providing valuable feedback on early versions. thanks
   again to marie beaugureau and jenn webb for dramatically improving the
   quality of the final version.


   this post is also available as [49]an oriole tutorial.
   article image: braided river. (source: [50]national park service,
   alaska region on flickr).

   share
    1. [51]tweet
    2.
    3.
     __________________________________________________________________

   [52]aaron schumacher

[53]aaron schumacher

   aaron schumacher is a data scientist and software engineer for deep
   learning analytics. he has taught with python and r for general
   assembly and the metis data science bootcamp. aaron has also worked
   with data at booz allen hamilton, new york university, and the new york
   city department of education. aaron   s career-best breakdancing result
   was advancing to the semi-finals of the r16 korea 2009 individual
   footwork battle. he is honored to now be the least significant
   contributor to tensorflow 0.9.
   [54]more
     __________________________________________________________________

   [55]bots landscape.

   [56]ai

[57]infographic: the bot platform ecosystem

   by [58]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [59]vertical forest, milan.

   [60]ai

[61]evolve ai

   by [62]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [63]welcome sign at o'reilly ai conference 2016

   [64]ai

[65]highlights from the o'reilly ai conference in new york 2016

   by [66]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [67]close up of uber's self driving car in pittsburgh.

   [68]ai

[69]how ai is propelling driverless cars, the future of surface transport

   by [70]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [71]our company
     * [72]teach/speak/write
     * [73]careers
     * [74]customer service
     * [75]contact us

site map

     * [76]ideas
     * [77]learning
     * [78]topics
     * [79]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [80]terms of service     [81]privacy policy     [82]editorial independence

   animal

   iframe: [83]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/aaron-schumacher
  28. https://www.flickr.com/photos/alaskanps/8029733984
  29. https://www.safaribooksonline.com/oriole/hello-tensorflow-oriole
  30. https://www.tensorflow.org/
  31. https://en.wikipedia.org/wiki/graph_(abstract_data_type)
  32. https://en.wikipedia.org/wiki/gradient_descent
  33. https://www.tensorflow.org/versions/r0.8/how_tos/summaries_and_tensorboard/
  34. https://twitter.com/hadleywickham/status/732288980549390336
  35. https://codehabitude.com/2013/12/24/python-objects-mutable-vs-immutable/
  36. https://www.tensorflow.org/versions/r0.8/get_started/os_setup.html
  37. https://github.com/tensorflow/tensorflow/blob/v0.8.0/tensorflow/python/framework/ops.py#l3399
  38. https://developers.google.com/protocol-buffers/
  39. http://www.json.org/
  40. https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros/index.html#deep-mnist-for-experts
  41. https://www.tensorflow.org/versions/r0.8/how_tos/variables/
  42. https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/
  43. https://en.wikipedia.org/wiki/floating_point#representable_numbers.2c_conversion_and_rounding
  44. http://neuralnetworksanddeeplearning.com/
  45. https://en.wikipedia.org/wiki/activation_function#comparison_of_activation_functions
  46. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models
  47. https://www.tensorflow.org/versions/r0.8/tutorials/
  48. https://www.tensorflow.org/versions/r0.8/how_tos/
  49. https://www.safaribooksonline.com/oriole/hello-tensorflow-oriole?utm_source=oreilly&utm_medium=newsite&utm_campaign=hello-tensorflow-post-bottom-cta
  50. https://www.flickr.com/photos/alaskanps/8029733984
  51. https://twitter.com/share
  52. https://www.oreilly.com/people/aaron-schumacher
  53. https://www.oreilly.com/people/aaron-schumacher
  54. https://www.oreilly.com/people/aaron-schumacher
  55. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  56. https://www.oreilly.com/topics/ai
  57. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  58. https://www.oreilly.com/people/b1d73-jon-bruner
  59. https://www.oreilly.com/ideas/evolve-ai
  60. https://www.oreilly.com/topics/ai
  61. https://www.oreilly.com/ideas/evolve-ai
  62. https://www.oreilly.com/people/14d38-naveen-rao
  63. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  64. https://www.oreilly.com/topics/ai
  65. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  66. https://www.oreilly.com/people/0d2c1-mac-slocum
  67. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  68. https://www.oreilly.com/topics/ai
  69. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  70. https://www.oreilly.com/people/c7521-shahin-farshchi
  71. http://oreilly.com/about/
  72. http://oreilly.com/work-with-us.html
  73. http://oreilly.com/careers/
  74. http://shop.oreilly.com/category/customer-service.do
  75. http://shop.oreilly.com/category/customer-service.do
  76. https://www.oreilly.com/ideas
  77. https://www.oreilly.com/topics/oreilly-learning
  78. https://www.oreilly.com/topics
  79. https://www.oreilly.com/all
  80. http://oreilly.com/terms/
  81. http://oreilly.com/privacy.html
  82. http://www.oreilly.com/about/editorial_independence.html
  83. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  85. https://www.oreilly.com/
  86. https://www.facebook.com/oreilly/
  87. https://twitter.com/oreillymedia
  88. https://www.youtube.com/user/oreillymedia
  89. https://plus.google.com/+oreillymedia
  90. https://www.linkedin.com/company/oreilly-media
  91. https://www.oreilly.com/
