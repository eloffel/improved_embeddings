   [1]ideas [2]learning platform [3]conferences [4]shop
   search ____________________ submit
   [5]sign in

on our radar

   [6]ai
   [7]data
   [8]economy
   [9]operations
   [10]software architecture
   [11]software engineering
   [12]web programming
   [13]see all

   [14]ideas [15]learning platform [16]conferences [17]shop search
   ____________________ submit

on our radar

   [18]ai
   [19]data
   [20]economy
   [21]operations
   [22]software architecture
   [23]software engineering
   [24]web programming
   [25]see all

   [26]ai

            perform id31 with lstms, using tensorflow

   explore a highly effective deep learning approach to id31
   using tensorflow and id137.

   by [27]adit deshpande

   july 13, 2017

   perform id31 with lstms, using tensorflow! perform
   id31 with lstms, using tensorflow! (source: [28]o'reilly)

   [29]check out the session, "tensorflow 2.0: machine learning for you,"
   at the artificial intelligence conference in new york, april 15-18,
   2019.

id31 with lstms

   [30]you can download and modify the code from this tutorial on github
   here.

   in this notebook, we'll be looking at how to apply deep learning
   techniques to the task of id31. id31 can be
   thought of as the exercise of taking a sentence, paragraph, document,
   or any piece of natural language, and determining whether that text's
   emotional tone is positive, negative or neutral.

   this notebook will go through numerous topics like word vectors,
   recurrent neural networks, and long short-term memory units (lstms).
   after getting a good understanding of these terms, we   ll walk through
   concrete code examples and a full tensorflow sentiment classifier at
   the end.

   before getting into the specifics, let's discuss the reasons why deep
   learning fits into natural language processing (nlp) tasks.

deep learning for nlp

   natural language processing is all about creating systems that process
   or    understand    language in order to perform certain tasks. these tasks
   could include:
     * id53 - the main job of technologies like siri, alexa,
       and cortana
     * id31 - determining the emotional tone behind a piece
       of text
     * image to text mappings - generating a caption for an input image
     * machine translation - translating a paragraph of text to another
       language
     * id103 - having computers recognize spoken words

   in the pre-deep learning era, nlp was a thriving field that saw lots of
   different advancements. however, in all of the successes in the
   aforementioned tasks, one needed to do a lot of feature enginering and
   thus had to have a lot of domain knowledge in linguistics. entire 4
   year degrees are devoted to this field of study, as practitioners
   needed to be comfortable with terms like phonemes and morphemes. in the
   past few years, deep learning has seen incredible progress and has
   largely removed the requirement of strong domain knowledge. as a result
   of the lower barrier to entry, applications to nlp tasks have been one
   of the biggest areas of deep learning research.

word vectors

   in order to understand how deep learning can be applied, think about
   all the different forms of data that are used as inputs into machine
   learning or deep learning models. convolutional neural networks use
   arrays of pixel values, id28 uses quantifiable features,
   and id23 models use reward signals. the common theme
   is that the inputs need to be scalar values, or matrices of scalar
   values. when you think of nlp tasks, however, a data pipeline like this
   may come to mind.

   caption

   this kind of pipeline is problematic. there is no way for us to do
   common operations like dot products or id26 on a single
   string. instead of having a string input, we will need to convert each
   word in the sentence to a vector.

   caption

   you can think of the input to the id31 module as being a
   16 x d dimensional matrix.

   we want these vectors to be created in such a way that they somehow
   represent the word and its context, meaning, and semantics. for
   example, we   d like the vectors for the words    love    and    adore    to
   reside in relatively the same area in the vector space since they both
   have similar definitions and are both used in similar contexts. the
   vector representation of a word is also known as a id27.

   caption

id97

   in order to create these id27s, we'll use a model that's
   commonly reffered to as "id97". without going into too much detail,
   the model creates word vectors by looking at the context with which
   words appear in sentences. words with similar contexts will be placed
   close together in the vector space. in natural language, the context of
   words can be very important when trying to determine their meanings.
   taking our previous example of the words "adore" and "love", consider
   the types of sentences we'd find these words in.

   caption

   from the context of the sentences, we can see that both words are
   generally used in sentences with positive connotations and generally
   precede nouns or noun phrases. this is an indication that both words
   have something in common and can possibly be synonyms. context is also
   very important when considering grammatical structure in sentences.
   most sentences will follow traditional paradigms of having verbs follow
   nouns, adjectives precede nouns, and so on. for this reason, the model
   is more likely to position nouns in the same general area as other
   nouns. the model takes in a large dataset of sentences (english
   wikipedia for example) and outputs vectors for each unique word in the
   corpus. the output of a id97 model is called an embedding matrix.

   caption

   this embedding matrix will contain vectors for every distinct word in
   the training corpus. traditionally, embedding matrices can contain over
   3 million word vectors.

   the id97 model is trained by taking each sentence in the dataset,
   sliding a window of fixed size over it, and trying to predict the
   center word of the window, given the other words. using a id168
   and optimization procedure, the model generates vectors for each unique
   word. the specifics of this training procedure can get a little
   complicated, so we   re going to skip over the details for now, but the
   main takeaway here is that inputs into any deep learning approach to an
   nlp task will likely have word vectors as input.

   for more information on the theory behind id97 and how you create
   your own embeddings, check out tensorflow's [31]tutorial

recurrent neural networks (id56s)

   now that we have our word vectors as input, let's look at the actual
   network architecture we're going to be building. the unique aspect of
   nlp data is that there is a temporal aspect to it. each word in a
   sentence depends greatly on what came before and comes after it. in
   order to account for this dependency, we use a recurrent neural
   network.

   the recurrent neural network structure is a little different from the
   traditional feedforward nn you may be accostumed to seeing. the
   feedforward network consists of input nodes, hidden units, and output
   nodes.

   caption

   the main difference between feedforward neural networks and recurrent
   ones is the temporal aspect of the latter. in id56s, each word in an
   input sequence will be associated with a specific time step. in effect,
   the number of time steps will be equal to the max sequence length.

   caption

   associated with each time step is also a new component called a hidden
   state vector h[t]. from a high level, this vector seeks to encapsulate
   and summarize all of the information that was seen in the previous time
   steps. just like x[t] is a vector that encapsulates all the information
   of a specific word, h[t] is a vector that summarizes information from
   previous time steps.

   the hidden state is a function of both the current word vector and the
   hidden state vector at the previous time step. the sigma indicates that
   the sum of the two terms will be put through an activation function
   (normally a sigmoid or tanh).

   caption

   the 2 w terms in the above formulation represent weight matrices. if
   you take a close look at the superscripts, you   ll see that there   s a
   weight matrix w^x which we   re going to multiply with our input, and
   there   s a recurrent weight matrix w^h which is multiplied with the
   hidden state vector at the previous time step. w^h is a matrix that
   stays the same across all time steps, and the weight matrix w^x is
   different for each input.

   the magnitude of these weight matrices impact the amount the hidden
   state vector is affected by either the current vector or the previous
   hidden state. as an exercise, take a look at the above formula, and
   consider how h[t] would change if either w^x or w^h had large or small
   values.

   let's look at a quick example. when the magnitude of w^h is large and
   the magnitude of w^x is small, we know that h[t] is largely affected by
   h[t-1] and unaffected by x[t]. in other words, the current hidden state
   vector sees that the current word is largely inconsequential to the
   overall summary of the sentence, and thus it will take on mostly the
   same value as the vector at the previous time step.

   the weight matrices are updated through an optimization process called
   id26 through time.

   the hidden state vector at the final time step is fed into a binary
   softmax classifier where it is multiplied by another weight matrix and
   put through a softmax function that outputs values between 0 and 1,
   effectively giving us the probabilities of positive and negative
   sentiment.

   caption

long short term memory units (lstms)

   long short term memory units are modules that you can place inside of
   reucrrent neural entworks. at a high level, they make sure that the
   hidden state vector h is able to encapsulate information about long
   term dependencies in the text. as we saw in the previous section, the
   formulation for h in traditional id56s is relatively simple. this
   approach won't be able to effectively connect together information that
   is separated by more than a couple time steps. we can illiustrate this
   idea of handling long term dependencies through an example in the field
   of id53. the function of id53 models is to
   take an a passage of text, and answer a question about its content.
   let's look at the following example.

   caption

   here, we see that the middle sentence had no impact on the question
   that was asked. however, there is a strong connection between the first
   and third sentences. with a classic id56, the hidden state vector at the
   end of the network might have stored more information about the dog
   sentence than about the first sentence about the number. basically, the
   addition of lstm units make it possible to determine the correct and
   useful information that needs to be stored in the hidden state vector.

   looking at lstm units from a more technical viewpoint, the units take
   in the current word vector x[t] and output the hidden state vector
   h[t]. in these units, the formulation for h[t] will be a bit more
   complex than that in a typical id56. the computation is broken up into 4
   components, an input gate, a forget gate, an output gate, and a new
   memory container.

   caption

   each gate will take in x[t] and h[t-1] (not shown in image) as inputs
   and will perform some computation on them to obtain intermediate
   states. each intermediate state gets fed into different pipelines and
   eventually the information is aggregated to form h[t]. for simplicity
   sake, we won't go into the specific formulations for each gate, but
   it's worth noting that each of these gates can be thought of as
   different modules within the lstm that each have different functions.
   the input gate determines how much emphasis to put on each of the
   inputs, the forget gate determines the information that we'll throw
   away, and the output gate determines the final h[t] based on the
   intermediate states. for more information on understanding the
   functions of the different gates and the full equations, check out
   christopher olah's great [32]blog post.

   looking back at the first example with question    what is the sum of the
   two numbers?   , the model would have to be trained on similar types of
   questions and answers. the lstm units would then be able to realize
   that any sentence without numbers will likely not have an impact on the
   answer to the question, and thus the unit will be able to utilize its
   forget gate to discard the unnecessary information about the dog, and
   rather keep the information regarding the numbers.

framing id31 as a deep learning problem

   as mentioned before, the task of id31 involves taking in
   an input sequence of words and determining whether the sentiment is
   positive, negative, or neutral. we can separate this specific task (and
   most other nlp tasks) into 5 different components.
1) training a word vector generation model (such as id97) or loading pretrai
ned word vectors
2) creating an id's matrix for our training set (we'll discuss this a bit later)
3) id56 (with lstm units) graph creation
4) training
5) testing

loading data

   first, we want to create our word vectors. for simplicity, we're going
   to be using a pretrained model.

   as one of the biggest players in the ml game, google was able to train
   a id97 model on a massive google news dataset that contained over
   100 billion different words! from that model, google [33]was able to
   create 3 million word vectors, each with a dimensionality of 300.

   in an ideal scenario, we'd use those vectors, but since the word
   vectors matrix is quite large (3.6 gb!), we'll be using a much more
   manageable matrix that is trained using [34]glove, a similar word
   vector generation model. the matrix will contain 400,000 word vectors,
   each with a dimensionality of 50.

   we're going to be importing two different data structures, one will be
   a python list with the 400,000 words, and one will be a 400,000 x 50
   dimensional embedding matrix that holds all of the word vector values.
import numpy as np
wordslist = np.load('wordslist.npy')
print('loaded the word list!')
wordslist = wordslist.tolist() #originally loaded as numpy array
wordslist = [word.decode('utf-8') for word in wordslist] #encode words as utf-8
wordvectors = np.load('wordvectors.npy')
print ('loaded the word vectors!')

   just to make sure everything has been loaded in correctly, we can look
   at the dimensions of the vocabulary list and the embedding matrix.
print(len(wordslist))
print(wordvectors.shape)

   we can also search our word list for a word like "baseball", and then
   access its corresponding vector through the embedding matrix.
baseballindex = wordslist.index('baseball')
wordvectors[baseballindex]

   now that we have our vectors, our first step is taking an input
   sentence and then constructing the its vector representation. let's say
   that we have the input sentence "i thought the movie was incredible and
   inspiring". in order to get the word vectors, we can use tensorflow's
   embedding lookup function. this function takes in two arguments, one
   for the embedding matrix (the wordvectors matrix in our case), and one
   for the ids of each of the words. the ids vector can be thought of as
   the integerized representation of the training set. this is basically
   just the row index of each of the words. let's look at a quick example
   to make this concrete.
import tensorflow as tf
maxseqlength = 10 #maximum length of sentence
numdimensions = 300 #dimensions for each word vector
firstsentence = np.zeros((maxseqlength), dtype='int32')
firstsentence[0] = wordslist.index("i")
firstsentence[1] = wordslist.index("thought")
firstsentence[2] = wordslist.index("the")
firstsentence[3] = wordslist.index("movie")
firstsentence[4] = wordslist.index("was")
firstsentence[5] = wordslist.index("incredible")
firstsentence[6] = wordslist.index("and")
firstsentence[7] = wordslist.index("inspiring")
#firstsentence[8] and firstsentence[9] are going to be 0
print(firstsentence.shape)
print(firstsentence) #shows the row index for each word

   the data pipeline can be illustrated below.

   caption

   the 10 x 50 output should contain the 50 dimensional word vectors for
   each of the 10 words in the sequence.
with tf.session() as sess:
    print(tf.nn.embedding_lookup(wordvectors,firstsentence).eval().shape)

   before creating the ids matrix for the whole training set, let   s first
   take some time to visualize the type of data that we have. this will
   help us determine the best value for setting our maximum sequence
   length. in the previous example, we used a max length of 10, but this
   value is largely dependent on the inputs you have.

   the training set we're going to use is the imdb movie review dataset.
   this set has 25,000 movie reviews, with 12,500 positive reviews and
   12,500 negative reviews. each of the reviews is stored in a txt file
   that we need to parse through. the positive reviews are stored in one
   directory and the negative reviews are stored in another. the following
   piece of code will determine total and average number of words in each
   review.
from os import listdir
from os.path import isfile, join
positivefiles = ['positivereviews/' + f for f in listdir('positivereviews/') if
isfile(join('positivereviews/', f))]
negativefiles = ['negativereviews/' + f for f in listdir('negativereviews/') if
isfile(join('negativereviews/', f))]
numwords = []
for pf in positivefiles:
    with open(pf, "r", encoding='utf-8') as f:
        line=f.readline()
        counter = len(line.split())
        numwords.append(counter)
print('positive files finished')

for nf in negativefiles:
    with open(nf, "r", encoding='utf-8') as f:
        line=f.readline()
        counter = len(line.split())
        numwords.append(counter)
print('negative files finished')

numfiles = len(numwords)
print('the total number of files is', numfiles)
print('the total number of words in the files is', sum(numwords))
print('the average number of words in the files is', sum(numwords)/len(numwords)
)

   we can also use the matplot library to visualize this data in a
   histogram format.
import matplotlib.pyplot as plt
%matplotlib inline
plt.hist(numwords, 50)
plt.xlabel('sequence length')
plt.ylabel('frequency')
plt.axis([0, 1200, 0, 8000])
plt.show()

   from the histogram as well as the average number of words per file, we
   can safely say that most reviews will fall under 250 words, which is
   the max sequence length value we will set.
maxseqlength = 250

   let's see how we can take a single file and transform it into our ids
   matrix. this is what one of the reviews looks like in text file format.
fname = positivefiles[3] #can use any valid index (not just 3)
with open(fname) as f:
    for lines in f:
        print(lines)
        exit

   now, let's convert to to an ids matrix
# removes punctuation, parentheses, question marks, etc., and leaves only alphan
umeric characters
import re
strip_special_chars = re.compile("[^a-za-z0-9 ]+")

def cleansentences(string):
    string = string.lower().replace("<br />", " ")
    return re.sub(strip_special_chars, "", string.lower())

firstfile = np.zeros((maxseqlength), dtype='int32')
with open(fname) as f:
    indexcounter = 0
    line=f.readline()
    cleanedline = cleansentences(line)
    split = cleanedline.split()
    for word in split:
        try:
            firstfile[indexcounter] = wordslist.index(word)
        except valueerror:
            firstfile[indexcounter] = 399999 #vector for unknown words
        indexcounter = indexcounter + 1
firstfile

   now, let's do the same for each of our 25,000 reviews. we'll load in
   the movie training set and integerize it to get a 25000 x 250 matrix.
   this was a computationally expensive process, so instead of having you
   run the whole piece, we   re going to load in a pre-computed ids matrix.
# ids = np.zeros((numfiles, maxseqlength), dtype='int32')
# filecounter = 0
# for pf in positivefiles:
#    with open(pf, "r") as f:
#        indexcounter = 0
#        line=f.readline()
#        cleanedline = cleansentences(line)
#        split = cleanedline.split()
#        for word in split:
#            try:
#                ids[filecounter][indexcounter] = wordslist.index(word)
#            except valueerror:
#                ids[filecounter][indexcounter] = 399999 #vector for unkown word
s
#            indexcounter = indexcounter + 1
#            if indexcounter >= maxseqlength:
#                break
#        filecounter = filecounter + 1

# for nf in negativefiles:
#    with open(nf, "r") as f:
#        indexcounter = 0
#        line=f.readline()
#        cleanedline = cleansentences(line)
#        split = cleanedline.split()
#        for word in split:
#            try:
#                ids[filecounter][indexcounter] = wordslist.index(word)
#            except valueerror:
#                ids[filecounter][indexcounter] = 399999 #vector for unkown word
s
#            indexcounter = indexcounter + 1
#            if indexcounter >= maxseqlength:
#                break
#        filecounter = filecounter + 1
# #pass into embedding function and see if it evaluates.

# np.save('idsmatrix', ids)

ids = np.load('idsmatrix.npy')

helper functions

   below you can find a couple of helper functions that will be useful
   when training the network in a later step.
from random import randint

def gettrainbatch():
    labels = []
    arr = np.zeros([batchsize, maxseqlength])
    for i in range(batchsize):
        if (i % 2 == 0):
            num = randint(1,11499)
            labels.append([1,0])
        else:
            num = randint(13499,24999)
            labels.append([0,1])
        arr[i] = ids[num-1:num]
    return arr, labels

def gettestbatch():
    labels = []
    arr = np.zeros([batchsize, maxseqlength])
    for i in range(batchsize):
        num = randint(11499,13499)
        if (num <= 12499):
            labels.append([1,0])
        else:
            labels.append([0,1])
        arr[i] = ids[num-1:num]
    return arr, labels

id56 model

   now, we   re ready to start creating our tensorflow graph. we   ll first
   need to define some hyperparameters, such as batch size, number of lstm
   units, number of output classes, and number of training iterations.
batchsize = 24
lstmunits = 64
numclasses = 2
iterations = 100000

   as with most tensorflow graphs, we   ll now need to specify two
   placeholders, one for the inputs into the network, and one for the
   labels. the most important part about defining these placeholders is
   understanding each of their dimensionalities.

   the labels placeholder represents a set of values, each either [1, 0]
   or [0, 1], depending on whether each training example is positive or
   negative. each row in the integerized input placeholder represents the
   integerized representation of each training example that we include in
   our batch.

   caption
import tensorflow as tf
tf.reset_default_graph()

labels = tf.placeholder(tf.float32, [batchsize, numclasses])
input_data = tf.placeholder(tf.int32, [batchsize, maxseqlength])

   once we have our input data placeholder, we   re going to call the
   tf.nn.lookup() function in order to get our word vectors. the call to
   that function will return a 3-d tensor of dimensionality batch size by
   max sequence length by word vector dimensions. in order to visualize
   this 3-d tensor, you can simply think of each data point in the
   integerized input tensor as the corresponding d dimensional vector that
   it refers to.

   caption
data = tf.variable(tf.zeros([batchsize, maxseqlength, numdimensions]),dtype=tf.f
loat32)
data = tf.nn.embedding_lookup(wordvectors,input_data)

   now that we have the data in the format that we want, let   s look at how
   we can feed this input into an lstm network. we   re going to call the
   tf.nn.id56_cell.basiclstmcell function. this function takes in an
   integer for the number of lstm units that we want. this is one of the
   hyperparameters that will take some tuning to figure out the optimal
   value. we   ll then wrap that lstm cell in a dropout layer to help
   prevent the network from overfitting.

   finally, we   ll feed both the lstm cell and the 3-d tensor full of input
   data into a function called tf.nn.dynamic_id56. this function is in
   charge of unrolling the whole network and creating a pathway for the
   data to flow through the id56 graph.
lstmcell = tf.contrib.id56.basiclstmcell(lstmunits)
lstmcell = tf.contrib.id56.dropoutwrapper(cell=lstmcell, output_keep_prob=0.75)
value, _ = tf.nn.dynamic_id56(lstmcell, data, dtype=tf.float32)

   as a side note, another more advanced network architecture choice is to
   stack multiple lstm cells on top of each other. this is where the final
   hidden state vector of the first lstm feeds into the second. stacking
   these cells is a great way to help the model retain more long term
   dependence information, but also introduces more parameters into the
   model, thus possibly increasing the training time, the need for
   additional training examples, and the chance of overfitting. for more
   information on how you can add stacked lstms to your model, check out
   tensorflow's excellent [35]documentation.

   the first output of the dynamic id56 function can be thought of as the
   last hidden state vector. this vector will be reshaped and then
   multiplied by a final weight matrix and a bias term to obtain the final
   output values.
weight = tf.variable(tf.truncated_normal([lstmunits, numclasses]))
bias = tf.variable(tf.constant(0.1, shape=[numclasses]))
value = tf.transpose(value, [1, 0, 2])
last = tf.gather(value, int(value.get_shape()[0]) - 1)
prediction = (tf.matmul(last, weight) + bias)

   next, we   ll define correct prediction and accuracy metrics to track how
   the network is doing. the correct prediction formulation works by
   looking at the index of the maximum value of the 2 output values, and
   then seeing whether it matches with the training labels.
correctpred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))
accuracy = tf.reduce_mean(tf.cast(correctpred, tf.float32))

   we   ll define a standard cross id178 loss with a softmax layer put on
   top of the final prediction values. for the optimizer, we   ll use adam
   and the default learning rate of .001.
loss = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(logits=prediction,
 labels=labels))
optimizer = tf.train.adamoptimizer().minimize(loss)

   if you   d like to use tensorboard to visualize the loss and accuracy
   values, you can also run and the modify the following code.
import datetime

tf.summary.scalar('loss', loss)
tf.summary.scalar('accuracy', accuracy)
merged = tf.summary.merge_all()
logdir = "tensorboard/" + datetime.datetime.now().strftime("%y%m%d-%h%m%s") + "/
"
writer = tf.summary.filewriter(logdir, sess.graph)

hyperparameter tuning

   choosing the right values for your hyperparameters is a crucial part of
   training deep neural networks effectively. you'll find that your
   training loss curves can vary with your choice of optimizer (adam,
   adadelta, sgd, etc), learning rate, and network architecture. with id56s
   and lstms in particular, some other important factors include the
   number of lstm units and the size of the word vectors.
     * learning rate: id56s are infamous for being diffult to train because
       of the large number of time steps they have. learning rate becomes
       extremely important since we don't want our weight values to
       fluctuate wildly as a result of a large learning rate, nor do we
       want a slow training process due to a low learning rate. the
       default value of 0.001 is a good place to start. you should
       increase this value if the training loss is changing very slowly,
       and decrease if the loss is unstable.
     * optimizer: there isn't a consensus choice among researchers, but
       adam has been widely popular due to having the adaptive learning
       rate property (keep in mind that optimal learning rates can differ
       with the choice of optimizer).
     * number of lstm units: this value is largely dependent on the
       average length of your input texts. while a greater number of units
       provides more expressibility for the model and allows the model to
       store more information for longer texts, the network will take
       longer to train and will be computationally expensive.
     * word vector size: dimensions for word vectors generally range from
       50 to 300. a larger size means that the vector is able to
       encapsulate more information about the word, but you should also
       expect a more computationally expensive model.

training

   the basic idea of the training loop is that we first define a
   tensorflow session. then, we load in a batch of reviews and their
   associated labels. next, we call the session   s run function. this
   function has two arguments. the first is called the "fetches" argument.
   it defines the value we   re interested in computing. we want our
   optimizer to be computed since that is the component that minimizes our
   id168. the second argument is where we input our feed_dict.
   this data structure is where we provide inputs to all of our
   placeholders. we need to feed our batch of reviews and our batch of
   labels. this loop is then repeated for a set number of training
   iterations.

   instead of training the network in this notebook (which will take at
   least a couple of hours), we   ll load in a pretrained model.

   if you decide to train this notebook on your own machine, note that you
   can track its progress using [36]tensorboard. while the following cell
   is running, use your terminal to enter the directory that contains this
   notebook, enter tensorboard --logdir=tensorboard, and visit
   [37]http://localhost:6006/ with a browser to keep an eye on your
   training progress.
# sess = tf.interactivesession()
# saver = tf.train.saver()
# sess.run(tf.global_variables_initializer())

# for i in range(iterations):
#    #next batch of reviews
#    nextbatch, nextbatchlabels = gettrainbatch();
#    sess.run(optimizer, {input_data: nextbatch, labels: nextbatchlabels})

#    #write summary to tensorboard
#    if (i % 50 == 0):
#        summary = sess.run(merged, {input_data: nextbatch, labels: nextbatchlab
els})
#        writer.add_summary(summary, i)

#    #save the network every 10,000 training iterations
#    if (i % 10000 == 0 and i != 0):
#        save_path = saver.save(sess, "models/pretrained_lstm.ckpt", global_step
=i)
#        print("saved to %s" % save_path)
# writer.close()

loading a pretrained model

   our pretrained model   s accuracy and loss curves during training can be
   found below.

   caption caption

   looking at the training curves above, it seems that the model's
   training is going well. the loss is decreasing steadily, and the
   accuracy is approaching 100 percent. however, when analyzing training
   curves, we should also pay special attention to the possibility of our
   model overfitting the training dataset. overfitting is a common
   phenomenon in machine learning where a model becomes so fit to the
   training data that it loses the ability to generalize to the test set.
   this means that training a network until you achieve 0 training loss
   might not be the best way to get an accurate model that performs well
   on data it has never seen before. early stopping is an intuitive
   technique commonly used with id137 to combat this issue. the
   basic idea is that we train the model on our training set, while also
   measuring its performance on the test set every now and again. once the
   test error stops its steady decrease and begins to increase instead,
   you'll know to stop training, since this is a sign that the network has
   begun to overfit.

   loading a pretrained model involves defining another tensorflow
   session, creating a saver object, and then using that object to call
   the restore function. this function takes into 2 arguments, one for the
   current session, and one for the name of the saved model.
sess = tf.interactivesession()
saver = tf.train.saver()
saver.restore(sess, tf.train.latest_checkpoint('models'))

   then we   ll load some movie reviews from our test set. remember, these
   are reviews that the model has not been trained on and has never seen
   before. the accuracy for each test batch can be seen when you run the
   following code.
iterations = 10
for i in range(iterations):
    nextbatch, nextbatchlabels = gettestbatch();
    print("accuracy for this batch:", (sess.run(accuracy, {input_data: nextbatch
, labels: nextbatchlabels})) * 100)

conclusion

   in this notebook, we went over a deep learning approach to sentiment
   analysis. we looked at the different components involved in the whole
   pipeline and then looked at the process of writing tensorflow code to
   implement the model in practice. finally, we trained and tested the
   model so that it is able to classify movie reviews.

   with the help of tensorflow, you can create your own sentiment
   classifiers to understand the large amounts of natural language in the
   world, and use the results to form actionable insights. thanks for
   reading and following along!
     __________________________________________________________________

   this post is part of a collaboration between o'reilly and
   [38]tensorflow. [39]see our statement of editorial independence.
   article image: perform id31 with lstms, using tensorflow!
   (source: [40]o'reilly).

   share
    1. [41]tweet
    2.
    3.
     __________________________________________________________________

[42]adit deshpande

   adit deshpande is a 2nd year undergraduate student majoring in computer
   science at ucla. he is vice president of acm ai, the artificial
   intelligence club on campus. with regular posts on his personal blog,
   he's interested in sharing and communicating his knowledge of different
   topics in computer science. he's passionate about applying knowledge of
   machine learning to important fields such as healthcare and education.
   [43]more
     __________________________________________________________________

   [44]bots landscape.

   [45]ai

[46]infographic: the bot platform ecosystem

   by [47]jon bruner

   a look at the artificial intelligence and messaging platforms behind
   the fast-growing chatbot community

   video
   [48]vertical forest, milan.

   [49]ai

[50]evolve ai

   by [51]naveen rao

   naveen rao explains how intel nervana is evolving the ai stack from
   silicon to the cloud.

   [52]welcome sign at o'reilly ai conference 2016

   [53]ai

[54]highlights from the o'reilly ai conference in new york 2016

   by [55]mac slocum

   watch highlights covering artificial intelligence, machine learning,
   intelligence engineering, and more. from the o'reilly ai conference in
   new york 2016.

   video
   [56]close up of uber's self driving car in pittsburgh.

   [57]ai

[58]how ai is propelling driverless cars, the future of surface transport

   by [59]shahin farshchi

   shahin farshchi examines role artificial intelligence will play in
   driverless cars.

about us

     * [60]our company
     * [61]teach/speak/write
     * [62]careers
     * [63]customer service
     * [64]contact us

site map

     * [65]ideas
     * [66]learning
     * [67]topics
     * [68]all

     *
     *
     *
     *
     *

      2019 o'reilly media, inc. all trademarks and registered trademarks
   appearing on oreilly.com are the property of their respective owners.
   [69]terms of service     [70]privacy policy     [71]editorial independence

   animal

   iframe: [72]https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

references

   visible links
   1. https://www.oreilly.com/ideas
   2. https://learning.oreilly.com/
   3. http://www.oreilly.com/conferences/
   4. http://shop.oreilly.com/
   5. https://www.oreilly.com/sign-in.html
   6. https://www.oreilly.com/topics/ai
   7. https://www.oreilly.com/topics/data
   8. https://www.oreilly.com/topics/economy
   9. https://www.oreilly.com/topics/operations
  10. https://www.oreilly.com/topics/software-architecture
  11. https://www.oreilly.com/topics/software-engineering
  12. https://www.oreilly.com/topics/web-programming
  13. https://www.oreilly.com/topics
  14. https://www.oreilly.com/ideas
  15. https://learning.oreilly.com/
  16. http://www.oreilly.com/conferences/
  17. http://shop.oreilly.com/
  18. https://www.oreilly.com/topics/ai
  19. https://www.oreilly.com/topics/data
  20. https://www.oreilly.com/topics/economy
  21. https://www.oreilly.com/topics/operations
  22. https://www.oreilly.com/topics/software-architecture
  23. https://www.oreilly.com/topics/software-engineering
  24. https://www.oreilly.com/topics/web-programming
  25. https://www.oreilly.com/topics
  26. https://www.oreilly.com/topics/ai
  27. https://www.oreilly.com/people/adit-deshpande
  28. http://www.oreilly.com/
  29. https://conferences.oreilly.com/artificial-intelligence/ai-ny/public/schedule/detail/73535
  30. https://github.com/adeshpande3/lstm-sentiment-analysis
  31. https://www.tensorflow.org/tutorials/id97
  32. http://colah.github.io/posts/2015-08-understanding-lstms/
  33. https://code.google.com/archive/p/id97/#pre-trained_word_and_phrase_vectors
  34. http://nlp.stanford.edu/projects/glove/
  35. https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms
  36. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  37. http://localhost:6006/
  38. https://www.tensorflow.org/
  39. http://www.oreilly.com/about/editorial_independence.html
  40. http://www.oreilly.com/
  41. https://twitter.com/share
  42. https://www.oreilly.com/people/adit-deshpande
  43. https://www.oreilly.com/people/adit-deshpande
  44. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  45. https://www.oreilly.com/topics/ai
  46. https://www.oreilly.com/ideas/infographic-the-bot-platform-ecosystem
  47. https://www.oreilly.com/people/b1d73-jon-bruner
  48. https://www.oreilly.com/ideas/evolve-ai
  49. https://www.oreilly.com/topics/ai
  50. https://www.oreilly.com/ideas/evolve-ai
  51. https://www.oreilly.com/people/14d38-naveen-rao
  52. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  53. https://www.oreilly.com/topics/ai
  54. https://www.oreilly.com/ideas/keynotes-from-ai-new-york-2016
  55. https://www.oreilly.com/people/0d2c1-mac-slocum
  56. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  57. https://www.oreilly.com/topics/ai
  58. https://www.oreilly.com/ideas/how-ai-is-propelling-driverless-cars-the-future-of-surface-transport
  59. https://www.oreilly.com/people/c7521-shahin-farshchi
  60. http://oreilly.com/about/
  61. http://oreilly.com/work-with-us.html
  62. http://oreilly.com/careers/
  63. http://shop.oreilly.com/category/customer-service.do
  64. http://shop.oreilly.com/category/customer-service.do
  65. https://www.oreilly.com/ideas
  66. https://www.oreilly.com/topics/oreilly-learning
  67. https://www.oreilly.com/topics
  68. https://www.oreilly.com/all
  69. http://oreilly.com/terms/
  70. http://oreilly.com/privacy.html
  71. http://www.oreilly.com/about/editorial_independence.html
  72. https://www.googletagmanager.com/ns.html?id=gtm-5p4v6z

   hidden links:
  74. https://www.oreilly.com/
  75. https://www.facebook.com/oreilly/
  76. https://twitter.com/oreillymedia
  77. https://www.youtube.com/user/oreillymedia
  78. https://plus.google.com/+oreillymedia
  79. https://www.linkedin.com/company/oreilly-media
  80. https://www.oreilly.com/
