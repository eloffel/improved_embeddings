   #[1]github [2]recent commits to id4:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]229
     * [35]star [36]4,438
     * [37]fork [38]1,537

[39]tensorflow/[40]id4

   [41]code [42]issues 214 [43]pull requests 20 [44]projects 0
   [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   tensorflow id4 tutorial
     * [47]203 commits
     * [48]3 branches
     * [49]0 releases
     * [50]23 contributors
     * [51]apache-2.0

    1. [52]python 97.3%
    2. [53]shell 2.7%

   (button) python shell
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/t
   [55]download zip

downloading...

   want to be notified of new releases in tensorflow/id4?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@ranjita-naik [63]@ebrevdo
   [64]ranjita-naik and [65]ebrevdo [66]fixing [67]#405 (button)    
piperorigin-revid: 233662426

   latest commit [68]0be8642 feb 13, 2019
   [69]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [70]id4 [71]fixing [72]#405 feb 13, 2019
   [73]contributing.md [74]piperorigin-revid: 157360504 may 29, 2017
   [75]license
   [76]readme.md [77]support coverage penalty for id125 decoder. feb
   12, 2019

readme.md

id4 (id195) tutorial

   authors: thang luong, eugene brevdo, rui zhao ([78]google research
   blogpost, [79]github)

   this version of the tutorial requires [80]tensorflow nightly. for using
   the stable tensorflow versions, please consider other branches such as
   [81]tf-1.4.

   if make use of this codebase for your research, please cite [82]this.
     * [83]introduction
     * [84]basic
          + [85]background on id4
          + [86]installing the tutorial
          + [87]training     how to build our first id4 system
               o [88]embedding
               o [89]encoder
               o [90]decoder
               o [91]loss
               o [92]gradient computation & optimization
          + [93]hands-on     let's train an id4 model
          + [94]id136     how to generate translations
     * [95]intermediate
          + [96]background on the attention mechanism
          + [97]attention wrapper api
          + [98]hands-on     building an attention-based id4 model
     * [99]tips & tricks
          + [100]building training, eval, and id136 graphs
          + [101]data input pipeline
          + [102]other details for better id4 models
               o [103]bidirectional id56s
               o [104]id125
               o [105]hyperparameters
               o [106]multi-gpu training
     * [107]benchmarks
          + [108]iwslt english-vietnamese
          + [109]wmt german-english
          + [110]wmt english-german     full comparison
          + [111]standard hparams
     * [112]other resources
     * [113]acknowledgment
     * [114]references
     * [115]bibtex

introduction

   sequence-to-sequence (id195) models ([116]sutskever et al., 2014,
   [117]cho et al., 2014) have enjoyed great success in a variety of tasks
   such as machine translation, id103, and text
   summarization. this tutorial gives readers a full understanding of
   id195 models and shows how to build a competitive id195 model from
   scratch. we focus on the task of id4 (id4) which
   was the very first testbed for id195 models with wild [118]success.
   the included code is lightweight, high-quality, production-ready, and
   incorporated with the latest research ideas. we achieve this goal by:
    1. using the recent decoder / attention wrapper [119]api, tensorflow
       1.2 data iterator
    2. incorporating our strong expertise in building recurrent and
       id195 models
    3. providing tips and tricks for building the very best id4 models and
       replicating [120]google   s id4 (gid4) system.

   we believe that it is important to provide benchmarks that people can
   easily replicate. as a result, we have provided full experimental
   results and pretrained our models on the following publicly available
   datasets:
    1. small-scale: english-vietnamese parallel corpus of ted talks (133k
       sentence pairs) provided by the [121]iwslt evaluation campaign.
    2. large-scale: german-english parallel corpus (4.5m sentence pairs)
       provided by the [122]wmt evaluation campaign.

   we first build up some basic knowledge about id195 models for id4,
   explaining how to build and train a vanilla id4 model. the second part
   will go into details of building a competitive id4 model with attention
   mechanism. we then discuss tips and tricks to build the best possible
   id4 models (both in speed and translation quality) such as tensorflow
   best practices (batching, bucketing), bidirectional id56s, id125,
   as well as scaling up to multiple gpus using gid4 attention.

basic

background on id4

   back in the old days, traditional phrase-based translation systems
   performed their task by breaking up source sentences into multiple
   chunks and then translated them phrase-by-phrase. this led to
   disfluency in the translation outputs and was not quite like how we,
   humans, translate. we read the entire source sentence, understand its
   meaning, and then produce a translation. id4
   (id4) mimics that!

                              [123][encdec.jpg]
   figure 1. encoder-decoder architecture     example of a general approach
   for id4. an encoder converts a source sentence into a "meaning" vector
         which is passed through a decoder to produce a translation.

   specifically, an id4 system first reads the source sentence using an
   encoder to build a [124]"thought" vector, a sequence of numbers that
   represents the sentence meaning; a decoder, then, processes the
   sentence vector to emit a translation, as illustrated in figure 1. this
   is often referred to as the encoder-decoder architecture. in this
   manner, id4 addresses the local translation problem in the traditional
   phrase-based approach: it can capture long-range dependencies in
   languages, e.g., gender agreements; syntax structures; etc., and
   produce much more fluent translations as demonstrated by [125]google
   id4 systems.

   id4 models vary in terms of their exact architectures. a natural choice
   for sequential data is the recurrent neural network (id56), used by most
   id4 models. usually an id56 is used for both the encoder and decoder.
   the id56 models, however, differ in terms of: (a) directionality    
   unidirectional or bidirectional; (b) depth     single- or multi-layer;
   and (c) type     often either a vanilla id56, a long short-term memory
   (lstm), or a gated recurrent unit (gru). interested readers can find
   more information about id56s and lstm on this [126]blog post.

   in this tutorial, we consider as examples a deep multi-layer id56 which
   is unidirectional and uses lstm as a recurrent unit. we show an example
   of such a model in figure 2. in this example, we build a model to
   translate a source sentence "i am a student" into a target sentence "je
   suis   tudiant". at a high level, the id4 model consists of two
   recurrent neural networks: the encoder id56 simply consumes the input
   source words without making any prediction; the decoder, on the other
   hand, processes the target sentence while predicting the next words.

   for more information, we refer readers to [127]luong (2016) which this
   tutorial is based on.

                             [128][id195.jpg]
    figure 2. id4     example of a deep recurrent
     architecture proposed by for translating a source sentence "i am a
   student" into a target sentence "je suis   tudiant". here, "<s>" marks
     the start of the decoding process while "</s>" tells the decoder to
                                    stop.

installing the tutorial

   to install this tutorial, you need to have tensorflow installed on your
   system. this tutorial requires tensorflow nightly. to install
   tensorflow, follow the [129]installation instructions here.

   once tensorflow is installed, you can download the source code of this
   tutorial by running:
git clone https://github.com/tensorflow/id4/

training     how to build our first id4 system

   let's first dive into the heart of building an id4 model with concrete
   code snippets through which we will explain figure 2 in more detail. we
   defer data preparation and the full code to later. this part refers to
   file [130]model.py.

   at the bottom layer, the encoder and decoder id56s receive as input the
   following: first, the source sentence, then a boundary marker "<s>"
   which indicates the transition from the encoding to the decoding mode,
   and the target sentence. for training, we will feed the system with the
   following tensors, which are in time-major format and contain word
   indices:
     * encoder_inputs [max_encoder_time, batch_size]: source input words.
     * decoder_inputs [max_decoder_time, batch_size]: target input words.
     * decoder_outputs [max_decoder_time, batch_size]: target output
       words, these are decoder_inputs shifted to the left by one time
       step with an end-of-sentence tag appended on the right.

   here for efficiency, we train with multiple sentences (batch_size) at
   once. testing is slightly different, so we will discuss it later.

embedding

   given the categorical nature of words, the model must first look up the
   source and target embeddings to retrieve the corresponding word
   representations. for this embedding layer to work, a vocabulary is
   first chosen for each language. usually, a vocabulary size v is
   selected, and only the most frequent v words are treated as unique. all
   other words are converted to an "unknown" token and all get the same
   embedding. the embedding weights, one set per language, are usually
   learned during training.
# embedding
embedding_encoder = variable_scope.get_variable(
    "embedding_encoder", [src_vocab_size, embedding_size], ...)
# look up embedding:
#   encoder_inputs: [max_time, batch_size]
#   encoder_emb_inp: [max_time, batch_size, embedding_size]
encoder_emb_inp = embedding_ops.embedding_lookup(
    embedding_encoder, encoder_inputs)

   similarly, we can build embedding_decoder and decoder_emb_inp. note
   that one can choose to initialize embedding weights with pretrained
   word representations such as id97 or glove vectors. in general,
   given a large amount of training data we can learn these embeddings
   from scratch.

encoder

   once retrieved, the id27s are then fed as input into the main
   network, which consists of two multi-layer id56s     an encoder for the
   source language and a decoder for the target language. these two id56s,
   in principle, can share the same weights; however, in practice, we
   often use two different id56 parameters (such models do a better job
   when fitting large training datasets). the encoder id56 uses zero
   vectors as its starting states and is built as follows:
# build id56 cell
encoder_cell = tf.nn.id56_cell.basiclstmcell(num_units)

# run dynamic id56
#   encoder_outputs: [max_time, batch_size, num_units]
#   encoder_state: [batch_size, num_units]
encoder_outputs, encoder_state = tf.nn.dynamic_id56(
    encoder_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=true)

   note that sentences have different lengths to avoid wasting
   computation, we tell dynamic_id56 the exact source sentence lengths
   through source_sequence_length. since our input is time major, we set
   time_major=true. here, we build only a single layer lstm, encoder_cell.
   we will describe how to build multi-layer lstms, add dropout, and use
   attention in a later section.

decoder

   the decoder also needs to have access to the source information, and
   one simple way to achieve that is to initialize it with the last hidden
   state of the encoder, encoder_state. in figure 2, we pass the hidden
   state at the source word "student" to the decoder side.
# build id56 cell
decoder_cell = tf.nn.id56_cell.basiclstmcell(num_units)

# helper
helper = tf.contrib.id195.traininghelper(
    decoder_emb_inp, decoder_lengths, time_major=true)
# decoder
decoder = tf.contrib.id195.basicdecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# dynamic decoding
outputs, _ = tf.contrib.id195.dynamic_decode(decoder, ...)
logits = outputs.id56_output

   here, the core part of this code is the basicdecoder object, decoder,
   which receives decoder_cell (similar to encoder_cell), a helper, and
   the previous encoder_state as inputs. by separating out decoders and
   helpers, we can reuse different codebases, e.g., traininghelper can be
   substituted with greedyembeddinghelper to do greedy decoding. see more
   in [131]helper.py.

   lastly, we haven't mentioned projection_layer which is a dense matrix
   to turn the top hidden states to logit vectors of dimension v. we
   illustrate this process at the top of figure 2.
projection_layer = layers_core.dense(
    tgt_vocab_size, use_bias=false)

loss

   given the logits above, we are now ready to compute our training loss:
crossent = tf.nn.sparse_softmax_cross_id178_with_logits(
    labels=decoder_outputs, logits=logits)
train_loss = (tf.reduce_sum(crossent * target_weights) /
    batch_size)

   here, target_weights is a zero-one matrix of the same size as
   decoder_outputs. it masks padding positions outside of the target
   sequence lengths with values 0.

   important note: it's worth pointing out that we divide the loss by
   batch_size, so our hyperparameters are "invariant" to batch_size. some
   people divide the loss by (batch_size * num_time_steps), which plays
   down the errors made on short sentences. more subtly, our
   hyperparameters (applied to the former way) can't be used for the
   latter way. for example, if both approaches use sgd with a learning of
   1.0, the latter approach effectively uses a much smaller learning rate
   of 1 / num_time_steps.

gradient computation & optimization

   we have now defined the forward pass of our id4 model. computing the
   id26 pass is just a matter of a few lines of code:
# calculate and clip gradients
params = tf.trainable_variables()
gradients = tf.gradients(train_loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(
    gradients, max_gradient_norm)

   one of the important steps in training id56s is gradient clipping. here,
   we clip by the global norm. the max value, max_gradient_norm, is often
   set to a value like 5 or 1. the last step is selecting the optimizer.
   the adam optimizer is a common choice. we also select a learning rate.
   the value of learning_rate can is usually in the range 0.0001 to 0.001;
   and can be set to decrease as training progresses.
# optimization
optimizer = tf.train.adamoptimizer(learning_rate)
update_step = optimizer.apply_gradients(
    zip(clipped_gradients, params))

   in our own experiments, we use standard sgd
   (tf.train.gradientdescentoptimizer) with a decreasing learning rate
   schedule, which yields better performance. see the [132]benchmarks.

hands-on     let's train an id4 model

   let's train our very first id4 model, translating from vietnamese to
   english! the entry point of our code is [133]id4.py.

   we will use a small-scale parallel corpus of ted talks (133k training
   examples) for this exercise. all data we used here can be found at:
   [134]https://nlp.stanford.edu/projects/id4/. we will use tst2012 as our
   dev dataset, and tst2013 as our test dataset.

   run the following command to download the data for training id4 model:
   id4/scripts/download_iwslt15.sh /tmp/id4_data

   run the following command to start the training:
mkdir /tmp/id4_model
python -m id4.id4 \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/id4_data/vocab  \
    --train_prefix=/tmp/id4_data/train \
    --dev_prefix=/tmp/id4_data/tst2012  \
    --test_prefix=/tmp/id4_data/tst2013 \
    --out_dir=/tmp/id4_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=id7

   the above command trains a 2-layer lstm id195 model with 128-dim
   hidden units and embeddings for 12 epochs. we use a dropout value of
   0.2 (keep id203 0.8). if no error, we should see logs similar to
   the below with decreasing perplexity values as we train.
# first evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# start epoch 0, step 0, lr 1, tue apr 25 23:17:41 2017
  sample train data:
    src_reverse: </s> </s>   i   u      , d   nhi  n , l   c  u chuy   n tr  ch ra t    h   c th
uy   t c   a karl marx .
    ref: that , of course , was the <unk> distilled from the theories of karl ma
rx . </s> </s> </s>
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78k ppl 1568.62 id7 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91k ppl 524.11 id7 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80k ppl 340.05 id7 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06k ppl 277.61 id7 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89k ppl 205.85 id7 0.00

   see [135]train.py for more details.

   we can start tensorboard to view the summary of the model during
   training:
tensorboard --port 22222 --logdir /tmp/id4_model/

   training the reverse direction from english and vietnamese can be done
   simply by changing:
   --src=en --tgt=vi

id136     how to generate translations

   while you're training your id4 models (and once you have trained
   models), you can obtain translations given previously unseen source
   sentences. this process is called id136. there is a clear
   distinction between training and id136 (testing): at id136
   time, we only have access to the source sentence, i.e., encoder_inputs.
   there are many ways to perform decoding. decoding methods include
   greedy, sampling, and beam-search decoding. here, we will discuss the
   greedy decoding strategy.

   the idea is simple and we illustrate it in figure 3:
    1. we still encode the source sentence in the same way as during
       training to obtain an encoder_state, and this encoder_state is used
       to initialize the decoder.
    2. the decoding (translation) process is started as soon as the
       decoder receives a starting symbol "<s>" (refer as tgt_sos_id in
       our code);
    3. for each timestep on the decoder side, we treat the id56's output as
       a set of logits. we choose the most likely word, the id associated
       with the maximum logit value, as the emitted word (this is the
       "greedy" behavior). for example in figure 3, the word "moi" has the
       highest translation id203 in the first decoding step. we then
       feed this word as input to the next timestep.
    4. the process continues until the end-of-sentence marker "</s>" is
       produced as an output symbol (refer as tgt_eos_id in our code).

                            [136][greedy_dec.jpg]
  figure 3. greedy decoding     example of how a trained id4 model produces
    a translation for a source sentence "je suis   tudiant" using greedy
                                   search.

   step 3 is what makes id136 different from training. instead of
   always feeding the correct target words as an input, id136 uses
   words predicted by the model. here's the code to achieve greedy
   decoding. it is very similar to the training decoder.
# helper
helper = tf.contrib.id195.greedyembeddinghelper(
    embedding_decoder,
    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)

# decoder
decoder = tf.contrib.id195.basicdecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# dynamic decoding
outputs, _ = tf.contrib.id195.dynamic_decode(
    decoder, maximum_iterations=maximum_iterations)
translations = outputs.sample_id

   here, we use greedyembeddinghelper instead of traininghelper. since we
   do not know the target sequence lengths in advance, we use
   maximum_iterations to limit the translation lengths. one heuristic is
   to decode up to two times the source sentence lengths.
maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)

   having trained a model, we can now create an id136 file and
   translate some sentences:
cat > /tmp/my_infer_file.vi
# (copy and paste some sentences from /tmp/id4_data/tst2013.vi)

python -m id4.id4 \
    --out_dir=/tmp/id4_model \
    --id136_input_file=/tmp/my_infer_file.vi \
    --id136_output_file=/tmp/id4_model/output_infer

cat /tmp/id4_model/output_infer # to view the id136 as output

   note the above commands can also be run while the model is still being
   trained as long as there exists a training checkpoint. see
   [137]id136.py for more details.

intermediate

   having gone through the most basic id195 model, let's get more
   advanced! to build state-of-the-art id4 systems,
   we will need more "secret sauce": the attention mechanism, which was
   first introduced by [138]bahdanau et al., 2015, then later refined by
   [139]luong et al., 2015 and others. the key idea of the attention
   mechanism is to establish direct short-cut connections between the
   target and the source by paying "attention" to relevant source content
   as we translate. a nice byproduct of the attention mechanism is an
   easy-to-visualize alignment matrix between the source and target
   sentences (as shown in figure 4).

                          [140][attention_vis.jpg]
   figure 4. attention visualization     example of the alignments between
     source and target sentences. image is taken from (bahdanau et al.,
                                   2015).

   remember that in the vanilla id195 model, we pass the last source
   state from the encoder to the decoder when starting the decoding
   process. this works well for short and medium-length sentences;
   however, for long sentences, the single fixed-size hidden state becomes
   an information bottleneck. instead of discarding all of the hidden
   states computed in the source id56, the attention mechanism provides an
   approach that allows the decoder to peek at them (treating them as a
   dynamic memory of the source information). by doing so, the attention
   mechanism improves the translation of longer sentences. nowadays,
   attention mechanisms are the defacto standard and have been
   successfully applied to many other tasks (including image caption
   generation, id103, and text summarization).

background on the attention mechanism

   we now describe an instance of the attention mechanism proposed in
   (luong et al., 2015), which has been used in several state-of-the-art
   systems including open-source toolkits such as [141]openid4 and in the
   tf id195 api in this tutorial. we will also provide connections to
   other variants of the attention mechanism.

                       [142][attention_mechanism.jpg]
     figure 5. attention mechanism     example of an attention-based id4
    system as described in (luong et al., 2015) . we highlight in detail
   the first step of the attention computation. for clarity, we don't show
             the embedding and projection layers in figure (2).

   as illustrated in figure 5, the attention computation happens at every
   decoder time step. it consists of the following stages:
    1. the current target hidden state is compared with all source states
       to derive attention weights (can be visualized as in figure 4).
    2. based on the attention weights we compute a context vector as the
       weighted average of the source states.
    3. combine the context vector with the current target hidden state to
       yield the final attention vector
    4. the attention vector is fed as an input to the next time step
       (input feeding). the first three steps can be summarized by the
       equations below:

                       [143][attention_equation_0.jpg]

   here, the function score is used to compared the target hidden state
   $$h_t$$ with each of the source hidden states $$\overline{h}_s$$, and
   the result is normalized to produced attention weights (a distribution
   over source positions). there are various choices of the scoring
   function; popular scoring functions include the multiplicative and
   additive forms given in eq. (4). once computed, the attention vector
   $$a_t$$ is used to derive the softmax logit and loss. this is similar
   to the target hidden state at the top layer of a vanilla id195 model.
   the function f can also take other forms.

                       [144][attention_equation_1.jpg]

   various implementations of attention mechanisms can be found in
   [145]attention_wrapper.py.

   what matters in the attention mechanism?

   as hinted in the above equations, there are many different attention
   variants. these variants depend on the form of the scoring function and
   the attention function, and on whether the previous state $$h_{t-1}$$
   is used instead of $$h_t$$ in the scoring function as originally
   suggested in (bahdanau et al., 2015). empirically, we found that only
   certain choices matter. first, the basic form of attention, i.e.,
   direct connections between target and source, needs to be present.
   second, it's important to feed the attention vector to the next
   timestep to inform the network about past attention decisions as
   demonstrated in (luong et al., 2015). lastly, choices of the scoring
   function can often result in different performance. see more in the
   [146]benchmark results section.

attention wrapper api

   in our implementation of the [147]attentionwrapper, we borrow some
   terminology from [148](weston et al., 2015) in their work on memory
   networks. instead of having readable & writable memory, the attention
   mechanism presented in this tutorial is a read-only memory.
   specifically, the set of source hidden states (or their transformed
   versions, e.g., $$w\overline{h}_s$$ in luong's scoring style or
   $$w_2\overline{h}_s$$ in bahdanau's scoring style) is referred to as
   the "memory". at each time step, we use the current target hidden state
   as a "query" to decide on which parts of the memory to read. usually,
   the query needs to be compared with keys corresponding to individual
   memory slots. in the above presentation of the attention mechanism, we
   happen to use the set of source hidden states (or their transformed
   versions, e.g., $$w_1h_t$$ in bahdanau's scoring style) as "keys". one
   can be inspired by this memory-network terminology to derive other
   forms of attention!

   thanks to the attention wrapper, extending our vanilla id195 code
   with attention is trivial. this part refers to file
   [149]attention_model.py

   first, we need to define an attention mechanism, e.g., from (luong et
   al., 2015):
# attention_states: [batch_size, max_time, num_units]
attention_states = tf.transpose(encoder_outputs, [1, 0, 2])

# create an attention mechanism
attention_mechanism = tf.contrib.id195.luongattention(
    num_units, attention_states,
    memory_sequence_length=source_sequence_length)

   in the previous [150]encoder section, encoder_outputs is the set of all
   source hidden states at the top layer and has the shape of [max_time,
   batch_size, num_units] (since we use dynamic_id56 with time_major set to
   true for efficiency). for the attention mechanism, we need to make sure
   the "memory" passed in is batch major, so we need to transpose
   attention_states. we pass source_sequence_length to the attention
   mechanism to ensure that the attention weights are properly normalized
   (over non-padding positions only).

   having defined an attention mechanism, we use attentionwrapper to wrap
   the decoding cell:
decoder_cell = tf.contrib.id195.attentionwrapper(
    decoder_cell, attention_mechanism,
    attention_layer_size=num_units)

   the rest of the code is almost the same as in the section [151]decoder!

hands-on     building an attention-based id4 model

   to enable attention, we need to use one of luong, scaled_luong,
   bahdanau or normed_bahdanau as the value of the attention flag during
   training. the flag specifies which attention mechanism we are going to
   use. in addition, we need to create a new directory for the attention
   model, so we don't reuse the previously trained basic id4 model.

   run the following command to start the training:
mkdir /tmp/id4_attention_model

python -m id4.id4 \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/id4_data/vocab  \
    --train_prefix=/tmp/id4_data/train \
    --dev_prefix=/tmp/id4_data/tst2012  \
    --test_prefix=/tmp/id4_data/tst2013 \
    --out_dir=/tmp/id4_attention_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=id7

   after training, we can use the same id136 command with the new
   out_dir for id136:
python -m id4.id4 \
    --out_dir=/tmp/id4_attention_model \
    --id136_input_file=/tmp/my_infer_file.vi \
    --id136_output_file=/tmp/id4_attention_model/output_infer

tips & tricks

building training, eval, and id136 graphs

   when building a machine learning model in tensorflow, it's often best
   to build three separate graphs:
     * the training graph, which:
          + batches, buckets, and possibly subsamples input data from a
            set of files/external inputs.
          + includes the forward and backprop ops.
          + constructs the optimizer, and adds the training op.
     * the eval graph, which:
          + batches and buckets input data from a set of files/external
            inputs.
          + includes the training forward ops, and additional evaluation
            ops that aren't used for training.
     * the id136 graph, which:
          + may not batch input data.
          + does not subsample or bucket input data.
          + reads input data from placeholders (data can be fed directly
            to the graph via feed_dict or from a c++ tensorflow serving
            binary).
          + includes a subset of the model forward ops, and possibly
            additional special inputs/outputs for storing state between
            session.run calls.

   building separate graphs has several benefits:
     * the id136 graph is usually very different from the other two,
       so it makes sense to build it separately.
     * the eval graph becomes simpler since it no longer has all the
       additional backprop ops.
     * data feeding can be implemented separately for each graph.
     * variable reuse is much simpler. for example, in the eval graph
       there's no need to reopen variable scopes with reuse=true just
       because the training model created these variables already. so the
       same code can be reused without sprinkling reuse= arguments
       everywhere.
     * in distributed training, it is commonplace to have separate workers
       perform training, eval, and id136. these need to build their
       own graphs anyway. so building the system this way prepares you for
       distributed training.

   the primary source of complexity becomes how to share variables across
   the three graphs in a single machine setting. this is solved by using a
   separate session for each graph. the training session periodically
   saves checkpoints, and the eval session and the infer session restore
   parameters from checkpoints. the example below shows the main
   differences between the two approaches.

   before: three models in a single graph and sharing a single session
with tf.variable_scope('root'):
  train_inputs = tf.placeholder()
  train_op, loss = buildtrainmodel(train_inputs)
  initializer = tf.global_variables_initializer()

with tf.variable_scope('root', reuse=true):
  eval_inputs = tf.placeholder()
  eval_loss = buildevalmodel(eval_inputs)

with tf.variable_scope('root', reuse=true):
  infer_inputs = tf.placeholder()
  id136_output = buildid136model(infer_inputs)

sess = tf.session()

sess.run(initializer)

for i in itertools.count():
  train_input_data = ...
  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})

  if i % eval_steps == 0:
    while data_to_eval:
      eval_input_data = ...
      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})

  if i % infer_steps == 0:
    sess.run(id136_output, feed_dict={infer_inputs: infer_input_data})

   after: three models in three graphs, with three sessions sharing the
   same variables
train_graph = tf.graph()
eval_graph = tf.graph()
infer_graph = tf.graph()

with train_graph.as_default():
  train_iterator = ...
  train_model = buildtrainmodel(train_iterator)
  initializer = tf.global_variables_initializer()

with eval_graph.as_default():
  eval_iterator = ...
  eval_model = buildevalmodel(eval_iterator)

with infer_graph.as_default():
  infer_iterator, infer_inputs = ...
  infer_model = buildid136model(infer_iterator)

checkpoints_path = "/tmp/model/checkpoints"

train_sess = tf.session(graph=train_graph)
eval_sess = tf.session(graph=eval_graph)
infer_sess = tf.session(graph=infer_graph)

train_sess.run(initializer)
train_sess.run(train_iterator.initializer)

for i in itertools.count():

  train_model.train(train_sess)

  if i % eval_steps == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, globa
l_step=i)
    eval_model.saver.restore(eval_sess, checkpoint_path)
    eval_sess.run(eval_iterator.initializer)
    while data_to_eval:
      eval_model.eval(eval_sess)

  if i % infer_steps == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, globa
l_step=i)
    infer_model.saver.restore(infer_sess, checkpoint_path)
    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_in
put_data})
    while data_to_infer:
      infer_model.infer(infer_sess)

   notice how the latter approach is "ready" to be converted to a
   distributed version.

   one other difference in the new approach is that instead of using
   feed_dicts to feed data at each session.run call (and thereby
   performing our own batching, bucketing, and manipulating of data), we
   use stateful iterator objects. these iterators make the input pipeline
   much easier in both the single-machine and distributed setting. we will
   cover the new input data pipeline (as introduced in tensorflow 1.2) in
   the next section.

data input pipeline

   prior to tensorflow 1.2, users had two options for feeding data to the
   tensorflow training and eval pipelines:
    1. feed data directly via feed_dict at each training session.run call.
    2. use the queueing mechanisms in tf.train (e.g. tf.train.batch) and
       tf.contrib.train.
    3. use helpers from a higher level framework like tf.contrib.learn or
       tf.contrib.slim (which effectively use #2).

   the first approach is easier for users who aren't familiar with
   tensorflow or need to do exotic input modification (i.e., their own
   minibatch queueing) that can only be done in python. the second and
   third approaches are more standard but a little less flexible; they
   also require starting multiple python threads (queue runners).
   furthermore, if used incorrectly queues can lead to deadlocks or opaque
   error messages. nevertheless, queues are significantly more efficient
   than using feed_dict and are the standard for both single-machine and
   distributed training.

   starting in tensorflow 1.2, there is a new system available for reading
   data into tensorflow models: dataset iterators, as found in the tf.data
   module. data iterators are flexible, easy to reason about and to
   manipulate, and provide efficiency and multithreading by leveraging the
   tensorflow c++ runtime.

   a dataset can be created from a batch data tensor, a filename, or a
   tensor containing multiple filenames. some examples:
# training dataset consists of multiple files.
train_dataset = tf.data.textlinedataset(train_files)

# evaluation dataset uses a single file, but we may
# point to a different file for each evaluation round.
eval_file = tf.placeholder(tf.string, shape=())
eval_dataset = tf.data.textlinedataset(eval_file)

# for id136, feed input data to the dataset directly via feed_dict.
infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))
infer_dataset = tf.data.dataset.from_tensor_slices(infer_batch)

   all datasets can be treated similarly via input processing. this
   includes reading and cleaning the data, bucketing (in the case of
   training and eval), filtering, and batching.

   to convert each sentence into vectors of word strings, for example, we
   use the dataset map transformation:
dataset = dataset.map(lambda string: tf.string_split([string]).values)

   we can then switch each sentence vector into a tuple containing both
   the vector and its dynamic length:
dataset = dataset.map(lambda words: (words, tf.size(words))

   finally, we can perform a vocabulary lookup on each sentence. given a
   lookup table object table, this map converts the first tuple elements
   from a vector of strings to a vector of integers.
dataset = dataset.map(lambda words, size: (table.lookup(words), size))

   joining two datasets is also easy. if two files contain line-by-line
   translations of each other and each one is read into its own dataset,
   then a new dataset containing the tuples of the zipped lines can be
   created via:
source_target_dataset = tf.data.dataset.zip((source_dataset, target_dataset))

   batching of variable-length sentences is straightforward. the following
   transformation batches batch_size elements from source_target_dataset,
   and respectively pads the source and target vectors to the length of
   the longest source and target vector in each batch.
batched_dataset = source_target_dataset.padded_batch(
        batch_size,
        padded_shapes=((tf.tensorshape([none]),  # source vectors of unknown siz
e
                        tf.tensorshape([])),     # size(source)
                       (tf.tensorshape([none]),  # target vectors of unknown siz
e
                        tf.tensorshape([]))),    # size(target)
        padding_values=((src_eos_id,  # source vectors padded on the right with
src_eos_id
                         0),          # size(source) -- unused
                        (tgt_eos_id,  # target vectors padded on the right with
tgt_eos_id
                         0)))         # size(target) -- unused

   values emitted from this dataset will be nested tuples whose tensors
   have a leftmost dimension of size batch_size. the structure will be:
     * iterator[0][0] has the batched and padded source sentence matrices.
     * iterator[0][1] has the batched source size vectors.
     * iterator[1][0] has the batched and padded target sentence matrices.
     * iterator[1][1] has the batched target size vectors.

   finally, bucketing that batches similarly-sized source sentences
   together is also possible. please see the file
   [152]utils/iterator_utils.py for more details and the full
   implementation.

   reading data from a dataset requires three lines of code: create the
   iterator, get its values, and initialize it.
batched_iterator = batched_dataset.make_initializable_iterator()

((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next
()

# at initialization time.
session.run(batched_iterator.initializer, feed_dict={...})

   once the iterator is initialized, every session.run call that accesses
   source or target tensors will request the next minibatch from the
   underlying dataset.

other details for better id4 models

bidirectional id56s

   bidirectionality on the encoder side generally gives better performance
   (with some degradation in speed as more layers are used). here, we give
   a simplified example of how to build an encoder with a single
   bidirectional layer:
# construct forward and backward cells
forward_cell = tf.nn.id56_cell.basiclstmcell(num_units)
backward_cell = tf.nn.id56_cell.basiclstmcell(num_units)

bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_id56(
    forward_cell, backward_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=true)
encoder_outputs = tf.concat(bi_outputs, -1)

   the variables encoder_outputs and encoder_state can be used in the same
   way as in section encoder. note that, for multiple bidirectional
   layers, we need to manipulate the encoder_state a bit, see
   [153]model.py, method _build_bidirectional_id56() for more details.

id125

   while greedy decoding can give us quite reasonable translation quality,
   a id125 decoder can further boost performance. the idea of beam
   search is to better explore the search space of all possible
   translations by keeping around a small set of top candidates as we
   translate. the size of the beam is called beam width; a minimal beam
   width of, say size 10, is generally sufficient. for more information,
   we refer readers to section 7.2.3 of [154]neubig, (2017). here's an
   example of how id125 can be done:
# replicate encoder infos beam_width times
decoder_initial_state = tf.contrib.id195.tile_batch(
    encoder_state, multiplier=hparams.beam_width)

# define a beam-search decoder
decoder = tf.contrib.id195.beamsearchdecoder(
        cell=decoder_cell,
        embedding=embedding_decoder,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=decoder_initial_state,
        beam_width=beam_width,
        output_layer=projection_layer,
        length_penalty_weight=0.0,
        coverage_penalty_weight=0.0)

# dynamic decoding
outputs, _ = tf.contrib.id195.dynamic_decode(decoder, ...)

   note that the same dynamic_decode() api call is used, similar to the
   section [155]decoder. once decoded, we can access the translations as
   follows:
translations = outputs.predicted_ids
# make sure translations shape is [batch_size, beam_width, time]
if self.time_major:
   translations = tf.transpose(translations, perm=[1, 2, 0])

   see [156]model.py, method _build_decoder() for more details.

hyperparameters

   there are several hyperparameters that can lead to additional
   performances. here, we list some based on our own experience [
   disclaimers: others might not agree on things we wrote! ].

   optimizer: while adam can lead to reasonable results for "unfamiliar"
   architectures, sgd with scheduling will generally lead to better
   performance if you can train with sgd.

   attention: bahdanau-style attention often requires bidirectionality on
   the encoder side to work well; whereas luong-style attention tends to
   work well for different settings. for this tutorial code, we recommend
   using the two improved variants of luong & bahdanau-style attentions:
   scaled_luong & normed bahdanau.

multi-gpu training

   training a id4 model may take several days. placing different id56
   layers on different gpus can improve the training speed. here   s an
   example to create id56 layers on multiple gpus.
cells = []
for i in range(num_layers):
  cells.append(tf.contrib.id56.devicewrapper(
      tf.contrib.id56.lstmcell(num_units),
      "/gpu:%d" % (num_layers % num_gpus)))
cell = tf.contrib.id56.multiid56cell(cells)

   in addition, we need to enable the colocate_gradients_with_ops option
   in tf.gradients to parallelize the gradients computation.

   you may notice the speed improvement of the attention based id4 model
   is very small as the number of gpus increases. one major drawback of
   the standard attention architecture is using the top (final) layer   s
   output to query attention at each time step. that means each decoding
   step must wait its previous step completely finished; hence, we can   t
   parallelize the decoding process by simply placing id56 layers on
   multiple gpus.

   the [157]gid4 attention architecture parallelizes the decoder's
   computation by using the bottom (first) layer   s output to query
   attention. therefore, each decoding step can start as soon as its
   previous step's first layer and attention computation finished. we
   implemented the architecture in [158]gid4attentionmulticell, a subclass
   of tf.contrib.id56.multiid56cell. here   s an example of how to create a
   decoder cell with the gid4attentionmulticell.
cells = []
for i in range(num_layers):
  cells.append(tf.contrib.id56.devicewrapper(
      tf.contrib.id56.lstmcell(num_units),
      "/gpu:%d" % (num_layers % num_gpus)))
attention_cell = cells.pop(0)
attention_cell = tf.contrib.id195.attentionwrapper(
    attention_cell,
    attention_mechanism,
    attention_layer_size=none,  # don't add an additional dense layer.
    output_attention=false,)
cell = gid4attentionmulticell(attention_cell, cells)

benchmarks

iwslt english-vietnamese

   train: 133k examples, vocab=vocab.(vi|en), train=train.(vi|en)
   dev=tst2012.(vi|en), test=tst2013.(vi|en), [159]download script.

   training details. we train 2-layer lstms of 512 units with
   bidirectional encoder (i.e., 1 bidirectional layers for the encoder),
   embedding dim is 512. luongattention (scale=true) is used together with
   dropout keep_prob of 0.8. all parameters are uniformly. we use sgd with
   learning rate 1.0 as follows: train for 12k steps (~ 12 epochs); after
   8k steps, we start halving learning rate every 1k step.

   results.

   below are the averaged results of 2 models ([160]model 1, [161]model
   2).
   we measure the translation quality in terms of id7 scores
   [162](papineni et al., 2002).
             systems            tst2012 (dev) test2013 (test)
   id4 (greedy)                     23.2           25.5
   id4 (beam=10)                    23.8           26.1
   [163](luong & manning, 2015)       -            23.3

   training speed: (0.37s step-time, 15.3k wps) on k40m & (0.17s
   step-time, 32.2k wps) on titanx.
   here, step-time means the time taken to run one mini-batch (of size
   128). for wps, we count words on both the source and target.

wmt german-english

   train: 4.5m examples, vocab=vocab.bpe.32000.(de|en),
   train=train.tok.clean.bpe.32000.(de|en),
   dev=newstest2013.tok.bpe.32000.(de|en),
   test=newstest2015.tok.bpe.32000.(de|en), [164]download script

   training details. our training hyperparameters are similar to the
   english-vietnamese experiments except for the following details. the
   data is split into subword units using [165]bpe (32k operations). we
   train 4-layer lstms of 1024 units with bidirectional encoder (i.e., 2
   bidirectional layers for the encoder), embedding dim is 1024. we train
   for 350k steps (~ 10 epochs); after 170k steps, we start halving
   learning rate every 17k step.

   results.

   the first 2 rows are the averaged results of 2 models ([166]model 1,
   [167]model 2). results in the third row is with gid4 attention
   ([168]model) ; trained with 4 gpus.
              systems             newstest2013 (dev) newstest2015
   id4 (greedy)                          27.1            27.6
   id4 (beam=10)                         28.0            28.9
   id4 + gid4 attention (beam=10)        29.0            29.9
   [169]wmt sota                          -              29.3

   these results show that our code builds strong baseline systems for
   id4.
   (note that wmt systems generally utilize a huge amount monolingual data
   which we currently do not.)

   training speed: (2.1s step-time, 3.4k wps) on nvidia k40m & (0.7s
   step-time, 8.7k wps) on nvidia titanx for standard models.
   to see the speed-ups with gid4 attention, we benchmark on k40m only:
               systems               1 gpu      4 gpus     8 gpus
   id4 (4 layers)                  2.2s, 3.4k 1.9s, 3.9k     -
   id4 (8 layers)                  3.5s, 2.0k     -      2.9s, 2.4k
   id4 + gid4 attention (4 layers) 2.6s, 2.8k 1.7s, 4.3k     -
   id4 + gid4 attention (8 layers) 4.2s, 1.7k     -      1.9s, 3.8k

   these results show that without gid4 attention, the gains from using
   multiple gpus are minimal.
   with gid4 attention, we obtain from 50%-100% speed-ups with multiple
   gpus.

wmt english-german     full comparison

   the first 2 rows are our models with gid4 attention: [170]model 1 (4
   layers), [171]model 2 (8 layers).
                  systems                 newstest2014 newstest2015
   ours     id4 + gid4 attention (4 layers)     23.7         26.5
   ours     id4 + gid4 attention (8 layers)     24.4         27.6
   [172]wmt sota                              20.6         24.9
   openid4 [173](klein et al., 2017)          19.3          -
   tf-id195 [174](britz et al., 2017)       22.2         25.2
   gid4 [175](wu et al., 2016)                24.6          -

   the above results show our models are very competitive among models of
   similar architectures.
   [note that openid4 uses smaller models and the current best result (as
   of this writing) is 28.4 obtained by the transformer network
   [176](vaswani et al., 2017) which has a significantly different
   architecture.]

standard hparams

   we have provided [177]a set of standard hparams for using pre-trained
   checkpoint for id136 or training id4 architectures used in the
   benchmark.

   we will use the wmt16 german-english data, you can download the data by
   the following command.
id4/scripts/wmt16_en_de.sh /tmp/wmt16

   here is an example command for loading the pre-trained gid4 wmt
   german-english checkpoint for id136.
python -m id4.id4 \
    --src=de --tgt=en \
    --ckpt=/path/to/checkpoint/translate.ckpt \
    --hparams_path=id4/standard_hparams/wmt16_gid4_4_layer.json \
    --out_dir=/tmp/deen_gid4 \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --id136_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \
    --id136_output_file=/tmp/deen_gid4/output_infer \
    --id136_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en

   here is an example command for training the gid4 wmt german-english
   model.
python -m id4.id4 \
    --src=de --tgt=en \
    --hparams_path=id4/standard_hparams/wmt16_gid4_4_layer.json \
    --out_dir=/tmp/deen_gid4 \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \
    --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \
    --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000

other resources

   for deeper reading on id4 and
   sequence-to-sequence models, we highly recommend the following
   materials by [178]luong, cho, manning, (2016); [179]luong, (2016); and
   [180]neubig, (2017).

   there's a wide variety of tools for building id195 models, so we pick
   one per language:
   stanford id4 [181]https://nlp.stanford.edu/projects/id4/ [matlab]
   tf-id195 [182]https://github.com/google/id195 [tensorflow]
   nemantus [183]https://github.com/rsennrich/nematus [theano]
   openid4 [184]http://openid4.net/ [torch]
   openid4-py [185]https://github.com/openid4/openid4-py [pytorch]

acknowledgment

   we would like to thank denny britz, anna goldie, derek murray, and
   cinjon resnick for their work bringing new features to tensorflow and
   the id195 library. additional thanks go to lukasz kaiser for the
   initial help on the id195 codebase; quoc le for the suggestion to
   replicate gid4; yonghui wu and zhifeng chen for details on the gid4
   systems; as well as the google brain team for their support and
   feedback!

references

     * dzmitry bahdanau, kyunghyun cho, and yoshua bengio. 2015.[186]
       id4 by jointly learning to align and
       translate. iclr.
     * minh-thang luong, hieu pham, and christopher d manning. 2015.[187]
       effective approaches to attention-based id4.
       emnlp.
     * ilya sutskever, oriol vinyals, and quoc v. le. 2014.[188] sequence
       to sequence learning with neural networks. nips.

bibtex

@article{luong17,
  author  = {minh{-}thang luong and eugene brevdo and rui zhao},
  title   = {id4 (id195) tutorial},
  journal = {https://github.com/tensorflow/id4},
  year    = {2017},
}

     *    2019 github, inc.
     * [189]terms
     * [190]privacy
     * [191]security
     * [192]status
     * [193]help

     * [194]contact github
     * [195]pricing
     * [196]api
     * [197]training
     * [198]blog
     * [199]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [200]reload to refresh your
   session. you signed out in another tab or window. [201]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/tensorflow/id4/commits/master.atom
   3. https://github.com/tensorflow/id4#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/tensorflow/id4
  32. https://github.com/join
  33. https://github.com/login?return_to=/tensorflow/id4
  34. https://github.com/tensorflow/id4/watchers
  35. https://github.com/login?return_to=/tensorflow/id4
  36. https://github.com/tensorflow/id4/stargazers
  37. https://github.com/login?return_to=/tensorflow/id4
  38. https://github.com/tensorflow/id4/network/members
  39. https://github.com/tensorflow
  40. https://github.com/tensorflow/id4
  41. https://github.com/tensorflow/id4
  42. https://github.com/tensorflow/id4/issues
  43. https://github.com/tensorflow/id4/pulls
  44. https://github.com/tensorflow/id4/projects
  45. https://github.com/tensorflow/id4/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/tensorflow/id4/commits/master
  48. https://github.com/tensorflow/id4/branches
  49. https://github.com/tensorflow/id4/releases
  50. https://github.com/tensorflow/id4/graphs/contributors
  51. https://github.com/tensorflow/id4/blob/master/license
  52. https://github.com/tensorflow/id4/search?l=python
  53. https://github.com/tensorflow/id4/search?l=shell
  54. https://github.com/tensorflow/id4/find/master
  55. https://github.com/tensorflow/id4/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/tensorflow/id4
  57. https://github.com/join?return_to=/tensorflow/id4
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/ranjita-naik
  63. https://github.com/ebrevdo
  64. https://github.com/tensorflow/id4/commits?author=ranjita-naik
  65. https://github.com/tensorflow/id4/commits?author=ebrevdo
  66. https://github.com/tensorflow/id4/commit/0be864257a76c151eef20ea689755f08bc1faf4e
  67. https://github.com/tensorflow/id4/issues/405
  68. https://github.com/tensorflow/id4/commit/0be864257a76c151eef20ea689755f08bc1faf4e
  69. https://github.com/tensorflow/id4/tree/0be864257a76c151eef20ea689755f08bc1faf4e
  70. https://github.com/tensorflow/id4/tree/master/id4
  71. https://github.com/tensorflow/id4/commit/0be864257a76c151eef20ea689755f08bc1faf4e
  72. https://github.com/tensorflow/id4/issues/405
  73. https://github.com/tensorflow/id4/blob/master/contributing.md
  74. https://github.com/tensorflow/id4/commit/d8e6e8355ed9ad55b502e1292583ac55f0d7f756
  75. https://github.com/tensorflow/id4/blob/master/license
  76. https://github.com/tensorflow/id4/blob/master/readme.md
  77. https://github.com/tensorflow/id4/commit/3bad10b10530f645d85f23b0a0b7ec96b50687b9
  78. https://research.googleblog.com/2017/07/building-your-own-neural-machine.html
  79. https://github.com/tensorflow/id4
  80. https://github.com/tensorflow/tensorflow/#installation
  81. https://github.com/tensorflow/id4/tree/tf-1.4
  82. https://github.com/tensorflow/id4#bibtex
  83. https://github.com/tensorflow/id4#introduction
  84. https://github.com/tensorflow/id4#basic
  85. https://github.com/tensorflow/id4#background-on-neural-machine-translation
  86. https://github.com/tensorflow/id4#installing-the-tutorial
  87. https://github.com/tensorflow/id4#training--how-to-build-our-first-id4-system
  88. https://github.com/tensorflow/id4#embedding
  89. https://github.com/tensorflow/id4#encoder
  90. https://github.com/tensorflow/id4#decoder
  91. https://github.com/tensorflow/id4#loss
  92. https://github.com/tensorflow/id4#gradient-computation--optimization
  93. https://github.com/tensorflow/id4#hands-on--lets-train-an-id4-model
  94. https://github.com/tensorflow/id4#id136--how-to-generate-translations
  95. https://github.com/tensorflow/id4#intermediate
  96. https://github.com/tensorflow/id4#background-on-the-attention-mechanism
  97. https://github.com/tensorflow/id4#attention-wrapper-api
  98. https://github.com/tensorflow/id4#hands-on--building-an-attention-based-id4-model
  99. https://github.com/tensorflow/id4#tips--tricks
 100. https://github.com/tensorflow/id4#building-training-eval-and-id136-graphs
 101. https://github.com/tensorflow/id4#data-input-pipeline
 102. https://github.com/tensorflow/id4#other-details-for-better-id4-models
 103. https://github.com/tensorflow/id4#bidirectional-id56s
 104. https://github.com/tensorflow/id4#beam-search
 105. https://github.com/tensorflow/id4#hyperparameters
 106. https://github.com/tensorflow/id4#multi-gpu-training
 107. https://github.com/tensorflow/id4#benchmarks
 108. https://github.com/tensorflow/id4#iwslt-english-vietnamese
 109. https://github.com/tensorflow/id4#wmt-german-english
 110. https://github.com/tensorflow/id4#wmt-english-german--full-comparison
 111. https://github.com/tensorflow/id4#standard-hparams
 112. https://github.com/tensorflow/id4#other-resources
 113. https://github.com/tensorflow/id4#acknowledgment
 114. https://github.com/tensorflow/id4#references
 115. https://github.com/tensorflow/id4#bibtex
 116. https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
 117. http://emnlp2014.org/papers/pdf/emnlp2014179.pdf
 118. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
 119. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/id195/python/ops
 120. https://research.google.com/pubs/pub45610.html
 121. https://sites.google.com/site/iwsltevaluation2015/
 122. http://www.statmt.org/wmt16/translation-task.html
 123. https://github.com/tensorflow/id4/blob/master/id4/g3doc/img/encdec.jpg
 124. https://www.theguardian.com/science/2015/may/21/google-a-step-closer-to-developing-machines-with-human-like-intelligence
 125. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
 126. http://colah.github.io/posts/2015-08-understanding-lstms/
 127. https://github.com/lmthang/thesis
 128. https://github.com/tensorflow/id4/blob/master/id4/g3doc/img/id195.jpg
 129. https://www.tensorflow.org/install/
 130. https://github.com/tensorflow/id4/blob/master/id4/model.py
 131. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/id195/python/ops/helper.py
 132. https://github.com/tensorflow/id4#benchmarks
 133. https://github.com/tensorflow/id4/blob/master/id4/id4.py
 134. https://nlp.stanford.edu/projects/id4/
 135. https://github.com/tensorflow/id4/blob/master/id4/train.py
 136. https://github.com/tensorflow/id4/blob/master/id4/g3doc/img/greedy_dec.jpg
 137. https://github.com/tensorflow/id4/blob/master/id4/id136.py
 138. https://arxiv.org/abs/1409.0473
 139. https://arxiv.org/abs/1508.04025
 140. https://github.com/tensorflow/id4/blob/master/id4/g3doc/img/attention_vis.jpg
 141. http://openid4.net/about/
 142. https://github.com/tensorflow/id4/blob/master/id4/g3doc/img/attention_mechanism.jpg
 143. https://github.com/tensorflow/id4/blob/master/id4/g3doc/img/attention_equation_0.jpg
 144. https://github.com/tensorflow/id4/blob/master/id4/g3doc/img/attention_equation_1.jpg
 145. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/id195/python/ops/attention_wrapper.py
 146. https://github.com/tensorflow/id4#benchmarks
 147. https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/id195/python/ops/attention_wrapper.py
 148. https://arxiv.org/abs/1410.3916
 149. https://github.com/tensorflow/id4/blob/master/id4/attention_model.py
 150. https://github.com/tensorflow/id4#encoder
 151. https://github.com/tensorflow/id4#decoder
 152. https://github.com/tensorflow/id4/blob/master/id4/utils/iterator_utils.py
 153. https://github.com/tensorflow/id4/blob/master/id4/model.py
 154. https://arxiv.org/abs/1703.01619
 155. https://github.com/tensorflow/id4#decoder
 156. https://github.com/tensorflow/id4/blob/master/id4/model.py
 157. https://arxiv.org/pdf/1609.08144.pdf
 158. https://github.com/tensorflow/id4/blob/master/id4/gid4_model.py
 159. https://github.com/tensorflow/id4/blob/master/id4/scripts/download_iwslt15.sh
 160. http://download.tensorflow.org/models/id4/envi_model_1.zip
 161. http://download.tensorflow.org/models/id4/envi_model_2.zip
 162. http://www.aclweb.org/anthology/p02-1040.pdf
 163. https://nlp.stanford.edu/pubs/luong-manning-iwslt15.pdf
 164. https://github.com/tensorflow/id4/blob/master/id4/scripts/wmt16_en_de.sh
 165. https://github.com/rsennrich/subword-id4
 166. http://download.tensorflow.org/models/id4/deen_model_1.zip
 167. http://download.tensorflow.org/models/id4/deen_model_2.zip
 168. http://download.tensorflow.org/models/id4/10122017/deen_gid4_model_4_layer.zip
 169. http://matrix.statmt.org/
 170. http://download.tensorflow.org/models/id4/10122017/ende_gid4_model_4_layer.zip
 171. http://download.tensorflow.org/models/id4/10122017/ende_gid4_model_8_layer.zip
 172. http://matrix.statmt.org/
 173. https://arxiv.org/abs/1701.02810
 174. https://arxiv.org/abs/1703.03906
 175. https://research.google.com/pubs/pub45610.html
 176. https://arxiv.org/abs/1706.03762
 177. https://github.com/tensorflow/id4/blob/master/id4/standard_hparams
 178. https://sites.google.com/site/acl16id4/
 179. https://github.com/lmthang/thesis
 180. https://arxiv.org/abs/1703.01619
 181. https://nlp.stanford.edu/projects/id4/
 182. https://github.com/google/id195
 183. https://github.com/rsennrich/nematus
 184. http://openid4.net/
 185. https://github.com/openid4/openid4-py
 186. https://arxiv.org/pdf/1409.0473.pdf
 187. https://arxiv.org/pdf/1508.04025.pdf
 188. https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf
 189. https://github.com/site/terms
 190. https://github.com/site/privacy
 191. https://github.com/security
 192. https://githubstatus.com/
 193. https://help.github.com/
 194. https://github.com/contact
 195. https://github.com/pricing
 196. https://developer.github.com/
 197. https://training.github.com/
 198. https://github.blog/
 199. https://github.com/about
 200. https://github.com/tensorflow/id4
 201. https://github.com/tensorflow/id4

   hidden links:
 203. https://github.com/
 204. https://github.com/tensorflow/id4
 205. https://github.com/tensorflow/id4
 206. https://github.com/tensorflow/id4
 207. https://help.github.com/articles/which-remote-url-should-i-use
 208. https://github.com/tensorflow/id4#neural-machine-translation-id195-tutorial
 209. https://github.com/tensorflow/id4#introduction
 210. https://github.com/tensorflow/id4#basic
 211. https://github.com/tensorflow/id4#background-on-neural-machine-translation
 212. https://github.com/tensorflow/id4#installing-the-tutorial
 213. https://github.com/tensorflow/id4#training--how-to-build-our-first-id4-system
 214. https://github.com/tensorflow/id4#embedding
 215. https://github.com/tensorflow/id4#encoder
 216. https://github.com/tensorflow/id4#decoder
 217. https://github.com/tensorflow/id4#loss
 218. https://github.com/tensorflow/id4#gradient-computation--optimization
 219. https://github.com/tensorflow/id4#hands-on--lets-train-an-id4-model
 220. https://github.com/tensorflow/id4#id136--how-to-generate-translations
 221. https://github.com/tensorflow/id4#intermediate
 222. https://github.com/tensorflow/id4#background-on-the-attention-mechanism
 223. https://github.com/tensorflow/id4#attention-wrapper-api
 224. https://github.com/tensorflow/id4#hands-on--building-an-attention-based-id4-model
 225. https://github.com/tensorflow/id4#tips--tricks
 226. https://github.com/tensorflow/id4#building-training-eval-and-id136-graphs
 227. https://github.com/tensorflow/id4#data-input-pipeline
 228. https://github.com/tensorflow/id4#other-details-for-better-id4-models
 229. https://github.com/tensorflow/id4#bidirectional-id56s
 230. https://github.com/tensorflow/id4#beam-search
 231. https://github.com/tensorflow/id4#hyperparameters
 232. https://github.com/tensorflow/id4#multi-gpu-training
 233. https://github.com/tensorflow/id4#benchmarks
 234. https://github.com/tensorflow/id4#iwslt-english-vietnamese
 235. https://github.com/tensorflow/id4#wmt-german-english
 236. https://github.com/tensorflow/id4#wmt-english-german--full-comparison
 237. https://github.com/tensorflow/id4#standard-hparams
 238. https://github.com/tensorflow/id4#other-resources
 239. https://github.com/tensorflow/id4#acknowledgment
 240. https://github.com/tensorflow/id4#references
 241. https://github.com/tensorflow/id4#bibtex
 242. https://github.com/
