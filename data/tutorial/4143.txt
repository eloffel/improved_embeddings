   #[1]index [2]search [3]denoising autoencoders (da) [4]multilayer
   id88

navigation

     * [5]index
     * [6]next |
     * [7]previous |
     * [8]deeplearning 0.1 documentation   

[9]table of contents

     * [10]convolutional neural networks (lenet)
          + [11]motivation
          + [12]sparse connectivity
          + [13]shared weights
          + [14]details and notation
          + [15]the convolution operator
          + [16]maxpooling
          + [17]the full model: lenet
          + [18]putting it all together
          + [19]running the code
          + [20]tips and tricks
               o [21]choosing hyperparameters
                    # [22]number of filters
                    # [23]filter shape
                    # [24]max pooling shape
                    # [25]tips

previous topic

   [26]multilayer id88

next topic

   [27]denoising autoencoders (da)

this page

     * [28]show source

quick search

   ____________________
   go

convolutional neural networks (lenet)[29]  

   note

   this section assumes the reader has already read through
   [30]classifying mnist digits using id28 and
   [31]multilayer id88. additionally, it uses the following new
   theano functions and concepts: [32]t.tanh, [33]shared variables,
   [34]basic arithmetic ops, [35]t.grad, [36]floatx, [37]pool ,
   [38]conv2d, [39]dimshuffle. if you intend to run the code on gpu also
   read [40]gpu.

   to run this example on a gpu, you need a good gpu. it needs at least
   1gb of gpu ram. more may be required if your monitor is connected to
   the gpu.

   when the gpu is connected to the monitor, there is a limit of a few
   seconds for each gpu function call. this is needed as current gpus
   can   t be used for the monitor while doing computation. without this
   limit, the screen would freeze for too long and make it look as if the
   computer froze. this example hits this limit with medium-quality gpus.
   when the gpu isn   t connected to a monitor, there is no time limit. you
   can lower the batch size to fix the time out problem.

   note

   the code for this section is available for download [41]here and the
   [42]3wolfmoon image

motivation[43]  

   convolutional neural networks (id98) are biologically-inspired variants
   of mlps. from hubel and wiesel   s early work on the cat   s visual cortex
   [44][hubel68], we know the visual cortex contains a complex arrangement
   of cells. these cells are sensitive to small sub-regions of the visual
   field, called a receptive field. the sub-regions are tiled to cover the
   entire visual field. these cells act as local filters over the input
   space and are well-suited to exploit the strong spatially local
   correlation present in natural images.

   additionally, two basic cell types have been identified: simple cells
   respond maximally to specific edge-like patterns within their receptive
   field. complex cells have larger receptive fields and are locally
   invariant to the exact position of the pattern.

   the animal visual cortex being the most powerful visual processing
   system in existence, it seems natural to emulate its behavior. hence,
   many neurally-inspired models can be found in the literature. to name a
   few: the neocognitron [45][fukushima], hmax [46][serre07] and lenet-5
   [47][lecun98], which will be the focus of this tutorial.

sparse connectivity[48]  

   id98s exploit spatially-local correlation by enforcing a local
   connectivity pattern between neurons of adjacent layers. in other
   words, the inputs of hidden units in layer m are from a subset of units
   in layer m-1, units that have spatially contiguous receptive fields. we
   can illustrate this graphically as follows:
   _images/sparse_1d_nn.png

   imagine that layer m-1 is the input retina. in the above figure, units
   in layer m have receptive fields of width 3 in the input retina and are
   thus only connected to 3 adjacent neurons in the retina layer. units in
   layer m+1 have a similar connectivity with the layer below. we say that
   their receptive field with respect to the layer below is also 3, but
   their receptive field with respect to the input is larger (5). each
   unit is unresponsive to variations outside of its receptive field with
   respect to the retina. the architecture thus ensures that the learnt
      filters    produce the strongest response to a spatially local input
   pattern.

   however, as shown above, stacking many such layers leads to
   (non-linear)    filters    that become increasingly    global    (i.e.
   responsive to a larger region of pixel space). for example, the unit in
   hidden layer m+1 can encode a non-linear feature of width 5 (in terms
   of pixel space).

shared weights[49]  

   in addition, in id98s, each filter h_i is replicated across the entire
   visual field. these replicated units share the same parameterization
   (weight vector and bias) and form a feature map.
   _images/conv_1d_nn.png

   in the above figure, we show 3 hidden units belonging to the same
   feature map. weights of the same color are shared   constrained to be
   identical. id119 can still be used to learn such shared
   parameters, with only a small change to the original algorithm. the
   gradient of a shared weight is simply the sum of the gradients of the
   parameters being shared.

   replicating units in this way allows for features to be detected
   regardless of their position in the visual field. additionally, weight
   sharing increases learning efficiency by greatly reducing the number of
   free parameters being learnt. the constraints on the model enable id98s
   to achieve better generalization on vision problems.

details and notation[50]  

   a feature map is obtained by repeated application of a function across
   sub-regions of the entire image, in other words, by convolution of the
   input image with a linear filter, adding a bias term and then applying
   a non-linear function. if we denote the k-th feature map at a given
   layer as h^k , whose filters are determined by the weights w^k and bias
   b_k , then the feature map h^k is obtained as follows (for tanh
   non-linearities):

   h^k_{ij} = \tanh ( (w^k * x)_{ij} + b_k ).

   note

   recall the following definition of convolution for a 1d signal. o[n] =
   f[n]*g[n] = \sum_{u=-\infty}^{\infty} f[u] g[n-u] =
   \sum_{u=-\infty}^{\infty} f[n-u] g[u] .

   this can be extended to 2d as follows: o[m,n] = f[m,n]*g[m,n] =
   \sum_{u=-\infty}^{\infty} \sum_{v=-\infty}^{\infty} f[u,v] g[m-u,n-v] .

   to form a richer representation of the data, each hidden layer is
   composed of multiple feature maps, \{h^{(k)}, k=0..k\} . the weights w
   of a hidden layer can be represented in a 4d tensor containing elements
   for every combination of destination feature map, source feature map,
   source vertical position, and source horizontal position. the biases b
   can be represented as a vector containing one element for every
   destination feature map. we illustrate this graphically as follows:
   _images/id98_explained.png

   figure 1: example of a convolutional layer

   the figure shows two layers of a id98. layer m-1 contains four feature
   maps. hidden layer m contains two feature maps ( h^0 and h^1 ). pixels
   (neuron outputs) in h^0 and h^1 (outlined as blue and red squares) are
   computed from pixels of layer (m-1) which fall within their 2x2
   receptive field in the layer below (shown as colored rectangles).
   notice how the receptive field spans all four input feature maps. the
   weights w^0 and w^1 of h^0 and h^1 are thus 3d weight tensors. the
   leading dimension indexes the input feature maps, while the other two
   refer to the pixel coordinates.

   putting it all together, w^{kl}_{ij} denotes the weight connecting each
   pixel of the k-th feature map at layer m, with the pixel at coordinates
   (i,j) of the l-th feature map of layer (m-1).

the convolution operator[51]  

   convop is the main workhorse for implementing a convolutional layer in
   theano. convop is used by theano.tensor.signal.conv2d, which takes two
   symbolic inputs:
     * a 4d tensor corresponding to a mini-batch of input images. the
       shape of the tensor is as follows: [mini-batch size, number of
       input feature maps, image height, image width].
     * a 4d tensor corresponding to the weight matrix w . the shape of the
       tensor is: [number of feature maps at layer m, number of feature
       maps at layer m-1, filter height, filter width]

   below is the theano code for implementing a convolutional layer similar
   to the one of figure 1. the input consists of 3 features maps (an rgb
   color image) of size 120x160. we use two convolutional filters with 9x9
   receptive fields.
import theano
from theano import tensor as t
from theano.tensor.nnet import conv2d

import numpy

rng = numpy.random.randomstate(23455)

# instantiate 4d tensor for input
input = t.tensor4(name='input')

# initialize shared variable for weights.
w_shp = (2, 3, 9, 9)
w_bound = numpy.sqrt(3 * 9 * 9)
w = theano.shared( numpy.asarray(
            rng.uniform(
                low=-1.0 / w_bound,
                high=1.0 / w_bound,
                size=w_shp),
            dtype=input.dtype), name ='w')

# initialize shared variable for bias (1d tensor) with random values
# important: biases are usually initialized to zero. however in this
# particular application, we simply apply the convolutional layer to
# an image without learning the parameters. we therefore initialize
# them to random values to "simulate" learning.
b_shp = (2,)
b = theano.shared(numpy.asarray(
            rng.uniform(low=-.5, high=.5, size=b_shp),
            dtype=input.dtype), name ='b')

# build symbolic expression that computes the convolution of input with filters
in w
conv_out = conv2d(input, w)

# build symbolic expression to add bias and apply activation function, i.e. prod
uce neural net layer output
# a few words on ``dimshuffle`` :
#   ``dimshuffle`` is a powerful tool in reshaping a tensor;
#   what it allows you to do is to shuffle dimension around
#   but also to insert new ones along which the tensor will be
#   broadcastable;
#   dimshuffle('x', 2, 'x', 0, 1)
#   this will work on 3d tensors with no broadcastable
#   dimensions. the first dimension will be broadcastable,
#   then we will have the third dimension of the input tensor as
#   the second of the resulting tensor, etc. if the tensor has
#   shape (20, 30, 40), the resulting tensor will have dimensions
#   (1, 40, 1, 20, 30). (axbxc tensor is mapped to 1xcx1xaxb tensor)
#   more examples:
#    dimshuffle('x') -> make a 0d (scalar) into a 1d vector
#    dimshuffle(0, 1) -> identity
#    dimshuffle(1, 0) -> inverts the first and second dimensions
#    dimshuffle('x', 0) -> make a row out of a 1d vector (n to 1xn)
#    dimshuffle(0, 'x') -> make a column out of a 1d vector (n to nx1)
#    dimshuffle(2, 0, 1) -> axbxc to cxaxb
#    dimshuffle(0, 'x', 1) -> axb to ax1xb
#    dimshuffle(1, 'x', 0) -> axb to bx1xa
output = t.nnet.sigmoid(conv_out + b.dimshuffle('x', 0, 'x', 'x'))

# create theano function to compute filtered images
f = theano.function([input], output)

   let   s have a little bit of fun with this...
import numpy
import pylab
from pil import image

# open random image of dimensions 639x516
img = image.open(open('doc/images/3wolfmoon.jpg'))
# dimensions are (height, width, channel)
img = numpy.asarray(img, dtype='float64') / 256.

# put image in 4d tensor of shape (1, 3, height, width)
img_ = img.transpose(2, 0, 1).reshape(1, 3, 639, 516)
filtered_img = f(img_)

# plot original image and first and second components of output
pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(img)
pylab.gray();
# recall that the convop output (filtered image) is actually a "minibatch",
# of size 1 here, so we take index 0 in the first dimension:
pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(filtered_img[0, 0, :, :]
)
pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(filtered_img[0, 1, :, :]
)
pylab.show()

   this should generate the following output.
   _images/3wolfmoon_output.png

   notice that a randomly initialized filter acts very much like an edge
   detector!

   note that we use the same weight initialization formula as with the
   mlp. weights are sampled randomly from a uniform distribution in the
   range [-1/fan-in, 1/fan-in], where fan-in is the number of inputs to a
   hidden unit. for mlps, this was the number of units in the layer below.
   for id98s however, we have to take into account the number of input
   feature maps and the size of the receptive fields.

maxpooling[52]  

   another important concept of id98s is max-pooling, which is a form of
   non-linear down-sampling. max-pooling partitions the input image into a
   set of non-overlapping rectangles and, for each such sub-region,
   outputs the maximum value.

   max-pooling is useful in vision for two reasons:

         1. by eliminating non-maximal values, it reduces computation for
            upper layers.
         2. it provides a form of translation invariance. imagine
            cascading a max-pooling layer with a convolutional layer.
            there are 8 directions in which one can translate the input
            image by a single pixel. if max-pooling is done over a 2x2
            region, 3 out of these 8 possible configurations will produce
            exactly the same output at the convolutional layer. for
            max-pooling over a 3x3 window, this jumps to 5/8.
            since it provides additional robustness to position,
            max-pooling is a    smart    way of reducing the dimensionality of
            intermediate representations.

   max-pooling is done in theano by way of
   theano.tensor.signal.pool.pool_2d. this function takes as input an n
   dimensional tensor (where n >= 2) and a downscaling factor and performs
   max-pooling over the 2 trailing dimensions of the tensor.

   an example is worth a thousand words:
from theano.tensor.signal import pool

input = t.dtensor4('input')
maxpool_shape = (2, 2)
pool_out = pool.pool_2d(input, maxpool_shape, ignore_border=true)
f = theano.function([input],pool_out)

invals = numpy.random.randomstate(1).rand(3, 2, 5, 5)
print 'with ignore_border set to true:'
print 'invals[0, 0, :, :] =\n', invals[0, 0, :, :]
print 'output[0, 0, :, :] =\n', f(invals)[0, 0, :, :]

pool_out = pool.pool_2d(input, maxpool_shape, ignore_border=false)
f = theano.function([input],pool_out)
print 'with ignore_border set to false:'
print 'invals[1, 0, :, :] =\n ', invals[1, 0, :, :]
print 'output[1, 0, :, :] =\n ', f(invals)[1, 0, :, :]

   this should generate the following output:
with ignore_border set to true:
    invals[0, 0, :, :] =
    [[  4.17022005e-01   7.20324493e-01   1.14374817e-04   3.02332573e-01 1.4675
5891e-01]
     [  9.23385948e-02   1.86260211e-01   3.45560727e-01   3.96767474e-01 5.3881
6734e-01]
     [  4.19194514e-01   6.85219500e-01   2.04452250e-01   8.78117436e-01 2.7387
5932e-02]
     [  6.70467510e-01   4.17304802e-01   5.58689828e-01   1.40386939e-01 1.9810
1489e-01]
     [  8.00744569e-01   9.68261576e-01   3.13424178e-01   6.92322616e-01 8.7638
9152e-01]]
    output[0, 0, :, :] =
    [[ 0.72032449  0.39676747]
     [ 0.6852195   0.87811744]]

with ignore_border set to false:
    invals[1, 0, :, :] =
    [[ 0.01936696  0.67883553  0.21162812  0.26554666  0.49157316]
     [ 0.05336255  0.57411761  0.14672857  0.58930554  0.69975836]
     [ 0.10233443  0.41405599  0.69440016  0.41417927  0.04995346]
     [ 0.53589641  0.66379465  0.51488911  0.94459476  0.58655504]
     [ 0.90340192  0.1374747   0.13927635  0.80739129  0.39767684]]
    output[1, 0, :, :] =
    [[ 0.67883553  0.58930554  0.69975836]
     [ 0.66379465  0.94459476  0.58655504]
     [ 0.90340192  0.80739129  0.39767684]]

   note that compared to most theano code, the max_pool_2d operation is a
   little special. it requires the downscaling factor ds (tuple of length
   2 containing downscaling factors for image width and height) to be
   known at graph build time. this may change in the near future.

the full model: lenet[53]  

   sparse, convolutional layers and max-pooling are at the heart of the
   lenet family of models. while the exact details of the model will vary
   greatly, the figure below shows a graphical depiction of a lenet model.
   _images/mylenet.png

   the lower-layers are composed to alternating convolution and
   max-pooling layers. the upper-layers however are fully-connected and
   correspond to a traditional mlp (hidden layer + id28).
   the input to the first fully-connected layer is the set of all features
   maps at the layer below.

   from an implementation point of view, this means lower-layers operate
   on 4d tensors. these are then flattened to a 2d matrix of rasterized
   feature maps, to be compatible with our previous mlp implementation.

   note

   note that the term    convolution    could corresponds to different
   mathematical operations:
    1. [54]theano.tensor.nnet.conv2d, which is the most common one in
       almost all of the recent published convolutional models. in this
       operation, each output feature map is connected to each input
       feature map by a different 2d filter, and its value is the sum of
       the individual convolution of all inputs through the corresponding
       filter.
    2. the convolution used in the original lenet model: in this work,
       each output feature map is only connected to a subset of input
       feature maps.
    3. the convolution used in signal processing:
       [55]theano.tensor.signal.conv.conv2d, which works only on single
       channel inputs.

   here, we use the first operation, so this models differ slightly from
   the original lenet paper. one reason to use 2. would be to reduce the
   amount of computation needed, but modern hardware makes it as fast to
   have the full connection pattern. another reason would be to slightly
   reduce the number of free parameters, but we have other id173
   techniques at our disposal.

putting it all together[56]  

   we now have all we need to implement a lenet model in theano. we start
   with the lenetconvpoollayer class, which implements a {convolution +
   max-pooling} layer.
class lenetconvpoollayer(object):
    """pool layer of a convolutional network """

    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):
        """
        allocate a lenetconvpoollayer with shared variable internal parameters.

        :type rng: numpy.random.randomstate
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.dtensor4
        :param input: symbolic image tensor, of shape image_shape

        :type filter_shape: tuple or list of length 4
        :param filter_shape: (number of filters, num input feature maps,
                              filter height, filter width)

        :type image_shape: tuple or list of length 4
        :param image_shape: (batch size, num input feature maps,
                             image height, image width)

        :type poolsize: tuple or list of length 2
        :param poolsize: the downsampling (pooling) factor (#rows, #cols)
        """

        assert image_shape[1] == filter_shape[1]
        self.input = input

        # there are "num input feature maps * filter height * filter width"
        # inputs to each hidden unit
        fan_in = numpy.prod(filter_shape[1:])
        # each unit in the lower layer receives a gradient from:
        # "num output feature maps * filter height * filter width" /
        #   pooling size
        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) //
                   numpy.prod(poolsize))
        # initialize weights with random weights
        w_bound = numpy.sqrt(6. / (fan_in + fan_out))
        self.w = theano.shared(
            numpy.asarray(
                rng.uniform(low=-w_bound, high=w_bound, size=filter_shape),
                dtype=theano.config.floatx
            ),
            borrow=true
        )

        # the bias is a 1d tensor -- one bias per output feature map
        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatx)
        self.b = theano.shared(value=b_values, borrow=true)

        # convolve input feature maps with filters
        conv_out = conv2d(
            input=input,
            filters=self.w,
            filter_shape=filter_shape,
            input_shape=image_shape
        )

        # pool each feature map individually, using maxpooling
        pooled_out = pool.pool_2d(
            input=conv_out,
            ds=poolsize,
            ignore_border=true
        )

        # add the bias term. since the bias is a vector (1d array), we first
        # reshape it to a tensor of shape (1, n_filters, 1, 1). each bias will
        # thus be broadcasted across mini-batches and feature map
        # width & height
        self.output = t.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))

        # store parameters of this layer
        self.params = [self.w, self.b]

        # keep track of model input
        self.input = input

   notice that when initializing the weight values, the fan-in is
   determined by the size of the receptive fields and the number of input
   feature maps.

   finally, using the logisticregression class defined in [57]classifying
   mnist digits using id28 and the hiddenlayer class
   defined in [58]multilayer id88 , we can instantiate the network
   as follows.
    x = t.matrix('x')   # the data is presented as rasterized images
    y = t.ivector('y')  # the labels are presented as 1d vector of
                        # [int] labels

    ######################
    # build actual model #
    ######################
    print('... building the model')

    # reshape matrix of rasterized images of shape (batch_size, 28 * 28)
    # to a 4d tensor, compatible with our lenetconvpoollayer
    # (28, 28) is the size of mnist images.
    layer0_input = x.reshape((batch_size, 1, 28, 28))

    # construct the first convolutional pooling layer:
    # filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)
    # maxpooling reduces this further to (24/2, 24/2) = (12, 12)
    # 4d output tensor is thus of shape (batch_size, nkerns[0], 12, 12)
    layer0 = lenetconvpoollayer(
        rng,
        input=layer0_input,
        image_shape=(batch_size, 1, 28, 28),
        filter_shape=(nkerns[0], 1, 5, 5),
        poolsize=(2, 2)
    )

    # construct the second convolutional pooling layer
    # filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)
    # maxpooling reduces this further to (8/2, 8/2) = (4, 4)
    # 4d output tensor is thus of shape (batch_size, nkerns[1], 4, 4)
    layer1 = lenetconvpoollayer(
        rng,
        input=layer0.output,
        image_shape=(batch_size, nkerns[0], 12, 12),
        filter_shape=(nkerns[1], nkerns[0], 5, 5),
        poolsize=(2, 2)
    )

    # the hiddenlayer being fully-connected, it operates on 2d matrices of
    # shape (batch_size, num_pixels) (i.e matrix of rasterized images).
    # this will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),
    # or (500, 50 * 4 * 4) = (500, 800) with the default values.
    layer2_input = layer1.output.flatten(2)

    # construct a fully-connected sigmoidal layer
    layer2 = hiddenlayer(
        rng,
        input=layer2_input,
        n_in=nkerns[1] * 4 * 4,
        n_out=500,
        activation=t.tanh
    )

    # classify the values of the fully-connected sigmoidal layer
    layer3 = logisticregression(input=layer2.output, n_in=500, n_out=10)

    # the cost we minimize during training is the nll of the model
    cost = layer3.negative_log_likelihood(y)

    # create a function to compute the mistakes that are made by the model
    test_model = theano.function(
        [index],
        layer3.errors(y),
        givens={
            x: test_set_x[index * batch_size: (index + 1) * batch_size],
            y: test_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )

    validate_model = theano.function(
        [index],
        layer3.errors(y),
        givens={
            x: valid_set_x[index * batch_size: (index + 1) * batch_size],
            y: valid_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )

    # create a list of all model parameters to be fit by id119
    params = layer3.params + layer2.params + layer1.params + layer0.params

    # create a list of gradients for all model parameters
    grads = t.grad(cost, params)

    # train_model is a function that updates the model parameters by
    # sgd since this model has many parameters, it would be tedious to
    # manually create an update rule for each model parameter. we thus
    # create the updates list by automatically looping over all
    # (params[i], grads[i]) pairs.
    updates = [
        (param_i, param_i - learning_rate * grad_i)
        for param_i, grad_i in zip(params, grads)
    ]

    train_model = theano.function(
        [index],
        cost,
        updates=updates,
        givens={
            x: train_set_x[index * batch_size: (index + 1) * batch_size],
            y: train_set_y[index * batch_size: (index + 1) * batch_size]
        }
    )

   we leave out the code that performs the actual training and
   early-stopping, since it is exactly the same as with an mlp. the
   interested reader can nevertheless access the code in the    code    folder
   of deeplearningtutorials.

running the code[59]  

   the user can then run the code by calling:
python code/convolutional_mlp.py

   the following output was obtained with the default parameters on a core
   i7-2600k cpu clocked at 3.40ghz and using flags    floatx=float32   :
optimization complete.
best validation score of 0.910000 % obtained at iteration 17800,with test
performance 0.920000 %
the code for file convolutional_mlp.py ran for 380.28m

   using a geforce gtx 285, we obtained the following:
optimization complete.
best validation score of 0.910000 % obtained at iteration 15500,with test
performance 0.930000 %
the code for file convolutional_mlp.py ran for 46.76m

   and similarly on a geforce gtx 480:
optimization complete.
best validation score of 0.910000 % obtained at iteration 16400,with test
performance 0.930000 %
the code for file convolutional_mlp.py ran for 32.52m

   note that the discrepancies in validation and test error (as well as
   iteration count) are due to different implementations of the rounding
   mechanism in hardware. they can be safely ignored.

tips and tricks[60]  

choosing hyperparameters[61]  

   id98s are especially tricky to train, as they add even more
   hyper-parameters than a standard mlp. while the usual rules of thumb
   for learning rates and id173 constants still apply, the
   following should be kept in mind when optimizing id98s.

number of filters[62]  

   when choosing the number of filters per layer, keep in mind that
   computing the activations of a single convolutional filter is much more
   expensive than with traditional mlps !

   assume layer (l-1) contains k^{l-1} feature maps and m \times n pixel
   positions (i.e., number of positions times number of feature maps), and
   there are k^l filters at layer l of shape m \times n . then computing a
   feature map (applying an m \times n filter at all (m-m) \times (n-n)
   pixel positions where the filter can be applied) costs (m-m) \times
   (n-n) \times m \times n \times k^{l-1} . the total cost is k^l times
   that. things may be more complicated if not all features at one level
   are connected to all features at the previous one.

   for a standard mlp, the cost would only be k^l \times k^{l-1} where
   there are k^l different neurons at level l . as such, the number of
   filters used in id98s is typically much smaller than the number of
   hidden units in mlps and depends on the size of the feature maps
   (itself a function of input image size and filter shapes).

   since feature map size decreases with depth, layers near the input
   layer will tend to have fewer filters while layers higher up can have
   much more. in fact, to equalize computation at each layer, the product
   of the number of features and the number of pixel positions is
   typically picked to be roughly constant across layers. to preserve the
   information about the input would require keeping the total number of
   activations (number of feature maps times number of pixel positions) to
   be non-decreasing from one layer to the next (of course we could hope
   to get away with less when we are doing supervised learning). the
   number of feature maps directly controls capacity and so that depends
   on the number of available examples and the complexity of the task.

filter shape[63]  

   common filter shapes found in the literature vary greatly, usually
   based on the dataset. best results on mnist-sized images (28x28) are
   usually in the 5x5 range on the first layer, while natural image
   datasets (often with hundreds of pixels in each dimension) tend to use
   larger first-layer filters of shape 12x12 or 15x15.

   the trick is thus to find the right level of    granularity    (i.e. filter
   shapes) in order to create abstractions at the proper scale, given a
   particular dataset.

max pooling shape[64]  

   typical values are 2x2 or no max-pooling. very large input images may
   warrant 4x4 pooling in the lower-layers. keep in mind however, that
   this will reduce the dimension of the signal by a factor of 16, and may
   result in throwing away too much information.

   footnotes
   [1] for clarity, we use the word    unit    or    neuron    to refer to the
   artificial neuron and    cell    to refer to the biological neuron.

tips[65]  

   if you want to try this model on a new dataset, here are a few tips
   that can help you get better results:

     * whitening the data (e.g. with pca)
     * decay the learning rate in each epoch

navigation

     * [66]index
     * [67]next |
     * [68]previous |
     * [69]deeplearning 0.1 documentation   

      copyright 2008--2010, lisa lab. last updated on jun 15, 2018. created
   using [70]sphinx 1.5.

references

   1. http://deeplearning.net/tutorial/genindex.html
   2. http://deeplearning.net/tutorial/search.html
   3. http://deeplearning.net/tutorial/da.html
   4. http://deeplearning.net/tutorial/mlp.html
   5. http://deeplearning.net/tutorial/genindex.html
   6. http://deeplearning.net/tutorial/da.html
   7. http://deeplearning.net/tutorial/mlp.html
   8. http://deeplearning.net/tutorial/contents.html
   9. http://deeplearning.net/tutorial/contents.html
  10. http://deeplearning.net/tutorial/lenet.html
  11. http://deeplearning.net/tutorial/lenet.html#motivation
  12. http://deeplearning.net/tutorial/lenet.html#sparse-connectivity
  13. http://deeplearning.net/tutorial/lenet.html#shared-weights
  14. http://deeplearning.net/tutorial/lenet.html#details-and-notation
  15. http://deeplearning.net/tutorial/lenet.html#the-convolution-operator
  16. http://deeplearning.net/tutorial/lenet.html#maxpooling
  17. http://deeplearning.net/tutorial/lenet.html#the-full-model-lenet
  18. http://deeplearning.net/tutorial/lenet.html#putting-it-all-together
  19. http://deeplearning.net/tutorial/lenet.html#running-the-code
  20. http://deeplearning.net/tutorial/lenet.html#tips-and-tricks
  21. http://deeplearning.net/tutorial/lenet.html#choosing-hyperparameters
  22. http://deeplearning.net/tutorial/lenet.html#number-of-filters
  23. http://deeplearning.net/tutorial/lenet.html#filter-shape
  24. http://deeplearning.net/tutorial/lenet.html#max-pooling-shape
  25. http://deeplearning.net/tutorial/lenet.html#tips
  26. http://deeplearning.net/tutorial/mlp.html
  27. http://deeplearning.net/tutorial/da.html
  28. http://deeplearning.net/tutorial/_sources/lenet.txt
  29. http://deeplearning.net/tutorial/lenet.html#convolutional-neural-networks-lenet
  30. http://deeplearning.net/tutorial/logreg.html
  31. http://deeplearning.net/tutorial/mlp.html
  32. http://deeplearning.net/software/theano/tutorial/examples.html?highlight=tanh
  33. http://deeplearning.net/software/theano/tutorial/examples.html#using-shared-variables
  34. http://deeplearning.net/software/theano/tutorial/adding.html#adding-two-scalars
  35. http://deeplearning.net/software/theano/tutorial/examples.html#computing-gradients
  36. http://deeplearning.net/software/theano/library/config.html#config.floatx
  37. http://deeplearning.net/software/theano/library/tensor/signal/pool.html
  38. http://deeplearning.net/software/theano/library/tensor/signal/conv.html#module-conv
  39. http://deeplearning.net/software/theano/library/tensor/basic.html#tensor._tensor_py_operators.dimshuffle
  40. http://deeplearning.net/software/theano/tutorial/using_gpu.html
  41. http://deeplearning.net/tutorial/code/convolutional_mlp.py
  42. https://raw.githubusercontent.com/lisa-lab/deeplearningtutorials/master/doc/images/3wolfmoon.jpg
  43. http://deeplearning.net/tutorial/lenet.html#motivation
  44. http://deeplearning.net/tutorial/references.html#hubel68
  45. http://deeplearning.net/tutorial/references.html#fukushima
  46. http://deeplearning.net/tutorial/references.html#serre07
  47. http://deeplearning.net/tutorial/references.html#lecun98
  48. http://deeplearning.net/tutorial/lenet.html#sparse-connectivity
  49. http://deeplearning.net/tutorial/lenet.html#shared-weights
  50. http://deeplearning.net/tutorial/lenet.html#details-and-notation
  51. http://deeplearning.net/tutorial/lenet.html#the-convolution-operator
  52. http://deeplearning.net/tutorial/lenet.html#maxpooling
  53. http://deeplearning.net/tutorial/lenet.html#the-full-model-lenet
  54. http://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv2d
  55. http://deeplearning.net/software/theano/library/tensor/signal/conv.html#theano.tensor.signal.conv.conv2d
  56. http://deeplearning.net/tutorial/lenet.html#putting-it-all-together
  57. http://deeplearning.net/tutorial/logreg.html
  58. http://deeplearning.net/tutorial/mlp.html
  59. http://deeplearning.net/tutorial/lenet.html#running-the-code
  60. http://deeplearning.net/tutorial/lenet.html#tips-and-tricks
  61. http://deeplearning.net/tutorial/lenet.html#choosing-hyperparameters
  62. http://deeplearning.net/tutorial/lenet.html#number-of-filters
  63. http://deeplearning.net/tutorial/lenet.html#filter-shape
  64. http://deeplearning.net/tutorial/lenet.html#max-pooling-shape
  65. http://deeplearning.net/tutorial/lenet.html#tips
  66. http://deeplearning.net/tutorial/genindex.html
  67. http://deeplearning.net/tutorial/da.html
  68. http://deeplearning.net/tutorial/mlp.html
  69. http://deeplearning.net/tutorial/contents.html
  70. http://sphinx-doc.org/
