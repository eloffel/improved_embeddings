   #[1]aifounded    feed [2]aifounded    comments feed [3]aifounded    recent
   evolution of qa datasets and going forward comments feed [4]exploring
   deep neural network loss surfaces [5]alternate [6]alternate

   [7]logo logo logo logo logo

     * [8]home
     * [9]search
     * [10]about us
     * [11]research
     * [12]blog
          + [13]aifounded
          + [14]big data
          + [15]machine learning

     * [16]home
     * [17]search
     * [18]about us
     * [19]research
     * [20]blog
          + [21]aifounded
          + [22]big data
          + [23]machine learning

   aifounded | recent evolution of qa datasets and going forward
   1192
   post-template-default,single,single-post,postid-1192,single-format-stan
   dard,ajax_fade,page_not_loaded,,qode-title-hidden,qode_grid_1300,qode-t
   heme-ver-10.1.1,wpb-js-composer js-comp-ver-5.0.1,vc_responsive

02 jul recent evolution of qa datasets and going forward

   posted at 02:41h in [24]aifounded by [25]jiwoong im

   even from the early 1960s, scientists have worked on using computers to
   answer textual questions     yet it is still in progress. in 2011, ibm   s
   watson     a [26]id53 (qa) computer system capable of
   answering questions posed in [27]natural language     entered the quiz
   show jeopardy!, and defeated two former champions. nevertheless,
   scientists were unsatisfied with the 2011 ibm   s watson technology,
   mainly due to the absence of learning. today, they are much more
   ambitious about solving qa tasks through the process of learning.


   the topic of a general qa in the open domain has always existed, but it
   has been gaining enormous interest as of late. this is easily seen by
   examining the past and current qa datasets. it is astonishing that more
   than ten qa datasets were released since 2015 by various academic
   groups. in this post, i will summarize and expound over the evolution
   of recent qa datasets with some bonus in the end.



   broadly speaking, we have learned that there are two paradigms that can
   be used in answering questions. one paradigm is knowledge-based qa and
   the other is information retrieval (ir)-based qa. these two paradigms
   are quite self-explanatory. interestingly, by observing the evolution
   of qa datasets, one can recognize the trend moving  from
   knowledge-based qa to ir-based qa over time.


   simpleqa [1] contains 100k qa pairs and it relies on freebase as a
   knowledge database to inquiry for answering questions. there are other
   several datasets based on knoweldge-base (kb) proposed by [2,3,4,18]
   and yet, the complexity and generalizability of the question and
   answers are extremely limited.


   some of the qa datasets are formed for the purpose of reading
   comprehension research. researchers were interested in teaching
   machines to comprehend the contents of documents. the obvious metric is
   to use different forms of questions and then measure the corresponding
   accuracy of the answers. for example, the children   s book test & id98 /
   daily news articles & webqa [5,6,7] tasks are in the form of
   fill-in-the-blank. mctest & science [8,9] are the multiple choice type
   of qa datasets. the drawback of these datasets is that the questions
   are not in the natural form that people would ask in the real world.


   squad [10] is another reading comprehension dataset that requires
   answering the question from given wikipedia passages (i.e. the dataset
   comes with question pair and a document (q,a,d)_i for n data points).
   this dataset consists of 100k question-answer pairs with 536 wikipedia
   articles in total. the questions are posed by crowdworkers. most of the
   questions are fact based questions and the answers span a few words
   (id203 average about 1.x tokens) and you can always find the
   answers from the corresponding passage. you can see the
   state-of-the-art scores from their [28]leaderboard.


   newsqa [11] is similar to squad. it was created by maluuba and it uses
   120k question and answer pairs based on the new articles gathered by
   deepmind. these questions are also crowdsourced for the purpose of
   reading comprehension tasks. one catch is that the questions are
   generated based on looking at the summary points of the documents
   rather than looking at the full document.


   ms marco [12] ms marco contains 100,000 questions and 1m passages
   (512471 unique urls). each question contains top 10 contextual passages
   extracted from public web documents. their questions are derived from
   search logs and the answers are generated by crowdworkers rather than
   the span of passage. note that these passages are not from search
   engine results. interestingly, one of the selling points in their paper
   was to release 1m queries (questions) and the corresponding answers in
   the future. you can see the state-of-the-art scores from
   their [29]leaderboard.


   the common factor between these reading comprehension datasets is that
   some of the answers must be inferred from incomplete information in the
   articles. interestingly, the performance has improved a lot since,
      machine comprehension using match-lstm and answer pointer    [19]
   introduced spanning mechanism (answer pointer). many other models
   adapted the spanning mechanism to their models. it is still
   questionable whether spanning mechanism is the correct way to go,
   especially as the complexity of the question increases.
   additional performances were also gained from using both character and
   word level embeddings rather than just word level embeddings. anyhow,
   the performance of these datasets have rapidly improved (see figures
   below). thus, the difficulty level of the qa tasks have increased, by
   transferring real world data from reading comprehension tasks to the
   open domain qa.



   open domain id53

   the common factor of the next two datasets is that the question-answer
   pairs come with search engine snippets. also, they were released only a
   few month ago.

   searchqa [13] rather than generating questions through id104,
   searchqa questions are retrieved from the quiz show, jeopardy!.
   searchqa has 140k question-answer pairs with 49.6 snippets of google
   search (on average per question). the answers are about 1.47 tokens
   long. they also separate the dataset in a way that the training
   question-answer pairs are from the years before the validation and test
   pairs.


   triviaqa [14] is the newest qa dataset that has been released. the
   questions are special in a sense that they are taken from trivia games,
   and the answers are generated from trivia enthusiasts. there are 95k
   question-answer pairs. one of their selling points is that the
   questions are naturally generated. this avoids trivial regularity in
   the questions and    potential bias in question style or content   

   all of these qa frameworks are in the closed-world environment, even if
   the documents in the dataset were gathered from the open-world
   environment. in an ideal scenario, we want to solve any of these qa
   pairs (from above datasets) in ir fashion under the open-world
   environment. some interesting questions to ask are: (1) how do we
   measure which dataset is better or worse?; (2) which dataset contains
   realistic/practical questions?; (3) are they sufficient for applying to
   a real search engine?



   this is how i foresee the future:

   previously, we tried to capture complex patterns (information) from
   datasets through training deep neural networks or some other ml models.
   nevertheless, it is hard to build a dataset that embraces large amounts
   of information about the world in practice, and storing these large
   amounts of information into the neural network is yet another
   challenge. later, researchers implanted memory modules into the deep
   neural networks. some of the earlier works of this kind are neural
   turing machines [15], memory networks [16], and nklm [17], which gives
   the memory to the neural network so it can hold the information for a
   long time.


   the following is a simple analogy of why the above approach makes
   sense. say you are giving an exam under two different settings. in the
   first setting, the student will take the exam in the standard way. in
   the second setting, the student will get a cheat sheet where he can
   write things down. the first student will be required to understand the
   entire material over night, whereas the second student can write down a
   portion of the material to the cheat sheet. clearly, the second student
   has the advantage, since he does not have to expend as much effort to
   study for the exam than the first student. this advantage of the second
   student is analogous to the effect of directly implanting memory
   modules into the neural network.

   now, consider going one step further     it would be the ability to take
   all the materials into the exam, or having internet access during the
   exam.


   accordingly, the natural next step is to give search engine access to a
   neural network.  by leveraging the search engine, there are numerous
   things that can be improved. the obvious one would be the qa task.
   thus, we are talking not only about learning to understand the
   information in a dataset, but also learning to access the information
   from an external database. people sometimes describe neural networks as
   black box models. however, in this case, the search engine becomes the
   black box model for neural networks where the neural network can only
   observe the output based on its input. over time, the neural network
   will learn to make good use of this black box (the search engine).


   there have been some works that hint at this. for example,    reading
   wikipedia to answer open-domain questions    or    search engine guided
   non-parametric id4   . these papers mention
   leveraging a search engine to improve qa tasks or neural machine
   translation. unfortunately, they do not actually use a search engine.

   hopefully, by now, you are excited to integrate a search engine into
   your neural network. you may also be wondering which search engine you
   should use. if so, you should give the aifounded [30]search engine a
   try ([31]search.aifounded.com). it   s currently in alpha, but as we
   progress, we plan to customize it for ml and rl purposes. our search
   api is also very easy to integrate. our search engine covers a variety
   of global topics and also ensures that all passages in the qa datasets
   above are included. additionally, we are currently providing a free
   search engine api key to university research labs. if you are
   interested, please e-mail daniel.im@aifounded.com.



   best,


   daniel jiwoong im

   founder and ceo, aifounded



   acknowledgements : i am very thankful to seth lim, sungjin ahn, zhouhan
   lin, and carolyn augusta for great feedbacks.

   reference :

   [1] a. bordes, n. usunier, s. chopra, and j. weston. large-scale simple
   id53 with memory networks. arxiv:1506.02075 2015

   [2] a. fader, s. soderland, and o. etzioni. 2011. identifying relations
   for id10. emnlp 2011

   [3] j. berant, a. chou, r. frostig, and p. liang. 2013. semantic
   parsing on freebase from question-answer pairs. emnlp 2013

   [4] a. bordes, s. chopra, and j weston. 2014a. id53 with
   subgraph embeddings.  emnlp 2014

   [5] f. hill, a. bordes, s. chopra, and j. weston. 2015. the goldilocks
   principle: reading children   s books with explicit memory
   representations. arxiv:1511.02301 2015

   [6] k.m. hermann, t. kocisky, e. grefenstette, l. espeholt, w. kay,
   m. suleyman, and p. blunsom.  teaching machines to read and
   comprehend. nips 2015

   [7] p. li, w. li, z. he, x. wang, y. cao, jie zhou, and wei xu. dataset
   and neural recurrent sequence labeling model for open-domain factoid
   id53. arxiv:1607.06275 2016.

   [8] m. richardson, c. j.c. burges, and e. renshaw. mctest: a challenge
   dataset for the open-domain machine comprehension of text. emnlp 2013.

   [9] p. clark, o. etzioni. my computer is an honor student but how
   intelligent is it? standardized tests as a measure of ai. ai magazine
   2016

   [10] p. rajpurkar, j. zhang, k. lopyrev, and p. liang. squad: 100,000+
   questions for machine comprehension of text emnlp 2016

   [11] a. trischler, t. wang, x. yuan, j. harris, a. sordoni, p. bachman,
   and k. suleman. newsqa: a machine comprehension
   dataset. arxiv:1611.09830. 2016

   [12] t. nguyen, m. rosenberg, x. song, j. gao, s. tiwary, r. majumder,
   and l. deng. ms marco: a human generated id17
   dataset. nips workshop 2016

   [13] m. dunn, l. sagun, m. higgins, u. guney, v. cirik, and k. cho.
   searchqa: a new q&a dataset augmented with context from a search
   engine.arxiv:1704.05179 2017

   [14] m. joshi, e. choi, d.s. weld, and l. zettlemoyer. triviaqa: a
   large scale distantly supervised challenge dataset for reading
   comprehension.arxiv:1705.03551 2017

   [15] a. graves, g. wayne, i. danihelka. neural turing
   machines arxiv:1410.5401 2014

   [16] j. weston, s. chopra, a. bordes, memory networks arxiv:1410.3916
   2014

   [17] s. ahn, h. choi, t. parnamaa, y. bengio. a neural knowledge
   language model. arxiv:1608.00318 2016

   [18] j. v. serban, a. g. duran, c. gulcehre, s. ahn, s. chandar,
   a. courvill, y. bengio. generating factoid questions with recurrent
   neural networks: the 30m factoid question-answer
   corpus arxiv:1603.06807 2016

   [19] s. wang and j. jiang, machine comprehension using match-lstm and
   answer pointer arxiv:1608.07905 2016

   [20] c. buck, j. bulian, m. ciaramita, a.  gesmundo, n. houlsby, w.
   gajewski, ask the right questions: active question reformulation with
   id23, arxiv:1705.07830 2017




      copyright 2016 aifounded inc.,
   all rights reserved.

contact us

[32]contact us

references

   visible links
   1. http://www.aifounded.com/feed/
   2. http://www.aifounded.com/comments/feed/
   3. http://www.aifounded.com/aifounded/recent-evolution-of-the-qa-datasets-and-going-forward/feed/
   4. http://www.aifounded.com/machine-learning/deep-loss/
   5. http://www.aifounded.com/wp-json/oembed/1.0/embed?url=http://www.aifounded.com/aifounded/recent-evolution-of-the-qa-datasets-and-going-forward/
   6. http://www.aifounded.com/wp-json/oembed/1.0/embed?url=http://www.aifounded.com/aifounded/recent-evolution-of-the-qa-datasets-and-going-forward/&format=xml
   7. http://www.aifounded.com/
   8. http://www.aifounded.com/
   9. http://search.aifounded.com/
  10. http://www.aifounded.com/aboutus/
  11. http://www.aifounded.com/research/
  12. http://www.aifounded.com/blogs/
  13. http://www.aifounded.com/aifounded/
  14. http://www.aifounded.com/big-data/
  15. http://www.aifounded.com/machine-learning/
  16. http://www.aifounded.com/
  17. http://search.aifounded.com/
  18. http://www.aifounded.com/aboutus/
  19. http://www.aifounded.com/research/
  20. http://www.aifounded.com/blogs/
  21. http://www.aifounded.com/aifounded/
  22. http://www.aifounded.com/big-data/
  23. http://www.aifounded.com/machine-learning/
  24. http://www.aifounded.com/category/aifounded/
  25. http://www.aifounded.com/author/jiwoong_im/
  26. https://en.wikipedia.org/wiki/question_answering
  27. https://en.wikipedia.org/wiki/natural_language
  28. https://rajpurkar.github.io/squad-explorer/
  29. http://www.msmarco.org/leaders.aspx
  30. http://search.aifounded.com/
  31. http://search.aifounded.com/
  32. http://www.aifounded.com/contact-us/

   hidden links:
  34. http://www.aifounded.com/aifounded/recent-evolution-of-the-qa-datasets-and-going-forward/
  35. http://www.aifounded.com/wp-content/uploads/2017/07/qa-dataset-2015-2017.jpg
  36. http://www.aifounded.com/wp-content/uploads/2017/03/qa_squad2.png
  37. http://www.aifounded.com/wp-content/uploads/2017/03/qa_marco.png
  38. http://www.aifounded.com/wp-content/uploads/2017/03/qa_human_performance.png
  39. http://www.aifounded.com/wp-content/uploads/2017/03/logo_stylized.png
  40. https://www.facebook.com/aifounded
  41. https://twitter.com/aifounded
  42. https://www.linkedin.com/company/aifounded
