   #[1]eric jang - atom [2]eric jang - rss [3]eric jang - atom

[4]eric jang

   technology, a.i., careers

sunday, july 24, 2016

why randomness is important for deep learning

   this afternoon i attempted to explain to my mom why [5]randomness is
   important for [6]deep learning without using any jargon from
   id203, statistics, or machine learning.
   the exercise was partially successful. maybe. i still don't think she
   knows what deep learning is, besides that i am a big fan of it and use
   it for my job.

   [7][wsfffku0apsxwcr8n3ytzhc_5oauuhhu26j7ho2p5owvj0xyggoppv_8ptwpoxzbozl
   7mzxt9z3pl-exk6fe7npqe1wcgskauxjf5y1zcebcrtnwloaqgr5mldn752kujliyplbdf3
                                   8=s0-d]
                                 i'm a big fan of deep learning

   this post is a slightly more technical version of the explanation i
   gave my mom, with the hope that it will help deep learning
   practitioners to better understand what is going on in neural networks.
   if you are getting started into deep learning research, you may
   discover that there are a whole bunch of seemingly arbitrary techniques
   used to train neural networks, with very little theoretical
   justification besides "it works". for example: dropout id173,
   adding gradient noise,, asynchronous stochastic descent.
   what do all these hocus pocus techniques have in common? they
   incorporate randomness!
   [8][k2j69gomshya_kus2mkgvtedc8cvjbbh28i2mqxntyatci1aqrcxrnhndcoshabzht3
   gr1niudebia7fv7mecn0plm0hr4cmu4wyoid35ijgay7v2wf1oaof0twpoxhviqycl0nnpaq
   2psitgvf7pftxl=s0-d]
   random noise actually is crucial for getting dnns to work well:
    1. random noise allows neural nets to produce multiple outputs given
       the same instance of input.
    2. random noise limits the amount of information flowing through the
       network, forcing the network to learn meaningful representations of
       data.
    3. random noise provides "exploration energy" for finding better
       optimization solutions during id119.

single input, multiple output

   suppose you are training a deep neural network (dnn) to classify
   images.
   [9][id163_example.png]
   for each cropped region, the network learns to convert an image into a
   number representing a class label, such as    dog    or "person".
   that   s all very well and good, and these kind of dnns do not require
   randomness in their id136 model. after all, any image of a dog
   should be mapped to the    dog    label, and there is nothing random about
   that.
   now suppose you are training a deep neural network (dnn) to play [10]go
   (game). in the case of the image below, the dnn has to make the first
   move.
   [11][2000px-blank_go_board.svg.png]
   if you used the same deterministic strategy described above, you will
   find that this network fails to give good results. why? because there
   is no single    optimal starting move    - for each possible stone
   placement on the board, there is a equally good stone placement on the
   other side board, via rotational symmetry. there are multiple best
   answers.
   if the network is deterministic and only capable of picking one output
   per input, then the optimization process will force the network to
   choose the move that averages all the best answers, which is smack-dab
   in the middle of the board. this behavior is highly undesirable, as the
   center of the board is generally regarded as a bad starting move.
   hence, randomness is important when you want the network to be able to
   output multiple possibilities given the same input, rather than
   generating the same output over and over again. this is crucial when
   there are underlying symmetries in the action space - incorporating
   randomness literally helps us break out of the "[12]stuck between two
   bales of hay" scenario.
   similarly, if we are training a neural net to compose music or draw
   pictures, we don   t want it to always draw the same thing or play the
   same music every time it is given a blank sheet of paper. we want some
   notion of    variation   , "surprise", and    creativity    in our generative
   models.
   one approach to incorporating randomness into dnns is to keep the
   network deterministic, but have its outputs be the parameters of a
   id203 distribution, which we can then draw samples from using
   conventional sampling methods to generate    stochastic outputs   .
   deepmind's alphago utilized this principle: given an image of a go
   board, it outputs the id203 of winning given each possible move.
   the practice of modeling a distribution after the output of the network
   is commonly used in other areas of deep id23.

randomness and id205

   during my first few courses in id203 & statistics, i really
   struggled to understand the physical meaning of randomness. when you
   flip a coin, where does this randomness come from? is randomness just
   [13]deterministic chaos? is it possible for something to be actually
   random?
   to be honest, i still don't fully understand.
   [14][maxresdefault.jpg]
   [15]id205 offers a definition of randomness that is
   grounded enough to use without being kept wide awake at night:
   "randomness" is nothing more than the "lack of information".
   more specifically, the amount of information in an object is the length
    (e.g. in bits, kilobytes, etc.)  of the shortest computer program
   needed to fully describe it. for example, the first 1 million digits of
   $\pi = 3.14159265....$ can be represented as a string of length
   1,000,002 characters, but it can be more compactly represented using 70
   characters, via an implementation of the leibniz formula:
   the above program is nothing more than a compressed version of a
   million digits of $\pi$. a more concise program could probably express
   the first million digits of $\pi$ in far fewer bits.
   under this interpretation, randomness is "that which cannot be
   compressed". while the first million digits of $\pi$ can be compressed
   and are thus not random, empirical evidence suggests (but has not
   proven) that $\pi$ itself is [16]normal number, and thus the amount of
   information encoded in $\pi$ is infinite.
   consider a number $a$ that is equal to the first trillion digits of
   $\pi$, $a = 3.14159265...$. if we add to that a uniform random number
   $r$ that lies in the range $(-0.001,0.001)$, we get a number that
   ranges in between $3.14059...$ and $3.14259...$. the resulting number
   $a + r$ now only carries ~three digits worth of information, because
   the process of adding random noise destroyed any information carried
   after the hundredth's decimal place.

limiting information in neural nets

   what does this definition of randomness have to deal with randomness?
   another way randomness is incorporated into dnns is by injecting noise
   directly into the network itself, rather than using the dnn to model a
   distribution. this makes the task "harder" to learn as the network has
   to overcome these internal "perturbations".
   why on earth would you want to do this? the basic intuition is that
   noise limits the amount of information you can pass through a channel.
   consider an autoencoder, a type of neural network architecture that
   attempts to learn an efficient encoding of data by "squeezing" the
   input into fewer dimensions in the middle and re-constituting the
   original data at the other end. a diagram is shown below:
   [17][yr89h3kzhbwts80fal93tankgjkywlcmjyj_rl31tztgkvkg4nzi3uj0nomtm2dpa-
   yuyoikmnkvijtioyj6uhxa3omorqc-9ovb3wsxid-2ngvwirqzkyyw=s0-d]
   during id136, inputs flow from the left through the nodes of the
   network and come out the other side, just like a pipe.
   if we consider a theoretical neural network that operates on real
   numbers (rather than floating point numbers), then without noise in the
   network, every layer of the dnn actually has access to infinite
   information bandwidth.
   even though we are squeezing representations (the pipe) into fewer
   hidden units, the network could still learn to encode the previous
   layer's data into the decimal point values without actually learning
   any meaningful features. in fact, we could represent all the
   information in the network with a single number. this is undesirable.
   by limiting the amount of information in a network, we force it to
   learn compact representations of input features. several ways to go
   about this:
     * [18]id5 (vae) add gaussian noise to the hidden
       layer. this noise destroys "excess information," forcing the
       network to learn compact representations of data.
     * closely related to vae noise (possibly equivalent?) to this is the
       idea of [19]dropout id173 - randomly zeroing out some
       fraction of units during training. like the vae, dropout noise
       forces the network to learn useful information under limited
       bandwidth.
     * [20]deep networks with stochastic depth - similar idea to dropout,
       but at a per-layer level rather than per-unit level.
     * there's a very interesting paper called [21]binarized neural
       networks that uses binary weights and activations in the id136
       pass, but real-valued gradients in the backward pass. the source of
       noise comes from the fact that the gradient is a noisy version of
       the binarized gradient. while binarynets are not necessarily more
       powerful than regular dnns, individual units can only encode one
       bit of information, which regularizes against two features being
       squeezed into a single unit via floating point encoding.

   more efficient compression schemes mean better generalization at
   test-time, which explains why dropout works so well for over-fitting.
   if you decide to use regular autoencoders over variational
   autoencoders, you must use a stochastic id173 trick such as
   dropout to control how many bits your compressed features should be,
   otherwise you will likely over-fit.
   i think vaes are objectively superior because they are easy to
   implement and allow you to specify exactly how many bits of information
   are passing through each layer.

exploration "energy"

   dnns are usually trained via some variant of id119, which
   basically amounts to finding the parameter update that "rolls downhill"
   along some id168. when you get to the bottom of the deepest
   valley, you've found the best possible parameters for your neural net.
   the problem with this approach is that neural network loss surfaces
   have a lot of local minima and plateaus. it's easy to get stuck in a
   small dip or a flat portion where the slope is already zero (local
   minima) but you are not done yet.
   [22][23.gif]
   the third interpretation of how randomness assists deep learning models
   is based on the idea of the exploration.
   because the datasets used to train dnns are huge, it   s too expensive to
   compute the gradient across terabytes of data for every single gradient
   descent step. instead, we use stochastic id119 (sgd), where
   we just compute the average gradient across a small subset of the
   dataset chosen randomly from the dataset.
   in evolution, if the success of a species is modeled by random variable
   $x$, then random mutation or noise increases the variance of $x$ - its
   progeny could either be far better off (adaptation, poison defense) or
   far worse off (lethal defects, sterility).
   in numerical optimization, this "genetic mutability" is called
   "thermodynamic energy", or temperature that allows the parameter update
   trajectory to not always "roll downhill", but occasionally bounce out
   of a local minima or "tunnel through" hills.
   this is all deeply related to the exploration-vs.-exploitation tradeoff
   as formulated in rl. training a purely deterministic dnn with zero
   gradient noise has zero exploitation capabilities - it converges
   straight to the nearest local minima, however shallow.
   using stochastic gradients (either via small minibatches or
   [23]literally adding noise to the gradients themselves) is an effective
   way to allow the optimization to do a bit of "searching" and "bouncing"
   out of weak local minima. asynchronous stochastic id119, in
   which many machines are performing id119 in parallel, is
   another possible source of noise.
   this "thermodynamic" energy ensures symmetry-breaking during early
   stages of training, to ensure that all the gradients in a layer are not
   synchronized to the same values. not only does noise perform symmetry
   breaking in action space of the neural net, but noise also performs
   symmetry breaking in the parameter space of the neural net.

closing thoughts

   i find it really interesting that random noise actually helps
   artificial intelligence algorithms to avoid over-fitting and explore
   the solution space during optimization or id23. it
   raises interesting philosophical questions on whether the [24]inherent
   noisiness of our neural code is a feature, not a bug.
   one theoretical ml research question i am interested in is whether all
   these neural network training tricks are actually variations of some
   general id173 theorem. perhaps [25]theoretical work on
   compression will be really useful for understanding this.
   it would be interesting to check the information capacity of various
   neural networks relative to hand-engineered feature representations,
   and see how that relates to overfitting tendency or quality of
   gradients. it's certainly not trivial to measure the information
   capacity of a network with dropout or trained via sgd, but i think it
   can be done. for example, constructing a database of synthetic vectors
   whose information content (in bits, kilobytes, etc) is exactly known,
   and seeing how networks of various sizes, in combination with
   techniques like dropout, deal with learning a generative model of that
   dataset.
   posted by eric at [26]11:56 pm
   [27]email this[28]blogthis![29]share to twitter[30]share to
   facebook[31]share to pinterest
   labels: [32]ai

6 comments:

    1. [33]lin yenchen[34]august 15, 2016 at 9:31 am
       nice article!
       but it seems like a typo: "the range (   0.01,0.01)" -> "the range
       (   0.001,0.001)"
       reply[35]delete
       replies
         1. [36]eric[37]september 3, 2016 at 5:13 pm
            thanks for your sharp eyes :) i've made the change.
            [38]delete
            replies
                 reply
            reply
    2. anonymous[39]november 8, 2016 at 6:23 pm
       every time i cannot decide which of which, i just turn to any
       random things i can think of. randomness for me is basically
       uncomplicating complicated circumstances. it's like giving you a
       choice but still your explanation here is so deep, i cannot but
       fathom but good work eric. i might want to [40]order research paper
       here instead.
       reply[41]delete
       replies
            reply
    3. [42]mueeid soomro[43]january 25, 2017 at 9:55 am
       all the above-mentioned trends should be watched closely by all
       those who are looking forward to developing e learning content.
       [44]free learning
       reply[45]delete
       replies
            reply
    4. [46]brock[47]march 18, 2017 at 7:49 am
       randomness is actually the need of time for all industries. deep
       learning has become very difficult in today   s world. deep learning
       is very important for various reasons and the most important reason
       is that distractions like smartphones have made it difficult for
       people to engulf in deep learning. during a study in an online
       institute that offers non [48]fake college degrees it was concluded
       that to engage properly in deep learning, it is important to keep
       away all the distractions similar to smartphones. the point is
       again the same that people who want to engage in deep learning,
       they should be fully focused and ready for it.
       reply[49]delete
       replies
            reply
    5. [50]kevin luther[51]april 25, 2017 at 10:24 pm
       simply want to say your work is outstanding. the clarity in your
       post is simply excellent and i can assume you   re an expert on this
       subject.. thanks a million and please carry on the rewarding work.
       i found some good websites to check [52]essay writing service
       reviews
       reply[53]delete
       replies
            reply

   add comment
   load more...

   comments will be reviewed by administrator (to filter for spam and
   irrelevant content).

   [54]newer post [55]older post [56]home
   subscribe to: [57]post comments (atom)

blog archive

     * [58]     [59]2019 (3)
          + [60]     [61]march (1)
          + [62]     [63]february (2)

     * [64]     [65]2018 (11)
          + [66]     [67]december (1)
          + [68]     [69]november (1)
          + [70]     [71]august (1)
          + [72]     [73]june (1)
          + [74]     [75]april (1)
          + [76]     [77]february (1)
          + [78]     [79]january (5)

     * [80]     [81]2017 (3)
          + [82]     [83]december (1)
          + [84]     [85]november (1)
          + [86]     [87]january (1)

     * [88]     [89]2016 (11)
          + [90]     [91]november (1)
          + [92]     [93]september (2)
          + [94]     [95]august (1)
          + [96]     [97]july (3)
               o [98]why randomness is important for deep learning
               o [99]what product breakthroughs will recent advances in...
               o [100]how to get an internship
          + [101]     [102]june (4)

   not for reproduction. simple theme. powered by [103]blogger.

references

   visible links
   1. https://blog.evjang.com/feeds/posts/default
   2. https://blog.evjang.com/feeds/posts/default?alt=rss
   3. https://blog.evjang.com/feeds/8955020110627899957/comments/default
   4. https://blog.evjang.com/
   5. https://en.wikipedia.org/wiki/randomness
   6. https://en.wikipedia.org/wiki/deep_learning
   7. http://www.nasa.gov/centers/glenn/images/content/508159main_1944_woodfan_946-710.jpg
   8. http://25.media.tumblr.com/ced892e7d72771e32ef50b905e8da33c/tumblr_mx2am9n0h61rioxyio1_500.gif
   9. https://devblogs.nvidia.com/wp-content/uploads/2014/11/id163_example.png
  10. https://en.wikipedia.org/wiki/go_(game)
  11. https://upload.wikimedia.org/wikipedia/commons/thumb/9/9b/blank_go_board.svg/2000px-blank_go_board.svg.png
  12. https://en.wikipedia.org/wiki/buridan's_ass
  13. https://en.wikipedia.org/wiki/chaos_theory
  14. https://i.ytimg.com/vi/qravxr5wm-q/maxresdefault.jpg
  15. https://en.wikipedia.org/wiki/information_theory
  16. https://en.wikipedia.org/wiki/normal_number
  17. http://nghiaho.com/wp-content/uploads/2012/12/autoencoder_network1.png
  18. https://arxiv.org/abs/1312.6114
  19. http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
  20. https://arxiv.org/abs/1603.09382
  21. http://arxiv.org/abs/1602.02830
  22. https://2.bp.blogspot.com/-sxvce9sbxzg/v5wrk03v8gi/aaaaaaaafdk/lpuzzghgyhy3aexxuimi2zfrrhfcanfowclcb/s1600/23.gif
  23. https://arxiv.org/abs/1511.06807
  24. http://www.scholarpedia.org/article/neuronal_noise
  25. http://www.eecs.tufts.edu/~dsculley/papers/compressionandvectors.pdf
  26. https://blog.evjang.com/2016/07/randomness-deep-learning.html
  27. https://www.blogger.com/share-post.g?blogid=842965756326639856&postid=8955020110627899957&target=email
  28. https://www.blogger.com/share-post.g?blogid=842965756326639856&postid=8955020110627899957&target=blog
  29. https://www.blogger.com/share-post.g?blogid=842965756326639856&postid=8955020110627899957&target=twitter
  30. https://www.blogger.com/share-post.g?blogid=842965756326639856&postid=8955020110627899957&target=facebook
  31. https://www.blogger.com/share-post.g?blogid=842965756326639856&postid=8955020110627899957&target=pinterest
  32. https://blog.evjang.com/search/label/ai
  33. https://www.blogger.com/profile/08888624022998741952
  34. https://blog.evjang.com/2016/07/randomness-deep-learning.html?showcomment=1471278703610#c4424725777934068713
  35. https://www.blogger.com/delete-comment.g?blogid=842965756326639856&postid=4424725777934068713
  36. https://www.blogger.com/profile/05932982386234738790
  37. https://blog.evjang.com/2016/07/randomness-deep-learning.html?showcomment=1472947998343#c9056956830124933246
  38. https://www.blogger.com/delete-comment.g?blogid=842965756326639856&postid=9056956830124933246
  39. https://blog.evjang.com/2016/07/randomness-deep-learning.html?showcomment=1478658238841#c8970177062134924811
  40. http://www.lordofpapers.com/research-papers
  41. https://www.blogger.com/delete-comment.g?blogid=842965756326639856&postid=8970177062134924811
  42. https://www.blogger.com/profile/15577809287812278966
  43. https://blog.evjang.com/2016/07/randomness-deep-learning.html?showcomment=1485366958645#c7084503564293369142
  44. http://www.freelearninghub.com/
  45. https://www.blogger.com/delete-comment.g?blogid=842965756326639856&postid=7084503564293369142
  46. https://www.blogger.com/profile/07393973403116078265
  47. https://blog.evjang.com/2016/07/randomness-deep-learning.html?showcomment=1489848576917#c3061478406539954117
  48. http://www.lifeexperiencedegreecampus.com/non-fake/
  49. https://www.blogger.com/delete-comment.g?blogid=842965756326639856&postid=3061478406539954117
  50. https://www.blogger.com/profile/00509605685275297754
  51. https://blog.evjang.com/2016/07/randomness-deep-learning.html?showcomment=1493184284582#c2639322122809614305
  52. https://reviews.clazwork.com/
  53. https://www.blogger.com/delete-comment.g?blogid=842965756326639856&postid=2639322122809614305
  54. https://blog.evjang.com/2016/08/variational-bayes.html
  55. https://blog.evjang.com/2016/07/what-product-breakthroughs-will-recent.html
  56. https://blog.evjang.com/
  57. https://blog.evjang.com/feeds/8955020110627899957/comments/default
  58. javascript:void(0)
  59. https://blog.evjang.com/2019/
  60. javascript:void(0)
  61. https://blog.evjang.com/2019/03/
  62. javascript:void(0)
  63. https://blog.evjang.com/2019/02/
  64. javascript:void(0)
  65. https://blog.evjang.com/2018/
  66. javascript:void(0)
  67. https://blog.evjang.com/2018/12/
  68. javascript:void(0)
  69. https://blog.evjang.com/2018/11/
  70. javascript:void(0)
  71. https://blog.evjang.com/2018/08/
  72. javascript:void(0)
  73. https://blog.evjang.com/2018/06/
  74. javascript:void(0)
  75. https://blog.evjang.com/2018/04/
  76. javascript:void(0)
  77. https://blog.evjang.com/2018/02/
  78. javascript:void(0)
  79. https://blog.evjang.com/2018/01/
  80. javascript:void(0)
  81. https://blog.evjang.com/2017/
  82. javascript:void(0)
  83. https://blog.evjang.com/2017/12/
  84. javascript:void(0)
  85. https://blog.evjang.com/2017/11/
  86. javascript:void(0)
  87. https://blog.evjang.com/2017/01/
  88. javascript:void(0)
  89. https://blog.evjang.com/2016/
  90. javascript:void(0)
  91. https://blog.evjang.com/2016/11/
  92. javascript:void(0)
  93. https://blog.evjang.com/2016/09/
  94. javascript:void(0)
  95. https://blog.evjang.com/2016/08/
  96. javascript:void(0)
  97. https://blog.evjang.com/2016/07/
  98. https://blog.evjang.com/2016/07/randomness-deep-learning.html
  99. https://blog.evjang.com/2016/07/what-product-breakthroughs-will-recent.html
 100. https://blog.evjang.com/2016/07/how-to-get-internship.html
 101. javascript:void(0)
 102. https://blog.evjang.com/2016/06/
 103. https://www.blogger.com/

   hidden links:
 105. https://www.blogger.com/post-edit.g?blogid=842965756326639856&postid=8955020110627899957&from=pencil
 106. https://www.blogger.com/comment-iframe.g?blogid=842965756326639856&postid=8955020110627899957
 107. https://www.blogger.com/rearrange?blogid=842965756326639856&widgettype=blogarchive&widgetid=blogarchive1&action=editwidget&sectionid=sidebar-right-1
 108. https://www.blogger.com/rearrange?blogid=842965756326639856&widgettype=attribution&widgetid=attribution1&action=editwidget&sectionid=footer-3
