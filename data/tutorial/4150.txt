   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

text classifier algorithms in machine learning

key text classification algorithms with use cases and tutorials

   [16]go to the profile of roman trusov
   [17]roman trusov (button) blockedunblock (button) followfollowing
   jul 12, 2017
   [1*fwrwhkgcw0a8_y-eqepacw.jpeg]

   one of the main ml problems is text classification, which is used, for
   example, to detect spam, define the topic of a news article, or choose
   the correct mining of a multi-valued word. the [18]statsbot team has
   already written [19]how to train your own model for detecting spam
   emails, spam messages, and spam user comments. for this article, we
   asked a data scientist, roman trusov, to go deeper with machine
   learning text analysis.
     __________________________________________________________________

   you may know it   s impossible to define the best text classifier. in
   fields such as id161, there   s a strong consensus about a
   general way of designing models     deep networks with lots of residual
   connections. unlike that, text classification is still far from
   convergence on some narrow area.

   in this article, we   ll focus on the few main generalized approaches of
   text classifier algorithms and their use cases. along with the
   high-level discussion, we offer a collection of hands-on tutorials and
   tools that can help with building your own models.

text classification benchmarks

   the toolbox of a modern machine learning practitioner who focuses on
   id111 spans from tf-idf features and linear id166s, to word
   embeddings (id97) and attention-based neural architectures.

     it   s important to distinguish two cases when the effectiveness of a
     certain method is demonstrated: research and competition.

   when researchers compare the text classification algorithms, they use
   them as they are, probably augmented with a few tricks, on well-known
   datasets that allow them to compare their results with many other
   attempts on the same problem.

some well-known text classification benchmarks:

     * ag   s news articles
     * sogou news corpora
     * amazon review full
     * amazon review polarity
     * dbpedia
     * yahoo answers
     * yelp review full
     * yelp review polarity

   we   ve made a [20]special folder on google drive so you could download
   them right away.

deep vs. shallow learning

   the really remarkable thing about the datasets widely adopted in nlp
   research is that both simple and very complex models work on them very
   well. to showcase this, let   s discuss two papers:
     * [21]a bag of tricks for efficient text classification by joulin et
       al
     * [22]character-level convolutional networks for text classification
       by zhang et al

   the datasets in both cases are the same, and the results in terms of
   precision are roughly the same across all the experiments. but the
   training and id136 time varies greatly between the two.

   the first model takes literally seconds to train, while the second
   needs several hours, which would be a game changer when it comes to
   choosing the hyperparameters.
   [0*jwd41v_apz4tpiqr.]
   [23]illustration source

   what makes this approach interesting is that their model doesn   t make
   any assumptions about the data. at the lowest level they treat the text
   as a sequence of characters, allowing the convolutional layers to build
   the features in a completely content-agnostic way.

   the second paper features a much lighter model that   s designed to work
   fast on a cpu and consists of a joint embedding layer and a softmax
   classifier.

     on the other hand, if you take a look at some of the winning
     solutions on kaggle, you   ll see they are dominated by highly
     customized complex ensembles.

   a good example would be the recent [24]quora question pairs competition
   and ongoing [25]deephack.turing, where top-ranking solutions consist of
   several different models: gradient boosting machines, id56s, and id98s.

   the practical lesson we can learn here is that despite the results of
   certain methods published in research, getting the best performance
   from the particular tasks in vivo is closer to art than to science,
   requiring careful tuning of complicated pipelines.

   the striking contrast with the research here can be seen in a writeup
   for [26]a winning solution on kaggle.

neural network-based text classifiers typically follow the same linear meta
architecture:

     * embedding
     * deep representation
     * fully connected part

embedding

   embedding layers take a sequence of word ids as an input and produce a
   sequence of corresponding vectors as an output. their functionality is
   really straightforward, and since the actual semantics of those vectors
   are not interesting for our problem, the only remaining question is
      what is the best way to initialize the weights?   
   [0*pqa56mixjkwh7laq.]
   [27]illustration source

   depending on the problem, the answers may be as counterintuitive as the
   advice    generate your own synthetic labels, train id97 on them, and
   init the embedding layer with them.   

   but for all practical purposes you can use a pre-trained set of
   embeddings and jointly fine-tune it for your particular model. it   s
   likely that resulting word vectors will cease to demonstrate the same
   properties as they do in a vanilla id97 model:
   [0*kje0-2fhqgrip930.]
   [28]illustration source

   but it doesn   t matter in this case.

   the go-to solution here is to use pretrained id97 embeddings and
   try to use lower learning rates for the embedding layer (multiply
   general learning rate by 0.1).

deep representation

   the main purpose of the deep representation part is to condense all
   relevant information in its output while suppressing the parts that
   could lead to identifying a single sample from it. this is highly
   desirable because the network with high capacity is likely to overfit
   on particular examples and perform poorly on the test set.

recurrent neural network (id56)

   when the problem consists of obtaining a single prediction for a given
   document (spam/not spam), the most straightforward and reliable
   architecture is a multilayer fully connected text classifier applied to
   the hidden state of a recurrent network. semantics of this state are
   considered irrelevant, and the entire vector is treated as a compressed
   description of the text.

   here are several useful sources:

       a great starting point for understanding [29]how to use lstms for
   text classification (in this case         id31).

       [30]lstm tutorial for pytorch

       [31]almost classic tutorial by chris olah
   [0*knlr9qcptqa6uys4.]
   [32]basic scheme of an id56-based classifier

   since the main work is being done in the recurrent layer, it   s
   important to make sure that it captures only the relevant information.
   it   s a frequent challenge for natural language applications and an open
   scientific problem.

   on a high level, there are two things that can be done here:
    1. use bidirectional lstms. this is almost always a good idea, because
       it essentially captures the context around each word, instead of
       sequential    reading.   
    2. use a transitional layer for embeddings. lstms learn to distinguish
       important and unimportant parts of the sequence by themselves, but
       we can   t be sure that the representation from the embedding layer
       is the best input, especially if we don   t finetune the embeddings.
       adding a layer that   s applied to each id27 independently
       can improve your results, acting as a simple attention layer.

convolutional neural network (id98)

   an alternative way to train a deep text classifier is to use
   convolutional networks. typically, given a large enough receptive
   field, you can achieve the same results as with a dedicated attention
   layer. there   s no single trick here, but keeping a lot of feature maps
   in the beginning and reducing their number exponentially later helps to
   avoid learning irrelevant patterns.
   [0*3dubbm00xucsoewj.]
   [33]illustration of id98-based classifier

   take a look at this[34] simple implementation of id98 classifier in
   pytorch. it shows how to train and evaluate a convolutional classifier
   with its own embedding layer.

dense classifier

   a fully-connected part performs a series of transformations on the deep
   representation and finally outputs the scores for each class. the best
   practice here is to apply the transformations as follows:
    1. fully-connected layer
    2. batch id172
    3. (optional) non-linear transformation (hyperbolic tangent or elu)
    4. dropout
     __________________________________________________________________

   i hope you like this overview of neural text classifier algorithms that
   can be further augmented with more sophisticated methods of your
   choice. a few tips and tricks mentioned here are going to help you with
   building better models and achieving faster convergence.

   in addition, below you   ll find a few links to tutorials and tools for
   classification and representation learning tasks.

helpful resources

     * [35]a collection of tools and implemented ready-to-train text
       classifiers (pytorch)
     * [36]fasttext, a library for efficient text classification and
       building word representations
     * skip-gram tutorial: [37]part 1, [38]part 2
     * [39]id98 text classifier in tensorflow
     * [40]id56 sentence classification tutorial in keras

   iframe: [41]/media/02231cd5403151a2a463e36cc3b88462?postid=acc115293278

you   d also like:

   [42]id126 algorithms
   main existing recommendation engines and how they
   workblog.statsbot.co
   [43]data scientist resume projects
   machine learning problems set to build a data scientist cv without work
   experienceblog.statsbot.co
   [44]time series anomaly detection algorithms
   the current state of anomaly detection techniques in plain
   languageblog.statsbot.co

     * [45]machine learning
     * [46]algorithms
     * [47]big data
     * [48]big data analysis
     * [49]data science

   (button)
   (button)
   (button) 745 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [50]go to the profile of roman trusov

[51]roman trusov

   researcher @ xix.ai & ml guy @ skoltech

     (button) follow
   [52]stats and bots

[53]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 745
     * (button)
     *
     *

   [54]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [55]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/acc115293278
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_l2v5wvulguid---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@lextal?source=post_header_lockup
  17. https://blog.statsbot.co/@lextal
  18. http://statsbot.co/?utm_source=blog&utm_medium=post&utm_campaign=text_classifier
  19. https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=post&utm_campaign=text_classifier
  20. https://drive.google.com/drive/u/0/folders/0bz8a_dbh9qhbfll6bvpmnutucfdjymf2sepmzuzucvnimuw1twn6rdv3a0jht3kxlvhvr2m
  21. https://arxiv.org/abs/1607.01759
  22. https://arxiv.org/abs/1509.01626
  23. https://arxiv.org/abs/1509.01626 
  24. https://www.kaggle.com/c/quora-question-pairs
  25. https://inclass.kaggle.com/c/human-or-machine-generated-text
  26. https://www.kaggle.com/c/quora-question-pairs/discussion/34355
  27. https://www.slideshare.net/hustwj/cikm-keynotenov2014
  28. http://blog.aylien.com/word-embeddings-and-their-challenges/
  29. http://deeplearning.net/tutorial/lstm.html
  30. http://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html
  31. http://colah.github.io/posts/2015-08-understanding-lstms/ 
  32. http://deeplearning.net/
  33. https://www.slideshare.net/ssuser9cc1bd/piji-li-dltm
  34. https://github.com/shawn1993/id98-text-classification-pytorch
  35. https://github.com/facebookresearch/infersent
  36. https://github.com/facebookresearch/fasttext
  37. http://mccormickml.com/2016/04/19/id97-tutorial-the-skip-gram-model/
  38. http://mccormickml.com/2017/01/11/id97-tutorial-part-2-negative-sampling/
  39. https://github.com/dennybritz/id98-text-classification-tf
  40. http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/
  41. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=acc115293278
  42. https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3
  43. https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6
  44. https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2
  45. https://blog.statsbot.co/tagged/machine-learning?source=post
  46. https://blog.statsbot.co/tagged/algorithms?source=post
  47. https://blog.statsbot.co/tagged/big-data?source=post
  48. https://blog.statsbot.co/tagged/big-data-analysis?source=post
  49. https://blog.statsbot.co/tagged/data-science?source=post
  50. https://blog.statsbot.co/@lextal?source=footer_card
  51. https://blog.statsbot.co/@lextal
  52. https://blog.statsbot.co/?source=footer_card
  53. https://blog.statsbot.co/?source=footer_card
  54. https://blog.statsbot.co/
  55. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  57. https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3
  58. https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6
  59. https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2
  60. https://medium.com/p/acc115293278/share/twitter
  61. https://medium.com/p/acc115293278/share/facebook
  62. https://medium.com/p/acc115293278/share/twitter
  63. https://medium.com/p/acc115293278/share/facebook
