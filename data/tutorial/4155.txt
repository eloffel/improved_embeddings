   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

neural networks from scratch for javascript linguists (part1         the
id88)

   [14]go to the profile of elyx0
   [15]elyx0 (button) blockedunblock (button) followfollowing
   apr 26, 2017
   [1*ixxnl7mpexthjvpukhpmsg.gif]
   snakes surviving thanks to machine learning: [16]here

   there is a big chance that you heard about neural network and
   artificial intelligence in the course of the previous months, which
   seem to accomplish wonders, from [17]rating your selfie to [18]siri
   understanding your voice, beating players at games such as chess and
   [19]go, [20]turning a horse into a zebra or making you look younger,
   older, or [21]from the other gender.

   those keywords have in fact been used so many times in so many
   different contexts, making this whole    a.i    lexical field so confusing
   that it does at best correlates with    something that looks smart   .

   iframe: [22]/media/a4e8fe2f94f1f9855dc23d94d2d04f99?postid=632a4d1fbad2

   maybe because of the confusion machine learning seems way too complex,
   way too hard to grasp, like    i bet there is so much math, this is not
   for me!!   .

   well, don   t worry, it was for me too, so let   s go on a journey, i   ll
   tell you everything i learned, some misconceptions i had, how to
   interpret the results, and some basic vocabulary and fun facts along
   the way.

what are we talking about?

   imagine a box in which you carve some holes then you throw in it a
   predefined amount of numbers, either 0 or 1. then the box vibrates
   violently and from each hole the box spurts out one number: 0 or1.
   great.
   [1*qlyvcq6yolcnqiqdptndng.png]
   now what?

   initially, your box is dumb as hell, it won   t magically give you the
   result you expect, you have to train it to achieve the goal you want.

understanding through history

   my biggest mistake was trying to wrap my head around concepts by just
   looking at the tip of the iceberg, playing with libraries and getting
   mad when it didn   t work. you can   t really afford that with neural nets.
   [1*eqfifkckck1cj1tnldxigq.gif]
   time to go back

   the invention of this wonderful cardboard cheese originated around 1943
   when the neurophysiologist [23]warren sturgis mcculloch, 45 years, and
   his colleague walter pitts wrote a paper named:    [24]a logical calculus
   of the ideas immanent in nervous activity   .

   pursuing the quest of the classical philosophers of greece, he
   attempted to model how the brain works mathematically.

   that was indeed pretty bright given since [25]how long we knew about
   the neurons (~1900) and that the electric nature of their signals to
   communicate wouldn   t be demonstrated before the late 1950s.

   let   s take a second here to remember that published papers do not mean
   that the person who wrote it was absolutely right, it meant this guy:
   had a hypothesis, knew a bit in this field, had some kind of results,
   sometimes not applicable and published it.
   then other qualified people from the field have to try it, reproduce
   the results and decide whether or not it   s a nice base to build upon.

   when left unchecked, and toyed with p-hacking, some papers can give
   birth to some aberrations like this 2011 paper saying that [26]people
   could see in the future.

   nevertheless, we now know that our [27]mcculloch was a good guy and in
   his paper he tried to model some algorithms after the physical brain,
   constituted itself of a nervous system (possessed by most multicellular
   animals), which is actually a net of neurons.
   [1*norbxixj54ickcjdfcc9og.gif]
   hoping you won   t lose too many of these reading the following.
   ([28]source)

   the human brain has ~86 billion of these fellas, each having axons and
   dendrites and synapses that connect each neuron to ~7000 others. which
   is almost as many connections as the [29]number of galaxies in the
   universe.

   since we   re not good at visualizing big numbers, here is how to see
   $1b:

   iframe: [30]/media/a9880564aa19841fee19779657053a41?postid=632a4d1fbad2

   now imagine 86 times that

   the problem for mcculloch was that the economic context wasn   t thriving
   in 1943: we   re at war, franklin d roosevelt froze the prices, salaries
   and wages to prevent inflation, nikola tesla passed away and the best
   computer available was the [31]eniac, which cost $7 millions, for
   [32]30 tons (the sexism in tech ran pretty wild(er?) at that time and
   the fact that eniac was [33]invented by 6 women caused their male
   colleagues to wrongly underestimate it).
   as a comparison, a standard samsung flip phone from 2005 had 1300x the
   eniac computing power.

   then in 1958, computers were doing a bit better, and frank rosenblatt,
   inspired by mcculloch, gifted us with the [34]id88.

   everybody was happy digging deeper until marvin minsky, 11 years later,
   decided he [35]did not like that idea and that frank id88 wasn   t
   cut for the job, as he explained by publishing a book in which he said:
      most of rosenblatt writing     is without scientific value       the impact
   of this book is that it drained the already low funding in this field.
   [1*hsupkzfztlbpagvkqz752w.gif]
   classic minsky.

   what did minsky have against mcculloch padawan?

the linearity conundrum

   while this may sound like a big bang theory episode title, it actually
   represents the basis of minsky theory to detract from the original
   id88.

   rosenblatt id88 looks very similar to our box, in which this time
   we drilled a single hole:
   [1*bj6cgh_gsusbcrps_n6xiq.png]
   a neural network can actually take inputs between 0 and 1

   if the sum of our inputs signals(x1   x4) multiplied by their respective
   weights (w1   w4) plus the bias (b) are enough to make the result gate go
   above the threshold (t), our door will liberate the value 1otherwise,
   0.

   for this to happen the threshold value is compared to the result of the
   [36]activation function. exactly like brain neurons respond to a
   stimulus. if the stimulus is too low the neuron doesn   t fire the signal
   along the axon to the dendrites.
   [1*ymbxdghv-ii7m9abmxey1g.gif]
   depolarization of a neuron thanks to the magic of the
   [37]sodium-potassium pump

   code wise it globally looks like this:
// defining the inputs
const x1 = 1;
const x2 = 0.3;
const x3 = 0.2;
const x4 = 0.5;
// defining the weights
const w1 = 1.5;
const w2 = 0.2;
const w3 = 1.1;
const w4 = 1.05;

const threshold = 1;
const bias = 0.3;
// the value evaluated against the threshold is the sum of the
// inputs multiplied by their weights
// (1*1.5)+(.3*0.2)+(.2*1.1)+(.5*1.05)
const suminputsweights = x1*w1 + x2*w2 + x3*w3 + x4*w4; // 2.305
const doorwillopen = activation(suminputsweights + bias) > threshold; // true

   in the human body a neuron electrical off state is at -70mv and its
   activation threshold is when it hits -55mv.

   in the case of the original id88, this activation is handled by
   the heaviside step function.
   [1*sg_yqs9th-v8getonu0loa.png]
   0 if x is negative, 1 if x is null or positive. x being the
   suminputsweights+bias.

   one of the most known activation function is the sigmoid function:
    f(x) = 1 / (1 + exp(-x)) and the bias is generally used to shift its
   activation threshold:
   [1*kvpmkquzrwke8lotfwmwya.png]
   tweaking the activation function can yield to adjusted results altering
   when the neuron fires

   some activations function allow negative values as output, some don   t.
   this will prove itself important when using results of the activation
   function of one id88 to feed it as the inputs of another
   id88.

   using 0 as an input always silences its associated weight, causing its
   connection to be non-existent in the sum.

   so instead of having the possibility of weighting it down, it acts like
   an abstention vote. whereas sometimes we might want0as an input to be a
   vote for the opposite candidate.
   [1*dq-qwog88odkgmkbecujtg.png]
   many other activations functions [38]exist

   the id88 is known as a binary classifier meaning it can only
   classify between 2 options (spam vs not spam, oranges vs not-oranges   
   etc)

   it   s also designated as a [39]linear classifier meaning its goal is to
   identify to which class an object belongs to according to its
   characteristics (or    features   : our x1 to x4)) by iterating until it
   finds a single line that correctly separates the entities from each
   class.

   we give our classifier some examples of expected results given some
   inputs, and it will train itself to find this separation, by adjusting
   the weights assigned to every input and its bias.

   as an example let   s classify some entities between    friendly or not   
   according to 2 characteristics: teeth and size using a id88
   [1*qewusffiilfoeennlxfygg.gif]
   depending on the sources the cat seems very borderline indeed

   now that we trained our id88, we can predict samples it   s never
   seen before:

   head [40]here for the live [41]demo. no seriously, check it.

   what minsky reproached to rosenblatt was the following: what if
   suddenly my training set contained a giant snake, with almost no teeth
   but quite as big as an elephant.
   [1*izrfoxru3b1dl2ys5fbzkg.png]
   the training set is not separable by one line anymore

   it now requires 2 lines to correctly separate the red entities from the
   green ones. using a single id88 this impossibility would cause
   the id88 to try and run forever, unable to classify with a single
   straight line.

   by this time you should be able to handle the usual representation of a
   id88:
   [1*k_-erxb47r6ceoju61yg6w.png]
   the bias can be applied either after the sum or as an added weight for
   a fictional input being always 1

solving the bilinear issue:

   one way to solve this is to simply train 2 id88s one responsible
   for the top left separation, the other for the bottom right, and we
   plug them together with an and rule, creating some kind of
   [42]multiclass id88.
const p1 = new id88();
p1.train(trainingsettopleft);
p1.learn(); // p1 ready.
const p2 = new id88();
p2.train(trainingsetbottomright);
p2.learn(); // p2 ready.
const inputs = [x1,x2];
const result = p1.predict(inputs) & p2.predict(inputs);

   [1*1wgdiz2ur1h0jwfzndulaq.png]
   there are other ways to solve this that we   ll explore in part 2

   the and operation aka a^b is part of the logical operations a
   id88 can perform, by putting w5 and w6 at around 0.6 and applying
   a bias of -1 to our sum, we indeed created an and predictor.
   remember that the inputs connected with w5 and w6 are coming from the
   activation of the heaviside step function, yielding only 0 or 1 as
   output.
for 0 & 0 : (0*0.6 + 0*0.6)-1 is <0, output: 0
for 0 & 1 : (0*0.6 + 1*0.6)-1 is <0, output: 0
for 1 & 1 : (1*0.6 + 1*0.6)-1 is >0, output: 1

   by doing this chaining and having    elongated    our original id88
   we have created what is called a hidden layer, which is basically some
   neurons plugged inbetween the original inputs, and the final output.
   [1*fibrm8jgbho8hlwgoyambq.png]
   they should really be called    [43]feature detectors layers    or    sub
   problem layers   

   the worst part is that minsky knew all this and still decided to focus
   on the simplest version of the id88 to spit on all of
   rosenblatt   s work. rosenblatt had already covered the multi-layer &
   cross-coupled id88 in his own book principles of neurodynamics:
   id88s and the theory of brain mechanisms in 1962.

   which is a bit like publishing a book named transistors, express that a
   single one of them is worthless and never acknowledge the computer.

but where is the id88 line coming from?

   you should all be familiar with the following equations: y=f(x)
   y = mx + c or y = ax + b .
   so how do we find this m and c from our trained id88?

   to figure this out we have to remember the original equation of our
   id88: x1*w1 + x2*w2 + bias > t

   bias and threshold are the [44]same concepts, and for the id88 t
   = 0 which means the equation becomes x1*w1 +x2*w2 > -bias which can be
   rewritten as:
   x2 > (-w1/w2)*x1 + (-bias/w2) comparing this to:
   y = m*x + b we can see that

   ystands for x2
   xfor x1
   m for(-w1/w2) and b for (-bias/w2)

   which means the gradient (m) of our line is determined by the 2
   weights, and the place where the line cuts the vertical axis is
   determined by the bias and the 2nd weight.

   we can now simply select two x values (0 and 1), replace them in the
   line equation and find the corresponding y values, then trace a line
   between the two points. (0;f(0))and (1;f(1)) .

   like a good crime scene having access to the equation allows us to make
   some observations:
y = (-w1/w2)x + (-bias/w2)

      the gradient (steepness) of the line depends only on the two weights
      the minus sign in front of w1 means that if both weights have the
   same sign, the line will slope down like a \, if they are exclusively
   different it will slope up /
      adjusting w1 will affect the steepness but not where it intersects
   the vertical axis, w2 instead will have an effect on both
      because the bias is the numerator (top half of the fraction)
   increasing it will push the line higher up the graph. (will cut the
   vertical axis at a higher point)

   you can check the final weights in the console of the [45]demo by
   entering app.id88.weights

   and that is exactly what our id88 tries to optimize until it
   finds the correct line
   [1*1hqqac67zms9nomtuxa7uw.gif]
   i know it might look like 2 lines but it   s really one moving super fast

what do you mean by    optimize    ?

   as you can guess, our id88 can   t blindly guess and try every
   value for its weights until it finds a line that correctly separates
   the entities. what we have to apply is the [46]delta rule.

   it   s a learning rule for updating the weights associated with the
   inputs in a single layer neural network, and is represented by the
   following:
   [1*ba8c5nlo8vkcu9t9go-28q.png]
   oh god math!

   don   t worry it can be    simplified    like this:
   [1*7bcdxvkt7tcvhqebhwfina.png]
   we do this for every item in the training set.

   doing expected         actual represents an error value (or cost). the goal
   is to iterate through the training set and reduce this error/cost to
   the minimum, by adding or subtracting a small amount to the weights of
   each input until it validates all the training set expectations.

   if after some iterations the error is 0 for every item in the training
   set . our id88 is trained, and our line equation using these
   final weights correctly separates in two.

beware of the learning rate

   in the equation above,    represents a constant: the learning rate, that
   will have an impact on how much each weight gets altered.

      if    is too small, it will require more iterations than needed to
   find the correct weights and you might get trapped in a local minima.
      if    is too big the learning might never find some correct weights.

   one way to see it is imagining a poor guy with[47] metal boots tied
   together that wants to reach a treasure at the bottom of a cliff but he
   can only move by jumping by    meters:
   [1*c__j4pzohc6nv5w7hv3r4w.png]
      too big
   [1*5lufc0ug0feob0hpjbutca.png]
      too small

   one thing you might want to do when reading an article on wikipedia is
   to head on the    talk    section which discusses disputed areas of the
   content.
   [1*hhncztt0uvp4g6pryefcna.png]

   in the case of the delta formula, the content said that it couldn   t be
   applied to the id88 because the heaviside derivative does not
   exist at 0 but the talk section provided articles of m.i.t teachers
   using it.

   by putting everything we learned we finally code a id88:

   iframe: [48]/media/75278313c6842ccf3ab9c26450b277c0?postid=632a4d1fbad2

   can   t be used as is. actual code [49]here
     __________________________________________________________________

   additionally, the id88 goes into the feedforward neural network
   category which is just fancy wording for saying that the connections
   between the units do not form a cycle.

   to cut some slack to minsky, while the id88 algorithm is
   guaranteed to converge on some solution in the case of a linearly
   separable training set, it may still pick any valid solution and some
   problems may admit many solutions of varying quality.

   your brain does a bit of the same as soon as it finds a correct way to
   talk to a muscle (ie: the muscle responds correctly,) it will settle
   for it. this brain-to-muscle code is different for everyone!

   intellectuals like to argue with each other all the time like minksy
   and rosenblatt did. even einstein was proven wrong in his fight against
   niels bohr on [50]quantum indeterminism, arguing with his famous quote
   that:    god does not play dice with the universe   .

   we   ll end with some poetry from the father of the neural networks our
   dear warren mcculloch (he [51]really was a poet).
   i hope you learned some things and i will see you soon for the part 2.

     our knowledge of the world, including ourselves, is incomplete as to
     space and indefinite as to time. this ignorance, implicit in all our
     brains, is the counterpart of the abstraction which renders our
     knowledge useful.

   [52][1*0hqoaabq7xgpt-oyngiubg.png]
   [53][1*vgw1jka6hgnvwztsfmlnpg.png]
   [54][1*gkbpq1ruui0fvk2um_i4tq.png]

     [55]hacker noon is how hackers start their afternoons. we   re a part
     of the [56]@ami family. we are now [57]accepting submissions and
     happy to [58]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [59]latest tech
     stories and [60]trending tech stories. until next time, don   t take
     the realities of the world for granted!

   [1*35tcjopcvq6lbb3i6wegqw.jpeg]

     * [61]machine learning
     * [62]neural networks
     * [63]javascript
     * [64]artificial intelligence
     * [65]bots

   (button)
   (button)
   (button) 524 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [66]go to the profile of elyx0

[67]elyx0

   gone rogue dailymotion engineer

     (button) follow
   [68]hacker noon

[69]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 524
     * (button)
     *
     *

   [70]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [71]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/632a4d1fbad2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/neural-networks-from-scratch-for-javascript-linguists-part1-the-id88-632a4d1fbad2&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/neural-networks-from-scratch-for-javascript-linguists-part1-the-id88-632a4d1fbad2&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_xxe5okys1o3a---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@elyx0?source=post_header_lockup
  15. https://hackernoon.com/@elyx0
  16. http://snakeneuralnetwork.herokuapp.com/
  17. http://karpathy.github.io/2015/10/25/selfie/
  18. http://mashable.com/2014/08/18/siri-fails/#hmwskramipqp
  19. http://www.cbc.ca/news/technology/go-google-alphago-lee-sedol-deepmind-1.3488913
  20. https://twitter.com/goodfellow_ian/status/851124988903997440
  21. https://www.faceapp.com/
  22. https://hackernoon.com/media/a4e8fe2f94f1f9855dc23d94d2d04f99?postid=632a4d1fbad2
  23. https://en.wikipedia.org/wiki/warren_sturgis_mcculloch
  24. http://www.cs.cmu.edu/~./epxing/class/10715/reading/mcculloch.and.pitts.pdf
  25. https://en.wikipedia.org/wiki/santiago_ram  n_y_cajal
  26. https://www.youtube.com/watch?v=42quxluch3q
  27. https://en.wikipedia.org/wiki/warren_sturgis_mcculloch
  28. https://www.youtube.com/watch?v=qpix_x-9t7e
  29. http://www.esa.int/our_activities/space_science/herschel/how_many_stars_are_there_in_the_universe
  30. https://hackernoon.com/media/a9880564aa19841fee19779657053a41?postid=632a4d1fbad2
  31. https://en.wikipedia.org/wiki/eniac
  32. http://img04.deviantart.net/73c7/i/2011/249/1/f/your_mom_by_khelsiieexkhaotika-d4943v7.jpg
  33. http://blogs.smithsonianmag.com/smartnews/2013/10/computer-programming-used-to-be-womens-work/
  34. https://en.wikipedia.org/wiki/id88
  35. http://blogs.umass.edu/brain-wars/the-debates/minsky-vs-rosenblatt/
  36. https://en.wikipedia.org/wiki/activation_function
  37. https://www.youtube.com/watch?v=ozg8m_lda1m
  38. https://en.wikipedia.org/wiki/activation_function
  39. https://en.wikipedia.org/wiki/linear_classifier
  40. https://rosenblattid88.herokuapp.com/
  41. https://rosenblattid88.herokuapp.com/
  42. https://en.wikipedia.org/wiki/id88#multiclass_id88
  43. https://www.cs.cmu.edu/~dst/pubs/byte-hiddenlayer-1989.pdf
  44. http://stackoverflow.com/questions/16609310/in-neural-networks-does-a-bias-change-the-threshold-of-an-activation-function
  45. https://rosenblattid88.herokuapp.com/
  46. https://en.wikipedia.org/wiki/delta_rule
  47. https://i.ytimg.com/vi/qhuhzxywifa/hqdefault.jpg
  48. https://hackernoon.com/media/75278313c6842ccf3ab9c26450b277c0?postid=632a4d1fbad2
  49. https://github.com/elyx0/rosenblattid88js/blob/master/src/id88.js
  50. https://en.wikipedia.org/wiki/bohr   einstein_debates
  51. https://en.wikipedia.org/wiki/warren_sturgis_mcculloch#biography
  52. http://bit.ly/hackernoonfb
  53. https://goo.gl/k7xybx
  54. https://goo.gl/4ofytp
  55. http://bit.ly/hackernoon
  56. http://bit.ly/atamiatami
  57. http://bit.ly/hackernoonsubmission
  58. mailto:partners@amipublications.com
  59. http://bit.ly/hackernoonlatestt
  60. https://hackernoon.com/trending
  61. https://hackernoon.com/tagged/machine-learning?source=post
  62. https://hackernoon.com/tagged/neural-networks?source=post
  63. https://hackernoon.com/tagged/javascript?source=post
  64. https://hackernoon.com/tagged/artificial-intelligence?source=post
  65. https://hackernoon.com/tagged/bots?source=post
  66. https://hackernoon.com/@elyx0?source=footer_card
  67. https://hackernoon.com/@elyx0
  68. https://hackernoon.com/?source=footer_card
  69. https://hackernoon.com/?source=footer_card
  70. https://hackernoon.com/
  71. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  73. https://medium.com/p/632a4d1fbad2/share/twitter
  74. https://medium.com/p/632a4d1fbad2/share/facebook
  75. https://medium.com/p/632a4d1fbad2/share/twitter
  76. https://medium.com/p/632a4d1fbad2/share/facebook
