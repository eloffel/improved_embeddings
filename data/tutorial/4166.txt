   #[1]tensorflow

   (button)
   [2]tensorflow
     *

   [3]install [4]learn
     * [5]introduction
       new to tensorflow?
     * [6]tensorflow
       the core open source ml library
     * [7]for javascript
       tensorflow.js for ml using javascript
     * [8]for mobile & iot
       tensorflow lite for mobile and embedded devices
     * [9]for production
       tensorflow extended for end-to-end ml components
     * [10]swift for tensorflow (in beta)

   [11]api
     * api r1
     * [12]r1.13 (stable)
     * [13]r1.12
     * [14]r1.11
     * [15]r1.10
     * [16]r1.9
     * [17]more   

     * api r2
     * [18]r2.0 (preview)

   [19]resources
     * [20]models & datasets
       pre-trained models and datasets built by google and the community
     * [21]tools
       ecosystem of tools to help you use tensorflow
     * [22]libraries & extensions
       libraries and extensions built on tensorflow

   [23]community [24]why tensorflow
     * [25]about
     * [26]case studies

   ____________________
   (button)
   (button)
   [27]github
     * [28]tensorflow core

   [29]overview [30]tutorials [31]guide [32]tf 2.0 alpha

   (button)
     * [33]install
     * [34]learn
          + more
          + [35]overview
          + [36]tutorials
          + [37]guide
          + [38]tf 2.0 alpha
     * [39]api
          + more
     * [40]resources
          + more
     * [41]community
     * [42]why tensorflow
          + more
     * [43]github

     * [44]get started with tensorflow
     * learn and use ml
          + [45]overview
          + [46]basic classification
          + [47]text classification
          + [48]regression
          + [49]overfitting and underfitting
          + [50]save and restore models
     * research and experimentation
          + [51]overview
          + [52]eager execution
          + [53]automatic differentiation
          + [54]custom training: basics
          + [55]custom layers
          + [56]custom training: walkthrough
     * ml at production scale
          + [57]linear model with estimators
          + [58]wide and deep learning
          + [59]boosted trees
          + [60]boosted trees model understanding
          + [61]build a id98 using estimators
     * generative models
          + [62]translation with attention
          + [63]image captioning
          + [64]dcgan
          + [65]vae
     * images
          + [66]image recognition
          + [67]pix2pix
          + [68]neural style transfer
          + [69]image segmentation
          + [70]advanced id98
     * sequences
          + [71]text generation with an id56
          + [72]recurrent neural network
          + [73]drawing classification
          + [74]simple audio recognition
          + [75]id4
     * load data
          + [76]load images
          + [77]tfrecords and tf.example
     * data representation
          + [78]vector representations of words
          + [79]kernel methods
          + [80]large-scale linear models
          + [81]unicode
     * non-ml
          + [82]mandelbrot set
          + [83]partial differential equations

     * [84]introduction
     * [85]tensorflow
     * [86]for javascript
     * [87]for mobile & iot
     * [88]for production
     * [89]swift for tensorflow (in beta)

     * api r1
     * [90]r1.13 (stable)
     * [91]r1.12
     * [92]r1.11
     * [93]r1.10
     * [94]r1.9
     * [95]more   
     * api r2
     * [96]r2.0 (preview)

     * [97]models & datasets
     * [98]tools
     * [99]libraries & extensions

     * [100]about
     * [101]case studies

   watch talks from the 2019 tensorflow dev summit [102]watch now
     * [103]tensorflow
     * [104]learn
     * [105]tensorflow core
     * [106]tutorials

improving linear models using explicit kernel methods

   note: this document uses a deprecated version of [107]tf.estimator,
   [108]tf.contrib.learn.estimator, which has a different interface. it
   also uses other contrib methods whose [109]api may not be stable.

   in this tutorial, we demonstrate how combining (explicit) kernel
   methods with linear models can drastically increase the latters'
   quality of predictions without significantly increasing training and
   id136 times. unlike dual kernel methods, explicit (primal) kernel
   methods scale well with the size of the training dataset both in terms
   of training/id136 times and in terms of memory requirements.

   intended audience: even though we provide a high-level overview of
   concepts related to explicit kernel methods, this tutorial primarily
   targets readers who already have at least basic knowledge of kernel
   methods and support vector machines (id166s). if you are new to kernel
   methods, refer to either of the following sources for an introduction:
     * if you have a strong mathematical background: [110]kernel methods
       in machine learning
     * [111]kernel method wikipedia page

   currently, tensorflow supports explicit kernel mappings for dense
   features only; tensorflow will provide support for sparse features at a
   later release.

   this tutorial uses [112]tf.contrib.learn (tensorflow's high-level
   machine learning api) estimators for our ml models. if you are not
   familiar with this api, the [113]estimator guide is a good place to
   start. we will use the mnist dataset. the tutorial consists of the
   following steps:
     * load and prepare mnist data for classification.
     * construct a simple linear model, train it, and evaluate it on the
       eval data.
     * replace the linear model with a kernelized linear model, re-train,
       and re-evaluate.

load and prepare mnist data for classification

   run the following utility command to load the mnist dataset:
data = tf.contrib.learn.datasets.mnist.load_mnist()

   the preceding method loads the entire mnist dataset (containing 70k
   samples) and splits it into train, validation, and test data with 55k,
   5k, and 10k samples respectively. each split contains one numpy array
   for images (with shape [sample_size, 784]) and one for labels (with
   shape [sample_size, 1]). in this tutorial, we only use the train and
   validation splits to train and evaluate our models respectively.

   in order to feed data to a tf.contrib.learn estimator, it is helpful to
   convert it to tensors. for this, we will use an input function which
   adds ops to the tensorflow graph that, when executed, create
   mini-batches of tensors to be used downstream. for more background on
   input functions, check [114]this section on input functions. in this
   example, we will use the [115]tf.train.shuffle_batch op which, besides
   converting numpy arrays to tensors, allows us to specify the batch_size
   and whether to randomize the input every time the input_fn ops are
   executed (randomization typically expedites convergence during
   training). the full code for loading and preparing the data is shown in
   the snippet below. in this example, we use mini-batches of size 256 for
   training and the entire sample (5k entries) for evaluation. feel free
   to experiment with different batch sizes.
import numpy as np
import tensorflow as tf

def get_input_fn(dataset_split, batch_size, capacity=10000, min_after_dequeue=30
00):

  def _input_fn():
    images_batch, labels_batch = tf.train.shuffle_batch(
        tensors=[dataset_split.images, dataset_split.labels.astype(np.int32)],
        batch_size=batch_size,
        capacity=capacity,
        min_after_dequeue=min_after_dequeue,
        enqueue_many=true,
        num_threads=4)
    features_map = {'images': images_batch}
    return features_map, labels_batch

  return _input_fn

data = tf.contrib.learn.datasets.mnist.load_mnist()

train_input_fn = get_input_fn(data.train, batch_size=256)
eval_input_fn = get_input_fn(data.validation, batch_size=5000)


training a simple linear model

   we can now train a linear model over the mnist dataset. we will use the
   [116]tf.contrib.learn.linearclassifier estimator with 10 classes
   representing the 10 digits. the input features form a 784-dimensional
   dense vector which can be specified as follows:
image_column = tf.contrib.layers.real_valued_column('images', dimension=784)

   the full code for constructing, training and evaluating a
   linearclassifier estimator is as follows:
import time

# specify the feature(s) to be used by the estimator.
image_column = tf.contrib.layers.real_valued_column('images', dimension=784)
estimator = tf.contrib.learn.linearclassifier(feature_columns=[image_column], n_
classes=10)

# train.
start = time.time()
estimator.fit(input_fn=train_input_fn, steps=2000)
end = time.time()
print('elapsed time: {} seconds'.format(end - start))

# evaluate and report metrics.
eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)
print(eval_metrics)

   the following table summarizes the results on the eval data.
      metric               value
   loss          0.25 to 0.30
   accuracy      92.5%
   training time ~25 seconds on my machine

   note: metrics will vary depending on various factors.

   in addition to experimenting with the (training) batch size and the
   number of training steps, there are a couple other parameters that can
   be tuned as well. for instance, you can change the optimization method
   used to minimize the loss by explicitly selecting another optimizer
   from the collection of [117]available optimizers. as an example, the
   following code constructs a linearclassifier estimator that uses the
   follow-the-regularized-leader (ftrl) optimization strategy with a
   specific learning rate and l2-id173.
optimizer = tf.train.ftrloptimizer(learning_rate=5.0, l2_id173_strength
=1.0)
estimator = tf.contrib.learn.linearclassifier(
    feature_columns=[image_column], n_classes=10, optimizer=optimizer)

   regardless of the values of the parameters, the maximum accuracy a
   linear model can achieve on this dataset caps at around 93%.

using explicit kernel mappings with the linear model.

   the relatively high error (~7%) of the linear model over mnist
   indicates that the input data is not linearly separable. we will use
   explicit kernel mappings to reduce the classification error.

   intuition: the high-level idea is to use a non-linear map to transform
   the input space to another feature space (of possibly higher dimension)
   where the (transformed) features are (almost) linearly separable and
   then apply a linear model on the mapped features. this is shown in the
   following figure:
   [kernel_mapping.png]

technical details

   in this example we will use random fourier features, introduced in the
   [118]"random features for large-scale kernel machines" paper by rahimi
   and recht, to map the input data. random fourier features map a vector
   \(\mathbf{x} \in \mathbb{r}^d\) to \(\mathbf{x'} \in \mathbb{r}^d\) via
   the following mapping:
   $$ rffm(\cdot): \mathbb{r}^d \to \mathbb{r}^d, \quad rffm(\mathbf{x}) =
   \cos(\mathbf{\omega} \cdot \mathbf{x}+ \mathbf{b}) $$

   where \(\mathbf{\omega} \in \mathbb{r}^{d \times d}\), \(\mathbf{x} \in
   \mathbb{r}^d,\) \(\mathbf{b} \in \mathbb{r}^d\) and the cosine is
   applied element-wise.

   in this example, the entries of \(\mathbf{\omega}\) and \(\mathbf{b}\)
   are sampled from distributions such that the mapping satisfies the
   following property:
   $$ rffm(\mathbf{x})^t \cdot rffm(\mathbf{y}) \approx
   e^{-\frac{\|\mathbf{x} - \mathbf{y}\|^2}{2 \sigma^2}} $$

   the right-hand-side quantity of the expression above is known as the
   rbf (or gaussian) id81. this function is one of the
   most-widely used id81s in machine learning and implicitly
   measures similarity in a different, much higher dimensional space than
   the original one. see [119]radial basis function kernel for more
   details.

kernel classifier

   [120]tf.contrib.kernel_methods.kernellinearclassifier is a pre-packaged
   [121]tf.contrib.learn estimator that combines the power of explicit
   kernel mappings with linear models. its constructor is almost identical
   to that of the linearclassifier estimator with the additional option to
   specify a list of explicit kernel mappings to be applied to each
   feature the classifier uses. the following code snippet demonstrates
   how to replace linearclassifier with kernellinearclassifier.
# specify the feature(s) to be used by the estimator. this is identical to the
# code used for the linearclassifier.
image_column = tf.contrib.layers.real_valued_column('images', dimension=784)
optimizer = tf.train.ftrloptimizer(
   learning_rate=50.0, l2_id173_strength=0.001)


kernel_mapper = tf.contrib.kernel_methods.randomfourierfeaturemapper(
  input_dim=784, output_dim=2000, stddev=5.0, name='rffm')
kernel_mappers = {image_column: [kernel_mapper]}
estimator = tf.contrib.kernel_methods.kernellinearclassifier(
   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)

# train.
start = time.time()
estimator.fit(input_fn=train_input_fn, steps=2000)
end = time.time()
print('elapsed time: {} seconds'.format(end - start))

# evaluate and report metrics.
eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=1)
print(eval_metrics)

   the only additional parameter passed to kernellinearclassifier is a
   dictionary from feature_columns to a list of kernel mappings to be
   applied to the corresponding feature column. the following lines
   instruct the classifier to first map the initial 784-dimensional images
   to 2000-dimensional vectors using random fourier features and then
   learn a linear model on the transformed vectors:
kernel_mapper = tf.contrib.kernel_methods.randomfourierfeaturemapper(
  input_dim=784, output_dim=2000, stddev=5.0, name='rffm')
kernel_mappers = {image_column: [kernel_mapper]}
estimator = tf.contrib.kernel_methods.kernellinearclassifier(
   n_classes=10, optimizer=optimizer, kernel_mappers=kernel_mappers)

   notice the stddev parameter. this is the standard deviation
   (\(\sigma\)) of the approximated rbf kernel and controls the similarity
   measure used in classification. stddev is typically determined via
   hyperparameter tuning.

   the results of running the preceding code are summarized in the
   following table. we can further increase the accuracy by increasing the
   output dimension of the mapping and tuning the standard deviation.
      metric               value
   loss          0.10
   accuracy      97%
   training time ~35 seconds on my machine

stddev

   the classification quality is very sensitive to the value of stddev.
   the following table shows the accuracy of the classifier on the eval
   data for different values of stddev. the optimal value is stddev=5.0.
   notice how too small or too high stddev values can dramatically
   decrease the accuracy of the classification.
   stddev eval accuracy
   1.0    0.1362
   2.0    0.4764
   4.0    0.9654
   5.0    0.9766
   8.0    0.9714
   16.0   0.8878

output dimension

   intuitively, the larger the output dimension of the mapping, the closer
   the inner product of two mapped vectors approximates the kernel, which
   typically translates to better classification accuracy. another way to
   think about this is that the output dimension equals the number of
   weights of the linear model; the larger this dimension, the larger the
   "degrees of freedom" of the model. however, after a certain threshold,
   higher output dimensions increase the accuracy by very little, while
   making training take more time. this is shown in the following two
   figures which depict the eval accuracy as a function of the output
   dimension and the training time, respectively.

   image image

summary

   explicit kernel mappings combine the predictive power of nonlinear
   models with the scalability of linear models. unlike traditional dual
   kernel methods, explicit kernel methods can scale to millions or
   hundreds of millions of samples. when using explicit kernel mappings,
   consider the following tips:
     * random fourier features can be particularly effective for datasets
       with dense features.
     * the parameters of the kernel mapping are often data-dependent.
       model quality can be very sensitive to these parameters. use
       hyperparameter tuning to find the optimal values.
     * if you have multiple numerical features, concatenate them into a
       single multi-dimensional feature and apply the kernel mapping to
       the concatenated vector.

   except as otherwise noted, the content of this page is licensed under
   the [122]creative commons attribution 3.0 license, and code samples are
   licensed under the [123]apache 2.0 license. for details, see the
   [124]google developers site policies. java is a registered trademark of
   oracle and/or its affiliates.

     * stay connected
          + [125]blog
          + [126]github
          + [127]twitter
          + [128]youtube
     * support
          + [129]issue tracker
          + [130]release notes
          + [131]stack overflow
          + [132]brand guidelines

     * [133]terms
     * [134]privacy

   [english_____]

references

   visible links
   1. https://www.tensorflow.org/s/opensearch.xml
   2. https://www.tensorflow.org/
   3. https://www.tensorflow.org/install
   4. https://www.tensorflow.org/learn
   5. https://www.tensorflow.org/learn
   6. https://www.tensorflow.org/overview
   7. https://www.tensorflow.org/js
   8. https://www.tensorflow.org/lite
   9. https://www.tensorflow.org/tfx
  10. https://www.tensorflow.org/swift
  11. https://www.tensorflow.org/api_docs/python/tf
  12. https://www.tensorflow.org/api_docs/python/tf
  13. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  14. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  15. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  16. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  17. https://www.tensorflow.org/versions
  18. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  19. https://www.tensorflow.org/resources/models-datasets
  20. https://www.tensorflow.org/resources/models-datasets
  21. https://www.tensorflow.org/resources/tools
  22. https://www.tensorflow.org/resources/libraries-extensions
  23. https://www.tensorflow.org/community
  24. https://www.tensorflow.org/about
  25. https://www.tensorflow.org/about
  26. https://www.tensorflow.org/about/case-studies
  27. https://github.com/tensorflow
  28. https://www.tensorflow.org/overview
  29. https://www.tensorflow.org/overview
  30. https://www.tensorflow.org/tutorials
  31. https://www.tensorflow.org/guide
  32. https://www.tensorflow.org/alpha
  33. https://www.tensorflow.org/install
  34. https://www.tensorflow.org/learn
  35. https://www.tensorflow.org/overview
  36. https://www.tensorflow.org/tutorials
  37. https://www.tensorflow.org/guide
  38. https://www.tensorflow.org/alpha
  39. https://www.tensorflow.org/api_docs/python/tf
  40. https://www.tensorflow.org/resources/models-datasets
  41. https://www.tensorflow.org/community
  42. https://www.tensorflow.org/about
  43. https://github.com/tensorflow
  44. https://www.tensorflow.org/tutorials
  45. https://www.tensorflow.org/tutorials/keras
  46. https://www.tensorflow.org/tutorials/keras/basic_classification
  47. https://www.tensorflow.org/tutorials/keras/basic_text_classification
  48. https://www.tensorflow.org/tutorials/keras/basic_regression
  49. https://www.tensorflow.org/tutorials/keras/overfit_and_underfit
  50. https://www.tensorflow.org/tutorials/keras/save_and_restore_models
  51. https://www.tensorflow.org/tutorials/eager
  52. https://www.tensorflow.org/tutorials/eager/eager_basics
  53. https://www.tensorflow.org/tutorials/eager/automatic_differentiation
  54. https://www.tensorflow.org/tutorials/eager/custom_training
  55. https://www.tensorflow.org/tutorials/eager/custom_layers
  56. https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough
  57. https://www.tensorflow.org/tutorials/estimators/linear
  58. https://github.com/tensorflow/models/tree/master/official/wide_deep
  59. https://www.tensorflow.org/tutorials/estimators/boosted_trees
  60. https://www.tensorflow.org/tutorials/estimators/boosted_trees_model_understanding
  61. https://www.tensorflow.org/tutorials/estimators/id98
  62. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/id4_with_attention/id4_with_attention.ipynb
  63. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/image_captioning_with_attention.ipynb
  64. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/dcgan.ipynb
  65. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/generative_examples/cvae.ipynb
  66. https://www.tensorflow.org/tutorials/images/hub_with_keras
  67. https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb
  68. https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_neural_style_transfer_with_eager_execution.ipynb
  69. https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb
  70. https://www.tensorflow.org/tutorials/images/deep_id98
  71. https://www.tensorflow.org/tutorials/sequences/text_generation
  72. https://www.tensorflow.org/tutorials/sequences/recurrent
  73. https://www.tensorflow.org/tutorials/sequences/recurrent_quickdraw
  74. https://www.tensorflow.org/tutorials/sequences/audio_recognition
  75. https://github.com/tensorflow/id4
  76. https://www.tensorflow.org/tutorials/load_data/images
  77. https://www.tensorflow.org/tutorials/load_data/tf_records
  78. https://www.tensorflow.org/tutorials/representation/id97
  79. https://www.tensorflow.org/tutorials/representation/kernel_methods
  80. https://www.tensorflow.org/tutorials/representation/linear
  81. https://www.tensorflow.org/tutorials/representation/unicode
  82. https://www.tensorflow.org/tutorials/non-ml/mandelbrot
  83. https://www.tensorflow.org/tutorials/non-ml/pdes
  84. https://www.tensorflow.org/learn
  85. https://www.tensorflow.org/overview
  86. https://www.tensorflow.org/js
  87. https://www.tensorflow.org/lite
  88. https://www.tensorflow.org/tfx
  89. https://www.tensorflow.org/swift
  90. https://www.tensorflow.org/api_docs/python/tf
  91. https://www.tensorflow.org/versions/r1.12/api_docs/python/tf
  92. https://www.tensorflow.org/versions/r1.11/api_docs/python/tf
  93. https://www.tensorflow.org/versions/r1.10/api_docs/python/tf
  94. https://www.tensorflow.org/versions/r1.9/api_docs/python/tf
  95. https://www.tensorflow.org/versions
  96. https://www.tensorflow.org/versions/r2.0/api_docs/python/tf
  97. https://www.tensorflow.org/resources/models-datasets
  98. https://www.tensorflow.org/resources/tools
  99. https://www.tensorflow.org/resources/libraries-extensions
 100. https://www.tensorflow.org/about
 101. https://www.tensorflow.org/about/case-studies
 102. https://www.youtube.com/playlist?list=plqy2h8rroyvzouyi26khmksjbedn3squb
 103. https://www.tensorflow.org/
 104. https://www.tensorflow.org/learn
 105. https://www.tensorflow.org/overview
 106. https://www.tensorflow.org/tutorials
 107. https://www.tensorflow.org/api_docs/python/tf/estimator
 108. https://www.tensorflow.org/api_docs/python/tf/contrib/learn/estimator
 109. https://www.tensorflow.org/guide/version_compat#not_covered
 110. https://arxiv.org/pdf/math/0701907.pdf
 111. https://en.wikipedia.org/wiki/kernel_method
 112. https://www.tensorflow.org/code/tensorflow/contrib/learn/python/learn
 113. https://www.tensorflow.org/guide/estimators
 114. https://www.tensorflow.org/guide/premade_estimators#create_input_functions
 115. https://www.tensorflow.org/api_docs/python/tf/train/shuffle_batch
 116. https://www.tensorflow.org/api_docs/python/tf/contrib/learn/linearclassifier
 117. https://www.tensorflow.org/code/tensorflow/python/training
 118. https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf
 119. https://en.wikipedia.org/wiki/radial_basis_function_kernel
 120. https://www.tensorflow.org/api_docs/python/tf/contrib/kernel_methods/kernellinearclassifier
 121. https://www.tensorflow.org/api_docs/python/tf/contrib/learn
 122. https://creativecommons.org/licenses/by/3.0/
 123. https://www.apache.org/licenses/license-2.0
 124. https://developers.google.com/site-policies
 125. https://medium.com/tensorflow
 126. https://github.com/tensorflow/
 127. https://twitter.com/tensorflow
 128. https://youtube.com/tensorflow
 129. https://github.com/tensorflow/tensorflow/issues
 130. https://github.com/tensorflow/tensorflow/blob/master/release.md
 131. https://stackoverflow.com/questions/tagged/tensorflow
 132. https://www.tensorflow.org/extras/tensorflow_brand_guidelines.pdf
 133. https://policies.google.com/terms
 134. https://policies.google.com/privacy

   hidden links:
 136. https://www.tensorflow.org/tutorials/representation/kernel_methods
 137. https://www.tensorflow.org/tutorials/representation/kernel_methods
 138. https://www.tensorflow.org/tutorials/representation/kernel_methods
 139. https://www.tensorflow.org/tutorials/representation/kernel_methods
