    #[1]index [2]search [3]id202 [4]glossary

   [5]ml cheatsheet
   ____________________

   basics
     * [6]id75
          + [7]introduction
          + [8]simple regression
               o [9]making predictions
               o [10]cost function
               o [11]id119
               o [12]training
               o [13]model evaluation
               o [14]summary
          + [15]multivariable regression
               o [16]growing complexity
               o [17]id172
               o [18]making predictions
               o [19]initialize weights
               o [20]cost function
               o [21]id119
               o [22]simplifying with matrices
               o [23]bias term
               o [24]model evaluation
     * [25]id119
          + [26]introduction
          + [27]learning rate
          + [28]cost function
          + [29]step-by-step
     * [30]id28
          + [31]introduction
               o [32]comparison to id75
               o [33]types of id28
          + [34]binary id28
               o [35]sigmoid activation
               o [36]decision boundary
               o [37]making predictions
               o [38]cost function
               o [39]id119
               o [40]mapping probabilities to classes
               o [41]training
               o [42]model evaluation
          + [43]multiclass id28
               o [44]procedure
               o [45]softmax activation
               o [46]scikit-learn example
     * [47]glossary

   math
     * [48]calculus
          + [49]introduction
          + [50]derivatives
               o [51]geometric definition
               o [52]taking the derivative
               o [53]step-by-step
               o [54]machine learning use cases
          + [55]chain rule
               o [56]how it works
               o [57]step-by-step
               o [58]multiple functions
          + [59]gradients
               o [60]partial derivatives
               o [61]step-by-step
               o [62]directional derivatives
               o [63]useful properties
          + [64]integrals
               o [65]computing integrals
               o [66]applications of integration
                    # [67]computing probabilities
                    # [68]expected value
                    # [69]variance
     * [70]id202
          + [71]vectors
               o [72]notation
               o [73]vectors in geometry
               o [74]scalar operations
               o [75]elementwise operations
               o [76]dot product
               o [77]hadamard product
               o [78]vector fields
          + [79]matrices
               o [80]dimensions
               o [81]scalar operations
               o [82]elementwise operations
               o [83]hadamard product
               o [84]matrix transpose
               o [85]id127
               o [86]test yourself
          + [87]numpy
               o [88]dot product
               o [89]broadcasting
     * [90]id203 (todo)
          + [91]links
          + [92]screenshots
          + [93]license
     * [94]statistics (todo)
     * [95]notation
          + [96]algebra
          + [97]calculus
          + [98]id202
          + [99]id203
          + [100]set theory
          + [101]statistics

   neural networks
     * [102]concepts
          + [103]neural network
          + [104]neuron
          + [105]synapse
          + [106]weights
          + [107]bias
          + [108]layers
          + [109]weighted input
          + [110]id180
          + [111]id168s
          + [112]optimization algorithms
     * [113]forwardpropagation
          + [114]simple network
               o [115]steps
               o [116]code
          + [117]larger network
               o [118]architecture
               o [119]weight initialization
               o [120]bias terms
               o [121]working with matrices
               o [122]dynamic resizing
               o [123]refactoring our code
               o [124]final result
     * [125]id26
          + [126]chain rule refresher
          + [127]applying the chain rule
          + [128]saving work with memoization
          + [129]code example
     * [130]id180
          + [131]linear
          + [132]elu
          + [133]relu
          + [134]leakyrelu
          + [135]sigmoid
          + [136]tanh
          + [137]softmax
     * [138]layers
          + [139]batchnorm
          + [140]convolution
          + [141]dropout
          + [142]linear
          + [143]lstm
          + [144]pooling
          + [145]id56
     * [146]id168s
          + [147]cross-id178
          + [148]hinge
          + [149]huber
          + [150]kullback-leibler
          + [151]mae (l1)
          + [152]mse (l2)
     * [153]optimizers
          + [154]adadelta
          + [155]adagrad
          + [156]adam
          + [157]conjugate gradients
          + [158]bfgs
          + [159]momentum
          + [160]nesterov momentum
          + [161]newton   s method
          + [162]rmsprop
          + [163]sgd
     * [164]id173
          + [165]data augmentation
          + [166]dropout
          + [167]early stopping
          + [168]ensembling
          + [169]injecting noise
          + [170]l1 id173
          + [171]l2 id173
     * [172]architectures
          + [173]autoencoder
          + [174]id98
          + [175]gan
          + [176]mlp
          + [177]id56
          + [178]vae

   algorithms (todo)
     * [179]classification
          + [180]bayesian
          + [181]boosting
          + [182]id90
          + [183]k-nearest neighbor
          + [184]id28
          + [185]id79s
          + [186]support vector machines
     * [187]id91
          + [188]centroid
          + [189]density
          + [190]distribution
          + [191]hierarchical
          + [192]id116
          + [193]mean shift
     * [194]regression
          + [195]lasso
          + [196]linear
          + [197]ordinary least squares
          + [198]polynomial
          + [199]ridge
          + [200]splines
          + [201]stepwise
     * [202]id23

   resources
     * [203]datasets
     * [204]libraries
     * [205]papers
     * [206]other

   contributing
     * [207]how to contribute

   [208]ml cheatsheet
     * [209]docs   
     * calculus
     * [210]edit on github
     __________________________________________________________________

calculus[211]  

     * [212]introduction
     * [213]derivatives
          + [214]geometric definition
          + [215]taking the derivative
          + [216]step-by-step
          + [217]machine learning use cases
     * [218]chain rule
          + [219]how it works
          + [220]step-by-step
          + [221]multiple functions
     * [222]gradients
          + [223]partial derivatives
          + [224]step-by-step
          + [225]directional derivatives
          + [226]useful properties
     * [227]integrals
          + [228]computing integrals
          + [229]applications of integration
               o [230]computing probabilities
               o [231]expected value
               o [232]variance

[233]introduction[234]  

   you need to know some basic calculus in order to understand how
   functions change over time (derivatives), and to calculate the total
   amount of a quantity that accumulates over a time period (integrals).
   the language of calculus will allow you to speak precisely about the
   properties of functions and better understand their behaviour.

   normally taking a calculus course involves doing lots of tedious
   calculations by hand, but having the power of computers on your side
   can make the process much more fun. this section describes the key
   ideas of calculus which you   ll need to know to understand machine
   learning concepts.

[235]derivatives[236]  

   a derivative can be defined in two ways:

    1. instantaneous rate of change (physics)
    2. slope of a line at a specific point (geometry)

   both represent the same principle, but for our purposes it   s easier to
   explain using the geometric definition.

[237]geometric definition[238]  

   in geometry slope represents the steepness of a line. it answers the
   question: how much does \(y\) or \(f(x)\) change given a specific
   change in \(x\)?
   _images/slope_formula.png

   using this definition we can easily calculate the slope between two
   points. but what if i asked you, instead of the slope between two
   points, what is the slope at a single point on the line? in this case
   there isn   t any obvious    rise-over-run    to calculate. derivatives help
   us answer this question.

   a derivative outputs an expression we can use to calculate the
   instantaneous rate of change, or slope, at a single point on a line.
   after solving for the derivative you can use it to calculate the slope
   at every other point on the line.

[239]taking the derivative[240]  

   consider the graph below, where \(f(x) = x^2 + 3\).
   _images/calculus_slope_intro.png

   the slope between (1,4) and (3,12) would be:
   \[slope = \frac{y2-y1}{x2-x1} = \frac{12-4}{3-1} = 4\]

   but how do we calculate the slope at point (1,4) to reveal the change
   in slope at that specific point?

   one way would be to find the two nearest points, calculate their slopes
   relative to \(x\) and take the average. but calculus provides an
   easier, more precise way: compute the derivative. computing the
   derivative of a function is essentially the same as our original
   proposal, but instead of finding the two closest points, we make up an
   imaginary point an infinitesimally small distance away from \(x\) and
   compute the slope between \(x\) and the new point.

   in this way, derivatives help us answer the question: how does \(f(x)\)
   change if we make a very very tiny increase to x? in other words,
   derivatives help estimate the slope between two points that are an
   infinitesimally small distance away from each other. a very, very, very
   small distance, but large enough to calculate the slope.

   in math language we represent this infinitesimally small increase using
   a limit. a limit is defined as the output value a function approaches
   as the input value approaches another value. in our case the target
   value is the specific point at which we want to calculate slope.

[241]step-by-step[242]  

   calculating the derivative is the same as calculating normal slope,
   however in this case we calculate the slope between our point and a
   point infinitesimally close to it. we use the variable \(h\) to
   represent this infinitesimally distance. here are the steps:
    1. given the function:

   \[f(x) = x^2\]
    2. increment \(x\) by a very small value \(h (h =   x)\)

   \[f(x + h) = (x + h)^2\]
    3. apply the slope formula

   \[\frac{f(x + h) - f(x)}{h}\]
    4. simplify the equation

   \[ \begin{align}\begin{aligned}\begin{split}\frac{x^2 + 2xh + h^2 -
   x^2}{h} \\\end{split}\\\frac{2xh+h^2}{h} = 2x+h\end{aligned}\end{align}
   \]
    5. set \(h\) to 0 (the limit as \(h\) heads toward 0)

   \[{2x + 0} = {2x}\]

   so what does this mean? it means for the function \(f(x) = x^2\), the
   slope at any point equals \(2x\). the formula is defined as:
   \[\lim_{h\to0}\frac{f(x+h) - f(x)}{h}\]

   code

   let   s write code to calculate the derivative of any function \(f(x)\).
   we test our function works as expected on the input \(f(x)=x^2\)
   producing a value close to the actual derivative \(2x\).
def get_derivative(func, x):
    """compute the derivative of `func` at the location `x`."""
    h = 0.0001                          # step size
    return (func(x+h) - func(x)) / h    # rise-over-run

def f(x): return x**2                   # some test function f(x)=x^2
x = 3                                   # the location of interest
computed = get_derivative(f, x)
actual = 2*x

computed, actual   # = 6.0001, 6        # pretty close if you ask me...

   in general it   s preferable to use the math to obtain exact derivative
   formulas, but keep in mind you can always compute derivatives
   numerically by computing the rise-over-run for a    small step    \(h\).

[243]machine learning use cases[244]  

   machine learning uses derivatives in optimization problems.
   optimization algorithms like id119 use derivatives to decide
   whether to increase or decrease weights in order to maximize or
   minimize some objective (e.g. a model   s accuracy or error functions).
   derivatives also help us approximate nonlinear functions as linear
   functions (tangent lines), which have constant slopes. with a constant
   slope we can decide whether to move up or down the slope (increase or
   decrease our weights) to get closer to the target value (class label).

[245]chain rule[246]  

   the chain rule is a formula for calculating the derivatives of
   composite functions. composite functions are functions composed of
   functions inside other function(s).

[247]how it works[248]  

   given a composite function \(f(x) = a(b(x))\), the derivative of
   \(f(x)\) equals the product of the derivative of \(a\) with respect to
   \(b(x)\) and the derivative of \(b\) with respect to \(x\).
   \[\mbox{composite function derivative} = \mbox{outer function
   derivative} * \mbox{inner function derivative}\]

   for example, given a composite function \(f(x)\), where:
   \[f(x) = h(g(x))\]

   the chain rule tells us that the derivative of \(f(x)\) equals:
   \[\frac{df}{dx} = \frac{dh}{dg} \cdot \frac{dg}{dx}\]

[249]step-by-step[250]  

   say \(f(x)\) is composed of two functions \(h(x) = x^3\) and \(g(x) =
   x^2\). and that:
   \[\begin{split}\begin{align} f(x) &= h(g(x)) \\ &= (x^2)^3 \\
   \end{align}\end{split}\]

   the derivative of \(f(x)\) would equal:
   \[\begin{split}\begin{align} \frac{df}{dx} &= \frac{dh}{dg}
   \frac{dg}{dx} \\ &= \frac{dh}{d(x^2)} \frac{dg}{dx}
   \end{align}\end{split}\]

   steps
    1. solve for the inner derivative of \(g(x) = x^2\)

   \[\frac{dg}{dx} = 2x\]
    2. solve for the outer derivative of \(h(x) = x^3\), using a
       placeholder \(b\) to represent the inner function \(x^2\)

   \[\frac{dh}{db} = 3b^2\]
    3. swap out the placeholder variable (b) for the inner function (g(x))

   \[\begin{split}\begin{gathered} 3(x^2)^2 \\ 3x^4
   \end{gathered}\end{split}\]
    4. return the product of the two derivatives

   \[3x^4 \cdot 2x = 6x^5\]

[251]multiple functions[252]  

   in the above example we assumed a composite function containing a
   single inner function. but the chain rule can also be applied to
   higher-order functions like:
   \[f(x) = a(b(c(x)))\]

   the chain rule tells us that the derivative of this function equals:
   \[\frac{df}{dx} = \frac{da}{db} \frac{db}{dc} \frac{dc}{dx}\]

   we can also write this derivative equation \(f'\) notation:
   \[f' = a'(b(c(x)) \cdot b'(c(x)) \cdot c'(x)\]

   steps

   given the function \(f(x) = a(b(c(x)))\), lets assume:
   \[\begin{split}\begin{align} a(x) & = sin(x) \\ b(x) & = x^2 \\ c(x) &
   = 4x \end{align}\end{split}\]

   the derivatives of these functions would be:
   \[\begin{split}\begin{align} a'(x) &= cos(x) \\ b'(x) &= 2x \\ c'(x) &=
   4 \end{align}\end{split}\]

   we can calculate the derivative of \(f(x)\) using the following
   formula:
   \[f'(x) = a'( (4x)^2) \cdot b'(4x) \cdot c'(x)\]

   we then input the derivatives and simplify the expression:
   \[\begin{split}\begin{align} f'(x) &= cos((4x)^2) \cdot 2(4x) \cdot 4
   \\ &= cos(16x^2) \cdot 8x \cdot 4 \\ &= cos(16x^2)32x
   \end{align}\end{split}\]

[253]gradients[254]  

   a gradient is a vector that stores the partial derivatives of
   multivariable functions. it helps us calculate the slope at a specific
   point on a curve for functions with multiple independent variables. in
   order to calculate this more complex slope, we need to isolate each
   variable to determine how it impacts the output on its own. to do this
   we iterate through each of the variables and calculate the derivative
   of the function after holding all other variables constant. each
   iteration produces a partial derivative which we store in the gradient.

[255]partial derivatives[256]  

   in functions with 2 or more variables, the partial derivative is the
   derivative of one variable with respect to the others. if we change
   \(x\), but hold all other variables constant, how does \(f(x,z)\)
   change? that   s one partial derivative. the next variable is \(z\). if
   we change \(z\) but hold \(x\) constant, how does \(f(x,z)\) change? we
   store partial derivatives in a gradient, which represents the full
   derivative of the multivariable function.

[257]step-by-step[258]  

   here are the steps to calculate the gradient for a multivariable
   function:
    1. given a multivariable function

   \[f(x,z) = 2z^3x^2\]
    2. calculate the derivative with respect to \(x\)

   \[\frac{df}{dx}(x,z)\]
    3. swap \(2z^3\) with a constant value \(b\)

   \[f(x,z) = bx^2\]
    4. calculate the derivative with \(b\) constant

   \[\begin{split}\begin{align} \frac{df}{dx} & = \lim_{h\to0}\frac{f(x+h)
   - f(x)}{h} \\ & = \lim_{h\to0}\frac{b(x+h)^2 - b(x^2)}{h} \\ & =
   \lim_{h\to0}\frac{b((x+h)(x+h)) - bx^2}{h} \\ & =
   \lim_{h\to0}\frac{b((x^2 + xh + hx + h^2)) - bx^2}{h} \\ & =
   \lim_{h\to0}\frac{bx^2 + 2bxh + bh^2 - bx^2}{h} \\ & =
   \lim_{h\to0}\frac{2bxh + bh^2}{h} \\ & = \lim_{h\to0} 2bx + bh \\
   \end{align}\end{split}\]

   as \(h    > 0\)   

   2bx + 0

    5. swap \(2z^3\) back into the equation, to find the derivative with
       respect to \(x\).

   \[\begin{split}\begin{align} \frac{df}{dx}(x,z) &= 2(2z^3)x \\ &= 4z^3x
   \end{align}\end{split}\]
    6. repeat the above steps to calculate the derivative with respect to
       \(z\)

   \[\frac{df}{dz}(x,z) = 6x^2z^2\]
    7. store the partial derivatives in a gradient

   \[\begin{split}\nabla f(x,z)=\begin{bmatrix} \frac{df}{dx} \\
   \frac{df}{dz} \\ \end{bmatrix} =\begin{bmatrix} 4z^3x \\ 6x^2z^2 \\
   \end{bmatrix}\end{split}\]

[259]directional derivatives[260]  

   another important concept is directional derivatives. when calculating
   the partial derivatives of multivariable functions we use our old
   technique of analyzing the impact of infinitesimally small increases to
   each of our independent variables. by increasing each variable we alter
   the function output in the direction of the slope.

   but what if we want to change directions? for example, imagine we   re
   traveling north through mountainous terrain on a 3-dimensional plane.
   the gradient we calculated above tells us we   re traveling north at our
   current location. but what if we wanted to travel southwest? how can we
   determine the steepness of the hills in the southwest direction?
   directional derivatives help us find the slope if we move in a
   direction different from the one specified by the gradient.

   math

   the directional derivative is computed by taking the dot product
   [261][11] of the gradient of \(f\) and a unit vector \(\vec{v}\) of
      tiny nudges    representing the direction. the unit vector describes the
   proportions we want to move in each direction. the output of this
   calculation is a scalar number representing how much \(f\) will change
   if the current input moves with vector \(\vec{v}\).

   let   s say you have the function \(f(x,y,z)\) and you want to compute
   its directional derivative along the following vector [262][2]:
   \[\begin{split}\vec{v}=\begin{bmatrix} 2 \\ 3 \\ -1 \\
   \end{bmatrix}\end{split}\]

   as described above, we take the dot product of the gradient and the
   directional vector:
   \[\begin{split}\begin{bmatrix} \frac{df}{dx} \\ \frac{df}{dy} \\
   \frac{df}{dz} \\ \end{bmatrix} \cdot \begin{bmatrix} 2 \\ 3 \\ -1 \\
   \end{bmatrix}\end{split}\]

   we can rewrite the dot product as:
   \[\nabla_\vec{v} f = 2 \frac{df}{dx} + 3 \frac{df}{dy} - 1
   \frac{df}{dz}\]

   this should make sense because a tiny nudge along \(\vec{v}\) can be
   broken down into two tiny nudges in the x-direction, three tiny nudges
   in the y-direction, and a tiny nudge backwards, by    1 in the
   z-direction.

[263]useful properties[264]  

   there are two additional properties of gradients that are especially
   useful in deep learning. the gradient of a function:

    1. always points in the direction of greatest increase of a function
       ([265]explained here)
    2. is zero at a local maximum or local minimum

[266]integrals[267]  

   the integral of \(f(x)\) corresponds to the computation of the area
   under the graph of \(f(x)\). the area under \(f(x)\) between the points
   \(x=a\) and \(x=b\) is denoted as follows:
   \[a(a,b) = \int_a^b f(x) \: dx.\]
   _images/integral_definition.png

   the area \(a(a,b)\) is bounded by the function \(f(x)\) from above, by
   the \(x\)-axis from below, and by two vertical lines at \(x=a\) and
   \(x=b\). the points \(x=a\) and \(x=b\) are called the limits of
   integration. the \(\int\) sign comes from the latin word summa. the
   integral is the    sum    of the values of \(f(x)\) between the two limits
   of integration.

   the integral function \(f(c)\) corresponds to the area calculation as a
   function of the upper limit of integration:
   \[f(c) \equiv \int_0^c \! f(x)\:dx\,.\]

   there are two variables and one constant in this formula. the input
   variable \(c\) describes the upper limit of integration. the
   integration variable \(x\) performs a sweep from \(x=0\) until \(x=c\).
   the constant \(0\) describes the lower limit of integration. note that
   choosing \(x=0\) for the starting point of the integral function was an
   arbitrary choice.

   the integral function \(f(c)\) contains the    precomputed    information
   about the area under the graph of \(f(x)\). the derivative function
   \(f'(x)\) tells us the    slope of the graph    property of the function
   \(f(x)\) for all values of \(x\). similarly, the integral function
   \(f(c)\) tells us the    area under the graph    property of the function
   \(f(x)\) for all possible limits of integration.

   the area under \(f(x)\) between \(x=a\) and \(x=b\) is obtained by
   calculating the change in the integral function as follows:
   \[a(a,b) = \int_a^b \! f(x)\:dx = f(b)-f(a).\]
   _images/integral_as_change_in_antriderivative.png

[268]computing integrals[269]  

   we can approximate the total area under the function \(f(x)\) between
   \(x=a\) and \(x=b\) by splitting the region into tiny vertical strips
   of width \(h\), then adding up the areas of the rectangular strips. the
   figure below shows how to compute the area under \(f(x)=x^2\) between
   \(x=1\) and \(x=3\) by approximating it as four rectangular strips of
   width \(h=0.5\).
   _images/integral_as_rectangular_strips.png

   usually we want to choose \(h\) to be a small number so that the
   approximation is accurate. here is some sample code that performs
   integration.
def get_integral(func, a, b):
    """compute the area under `func` between x=a and x=b."""
    h = 0.0001               # width of small rectangle
    x = a                    # start at x=a
    total = 0
    while x <= b:            # continue until x=b
        total += h*func(x)   # area of rect is base*height
        x += h
    return total

def f(x): return x**2                    # some test function f(x)=x^2
computed = get_integral(f, 1, 3)
def actualf(x): return 1.0/3.0*x**3
actual = actualf(3) - actualf(1)
computed, actual    # = 8.6662, 8.6666   # pretty close if you ask me...

   you can find integral functions using the derivative formulas and some
   reverse engineering. to find an integral function of the function
   \(f(x)\), we must find a function \(f(x)\) such that \(f'(x)=f(x)\).
   suppose you   re given a function \(f(x)\) and asked to find its integral
   function \(f(x)\):
   \[f(x) = \int \! f(x)\: dx.\]

   this problem is equivalent to finding a function \(f(x)\) whose
   derivative is \(f(x)\):
   \[f'(x) = f(x).\]

   for example, suppose you want to find the indefinite integral \(\int
   \!x^2\:dx\). we can rephrase this problem as the search for some
   function \(f(x)\) such that
   \[f'(x) = x^2.\]

   remembering the derivative formulas we saw above, you guess that
   \(f(x)\) must contain an \(x^3\) term. taking the derivative of a cubic
   term results in a quadratic term. therefore, the function you are
   looking for has the form \(f(x)=cx^3\), for some constant \(c\). pick
   the constant \(c\) that makes this equation true:
   \[f'(x) = 3cx^2 = x^2.\]

   solving \(3c=1\), we find \(c=\frac{1}{3}\) and so the integral
   function is
   \[f(x) = \int x^2 \:dx = \frac{1}{3}x^3 + c.\]

   you can verify that \(\frac{d}{dx}\left[\frac{1}{3}x^3 + c\right] =
   x^2\).

[270]applications of integration[271]  

   integral calculations have widespread applications to more areas of
   science than are practical to list here. let   s explore a few examples
   related to probabilities.

[272]computing probabilities[273]  

   a continuous random variable \(x\) is described by its id203
   density function \(p(x)\). a id203 density function \(p(x)\) is a
   positive function for which the total area under the curve is \(1\):
   \[ p(x) \geq 0, \forall x \qquad \textrm{and} \qquad
   \int_{-\infty}^\infty p(x)\; dx = 1.\]

   the id203 of observing a value of \(x\) between \(a\) and \(b\)
   is given by the integral
   \[ \textrm{pr}(a \leq x \leq b) = \int_a^b p(x)\; dx.\]

   thus, the notion of integration is central to id203 theory with
   continuous random variables.

   we also use integration to compute certain characteristic properties of
   the random variable. the expected value and the variance are two
   properties of any random variable \(x\) that capture important aspects
   of its behaviour.

[274]expected value[275]  

   the expected value of the random variable \(x\) is computed using the
   formula
   \[\mu % \equiv \mathbb{e}_x[x] = \int_{-\infty}^\infty x\, p(x).\]

   the expected value is a single number that tells us what value of \(x\)
   we can expect to obtain on average from the random variable \(x\). the
   expected value is also called the average or the mean of the random
   variable \(x\).

[276]variance[277]  

   the variance of the random variable \(x\) is defined as follows:
   \[\sigma^2 % \equiv \mathbb{e}_x\!\big[(x-\mu)^2\big] =
   \int_{-\infty}^\infty (x-\mu)^2 \, p(x).\]

   the variance formula computes the expectation of the squared distance
   of the random variable \(x\) from its expected value. the variance
   \(\sigma^2\), also denoted \(\textrm{var}(x)\), gives us an indication
   of how clustered or spread the values of \(x\) are. a small variance
   indicates the outcomes of \(x\) are tightly clustered near the expected
   value \(\mu\), while a large variance indicates the outcomes of \(x\)
   are widely spread. the square root of the variance is called the
   standard deviation and is usually denoted \(\sigma\).

   the expected value \(\mu\) and the variance \(\sigma^2\) are two
   central concepts in id203 theory and statistics because they
   allow us to characterize any random variable. the expected value is a
   measure of the central tendency of the random variable, while the
   variance \(\sigma^2\) measures its dispersion. readers familiar with
   concepts from physics can think of the expected value as the centre of
   mass of the distribution, and the variance as the moment of inertia of
   the distribution.

   references
   [1] [278]https://en.wikipedia.org/wiki/derivative
   [279][2]
   [280]https://www.khanacademy.org/math/multivariable-calculus/multivaria
   ble-derivatives/partial-derivative-and-gradient-articles/a/directional-
   derivative-introduction
   [3] [281]https://en.wikipedia.org/wiki/partial_derivative
   [4] [282]https://en.wikipedia.org/wiki/gradient
   [5]
   [283]https://betterexplained.com/articles/vector-calculus-understanding
   -the-gradient
   [6]
   [284]https://www.mathsisfun.com/calculus/derivatives-introduction.html
   [7]
   [285]http://tutorial.math.lamar.edu/classes/calci/defnofderivative.aspx
   [8]
   [286]https://www.khanacademy.org/math/calculus-home/taking-derivatives-
   calc/chain-rule-calc/v/chain-rule-introduction
   [9] [287]http://tutorial.math.lamar.edu/classes/calci/chainrule.aspx
   [10] [288]https://youtu.be/phmznw8agq4?t=1m5s
   [289][11] [290]https://en.wikipedia.org/wiki/dot_product

   [291]next [292]previous
     __________________________________________________________________

      copyright 2017 revision 5f00adef.
   built with [293]sphinx using a [294]theme provided by [295]read the
   docs.

   read the docs v: latest

   versions
          [296]latest

   downloads
          [297]pdf
          [298]htmlzip
          [299]epub

   on read the docs
          [300]project home
          [301]builds
     __________________________________________________________________

   free document hosting provided by [302]read the docs.

references

   1. https://ml-cheatsheet.readthedocs.io/en/latest/genindex.html
   2. https://ml-cheatsheet.readthedocs.io/en/latest/search.html
   3. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html
   4. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html
   5. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
   6. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html
   7. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#introduction
   8. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simple-regression
   9. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#making-predictions
  10. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#cost-function
  11. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#gradient-descent
  12. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#training
  13. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#model-evaluation
  14. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#summary
  15. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multivariable-regression
  16. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#growing-complexity
  17. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#id172
  18. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#multiple-linear-regression-predict
  19. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#initialize-weights
  20. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  21. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  22. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#simplifying-with-matrices
  23. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html#bias-term
  24. https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html# 
  25. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html
  26. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#introduction
  27. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#learning-rate
  28. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#cost-function
  29. https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html#step-by-step
  30. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html
  31. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#introduction
  32. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#comparison-to-linear-regression
  33. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#types-of-logistic-regression
  34. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#binary-logistic-regression
  35. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#sigmoid-activation
  36. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#decision-boundary
  37. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#making-predictions
  38. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function
  39. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#gradient-descent
  40. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#mapping-probabilities-to-classes
  41. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#training
  42. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#model-evaluation
  43. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#multiclass-logistic-regression
  44. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#procedure
  45. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#softmax-activation
  46. https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#scikit-learn-example
  47. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html
  48. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html
  49. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#introduction
  50. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#derivatives
  51. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#geometric-definition
  52. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#taking-the-derivative
  53. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#step-by-step
  54. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#machine-learning-use-cases
  55. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#chain-rule
  56. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#how-it-works
  57. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
  58. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#multiple-functions
  59. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#gradients
  60. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#partial-derivatives
  61. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
  62. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#directional-derivatives
  63. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#useful-properties
  64. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#integrals
  65. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-integrals
  66. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#applications-of-integration
  67. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-probabilities
  68. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#expected-value
  69. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#variance
  70. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html
  71. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vectors
  72. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#notation
  73. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vectors-in-geometry
  74. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#scalar-operations
  75. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations
  76. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#dot-product
  77. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#hadamard-product
  78. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#vector-fields
  79. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrices
  80. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#dimensions
  81. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  82. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  83. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  84. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrix-transpose
  85. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#matrix-multiplication
  86. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#test-yourself
  87. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#numpy
  88. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html# 
  89. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#broadcasting
  90. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html
  91. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#links
  92. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#screenshots
  93. https://ml-cheatsheet.readthedocs.io/en/latest/id203.html#license
  94. https://ml-cheatsheet.readthedocs.io/en/latest/statistics.html
  95. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html
  96. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#algebra
  97. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#calculus
  98. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#linear-algebra
  99. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#id203
 100. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#set-theory
 101. https://ml-cheatsheet.readthedocs.io/en/latest/math_notation.html#statistics
 102. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html
 103. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#neural-network
 104. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#neuron
 105. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#synapse
 106. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#weights
 107. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#bias
 108. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#layers
 109. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#weighted-input
 110. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#activation-functions
 111. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#loss-functions
 112. https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#optimization-algorithms
 113. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html
 114. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#simple-network
 115. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#steps
 116. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#code
 117. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#larger-network
 118. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#architecture
 119. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#weight-initialization
 120. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#bias-terms
 121. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#working-with-matrices
 122. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#dynamic-resizing
 123. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#refactoring-our-code
 124. https://ml-cheatsheet.readthedocs.io/en/latest/forwardpropagation.html#final-result
 125. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html
 126. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#chain-rule-refresher
 127. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#applying-the-chain-rule
 128. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#saving-work-with-memoization
 129. https://ml-cheatsheet.readthedocs.io/en/latest/id26.html#code-example
 130. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html
 131. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#linear
 132. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#elu
 133. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#relu
 134. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu
 135. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid
 136. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh
 137. https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#softmax
 138. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html
 139. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#batchnorm
 140. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#convolution
 141. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#dropout
 142. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#linear
 143. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#lstm
 144. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#pooling
 145. https://ml-cheatsheet.readthedocs.io/en/latest/layers.html#id56
 146. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html
 147. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-id178
 148. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#hinge
 149. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#huber
 150. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#kullback-leibler
 151. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mae-l1
 152. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse-l2
 153. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html
 154. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adadelta
 155. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adagrad
 156. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#adam
 157. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#conjugate-gradients
 158. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#bfgs
 159. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#momentum
 160. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#nesterov-momentum
 161. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#newton-s-method
 162. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#rmsprop
 163. https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#sgd
 164. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html
 165. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#data-augmentation
 166. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#dropout
 167. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#early-stopping
 168. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#ensembling
 169. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#injecting-noise
 170. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#l1-id173
 171. https://ml-cheatsheet.readthedocs.io/en/latest/id173.html#l2-id173
 172. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html
 173. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#autoencoder
 174. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#id98
 175. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#gan
 176. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#mlp
 177. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#id56
 178. https://ml-cheatsheet.readthedocs.io/en/latest/architectures.html#vae
 179. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html
 180. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#bayesian
 181. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#boosting
 182. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#decision-trees
 183. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#k-nearest-neighbor
 184. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#logistic-regression
 185. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#random-forests
 186. https://ml-cheatsheet.readthedocs.io/en/latest/classification_algos.html#support-vector-machines
 187. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html
 188. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#centroid
 189. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#density
 190. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#distribution
 191. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#hierarchical
 192. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#id116
 193. https://ml-cheatsheet.readthedocs.io/en/latest/id91_algos.html#mean-shift
 194. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html
 195. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#lasso
 196. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#linear
 197. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#ordinary-least-squares
 198. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#polynomial
 199. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#ridge
 200. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#splines
 201. https://ml-cheatsheet.readthedocs.io/en/latest/regression_algos.html#stepwise
 202. https://ml-cheatsheet.readthedocs.io/en/latest/reinforcement_learning.html
 203. https://ml-cheatsheet.readthedocs.io/en/latest/datasets.html
 204. https://ml-cheatsheet.readthedocs.io/en/latest/libraries.html
 205. https://ml-cheatsheet.readthedocs.io/en/latest/papers.html
 206. https://ml-cheatsheet.readthedocs.io/en/latest/other_content.html
 207. https://ml-cheatsheet.readthedocs.io/en/latest/contribute.html
 208. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
 209. https://ml-cheatsheet.readthedocs.io/en/latest/index.html
 210. https://github.com/bfortuner/ml-cheatsheet/blob/master/docs/calculus.rst
 211. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#calculus
 212. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#introduction
 213. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#derivatives
 214. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#geometric-definition
 215. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#taking-the-derivative
 216. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#step-by-step
 217. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#machine-learning-use-cases
 218. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#chain-rule
 219. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#how-it-works
 220. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 221. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#multiple-functions
 222. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#gradients
 223. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#partial-derivatives
 224. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 225. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#directional-derivatives
 226. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#useful-properties
 227. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#integrals
 228. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-integrals
 229. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#applications-of-integration
 230. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-probabilities
 231. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#expected-value
 232. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#variance
 233. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 0
 234. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#introduction
 235. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 1
 236. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#derivatives
 237. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 2
 238. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#geometric-definition
 239. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 3
 240. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#taking-the-derivative
 241. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 4
 242. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#step-by-step
 243. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 5
 244. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#machine-learning-use-cases
 245. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 6
 246. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#chain-rule
 247. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 7
 248. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#how-it-works
 249. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 8
 250. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 251. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 9
 252. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#multiple-functions
 253. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 0
 254. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#gradients
 255. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 1
 256. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#partial-derivatives
 257. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 2
 258. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 259. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 3
 260. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#directional-derivatives
 261. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 262. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 263. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 4
 264. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#useful-properties
 265. https://betterexplained.com/articles/understanding-pythagorean-distance-and-the-gradient
 266. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 5
 267. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#integrals
 268. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 6
 269. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-integrals
 270. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 7
 271. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#applications-of-integration
 272. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 8
 273. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#computing-probabilities
 274. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 9
 275. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#expected-value
 276. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 0
 277. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html#variance
 278. https://en.wikipedia.org/wiki/derivative
 279. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 280. https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction
 281. https://en.wikipedia.org/wiki/partial_derivative
 282. https://en.wikipedia.org/wiki/gradient
 283. https://betterexplained.com/articles/vector-calculus-understanding-the-gradient
 284. https://www.mathsisfun.com/calculus/derivatives-introduction.html
 285. http://tutorial.math.lamar.edu/classes/calci/defnofderivative.aspx
 286. https://www.khanacademy.org/math/calculus-home/taking-derivatives-calc/chain-rule-calc/v/chain-rule-introduction
 287. http://tutorial.math.lamar.edu/classes/calci/chainrule.aspx
 288. https://youtu.be/phmznw8agq4?t=1m5s
 289. https://ml-cheatsheet.readthedocs.io/en/latest/calculus.html# 
 290. https://en.wikipedia.org/wiki/dot_product
 291. https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html
 292. https://ml-cheatsheet.readthedocs.io/en/latest/glossary.html
 293. http://sphinx-doc.org/
 294. https://github.com/rtfd/sphinx_rtd_theme
 295. https://readthedocs.org/
 296. https://ml-cheatsheet.readthedocs.io/en/latest/
 297. https://readthedocs.org/projects/ml-cheatsheet/downloads/pdf/latest/
 298. https://readthedocs.org/projects/ml-cheatsheet/downloads/htmlzip/latest/
 299. https://readthedocs.org/projects/ml-cheatsheet/downloads/epub/latest/
 300. https://readthedocs.org/projects/ml-cheatsheet/?fromdocs=ml-cheatsheet
 301. https://readthedocs.org/builds/ml-cheatsheet/?fromdocs=ml-cheatsheet
 302. http://www.readthedocs.org/
