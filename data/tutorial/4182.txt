   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

creating insanely fast image classifiers with mobilenet in tensorflow

   it   s like hot dog not hot dog, but for roads.   

   [14]go to the profile of matt harvey
   [15]matt harvey (button) blockedunblock (button) followfollowing
   jul 11, 2017

   mobilenets are a new family of convolutional neural networks that are
   set to blow your mind, and today we   re going to train one on a custom
   dataset.

   there are a few things that make mobilenets awesome:
    1. they   re insanely small
    2. they   re insanely fast
    3. they   re remarkably accurate
    4. they   re easy to tune for resources vs. accuracy

   why is this important? many mobile deep learning tasks are actually
   performed in the cloud. when you want to classify an image, that image
   is sent to a web service, it   s classified on a remote server, and the
   result is sent back to your phone.

   that   s changing quickly. the computational power on your phone is
   increasing rapidly, and the network complexity required for computer
   vision is shrinking (thanks to architectures like squeezenet and
   mobilenet).

   besides the clear benefit of ai without an internet connection, sending
   images to the cloud is impractical in speed-hungry situations, like in
   the vehicle safety apps we   re developing at [16]coastline.

   with that, let   s learn the following:
     * what are mobilenets?
     * how to build a custom dataset to train a mobilenet with tensorflow
     * how to train a mobilenet that   s pretrained on id163 with
       tensorflow
     * how mobilenets perform against inception v3
     * how to use your retrained mobilenet to classify images

   [1*abxyqkigauui-gxukp5qfw.jpeg]
   we   ll use mobilenets to build a road, not road classifer to identify
   images like this one.

what are mobilenets?

   mobilenets are a class of convolutional neural network designed by
   researches at google. they are coined    mobile-first    in that they   re
   architected from the ground up to be resource-friendly and run quickly,
   right on your phone.

   the main difference between the mobilenet architecture and a
      traditional    id98   s is instead of a single 3x3 convolution layer
   followed by batch norm and relu, mobilenets split the convolution into
   a 3x3 depthwise conv and a 1x1 pointwise conv. the details of why this
   is so significant can be found in the [17]mobilenet paper, which i
   strongly encourage you to read.

   so what   s the catch? accuracy. mobilenets are not usually as accurate
   as the bigger, more resource-intensive networks we   ve come to know and
   love. but finding that resource/accuracy trade-off is where mobilenets
   really shine.

   mobilenets surface two parameters that we can tune to fit the
   resource/accuracy trade-off of our exact problem: width multiplier and
   resolution multiplier. the width multiplier allows us to thin the
   network, while the resolution multiplier changes the input dimensions
   of the image, reducing the internal representation at every layer.

   google [18]open-sourced the mobilenet architecture and released 16
   id163 checkpoints, each corresponding to a different parameter
   configuration. this gives us an excellent starting point for training
   our own classifiers that are insanely small and insanely fast.

   to learn more about how mobilenets work, read[19] mobilenets: efficient
   convolutional neural networks for mobile vision applications.

how to build your own dataset to train a mobilenet

   our challenge today is to build an image classifier that can detect if
   an image is a road or not a road. it   s like [20]hot dog, not hot dog,
   but for roads.

   why road, not road? at [21]coastline, we   re developing safety features
   for your car with mobile apps that use id161. as with all
   vision problems, user privacy is critical. so one of the first checks
   we do when a user turns on the camera in our app is check if it sees a
   road. if it doesn   t, we turn off the camera. we want to be able to do
   this fast and at as little computational cost to the user as possible.

   since we   re tackling a custom problem, we need to start with creating
   our dataset. our target is to collect 10,000 images, split roughly
   evenly 50/50 road/not road.

   we   ll get data from a few different places:
     * 4,000 obviously road images sampled randomly from the coastline
       driving dataset
     * 2,000 obviously not road images sampled randomly from the id163
       dataset
     * 3,000 less obviously not road scenics sampled from the internet to
       make sure the classifier doesn   t just learn sky, not sky
     * 1,000 less obviously road scenics also sampled from the internet to
       make sure the classifier doesn   t recognize windshield reflections
       or other commonalities in the coastline data

   we   ll place each image into one of two folders, each representing the
   class of that image: road and not-road. that   s all we have to do to
   prepare our images for retraining!

   however, while grabbing [22]cc images from the internet is a great
   place to add diversity to your dataset, it comes with a drawback: the
   labels are noisy. for instance, an image found by searching    road
   landscape    could have a road front and center with a nice scene in the
   background, or it could be a mountain scene with a tiny road off in the
   distance.
   [1*rlaablfaw4khpsdlq4oa-w.jpeg]
   [1*is6atuffompayj5fsq4vyq.jpeg]
   two images both labeled    road   . for our problem, the first should be
      not road    and the second should be    road   . left: [23]   scenic mountain
   road    by [24]tyler thompson is licensed under [25]cc by 2.0 right:
   [26]   150927-road-scenic-bypass-mountains.jpg    by [27]r. nial bradshaw
   is licensed under [28]cc by 2.0

   we could solve for this by going through each image and hand-labeling
   it, but why do that when we have deep learning?! instead, we retrain a
   big network (like inception v3) on all our data, paying special
   attention not to over-fit on our training data by early stopping and
   heavy data augmentation. then we run every image of our dataset (even
   those images we just used to train!) through the network and keep track
   of the images it classified incorrectly or with little confidence. then
   we go through each of those images and move them to their proper
   classes, if applicable. this reduces the number of images we have to
   manually clean up significantly. and doing multiple passes of this
   technique helped us increase our accuracy by seven percentage points on
   inception.

   now that we have 5,000 road images and 5,000 not road images, in a
   structure like this   
data/
  road/[images...]
  not-road/[images...]

      we   ll use tensorflow and id21 to fine-tune mobilenets on
   our custom dataset.

how to retrain a mobilenet that   s pretrained on id163

   tensorflow comes packaged with great tools that you can use to retrain
   mobilenets without having to actually write any code. this stuff is
   fresh off the presses: retraining support for mobilenet was [29]added
   less than a week ago!
   [1*rhlma6ygvmvx82uhf73pxq.jpeg]
   [1*ljuckvnqw2qq4_vgc5icwg.jpeg]
   a couple very obviously road images from the coastline dataset.

   if you don   t have it downloaded already, fork and/or clone the
   tensorflow repo:
git clone https://github.com/tensorflow/tensorflow.git

   now you can use the scripts in the example folder to retrain mobilenet
   on your own data.

   but wait! which mobilenet should you use? that   s a good question. let   s
   retrain a small assortment and see how they perform. to kick off
   training, we   ll run the following command from the root of the
   tensorflow repo:
python tensorflow/examples/image_retraining/retrain.py \
    --image_dir ~/ml/blogs/road-not-road/data/ \
    --learning_rate=0.0001 \
    --testing_percentage=20 \
    --validation_percentage=20 \
    --train_batch_size=32 \
    --validation_batch_size=-1 \
    --flip_left_right true \
    --random_scale=30 \
    --random_brightness=30 \
    --eval_step_interval=100 \
    --how_many_training_steps=600 \
    --architecture mobilenet_1.0_224

   the architecture flag is where we tell the retraining script which
   version of mobilenet we want to use. the 1.0 corresponds to the width
   multiplier, and can be 1.0, 0.75, 0.50 or 0.25. the 224 corresponds to
   image resolution, and can be 224, 192, 160 or 128. for example, to
   train the smallest version, you   d use --architecture
   mobilenet_0.25_128.

   some other important parameters:
     * learning_rate: this is something you   ll want to play with. i found
       0.0001 to work well.
     * testing and validation percentage: the script will split your data
       into train/val/test for you. it will use train to train, val to
       give performance updates every    eval_step_interval   , and test will
       run after    how_many_training_steps    to give you your final score.
     * validation_batch_size: setting this to -1 tells the script to use
       all your data to validate on. when you don   t have a lot of data
       (like only 10,000 images), it   s a good idea to use -1 here to
       reduce variance between evaluation steps.

   after retraining on several model architectures, let   s see how they
   compare.

comparing mobilenet parameters and their performance against inception

   after just 600 steps on training inception to get a baseline (by
   setting the         architecture flag to inception_v3), we hit 95.9%.
   training took 18 minutes. (there is a lot of room for improvement here,
   but we don   t have all day!) the resulting checkpoint landed at 84mb.
   and doing a quick speed test by running 1,000 images through it shows
   it can classify images on an nvidia geforce 960m gpu at ~19fps.

   aside: why    only    95.9% and not 100%? it seems like a pretty simple
   problem, right? well, besides the ample tuning we could do to the
   training parameters (we actually achieved 98.9% with the same data
   using a different configuration in another go), it turns out the
   distinction between classes is a bit more subtle than it seems on the
   surface. take these cases:
     * the image is a one-lane dirt road in the woods: is it a road or a
       trail? i don   t even know.
     * it   s a landscape scenic with a road in the distance. is it a
       picture or a road, or is there just a road in the picture? at what
       point does the scenic move classes?
     * it   s an artsy tilt-shift photo of a couple holding hands in the
       foreground and a time-lapsed street in the background. road or not
       road? flip a coin.

   so, how do the mobilenets perform? not surprisingly, not quite as well.
   however, the tradeoff benefit is astounding.

   using the biggest mobilenet (1.0, 224), we were able to achieve 95.5%
   accuracy with just 4 minutes of training. the resulting model size was
   just 17mb, and it can run on the same gpu at ~135fps.

   for those keeping score, that   s 7 times faster and a quarter the size.
   all for just 0.4% loss in accuracy.
   [1*bqlt4rq8upbb8-nkhr9vca.gif]

   how about the smallest mobilenet (0.24, 128), using quantized weights?
   big accuracy tradeoff, achieving just 89.2%. but get this: 450 frames
   per second, and the model takes just 930kb of memory. that   s kilobytes!
   [1*qlsded8p1x19mifla734ww.gif]

using your retrained mobilenet to classify images

   now that you have your mobilenet retrained on your custom dataset, it   s
   time to give it a try. not surprisingly, tensorflow comes with a script
   to do that, too.
python3 tensorflow/examples/label_image/label_image.py \
  --graph=/tmp/mobilenet_0.50_192.pb \
  --labels=/tmp/output_labels.txt \
  --image=/home/harvitronix/ml/blogs/road-not-road/test-image.jpg \
  --input_layer=input \
  --output_layer=final_result \
  --input_mean=128 \
  --input_std=128 \
  --input_width=192 \
  --input_height=192

   [1*ltlv8tnu4w4abi2qmbf7pw.jpeg]
   our network classifies this as road, with confidence of 0.686811. not
   too confidence, but hey, that   s a fast road!

   aside: it should be noted that in our fairly simple two-class problem,
   the accuracy trade-off is not that big. in the case of id163 with
   its 1,001 classes, the accuracy tradeoffs are much more significant.
   see [30]the table here.

next steps

   okay, so the whole point of mobilenets is to run on mobile, right? stay
   tuned! in our next post, we   ll create some purpose-built training data,
   fine-tune again, and load our retrained mobilenet into an android app.
   we   ll see how fast it can run on a mobile device and how accurate it is
   in the real world.

     update: part 2 is now live! don   t miss [31]building an insanely fast
     image classifier on android with mobilenets in tensorflow.

     * [32]machine learning
     * [33]deep learning
     * [34]mobilenet
     * [35]tensorflow
     * [36]artificial intelligence

   (button)
   (button)
   (button) 1.7k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [37]go to the profile of matt harvey

[38]matt harvey

   founder of coastline automation, using ai to make every car
   crash-proof.

     (button) follow
   [39]hacker noon

[40]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 1.7k
     * (button)
     *
     *

   [41]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [42]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f030ce0a2991
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/creating-insanely-fast-image-classifiers-with-mobilenet-in-tensorflow-f030ce0a2991&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/creating-insanely-fast-image-classifiers-with-mobilenet-in-tensorflow-f030ce0a2991&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_aa1tocx7ecgt---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@harvitronix?source=post_header_lockup
  15. https://hackernoon.com/@harvitronix
  16. http://coast.ai/
  17. https://arxiv.org/pdf/1704.04861.pdf
  18. https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html
  19. https://arxiv.org/pdf/1704.04861.pdf
  20. https://www.youtube.com/watch?v=acmydtfdtgs
  21. https://coast.ai/
  22. https://search.creativecommons.org/
  23. https://www.flickr.com/photos/82036561@n00/7524452224
  24. https://www.flickr.com/people/82036561@n00/
  25. https://creativecommons.org/licenses/by/2.0
  26. https://www.flickr.com/photos/zionfiction/21808267691
  27. https://www.flickr.com/people/zionfiction/
  28. https://creativecommons.org/licenses/by/2.0
  29. https://github.com/tensorflow/tensorflow/commit/055500bbcea60513c0160d213a10a7055f079312
  30. https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html
  31. https://hackernoon.com/building-an-insanely-fast-image-classifier-on-android-with-mobilenets-in-tensorflow-dc3e0c4410d4
  32. https://hackernoon.com/tagged/machine-learning?source=post
  33. https://hackernoon.com/tagged/deep-learning?source=post
  34. https://hackernoon.com/tagged/mobilenet?source=post
  35. https://hackernoon.com/tagged/tensorflow?source=post
  36. https://hackernoon.com/tagged/artificial-intelligence?source=post
  37. https://hackernoon.com/@harvitronix?source=footer_card
  38. https://hackernoon.com/@harvitronix
  39. https://hackernoon.com/?source=footer_card
  40. https://hackernoon.com/?source=footer_card
  41. https://hackernoon.com/
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://medium.com/p/f030ce0a2991/share/twitter
  45. https://medium.com/p/f030ce0a2991/share/facebook
  46. https://medium.com/p/f030ce0a2991/share/twitter
  47. https://medium.com/p/f030ce0a2991/share/facebook
