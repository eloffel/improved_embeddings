   #[1]vivek panyam

   [2]vivek panyam
   [3]contact

deep learning made simple [part 1]

   14 july 2017

   this series of posts was initially created as a way to explain neural
   networks and deep learning to my younger brother. therefore, we are not
   going to assume any prior knowledge of calculus or id202,
   among other things.

   disclaimer: the goal of this post is to be as easy to understand as
   possible. because of that, some of the statements below aren't entirely
   accurate. many of the larger inaccuracies are marked with *

   contents:
    1. deep learning made simple [part 1] - this post
    2. teaching a neural network [part 2] (to be published)
    3. ...

what can deep learning do?

   deep learning is an amazing tool that can generate some very impressive
   results. let's look at some of these before we dig into how it actually
   works.
     * deep learning can turn a photo into a painting of a particular
       style:

   [4]neural style on github
     * or better yet, turn a photo into a photo of a different style:

   [5]deep photo style transfer on github
     * [6]it can beat the world's best go players
     * and can beat humans at a bunch of other games:

   [7]human-level control through deep id23 from
   deepmind
     * it can detect and identify objects:

   [8]tensorflow id164 api
     * it can caption images:

   [9]deep visual-semantic alignments for generating image descriptions
     * or even colorize them:

   [10]colorful image colorization
     * it can translate between languages
     * it can recognize speech
     * it can generate voices
     * it can turn a drawing into a picture
     * it can understand handwriting

   and much much more.

... how?

   in order to understand that, we need to dig into some basic math first.
   let's define what a function is.

   the definition we're going to use is as follows (from wikipedia):

     in mathematics, a function is a relation between a set of inputs and
     a set of permissible outputs with the property that each input is
     related to exactly one output.

   basically, a function takes in some number of inputs and has an output.
   every time the same inputs are passed into a function, it should give
   us the same output.

   also note that the output of a function doesn't need to be just one
   number. it can also be a vector.

     for our purposes, a vector is an ordered list of numbers

   for example if we have a vector $v$:
     * $v_1$ is the first element
     * $v_2$ is the second element
     * ...

   functions can be as complex or as simple as we want. here are some
   examples:
     * $y = f(x) = x + 1$
     * $y = f(x) = x^2 + 1$
     * $y = f(a,b,c,d) = 23 * a + 42 * b + 51 * c + d$
     * items_in_picture = f(image) = ?
     * speaker_identity = f(audio_signal) = ?

   in the above list, think of ? as "something super complicated that we
   don't know how to write down."

   at first glance, the last two items in the list of examples might look
   weird, but if you look carefully, they fit all our rules for functions:
     * they relate inputs and outputs.
     * for a given input, they always have the same output.

   now that we know what a function is, let's define what the purpose of
   deep learning is.

     the end goal of most machine learning methods, including deep
     learning, is to figure out how to write down complex functions. in
     more concrete terms, machine learning methods try to estimate a
     function.

   so how can we estimate a function we don't know how to write down?
   let's look at this problem a slightly different way.

estimating a function

   let's pretend someone told us that there's a function $y = f(x)$ that
   does something interesting and they asked us to figure out what $f(x)$
   is. since they're feeling particularly generous, they'll also give us a
   bunch of examples of corresponding $x$ and $y$ values.

   in mathematical terms, they asked us to come up with a function $g(x)$
   such that $f(x) = g(x)$ for all possible $x$ values.

   so how do we know if we're doing a good job? we need some way to tell
   how close $g(x)$ is to $f(x)$.

   let's use the distance formula that we learned in middle school
   (actually called the [11]euclidean distance or l2 norm). we'll call
   this $distance(v, w)$ where $v$ and $w$ are two vectors.

   great! so if we minimize $distance(f(x), g(x))$ for all points $x$, we
   should come up with the correct $g(x)$.

   one slight hiccup; we don't know what $g(x)$ looks like and $f(x)$ can
   be "something super complicated that we don't know how to write down".
   how can we minimize distance if we can't write it down?

   we'll tackle these problems over the next two sections.

taking inspiration from nature

   this seems like a good time to look at the human brain. our brain is
   made of 86 billion neurons. a very simple diagram of a neuron is below.

   diagram of a neuron (a modified version of [12]this image)

   the basic way a neuron works is as follows:
     * dendrites take signals from other neurons and send them to the cell
       body.
     * the cell body takes the input signals, does some processing, and
       decides to "fire" or not.
     * if the neuron is going to "fire," the axon sends the output to
       other neurons (using the axon terminal).

   the places where the axon of one neuron connects to a dendrite of
   another neuron is called a synapse. the human brain has between 100
   trillion and 1 quadrillion synapses!

id158s

   let's take some inspiration from the neurons in our brains and create
   an artificial neuron. let's say our neuron takes 3 inputs and has one
   output:

   so that means our neuron is a function that looks like this:

   $output = f(x,y,z)$

   let's also say that our neuron is a linear function.

     a linear function is a function that represents a straight line or
     plane.

   so now we have

   $output = f(x,y,z) = some\_constant * x + some\_constant * y +
   some\_constant * z + some\_constant$

   (where all the constants can be different)

   now that we've built a neuron, let's put a bunch of these together to
   build a neural network!

   let's run through the math really quickly to see what $output$ actually
   is.

   the output of the final layer of the neural network (n8) is:
     * $network\_output = n8_{output}$
     * $n8_{output} = some\_constant * n4_{output} + some\_constant *
       n5_{output} + some\_constant * n6_{output} + some\_constant$

   the outputs of the second layer of the network (n4, n5, and n6) are:
     * $n4_{output} = some\_constant * n1_{output} + some\_constant *
       n2_{output} + some\_constant * n3_{output} + some\_constant$
     * $n5_{output} = some\_constant * n1_{output} + some\_constant *
       n2_{output} + some\_constant * n3_{output} + some\_constant$
     * $n6_{output} = some\_constant * n1_{output} + some\_constant *
       n2_{output} + some\_constant * n3_{output} + some\_constant$

   the outputs of the first layer of the network (n1, n2, and n3) are:
     * $n1_{output} = some\_constant * x + some\_constant * y +
       some\_constant * z + some\_constant$
     * $n2_{output} = some\_constant * x + some\_constant * y +
       some\_constant * z + some\_constant$
     * $n3_{output} = some\_constant * x + some\_constant * y +
       some\_constant * z + some\_constant$

   after combining all the equations above, we get:

   $network\_output = some\_constant * x + some\_constant * y +
   some\_constant * z + some\_constant$

   (again, where $some\_constant$ can be different every time it's
   mentioned)

   so, that's a problem. even if we have a super complicated combination
   of these neurons, it still can't do anything except fit a line or a
   plane. why?

     a linear combination of linear functions is linear.

   read that a few times and convince yourself it's true. basically, if
   all you can do is add and multiply by constants, no matter how many
   times you do it, you'll still end up with a linear function.

   so to make neural networks useful, we need to introduce nonlinearities.

     a nonlinearity (or nonlinear function) is a function that does not
     have a linear relationship between its inputs and output. to keep it
     simple we'll just call these functions $nl$.

   let's change our function for each neuron to include a nonlinearity

   $output = f(x,y,z) = nl(some\_constant * x + some\_constant * y +
   some\_constant * z + some\_constant)$

   we'll dig into this more in the next post, but, for now, nonlinearities
   solve our problem!

   in fact, now that we have nonlinearities, we can show that "for every
   function, there exists some neural network to represent it".* this
   means that any function you can come up with can be represented by a
   neural network!*

     the proof, exact wording, and caveats of this statement are out of
     the scope of this post, but if you want more detail, you can look at
     the [13]universal approximation theorem.

   since we know $g(x)$ is a function and we know that there exists some
   neural network that can represent any function*, let's make $g(x)$ a
   neural network!

hot and cold

   unfortunately, we still don't know how to minimize our distance
   function. let's try to figure out how to do that.

   remember the game "hot and cold"? you try to find an object and as you
   move, your friend tells you "hotter" or "colder" depending on whether
   you're moving towards the target object or not.

   the game looks something like this:

    1. a target object is chosen
    2. start somewhere in the center of the room
    3. until you find the object:
         1. you move in a direction
         2. your friend yells out "hot" or "cold"
         3. if they yelled out "cold," move in the opposite direction

   what if we played that game to minimize a function? let's say our
   "target" is to find the minimum value of our distance function.

   to make things easier to write down, let's also say

   $d(x) = distance(f(x), g(x))$

   where $g(x)$ is our neural network and $f(x)$ is the function we're
   trying to estimate.

   let's rewrite how "how and cold" is played:

    1. a target value is chosen: the minimum value of $d(x)$
    2. start with a random $x$ value
    3. until you can't decrease $d(x)$ any more:
         1. move $x$ in a direction
         2. check if $d(x)$ is less than it was before
         3. if $d(x)$ increased, move $x$ in the opposite direction and
            check again

   how do we know if we can't decrease $d(x)$ any more? if we move $x$ in
   both possible directions and $d(x)$ increases both times, that means we
   found the minimum!*

let's walk through that game on an example distance function.

   our random starting $x$ value is -2. so, based on the graph below,
   $d(x) = 5$.

   let's move to $x = -1$ and see if $d(x)$ is less than it was before.

   $d(-1) = 2 < 5$ so we're moving in the right direction! let's move to
   $x = 0$.

   $d(0) = 1 < 2$ so we're still doing a good job! let's move to $x = 1$.

   $d(1) = 2 > 1$ so we're not going in the right direction. let's do one
   last check at $x = -1$.

   $d(-1) = 2 > 1$ so looks like we found the minimum value at $d(0) = 1$!

   is there a better way to do this? we could have easily overshot the
   minimum point if we started at a different point or if we chose a
   different step size. for example, if we started at $x = -1.5$ instead
   of at $x = -2$, we would have never reached $x = 0$ with a step size of
   $1$ (we would have gone from $-1.5$ to $-0.5$ to $0.5$).

   can the slope of a line help us?

     the slope of a line between two points $(x_1, y_1)$ and $(x_2, y_2)$
     is $\frac{y_2 - y_1}{x_2 - x_1}$.

   therefore, the slope of $d(x)$ is defined as:

   $slope_d(x_1, x_2) = \frac{d(x_2) - d(x_1)}{x_2 - x_1}$

   now let's say that $x_2 = x_1 + q$ where $q$ is some small number.
   let's substitute that in:

   $slope_d(x_1) = \frac{d(x_1 + q) - d(x_1)}{(x_1 + q) - x_1} =
   \frac{d(x_1 + q) - d(x_1)}{q}$

   so if $d(x_1 + q)$ is less than $d(x_1)$, then $slope_d(x_1) < 0$
   otherwise $slope_d(x_1) > 0$

   with this new notation, let's change our game to the following:

    1. a target value is chosen: the minimum value of $d(x)$
    2. start with a random $x$ value
    3. until you can't decrease $d(x)$ any more:
         1. subtract $slope_d(x)$ from $x$ (i.e. $x = x - slope_d(x)$)

   you'll notice that the last step is the same as this:
    1. if $slope_d(x) < 0$, we want to increase $x$ so we'll subtract
       $slope_d(x)$ from $x$ (i.e. $x = x - slope_d(x)$)
          + subtracting a negative number from $x$ will increase $x$
    2. if $slope_d(x) > 0$, we want to decrease $x$ so we'll subtract
       $slope_d(x)$ from $x$ (i.e. $x = x - slope_d(x)$)
          + subtracting a positive number from $x$ will decrease $x$

   since we do the same thing if $slope_d(x) < 0$ or if $slope_d(x) > 0$,
   i collapsed them down into one case.

   this version of the game also has the interesting property that the
   step size changes depending on the slope. as the slope gets smaller and
   smaller, our step size gets smaller. that means we slow down as we get
   closer to our target and we're less likely to overshoot the minimum!

   so, now we know how to minimize a function, but the $f(x_1 + q)$ thing
   is a little weird, right? what if we made $q$ really really small? this
   is called a derivative.

     the derivative of a function at a point is the slope of the function
     at that point.

   that's pretty much all you need to know about derivatives for now.
   let's look at minimizing a function that is a little more complicated.

finding the minimum point on a surface

   imagine you're standing on a landscape with a bunch of hills and
   valleys. it's pitch black outside and you can't see anything. all you
   know is that you want to get to the bottom of a valley. so what do you
   do? you feel forward a little bit and see if the surface goes up or
   down. then you feel towards the right and see if the surface goes up or
   down. you keep feeling around and then you finally take a step in the
   direction that slopes the most downwards.

   this is the exact same approach we're going to take.

   let's say that we're trying to minimize some function $z = f(x,y)$:

   3d view of $f(x,y)$

   top view of $f(x,y)$. light yellow is the largest $z$ value and dark
   red is the smallest $z$ value

   we can't just say $slope_z(x, y)$ though, because there are multiple
   slopes to measure now. for example, at the point $(1,0)$, the slope in
   the $x$ direction (as we decrease $x$) is steeper than the slope in the
   $y$ direction (as we decrease $y$).

   that's pretty easily solvable. let's just say "the slope in the $y$
   direction" or "the slope in the $x$ direction" instead of just "the
   slope". let's write them down as follows:
     * the slope in the $x$ direction of $f(x,y)$ = $slope_{z\{x\}}(x, y)$
     * the slope in the $y$ direction of $f(x,y)$ = $slope_{z\{y\}}(x, y)$

   these are called partial derivatives.

   let's also define one more term: the gradient. the gradient of $f(x,y)$
   = $(slope_{z\{x\}}(x, y), slope_{z\{y\}}(x, y))$. basically the
   gradient is a vector made up of all the partial derivatives of a
   function. it has a really useful property:

     the gradient of a function points in the direction of the greatest
     rate of increase of the function

   that means the negative of the gradient is the direction that slopes
   the most downwards. sound familiar? that's exactly what we were doing
   in our pitch black landscape example!

   so now our game looks like this:

    1. a target value is chosen: the minimum value of $f(x,y)$
    2. start with a random $x$ value and a random $y$ value
    3. until you can't decrease $f(x,y)$ any more:
         1. compute the gradient of $f$
         2. subtract $slope_{z\{x\}}(x, y)$ from $x$ (i.e. $x = x -
            slope_{z\{x\}}(x, y)$)
         3. subtract $slope_{z\{y\}}(x, y)$ from $y$ (i.e. $y = y -
            slope_{z\{y\}}(x, y)$)

   this process is called id119 and it can be used to minimize
   any function.*

putting it together

   now that we have a $g(x)$ function that can represent any other
   function*, a $distance$ measure to see how close we are to correct, and
   a way to minimize any function*, we've solved the problems that we
   identified at the beginning of the post!

   this also means we can concretely answer the "how" question:

     deep learning uses neural networks with many layers combined with a
     distance function and a large list of examples to estimate a
     complicated function.

   let's apply this explanation to the colorization example at the
   beginning of the post:
     * the function we're trying to estimate is $real\_color\_image =
       f(black\_and\_white\_image)$
     * since we don't know how to write that function down, we have a
       large list of corresponding $(real\_color\_image,
       black\_and\_white\_image)$ pairs
     * our neural network is some function $colorized\_image =
       g(black\_and\_white\_image)$
          + (we'll look into this in more detail in a future post)
     * our distance function looks like this:
       $distance(real\_color\_image, colorized\_image)$

   now we can start to put all of these pieces together. let's substitute
   $f$ and $g$ into our $distance$ function:

   $d(black\_and\_white\_image) = distance(f(black\_and\_white\_image),
   g(black\_and\_white\_image))$

   since we don't have $f$, let's use one of our example image pairs
   instead:

   $d(black\_and\_white\_image, real\_color\_image) =
   distance(real\_color\_image, g(black\_and\_white\_image))$

   now we just have to minimize $d$ for every example we have!

   this presents a problem, however. in our "hot and cold" game, we
   modified an input to $d$ in order to minimize it, but we can't modify
   $black\_and\_white\_image$ or $real\_color\_image$ in the formula
   above. let's rewrite $d$ to make it more clear what we need to modify:

   $d(black\_and\_white\_image, real\_color\_image, g) =
   distance(real\_color\_image, g(black\_and\_white\_image))$

   we change our neural network.

   to minimize $d$, we're just playing the "hot and cold" game from above
   "in the $g$ direction"! remember all the $some\_constant$s? we can
   change all of those to make the network learn!

   so we start with a "random" neural network* and then slowly modify the
   constants until it does a good job on our examples. if we have enough
   examples and our network learns how to do a good job on them, it should
   be able to do a good job on inputs it hasn't seen before!*

   this process is called training a neural network.

   in the next post, we'll look into how exactly we do this. we'll also
   build a neural network to recognize handwritten digits!

   if you want to be notified when i publish the next part, you can follow
   me on twitter [14]here.

   please feel free to comment on [15]hacker news or [16]reddit or
   [17]email me if you have any questions!

   if you want to learn a little more about me, check out my [18]website
   or [19]linkedin

   while you're waiting on part 2, you can also check out [20]my primer on
   load balancing at scale.

   header image from [21]neural-style on github

   [22]vivek panyam's picture

[23]vivek panyam

   i love technology.
   tech lead / deep learning engineer @ uber atg
   previously @ uber, facebook, and google
   university of pennsylvania

share this post

   [24]twitter [25]facebook [26]google+

scaling a web service: load balancing

   this post is going to look at one aspect of how sites like facebook
   handle billions of requests and   

   [27]vivek panyam    2019

   proudly published with [28]ghost

references

   visible links
   1. http://blog.vivekpanyam.com/rss/
   2. http://blog.vivekpanyam.com/
   3. http://blog.vivekpanyam.com/contact
   4. https://github.com/jcjohnson/neural-style
   5. https://github.com/luanfujun/deep-photo-styletransfer
   6. https://www.wired.com/2016/01/in-a-huge-breakthrough-googles-ai-beats-a-top-player-at-the-game-of-go/
   7. https://storage.googleapis.com/deepmind-media/id25/id25naturepaper.pdf
   8. https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html
   9. http://cs.stanford.edu/people/karpathy/deepimagesent/
  10. http://richzhang.github.io/colorization
  11. https://en.wikipedia.org/wiki/euclidean_distance
  12. https://commons.wikimedia.org/wiki/file:neuron_hand-tuned.svg
  13. https://en.wikipedia.org/wiki/universal_approximation_theorem
  14. http://pany.am/twitter
  15. https://news.ycombinator.com/item?id=14777787
  16. https://www.reddit.com/r/programming/comments/6narhs/deep_learning_made_simple_part_1/
  17. https://blog.vivekpanyam.com/contact/
  18. https://www.vivekpanyam.com/
  19. http://pany.am/linkedin
  20. https://blog.vivekpanyam.com/scaling-a-web-service-load-balancing/
  21. https://github.com/jcjohnson/neural-style
  22. https://blog.vivekpanyam.com/author/vivek/
  23. https://blog.vivekpanyam.com/author/vivek/
  24. https://twitter.com/share?text=deep learning made simple [part 1]&url=http://blog.vivekpanyam.com/deep-learning-made-simple-part-1/
  25. https://www.facebook.com/sharer/sharer.php?u=http://blog.vivekpanyam.com/deep-learning-made-simple-part-1/
  26. https://plus.google.com/share?url=http://blog.vivekpanyam.com/deep-learning-made-simple-part-1/
  27. http://blog.vivekpanyam.com/
  28. https://ghost.org/

   hidden links:
  30. https://blog.vivekpanyam.com/scaling-a-web-service-load-balancing/
