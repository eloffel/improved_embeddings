                        [1]emnlp 2011 sixth workshop
                     on id151

                      shared task: machine translation

                             july 30 - 31, 2011
                                edinburgh, uk

      [[2]home] | [translation task] | [[3]featured translation task] |
      [[4]system combination task] [[5]evaluation task] | [[6]results]
                [[7]baseline system] | [[8]baseline system 2]
                [[9]schedule] | [[10]papers] | [[11]authors]

   the recurring translation task of the [12]wmt workshops focuses on
   european language pairs. translation quality will be evaluated on a
   shared, unseen test set of news stories. we provide a parallel corpus
   as training data, a baseline system, and additional resources [13]for
   download. participants may augment the baseline system or use their own
   system.

   this year we also have a [14]featured translation task: translating
   haitian creole sms messages into english. this task includes real-world
   data in the form of emergency response messages that were sent in the
   aftermath of the devastating 2010 haitian earthquake.

  goals

   the goals of the shared translation task are:
     * to investigate the applicability of current mt techniques when
       translating into languages other than english
     * to examine special challenges in translating between european
       languages, including word order differences and morphology
     * to create publicly available corpora for machine translation and
       machine translation evaluation
     * to generate up-to-date performance numbers for european languages
       in order to provide a basis of comparison in future research
     * to offer newcomers a smooth start with hands-on experience in
       state-of-the-art id151 methods

   we hope that both beginners and established research groups will
   participate in this task.

  task description

   we provide training data for four european language pairs, and a common
   framework (including a baseline system). the task is to improve methods
   current methods. this can be done in many ways. for instance
   participants could try to:
     * improve word alignment quality, phrase extraction, phrase scoring
     * add new components to the open source software of the baseline
       system
     * augment the system otherwise (e.g. by preprocessing, reranking,
       etc.)
     * build an entirely new translation systems

   participants will use their systems to translate a test set of unseen
   sentences in the source language. the translation quality is measured
   by a manual evaluation and various automatic id74.
   participants agree to contribute to the manual evaluation about eight
   hours of work.

   you may participate in any or all of the following language pairs:
     * french-english
     * spanish-english
     * german-english
     * czech-english

   for all language pairs we will test translation in both directions. to
   have a common framework that allows for comparable results, and also to
   lower the barrier to entry, we provide a common training set and
   baseline system.

   we also strongly encourage your participation, if you use your own
   training corpus, your own sentence alignment, your own language model,
   or your own decoder.

   if you use additional training data or existing translation systems,
   you must flag that your system uses additional data. we will
   distinguish system submissions that used the provided training data
   (constrained) from submissions that used significant additional data
   resources. note that basic linguistic tools such as taggers, parsers,
   or morphological analyzers are allowed in the constrained condition.

   your submission report should highlight in which ways your own methods
   and data differ from the standard task. we may break down submitted
   results in different tracks, based on what resources were used. we are
   mostly interested in submission that are constraint to the provided
   training data, so that the comparison is focused on the methods, not on
   the data used. you may submit contrastive runs to demonstrate the
   benefit of additional training data.

  training data

   the provided data is mainly taken from version 6 of the [15]europarl
   corpus, which is freely available. please click on the links below to
   download the sentence-aligned data, or go to the europarl website for
   the source release.

   additional training data is taken from the new news commentary corpus.
   there are about 45 million words of training data per language from the
   europarl corpus and 2 million words from the news commentary corpus.

   europarl
     * french-english
     * spanish-english
     * german-english
     * czech-english
     * french monolingual
     * spanish monolingual
     * german monolingual
     * czech monolingual
     * english monolingual

   news commentary
     * french-english
     * spanish-english
     * german-english
     * czech-english
     * french monolingual
     * spanish monolingual
     * german monolingual
     * czech monolingual
     * english monolingual

   news
     * french monolingual
     * spanish monolingual
     * german monolingual
     * english monolingual
     * czech monolingual

   united nations
     * french-english
     * spanish-english

   french-english 10^9 corpus
     * french-english

   crawled from canadian and european union sources. czeng
     * czech-english

   the current version of the czeng corpus (version v0.9) is available
   from the [16]czeng web site (note: same as last year).

   you may also use the following monolingual corpora released by the ldc:
     * ldc2009t13 [17]english gigaword fourth edition
     * ldc2007t07 [18]english gigaword third edition
     * ldc2006t17 [19]french gigaword first edition
     * ldc2009t28 [20]french gigaword second edition
     * ldc2006t12 [21]spanish gigaword first edition
     * ldc2009t21 [22]spanish gigaword second edition

   note that the released data is not tokenized and includes sentences of
   any length (including empty sentences). all data is in unicode (utf-8)
   format. the following tools allow the processing of the training data
   into tokenized format:
     * tokenizer tokenizer.perl
     * detokenizer detokenizer.perl
     * lowercaser lowercase.perl
     * sgml wrapper wrap-xml.perl

  development data

   to tune your system during development, we suggest using the 2008 test
   set of 2051 sentences. the data is provided in raw text format and in
   an sgml format that suits the nist scoring tool. we also release the
   2009 test set of 2525 sentences, a system combination tuning set of 502
   sentences. and the 2010 test set.

   news news-test2008
     * english
     * french
     * spanish
     * german
     * czech
     * hungarian

   this data is a cleaned version of the 2008 test set. news news-test2009
     * english
     * french
     * spanish
     * german
     * czech
     * hungarian
     * italian

   news news-test2010
     * english
     * french
     * spanish
     * german
     * czech

  download

     * [23]parallel corpus training data (612 mb) [24]md5 [25]sha1
     * [26]10^9french-english corpus (2.3 gb) ** [27]md5 [28]sha1
     * monolingual language model training data:
          + [29]download it all in one file (11 gb)
          + or download the data as separate files:
               o [30]from europarl (403mb) [31]md5 [32]sha1
               o [33]from the news commentary corpus (41mb) [34]md5
                 [35]sha1
               o [36]from the news crawl corpus (2007 only) (1.1 gb)
                 [37]md5 [38]sha1
               o [39]from the news crawl corpus (2008 only) (3.4 gb)
                 [40]md5 [41]sha1
               o [42]from the news crawl corpus (2009 only) (3.7 gb)
                 [43]md5 [44]sha1
               o [45]from the news crawl corpus (2010 only) (1.4 gb)
                 [46]md5 [47]sha1
               o [48]from the news crawl corpus (2011 only) (229 mb)
                 [49]md5 [50]sha1
     * [51]un corpus french-english (1.1 gb) [52]md5 [53]sha1
     * [54]un corpus spanish-english (961 mb) [55]md5 [56]sha1
     * [57]development sets (5.3 mb) [58]md5 [59]sha1
     * [60]tools (3 kb)
     * [61]script to normalize punctuation encoding (2kb)
     * [62]test sets (1.7 mb) [63]md5 [64]sha1

     ** nb same file as last year

  test set submission

   punctuation in the official test sets will be encoded with ascii
   characters (not complex unicode characters) as much as possible. you
   may want to [65]normalize your system's output before submission. you
   are able able to use a rawer version of the test sets that does not
   have this id172.

   to submit your results, please first convert into into sgml format as
   required by the nist id7 scorer, and then upload it to the website
   [66]matrix.statmt.org.

    sgml format

   each submitted file has to be in a format that is used by standard
   scoring scripts such as nist id7 or ter.

   this format is similar to the one used in the source test set files
   that were released, except for:
     * first line is <tstset trglang="en" setid="newstest2011"
       srclang="any">, with trglang set to either en, de, fr, es, or cz.
       important: srclang is always any.
     * each document tag also has to include the system name, e.g.
       sysid="uedin".
     * closing tag (last line) is </tstset>

   the script [67]wrap-xml.perl makes the conversion of a output file in
   one-segment-per-line format into the required sgml file very easy:

   format: wrap-xml.perl language src_sgml_file system_name < in > out
   example: wrap-xml.perl en newstest2011-src.de.sgm google <
   decoder-output > decoder-output.sgm

    upload to website

   upload happens in three easy steps:
     * go to the website [68]matrix.statmt.org.
     * create an account under the menu item account -> create account.
     * go to account -> upload/edit content, and follow the link "submit a
       system run"
          + select as test set "newstest2011" and the language pair you
            are submitting
          + select "create new system"
          + click "continue"
          + on the next page, upload your file and add some description

   if you are submitting contrastive runs, please submit your primary
   system first and mark it clearly as the primary submission.

  evaluation

   evaluation will be done both automatically as well as by human
   judgement.
     * manual scoring: we will collect subjective judgments about
       translation quality from human annotators. if you participate in
       the shared task, we ask you to commit about 8 hours of time to do
       the manual evaluation. the evaluation will be done with an online
       tool.
     * as in previous years, we expect the translated submissions to be in
       recased, detokenized, xml format, just as in most other translation
       campaigns (nist, tc-star).

  dates

   release of training data                  december 15, 2010
   test set distributed for translation task march 14, 2011
   submission deadline for translation task  march 20, 2011
   paper due date                            may 19, 2011

        [69][emplus_100px.png] supported by the [70]euromatrixplus project
                                                         p7-ist-231720-stp
                                         funded by the european commission
                                               under framework programme 7

references

   1. http://statmt.org/wmt11/index.html
   2. http://statmt.org/wmt11/index.html
   3. http://statmt.org/wmt11/featured-translation-task.html
   4. http://statmt.org/wmt11/system-combination-task.html
   5. http://statmt.org/wmt11/evaluation-task.html
   6. http://statmt.org/wmt11/results.html
   7. http://statmt.org/wmt11/baseline.html
   8. http://statmt.org/wmt11/baseline2.html
   9. http://statmt.org/wmt11/program.html
  10. http://statmt.org/wmt11/papers.html
  11. http://statmt.org/wmt11/authors.html
  12. http://statmt.org/wmt11/index.html
  13. http://statmt.org/wmt11/translation-task.html#download
  14. http://statmt.org/wmt11/featured-translation-task.html
  15. http://statmt.org/europarl/
  16. http://ufal.mff.cuni.cz/czeng/czeng09/
  17. http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2009t13
  18. http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2007t07
  19. http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2006t17
  20. http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2009t28
  21. http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2006t12
  22. http://www.ldc.upenn.edu/catalog/catalogentry.jsp?catalogid=ldc2009t21
  23. http://statmt.org/wmt11/training-parallel.tgz
  24. http://statmt.org/wmt11/training-parallel.tgz.md5
  25. http://statmt.org/wmt11/training-parallel.tgz.sha1
  26. http://statmt.org/wmt10/training-giga-fren.tar
  27. http://statmt.org/wmt10/training-giga-fren.tar.md5
  28. http://statmt.org/wmt10/training-giga-fren.tar.sha1
  29. http://statmt.org/wmt11/training-monolingual.tgz
  30. http://statmt.org/wmt11/training-monolingual-europarl.tgz
  31. http://statmt.org/wmt11/training-monolingual-europarl.tgz.md5
  32. http://statmt.org/wmt11/training-monolingual-europarl.tgz.sha1
  33. http://statmt.org/wmt11/training-monolingual-news-commentary.tgz
  34. http://statmt.org/wmt11/training-monolingual-news-commentary.tgz.md5
  35. http://statmt.org/wmt11/training-monolingual-news-commentary.tgz.sha1
  36. http://statmt.org/wmt11/training-monolingual-news-2007.tgz
  37. http://statmt.org/wmt11/training-monolingual-news-2007.tgz.md5
  38. http://statmt.org/wmt11/training-monolingual-news-2007.tgz.sha1
  39. http://statmt.org/wmt11/training-monolingual-news-2008.tgz
  40. http://statmt.org/wmt11/training-monolingual-news-2008.tgz.md5
  41. http://statmt.org/wmt11/training-monolingual-news-2008.tgz.sha1
  42. http://statmt.org/wmt11/training-monolingual-news-2009.tgz
  43. http://statmt.org/wmt11/training-monolingual-news-2009.tgz.md5
  44. http://statmt.org/wmt11/training-monolingual-news-2009.tgz.sha1
  45. http://statmt.org/wmt11/training-monolingual-news-2010.tgz
  46. http://statmt.org/wmt11/training-monolingual-news-2010.tgz.md5
  47. http://statmt.org/wmt11/training-monolingual-news-2010.tgz.sha1
  48. http://statmt.org/wmt11/training-monolingual-news-2011.tgz
  49. http://statmt.org/wmt11/training-monolingual-news-2011.tgz.md5
  50. http://statmt.org/wmt11/training-monolingual-news-2011.tgz.sha1
  51. http://statmt.org/wmt11/un.en-fr.tgz
  52. http://statmt.org/wmt11/un.en-fr.tgz.md5
  53. http://statmt.org/wmt11/un.en-fr.tgz.sha1
  54. http://statmt.org/wmt11/un.en-es.tgz
  55. http://statmt.org/wmt11/un.en-es.tgz.md5
  56. http://statmt.org/wmt11/un.en-es.tgz.sha1
  57. http://statmt.org/wmt11/dev.tgz
  58. http://statmt.org/wmt11/dev.tgz.md5
  59. http://statmt.org/wmt11/dev.tgz.sha1
  60. http://statmt.org/wmt08/scripts.tgz
  61. http://statmt.org/wmt11/normalize-punctuation.perl
  62. http://statmt.org/wmt11/test.tgz
  63. http://statmt.org/wmt11/test.tgz.md5
  64. http://statmt.org/wmt11/test.tgz.sha1
  65. http://statmt.org/wmt11/normalize-punctuation.perl
  66. http://matrix.statmt.org/
  67. http://statmt.org/wmt11/wrap-xml.perl
  68. http://matrix.statmt.org/
  69. http://www.euromatrixplus.net/
  70. http://www.euromatrixplus.net/
