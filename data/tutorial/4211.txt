   #[1]the keras blog atom feed

                               [2]the keras blog

   [3]keras is a deep learning library for python, that is simple,
   modular, and extensible.

     * [4]archives
     * [5]github
     * [6]documentation
     * [7]google group

                      [8]the limitations of deep learning

   mon 17 july 2017


    by [9]francois chollet

   in [10]essays.

   this post is adapted from section 2 of chapter 9 of my book, [11]deep
   learning with python (manning publications).

   [12]deep learning with python

   it is part of a series of two posts on the current limitations of deep
   learning, and its future.

   this post is targeted at people who already have significant experience
   with deep learning (e.g. people who have read chapters 1 through 8 of
   the book). we assume a lot of pre-existing knowledge.
     __________________________________________________________________

deep learning: the geometric view

   the most surprising thing about deep learning is how simple it is. ten
   years ago, no one expected that we would achieve such amazing results
   on machine perception problems by using simple parametric models
   trained with id119. now, it turns out that all you need is
   sufficiently large parametric models trained with id119 on
   sufficiently many examples. as feynman once said about the universe,
   "it's not complicated, it's just a lot of it".

   in deep learning, everything is a vector, i.e. everything is a point in
   a geometric space. model inputs (it could be text, images, etc) and
   targets are first "vectorized", i.e. turned into some initial input
   vector space and target vector space. each layer in a deep learning
   model operates one simple geometric transformation on the data that
   goes through it. together, the chain of layers of the model forms one
   very complex geometric transformation, broken down into a series of
   simple ones. this complex transformation attempts to maps the input
   space to the target space, one point at a time. this transformation is
   parametrized by the weights of the layers, which are iteratively
   updated based on how well the model is currently performing. a key
   characteristic of this geometric transformation is that it must be
   differentiable, which is required in order for us to be able to learn
   its parameters via id119. intuitively, this means that the
   geometric morphing from inputs to outputs must be smooth and
   continuous   a significant constraint.

   the whole process of applying this complex geometric transformation to
   the input data can be visualized in 3d by imagining a person trying to
   uncrumple a paper ball: the crumpled paper ball is the manifold of the
   input data that the model starts with. each movement operated by the
   person on the paper ball is similar to a simple geometric
   transformation operated by one layer. the full uncrumpling gesture
   sequence is the complex transformation of the entire model. deep
   learning models are mathematical machines for uncrumpling complicated
   manifolds of high-dimensional data.

   that's the magic of deep learning: turning meaning into vectors, into
   geometric spaces, then incrementally learning complex geometric
   transformations that map one space to another. all you need are spaces
   of sufficiently high dimensionality in order to capture the full scope
   of the relationships found in the original data.

the limitations of deep learning

   the space of applications that can be implemented with this simple
   strategy is nearly infinite. and yet, many more applications are
   completely out of reach for current deep learning techniques   even given
   vast amounts of human-annotated data. say, for instance, that you could
   assemble a dataset of hundreds of thousands   even millions   of english
   language descriptions of the features of a software product, as written
   by a product manager, as well as the corresponding source code
   developed by a team of engineers to meet these requirements. even with
   this data, you could not train a deep learning model to simply read a
   product description and generate the appropriate codebase. that's just
   one example among many. in general, anything that requires
   reasoning   like programming, or applying the scientific method   long-term
   planning, and algorithmic-like data manipulation, is out of reach for
   deep learning models, no matter how much data you throw at them. even
   learning a sorting algorithm with a deep neural network is tremendously
   difficult.

   this is because a deep learning model is "just" a chain of simple,
   continuous geometric transformations mapping one vector space into
   another. all it can do is map one data manifold x into another manifold
   y, assuming the existence of a learnable continuous transform from x to
   y, and the availability of a dense sampling of x:y to use as training
   data. so even though a deep learning model can be interpreted as a kind
   of program, inversely most programs cannot be expressed as deep
   learning models   for most tasks, either there exists no corresponding
   practically-sized deep neural network that solves the task, or even if
   there exists one, it may not be learnable, i.e. the corresponding
   geometric transform may be far too complex, or there may not be
   appropriate data available to learn it.

   scaling up current deep learning techniques by stacking more layers and
   using more training data can only superficially palliate some of these
   issues. it will not solve the more fundamental problem that deep
   learning models are very limited in what they can represent, and that
   most of the programs that one may wish to learn cannot be expressed as
   a continuous geometric morphing of a data manifold.

the risk of anthropomorphizing machine learning models

   one very real risk with contemporary ai is that of misinterpreting what
   deep learning models do, and overestimating their abilities. a
   fundamental feature of the human mind is our "theory of mind", our
   tendency to project intentions, beliefs and knowledge on the things
   around us. drawing a smiley face on a rock suddenly makes it "happy"   in
   our minds. applied to deep learning, this means that when we are able
   to somewhat successfully train a model to generate captions to describe
   pictures, for instance, we are led to believe that the model
   "understands" the contents of the pictures, as well as the captions it
   generates. we then proceed to be very surprised when any slight
   departure from the sort of images present in the training data causes
   the model to start generating completely absurd captions.

   failure of a deep learning-based image captioning system.

   in particular, this is highlighted by "adversarial examples", which are
   input samples to a deep learning network that are designed to trick the
   model into misclassifying them. you are already aware that it is
   possible to do gradient ascent in input space to generate inputs that
   maximize the activation of some convnet filter, for instance   this was
   the basis of the filter visualization technique we introduced in
   chapter 5 (note: of [13]deep learning with python), as well as the deep
   dream algorithm from chapter 8. similarly, through gradient ascent, one
   can slightly modify an image in order to maximize the class prediction
   for a given class. by taking a picture of a panda and adding to it a
   "gibbon" gradient, we can get a neural network to classify this panda
   as a gibbon. this evidences both the brittleness of these models, and
   the deep difference between the input-to-output mapping that they
   operate and our own human perception.

   an adversarial example: imperceptible changes in an image can upend a
   model's classification of the image.

   in short, deep learning models do not have any understanding of their
   input, at least not in any human sense. our own understanding of
   images, sounds, and language, is grounded in our sensorimotor
   experience as humans   as embodied earthly creatures. machine learning
   models have no access to such experiences and thus cannot "understand"
   their inputs in any human-relatable way. by annotating large numbers of
   training examples to feed into our models, we get them to learn a
   geometric transform that maps data to human concepts on this specific
   set of examples, but this mapping is just a simplistic sketch of the
   original model in our minds, the one developed from our experience as
   embodied agents   it is like a dim image in a mirror.

   current machine learning models: like a dim image in a mirror.

   as a machine learning practitioner, always be mindful of this, and
   never fall into the trap of believing that neural networks understand
   the task they perform   they don't, at least not in a way that would make
   sense to us. they were trained on a different, far narrower task than
   the one we wanted to teach them: that of merely mapping training inputs
   to training targets, point by point. show them anything that deviates
   from their training data, and they will break in the most absurd ways.

local generalization versus extreme generalization

   there just seems to be fundamental differences between the
   straightforward geometric morphing from input to output that deep
   learning models do, and the way that humans think and learn. it isn't
   just the fact that humans learn by themselves from embodied experience
   instead of being presented with explicit training examples. aside from
   the different learning processes, there is a fundamental difference in
   the nature of the underlying representations.

   humans are capable of far more than mapping immediate stimuli to
   immediate responses, like a deep net, or maybe an insect, would do.
   they maintain complex, abstract models of their current situation, of
   themselves, of other people, and can use these models to anticipate
   different possible futures and perform long-term planning. they are
   capable of merging together known concepts to represent something they
   have never experienced before   like picturing a horse wearing jeans, for
   instance, or imagining what they would do if they won the lottery. this
   ability to handle hypotheticals, to expand our mental model space far
   beyond what we can experience directly, in a word, to perform
   abstraction and reasoning, is arguably the defining characteristic of
   human cognition. i call it "extreme generalization": an ability to
   adapt to novel, never experienced before situations, using very little
   data or even no new data at all.

   this stands in sharp contrast with what deep nets do, which i would
   call "local generalization": the mapping from inputs to outputs
   performed by deep nets quickly stops making sense if new inputs differ
   even slightly from what they saw at training time. consider, for
   instance, the problem of learning the appropriate launch parameters to
   get a rocket to land on the moon. if you were to use a deep net for
   this task, whether training using supervised learning or reinforcement
   learning, you would need to feed it with thousands or even millions of
   launch trials, i.e. you would need to expose it to a dense sampling of
   the input space, in order to learn a reliable mapping from input space
   to output space. by contrast, humans can use their power of abstraction
   to come up with physical models   rocket science   and derive an exact
   solution that will get the rocket on the moon in just one or few
   trials. similarly, if you developed a deep net controlling a human
   body, and wanted it to learn to safely navigate a city without getting
   hit by cars, the net would have to die many thousands of times in
   various situations until it could infer that cars and dangerous, and
   develop appropriate avoidance behaviors. dropped into a new city, the
   net would have to relearn most of what it knows. on the other hand,
   humans are able to learn safe behaviors without having to die even
   once   again, thanks to their power of abstract modeling of hypothetical
   situations.

   local generalization vs. extreme generalization.

   in short, despite our progress on machine perception, we are still very
   far from human-level ai: our models can only perform local
   generalization, adapting to new situations that must stay very close
   from past data, while human cognition is capable of extreme
   generalization, quickly adapting to radically novel situations, or
   planning very for long-term future situations.

take-aways

   here's what you should remember: the only real success of deep learning
   so far has been the ability to map space x to space y using a
   continuous geometric transform, given large amounts of human-annotated
   data. doing this well is a game-changer for essentially every industry,
   but it is still a very long way from human-level ai.

   to lift some of these limitations and start competing with human
   brains, we need to move away from straightforward input-to-output
   mappings, and on to reasoning and abstraction. a likely appropriate
   substrate for abstract modeling of various situations and concepts is
   that of computer programs. we have said before (note: in [14]deep
   learning with python) that machine learning models could be defined as
   "learnable programs"; currently we can only learn programs that belong
   to a very narrow and specific subset of all possible programs. but what
   if we could learn any program, in a modular and reusable way? let's see
   in the next post what the road ahead may look like.

   you can read the second part here: [15]the future of deep learning.

   [16]@fchollet, may 2017


    powered by [17]pelican, which takes great advantages of [18]python.

references

   1. https://blog.keras.io/
   2. https://blog.keras.io/index.html
   3. https://github.com/fchollet/keras
   4. https://blog.keras.io/
   5. https://github.com/fchollet/keras
   6. http://keras.io/
   7. https://groups.google.com/forum/#!forum/keras-users
   8. https://blog.keras.io/the-limitations-of-deep-learning.html
   9. https://twitter.com/fchollet
  10. https://blog.keras.io/category/essays.html
  11. https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff
  12. https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff
  13. https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff
  14. https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff
  15. https://blog.keras.io/the-future-of-deep-learning.html
  16. https://twitter.com/fchollet
  17. http://alexis.notmyidea.org/pelican/
  18. http://python.org/
