    #[1]nick becker feed

   (button)
     * [2]nick becker
     * [3]about
     * [4]data science
     * [5]visuals

   understanding convolution, the core of convolutional neural networks
   photo credit: [6]original photo: wikia, edited by nick becker

   nick becker

nick becker

   rapids team at nvidia
   (button) follow
     * new york, ny
     * [7]linkedin
     * [8]github

understanding convolution, the core of convolutional neural networks

   9 minute read

   deep learning is all the rage right now. convolutional neural networks
   are particularly hot, achieving state of the art performance on image
   recognition, text classification, and even drug discovery.

   since i didn   t take any courses on deep learning in college, i figured
   i should start at the core of convolutional nets to really understand
   them. in a series of posts, i   ll walk through convolution, standard
   neural networks, and convolutional networks in python/tensorflow.

   so here we go. time to dive into image transformation with convolution.

what is convolution?

   in math, convolution is essentially the blending of two functions into
   a third function. in the context of image processing, convolution is
   kind of like transforming image pixels in a structured way, taking
   nearby pixels into account. in terms of coding, let   s think of an image
   as a 2-d array of pixels with 3 channels (reg, green, and blue). i   m
   going to abstract away from the color aspect of the image (so grayscale
   only), but the logic of this post extends naturally to the multichannel
   2-d image. we want to transform each element of the array in a
   structured way, taking into account nearby elements.

   let   s look at an image and its pixel array. for this post, i   ll
   download a grayscale picture of a lion from google images. the lion
   image i   m using comes from [9]here.
import numpy as np
from pil import image
from scipy import misc
from skimage import data
from skimage.color import rgb2gray
import matplotlib.pyplot as plt
%matplotlib inline
import requests
from stringio import stringio
from __future__ import division

response = requests.get('http://vignette2.wikia.nocookie.net/grayscale/images/4/
47/lion.png/revision/latest?cb=20130926182831')
lion_arr = np.array(image.open(stringio(response.content)))

   first let   s see the lion.
plt.imshow(lion_arr)

   png

   looks good. however, though it does look grayscale, this image actually
   still has three channels (red, green, and blue). we can see that by
   looking at the shape of the array. maybe all the color channels are
   actually the same?
print lion_arr.shape
print np.array_equal(lion_arr[:, :, 0], lion_arr[:, :, 1])
print np.array_equal(lion_arr[:, :, 1], lion_arr[:, :, 2])

(303, 497, 3)
true
true

   perfect. if we imagine this 3-d array as 3 separate 2-d arrays, each
   2-d array is identical. let   s just use the first channel as the image.
lion_arr = lion_arr[:, :, 0]

   let   s take a look at the image array. we   ve got rows and columns of
   numbers between 0 and 255 representing pixels.
lion_arr[:200, :400]

array([[  0,   0,   0, ...,   0,   0,   0],
       [  2,   2,   1, ...,   0,   0,   0],
       [  0,   0,   0, ...,   0,   0,   0],
       ...,
       [  0,   1,   0, ...,   5,  86,  97],
       [  0,   1,   0, ...,  56, 114, 114],
       [  0,   1,   0, ..., 122, 130, 143]], dtype=uint8)

   if we wanted to perform a convolution on the array, we   d loop through
   and transform each element in the same way. we   ll define a 2-d kernel
   that we   ll then apply to (multiply with) every element in the array.
   but how do we actually use the 2-d kernel?

convolution with 2-d kernels

   with a 2-d kernel, we need to apply our kernel to patches of the image
   with the same shape as the kernel. since we still want to output a
   scalar from our convolution, we   ll multiply our kernel and the patch
   and then take the sum of the resulting output array.

   there   s a problem though. imagine sequentially moving a 3 x 3 patch
   through a 5 x 5 image. you   d start with the top left corner, and move
   across the image until you hit the top right corner. that would only
   take take 3 moves. by this logic, we   d end up with a smaller image than
   we started with. to avoid this, we pad the array with zeros on every
   side. the zeros allow us to apply our kernel to every pixel.

   more generally, to pass an m x m kernel over a p x q image, we   d need
   to pad the image with m - 2 zeros on every side (where m is an odd
   number). we assume m is odd so that the kernel has a    center   .

   time to do it. i   ll convolve a 3 x 3 kernel on the lion image. first
   i   ll pad the array with a zero at the end of each row and column. then
   i   ll define a 3 x 3 kernel, pass it over every 3 x 3 patch in the
   padded image, and do elementwise multiplication of the 3 x 3 kernel and
   3 x 3 array. i   ll store the sum of that transformation in an output
   array, which will be the same size as our original lion array.
padded_array = np.pad(lion_arr, (1, 1), 'constant')
kernel = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])
output_array = np.zeros(lion_arr.shape)

for i in xrange(padded_array.shape[0]-2):
    for j in xrange(padded_array.shape[1]-2):
        temp_array = padded_array[i:i+3, j:j+3]
        output_array[i, j] = np.sum(temp_array*kernel)

   let   s look at the output.
plt.imshow(output_array, cmap = plt.get_cmap('gray'))

   png

   it   s the same! why? because of the kernel we chose. for any given patch
   in the image, our convolution is just outputting 1 * the middle element
   of the patch. every other element-to-element multiplication becomes 0
   due to the kernel. for this reason, we call this kernel the identity
   kernel.

standard convolution

   let   s go through the example kernels listed on this wikipedia [10]page.
   first, i   ll define a function to convolve a 2-d kernel on an image.
   since we   re calculating sums, our output values can be greater than 255
   or less than 0. we might want to squash those values to 255 and 0,
   respectively, so i   ll write a function to do that. there are better
   ways to handle this phenomenon (such as biasing the output value by a
   certain amount), but this works for now.
def squash_pixel_value(value):
    if value < 0:
        return 0
    elif value < 255:
        return value
    else:
        return 255

def conv_2d_kernel(image_array_2d, kernel, squash_pixels = true):
    padded_array = np.pad(image_array_2d, (1, 1), 'constant')

    kernel_width = kernel.shape[0]
    kernel_height = kernel.shape[1]

    transformed_array = np.zeros(image_array_2d.shape)

    for i in xrange(padded_array.shape[0] - kernel_width + 1):
        for j in xrange(padded_array.shape[1] - kernel_height + 1):
            temp_array = padded_array[i:i+kernel_width, j:j+kernel_height]
            #print temp_array.shape
            if squash_pixels:
                transformed_array[i, j] = squash_pixel_value(np.sum(temp_array*k
ernel))
            else:
                transformed_array[i, j] = np.sum(temp_array*kernel)
    return transformed_array

   all set. we   re ready to go through each transformation on the wikipedia
   page. let   s see how they turn out.

edge detection

edge_kernel_1 = np.array([[1, 0, -1],
                          [0, 0, 0],
                          [-1, 0, 1]])

edge_kernel_2 = np.array([[0, 1, 0],
                          [1, -4, 1],
                          [0, 1, 0]])

edge_kernel_3 = np.array([[-1, -1, -1],
                          [-1, 8, -1],
                          [-1, -1, -1]])

lion_transf_edge1 = conv_2d_kernel(lion_arr, kernel = edge_kernel_1, squash_pixe
ls = true)
lion_transf_edge2 = conv_2d_kernel(lion_arr, kernel = edge_kernel_2, squash_pixe
ls = true)
lion_transf_edge3 = conv_2d_kernel(lion_arr, kernel = edge_kernel_3, squash_pixe
ls = true)

f, ax_array = plt.subplots(2, 2)
f.set_figheight(10)
f.set_figwidth(15)
ax_array[0, 0].imshow(lion_arr, cmap = plt.get_cmap('gray'))
ax_array[0, 0].set_title('original image')
ax_array[0, 0].axis('off')
ax_array[0, 1].imshow(lion_transf_edge1, cmap = plt.get_cmap('gray'))
ax_array[0, 1].set_title('edge kernel 1')
ax_array[0, 1].axis('off')
ax_array[1, 0].imshow(lion_transf_edge2, cmap = plt.get_cmap('gray'))
ax_array[1, 0].set_title('edge kernel 2')
ax_array[1, 0].axis('off')
ax_array[1, 1].imshow(lion_transf_edge3, cmap = plt.get_cmap('gray'))
ax_array[1, 1].set_title('edge kernel 3')
ax_array[1, 1].axis('off')

   png

   awesome! looks like each of the edge kernels make the edges
   successively more distinct.

sharpen

sharpen_kernel = np.array([[0, -1, 0],
                           [-1, 5, -1],
                           [0, -1, 0]])

unsharp_kernel = np.array([[1, 4, 6, 4, 1],
                           [4, 16, 24, 16, 4],
                           [6, 24, -476, 24, 6],
                           [4, 16, 24, 16, 4],
                           [1, 4, 6, 4, 1]]) / -256

lion_transf_sharpen = conv_2d_kernel(lion_arr, kernel = sharpen_kernel, squash_p
ixels = true)
lion_transf_unsharp = conv_2d_kernel(lion_arr, kernel = unsharp_kernel, squash_p
ixels = true)

f, ax_array = plt.subplots(3, 1)
f.set_figheight(15)
f.set_figwidth(12)
ax_array[0].imshow(lion_arr, cmap = plt.get_cmap('gray'))
ax_array[0].set_title('original image')
ax_array[0].axis('off')
ax_array[1].imshow(lion_transf_sharpen, cmap = plt.get_cmap('gray'))
ax_array[1].set_title('sharpen kernel')
ax_array[1].axis('off')
ax_array[2].imshow(lion_transf_unsharp, cmap = plt.get_cmap('gray'))
ax_array[2].set_title('unsharp kernel')
ax_array[2].axis('off')

(-0.5, 496.5, 302.5, -0.5)

   png

   these look good, too. the convolution with the sharpen kernel clearly
   sharpened the image, and the unsharp kernel does look slightly sharper
   than the original image (though not by much).

blur

blur_box_kernel = np.ones((3, 3)) / 9
blur_gaussian_kernel = np.array([[1,2,1],
                                 [2,4,2],
                                 [1,2,1]]) / 16

lion_transf_blur_box = conv_2d_kernel(lion_arr, kernel = blur_box_kernel, squash
_pixels = true)
lion_transf_blur_gaussian = conv_2d_kernel(lion_arr, kernel = blur_gaussian_kern
el, squash_pixels = true)

f, ax_array = plt.subplots(3, 1)
f.set_figheight(15)
f.set_figwidth(12)

ax_array[0].imshow(lion_arr, cmap = plt.get_cmap('gray'))
ax_array[0].set_title('original image')
ax_array[0].axis('off')
ax_array[1].imshow(lion_transf_blur_box, cmap = plt.get_cmap('gray'))
ax_array[1].set_title('box kernel blur')
ax_array[1].axis('off')
ax_array[2].imshow(lion_transf_blur_gaussian, cmap = plt.get_cmap('gray'))
ax_array[2].set_title('gaussian kernel blur')
ax_array[2].axis('off')

   png

   definitely blurrier.

   okay. so i   ve got the convolution math and application down. since i   m
   going to eventually build a convolutional neural net using tensorflow,
   i should really understand tensorflow   s 2-d convolution function,
   nn.conv2d. so how does it work?

convolution in tensorflow

   from the [11]documentation, conv2d:

   computes a 2-d convolution given 4-d input and filter tensors. given an
   input tensor of shape [batch, in_height, in_width, in_channels] and a
   filter / kernel tensor of shape [filter_height, filter_width,
   in_channels, out_channels]

   for the input tensor, since we   re only convolving one image, our batch
   = 1. the in_height and in_width are the dimensions of the image, (303,
   497), and in_channels = 1 since we have a grayscale image.

   for the kernel tensor, the filter_height and filter_width are the
   dimensions of the kernel (3, 3). in_channels = 1 since it has to match
   the input tensor, and out_channels = 1 since we want another grayscale
   image. so i need to reshape my 2-d arrays into this 4-d shape. i   ll
   choose the blur kernel for this example.
lion_array_4d = lion_arr.reshape(-1, 303, 497, 1)
blur_kernel_4d = blur_box_kernel.reshape(3, 3, 1, 1)

   understanding the next code block requires some knowledge of
   tensorflow, so if anyone is interested in learning about it i recommend
   checking out one of google   s [12]tutorials.
import tensorflow as tf

graph = tf.graph()
with graph.as_default():
    tf_input_image = tf.variable(np.array(lion_array_4d, dtype = np.float32))
    tf_blur_kernel = tf.variable(np.array(blur_kernel_4d, dtype = np.float32))

    tf_convolution_output = tf.nn.conv2d(tf_input_image, tf_blur_kernel, strides
 = [1, 1, 1, 1], padding = 'same')

with tf.session(graph = graph) as sess:
    tf.initialize_all_variables().run()
    transformed_image = tf_convolution_output.eval()
    transformed_image = transformed_image[0, :, :, 0]

   so what did that code do? in the first with statement, i initialized
   the input and kernel tensors (with values as floats) and the
   convolution. in the second with statement, i executed the tensorflow
   graph and evaluated the convolution. conv2d also needs parameters
   strides and padding. strides = [1, 1, 1, 1] results in a convolution on
   every pixel and padding = 'same' is the standard zero padding that
   results in an output array with the same shape as the input array.

   our new output array should be the same as our hand-calculated output
   array lion_transf_blur_box. i   ll test whether they are equal to 4
   decimal places using np.testing.assert_array_almost_equal. if they
   aren   t equal, this will throw an error.
np.testing.assert_array_almost_equal(lion_transf_blur_box, transformed_image,
                             decimal = 4)

   perfect! i got the same output using my conv_2d_id81 and
   tensorflow   s tf.nn.conv2d function. let   s compare the original image,
   the original blur, and the new blur.
f, ax_array = plt.subplots(3, 1)
f.set_figheight(15)
f.set_figwidth(12)
ax_array[0].imshow(lion_arr, cmap = plt.get_cmap('gray'))
ax_array[0].axis('off')
ax_array[1].imshow(lion_transf_blur_box, cmap = plt.get_cmap('gray'))
ax_array[1].axis('off')
ax_array[2].imshow(transformed_image, cmap = plt.get_cmap('gray'))
ax_array[2].axis('off')

   png

next steps

   i   ve got a pretty good handle on convolution for image analysis at this
   point. but it   s not clear how we go from this to convolutional neural
   networks. in the next post, i   ll walk through a simple neural network,
   and then eventually build a convolutional net.

   tags: [13]neural networks

   updated: september 17, 2016

share on

   [14]twitter [15]facebook [16]google+ [17]linkedin

   [18]previous [19]next

leave a comment

you may also enjoy

[20]evaluating models with small data

   8 minute read

   or, why point estimates only get you so far.

[21]deriving the sigmoid derivative for neural networks

   3 minute read

   sigmoid, derivatives, mathematics

[22]finding the most one-dimensional popular artist

   5 minute read

   music analysis, web apis, math

[23]the right way to oversample in predictive modeling

   6 minute read

   model evaluation, oversampling, predictive modeling

     * follow:
     * [24]github
     * [25]feed

      2019 nick becker. powered by [26]jekyll & [27]minimal mistakes.

   all analyses and conclusions presented on this website reflect my views
   and do not indicate concurrence by the board of governors or the
   federal reserve system.

   please enable javascript to view the [28]comments powered by disqus.

references

   1. https://beckernick.github.io/feed.xml
   2. https://beckernick.github.io/
   3. https://beckernick.github.io/about/
   4. https://beckernick.github.io/datascience/
   5. https://beckernick.github.io/visuals/
   6. http://grayscale.wikia.com/wiki/file:lion.png
   7. https://www.linkedin.com/in/nicholasebecker
   8. https://github.com/beckernick
   9. http://grayscale.wikia.com/wiki/file:lion.png
  10. https://en.wikipedia.org/wiki/kernel_(image_processing)
  11. https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#conv2d
  12. https://www.tensorflow.org/versions/r0.10/tutorials/index.html
  13. https://beckernick.github.io/tags/#neural-networks
  14. https://twitter.com/intent/tweet?text=https://beckernick.github.io/convolutions/
  15. https://www.facebook.com/sharer/sharer.php?u=https://beckernick.github.io/convolutions/
  16. https://plus.google.com/share?url=https://beckernick.github.io/convolutions/
  17. https://www.linkedin.com/sharearticle?mini=true&url=https://beckernick.github.io/convolutions/
  18. https://beckernick.github.io/yelp-reviews/
  19. https://beckernick.github.io/job-discovery/
  20. https://beckernick.github.io/evaluating_models_with_small_data/
  21. https://beckernick.github.io/sigmoid-derivative-neural-network/
  22. https://beckernick.github.io/one-dimensional-music-ranking/
  23. https://beckernick.github.io/oversampling-modeling/
  24. http://github.com/beckernick
  25. https://beckernick.github.io/feed.xml
  26. http://jekyllrb.com/
  27. https://mademistakes.com/work/minimal-mistakes-jekyll-theme/
  28. http://disqus.com/?ref_noscript
