    #[1]nick becker feed

   (button)
     * [2]nick becker
     * [3]about
     * [4]data science
     * [5]visuals

   building a neural network from scratch in python and in tensorflow
   photo credit: ginny lehman

   nick becker

nick becker

   rapids team at nvidia
   (button) follow
     * new york, ny
     * [6]linkedin
     * [7]github

building a neural network from scratch in python and in tensorflow

   19 minute read

   this is part two of a three part series on [8]convolutional neural
   networks.

   [9]part one detailed the basics of image convolution. this post will
   detail the basics of neural networks with hidden layers. as in the last
   post, i   ll implement the code in both standard python and tensorflow.

   let me say at the outset that this post will be similar to several
   other posts walking through neural networks (google   s tensorflow
   tutorials, andrej karpathy   s lecture notes, and others   s posts). this
   somewhat begs the question: why make this post?

   the main reason is that doing this on my own signifcantly improved my
   understanding of neural networks. by actually trying to build a neural
   network from scratch, i went from a general, high-level understanding
   of neural networks to a detailed, low-level understanding.

   the secondary reason is that if i phrase things slightly differently
   and someone is able to understand neural networks a little better by
   reading this in addition to the other (vastly superior) resources, this
   was worth putting out there.

   okay. enough introduction. time to briefly and poorly explain what
   neural networks are, generate some data, and start building a neural
   network.

what is a neural network?

   i think a good way to think of a neural network is as a multi-layered
   decision system. information is fed into the network, linearly
   transformed and then non-linearly transformed, then linearly
   transformed and non-linearly transformed again, and so on until a
   decision is made after the final transformation. the key to the
   network, then, is figuring out effective transformations of the data to
   get good decisions.

   decades of research have produced some pretty amazing insights into how
   we might want to transform the data. though neural networks have been
   around for a while, they went out of fashion for a couple of decades. a
   key insight that helped bring them back into the fold was a clever way
   to efficiently determine effective linear transformations, given
   specified non-linear ones.

   now, the research field is more active than ever before. people are
   developing unbelievably effective and sophisticated networks for tons
   of problems. i won   t cover any of these networks in this post, but they
   are powering a good chunk of the technology we use today.

   before designing a network, though, i need some data to play with.

generating data

   to start off, i   ll create a dataset with several mostly separable
   clusters of points. though how clear the clusters are doesn   t really
   affect how i   m going to build the network, my ego makes me want to
   ultimately have a successful model. i can easily generate random
   clusters of data by sampling from a multivariate normal distribution
   centered at different points. i   ll use different covariance matrices
   just because it makes the data look more interesting.
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
%matplotlib inline

np.random.seed(12)
num_observations = 5000

x1 = np.random.multivariate_normal([0, 0], [[2, .75],[.75, 2]], num_observations
)
x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations
)
x3 = np.random.multivariate_normal([2, 8], [[0, .75],[.75, 0]], num_observations
)

simulated_separableish_features = np.vstack((x1, x2, x3)).astype(np.float32)
simulated_labels = np.hstack((np.zeros(num_observations),
                                np.ones(num_observations), np.ones(num_observati
ons) + 1))

plt.figure(figsize=(12,8))
plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_featur
es[:, 1],
            c = simulated_labels, alpha = .4)

   png

   okay, so the dataset looks good. time to get it ready for tensorflow.

data preparation

   since i   m using simulated data, i don   t have to do any data cleaning
   and i barely have to do any prep. tensorflow expects each feature label
   to be a one-hot encoded vector, so i   ll reformat simulated_labels. then
   i   ll create a training and test set so we can see how well the model
   generalizes to unseen data.

   in general, i would want train a predictive model with a validation
   dataset or by doing cross-validation. but, since i   m not going to be
   tuning any hyperparameters in this example, i   m not that worried about
   it.
labels_onehot = np.zeros((simulated_labels.shape[0], 3)).astype(int)
labels_onehot[np.arange(len(simulated_labels)), simulated_labels.astype(int)] =
1

train_dataset, test_dataset, \
train_labels, test_labels = train_test_split(
    simulated_separableish_features, labels_onehot, test_size = .1, random_state
 = 12)

   with the data split, it   s time to train the model.

building a neural network in tensorflow

   in tensorflow, there are two high level steps to in building a network:
    1. setting up the graph.
    2. executing the graph to train the model.

   i   m not going to walk through every step of this code, since the focus
   of this post is building the network without tensorflow. however, my
   tensorflow model was based on one of the modules from the udacity
   course on deep learning with tensorflow. i would highly encourage
   anyone curious about tensorflow to check it out.

   to classify my data, i   m going to build a neural network with three
   layers: an input layer, a hidden layer, and an output layer. the
   following code sets up my neural network with 5 neurons in the hidden
   layer.
hidden_nodes = 5
num_labels = train_labels.shape[1]
batch_size = 100
num_features = train_dataset.shape[1]
learning_rate = .01

graph = tf.graph()
with graph.as_default():

    # data
    tf_train_dataset = tf.placeholder(tf.float32, shape = [none, num_features])
    tf_train_labels = tf.placeholder(tf.float32, shape = [none, num_labels])
    tf_test_dataset = tf.constant(test_dataset)

    # weights and biases
    layer1_weights = tf.variable(tf.truncated_normal([num_features, hidden_nodes
]))
    layer1_biases = tf.variable(tf.zeros([hidden_nodes]))

    layer2_weights = tf.variable(tf.truncated_normal([hidden_nodes, num_labels])
)
    layer2_biases = tf.variable(tf.zeros([num_labels]))

    # three-layer network
    def three_layer_network(data):
        input_layer = tf.matmul(data, layer1_weights)
        hidden = tf.nn.relu(input_layer + layer1_biases)
        output_layer = tf.matmul(hidden, layer2_weights) + layer2_biases
        return output_layer

    # model scores
    model_scores = three_layer_network(tf_train_dataset)

    # loss
    loss = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(model_scores,
tf_train_labels))

    # optimizer.
    optimizer = tf.train.gradientdescentoptimizer(learning_rate).minimize(loss)

    # predictions
    train_prediction = tf.nn.softmax(model_scores)
    test_prediction = tf.nn.softmax(three_layer_network(tf_test_dataset))

   with the network set up, i can now train it in batches with mini batch
   id119 (or with full passes over the data). since i want to
   pass multiple times over the data, i   ll define an offset variable that
   will let me partition the data into mini batches and wrap around to the
   beginning again. i also need an accuracy function to take probabilities
   and one hot encoded label arrays, so i   ll define that, too.
def accuracy(predictions, labels):
    preds_correct_boolean =  np.argmax(predictions, 1) == np.argmax(labels, 1)
    correct_predictions = np.sum(preds_correct_boolean)
    accuracy = 100.0 * correct_predictions / predictions.shape[0]
    return accuracy

num_steps = 10001

with tf.session(graph=graph) as session:
    tf.initialize_all_variables().run()
    for step in range(num_steps):
        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
        minibatch_data = train_dataset[offset:(offset + batch_size), :]
        minibatch_labels = train_labels[offset:(offset + batch_size)]

        feed_dict = {tf_train_dataset : minibatch_data, tf_train_labels : miniba
tch_labels}

        _, l, predictions = session.run(
            [optimizer, loss, train_prediction], feed_dict = feed_dict
            )

        if step % 1000 == 0:
            print 'minibatch loss at step {0}: {1}'.format(step, l)

    print 'test accuracy: {0}%'.format(accuracy(test_prediction.eval(), test_lab
els))

minibatch loss at step 0: 10.4225320816
[...]
minibatch loss at step 10000: 0.103610478342
test accuracy: 96.2666666667%

   the network is excellent. this makes sense, since the data is nearly
   separable, but it   s still cool to see that it works.

   this was pretty simple in tensorflow, because it allows me to build the
   network at a high level of abstraction. i don   t need to know how to
   code an activation function, calculate the loss, or update the
   parameter weights by error id26 to use the model.

   but, if you   re serious about trying to build better neural networks (or
   even just innately curious), understanding the details is crucial.
   there   s no better way to understand how something works than to build
   it from scratch. so here we go.

building a neural network from scratch

   where to begin? well, for a neural network, i need:
    1. a model architecture
    2. arrays of weight vectors
    3. arrays of bias vectors
    4. an activation function
    5. a function to convert the output to a id203 distribution
    6. a id168
    7. a way to use the output of the model to tune the model iteratively
       and improve it (error id26)

model architecture

   because my network structure determines the weights and biases array
   shapes, i   ll start by defining the model   s architecture using this
   example network from [10]wikipedia.

   png

   each black arrow represents one weight vector, and the three layers are
   labeled clearly. we can represent all of the weights at any layer as an
   array of the individual weight vectors. a basic, three-layer network
   has two sets of weights arrays and bias vectors. there   s no bias in the
   image, but the bias vectors don   t change the architecture in any
   meaningful way (they just get added in to change the neuron   s
   threshold). the way to go from the input to the output:
    1. apply the first layer   s weights to the raw data (via matrix
       multiplication). i   ll call this the input layer.
    2. add a bias vector and apply one of many possible activation
       functions to that new matrix, resulting in the hidden layer.
    3. apply the second layer   s weights to the hidden layer matrix and add
       a bias vector. i   ll call this the output layer.

   once we have the output layer, we apply some kind of non-linear
   id172 function in order to let the output values represent a
   probabilitiy distribution. a very common choice is the softmax
   function, because it   s easily differentiable and intuitively fits
   classification tasks.

   with the basic architecture defined, i can start initializing my
   weights and biases.

weights and biases

   a neural network is all about id127, so i need the
   array shapes to line-up correctly for id127. i   ll first
   define variables to represent the number of hidden nodes, the number of
   features, and the number of output labels. the number of hidden nodes
   is a hyperparameter i could tune with cross-validation, but for a
   dataset as simple as mine it won   t matter that much.
hidden_nodes = 5
num_features = data.shape[1]
num_labels = labels.shape[1]

   with these, i can initialize starting weights and biases for the
   network. since the key idea behind a neural network is to iteratively
   improve the weights, i can just start with random weights (there are
   some smart ways to initialize, but random is usually fine). i need to
   matrix multiply the first set of weights with the input data, which has
   shape (number of observations x num_features). so num_features needs to
   be a dimension of the weights array.

   but, since the output of that calculation defines the hidden layer, i
   also need to end up with an array that represents my chosen number of
   hidden_nodes vectors collectively. so i need my layer 1 weights array
   to have a shape of (num_features x hidden_nodes). the layer 1 bias
   array gets added to the input layer, so it needs to have shape (1,
   hidden_nodes) for the math to line up.

   the same logic determines the shapes of the next weights and bias
   arrays, except instead of starting from the raw data and going to the
   hidden layer, we   re going from the hidden layer to the output. we want
   the output to have one neuron for each possible class, so
   layer2_weights_array needs shape (hidden_nodes, num_labels). the layer
   2 bias array gets added to the output layer, so it needs to have shape
   (1, num_labels).
layer1_weights_array = np.random.normal(0, 1, [num_features, hidden_nodes])
layer1_biases_array = np.zeros((1, hidden_nodes))

layer2_weights_array = np.random.normal(0, 1, [hidden_nodes, num_labels])
layer2_biases_array = np.zeros((1, num_labels))

relu activation

   okay, now i need the activation function i   m going to use to get the
   output at the hidden layer. the relu (rectified linear unit) is a
   commonly chosen function because of several mathematical advantages
   (see [11]here for details). it   s a simple function that has output = 0
   for any input < 0 and output = input for any input >= 0. i can code
   that easily.
def relu_activation(data_array):
    return np.maximum(data_array, 0)

output id203 distribution

   when we think about a model choosing between various output classes for
   a single observation, we represent the    confidence    of that choice as a
   id203 distribution over the possible choices. to get that
   distribution, we need a function to transform the output layer of our
   network. among other reasons, we use the softmax function because it   s
   derivative is computationally cheap to calculate.

   the softmax function is:

   where z represents the values from the output layer (in our case) and k
   represents the number of output classes.

   it   s also very easy to implement this.
def softmax(output_array):
    logits_exp = np.exp(output_array)
    return logits_exp / np.sum(logits_exp, axis = 1, keepdims = true)

id168

   since we   re using calculating softmax values, we   ll calculate the cross
   id178 loss for every observation:

   where p(x) is the target label and q(x) is the predicted id203 of
   that label for a given observation.

   i don   t have the knowledge to give a really good explanation of cross
   id178, but it   s essentially a measure of the similarity between two
   distributions. chris olah (google brain) has an awesome explanation of
   various id205 concepts (which includes cross id178)
   [12]here. in the case of a neural network, it   s a measure of how
   similar the predicted probabilities for the correct target class are to
   the actual target values.

   either way, it   s not that hard to calculate the total cross id178 for
   the predicted probabilities. since our target value for every
   observation is one, we can effectively ignore that part of the loss,
   making the loss for every individual observation:

   we   ll use this to calculate the loss for every observation, and then
   take the average to be the overall loss.
def cross_id178_softmax_loss_array(softmax_probs_array, y_onehot):
    indices = np.argmax(y_onehot, axis = 1).astype(int)
    predicted_id203 = softmax_probs_array[np.arange(len(softmax_probs_arra
y)), indices]
    log_preds = np.log(predicted_id203)
    loss = -1.0 * np.sum(log_preds) / len(log_preds)
    return loss

   to avoid overfitting in predictive modeling, we often use l2
   id173. this kind of id173 works by squashing weights
   closer to zero, and it also contributes to the total loss of the model.
   so we need a function to calculate the id173 loss.
   fortunately, l2 id173 loss has a simple formula:

   for every array of weights in the model.

   so i   ll define a second function to calculate the id173 loss.
   i didn   t do this in the tensorflow model, but for a more complex
   problem i really should.
def id173_l2_softmax_loss(reg_lambda, weight1, weight2):
    weight1_loss = 0.5 * reg_lambda * np.sum(weight1 * weight1)
    weight2_loss = 0.5 * reg_lambda * np.sum(weight2 * weight2)
    return weight1_loss + weight2_loss

   at this point, we   ve finished with the feed-forward part of the model.
   we can take raw data, get the predictions from the model, and calculate
   the error (loss). what we haven   t done is design a way to modify the
   weights, based on our predictions, that tries to reduce the loss on our
   next prediction with the updated weights.

   we do this by something called id26. it   s a very simple
   concept that   s surprisingly difficult to wrap your head around
   mathematically at first. i spent a couple of days reading lecture notes
   and processing the math derivations until i was able to confidently
   code it myself on any arbitrary network architecture.

id26 discussion (warning: wall of text)

   everything i   ve done so far has been fairly straightforward. the output
   and error (loss) calculations are key parts of the neural network, but
   they   re both much more straightforward than the backpropogation of the
   error. before starting this post, i knew that backprop is, at a high
   level, just the chain rule. but i didn   t really know the details of how
   it actually works in the neural network context.

   okay, so what is id26? and why do we do it?

   backprop is a method for calculating the gradient of the model   s error
   with respect to every weight in the model. we do it so that we can
   slightly update each weight via id119 in order to reduce the
   model   s error.

   with some effort, it   s possible to show that the gradient of the error
   with respect to any given weight is the activated values array at the
   layer from which the weight originates matrix multiplied by the error
   signal at the layer at which the weight ends. so, the error gradient
   for layer2_weights_array would be the hidden_layer values times the
   error signal of the softmaxed output_layer.

   but how do we backpropagate the error? we   d need the error signals at
   any given layer. fortunately, we can also calculate the error signal at
   any given layer so long as we have the output error. a key derivation
   in the backprop math illustrates that the error signal at the end of a
   given layer is the id127 of the error signal from the
   next layer in the network and the transpose of the weights coming out
   of the current layer. this is then multiplied by the derivative of the
   layer   s activation function. this is why the backprop algorithm is
   often called an application of the chain rule. you can nest these
   calculations to get the error signal at any layer.

   i   m not going to walk through the math here. but i will list a few
   resources that go through the math in detail.

   i know i said this already, but i want to hammer this point home in
   case someone gets stuck trying to understand backprop and gives up. i
   seriously spent hours staring at the math, reading hinton et. al   s
   backprop paper, watching hinton   s coursera lecture on backprop, and
   reading lecture notes and posts about backprop before i actually
   understood it in detail. to get a strong technical and theoretical
   understanding, i   d recommend the following:
    1. listen to geoffrey hinton   s recorded lectures on backprop on
       [13]coursera.
    2. read hinton et. al   s 1985 [14]paper on backprop
    3. go through fei-fei li (stanford) and andrej karpathy   s (openai)
       slides for cs 231 lectures [15]3, [16]4, [17]5, and the
       accompanying lecture note [18]posts on neural networks and
       backprop.
         1. if you   re mathematically oriented, check out peter sadowski   s
            (ph.d student at uc irvine) [19]mathematical notes on
            backprop. i found them extremely helpful.
    4. read chapter [20]2 and [21]3 of michael nielsen   s (y combinator
       research fellow) book on neural networks.

how do i backprop in my network?

   since i   m using a cross id178 id168, the error signal at the
   end of the network is just the predictions array minus the actual
   labels in one hot encoded form. we plug that output error signal into
   the backprop chain rule, and can calculate the error at the hidden
   layer and the gradient on the layer 2 weights. then, with the hidden
   layer   s error signal, we can continue through the network.

   with this general framework, we can update all the weights and biases
   by their respective gradients. i   ve ignored the bias terms in this
   discussion, because for any given bias, the error signal is just the
   combined error signal at the layer to which the bias is applied (see
   the resources i listed above for the math).

   end of wall of text. now i can implement the algorithm on my network.

implementing id26

   first, i   ll calculate the error signal at the output,
   output_error_signal. i   ll use the network output array output_probs
   (the softmaxed output of my network) and the labels array
   labels_onehot. since the network error is the average of every
   individual loss, i need to divide the difference of those two matrices
   by the number of observations to get the true error signal matrix (just
   like i did when calculating the loss with the
   cross_id178_softmax_loss_array function) since the model   s loss is an
   average of all the observation   s losses.
output_error_signal = (output_probs - labels_onehot) / output_probs.shape[0]

   with the output error signal, i can get the error signal on the hidden
   layer hidden_layer in the way i described in bold above. the derivative
   of the relu activation function is 1 when the value is greater than 0,
   and 0 otherwise. due to that derivative structure, i update the
   error_signal_hidden to be 0 when the hidden layer   s value is less than
   0 and don   t need to do anything else.
error_signal_hidden = np.dot(output_error_signal, layer2_weights_array.t)
error_signal_hidden[hidden_layer <= 0] = 0

   i can also calculate the gradient on the layer 2 weights and biases.
gradient_layer2_weights = np.dot(hidden_layer.t, output_error_signal)
gradient_layer2_bias = np.sum(output_error_signal, axis = 0, keepdims = true)

   at this point, i   ve backpropagated the error signal to the second
   layer. time to do the same thing for the first layer, but using the
   error signal for the hidden layer i just calculated and the input to
   the first layer (the raw data, data).
gradient_layer1_weights = np.dot(data.t, error_signal_hidden)
gradient_layer1_bias = np.sum(error_signal_hidden, axis = 0, keepdims = true)

   now i just add each weight   s id173 contribution to the
   respective gradients, and i   m ready to update the weights and biases.
   since i want to move in the negative gradient direction, i subtract the
   gradient (times the learning rate) from the weights and biases.
reg_lambda = .01
learning_rate = .01

gradient_layer2_weights += reg_lambda * layer2_weights_array
gradient_layer1_weights += reg_lambda * layer1_weights_array

layer1_weights_array -= learning_rate * gradient_layer1_weights
layer1_biases_array -= learning_rate * gradient_layer1_bias
layer2_weights_array -= learning_rate * gradient_layer2_weights
layer2_biases_array -= learning_rate * gradient_layer2_bias

   that   s it. finally, i   m ready to train the neural network i made from
   scratch.

full network from scratch

   so how do i train it? essentially, i   m just going to loop over the
   dataset enough times for the id119 algorithm to reach the
   minimum of the id168 (the optimal parameters). i don   t know ex
   ante how many iterations it will take, but for this toy dataset it
   won   t take that many.

   the proceeding code is just the combination of everything i   ve written
   above, wrapped in a loop.
training_data = train_dataset
training_labels = train_labels

hidden_nodes = 5
num_labels = training_labels.shape[1]
num_features = training_data.shape[1]
learning_rate = .01
reg_lambda = .01

# weights and bias arrays, just like in tensorflow
layer1_weights_array = np.random.normal(0, 1, [num_features, hidden_nodes])
layer2_weights_array = np.random.normal(0, 1, [hidden_nodes, num_labels])

layer1_biases_array = np.zeros((1, hidden_nodes))
layer2_biases_array = np.zeros((1, num_labels))


for step in xrange(5001):

    input_layer = np.dot(training_data, layer1_weights_array)
    hidden_layer = relu_activation(input_layer + layer1_biases_array)
    output_layer = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_ar
ray
    output_probs = softmax(output_layer)

    loss = cross_id178_softmax_loss_array(output_probs, training_labels)
    loss += id173_l2_softmax_loss(reg_lambda, layer1_weights_array, lay
er2_weights_array)

    output_error_signal = (output_probs - training_labels) / output_probs.shape[
0]

    error_signal_hidden = np.dot(output_error_signal, layer2_weights_array.t)
    error_signal_hidden[hidden_layer <= 0] = 0

    gradient_layer2_weights = np.dot(hidden_layer.t, output_error_signal)
    gradient_layer2_bias = np.sum(output_error_signal, axis = 0, keepdims = true
)

    gradient_layer1_weights = np.dot(training_data.t, error_signal_hidden)
    gradient_layer1_bias = np.sum(error_signal_hidden, axis = 0, keepdims = true
)

    gradient_layer2_weights += reg_lambda * layer2_weights_array
    gradient_layer1_weights += reg_lambda * layer1_weights_array

    layer1_weights_array -= learning_rate * gradient_layer1_weights
    layer1_biases_array -= learning_rate * gradient_layer1_bias
    layer2_weights_array -= learning_rate * gradient_layer2_weights
    layer2_biases_array -= learning_rate * gradient_layer2_bias

    if step % 500 == 0:
            print 'loss at step {0}: {1}'.format(step, loss)

loss at step 0: 2.97031571897
[...]
loss at step 5000: 0.227432713332

predicting with the learned model

   to predict with the model on the test set, all i need to do is excute
   the forward pass with the learned weights and biases. then i can
   compute the accuracy by using the function i designed for the
   probabilities array and one hot labels array.
input_layer = np.dot(test_dataset, layer1_weights_array)
hidden_layer = relu_activation(input_layer + layer1_biases_array)
scores = np.dot(hidden_layer, layer2_weights_array) + layer2_biases_array
probs = softmax(scores)
print 'test accuracy: {0}%'.format(accuracy(probs, test_labels))

test accuracy: 96.4%

   nearly identical to the results from tensorflow! even though my model
   was slower, it still works.

   let   s plot the correct predictions in blue and the incorrect ones in
   red.
labels_flat = np.argmax(test_labels, axis = 1)
predictions = np.argmax(probs, axis = 1)
plt.figure(figsize = (12, 8))
plt.scatter(test_dataset[:, 0], test_dataset[:, 1], c = predictions == labels_fl
at - 1, alpha = .8, s = 50)

   png

   as expected, the model makes a few mistakes near the cluster
   boundaries, but does very well otherwise.

conclusion

   making this post was an unbelievable learning experience for me. i went
   from having a general understanding of neural networks to being able to
   write one entirely from scratch.

   am i going to consistently build standard networks from scratch?
   absolutely not. but, without the foundation i developed from preparing
   for this blog post, i would have a much more difficult time trying to
   understand deeper and more complex networks.

   convolutional neural networks are one of those deeper and more complex
   networks. in two blog posts, i   ve now gone through image convolution
   and basic neural networks with a hidden layer. in the next post, i   ll
   finally go through building a convolutional network.

   tags: [22]machine learning, [23]neural networks

   updated: october 17, 2016

share on

   [24]twitter [25]facebook [26]google+ [27]linkedin

   [28]previous [29]next

leave a comment

you may also enjoy

[30]evaluating models with small data

   8 minute read

   or, why point estimates only get you so far.

[31]deriving the sigmoid derivative for neural networks

   3 minute read

   sigmoid, derivatives, mathematics

[32]finding the most one-dimensional popular artist

   5 minute read

   music analysis, web apis, math

[33]the right way to oversample in predictive modeling

   6 minute read

   model evaluation, oversampling, predictive modeling

     * follow:
     * [34]github
     * [35]feed

      2019 nick becker. powered by [36]jekyll & [37]minimal mistakes.

   all analyses and conclusions presented on this website reflect my views
   and do not indicate concurrence by the board of governors or the
   federal reserve system.

   please enable javascript to view the [38]comments powered by disqus.

references

   1. https://beckernick.github.io/feed.xml
   2. https://beckernick.github.io/
   3. https://beckernick.github.io/about/
   4. https://beckernick.github.io/datascience/
   5. https://beckernick.github.io/visuals/
   6. https://www.linkedin.com/in/nicholasebecker
   7. https://github.com/beckernick
   8. https://en.wikipedia.org/wiki/convolutional_neural_network
   9. https://beckernick.github.io/convolutions/
  10. https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/colored_neural_network.svg/2000px-colored_neural_network.svg.png
  11. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  12. http://colah.github.io/posts/2015-09-visual-information/
  13. https://www.coursera.org/learn/neural-networks/home/week/3
  14. https://d18ky98rnyall9.cloudfront.net/_f769dac2dfe1f9171ebc85265ac03093_learning-representations-by-back-propagating-errors.pdf?expires=1476748800&signature=wa8idn8euxviwvvhkc0ka6fli4shbepvwoea4mlhgjn~4ivyck0rlqao-e42aehmavt~zntumny20fsi2lm~winudaamnph-ryakcxbcwobtndczcaijn1pvkgnrslfuwmbpw~c-s2r-v~jnihjfvppjapu9vtndau636e3seqo_&key-pair-id=apkajltne6qmuy6hbc5a
  15. http://vision.stanford.edu/teaching/cs231n/slides/lecture3.pdf
  16. http://vision.stanford.edu/teaching/cs231n/slides/lecture4.pdf
  17. http://vision.stanford.edu/teaching/cs231n/slides/lecture5.pdf
  18. http://cs231n.github.io/neural-networks-case-study/
  19. https://www.ics.uci.edu/~pjsadows/notes.pdf
  20. http://neuralnetworksanddeeplearning.com/chap2.html
  21. http://neuralnetworksanddeeplearning.com/chap3.html
  22. https://beckernick.github.io/tags/#machine-learning
  23. https://beckernick.github.io/tags/#neural-networks
  24. https://twitter.com/intent/tweet?text=https://beckernick.github.io/neural-network-scratch/
  25. https://www.facebook.com/sharer/sharer.php?u=https://beckernick.github.io/neural-network-scratch/
  26. https://plus.google.com/share?url=https://beckernick.github.io/neural-network-scratch/
  27. https://www.linkedin.com/sharearticle?mini=true&url=https://beckernick.github.io/neural-network-scratch/
  28. https://beckernick.github.io/dynamic-programming-knapsack/
  29. https://beckernick.github.io/mapreduce-python-hive/
  30. https://beckernick.github.io/evaluating_models_with_small_data/
  31. https://beckernick.github.io/sigmoid-derivative-neural-network/
  32. https://beckernick.github.io/one-dimensional-music-ranking/
  33. https://beckernick.github.io/oversampling-modeling/
  34. http://github.com/beckernick
  35. https://beckernick.github.io/feed.xml
  36. http://jekyllrb.com/
  37. https://mademistakes.com/work/minimal-mistakes-jekyll-theme/
  38. http://disqus.com/?ref_noscript
