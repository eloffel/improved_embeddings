    #[1]nick becker feed

   (button)
     * [2]nick becker
     * [3]about
     * [4]data science
     * [5]visuals

   id28 from scratch in python photo credit: ginny lehman

   nick becker

nick becker

   rapids team at nvidia
   (button) follow
     * new york, ny
     * [6]linkedin
     * [7]github

id28 from scratch in python

   5 minute read

   in this post, i   m going to implement standard id28 from
   scratch. id28 is a generalized linear model that we can
   use to model or predict categorical outcome variables. for example, we
   might use id28 to predict whether someone will be denied
   or approved for a loan, but probably not to predict the value of
   someone   s house.

   so, how does it work? in id28, we   re essentially trying
   to find the weights that maximize the likelihood of producing our given
   data and use them to categorize the response variable. maximum
   likelihood estimation is a well covered topic in statistics courses (my
   intro to statistics professor has a straightforward, high-level
   description [8]here), and it is extremely useful.

   since the likelihood maximization in id28 doesn   t have a
   closed form solution, i   ll solve the optimization problem with gradient
   ascent. gradient ascent is the same as id119, except i   m
   maximizing instead of minimizing a function. before i do any of that,
   though, i need some data.

generating data

   like i did in my post on [9]building neural networks from scratch, i   m
   going to use simulated data. i can easily simulate separable data by
   sampling from a multivariate normal distribution.
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

np.random.seed(12)
num_observations = 5000

x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations
)
x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations
)

simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)
simulated_labels = np.hstack((np.zeros(num_observations),
                              np.ones(num_observations)))

   let   s see how it looks.
plt.figure(figsize=(12,8))
plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_featur
es[:, 1],
            c = simulated_labels, alpha = .4)

   png

picking a link function

   generalized linear models usually tranform a linear model of the
   predictors by using a [10]link function. in id28, the
   link function is the [11]sigmoid. we can implement this really easily.
def sigmoid(scores):
    return 1 / (1 + np.exp(-scores))

maximizing the likelihood

   to maximize the likelihood, i need equations for the likelihood and the
   gradient of the likelihood. fortunately, the likelihood (for binary
   classification) can be reduced to a fairly intuitive form by switching
   to the log-likelihood. we   re able to do this without affecting the
   weights parameter estimation because log transformations are
   [12]monotonic.

   for anyone interested in the derivations of the functions i   m using,
   check out section 4.4.1 of hastie, tibsharani, and friedman   s
   [13]elements of statistical learning. for those less comfortable
   reading math, carlos guestrin (univesity of washington) has a fantastic
   walkthrough of one possible formulation of the likelihood and gradient
   in a series of short lectures on [14]coursera.

calculating the log-likelihood

   the log-likelihood can be viewed as a sum over all the training data.
   mathematically,

   where is the target class (0 or 1), is an individual data point, and is
   the weights vector.

   i can easily turn that into a function and take advantage of matrix
   algebra.
def log_likelihood(features, target, weights):
    scores = np.dot(features, weights)
    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )
    return ll

calculating the gradient

   now i need an equation for the gradient of the log-likelihood. by
   taking the derivative of the equation above and reformulating in matrix
   form, the gradient becomes:

   like the other equation, this is really easy to implement. it   s so
   simple i don   t even need to wrap it into a function.

building the id28 function

   finally, i   m ready to build the model function. i   ll add in the option
   to calculate the model with an intercept, since it   s a good option to
   have.
def logistic_regression(features, target, num_steps, learning_rate, add_intercep
t = false):
    if add_intercept:
        intercept = np.ones((features.shape[0], 1))
        features = np.hstack((intercept, features))

    weights = np.zeros(features.shape[1])

    for step in xrange(num_steps):
        scores = np.dot(features, weights)
        predictions = sigmoid(scores)

        # update weights with gradient
        output_error_signal = target - predictions
        gradient = np.dot(features.t, output_error_signal)
        weights += learning_rate * gradient

        # print log-likelihood every so often
        if step % 10000 == 0:
            print log_likelihood(features, target, weights)

    return weights

   time to run the model.
weights = logistic_regression(simulated_separableish_features, simulated_labels,
                     num_steps = 300000, learning_rate = 5e-5, add_intercept=tru
e)

-4346.26477915
[...]
-140.725421362
-140.725421357
-140.725421355

comparing with sk-learn   s logisticregression

   how do i know if my algorithm spit out the right weights? well, on the
   one hand, the math looks right     so i should be confident it   s correct.
   on the other hand, it would be nice to have a ground truth.

   fortunately, i can compare my function   s weights to the weights from
   sk-learn   s id28 function, which is known to be a correct
   implementation. they should be the same if i did everything correctly.
   since sk-learn   s logisticregression automatically does l2
   id173 (which i didn   t do), i set c=1e15 to essentially turn
   off id173.
from sklearn.linear_model import logisticregression

clf = logisticregression(fit_intercept=true, c = 1e15)
clf.fit(simulated_separableish_features, simulated_labels)

print clf.intercept_, clf.coef_
print weights

[-13.99400797] [[-5.02712572  8.23286799]]
[-14.09225541  -5.05899648   8.28955762]

   as expected, my weights nearly perfectly match the sk-learn
   logisticregression weights. if i trained the algorithm longer and with
   a small enough learning rate, they would eventually match exactly. why?
   because gradient ascent on a concave function will always reach the
   global optimum, given enough time and sufficiently small learning rate.

what   s the accuracy?

   to get the accuracy, i just need to use the final weights to get the
   logits for the dataset (final_scores). then i can use sigmoid to get
   the final predictions and round them to the nearest integer (0 or 1) to
   get the predicted class.
data_with_intercept = np.hstack((np.ones((simulated_separableish_features.shape[
0], 1)),
                                 simulated_separableish_features))
final_scores = np.dot(data_with_intercept, weights)
preds = np.round(sigmoid(final_scores))

print 'accuracy from scratch: {0}'.format((preds == simulated_labels).sum().asty
pe(float) / len(preds))
print 'accuracy from sk-learn: {0}'.format(clf.score(simulated_separableish_feat
ures, simulated_labels))

accuracy from scratch: 0.9948
accuracy from sk-learn: 0.9948

   nearly perfect (which makes sense given the data). we should only have
   made mistakes right in the middle between the clusters. let   s make sure
   that   s what happened. in the following plot, blue points are correct
   predictions and red points are incorrect predictions.
plt.figure(figsize = (12, 8))
plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_featur
es[:, 1],
            c = preds == simulated_labels - 1, alpha = .8, s = 50)

   png

conclusion

   in this post, i built a id28 function from scratch and
   compared it with sk-learn   s id28 function. while both
   functions give essentially the same result, my own function is
   significantly slower because sklearn uses a highly optimized solver.
   while i   d probably never use my own algorithm in production, building
   algorithms from scratch makes it easier to think about how you could
   design extensions to fit more complex problems or problems in new
   domains.
     __________________________________________________________________

   for those interested, the jupyter notebook with all the code can be
   found in the [15]github repository for this post.

   tags: [16]machine learning

   updated: november 05, 2016

share on

   [17]twitter [18]facebook [19]google+ [20]linkedin

   [21]previous [22]next

leave a comment

you may also enjoy

[23]evaluating models with small data

   8 minute read

   or, why point estimates only get you so far.

[24]deriving the sigmoid derivative for neural networks

   3 minute read

   sigmoid, derivatives, mathematics

[25]finding the most one-dimensional popular artist

   5 minute read

   music analysis, web apis, math

[26]the right way to oversample in predictive modeling

   6 minute read

   model evaluation, oversampling, predictive modeling

     * follow:
     * [27]github
     * [28]feed

      2019 nick becker. powered by [29]jekyll & [30]minimal mistakes.

   all analyses and conclusions presented on this website reflect my views
   and do not indicate concurrence by the board of governors or the
   federal reserve system.

   please enable javascript to view the [31]comments powered by disqus.

references

   1. https://beckernick.github.io/feed.xml
   2. https://beckernick.github.io/
   3. https://beckernick.github.io/about/
   4. https://beckernick.github.io/datascience/
   5. https://beckernick.github.io/visuals/
   6. https://www.linkedin.com/in/nicholasebecker
   7. https://github.com/beckernick
   8. http://www2.stat.duke.edu/~banks/111-lectures.dir/lect10.pdf
   9. https://beckernick.github.io/neural-network-scratch/
  10. https://en.wikipedia.org/wiki/generalized_linear_model#link_function
  11. https://en.wikipedia.org/wiki/sigmoid_function
  12. https://en.wikipedia.org/wiki/monotonic_function
  13. http://statweb.stanford.edu/~tibs/elemstatlearn/
  14. https://www.coursera.org/learn/ml-classification/lecture/1zetc/very-optional-expressing-the-log-likelihood
  15. https://github.com/beckernick/logistic_regression_from_scratch
  16. https://beckernick.github.io/tags/#machine-learning
  17. https://twitter.com/intent/tweet?text=https://beckernick.github.io/logistic-regression-from-scratch/
  18. https://www.facebook.com/sharer/sharer.php?u=https://beckernick.github.io/logistic-regression-from-scratch/
  19. https://plus.google.com/share?url=https://beckernick.github.io/logistic-regression-from-scratch/
  20. https://www.linkedin.com/sharearticle?mini=true&url=https://beckernick.github.io/logistic-regression-from-scratch/
  21. https://beckernick.github.io/mapreduce-python-hive/
  22. https://beckernick.github.io/matrix-factorization-recommender/
  23. https://beckernick.github.io/evaluating_models_with_small_data/
  24. https://beckernick.github.io/sigmoid-derivative-neural-network/
  25. https://beckernick.github.io/one-dimensional-music-ranking/
  26. https://beckernick.github.io/oversampling-modeling/
  27. http://github.com/beckernick
  28. https://beckernick.github.io/feed.xml
  29. http://jekyllrb.com/
  30. https://mademistakes.com/work/minimal-mistakes-jekyll-theme/
  31. http://disqus.com/?ref_noscript
