   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    mcmc: the gibbs sampler comments feed [4]mcmc:
   multivariate distributions, block-wise, & component-wise updates
   [5]mcmc: hamiltonian monte carlo (a.k.a. hybrid monte carlo)
   [6]alternate [7]alternate [8]the clever machine [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    mcmc: multivariate distributions, block-wise, &
   component-wise updates
   [21]mcmc: hamiltonian monte carlo (a.k.a. hybrid monte carlo)    

mcmc: the gibbs sampler

   [22]nov 5

   posted by [23]dustinstansbury

   in the previous [24]post, we compared using block-wise and
   component-wise implementations of the metropolis-hastings algorithm for
   sampling from a multivariate id203 distribution p(\bold x) .
   component-wise updates for mcmc algorithms are generally more efficient
   for multivariate problems than blockwise updates in that we are more
   likely to accept a proposed sample by drawing each component/dimension
   independently of the others. however, samples may still be rejected,
   leading to excess computation that is never used. the gibbs sampler,
   another popular mcmc sampling technique, provides a means of avoiding
   such wasted computation. like the component-wise implementation of the
   metropolis-hastings algorithm, the gibbs sampler also uses
   component-wise updates. however, unlike in the metropolis-hastings
   algorithm, all proposed samples are accepted, so there is no wasted
   computation.

   the gibbs sampler is applicable for certain classes of problems, based
   on two main criterion. given a target distribution p(\bold x) , where
   \bold x = (x_1, x_2, \dots, x_d , ),  the first criterion is 1) that it
   is necessary that we have an analytic (mathematical) expression for the
   conditional distribution of each variable in the joint distribution
   given all other variables in the joint. formally, if the target
   distribution p(\bold x) is d -dimensional, we must have d individual
   expressions for

   p(x_i|x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_d)

   = p(x_i | x_j), j\neq i .

   each of these expressions defines the id203 of the i -th
   dimension given that we have values for all other ( j \neq i )
   dimensions. having the conditional distribution for each variable means
   that we don   t need a proposal distribution or an accept/reject
   criterion, like in the metropolis-hastings algorithm. therefore, we can
   simply sample from each conditional while keeping all other variables
   held fixed. this leads to the second criterion 2) that we must be able
   to sample from each conditional distribution. this caveat is obvious if
   we want an implementable algorithm.

   the gibbs sampler works in much the same way as the component-wise
   metropolis-hastings algorithms except that instead drawing from a
   proposal distribution for each dimension, then accepting or rejecting
   the proposed sample, we simply draw a value for that dimension
   according to the variable   s corresponding conditional distribution. we
   also accept all values that are drawn. similar to the component-wise
   metropolis-hastings algorithm, we step through each variable
   sequentially, sampling it while keeping all other variables fixed. the
   id150 procedure is outlined below
    1. set t = 0
    2. generate an initial state \bold x^{(0)} \sim \pi^{(0)}
    3. repeat until t = m

   set t = t+1

   for each dimension i = 1..d

   draw x_i from p(x_i|x_1,x_2,\dots,x_{i-1},x_{i+1},\dots,x_d)

   to get a better understanding of the gibbs sampler at work, let   s
   implement the gibbs sampler to solve the same multivariate sampling
   problem addressed in the previous post.

example: sampling from a bivariate a normal distribution

   this example parallels the examples in the previous post where we
   sampled from a 2-d normal distribution using block-wise and
   component-wise metropolis-hastings algorithms. here, we show how to
   implement a gibbs sampler to draw samples from the same target
   distribution. as a reminder, the target distribution p(\bold x) is a
   normal form with following parameterization:

   p(\bold x) = \mathcal n (\bold{\mu}, \bold \sigma)

   with mean

   \mu = [\mu_1,\mu_2]= [0, 0]

   and covariance

   \bold \sigma = \begin{bmatrix} 1 & \rho_{12} \\ \rho_{21} &
   1\end{bmatrix} = \begin{bmatrix} 1 & 0.8 \\ 0.8 & 1\end{bmatrix}

   in order to sample from this distribution using a gibbs sampler, we
   need to have in hand the conditional distributions for
   variables/dimensions x_1 and x_2 :

   p(x_1 | x_2^{(t-1)}) (i.e. the conditional for the first dimension, x_1
   )

   and

   p(x_2 | x_1^{(t)}) (the conditional for the second dimension, x_2 )

   where x_2^{(t-1)} is the previous state of the second dimension, and
   x_1^{(t)} is the state of the first dimension after drawing from p(x_1
   | x_2^{(t-1)}) . the reason for the discrepancy between updating x_1
   and x_2 using states (t-1) and (t) , can be is seen in step 3 of the
   algorithm outlined in the previous section. at iteration t we first
   sample a new state for variable x_1 conditioned on the most recent
   state of variable x_2 , which is from iteration (t-1) . we then sample
   a new state for the variable x_2 conditioned on the most recent state
   of variable x_1 , which is now from the current iteration, t .

   after some math (which which i will skip for some brevity, but see the
   [25]following for some details), we find that the two conditional
   distributions for the target normal distribution are:

   p(x_1 | x_2^{(t-1)}) = \mathcal n(\mu_1 + \rho_{21}(x_2^{(t-1)} -
   \mu_2),\sqrt{1-\rho_{21}^2})

   and

   p(x_2 | x_1^{(t)})=\mathcal n(\mu_2 +
   \rho_{12}(x_1^{(t)}-\mu_1),\sqrt{1-\rho_{12}^2}) ,

   which are both univariate normal distributions, each with a mean that
   is dependent on the value of the most recent state of the conditioning
   variable, and a variance that is dependent on the target covariances
   between the two variables.

   using the above expressions for the conditional probabilities of
   variables x_1 and x_2 , we implement the gibbs sampler using matlab
   below. the output of the sampler is shown here:

   gibbs sampler markov chain and samples for bivariate normal target
   distribution

   inspecting the figure above, note how at each iteration the markov
   chain for the gibbs sampler first takes a step only along the x_1
   direction, then only along the x_2 direction.  this shows how the gibbs
   sampler sequentially samples the value of each variable separately, in
   a component-wise fashion.
% example: gibbs sampler for bivariate normal
rand('seed' ,12345);
nsamples = 5000;

mu = [0 0]; % target mean
rho(1) = 0.8; % rho_21
rho(2) = 0.8; % rho_12

% initialize the gibbs sampler
propsigma = 1; % proposal variance
minn = [-3 -3];
maxx = [3 3];

% initialize samples
x = zeros(nsamples,2);
x(1,1) = unifrnd(minn(1), maxx(1));
x(1,2) = unifrnd(minn(2), maxx(2));

dims = 1:2; % index into each dimension

% run gibbs sampler
t = 1;
while t < nsamples
    t = t + 1;
    t = [t-1,t];
    for id = 1:2 % loop over dimensions
        % update samples
        nix = dims~=id; % *not* the current dimension
        % conditional mean
        mucond = mu(id) + rho(id)*(x(t(id),nix)-mu(nix));
        % conditional variance
        varcond = sqrt(1-rho(id)^2);
        % draw from conditional
        x(t,id) = normrnd(mucond,varcond);
    end
end

% display sampling dynamics
figure;
h1 = scatter(x(:,1),x(:,2),'r.');

% conditional steps/samples
hold on;
for t = 1:50
    plot([x(t,1),x(t+1,1)],[x(t,2),x(t,2)],'k-');
    plot([x(t+1,1),x(t+1,1)],[x(t,2),x(t+1,2)],'k-');
    h2 = plot(x(t+1,1),x(t+1,2),'ko');
end

h3 = scatter(x(1,1),x(1,2),'go','linewidth',3);
legend([h1,h2,h3],{'samples','1st 50 samples','x(t=0)'},'location','northwest')
hold off;
xlabel('x_1');
ylabel('x_2');
axis square

wrapping up

   the gibbs sampler is a popular mcmc method for sampling from complex,
   multivariate id203 distributions. however, the gibbs sampler
   cannot be used for general sampling problems. for many target
   distributions, it may difficult or impossible to obtain a closed-form
   expression for all the needed conditional distributions. in other
   scenarios, analytic expressions may exist for all conditionals but it
   may be difficult to sample from any or all of the conditional
   distributions (in these scenarios it is common to use univariate
   sampling methods such as rejection sampling and (surprise!)
   metropolis-type mcmc techniques to approximate samples from each
   conditional). gibbs samplers are very popular for bayesian methods
   where models are often devised in such a way that conditional
   expressions for all model variables are easily obtained and take
   well-known forms that can be sampled from efficiently.

   id150, like many mcmc techniques suffer from what is often
   called    slow mixing.    slow mixing occurs when the underlying markov
   chain takes a long time to sufficiently explore the values of \bold x
   in order to give a good characterization of p(\bold x) . slow mixing is
   due to a number of factors including the    random walk    nature of the
   markov chain, as well as the tendency of the markov chain to get
      stuck,    only sampling a single region of \bold x having
   high-id203 under p(\bold x) . such behaviors are bad for sampling
   distributions with multiple modes or heavy tails. more advanced
   techniques, such as hybrid monte carlo have been developed to
   incorporate additional dynamics that increase the efficiency of the
   markov chain path. we will discuss hybrid monte carlo in a future post.
   advertisements

share this:

     * [26]twitter
     * [27]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [28]view all posts by dustinstansbury   

   posted on november 5, 2012, in [29]algorithms, [30]density estimation,
   [31]sampling, [32]sampling methods, [33]statistics and tagged
   [34]conditional distribution, [35]gibbs sampler, [36]mcmc,
   [37]multivariate normal. bookmark the [38]permalink. [39]4 comments.
   [40]    mcmc: multivariate distributions, block-wise, &
   component-wise updates
   [41]mcmc: hamiltonian monte carlo (a.k.a. hybrid monte carlo)    
     * [42]leave a comment
     * [43]trackbacks 2
     * [44]comments 2

    1. [45]markovmagic | [46]august 28, 2013 at 6:21 pm
       reblogged this on [47]machine learning magic and commented:
       this man is a genius.
       [48]reply
    2. [49]chjko0206 | [50]february 9, 2015 at 12:11 am
       reblogged this on [51]michaelhjc.
       [52]reply

    1. pingback: [53]a gentle introduction to id115
       (mcmc)    the clever machine
    2. pingback: [54]mcmc: the gibbs sampler, simple example w/ matlab
       code | victor fang's computing space

leave a reply [55]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [56]googleplus-sign-in

     *
     *

   [57]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [58]log out /
   [59]change )
   google photo

   you are commenting using your google account. ( [60]log out /
   [61]change )
   twitter picture

   you are commenting using your twitter account. ( [62]log out /
   [63]change )
   facebook photo

   you are commenting using your facebook account. ( [64]log out /
   [65]change )
   [66]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [67]algorithms [68]classification [69]id174
       [70]density estimation [71]derivations [72]id171
       [73]fmri [74]id119 [75]latex [76]machine learning
       [77]matlab [78]maximum likelihood [79]mcmc [80]neural networks
       [81]neuroscience [82]optimization [83]proofs [84]regression
       [85]sampling [86]sampling methods [87]simulations [88]statistics
       [89]theory [90]tips & tricks [91]uncategorized
     * recent posts
          + [92]derivation: maximum likelihood for id82s
          + [93]a gentle introduction to id158s
          + [94]derivation: derivatives for common neural network
            id180
          + [95]derivation: error id26 & id119 for
            neural networks
          + [96]model selection: underfitting, overfitting, and the
            id160
          + [97]supplemental proof 1
          + [98]the statistical whitening transform
          + [99]covariance matrices and data distributions
          + [100]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [101]derivation: the covariance matrix of an ols estimator
            (and applications to gls)
     * archives
          + [102]september 2014
          + [103]april 2013
          + [104]march 2013
          + [105]january 2013
          + [106]december 2012
          + [107]november 2012
          + [108]october 2012
          + [109]september 2012
          + [110]march 2012
          + [111]february 2012
          + [112]january 2012
     * meta
          + [113]register
          + [114]log in
          + [115]entries rss
          + [116]comments rss
          + [117]wordpress.com
       advertisements

   [118]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [119]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [120]cookie policy

   iframe: [121]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/feed/
   4. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
   5. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#access
  11. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#main
  12. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#sidebar
  13. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#sidebar2
  14. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
  21. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  22. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
  25. http://fourier.eng.hmc.edu/e161/lectures/gaussianprocess/node7.html
  26. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/?share=twitter
  27. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/?share=facebook
  28. https://theclevermachine.wordpress.com/author/dustinstansbury/
  29. https://theclevermachine.wordpress.com/category/algorithms/
  30. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  31. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  32. https://theclevermachine.wordpress.com/category/sampling-methods/
  33. https://theclevermachine.wordpress.com/category/statistics/
  34. https://theclevermachine.wordpress.com/tag/conditional-distribution/
  35. https://theclevermachine.wordpress.com/tag/gibbs-sampler/
  36. https://theclevermachine.wordpress.com/tag/mcmc/
  37. https://theclevermachine.wordpress.com/tag/multivariate-normal/
  38. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  39. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comments
  40. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
  41. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  42. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#respond
  43. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#trackbacks
  44. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comments
  45. http://mlmagic.wordpress.com/
  46. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comment-95
  47. http://mlmagic.wordpress.com/2013/08/29/mcmc-the-gibbs-sampler/
  48. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/?replytocom=95#respond
  49. http://michaelhjc.wordpress.com/
  50. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comment-287
  51. https://michaelhjc.wordpress.com/2015/02/09/mcmc-the-gibbs-sampler/
  52. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/?replytocom=287#respond
  53. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
  54. http://victorfang.wordpress.com/2014/04/29/mcmc-the-gibbs-sampler-simple-example-w-matlab-code/
  55. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#respond
  56. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  57. https://gravatar.com/site/signup/
  58. javascript:highlandercomments.doexternallogout( 'wordpress' );
  59. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  60. javascript:highlandercomments.doexternallogout( 'googleplus' );
  61. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  62. javascript:highlandercomments.doexternallogout( 'twitter' );
  63. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  64. javascript:highlandercomments.doexternallogout( 'facebook' );
  65. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  66. javascript:highlandercomments.cancelexternalwindow();
  67. https://theclevermachine.wordpress.com/category/algorithms/
  68. https://theclevermachine.wordpress.com/category/algorithms/classification/
  69. https://theclevermachine.wordpress.com/category/data-preprocessing/
  70. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  71. https://theclevermachine.wordpress.com/category/derivations/
  72. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
  73. https://theclevermachine.wordpress.com/category/fmri/
  74. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  75. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
  76. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  77. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
  78. https://theclevermachine.wordpress.com/category/maximum-likelihood/
  79. https://theclevermachine.wordpress.com/category/mcmc/
  80. https://theclevermachine.wordpress.com/category/neural-networks/
  81. https://theclevermachine.wordpress.com/category/neuroscience/
  82. https://theclevermachine.wordpress.com/category/optimization/
  83. https://theclevermachine.wordpress.com/category/proofs/
  84. https://theclevermachine.wordpress.com/category/algorithms/regression/
  85. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  86. https://theclevermachine.wordpress.com/category/sampling-methods/
  87. https://theclevermachine.wordpress.com/category/simulations/
  88. https://theclevermachine.wordpress.com/category/statistics/
  89. https://theclevermachine.wordpress.com/category/theory/
  90. https://theclevermachine.wordpress.com/category/tips-tricks/
  91. https://theclevermachine.wordpress.com/category/uncategorized/
  92. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
  93. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  94. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  95. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  96. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
  97. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
  98. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
  99. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
 100. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
 101. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
 102. https://theclevermachine.wordpress.com/2014/09/
 103. https://theclevermachine.wordpress.com/2013/04/
 104. https://theclevermachine.wordpress.com/2013/03/
 105. https://theclevermachine.wordpress.com/2013/01/
 106. https://theclevermachine.wordpress.com/2012/12/
 107. https://theclevermachine.wordpress.com/2012/11/
 108. https://theclevermachine.wordpress.com/2012/10/
 109. https://theclevermachine.wordpress.com/2012/09/
 110. https://theclevermachine.wordpress.com/2012/03/
 111. https://theclevermachine.wordpress.com/2012/02/
 112. https://theclevermachine.wordpress.com/2012/01/
 113. https://wordpress.com/start?ref=wplogin
 114. https://theclevermachine.wordpress.com/wp-login.php
 115. https://theclevermachine.wordpress.com/feed/
 116. https://theclevermachine.wordpress.com/comments/feed/
 117. https://wordpress.com/
 118. https://wordpress.com/?ref=footer_website
 119. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
 120. https://automattic.com/cookies
 121. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 123. https://theclevermachine.files.wordpress.com/2012/11/gibbssampler-2dnormal1.png
 124. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comment-form-guest
 125. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comment-form-load-service:wordpress.com
 126. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comment-form-load-service:twitter
 127. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/#comment-form-load-service:facebook
