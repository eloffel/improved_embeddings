   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    mcmc: the metropolis sampler comments feed [4]a
   brief introduction to markov chains [5]mcmc: the metropolis-hastings
   sampler [6]alternate [7]alternate [8]the clever machine
   [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    a brief introduction to markov chains
   [21]mcmc: the metropolis-hastings sampler    

mcmc: the metropolis sampler

   [22]oct 5

   posted by [23]dustinstansbury

   as discussed in an earlier [24]post, we can use a markov chain to
   sample from some target id203 distribution p(x) from which
   drawing samples directly is difficult. to do so, it is necessary to
   design a transition operator for the markov chain which makes the
   chain   s stationary distribution match the target distribution. the
   metropolis sampling algorithm  (and the more general
   metropolis-hastings sampling algorithm) uses simple heuristics to
   implement such a transition operator.

metropolis sampling

   starting from some random initial state x^{(0)} \sim \pi^{(0)} , the
   algorithm first draws a possible sample x^* from a  proposal
   distribution q(x | x^{(t-1)}) .  much like a conventional transition
   operator for a markov chain, the proposal distribution depends only on
   the previous state in the chain. however, the transition operator for
   the metropolis algorithm has an additional step that assesses whether
   or not the target distribution has a sufficiently large density near
   the proposed state to warrant accepting the proposed state as a sample
   and setting it to the next state in the chain. if the density of p(x)
   is low near the proposed state, then it is likely (but not guaranteed)
   that it will be rejected. the criterion for accepting or rejecting a
   proposed state are defined by the following heuristics:
    1. if p(x^*) \geq p(x^{(t-1)}) ,  the proposed state is kept x^* as a
       sample and is set as the next state in the chain (i.e. move the
       chain   s state to a location  where p(x) has equal or greater
       density).
    2. if p(x^*) < p(x^{(t-1)})    indicating that p(x) has low density near
       x^*    then the proposed state may still be accepted, but only
       randomly, and with a id203 \frac{p(x^*)}{p(x^{(t-1)})}

   these heuristics can be instantiated by calculating the acceptance
   id203 for the proposed state.

   \alpha = \min \left(1, \frac{p(x^*)}{p(x^{(t-1)})}\right)

   having the acceptance id203 in hand, the transition operator for
   the metropolis algorithm works like this: if a random uniform number u
   is less than or equal to \alpha , then the state x^* is accepted (as in
   (1) above), if not, it is rejected and another state is proposed (as in
   (2) above). in order to collect m samples using  metropolis sampling we
   run the following algorithm:
    1. set t = 0
    2. generate an initial state x^{(0)} from a prior distribution
       \pi^{(0)} over initial states
    3. repeat until t = m

   set t = t+1

   generate a proposal state x^* from q(x | x^{(t-1)})

   calculate the acceptance id203  \alpha = \min \left(1,
   \frac{p(x^*)}{p(x^{(t-1)})}\right)

   draw a random number u from \text{unif}(0,1)

   if u \leq \alpha , accept the proposal and set x^{(t)} = x^*

   else  set x^{(t)} = x^{(t-1)}

example: using the metropolis algorithm to sample from an unknown
distribution

   say that we have some mysterious function

   p(x) = (1 + x^2)^{-1}

   from which we would like to draw samples. to do so using metropolis
   sampling we need to define two things: (1) the prior distribution
   \pi^{(0)} over the initial state of the markov chain, and (2)  the
   proposal distribution q(x | x^{(t-1)}) . for this example we define:

   \pi^{(0)} \sim \mathcal n(0,1)

   q(x | x^{(t-1)}) \sim \mathcal n(x^{(t-1)},1) ,

   both of which are simply a normal distribution, one centered at zero,
   the other centered at previous state of the chain. the following chunk
   of matlab code runs the metropolis sampler with this proposal
   distribution and prior.
% metropolis sampling example
randn('seed',12345);

% define the target distribution
p = inline('(1 + x.^2).^-1','x')

% some constants
nsamples = 5000;
burnin = 500;
ndisplay = 30;
sigma = 1;
minn = -20; maxx = 20;
xx = 3*minn:.1:3*maxx;
target = p(xx);
pausedur = .8;

% initialze sampler
x = zeros(1 ,nsamples);
x(1) = randn;
t = 1;

% run sampler
while t < nsamples
        t = t+1;

        % sample from proposal
        xstar = normrnd(x(t-1) ,sigma);
        proposal = normpdf(xx,x(t-1),sigma);

        % calculate the acceptance id203
        alpha = min([1, p(xstar)/p(x(t-1))]);

        % accept or reject?
        u = rand;
        if u < alpha
                x(t) = xstar;
                str = 'accepted';
        else
                x(t) = x(t-1);
                str = 'rejected';
        end

        % display sampling dynamics
        if t < ndisplay + 1
                figure(1);
                subplot(211);
                cla
                plot(xx,target,'k');
                hold on;
                plot(xx,proposal,'r');
                line([x(t-1),x(t-1)],[0 p(x(t-1))],'color','b','linewidth',2)
                scatter(xstar,0,'ro','linewidth',2)
                line([xstar,xstar],[0 p(xstar)],'color','r','linewidth',2)
                plot(x(1:t),zeros(1,t),'ko')
                legend({'target','proposal','p(x^{(t-1)})','x^*','p(x^*)','kept
samples'})

                switch str
                        case 'rejected'
                                scatter(xstar,p(xstar),'rx','linewidth',3)
                        case 'accepted'
                                scatter(xstar,p(xstar),'rs','linewidth',3)
                end
                scatter(x(t-1),p(x(t-1)),'bo','linewidth',3)
                title(sprintf('sample % d %s',t,str))
                xlim([minn,maxx])
                subplot(212);
                hist(x(1:t),50); colormap hot;
                xlim([minn,maxx])
                title(['sample ',str]);
                drawnow
                pause(pausedur);
        end
end

% display markov chain
figure(1); clf
subplot(211);
stairs(x(1:t),1:t, 'k');
hold on;
hb = plot([-10 10],[burnin burnin],'b--')
ylabel('t'); xlabel('samples, x');
set(gca , 'ydir', 'reverse');
ylim([0 t])
axis tight;
xlim([-10 10]);
title('markov chain path');
legend(hb,'burnin');

% display samples
subplot(212);
nbins = 200;
samplebins = linspace(minn,maxx,nbins);
counts = hist(x(burnin:end), samplebins);
bar(samplebins, counts/sum(counts), 'k');
xlabel('samples, x' ); ylabel( 'p(x)' );
title('samples');

% overlay analytic density of student t
nu = 1;
y = tpdf(samplebins,nu)
hold on;
plot(samplebins, y/sum(y) , 'r-', 'linewidth', 2);
legend('samples',sprintf('theoretic\nstudent''s t'))
axis tight
xlim([-10 10]);

   using the metropolis algorithm to sample from a continuous distribution
   (black)

   in the figure above, we visualize the first 50 iterations of the
   metropolis sampler.the black curve represents the target distribution
   p(x) . the red curve that is bouncing about the x-axis is the proposal
   distribution q(x | x^{(t-1)}) (if the figure is not animated, just
   click on it). the vertical blue line (about which the bouncing proposal
   distribution is centered) represents the quantity p(x^{(t-1)}) , and
   the vertical red line represents the quantity p(x^*) , for a proposal
   state x^* sampled according to the red  curve. at every iteration, if
   the vertical red line is longer than the blue line, then the sample x^*
   is accepted, and the proposal distribution becomes centered about the
   newly accepted sample. if the blue line is longer, the sample is
   randomly rejected or accepted.

   but why randomly keep    bad    proposal samples? it turns out that doing
   this allows the markov chain to every-so-often visit states of low
   id203 under the target distribution. this is a desirable property
   if we want the chain to adequately sample the entire target
   distribution, including any tails.

   an attractive property of the metropolis algorithm is that the target
   distribution p(x) does not have to be a properly normalized id203
   distribution. this is due to the fact that the acceptance id203
   is based on the ratio of two values of the target distribution. i   ll
   show you what i mean. if p(x) is an unnormalized distribution and

   p^*(x) = \frac{p(x)}{z}

   is a properly normalized id203 distribution with normalizing
   constant z , then

   p(x) = zp^*(x)

   and a ratio like that used in calculating the acceptance id203
   \alpha is

   \frac{p(a)}{p(b)} = \frac{zp^*(a)}{zp^*(b)} = \frac{p^*(a)}{p^*(b)}

   the normalizing constants z cancel! this attractive property is quite
   useful in the context of bayesian methods, where determining the
   normalizing constant for a distribution may be impractical to calculate
   directly. this property is demonstrated in current example. it turns
   out that the    mystery    distribution that we sampled from using the
   metropolis algorithm is an unnormalized form of the student   s-t
   distribution with one degree of freedom. comparing p(x) to the
   definition of the definition student   s-t

   student(x,\nu) = \frac{\gamma\left(\frac{\nu + 1}{2}
   \right)}{\sqrt{\nu\pi} \gamma\left( \frac{\nu}{2}\right)}\left(
   1+\frac{x^2}{\nu}\right)^{-\frac{\nu+1}{2}} = \frac{(1 + x^2)^{-1}}{z}
   = \frac{p(x)}{z}

   we see that p(x) is a student   s-t distribution with degrees of freedom
   \nu=1 , but missing the normalizing constant

   z = \left(\frac{\gamma\left(\frac{\nu + 1}{2} \right)}{\sqrt{\nu\pi}
   \gamma\left( \frac{\nu}{2}\right)}\right )^{-1}

   below is additional output from the code above showing that the samples
   from metropolis sampler draws samples that follow a
   normalized student   s-t distribution, even though p(x) is not
   normalized.

   metropolis samples from an unnormalized t-distribution follow the
   normalized distribution

   the upper plot shows the progression of the markov chain   s progression
   from state x^{(0)} (top) to state x^{(5000)} (bottom). the burn in
   period for this chain was chosen to be 500 transitions, and is
   indicated by the dashed blue line (for more on burnin see this previous
   [25]post).

   the bottom plot shows samples from the markov chain in black (with burn
   in samples removed). the theoretical curve for the student   s-t with one
   degree of freedom is overlayed in red. we see that the states kept by
   the metropolis sampler transition operator sample from values that
   follow the student   s-t, even though the function p(x) used in the
   transition operator was not a properly normalized id203
   distribution.

reversibility of the transition operator

   it turns out that there is a theoretical constraint on the markov chain
   the transition operator in order for it settle into a stationary
   distribution (i.e. a target distribution we care about). the constraint
   states that the id203 of the transition x^{(t)} \to x^{(t+1)}
   must be equal to the id203 of the reverse transition x^{(t+1)}
   \to x^{(t)} . this reversibility property is often referred to as
   detailed balance. using the metropolis algorithm transition operator,
   reversibility is assured if the proposal distribution q(x|x^{(t-1)}) is
   symmetric. such symmetric proposal distributions are the normal,
   cauchy, student   s-t, and uniform distributions.

   however, using a symmetric proposal distribution may not be reasonable
   to adequately or efficiently sample all possible target distributions.
   for instance if a target distribution is bounded on the positive
   numbers 0 < x \leq \infty , we would like to use a proposal
   distribution that has the same support, and will thus be assymetric.
   this is where the metropolis-hastings sampling algorithm comes in. we
   will discuss in a later post how the metropolis-hastings sampler uses a
   simple change to the calculation of the acceptance id203 which
   allows us to use non-symmetric proposal distributions.
   advertisements

share this:

     * [26]twitter
     * [27]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [28]view all posts by dustinstansbury   

   posted on october 5, 2012, in [29]algorithms, [30]sampling methods,
   [31]statistics and tagged [32]acceptance id203, [33]detailed
   balance, [34]id115, [35]mcmc, [36]metropolis
   sampling, [37]metropolis-hastings sampling, [38]proposal distribution,
   [39]reversibility, [40]target distribution. bookmark the [41]permalink.
   [42]3 comments.
   [43]    a brief introduction to markov chains
   [44]mcmc: the metropolis-hastings sampler    
     * [45]leave a comment
     * [46]trackbacks 2
     * [47]comments 1

    1. aparna sen | [48]july 21, 2018 at 10:01 pm
       great post. unpretentious and easy to follow.
       [49]reply

    1. pingback: [50]mcmc: the metropolis-hastings sampler    the clever
       machine
    2. pingback: [51]a gentle introduction to id115
       (mcmc)    the clever machine

leave a reply [52]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [53]googleplus-sign-in

     *
     *

   [54]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [55]log out /
   [56]change )
   google photo

   you are commenting using your google account. ( [57]log out /
   [58]change )
   twitter picture

   you are commenting using your twitter account. ( [59]log out /
   [60]change )
   facebook photo

   you are commenting using your facebook account. ( [61]log out /
   [62]change )
   [63]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [64]algorithms [65]classification [66]id174
       [67]density estimation [68]derivations [69]id171
       [70]fmri [71]id119 [72]latex [73]machine learning
       [74]matlab [75]maximum likelihood [76]mcmc [77]neural networks
       [78]neuroscience [79]optimization [80]proofs [81]regression
       [82]sampling [83]sampling methods [84]simulations [85]statistics
       [86]theory [87]tips & tricks [88]uncategorized
     * recent posts
          + [89]derivation: maximum likelihood for id82s
          + [90]a gentle introduction to id158s
          + [91]derivation: derivatives for common neural network
            id180
          + [92]derivation: error id26 & id119 for
            neural networks
          + [93]model selection: underfitting, overfitting, and the
            id160
          + [94]supplemental proof 1
          + [95]the statistical whitening transform
          + [96]covariance matrices and data distributions
          + [97]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [98]derivation: the covariance matrix of an ols estimator (and
            applications to gls)
     * archives
          + [99]september 2014
          + [100]april 2013
          + [101]march 2013
          + [102]january 2013
          + [103]december 2012
          + [104]november 2012
          + [105]october 2012
          + [106]september 2012
          + [107]march 2012
          + [108]february 2012
          + [109]january 2012
     * meta
          + [110]register
          + [111]log in
          + [112]entries rss
          + [113]comments rss
          + [114]wordpress.com
       advertisements

   [115]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [116]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [117]cookie policy

   iframe: [118]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/feed/
   4. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
   5. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#access
  11. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#main
  12. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#sidebar
  13. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#sidebar2
  14. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
  21. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  22. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
  25. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
  26. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/?share=twitter
  27. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/?share=facebook
  28. https://theclevermachine.wordpress.com/author/dustinstansbury/
  29. https://theclevermachine.wordpress.com/category/algorithms/
  30. https://theclevermachine.wordpress.com/category/sampling-methods/
  31. https://theclevermachine.wordpress.com/category/statistics/
  32. https://theclevermachine.wordpress.com/tag/acceptance-id203/
  33. https://theclevermachine.wordpress.com/tag/detailed-balance/
  34. https://theclevermachine.wordpress.com/tag/markov-chain-monte-carlo/
  35. https://theclevermachine.wordpress.com/tag/mcmc/
  36. https://theclevermachine.wordpress.com/tag/metropolis-sampling/
  37. https://theclevermachine.wordpress.com/tag/metropolis-hastings-sampling/
  38. https://theclevermachine.wordpress.com/tag/proposal-distribution/
  39. https://theclevermachine.wordpress.com/tag/reversibility/
  40. https://theclevermachine.wordpress.com/tag/target-distribution/
  41. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  42. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#comments
  43. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
  44. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  45. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#respond
  46. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#trackbacks
  47. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#comments
  48. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#comment-1453
  49. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/?replytocom=1453#respond
  50. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  51. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
  52. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#respond
  53. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  54. https://gravatar.com/site/signup/
  55. javascript:highlandercomments.doexternallogout( 'wordpress' );
  56. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  57. javascript:highlandercomments.doexternallogout( 'googleplus' );
  58. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  59. javascript:highlandercomments.doexternallogout( 'twitter' );
  60. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  61. javascript:highlandercomments.doexternallogout( 'facebook' );
  62. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  63. javascript:highlandercomments.cancelexternalwindow();
  64. https://theclevermachine.wordpress.com/category/algorithms/
  65. https://theclevermachine.wordpress.com/category/algorithms/classification/
  66. https://theclevermachine.wordpress.com/category/data-preprocessing/
  67. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  68. https://theclevermachine.wordpress.com/category/derivations/
  69. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
  70. https://theclevermachine.wordpress.com/category/fmri/
  71. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  72. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
  73. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  74. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
  75. https://theclevermachine.wordpress.com/category/maximum-likelihood/
  76. https://theclevermachine.wordpress.com/category/mcmc/
  77. https://theclevermachine.wordpress.com/category/neural-networks/
  78. https://theclevermachine.wordpress.com/category/neuroscience/
  79. https://theclevermachine.wordpress.com/category/optimization/
  80. https://theclevermachine.wordpress.com/category/proofs/
  81. https://theclevermachine.wordpress.com/category/algorithms/regression/
  82. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  83. https://theclevermachine.wordpress.com/category/sampling-methods/
  84. https://theclevermachine.wordpress.com/category/simulations/
  85. https://theclevermachine.wordpress.com/category/statistics/
  86. https://theclevermachine.wordpress.com/category/theory/
  87. https://theclevermachine.wordpress.com/category/tips-tricks/
  88. https://theclevermachine.wordpress.com/category/uncategorized/
  89. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
  90. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  91. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  92. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  93. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
  94. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
  95. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
  96. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
  97. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
  98. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
  99. https://theclevermachine.wordpress.com/2014/09/
 100. https://theclevermachine.wordpress.com/2013/04/
 101. https://theclevermachine.wordpress.com/2013/03/
 102. https://theclevermachine.wordpress.com/2013/01/
 103. https://theclevermachine.wordpress.com/2012/12/
 104. https://theclevermachine.wordpress.com/2012/11/
 105. https://theclevermachine.wordpress.com/2012/10/
 106. https://theclevermachine.wordpress.com/2012/09/
 107. https://theclevermachine.wordpress.com/2012/03/
 108. https://theclevermachine.wordpress.com/2012/02/
 109. https://theclevermachine.wordpress.com/2012/01/
 110. https://wordpress.com/start?ref=wplogin
 111. https://theclevermachine.wordpress.com/wp-login.php
 112. https://theclevermachine.wordpress.com/feed/
 113. https://theclevermachine.wordpress.com/comments/feed/
 114. https://wordpress.com/
 115. https://wordpress.com/?ref=footer_website
 116. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
 117. https://automattic.com/cookies
 118. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 120. https://theclevermachine.files.wordpress.com/2012/10/metropolis2.gif
 121. https://theclevermachine.files.wordpress.com/2012/10/metropolisstudentst2.png
 122. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#comment-form-guest
 123. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#comment-form-load-service:wordpress.com
 124. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#comment-form-load-service:twitter
 125. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/#comment-form-load-service:facebook
