   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    mcmc: the metropolis-hastings sampler comments
   feed [4]mcmc: the metropolis sampler [5]mcmc: multivariate
   distributions, block-wise, & component-wise updates [6]alternate
   [7]alternate [8]the clever machine [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    mcmc: the metropolis sampler
   [21]mcmc: multivariate distributions, block-wise, &
   component-wise updates    

mcmc: the metropolis-hastings sampler

   [22]oct 20

   posted by [23]dustinstansbury

   in an earlier [24]post we discussed how the metropolis sampling
   algorithm can draw samples from a complex and/or unnormalized target
   id203 distributions using a markov chain. the metropolis
   algorithm first proposes a possible new state x^* in the markov chain,
   based on a previous state x^{(t-1)} , according to the proposal
   distribution q(x^* | x^{(t-1)}) . the algorithm accepts or rejects the
   proposed state based on the density of the the target distribution p(x)
   evaluated at x^* . (if any of this markov-speak is gibberish to the
   reader, please refer to the previous posts on [25]markov chains, mcmc,
   and the [26]metropolis algorithm for some clarification).

   one constraint of the metropolis sampler is that the proposal
   distribution q(x^* | x^{(t-1)}) must be symmetric. the constraint
   originates from using a markov chain to draw samples: a necessary
   condition for drawing from a markov chain   s stationary distribution is
   that at any given point in time t , the id203 of moving from
   x^{(t-1)} \rightarrow x^{(t)} must be equal to the id203 of
   moving from x^{(t-1)} \rightarrow x^{(t)} , a condition known as
   reversibility or detailed balance. however, a symmetric proposal
   distribution may be ill-fit for many problems, like when we want to
   sample from distributions that are bounded on semi infinite intervals
   (e.g. [0, \infty) ).

   in order to be able to use an asymmetric proposal distributions, the
   metropolis-hastings algorithm implements an additional correction
   factor c , defined from the proposal distribution as

   c = \frac{q(x^{(t-1)} | x^*) }{q(x^* | x^{(t-1)})}

   the correction factor adjusts the transition operator to ensure that
   the id203 of moving from x^{(t-1)} \rightarrow x^{(t)} is equal
   to the id203 of moving from x^{(t-1)} \rightarrow x^{(t)} , no
   matter the proposal distribution.

   the metropolis-hastings algorithm is implemented with essentially the
   same procedure as the metropolis sampler, except that the correction
   factor is used in the evaluation of acceptance id203 \alpha .
   specifically, to draw m samples using the metropolis-hastings sampler:
    1. set t = 0
    2. generate an initial state x^{(0)} \sim \pi^{(0)}
    3. repeat until t = m

   set t = t+1

   generate a proposal state x^* from q(x | x^{(t-1)})

   calculate the proposal correction factor c = \frac{q(x^{(t-1)} | x^*)
   }{q(x^*|x^{(t-1)})}

   calculate the acceptance id203 \alpha = \text{min} \left
   (1,\frac{p(x^*)}{p(x^{(t-1)})} \times c\right )

   draw a random number u from \text{unif}(0,1)

   if u \leq \alpha accept the proposal state x^* and set x^{(t)}=x^*

   else set x^{(t)} = x^{(t-1)}

   many consider the metropolis-hastings algorithm to be a generalization
   of the metropolis algorithm. this is because when the proposal
   distribution is symmetric, the correction factor is equal to one,
   giving the transition operator for the metropolis sampler.

example: sampling from a bayesian posterior with improper prior

   for a number of applications, including regression and density
   estimation, it is usually necessary to determine a set of parameters
   \theta to an assumed model p(y | \theta) such that the model can best
   account for some observed data y . the model function p(y | \theta) is
   often referred to as the likelihood function. in bayesian methods there
   is often an explicit prior distribution p(\theta) that is placed on the
   model parameters and controls the values that the parameters can take.

   the parameters are determined based on the posterior distribution
   p(\theta | y) , which is a id203 distribution over the possible
   parameters based on the observed data. the posterior can be determined
   using bayes    theorem:

   p(\theta | y) = \frac{p(y | \theta) p(\theta)}{p(y)}

   where, p(y) is a id172 constant that is often quite difficult
   to determine explicitly, as it involves computing sums over every
   possible value that the parameters and y can take.

   let   s say that we assume the following model (likelihood function):

   p(y | \theta) = \text{gamma}(y;a,b) , where

   \text{gamma}(y;a,b) = \frac{b^a}{\gamma(a)} y^{a-1}e^{-by} , where

   \gamma( ) is the [27]gamma function. thus, the model parameters are

   \theta = [a,b]

   the parameter a controls the shape of the distribution, and b controls
   the scale. the likelihood surface for b = 1 , and a number of values of
   a ranging from zero to five are shown below.

   likelihood surface and id155 p(y|a=2,b=1) in green


   the conditional distribution p(y | a=2, b = 1) is plotted in green
   along the likelihood surface. you can verify this is a valid
   conditional in matlab with the following command:
 plot(0:.1:10,gampdf(0:.1:10,4,1)); % gamma(4,1)

   now, let   s assume the following priors on the model parameters:

   p(b = 1) = 1

   and

   p(a) = \text{sin}(\pi a)^2

   the first prior states that b only takes a single value (i.e. 1),
   therefore we can treat it as a constant. the second (rather
   non-conventional) prior states that the id203 of a varies as a
   sinusoidal function. (note that both of these prior distributions are
   called improper priors because they do not integrate to one). because b
   is constant, we only need to estimate the value of a .

   it turns out that even though the id172 constant p(y) may be
   difficult to compute, we can sample from p(a | y) without knowing p(x)
   using the metropolis-hastings algorithm. in particular, we can ignore
   the id172 constant p(x) and sample from the unnormalized
   posterior:

   p(a | y) \propto p(y |a) p(a)

   the surface of the (unnormalized) posterior for y ranging from zero to
   ten are shown below. the prior p(a) is displayed in blue on the right
   of the plot. let   s say that we have a datapoint y = 1.5 and would like
   to estimate the posterior distribution p(a|y=1.5) using the
   metropolis-hastings algorithm. this particular target distribution is
   plotted in magenta in the plot below.

   posterior surface, prior distribution (blue), and target distribution
   (pink)

   using a symmetric proposal distribution like the normal distribution is
   not efficient for sampling from p(a|y=1.5) due to the fact that the
   posterior only has support on the real positive numbers a \in [0
   ,\infty) . an asymmetric proposal distribution with the same support,
   would provide a better coverage of the posterior. one distribution that
   operates on the positive real numbers is the exponential distribution.

   q(a) = \text{exp}(\mu) = \mu e^{-\mu a} ,

   this distribution is parameterized by a single variable \mu that
   controls the scale and location of the distribution id203 mass.
   the target posterior and a proposal distribution (for \mu = 5 ) are
   shown in the plot below.

   target posterior p(a|y) and proposal distribution q(a)

   we see that the proposal has a fairly good coverage of the posterior
   distribution. we run the metropolis-hastings sampler in the block of
   matlab code at the bottom. the markov chain path and the resulting
   samples are shown in plot below.

   metropolis-hastings markov chain and samples

   as an aside, note that the proposal distribution for this sampler does
   not depend on past samples, but only on the parameter \mu (see line 88
   in the matlab code below). each proposal states x^* is drawn
   independently of the previous state. therefore this is an example of an
   independence sampler, a specific type of metropolis-hastings sampling
   algorithm. independence samplers are notorious for being either very
   good or very poor sampling routines. the quality of the routine depends
   on the choice of the proposal distribution, and its coverage of the
   target distribution. identifying such a proposal distribution is often
   difficult in practice.

   the matlab  code for running the metropolis-hastings sampler is below.
   use the copy icon in the upper right of the code block to copy it to
   your clipboard. paste in a matlab terminal to output the figures above.
% metropolis-hastings bayesian posterior
rand('seed',12345)

% prior over scale parameters
b = 1;

% define likelihood
likelihood = inline('(b.^a/gamma(a)).*y.^(a-1).*exp(-(b.*y))','y','a','b');

% calculate and visualize the likelihood surface
yy = linspace(0,10,100);
aa = linspace(0.1,5,100);
likesurf = zeros(numel(yy),numel(aa));
for ia = 1:numel(aa); likesurf(:,ia)=likelihood(yy(:),aa(ia),b); end;

figure;
surf(likesurf); ylabel('p(y|a)'); xlabel('a'); colormap hot

% display conditional at a = 2
hold on; ly = plot3(ones(1,numel(aa))*40,1:100,likesurf(:,40),'g','linewidth',3)
xlim([0 100]); ylim([0 100]);  axis normal
set(gca,'xtick',[0,100]); set(gca,'xticklabel',[0 5]);
set(gca,'ytick',[0,100]); set(gca,'yticklabel',[0 10]);
view(65,25)
legend(ly,'p(y|a=2)','location','northeast');
hold off;
title('p(y|a)');

% define prior over shape parameters
prior = inline('sin(pi*a).^2','a');

% define the posterior
p = inline('(b.^a/gamma(a)).*y.^(a-1).*exp(-(b.*y)).*sin(pi*a).^2','y','a','b');

% calculate and display the posterior surface
postsurf = zeros(size(likesurf));
for ia = 1:numel(aa); postsurf(:,ia)=p(yy(:),aa(ia),b); end;

figure
surf(postsurf); ylabel('y'); xlabel('a'); colormap hot

% display the prior
hold on; pa = plot3(1:100,ones(1,numel(aa))*100,prior(aa),'b','linewidth',3)

% sample from p(a | y = 1.5)
y = 1.5;
target = postsurf(16,:);

% display posterior
psa = plot3(1:100, ones(1,numel(aa))*16,postsurf(16,:),'m','linewidth',3)
xlim([0 100]); ylim([0 100]);  axis normal
set(gca,'xtick',[0,100]); set(gca,'xticklabel',[0 5]);
set(gca,'ytick',[0,100]); set(gca,'yticklabel',[0 10]);
view(65,25)
legend([pa,psa],{'p(a)','p(a|y = 1.5)'},'location','northeast');
hold off
title('p(a|y)');

% initialize the metropolis-hastings sampler
% define proposal density
q = inline('exppdf(x,mu)','x','mu');

% mean for proposal density
mu = 5;

% display target and proposal
figure; hold on;
th = plot(aa,target,'m','linewidth',2);
qh = plot(aa,q(aa,mu),'k','linewidth',2)
legend([th,qh],{'target, p(a)','proposal, q(a)'});
xlabel('a');

% some constants
nsamples = 5000;
burnin = 500;
minn = 0.1; maxx = 5;

% intiialze sampler
x = zeros(1 ,nsamples);
x(1) = mu;
t = 1;

% run metropolis-hastings sampler
while t < nsamples
        t = t+1;

        % sample from proposal
        xstar = exprnd(mu);

        % correction factor
        c = q(x(t-1),mu)/q(xstar,mu);

        % calculate the (corrected) acceptance ratio
        alpha = min([1, p(y,xstar,b)/p(y,x(t-1),b)*c]);

        % accept or reject?
        u = rand;
        if u < alpha
                x(t) = xstar;
        else
                x(t) = x(t-1);
        end
end

% display markov chain
figure;
subplot(211);
stairs(x(1:t),1:t, 'k');
hold on;
hb = plot([0 maxx/2],[burnin burnin],'g--','linewidth',2)
ylabel('t'); xlabel('samples, a');
set(gca , 'ydir', 'reverse');
ylim([0 t])
axis tight;
xlim([0 maxx]);
title('markov chain path');
legend(hb,'burnin');

% display samples
subplot(212);
nbins = 100;
samplebins = linspace(minn,maxx,nbins);
counts = hist(x(burnin:end), samplebins);
bar(samplebins, counts/sum(counts), 'k');
xlabel('samples, a' ); ylabel( 'p(a | y)' );
title('samples');
xlim([0 10])

% overlay target distribution
hold on;
plot(aa, target/sum(target) , 'm-', 'linewidth', 2);
legend('sampled distribution',sprintf('target posterior'))
axis tight

wrapping up

   here we explored how the metorpolis-hastings sampling algorithm can be
   used to generalize the metropolis algorithm in order to sample from
   complex (an unnormalized) id203 distributions using asymmetric
   proposal distributions. one shortcoming of the metropolis-hastings
   algorithm is that not all of the proposed samples are accepted, wasting
   valuable computational resources. this becomes even more of an issue
   for sampling distributions in higher dimensions. this is where gibbs
   sampling comes in. we   ll see in a later post that id150 can be
   used to keep all proposal states in the markov chain by taking
   advantage of conditional probabilities.
   advertisements

share this:

     * [28]twitter
     * [29]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [30]view all posts by dustinstansbury   

   posted on october 20, 2012, in [31]algorithms, [32]sampling methods,
   [33]simulations, [34]statistics and tagged [35]bayesian likelihood,
   [36]bayesian methods, [37]bayesian posterior, [38]improper prior,
   [39]independence sampler, [40]mcmc, [41]metropolis sampling,
   [42]metropolis-hastings sampling. bookmark the [43]permalink. [44]5
   comments.
   [45]    mcmc: the metropolis sampler
   [46]mcmc: multivariate distributions, block-wise, &
   component-wise updates    
     * [47]leave a comment
     * [48]trackbacks 2
     * [49]comments 3

    1. [50]markovmagic | [51]august 28, 2013 at 6:15 pm
       reblogged this on [52]machine learning magic.
       [53]reply
    2. michael cheng | [54]september 12, 2017 at 12:00 am
       i appreciate your series. the best tutorial i   ve ever seen!
       i   m not sure, around line 14-15, it seems that the second
       $x^{(t-1)} \to x^{(t)}$ should be $x^{(t)} \to x^{(t-1)}$.
       [55]reply
    3. john | [56]may 27, 2018 at 4:00 pm
       one of the two states expressions in the sentence here under should
       be flipped, small typo      thanks a lot for those good explanations!
         the correction factor adjusts the transition operator to ensure
       that the id203 of moving from x^{(t-1)} \rightarrow x^{(t)}
       is equal to the id203 of moving from x^{(t-1)} \rightarrow
       x^{(t)}, no matter the proposal distribution.  
       [57]reply

    1. pingback: [58]mcmc: multivariate distributions, block-wise, &
       component-wise updates    the clever machine
    2. pingback: [59]a gentle introduction to id115
       (mcmc)    the clever machine

leave a reply [60]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [61]googleplus-sign-in

     *
     *

   [62]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [63]log out /
   [64]change )
   google photo

   you are commenting using your google account. ( [65]log out /
   [66]change )
   twitter picture

   you are commenting using your twitter account. ( [67]log out /
   [68]change )
   facebook photo

   you are commenting using your facebook account. ( [69]log out /
   [70]change )
   [71]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [72]algorithms [73]classification [74]id174
       [75]density estimation [76]derivations [77]id171
       [78]fmri [79]id119 [80]latex [81]machine learning
       [82]matlab [83]maximum likelihood [84]mcmc [85]neural networks
       [86]neuroscience [87]optimization [88]proofs [89]regression
       [90]sampling [91]sampling methods [92]simulations [93]statistics
       [94]theory [95]tips & tricks [96]uncategorized
     * recent posts
          + [97]derivation: maximum likelihood for id82s
          + [98]a gentle introduction to id158s
          + [99]derivation: derivatives for common neural network
            id180
          + [100]derivation: error id26 & id119 for
            neural networks
          + [101]model selection: underfitting, overfitting, and the
            id160
          + [102]supplemental proof 1
          + [103]the statistical whitening transform
          + [104]covariance matrices and data distributions
          + [105]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [106]derivation: the covariance matrix of an ols estimator
            (and applications to gls)
     * archives
          + [107]september 2014
          + [108]april 2013
          + [109]march 2013
          + [110]january 2013
          + [111]december 2012
          + [112]november 2012
          + [113]october 2012
          + [114]september 2012
          + [115]march 2012
          + [116]february 2012
          + [117]january 2012
     * meta
          + [118]register
          + [119]log in
          + [120]entries rss
          + [121]comments rss
          + [122]wordpress.com
       advertisements

   [123]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [124]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [125]cookie policy

   iframe: [126]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/feed/
   4. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
   5. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#access
  11. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#main
  12. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#sidebar
  13. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#sidebar2
  14. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  21. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
  22. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  25. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
  26. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  27. http://en.wikipedia.org/wiki/gamma_function
  28. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/?share=twitter
  29. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/?share=facebook
  30. https://theclevermachine.wordpress.com/author/dustinstansbury/
  31. https://theclevermachine.wordpress.com/category/algorithms/
  32. https://theclevermachine.wordpress.com/category/sampling-methods/
  33. https://theclevermachine.wordpress.com/category/simulations/
  34. https://theclevermachine.wordpress.com/category/statistics/
  35. https://theclevermachine.wordpress.com/tag/bayesian-likelihood/
  36. https://theclevermachine.wordpress.com/tag/bayesian-methods/
  37. https://theclevermachine.wordpress.com/tag/bayesian-posterior/
  38. https://theclevermachine.wordpress.com/tag/improper-prior/
  39. https://theclevermachine.wordpress.com/tag/independence-sampler/
  40. https://theclevermachine.wordpress.com/tag/mcmc/
  41. https://theclevermachine.wordpress.com/tag/metropolis-sampling/
  42. https://theclevermachine.wordpress.com/tag/metropolis-hastings-sampling/
  43. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  44. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comments
  45. https://theclevermachine.wordpress.com/2012/10/05/mcmc-the-metropolis-sampler/
  46. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
  47. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#respond
  48. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#trackbacks
  49. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comments
  50. http://mlmagic.wordpress.com/
  51. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comment-94
  52. http://mlmagic.wordpress.com/2013/08/29/mcmc-the-metropolis-hastings-sampler/
  53. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/?replytocom=94#respond
  54. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comment-1098
  55. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/?replytocom=1098#respond
  56. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comment-1406
  57. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/?replytocom=1406#respond
  58. https://theclevermachine.wordpress.com/2012/11/04/mcmc-multivariate-distributions-block-wise-component-wise-updates/
  59. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
  60. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#respond
  61. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  62. https://gravatar.com/site/signup/
  63. javascript:highlandercomments.doexternallogout( 'wordpress' );
  64. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  65. javascript:highlandercomments.doexternallogout( 'googleplus' );
  66. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  67. javascript:highlandercomments.doexternallogout( 'twitter' );
  68. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  69. javascript:highlandercomments.doexternallogout( 'facebook' );
  70. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
  71. javascript:highlandercomments.cancelexternalwindow();
  72. https://theclevermachine.wordpress.com/category/algorithms/
  73. https://theclevermachine.wordpress.com/category/algorithms/classification/
  74. https://theclevermachine.wordpress.com/category/data-preprocessing/
  75. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  76. https://theclevermachine.wordpress.com/category/derivations/
  77. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
  78. https://theclevermachine.wordpress.com/category/fmri/
  79. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  80. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
  81. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  82. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
  83. https://theclevermachine.wordpress.com/category/maximum-likelihood/
  84. https://theclevermachine.wordpress.com/category/mcmc/
  85. https://theclevermachine.wordpress.com/category/neural-networks/
  86. https://theclevermachine.wordpress.com/category/neuroscience/
  87. https://theclevermachine.wordpress.com/category/optimization/
  88. https://theclevermachine.wordpress.com/category/proofs/
  89. https://theclevermachine.wordpress.com/category/algorithms/regression/
  90. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  91. https://theclevermachine.wordpress.com/category/sampling-methods/
  92. https://theclevermachine.wordpress.com/category/simulations/
  93. https://theclevermachine.wordpress.com/category/statistics/
  94. https://theclevermachine.wordpress.com/category/theory/
  95. https://theclevermachine.wordpress.com/category/tips-tricks/
  96. https://theclevermachine.wordpress.com/category/uncategorized/
  97. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
  98. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  99. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
 100. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
 101. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
 102. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
 103. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
 104. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
 105. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
 106. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
 107. https://theclevermachine.wordpress.com/2014/09/
 108. https://theclevermachine.wordpress.com/2013/04/
 109. https://theclevermachine.wordpress.com/2013/03/
 110. https://theclevermachine.wordpress.com/2013/01/
 111. https://theclevermachine.wordpress.com/2012/12/
 112. https://theclevermachine.wordpress.com/2012/11/
 113. https://theclevermachine.wordpress.com/2012/10/
 114. https://theclevermachine.wordpress.com/2012/09/
 115. https://theclevermachine.wordpress.com/2012/03/
 116. https://theclevermachine.wordpress.com/2012/02/
 117. https://theclevermachine.wordpress.com/2012/01/
 118. https://wordpress.com/start?ref=wplogin
 119. https://theclevermachine.wordpress.com/wp-login.php
 120. https://theclevermachine.wordpress.com/feed/
 121. https://theclevermachine.wordpress.com/comments/feed/
 122. https://wordpress.com/
 123. https://wordpress.com/?ref=footer_blog
 124. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/
 125. https://automattic.com/cookies
 126. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 128. https://theclevermachine.files.wordpress.com/2012/10/metropolishastings-gamma-nonstandard-prior-likelihood5.png
 129. https://theclevermachine.files.wordpress.com/2012/10/metropolishastings-gamma-nonstandard-prior-posterior4.png
 130. https://theclevermachine.files.wordpress.com/2012/10/metropolishastings-gamma-nonstandard-prior-target-proposal2.png
 131. https://theclevermachine.files.wordpress.com/2012/10/metropolishastings-gamma-nonstandard-prior-samples2.png
 132. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comment-form-guest
 133. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comment-form-load-service:wordpress.com
 134. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comment-form-load-service:twitter
 135. https://theclevermachine.wordpress.com/2012/10/20/mcmc-the-metropolis-hastings-sampler/#comment-form-load-service:facebook
