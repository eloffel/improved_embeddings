   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    mcmc: hamiltonian monte carlo (a.k.a. hybrid
   monte carlo) comments feed [4]mcmc: the gibbs sampler [5]a gentle
   introduction to id115 (mcmc) [6]alternate
   [7]alternate [8]the clever machine [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    mcmc: the gibbs sampler
   [21]a gentle introduction to id115 (mcmc)    

mcmc: hamiltonian monte carlo (a.k.a. hybrid monte carlo)

   [22]nov 18

   posted by [23]dustinstansbury

   the random-walk behavior of many id115 (mcmc)
   algorithms makes markov chain convergence to a target stationary
   distribution p(x) inefficient, resulting in slow mixing.
   hamiltonian/hybrid monte carlo (hmc), is a mcmc method that adopts
   physical system dynamics rather than a id203 distribution to
   propose future states in the markov chain. this allows the markov chain
   to explore the target distribution much more efficiently, resulting in
   faster convergence. here we introduce basic analytic and numerical
   concepts for simulation of hamiltonian dynamics. we then show how
   hamiltonian dynamics can be used as the markov chain proposal function
   for an mcmc sampling algorithm (hmc).

first off, a brief physics lesson in hamiltonian dynamics

   before we can develop hamiltonian monte carlo, we need to become
   familiar with the concept of hamiltonian dynamics. hamiltonian dynamics
   is one way that physicists describe how objects move throughout a
   system. hamiltonian dynamics describe an object   s motion in terms of
   its location \bold x and momentum \bold p (equivalent to the object   s
   mass times its velocity) at some time t . for each location the object
   takes, there is an associated potential energy u(\bold x) , and for
   each momentum there is an associated kinetic energy k(\bold p) . the
   total energy of the system is constant and known as the hamiltonian
   h(\bold x, \bold p) , defined simply as the sum of the potential and
   kinetic energies:

   h(\bold x,\bold p) = u(\bold x) + k(\bold p)

   hamiltonian dynamics describe how kinetic energy is converted to
   potential energy (and vice versa) as an object moves throughout a
   system in time. this description is implemented quantitatively via a
   set of differential equations known as the hamiltonian equations:

   \frac{\partial x_i}{\partial t} = \frac{\partial h}{\partial p_i} =
   \frac{\partial k(\bold p)}{\partial p_i}
   \frac{\partial p_i}{\partial t} = -\frac{\partial h}{\partial x_i} = -
   \frac{\partial u(\bold x)}{\partial x_i}

   therefore, if we have expressions for \frac{\partial u(\bold
   x)}{\partial x_i} and \frac{\partial k(\bold p)}{\partial p_i} and a
   set of initial conditions (i.e. an initial position \bold x_0 and
   initial momentum \bold p_0 at time t_0 ), it is possible to predict the
   location and momentum of an object at any point in time t = t_0 + t by
   simulating these dynamics for a duration t

simulating hamiltonian dynamics     the leap frog method

   the hamiltonian equations describe an object   s motion in time, which is
   a continuous variable. in order to simulate hamiltonian dynamics
   numerically on a computer, it is necessary to approximate the
   hamiltonian equations by discretizing  time. this is done by splitting
   the interval t up into a series of smaller intervals of length \delta .
   the smaller the value of \delta the closer the approximation is to the
   dynamics in continuous time. there are a number of procedures that have
   been developed for discretizing time including [24]euler   s method and
   the leap frog method, which i will introduce briefly in the context of
   hamiltonian dynamics. the leap frog method updates the momentum and
   position variables sequentially, starting by simulating the momentum
   dynamics over a small interval of time \delta /2 , then simulating the
   position dynamics over a slightly longer interval in time \delta , then
   completing the momentum simulation over another small interval of time
   \delta /2 so that \bold x and \bold p now exist at the same point in
   time. specifically, the leap frog method is as follows: 1. take a half
   step in time to update the momentum variable:

   p_i(t + \delta/2) = p_i(t) - (\delta /2)\frac{\partial u}{\partial
   x_i(t)}

   2. take a full step in time to update the position variable

   x_i(t + \delta) = x_i(t) + \delta \frac{\partial k}{\partial p_i(t +
   \delta/2)}

   3. take the remaining half step in time to finish updating the momentum
   variable

   p_i(t + \delta) = p_i(t + \delta/2) - (\delta/2) \frac{\partial
   u}{\partial x_i(t+\delta)}

   the leap fog method can be run for l steps to simulate dynamics over l
   \times \delta units of time. this particular discretization method has
   a number of properties that make it preferable to other approximation
   methods like euler   s method, particularly for use in mcmc, but
   discussion of these properties are beyond the scope of this post. let   s
   see how we can use the leap frog method to simulate hamiltonian
   dynamics in a simple 1d example.

example 1: simulating hamiltonian dynamics of an harmonic oscillator

   imagine a ball with mass equal to one is attached to a
   horizontally-oriented spring. the spring exerts a force on the ball
   equal to

   f = -kx

   which works to restore the ball   s position to the equilibrium position
   of the spring  at x = 0 . let   s assume that the spring constant k ,
   which defines the strength of the restoring force is also equal to one.
   if the ball is displaced by some distance x from equilibrium, then the
   potential energy is

   u(x) = \int f dx = \int -x dx = \frac{x^2}{2}

   in addition, the kinetic energy an object with mass m moving with
   velocity v within a linear system is [25]known to be

   k(v) = \frac{(mv)^2}{2m} = \frac{v^2}{2} = \frac{p^2}{2} = k(p) ,

   if the object   s mass is equal to one, like the ball this example.
   notice that we now have in hand the expressions for both u(x) and k(p)
   . in order to simulate the hamiltonian dynamics of the system using the
   leap frog method, we also need expressions for the partial derivatives
   of each variable (in this 1d example there are only one for each
   variable):

   \frac{\partial u(x)}{\partial x} =x \frac{\partial k(p)}{\partial p} =
   p

   therefore one iteration the leap frog algorithm for simulating
   hamiltonian dynamics in this system is:

   1.   p(t + \delta/2) = p(t) - (\delta/2)x(t)
   2. x(t + \delta) = x(t) + (\delta) p(t + \delta/2) 3. p(t + \delta) =
   p(t + \delta /2) - (\delta/2)x(t + \delta)

   we simulate the dynamics of the spring-mass system described using the
   leap frog method in matlab below (if the graph is not animated, try
   clicking on it to open up the linked .gif). the left bar in the bottom
   left subpanel of the simulation output demonstrates the trade-off
   between potential and kinetic energy described by hamiltonian dynamics.
   the cyan portion of the bar is the proportion of the hamiltonian
   contributed by  the potential energy u(x) , and the yellow portion
   represents is the contribution of the kinetic energy k(p) . the right
   bar (in all yellow), is the total value of the hamiltonian h(x,p) .
   here we see that the ball oscillates about the equilibrium position of
   the spring with a constant period/frequency.  as the ball passes the
   equilibrium position x= 0 , it has a minimum potential energy and
   maximum kinetic energy. at the extremes of the ball   s trajectory, the
   potential energy is at a maximum, while the kinetic energy is
   minimized. the  procession of momentum and position map out positions
   in what is referred to as phase space, which is displayed in the bottom
   right subpanel of the output. the harmonic oscillator maps out an
   ellipse in phase space. the size of the ellipse depends on the energy
   of the system defined by initial conditions.

   simple example of hamiltonian dynamics: 1d harmonic oscillator (click
   to see animated)

   you may also notice that the value of the hamiltonian h is not a
   exactly constant in the simulation, but oscillates slightly. this is an
   artifact known as energy drift due to approximations used to the
   discretize time.
% example 1: simulating hamiltonian dynamics
%            of harmonic oscillator
% step size
delta = 0.1;

% # leap frog
l = 70;

% define kinetic energy function
k = inline('p^2/2','p');

% define potential energy function for spring (k =1)
u = inline('1/2*x^2','x');

% define gradient of potential energy
du = inline('x','x');

% initial conditions
x0 = -4; % postiion
p0 = 1;  % momentum
figure

%% simulate hamiltonian dynamics with leapfrog method
% first half step for momentum
pstep = p0 - delta/2*du(x0)';

% first full step for position/sample
xstep = x0 + delta*pstep;

% full steps
for jl = 1:l-1
        % update momentum
        pstep = pstep - delta*du(xstep);

        % update position
        xstep = xstep + delta*pstep;

        % update displays
        subplot(211), cla
        hold on;
        xx = linspace(-6,xstep,1000);
        plot(xx,sin(6*linspace(0,2*pi,1000)),'k-');
        plot(xstep+.5,0,'bo','linewidth',20)
        xlim([-6 6]);ylim([-1 1])
        hold off;
        title('harmonic oscillator')
        subplot(223), cla
        b = bar([u(xstep),k(pstep);0,u(xstep)+k(pstep)],'stacked');
        set(gca,'xticklabel',{'u+k','h'})
        ylim([0 10]);
        title('energy')
        subplot(224);
        plot(xstep,pstep,'ko','linewidth',20);
        xlim([-6 6]); ylim([-6 6]); axis square
        xlabel('x'); ylabel('p');
        title('phase space')
        pause(.1)
end
% (last half step for momentum)
pstep = pstep - delta/2*du(xstep);

hamiltonian dynamics and the target distribution p(\bold x)

   now that we have a better understanding of what hamiltonian dynamics
   are and how they can be simulated, let   s now discuss how we can use
   hamiltonian dynamics for mcmc. the main idea behind hamiltonian/hibrid
   monte carlo is to develop a hamiltonian function h(\bold x, \bold p)
   such that the resulting hamiltonian dynamics allow us to efficiently
   explore some target distribution p(\bold x) . how can we choose such a
   hamiltonian function? it turns out it is pretty simple to relate a
   h(\bold x, \bold p) to p(\bold x) using a basic concept adopted from
   statistical mechanics known as the canonical distribution. for any
   energy function e(\bf\theta) over a set of variables \theta , we can
   define the corresponding canonical distribution as: p(\theta) =
   \frac{1}{z}e^{-e(\bf\theta)} where we simply take the exponential of
   the negative of the energy function. the variable z is a normalizing
   constant called the partition function that scales the canonical
   distribution such that is sums to one, creating a valid id203
   distribution. don   t worry about z , it isn   t really important because,
   as you may recall from an earlier post, mcmc methods can sample from
   unscaled id203 distributions. now, as we saw above, the energy
   function for hamiltonian dynamics is a combination of potential and
   kinetic energies: e(\theta) = h(\bold x,\bold p) = u(\bold x) + k(\bold
   p)

   therefore the canoncial distribution for the hamiltonian dynamics
   energy function is

   p(\bold x,\bold p) \propto e^{-h(\bold x,\bold p)} \\ = e^{-[u(\bold x)
   - k(\bold p)]} \\ = e^{-u(\bold x)}e^{-k(\bold p)} \\ \propto p(\bold
   x)p(\bold p)

   here we see that joint (canonical) distribution for \bold x and \bold p
   factorizes. this means that the two variables are independent, and the
   canoncial distribution p(\bold x) is independent of the analogous
   distribution for the momentum. therefore, as we   ll see shortly, we can
   use hamiltonian dynamics to sample from the joint canonical
   distribution over \bold p and \bold x and simply ignore the momentum
   contributions. note that this is an example of introducing auxiliary
   variables to facilitate the markov chain path. introducing the
   auxiliary variable \bold p allows us to use hamiltonian dynamics, which
   are unavailable without them. because the canonical distribution for
   \bold x is independent of the canonical distribution for \bold p , we
   can choose any distribution from which to sample the momentum
   variables. a common choice is to use a zero-mean normal distribution
   with unit variance:

   p(\bold p) \propto \frac{\bold{p^tp}}{2}

   note that this is equivalent to having a quadratic potential energy
   term in the hamiltonian:

   k(\bold p) = \frac{\bold{p^tp}}{2}

   recall that this is is the exact quadratic kinetic energy function
   (albeit in 1d) used in the harmonic oscillator example above. this is a
   convenient choice for the kinetic energy function as all partial
   derivatives are easy to compute. now that we have defined a kinetic
   energy function, all we have to do is find a potential energy function
   u(\bold x) that when negated and run through the exponential function,
   gives the target distribution p(\bold x) (or an unscaled version of
   it). another way of thinking of it is that we can define the potential
   energy function as

   u(\bold x) = -\log p(\bold x) .

   if we can calculate -\frac{\partial \log(p(\bold x)) }{\partial x_i} ,
   then we   re in business and we can simulate hamiltonian dynamics that
   can be used in an mcmc technique.

hamiltonian monte carlo

   in hmc we use hamiltonian dynamics as a proposal function for a markov
   chain in order to explore the target (canonical) density  p(\bold x)
   defined by u(\bold x) more efficiently than using a proposal
   id203 distribution. starting at an initial state [\bold x_0,
   \bold p_0] , we simulate hamiltonian dynamics for a short time using
   the leap frog method. we then use the state of the position and
   momentum variables at the end of the simulation as our proposed states
   variables \bold x^* and \bold p^* . the proposed state is accepted
   using an update rule analogous to the metropolis acceptance criterion.
   specifically if the id203 of the proposed state after hamiltonian
   dynamics

   p(\bold x^*, \bold p^*) \propto e^{-[u(\bold x^*) + k{\bold p^*}]}

   is greater than id203 of the state prior to the hamiltonian
   dynamics

   p(\bold x_0,\bold p_0) \propto e^{-[u(\bold x^{(t-1)}), k(\bold
   p^{(t-1)})]}

   then the proposed state is accepted, otherwise, the proposed state is
   accepted randomly. if the state is rejected, the next state of the
   markov chain is set as the state at (t-1) . for a given set of initial
   conditions, hamiltonian dynamics will follow contours of constant
   energy in phase space (analogous to the circle traced out in phase
   space in the example above). therefore we must randomly perturb the
   dynamics so as to explore all of p(\bold x) . this is done by simply
   drawing a random momentum from the corresponding canonical distribution
   p(\bold p)  before running the dynamics prior to each sampling
   iteration t . combining these steps, sampling random momentum, followed
   by hamiltonian dynamics and metropolis acceptance criterion defines the
   hmc algorithm for drawing m samples from a target distribution:
    1. set t = 0
    2. generate an initial position state \bold x^{(0)} \sim \pi^{(0)}
    3. repeat until t = m

   set t = t+1

       sample a new initial momentum variable from the momentum canonical
   distribution \bold p_0 \sim p(\bold p)

       set \bold x_0 = \bold x^{(t-1)}

       run leap frog algorithm starting at [\bold x_0, \bold p_0]  for l
   steps and stepsize \delta to obtain proposed states \bold x^* and \bold
   p^*

       calculate the metropolis acceptance id203:

   \alpha = \text{min}(1,\exp(-u(\bold x^*) + u(\bold x_0) - k(\bold p^*)
   + k(\bold p_0)))

       draw a random number u from \text{unif}(0,1)

   if u \leq \alpha accept the proposed state position \bold x^* and set
   the next state in the markov chain \bold x^{(t)}=\bold x^*

   else set \bold x^{(t)} = \bold x^{(t-1)}

   in the next example we implement hmc  to sample from a multivariate
   target distribution that we have sampled from previously using
   multi-variate metropolis-hastings, the bivariate normal. we also
   qualitatively compare the sampling dynamics of hmc to multivariate
   metropolis-hastings for the sampling the same distribution.

example 2: hamiltonian monte for sampling a bivariate normal distribution

   as a reminder, the target distribution p(\bold x) for this exampleis a
   normal form with following parameterization:

   p(\bold x) = \mathcal n (\bold{\mu}, \bold \sigma)

   with mean \mu = [\mu_1,\mu_2]= [0, 0]

   and covariance

   \bold \sigma = \begin{bmatrix} 1 & \rho_{12} \\ \rho_{21} &
   1\end{bmatrix} = \begin{bmatrix} 1 & 0.8 \\ 0.8 & 1\end{bmatrix}

   in order to sample from p(\bold x) (assuming that we are using a
   quadratic energy function), we need to determine the expressions for
   u(\bold x) and

   \frac{\partial u(\bold x) }{ \partial x_i} .

   recall that the target potential energy function can be defined from
   the canonical form as

   u(\bold x) = -\log(p(\bold x))

   if we take the negative log of the normal distribution outline above,
   this defines the following potential energy function:

   e(\bold x) = -\log \left(e^{-\frac{\bold{x^t \sigma^{-1} x}}{2}}\right)
   - \log z

   where z is the normalizing constant for a normal distribution (and can
   be ignored because it will eventually cancel). the potential energy
   function is then simply:

   u(\bold x) = \frac{\bold{x^t \sigma^{-1}x}}{2}

   with partial derivatives

   \frac{\partial u(\bold x)}{\partial x_i} = x_i

   using these expressions for the potential energy and its partial
   derivatives, we implement hmc for sampling from the bivariate normal in
   matlab:

   hybrid monte carlo samples from bivariate normal target distribution

   in the graph above we display hmc samples of the target distribution,
   starting from an initial position very far from the mean of the target.
   we can see that hmc rapidly approaches areas of high density under the
   target distribution. we compare these samples with samples drawn using
   the metropolis-hastings (mh) algorithm below. the mh algorithm
   converges much slower than hmc, and consecutive samples have much
   higher autocorrelation than samples drawn using hmc.
   [26]mhgaussiansamples

   metropolis-hasting (mh) samples of the same target distribution.
   autocorrelation is evident. hmc is much more efficient than mh.

   the matlab code for the hmc sampler:
% example 2: hybrid monte carlo sampling -- bivariate normal
rand('seed',12345);
randn('seed',12345);

% step size
delta = 0.3;
nsamples = 1000;
l = 20;

% define potential energy function
u = inline('transp(x)*inv([1,.8;.8,1])*x','x');

% define gradient of potential energy
du = inline('transp(x)*inv([1,.8;.8,1])','x');

% define kinetic energy function
k = inline('sum((transp(p)*p))/2','p');

% initial state
x = zeros(2,nsamples);
x0 = [0;6];
x(:,1) = x0;

t = 1;
while t < nsamples
        t = t + 1;

        % sample random momentum
        p0 = randn(2,1);

        %% simulate hamiltonian dynamics
        % first 1/2 step of momentum
        pstar = p0 - delta/2*du(x(:,t-1))';

        % first full step for position/sample
        xstar = x(:,t-1) + delta*pstar;

        % full steps
        for jl = 1:l-1
                % momentum
                pstar = pstar - delta*du(xstar)';
                % position/sample
                xstar = xstar + delta*pstar;
        end

        % last halp step
        pstar = pstar - delta/2*du(xstar)';

        % could negate momentum here to leave
        % the proposal distribution symmetric.
        % however we throw this away for next
        % sample, so it doesn't matter

        % evaluate energies at
        % start and end of trajectory
        u0 = u(x(:,t-1));
        ustar = u(xstar);

        k0 = k(p0);
        kstar = k(pstar);

        % acceptance/rejection criterion
        alpha = min(1,exp((u0 + k0) - (ustar + kstar)));

        u = rand;
        if u < alpha
                x(:,t) = xstar;
        else
                x(:,t) = x(:,t-1);
        end
end

% display
figure
scatter(x(1,:),x(2,:),'k.'); hold on;
plot(x(1,1:50),x(2,1:50),'ro-','linewidth',2);
xlim([-6 6]); ylim([-6 6]);
legend({'samples','1st 50 states'},'location','northwest')
title('hamiltonian monte carlo')

wrapping up

   in this post we introduced the hamiltonian/hybrid monte carlo algorithm
   for more efficient mcmc sampling. the hmc algorithm is extremely
   powerful for sampling distributions that can be represented terms of a
   potential energy function and its partial derivatives. despite the
   efficiency and elegance of hmc, it is an underrepresented sampling
   routine in the literature. this may be due to the popularity of simpler
   algorithms such as id150 or metropolis-hastings, or perhaps
   due to the fact that one must select hyperparameters such as the number
   of leap frog steps and leap frog step size when using hmc. however,
   recent research has provided effective heuristics such as adapting the
   leap frog step size in order to maintain a constant metropolis
   rejection rate, which facilitate the use of hmc for general
   applications.
   advertisements

share this:

     * [27]twitter
     * [28]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [29]view all posts by dustinstansbury   

   posted on november 18, 2012, in [30]algorithms, [31]sampling,
   [32]simulations and tagged [33]auxiliary variables, [34]canonical
   distribution, [35]energy drift, [36]hamiltonian dynamics,
   [37]hamiltonian monte carlo, [38]hybrid monte carlo, [39]markov chain
   monte carlo, [40]mcmc, [41]partition function, [42]phase space,
   [43]stationary distribution. bookmark the [44]permalink. [45]11
   comments.
   [46]    mcmc: the gibbs sampler
   [47]a gentle introduction to id115 (mcmc)    
     * [48]leave a comment
     * [49]trackbacks 3
     * [50]comments 8

    1. prajit | [51]july 2, 2014 at 7:08 pm
       i believe there is an error in your hmc sampler. your code has `u0
       = u(x0)`. however, x0 is never updated to be the latest sample in
       your loop. the correct line of code should be `u0 = u(x(:, t-1))`.
       alternatively, you could add `x0 = x(:, t-1)` after `t = t + 1`.
       thank you for the great tutorial.
       [52]reply
          + [53]dustinstansbury | [54]september 19, 2014 at 3:59 pm
            prajit, you   re correct, the code was missing the updated
            sample states at each iteration. the code has been updated.
            thanks!
            [55]reply
    2. [56]perryzhao | [57]november 24, 2015 at 11:04 pm
       reblogged this on [58]            .
       [59]reply
    3. [60]piggy | [61]january 11, 2016 at 11:41 pm
       beautiful tutorial! very well explained.
       [62]reply
    4. [63]tomerwei | [64]april 19, 2016 at 10:57 am
       reblogged this on [65]solving one problem at a time and commented:
       aesome blog post! want to use hmc in my research
       [66]reply
    5. [67]sandipan karmakar | [68]november 26, 2016 at 6:48 am
       hello, many many thanks for such a nice short tutorial on hmc    i am
       a newbie in hmc    i want to know how sampling from a assymetric
       distribution can be done using hmc    i am talking about
       distributions like gamma, inverse gaussian etc which we frequently
       encounter in bayesian machine learning models. like for linear
       regression if i want to estimate the posterior of noise variance
       which is generally endowed with inverse gamma prior    it will be
       very helpful if you can enlighten me regarding this    thanks in
       advance   
       [69]reply
    6. [70]ddynamic | [71]may 10, 2017 at 7:38 pm
       reblogged this on [72]flaw //        and commented:
       still reading   ..
       [73]reply
    7. [74]ddynamic | [75]may 10, 2017 at 8:15 pm
       thanks for the awesome post. reblogged to the       //flaw.
       [76]reply

    1. pingback: [77]a gentle introduction to id115
       (mcmc)    the clever machine
    2. pingback: [78]mcmc: hamiltonian monte carlo (a.k.a. hybrid monte
       carlo)     tingting zhao's research blog
    3. pingback: [79]if you did not already know | data analytics & r

leave a reply [80]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [81]googleplus-sign-in

     *
     *

   [82]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [83]log out /
   [84]change )
   google photo

   you are commenting using your google account. ( [85]log out /
   [86]change )
   twitter picture

   you are commenting using your twitter account. ( [87]log out /
   [88]change )
   facebook photo

   you are commenting using your facebook account. ( [89]log out /
   [90]change )
   [91]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [92]algorithms [93]classification [94]id174
       [95]density estimation [96]derivations [97]id171
       [98]fmri [99]id119 [100]latex [101]machine learning
       [102]matlab [103]maximum likelihood [104]mcmc [105]neural networks
       [106]neuroscience [107]optimization [108]proofs [109]regression
       [110]sampling [111]sampling methods [112]simulations
       [113]statistics [114]theory [115]tips & tricks [116]uncategorized
     * recent posts
          + [117]derivation: maximum likelihood for id82s
          + [118]a gentle introduction to id158s
          + [119]derivation: derivatives for common neural network
            id180
          + [120]derivation: error id26 & id119 for
            neural networks
          + [121]model selection: underfitting, overfitting, and the
            id160
          + [122]supplemental proof 1
          + [123]the statistical whitening transform
          + [124]covariance matrices and data distributions
          + [125]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [126]derivation: the covariance matrix of an ols estimator
            (and applications to gls)
     * archives
          + [127]september 2014
          + [128]april 2013
          + [129]march 2013
          + [130]january 2013
          + [131]december 2012
          + [132]november 2012
          + [133]october 2012
          + [134]september 2012
          + [135]march 2012
          + [136]february 2012
          + [137]january 2012
     * meta
          + [138]register
          + [139]log in
          + [140]entries rss
          + [141]comments rss
          + [142]wordpress.com
       advertisements

   [143]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [144]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [145]cookie policy

   iframe: [146]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/feed/
   4. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
   5. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#access
  11. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#main
  12. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#sidebar
  13. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#sidebar2
  14. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  21. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
  22. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. http://math.usu.edu/~powell/biomath/mlab3-02/node1.html
  25. http://en.wikipedia.org/wiki/kinetic_energy
  26. https://theclevermachine.files.wordpress.com/2012/11/mhgaussiansamples1.png
  27. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?share=twitter
  28. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?share=facebook
  29. https://theclevermachine.wordpress.com/author/dustinstansbury/
  30. https://theclevermachine.wordpress.com/category/algorithms/
  31. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  32. https://theclevermachine.wordpress.com/category/simulations/
  33. https://theclevermachine.wordpress.com/tag/auxiliary-variables/
  34. https://theclevermachine.wordpress.com/tag/canonical-distribution/
  35. https://theclevermachine.wordpress.com/tag/energy-drift/
  36. https://theclevermachine.wordpress.com/tag/hamiltonian-dynamics/
  37. https://theclevermachine.wordpress.com/tag/hamiltonian-monte-carlo/
  38. https://theclevermachine.wordpress.com/tag/hybrid-monte-carlo/
  39. https://theclevermachine.wordpress.com/tag/markov-chain-monte-carlo/
  40. https://theclevermachine.wordpress.com/tag/mcmc/
  41. https://theclevermachine.wordpress.com/tag/partition-function/
  42. https://theclevermachine.wordpress.com/tag/phase-space/
  43. https://theclevermachine.wordpress.com/tag/stationary-distribution/
  44. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  45. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comments
  46. https://theclevermachine.wordpress.com/2012/11/05/mcmc-the-gibbs-sampler/
  47. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
  48. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#respond
  49. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#trackbacks
  50. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comments
  51. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-160
  52. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=160#respond
  53. http://machinelearnings.wordpress.com/
  54. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-196
  55. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=196#respond
  56. https://perryzhao.wordpress.com/
  57. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-426
  58. https://perryzhao.wordpress.com/2015/11/25/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  59. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=426#respond
  60. http://www.worldofpiggy.com/
  61. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-528
  62. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=528#respond
  63. http://tomerslist.wordpress.com/
  64. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-637
  65. https://tomerslist.wordpress.com/2016/04/19/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  66. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=637#respond
  67. http://gravatar.com/sandipank
  68. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-786
  69. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=786#respond
  70. http://shouldseeblog.wordpress.com/
  71. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-958
  72. https://shouldseeblog.wordpress.com/2017/05/11/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  73. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=958#respond
  74. http://shouldseeblog.wordpress.com/
  75. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-959
  76. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/?replytocom=959#respond
  77. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
  78. https://zhaott0416.wordpress.com/2016/03/20/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  79. http://advanceddataanalytics.net/2018/05/05/if-you-did-not-already-know-354/
  80. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#respond
  81. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  82. https://gravatar.com/site/signup/
  83. javascript:highlandercomments.doexternallogout( 'wordpress' );
  84. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  85. javascript:highlandercomments.doexternallogout( 'googleplus' );
  86. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  87. javascript:highlandercomments.doexternallogout( 'twitter' );
  88. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  89. javascript:highlandercomments.doexternallogout( 'facebook' );
  90. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
  91. javascript:highlandercomments.cancelexternalwindow();
  92. https://theclevermachine.wordpress.com/category/algorithms/
  93. https://theclevermachine.wordpress.com/category/algorithms/classification/
  94. https://theclevermachine.wordpress.com/category/data-preprocessing/
  95. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  96. https://theclevermachine.wordpress.com/category/derivations/
  97. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
  98. https://theclevermachine.wordpress.com/category/fmri/
  99. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
 100. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
 101. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
 102. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
 103. https://theclevermachine.wordpress.com/category/maximum-likelihood/
 104. https://theclevermachine.wordpress.com/category/mcmc/
 105. https://theclevermachine.wordpress.com/category/neural-networks/
 106. https://theclevermachine.wordpress.com/category/neuroscience/
 107. https://theclevermachine.wordpress.com/category/optimization/
 108. https://theclevermachine.wordpress.com/category/proofs/
 109. https://theclevermachine.wordpress.com/category/algorithms/regression/
 110. https://theclevermachine.wordpress.com/category/algorithms/sampling/
 111. https://theclevermachine.wordpress.com/category/sampling-methods/
 112. https://theclevermachine.wordpress.com/category/simulations/
 113. https://theclevermachine.wordpress.com/category/statistics/
 114. https://theclevermachine.wordpress.com/category/theory/
 115. https://theclevermachine.wordpress.com/category/tips-tricks/
 116. https://theclevermachine.wordpress.com/category/uncategorized/
 117. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
 118. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 119. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
 120. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
 121. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
 122. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
 123. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
 124. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
 125. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
 126. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
 127. https://theclevermachine.wordpress.com/2014/09/
 128. https://theclevermachine.wordpress.com/2013/04/
 129. https://theclevermachine.wordpress.com/2013/03/
 130. https://theclevermachine.wordpress.com/2013/01/
 131. https://theclevermachine.wordpress.com/2012/12/
 132. https://theclevermachine.wordpress.com/2012/11/
 133. https://theclevermachine.wordpress.com/2012/10/
 134. https://theclevermachine.wordpress.com/2012/09/
 135. https://theclevermachine.wordpress.com/2012/03/
 136. https://theclevermachine.wordpress.com/2012/02/
 137. https://theclevermachine.wordpress.com/2012/01/
 138. https://wordpress.com/start?ref=wplogin
 139. https://theclevermachine.wordpress.com/wp-login.php
 140. https://theclevermachine.wordpress.com/feed/
 141. https://theclevermachine.wordpress.com/comments/feed/
 142. https://wordpress.com/
 143. https://wordpress.com/?ref=footer_blog
 144. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/
 145. https://automattic.com/cookies
 146. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 148. https://theclevermachine.files.wordpress.com/2012/11/hamiltoniandynamics.gif
 149. https://theclevermachine.files.wordpress.com/2012/11/hmcgaussiansamples.png
 150. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-form-guest
 151. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-form-load-service:wordpress.com
 152. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-form-load-service:twitter
 153. https://theclevermachine.wordpress.com/2012/11/18/mcmc-hamiltonian-monte-carlo-a-k-a-hybrid-monte-carlo/#comment-form-load-service:facebook
