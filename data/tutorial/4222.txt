   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    monte carlo approximations comments feed
   [4]sampling from the normal distribution using the box-muller transform
   [5]a brief introduction to markov chains [6]alternate [7]alternate
   [8]the clever machine [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    sampling from the normal distribution using the
   box-muller transform
   [21]a brief introduction to markov chains    

monte carlo approximations

   [22]sep 22

   posted by [23]dustinstansbury

monte carlo approximation for integration

   using statistical methods we often run into integrals that take the
   form:

   i = \int_a^b h(x)g(x) dx

   for instance, the expected value of a some function f(x) of a random
   variable x

   \mathbb e[x] = \int_{p(x)} p(x) f(x) dx

   and many quantities essential for bayesian methods such as the marginal
   likelihood a.k.a    model evidence   

   p(x) = \int_\theta p(x | \theta) p(\theta) dx

   involve integrals of this form.  sometimes (not often) such an integral
   can be evaluated analytically. when a closed form solution does not
   exist, [24]numeric integration methods can be applied. however
   numerical methods quickly become intractable for any practical
   application that requires more than a small number of dimensions. this
   is where monte carlo approximation comes in. monte carlo approximation
   allows us to calculate an estimate for the value of i by transforming
   the  integration problem into a procedure of sampling values from a
   tractable id203 distribution and calculating the average of those
   samples. here   s what i mean:

   if the function g(x) fullfills two simple criteria, namely that the
   function is always positive on the interval (a,b)

   g(x) \geq 0, x \in (a,b)

   and that the integral of the function is finite

   \int_a^b g(x) = c < \infty

   then we can define a corresponding id203 distribution on the
   interval (a,b) :

   p(x) = \frac{g(x)}{c}

   another way to think of it is that g(x) is a id203 distribution
   scaled by a constant c .

   using this link between id203 distributions p(x) and g(x) , we
   can restate the original integration as

   i = c \int_a^b h(x) p(x)dx = c \mathbb e_{p(x)}[h(x)]

   this statement says that if we can sample values of x using p(x) , then
   the value of  the original integral i is simply a scaled version of the
   expected value of the integrand function h(x) calculated using those
   samples. turns out that the expected value \mathbb e_{p(x)}[h(x)] can
   be easily approximated by the sample mean:

   \mathbb e_{p(x)} [h(x)] \approx \frac{1}{n} \sum_i^n h(x_i)

   where samples x_i, i = 1..n are drawn independently from p(x) . this
   leads to a simple 4-step procedure for performing monte carlo
   approximation to the integral i :
    1. identify h(x)
    2. identify g(x) and from it determine p(x) and c
    3. draw n independent samples from p(x)
    4. evaluate i = c \mathbb e[h(x)] \approx \frac{c}{n} \sum_i^n h(x_i)

   the larger the number of samples n we draw, the better our
   approximation to the actual value of i . this 4-step procedure is
   demonstrated in a some toy examples below:

example 1: approximating the integral xe^x

   say we want to calculate the integral:

   i = \int_0^1 x e^x dx

   we can calculate the closed form solution of this integral using
   integration by parts:

   u = x, dv = e^x

   du = dx, v = e^x

   and

   i = uv - \int v du

   = xe^x - \int e^x dx

   = \left. xe^x - e^x \right|_0^1

   = \left. e^x(x-1)\right|_0^1

   = 0 - (-1) = 1

   orr   we could calculate the  monte carlo approximation of this integral.

   step 1 we identify

   h(x) = xe^x

   step 2 we identify

   g(x) = 1

   and from this can also determine the id203 distribution function
   p(x) \in (a,b) = (0,1) . according to the definition expression for
   p(x) given above we detemine p(x) to be:

   p(x) = \frac{g(x)}{\int_a^b g(x)dx} = \frac{1}{b-a} .

   step 3: the expression on the right is the definition for the uniform
   distribution unif(0,1) , which is easy to sample from using the matlab
   \text{rand()} (notice too that the constant c = b-a = 1 ).

   step 4: we calculate the monte carlo approximation as

   i = c \mathbb e_{p(x)} h(x)

   \approx \frac{1}{n}\sum_{i=1}^n x_i e^{x_i} ,

   where each x_i is sampled from the standard uniform distribution. below
   is some matlab code running the monte carlo approximation for two
   different values of n
% monte carlo approximation of int(xexp(x))dx
% for two different sample sizes
rand('seed',12345);

% the first approximation using n1 = 100 samples
n1 = 100;
x = rand(n1,1);
ihat1 = sum(x.*exp(x))/n1

% a second approximation using n2 = 5000 samples
n2 = 5000;
x = rand(n2,1);
ihat2 = sum(x.*exp(x))/n2

   comparing the values of the variables \text{ihat1} and \text{ihat2} we
   see that the monte carlo approximation is better for a larger number of
   samples.

example 2: approximating the expected value of the beta distribution

   lets look at how the 4-step monte carlo approximation procedure can be
   used to calculate expectations. in this example we will calculate

   \mathbb e[x] = \int_{p(x)} p(x)x dx ,

   where x \sim p(x) = beta(\alpha_1,\alpha_2) .

   step 1: we identify h(x) = x .

   step 2: the function g(x) is simply the id203 density function
   p(x) due the expression for p(x) above:

   p(x)^\star = \frac{p(x)}{\int p(x)dx} = p(x) .

   step 3 we can use matlab to easily draw n independent samples p(x)
   using the function \text{betarnd()} . and finally,

   step 4 we approximate the expectation with the expression

   \mathbb e[x]_{beta(\alpha_1,\alpha_2)} \approx \frac{1}{n} \sum_i x_i

   below is some matlab code that performs this approximation of the
   expected value.
rand('seed',12345);
alpha1 = 2; alpha2 = 10;
n = 10000;
x = betarnd(alpha1,alpha2,1,n);

% monte carlo expectation
expectmc = mean(x);

% analytic expression for beta mean
expectanalytic = alpha1/(alpha1 + alpha2);

% display
figure;
bins = linspace(0,1,50);
counts = histc(x,bins);
probsampled = counts/sum(counts);
probtheory = betapdf(bins,alpha1,alpha2);
b = bar(bins,probsampled); colormap hot; hold on;
t = plot(bins,probtheory/sum(probtheory),'r','linewidth',2)
m = plot([expectmc,expectmc],[0 .1],'g')
e = plot([expectanalytic,expectanalytic],[0 .1],'b')
xlim([expectanalytic - .05,expectanalytic + 0.05])
legend([b,t,m,e],{'samples','theory','$\hat{e}$','$e_{theory}$'},'interpreter','
latex');
title(['e_{mc} = ',num2str(expectmc),'; e_{theory} = ',num2str(expectanalytic)])
hold off

   and the output of the code:

   monte carlo approximation of the the expected value of beta(2,10)

   the[25] analytical solution for the expected value of this beta
   distribution:

   \mathbb e_{beta(2,10)}[x] = \frac{\alpha_1}{\alpha_1 + \alpha_2} =
   \frac{2}{12} = 0.167

   is quite close to our approximation (also indicated by the small
   distance between the blue and green lines on the plot above).

monte carlo approximation for optimization

   monte carlo approximation can also be used to solve optimization
   problems of the form:

   \hat x = \underset{x \in (a,b)}{\text{argmax }} g(x)

   if g(x) fulfills the same criteria described above (namely that it is a
   scaled version of a id203 distribution), then (as above) we can
   define the id203 function

   p(x) = \frac{g(x)}{c}

   this allows us to instead solve the problem

   \hat x = \underset{x}{\text{argmax }}c p(x)

   if we can sample from p(x) , the solution \hat x is easily found by
   drawing samples from p(x) and determining the location of the samples
   that has the highest density (note that the solution is not dependent
   of the value of c ). the following example demonstrates monte carlo
   optimization:

example: monte carlo optimization of g(x) = e^{-\frac{(x-4)^2}{2}}

   say we would like to find the value of x_{opt} which optimizes the
   function  g(x) = e^{-((x-4)^2)/2} . in other words we want to solve the
   problem

   x_{opt} = \underset{x}{\text{argmax }} e^{-\frac{(x-4)^2}{2}}

   we could solve for x_{opt} using standard calculus methods, but a
   clever trick is to use monte carlo approximation to solve the problem.
   first, notice that g(x)  is  a scaled version of a normal distribution
   with mean equal to 4 and unit variance:

   g(x) = c \times \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-4)^2}{2}}

   = c \times \mathcal n(4,1)

   where c = \sqrt{2\pi} . this means we can solve for x_{opt} by drawing
   samples from the normal distribution and determining where those
   samples have the highest density. the following chunk of matlab code
   solves the optimization problem in this way.
% monte carlo optimization of exp(x-4)^2
randn('seed',12345)

% initialzie
n = 100000;
x = 0:.1:6;
c = sqrt(2*pi);
g = inline('exp(-.5*(x-4).^2)','x');
ph = plot(x,g(x)/c,'r','linewidth',3); hold on
gh = plot(x,g(x),'b','linewidth',2); hold on;
y = normpdf(x,4,1);

% calculate monte carlo approximation
x = normrnd(4,1,1,n);
bins = linspace(min(x),max(x),100);
counts = histc(x,bins);
[~,optidx] = max(counts);
xhat = bins(optidx);

% optima and estimated optima
oh = plot([4 4],[0,1],'k');
hh = plot([xhat,xhat],[0,1],'g');

set(gca,'fontsize',16)
legend([gh,ph,oh,hh],{'g(x)','$p(x)=\frac{g(x)}{c}$','$x_{opt}$','$\hat{x}$'},'i
nterpreter','latex','location','northwest');

   monte carlo optimization

   in the code output above we see the function g(x) we want to optimize
   in blue and the normal distribution p(x) from which we draw samples in
   red. the monte carlo method provides a good approximation (green) to
   the real solution (black).

wrapping up

   in the toy examples above it was easy to sample from p(x) . however,
   for practical problems the distributions we want to sample from are
   often complex and operate in many dimensions. for these problems more
   clever sampling methods have to be used. such sampling methods include
   inverse transform sampling, rejection sampling, importance sampling,
   and id115 methods such as the metropolis hasting
   algorithm and the gibbs sampler, each of which i plan to cover in
   separate posts.
   advertisements

share this:

     * [26]twitter
     * [27]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [28]view all posts by dustinstansbury   

   posted on september 22, 2012, in [29]sampling methods,
   [30]uncategorized and tagged [31]monte carlo approximation, [32]monte
   carlo integration, [33]monte carlo optimization. bookmark the
   [34]permalink. [35]4 comments.
   [36]    sampling from the normal distribution using the
   box-muller transform
   [37]a brief introduction to markov chains    
     * [38]leave a comment
     * [39]trackbacks 2
     * [40]comments 2

    1. [41]bendichter | [42]november 9, 2012 at 2:31 pm
       dustin,
       nice job! this was helpful. i have two comments:
       1) i think it would help build intuition for how this algorithm
       works if you show the histc results. it would also give a better
       sense of how reliable this algorithm is. you could do this with one
       line: bar(bins,counts,   hist   ); or plot(bins,counts);
       2) it would be nice to see a more complex example where one might
       actually use the monte carlo approximation.
       [43]reply
    2. [44]giasxoleio | [45]september 12, 2018 at 8:38 am
       great job!! please simply continue
       [46]reply

    1. pingback: [47]a gentle introduction to id115
       (mcmc)    the clever machine
    2. pingback: [48]background research in attempts to follow a talk on
          quantitative evaluation of deep generative models    by
       salakhutdinov     the joy of pontificating

leave a reply [49]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [50]googleplus-sign-in

     *
     *

   [51]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [52]log out /
   [53]change )
   google photo

   you are commenting using your google account. ( [54]log out /
   [55]change )
   twitter picture

   you are commenting using your twitter account. ( [56]log out /
   [57]change )
   facebook photo

   you are commenting using your facebook account. ( [58]log out /
   [59]change )
   [60]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [61]algorithms [62]classification [63]id174
       [64]density estimation [65]derivations [66]id171
       [67]fmri [68]id119 [69]latex [70]machine learning
       [71]matlab [72]maximum likelihood [73]mcmc [74]neural networks
       [75]neuroscience [76]optimization [77]proofs [78]regression
       [79]sampling [80]sampling methods [81]simulations [82]statistics
       [83]theory [84]tips & tricks [85]uncategorized
     * recent posts
          + [86]derivation: maximum likelihood for id82s
          + [87]a gentle introduction to id158s
          + [88]derivation: derivatives for common neural network
            id180
          + [89]derivation: error id26 & id119 for
            neural networks
          + [90]model selection: underfitting, overfitting, and the
            id160
          + [91]supplemental proof 1
          + [92]the statistical whitening transform
          + [93]covariance matrices and data distributions
          + [94]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [95]derivation: the covariance matrix of an ols estimator (and
            applications to gls)
     * archives
          + [96]september 2014
          + [97]april 2013
          + [98]march 2013
          + [99]january 2013
          + [100]december 2012
          + [101]november 2012
          + [102]october 2012
          + [103]september 2012
          + [104]march 2012
          + [105]february 2012
          + [106]january 2012
     * meta
          + [107]register
          + [108]log in
          + [109]entries rss
          + [110]comments rss
          + [111]wordpress.com
       advertisements

   [112]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [113]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [114]cookie policy

   iframe: [115]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/feed/
   4. https://theclevermachine.wordpress.com/2012/09/11/sampling-from-the-normal-distribution-using-the-box-muller-transform/
   5. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#access
  11. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#main
  12. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#sidebar
  13. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#sidebar2
  14. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2012/09/11/sampling-from-the-normal-distribution-using-the-box-muller-transform/
  21. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
  22. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. http://en.wikipedia.org/wiki/numerical_integration
  25. http://en.wikipedia.org/wiki/beta_distribution
  26. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/?share=twitter
  27. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/?share=facebook
  28. https://theclevermachine.wordpress.com/author/dustinstansbury/
  29. https://theclevermachine.wordpress.com/category/sampling-methods/
  30. https://theclevermachine.wordpress.com/category/uncategorized/
  31. https://theclevermachine.wordpress.com/tag/monte-carlo-approximation/
  32. https://theclevermachine.wordpress.com/tag/monte-carlo-integration/
  33. https://theclevermachine.wordpress.com/tag/monte-carlo-optimization/
  34. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/
  35. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comments
  36. https://theclevermachine.wordpress.com/2012/09/11/sampling-from-the-normal-distribution-using-the-box-muller-transform/
  37. https://theclevermachine.wordpress.com/2012/09/24/a-brief-introduction-to-markov-chains/
  38. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#respond
  39. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#trackbacks
  40. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comments
  41. http://bendichter.wordpress.com/
  42. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comment-17
  43. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/?replytocom=17#respond
  44. http://ireneyogatraining.wordpress.com/
  45. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comment-1541
  46. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/?replytocom=1541#respond
  47. https://theclevermachine.wordpress.com/2012/11/19/a-gentle-introduction-to-markov-chain-monte-carlo-mcmc/
  48. https://thejoyofpontificating.wordpress.com/2017/03/21/background-research-in-attempts-to-follow-a-talk-on-quantitative-evaluation-of-deep-generative-models-by-salakhutdinov/
  49. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#respond
  50. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  51. https://gravatar.com/site/signup/
  52. javascript:highlandercomments.doexternallogout( 'wordpress' );
  53. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/
  54. javascript:highlandercomments.doexternallogout( 'googleplus' );
  55. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/
  56. javascript:highlandercomments.doexternallogout( 'twitter' );
  57. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/
  58. javascript:highlandercomments.doexternallogout( 'facebook' );
  59. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/
  60. javascript:highlandercomments.cancelexternalwindow();
  61. https://theclevermachine.wordpress.com/category/algorithms/
  62. https://theclevermachine.wordpress.com/category/algorithms/classification/
  63. https://theclevermachine.wordpress.com/category/data-preprocessing/
  64. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  65. https://theclevermachine.wordpress.com/category/derivations/
  66. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
  67. https://theclevermachine.wordpress.com/category/fmri/
  68. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  69. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
  70. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  71. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
  72. https://theclevermachine.wordpress.com/category/maximum-likelihood/
  73. https://theclevermachine.wordpress.com/category/mcmc/
  74. https://theclevermachine.wordpress.com/category/neural-networks/
  75. https://theclevermachine.wordpress.com/category/neuroscience/
  76. https://theclevermachine.wordpress.com/category/optimization/
  77. https://theclevermachine.wordpress.com/category/proofs/
  78. https://theclevermachine.wordpress.com/category/algorithms/regression/
  79. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  80. https://theclevermachine.wordpress.com/category/sampling-methods/
  81. https://theclevermachine.wordpress.com/category/simulations/
  82. https://theclevermachine.wordpress.com/category/statistics/
  83. https://theclevermachine.wordpress.com/category/theory/
  84. https://theclevermachine.wordpress.com/category/tips-tricks/
  85. https://theclevermachine.wordpress.com/category/uncategorized/
  86. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
  87. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  88. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  89. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  90. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
  91. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
  92. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
  93. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
  94. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
  95. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
  96. https://theclevermachine.wordpress.com/2014/09/
  97. https://theclevermachine.wordpress.com/2013/04/
  98. https://theclevermachine.wordpress.com/2013/03/
  99. https://theclevermachine.wordpress.com/2013/01/
 100. https://theclevermachine.wordpress.com/2012/12/
 101. https://theclevermachine.wordpress.com/2012/11/
 102. https://theclevermachine.wordpress.com/2012/10/
 103. https://theclevermachine.wordpress.com/2012/09/
 104. https://theclevermachine.wordpress.com/2012/03/
 105. https://theclevermachine.wordpress.com/2012/02/
 106. https://theclevermachine.wordpress.com/2012/01/
 107. https://wordpress.com/start?ref=wplogin
 108. https://theclevermachine.wordpress.com/wp-login.php
 109. https://theclevermachine.wordpress.com/feed/
 110. https://theclevermachine.wordpress.com/comments/feed/
 111. https://wordpress.com/
 112. https://wordpress.com/?ref=footer_blog
 113. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/
 114. https://automattic.com/cookies
 115. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 117. https://theclevermachine.files.wordpress.com/2012/09/montecarloexpectation1.png
 118. https://theclevermachine.files.wordpress.com/2012/09/montecarlooptimization.png
 119. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comment-form-guest
 120. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comment-form-load-service:wordpress.com
 121. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comment-form-load-service:twitter
 122. https://theclevermachine.wordpress.com/2012/09/22/monte-carlo-approximations/#comment-form-load-service:facebook
