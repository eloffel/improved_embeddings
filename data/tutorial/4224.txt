   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    rejection sampling comments feed [4]inverse
   transform sampling [5]sampling from the normal distribution using the
   box-muller transform [6]alternate [7]alternate [8]the clever machine
   [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    inverse transform sampling
   [21]sampling from the normal distribution using the
   box-muller transform    

rejection sampling

   [22]sep 10

   posted by [23]dustinstansbury

   suppose that we want to sample from a distribution f(x) that is
   difficult or impossible to sample from directly, but instead have a
   simpler distribution q(x) from which sampling is easy.  the idea behind
   rejection sampling (aka acceptance-rejection sampling) is to sample
   from q(x) and apply some rejection/acceptance criterion such that the
   samples that are accepted are distributed according to f(x) .

envelope distribution and rejection criterion

   in order to be able to reject samples from q(x) such that they are
   sampled from f(x) , q(x) must    cover    or envelop the distribution f(x)
   . this is generally done by choosing a constant c > 1 such that   cq(x)
   > f(x) for all x . for this reason cq(x) is often called the envelope
   distribution. a common criterion for accepting samples from x \sim q(x)
   is based on the ratio of the target distribution to that of the
   envelope distribution. the samples are accepted if

   \frac{f(x)}{cq(x)} > u

   where u \sim unif(0,1) , and rejected otherwise. if the ratio is close
   to one, then f(x) must have a large amount of id203 mass around x
   and that sample should  be more likely accepted. if the ratio is small,
   then it means that f(x) has low id203 mass around x and we should
   be less likely to accept the sample. this criterion is demonstrated in
   the chunk of matlab code and the resulting figure below:
rand('seed',12345);
x = -10:.1:10;
% create a "complex distribution" f(x) as a mixture of two normal
% distributions
f = inline('normpdf(x,3,2) + normpdf(x,-5,1)','x');
t = plot(x,f(x),'b','linewidth',2); hold on;

% proposal is a centered normal distribution
q = inline('normpdf(x,0,4)','x');

% determine scaling constant
c = max(f(x)./q(x))

%plot scaled proposal/envelop distribution
p = plot(x,c*q(x),'k--');

% draw a sample from q(x);
qx = normrnd(0,4);
fx = f(qx);

% plot the ratio of f(q(x)) to cq(x)
a = plot([qx,qx],[0 fx],'g','linewidth',2);
r = plot([qx,qx],[fx,c*q(qx)],'r','linewidth',2);
legend([t,p,a,r],{'target','proposal','accept','reject'});
xlabel('x');

   rejection sampling with a normal proposal distribution

   here a zero-mean normal distribution is used as the proposal
   distribution. this distribution is scaled by a factor c = 9.2 ,
   determined from f(x) and q(x) to ensure that the proposal distribution
   covers f(x) . we then sample from q(x) , and compare the proportion of
   cq(x) occupied by f(x) . if we compare this proportion to a random
   number sampled  from unif(0,1) (i.e. the criterion outlined above),
   then we would accept this sample with id203 proportional to the
   length of the green line segment and reject the sample with id203
   proportional to the length of the red line segment.

rejection sampling of a random discrete distribution

   this next example shows how rejection sampling can be used to sample
   from any arbitrary distribution, continuous or not, and with or without
   an analytic id203 density function.

   random discrete target distribution and proposal that bounds it.

   the figure above shows a random discrete id203 density function
   f(x) generated on the interval (0,15). we will use rejection sampling
   as described above to sample from f(x) . our proposal/envelope
   distribution is the uniform discrete distribution on the same interval
   (i.e. any of the integers from 1-15 are equally probable) multiplied by
   a constant c that is determined such that the maximum value of f(x)
   lies under (or equal to) cq(x) .

   rejection samples for discrete distribution on interval [1 15]
   plotted above is the target distribution (in red) along with the
   discrete samples obtained using the rejection sampling. the matlab code
   used to sample from the target distribution and display the plot above
   is here:
rand('seed',12345)
randn('seed',12345)

flength = 15;
% create a random distribution on the interval [1 flength]
f = rand(1,flength); f = f/sum(f);

figure; h = plot(f,'r','linewidth',2);
hold on;
l = plot([1 flength],[max(f) max(f)],'k','linewidth',2);

legend([h,l],{'f(x)','q(x)'},'location','southwest');
xlim([0 flength + 1])
xlabel('x');
ylabel('p(x)');
title('target (f(x)) and proposal (q(x)) distributions');

% our proposal is the discrete uniform on the interval [1 flength]
% so our constant is
c = max(f/(1/flength));

nsamples = 10000;
i = 1;
while i < nsamples
   proposal = unidrnd(flength);
   q = c*1/flength; % envelope distribution
   if rand < f(proposal)/q
      samps(i) = proposal;
      i = i + 1;
   end
end

% display the samples and compare to the target distribution
bins = 1:flength;
counts = histc(samps,bins);
figure
b = bar(1:flength,counts/sum(counts),'facecolor',[.8 .8 .8])
hold on;
h = plot(f,'r','linewidth',2)
legend([h,b],{'f(x)','samples'});
xlabel('x'); ylabel('p(x)');
xlim([0 flength + 1]);

rejection sampling from the unit circle to estimate \pi

   though the ratio-based acceptance-rejection criterion introduced above
   is a common choice for drawing samples from complex distributions, it
   is not the only criterion we could use. for instance we could use a
   different set of criteria to generate some geometrically-bounded
   distribution. if we wanted to generate points uniformly within the unit
   circle (i.e. a circle centered at (y,x) = 0 and with radius r = 1 ), we
   could do so by sampling cartesian spatial coordinates x and y uniformly
   from the interval (-1,1)   which samples form a square centered at
   (0,0)   and reject those points that lie outside of the radius r =
   \sqrt{x^2 + y^2} = 1

   unit circle inscribed in square
   something clever that we can do with such a set of samples is to
   approximate the value \pi : because a square that inscribes the unit
   circle has area:

   a_{square} = (2r)^2 = 4r^2

   and the unit circle has the area:

   a_{circle} = \pi r^2

   we can use the ratio of their areas to approximate \pi :

   \pi = 4\frac{a_{circle}}{a_{square}}

   the figure below shows the rejection sampling process and the resulting
   estimate of \pi from the samples. one-hundred thousand 2d points are
   sampled uniformly from the interval (-1,1).  those points that lie
   within the unit circle are plotted as blue dots. those points that lie
   outside of the unit circle are plotted as red x   s. if we take four
   times the ratio of the area in blue to the entire area, we get a very
   close approximation to 3.14 for \pi .

   rejection criterion

   the matlab code used to generate the example figures is below:
% display a circle inscribed in a square

figure;
a = 0:.01:2*pi;
x = cos(a); y = sin(a);
hold on
plot(x,y,'k','linewidth',2)

t = text(0.5, 0.05,'r');
l = line([0 1],[0 0],'linewidth',2);
axis equal
box on
xlim([-1 1])
ylim([-1 1])
title('unit circle inscribed in a square')

pause;
rand('seed',12345)
randn('seed',12345)
delete(l); delete(t);

% draw samples from proposal distribution
samples = 2*rand(2,100000) - 1;

% rejection
reject = sum(samples.^2) > 1;

% display rejection criterion
scatter(samples(1,~reject),samples(2,~reject),'b.')
scatter(samples(1,reject),samples(2,reject),'rx')
hold off
xlim([-1 1])
ylim([-1 1])

pihat = mean(sum(samples.*samples)<1)*4;

title(['estimate of \pi = ',num2str(pihat)]);

wrapping up

   rejection sampling is a simple way to generate samples from complex
   distributions. however, rejection sampling also has a number of
   weaknesses:
     * finding a proposal distribution that can cover the support of the
       target distribution is a non-trivial task.
     * additionally, as the dimensionality of the target distribution
       increases, the proportion of points that are rejected also
       increases. this curse of dimensionality makes rejection sampling an
       inefficient technique for sampling multi-dimensional distributions,
       as the majority of the points proposed are not accepted as valid
       samples.
     * some of these problems are solved by changing the form of the
       proposal distribution to    hug    the target distribution as we gain
       knowledge of the target from observing accepted samples. such a
       process is called adaptive rejection sampling, which will be
       covered in another post.

   advertisements

share this:

     * [24]twitter
     * [25]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [26]view all posts by dustinstansbury   

   posted on september 10, 2012, in [27]density estimation, [28]sampling
   methods, [29]statistics and tagged [30]curse of dimensionality,
   [31]envelope distribution, [32]proposal distribution, [33]rejection
   sampling, [34]sampling methods, [35]target distribution. bookmark the
   [36]permalink. [37]4 comments.
   [38]    inverse transform sampling
   [39]sampling from the normal distribution using the
   box-muller transform    
     * [40]leave a comment
     * [41]trackbacks 1
     * [42]comments 3

    1. leo | [43]june 30, 2014 at 7:19 pm
       example 2, line 26
       q = c*/flenth; % envelope distribution
       should be
       q = c*1/flength; % envelope distribution
       [44]reply
          + [45]dustinstansbury | [46]september 19, 2014 at 3:33 pm
            thanks leo! the code has been corrected.
            [47]reply
    2. albert yao | [48]october 2, 2014 at 5:06 pm
       it   s so clear explained. thanks for ur sharing !
       [49]reply

    1. pingback: [50]bayes    theorem primer     connections

leave a reply [51]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [52]googleplus-sign-in

     *
     *

   [53]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [54]log out /
   [55]change )
   google photo

   you are commenting using your google account. ( [56]log out /
   [57]change )
   twitter picture

   you are commenting using your twitter account. ( [58]log out /
   [59]change )
   facebook photo

   you are commenting using your facebook account. ( [60]log out /
   [61]change )
   [62]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [63]algorithms [64]classification [65]id174
       [66]density estimation [67]derivations [68]id171
       [69]fmri [70]id119 [71]latex [72]machine learning
       [73]matlab [74]maximum likelihood [75]mcmc [76]neural networks
       [77]neuroscience [78]optimization [79]proofs [80]regression
       [81]sampling [82]sampling methods [83]simulations [84]statistics
       [85]theory [86]tips & tricks [87]uncategorized
     * recent posts
          + [88]derivation: maximum likelihood for id82s
          + [89]a gentle introduction to id158s
          + [90]derivation: derivatives for common neural network
            id180
          + [91]derivation: error id26 & id119 for
            neural networks
          + [92]model selection: underfitting, overfitting, and the
            id160
          + [93]supplemental proof 1
          + [94]the statistical whitening transform
          + [95]covariance matrices and data distributions
          + [96]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [97]derivation: the covariance matrix of an ols estimator (and
            applications to gls)
     * archives
          + [98]september 2014
          + [99]april 2013
          + [100]march 2013
          + [101]january 2013
          + [102]december 2012
          + [103]november 2012
          + [104]october 2012
          + [105]september 2012
          + [106]march 2012
          + [107]february 2012
          + [108]january 2012
     * meta
          + [109]register
          + [110]log in
          + [111]entries rss
          + [112]comments rss
          + [113]wordpress.com
       advertisements

   [114]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [115]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [116]cookie policy

   iframe: [117]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/feed/
   4. https://theclevermachine.wordpress.com/2012/09/09/inverse-transform-sampling/
   5. https://theclevermachine.wordpress.com/2012/09/11/sampling-from-the-normal-distribution-using-the-box-muller-transform/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#access
  11. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#main
  12. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#sidebar
  13. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#sidebar2
  14. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2012/09/09/inverse-transform-sampling/
  21. https://theclevermachine.wordpress.com/2012/09/11/sampling-from-the-normal-distribution-using-the-box-muller-transform/
  22. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/?share=twitter
  25. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/?share=facebook
  26. https://theclevermachine.wordpress.com/author/dustinstansbury/
  27. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  28. https://theclevermachine.wordpress.com/category/sampling-methods/
  29. https://theclevermachine.wordpress.com/category/statistics/
  30. https://theclevermachine.wordpress.com/tag/curse-of-dimensionality/
  31. https://theclevermachine.wordpress.com/tag/envelope-distribution/
  32. https://theclevermachine.wordpress.com/tag/proposal-distribution/
  33. https://theclevermachine.wordpress.com/tag/rejection-sampling/
  34. https://theclevermachine.wordpress.com/tag/sampling-methods/
  35. https://theclevermachine.wordpress.com/tag/target-distribution/
  36. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/
  37. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comments
  38. https://theclevermachine.wordpress.com/2012/09/09/inverse-transform-sampling/
  39. https://theclevermachine.wordpress.com/2012/09/11/sampling-from-the-normal-distribution-using-the-box-muller-transform/
  40. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#respond
  41. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#trackbacks
  42. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comments
  43. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comment-159
  44. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/?replytocom=159#respond
  45. http://machinelearnings.wordpress.com/
  46. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comment-193
  47. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/?replytocom=193#respond
  48. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comment-211
  49. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/?replytocom=211#respond
  50. https://plnotes.wordpress.com/2017/09/29/bayes-theorem-primer/
  51. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#respond
  52. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  53. https://gravatar.com/site/signup/
  54. javascript:highlandercomments.doexternallogout( 'wordpress' );
  55. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/
  56. javascript:highlandercomments.doexternallogout( 'googleplus' );
  57. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/
  58. javascript:highlandercomments.doexternallogout( 'twitter' );
  59. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/
  60. javascript:highlandercomments.doexternallogout( 'facebook' );
  61. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/
  62. javascript:highlandercomments.cancelexternalwindow();
  63. https://theclevermachine.wordpress.com/category/algorithms/
  64. https://theclevermachine.wordpress.com/category/algorithms/classification/
  65. https://theclevermachine.wordpress.com/category/data-preprocessing/
  66. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  67. https://theclevermachine.wordpress.com/category/derivations/
  68. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
  69. https://theclevermachine.wordpress.com/category/fmri/
  70. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  71. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
  72. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  73. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
  74. https://theclevermachine.wordpress.com/category/maximum-likelihood/
  75. https://theclevermachine.wordpress.com/category/mcmc/
  76. https://theclevermachine.wordpress.com/category/neural-networks/
  77. https://theclevermachine.wordpress.com/category/neuroscience/
  78. https://theclevermachine.wordpress.com/category/optimization/
  79. https://theclevermachine.wordpress.com/category/proofs/
  80. https://theclevermachine.wordpress.com/category/algorithms/regression/
  81. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  82. https://theclevermachine.wordpress.com/category/sampling-methods/
  83. https://theclevermachine.wordpress.com/category/simulations/
  84. https://theclevermachine.wordpress.com/category/statistics/
  85. https://theclevermachine.wordpress.com/category/theory/
  86. https://theclevermachine.wordpress.com/category/tips-tricks/
  87. https://theclevermachine.wordpress.com/category/uncategorized/
  88. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
  89. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  90. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  91. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  92. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
  93. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
  94. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
  95. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
  96. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
  97. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
  98. https://theclevermachine.wordpress.com/2014/09/
  99. https://theclevermachine.wordpress.com/2013/04/
 100. https://theclevermachine.wordpress.com/2013/03/
 101. https://theclevermachine.wordpress.com/2013/01/
 102. https://theclevermachine.wordpress.com/2012/12/
 103. https://theclevermachine.wordpress.com/2012/11/
 104. https://theclevermachine.wordpress.com/2012/10/
 105. https://theclevermachine.wordpress.com/2012/09/
 106. https://theclevermachine.wordpress.com/2012/03/
 107. https://theclevermachine.wordpress.com/2012/02/
 108. https://theclevermachine.wordpress.com/2012/01/
 109. https://wordpress.com/start?ref=wplogin
 110. https://theclevermachine.wordpress.com/wp-login.php
 111. https://theclevermachine.wordpress.com/feed/
 112. https://theclevermachine.wordpress.com/comments/feed/
 113. https://wordpress.com/
 114. https://wordpress.com/?ref=footer_website
 115. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/
 116. https://automattic.com/cookies
 117. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 119. https://theclevermachine.files.wordpress.com/2012/09/rejectionsamplingcriterion.png
 120. https://theclevermachine.files.wordpress.com/2012/09/rejectionsamplingtargetproposal.png
 121. https://theclevermachine.files.wordpress.com/2012/09/rejectionsamplingdiscrete.png
 122. https://theclevermachine.files.wordpress.com/2012/09/unitcircleinsquare2.png
 123. https://theclevermachine.files.wordpress.com/2012/09/rejectionsamplingpi.png
 124. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comment-form-guest
 125. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comment-form-load-service:wordpress.com
 126. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comment-form-load-service:twitter
 127. https://theclevermachine.wordpress.com/2012/09/10/rejection-sampling/#comment-form-load-service:facebook
