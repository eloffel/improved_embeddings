   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    a gentle introduction to artificial neural
   networks comments feed [4]derivation: derivatives for common neural
   network id180 [5]derivation: maximum likelihood for
   id82s [6]alternate [7]alternate [8]the clever machine
   [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    derivation: derivatives for common neural network
   id180
   [21]derivation: maximum likelihood for id82s    

a gentle introduction to id158s

   [22]sep 11

   posted by [23]dustinstansbury

introduction

   though many phenomena in the world can be adequately modeled using
   id75 or classification, most interesting phenomena are
   generally nonlinear in nature. in order to deal with nonlinear
   phenomena, there have been a diversity of nonlinear models developed.
   for example parametric models assume that data follow some parameteric
   class of nonlinear function (e.g. polynomial, power, or exponential),
   then fine-tune the shape of the parametric function to fit observed
   data. however this approach is only helpful if data are fit nicely
   by the available catalog of parametric functions. another approach,
   kernel-based methods, transforms data non-linearly into an abstract
   space that measures distances between observations, then predicts new
   values or classes based on these distances. however, kernel methods
   generally involve constructing a kernel matrix that depends on the
   number of training observations and can thus be prohibitive for large
   data sets. another class of models, the ones that are the focus of this
   post, are id158s (anns). anns are nonlinear models
   motivated by the physiological architecture of the nervous system. they
   involve a cascade of simple nonlinear computations that when aggregated
   can implement robust and complex nonlinear functions. in fact,
   depending on how they are constructed, anns can approximate any
   nonlinear function, making them a quite powerful class of models (note
   that this property is not reserved for anns; kernel methods are also
   considered    universal approximators   ; however, it turns out that neural
   networks with multiple layers are more efficient at approximating
   arbitrary functions than other methods. i refer the interested reader
   to [24]more in-depth discussion on the topic.).

   in recent years anns that use multiple stages of nonlinear computation
   (aka    deep learning   )  have been able obtain outstanding performance on
   an array of complex tasks ranging from visual object recognition to
   natural language processing. i find anns super interesting due to their
   computational power and their intersection with
   computational neuroscience.  however, i   ve found that most of the
   available tutorials on anns are either dense with formal details and
   contain little information about implementation or any examples, while
   others skip a lot of the mathematical detail and provide
   implementations that seem to come from thin air.  this post aims at
   giving a more complete overview of anns, including (varying degrees of)
   the math behind anns, how anns are implemented in code, and finally
   some toy examples that point out the strengths and weaknesses of anns.

single-layer neural networks

   the simplest ann (figure 1) takes a set of observed inputs
   \mathbf{a}=(a_1, a_2, ..., a_n) , multiplies each of them by their own
   associated weight \mathbf{w} = (w_1, w_2, ...w_n) , and sums the
   weighted values to form a pre-activation z .oftentimes there is also a
   bias b that is tied to an input that is always +1 included in the
   preactivation calculation. the network then transforms the
   pre-activation using a nonlinear activation function g(z) to output a
   final activation a_{\text{out}} .
   [25]figure 1: diagram of a single-layered id158.

   figure 1: diagram of a single-layered id158.

   there are many options available for the form of the activation
   function g(z) , and the choice generally depends on the task we would
   like the network to perform. for instance, if the activation function
   is the identity function:

   \large{\begin{array}{rcl}g_{\text{linear}}(z) = z\end{array}} ,

   which outputs continuous values a_{linear}\in (-\infty, \infty) , then
   the network implements a linear model akin to used in standard linear
   regression. another choice for the activation function is the logistic
   sigmoid:

   \large{ \begin{array}{rcl}g_{\text{logistic}}(z) =
   \frac{1}{1+e^{-z}}\end{array}} ,

   which outputs values  a_{logistic} \in (0,1) . when the network outputs
   use the logistic sigmoid activation function, the network
   implements linear binary classification. binary classification can also
   be implemented using the hyperbolic tangent function,   \text{tanh}(z)
   , which outputs values a_{\text{tanh}}\in (-1, 1) (note that the
   classes must also be coded as either -1 or 1 when using  \text{tanh} .
   single-layered neural networks used for classification are often
   referred to as    id88s,    a name given to them when they were first
   developed in the late 1950s.
   [26]figure 2: common id180 functions used in artificial
   neural, along with their derivatives

   figure 2: common id180 functions used in artificial
   neural, along with their derivatives

   to get a better idea of what these activation function do, their
   outputs for a given range of input values are plotted in the left of
   figure 2. we see that the logistic and tanh id180 (blue
   and green) have the quintessential sigmoidal    s    shape that saturates
   for inputs of large magnitude. this behavior makes them useful for
   categorization. the identity / linear activation (red), however forms a
   linear mapping between the input to the activation function, which
   makes it useful for predicting continuous values.

   a key property of these id180 is that they are all
   smooth and differentiable. we   ll see later in this post why
   differentiability is important for training neural networks. the
   derivatives for each of these common id180 are given
   by (for mathematical details on calculating these derivatives,  see
   [27]this post):

   \large{\begin{array}{rcl} g'_{\text{linear}}(z) &=& 1 \\
   g'_{\text{logistic}}(z) &=& g_{\text{logistic}}(z)(1-
   g_{\text{logistic}}(z)) \\ g'_{\text{tanh}}(z) &=& 1 -
   g_{\text{tanh}}^2(z) \\ \end{array}}

   each of the derivatives are plotted in the right of figure 2. what is
   interesting about these derivatives is that they are either a constant
   (i.e. 1), or are can be defined in terms of the original function. this
   makes them extremely convenient for efficiently training neural
   networks, as we can implement the gradient using simple manipulations
   of the feed-forward states of the network.

   code block 1: defines standard id180 and generates
   figure 2:
% define a few common id180
glinear = inline('z','z');
gsigmoid = inline('1./(1+exp(-z))','z');
gtanh = inline('tanh(z)','z');

% ...define their derivatives
gprimelinear = inline('ones(size(z))','z');
gprimesigmoid = inline('1./(1+exp(-z)).*(1-1./(1+exp(-z)))','z');
gprimetanh = inline('1-tanh(z).^2','z');

% visualize each g(z)
z = linspace(-4,4,100);
figure
set(gcf,'position',[100,100,960,420])
subplot(121);  hold on;
h(1) = plot(z,glinear(z),'r','linewidth',2);
h(2) = plot(z,gsigmoid(z),'b','linewidth',2);
h(3) = plot(z,gtanh(z),'g','linewidth',2);
set(gca,'fontsize',16)
xlabel('z')
legend(h,{'g_{linear}(z)','g_{logistic}(z)','g_{tanh}(z)'},'location','southeast
')
title('some common id180')
hold off, axis square, grid
ylim([-1.1 1.1])

% visualize each g'(z)
subplot(122); hold on
h(1) = plot(z,gprimelinear(z),'r','linewidth',2);
h(2) = plot(z,gprimesigmoid(z),'b','linewidth',2);
h(3) = plot(z,gprimetanh(z),'g','linewidth',2);
set(gca,'fontsize',16)
xlabel('z')
legend(h,{'g''_{linear}(z)','g''_{logistic}(z)','g''_{tanh}(z)'},'location','sou
th')
title('activation function derivatives')
hold off, axis square, grid
ylim([-.5 1.1])


multi-layer neural networks

   as was mentioned above, single-layered networks implement linear
   models, which doesn   t really help us if we want to model nonlinear
   phenomena. however, by considering the single layer network  diagrammed
   in figure 1 to be a basic building block, we can construct more
   complicated networks, ones that perform powerful, nonlinear
   computations. figure 3 demonstrates this concept. instead of a single
   layer of weights between inputs and output, we introduce a set
   of  single-layer networks between the two. this set of intermediate
   networks is often referred to as a    hidden    layer, as it doesn   t
   directly observe input or directly compute the output. by using a
   hidden layer, we form a mult-layered ann. though there are many
   different conventions for declaring the actual number of layers in a
   multi-layer network, for this discussion we will use the convention of
   the number of distinct sets of trainable weights as the number of
   layers. for example, the network in figure 3 would be considered a
   2-layer ann because it has two layers of weights: those connecting the
   inputs to the hidden layer ( w_{ij} ), and those connecting the output
   of the hidden layer to the output layer( w_{jk} ).
   [28]diagram of a mult-layer neural network. each neuron in the networ.k
   can be considered a id88

   figure 3: diagram of a multi-layer ann. each node in the network can be
   considered a single-layered ann (for simplicity, biases are not
   visualized in graphical model)

   multi-layer neural networks form compositional functions that map the
   inputs nonlinearly to outputs. if we associate index i with the input
   layer, index j with the hidden layer, and index k with the output
   layer, then an output unit in the network diagrammed in figure 3
   computes an output value a_k given and input a_i via the following
   compositional function:

   \large{ \begin{array}{rcl}a_{\text{out}} = a_k = g_k(b_k +
   \sum_jg_j(b_j + \sum_i a_i w_{ij})w_{jk}\end{array}} .

   here z_l is the  is the pre-activation values for units for layer l ,
   g_l() is the activation function for units  in that layer (assuming
   they are the same), and a_l = g_l(z_l) is the output activation for
   units in that layer. the weight w_{l-1, l} links the outputs of units
   feeding into layer l to the activation function of units for that
   layer. the term b_l is the bias for units in layer l .

   as with the single-layered ann, the choice of activation function for
   the output layer will depend on the task that we would like the network
   to perform (i.e. categorization or regression), and follows similar
   rules outlined above. however, it is generally desirable for the hidden
   units to have nonlinear id180 (e.g. logistic sigmoid or
   tanh). this is because multiple layers of linear computations can be
   equally formulated as a single layer of linear computations. thus using
   linear activations for the hidden layers doesn   t buy us much. however,
   as we   ll see shortly, using linear activations for the output unit
   activation function (in conjunction with nonlinear activations for the
   hidden units) allows the network to perform nonid75.

training neural networks & id119

   training neural networks involves determining the network parameters
   that minimize the errors that the network makes. this first requires
   that we have a way of quantifying error. a standard way of
   quantifying error is to take the squared difference between the network
   output and the target value:

   \large{\begin{array}{rcl}e &=& \frac{1}{2}(\text{output} -
   \text{target})^2\end{array}}

   (note that the squared error is not chosen arbitrarily, but has a
   number of theoretical benefits and considerations. for more detail, see
   the [29]following post) with an error function in hand, we then aim to
   find the setting of parameters that minimizes this error function. this
   concept can be interpreted spatially by imagining a    parameter space   
   whose dimensions are the values of each of the model parameters, and
   for which the error function will form a surface of varying height
   depending on its value for each parameter. model training is thus
   equivalent to finding point in parameter space that makes the height of
   the error surface small.

   to get a better intuition behind this concept, let   s define a super
   simple neural network, one that has a single input and a single output
   (figure 4, bottom left). for further simplicity, we   ll assume the
   network has no bias term and thus has a single parameter, w_1 . we will
   also assume that the output layer uses the logistic sigmoid activation
   function. accordingly, the network will map some input value a_0 onto a
   predicted output a_{\text{out}} via the following function.

   \large{\begin{array}{rcl}a_{\text{out}} =
   g_{\text{logistic}}(a_0w_1)\end{array}}

   now let   s say we want this simple network to learn the
   identity function: given an input of 1 it should return a target value
   of 1. given this target value we can now calculate the value of the
   error function for each setting of w_1 . varying the value of w_1 from
   -10 to 10 results in the error surface displayed in the left of figure
   4.  we see that the error is small for large positive values of w_1 ,
   while the error is large for strongly negative values of w_1 . this not
   surprising, given that the output activation function is the logistic
   sigmoid, which will map large values onto an output of 1.

   things become more interesting when we move from a single-layered
   network to a multi-layered network. let   s repeat the above exercise,
   but include a single hidden node between the input and the output
   (figure 4, bottom right). again, we will assume no biases, and logistic
   sigmoid activations for both the hidden and output nodes. thus the
   network will have two parameters: \large{(w_1, w_2)} . accordingly the
   2-layered network will predict an output with the  following function:

   \large{\begin{array}{rcl}a_{\text{out}} =
   g_{\text{logistic}}(g_{\text{logistic}}(a_0w_1)w_2)\end{array}}

   now, if we vary both w_1 and w_2 , we obtain the error surface in the
   right of figure 4.
   [30]figure 4: error surface for a simple, single-layer neural network
   (left) and a 2-layer network (right). the goal is to map the input
   value 1 to the output value 1.

   figure 4: error surface for a simple, single-layer neural network
   (left) and a 2-layer network (right). the goal is to map the input
   value 1 to the output value 1.


   we see that the error function is minimized when both w_1 and w_2 are
   large and positive. we also see that the error surface is more complex
   than for the single-layered model, exhibiting a number of wide plateau
   regions. it turns out that the error surface gets more and more
   complicated as you increase the number of layers in the network and the
   number of units in each hidden layer. thus, it is important to consider
   these phenomena when constructing neural network models.

   code block 2: generates figure 4 (assumes you have run code block 1):
% visualize error surface of simple anns
e = {};
[w1,w2] = meshgrid(linspace(-10,10,50));
g = gsigmoid;
target = 1;
net1output = g(w1.*target);
net2output = g(w2.*g(w1.*target));
e{1} = (net1output - target).^2;
e{2} = (net2output - target).^2;
figure
for ii = 1:2
        set(gcf,'position',[100,100,960,420])
        subplot(1,2,ii)
        surf(w1,w2,e{ii}); shading faceted;
        colormap(flipud(hot)); caxis([0,max(max(e{ii}))])
        set(gca,'fontsize',16)
        xlabel('w_{1}'), ylabel('w_{2}'), zlabel('e(w)')
        axis square;
        title(sprintf('error surface: %d-layer network',ii))
        [az, el] = view;
        view([az + 180, el]);
        set(gcf,'position',[100,100,1020,440])
        drawnow
end


   the examples in figure 4 gives us a qualitative idea of how to
   train the parameters of an ann, but we would like a more automatic way
   of doing so. generally this problem is solved  using id119:
   the id119 algorithm first calculates the derivative /
   gradient of the error function with respect  to each of the model
   parameters. this gradient information will give us the direction in
   parameter space that decreases the height of the error surface. we then
   take a step in that direction and repeat, iteratively calculating the
   gradient and taking steps in parameter space.

the id26 algorithm

   it turns out that the gradient information for the ann error surface
   can be calculated efficiently using a message passing algorithm known
   as the id26 algorithm. during id26, input signals
   are forward-propagated through the network toward the outputs,
   and network errors are then calculated with respect to target variables
   and back-propagated backwards towards the inputs. the forward and
   backward signals are then used to determine the direction in the
   parameter space to move that lowers the network error.

   the formal calculations behind the id26 algorithm can
   be somewhat mathematically involved and may detract from the general
   ideas behind the learning algorithm. for those readers who are
   interested in the math, i have provided the formal derivation of the
   id26 algorithm in the [31]following post (for those of you
   who are not interested in the math, i would also encourage you go over
   the derivation and try to make connections to the source code
   implementations provided later in the post).

   figure 5 demonstrates the key steps of the id26 algorithm.
   the main concept underlying the algorithm is that for a given
   observation we want to determine the degree of    responsibility    that
   each network parameter has for mis-predicting a target value associated
   with the observation. we then change that parameter according to this
   responsibility so that it reduces the network error.
   [32]figure 5: the 4 main steps of the bacdkpropagation algorithm: i
   forward propagate error signals to output, ii calculate output error e,
   and backpropagate error signal, iii use forward signal and backward
   signals to calculate parameter gradients, iv update network parameters.

   figure 5: the 4 main steps of the bacdkpropagation algorithm: i forward
   propagate error signals to output, ii calculate output error e, and
   backpropagate error signal, iii use forward signal and backward signals
   to calculate parameter gradients, iv update network parameters.

   in order to determine the network error, we first propagate the
   observed input forward through the network layers. this is step i of
   the id26 algorithm, and is demonstrated in figure 5-i. note
   that in the figure a_k  could be considered network output  (for a
   network with one hidden layer) or the output of a hidden layer that
   projects the remainder of the network (in the case of a network with
   more than one hidden layer). for this discussion, however, we assume
   that the index k is associated with the output layer of the network,
   and thus each of the network outputs is designated by a_k . also note
   that when implementing this forward-propagation step, we should keep
   track of the feed-forward pre-activations z_l and activations a_l for
   all layers l , as these will be used for calculating backpropagated
   errors and error function gradients.

   step ii of the algorithm is to calculate the network output error and
   backpropagate it toward the input. let   s again that we are using the
   sum of squared differences error function:

   \large{\begin{array}{rcl}e = \frac{1}{2}\sum_{k \in k}(a_k -
   t_k)^2\end{array}} ,

   where we sum over the values of all k output units (one in this
   example). we can now define an    error signal      \delta_k at the output
   node that will be backpropagated toward the input. the error signal is
   calculated as follows:

   \large{\begin{array}{rcl} \begin{array}{rcl} \delta_k &=&
   g_k'(z_k)e'(a_k,t_k) \\ &=& g_k'(z_k)(a_k - t_k)\end{array}\end{array}}
   .

   thus the error signal essentially weights the gradient of the error
   function by the gradient of the output activation function (notice
   there is a z_k term is used in this calculation, which is why we keep
   it around during the forward-propagation step). we can continue
   backpropagating the error signal toward the input by passing \delta_k
   through the output layer weights w_{jk} , summing over all output
   nodes, and passing the result through the gradient of the activation
   function at the hidden layer g_j'(z_j) (figure 5-ii). performing these
   operations results in the back-propagated error signal for the hidden
   layer, \delta_j :

   \large{\begin{array}{rcl} \delta_j = g_j'(z_j)\sum_k \delta_k
   w_{jk}\end{array}} ,

   for networks that have more than one hidden layer, this error
   id26 procedure can continue for layers j-1, j-2, ... , etc.

   step iii of the id26 algorithm is to calculate the gradients
   of the error function with respect to the model parameters at each
   layer l using the forward signals a_{l-1} , and the backward error
   signals \delta_l . if one considers the model weights w_{l-1, l}  at a
   layer l as linking the forward signal a_{l-1} to the error signal
   \delta_l (figure 5-iii), then the gradient of the error function with
   respect to those weights is:

   \large{ \begin{array}{rcl} \frac{\partial e}{\partial w_{l-1, l}} =
   a_{l-1}\delta_l\end{array}}

   note that this result is closely related to the concept of [33]hebbian
   learning in neuroscience. thus the gradient of the error function with
   respect to the model weight at each layer can be efficiently calculated
   by simply keeping track of the forward-propagated activations feeding
   into that layer from below, and weighting those activations by the
   backward-propagated error signals feeding into that layer from above!

   what about the bias parameters? it turns out that the same gradient
   rule used for the weight weights applies, except that    feed-forward
   activations    for biases are always +1 (see figure 1). thus the bias
   gradients for layer l are simply:

   \large{\begin{array}{rcl}\frac{\partial e}{\partial b_{l}} =
   (1)\delta_l = \delta_l \end{array}}

   the fourth and final step of the id26 algorithm is to update
   the model parameters based on the gradients calculated in step iii.
   note that the gradients point in the direction in parameter space that
   will increase the value of the error function. thus when updating the
   model parameters we should choose to go in the opposite direction. how
   far do we travel in that direction? that is generally determined by a
   user-defined step size (aka learning rate) parameter, \eta . thu,s
   given the parameter gradients and the step size, the weights and biases
   for a given layer are updated accordingly:

   \large{ \begin{array}{rcl} w_{l-1,l} &\leftarrow& w_{l-1,l} - \eta
   \frac{\partial e}{\partial w_{l-1, l}} \\ b_l &\leftarrow& b_{l} - \eta
   \frac{\partial e}{\partial b_{l}}\end{array}} .

   to train an ann, the four steps outlined above and in figure 5 are
   repeated iteratively by observing many input-target pairs and updating
   the parameters until either the network error reaches a tolerably low
   value, the parameters cease to update (convergence), or a set number of
   parameter updates has been achieved. some readers may find the steps of
   the id26 somewhat ad hoc. however, keep in mind that these
   steps are formally coupled to the calculus of the optimization problem.
   thus i again refer the curious reader to check out [34]the
   derivation in order to make connections between the algorithm and the
   math.

example: learning the or & and logical operators using a single layer neural
network

   here we go over an example of training a single-layered neural network
   to perform a classification problem. the network is trained to learn a
   set of logical operators including the  and, or, or xor. to train the
   network we first generate training data. the inputs consist of
   2-dimensional coordinates that span the input values (x_1, x_2) values
   for a 2-bit truth table:
   [35]truth table values learned in classification examples

   figure 6: truth table values learned in classification examples

   we then perturb these observations by adding normally-distributed
   noise. to generate target variables, we categorize each observations by
   applying one of logic operators (see figure 6) to the original
   (no-noisy) coordinates. we then train the network with the noisy inputs
   and binary categories targets using the id119 /
   id26 algorithm. the code implementation of the network and
   training procedures, as well as the resulting learning process are
   displayed below. (note that in this implementation, i do not use the
   feed-forward activations to calculate the gradients as suggested above.
   this is simply to make the implementation of the learning algorithm
   more explicit in terms of the math. the same situation also applies to
   the other examples in this post).

   code block 3: implements and trains a single-layer neural network for
   classification to learn logical operators (assumes you have run code
   block 1):
%% example: single-layered network

% define data and targets
data = [0 0; 0 1; 1 0; 1 1;];
classand = and(data(:,1)&gt;0,data(:,2)&gt;0);
classor = or(data(:,1)&gt;0,data(:,2)&gt;0);
classxor = xor(data(:,1)&gt;0,data(:,2)&gt;0);

% the type of truth table to learn (uncomment for others)
classes = classor
% classes = classand;
% classes = classxor;

% make multiple noisy training observations
nrepats = 30;
data = repmat(data, [nrepats, 1]);
classes = repmat(classes, [nrepats, 1]);
data = data + .15*randn(size(data));

% shuffle data
shuffleidx = randperm(size(data,1));
data = data(shuffleidx,:);
classes = classes(shuffleidx);

% initialize model parameters
[nobs,ninput] = size(data); % # of input dimensions
noutput = 1;                            % # of target/output dimensions
lrate = 3;                              % learning rate for parameters update
niters = 80;                            % # of iterations

% declare id180 (and derivatives)
g_out = gsigmoid; gprime_out = gprimesigmoid;

% initialize random weights
w_out = (rand(ninput,noutput)-.5);
b_out = (rand(1,noutput)-.5);

% some other initializations
% (for visualization)
visrange = [-.2 1.2];
[xx,yy] = meshgrid(linspace(visrange(1), visrange(2),100));
iter = 1;
mse = zeros(1,niters);

figure
set(gcf,'position',[100,100,960,420])
while 1
        err = zeros(1,nobs);
        % loop through the examples
        for io = 1:nobs

                % get current network input data and target
                input = data(io,:);
                target = classes(io);

                %% i. forward propagate data through network
                z_out = input*w_out + b_out; % output unit pre-activations
                a_out = g_out(z_out);        % output unit activations

                %% ii. backpropagate error signal
                % calculate error derivative w.r.t. output
                delta_out = gprime_out(z_out).*(a_out - target);

                %% iii. calculate gradient w.r.t. parameters...
                dedw_out = delta_out*input;
                dedb_out = delta_out*1;

                %% iv. update network parameters
                w_out = w_out - lrate*dedw_out';
                b_out = b_out - lrate*dedb_out';

                % calculate error function
                err(io) = .5*(a_out-target).^2;
        end
        mse(iter) = mean(err);

        % display learning
        clf; subplot(121); hold on;
        set(gca,'fontsize',16)
        netout = g_out(bsxfun(@plus,[xx(:),yy(:)]*w_out, b_out));
        contourf(xx,yy,reshape(netout,100,100)); colormap(flipud(spring))
        hold on;
        gscatter(data(:,1),data(:,2),classes,[0 0 0 ; 1 1 1],[],20,'off');
        title(sprintf('iteration %d',iter))
        xlim([visrange(1) visrange(2)]),ylim([visrange(1) visrange(2)]);
        axis square

        subplot(122);
        set(gca,'fontsize',16)
        plot(1:iter,mse(1:iter));
        xlabel('iteration')
        ylabel('mean squared error')
        axis square
        m1(iter) = getframe(gcf);

        if iter &gt;= niters
                break
        end
        iter = iter + 1;
end

   [36]single layer neural network (id88) learning a noisy or
   mapping.

   figure 7: single layer neural network (id88) learning a noisy or
   mapping.

   figure 7 displays the procedure for learning the or mapping. the left
   plot displays the training data and the network output at each
   iteration. white dots are training points categorized    1    while black
   dots are categorized    0   . yellow regions are where the network predicts
   values of    0   , while magenta highlights areas where the network
   predicts    1   . we see that the single-layer network is able to easily
   separate the two classes.  the right plot shows how the error function
   decreases with each training iteration. the smooth trajectory of the
   error indicates that the error surface is also fairly smooth.
   [37]single layer neural network (id88) learning a noisy and
   mapping.

   figure 8: single layer neural network (id88) learning a noisy and
   mapping.

   figure 8 demonstrates an analogous example, but instead learning the
   and operator (by executing code block 3, after un-commenting line 11).
   again, the  categories can be easily separated by a plane, and thus the
   single-layered network easily learns an accurate predictor of the data.

going deeper: nonlinear classification and multi-layer neural networks

   figures 7 and 8 demonstrate how a single-layered ann can easily
   learn the or and and operators. this is because the categorization
   criterion for these logical operators can be represented in the input
   space by a single linear function (i.e. line/plane). what about more
   complex categorization criterion that cannot be represented by a single
   plane? an example of a more complex binary classification criterion is
   the xor operator (figure 6, far right column).

   below we attempt to train the single-layer network to learn the xor
   operator (by executing code block 3, after un-commenting line 12). the
   single layer network is unable to learn this nonlinear mapping between
   the inputs and the targets. however, it turns out we can learn the xor
   operator using a multi-layered neural network.
   [38]single layer neural network (id88) attempting to learn a
   noisy xor mapping. the single layer network chokes on this nonlinear
   problem.

   figure 9: single layer neural network (id88) attempting to learn
   a noisy xor mapping. the single layer network chokes on this nonlinear
   problem.

   below we train a two-layer neural network on the xor dataset. the
   network incorporates a hidden layer with 3 hidden units and logistic
   sigmoid id180 for all units in the hidden and output
   layers (see code block 4, lines 32-33).

   code block 4: implements and trains a two-layer neural network for
   classification to learn xor operator and more difficult    ring    problem
   (figures 10 & 11; assumes you have run code block 1):

%% example: multi-layer neural network for classification
data = [0 0; 0 1; 1 0; 1 1;];
classxor = xor(data(:,1)&gt;0,data(:,2)&gt;0);

% the type of truth table to learn
classes = classxor;

% uncomment for mor difficult data...
% data = [data; .5 .5; 1 .5; 0 .5; .5 0; .5 1];
% classring = [1; 1; 1; 1; 0; 1; 1; 1; 1];
% classes = classring;

% create many noisy observations
nrepats = 30;
data = repmat(data, [nrepats, 1]);
classes = repmat(classes, [nrepats, 1]);
data = data + .15*randn(size(data));

% shuffle observations
shuffleidx = randperm(size(data,1));
data = data(shuffleidx,:);
classes = classes(shuffleidx);

% initialize model parameters
[nobs,ninput] = size(data);     % # of input dimensions
nhidden = 3;                                    % # of hidden units

lrate = 2;      % learning rate for parameters update
niters = 300;   % # of iterations

% declare id180 (and derivatives)
g_hid = gsigmoid; gprime_hid = gprimesigmoid;
g_out = gsigmoid; gprime_out = gprimesigmoid;

% initialize weights
w_hid = (rand(ninput,nhidden)-.5);
b_hid = (rand(1,nhidden)-.5);
w_out = (rand(nhidden,noutput)-.5);
b_out = (rand(1,noutput)-.5);

iter = 1;
mse = zeros(1,niters);
figure
set(gcf,'position',[100,100,960,420])
% main training algorithm
while 1
        err = zeros(1,nobs);
        % loop through the examples
        for io = 1:nobs

                % get current network input data and target
                input = data(io,:);
                target = classes(io);

                %% i. forward propagate data through network
                z_hid = input*w_hid + b_hid; % hidden unit pre-activations
                a_hid = g_hid(z_hid);        % hidden unit activations
                z_out = a_hid*w_out + b_out; % output unit pre-activations
                a_out = g_out(z_out);        % output unit activations

                %% ii.  backpropagate error signal
                % calculate error derivative w.r.t. output
                delta_out = gprime_out(z_out).*(a_out - target);

                % calculate error contributions for hidden nodes...
                delta_hid = gprime_hid(z_hid)'.*(delta_out*w_out);

                %% iii. calculate gradient w.r.t. parameters...
                dedw_out = delta_out*a_hid;
                dedb_out = delta_out*1;
                dedw_hid = delta_hid*input;
                dedb_hid = delta_hid*1;

                %% iv. update network parameters
                w_out = w_out - lrate*dedw_out';
                b_out = b_out - lrate*dedb_out';

                w_hid = w_hid - lrate*dedw_hid';
                b_hid = b_hid - lrate*dedb_hid';

                % calculate error function
                err(io) = .5*(a_out-target).^2;
        end
        mse(iter) = mean(err);

        % display learning
        clf; subplot(121); hold on;
        set(gca,'fontsize',16)

        netout = g_out(bsxfun(@plus,g_hid(bsxfun(@plus,[xx(:),yy(:)]*w_hid, b_hi
d))*w_out, b_out));
        contourf(xx,yy,reshape(netout,100,100)); colormap(flipud(spring))
        hold on;
        gscatter(data(:,1),data(:,2),classes,[0 0 0; 1 1 1],[],20,'off');
        title(sprintf('iteration %d',iter))
        xlim([visrange(1), visrange(2)]),ylim([visrange(1), visrange(2)]);
        axis square

        subplot(122);
        set(gca,'fontsize',16)
        plot(1:iter,mse(1:iter));
        xlabel('iteration')
        ylabel('mean squared error')
        axis square
        m2(iter) = getframe(gcf);

        if iter &gt;= niters
                break
        end
        iter = iter + 1;
end

   [39]a multi-layer neural network (id88) attempting to learn a
   noisy xor mapping. the multi-layer network easily learns this nonlinear
   problem.

   figure 10: a multi-layer neural network (id88) attempting to
   learn a noisy xor mapping. the multi-layer network easily learns this
   nonlinear problem.

   figure 10 displays the learning process for the 2-layer network. the
   formatting for figure 10 is analogous to that for figures 7-9. the
   2-layer network is easily able to learn the xor operator. we see that
   by adding a hidden layer between the input and output, the ann is able
   to learn the nonlinear categorization criterion!

   figure 11 shows the results for learning a even more difficult
   nonlinear categorization function: points in and around (x1, x2) = (0.5
   0.5) are categorized as    0   , while points in a ring surrounding the    0   
   datapoints are categorized as a    1    (figure 11). this example is run by
   executing code block 4 after un-commenting  lines 9-11.
   [40]multilayer neural network learning a nonlinear binary
   classification task

   figure 11: multilayer neural network learning a nonlinear binary
   classification task

   figure 11 shows the learning process. again formatting is analogous to
   the formatting in figures 8-10. the 2-layer ann is able to learn
   this difficult classification criterion.

example: neural networks for regression

   the previous examples demonstrated how anns can be used for
   classification by using a logistic sigmoid as the output activation
   function. here we demonstrate how, by making the output activation
   function the linear/identity function, the same 2-layer network
   architecture can be used to implement nonid75.

   for this example we define a dataset comprised of 1d inputs, \mathbf{x}
   that range from  (-5, 5) . we then generate noisy targets \mathbf y
   according to the function:

   \large{\begin{array}{rcl}\mathbf{y} = f(\mathbf{x}) +
   \mathbf{\epsilon}\end{array}}

   where f(x) is a nonlinear data-generating function and \mathbf \epsilon
   is normally-distributed noise. we then construct a two-layered network
   with tanh id180 used in the hidden layer and linear
   outputs. for this example we set the number of hidden units to 3 and
   train the model as we did for categorization using id119 /
   id26. the results of the example are displayed below.

   code block 5: trains two-layer network for regression problems (figures
   11 & 12; assumes you have run code block 1):
%% example: nonid75

% define data-generating functions f(x)
xmin = -5; xmax = 5;
xx = linspace(xmin, xmax, 100);
f = inline('2.5 + sin(x)','x');
% f = inline('abs(x)','x'); % uncomment for figure 13
yy = f(xx) + randn(size(xx))*.5;

% for shuffling observations
shuffleidx = randperm(length(xx));
data = xx;
targets = yy;

% initialize model parameters
nobs = length(data);    % # of input dimensions
ninput = 1;                             % # of inputs
nhidden = 3;                    % # of hidden units
noutput = 1;                    % # of target/output dimensions
lrate = .15;                    % learning rate for parameters update
niters = 200;                   % # of iterations

cols = lines(nhidden);

% declare id180 (and derivatives)
g_hid = gtanh;                     % hidden unit activation
gprime_hid = gprimetanh;   % grad of hidden unit activation
g_out = glinear;                   % output activation
gprime_out = gprimelinear; % grad. of output ativation

% % initialize weights
w_hid = (rand(ninput,nhidden)-.5);
b_hid = (rand(1,nhidden)-.5);
w_out = (rand(nhidden,noutput)-.5);
b_out = (rand(1,noutput)-.5);

% initialize some things..
% (for visualization)
mse = zeros(1,niters);
visrange = [xmin, xmax];
figure
set(gcf,'position',[100,100,960,420])
iter = 1;
while 1

        err = zeros(1,nobs);
        % loop through the examples
        for io = 1:nobs

                % get current network input data and target
                input = data(io);
                target = targets(io);

                %% i. forward propagate data through network
                z_hid = input*w_hid + b_hid; % hidden unit pre-activations
                a_hid = g_hid(z_hid);        % hidden unit activations
                z_out = a_hid*w_out + b_out; % output unit pre-activations
                a_out = g_out(z_out);        % output unit activations

                %% ii. backpropagate error signal
                % calculate error derivative w.r.t. output
                delta_out = gprime_out(z_out).*(a_out - target);

                %% calculate error contributions for hidden nodes...
                delta_hid = gprime_hid(z_hid)'.*(delta_out*w_out);

                %% iii. calculate gradient w.r.t. parameters...
                dedw_out = delta_out*a_hid;
                dedb_out = delta_out*1;
                dedw_hid = delta_hid*input;
                dedb_hid = delta_hid*1;

                %% iv. update network parameters
                w_out = w_out - lrate*dedw_out';
                b_out = b_out - lrate*dedb_out';

                w_hid = w_hid - lrate*dedw_hid';
                b_hid = b_hid - lrate*dedb_hid';

                % calculate error function for batch
                err(io) = .5*(a_out-target).^2;
        end
        mse(iter) = mean(err); % update error

        % display learning
        clf; subplot(121); hold on;
        set(gca,'fontsize',14)

        plot(xx,f(xx),'m','linewidth',2);
        hold on;
        scatter(xx, yy ,'m');

        % plot total network output
        netout = g_out(g_hid(bsxfun(@plus, xx'*w_hid, b_hid))*w_out + b_out);
        plot(xx, netout, 'k','linewidth', 2)

        % plot each hidden unit's output function
        for iu = 1:nhidden
                plot(xx,g_hid(xx*w_hid(iu) + b_hid(iu)),'color',cols(iu,:),'line
width',2, ...
                                                                           'line
style','--');
        end

        % title and legend
        title(sprintf('iteration %d',iter))
        xlim([visrange(1) visrange(2)]),ylim([visrange(1) visrange(2)]);
        axis square
        legend('f(x)', 'targets', 'network output','hidden unit outputs','locati
on','southwest')

        % plot error
        subplot(122);
        set(gca,'fontsize',14)
        plot(1:iter,mse(1:iter));
        xlabel('iteration')
        ylabel('mean squared error')
        axis square; drawnow

        % anneal learning rate
        lrate = lrate *.99;
        if iter &gt;= niters
                break
        end
        iter = iter + 1;
end

   [41]a two-layered ann used for regression. the network approximates the
   function f(x) = sin(x) + 2.5

   figure 12: a two-layered ann used for regression. the network
   approximates the function f(x) = sin(x) + 2.5

   the training procedure for f(x): \sin(x) + 2.5 is visualized in the
   left plot of figure 12. the data-generating function f(x) is plotted as
   the solid magenta line, and the noisy target values used to train the
   network are plotted as magenta circles. the output of the network at
   each training iteration is plotted in solid black while the output of
   each of the tanh hidden units is plotted in dashed lines. this
   visualization demonstrates how multiple nonlinear functions can be
   combined to form the complex output target function. the mean squared
   error at each iteration is plotted in the right plot of figure 12. we
   see that the error does not follow a simple trajectory during learning,
   but rather undulates, demonstrating the non-convexity of the error
   surface.

   figure 13 visualizes the training procedure for trying to learn a
   different nonlinear function, namely f(x): \text{abs}(x) (by running
   code block 5, after un-commenting out line 7). again, we see how the
   outputs of the hidden units are combined to fit the desired
   data-generating function. the mean squared error again follows an
   erratic path during learning.
   [42]a two-layered ann used for regression. the network approximates the
   function f(x) = abs(x)

   figure 13: a two-layered ann used for regression. the network
   approximates the function f(x) = abs(x)

   notice for this example that i added an extra implementation detail
   known as simulated annealing (line 118) that was absent in the
   classification examples. this technique decreases the learning rate
   after every iteration thus making the algorithm take smaller and
   smaller steps in parameter space.  this technique can be useful when
   the gradient updates begin oscillating between two or more locations in
   the parameter space. it is also helpful for influencing the algorithm
   to settle down into a steady state.

wrapping up

   in this post we covered the main ideas behind artificial neural
   networks including: single- and multi-layer anns, id180
   and their derivatives, a high-level description of the id26
   algorithm, and a number of classification and regression examples.
   anns, particularly mult-layer anns, are a robust and powerful class of
   models that can be used to learn complex, nonlinear functions. however,
   there are a number of considerations when using neural networks
   including:
     * how many hidden layers should one use?
     * how many hidden units in each layer?
     * how do these relate to overfitting and generalization?
     * are there better error functions than the squared difference?
     * what should the learning rate be?
     * what can we do about the complexity of error surface with deep
       networks?
     * should we use simulated annealing?
     * what about other id180?

   it turns out that there are no easy or definite answers to any of these
   questions, and there is active research focusing on each topic. this is
   why using anns is often considered as much as a    black art    as it is
   a quantitative technique.

   one primary limitation of anns is that they are supervised algorithms,
   requiring a target value for each input observation in order to train
   the network. this can be prohibitive for training large networks that
   may require lots of training data to adequately adjust the parameters.
   however, there are a set of unsupervised variants of anns that can be
   used to learn an initial condition for the ann (rather than from
   randomly-generated initial weights) without the need of target values.
   this technique of    unsupervised pretraining    has been an important
   component of many    deep learning    models used in ai and machine
   learning. in future posts, i look forward to covering two of these
   unsupervised neural networks: autoencoders and restricted boltzmann
   machines.


   advertisements

share this:

     * [43]twitter
     * [44]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [45]view all posts by dustinstansbury   

   posted on september 11, 2014, in [46]classification, [47]gradient
   descent, [48]machine learning, [49]neural networks, [50]neuroscience,
   [51]regression and tagged [52]id26, [53]classification,
   [54]deep learning, [55]id119, [56]neural networks,
   [57]regression. bookmark the [58]permalink. [59]12 comments.
   [60]    derivation: derivatives for common neural network
   id180
   [61]derivation: maximum likelihood for id82s    
     * [62]leave a comment
     * [63]trackbacks 2
     * [64]comments 10

    1. [65]bendichter | [66]september 11, 2014 at 1:36 pm
       dustin, this intro to anns is outstanding! i   ve seen recent
       interest in rectified linear id180. do you have any
       thoughts on these?
       [67]reply
          + [68]dustinstansbury | [69]september 11, 2014 at 8:44 pm
            thanks ben, hopefully you find the post helpful.
            yeah, relus are pretty sweet! they exhibit some nice
            invariance properties that are useful for pattern recognition
            (for details, see work from bengio   s group on rectifier nets).
            i often use the softrect/softplus function, an analytic
            approximation to the relu (in matlab-ish syntax):
            g(z) = 1/k.*log(1 + exp(k*z))
            g'(z) = 1./(1 + exp(-k*z)),
            where k is a hyperparameter that controls the smoothness of
            the rectification around zero.
            [70]reply
    2. [71]fifinonz | [72]february 3, 2015 at 7:23 am
       indepth, informative yet simple! very helpful, thank you
       [73]reply
    3. [74]dhliuvip | [75]march 20, 2016 at 9:26 am
       reblogged this on [76]robotic run and commented:
       fantastic introduction to ann
       [77]reply
    4. sam | [78]may 1, 2016 at 10:23 pm
       this is excellent; good work sir.
       [79]reply
    5. [80]harish narayanan | [81]june 16, 2016 at 1:48 pm
       thank you for this excellently written post. in literally one
       sentence:    using linear activations for the output unit activation
       function (in conjunction with nonlinear activations for the hidden
       units) allows the network to perform nonid75   , you   ve
       clarified an idea i   ve been grasping at but couldn   t get to
       earlier.
       [82]reply
    6. george gillams | [83]december 31, 2016 at 5:56 am
       this has greatly simplified this topic for me. thank you so much
       [84]reply
    7. riva | [85]february 18, 2017 at 7:28 am
       hi dustin, can you please give me permission to use figure 1
       (figure 1: diagram of a single-layered id158.)
       please for my research paper?
       [86]reply
    8. igor | [87]april 5, 2017 at 5:27 am
       short and simple. love it!
       [88]reply
    9. [89]umut can   akmak | [90]july 25, 2017 at 10:46 pm
       hi, is the    weight update    in the id26 algorithm figure
          id119   ? like, the first three steps are of
       id26 algorithm but then the weight update is performed
       with a id119, right. so, if i am using adam optimizer or
       something similar, then, the fourth step will not be the one in the
       figure but adam   s own parameter update formulations?
       thank you for this great introduction.
       [91]reply

    1. pingback: [92]distilled news | data analytics & r
    2. pingback: [93]a gentle introduction to id158s    
       robotic run

leave a reply [94]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [95]googleplus-sign-in

     *
     *

   [96]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [97]log out /
   [98]change )
   google photo

   you are commenting using your google account. ( [99]log out /
   [100]change )
   twitter picture

   you are commenting using your twitter account. ( [101]log out /
   [102]change )
   facebook photo

   you are commenting using your facebook account. ( [103]log out /
   [104]change )
   [105]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [106]algorithms [107]classification [108]id174
       [109]density estimation [110]derivations [111]id171
       [112]fmri [113]id119 [114]latex [115]machine learning
       [116]matlab [117]maximum likelihood [118]mcmc [119]neural networks
       [120]neuroscience [121]optimization [122]proofs [123]regression
       [124]sampling [125]sampling methods [126]simulations
       [127]statistics [128]theory [129]tips & tricks [130]uncategorized
     * recent posts
          + [131]derivation: maximum likelihood for id82s
          + [132]a gentle introduction to id158s
          + [133]derivation: derivatives for common neural network
            id180
          + [134]derivation: error id26 & id119 for
            neural networks
          + [135]model selection: underfitting, overfitting, and the
            id160
          + [136]supplemental proof 1
          + [137]the statistical whitening transform
          + [138]covariance matrices and data distributions
          + [139]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [140]derivation: the covariance matrix of an ols estimator
            (and applications to gls)
     * archives
          + [141]september 2014
          + [142]april 2013
          + [143]march 2013
          + [144]january 2013
          + [145]december 2012
          + [146]november 2012
          + [147]october 2012
          + [148]september 2012
          + [149]march 2012
          + [150]february 2012
          + [151]january 2012
     * meta
          + [152]register
          + [153]log in
          + [154]entries rss
          + [155]comments rss
          + [156]wordpress.com
       advertisements

   [157]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [158]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [159]cookie policy

   iframe: [160]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/feed/
   4. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
   5. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#access
  11. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#main
  12. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#sidebar
  13. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#sidebar2
  14. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  21. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
  22. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf
  25. https://theclevermachine.files.wordpress.com/2014/09/id882.png
  26. https://theclevermachine.files.wordpress.com/2014/09/act-funs.png
  27. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  28. https://theclevermachine.files.wordpress.com/2014/09/neural-net1.png
  29. https://theclevermachine.wordpress.com/2012/02/13/cutting-your-losses-loss-functions-predominance-of-sum-of-squares/
  30. https://theclevermachine.files.wordpress.com/2014/09/nnet-error-surface8.png
  31. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  32. https://theclevermachine.files.wordpress.com/2014/09/fprop_bprop5.png
  33. http://en.wikipedia.org/wiki/hebbian_theory
  34. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  35. https://theclevermachine.files.wordpress.com/2014/09/truth-table1.png
  36. https://theclevermachine.files.wordpress.com/2014/09/1layer-net-or.gif
  37. https://theclevermachine.files.wordpress.com/2014/09/1layer-net-and1.gif
  38. https://theclevermachine.files.wordpress.com/2014/09/1layer-net-xor.gif
  39. https://theclevermachine.files.wordpress.com/2014/09/2layer-net-xor.gif
  40. https://theclevermachine.files.wordpress.com/2014/09/2layer-net-ring.gif
  41. https://theclevermachine.files.wordpress.com/2014/09/2layer-net-regression-sine.gif
  42. https://theclevermachine.files.wordpress.com/2014/09/2layer-net-regression-abs.gif
  43. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?share=twitter
  44. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?share=facebook
  45. https://theclevermachine.wordpress.com/author/dustinstansbury/
  46. https://theclevermachine.wordpress.com/category/algorithms/classification/
  47. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  48. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  49. https://theclevermachine.wordpress.com/category/neural-networks/
  50. https://theclevermachine.wordpress.com/category/neuroscience/
  51. https://theclevermachine.wordpress.com/category/algorithms/regression/
  52. https://theclevermachine.wordpress.com/tag/id26/
  53. https://theclevermachine.wordpress.com/tag/classification/
  54. https://theclevermachine.wordpress.com/tag/deep-learning/
  55. https://theclevermachine.wordpress.com/tag/gradient-descent/
  56. https://theclevermachine.wordpress.com/tag/neural-networks/
  57. https://theclevermachine.wordpress.com/tag/regression/
  58. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  59. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comments
  60. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  61. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
  62. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#respond
  63. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#trackbacks
  64. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comments
  65. http://gravatar.com/bendichter
  66. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-186
  67. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=186#respond
  68. http://machinelearnings.wordpress.com/
  69. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-187
  70. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=187#respond
  71. http://www.betterworldsystems.co.ke/
  72. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-286
  73. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=286#respond
  74. http://dhliuvip.wordpress.com/
  75. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-615
  76. https://dhliuvip.wordpress.com/2016/03/20/a-gentle-introduction-to-artificial-neural-networks/
  77. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=615#respond
  78. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-644
  79. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=644#respond
  80. https://harishnarayanan.org/
  81. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-672
  82. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=672#respond
  83. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-830
  84. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=830#respond
  85. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-893
  86. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=893#respond
  87. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-939
  88. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=939#respond
  89. https://www.facebook.com/app_scoped_user_id/10154156352703405/
  90. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-1048
  91. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/?replytocom=1048#respond
  92. http://advanceddataanalytics.net/2015/04/17/distilled-news-70/
  93. https://dhliuvip.wordpress.com/2016/03/20/a-gentle-introduction-to-artificial-neural-networks-2/
  94. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#respond
  95. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  96. https://gravatar.com/site/signup/
  97. javascript:highlandercomments.doexternallogout( 'wordpress' );
  98. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  99. javascript:highlandercomments.doexternallogout( 'googleplus' );
 100. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 101. javascript:highlandercomments.doexternallogout( 'twitter' );
 102. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 103. javascript:highlandercomments.doexternallogout( 'facebook' );
 104. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 105. javascript:highlandercomments.cancelexternalwindow();
 106. https://theclevermachine.wordpress.com/category/algorithms/
 107. https://theclevermachine.wordpress.com/category/algorithms/classification/
 108. https://theclevermachine.wordpress.com/category/data-preprocessing/
 109. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
 110. https://theclevermachine.wordpress.com/category/derivations/
 111. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
 112. https://theclevermachine.wordpress.com/category/fmri/
 113. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
 114. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
 115. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
 116. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
 117. https://theclevermachine.wordpress.com/category/maximum-likelihood/
 118. https://theclevermachine.wordpress.com/category/mcmc/
 119. https://theclevermachine.wordpress.com/category/neural-networks/
 120. https://theclevermachine.wordpress.com/category/neuroscience/
 121. https://theclevermachine.wordpress.com/category/optimization/
 122. https://theclevermachine.wordpress.com/category/proofs/
 123. https://theclevermachine.wordpress.com/category/algorithms/regression/
 124. https://theclevermachine.wordpress.com/category/algorithms/sampling/
 125. https://theclevermachine.wordpress.com/category/sampling-methods/
 126. https://theclevermachine.wordpress.com/category/simulations/
 127. https://theclevermachine.wordpress.com/category/statistics/
 128. https://theclevermachine.wordpress.com/category/theory/
 129. https://theclevermachine.wordpress.com/category/tips-tricks/
 130. https://theclevermachine.wordpress.com/category/uncategorized/
 131. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
 132. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 133. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
 134. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
 135. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
 136. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
 137. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
 138. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
 139. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
 140. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
 141. https://theclevermachine.wordpress.com/2014/09/
 142. https://theclevermachine.wordpress.com/2013/04/
 143. https://theclevermachine.wordpress.com/2013/03/
 144. https://theclevermachine.wordpress.com/2013/01/
 145. https://theclevermachine.wordpress.com/2012/12/
 146. https://theclevermachine.wordpress.com/2012/11/
 147. https://theclevermachine.wordpress.com/2012/10/
 148. https://theclevermachine.wordpress.com/2012/09/
 149. https://theclevermachine.wordpress.com/2012/03/
 150. https://theclevermachine.wordpress.com/2012/02/
 151. https://theclevermachine.wordpress.com/2012/01/
 152. https://wordpress.com/start?ref=wplogin
 153. https://theclevermachine.wordpress.com/wp-login.php
 154. https://theclevermachine.wordpress.com/feed/
 155. https://theclevermachine.wordpress.com/comments/feed/
 156. https://wordpress.com/
 157. https://wordpress.com/?ref=footer_website
 158. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 159. https://automattic.com/cookies
 160. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 162. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-form-guest
 163. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-form-load-service:wordpress.com
 164. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-form-load-service:twitter
 165. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/#comment-form-load-service:facebook
