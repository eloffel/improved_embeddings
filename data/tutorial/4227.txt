   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    derivation: derivatives for common neural
   network id180 comments feed [4]derivation: error
   id26 & id119 for neural networks [5]a gentle
   introduction to id158s [6]alternate [7]alternate
   [8]the clever machine [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    derivation: error id26 & id119 for
   neural networks
   [21]a gentle introduction to id158s    

derivation: derivatives for common neural network id180

   [22]sep 8

   posted by [23]dustinstansbury

introduction

   when constructing id158 (ann) models, one of the
   primary considerations is choosing id180 for hidden and
   output layers that are differentiable. this is because calculating the
   backpropagated error signal that is used to determine ann parameter
   updates requires the gradient of the activation function gradient .
   three of the most commonly-used id180 used in anns are
   the identity function, the logistic sigmoid function, and the
   hyperbolic tangent function. examples of these functions and their
   associated gradients (derivatives in 1d) are plotted in figure 1.
   [24]common id180 functions used in artificial neural,
   along with their derivatives

   common id180 functions used in artificial neural, along
   with their derivatives

   in the remainder of this post, we derive the derivatives/gradients for
   each of these common id180.

the identity activation function

   the simplest activation function, one that is commonly used for the
   output layer activation function in regression problems,  is the
   identity/linear activation function:

   \large{ \begin{array}{rcl}g_{\text{linear}}(z) = z \end{array}}

   (figure 1, red curves). this activation function simply maps the
   pre-activation to itself and can output values that range (-\infty,
   \infty) . why would one want to do use an identity activation function?
   after all, a multi-layered network with linear activations at each
   layer can be equally-formulated as a single-layered linear network. it
   turns out that the identity activation function is surprisingly useful.
   for example, a multi-layer network that has nonlinear activation
   functions amongst the hidden units and an output layer that uses the
   identity activation function implements a powerful form of nonlinear
   regression. specifically, the network can predict continuous target
   values using a linear combination of signals that arise from one or
   more layers of nonlinear transformations of the input.

   the derivative of g_{\text{linear}} ,   g'_{\text{linear}} ,  is
   simply 1, in the case of 1d inputs. for vector inputs of length d the
   gradient is \vec{1}^{1 x d} , a vector of ones of length d .

the logistic sigmoid activation function

   another function that is often used as the output activation function
   for binary classification problems (i.e. outputs values that range (0,
   1) ), is the logistic sigmoid. the logistic sigmoid has the following
   form:

   \large{\begin{array}{rcl} g_{\text{logistic}}(z) = \frac{1}{1 +
   e^{-z}}\end{array}}

   (figure 1, blue curves) and outputs values that range (0, 1) . the
   logistic sigmoid is motivated somewhat by biological neurons and can be
   interpreted as the id203 of an artificial neuron    firing    given
   its inputs. (it turns out that the logistic sigmoid can also be derived
   as the maximum likelihood solution to for [25]id28 in
   statistics). calculating the derivative of the logistic sigmoid
   function makes use of the quotient rule and a clever trick that both
   adds and subtracts a one from the numerator:

   \large{\begin{array}{rcl} g'_{\text{logistic}}(z) &=&
   \frac{\partial}{\partial z} \left ( \frac{1}{1 + e^{-z}}\right ) \\ &=&
   \frac{e^{-z}}{(1 + e^{-z})^2} \text{(chain rule)} \\ &=& \frac{1 +
   e^{-z} - 1}{(1 + e^{-z})^2} \\ &=& \frac{1 + e^{-z}}{(1 + e^{-z})^2} -
   \left ( \frac{1}{1+e^{-z}} \right )^2 \\ &=& \frac{1}{(1 + e^{-z})} -
   \left ( \frac{1}{1+e^{-z}} \right )^2 \\ &=& g_{\text{logistic}}(z)-
   g_{\text{logistic}}(z)^2 \\ &=& g_{\text{logistic}}(z)(1 -
   g_{\text{logistic}}(z)) \end{array}}

   here we see that g'_{logistic}(z) evaluated at z is simply
   g_{logistic}(z) weighted by 1-minus- g_{logistic}(z) . this turns out
   to be a convenient form for efficiently calculating gradients used
   in neural networks: if one keeps in memory the feed-forward activations
   of the logistic function for a given layer, the gradients for that
   layer can be evaluated using simple multiplication and subtraction
   rather than performing any re-evaluating the sigmoid function, which
   requires extra exponentiation.

the hyperbolic tangent activation function

   though the logistic sigmoid has a nice biological interpretation, it
   turns out that the logistic sigmoid can cause a neural network to get
      stuck    during training. this is due in part to the fact that if a
   strongly-negative input is provided to the logistic sigmoid, it outputs
   values very near zero. since neural networks use the
   feed-forward activations to calculate parameter gradients (again, see
   this [26]previous post for details), this can result in model
   parameters that are updated less regularly than we would like, and are
   thus    stuck    in their current state.

   an alternative to the logistic sigmoid is the hyperbolic tangent, or
   tanh function (figure 1, green curves):

   \large{\begin{array}{rcl} g_{\text{tanh}}(z) &=&
   \frac{\text{sinh}(z)}{\text{cosh}(z)} \\ &=& \frac{\mathrm{e}^z -
   \mathrm{e}^{-z}}{\mathrm{e}^z + \mathrm{e}^{-z}}\end{array}} .

   like the logistic sigmoid, the tanh function is also sigmoidal
   (   s   -shaped), but instead outputs values that range (-1, 1) . thus
   strongly negative inputs to the tanh will map to negative outputs.
   additionally, only zero-valued inputs are mapped to near-zero outputs.
   these properties make the network less likely to get    stuck    during
   training. calculating the gradient for the tanh function also uses the
   quotient rule:

   \large{\begin{array}{rcl} g'_{\text{tanh}}(z) &=&
   \frac{\partial}{\partial z} \frac{\text{sinh}(z)}{\text{cosh}(z)} \\
   &=& \frac{\frac{\partial}{\partial z} \text{sinh}(z) \times
   \text{cosh}(z) - \frac{\partial}{\partial z} \text{cosh}(z) \times
   \text{sinh}(z)}{\text{cosh}^2(z)} \\ &=& \frac{\text{cosh}^2(z) -
   \text{sinh}^2(z)}{\text{cosh}^2(z)} \\ &=& 1 -
   \frac{\text{sinh}^2(z)}{\text{cosh}^2(z)} \\ &=& 1 -
   \text{tanh}^2(z)\end{array}}

   similar to the derivative for the logistic sigmoid, the derivative of
   g_{\text{tanh}}(z) is a function of feed-forward activation evaluated
   at z , namely (1-g_{\text{tanh}}(z)^2) . thus the same caching trick
   can be used for layers that implement tanh id180.

wrapping up

   in this post we reviewed a few commonly-used id180 in
   neural network literature and their derivative calculations. these
   id180 are motivated by biology and/or provide some handy
   implementation tricks like calculating derivatives using cached
   feed-forward activation values. note that there are also many other
   options for id180 not covered here: e.g. rectification,
   soft rectification, polynomial kernels, etc. indeed, finding and
   evaluating novel id180 is an active subfield of machine
   learning research. however, the three basic activations covered here
   can be used to solve a majority of the machine learning problems one
   will likely face.
   advertisements

share this:

     * [27]twitter
     * [28]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [29]view all posts by dustinstansbury   

   posted on september 8, 2014, in [30]classification, [31]derivations,
   [32]machine learning, [33]neural networks, [34]regression and tagged
   [35]id26, [36]id26 algorithm, [37]logistic
   sigmoid, [38]neural networks, [39]quotient rule, [40]tanh function.
   bookmark the [41]permalink. [42]9 comments.
   [43]    derivation: error id26 & id119 for
   neural networks
   [44]a gentle introduction to id158s    
     * [45]leave a comment
     * [46]trackbacks 2
     * [47]comments 7

    1. james | [48]june 7, 2015 at 5:16 am
       it was very helpful! thanks!
       what about softmax function?
       [49]reply
    2. james | [50]february 4, 2016 at 12:54 am
       related:
       [51]https://brenocon.com/blog/2013/10/tanh-is-a-rescaled-logistic-s
       igmoid-function/
       [52]reply
    3. [53]khang pham | [54]march 6, 2017 at 7:13 pm
       thank you for this great explanation!
       [55]reply
    4. pradee | [56]march 23, 2017 at 5:52 am
       i really very good what book do you follow ian livingworth??
       [57]reply
    5. [58]onur boyar | [59]september 8, 2017 at 7:27 am
       thanks for great explanation!
       [60]reply
    6. vinay kumar r | [61]october 17, 2017 at 4:50 am
       video tutorial on id180:
       [62]https://quickkt.com/tutorials/artificial-intelligence/deep-lear
       ning/activation-function/
       [63]reply
    7. ats | [64]march 23, 2018 at 2:46 pm
       thanks a lot.
       [65]reply

    1. pingback: [66]a gentle introduction to id158s |
       the clever machine
    2. pingback: [67]a gentle introduction to image recognition by
       convolutional neural network     sopra steria analytics sweden

leave a reply [68]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [69]googleplus-sign-in

     *
     *

   [70]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [71]log out /
   [72]change )
   google photo

   you are commenting using your google account. ( [73]log out /
   [74]change )
   twitter picture

   you are commenting using your twitter account. ( [75]log out /
   [76]change )
   facebook photo

   you are commenting using your facebook account. ( [77]log out /
   [78]change )
   [79]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [80]algorithms [81]classification [82]id174
       [83]density estimation [84]derivations [85]id171
       [86]fmri [87]id119 [88]latex [89]machine learning
       [90]matlab [91]maximum likelihood [92]mcmc [93]neural networks
       [94]neuroscience [95]optimization [96]proofs [97]regression
       [98]sampling [99]sampling methods [100]simulations [101]statistics
       [102]theory [103]tips & tricks [104]uncategorized
     * recent posts
          + [105]derivation: maximum likelihood for id82s
          + [106]a gentle introduction to id158s
          + [107]derivation: derivatives for common neural network
            id180
          + [108]derivation: error id26 & id119 for
            neural networks
          + [109]model selection: underfitting, overfitting, and the
            id160
          + [110]supplemental proof 1
          + [111]the statistical whitening transform
          + [112]covariance matrices and data distributions
          + [113]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [114]derivation: the covariance matrix of an ols estimator
            (and applications to gls)
     * archives
          + [115]september 2014
          + [116]april 2013
          + [117]march 2013
          + [118]january 2013
          + [119]december 2012
          + [120]november 2012
          + [121]october 2012
          + [122]september 2012
          + [123]march 2012
          + [124]february 2012
          + [125]january 2012
     * meta
          + [126]register
          + [127]log in
          + [128]entries rss
          + [129]comments rss
          + [130]wordpress.com
       advertisements

   [131]blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [132]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [133]cookie policy

   iframe: [134]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/feed/
   4. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
   5. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#access
  11. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#main
  12. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#sidebar
  13. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#sidebar2
  14. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  21. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  22. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. https://theclevermachine.files.wordpress.com/2014/09/nnet-error-functions2.png
  25. http://en.wikipedia.org/wiki/logistic_regression
  26. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  27. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?share=twitter
  28. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?share=facebook
  29. https://theclevermachine.wordpress.com/author/dustinstansbury/
  30. https://theclevermachine.wordpress.com/category/algorithms/classification/
  31. https://theclevermachine.wordpress.com/category/derivations/
  32. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  33. https://theclevermachine.wordpress.com/category/neural-networks/
  34. https://theclevermachine.wordpress.com/category/algorithms/regression/
  35. https://theclevermachine.wordpress.com/tag/id26/
  36. https://theclevermachine.wordpress.com/tag/id26-algorithm/
  37. https://theclevermachine.wordpress.com/tag/logistic-sigmoid/
  38. https://theclevermachine.wordpress.com/tag/neural-networks/
  39. https://theclevermachine.wordpress.com/tag/quotient-rule/
  40. https://theclevermachine.wordpress.com/tag/tanh-function/
  41. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  42. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comments
  43. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  44. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  45. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#respond
  46. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#trackbacks
  47. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comments
  48. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-333
  49. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?replytocom=333#respond
  50. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-574
  51. https://brenocon.com/blog/2013/10/tanh-is-a-rescaled-logistic-sigmoid-function/
  52. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?replytocom=574#respond
  53. https://plus.google.com/111377040041608527434
  54. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-913
  55. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?replytocom=913#respond
  56. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-926
  57. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?replytocom=926#respond
  58. http://boyaronur.blogspot.com/
  59. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-1095
  60. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?replytocom=1095#respond
  61. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-1164
  62. https://quickkt.com/tutorials/artificial-intelligence/deep-learning/activation-function/
  63. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?replytocom=1164#respond
  64. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-1339
  65. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/?replytocom=1339#respond
  66. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  67. http://soprasteriaanalytics.se/2018/04/13/a-gentle-introduction-to-image-recognition-by-convolutional-neural-network/
  68. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#respond
  69. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  70. https://gravatar.com/site/signup/
  71. javascript:highlandercomments.doexternallogout( 'wordpress' );
  72. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  73. javascript:highlandercomments.doexternallogout( 'googleplus' );
  74. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  75. javascript:highlandercomments.doexternallogout( 'twitter' );
  76. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  77. javascript:highlandercomments.doexternallogout( 'facebook' );
  78. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  79. javascript:highlandercomments.cancelexternalwindow();
  80. https://theclevermachine.wordpress.com/category/algorithms/
  81. https://theclevermachine.wordpress.com/category/algorithms/classification/
  82. https://theclevermachine.wordpress.com/category/data-preprocessing/
  83. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  84. https://theclevermachine.wordpress.com/category/derivations/
  85. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
  86. https://theclevermachine.wordpress.com/category/fmri/
  87. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  88. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
  89. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  90. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
  91. https://theclevermachine.wordpress.com/category/maximum-likelihood/
  92. https://theclevermachine.wordpress.com/category/mcmc/
  93. https://theclevermachine.wordpress.com/category/neural-networks/
  94. https://theclevermachine.wordpress.com/category/neuroscience/
  95. https://theclevermachine.wordpress.com/category/optimization/
  96. https://theclevermachine.wordpress.com/category/proofs/
  97. https://theclevermachine.wordpress.com/category/algorithms/regression/
  98. https://theclevermachine.wordpress.com/category/algorithms/sampling/
  99. https://theclevermachine.wordpress.com/category/sampling-methods/
 100. https://theclevermachine.wordpress.com/category/simulations/
 101. https://theclevermachine.wordpress.com/category/statistics/
 102. https://theclevermachine.wordpress.com/category/theory/
 103. https://theclevermachine.wordpress.com/category/tips-tricks/
 104. https://theclevermachine.wordpress.com/category/uncategorized/
 105. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
 106. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 107. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
 108. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
 109. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
 110. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
 111. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
 112. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
 113. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
 114. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
 115. https://theclevermachine.wordpress.com/2014/09/
 116. https://theclevermachine.wordpress.com/2013/04/
 117. https://theclevermachine.wordpress.com/2013/03/
 118. https://theclevermachine.wordpress.com/2013/01/
 119. https://theclevermachine.wordpress.com/2012/12/
 120. https://theclevermachine.wordpress.com/2012/11/
 121. https://theclevermachine.wordpress.com/2012/10/
 122. https://theclevermachine.wordpress.com/2012/09/
 123. https://theclevermachine.wordpress.com/2012/03/
 124. https://theclevermachine.wordpress.com/2012/02/
 125. https://theclevermachine.wordpress.com/2012/01/
 126. https://wordpress.com/start?ref=wplogin
 127. https://theclevermachine.wordpress.com/wp-login.php
 128. https://theclevermachine.wordpress.com/feed/
 129. https://theclevermachine.wordpress.com/comments/feed/
 130. https://wordpress.com/
 131. https://wordpress.com/?ref=footer_blog
 132. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
 133. https://automattic.com/cookies
 134. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 136. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-form-guest
 137. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-form-load-service:wordpress.com
 138. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-form-load-service:twitter
 139. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/#comment-form-load-service:facebook
