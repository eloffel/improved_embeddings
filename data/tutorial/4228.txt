   #[1]the clever machine    feed [2]the clever machine    comments feed
   [3]the clever machine    derivation: error id26 & gradient
   descent for neural networks comments feed [4]model selection:
   underfitting, overfitting, and the id160
   [5]derivation: derivatives for common neural network activation
   functions [6]alternate [7]alternate [8]the clever machine
   [9]wordpress.com

     * [10]skip to navigation
     * [11]skip to main content
     * [12]skip to primary sidebar
     * [13]skip to secondary sidebar
     * [14]skip to footer

   [15]

the clever machine

topics in computational neuroscience & machine learning

     * [16]home
     * [17]about the author
     * [18]about the clever machine
     * [19]blog interface

   [20]    model selection: underfitting, overfitting, and the
   id160
   [21]derivation: derivatives for common neural network
   id180    

derivation: error id26 & id119 for neural networks

   [22]sep 6

   posted by [23]dustinstansbury

introduction

   id158s (anns) are a powerful class of models used
   for nonid75 and classification tasks that are motivated by
   biological neural computation. the general idea behind anns is pretty
   straightforward: map some input onto a desired target value using a
   distributed cascade of nonlinear transformations (see figure 1).
   however, for many, myself included, the learning algorithm used to
   train anns can be difficult to get your head around at first. in this
   post i give a step-by-step walk-through of the derivation of gradient
   descent learning algorithm commonly used to train anns (aka the
   id26 algorithm) and try to provide some high-level insights
   into the computations being performed during learning.
   [24]id158

   figure 1: diagram of an id158 with one hidden layer


some background and notation

   an ann consists of an input layer, an output layer, and any number
   (including zero) of hidden layers situated between the input and output
   layers. figure 1 diagrams an ann with a single hidden layer. the
   feed-forward computations performed by the ann are as follows: the
   signals from the input layer a_i are multiplied by a set of
   fully-connected weights w_{ij} connecting the input layer to the hidden
   layer. these weighted signals are then summed and combined with a bias
   b_i (not displayed in the graphical model in figure 1). this
   calculation forms the pre-activation signal z_j = b_j + \sum_i a_i
   w_{ij} for the hidden layer. the pre-activation signal is then
   transformed by the hidden layer activation function g_j to form the
   feed-forward activation signals leaving leaving the hidden layer a_j .
   in a similar fashion, the hidden layer activation signals a_j  are
   multiplied by the weights connecting the hidden layer to the output
   layer w_{jk} , a bias b_k is added, and the resulting signal is
   transformed by the output activation function g_k to form the network
   output a_k . the output is then compared to a desired target t_k and
   the error between the two is calculated.

   training a neural network involves determining the set of parameters
   \theta = \{\mathbf{w},\mathbf{b}\} that minimize the errors that the
   network makes. often the choice for the error function is the sum of
   the squared difference between the target values t_k and the network
   output a_k (for more detail on this choice of error function [25]see):

   \large{\begin{array}{rcl} e &=& \frac{1}{2} \sum_{k \in k}(a_k - t_k)^2
   \end{array}}

   equation (1)

   this problem can be solved using id119, which requires
   determining \frac{\partial e}{\partial \theta} for all \theta in the
   model. note that, in general, there are two sets of parameters: those
   parameters that are associated with the output layer (i.e. \theta_k =
   \{w_{jk}, b_k\} ), and thus directly affect the network output error;
   and the remaining parameters that are associated with the hidden
   layer(s), and thus affect the output error indirectly.

   before we begin, let   s define the notation that will be used in
   remainder of the derivation. please refer to figure 1 for any
   clarification.
     * {z_j} : input to node j for layer l
     * {g_j} : activation function for node j in layer l (applied to {z_j}
       )
     * a_j=g_j(z_j) : ouput/activation of node j in layer l
     * {w_{ij}} : weights connecting node i in layer (l-1) to node j in
       layer l
     * {b_{j}} : bias for unit j in layer l
     * {t_{k}} : target value for node k in the output layer

gradients for output layer weights

output layer connection weights, w_{jk}

   since the output layer parameters directly affect the value of the
   error function, determining the gradients for those parameters
   is fairly straight-forward:

   \large{\begin{array}{rcl} \frac{\partial e }{\partial w_{jk}} &=&
   \frac{1}{2} \sum_{k \in k}(a_k - t_k)^2 \\ &=& (a_k -
   t_k)\frac{\partial}{\partial w_{jk}}(a_k - t_k) \end{array}}

   equation (2)

   here, we   ve used the [26]chain rule. (also notice that the summation
   disappears in the derivative. this is because when we take the partial
   derivative with respect to the j -th dimension/node, the only term that
   survives in the error gradient is j -th, and thus we can ignore the
   remaining terms in the summation). the derivative with respect to t_k
   is zero because it does not depend on w_{jk} . also, we note that a_k =
   g(z_k) . thus

   \large{\begin{array}{rcl}\frac{\partial e }{\partial w_{jk}} &=& (a_k -
   t_k)\frac{\partial}{\partial w_{jk}}a_k \\ &=& (a_k -
   t_k)\frac{\partial}{\partial w_{jk}}g_k(z_k) \\ &=& (a_k -
   t_k)g_k'(z_k)\frac{\partial}{\partial w_{jk}}z_k, \end{array}}

   equation (3)

   where, again we use the chain rule. now, recall that z_k = b_j + \sum_j
   g_j(z_j)w_{jk} and thus \frac{\partial z_{k}}{\partial w_{jk}} =
   g_j(z_j) = a_j , giving:

   \large{\begin{array}{rcl} \frac{\partial e }{\partial w_{jk}} &=& (a_k
   - t_k)g_k'(z_k)a_j \end{array}}

   equation (4)

   the gradient of the error function with respect to the output layer
   weights is a product of three terms. the first term is the difference
   between the network output and the target value t_k . the second term
   is the derivative of output layer activation function. and the third
   term is the activation output of node j in the hidden layer.

   if we define \delta_k to be all the terms that involve index k:

   \large{\begin{array}{rcl} \delta_k &=& (a_k - t_k)g_k'(z_k)\end{array}}

   we obtain the following expression for the derivative of the error with
   respect to the output weights w_{jk} :

   \large{\begin{array}{rcl} \frac{\partial e }{\partial w_{jk}} =
   \delta_k a_j \end{array}}

   equation (5)

   here the \delta_k terms can be interpreted as the network output error
   after being back-propagated through the output activation function,
   thus creating an error    signal   . loosely speaking, equation (5) can be
   interpreted as determining how much each w_{jk} contributes to the
   error signal by weighting the error signal by the magnitude of the
   output activation from the previous (hidden) layer associated with each
   weight (see figure 1). the gradients with respect to each parameter
   are thus considered to be the    contribution    of the parameter to the
   error signal and should be negated during learning. thus the output
   weights are updated as w_{jk}\leftarrow w_{jk} - \eta \frac{\partial e
   }{\partial w_{jk}} , where \eta is some step size (   learning rate   )
   along the negative gradient.

   as we   ll see shortly, the process of backpropagating the error signal
   can iterate all the way back to the input layer by
   successively projecting \delta_k back through  w_{jk} , then through
   the activation function for the hidden layer via g'_j to give the error
   signal \delta_j , and so on. this id26 concept is central to
   training neural networks with more than one layer.

output layer biases, \large{b_{k}}

   as far as the gradient with respect to the output layer biases, we
   follow the same routine as above for w_{jk} . however, the third term
   in equation (3) is \frac{\partial}{\partial b_k} z_k =
   \frac{\partial}{\partial b_k} \left[ b_k + \sum_j g_j(z_j)\right] = 1 ,
   giving the following gradient for the output biases:

   \large{\begin{array}{rcl} \frac{\partial e }{\partial b_k} &=& (a_k -
   t_k)g_k'(z_k)(1) \\ &=& \delta_k \end{array}}

   equation (6)

   thus the gradient for the biases is simply the back-propagated error
   from the output units. one interpretation of this is that the biases
   are weights on activations that are always equal to one, regardless of
   the feed-forward signal. thus the bias gradients aren   t affected by the
   feed-forward signal, only by the error.


gradients for hidden layer weights

   due to the indirect affect of the hidden layer on the output error,
   calculating the gradients for the hidden layer weights w_{ij}  is
   somewhat more involved. however, the process starts just the same:

   \large{\begin{array}{rcl} \frac{\partial e }{\partial
   w_{ij}}&=&\frac{1}{2} \sum_{k \in k}(a_k - t_k)^2 \\ &=& \sum_{k \in k}
   (a_k - t_k) \frac{\partial}{\partial w_{ij}}a_k \end{array}}

   notice here that the sum does not disappear because, due to the fact
   that the layers are fully connected, each of the hidden unit outputs
   affects the state of each output unit. continuing on, noting that a_k =
   g_k(z_k)    

   \large{\begin{array}{rcl} \frac{\partial e }{\partial w_{ij}}&=&
   \sum_{k \in k} (a_k - t_k) \frac{\partial }{\partial w_{ij}}g_k(z_k) \\
   &=& \sum_{k \in k} (a_k - t_k)g'_k(z_k)\frac{\partial }{\partial
   w_{ij}}z_k \end{array}}

   equation (7)

   here, again we use the chain rule. ok, now here   s where things get
      slightly more involved   . notice that the partial derivative in the
   third term in equation (7) is with respect to w_{ij} , but the target
   z_j is a function of index j . how the heck do we deal with that!?
   well, if we expand z_k , we find that it is composed of other sub
   functions (also see figure 1):

   \large{\begin{array}{rcl} z_k &=& b_k + \sum_j a_jw_{jk} \\ &=& b_k +
   \sum_j g_j(z_j)w_{jk} \\ &=& b_k + \sum_j g_j(b_i + \sum_i z_i
   w_{ij})w_{jk}\end{array}}

   equation (8)

   from the last term in equation (8) we see that z_k  is indirectly
   dependent on w_{ij} .  equation (8) also suggests that we can use the
   chain rule to calculate \frac{\partial z_k }{\partial w_{ij}} . this is
   probably the trickiest part of the derivation, and goes like   

   \large{\begin{array}{rcl} \frac{\partial z_k }{\partial w_{ij}} &=&
   \frac{\partial z_k}{\partial a_j}\frac{\partial a_j}{\partial w_{ij}}
   \\ &=& \frac{\partial}{\partial a_j}a_jw_{jk}\frac{\partial
   a_j}{\partial w_{ij}} \\ &=& w_{jk}\frac{\partial a_j}{\partial w_{ij}}
   \\ &=& w_{jk}\frac{\partial g_j(z_j)}{\partial w_{ij}} \\ &=&
   w_{jk}g_j'(z_j)\frac{\partial z_j}{\partial w_{ij}} \\ &=&
   w_{jk}g_j'(z_j)\frac{\partial}{\partial w_{ij}}(b_i + \sum_i a_i
   w_{ij}) \\ &=& w_{jk}g_j'(z_j)a_i \end{array}}

   equation (9)

   now, plugging equation (9) into z_k in equation (7) gives the following
   for \frac{\partial e}{\partial w_{ij}} :

   \large{\begin{array}{rcl} \frac{\partial e }{\partial w_{ij}}&=&
   \sum_{k \in k} (a_k - t_k)g'_k(z_k)w_{jk} g'_j(z_j)a_i \\ &=&
   g'_j(z_j)a_i \sum_{k \in k} (a_k - t_k)g'_k(z_k)w_{jk} \\ &=& a_i
   g'_j(z_j) \sum_{k \in k} \delta_k w_{jk} \end{array}}

   equation (10)

   notice that the gradient for the hidden layer weights has a similar
   form to that of the gradient for the output layer weights. namely the
   gradient is some term weighted by the output activations from the layer
   below ( a_i ). for the output weight gradients, the term that was
   weighted by a_j was the back-propagated error signal \delta_k (i.e.
   equation (5)). here, the weighted term includes \delta_k , but the
   error signal is further projected onto w_{jk} and then weighted by the
   derivative of hidden layer activation function g'_j . thus, the
   gradient for the hidden layer weights is simply the output error signal
   backpropagated to the hidden layer, then weighted by the input to the
   hidden layer. to make this idea more explicit, we can define the
   resulting error signal backpropagated to layer j as \delta_j , and
   includes all terms in equation (10) that involve index j . this
   definition results in the following gradient for the hidden unit
   weights:

   \large{\begin{array}{rcl} \frac{\partial e }{\partial w_{ij}}&=& a_i
   g'_j(z_j) \sum_{k \in k} \delta_k w_{jk} \\ &=& \delta_j a_i \\
   \text{where} \\ \delta_j &=& g'_j(z_j) \sum_{k \in k} \delta_k w_{jk}
   \end{array}}

   equation (11)

   this suggests that in order to calculate the weight gradients at any
   layer l in an arbitrarily-deep neural network, we simply need to
   calculate the backpropagated error signal that reaches that layer
   \delta_l and weight it by the feed-forward signal a_{l-1} feeding into
   that layer! analogously, the gradient for the hidden layer weights can
   be interpreted as a proxy for the    contribution    of the weights to the
   output error signal, which can only be observed   from the point of view
   of the weights   by backpropagating the error signal to the hidden layer.

output layer biases, \large{w_{ij}}

   calculating the gradients for the hidden layer biases follows a very
   similar procedure to that for the hidden layer weights where, as in
   equation (9), we use the chain rule to calculate \frac{\partial
   z_k}{\partial b_i} . however, unlike equation (9) the third term that
   results for the biases is slightly different:

   \large{\begin{array}{rcl} \frac{\partial z_k }{\partial b_i} &=&
   w_{jk}g_j'(z_j)\frac{\partial z_j}{\partial b_i} \\ &=&
   w_{jk}g_j'(z_j)\frac{\partial}{\partial b_i}(b_i + \sum_i a_i w_{ij})
   \\ &=& w_{jk}g_j'(z_j)(1), \\ \text{giving} \\ \frac{\partial e
   }{\partial b_i}&=& g'_j(z_j) \sum_{k \in k} \delta_k w_{jk} \\ &=&
   \delta_j \end{array}}

   equation (12)

   in a similar fashion to calculation of the bias gradients for the
   output layer, the gradients for the hidden layer biases are simply the
   backpropagated error signal reaching that layer. this suggests that we
   can also calculate the bias gradients at any layer l in an
   arbitrarily-deep network by simply calculating the backpropagated error
   signal reaching that layer \delta_l !

wrapping up

   in this post we went over some of the formal details of the
   id26 learning algorithm. the math covered in this post
   allows us to train arbitrarily deep neural networks by re-applying the
   same basic computations. those computations are:
    1. calculated the feed-forward signals from the input to the output.
    2. calculate output error e  based on the predictions a_k  and the
       target t_k
    3. backpropagate the error signals by weighting it by the weights in
       previous layers and the gradients of the associated activation
       functions
    4. calculating the gradients \frac{\partial e}{\partial \theta} for
       the parameters based on the backpropagated error signal and the
       feedforward signals from the inputs.
    5. update the parameters using the calculated gradients \theta
       \leftarrow \theta - \eta\frac{\partial e}{\partial \theta}

   the only real constraints on model construction is ensuring that
   the error function e and the id180 g_l are
   differentiable. for more details on implementing anns and seeing them
   at work, stay tuned for the next post.
   advertisements

share this:

     * [27]twitter
     * [28]facebook
     *

like this:

   like loading...

related

about dustinstansbury

   i recently received my phd from uc berkeley where i studied
   computational neuroscience and machine learning.
   [29]view all posts by dustinstansbury   

   posted on september 6, 2014, in [30]algorithms, [31]classification,
   [32]derivations, [33]id119, [34]machine learning, [35]neural
   networks, [36]optimization, [37]regression, [38]theory and tagged
   [39]backprop derivation, [40]id26 algorithm,
   [41]id26 derivation, [42]derivation, [43]machine learning,
   [44]neural networks. bookmark the [45]permalink. [46]15 comments.
   [47]    model selection: underfitting, overfitting, and the
   id160
   [48]derivation: derivatives for common neural network
   id180    
     * [49]leave a comment
     * [50]trackbacks 4
     * [51]comments 11

    1. dafeda | [52]march 31, 2015 at 1:18 am
       hi, this is the first write-up on id26 i actually
       understand. thanks.
       a few possible bugs:
       1. last part of eq.8 should i think sum over a_i and not z_i.
       2. between eq.3 and eq.4 it should i think be z_k=b_k +     and not
       z_k=b_j    
       3. last section says output layer bias while the derivation is for
       hidden layer bias. also,
       b_i seems to be used as the notation for hidden layer bias while it
       should be b_j.
       all in all, a very helpful post.
       [53]reply
    2. dafeda | [54]march 31, 2015 at 1:19 am
       reblogged this on [55]dafeda's blog and commented:
       the easiest to follow derivation of id26 i   ve come
       across.
       [56]reply
    3. ayan das | [57]july 4, 2015 at 9:46 am
       probably the best derivation of backprop i   ve ever seen on internet
           
       [58]reply
    4. devin | [59]august 12, 2015 at 12:08 pm
       thanks. nice clean explanation.
       [60]reply
    5. arnab kanti kar | [61]august 28, 2015 at 10:33 am
       thank you !
       second time benefited from your blog ..
       [62]reply
    6. donghao liu | [63]february 17, 2016 at 5:45 pm
       best introduction about back prop ever!
       thank you so much.
       [64]reply
    7. [65]mysticprince93 | [66]january 27, 2017 at 6:51 am
       really useful! though there are a few typos, as dafeda has
       mentioned.
       [67]reply
    8. [68]unpracticalconsiderations | [69]june 22, 2017 at 8:32 pm
       really helpful man,
       i just have one small question im hopeing somebody can answer..
       i understand this algebraically, and i understand the iterative
       patterns created with the deltas when calculating the weights from
       different layers starting backwards,
       but why does (a_k     t_k) * the derivative mean that the error
       (which is equal to a_k     t_k) is being    back propagated   . what   s
       the intuition behind multiplying by the derivative which makes us
       saying this.
       ?
       [70]reply
    9. ugenteraan | [71]july 31, 2017 at 12:34 pm
       this really cleared up all the confusions that i had in
       id26. thanks a bunch !
       [72]reply
   10. [73]pradip nichite | [74]august 19, 2017 at 2:58 am
       thank you              dustinstansbury. finally, i understood back
       propagation.
       [75]reply
   11. edmund | [76]december 3, 2017 at 6:20 pm
       25 years ago i had these formulae in my phd, but i couldn   t
       retrieve a copy, luckily i found your blog (true story) and your
       very clear exposition refreshed my memory.
       [77]reply

    1. pingback: [78]derivation: derivatives for common neural network
       id180 | the clever machine
    2. pingback: [79]a gentle introduction to id158s |
       the clever machine
    3. pingback: [80]some sites i found helpful in reviewing backprop    
       into dl and beyond
    4. pingback: [81]derivation: error id26 & id119
       for neural networks     collection of dev articles

leave a reply [82]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [83]googleplus-sign-in

     *
     *

   [84]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [85]log out /
   [86]change )
   google photo

   you are commenting using your google account. ( [87]log out /
   [88]change )
   twitter picture

   you are commenting using your twitter account. ( [89]log out /
   [90]change )
   facebook photo

   you are commenting using your facebook account. ( [91]log out /
   [92]change )
   [93]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * search for: ____________________ go
     * follow theclevermachine
       to receive update notifications, enter your email here
       ____________________
       (button) follow
     * categories
       [94]algorithms [95]classification [96]id174
       [97]density estimation [98]derivations [99]id171
       [100]fmri [101]id119 [102]latex [103]machine learning
       [104]matlab [105]maximum likelihood [106]mcmc [107]neural networks
       [108]neuroscience [109]optimization [110]proofs [111]regression
       [112]sampling [113]sampling methods [114]simulations
       [115]statistics [116]theory [117]tips & tricks [118]uncategorized
     * recent posts
          + [119]derivation: maximum likelihood for id82s
          + [120]a gentle introduction to id158s
          + [121]derivation: derivatives for common neural network
            id180
          + [122]derivation: error id26 & id119 for
            neural networks
          + [123]model selection: underfitting, overfitting, and the
            id160
          + [124]supplemental proof 1
          + [125]the statistical whitening transform
          + [126]covariance matrices and data distributions
          + [127]fmri in neuroscience: efficiency of event-related
            experiment designs
          + [128]derivation: the covariance matrix of an ols estimator
            (and applications to gls)
     * archives
          + [129]september 2014
          + [130]april 2013
          + [131]march 2013
          + [132]january 2013
          + [133]december 2012
          + [134]november 2012
          + [135]october 2012
          + [136]september 2012
          + [137]march 2012
          + [138]february 2012
          + [139]january 2012
     * meta
          + [140]register
          + [141]log in
          + [142]entries rss
          + [143]comments rss
          + [144]wordpress.com
       advertisements

   [145]create a free website or blog at wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [146]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [147]cookie policy

   iframe: [148]likes-master

   %d bloggers like this:

references

   visible links
   1. https://theclevermachine.wordpress.com/feed/
   2. https://theclevermachine.wordpress.com/comments/feed/
   3. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/feed/
   4. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
   5. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/&for=wpcom-auto-discovery
   8. https://theclevermachine.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#access
  11. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#main
  12. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#sidebar
  13. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#sidebar2
  14. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#footer
  15. https://theclevermachine.wordpress.com/
  16. https://theclevermachine.wordpress.com/
  17. https://theclevermachine.wordpress.com/about-me/
  18. https://theclevermachine.wordpress.com/about-theclevermachine/
  19. https://theclevermachine.wordpress.com/interact/
  20. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
  21. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  22. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  23. https://theclevermachine.wordpress.com/author/dustinstansbury/
  24. https://theclevermachine.files.wordpress.com/2014/09/neural-net.png
  25. https://theclevermachine.wordpress.com/2012/02/13/cutting-your-losses-loss-functions-predominance-of-sum-of-squares/
  26. http://en.wikipedia.org/wiki/chain_rule
  27. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?share=twitter
  28. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?share=facebook
  29. https://theclevermachine.wordpress.com/author/dustinstansbury/
  30. https://theclevermachine.wordpress.com/category/algorithms/
  31. https://theclevermachine.wordpress.com/category/algorithms/classification/
  32. https://theclevermachine.wordpress.com/category/derivations/
  33. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
  34. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
  35. https://theclevermachine.wordpress.com/category/neural-networks/
  36. https://theclevermachine.wordpress.com/category/optimization/
  37. https://theclevermachine.wordpress.com/category/algorithms/regression/
  38. https://theclevermachine.wordpress.com/category/theory/
  39. https://theclevermachine.wordpress.com/tag/backprop-derivation/
  40. https://theclevermachine.wordpress.com/tag/id26-algorithm/
  41. https://theclevermachine.wordpress.com/tag/id26-derivation/
  42. https://theclevermachine.wordpress.com/tag/derivation/
  43. https://theclevermachine.wordpress.com/tag/machine-learning/
  44. https://theclevermachine.wordpress.com/tag/neural-networks/
  45. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  46. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comments
  47. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
  48. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  49. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#respond
  50. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#trackbacks
  51. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comments
  52. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-299
  53. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=299#respond
  54. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-300
  55. https://dafeda.wordpress.com/2015/03/31/derivation-error-id26-gradient-descent-for-neural-networks/
  56. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=300#respond
  57. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-344
  58. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=344#respond
  59. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-358
  60. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=358#respond
  61. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-363
  62. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=363#respond
  63. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-588
  64. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=588#respond
  65. http://gravatar.com/mysticprince93
  66. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-870
  67. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=870#respond
  68. http://unpracticalconsiderations.wordpress.com/
  69. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-1009
  70. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=1009#respond
  71. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-1063
  72. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=1063#respond
  73. https://plus.google.com/105459117184166267735
  74. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-1076
  75. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=1076#respond
  76. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-1216
  77. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/?replytocom=1216#respond
  78. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
  79. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
  80. https://intodlandbeyond.wordpress.com/2017/09/21/some-sites-i-found-helpful-in-reviewing-backprop/
  81. https://todaydev.wordpress.com/2018/01/06/derivation-error-id26-gradient-descent-for-neural-networks/
  82. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#respond
  83. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://theclevermachine.wordpress.com&color_scheme=light
  84. https://gravatar.com/site/signup/
  85. javascript:highlandercomments.doexternallogout( 'wordpress' );
  86. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  87. javascript:highlandercomments.doexternallogout( 'googleplus' );
  88. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  89. javascript:highlandercomments.doexternallogout( 'twitter' );
  90. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  91. javascript:highlandercomments.doexternallogout( 'facebook' );
  92. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
  93. javascript:highlandercomments.cancelexternalwindow();
  94. https://theclevermachine.wordpress.com/category/algorithms/
  95. https://theclevermachine.wordpress.com/category/algorithms/classification/
  96. https://theclevermachine.wordpress.com/category/data-preprocessing/
  97. https://theclevermachine.wordpress.com/category/algorithms/density-estimation/
  98. https://theclevermachine.wordpress.com/category/derivations/
  99. https://theclevermachine.wordpress.com/category/algorithms/feature-learning/
 100. https://theclevermachine.wordpress.com/category/fmri/
 101. https://theclevermachine.wordpress.com/category/algorithms/gradient-descent/
 102. https://theclevermachine.wordpress.com/category/tips-tricks/latex/
 103. https://theclevermachine.wordpress.com/category/algorithms/machine-learning/
 104. https://theclevermachine.wordpress.com/category/tips-tricks/matlab/
 105. https://theclevermachine.wordpress.com/category/maximum-likelihood/
 106. https://theclevermachine.wordpress.com/category/mcmc/
 107. https://theclevermachine.wordpress.com/category/neural-networks/
 108. https://theclevermachine.wordpress.com/category/neuroscience/
 109. https://theclevermachine.wordpress.com/category/optimization/
 110. https://theclevermachine.wordpress.com/category/proofs/
 111. https://theclevermachine.wordpress.com/category/algorithms/regression/
 112. https://theclevermachine.wordpress.com/category/algorithms/sampling/
 113. https://theclevermachine.wordpress.com/category/sampling-methods/
 114. https://theclevermachine.wordpress.com/category/simulations/
 115. https://theclevermachine.wordpress.com/category/statistics/
 116. https://theclevermachine.wordpress.com/category/theory/
 117. https://theclevermachine.wordpress.com/category/tips-tricks/
 118. https://theclevermachine.wordpress.com/category/uncategorized/
 119. https://theclevermachine.wordpress.com/2014/09/23/derivation-maximum-likelihood-for-boltzmann-machines/
 120. https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/
 121. https://theclevermachine.wordpress.com/2014/09/08/derivation-derivatives-for-common-neural-network-activation-functions/
 122. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
 123. https://theclevermachine.wordpress.com/2013/04/21/model-selection-underfitting-overfitting-and-the-bias-variance-tradeoff/
 124. https://theclevermachine.wordpress.com/2013/04/21/supplemental-proof-1/
 125. https://theclevermachine.wordpress.com/2013/03/30/the-statistical-whitening-transform/
 126. https://theclevermachine.wordpress.com/2013/03/29/covariance-matrices-and-data-distributions/
 127. https://theclevermachine.wordpress.com/2013/01/14/fmri-in-neuroscience-efficiency-of-event-related-experiment-designs/
 128. https://theclevermachine.wordpress.com/2013/01/14/derivation-the-covariance-matrix-of-an-ols-estimator-and-applications-to-gls/
 129. https://theclevermachine.wordpress.com/2014/09/
 130. https://theclevermachine.wordpress.com/2013/04/
 131. https://theclevermachine.wordpress.com/2013/03/
 132. https://theclevermachine.wordpress.com/2013/01/
 133. https://theclevermachine.wordpress.com/2012/12/
 134. https://theclevermachine.wordpress.com/2012/11/
 135. https://theclevermachine.wordpress.com/2012/10/
 136. https://theclevermachine.wordpress.com/2012/09/
 137. https://theclevermachine.wordpress.com/2012/03/
 138. https://theclevermachine.wordpress.com/2012/02/
 139. https://theclevermachine.wordpress.com/2012/01/
 140. https://wordpress.com/start?ref=wplogin
 141. https://theclevermachine.wordpress.com/wp-login.php
 142. https://theclevermachine.wordpress.com/feed/
 143. https://theclevermachine.wordpress.com/comments/feed/
 144. https://wordpress.com/
 145. https://wordpress.com/?ref=footer_website
 146. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/
 147. https://automattic.com/cookies
 148. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
 150. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-form-guest
 151. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-form-load-service:wordpress.com
 152. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-form-load-service:twitter
 153. https://theclevermachine.wordpress.com/2014/09/06/derivation-error-id26-gradient-descent-for-neural-networks/#comment-form-load-service:facebook
