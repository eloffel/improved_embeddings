   #[1]openai

     * [2]about
     * [3]progress
     * [4]resources
     * [5]blog

   (button)

   (button)

     * [6]about
         ______________________________________________________________

     * [7]progress
         ______________________________________________________________

     * [8]resources
         ______________________________________________________________

     * [9]blog
         ______________________________________________________________

     * [10]jobs
     __________________________________________________________________

   march 16, 2017     7 minute read

learning to communicate

   in this post we'll outline new openai research in which agents develop
   their own language.

   our hypothesis is that true language understanding will come from
   agents that learn words in combination with how they affect the world,
   rather than spotting patterns in a huge corpus of text. as a first
   step, we wanted to see if cooperative agents could develop a simple
   language amongst themselves.
     __________________________________________________________________

training agents to invent a language

   we've just released [11]initial results in which we teach ai agents to
   create language by dropping them into a set of simple worlds, giving
   them the ability to communicate, and then giving them goals that can be
   best achieved by communicating with other agents. if they achieve a
   goal, then they get rewarded. we train them using reinforcement
   learning and, due to careful experiment design, they develop a shared
   language to help them achieve their goals.

   our approach yields agents that invent a (simple!) language which is
   [12]grounded and [13]compositional. grounded means that words in a
   language are tied to something directly experienced by a speaker in
   their environment, for example, a speaker forming an association
   between the word "tree" and images or experiences of trees.
   compositional means that speakers can assemble multiple words into a
   sentence to represent a specific idea, such as getting another agent to
   go to a specific location.
   [multi_agent_comm_1-471605aa1baa2c145933775e1a02f63d0db4ffe11436171f109
   b96a9fdd1b5fd.png] our agents exist in a simple, 2d world, and are able
   to take actions such as moving to locations, looking at things, or
   saying things to communicate with other agents. in this picture, agent
   1 is saying something while staring at a point in the center of the
   map.

   to train the agents, we represent the experiment as a cooperative    
   rather than competitive     multi-agent id23 problem.
   the agents exist in a two-dimensional world with simple landmarks, and
   each agent has a goal. goals can vary from looking at or moving to a
   specific location, to encouraging a separate agent to move to a
   location. each agent can broadcast messages to the group. every agent's
   reward is the sum of the rewards paid out to all agents, encouraging
   collaboration.

   at each time step, our rl agents can take two kinds of actions     (i)
   environment actions, like moving around or looking at things, and (ii)
   communication actions, like broadcasting a word to all other agents.
   (note that though the agents come up with words that we found to
   correspond to objects and other agents, as well as actions like 'look
   at' or 'go to', to the agents these words are abstract symbols
   represented by [14]one-hot vector     we label these one-hot vectors with
   english words that capture their meaning for the sake of
   interpretability.) before an agent takes an action, it observes the
   communications from other agents from the previous time step as well as
   the locations of all entities and objects in the world. it stores that
   communication in a private recurrent neural network, giving it a memory
   for the words it hears.
   [env-streams-1-db0b311bf3c878dc1fd16fd222f466753c3911b031a3baecba9ae37d
   3fca9b1a.png] in the above step-by-step run, at t=0 the red agent says
   a word corresponding to the red landmark (center right), then at t=1
   says a word that is equivalent to 'goto', then in t=2 says
   'green-agent'. the green-agent hears its instructions and immediately
   moves to the red landmark.

   we use discrete communication actions (messages formed of separate,
   word-like symbols) sent over a differentiable communication channel. a
   communication channel is differentiable if it allows agents to directly
   inform each other about what message they should have sent at each time
   step, by slightly altering their messages to make a positive change in
   the reward both agents expect to receive. agents accomplish this by
   calculating the [15]gradient of future reward with respect to changes
   in the sent messages (i.e. how much rewards would change with different
   messages). for example, if one agent realizes that it could have
   performed a task better if a second agent had sent different
   information, the first agent can tell the second exactly how to modify
   its messages to make them as useful as possible. in other words, agents
   ask the question: 'how should i modify my communication output to get
   the most communal reward in the future?'.

   previous efforts achieved this sort of differentiable communication by
   having the agents send a [16]vector of real numbers or a [17]continuous
   approximation to binary values to each other, or used
   [18]non-differentiable communication and training. we use the
   [19]gumbel-softmax trick, to approximate discrete communication
   decisions with a continuous representation during training. this gets
   us the best of both worlds: during training the differentiable channel
   means agents can rapidly learn how to communicate with each other via
   using continuous representation, which at the end of training ends up
   converging on discrete outputs that are more interpretable and show
   traits like compositionality.

   in the video that follows, we show how our agents evolve languages to
   fit the complexity of their situation, with solitary agents not needing
   to communicate, two agents inventing one-word phrases to coordinate
   with each other in simple tasks, and three agents composing multiple
   words in sentences to accomplish more challenging tasks.

   iframe: [20]https://www.youtube.com/embed/livfy7zo4oa
     __________________________________________________________________

how experimental setup influences how language evolves

   all [21]research projects have complications; in this case, our agents
   frequently invented languages that didn't display the compositional
   traits we wanted. and even when they succeeded, their solutions had
   their own idiosyncrasies.

   the first problem we ran into was the agents' tendency to create a
   single utterance and intersperse it with spaces to create meaning. this
   morse code language was hard to decipher and non-compositional. to
   correct this, we imposed a slight cost on every utterance and added a
   preference for achieving the task quickly. this encouraged the agents
   to use their communication channel concisely, which led to the
   development of a larger vocabulary.

   another issue we faced was agents trying to use single words to encode
   the meaning of entire sentences. this happened when we gave them the
   ability to use large vocabularies; they'd eventually create a single
   utterance that encoded the meaning of an entire sentence such as    red
   agent, go to blue landmark   . while useful for the agents, this approach
   requires vocabulary size to grow exponentially with the sentence length
   and doesn't fit with our broader goal of creating ai that is
   interpretable to humans.) to deter agents from creating this sort of
   language we incorporated a preference for compact vocabulary sizes
   through a preference for using already-popular words, inspired by ideas
   outlined in [22]the evolution of syntactic communication. we
   incorporate this by putting a reward for speaking a particular word
   that is proportional to how frequently that word has been spoken
   previously.

   lastly, we encountered agents inventing landmark references not based
   on color, but other cues such as spatial relationships. for example,
   agents would invent words like "top-most" or "left-most" landmark to
   refer to locations based on a global 2d coordinate system. while such
   behavior is very inventive, it is fairly specific to our particular
   environment implementation, and could cause problems if we
   substantially changed the geography of the worlds the agents live in.
   to fix this, we placed agents in an ego-centric coordinate frame (so
   that there is no single shared coordinate frame). this dealt with the
   odd directions, and led to them referring to landmarks by their color
   property.
     __________________________________________________________________

can't speak? let me point the way. can't hear? let me guide you.

   this method of training also works when agents are unable to
   communicate with each other via text, and have to instead carry out
   physical actions actions in the simulated environment. in the
   animations that follow we show agents improvising in this way by
   pointing or guiding other agents to targets, or in extreme cases
   pushing sightless agents to their goal.
   [pointing-cb8c4dd7f3c849ac02fe8e7a63ebce42fe4514084e127b286c0ebbcf9dedf
   432.gif]
   [guiding-e09dcb98956df2a6682b57174c84c930a6fec4ef333604af16a37e78c810bc
   3a.gif]
   [pushing-f2d7233de9ce151de78b6ffdd718e750f8128b567a740a4ca09430a28260da
   a9.gif]
   left to right: an ai agent signals the location of the goal to another
   agent by pointing; a smaller agent guides a larger agent toward a goal;
   agents push a sightless agents toward a goal.
     __________________________________________________________________

inferred language and grounded language

   today, many people have applied machine learning to language-related
   tasks with great success. large-scale ml techniques have led to
   significant advances in translation, verbal reasoning, language
   understanding, sentence generation, and other areas. all of these
   approaches work by feeding them extremely large amounts of textual
   data, from which the systems extract features and discover patterns.
   while this work has yielded numerous inventions and innovations, it has
   drawbacks relating to the representational quality of the language that
   is learned. there's not much indication that if you train a computer on
   language in this way it will have a deep understanding of how that
   language is attached to the real world. with our research, we're trying
   to deal with this grounding problem by training our agents to invent
   language which is tied to their perception of the world.

   computers whose language models are trained without grounding are much
   like the character trapped in john searle   s [23]chinese room, where
   they compare incoming text against a kind of dictionary of semantic
   meaning which has been created through the analysis of large quantities
   of text. it's unclear how much of an idea these computers have about
   what the text represents, as they've never left this room and been able
   to interact with the world the text describes.
     __________________________________________________________________

next steps

   we hope that this research into growing a language will let us develop
   machines that have their own language tied to their own lived
   experience. we think that if we slowly increase the complexity of their
   environment, and the range of actions the agents themselves are allowed
   to take, it   s possible they   ll create an expressive language which
   contains concepts beyond the basic verbs and nouns that evolved here.

   as the complexity of this invented language increases, it's going to
   become challenging for us to make these languages interpretable by
   humans. that's why for our next project, ryan lowe and igor mordatch
   are going to investigate ways to connect the invented languages with
   english via having the agents communicate with english-speaking agents.
   this will automate the translation of their language into ours. this is
   an interdisciplinary undertaking, spanning areas of ai, linguistics,
   and cognitive science, and as part of it we'll be collaborating with
   researchers at uc berkeley. if you're interested in developing smarter
   language models, then consider [24]working at openai.
     __________________________________________________________________

for more information

   you can find out more information about the technical specifics of our
   research in this research paper: [25]emergence of grounded
   compositional language in multi-agent populations, and more about the
   motivations for it in: [26]a paradigm for situated and goal-driven
   language learning.
     __________________________________________________________________

   authors
   [27]pieter abbeel[28]igor mordatch[29]ryan lowe[30]jon gauthier[31]jack
   clark
     __________________________________________________________________

     * [32]about
     * [33]progress
     * [34]resources
     * [35]blog
     * [36]charter
     * [37]jobs
     * [38]press

     *
     *

   sign up for our newsletter
   ____________________ (button)
   right

references

   visible links
   1. https://openai.com/blog/rss/
   2. https://openai.com/about/
   3. https://openai.com/progress/
   4. https://openai.com/resources/
   5. https://openai.com/blog/
   6. https://openai.com/about/
   7. https://openai.com/progress/
   8. https://openai.com/resources/
   9. https://openai.com/blog/
  10. https://openai.com/jobs/
  11. https://arxiv.org/abs/1703.04908
  12. https://www.media.mit.edu/projects/grounded-language-learning-and-understanding/overview/
  13. https://plato.stanford.edu/entries/compositionality/
  14. https://www.quora.com/what-is-one-hot-encoding-and-when-is-it-used-in-data-science
  15. https://en.wikipedia.org/wiki/gradient
  16. https://arxiv.org/abs/1605.07736
  17. https://arxiv.org/abs/1605.06676
  18. https://arxiv.org/abs/1612.07182
  19. https://arxiv.org/abs/1611.01144
  20. https://www.youtube.com/embed/livfy7zo4oa
  21. https://openai.com/blog/faulty-reward-functions/
  22. http://www.nature.com/nature/journal/v404/n6777/full/404495a0.html
  23. https://plato.stanford.edu/entries/chinese-room/
  24. https://openai.com/jobs/
  25. https://arxiv.org/abs/1703.04908
  26. https://arxiv.org/abs/1610.03585
  27. https://openai.com/blog/authors/pieter/
  28. https://openai.com/blog/authors/igor/
  29. https://openai.com/blog/authors/ryan/
  30. https://openai.com/blog/authors/jon/
  31. https://openai.com/blog/authors/jack-clark/
  32. https://openai.com/about/
  33. https://openai.com/progress/
  34. https://openai.com/resources/
  35. https://openai.com/blog/
  36. https://openai.com/charter/
  37. https://openai.com/jobs/
  38. https://openai.com/press/

   hidden links:
  40. https://openai.com/
  41. https://openai.com/
  42. https://twitter.com/openai
  43. https://www.facebook.com/openai.research
