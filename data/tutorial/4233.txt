   #[1]github [2]recent commits to models:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]2,874
     * [35]star [36]50,805
     * [37]fork [38]31,195

[39]tensorflow/[40]models

   [41]code [42]issues 1,174 [43]pull requests 360 [44]projects 2
   [45]insights
   branch: master
   (button) create new file [46]find file [47]history
   [48]models/[49]research/skip_thoughts/
   [50]@bapfeld
   [51]bapfeld [52]prevents typeerror:'dict_keys' object does not support
   indexing
   latest commit [53]eb784c0 oct 11, 2018
   [54]permalink
   type name latest commit message commit time
   [55]..
   failed to load latest commit information.
   [56]skip_thoughts [57]prevents typeerror:'dict_keys' object does not
   support indexing oct 11, 2018
   [58].gitignore [59]move the research models into a research subfolder
   ([60]#2430[61]) sep 21, 2017
   [62]readme.md
   [63]workspace

readme.md

skip-thought vectors

   this is a tensorflow implementation of the model described in:

   jamie ryan kiros, yukun zhu, ruslan salakhutdinov, richard s. zemel,
   antonio torralba, raquel urtasun, sanja fidler. [64]skip-thought
   vectors. in nips, 2015.

contact

   code author: chris shallue

   pull requests and issues: @cshallue

contents

     * [65]model overview
     * [66]getting started
          + [67]install required packages
          + [68]download pretrained models (optional)
     * [69]training a model
          + [70]prepare the training data
          + [71]run the training script
          + [72]track training progress
     * [73]expanding the vocabulary
          + [74]overview
          + [75]preparation
          + [76]run the vocabulary expansion script
     * [77]evaluating a model
          + [78]overview
          + [79]preparation
          + [80]run the evaluation tasks
     * [81]encoding sentences

model overview

   the skip-thoughts model is a sentence encoder. it learns to encode
   input sentences into a fixed-dimensional vector representation that is
   useful for many tasks, for example to detect paraphrases or to classify
   whether a product review is positive or negative. see the
   [82]skip-thought vectors paper for details of the model architecture
   and more example applications.

   a trained skip-thoughts model will encode similar sentences nearby each
   other in the embedding vector space. the following examples show the
   nearest neighbor by cosine similarity of some sentences from the
   [83]movie review dataset.
   input sentence nearest neighbor
   simplistic, silly and tedious. trite, banal, cliched, mostly
   inoffensive.
   not so much farcical as sour. not only unfunny, but downright
   repellent.
   a sensitive and astute first feature by anne-sophie birot. absorbing
   character study by andr   turpin .
   an enthralling, entertaining feature. a slick, engrossing melodrama.

getting started

install required packages

   first ensure that you have installed the following required packages:
     * bazel ([84]instructions)
     * tensorflow ([85]instructions)
     * numpy ([86]instructions)
     * scikit-learn ([87]instructions)
     * natural language toolkit (nltk)
          + first install nltk ([88]instructions)
          + then install the nltk data ([89]instructions)
     * gensim ([90]instructions)
          + only required if you will be expanding your vocabulary with
            the [91]id97 model.

download pretrained models (optional)

   you can download model checkpoints pretrained on the [92]bookcorpus
   dataset in the following configurations:
     * unidirectional id56 encoder ("uni-skip" in the paper)
     * bidirectional id56 encoder ("bi-skip" in the paper)

# directory to download the pretrained models to.
pretrained_models_dir="${home}/skip_thoughts/pretrained/"

mkdir -p ${pretrained_models_dir}
cd ${pretrained_models_dir}

# download and extract the unidirectional model.
wget "http://download.tensorflow.org/models/skip_thoughts_uni_2017_02_02.tar.gz"
tar -xvf skip_thoughts_uni_2017_02_02.tar.gz
rm skip_thoughts_uni_2017_02_02.tar.gz

# download and extract the bidirectional model.
wget "http://download.tensorflow.org/models/skip_thoughts_bi_2017_02_16.tar.gz"
tar -xvf skip_thoughts_bi_2017_02_16.tar.gz
rm skip_thoughts_bi_2017_02_16.tar.gz

   you can now skip to the sections [93]evaluating a model and
   [94]encoding sentences.

training a model

prepare the training data

   to train a model you will need to provide training data in tfrecord
   format. the tfrecord format consists of a set of sharded files
   containing serialized tf.example protocol buffers. each tf.example
   proto contains three sentences:
     * encode: the sentence to encode.
     * decode_pre: the sentence preceding encode in the original text.
     * decode_post: the sentence following encode in the original text.

   each sentence is a list of words. during preprocessing, a dictionary is
   created that assigns each word in the vocabulary to an integer-valued
   id. each sentence is encoded as a list of integer word ids in the
   tf.example protos.

   we have provided a script to preprocess any set of text-files into this
   format. you may wish to use the [95]bookcorpus dataset. note that the
   preprocessing script may take 12 hours or more to complete on this
   large dataset.
# comma-separated list of globs matching the input input files. the format of
# the input files is assumed to be a list of newline-separated sentences, where
# each sentence is already tokenized.
input_files="${home}/skip_thoughts/bookcorpus/*.txt"

# location to save the preprocessed training and validation data.
data_dir="${home}/skip_thoughts/data"

# build the preprocessing script.
cd tensorflow-models/skip_thoughts
bazel build -c opt //skip_thoughts/data:preprocess_dataset

# run the preprocessing script.
bazel-bin/skip_thoughts/data/preprocess_dataset \
  --input_files=${input_files} \
  --output_dir=${data_dir}

   when the script finishes you will find 100 training files and 1
   validation file in data_dir. the files will match the patterns
   train-?????-of-00100 and validation-00000-of-00001 respectively.

   the script will also produce a file named vocab.txt. the format of this
   file is a list of newline-separated words where the word id is the
   corresponding 0- based line index. words are sorted by descending order
   of frequency in the input data. only the top 20,000 words are assigned
   unique ids; all other words are assigned the "unknown id" of 1 in the
   processed data.

run the training script

   execute the following commands to start the training script. by default
   it will run for 500k steps (around 9 days on a geforce gtx 1080 gpu).
# directory containing the preprocessed data.
data_dir="${home}/skip_thoughts/data"

# directory to save the model.
model_dir="${home}/skip_thoughts/model"

# build the model.
cd tensorflow-models/skip_thoughts
bazel build -c opt //skip_thoughts/...

# run the training script.
bazel-bin/skip_thoughts/train \
  --input_file_pattern="${data_dir}/train-?????-of-00100" \
  --train_dir="${model_dir}/train"

track training progress

   optionally, you can run the track_perplexity script in a separate
   process. this will log per-word perplexity on the validation set which
   allows training progress to be monitored on [96]tensorboard.

   note that you may run out of memory if you run the this script on the
   same gpu as the training script. you can set the environment variable
   cuda_visible_devices="" to force the script to run on cpu. if it runs
   too slowly on cpu, you can decrease the value of --num_eval_examples.
data_dir="${home}/skip_thoughts/data"
model_dir="${home}/skip_thoughts/model"

# ignore gpu devices (only necessary if your gpu is currently memory
# constrained, for example, by running the training script).
export cuda_visible_devices=""

# run the evaluation script. this will run in a loop, periodically loading the
# latest model checkpoint file and computing id74.
bazel-bin/skip_thoughts/track_perplexity \
  --input_file_pattern="${data_dir}/validation-?????-of-00001" \
  --checkpoint_dir="${model_dir}/train" \
  --eval_dir="${model_dir}/val" \
  --num_eval_examples=50000

   if you started the track_perplexity script, run a [97]tensorboard
   server in a separate process for real-time monitoring of training
   summaries and validation perplexity.
model_dir="${home}/skip_thoughts/model"

# run a tensorboard server.
tensorboard --logdir="${model_dir}"

expanding the vocabulary

overview

   the vocabulary generated by the preprocessing script contains only
   20,000 words which is insufficient for many tasks. for example, a
   sentence from wikipedia might contain nouns that do not appear in this
   vocabulary.

   a solution to this problem described in the [98]skip-thought vectors
   paper is to learn a mapping that transfers word representations from
   one model to another. this idea is based on the "translation matrix"
   method from the paper [99]exploiting similarities among languages for
   machine translation.

   specifically, we will load the id27s from a trained
   skip-thoughts model and from a trained [100]id97 model (which has a
   much larger vocabulary). we will train a id75 model
   without id173 to learn a linear mapping from the id97
   embedding space to the skip-thoughts embedding space. we will then
   apply the linear model to all words in the id97 vocabulary,
   yielding vectors in the skip- thoughts id27 space for the
   union of the two vocabularies.

   the id75 task is to learn a parameter matrix w to minimize
   || x - y * w ||^2, where x is a matrix of skip-thoughts embeddings of
   shape [num_words, dim1], y is a matrix of id97 embeddings of shape
   [num_words, dim2], and w is a matrix of shape [dim2, dim1].

preparation

   first you will need to download and unpack a pretrained [101]id97
   model from [102]this website ([103]direct download link). this model
   was trained on the google news dataset (about 100 billion words).

   also ensure that you have already [104]installed gensim.

run the vocabulary expansion script

# path to checkpoint file or a directory containing checkpoint files (the script
# will select the most recent).
checkpoint_path="${home}/skip_thoughts/model/train"

# vocabulary file generated by the preprocessing script.
skip_thoughts_vocab="${home}/skip_thoughts/data/vocab.txt"

# path to downloaded id97 model.
id97_model="${home}/skip_thoughts/googlenews/googlenews-vectors-negative300.
bin"

# output directory.
exp_vocab_dir="${home}/skip_thoughts/exp_vocab"

# build the vocabulary expansion script.
cd tensorflow-models/skip_thoughts
bazel build -c opt //skip_thoughts:vocabulary_expansion

# run the vocabulary expansion script.
bazel-bin/skip_thoughts/vocabulary_expansion \
  --skip_thoughts_model=${checkpoint_path} \
  --skip_thoughts_vocab=${skip_thoughts_vocab} \
  --id97_model=${id97_model} \
  --output_dir=${exp_vocab_dir}

evaluating a model

overview

   the model can be evaluated using the benchmark tasks described in the
   [105]skip-thought vectors paper. the following tasks are supported
   (refer to the paper for full details):
     * sick semantic relatedness task.
     * msrp (microsoft research paraphrase corpus) paraphrase detection
       task.
     * binary classification tasks:
          + mr movie review sentiment task.
          + cr customer product review task.
          + subj subjectivity/objectivity task.
          + mpqa opinion polarity task.
          + trec question-type classification task.

preparation

   you will need to clone or download the [106]skip-thoughts github
   repository by [107]ryankiros (the first author of the skip-thoughts
   paper):
# folder to clone the repository to.
st_kiros_dir="${home}/skip_thoughts/skipthoughts_kiros"

# clone the repository.
git clone git@github.com:ryankiros/skip-thoughts.git "${st_kiros_dir}/skipthough
ts"

# make the package importable.
export pythonpath="${st_kiros_dir}/:${pythonpath}"

   you will also need to download the data needed for each evaluation
   task. see the instructions [108]here.

   for example, the cr (customer review) dataset is found [109]here. for
   this task we want the files custrev.pos and custrev.neg.

run the evaluation tasks

   in the following example we will evaluate a unidirectional model
   ("uni-skip" in the paper) on the cr task. to use a bidirectional model
   ("bi-skip" in the paper), simply pass the flags --bi_vocab_file,
   --bi_embeddings_file and --bi_checkpoint_path instead. to use the
   "combine-skip" model described in the paper you will need to pass both
   the unidirectional and bidirectional flags.
# path to checkpoint file or a directory containing checkpoint files (the script
# will select the most recent).
checkpoint_path="${home}/skip_thoughts/model/train"

# vocabulary file generated by the vocabulary expansion script.
vocab_file="${home}/skip_thoughts/exp_vocab/vocab.txt"

# embeddings file generated by the vocabulary expansion script.
embeddings_file="${home}/skip_thoughts/exp_vocab/embeddings.npy"

# directory containing files custrev.pos and custrev.neg.
eval_data_dir="${home}/skip_thoughts/eval_data"

# build the evaluation script.
cd tensorflow-models/skip_thoughts
bazel build -c opt //skip_thoughts:evaluate

# run the evaluation script.
bazel-bin/skip_thoughts/evaluate \
  --eval_task=cr \
  --data_dir=${eval_data_dir} \
  --uni_vocab_file=${vocab_file} \
  --uni_embeddings_file=${embeddings_file} \
  --uni_checkpoint_path=${checkpoint_path}

   output:
[0.82539682539682535, 0.84084880636604775, 0.83023872679045096,
 0.86206896551724133, 0.83554376657824936, 0.85676392572944293,
 0.84084880636604775, 0.83023872679045096, 0.85145888594164454,
 0.82758620689655171]

   the output is a list of accuracies of 10 cross-validation
   classification models. to get a single number, simply take the average:
ipython  # launch ipython.

in [0]:
import numpy as np
np.mean([0.82539682539682535, 0.84084880636604775, 0.83023872679045096,
         0.86206896551724133, 0.83554376657824936, 0.85676392572944293,
         0.84084880636604775, 0.83023872679045096, 0.85145888594164454,
         0.82758620689655171])

out [0]: 0.84009936423729525

encoding sentences

   in this example we will encode data from the [110]movie review dataset
   (specifically the [111]sentence polarity dataset v1.0).
ipython  # launch ipython.

in [0]:

# imports.
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
import os.path
import scipy.spatial.distance as sd
from skip_thoughts import configuration
from skip_thoughts import encoder_manager

in [1]:
# set paths to the model.
vocab_file = "/path/to/vocab.txt"
embedding_matrix_file = "/path/to/embeddings.npy"
checkpoint_path = "/path/to/model.ckpt-9999"
# the following directory should contain files rt-polarity.neg and
# rt-polarity.pos.
mr_data_dir = "/dir/containing/mr/data"

in [2]:
# set up the encoder. here we are using a single unidirectional model.
# to use a bidirectional model as well, call load_model() again with
# configuration.model_config(bidirectional_encoder=true) and paths to the
# bidirectional model's files. the encoder will use the concatenation of
# all loaded models.
encoder = encoder_manager.encodermanager()
encoder.load_model(configuration.model_config(),
                   vocabulary_file=vocab_file,
                   embedding_matrix_file=embedding_matrix_file,
                   checkpoint_path=checkpoint_path)

in [3]:
# load the movie review dataset.
data = []
with open(os.path.join(mr_data_dir, 'rt-polarity.neg'), 'rb') as f:
  data.extend([line.decode('latin-1').strip() for line in f])
with open(os.path.join(mr_data_dir, 'rt-polarity.pos'), 'rb') as f:
  data.extend([line.decode('latin-1').strip() for line in f])

in [4]:
# generate skip-thought vectors for each sentence in the dataset.
encodings = encoder.encode(data)

in [5]:
# define a helper function to generate nearest neighbors.
def get_nn(ind, num=10):
  encoding = encodings[ind]
  scores = sd.cdist([encoding], encodings, "cosine")[0]
  sorted_ids = np.argsort(scores)
  print("sentence:")
  print("", data[ind])
  print("\nnearest neighbors:")
  for i in range(1, num + 1):
    print(" %d. %s (%.3f)" %
          (i, data[sorted_ids[i]], scores[sorted_ids[i]]))

in [6]:
# compute nearest neighbors of the first sentence in the dataset.
get_nn(0)

   output:
sentence:
 simplistic , silly and tedious .

nearest neighbors:
 1. trite , banal , cliched , mostly inoffensive . (0.247)
 2. banal and predictable . (0.253)
 3. witless , pointless , tasteless and idiotic . (0.272)
 4. loud , silly , stupid and pointless . (0.295)
 5. grating and tedious . (0.299)
 6. idiotic and ugly . (0.330)
 7. black-and-white and unrealistic . (0.335)
 8. hopelessly inane , humorless and under-inspired . (0.335)
 9. shallow , noisy and pretentious . (0.340)
 10. . . . unlikable , uninteresting , unfunny , and completely , utterly inept
. (0.346)

     *    2019 github, inc.
     * [112]terms
     * [113]privacy
     * [114]security
     * [115]status
     * [116]help

     * [117]contact github
     * [118]pricing
     * [119]api
     * [120]training
     * [121]blog
     * [122]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [123]reload to refresh your
   session. you signed out in another tab or window. [124]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/tensorflow/models/commits/master.atom
   3. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/tensorflow/models/tree/master/research/skip_thoughts
  32. https://github.com/join
  33. https://github.com/login?return_to=/tensorflow/models
  34. https://github.com/tensorflow/models/watchers
  35. https://github.com/login?return_to=/tensorflow/models
  36. https://github.com/tensorflow/models/stargazers
  37. https://github.com/login?return_to=/tensorflow/models
  38. https://github.com/tensorflow/models/network/members
  39. https://github.com/tensorflow
  40. https://github.com/tensorflow/models
  41. https://github.com/tensorflow/models
  42. https://github.com/tensorflow/models/issues
  43. https://github.com/tensorflow/models/pulls
  44. https://github.com/tensorflow/models/projects
  45. https://github.com/tensorflow/models/pulse
  46. https://github.com/tensorflow/models/find/master
  47. https://github.com/tensorflow/models/commits/master/research/skip_thoughts
  48. https://github.com/tensorflow/models
  49. https://github.com/tensorflow/models/tree/master/research
  50. https://github.com/bapfeld
  51. https://github.com/tensorflow/models/commits?author=bapfeld
  52. https://github.com/tensorflow/models/commit/eb784c09b2c1206551e474ab841d6fd6f424a984
  53. https://github.com/tensorflow/models/commit/eb784c09b2c1206551e474ab841d6fd6f424a984
  54. https://github.com/tensorflow/models/tree/3f94db4e9fe079e58623bef8d605eead315aba4d/research/skip_thoughts
  55. https://github.com/tensorflow/models/tree/master/research
  56. https://github.com/tensorflow/models/tree/master/research/skip_thoughts/skip_thoughts
  57. https://github.com/tensorflow/models/commit/eb784c09b2c1206551e474ab841d6fd6f424a984
  58. https://github.com/tensorflow/models/blob/master/research/skip_thoughts/.gitignore
  59. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  60. https://github.com/tensorflow/models/pull/2430
  61. https://github.com/tensorflow/models/commit/f87a58cd96d45de73c9a8330a06b2ab56749a7fa
  62. https://github.com/tensorflow/models/blob/master/research/skip_thoughts/readme.md
  63. https://github.com/tensorflow/models/blob/master/research/skip_thoughts/workspace
  64. https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf
  65. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#model-overview
  66. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#getting-started
  67. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#install-required-packages
  68. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#download-pretrained-models-optional
  69. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#training-a-model
  70. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#prepare-the-training-data
  71. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#run-the-training-script
  72. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#track-training-progress
  73. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#expanding-the-vocabulary
  74. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#overview
  75. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#preparation
  76. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#run-the-vocabulary-expansion-script
  77. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#evaluating-a-model
  78. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#overview-1
  79. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#preparation-1
  80. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#run-the-evaluation-tasks
  81. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#encoding-sentences
  82. https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf
  83. https://www.cs.cornell.edu/people/pabo/movie-review-data/
  84. http://bazel.build/docs/install.html
  85. https://www.tensorflow.org/install/
  86. http://www.scipy.org/install.html
  87. http://scikit-learn.org/stable/install.html
  88. http://www.nltk.org/install.html
  89. http://www.nltk.org/data.html
  90. https://radimrehurek.com/gensim/install.html
  91. https://code.google.com/archive/p/id97/
  92. http://yknzhu.wixsite.com/mbweb
  93. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#evaluating-a-model
  94. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#encoding-sentences
  95. http://yknzhu.wixsite.com/mbweb
  96. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  97. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  98. https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf
  99. https://arxiv.org/abs/1309.4168
 100. https://arxiv.org/pdf/1301.3781.pdf
 101. https://arxiv.org/pdf/1301.3781.pdf
 102. https://code.google.com/archive/p/id97/
 103. https://drive.google.com/file/d/0b7xkcwpi5kdynlnuttlss21pqmm/edit?usp=sharing
 104. https://radimrehurek.com/gensim/install.html
 105. https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf
 106. https://github.com/ryankiros/skip-thoughts
 107. https://github.com/ryankiros
 108. https://github.com/ryankiros/skip-thoughts
 109. http://nlp.stanford.edu/~sidaw/home/projects:nbid166
 110. https://www.cs.cornell.edu/people/pabo/movie-review-data/
 111. https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz
 112. https://github.com/site/terms
 113. https://github.com/site/privacy
 114. https://github.com/security
 115. https://githubstatus.com/
 116. https://help.github.com/
 117. https://github.com/contact
 118. https://github.com/pricing
 119. https://developer.github.com/
 120. https://training.github.com/
 121. https://github.blog/
 122. https://github.com/about
 123. https://github.com/tensorflow/models/tree/master/research/skip_thoughts
 124. https://github.com/tensorflow/models/tree/master/research/skip_thoughts

   hidden links:
 126. https://github.com/
 127. https://github.com/tensorflow/models/tree/master/research/skip_thoughts
 128. https://github.com/tensorflow/models/tree/master/research/skip_thoughts
 129. https://github.com/tensorflow/models/tree/master/research/skip_thoughts
 130. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#skip-thought-vectors
 131. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#contact
 132. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#contents
 133. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#model-overview
 134. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#getting-started
 135. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#install-required-packages
 136. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#download-pretrained-models-optional
 137. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#training-a-model
 138. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#prepare-the-training-data
 139. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#run-the-training-script
 140. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#track-training-progress
 141. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#expanding-the-vocabulary
 142. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#overview
 143. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#preparation
 144. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#run-the-vocabulary-expansion-script
 145. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#evaluating-a-model
 146. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#overview-1
 147. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#preparation-1
 148. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#run-the-evaluation-tasks
 149. https://github.com/tensorflow/models/tree/master/research/skip_thoughts#encoding-sentences
 150. https://github.com/
