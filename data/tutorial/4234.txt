   #[1]rss

     * [2]about
     * [3]software
     * [4]demos
     * [5]blog

   [syntaxnet.jpg]    [6]kemal   anl  

syntaxnet in context: understanding google   s new tensorflow nlp model

   may 13, 2016    by matthew honnibal

   yesterday, google open sourced their tensorflow-based dependency
   parsing library, syntaxnet. the library gives access to a line of
   neural network parsing models published by google researchers over the
   last two years. i've been following this work closely since it was
   published, and have been looking forward to the software being
   published. this post tries to provide some context around the release    
   what's new here, and how important is it?

   [7]syntaxnet provides an important module in a [8]natural language
   processing (nlp) pipeline such as [9]spacy. if you zoom out of nlp a
   little, the technology trend you see is all about extending the range
   of work computers can take on. until recently, you couldn't write
   software to control a car, and you couldn't write software to tweak
   [10]the tone of your emails, analyse [11]customer feedback or monitor
   global news for [12]significant business risks. admittedly, none of
   those capabilities are a self-driving car. but just wait. language is
   at the heart of human endeavour. our total mastery of moving stuff
   around has long been inevitable, but the potential upside of nlp
   technologies is so good that it's hard to predict.nlp's killer
   appgoogle search is an nlp application, so in some ways its weird to
   talk about waiting for nlp to change the world. it already did. but i
   definitely think there's still much more to come.

   within this larger value chain, syntaxnet is a fairly low-level
   technology. it's like an improved drill bit. by itself, it doesn't give
   you any oil     and oil, by itself, doesn't give you any energy or
   plastics, and energy and plastics by themselves don't give you any work
   or any products. but if the bottle-neck in that whole value chain was
   in the efficiency of oil extraction, and your drill bit improves that
   substantially, the low-level technology can prove [13]pretty important.

   i think that syntactic parsing is a bottle-neck technology in nlp, and
   that the last 4 or 5 years of improvements to this technology will have
   outsize impacts. now, you could argue that i think that because this is
   the problem i've been working on for all my adult life, and the
   technology that i left academia to [14]take commercial. all i can say
   is that this reverses causation: it's my belief in the problem's
   importance that's caused my investment in it, not the other way around.

   okay, so i think the problem is important. but how big a step forward
   is syntaxnet? if you've been using the neural network model in
   [15]stanford corenlp, you're using an algorithm that's almost identical
   in design, but not in detail. the [16]parsing model used by spacy is
   also similar. conceptually, the contribution of the syntaxnet work is
   pretty subtle. it's mostly about careful experimentation, tuning, and
   refinement. however, if google didn't do this work, it's possible that
   nobody else would have. the neural network models that make syntaxnet
   tick have also opened up a rich landscape of sexier ideas, and
   researchers are busily exploring them. there's a bias towards ideas
   that make researchers look (and feel!) clever. probably, we would have
   ended up with parsing models that were just as accurate, but with
   incorrect assumptions about which aspects of the system design were
   important to accuracy, leading to slower progress in the
   future.research notes[17]the first syntaxnet paper was published about
   six months after the description of [18]the corenlp model. mostly, they
   used a larger network, a better activation function, and a different
   optimisation method. they also applied a somewhat hacky approach to
   beam-search, which [19]their more recent work replaced with a more
   principled approach. a parallel line of work that achieves equivalent
   accuracy using [20]lstm models instead of feed-forward networks was
   published at the same time as the first syntaxnet paper.

[21]what syntaxnet does

   a [22]syntactic parser describes a sentence   s grammatical structure, to
   help another application reason about it. natural languages introduce
   many unexpected ambiguities, which our world-knowledge immediately
   filters out. a favourite example:

   they ate the pizza with anchovies
   eat-with pizza-with ambiguity

   a correct parse would link    with    to    pizza   , while an incorrect parse
   would link    with    to    eat   :
   theyprpatevbdthe pizzannwithinanchoviesnnsnsubjdobjpreppobjprep

   you can explore the technology visually with our [23]displacy demo, or
   see a terse example of a [24]rule-based approach to computing with the
   parse tree. the tree of word-word relationships can also be used to
   recognise simple phrases, which makes it easy to extend "bag-of-words"
   technologies such as id97. for instance, we parsed every comment
   posted to reddit in 2015, and used id97 on the phrases, entities
   and words. this produces a nice [25]conceptual map that's more useful
   than a model strictly limited to whitespace-delimited words.

   syntaxnet is a library for training and running syntactic dependency
   parsing models. one model that it provides offers a particularly good
   speed/accuracy trade-off. in keeping with the fashion of the moment,
   they've called this model parsey mcparseface. hopefully, they can
   maintain this sort of meme-based naming system. i think it'll be a good
   way to keep the timeline clear. memes get old fast, and so does nlp
   technology.

[26]how big is the advance?

   despite the "most accurate in the world" billing, parsey mcparseface is
   really only inches ahead of [27]equivalent recent research, that uses a
   more complicated neural network architecture, but with more limited
   parameter tuning. so, similar technologies are out there in academia.
   on the other hand, if what you care about is actually doing things,
   those technologies haven't been available yet.why doesn't spacy use a
   neural network?i've been working on a neural network model for spacy on
   and off since the first syntaxnet paper was published last year.
   however, we want to keep spacy easy to install, we want it to be fast
   on a cpu, and we want it to stay multi-threaded. this has turned out to
   be a tricky mix of requirements to deliver.

   on the time-honoured benchmark for this task, parsey mcparseface
   achieves over 94% accuracy, at around 600 words per second. on the same
   task, spacy achieves 92.4%, at around 15,000 words per second. the
   extra accuracy might not sound like much, but for applications, it's
   likely to be pretty significant.

   for any predictive system, often the important consideration is the
   difference to a baseline predictor, rather than the absolute accuracy.
   a model that predicts that the weather will be the same as yesterday
   will often be accurate, but it's not adding any value. the thing about
   id33 is that about 80% of the dependencies are very easy
   and unambiguous, which means that a system that only predicts those
   dependencies correctly is injecting very little extra information, that
   wasn't trivially available by just looking at each word and its
   neighbours.

   in summary, i think parsey mcparseface is a very nice milestone on a
   larger trend. the thing that's really significant is how quickly the
   speed and accuracy of natural language processing technologies is
   advancing. i think there are lots of ideas that didn't work yesterday,
   that are suddenly becoming very viable.

[28]what's next?

   one of the things i'm most excited about is that there's a very clear
   way forward, given the design of the parsey mcparseface model. it's one
   of those things where you can say, "okay, if this works, that's great
   news". this type of parser, pioneered by [29]joakim nivre in 2004,
   reads the sentence one word at a time, and maintains only a handful of
   competing representations. you can slot in any state representation,
   any set of actions, and any id203 model into this architecture.
   for instance, if you're parsing the output of a id103
   system, you could have the parser refine the speech recogniser's guess
   at the words, based on the syntactic context. if you're populating a
   knowledge base, you can extend the state representation to include your
   target semantics, and learn it jointly with the syntax.

   joint models and semi-supervised learning have always been the
   "motherhood and apple pie" of natural language understanding research.
   there was never any doubt that these things were good     but without a
   concrete proposal, they're just platitudes. it was always clear that
   chopping the task of understanding a sentence up into lots of
   subproblems, and having a pipeline of distinct models, was an
   unsatisfying solution. it was also obvious that a natural language
   understanding system should be able to make use of the vast quantities
   of unannotated text available. i think a transition-based neural
   network model gives a convincing answer to both questions. you can
   learn any structure you like this way, and the more text you see, the
   more you learn, without adding any new parameters to the model.

   obviously, we want to build a bridge between parsey mcparseface and
   spacy, so that you can use the more accurate model with the sweeter
   spacy api. however, for any individual use-case, there's always a lot
   of tuning to do to make these technologies really perform. in
   particular, every application sees a different type of text. accuracy
   goes up substantially if the statistical model is tuned to the domain.
   for instance, in well edited text such as a financial report, you want
   the model to consider capitalisation as a decisive indicator     but if
   you're parsing tweets, capitalisation is almost meaningless.

   our plan is to solve this problem by providing a range of pre-trained
   models, adapted to different languages and genres. we also have some
   ideas we're really excited about, to help each user train their own
   custom model, as painlessly as possible. we think that in nlp, the
   algorithms are racing ahead, while the data is lagging behind. we want
   to fix that. if you can't wait for the result, i hope you'll [30]get in
   touch.

   matthew honnibal
   about the author

matthew honnibal

   matthew is a leading expert in ai technology, known for his research,
   software and writings. he completed his phd in 2009, and spent a
   further 5 years publishing research on state-of-the-art natural
   language understanding systems. anticipating the ai boom, he left
   academia in 2014 to develop spacy, an open-source library for
   industrial-strength nlp.

read more

[31]introducing spacy v2.1

[32]explosion ai in 2017: our year in review

[33]introducing custom pipelines and extensions for spacy v2.0

[34]pseudo-rehearsal: a simple solution to catastrophic forgetting for nlp

[35]prodigy: a new tool for radically efficient machine teaching

[36]supervised learning is great     it   s data collection that   s broken

    about us

   explosion ai is a digital studio specialising in artificial
   intelligence and natural language processing. we   re the makers of
   spacy, the leading open-source nlp library.

    navigation

     * [37]home
     * [38]about
     * [39]software
     * [40]demos
     * [41]blog
     * [42]legal / imprint

    our software

     * [43]spacy
       industrial-strength nlp
     * [44]prodigy
       radically efficient machine teaching

   [45]see more    

references

   visible links
   1. https://explosion.ai/feed.xml
   2. https://explosion.ai/about
   3. https://explosion.ai/software
   4. https://explosion.ai/demos
   5. https://explosion.ai/blog
   6. https://dribbble.com/kemal
   7. http://googleresearch.blogspot.de/2016/05/announcing-syntaxnet-worlds-most.html
   8. https://en.wikipedia.org/wiki/natural_language_processing
   9. https://spacy.io/
  10. https://foxtype.com/
  11. https://wonderflo.co/
  12. http://cytora.com/
  13. https://en.wikipedia.org/wiki/hughes_tool_company
  14. https://explosion.ai/blog/introducing-spacy
  15. http://stanfordnlp.github.io/corenlp/
  16. https://aclweb.org/anthology/d/d15/d15-1162.pdf
  17. http://www.petrovi.de/data/acl15.pdf
  18. http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf
  19. http://arxiv.org/pdf/1603.06042v1.pdf
  20. http://www.cs.cmu.edu/~lingwang/papers/acl2015.pdf
  22. http://googleresearch.blogspot.de/2013/05/syntactic-ngrams-over-time.html
  23. https://demos.explosion.ai/displacy
  24. https://spacy.io/docs/tutorials/syntax-search
  25. https://demos.explosion.ai/sense2vec
  27. http://arxiv.org/pdf/1603.04351v1.pdf
  29. https://www.semanticscholar.org/author/joakim-nivre/1720988
  30. https://explosion.ai/about
  31. https://explosion.ai/blog/spacy-v2-1
  32. https://explosion.ai/blog/year-in-review-2017
  33. https://explosion.ai/blog/spacy-v2-pipelines-extensions
  34. https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
  35. https://explosion.ai/blog/prodigy-annotation-tool-active-learning
  36. https://explosion.ai/blog/supervised-learning-data-collection
  37. https://explosion.ai/
  38. https://explosion.ai/about
  39. https://explosion.ai/software
  40. https://explosion.ai/demos
  41. https://explosion.ai/blog
  42. https://explosion.ai/legal
  43. https://spacy.io/
  44. https://prodi.gy/
  45. https://explosion.ai/software

   hidden links:
  47. https://explosion.ai/
  48. mailto:matt@explosion.ai
  49. https://twitter.com/honnibal
  50. https://github.com/honnibal
  51. https://www.semanticscholar.org/search?q=matthew%20honnibal
  52. https://explosion.ai/blog/spacy-v2-1
  53. https://explosion.ai/blog/year-in-review-2017
  54. https://explosion.ai/blog/spacy-v2-pipelines-extensions
  55. https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
  56. https://explosion.ai/blog/prodigy-annotation-tool-active-learning
  57. https://explosion.ai/blog/supervised-learning-data-collection
  58. mailto:contact@explosion.ai
  59. https://twitter.com/explosion_ai
  60. https://github.com/explosion
  61. https://youtube.com/c/explosionai
  62. https://explosion.ai/feed
