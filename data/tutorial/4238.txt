   #[1]github [2]recent commits to nematus:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]77
     * [35]star [36]547
     * [37]fork [38]216

[39]edinburghnlp/[40]nematus

   [41]code [42]issues 4 [43]pull requests 0 [44]projects 0 [45]wiki
   [46]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   open-source id4 in tensorflow
   [48]neural-machine-translation [49]sequence-to-sequence
   [50]machine-translation [51]id4 [52]mt
     * [53]817 commits
     * [54]22 branches
     * [55]7 releases
     * [56]fetching contributors
     * [57]bsd-3-clause

    1. [58]python 81.9%
    2. [59]perl 6.2%
    3. [60]emacs lisp 3.6%
    4. [61]javascript 3.6%
    5. [62]php 2.3%
    6. [63]shell 1.2%
    7. other 1.2%

   (button) python perl emacs lisp javascript php shell other
   branch: master (button) new pull request
   [64]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/e
   [65]download zip

downloading...

   want to be notified of new releases in edinburghnlp/nematus?
   [66]sign in [67]sign up

launching github desktop...

   if nothing happens, [68]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [69]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [70]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [71]download the github extension for visual studio
   and try again.

   (button) go back
   [72]@rsennrich
   [73]rsennrich [74]support id172 alpha at training time
   latest commit [75]b818412 mar 20, 2019
   [76]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [77]data [78]update shebang and documentation to python3 mar 14, 2019
   [79]doc [80]factored id4 documentation (from theano branch) jul 6, 2018
   [81]nematus [82]support id172 alpha at training time mar 26,
   2019
   [83]test [84]update shebang and documentation to python3 mar 14, 2019
   [85]utils [86]remove '-*- coding: utf-8 -*-' declarations dec 16, 2018
   [87].gitignore [88]cleanup and reorganize files; install script apr 26,
   2016
   [89]changelog.md
   [90]dockerfile.cpu
   [91]dockerfile.gpu
   [92]license
   [93]readme.md
   [94]setup.py

readme.md

nematus

   attention-based encoder-decoder model for id4
   built in tensorflow.

   notable features include:
     * support for id56 and transformer architectures
     * support for advanced id56 architectures:
          + arbitrary input features (factored id4)
            [95]http://www.statmt.org/wmt16/pdf/w16-2209.pdf
          + deep models (miceli barone et al., 2017)
            [96]https://arxiv.org/abs/1707.07631
          + dropout on all layers (gal, 2015)
            [97]http://arxiv.org/abs/1512.05287
          + tied embeddings (press and wolf, 2016)
            [98]https://arxiv.org/abs/1608.05859
          + layer normalisation (ba et al, 2016)
            [99]https://arxiv.org/abs/1607.06450
          + mixture of softmaxes (yang et al., 2017)
            [100]https://arxiv.org/abs/1711.03953
     * training features:
          + multi-gpu support
          + label smoothing
          + early stopping with user-defined stopping criterion
          + resume training (optionally with map-l2 id173 towards
            original model)
     * scoring and decoding features:
          + batch decoding
          + n-best output
          + scripts for scoring (given parallel corpus) and rescoring (of
            n-best output)
          + server mode
     * other usability features:
          + command line interface for training, scoring, and decoding
          + json-formatted storage of model hyperparameters, vocabulary
            files and training progress
          + pretrained models for 13 translation directions (many
            top-performing at wmt shared task of respective year):
               o [101]http://data.statmt.org/rsennrich/wmt16_systems/
               o [102]http://data.statmt.org/rsennrich/wmt17_systems/
          + backward compatibility: continue using publicly released
            models with current codebase (scripts to convert from theano
            to tensorflow-style models are provided)

support

   for general support requests, there is a google groups mailing list at
   [103]https://groups.google.com/d/forum/nematus-support . you can also
   send an e-mail to [104]nematus-support@googlegroups.com .

installation

   nematus requires the following packages:
     * python 3 (tested on version 3.5.2)
     * tensorflow (tested on version 1.12)

   to install tensorflow, we recommend following the steps at: (
   [105]https://www.tensorflow.org/install/ )

   the following packages are optional, but highly recommended
     * cuda >= 7 (only gpu training is sufficiently fast)
     * cudnn >= 4 (speeds up training substantially)

legacy theano version

   nematus originated as a fork of dl4mt-tutorial by kyunghyun cho et al.
   ( [106]https://github.com/nyu-dl/dl4mt-tutorial ), and was implemented
   in theano. see [107]https://github.com/edinburghnlp/nematus/tree/theano
   for this theano-based version of nematus.

   to use models trained with theano with the current tensorflow codebase,
   use the script nematus/theano_tf_convert.py.

docker usage

   you can also create docker image by running following command, where
   you change suffix to either cpu or gpu:

   docker build -t nematus-docker -f dockerfile.suffix .

   to run a cpu docker instance with the current working directory shared
   with the docker container, execute:

   docker run -v `pwd`:/playground -it nematus-docker

   for gpu you need to have nvidia-docker installed and run:

   nvidia-docker run -v `pwd`:/playground -it nematus-docker

training speed

   training speed depends heavily on having appropriate hardware (ideally
   a recent nvidia gpu), and having installed the appropriate software
   packages.

   to test your setup, we provide some speed benchmarks with
   `test/test_train.sh', on an intel xeon cpu e5-2620 v4, with a nvidia
   geforce gtx titan x (pascal) and cuda 9.0:

   gpu, cudnn 5.1, tensorflow 1.0.1:

   cuda_visible_devices=0 ./test_train.sh

     225.25 sentenses/s

usage instructions

   all of the scripts below can be run with --help flag to get usage
   information.

   sample commands with toy examples are available in the test directory;
   for training a full-scale id56 system, consider the training scripts at
   [108]http://data.statmt.org/wmt17_systems/training/

   an updated version of these scripts that uses the transformer model can
   be found at
   [109]https://github.com/edinburghnlp/wmt17-transformer-scripts

nematus/train.py : use to train a new model

data sets; model loading and saving

   parameter description
   --source_dataset path parallel training corpus (source)
   --target_dataset path parallel training corpus (target)
   --dictionaries path [path ...] network vocabularies (one per source
   factor, plus target vocabulary)
   --save_freq int save frequency (default: 30000)
   --model path model file name (default: model)
   --reload path load existing model from this path. set to
   "latest_checkpoint" to reload the latest checkpoint in the same
   directory of --model
   --no_reload_training_progress don't reload training progress (only used
   if --reload is enabled)
   --summary_dir path directory for saving summaries (default: same
   directory as the --model file)
   --summary_freq int save summaries after int updates, if 0 do not save
   summaries (default: 0)

network parameters (all model types)

   parameter description
   --model_type {id56,transformer} model type (default: id56)
   --embedding_size int embedding layer size (default: 512)
   --state_size int hidden state size (default: 1000)
   --source_vocab_sizes int [int ...] source vocabulary sizes (one per
   input factor) (default: none)
   --target_vocab_size int target vocabulary size (default: -1)
   --factors int number of input factors (default: 1) - currently only
   works for 'id56' model
   --dim_per_factor int [int ...] list of word vector dimensionalities
   (one per factor): '--dim_per_factor 250 200 50' for total
   dimensionality of 500 (default: none)
   --tie_encoder_decoder_embeddings tie the input embeddings of the
   encoder and the decoder (first factor only). source and target
   vocabulary size must be the same
   --tie_decoder_embeddings tie the input embeddings of the decoder with
   the softmax output embeddings
   --output_hidden_activation {tanh,relu,prelu,linear} activation function
   in hidden layer of the output network (default: tanh) - currently only
   works for 'id56' model
   --softmax_mixture_size int number of softmax components to use
   (default: 1) - currently only works for 'id56' model

network parameters (id56-specific)

   parameter description
   --id56_enc_depth int number of encoder layers (default: 1)
   --id56_enc_transition_depth int number of gru transition operations
   applied in the encoder. minimum is 1. (only applies to gru). (default:
   1)
   --id56_dec_depth int number of decoder layers (default: 1)
   --id56_dec_base_transition_depth int number of gru transition operations
   applied in the first layer of the decoder. minimum is 2. (only applies
   to gru_cond). (default: 2)
   --id56_dec_high_transition_depth int number of gru transition operations
   applied in the higher layers of the decoder. minimum is 1. (only
   applies to gru). (default: 1)
   --id56_dec_deep_context pass context vector (from first layer) to deep
   decoder layers
   --id56_use_dropout use dropout layer (default: false)
   --id56_dropout_embedding float dropout for input embeddings (0: no
   dropout) (default: 0.2)
   --id56_dropout_hidden float dropout for hidden layer (0: no dropout)
   (default: 0.2)
   --id56_dropout_source float dropout source words (0: no dropout)
   (default: 0.0)
   --id56_dropout_target float dropout target words (0: no dropout)
   (default: 0.0)
   --id56_layer_normalisation set to use layer id172 in encoder and
   decoder

network parameters (transformer-specific)

   parameter description
   --transformer_enc_depth int number of encoder layers (default: 6)
   --transformer_dec_depth int number of decoder layers (default: 6)
   --transformer_ffn_hidden_size int inner dimensionality of feed-forward
   sub-layers (default: 2048)
   --transformer_num_heads int number of attention heads used in
   multi-head attention (default: 8)
   --transformer_dropout_embeddings float dropout applied to sums of word
   embeddings and positional encodings (default: 0.1)
   --transformer_dropout_residual float dropout applied to residual
   connections (default: 0.1)
   --transformer_dropout_relu float dropout applied to the internal
   activation of the feed-forward sub-layers (default: 0.1)
   --transformer_dropout_attn float dropout applied to attention weights
   (default: 0.1)

training parameters

   parameter description
   --loss_function {cross-id178,per-token-cross-id178} id168
   (default: cross-id178)
   --decay_c float l2 id173 penalty (default: 0.0)
   --map_decay_c float map-l2 id173 penalty towards original
   weights (default: 0.0)
   --prior_model path prior model for map-l2 id173. unless using
   " --reload", this will also be used for initialization.
   --clip_c float gradient clipping threshold (default: 1.0)
   --label_smoothing float label smoothing (default: 0.0)
   --optimizer {adam} optimizer (default: adam)
   --adam_beta1 float exponential decay rate for the first moment
   estimates (default: 0.9)
   --adam_beta2 float exponential decay rate for the second moment
   estimates (default: 0.999)
   --adam_epsilon float constant for numerical stability (default: 1e-08)
   --learning_schedule {constant,transformer} learning schedule (default:
   constant)
   --learning_rate float learning rate (default: 0.0001)
   --warmup_steps int number of initial updates during which the learning
   rate is increased linearly during learning rate scheduling (default:
   8000)
   --maxlen int maximum sequence length for training and validation
   (default: 100)
   --batch_size int minibatch size (default: 80)
   --token_batch_size int minibatch size (expressed in number of source or
   target tokens). sentence-level minibatch size will be dynamic. if this
   is enabled, batch_size only affects sorting by length. (default: 0)
   --maxibatch_size int size of maxibatch (number of minibatches that are
   sorted by length) (default: 20)
   --no_sort_by_length do not sort sentences in maxibatch by length
   --no_shuffle disable shuffling of training data (for each epoch)
   --keep_train_set_in_memory keep training dataset lines stores in ram
   during training
   --max_epochs int maximum number of epochs (default: 5000)
   --finish_after int maximum number of updates (minibatches) (default:
   10000000)

validation parameters

   parameter description
   --valid_source_dataset path source validation corpus (default: none)
   --valid_target_dataset path target validation corpus (default: none)
   --valid_batch_size int validation minibatch size (default: 80)
   --valid_token_batch_size int validation minibatch size (expressed in
   number of source or target tokens). sentence-level minibatch size will
   be dynamic. if this is enabled, valid_batch_size only affects sorting
   by length. (default: 0)
   --valid_freq int validation frequency (default: 10000)
   --valid_script path path to script for external validation (default:
   none). the script will be passed an argument specifying the path of a
   file that contains translations of the source validation corpus. it
   must write a single score to standard output.
   --patience int early stopping patience (default: 10)

display parameters

   parameter description
   --disp_freq int display loss after int updates (default: 1000)
   --sample_freq int display some samples after int updates (default:
   10000)
   --beam_freq int display some beam_search samples after int updates
   (default: 10000)
   --beam_size int size of the beam (default: 12)

translate parameters

   parameter description
   --no_normalize cost of sentences will not be normalized by length
   --n_best print full beam
   --translation_maxlen int maximum length of translation output sentence
   (default: 200)

nematus/translate.py : use an existing model to translate a source text

   parameter description
   -v, --verbose verbose mode
   -m path [path ...], --models path [path ...] model to use; provide
   multiple models (with same vocabulary) for ensemble decoding
   -b int, --minibatch_size int minibatch size (default: 80)
   -i path, --input path input file (default: standard input)
   -o path, --output path output file (default: standard output)
   -k int, --beam_size int beam size (default: 5)
   -n [alpha], --id172_alpha [alpha] normalize scores by sentence
   length (with argument, exponentiate lengths by alpha)
   --n_best write n-best list (of size k)
   --maxibatch_size int size of maxibatch (number of minibatches that are
   sorted by length) (default: 20)

nematus/score.py : use an existing model to score a parallel corpus

   parameter description
   -v, --verbose verbose mode
   -m path [path ...], --models path [path ...] model to use; provide
   multiple models (with same vocabulary) for ensemble decoding
   -b int, --minibatch_size int minibatch size (default: 80)
   -n [alpha], --id172_alpha [alpha] normalize scores by sentence
   length (with argument, exponentiate lengths by alpha)
   -o path, --output path output file (default: standard output)
   -s path, --source path source text file
   -t path, --target path target text file

nematus/rescore.py : use an existing model to rescore an n-best list.

   the n-best list is assumed to have the same format as moses:
sentence-id (starting from 0) ||| translation ||| scores

   new scores will be appended to the end. rescore.py has the same
   arguments as score.py, with the exception of this additional parameter:
         parameter                         description
   -i path, --input path input n-best list file (default: standard input)

nematus/theano_tf_convert.py : convert an existing theano model to a
tensorflow model

   if you have a theano model (model.npz) with network architecture
   features that are currently supported then you can convert it into a
   tensorflow model using nematus/theano_tf_convert.py.
     parameter                 description
   --from_theano convert from theano to tensorflow format
   --from_tf     convert from tensorflow to theano format
   --in path     path to input model
   --out path    path to output model

publications

   if you use nematus, please cite the following paper:

   rico sennrich, orhan firat, kyunghyun cho, alexandra birch, barry
   haddow, julian hitschler, marcin junczys-dowmunt, samuel l  ubli,
   antonio valerio miceli barone, jozef mokry and maria nadejde (2017):
   nematus: a toolkit for id4. in proceedings of
   the software demonstrations of the 15th conference of the european
   chapter of the association for computational linguistics, valencia,
   spain, pp. 65-68.
@inproceedings{sennrich-etal:2017:eacldemo,
  author    = {sennrich, rico  and  firat, orhan  and  cho, kyunghyun  and  birc
h, alexandra  and  haddow, barry  and  hitschler, julian  and  junczys-dowmunt,
marcin  and  l\"{a}ubli, samuel  and  miceli barone, antonio valerio  and  mokry
, jozef  and  nadejde, maria},
  title     = {nematus: a toolkit for id4},
  booktitle = {proceedings of the software demonstrations of the 15th conference
 of the european chapter of the association for computational linguistics},
  month     = {april},
  year      = {2017},
  address   = {valencia, spain},
  publisher = {association for computational linguistics},
  pages     = {65--68},
  url       = {http://aclweb.org/anthology/e17-3017}
}

   the code is based on the following models:

   dzmitry bahdanau, kyunghyun cho, yoshua bengio (2015): neural machine
   translation by jointly learning to align and translate, proceedings of
   the international conference on learning representations (iclr).

   ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion
   jones, aidan n. gomez, lukasz kaiser, illia polosukhin (2017):
   attention is all you need, advances in neural information processing
   systems (nips).

   please refer to the nematus paper for a description of implementation
   differences to the id56 model.

acknowledgments

   this project has received funding from the european union   s horizon
   2020 research and innovation programme under grant agreements 645452
   (qt21), 644333 (tramooc), 644402 (himl) and 688139 (summa).

     *    2019 github, inc.
     * [110]terms
     * [111]privacy
     * [112]security
     * [113]status
     * [114]help

     * [115]contact github
     * [116]pricing
     * [117]api
     * [118]training
     * [119]blog
     * [120]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [121]reload to refresh your
   session. you signed out in another tab or window. [122]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/edinburghnlp/nematus/commits/master.atom
   3. https://github.com/edinburghnlp/nematus#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/edinburghnlp/nematus
  32. https://github.com/join
  33. https://github.com/login?return_to=/edinburghnlp/nematus
  34. https://github.com/edinburghnlp/nematus/watchers
  35. https://github.com/login?return_to=/edinburghnlp/nematus
  36. https://github.com/edinburghnlp/nematus/stargazers
  37. https://github.com/login?return_to=/edinburghnlp/nematus
  38. https://github.com/edinburghnlp/nematus/network/members
  39. https://github.com/edinburghnlp
  40. https://github.com/edinburghnlp/nematus
  41. https://github.com/edinburghnlp/nematus
  42. https://github.com/edinburghnlp/nematus/issues
  43. https://github.com/edinburghnlp/nematus/pulls
  44. https://github.com/edinburghnlp/nematus/projects
  45. https://github.com/edinburghnlp/nematus/wiki
  46. https://github.com/edinburghnlp/nematus/pulse
  47. https://github.com/join?source=prompt-code
  48. https://github.com/topics/neural-machine-translation
  49. https://github.com/topics/sequence-to-sequence
  50. https://github.com/topics/machine-translation
  51. https://github.com/topics/id4
  52. https://github.com/topics/mt
  53. https://github.com/edinburghnlp/nematus/commits/master
  54. https://github.com/edinburghnlp/nematus/branches
  55. https://github.com/edinburghnlp/nematus/releases
  56. https://github.com/edinburghnlp/nematus/graphs/contributors
  57. https://github.com/edinburghnlp/nematus/blob/master/license
  58. https://github.com/edinburghnlp/nematus/search?l=python
  59. https://github.com/edinburghnlp/nematus/search?l=perl
  60. https://github.com/edinburghnlp/nematus/search?l=emacs-lisp
  61. https://github.com/edinburghnlp/nematus/search?l=javascript
  62. https://github.com/edinburghnlp/nematus/search?l=php
  63. https://github.com/edinburghnlp/nematus/search?l=shell
  64. https://github.com/edinburghnlp/nematus/find/master
  65. https://github.com/edinburghnlp/nematus/archive/master.zip
  66. https://github.com/login?return_to=https://github.com/edinburghnlp/nematus
  67. https://github.com/join?return_to=/edinburghnlp/nematus
  68. https://desktop.github.com/
  69. https://desktop.github.com/
  70. https://developer.apple.com/xcode/
  71. https://visualstudio.github.com/
  72. https://github.com/rsennrich
  73. https://github.com/edinburghnlp/nematus/commits?author=rsennrich
  74. https://github.com/edinburghnlp/nematus/commit/b8184122df2010bca0d9d343348574e2fc4f7ae6
  75. https://github.com/edinburghnlp/nematus/commit/b8184122df2010bca0d9d343348574e2fc4f7ae6
  76. https://github.com/edinburghnlp/nematus/tree/b8184122df2010bca0d9d343348574e2fc4f7ae6
  77. https://github.com/edinburghnlp/nematus/tree/master/data
  78. https://github.com/edinburghnlp/nematus/commit/10e6d0e7f070803c3eb0c0098940f8f60cc88f9d
  79. https://github.com/edinburghnlp/nematus/tree/master/doc
  80. https://github.com/edinburghnlp/nematus/commit/a63487e17cc717be54b296e11693795f887d7d03
  81. https://github.com/edinburghnlp/nematus/tree/master/nematus
  82. https://github.com/edinburghnlp/nematus/commit/b8184122df2010bca0d9d343348574e2fc4f7ae6
  83. https://github.com/edinburghnlp/nematus/tree/master/test
  84. https://github.com/edinburghnlp/nematus/commit/10e6d0e7f070803c3eb0c0098940f8f60cc88f9d
  85. https://github.com/edinburghnlp/nematus/tree/master/utils
  86. https://github.com/edinburghnlp/nematus/commit/ac75471c9c73f3892ebe42d15fc20696c9220f1c
  87. https://github.com/edinburghnlp/nematus/blob/master/.gitignore
  88. https://github.com/edinburghnlp/nematus/commit/e2f42f1f531b6424f4d47a5c168cfad29b5be92d
  89. https://github.com/edinburghnlp/nematus/blob/master/changelog.md
  90. https://github.com/edinburghnlp/nematus/blob/master/dockerfile.cpu
  91. https://github.com/edinburghnlp/nematus/blob/master/dockerfile.gpu
  92. https://github.com/edinburghnlp/nematus/blob/master/license
  93. https://github.com/edinburghnlp/nematus/blob/master/readme.md
  94. https://github.com/edinburghnlp/nematus/blob/master/setup.py
  95. http://www.statmt.org/wmt16/pdf/w16-2209.pdf
  96. https://arxiv.org/abs/1707.07631
  97. http://arxiv.org/abs/1512.05287
  98. https://arxiv.org/abs/1608.05859
  99. https://arxiv.org/abs/1607.06450
 100. https://arxiv.org/abs/1711.03953
 101. http://data.statmt.org/rsennrich/wmt16_systems/
 102. http://data.statmt.org/rsennrich/wmt17_systems/
 103. https://groups.google.com/d/forum/nematus-support
 104. mailto:nematus-support@googlegroups.com
 105. https://www.tensorflow.org/install/
 106. https://github.com/nyu-dl/dl4mt-tutorial
 107. https://github.com/edinburghnlp/nematus/tree/theano
 108. http://data.statmt.org/wmt17_systems/training/
 109. https://github.com/edinburghnlp/wmt17-transformer-scripts
 110. https://github.com/site/terms
 111. https://github.com/site/privacy
 112. https://github.com/security
 113. https://githubstatus.com/
 114. https://help.github.com/
 115. https://github.com/contact
 116. https://github.com/pricing
 117. https://developer.github.com/
 118. https://training.github.com/
 119. https://github.blog/
 120. https://github.com/about
 121. https://github.com/edinburghnlp/nematus
 122. https://github.com/edinburghnlp/nematus

   hidden links:
 124. https://github.com/
 125. https://github.com/edinburghnlp/nematus
 126. https://github.com/edinburghnlp/nematus
 127. https://github.com/edinburghnlp/nematus
 128. https://help.github.com/articles/which-remote-url-should-i-use
 129. https://github.com/edinburghnlp/nematus#nematus
 130. https://github.com/edinburghnlp/nematus#support
 131. https://github.com/edinburghnlp/nematus#installation
 132. https://github.com/edinburghnlp/nematus#legacy-theano-version
 133. https://github.com/edinburghnlp/nematus#docker-usage
 134. https://github.com/edinburghnlp/nematus#training-speed
 135. https://github.com/edinburghnlp/nematus#usage-instructions
 136. https://github.com/edinburghnlp/nematus#nematustrainpy--use-to-train-a-new-model
 137. https://github.com/edinburghnlp/nematus#data-sets-model-loading-and-saving
 138. https://github.com/edinburghnlp/nematus#network-parameters-all-model-types
 139. https://github.com/edinburghnlp/nematus#network-parameters-id56-specific
 140. https://github.com/edinburghnlp/nematus#network-parameters-transformer-specific
 141. https://github.com/edinburghnlp/nematus#training-parameters
 142. https://github.com/edinburghnlp/nematus#validation-parameters
 143. https://github.com/edinburghnlp/nematus#display-parameters
 144. https://github.com/edinburghnlp/nematus#translate-parameters
 145. https://github.com/edinburghnlp/nematus#nematustranslatepy--use-an-existing-model-to-translate-a-source-text
 146. https://github.com/edinburghnlp/nematus#nematusscorepy--use-an-existing-model-to-score-a-parallel-corpus
 147. https://github.com/edinburghnlp/nematus#nematusrescorepy--use-an-existing-model-to-rescore-an-n-best-list
 148. https://github.com/edinburghnlp/nematus#nematustheano_tf_convertpy--convert-an-existing-theano-model-to-a-tensorflow-model
 149. https://github.com/edinburghnlp/nematus#publications
 150. https://github.com/edinburghnlp/nematus#acknowledgments
 151. https://github.com/
