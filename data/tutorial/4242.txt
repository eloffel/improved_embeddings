   #[1]the keras blog atom feed

                               [2]the keras blog

   [3]keras is a deep learning library for python, that is simple,
   modular, and extensible.

     * [4]archives
     * [5]github
     * [6]documentation
     * [7]google group

                         [8]the future of deep learning

   tue 18 july 2017


    by [9]francois chollet

   in [10]essays.

   this post is adapted from section 3 of chapter 9 of my book, [11]deep
   learning with python (manning publications).

   [12]deep learning with python

   it is part of a series of two posts on the current limitations of deep
   learning, and its future. you can read the first part here: [13]the
   limitations of deep learning.
     __________________________________________________________________

   given what we know of how deep nets work, of their limitations, and of
   the current state of the research landscape, can we predict where
   things are headed in the medium term? here are some purely personal
   thoughts. note that i don't have a crystal ball, so a lot of what i
   anticipate might fail to become reality. this is a completely
   speculative post. i am sharing these predictions not because i expect
   them to be proven completely right in the future, but because they are
   interesting and actionable in the present.

   at a high-level, the main directions in which i see promise are:
     * models closer to general-purpose computer programs, built on top of
       far richer primitives than our current differentiable layers   this
       is how we will get to reasoning and abstraction, the fundamental
       weakness of current models.
     * new forms of learning that make the above possible   allowing models
       to move away from just differentiable transforms.
     * models that require less involvement from human engineers   it
       shouldn't be your job to tune knobs endlessly.
     * greater, systematic reuse of previously learned features and
       architectures; meta-learning systems based on reusable and modular
       program subroutines.

   additionally, do note that these considerations are not specific to the
   sort of supervised learning that has been the bread and butter of deep
   learning so far   rather, they are applicable to any form of machine
   learning, including unsupervised, self-supervised, and reinforcement
   learning. it is not fundamentally important where your labels come from
   or what your training loop looks like; these different branches of
   machine learning are just different facets of a same construct.

   let's dive in.

models as programs

   as we noted in our previous post, a necessary transformational
   development that we can expect in the field of machine learning is a
   move away from models that perform purely pattern recognition and can
   only achieve local generalization, towards models capable of
   abstraction and reasoning, that can achieve extreme generalization.
   current ai programs that are capable of basic forms of reasoning are
   all hard-coded by human programmers: for instance, software that relies
   on search algorithms, graph manipulation, formal logic. in deepmind's
   alphago, for example, most of the "intelligence" on display is designed
   and hard-coded by expert programmers (e.g. monte-carlo tree search);
   learning from data only happens in specialized submodules (value
   networks and policy networks). but in the future, such ai systems may
   well be fully learned, with no human involvement.

   what could be the path to make this happen? consider a well-known type
   of network: id56s. importantly, id56s have slightly less limitations than
   feedforward networks. that is because id56s are a bit more than a mere
   geometric transformation: they are geometric transformations repeatedly
   applied inside a for loop. the temporal for loop is itself hard-coded
   by human developers: it is a built-in assumption of the network.
   naturally, id56s are still extremely limited in what they can represent,
   primarily because each step they perform is still just a differentiable
   geometric transformation, and the way they carry information from step
   to step is via points in a continuous geometric space (state vectors).
   now, imagine neural networks that would be "augmented" in a similar way
   with programming primitives such as for loops   but not just a single
   hard-coded for loop with a hard-coded geometric memory, rather, a large
   set of programming primitives that the model would be free to
   manipulate to expand its processing function, such as if branches,
   while statements, variable creation, disk storage for long-term memory,
   sorting operators, advanced datastructures like lists, graphs, and
   hashtables, and many more. the space of programs that such a network
   could represent would be far broader than what can be represented with
   current deep learning models, and some of these programs could achieve
   superior generalization power.

   in a word, we will move away from having on one hand "hard-coded
   algorithmic intelligence" (handcrafted software) and on the other hand
   "learned geometric intelligence" (deep learning). we will have instead
   a blend of formal algorithmic modules that provide reasoning and
   abstraction capabilities, and geometric modules that provide informal
   intuition and pattern recognition capabilities. the whole system would
   be learned with little or no human involvement.

   a related subfield of ai that i think may be about to take off in a big
   way is that of program synthesis, in particular neural program
   synthesis. program synthesis consists in automatically generating
   simple programs, by using a search algorithm (possibly genetic search,
   as in genetic programming) to explore a large space of possible
   programs. the search stops when a program is found that matches the
   required specifications, often provided as a set of input-output pairs.
   as you can see, is it highly reminiscent of machine learning: given
   "training data" provided as input-output pairs, we find a "program"
   that matches inputs to outputs and can generalize to new inputs. the
   difference is that instead of learning parameter values in a hard-coded
   program (a neural network), we generate source code via a discrete
   search process.

   i would definitely expect this subfield to see a wave of renewed
   interest in the next few years. in particular, i would expect the
   emergence of a crossover subfield in-between deep learning and program
   synthesis, where we would not quite be generating programs in a
   general-purpose language, but rather, where we would be generating
   neural networks (geometric data processing flows) augmented with a rich
   set of algorithmic primitives, such as for loops   and many others. this
   should be far more tractable and useful than directly generating source
   code, and it would dramatically expand the scope of problems that can
   be solved with machine learning   the space of programs that we can
   generate automatically given appropriate training data. a blend of
   symbolic ai and geometric ai. contemporary id56s can be seen as a
   prehistoric ancestor to such hybrid algorithmic-geometric models.

   a learned program relying on both geometric (pattern recognition,
   intuition) and algorithmic (reasoning, search, memory) primitives.

   figure: a learned program relying on both geometric primitives (pattern
   recognition, intuition) and algorithmic primitives (reasoning, search,
   memory).

beyond id26 and differentiable layers

   if machine learning models become more like programs, then they will
   mostly no longer be differentiable   certainly, these programs will still
   leverage continuous geometric layers as subroutines, which will be
   differentiable, but the model as a whole would not be. as a result,
   using id26 to adjust weight values in a fixed, hard-coded
   network, cannot be the method of choice for training models in the
   future   at least, it cannot be the whole story. we need to figure out to
   train non-differentiable systems efficiently. current approaches
   include id107, "evolution strategies", certain
   id23 methods, and admm (alternating direction method
   of multipliers). naturally, id119 is not going
   anywhere   gradient information will always be useful for optimizing
   differentiable parametric functions. but our models will certainly
   become increasingly more ambitious than mere differentiable parametric
   functions, and thus their automatic development (the "learning" in
   "machine learning") will require more than id26.

   besides, id26 is end-to-end, which is a great thing for
   learning good chained transformations, but is rather computationally
   inefficient since it doesn't fully leverage the modularity of deep
   networks. to make something more efficient, there is one universal
   recipe: introduce modularity and hierarchy. so we can make backprop
   itself more efficient by introducing decoupled training modules with
   some synchronization mechanism between them, organized in a
   hierarchical fashion. this strategy is somewhat reflected in deepmind's
   recent work on "synthetic gradients". i would expect more more work
   along these lines in the near future.

   one can imagine a future where models that would be globally
   non-differentiable (but would feature differentiable parts) would be
   trained   grown   using an efficient search process that would not leverage
   gradients, while the differentiable parts would be trained even faster
   by taking advantage of gradients using some more efficient version of
   id26.

automated machine learning

   in the future, model architectures will be learned, rather than
   handcrafted by engineer-artisans. learning architectures automatically
   goes hand in hand with the use of richer sets of primitives and
   program-like machine learning models.

   currently, most of the job of a deep learning engineer consists in
   munging data with python scripts, then lengthily tuning the
   architecture and hyperparameters of a deep network to get a working
   model   or even, to get to a state-of-the-art model, if the engineer is
   so ambitious. needless to say, that is not an optimal setup. but ai can
   help there too. unfortunately, the data munging part is tough to
   automate, since it often requires domain knowledge as well as a clear
   high-level understanding of what the engineer wants to achieve.
   hyperparameter tuning, however, is a simple search procedure, and we
   already know what the engineer wants to achieve in this case: it is
   defined by the id168 of the network being tuned. it is already
   common practice to set up basic "automl" systems that will take care of
   most of the model knob tuning. i even set up my own years ago to win
   kaggle competitions.

   at the most basic level, such a system would simply tune the number of
   layers in a stack, their order, and the number of units or filters in
   each layer. this is commonly done with libraries such as hyperopt,
   which we discussed in chapter 7 (note: of [14]deep learning with
   python). but we can also be far more ambitious, and attempt to learn an
   appropriate architecture from scratch, with as few constraints as
   possible. this is possible via id23, for instance, or
   id107.

   another important automl direction is to learn model architecture
   jointly with model weights. because training a new model from scratch
   every time we try a slightly different architecture is tremendously
   inefficient, a truly powerful automl system would manage to evolve
   architectures at the same time as the features of the model are being
   tuned via backprop on the training data, thus eliminating all
   computational redundancy. such approaches are already starting to
   emerge as i am writing these lines.

   when this starts happening, the jobs of machine learning engineers will
   not disappear   rather, engineers will move higher up the value creation
   chain. they will start putting a lot more effort into crafting complex
   id168s that truly reflect business goals, and understanding
   deeply how their models impact the digital ecosystems in which they are
   deployed (e.g. the users that consume the model's predictions and
   generate the model's training data)    problems that currently only the
   largest company can afford to consider.

lifelong learning and modular subroutine reuse

   if models get more complex and are built on top of richer algorithmic
   primitives, then this increased complexity will require higher reuse
   between tasks, rather than training a new model from scratch every time
   we have a new task or a new dataset. indeed, a lot datasets would not
   contain enough information to develop a new complex model from scratch,
   and it will become necessary to leverage information coming from
   previously encountered datasets. much like you don't learn english from
   scratch every time you open a new book   that would be impossible.
   besides, training models from scratch on every new task is very
   inefficient due to the large overlap between the current tasks and
   previously encountered tasks.

   additionally, a remarkable observation that has been made repeatedly in
   recent years is that training a same model to do several loosely
   connected tasks at the same time results in a model that is better at
   each task. for instance, training a same id4
   model to cover both english-to-german translation and french-to-italian
   translation will result in a model that is better at each language
   pair. training an image classification model jointly with an image
   segmentation model, sharing the same convolutional base, results in a
   model that is better at both tasks. and so on. this is fairly
   intuitive: there is always some information overlap between these
   seemingly disconnected tasks, and the joint model has thus access to a
   greater amount of information about each individual task than a model
   trained on that specific task only.

   what we currently do along the lines of model reuse across tasks is to
   leverage pre-trained weights for models that perform common functions,
   like visual feature extraction. you saw this in action in chapter 5. in
   the future, i would expect a generalized version of this to be
   commonplace: we would not only leverage previously learned features
   (submodel weights), but also model architectures and training
   procedures. as models become more like programs, we would start reusing
   program subroutines, like the functions and classes found in human
   programming languages.

   think of the process of software development today: once an engineer
   solves a specific problem (http queries in python, for instance), they
   will package it as an abstract and reusable library. engineers that
   face a similar problem in the future can simply search for existing
   libraries, download one and use it in their own project. in a similar
   way, in the future, meta-learning systems will be able to assemble new
   programs by sifting through a global library of high-level reusable
   blocks. when the system would find itself developing similar program
   subroutines for several different tasks, if would come up with an
   "abstract", reusable version of the subroutine and would store it in
   the global library. such a process would implement the capability for
   abstraction, a necessary component for achieving "extreme
   generalization": a subroutine that is found to be useful across
   different tasks and domains can be said to "abstract" some aspect of
   problem-solving. this definition of "abstraction" is similar to the
   notion of abstraction in software engineering. these subroutines could
   be either geometric (deep learning modules with pre-trained
   representations) or algorithmic (closer to the libraries that
   contemporary software engineers manipulate).

   a meta-learner capable of quickly developing task-specific models using
   reusable primitives (both algorithmic and geometric), thus achieving
   "extreme generalization".

   figure: a meta-learner capable of quickly developing task-specific
   models using reusable primitives (both algorithmic and geometric), thus
   achieving "extreme generalization".

in summary: the long-term vision

   in short, here is my long-term vision for machine learning:
     * models will be more like programs, and will have capabilities that
       go far beyond the continuous geometric transformations of the input
       data that we currently work with. these programs will arguably be
       much closer to the abstract mental models that humans maintain
       about their surroundings and themselves, and they will be capable
       of stronger generalization due to their rich algorithmic nature.
     * in particular, models will blend algorithmic modules providing
       formal reasoning, search, and abstraction capabilities, with
       geometric modules providing informal intuition and pattern
       recognition capabilities. alphago (a system that required a lot of
       manual software engineering and human-made design decisions)
       provides an early example of what such a blend between symbolic and
       geometric ai could look like.
     * they will be grown automatically rather than handcrafted by human
       engineers, using modular parts stored in a global library of
       reusable subroutines   a library evolved by learning high-performing
       models on thousands of previous tasks and datasets. as common
       problem-solving patterns are identified by the meta-learning
       system, they would be turned into a reusable subroutine   much like
       functions and classes in contemporary software engineering   and
       added to the global library. this achieves the capability for
       abstraction.
     * this global library and associated model-growing system will be
       able to achieve some form of human-like "extreme generalization":
       given a new task, a new situation, the system would be able to
       assemble a new working model appropriate for the task using very
       little data, thanks to 1) rich program-like primitives that
       generalize well and 2) extensive experience with similar tasks. in
       the same way that humans can learn to play a complex new video game
       using very little play time because they have experience with many
       previous games, and because the models derived from this previous
       experience are abstract and program-like, rather than a basic
       mapping between stimuli and action.
     * as such, this perpetually-learning model-growing system could be
       interpreted as an agi   an artificial general intelligence. but don't
       expect any singularitarian robot apocalypse to ensue: that's a pure
       fantasy, coming from a long series of profound misunderstandings of
       both intelligence and technology. this critique, however, does not
       belong here.

   [15]@fchollet, may 2017


    powered by [16]pelican, which takes great advantages of [17]python.

references

   1. https://blog.keras.io/
   2. https://blog.keras.io/index.html
   3. https://github.com/fchollet/keras
   4. https://blog.keras.io/
   5. https://github.com/fchollet/keras
   6. http://keras.io/
   7. https://groups.google.com/forum/#!forum/keras-users
   8. https://blog.keras.io/the-future-of-deep-learning.html
   9. https://twitter.com/fchollet
  10. https://blog.keras.io/category/essays.html
  11. https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff
  12. https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff
  13. https://blog.keras.io/the-limitations-of-deep-learning.html
  14. https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff
  15. https://twitter.com/fchollet
  16. http://alexis.notmyidea.org/pelican/
  17. http://python.org/
