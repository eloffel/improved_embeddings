   #[1]computational neuroscience lab    feed [2]computational neuroscience
   lab    comments feed [3]computational neuroscience lab    demystifying
   deep id23 comments feed [4]our new article now in
   arxiv:    multiagent cooperation and competition with deep reinforcement
   learning    [5]deep id23 with neon

[6]computational neuroscience lab

institute of computer science, university of tartu

main menu

   [7]skip to content
     * [8]recent
     * [9]people
     * [10]research
          + [11]thesis proposals
     * [12]publications
     * [13]teaching
     * [14]resources
     * [15]contact

demystifying deep id23

   [16]december 19, 2015    by [17]tambet matiisen   
   demystify_figures

   this is the part 1 of my series on deep id23. see
   part 2    [18]deep id23 with neon    for an actual
   implementation with neon deep learning toolkit.

   today, exactly two years ago, a small company in london called deepmind
   uploaded their pioneering paper    [19]playing atari with deep
   id23    to arxiv. in this paper they demonstrated how a
   computer learned to play atari 2600 video games by observing just the
   screen pixels and receiving a reward when the game score increased. the
   result was remarkable, because the games and the goals in every game
   were very different and designed to be challenging for humans. the same
   model architecture, without any change, was used to learn seven
   different games, and in three of them the algorithm performed even
   better than a human!

   it has been hailed since then as the first step towards [20]general
   artificial intelligence     an ai that can survive in a variety of
   environments, instead of being confined to strict realms such as
   playing chess. no wonder [21]deepmind was immediately bought by google
   and has been on the forefront of deep learning research ever since. in
   february 2015 their paper    [22]human-level control through deep
   id23    was featured on the cover of nature, one of the
   most prestigious journals in science. in this paper they applied the
   same model to 49 different games and achieved superhuman performance in
   half of them.

   still, while deep models for supervised and unsupervised learning have
   seen widespread adoption in the community, deep id23
   has remained a bit of a mystery. in this blog post i will be trying to
   demystify this technique and understand the rationale behind it. the
   intended audience is someone who already has background in machine
   learning and possibly in neural networks, but hasn   t had time to delve
   into id23 yet.

   the roadmap ahead:
    1. what are the main challenges in id23? we will
       cover the credit assignment problem and the
       exploration-exploitation dilemma here.
    2. how to formalize id23 in mathematical terms? we
       will define markov decision process and use it for reasoning about
       id23.
    3. how do we form long-term strategies? we define    discounted future
       reward   , that forms the main basis for the algorithms in the next
       sections.
    4. how can we estimate or approximate the future reward? simple
       table-based id24 algorithm is defined and explained here.
    5. what if our state space is too big? here we see how q-table can be
       replaced with a (deep) neural network.
    6. what do we need to make it actually work? experience replay
       technique will be discussed here, that stabilizes the learning with
       neural networks.
    7. are we done yet? finally we will consider some simple solutions to
       the exploration-exploitation problem.

id23

   consider the game breakout. in this game you control a paddle at the
   bottom of the screen and have to bounce the ball back to clear all the
   bricks in the upper half of the screen. each time you hit a brick, it
   disappears and your score increases     you get a reward.

   breakout
   figure 1: atari breakout game. image credit: deepmind.

   suppose you want to teach a neural network to play this game. input to
   your network would be screen images, and output would be three actions:
   left, right or fire (to launch the ball). it would make sense to treat
   it as a classification problem     for each game screen you have to
   decide, whether you should move left, right or press fire. sounds
   straightforward? sure, but then you need training examples, and a lots
   of them. of course you could go and record game sessions using expert
   players, but that   s not really how we learn. we don   t need somebody to
   tell us a million times which move to choose at each screen. we just
   need occasional feedback that we did the right thing and can then
   figure out everything else ourselves.

   this is the task id23 tries to solve. reinforcement
   learning lies somewhere in between supervised and unsupervised
   learning. whereas in supervised learning one has a target label for
   each training example and in unsupervised learning one has no labels at
   all, in id23 one has sparse and time-delayed labels    
   the rewards. based only on those rewards the agent has to learn to
   behave in the environment.

   while the idea is quite intuitive, in practice there are numerous
   challenges. for example when you hit a brick and score a reward in the
   breakout game, it often has nothing to do with the actions (paddle
   movements) you did just before getting the reward. all the hard work
   was already done, when you positioned the paddle correctly and bounced
   the ball back. this is called the credit assignment problem     i.e.,
   which of the preceding actions were responsible for getting the reward
   and to what extent.

   once you have figured out a strategy to collect a certain number of
   rewards, should you stick with it or experiment with something that
   could result in even bigger rewards? in the above breakout game a
   simple strategy is to move to the left edge and wait there. when
   launched, the ball tends to fly left more often than right and you will
   easily score on about 10 points before you die. will you be satisfied
   with this or do you want more? this is called the explore-exploit
   dilemma     should you exploit the known working strategy or explore
   other, possibly better strategies.

   id23 is an important model of how we (and all animals
   in general) learn. praise from our parents, grades in school, salary at
   work     these are all examples of rewards. credit assignment problems
   and exploration-exploitation dilemmas come up every day both in
   business and in relationships. that   s why it is important to study this
   problem, and games form a wonderful sandbox for trying out new
   approaches.

markov decision process

   now the question is, how do you formalize a id23
   problem, so that you can reason about it? the most common method is to
   represent it as a markov decision process.

   suppose you are an agent, situated in an environment (e.g. breakout
   game). the environment is in a certain state (e.g. location of the
   paddle, location and direction of the ball, existence of every brick
   and so on). the agent can perform certain actions in the environment
   (e.g. move the paddle to the left or to the right). these actions
   sometimes result in a reward (e.g. increase in score). actions
   transform the environment and lead to a new state, where the agent can
   perform another action, and so on. the rules for how you choose those
   actions are called policy. the environment in general is stochastic,
   which means the next state may be somewhat random (e.g. when you lose a
   ball and launch a new one, it goes towards a random direction).

   id23 markov decision process
   figure 2: left: id23 problem. right: markov decision
   process. image credit: wikipedia.

   the set of states and actions, together with rules for transitioning
   from one state to another and for getting rewards, make up a markov
   decision process. one episode of this process (e.g. one game) forms a
   finite sequence of states, actions and rewards:

   \(s_0, a_0, r_1, s_1, a_1, r_2, s_2,    , s_{n-1}, a_{n-1}, r_n, s_n\)

   here \(s_i\) represents the state, \(a_i\) is the action and
   \(r_{i+1}\) is the reward after performing the action. the episode ends
   with terminal state \(s_n\) (e.g.    game over    screen). a markov
   decision process relies on the markov assumption, that the id203
   of the next state \(s_{i+1}\) depends only on current state \(s_i\) and
   performed action \(a_i\), but not on preceding states or actions.

discounted future reward

   to perform well in long-term, we need to take into account not only the
   immediate rewards, but also the future awards we are going to get. how
   should we go about that?

   given one run of markov decision process, we can easily calculate the
   total reward for one episode:

   \(r=r_1+r_2+r_3+   +r_n\)

   given that, the total future reward from time point t onward can be
   expressed as:

   \(r_t=r_t+r_{t+1}+r_{t+2}+   +r_n\)

   but because our environment is stochastic, we can never be sure, if we
   will get the same rewards the next time we perform the same actions.
   the more into the future we go, the more it may diverge. for that
   reason it is common to use discounted future reward instead:

   \(r_t=r_t+\gamma r_{t+1}+\gamma^2 r_{t+2}   +\gamma^{n-t} r_n\)

   here \(\gamma\) is the discount factor between 0 and 1     the more into
   the future the reward is, the less we take it into consideration. it is
   easy to see, that discounted future reward at time step t can be
   expressed in terms of the same thing at time step t+1:

   \(r_t=r_t+\gamma (r_{t+1}+\gamma (r_{t+2}+   ))=r_t+\gamma r_{t+1}\)

   if we set the discount factor \(\gamma=0\), then our strategy will be
   short-sighted and we rely only on the immediate rewards. if we want to
   balance between immediate and future rewards, we should set discount
   factor to something like \(\gamma=0.9\). if our environment is
   deterministic and the same actions always result in same rewards, then
   we can set discount factor \(\gamma=1\).

   a good strategy for an agent would be to always choose an action, that
   maximizes the discounted future reward.

id24

   in id24 we define a function \(q(s,a)\) representing the
   discounted future reward when we perform action \(a\) in state \(s\),
   and continue optimally from that point on.

   \(q(s_t,a_t)=max_{\pi} r_{t+1}\)

   the way to think about \(q(s,a)\) is that it is    the best possible
   score at the end of game after performing action \(a\) in state \(s\)   .
   it is called q-function, because it represents the    quality    of certain
   action in given state.

   this may sound quite a puzzling definition. how can we estimate the
   score at the end of game, if we know just current state and action, and
   not the actions and rewards coming after that? we really can   t. but as
   a theoretical construct we can assume existence of such a function.
   just close your eyes and repeat to yourself five times:    \(q(s,a)\)
   exists, \(q(s,a)\) exists,       . feel it?

   if you   re still not convinced, then consider what the implications of
   having such a function would be. suppose you are in state \(s\) and
   pondering whether you should take action \(a\) or \(b\). you want to
   select the action, that results in the highest score at the end of
   game. once you have the magical q-function, the answer becomes really
   simple     pick the action with the highest q-value!

   \(\pi(s) =argmax_a q(s,a)\)

   here \(\pi\) represents the policy, the rule how we choose an action in
   each state.

   ok, how do we get that q-function then? let   s focus on just one
   transition \(<s,a,r,s   >\). just like with discounted future rewards in
   previous section we can express q-value of state \(s\) and action \(a\)
   in terms of q-value of next state \(s   \).

   \(q(s,a)=r + \gamma max_{a   }q(s   ,a   )\)

   this is called the bellman equation. if you think about it, it is quite
   logical     maximum future reward for this state and action is the
   immediate reward plus maximum future reward for the next state.

   the main idea in id24 is that we can iteratively approximate the
   q-function using the bellman equation. in the simplest case the
   q-function is implemented as a table, with states as rows and actions
   as columns. the gist of id24 algorithm is as simple as the
   following:
initialize q[numstates,numactions] arbitrarily
observe initial state s
repeat
    select and carry out an action a
    observe reward r and new state s'
    q[s,a] = q[s,a] +   (r +   maxa' q[s',a'] - q[s,a])
    s = s'
until terminated

      in the algorithm is a learning rate that controls how much of the
   difference between previous q-value and newly proposed q-value is taken
   into account. in particular, when   =1, then two q[s,a]-s cancel and the
   update is exactly the same as bellman equation.

   max[a   ] q[s',a'] that we use to update q[s,a] is only an estimation and
   in early stages of learning it may be completely wrong. however the
   estimations get more and more accurate with every iteration and [23]it
   has been shown, that if we perform this update enough times, then the
   q-function will converge and represent the true q-value.

deep q network

   the state of the environment in the breakout game can be defined by the
   location of the paddle, location and direction of the ball and the
   existence of each individual brick. this intuitive representation is
   however game specific. could we come up with something more universal,
   that would be suitable for all the games? obvious choice is screen
   pixels     they implicitly contain all of the relevant information about
   the game situation, except for the speed and direction of the ball. two
   consecutive screens would have these covered as well.

   if we would apply the same preprocessing to game screens as in the
   deepmind paper     take four last screen images, resize them to 84  84 and
   convert to grayscale with 256 gray levels     we would have \(256^{84
   \times 84 \times 4} \approx 10^{67970}\) possible game states. this
   means \(10^{67970}\) rows in our imaginary q-table     that is more than
   the number of atoms in the known universe! one could argue that many
   pixel combinations and therefore states never occur     we could possibly
   represent it as a sparse table containing only visited states. even so,
   most of the states are very rarely visited and it would take a lifetime
   of the universe for the q-table to converge. ideally we would also like
   to have a good guess for q-values for states we have never seen before.

   this is the point, where deep learning steps in. neural networks are
   exceptionally good in coming up with good features for highly
   structured data. we could represent our q-function with a neural
   network, that takes the state (four game screens) and action as input
   and outputs the corresponding q-value. alternatively we could take only
   game screens as input and output the q-value for each possible action.
   this approach has the advantage, that if we want to perform a q-value
   update or pick the action with highest q-value, we only have to do one
   forward pass through the network and have all q-values for all actions
   immediately available.

   deep q networks
   figure 3: left: naive formulation of deep q-network. right: more
   optimal architecture of deep q-network, used in deepmind paper.

   the network architecture that deepmind used is as follows:
   layer input    filter size stride num filters activation output
   conv1 84x84x4  8  8         4      32          relu       20x20x32
   conv2 20x20x32 4  4         2      64          relu       9x9x64
   conv3 9x9x64   3  3         1      64          relu       7x7x64
   fc4   7x7x64                      512         relu       512
   fc5   512                         18          linear     18

   this is a classical convolutional neural network with three
   convolutional layers, followed by two fully connected layers. people
   familiar with object recognition networks may notice that there are no
   pooling layers. but if you really think about that, then pooling layers
   buy you a translation invariance     the network becomes insensitive to
   the location of an object in the image. that makes perfectly sense for
   a classification task like id163, but for games the location of the
   ball is crucial in determining the potential reward and we wouldn   t
   want to discard this information!

   input to the network are four 84  84 grayscale game screens. outputs of
   the network are q-values for each possible action (18 in atari).
   q-values can be any real values, which makes it a regression task, that
   can be optimized with a simple squared error loss.

   \(l=\frac{1}{2}[\underbrace{r + \gamma
   max_{a'}q(s',a')}_{\text{target}} -
   \underbrace{q(s,a)}_{\text{prediction}}]^2\)

   given a transition \(<s,a,r,s   >\), the q-table update rule in the
   previous algorithm must be replaced with the following:
    1. do a feedforward pass for the current state \(s\) to get predicted
       q-values for all actions.
    2. do a feedforward pass for the next state \(s   \) and calculate
       maximum over all network outputs \(max_{a   }q(s   ,a   )\).
    3. set q-value target for action \(a\) to \(r+\gamma
       max_{a   }q(s   ,a   )\) (use the max calculated in step 2). for all
       other actions, set the q-value target to the same as originally
       returned from step 1, making the error 0 for those outputs.
    4. update the weights using id26.

experience replay

   by now we have an idea how to estimate the future reward in each state
   using id24 and approximate the q-function using a convolutional
   neural network. but it turns out that approximation of q-values using
   non-linear functions is not very stable. there is a whole bag of tricks
   that you have to use to actually make it converge. and it takes a long
   time, almost a week on a single gpu.

   the most important trick is experience replay. during gameplay all the
   experiences \(<s,a,r,s   >\) are stored in a replay memory. when training
   the network, random samples from the replay memory are used instead of
   the most recent transition. this breaks the similarity of subsequent
   training samples, which otherwise might drive the network into a local
   minimum. also experience replay makes the training task more similar to
   usual supervised learning, which simplifies debugging and testing the
   algorithm. one could actually collect all those experiences from human
   gameplay and the train network on these.

exploration-exploitation

   id24 attempts to solve the credit assignment problem     it
   propagates rewards back in time, until it reaches the crucial decision
   point which was the actual cause for the obtained reward. but we
   haven   t touched the exploration-exploitation dilemma yet   

   firstly observe, that when a q-table or q-network is initialized
   randomly, then its predictions are initially random as well. if we pick
   an action with the highest q-value, the action will be random and the
   agent performs crude    exploration   . as a q-function converges, it
   returns more consistent q-values and the amount of exploration
   decreases. so one could say, that id24 incorporates the
   exploration as part of the algorithm. but this exploration is    greedy   ,
   it settles with the first effective strategy it finds.

   a simple and effective fix for the above problem is   -greedy
   exploration     with id203    choose a random action, otherwise go
   with the    greedy    action with the highest q-value. in their system
   deepmind actually decreases    over time from 1 to 0.1     in the
   beginning the system makes completely random moves to explore the state
   space maximally, and then it settles down to a fixed exploration rate.

deep id24 algorithm

   this gives us the final deep id24 algorithm with experience
   replay:
initialize replay memory d
initialize action-value function q with random weights
observe initial state s
repeat
    select an action a
        with id203    select a random action
        otherwise select a = argmaxa   q(s,a   )
    carry out action a
    observe reward r and new state s   
    store experience <s, a, r, s   > in replay memory d

    sample random transitions <ss, aa, rr, ss   > from replay memory d
    calculate target for each minibatch transition
        if ss    is terminal state then tt = rr
        otherwise tt = rr +   maxa   q(ss   , aa   )
    train the q network using (tt - q(ss, aa))^2 as loss

    s = s'
until terminated

   there are many more tricks that deepmind used to actually make it work
       like target network, error clipping, reward clipping etc, but these
   are out of scope for this introduction.

   the most amazing part of this algorithm is that it learns anything at
   all. just think about it     because our q-function is initialized
   randomly, it initially outputs complete garbage. and we are using this
   garbage (the maximum q-value of the next state) as targets for the
   network, only occasionally folding in a tiny reward. that sounds
   insane, how could it learn anything meaningful at all? the fact is,
   that it does.

final notes

   many improvements to deep id24 have been proposed since its first
   introduction     [24]double id24, [25]prioritized experience
   replay, [26]dueling network architecture and [27]extension to
   continuous action space to name a few. for latest advancements check
   out the [28]nips 2015 deep id23 workshop and [29]iclr
   2016 (search for    reinforcement    in title). but beware, that [30]deep
   id24 has been patented by google.

   it is often said, that artificial intelligence is something we haven   t
   figured out yet. once we know how it works, it doesn   t seem intelligent
   any more. but deep q-networks still continue to amaze me. watching them
   figure out a new game is like observing an animal in the wild     a
   rewarding experience by itself.

credits

   thanks to ardi tampuu, tanel p  rnamaa, jaan aru, ilya kuzovkin, arjun
   bansal and urs k  ster for comments and suggestions on the drafts of
   this post.

links

     * [31]david silver   s lecture about deep id23
     * [32]slightly awkward but accessible illustration of id24
     * [33]uc berkley   s course on deep id23
     * [34]rich sutton   s tutorial on id23
     * [35]david silver   s id23 course
     * [36]nando de freitas    course on machine learning (two lectures
       about id23 in the end)
     * [37]andrej karpathy   s course on convolutional neural networks


   tags: [38]ai, [39]deep learning, [40]id23

70 responses on    demystifying deep id23   

    1. pingback: [41]deep id23 with neon | computational
       neuroscience lab  
    2.
   antianticamper [42]december 26, 2015 at 1:58 pm       [43]reply    
       very nice explanation, especially subtle points like the update
       rule for all actions simultaneously. but even without pooling,
       parameter tying in the convolutional layers implements a form of
       translational invariance which, as you mention, does not seem
       appropriate in this situation. so why the use of shared parameters?
          +
        tambet matiisen [44]december 26, 2015 at 9:45 pm       [45]reply    
            you are absolutely right     convolutional layers implement
            translation invariance too, but they don   t throw away the
            location information the same way as pooling layers do. the
            feature map produced by convolutional filter still has the
            information where the filter matched. this opinion about
            pooling layers has been expressed several times by geoffrey
            hinton:
            [46]https://www.quora.com/why-does-geoff-hinton-say-that-its-s
            urprising-that-pooling-in-id98s-work
            other than that, using convolutional layers makes sense here,
            because the same object (the ball for example) can occur
            anywhere on the screen. it would be wasteful to learn separate
               ball detector    for each part of the screen. weight sharing
            saves a lot of parameters in those layers and allows learning
            to progress much faster.
            this is not the same for all use cases     for example if your
            task is face recognition and input to your network is
            pre-cropped face, then the nose doesn   t appear anywhere on the
            image, it is predominantly in the center. that   s why facebook
            uses locally connected layers instead of convolutional layers
            in their deepface implementation.
    3.
   stewie [47]january 7, 2016 at 4:02 pm       [48]reply    
       thank you so much for this article.
       but i   m still not clear how to get the immediate reward value by
       looking at the pixels.
       for example, in the end of a game, you observed the final score is
       54, what does it mean?
       for most of the games, the higher is better, but for games like
       uno, less is better.
       how can you determine the performance of an action by looking at
       pixels?
       this is even harder for the immediate reward as the score might not
       be changed.
          +
        [49]mat kelcey [50]january 8, 2016 at 12:56 am       [51]reply    
            in the deepmind result the score wasn   t derived from the
            pixels, it was explicitly provided. reward = change in score.
            see section 2 of their    playing atari with deep reinforcement
            learning    paper.
               o
             stewie [52]january 8, 2016 at 9:26 pm       [53]reply    
                 thank you so much.
                 i found this in that paper: in addition it receives a
                 reward rt representing the change in game score.
                 they need human labelled reward rt after all.
                    #
                  tambet matiisen [54]january 8, 2016 at 9:55 pm      
                      [55]reply    
                      arcade learning environment has    plugin    for each
                      game, which knows where in memory the game keeps the
                      score. because atari 2600 has only 128 bytes of ram,
                      figuring out the memory location and writing a
                      plugin is not that hard. list of plugins for
                      supported games in a.l.e. can be found here:
                      [56]https://github.com/mgbellemare/arcade-learning-e
                      nvironment/tree/master/src/games/supported
    4.
   alex [57]january 25, 2016 at 1:57 pm       [58]reply    
       thank you for this valuable post.
       i have become interested in id23 and i would like
       to explore its potential applications in business and economic
       research. in economics we are interested in not only the
       performance of a model but also its interpretability. could you
       please advise me as to whether it is possible in reinforcement
       learning to understand why a decision is taken or visualise a
       pattern of action or to interpret the overall result?
       many thanks,
       alex
          +
        tambet matiisen [59]january 27, 2016 at 9:02 pm       [60]reply    
            no, i don   t think id23 helps you with
            interpretability. yes, you can say that this particular action
            was chosen because it had the highest future reward. but how
            it estimated this future reward still depends on what function
            approximation technique you use. in case of deep neural
            networks the guided id26 technique
            ([61]https://github.com/lasagne/recipes/blob/master/examples/s
            aliency%20maps%20and%20guided%20id26.ipynb) has
            been found useful by some.
    5.
   shi [62]february 24, 2016 at 12:47 am       [63]reply    
       hi tambet,
       thank you for this article. but i really had some hard-time
       understanding why the bellman equation iteration thing converges.
       then i realized that one of the equations in your article seems to
       be wrong:
       in this equation, you missed the    (discount factor), without   , it
       shouldn   t always converge?
       q(s,a)=r+maxa   q(s   ,a   )
       as in wikipedia
       [64]https://en.wikipedia.org/wiki/bellman_equation#cite_note-neurod
       ynprog-4
       the discount factor    b    should be 0 < b < 1
       so you can't ignore it. otherwise it won't necessarily converge, am
       i right?
          +
        tambet matiisen [65]february 25, 2016 at 6:23 am       [66]reply    
            yes shi, you are right, it wouldn   t converge without discount
            factor. i fixed the formula, thanks for spotting this!
            you may spot some other simplifications, for example the
            definition of q-value should include expectation over all
            possible paths (because our environment might be stochastic).
            i deliberately chose to do this, to keep the post simple and
            notation minimal. refer to original papers or links in the end
            for more formal treatment.
    6.
   shuang [67]march 10, 2016 at 11:49 pm       [68]reply    
       thanks for the post, very clear. i have one question: is there a
       missing    in the equation for squared error loss l?
          +
        tambet matiisen [69]march 11, 2016 at 6:10 am       [70]reply    
            thanks shuang, yes i was missing gamma there. now fixed.
    7.
   pedro marcal [71]april 22, 2016 at 5:28 pm       [72]reply    
       very useful review of id25. i think a lot of credit on id24
       should go to c. watkins, whose thesis at cambridge established the
       q theory that was so useful to deep mind. in any case watkins
       theory was an important backgrounder to understanding the delayed
       reward system in id23. i am puzzled as to why
       google was granted a patent on deep id23 given
       the general theory formulated by watkins.
          +
        tambet matiisen [73]april 24, 2016 at 5:06 am       [74]reply    
            thanks pedro for pointing that out. as this is blog post not
            academical paper, i will leave proper referencing to original
            papers. both deepmind   s papers cite watkin   s id24 paper
            from 1992.
    8.
   serra [75]april 29, 2016 at 9:28 pm       [76]reply    
       hi, very good article.
       i just compared the network architecture from the deepmind paper
       ([77]http://arxiv.org/abs/1312.5602) with the one you listed and
       they seem to differ. in the paper they used only 3 hidden layers (2
       conv + 1 fc). do you have a different source or am i missing
       something?
       thanks.
          +
        tambet matiisen [78]april 30, 2016 at 5:19 am       [79]reply    
            yes, i   m using network architecture from their nature paper
               human-level control through deep reinforcement
            learning   .
            [80]https://storage.googleapis.com/deepmind-data/assets/papers
            /deepmindnature14236paper.pdf
    9.
   ricard racinskij [81]june 12, 2016 at 3:55 pm       [82]reply    
       thank you for such a great article!
       i created a toy scene in unity and adapted an available torch
       implementation of your blog post for it
       (www.github.com/seannaren/torchqlearningexample). it looks like the
       neural network tends to learn some set of q values and then sticks
       with them, even if subsequent behaviour of the agent is obviously
       suboptimal. which parameter (lr, epsilon, discount factor os
       something else) is the key to solving this issue? does it make
       sense to use a recurrent net, such as lstm, instead of feedforward
       network?
   10.
   andy t [83]june 20, 2016 at 8:04 pm       [84]reply    
       dear tambet,
       thank you for the excellent explanation!
       i have some beginner questions that i hope you could help with:
       the    deep id24 algorithm    section makes sense, but i am
       wondering if we actually store random transitions of sufficient
       length in our replay memory (i.e. four last screen images, the
       action taken on the last frame, the next reward, and the next
       state) rather than just one state per entry. otherwise, how could
       we recover the frames?
       next, once we retrieve these four last states, i notice we only
       choose the last action as the input to the neural network. if a
       critical step occurred before this time step, it would need to be
       randomly sampled to be properly captured (that   s fine, since the
       same argument applies as we move into the future, which is why this
       works to begin with). it seems that this sampling actually provides
       a chance to ignore updates when there is too little of a difference
       between actions (i.e. none of the actions really made a big
       difference).
   11. pingback: [85]deep-q     m  moires  
   12.
   thijs [86]november 6, 2016 at 10:09 am       [87]reply    
       i have 2 questions:
       1) when implementing this, what do you store in the experience
       replay buffer? the 512 inputs of fc5? that would be a compact state
       representation.
       2) (if so) then while training, the mapping function between the
       screenshots and the 512 size state representation stored in the
       buffer will change -it will learn more relevant representations-.
       that change will invalidate the state representations in the
       experience buffer. how is this handled? is the experience buffer
       being refreshed like a fifo buffer?
          +
        tambet matiisen [88]november 6, 2016 at 6:32 pm       [89]reply    
            the states are stored in replay memory as raw images.
            otherwise it wouldn   t be clear how you learn the convolutional
            layers.
   13.
   lorenzo [90]november 9, 2016 at 3:35 pm       [91]reply    
       thank you very much for your tutorial!
       finally someone explained easily how id26 with multiple
       action output works.
       i have two questions:
       1) normally in id24 we have an alfa in order to decide how
       easily new information will be overwritten. how can we express that
       now?
       2) when we are backpropagating with the error on the chosen action
       and 0 error for all the other one, aren   t we giving    false   
       information to the network? the other qvalues might be very far
       from target value, but we are basically telling the network that
       they are perfect values ( 0 error). is it ok since all q values
       will eventually converge to q*?
          +
        tambet matiisen [92]november 10, 2016 at 3:22 pm       [93]reply    
            1) the learning rate of network now plays the role of alfa.
            2) you are right. because we only have information about the
            action we took, there is now good basis for changing the other
            q-values. one way to overcome this is to use dueling
            architecture ([94]https://arxiv.org/abs/1511.06581) that
            decomposes q-value into state-value and advantage. this way
            the q-values of other actions change as well, because you are
            also modifying the baseline state-value. i have basic version
            of dueling architecture implemented here:
            [95]https://github.com/tambetm/gymexperiments. if you dig
            deeper, you can also find policy gradient and a3c
            implementations, but they are not mentioned in the readme :) .
               o
             naim [96]november 29, 2016 at 10:05 pm       [97]reply    
                 i   m trying to implement the dueling architecture, but i   m
                 having trouble understanding how to do id26
                 after calculating the error.
                 i can   t understand how to calculate deltas of value
                 function and advantage functions using the error i got
                 from q(s,a).
                 it   s difficult because we get from the previous duel
                 streams to the final q output without using weights.
                 any help is appreciated, and thanks a lot for the
                 tutorial it has been very helpful. i wish there were
                 tutorials like that for all other machine learning
                 algorithms.
                    #
                  tambet matiisen [98]december 12, 2016 at 12:54 am      
                      [99]reply    
                      i   m not sure if i got you right, but if your q-value
                      q(s,a) = a(s,a) + v(s), then gradient (delta) for
                      a(s,a) is the same as for q(s,a). the gradient for
                      v(s) is sum of all gradients for q(s,a)-s. but if
                      you are using modern automatic differentiation
                      library then you really shouldn   t worry too much
                      about that. for example here is one implementation
                      of dueling architectures:
                      [100]https://github.com/tambetm/gymexperiments/blob/
                      master/duel.py.
   14.
   joshua staker [101]november 18, 2016 at 6:25 pm       [102]reply    
       i have a question regarding this paragraph:
          this is a classical convolutional neural network with three
       convolutional layers, followed by two fully connected layers.
       people familiar with object recognition networks may notice that
       there are no pooling layers. but if you really think about that,
       then pooling layers buy you a translation invariance     the network
       becomes insensitive to the location of an object in the image. that
       makes perfectly sense for a classification task like id163, but
       for games the location of the ball is crucial in determining the
       potential reward and we wouldn   t want to discard this information!   
       but in their architecture, they are performing convolution with
       stride, so the data is still getting downsampled. my intuition with
       max-pooling is that pooling happens after the learned filters are
       applied to the data, so the network learns what information is
       useful to pass to the next layer, and if downsampling is to be
       performed, it might as well be pooling so that we also get the
       invariance benefits at the same time. what am i missing? how does
       convolution with stride>1 preserve spatial information better that
       stride=1 with pooling?
          +
        tambet matiisen [103]november 19, 2016 at 6:52 pm       [104]reply    
            joshua, you are right, convolution with stride >1 is basically
            equivalent to max pooling     see this paper
            [105]https://arxiv.org/abs/1412.6806. so in the hindsight
            maybe that comment wasn   t too well justified, but at least it
            makes a good story :) .
               o
             joshua staker [106]november 19, 2016 at 7:30 pm      
                 [107]reply    
                 gotcha. thanks so much for the clarification.
   15.
   omer korech [108]november 26, 2016 at 6:09 am       [109]reply    
       thanks a lot.
       a small typo: the first word in the following citation from your
       blog should be deleted
          maximum future reward for this state and action is the immediate
       reward plus maximum future reward for the next state.   
          +
        tambet matiisen [110]december 12, 2016 at 12:47 am       [111]reply
               
            thanks omer! i guess i wanted to emphasize the case when
            reward function r(s,a) is deterministic and by adding reward
            and max discounted future reward from next state you actually
            get max discounted future reward for this state. but i must
            admit i was bit lax with definitions and expectations, for the
            sake of making this approachable to mere programmers as
            myself.
   16.
   fazil [112]december 6, 2016 at 8:38 pm       [113]reply    
       thank you for the article it is very clear
       i have one question
       is i can apply id24 for two agents at the same time for
       example (avoid collision for two people face to face)
          +
        tambet matiisen [114]december 12, 2016 at 12:57 am       [115]reply
               
            yes, you can apply id24 to two agents interacting with
            each other, see our paper on cooperative and competitive
            behaviour: [116]https://arxiv.org/abs/1511.08779. although
            there are no theoretical guarantees of convergence, in
            practice it tends to work fine.
               o
             fazil [117]january 16, 2017 at 8:08 pm       [118]reply    
                 thank   s
                 i need an implementation model for id24 if possible
   17.
   kerawit [119]january 30, 2017 at 10:26 am       [120]reply    
       i love your article. thank you so much. i   m trying to implement id25
       with tensorflow to learn breakout wih gym. it is normal that in the
       beginning qmax from my networks starts at a very high value
       (thousands, sometimes millions)?
          +
        tambet matiisen [121]january 31, 2017 at 8:45 am       [122]reply    
            that must be a bug. randomly initialized network produces
            q-values around 0 and it should gradually go up.
               o
             kerawit [123]january 31, 2017 at 1:56 pm       [124]reply    
                 thanks to you, i have managed to get the qmax in the
                 beginning down close to zero by setting stddev of the
                 initial random weights to 0.01 instead of 0.1.
                 is there any particular reason that you chose not to do
                 padding at the convolution layers?
                    #
                  tambet matiisen [125]february 5, 2017 at 5:20 pm      
                      [126]reply    
                      no.
   18.
   dora [127]march 1, 2017 at 4:45 am       [128]reply    
       very well explained.
       i   m trying to learn a policy for a game using rl. sometimes there
       are invalid moves then the agent gets negative reward & the game
       state doesn   t change. the agent is trained with these experiences.
       but the agent is making the same invalid move again and again. also
       with this, the game state is not changing. it is stuck in infinite
       recursive loop. any suggestions to improve?
   19.
   [129]adam [130]march 10, 2017 at 1:52 am       [131]reply    
       tambet,
       i   m a bit confused     i have a copy of the atari paper that
       describes a different network structure:
          we now describe the exact architecture used for all seven atari
       games. the input to the neural
       network consists is an 84    84    4 image produced by   . the first
       hidden layer convolves 16 8    8
       filters with stride 4 with the input image and applies a rectifier
       nonlinearity [10, 18]. the second
       hidden layer convolves 32 4    4 filters with stride 2, again
       followed by a rectifier nonlinearity. the
       final hidden layer is fully-connected and consists of 256 rectifier
       units.   
       this is different from the network you describe in the table above
           the network you describe is a five layer network.
          +
        tambet matiisen [132]march 10, 2017 at 7:05 am       [133]reply    
            i think you are referring to the earlier nips workshop paper.
            my post was based on the later nature article:
            [134]http://web.stanford.edu/class/psych209/readings/mnihetalh
            assibis15naturecontroldeeprl.pdf
   20.
   vobien [135]april 2, 2017 at 4:30 am       [136]reply    
       perfect! helpful for newbie about id23.
       thank you.
   21.
   vineet mehta [137]april 11, 2017 at 11:03 am       [138]reply    
       superb. i have been searching for this since months. i loved the
       way you explained each and everything. please continue doing this.
       really helpful!
       thanks alot again.
   22.
   digital nomad [139]april 18, 2017 at 3:29 pm       [140]reply    
       did you know google has pattented id25. your comment on that?
          +
        tambet matiisen [141]april 19, 2017 at 5:25 am       [142]reply    
            you can check the discussion from reddit:
            [143]https://www.reddit.com/r/machinelearning/comments/4z5oa4/
            is_google_patenting_id25_really_justified/
            in short:
            1. it is arguable if the patent is justified.
            2. google probably did it just to prevent others from
            patenting it.
            3. probably it will never be enforced, and by now there are
            many id25 variations + a3c and others, which makes it
            questionable if the patent applies to them.
   23. pingback: [144]design an artificial agent to play vikingmud    
       grensesnittet  
   24.
   dan [145]june 8, 2017 at 6:59 pm       [146]reply    
       hi.
       in the algorithm section:
       select an action a
       with id203    select a random action
       otherwise select a = argmaxa   q(s,a   )
       carry out action a
       should the >ss   < ?
          +
        tambet matiisen [147]june 9, 2017 at 8:33 pm       [148]reply    
            no, this is the epsilon-exploration. a    might confuse you, but
            it is just placeholder for argmax.
   25.
   mark [149]june 23, 2017 at 10:25 am       [150]reply    
       hi tambet,
       really useful and helpful article     thank you! one question, is it
       possible to use deep id24 when we have continuous rather than
       discrete actions? for example, i understand how it works when the
       possible actions are discrete like    up   ,    down   ,    left    or    right   ,
       but what if say we are trying to train a car to drive autonomously
       by id23 and want the nn to give us a steering
       angle output. obviously we would want the steering angle which
       maximises q but is this achievable when the possible values of
       steering angle are continuous rather than discrete?
       thanks for the help!
          +
        tambet matiisen [151]june 26, 2017 at 9:05 am       [152]reply    
            take a look on [153]https://arxiv.org/abs/1509.02971.
   26.
   [154]alexander yau [155]july 5, 2017 at 11:28 am       [156]reply    
       hi, thank you, a good post, my question is why the id168
       (tt     q(ss, aa))^2 is right and works?
          +
        tambet matiisen [157]august 21, 2017 at 6:04 am       [158]reply    
            squared error is the most common id168 for regression
            tasks.
   27.
   xiaokang wang [159]august 19, 2017 at 12:06 am       [160]reply    
       thanks tambet for very informative and clear blog.
       i have a question about learning rate, alpha, which is involved in
       the following equation.
       q[s,a] = q[s,a] +   (r +   maxa    q[s',a']     q[s,a])
       but how does the alpha play a role in the regression task as it is
       not involved in the mean square loss?
          +
        tambet matiisen [161]august 21, 2017 at 6:02 am       [162]reply    
            alpha is included implicitly in the method that you use to
            optimize the id168. for example if you use gradient
            descent for neural networks, learning rate plays the role of
            alpha.
               o
             xiaokang wang [163]august 31, 2017 at 10:07 pm       [164]reply
                    
                 hi tambet,
                 thanks for your reply. the alpha just scales down or up
                 the loss and also the gradient in   (r +   maxa    q[s',a']    
                 q[s,a]), so it in fact serves as tuning the learning rate
                 when updating the weights.
   28.
   gordon boateng [165]october 30, 2017 at 6:42 am       [166]reply    
       thanks, tambet for your post. very helpful. i am new to rl and i
       would like to ask whether you have other posts that link id25 to
       cloud resource allocation for delay sensitive industrial networks
          +
        tambet matiisen [167]october 30, 2017 at 12:56 pm       [168]reply    
            i haven   t heard much about deep id23
            deployed in industry. there is deepmind   s blog post about
            [169]improving cooling efficiency in google datacenters, but
            it does not give out any technical details. i   d be happy to
            have this information myself.
   29.
   mian shah [170]december 8, 2017 at 7:42 pm       [171]reply    
       thank you tambet. this was a great post. very helpful.
   30.
   tony [172]february 13, 2018 at 7:59 am       [173]reply    
       your improper grasp of comma usage was highly distracting! other
       than that, it   s a very good post. thank you.
   31.
   maxim afteniy [174]march 25, 2018 at 8:11 pm       [175]reply    
       hi, i   ve tried to implement the performance of deepmind   s paper,
       but i   ve stuck because my id25 is hardly learning anything. i think
       that the problem is in frame-skipping. (atleast i havent found
       anything else). currently i push the skipped memory transition
       after taking the repeat action. is it correct?
   32. pingback: [176]tensorflowda basit takviyeli     renme: tablolar ve
       sinir a  lar   ile id24     devhunter  
   33. pingback: [177]deep id24 for video games - the math of
       intelligence #9 - artificial intelligence videos  
   34. pingback: [178]id23 a-z                               
   35.
   mohamed hammad [179]may 12, 2018 at 11:16 am       [180]reply    
       that was an excellent explanation to the blend between
       id23 and deep learning.
       thank you.
   36.
   [181]deep learning training in pune [182]june 7, 2018 at 10:55 am      
       [183]reply    
       this is a very informative discussion about deep learning
       thanks for sharing
   37.
   tiago [184]june 20, 2018 at 11:58 am       [185]reply    
       from the example i saw, it is all the time, related to images.
       is deep id24 useful in cases where we have the prescreens of
       the graphical component of the game?
          +
        tambet matiisen [186]june 20, 2018 at 12:25 pm       [187]reply    
            the question is not about id24, but what architecture of
            network to use for function approximator in id24. with
            images you use convolutional neural network, but if your input
            is not spatial in nature, you can use just vector as input and
            fully connected network. id24 does not care.
   38.
   aj [188]july 31, 2018 at 11:22 am       [189]reply    
       thanks for an excellent explanation   . can you please clarify what
       exactly is meant by    .    maximum over all network       in the neural
       network q update learning in the line   ..    do a feedforward pass for
       the next state s    and calculate maximum over all network outputs
       maxa   q(s   ,a   ).   
   39.
   mohsen afsahi [190]august 21, 2018 at 6:03 am       [191]reply    
       thanks for the good explanation.
       i am looking for a paper or any other informative websites with
       clues about the modeling of the environment other than images.
       as you know most of the deeprl applications consider images as
       states. i am interested in the applications other than games and
       images.
          +
        tambet matiisen [192]august 29, 2018 at 10:10 am       [193]reply    
            there are not many examples. you can find some works in
            financial field here:
            [194]https://www.quora.com/what-are-the-latest-works-on-reinfo
            rcement-learning-in-the-financial-field.

leave a reply [195]cancel reply

   your email address will not be published. required fields are marked *

   name * ______________________________

   email * ______________________________

   website ______________________________

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   you may use these html tags and attributes: <a href="" title=""> <abbr
   title=""> <acronym title=""> <b> <blockquote cite=""> <cite> <code>
   <del datetime=""> <em> <i> <q cite=""> <strike> <strong>

   post comment

   search ____________________ search

   caption:

   [196]back
   [197]next
   april  2019
   m  t  w  t  f  s s
   1
     * cns group meeting
       starts: 12:15 pm
       ends: april 1, 2019 - 1:30 pm
       [198]more details...

   2  3  4  5  6  7
   8
     * cns group meeting
       starts: 12:15 pm
       ends: april 8, 2019 - 1:30 pm
       [199]more details...

   9  10 11 12 13 14
   15
     * cns group meeting
       starts: 12:15 pm
       ends: april 15, 2019 - 1:30 pm
       [200]more details...

   16 17 18 19 20 21
   22
     * cns group meeting
       starts: 12:15 pm
       ends: april 22, 2019 - 1:30 pm
       [201]more details...

   23 24 25 26 27 28
   29
     * cns group meeting
       starts: 12:15 pm
       ends: april 29, 2019 - 1:30 pm
       [202]more details...

   30

tags

   [203]ai [204]bci [205]brain-computer interface [206]id98 [207]computer
   science [208]control problem [209]course [210]data analysis [211]deep
   learning [212]eeg [213]emotiv epoc [214]gamma [215]human brain
   [216]intracranial [217]neon [218]openbci [219]opengl [220]optimization
   [221]python [222]id23 [223]robotex [224]seminar
   [225]source localization [226]teaching [227]webpage

   search ____________________ search

blogroll

meta

     * [228]log in

   [229]proudly powered by wordpress | theme: oxygen by [230]alienwp.

references

   1. https://neuro.cs.ut.ee/feed/
   2. https://neuro.cs.ut.ee/comments/feed/
   3. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/feed/
   4. https://neuro.cs.ut.ee/our-new-article-now-in-arxiv-multiagent-cooperation-and-competition-with-deep-reinforcement-learning/
   5. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/
   6. https://neuro.cs.ut.ee/
   7. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#content
   8. http://neuro.cs.ut.ee/
   9. https://neuro.cs.ut.ee/people/
  10. https://neuro.cs.ut.ee/lab/
  11. https://neuro.cs.ut.ee/lab/topics/
  12. https://neuro.cs.ut.ee/publications/
  13. https://neuro.cs.ut.ee/teaching/
  14. https://neuro.cs.ut.ee/resources/
  15. https://neuro.cs.ut.ee/contact/
  16. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
  17. https://neuro.cs.ut.ee/author/tambet-matiisen/
  18. http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/
  19. http://arxiv.org/abs/1312.5602
  20. https://en.wikipedia.org/wiki/artificial_general_intelligence
  21. http://techcrunch.com/2014/01/26/google-deepmind/
  22. http://www.nature.com/articles/nature14236
  23. http://users.isr.ist.utl.pt/~mtjspaan/readinggroup/proofqlearning.pdf
  24. http://arxiv.org/abs/1509.06461
  25. http://arxiv.org/abs/1511.05952
  26. http://arxiv.org/abs/1511.06581
  27. http://arxiv.org/abs/1509.02971
  28. http://rll.berkeley.edu/deeprlworkshop/
  29. https://cmt.research.microsoft.com/iclr2016conference/protected/publiccomment.aspx
  30. http://www.google.com/patents/us20150100530
  31. http://videolectures.net/rldm2015_silver_reinforcement_learning/
  32. https://www.youtube.com/watch?v=b1a53he0yqs
  33. http://rll.berkeley.edu/deeprlcourse/
  34. http://research.microsoft.com/apps/video/?id=259577
  35. http://www0.cs.ucl.ac.uk/staff/d.silver/web/teaching.html
  36. https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/
  37. http://cs231n.github.io/
  38. https://neuro.cs.ut.ee/tag/ai/
  39. https://neuro.cs.ut.ee/tag/deep-learning/
  40. https://neuro.cs.ut.ee/tag/reinforcement-learning/
  41. http://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/
  42. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-3
  43. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=3#respond
  44. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-4
  45. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=4#respond
  46. https://www.quora.com/why-does-geoff-hinton-say-that-its-surprising-that-pooling-in-id98s-work
  47. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-5
  48. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=5#respond
  49. http://matpalm.com/
  50. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-6
  51. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=6#respond
  52. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-7
  53. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=7#respond
  54. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-8
  55. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=8#respond
  56. https://github.com/mgbellemare/arcade-learning-environment/tree/master/src/games/supported
  57. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-9
  58. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=9#respond
  59. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-10
  60. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=10#respond
  61. https://github.com/lasagne/recipes/blob/master/examples/saliency maps and guided id26.ipynb
  62. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-11
  63. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=11#respond
  64. https://en.wikipedia.org/wiki/bellman_equation#cite_note-neurodynprog-4
  65. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-12
  66. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=12#respond
  67. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-13
  68. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=13#respond
  69. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-15
  70. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=15#respond
  71. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-21
  72. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=21#respond
  73. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-22
  74. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=22#respond
  75. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-23
  76. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=23#respond
  77. http://arxiv.org/abs/1312.5602
  78. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-24
  79. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=24#respond
  80. https://storage.googleapis.com/deepmind-data/assets/papers/deepmindnature14236paper.pdf
  81. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-33
  82. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=33#respond
  83. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-34
  84. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=34#respond
  85. https://tdeep.wordpress.com/2016/08/21/deep-q/
  86. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-44
  87. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=44#respond
  88. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-45
  89. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=45#respond
  90. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-48
  91. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=48#respond
  92. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-49
  93. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=49#respond
  94. https://arxiv.org/abs/1511.06581
  95. https://github.com/tambetm/gymexperiments
  96. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-54
  97. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=54#respond
  98. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-58
  99. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=58#respond
 100. https://github.com/tambetm/gymexperiments/blob/master/duel.py
 101. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-50
 102. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=50#respond
 103. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-51
 104. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=51#respond
 105. https://arxiv.org/abs/1412.6806
 106. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-52
 107. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=52#respond
 108. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-53
 109. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=53#respond
 110. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-57
 111. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=57#respond
 112. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-55
 113. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=55#respond
 114. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-59
 115. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=59#respond
 116. https://arxiv.org/abs/1511.08779
 117. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-69
 118. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=69#respond
 119. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-70
 120. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=70#respond
 121. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-71
 122. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=71#respond
 123. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-72
 124. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=72#respond
 125. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-75
 126. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=75#respond
 127. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-78
 128. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=78#respond
 129. http://adgefficiency.com/
 130. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-80
 131. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=80#respond
 132. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-81
 133. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=81#respond
 134. http://web.stanford.edu/class/psych209/readings/mnihetalhassibis15naturecontroldeeprl.pdf
 135. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-82
 136. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=82#respond
 137. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-84
 138. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=84#respond
 139. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-85
 140. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=85#respond
 141. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-86
 142. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=86#respond
 143. https://www.reddit.com/r/machinelearning/comments/4z5oa4/is_google_patenting_id25_really_justified/
 144. https://grensesnittet.computas.com/design-an-artificial-agent-to-play-vikingmud/
 145. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-88
 146. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=88#respond
 147. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-89
 148. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=89#respond
 149. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-90
 150. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=90#respond
 151. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-91
 152. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=91#respond
 153. https://arxiv.org/abs/1509.02971
 154. http://goingmyway.cn/
 155. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-96
 156. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=96#respond
 157. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-106
 158. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=106#respond
 159. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-103
 160. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=103#respond
 161. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-105
 162. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=105#respond
 163. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-109
 164. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=109#respond
 165. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-113
 166. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=113#respond
 167. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-114
 168. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=114#respond
 169. https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/
 170. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-115
 171. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=115#respond
 172. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-123
 173. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=123#respond
 174. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-248
 175. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=248#respond
 176. https://devhunteryz.wordpress.com/2018/04/15/tensorflowda-basit-takviyeli-ogrenme-tablolar-ve-sinir-aglari-ile-id24/
 177. https://www.artificial-intelligence.video/deep-id24-for-video-games-the-math-of-intelligence-9
 178. https://beaglechronicbackpain.wordpress.com/2018/05/09/reinforcement-learning-a-z/
 179. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-756
 180. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=756#respond
 181. http://www.analyticspath.com/deep-learning-training-in-pune
 182. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-1456
 183. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=1456#respond
 184. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-1641
 185. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=1641#respond
 186. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-1645
 187. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=1645#respond
 188. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-2084
 189. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=2084#respond
 190. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-2535
 191. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=2535#respond
 192. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#comment-2626
 193. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/?replytocom=2626#respond
 194. https://www.quora.com/what-are-the-latest-works-on-reinforcement-learning-in-the-financial-field
 195. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/#respond
 196. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
 197. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
 198. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mdfumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 199. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mdhumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 200. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mtvumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 201. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mjjumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 202. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mjlumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 203. https://neuro.cs.ut.ee/tag/ai/
 204. https://neuro.cs.ut.ee/tag/bci/
 205. https://neuro.cs.ut.ee/tag/brain-computer-interface/
 206. https://neuro.cs.ut.ee/tag/id98/
 207. https://neuro.cs.ut.ee/tag/computer-science/
 208. https://neuro.cs.ut.ee/tag/control-problem/
 209. https://neuro.cs.ut.ee/tag/course/
 210. https://neuro.cs.ut.ee/tag/data-analysis/
 211. https://neuro.cs.ut.ee/tag/deep-learning/
 212. https://neuro.cs.ut.ee/tag/eeg/
 213. https://neuro.cs.ut.ee/tag/emotiv-epoc/
 214. https://neuro.cs.ut.ee/tag/gamma/
 215. https://neuro.cs.ut.ee/tag/human-brain/
 216. https://neuro.cs.ut.ee/tag/intracranial/
 217. https://neuro.cs.ut.ee/tag/neon/
 218. https://neuro.cs.ut.ee/tag/openbci/
 219. https://neuro.cs.ut.ee/tag/opengl/
 220. https://neuro.cs.ut.ee/tag/optimization/
 221. https://neuro.cs.ut.ee/tag/python/
 222. https://neuro.cs.ut.ee/tag/reinforcement-learning/
 223. https://neuro.cs.ut.ee/tag/robotex/
 224. https://neuro.cs.ut.ee/tag/seminar/
 225. https://neuro.cs.ut.ee/tag/source-localization/
 226. https://neuro.cs.ut.ee/tag/teaching/
 227. https://neuro.cs.ut.ee/tag/webpage/
 228. https://neuro.cs.ut.ee/wp-login.php
 229. http://wordpress.org/
 230. http://alienwp.com/
