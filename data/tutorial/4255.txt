   #[1]computational neuroscience lab    feed [2]computational neuroscience
   lab    comments feed [3]computational neuroscience lab    deep
   id23 with neon comments feed [4]demystifying deep
   id23 [5]poster at bci meeting 2016

[6]computational neuroscience lab

institute of computer science, university of tartu

main menu

   [7]skip to content
     * [8]recent
     * [9]people
     * [10]research
          + [11]thesis proposals
     * [12]publications
     * [13]teaching
     * [14]resources
     * [15]contact

deep id23 with neon

   [16]december 19, 2015    by [17]tambet matiisen   
   breakout

   this is the part 2 of my series on deep id23. see
   part 1    [18]demystifying deep id23    for an
   introduction to the topic.

   the first time we read deepmind   s paper    [19]playing atari with deep
   id23    in our research group, we immediately knew that
   we wanted to replicate this incredible result. it was the beginning of
   2014, [20]cuda-convnet2 was the top-performing convolutional network
   implementation and rmsprop was just [21]one slide in hinton   s coursera
   course. we struggled with debugging, learned a lot, but when deepmind
   also published their code alongside their nature paper    [22]human-level
   control through deep id23   , we started working on
   their code instead.

   the deep learning ecosystem has evolved a lot since then. supposedly a
   new deep learning toolkit was released once [23]every 22 days in 2015.
   amongst the popular ones are both the old-timers like [24]theano,
   [25]torch7 and [26]caffe, as well as the newcomers like [27]neon,
   [28]keras and [29]tensorflow. new algorithms are getting implemented
   within days of publishing.

   at some point i realized that all the complicated parts that caused us
   headaches a year earlier are now readily implemented in most deep
   learning toolkits. and when [30]arcade learning environment     the
   system used for emulating atari 2600 games     released a [31]native
   python api, the time was right for a new deep id23
   implementation. writing the main code took just a weekend, followed by
   weeks of debugging. but finally it worked! you can see the result here:
   [32]https://github.com/tambetm/simple_id25

   i chose to base it on neon, because it has
     * the [33]fastest convolutional kernels,
     * all the required algorithms implemented (e.g. rmsprop),
     * a reasonable python api.

   i tried to keep my code simple and easy to extend, while also keeping
   performance in mind. currently the most notable restriction in neon is
   that it only runs on the latest nvidia maxwell gpus, but that   s
   [34]about to change soon.

   in the following i will describe
    1. how to install it
    2. what you can do with it
    3. how does it compare to others
    4. and finally how to modify it

how to install it?

   there is not much to talk about here, just follow the instructions in
   the [35]readme. basically all you need are [36]neon, [37]arcade
   learning environment and [38]simple_id25 itself. for trying out
   pre-trained models you don   t even need a gpu, because they also run on
   cpu, albeit slowly.

what you can do with it?

running a pre-trained model

   once you have everything installed, the first thing to try out is
   running pre-trained models. for example to run a pre-trained model for
   breakout, type:
./play.sh snapshots/breakout_77.pkl

   or if you don   t have a (maxwell) gpu available, then you can switch to
   cpu:
./play.sh snapshots/breakout_77.pkl --backend cpu

   you should be seeing something like this, possibly even accompanied by
   annoying sounds :)

   breakout

   you can switch off the ai and take control in the game by pressing    m   
   on keyboard     see how long you last! you can give the control back to
   the ai by pressing    m    again.

   it is actually quite useful to slow down the gameplay by pressing    a   
   repeatedly     this way you can observe what is really happening in the
   game. press    s    to speed the game up again.

recording a video

   just as easily you can also record a video of one game (in breakout
   until 5 lives are lost):
./record.sh snapshots/breakout_77.pkl

   the recorded video is stored as videos/breakout_77.mov and screenshots
   from the game can be found in videos/breakout. you can watch an example
   video here:

   iframe: [39]https://www.youtube.com/embed/kkif0ok5gce

training a new model

   to train a new model, you first need an atari 2600 rom file for the
   game. once you have the rom, save it to roms folder and run training
   like this:
./train.sh roms/pong.rom

   as a result of training the following files are created:
     * results/pong.csv contains various statistics of the training
       process,
     * snapshots/pong_<epoch>.pkl are model snapshots after every epoch.

testing a model

   during training there is a testing phase after each epoch. if you would
   like to re-test your pre-trained model later, you can use the testing
   script:
./test.sh snapshots/pong_141.pkl

   it prints the testing results to console. to save the results to file,
   add --csv_file <filename> parameter.

plotting statistics

   during and after training you might like to see how your model is
   doing. there is a simple plotting script to produce graphs from the
   statistics file. for example:
./plot.sh results/pong.csv

   this produces the file results/pong.png.

   pong results

   by default it produces four plots:
    1. average score,
    2. number of played games,
    3. average maximum q-value of validation set states,
    4. average training loss.

   for all the plots you can see random baseline (where it makes sense)
   and result from training phase and testing phase. you can actually plot
   any field from the statistics file, by listing names of the fields in
   the --fields parameter. the default plot is achieved with --fields
   average_reward,meanq,nr_games,meancost. names of the fields can be
   taken from the first line of the csv file.

visualizing the filters

   the most exciting thing you can do with this code is to peek into the
   mind of the ai. for that we are going to use [40]guided
   id26, that comes [41]out-of-the-box with neon. in simplified
   terms, for each convolutional filter it finds an image from a given
   dataset that activates this filter the most. then it performs
   id26 with respect to the input image, to see which parts of
   the image affect the    activeness    of that filter most. this can be seen
   as a form of saliency detection.

   to perform filter visualization run the following:
./nvis.sh snapshots/breakout_77.pkl

   it starts by playing a game to collect sample data and then performs
   the guided id26. the results can be found in
   results/breakout.html and they look like this:

   iframe:
   [42]http://neuro.cs.ut.ee/wp-content/uploads/2015/12/breakout_2.html

   there are 3 convolutional layers (named 0000, 0002 and 0004) and for
   this post i have visualized only 2 filters from each (feature map
   0-1). there is also more detailed [43]file for breakout which has 16
   filters visualized. for each filter an image was chosen that activates
   it the most. right image shows the original input, left image shows the
   guided id26 result. you can think of every filter as an
      eye    of the ai. the left image shows where this particular    eye    was
   directed to, given the image on the right. you can use mouse wheel to
   zoom in!

   because input to our network is a sequence of 4 grayscale images, it   s
   not very clear how to visualize it. i made a following simplification:
   i   m using only the last 3 screens of a state and putting them into
   different rgb color channels. so everything that is gray hasn   t changed
   over 3 images; blue was the most recent change, then green, then red.
   you can easily follow the idea if you zoom in to the ball     it   s
   trajectory is marked by red-green-blue. it   s harder to make sense of
   the id26 result, but sometimes you can guess that the filter
   tracks movement     the color from one corner to another progresses from
   red to green to blue.

   filter visualization is an immensely useful tool, you can immediately
   make interesting observations.
     * the first layer filters focus on abstract patterns and cannot be
       reliably associated with any particular object. they often activate
       the most on score and lives, possibly because these have many edges
       and corners.
     * as expected, there are filters that track the ball and the paddle.
       there are also filters that activate when the ball is about to hit
       a brick or the paddle.
     * also as expected, higher layer filters have bigger receptive
       fields. this is not so evident in breakout, but can be clearly seen
       in this [44]file for pong. it   s interesting that filters in
       different layers are more similar in breakout than in pong.

   the guided id26 is implemented in neon as a callback, called
   at the end of training. i made a [45]simple wrapper that allows using
   it on pre-trained models. one advantage of using the wrapper is that it
   incorporates guided id26 and visualization into one step and
   doesn   t need a temporary file to write the intermediate results. but
   for that i needed to make few modifications to the neon code, that are
   stored in the [46]nvis folder.

how does it compare to others?

   there are a few other deep id23 implementations out
   there and it would be interesting to see how the implementation in neon
   compares to them. the most well-known is the [47]original deepmind code
   published with their nature article. another maintained version is
   [48]deep_q_rl created by nathan sprague from james madison university
   in virginia.

   the most common metric used in deep id23 is the
   average score per game. to calculate it for simple_id25 i used the same
   evaluation procedure as in the nature article     average scores of 30
   games, played with different initial random conditions and an   -greedy
   policy with   =0.05. i didn   t bother to implement the 5 minutes per game
   restriction, because games in breakout and pong don   t last that long.
   for deepmind i used the values from nature paper. for deep_q_rl i i
   asked the deep-id24 list members to provide the numbers. their
   scores are not collected using exactly the same protocol (the
   particular number below was average of 11 games) and may be therefore
   considered a little bit inflated.

   another interesting measure is the number of training steps per second.
   deepmind and simple_id25 report average number of steps per second for
   each epoch (250000 steps). deep_q_rl reports number of steps per second
   on the fly and i just used a perceived average of numbers flowing over
   the screen :) . in all cases i looked at the first epoch, where
   exploration rate is close to 1 and the results therefore reflect more
   of the training speed than the prediction speed. all tests were done on
   nvidia geforce titan x.
   implemented in breakout average score pong average score training steps
   per second
   deepmind lua + torch 401.2 (  26.9) 18.9 (  1.3) 330
   deep_q_rl python + theano +
   lasagne + pylearn2 350.7 ~20 ~300
   simple_id25 python + neon 338.0 19.7 366

   as it can be seen, in terms of speed simple_id25 compares quite
   favorably against the others. the learning results are not on-par with
   deepmind yet, but they are close enough to run interesting experiments
   with it.

how can you modify it?

   the main idea in publishing the simple_id25 code was to show how simple
   the implementation could actually be and that everybody can extend it
   to perform interesting research.

   there are four main classes in the code: environment, replaymemeory,
   deepqnetwork and agent. there is also main.py, that handles parameter
   parsing and instantiates the above classes, and the statistics class
   that implements the basic callback mechanism to keep statistics
   collection separate from the main loop. but the gist of the deep
   id23 algorithm is implemented in the aforementioned
   four classes.

   [49]simple_id25 classes

environment

   environment is just a lightweight wrapper around the [50]a.l.e python
   api. it should be easy enough to add other environments besides a.l.e,
   for example [51]flappy bird or [52]torcs     you just have to implement a
   new environment class. give it a try and let me know!

replaymemory

   replay memory stores state transitions or experiences. basically it   s
   just four big arrays for screens, actions, rewards and terminal state
   indicators.

   replay memory is stored as a sequence of screens, not as a sequence of
   states consisting of four screens. this results in a huge decrease in
   memory usage, without significant loss in performance. assembling
   screens into states can be done fast with numpy array slicing.

   datatype for the screen pixels is uint8, which means 1m experiences
   take 6.57gb     you can run it with only 8gb of memory! default would
   have been float32, which would have taken ~26gb.

   if you would like to implement [53]prioritized experience replay, then
   this is the main class you need to change.

deepqnetwork

   this class implements the deep q-network. it is actually the only class
   that depends on neon.

   because deep id23 handles minibatching differently,
   there was no reason to use neon   s dataiterator class. therefore the
   lower level model.fprop() and model.bprop() are used instead. a few
   suggestions for anybody attempting to do the same thing:
    1. you need to call model.initialize() after constructing the model.
       this allocates the tensors for layer activations and weights in gpu
       memory.
    2. neon tensors have dimensions (channels, height, width, batch_size).
       in particular batch size is the last dimension. this data layout
       allows for the fastest convolution kernels.
    3. after transposing the dimensions of a numpy array to match neon
       tensor requirements, you need to make a copy of that array!
       otherwise the actual memory layout hasn   t changed and the array
       cannot be directly copied to gpu.
    4. instead of accessing single tensor elements with array indices
       (like tensor[i,j]), try to copy the tensor to a numpy array as a
       whole with tensor.set() and tensor.get() methods. this results in
       less round-trips between cpu and gpu.
    5. consider doing tensor arithmetics in gpu, the [54]neon backend
       provides nice methods for that. also note that these operations are
       not immediately evaluated, but stored as a optree. the tensor will
       be actually evaluated when you use it or transfer it to cpu.

   if you would like to implement [55]double id24, then this is the
   class that needs modifications.

agent

   agent class just ties everything together and implements the main loop.

conclusion

   i have shown, that using a well-featured deep learning toolkit such as
   neon, implementing deep id23 for atari video games is
   a breeze. filter visualization features in neon provide important
   insights into what the model has actually learned.

   while not the goal on its own, computer games provide an excellent
   sandbox for trying out new id23 approaches. hopefully
   my [56]simple_id25 implementation provides a stepping stone for a number
   of people into the fascinating area of deep id23
   research.

credits

   thanks to ardi tampuu, jaan aru, tanel p  rnamaa, raul vicente, arjun
   bansal and urs k  ster for comments and suggestions on the drafts of
   this post.

   tags: [57]ai, [58]deep learning, [59]neon, [60]id23

20 responses on    deep id23 with neon   

    1. pingback: [61]demystifying deep id23 |
       computational neuroscience lab  
    2.
   sk [62]march 11, 2016 at 12:43 am       [63]reply    
       thank you the great tutorial and github code. i followed the
       instructions you have provided for installation and as a starting
       point tried to run the test and play script using pretrained
       models:
       ./test.sh snapshots/breakout_77.pkl
       ./play.sh snapshots/breakout_77.pkl
       in each case, i got the following error at line 528 of
       neon/layers/layer.py in set_params (if not hasattr(self, key)):
       typeerror: hasattr(): attribute name must be a string
       if you have any suggestion for how i can fix this, please let me
       know. thanks for your help.
          +
        tambet matiisen [64]march 11, 2016 at 6:43 am       [65]reply    
            yes, there seem to be issues with the latest neon code. i
            posted an issue to neon repo:
            [66]https://github.com/nervanasystems/neon/issues/214
            as a workaround you can checkout the known good neon commit:
            git clone [67]https://github.com/nervanasystems/neon.git
            cd neon
            git checkout 7a56fa9645a51e97c05f2e5afbbd1df7057ae832
            make
            i also updated readme to suggest that commit by default.
    3.
   sk [68]march 12, 2016 at 6:56 am       [69]reply    
       i followed your instructions and installed the branch. but now when
       i run play.sh or test.sh with pretrained models, i get the
       following:
       traceback (most recent call last):
       file    src/main.py   , line 93, in
       net = deepqnetwork(env.numactions(), args)
       file    simple_id25/src/deepqnetwork.py   , line 31, in __init__
       stochastic_round = args.stochastic_round)
       typeerror: gen_backend() got an unexpected keyword argument
          datatype   
       can you please let me know how i may be able to fix this?
       thanks
          +
        tambet matiisen [70]march 14, 2016 at 10:58 am       [71]reply    
            as there are still issues with latest neon, i included the sha
            for old version in installation instructions and changed the
            code in simple_id25 repository to reflect that. so actually if
            you just do pull, you should be fine.
            neon has fix underway, but it needs more testing and in
            meantime they suggested workaround:
            [72]https://github.com/nervanasystems/neon/issues/153#issuecom
            ment-196150383
            i haven   t tested it yet. once i get to that i will include it
            in repo.
    4.
   ben israel [73]june 1, 2016 at 5:25 pm       [74]reply    
       i have gotten the code to run without errors. but after 3 epochs of
       breakout it hasn   t learned. meanq is at -1e-5 after the third
       epoch. by the third epoch in results your net was at 0.19. is
       perhaps one of the parameters not right?
          +
        tambet matiisen [75]june 1, 2016 at 5:50 pm       [76]reply    
            by now all the suggested fixes are merged and i   ve been able
            to successfully train new models with the latest neon. 3
            epochs is too little to say anything conclusive, please train
            for 15-20 epochs and raise issue in github, if still no
            progress.
               o
             ben israel [77]june 1, 2016 at 10:05 pm       [78]reply    
                 thanks for responding so quickly. the    meanq    in
                    breakout.csv    starts to immediately rise in the results
                 directory. i   m running with a cpu backend. 15 epochs is 4
                 days of tying up one computer.
                 this is from the csv.
                 epoch phase meanq
                 0 random 8.22e-05
                 1 train 0.0744705
                 1 test 0.0744699
                 2 train 0.12585
                 2 test 0.125854
                 3 train 0.195674
                 3 test 0.195372
                 this is appears to be a very early signal that the net is
                 learning.
                    #
                  tambet matiisen [79]june 2, 2016 at 8:22 am      
                      [80]reply    
                      meanq rising sounds good. are you using cpu for
                      training? that sounds like a long wait. the cheapest
                      option would be to grab one of the gpu nodes from
                      amazon ec2.
                         @
                       ben israel [81]june 2, 2016 at 7:21 pm      
                           it is now 2 days and 5 epochs later and no
                           change in meanq. the immediately rising meanq
                           that i listed was from your run in the results
                           directory. it continues to rise to 2.5 at epoch
                           30. the rise on your results run is nearly
                           linear starting in epoch 1.
                         @
                       tambet matiisen [82]june 3, 2016 at 5:01 am      
                           i   ve never tried training with cpu so long.
                           there might be some caveats, i.e. see this:
                           [83]https://github.com/nervanasystems/neon/issu
                           es/218#issuecomment-198005136
    5.
   ben israel [84]june 1, 2016 at 5:28 pm       [85]reply    
       the neon environment with all the information it generates is very
       exciting. but please help me get the net to learn.
    6.
   chai [86]december 18, 2016 at 3:35 am       [87]reply    
       1st of all. english isn   t my language. i   m apologize if i   s using
       it wrong
       i   m training game phoenix.bin with i7 6700k 4ghz not overclock,
       nvidia gtx 1080, ram 16gb
       ubuntu 16.04. nvidia driver version 367
       cuda 8.0 driver
       using openai gym for training
       my training speed is surprising low just 277 steps / sec.
       my training+testing speed is about 19 and a half minutes
       i just want to know if this   s normal. i   m now on epoch#13. how many
       epoch left with default settings?
          +
        chai [88]december 18, 2016 at 3:37 am       [89]reply    
            i means my approximate time for training + testing time is
            19.5 minutes per epoch.
          +
        chai [90]december 18, 2016 at 7:50 pm       [91]reply    
            ok, my gpu   s speed seem to be normal because i test for
            breakout game.
            now my training steps soar up to 427 steps/sec
            and the training took 10 minute 15 secs per epoch
    7.
   chai [92]december 18, 2016 at 3:46 am       [93]reply    
       may be this is useful for someone else want to know about how
       nvidia gtx 1080 gpu did when training+testing on simple_dql.
       for training+tesing first 13   s epoch. it took me about 4 hours on
       nvidia gtx 1080 gpu. non overclocked
       is this a but too slow? considering. it   s a nvidia gtx 1080 gpu
          +
        chai [94]december 19, 2016 at 1:35 am       [95]reply    
            now, this time i ran training again using train.sh script. on
            game phoenix.
            look like train.sh ale environment running phoenix faster.
            it   s training at speed 302-336 steps/sec
            at 12-14 min / epoch.
            considering running more complex game than breakout at > 3
            00 steps/sec. not bad at all. not bad.
               o
             chai [96]december 26, 2016 at 3:29 am       [97]reply    
                 look like after after 5 epochs. training time for both
                 ale and gym is about to be the same.
                 i   m apologized for leading to mead understood of my
                 comment
    8.
   sergey [98]december 26, 2016 at 1:25 pm       [99]reply    
       thanks for sharing your work!
       the link to the double id24 paper seems to lead to the
       prioritized experience replay paper by a misprint. did you mean to
       link to this one [100]https://arxiv.org/abs/1509.06461 ?
          +
        tambet matiisen [101]december 28, 2016 at 6:39 am       [102]reply    
            thanks sergey, that was a mistake. fixed!

leave a reply [103]cancel reply

   your email address will not be published. required fields are marked *

   name * ______________________________

   email * ______________________________

   website ______________________________

   comment
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________
   _____________________________________________

   you may use these html tags and attributes: <a href="" title=""> <abbr
   title=""> <acronym title=""> <b> <blockquote cite=""> <cite> <code>
   <del datetime=""> <em> <i> <q cite=""> <strike> <strong>

   post comment

   search ____________________ search

   caption:

   [104]back
   [105]next
   april  2019
   m  t  w  t  f  s s
   1
     * cns group meeting
       starts: 12:15 pm
       ends: april 1, 2019 - 1:30 pm
       [106]more details...

   2  3  4  5  6  7
   8
     * cns group meeting
       starts: 12:15 pm
       ends: april 8, 2019 - 1:30 pm
       [107]more details...

   9  10 11 12 13 14
   15
     * cns group meeting
       starts: 12:15 pm
       ends: april 15, 2019 - 1:30 pm
       [108]more details...

   16 17 18 19 20 21
   22
     * cns group meeting
       starts: 12:15 pm
       ends: april 22, 2019 - 1:30 pm
       [109]more details...

   23 24 25 26 27 28
   29
     * cns group meeting
       starts: 12:15 pm
       ends: april 29, 2019 - 1:30 pm
       [110]more details...

   30

tags

   [111]ai [112]bci [113]brain-computer interface [114]id98 [115]computer
   science [116]control problem [117]course [118]data analysis [119]deep
   learning [120]eeg [121]emotiv epoc [122]gamma [123]human brain
   [124]intracranial [125]neon [126]openbci [127]opengl [128]optimization
   [129]python [130]id23 [131]robotex [132]seminar
   [133]source localization [134]teaching [135]webpage

   search ____________________ search

blogroll

meta

     * [136]log in

   [137]proudly powered by wordpress | theme: oxygen by [138]alienwp.

references

   1. https://neuro.cs.ut.ee/feed/
   2. https://neuro.cs.ut.ee/comments/feed/
   3. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/feed/
   4. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
   5. https://neuro.cs.ut.ee/976/
   6. https://neuro.cs.ut.ee/
   7. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#content
   8. http://neuro.cs.ut.ee/
   9. https://neuro.cs.ut.ee/people/
  10. https://neuro.cs.ut.ee/lab/
  11. https://neuro.cs.ut.ee/lab/topics/
  12. https://neuro.cs.ut.ee/publications/
  13. https://neuro.cs.ut.ee/teaching/
  14. https://neuro.cs.ut.ee/resources/
  15. https://neuro.cs.ut.ee/contact/
  16. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/
  17. https://neuro.cs.ut.ee/author/tambet-matiisen/
  18. http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
  19. http://arxiv.org/abs/1312.5602
  20. https://code.google.com/p/cuda-convnet2/
  21. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
  22. http://www.nature.com/articles/nature14236
  23. https://twitter.com/kcimc/status/664217437840257024
  24. http://deeplearning.net/software/theano/
  25. http://torch.ch/
  26. http://caffe.berkeleyvision.org/
  27. http://neon.nervanasys.com/
  28. http://keras.io/
  29. https://tensorflow.org/
  30. http://www.arcadelearningenvironment.org/
  31. https://github.com/bbitmaster/ale_python_interface/wiki/code-tutorial
  32. https://github.com/tambetm/simple_id25
  33. https://github.com/soumith/convnet-benchmarks
  34. https://github.com/nervanasystems/neon/issues/80
  35. https://github.com/tambetm/simple_id25
  36. https://github.com/nervanasystems/neon
  37. https://github.com/mgbellemare/arcade-learning-environment
  38. https://github.com/tambetm/simple_id25
  39. https://www.youtube.com/embed/kkif0ok5gce
  40. http://arxiv.org/abs/1412.6806
  41. http://neon.nervanasys.com/docs/latest/tools.html#layer-deconvolution-visualization
  42. http://neuro.cs.ut.ee/wp-content/uploads/2015/12/breakout_2.html
  43. http://neuro.cs.ut.ee/wp-content/uploads/2015/12/breakout_16.html
  44. http://neuro.cs.ut.ee/wp-content/uploads/2015/12/pong_16.html
  45. https://github.com/tambetm/simple_id25/blob/master/src/visualization.py
  46. https://github.com/tambetm/simple_id25/tree/master/src/nvis
  47. https://sites.google.com/a/deepmind.com/id25/
  48. https://github.com/spragunr/deep_q_rl
  49. http://neuro.cs.ut.ee/wp-content/uploads/2015/12/classes.png
  50. https://github.com/bbitmaster/ale_python_interface/wiki/code-tutorial
  51. https://github.com/sourabhv/flappybirdclone
  52. http://torcs.sourceforge.net/
  53. http://arxiv.org/abs/1511.05952
  54. http://neon.nervanasys.com/docs/latest/generated/neon.backends.backend.backend.html
  55. https://arxiv.org/abs/1509.06461
  56. https://github.com/tambetm/simple_id25
  57. https://neuro.cs.ut.ee/tag/ai/
  58. https://neuro.cs.ut.ee/tag/deep-learning/
  59. https://neuro.cs.ut.ee/tag/neon/
  60. https://neuro.cs.ut.ee/tag/reinforcement-learning/
  61. http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
  62. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-14
  63. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=14#respond
  64. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-16
  65. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=16#respond
  66. https://github.com/nervanasystems/neon/issues/214
  67. https://github.com/nervanasystems/neon.git
  68. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-17
  69. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=17#respond
  70. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-18
  71. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=18#respond
  72. https://github.com/nervanasystems/neon/issues/153#issuecomment-196150383
  73. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-26
  74. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=26#respond
  75. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-28
  76. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=28#respond
  77. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-29
  78. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=29#respond
  79. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-30
  80. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=30#respond
  81. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-31
  82. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-32
  83. https://github.com/nervanasystems/neon/issues/218#issuecomment-198005136
  84. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-27
  85. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=27#respond
  86. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-60
  87. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=60#respond
  88. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-61
  89. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=61#respond
  90. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-63
  91. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=63#respond
  92. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-62
  93. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=62#respond
  94. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-64
  95. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=64#respond
  96. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-65
  97. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=65#respond
  98. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-66
  99. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=66#respond
 100. https://arxiv.org/abs/1509.06461
 101. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#comment-67
 102. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/?replytocom=67#respond
 103. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/#respond
 104. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/
 105. https://neuro.cs.ut.ee/deep-reinforcement-learning-with-neon/
 106. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mdfumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 107. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mdhumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 108. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mtvumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 109. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mjjumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 110. https://www.google.com/calendar/event?eid=dtjjazvnodrlbnrwajzybmxnynm3b3qzzw9fmjaxota0mjlumdkxntawwiaznmowohbsmjg2dwhsdgs0odfnym1pmjhpnebn
 111. https://neuro.cs.ut.ee/tag/ai/
 112. https://neuro.cs.ut.ee/tag/bci/
 113. https://neuro.cs.ut.ee/tag/brain-computer-interface/
 114. https://neuro.cs.ut.ee/tag/id98/
 115. https://neuro.cs.ut.ee/tag/computer-science/
 116. https://neuro.cs.ut.ee/tag/control-problem/
 117. https://neuro.cs.ut.ee/tag/course/
 118. https://neuro.cs.ut.ee/tag/data-analysis/
 119. https://neuro.cs.ut.ee/tag/deep-learning/
 120. https://neuro.cs.ut.ee/tag/eeg/
 121. https://neuro.cs.ut.ee/tag/emotiv-epoc/
 122. https://neuro.cs.ut.ee/tag/gamma/
 123. https://neuro.cs.ut.ee/tag/human-brain/
 124. https://neuro.cs.ut.ee/tag/intracranial/
 125. https://neuro.cs.ut.ee/tag/neon/
 126. https://neuro.cs.ut.ee/tag/openbci/
 127. https://neuro.cs.ut.ee/tag/opengl/
 128. https://neuro.cs.ut.ee/tag/optimization/
 129. https://neuro.cs.ut.ee/tag/python/
 130. https://neuro.cs.ut.ee/tag/reinforcement-learning/
 131. https://neuro.cs.ut.ee/tag/robotex/
 132. https://neuro.cs.ut.ee/tag/seminar/
 133. https://neuro.cs.ut.ee/tag/source-localization/
 134. https://neuro.cs.ut.ee/tag/teaching/
 135. https://neuro.cs.ut.ee/tag/webpage/
 136. https://neuro.cs.ut.ee/wp-login.php
 137. http://wordpress.org/
 138. http://alienwp.com/
