   #[1]sebastian ruder

   [2]sebastian ruder
     * [3]about
     * [4]tags
     * [5]papers
     * [6]talks
     * [7]news
     * [8]faq
     * [9]nlp news
     * [10]nlp progress
     * [11]contact

   25 july 2017 / [12]natural language processing

deep learning for nlp best practices

   deep learning for nlp best practices

   this post gives an overview of best practices relevant for most tasks
   in natural language processing.

   update july 26, 2017: for additional context, the [13]hackernews
   discussion about this post.

   table of contents:
     * [14]introduction
     * [15]best practices
     * [16]id27s
     * [17]depth
     * [18]layer connections
     * [19]dropout
     * [20]id72
     * [21]attention
     * [22]optimization
     * [23]ensembling
     * [24]hyperparameter optimization
     * [25]lstm tricks
     * [26]task-specific best practices
     * [27]classification
     * [28]sequence labelling
     * [29]id86
     * [30]id4

introduction

   this post is a collection of best practices for using neural networks
   in natural language processing. it will be updated periodically as new
   insights become available and in order to keep track of our evolving
   understanding of deep learning for nlp.

   there has been a [31]running joke in the nlp community that an lstm
   with attention will yield state-of-the-art performance on any task.
   while this has been true over the course of the last two years, the nlp
   community is slowly moving away from this now standard baseline and
   towards more interesting models.

   however, we as a community do not want to spend the next two years
   independently (re-)discovering the next lstm with attention. we do not
   want to reinvent tricks or methods that have already been shown to
   work. while many existing deep learning libraries already encode best
   practices for working with neural networks in general, such as
   initialization schemes, many other details, particularly task or
   domain-specific considerations, are left to the practitioner.

   this post is not meant to keep track of the state-of-the-art, but
   rather to collect best practices that are relevant for a wide range of
   tasks. in other words, rather than describing one particular
   architecture, this post aims to collect the features that underly
   successful architectures. while many of these features will be most
   useful for pushing the state-of-the-art, i hope that wider knowledge of
   them will lead to stronger evaluations, more meaningful comparison to
   baselines, and inspiration by shaping our intuition of what works.

   i assume you are familiar with neural networks as applied to nlp (if
   not, i recommend yoav goldberg's [32]excellent primer ^[33][1]) and are
   interested in nlp in general or in a particular task. the main goal of
   this article is to get you up to speed with the relevant best practices
   so you can make meaningful contributions as soon as possible.

   i will first give an overview of best practices that are relevant for
   most tasks. i will then outline practices that are relevant for the
   most common tasks, in particular classification, sequence labelling,
   id86, and id4.

   disclaimer: treating something as best practice is notoriously
   difficult: best according to what? what if there are better
   alternatives? this post is based on my (necessarily incomplete)
   understanding and experience. in the following, i will only discuss
   practices that have been reported to be beneficial independently by at
   least two different groups. i will try to give at least two references
   for each best practice.

best practices

id27s

   id27s are arguably the most widely known best practice in the
   recent history of nlp. it is well-known that using pre-trained
   embeddings helps (kim, 2014) ^[34][2]. the optimal dimensionality of
   id27s is mostly task-dependent: a smaller dimensionality
   works better for more syntactic tasks such as id39
   (melamud et al., 2016) ^[35][3] or part-of-speech (pos) tagging (plank
   et al., 2016) ^[36][4], while a larger dimensionality is more useful
   for more semantic tasks such as id31 (ruder et al., 2016)
   ^[37][5].

depth

   while we will not reach the depths of id161 for a while,
   neural networks in nlp have become progressively deeper.
   state-of-the-art approaches now regularly use deep bi-lstms, typically
   consisting of 3-4 layers, e.g. for id52 (plank et al., 2016) and
   semantic role labelling (he et al., 2017) ^[38][6]. models for some
   tasks can be even deeper, cf. google's id4 model with 8 encoder and 8
   decoder layers (wu et al., 2016) ^[39][7]. in most cases, however,
   performance improvements of making the model deeper than 2 layers are
   minimal (reimers & gurevych, 2017) ^[40][8].

   these observations hold for most sequence tagging and structured
   prediction problems. for classification, deep or very deep models
   perform well only with character-level input and shallow word-level
   models are still the state-of-the-art (zhang et al., 2015; conneau et
   al., 2016; le et al., 2017) ^[41][9] ^[42][10] ^[43][11].

layer connections

   for training deep neural networks, some tricks are essential to avoid
   the vanishing gradient problem. different layers and connections have
   been proposed. here, we will discuss three: i) highway layers, ii)
   residual connections, and iii) dense connections.

   highway layers   highway layers (srivastava et al., 2015) ^[44][12] are
   inspired by the gates of an lstm. first let us assume a one-layer mlp,
   which applies an affine transformation followed by a non-linearity
   \(g\) to its input \(\mathbf{x}\):

   \(\mathbf{h} = g(\mathbf{w}\mathbf{x} + \mathbf{b})\)

   a highway layer then computes the following function instead:

   \(\mathbf{h} = \mathbf{t} \odot g(\mathbf{w} \mathbf{x} + \mathbf{b}) +
   (1-\mathbf{t}) \odot \mathbf{x} \)

   where \(\odot\) is elementwise multiplication, \(\mathbf{t} =
   \sigma(\mathbf{w}_t \mathbf{x} + \mathbf{b}_t)\) is called the
   transform gate, and \((1-\mathbf{t})\) is called the carry gate. as we
   can see, highway layers are similar to the gates of an lstm in that
   they adaptively carry some dimensions of the input directly to the
   output.

   highway layers have been used pre-dominantly to achieve
   state-of-the-art results for language modelling (kim et al., 2016;
   jozefowicz et al., 2016; zilly et al., 2017) ^[45][13] ^[46][14]
   ^[47][15], but have also been used for other tasks such as speech
   recognition (zhang et al., 2016) ^[48][16]. [49]sristava's page
   contains more information and code regarding highway layers.

   residual connections   residual connections (he et al., 2016) ^[50][17]
   have been first proposed for id161 and were the main factor
   for winning id163 2016. residual connections are even more
   straightforward than highway layers and learn the following function:

   \(\mathbf{h} = g(\mathbf{w}\mathbf{x} + \mathbf{b}) + \mathbf{x}\)

   which simply adds the input of the current layer to its output via a
   short-cut connection. this simple modification mitigates the vanishing
   gradient problem, as the model can default to using the identity
   function if the layer is not beneficial.

   dense connections   rather than just adding layers from each layer to
   the next, dense connections (huang et al., 2017) ^[51][18] (best paper
   award at cvpr 2017) add direct connections from each layer to all
   subsequent layers. let us augment the layer output \(h\) and layer
   input \(x\) with indices \(l\) indicating the current layer. dense
   connections then feed the concatenated output from all previous layers
   as input to the current layer:

   \(\mathbf{h}^l = g(\mathbf{w}[\mathbf{x}^1; \ldots; \mathbf{x}^l] +
   \mathbf{b})\)

   where \([\cdot; \cdot]\) represents concatenation. dense connections
   have been successfully used in id161. they have also found to
   be useful for id72 of different nlp tasks (ruder et al.,
   2017) ^[52][19], while a residual variant that uses summation has been
   shown to consistently outperform residual connections for neural
   machine translation (britz et al., 2017) ^[53][20].

dropout

   while batch normalisation in id161 has made other
   regularizers obsolete in most applications, dropout (srivasta et al.,
   2014) ^[54][21] is still the go-to regularizer for deep neural networks
   in nlp. a dropout rate of 0.5 has been shown to be effective in most
   scenarios (kim, 2014). in recent years, variations of dropout such as
   adaptive (ba & frey, 2013) ^[55][22] and evolutional dropout (li et
   al., 2016) ^[56][23] have been proposed, but none of these have found
   wide adoption in the community. the main problem hindering dropout in
   nlp has been that it could not be applied to recurrent connections, as
   the aggregating dropout masks would effectively zero out embeddings
   over time.

   recurrent dropout   recurrent dropout (gal & ghahramani, 2016)
   ^[57][24] addresses this issue by applying the same dropout mask across
   timesteps at layer \(l\). this avoids amplifying the dropout noise
   along the sequence and leads to effective id173 for sequence
   models. recurrent dropout has been used for instance to achieve
   state-of-the-art results in semantic role labelling (he et al., 2017)
   and language modelling (melis et al., 2017) ^[58][25].

id72

   if additional data is available, id72 (mtl) can often be
   used to improve performance on the target task. have a look [59]this
   blog post for more information on mtl.

   auxiliary objectives   we can often find auxiliary objectives that are
   useful for the task we care about (ruder, 2017) ^[60][26]. while we can
   already predict surrounding words in order to pre-train id27s
   (mikolov et al., 2013), we can also use this as an auxiliary objective
   during training (rei, 2017) ^[61][27]. a similar objective has also
   been used by (ramachandran et al., 2016) ^[62][28] for
   sequence-to-sequence models.

   task-specific layers   while the standard approach to mtl for nlp is
   hard parameter sharing, it is beneficial to allow the model to learn
   task-specific layers. this can be done by placing the output layer of
   one task at a lower level (s  gaard & goldberg, 2016) ^[63][29]. another
   way is to induce private and shared subspaces (liu et al., 2017; ruder
   et al., 2017) ^[64][30] ^[65][19:1].

attention

   attention is most commonly used in sequence-to-sequence models to
   attend to encoder states, but can also be used in any sequence model to
   look back at past states. using attention, we obtain a context vector
   \(\mathbf{c}_i\) based on hidden states \(\mathbf{s}_1, \ldots,
   \mathbf{s}_m\) that can be used together with the current hidden state
   \(\mathbf{h}_i\) for prediction. the context vector \(\mathbf{c}_i\) at
   position is calculated as an average of the previous states weighted
   with the attention scores \(\mathbf{a}_i\):

   \(\begin{align}\begin{split}
   \mathbf{c}_i &= \sum\limits_j a_{ij}\mathbf{s}_j\\
   \mathbf{a}_i &= \text{softmax}(f_{att}(\mathbf{h}_i, \mathbf{s}_j))
   \end{split}\end{align}\)

   the attention function \(f_{att}(\mathbf{h}_i, \mathbf{s}_j)\)
   calculates an unnormalized alignment score between the current hidden
   state \(\mathbf{h}_i\) and the previous hidden state \(\mathbf{s}_j\).
   in the following, we will discuss four attention variants: i) additive
   attention, ii) multiplicative attention, iii) self-attention, and iv)
   key-value attention.

   additive attention   the original attention mechanism (bahdanau et al.,
   2015) ^[66][31] uses a one-hidden layer feed-forward network to
   calculate the attention alignment:

   \(f_{att}(\mathbf{h}_i, \mathbf{s}_j) = \mathbf{v}_a{}^\top
   \text{tanh}(\mathbf{w}_a[\mathbf{h}_i; \mathbf{s}_j]) \)

   where \(\mathbf{v}_a\) and \(\mathbf{w}_a\) are learned attention
   parameters. analogously, we can also use matrices \(\mathbf{w}_1\) and
   \(\mathbf{w}_2\) to learn separate transformations for \(\mathbf{h}_i\)
   and \(\mathbf{s}_j\) respectively, which are then summed:

   \(f_{att}(\mathbf{h}_i, \mathbf{s}_j) = \mathbf{v}_a{}^\top
   \text{tanh}(\mathbf{w}_1 \mathbf{h}_i + \mathbf{w}_2 \mathbf{s}_j) \)

   multiplicative attention   multiplicative attention (luong et al.,
   2015) ^[67][32] simplifies the attention operation by calculating the
   following function:

   \(f_{att}(h_i, s_j) = h_i^\top \mathbf{w}_a s_j \)

   additive and multiplicative attention are similar in complexity,
   although multiplicative attention is faster and more space-efficient in
   practice as it can be implemented more efficiently using matrix
   multiplication. both variants perform similar for small dimensionality
   \(d_h\) of the decoder states, but additive attention performs better
   for larger dimensions. one way to mitigate this is to scale
   \(f_{att}(\mathbf{h}_i, \mathbf{s}_j)\) by \(1 / \sqrt{d_h}\) (vaswani
   et al., 2017) ^[68][33].

   attention cannot only be used to attend to encoder or previous hidden
   states, but also to obtain a distribution over other features, such as
   the id27s of a text as used for reading comprehension (kadlec
   et al., 2017) ^[69][34]. however, attention is not directly applicable
   to classification tasks that do not require additional information,
   such as id31. in such models, the final hidden state of
   an lstm or an aggregation function such as max pooling or averaging is
   often used to obtain a sentence representation.

   self-attention   without any additional information, however, we can
   still extract relevant aspects from the sentence by allowing it to
   attend to itself using self-attention (lin et al., 2017) ^[70][35].
   self-attention, also called intra-attention has been used successfully
   in a variety of tasks including reading comprehension (cheng et al.,
   2016) ^[71][36], id123 (parikh et al., 2016) ^[72][37],
   and abstractive summarization (paulus et al., 2017) ^[73][38].

   we can simplify additive attention to compute the unnormalized
   alignment score for each hidden state \(\mathbf{h}_i\):

   \(f_{att}(\mathbf{h}_i) = \mathbf{v}_a{}^\top \text{tanh}(\mathbf{w}_a
   \mathbf{h}_i) \)

   in matrix form, for hidden states \(\mathbf{h} = \mathbf{h}_1, \ldots,
   \mathbf{h}_n\) we can calculate the attention vector \(\mathbf{a}\) and
   the final sentence representation \(\mathbf{c}\) as follows:

   \(\begin{align}\begin{split}
   \mathbf{a} &= \text{softmax}(\mathbf{v}_a \text{tanh}(\mathbf{w}_a
   \mathbf{h}^\top))\\
   \mathbf{c} & = \mathbf{h} \mathbf{a}^\top
   \end{split}\end{align}\)

   rather than only extracting one vector, we can perform several hops of
   attention by using a matrix \(\mathbf{v}_a\) instead of
   \(\mathbf{v}_a\), which allows us to extract an attention matrix
   \(\mathbf{a}\):

   \(\begin{align}\begin{split}
   \mathbf{a} &= \text{softmax}(\mathbf{v}_a \text{tanh}(\mathbf{w}_a
   \mathbf{h}^\top))\\
   \mathbf{c} & = \mathbf{a} \mathbf{h}
   \end{split}\end{align}\)

   in practice, we enforce the following orthogonality constraint to
   penalize redundancy and encourage diversity in the attention vectors in
   the form of the squared frobenius norm:

   \(\omega = |(\mathbf{a}\mathbf{a}^\top - \mathbf{i} |^2_f \)

   a similar multi-head attention is also used by vaswani et al. (2017).

   key-value attention   finally, key-value attention (daniluk et al.,
   2017) ^[74][39] is a recent attention variant that separates form from
   function by keeping separate vectors for the attention calculation. it
   has also been found useful for different document modelling tasks (liu
   & lapata, 2017) ^[75][40]. specifically, key-value attention splits
   each hidden vector \(\mathbf{h}_i\) into a key \(\mathbf{k}_i\) and a
   value \(\mathbf{v}_i\): \([\mathbf{k}_i; \mathbf{v}_i] =
   \mathbf{h}_i\). the keys are used for calculating the attention
   distribution \(\mathbf{a}_i\) using additive attention:

   \(\mathbf{a}_i = \text{softmax}(\mathbf{v}_a{}^\top
   \text{tanh}(\mathbf{w}_1 [\mathbf{k}_{i-l}; \ldots; \mathbf{k}_{i-1}] +
   (\mathbf{w}_2 \mathbf{k}_i)\mathbf{1}^\top)) \)

   where \(l\) is the length of the attention window and \(\mathbf{1}\) is
   a vector of ones. the values are then used to obtain the context
   representation \(\mathbf{c}_i\):

   \(\mathbf{c}_i = [\mathbf{v}_{i-l}; \ldots; \mathbf{v}_{i-1}]
   \mathbf{a}^\top\)

   the context \(\mathbf{c}_i\) is used together with the current value
   \(\mathbf{v}_i\) for prediction.

optimization

   the optimization algorithm and scheme is often one of the parts of the
   model that is used as-is and treated as a black-box. sometimes, even
   slight changes to the algorithm, e.g. reducing the \(\beta_2\) value in
   adam (dozat & manning, 2017) ^[76][41] can make a large difference to
   the optimization behaviour.

   optimization algorithm   adam (kingma & ba, 2015) ^[77][42] is one of
   the most popular and widely used optimization algorithms and often the
   go-to optimizer for nlp researchers. it is often thought that adam
   clearly outperforms vanilla stochastic id119 (sgd). however,
   while it converges much faster than sgd, it has been observed that sgd
   with learning rate annealing slightly outperforms adam (wu et al.,
   2016). recent work furthermore shows that sgd with properly tuned
   momentum outperforms adam (zhang et al., 2017) ^[78][43].

   optimization scheme   while adam internally tunes the learning rate for
   every parameter (ruder, 2016) ^[79][44], we can explicitly use
   sgd-style annealing with adam. in particular, we can perform learning
   rate annealing with restarts: we set a learning rate and train the
   model until convergence. we then halve the learning rate and restart by
   loading the previous best model. in adam's case, this causes the
   optimizer to forget its per-parameter learning rates and start fresh.
   denkowski & neubig (2017) ^[80][45] show that adam with 2 restarts and
   learning rate annealing is faster and performs better than sgd with
   annealing.

ensembling

   combining multiple models into an ensemble by averaging their
   predictions is a proven strategy to improve model performance. while
   predicting with an ensemble is expensive at test time, recent advances
   in distillation allow us to compress an expensive ensemble into a much
   smaller model (hinton et al., 2015; kuncoro et al., 2016; kim & rush,
   2016) ^[81][46] ^[82][47] ^[83][48].

   ensembling is an important way to ensure that results are still
   reliable if the diversity of the evaluated models increases (denkowski
   & neubig, 2017). while ensembling different checkpoints of a model has
   been shown to be effective (jean et al., 2015; sennrich et al., 2016)
   ^[84][49] ^[85][50], it comes at the cost of model diversity. cyclical
   learning rates can help to mitigate this effect (huang et al., 2017)
   ^[86][51]. however, if resources are available, we prefer to ensemble
   multiple independently trained models to maximize model diversity.

hyperparameter optimization

   rather than pre-defining or using off-the-shelf hyperparameters, simply
   tuning the hyperparameters of our model can yield significant
   improvements over baselines. recent advances in bayesian optimization
   have made it an ideal tool for the black-box optimization of
   hyperparameters in neural networks (snoek et al., 2012) ^[87][52] and
   far more efficient than the widely used grid search. automatic tuning
   of hyperparameters of an lstm has led to state-of-the-art results in
   id38, outperforming models that are far more complex
   (melis et al., 2017).

lstm tricks

   learning the initial state   we generally initialize the initial lstm
   states with a \(0\) vector. instead of fixing the initial state, we can
   learn it like any other parameter, which can improve performance and is
   also [88]recommended by hinton. refer to [89]this blog post for a
   tensorflow implementation.

   tying input and output embeddings   input and output embeddings account
   for the largest number of parameters in the lstm model. if the lstm
   predicts words as in language modelling, input and output parameters
   can be shared (inan et al., 2016; press & wolf, 2017) ^[90][53]
   ^[91][54]. this is particularly useful on small datasets that do not
   allow to learn a large number of parameters.

   gradient norm clipping   one way to decrease the risk of exploding
   gradients is to clip their maximum value (mikolov, 2012) ^[92][55].
   this, however, does not improve performance consistently (reimers &
   gurevych, 2017). rather than clipping each gradient independently,
   clipping the global norm of the gradient (pascanu et al., 2013)
   ^[93][56] yields more significant improvements (a tensorflow
   implementation can be found [94]here).

   down-projection   to reduce the number of output parameters further,
   the hidden state of the lstm can be projected to a smaller size. this
   is useful particularly for tasks with a large number of outputs, such
   as language modelling (melis et al., 2017).

task-specific best practices

   in the following, we will discuss task-specific best practices. most of
   these perform best for a particular type of task. some of them might
   still be applied to other tasks, but should be validated before. we
   will discuss the following tasks: classification, sequence labelling,
   id86 (id86), and -- as a special case of id86 --
   id4.

classification

   more so than for sequence tasks, where id98s have only recently found
   application due to more efficient convolutional operations, id98s have
   been popular for classification tasks in nlp. the following best
   practices relate to id98s and capture some of their optimal
   hyperparameter choices.

   id98 filters   combining filter sizes near the optimal filter size, e.g.
   (3,4,5) performs best (kim, 2014; kim et al., 2016). the optimal number
   of feature maps is in the range of 50-600 (zhang & wallace, 2015)
   ^[95][57].

   aggregation function   1-max-pooling outperforms average-pooling and
   \(k\)-max pooling (zhang & wallace, 2015).

sequence labelling

   sequence labelling is ubiquitous in nlp. while many of the existing
   best practices are with regard to a particular part of the model
   architecture, the following guidelines discuss choices for the model's
   output and prediction stage.

   tagging scheme   for some tasks, which can assign labels to segments of
   texts, different tagging schemes are possible. these are: bio, which
   marks the first token in a segment with a b- tag, all remaining tokens
   in the span with an _i-_tag, and tokens outside of segments with an o-
   tag; iob, which is similar to bio, but only uses b- if the previous
   token is of the same class but not part of the segment; and iobes,
   which in addition distinguishes between single-token entities (s-) and
   the last token in a segment (e-). using iobes and bio yield similar
   performance (lample et al., 2017)

   crf output layer   if there are any dependencies between outputs, such
   as in id39 the final softmax layer can be replaced
   with a linear-chain conditional random field (crf). this has been shown
   to yield consistent improvements for tasks that require the modelling
   of constraints (huang et al., 2015; max & hovy, 2016; lample et al.,
   2016) ^[96][58] ^[97][59] ^[98][60].

   constrained decoding   rather than using a crf output layer,
   constrained decoding can be used as an alternative approach to reject
   erroneous sequences, i.e. such that do not produce valid bio
   transitions (he et al., 2017). constrained decoding has the advantage
   that arbitrary constraints can be enforced this way, e.g. task-specific
   or syntactic constraints.

id86

   most of the existing best practices can be applied to natural language
   generation (id86). in fact, many of the tips presented so far stem from
   advances in language modelling, the most prototypical nlp task.

   modelling coverage   repetition is a big problem in many id86 tasks as
   current models do not have a good way of remembering what outputs they
   already produced. modelling coverage explicitly in the model is a good
   way of addressing this issue. a checklist can be used if it is known in
   advances, which entities should be mentioned in the output, e.g.
   ingredients in recipes (kiddon et al., 2016) ^[99][61]. if attention is
   used, we can keep track of a coverage vector \(\mathbf{c}_i\), which is
   the sum of attention distributions \(\mathbf{a}_t\) over previous time
   steps (tu et al., 2016; see et al., 2017) ^[100][62] ^[101][63]:

   \(\mathbf{c}_i = \sum\limits^{i-1}_{t=1} \mathbf{a}_t \)

   this vector captures how much attention we have paid to all words in
   the source. we can now condition additive attention additionally on
   this coverage vector in order to encourage our model not to attend to
   the same words repeatedly:

   \(f_{att}(\mathbf{h}_i,\mathbf{s}_j,\mathbf{c}_i) = \mathbf{v}_a{}^\top
   \text{tanh}(\mathbf{w}_1 \mathbf{h}_i + \mathbf{w}_2 \mathbf{s}_j +
   \mathbf{w}_3 \mathbf{c}_i )\)

   in addition, we can add an auxiliary loss that captures the
   task-specific attention behaviour that we would like to elicit: for
   id4, we would like to have a roughly one-to-one alignment; we thus
   penalize the model if the final coverage vector is more or less than
   one at every index (tu et al., 2016). for summarization, we only want
   to penalize the model if it repeatedly attends to the same location
   (see et al., 2017).

id4

   while id4 (id4) is an instance of id86, id4
   receives so much attention that many methods have been developed
   specifically for the task. similarly, many best practices or
   hyperparameter choices apply exclusively to it.

   embedding dimensionality   2048-dimensional embeddings yield the best
   performance, but only do so by a small margin. even 128-dimensional
   embeddings perform surprisingly well and converge almost twice as
   quickly (britz et al., 2017).

   encoder and decoder depth   the encoder does not need to be deeper than
   \(2-4\) layers. deeper models outperform shallower ones, but more than
   \(4\) layers is not necessary for the decoder (britz et al., 2017).

   directionality   bidirectional encoders outperform unidirectional ones
   by a small margin. sutskever et al., (2014) ^[102][64] proposed to
   reverse the source sequence to reduce the number of long-term
   dependencies. reversing the source sequence in unidirectional encoders
   outperforms its non-reversed counter-part (britz et al., 2017).

   id125 strategy   medium beam sizes around \(10\) with length
   id172 penalty of \(1.0\) (wu et al., 2016) yield the best
   performance (britz et al., 2017).

   sub-word translation   senrich et al. (2016) ^[103][65] proposed to
   split words into sub-words based on a byte-pair encoding (bpe). bpe
   iteratively merges frequent symbol pairs, which eventually results in
   frequent character id165s being merged into a single symbol, thereby
   effectively eliminating out-of-vocabulary-words. while it was
   originally meant to handle rare words, a model with sub-word units
   outperforms full-word systems across the board, with 32,000 being an
   effective vocabulary size for sub-word units (denkowski & neubig,
   2017).

conclusion

   i hope this post was helpful in kick-starting your learning of a new
   nlp task. even if you have already been familiar with most of these, i
   hope that you still learnt something new or refreshed your knowledge of
   useful tips.

   i am sure that i have forgotten many best practices that deserve to be
   on this list. similarly, there are many tasks such as parsing,
   information extraction, etc., which i do not know enough about to give
   recommendations. if you have a best practice that should be on this
   list, do let me know in the comments below. please provide at least one
   reference and your handle for attribution. if this gets very
   collaborative, i might open a github repository rather than collecting
   feedback here (i won't be able to accept prs submitted directly to the
   generated html source of this article).

   credit for the cover image goes to bahdanau et al. (2015).
     __________________________________________________________________

    1. goldberg, y. (2016). a primer on neural network models for natural
       language processing. journal of artificial intelligence research,
       57, 345   420. [104]https://doi.org/10.1613/jair.4992 [105]      
    2. kim, y. (2014). convolutional neural networks for sentence
       classification. proceedings of the conference on empirical methods
       in natural language processing, 1746   1751. retrieved from
       [106]http://arxiv.org/abs/1408.5882 [107]      
    3. melamud, o., mcclosky, d., patwardhan, s., & bansal, m. (2016). the
       role of context types and dimensionality in learning word
       embeddings. in proceedings of naacl-hlt 2016 (pp. 1030   1040).
       retrieved from [108]http://arxiv.org/abs/1601.00893 [109]      
    4. plank, b., s  gaard, a., & goldberg, y. (2016). multilingual
       part-of-speech tagging with bidirectional long short-term memory
       models and auxiliary loss. in proceedings of the 54th annual
       meeting of the association for computational linguistics. [110]      
    5. ruder, s., ghaffari, p., & breslin, j. g. (2016). a hierarchical
       model of reviews for aspect-based id31. proceedings
       of the 2016 conference on empirical methods in natural language
       processing (emnlp-16), 999   1005. retrieved from
       [111]http://arxiv.org/abs/1609.02745 [112]      
    6. he, l., lee, k., lewis, m., & zettlemoyer, l. (2017). deep semantic
       role labeling: what works and what   s next. acl. [113]      
    7. wu, y., schuster, m., chen, z., le, q. v, norouzi, m., macherey,
       w.,     dean, j. (2016). google   s id4 system:
       bridging the gap between human and machine translation. arxiv
       preprint arxiv:1609.08144. [114]      
    8. reimers, n., & gurevych, i. (2017). optimal hyperparameters for
       deep lstm-networks for sequence labeling tasks. in arxiv preprint
       arxiv:1707.06799: retrieved from
       [115]https://arxiv.org/pdf/1707.06799.pdf [116]      
    9. zhang, x., zhao, j., & lecun, y. (2015). character-level
       convolutional networks for text classification. advances in neural
       information processing systems, 649   657. retrieved from
       [117]http://arxiv.org/abs/1509.01626 [118]      
   10. conneau, a., schwenk, h., barrault, l., & lecun, y. (2016). very
       deep convolutional networks for natural language processing. arxiv
       preprint arxiv:1606.01781. retrieved from
       [119]http://arxiv.org/abs/1606.01781 [120]      
   11. le, h. t., cerisara, c., & denis, a. (2017). do convolutional
       networks need to be deep for text classification ? in arxiv
       preprint arxiv:1707.04108. [121]      
   12. srivastava, r. k., greff, k., & schmidhuber, j. (2015). training
       very deep networks. in advances in neural information processing
       systems. [122]      
   13. kim, y., jernite, y., sontag, d., & rush, a. m. (2016).
       character-aware neural language models. aaai. retrieved from
       [123]http://arxiv.org/abs/1508.06615 [124]      
   14. jozefowicz, r., vinyals, o., schuster, m., shazeer, n., & wu, y.
       (2016). exploring the limits of id38. arxiv preprint
       arxiv:1602.02410. retrieved from
       [125]http://arxiv.org/abs/1602.02410 [126]      
   15. zilly, j. g., srivastava, r. k., koutnik, j., & schmidhuber, j.
       (2017). recurrent id199. in international conference on
       machine learning (icml 2017). [127]      
   16. zhang, y., chen, g., yu, d., yao, k., kudanpur, s., & glass, j.
       (2016). highway long short-term memory id56s for distant speech
       recognition. in 2016 ieee international conference on acoustics,
       speech and signal processing (icassp). [128]      
   17. he, k., zhang, x., ren, s., & sun, j. (2016). deep residual
       learning for image recognition. in cvpr. [129]      
   18. huang, g., weinberger, k. q., & maaten, l. van der. (2016). densely
       connected convolutional networks. cvpr 2017. [130]      
   19. ruder, s., bingel, j., augenstein, i., & s  gaard, a. (2017). sluice
       networks: learning what to share between loosely related tasks.
       arxiv preprint arxiv:1705.08142. retrieved from
       [131]http://arxiv.org/abs/1705.08142 [132]       [133]      
   20. britz, d., goldie, a., luong, t., & le, q. (2017). massive
       exploration of id4 architectures. in arxiv
       preprint arxiv:1703.03906. [134]      
   21. srivastava, n., hinton, g., krizhevsky, a., sutskever, i., &
       salakhutdinov, r. (2014). dropout: a simple way to prevent neural
       networks from overfitting. journal of machine learning research,
       15, 1929   1958.
       [135]http://www.cs.cmu.edu/~rsalakhu/papers/srivastava14a.pdf
       [136]      
   22. ba, j., & frey, b. (2013). adaptive dropout for training deep
       neural networks. in advances in neural information processing
       systems. retrieved from
       file:///files/a5/a51d0755-5cef-4772-942d-c5b8157fbe5e.pdf [137]      
   23. li, z., gong, b., & yang, t. (2016). improved dropout for shallow
       and deep learning. in advances in neural information processing
       systems 29 (nips 2016). retrieved from
       [138]http://arxiv.org/abs/1602.02220 [139]      
   24. gal, y., & ghahramani, z. (2016). a theoretically grounded
       application of dropout in recurrent neural networks. in advances in
       neural information processing systems. retrieved from
       [140]http://arxiv.org/abs/1512.05287 [141]      
   25. melis, g., dyer, c., & blunsom, p. (2017). on the state of the art
       of evaluation in neural language models. [142]      
   26. ruder, s. (2017). an overview of id72 in deep neural
       networks. in arxiv preprint arxiv:1706.05098. [143]      
   27. rei, m. (2017). semi-supervised multitask learning for sequence
       labeling. in proceedings of acl 2017. [144]      
   28. ramachandran, p., liu, p. j., & le, q. v. (2016). unsupervised
       pretrainig for sequence to sequence learning. arxiv preprint
       arxiv:1611.02683. [145]      
   29. s  gaard, a., & goldberg, y. (2016). deep id72 with
       low level tasks supervised at lower layers. proceedings of the 54th
       annual meeting of the association for computational linguistics,
       231   235. [146]      
   30. liu, p., qiu, x., & huang, x. (2017). adversarial multi-task
       learning for text classification. in acl 2017. retrieved from
       [147]http://arxiv.org/abs/1704.05742 [148]      
   31. bahdanau, d., cho, k., & bengio, y.. id4 by
       jointly learning to align and translate. iclr 2015.
       [149]https://doi.org/10.1146/annurev.neuro.26.041002.131047 [150]      
   32. luong, m.-t., pham, h., & manning, c. d. (2015). effective
       approaches to attention-based id4. emnlp
       2015. retrieved from [151]http://arxiv.org/abs/1508.04025 [152]      
   33. vaswani, a., shazeer, n., parmar, n., uszkoreit, j., jones, l.,
       gomez, a. n.,     polosukhin, i. (2017). attention is all you need.
       arxiv preprint arxiv:1706.03762. [153]      
   34. kadlec, r., schmid, m., bajgar, o., & kleindienst, j. (2016). text
       understanding with the attention sum reader network. in proceedings
       of the 54th annual meeting of the association for computational
       linguistics. [154]      
   35. lin, z., feng, m., santos, c. n. dos, yu, m., xiang, b., zhou, b.,
       & bengio, y. (2017). a structured self-attentive sentence
       embedding. in iclr 2017. [155]      
   36. cheng, j., dong, l., & lapata, m. (2016). long short-term
       memory-networks for machine reading. arxiv preprint
       arxiv:1601.06733. retrieved from
       [156]http://arxiv.org/abs/1601.06733 [157]      
   37. parikh, a. p., t  ckstr  m, o., das, d., & uszkoreit, j. (2016). a
       decomposable attention model for natural language id136. in
       proceedings of the 2016 conference on empirical methods in natural
       language processing. retrieved from
       [158]http://arxiv.org/abs/1606.01933 [159]      
   38. paulus, r., xiong, c., & socher, r. (2017). a deep reinforced model
       for abstractive summarization. in arxiv preprint arxiv:1705.04304,.
       retrieved from [160]http://arxiv.org/abs/1705.04304 [161]      
   39. daniluk, m., rockt, t., welbl, j., & riedel, s. (2017).
       frustratingly short attention spans in neural id38. in
       iclr 2017. [162]      
   40. liu, y., & lapata, m. (2017). learning structured text
       representations. in arxiv preprint arxiv:1705.09207. retrieved from
       [163]http://arxiv.org/abs/1705.09207 [164]      
   41. dozat, t., & manning, c. d. (2017). deep biaffine attention for
       neural id33. in iclr 2017. retrieved from
       [165]http://arxiv.org/abs/1611.01734 [166]      
   42. kingma, d. p., & ba, j. l. (2015). adam: a method for stochastic
       optimization. international conference on learning representations.
       [167]      
   43. zhang, j., mitliagkas, i., & r  , c. (2017). yellowfin and the art
       of momentum tuning. arxiv preprint arxiv:1706.03471. [168]      
   44. ruder, s. (2016). an overview of id119 optimization.
       arxiv preprint arxiv:1609.04747. [169]      
   45. denkowski, m., & neubig, g. (2017). stronger baselines for
       trustable results in id4. [170]      
   46. hinton, g., vinyals, o., & dean, j. (2015). distilling the
       knowledge in a neural network. arxiv preprint arxiv:1503.02531.
       [171]https://doi.org/10.1063/1.4931082 [172]      
   47. kuncoro, a., ballesteros, m., kong, l., dyer, c., & smith, n. a.
       (2016). distilling an ensemble of greedy dependency parsers into
       one mst parser. empirical methods in natural language processing.
       [173]      
   48. kim, y., & rush, a. m. (2016). sequence-level knowledge
       distillation. proceedings of the 2016 conference on empirical
       methods in natural language processing (emnlp-16). [174]      
   49. jean, s., cho, k., memisevic, r., & bengio, y. (2015). on using
       very large target vocabulary for id4.
       proceedings of the 53rd annual meeting of the association for
       computational linguistics and the 7th international joint
       conference on natural language processing (volume 1: long papers),
       1   10. retrieved from [175]http://www.aclweb.org/anthology/p15-1001
       [176]      
   50. sennrich, r., haddow, b., & birch, a. (2016). edinburgh neural
       machine translation systems for wmt 16. in proceedings of the first
       conference on machine translation (wmt 2016). retrieved from
       [177]http://arxiv.org/abs/1606.02891 [178]      
   51. huang, g., li, y., pleiss, g., liu, z., hopcroft, j. e., &
       weinberger, k. q. (2017). snapshot ensembles: train 1, get m for
       free. in iclr 2017. [179]      
   52. snoek, j., larochelle, h., & adams, r. p. (2012). practical
       bayesian optimization of machine learning algorithms. neural
       information processing systems conference (nips 2012).
       [180]https://papers.nips.cc/paper/4522-practical-bayesian-optimizat
       ion-of-machine-learning-algorithms.pdf [181]      
   53. inan, h., khosravi, k., & socher, r. (2016). tying word vectors and
       word classifiers: a loss framework for id38. arxiv
       preprint arxiv:1611.01462. [182]      
   54. press, o., & wolf, l. (2017). using the output embedding to improve
       language models. proceedings of the 15th conference of the european
       chapter of the association for computational linguistics: volume 2,
       short papers, 2, 157--163. [183]      
   55. mikolov, t. (2012). statistical language models based on neural
       networks (doctoral dissertation, phd thesis, brno university of
       technology). [184]      
   56. pascanu, r., mikolov, t., & bengio, y. (2013). on the difficulty of
       training recurrent neural networks. international conference on
       machine learning, (2), 1310   1318.
       [185]https://doi.org/10.1109/72.279181 [186]      
   57. zhang, y., & wallace, b. (2015). a sensitivity analysis of (and
       practitioners    guide to) convolutional neural networks for sentence
       classification. arxiv preprint arxiv:1510.03820, (1). retrieved
       from [187]http://arxiv.org/abs/1510.03820 [188]      
   58. huang, z., xu, w., & yu, k. (2015). bidirectional lstm-crf models
       for sequence tagging. arxiv preprint arxiv:1508.01991. [189]      
   59. ma, x., & hovy, e. (2016). end-to-end sequence labeling via
       bi-directional lstm-id98s-crf. arxiv preprint arxiv:1603.01354.
       [190]      
   60. lample, g., ballesteros, m., subramanian, s., kawakami, k., & dyer,
       c. (2016). neural architectures for id39.
       naacl-hlt 2016. [191]      
   61. kiddon, c., zettlemoyer, l., & choi, y. (2016). globally coherent
       text generation with neural checklist models. proceedings of the
       2016 conference on empirical methods in natural language processing
       (emnlp2016), 329   339. [192]      
   62. tu, z., lu, z., liu, y., liu, x., & li, h. (2016). modeling
       coverage for id4. proceedings of the 54th
       annual meeting of the association for computational linguistics.
       [193]https://doi.org/10.1145/2856767.2856776 [194]      
   63. see, a., liu, p. j., & manning, c. d. (2017). get to the point:
       summarization with pointer-generator networks. in acl 2017. [195]      
   64. sutskever, i., vinyals, o., & le, q. v. (2014). sequence to
       sequence learning with neural networks. advances in neural
       information processing systems, 9. retrieved from
       [196]http://arxiv.org/abs/1409.3215\nhttp://papers.nips.cc/paper/53
       46-sequence-to-sequence-learning-with-neural-networks [197]      
   65. sennrich, r., haddow, b., & birch, a. (2016). neural machine
       translation of rare words with subword units. in proceedings of the
       54th annual meeting of the association for computational
       linguistics (acl 2016). retrieved from
       [198]http://arxiv.org/abs/1508.07909 [199]      

   sebastian ruder

[200]sebastian ruder

   read [201]more posts by this author.
   [202]read more

       sebastian ruder    

[203]natural language processing

     * [204]neural id21 for natural language processing (phd
       thesis)
     * [205]aaai 2019 highlights: dialogue, reproducibility, and more
     * [206]the 4 biggest open problems in nlp

   [207]see all 23 posts    

   [208]learning to select data for id21

   id20

learning to select data for id21

   id20 methods typically seek to identify features that are
   shared between the domains or learn representations that are general
   enough to be useful for both domains. this post discusses a
   complementary approach to id20 that selects data that is
   useful for training the model.

     * sebastian ruder
       [209]sebastian ruder

   [210]an overview of id72 in deep neural networks

   id72

an overview of id72 in deep neural networks

   id72 is becoming more and more popular. this post gives
   a general overview of the current state of id72. in
   particular, it provides context for current neural network-based
   methods by discussing the extensive id72 literature.

     * sebastian ruder
       [211]sebastian ruder

   [212]sebastian ruder
      
   deep learning for nlp best practices
   share this
   please enable javascript to view the [213]comments powered by disqus.

   [214]sebastian ruder    2019

   [215]latest posts [216]twitter [217]ghost

references

   visible links
   1. http://ruder.io/rss/
   2. http://ruder.io/
   3. http://ruder.io/about/
   4. http://ruder.io/tags/
   5. http://ruder.io/publications/
   6. http://ruder.io/talks/
   7. http://ruder.io/news/
   8. http://ruder.io/faq/
   9. http://ruder.io/nlp-news/
  10. https://nlpprogress.com/
  11. http://ruder.io/contact/
  12. http://ruder.io/tag/natural-language-processing/index.html
  13. https://news.ycombinator.com/item?id=14852704
  14. http://ruder.io/deep-learning-nlp-best-practices/index.html#introduction
  15. http://ruder.io/deep-learning-nlp-best-practices/index.html#bestpractices
  16. http://ruder.io/deep-learning-nlp-best-practices/index.html#wordembeddings
  17. http://ruder.io/deep-learning-nlp-best-practices/index.html#depth
  18. http://ruder.io/deep-learning-nlp-best-practices/index.html#layerconnections
  19. http://ruder.io/deep-learning-nlp-best-practices/index.html#dropout
  20. http://ruder.io/deep-learning-nlp-best-practices/index.html#multitasklearning
  21. http://ruder.io/deep-learning-nlp-best-practices/index.html#attention
  22. http://ruder.io/deep-learning-nlp-best-practices/index.html#optimization
  23. http://ruder.io/deep-learning-nlp-best-practices/index.html#ensembling
  24. http://ruder.io/deep-learning-nlp-best-practices/index.html#hyperparameteroptimization
  25. http://ruder.io/deep-learning-nlp-best-practices/index.html#lstmtricks
  26. http://ruder.io/deep-learning-nlp-best-practices/index.html#taskspecificbestpractices
  27. http://ruder.io/deep-learning-nlp-best-practices/index.html#classification
  28. http://ruder.io/deep-learning-nlp-best-practices/index.html#sequencelabelling
  29. http://ruder.io/deep-learning-nlp-best-practices/index.html#naturallanguagegeneration
  30. http://ruder.io/deep-learning-nlp-best-practices/index.html#neuralmachinetranslation
  31. https://twitter.com/iaugenstein/status/710837374473920512
  32. https://www.jair.org/media/4992/live-4992-9623-jair.pdf
  33. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn1
  34. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn2
  35. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn3
  36. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn4
  37. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn5
  38. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn6
  39. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn7
  40. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn8
  41. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn9
  42. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn10
  43. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn11
  44. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn12
  45. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn13
  46. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn14
  47. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn15
  48. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn16
  49. http://people.idsia.ch/~rupesh/very_deep_learning/
  50. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn17
  51. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn18
  52. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn19
  53. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn20
  54. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn21
  55. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn22
  56. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn23
  57. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn24
  58. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn25
  59. http://ruder.io/multi-task/index.html
  60. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn26
  61. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn27
  62. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn28
  63. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn29
  64. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn30
  65. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn19
  66. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn31
  67. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn32
  68. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn33
  69. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn34
  70. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn35
  71. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn36
  72. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn37
  73. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn38
  74. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn39
  75. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn40
  76. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn41
  77. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn42
  78. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn43
  79. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn44
  80. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn45
  81. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn46
  82. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn47
  83. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn48
  84. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn49
  85. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn50
  86. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn51
  87. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn52
  88. https://www.cs.toronto.edu/~hinton/csc2535/notes/lec10new.pdf
  89. https://r2rt.com/non-zero-initial-states-for-recurrent-neural-networks.html
  90. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn53
  91. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn54
  92. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn55
  93. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn56
  94. https://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow
  95. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn57
  96. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn58
  97. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn59
  98. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn60
  99. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn61
 100. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn62
 101. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn63
 102. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn64
 103. http://ruder.io/deep-learning-nlp-best-practices/index.html#fn65
 104. https://doi.org/10.1613/jair.4992
 105. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref1
 106. http://arxiv.org/abs/1408.5882
 107. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref2
 108. http://arxiv.org/abs/1601.00893
 109. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref3
 110. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref4
 111. http://arxiv.org/abs/1609.02745
 112. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref5
 113. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref6
 114. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref7
 115. https://arxiv.org/pdf/1707.06799.pdf
 116. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref8
 117. http://arxiv.org/abs/1509.01626
 118. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref9
 119. http://arxiv.org/abs/1606.01781
 120. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref10
 121. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref11
 122. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref12
 123. http://arxiv.org/abs/1508.06615
 124. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref13
 125. http://arxiv.org/abs/1602.02410
 126. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref14
 127. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref15
 128. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref16
 129. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref17
 130. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref18
 131. http://arxiv.org/abs/1705.08142
 132. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref19
 133. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref19:1
 134. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref20
 135. http://www.cs.cmu.edu/~rsalakhu/papers/srivastava14a.pdf
 136. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref21
 137. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref22
 138. http://arxiv.org/abs/1602.02220
 139. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref23
 140. http://arxiv.org/abs/1512.05287
 141. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref24
 142. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref25
 143. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref26
 144. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref27
 145. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref28
 146. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref29
 147. http://arxiv.org/abs/1704.05742
 148. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref30
 149. https://doi.org/10.1146/annurev.neuro.26.041002.131047
 150. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref31
 151. http://arxiv.org/abs/1508.04025
 152. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref32
 153. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref33
 154. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref34
 155. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref35
 156. http://arxiv.org/abs/1601.06733
 157. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref36
 158. http://arxiv.org/abs/1606.01933
 159. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref37
 160. http://arxiv.org/abs/1705.04304
 161. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref38
 162. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref39
 163. http://arxiv.org/abs/1705.09207
 164. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref40
 165. http://arxiv.org/abs/1611.01734
 166. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref41
 167. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref42
 168. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref43
 169. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref44
 170. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref45
 171. https://doi.org/10.1063/1.4931082
 172. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref46
 173. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref47
 174. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref48
 175. http://www.aclweb.org/anthology/p15-1001
 176. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref49
 177. http://arxiv.org/abs/1606.02891
 178. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref50
 179. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref51
 180. https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf
 181. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref52
 182. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref53
 183. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref54
 184. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref55
 185. https://doi.org/10.1109/72.279181
 186. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref56
 187. http://arxiv.org/abs/1510.03820
 188. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref57
 189. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref58
 190. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref59
 191. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref60
 192. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref61
 193. https://doi.org/10.1145/2856767.2856776
 194. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref62
 195. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref63
 196. http://arxiv.org/abs/1409.3215\nhttp://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks
 197. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref64
 198. http://arxiv.org/abs/1508.07909
 199. http://ruder.io/deep-learning-nlp-best-practices/index.html#fnref65
 200. http://ruder.io/author/sebastian/index.html
 201. http://ruder.io/author/sebastian/index.html
 202. http://ruder.io/author/sebastian/index.html
 203. http://ruder.io/tag/natural-language-processing/index.html
 204. http://ruder.io/thesis/index.html
 205. http://ruder.io/aaai-2019-highlights/index.html
 206. http://ruder.io/4-biggest-open-problems-in-nlp/index.html
 207. http://ruder.io/tag/natural-language-processing/index.html
 208. http://ruder.io/index.html
 209. http://ruder.io/author/sebastian/index.html
 210. http://ruder.io/index.html
 211. http://ruder.io/author/sebastian/index.html
 212. http://ruder.io/
 213. https://disqus.com/?ref_noscript
 214. http://ruder.io/
 215. http://ruder.io/
 216. https://twitter.com/seb_ruder
 217. https://ghost.org/

   hidden links:
 219. https://twitter.com/seb_ruder
 220. http://ruder.io/rss/index.rss
 221. http://ruder.io/index.html
 222. http://ruder.io/index.html
 223. https://twitter.com/share?text=deep%20learning%20for%20nlp%20best%20practices&url=http://ruder.io/deep-learning-nlp-best-practices/
 224. https://www.facebook.com/sharer/sharer.php?u=http://ruder.io/deep-learning-nlp-best-practices/
