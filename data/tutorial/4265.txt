   [1]cvpr 2017 workshop on visual understanding across modalities [2]home
   [3]thor [4]charades [5]tqa

thor challenge

   navigate and find objects in a virtual environment

for the full release of ai2-thor, please refer to this [6]link.

   in this challenge, the task is to navigate towards a set of objects in
   indoor scenes based only on visual input. ai2-thor enables navigation
   in a set of near-photo-realistic indoor scenes, and we will use this
   framework for the challenge. the agents that the challenge participants
   provide receive visual input from thor and should perform an action
   based on the visual input.
     __________________________________________________________________

competition conclusion

   a team from national tsing hua university (nthu) has won the thor
   challenge with their strong showing on the discrete navigation
   challenge.

   the nthu team will present their work in the workshop on visual
   understanding across modalities @ cvpr 2:00-2:30 pm on 7/26. everyone
   is encouraged to attend!

discrete navigation leaderboard

   rank   entrant    score
    1   jameschuang 0.04248
    2     phunghx   0.01651
    3     random    0.00001
     __________________________________________________________________

test data

   the test data for the thor challenge is now available. to use it,
   please download the test target definition file and the thor build
   corresponding to your architecture linked here. installation is
   unchanged from the instructions below, simply replace the thor build
   where it appears.
wget https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/thor-challeng
e-targets-test.zip
wget https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/thor-20170629
1201-osxintel64.zip
wget https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/thor-20170629
1201-linux64.zip

   the instructions for running the robosims framework are also unchanged
   from those given below. you need only to replace the training targets
   file with thor-challenge-targets-test.zip and the thor build given in
   unity_path with the appropriate osx or linux build linked here.
     __________________________________________________________________

tasks

   the task is to navigate and find objects using a set of predefined
   actions. the agent is placed in a random location within the scene and
   provided a target image (e.g. image of an apple). the action set is
   (moveahead, moveright, moveleft, moveback, lookup, lookdown,
   rotateright, rotateleft). the agent must not only navigate to the
   closest point possible to the object, but also be looking at the
   object. the release for the challenge includes 30 scenes (15 kitchens
   and 15 living rooms), where 20 scenes are training scenes, and 10
   scenes are for validation. we will release the test scenes later.
   target image agent start position agent target position
   [fork.png]   [start_position.png] [target_position.png]
     __________________________________________________________________

competition

   the evaluation server is up and running!
   the evaluation server is being hosted by codalab and can be found
   [7]here.

   a prize of $3,000 will be awarded separately to the highest scoring
   entrant of the discrete and continuous navigation subtasks.

   to submit results, participants should:
     * 1) sign up for a codalab account by clicking the sign up button on
       the competition page linked above.
     * 2) click on the participate tab, agree to the terms and conditions,
       and click register.
     * 3) if needed, navigate to the participate tab and click on the get
       data side tab to download the train and dev sets.
     * 4) navigate to the learn the details tab and click on the
       evaluation tab in the sidebar to read about the submission format
       required.
     * 5) after your request is approved, navigate to the participate tab
       and then click on the submit/view results tab in the sidebar.
     * 6) click the submit button and upload a results file.
     * 7) after your submission is scored you will have a chance to review
       it before posting it to the leaderboard.
     * 8) if you have questions, please ask them in the thor competition
       forum (located under the forum tab)
     __________________________________________________________________

dates

   the tentative dates for the competition are:

   test set is released june 26th (tentative)
   final submission     july 15th (11:59pm gmt)
   winners announced    july 20th
   conference workshop  july 26th
     __________________________________________________________________

installation

   the framework for the challenge consists of three parts:
     * thor: game engine that hosts the scenes (linux/osx)
     * robosims: python framework which interacts with the game engine
     * targets: zip file containing a json targets file as well as target
       images

   osx installation
pip install https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/robosi
ms-0.0.9-py2.py3-none-any.whl
wget https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/thor-20170501
1400-osxintel64.zip
wget https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/thor-challeng
e-targets.zip
unzip thor-challenge-targets.zip
unzip thor-201705011400-osxintel64.zip

   linux installation
to run on linux you must have an x server running that has a glx module enabled.
pip install https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/robosi
ms-0.0.9-py2.py3-none-any.whl
wget https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/thor-20170501
1400-linux64.zip
wget https://s3-us-west-2.amazonaws.com/ai2-vision-robosims/builds/thor-challeng
e-targets.zip
unzip thor-challenge-targets.zip
unzip thor-201705011400-linux64.zip

robosims example - training

#!/usr/bin/env python
import robosims
import json
env = robosims.controller.challengecontroller(
    # use unity_path=thor-201705011400-osxintel64.app/contents/macos/thor-201705
011400-osxintel64 for osx
    unity_path='projects/thor-201705011400-linux64',
    x_display="0.0" # this parameter is ignored on osx, but you must set this to
 the appropriate display on linux
)
env.start()
with open("thor-challenge-targets/targets-train.json") as f:
    t = json.loads(f.read())
    for target in t:
        env.initialize_target(target)
        # possible actions are: moveleft, moveright, moveahead, moveback, lookup
, lookdown, rotateright, rotateleft
        event = env.step(action=dict(action='moveleft'))

        # path to the target image (e.g. apple, lettuce, keychain, etc.)
        print(target['targetimage'])

        # target_found() returns true/false if the target has been found
        print(env.target_found())

        # image of the current frame from the agent - numpy array of shape (300,
300,3) in rgb order
        image = event.frame

        # true/false whether the last action was successful.  actions will fail
if you attempt to move into a wall or
        # lookup/down beyond the allowed range.
        print(event.metadata['lastactionsuccess'])


   the structure of the target definition:
{
        "agentpositionindex": 102,      # corresponds to a point on the grid
        "sceneindex": 0,  # which configuration for the scene (0-4)
        "scenename": "floorplan1",  # the name of the scene
        "startinghorizon": 330.0,  # starting camera horizon (up,down) in degree
s of the agent's camera
        "startingrotation": 90.0,  # global rotation of the agent
        "targetagenthorizon": 60.0,  # where agent should be looking to see the
target object
        "targetagentrotation": 180.0,  # how the agent should be rotated to see
the target object
        "targetimage": "images/floorplan1/spoon.png",  # path to image file that
 shows the target image
        "targetobjectid": "spoon|-01.73|+00.90|-00.95",  # internal id of the ta
rget object
        "targetposition": {
            "x": -0.6949999928474426,
            "y": 0.9800000190734863,
            "z": -1.7799999713897705
        },  # final position to see the target object
        "uuid": "a27cb81c-5f68-4f90-80e8-e01e49ab188d"  # unique identifier for
the task
    },
     __________________________________________________________________

challenge submission

   the participants should submit a file that includes the taken actions.
   the code below shows how to use the api. while running against the
   validation scenes, the target_found() method will never return true.
   the agent must decide when it has reached the goal and cease performing
   actions. once the agent has iterated through all the targets defined in
   the targets file, you can save the actions to a json file and submit to
   the codalab challenge.
#!/usr/bin/env python
import robosims
import json
env = robosims.controller.challengecontroller(
    # use for osx
    # unity_path='thor-201705011400-osxintel64.app/contents/macos/thor-201705011
400-osxintel64',
    unity_path='projects/thor-201705011400-linux64',
    x_display="0.0",
    record_actions=true
)
env.start()
with open("thor-challenge-targets/targets-val.json") as f:
    t = json.loads(f.read())
    for target in t:
        env.initialize_target(target)
        # navigate to the target
        event = env.step(action=dict(action='moveleft'))

    env.save_actions('submission-discrete.json')

continuous vs discrete track

   the challenge has two settings: (1) discrete (2) continuous. by
   default, the framework operates in discrete mode. this means that the
   agent navigates on a predefined grid. the agent is only able to look up
   and down in 30 degree increments and rotate in 90 degree increments. if
   an action is issued that pushes the agent into a wall, the agent will
   remain on the grid, but the lastactionsuccess value will be set to
   false. in continuous mode, the agent is free to rotate, look up/down
   and move in any amount. this allows the agent to collide with walls,
   move towards an object in fewer steps and rotate with greater
   precision.
# to enable continuous mode, pass the mode to the constructor
env = robosims.controller.challengecontroller(
    # use for osx
    # unity_path='thor-201705011400-osxintel64.app/contents/macos/thor-201705011
400-osxintel64',
    unity_path='projects/thor-201705011400-linux64',
    x_display="0.0",
    mode='continuous'
)

   continuous actions:
# all move actions must include the movemagnitude parameter
event = env.step(action=dict(action='moveleft', movemagnitude=0.25))

# rotate the agent to a specific rotation in global space
event = env.step(action=dict(action='rotate', rotation=75.0))

# move the horizon of the agent's camera (up/down)
event = env.step(action=dict(action='look', horizon=35.0))

# save actions for submission
env.save_actions('submission-continuous.json')
     __________________________________________________________________

evaluation

   the evaluation score is calculated as the average of the optimum number
   of actions (o[t]) over the predicted set of actions (p[t]) over all the
   tasks:
          [evaluation.gif]
   if the predicted set of actions do not place the agent in the correct
   location for a task, the score for that task will be zero. the tasks
   include finding objects in different configurations of a scene from
   different starting locations.
     __________________________________________________________________

organizers

                                   pic 01

                              roozbeh mottaghi

                           allen institute for ai

                                   pic 02

                                 eric kolve

                           allen institute for ai

                                   pic 03

                                daniel gordon

                          university of washington
     __________________________________________________________________

      the allen institute for artificial intelligence - all rights
   reserved.
   [8]ai2 cv group           [9]vuchallenge@allenai.org

   [10]free web stats

references

   visible links
   1. http://vuchallenge.org/index.html
   2. http://vuchallenge.org/index.html
   3. http://vuchallenge.org/thor.html
   4. http://vuchallenge.org/charades.html
   5. http://vuchallenge.org/tqa.html
   6. http://ai2thor.allenai.org/
   7. https://competitions.codalab.org/competitions/16929
   8. http://allenai.org/plato/
   9. mailto:vuchallenge@allenai.org?subject=vu challenge
  10. http://statcounter.com/

   hidden links:
  12. http://vuchallenge.org/thor#navpanel
