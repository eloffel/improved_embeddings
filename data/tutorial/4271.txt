   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]slav
   [7]slav
   [8]sign in[9]get started
     __________________________________________________________________

37 reasons why your neural network is not working

   [10]go to the profile of slav ivanov
   [11]slav ivanov (button) blockedunblock (button) followfollowing
   jul 25, 2017

   the network had been training for the last 12 hours. it all looked
   good: the gradients were flowing and the loss was decreasing. but then
   came the predictions: all zeroes, all background, nothing detected.
      what did i do wrong?            i asked my computer, who didn   t answer.

   where do you start checking if your model is outputting garbage (for
   example predicting the mean of all outputs, or it has really poor
   accuracy)?

   a network might not be training for a number of reasons. over the
   course of many debugging sessions, i would often find myself doing the
   same checks. i   ve compiled my experience along with the best ideas
   around in this handy list. i hope they would be of use to you, too.

table of contents

     [12]0. how to use this guide?

     [13]i. dataset issues

     [14]ii. data id172/augmentation issues

     [15]iii. implementation issues

     [16]iv. training issues

0. how to use this guide?

   a lot of things can go wrong. but some of them are more likely to be
   broken than others. i usually start with this short list as an
   emergency first response:
    1. start with a simple model that is known to work for this type of
       data (for example, vgg for images). use a standard loss if
       possible.
    2. turn off all bells and whistles, e.g. id173 and data
       augmentation.
    3. if finetuning a model, double check the preprocessing, for it
       should be the same as the original model   s training.
    4. verify that the input data is correct.
    5. start with a really small dataset (2   20 samples). overfit on it and
       gradually add more data.
    6. start gradually adding back all the pieces that were omitted:
       augmentation/id173, custom id168s, try more
       complex models.

   if the steps above don   t do it, start going down the following big list
   and verify things one by one.
     __________________________________________________________________

i. dataset issues

   [1*xfibykkmdmjqf9jfuk2ykg.png]
   source: [17]http://dilbert.com/strip/2014-05-07

1. check your input data

   check if the input data you are feeding the network makes sense. for
   example, i   ve more than once mixed the width and the height of an
   image. sometimes, i would feed all zeroes by mistake. or i would use
   the same batch over and over. so print/display a couple of batches of
   input and target output and make sure they are ok.

2. try random input

   try passing random numbers instead of actual data and see if the error
   behaves the same way. if it does, it   s a sure sign that your net is
   turning data into garbage at some point. try debugging layer by layer
   /op by op/ and see where things go wrong.

3. check the data loader

   your data might be fine but the code that passes the input to the net
   might be broken. print the input of the first layer before any
   operations and check it.

4. make sure input is connected to output

   check if a few input samples have the correct labels. also make sure
   shuffling input samples works the same way for output labels.

5. is the relationship between input and output too random?

   maybe the non-random part of the relationship between the input and
   output is too small compared to the random part (one could argue that
   stock prices are like this). i.e. the input are not sufficiently
   related to the output. there isn   t an universal way to detect this as
   it depends on the nature of the data.

6. is there too much noise in the dataset?

   this happened to me once when i scraped an image dataset off a food
   site. there were so many bad labels that the network couldn   t learn.
   check a bunch of input samples manually and see if labels seem off.

   the cutoff point is up for debate, as [18]this paper got above 50%
   accuracy on mnist using 50% corrupted labels.

7. shuffle the dataset

   if your dataset hasn   t been shuffled and has a particular order to it
   (ordered by label) this could negatively impact the learning. shuffle
   your dataset to avoid this. make sure you are shuffling input and
   labels together.

8. reduce class imbalance

   are there a 1000 class a images for every class b image? then you might
   need to balance your id168 or [19]try other class imbalance
   approaches.

9. do you have enough training examples?

   if you are training a net from scratch (i.e. not finetuning), you
   probably need lots of data. for image classification, [20]people say
   you need a 1000 images per class or more.

10. make sure your batches don   t contain a single label

   this can happen in a sorted dataset (i.e. the first 10k samples contain
   the same class). easily fixable by shuffling the dataset.

11. reduce batch size

   [21]this paper points out that having a very large batch can reduce the
   generalization ability of the model.

addition 1. use standard dataset (e.g. mnist, cifar10)

   thanks to @[22]hengcherkeng for this one:

     when testing new network architecture or writing a new piece of
     code, use the standard datasets first, instead of your own data.
     this is because there are many reference results for these datasets
     and they are proved to be    solvable   . there will be no issues of
     label noise, train/test distribution difference , too much
     difficulty in dataset, etc.
     __________________________________________________________________

ii. data id172/augmentation

   [1*uqlmfdki5d4nndn6oxa5ma.png]

12. standardize the features

   did you standardize your input to have zero mean and unit variance?

13. do you have too much data augmentation?

   augmentation has a regularizing effect. too much of this combined with
   other forms of id173 (weight l2, dropout, etc.) can cause the
   net to underfit.

14. check the preprocessing of your pretrained model

   if you are using a pretrained model, make sure you are using the same
   id172 and preprocessing as the model was when training. for
   example, should an image pixel be in the range [0, 1], [-1, 1] or [0,
   255]?

15. check the preprocessing for train/validation/test set

   cs231n points out a [23]common pitfall:

            any preprocessing statistics (e.g. the data mean) must only be
     computed on the training data, and then applied to the
     validation/test data. e.g. computing the mean and subtracting it
     from every image across the entire dataset and then splitting the
     data into train/val/test splits would be a mistake.    

   also, check for different preprocessing in each sample or batch.
     __________________________________________________________________

iii. implementation issues

   [1*evy3hnsf4nq7v7bnyoyncq.png]
   credit: [24]https://xkcd.com/1838/

16. try solving a simpler version of the problem

   this will help with finding where the issue is. for example, if the
   target output is an object class and coordinates, try limiting the
   prediction to object class only.

17. look for correct loss    at chance   

   again from the excellent [25]cs231n: initialize with small parameters,
   without id173. for example, if we have 10 classes, at chance
   means we will get the correct class 10% of the time, and the softmax
   loss is the negative log id203 of the correct class so: -ln(0.1)
   = 2.302.

   after this, try increasing the id173 strength which should
   increase the loss.

18. check your id168

   if you implemented your own id168, check it for bugs and add
   unit tests. often, my loss would be slightly incorrect and hurt the
   performance of the network in a subtle way.

19. verify loss input

   if you are using a id168 provided by your framework, make sure
   you are passing to it what it expects. for example, in pytorch i would
   mix up the nllloss and crossid178loss as the former requires a
   softmax input and the latter doesn   t.

20. adjust loss weights

   if your loss is composed of several smaller id168s, make sure
   their magnitude relative to each is correct. this might involve testing
   different combinations of loss weights.

21. monitor other metrics

   sometimes the loss is not the best predictor of whether your network is
   training properly. if you can, use other metrics like accuracy.

22. test any custom layers

   did you implement any of the layers in the network yourself? check and
   double-check to make sure they are working as intended.

23. check for    frozen    layers or variables

   check if you unintentionally disabled gradient updates for some
   layers/variables that should be learnable.

24. increase network size

   maybe the expressive power of your network is not enough to capture the
   target function. try adding more layers or more hidden units in fully
   connected layers.

25. check for hidden dimension errors

   if your input looks like (k, h, w) = (64, 64, 64) it   s easy to miss
   errors related to wrong dimensions. use weird numbers for input
   dimensions (for example, different prime numbers for each dimension)
   and check how they propagate through the network.

26. explore gradient checking

   if you implemented id119 by hand, gradient checking makes
   sure that your id26 works like it should. more info: [26]1
   [27]2 [28]3.
     __________________________________________________________________

iv. training issues

   [1*gfcjd0eymh5sguquzuvpig.png]
   credit: [29]http://carlvondrick.com/ihog/

27. solve for a really small dataset

   overfit a small subset of the data and make sure it works. for example,
   train with just 1 or 2 examples and see if your network can learn to
   differentiate these. move on to more samples per class.

28. check weights initialization

   if unsure, use [30]xavier or [31]he initialization. also, your
   initialization might be leading you to a bad local minimum, so try a
   different initialization and see if it helps.

29. change your hyperparameters

   maybe you using a particularly bad set of hyperparameters. if feasible,
   try a [32]grid search.

30. reduce id173

   too much id173 can cause the network to underfit badly. reduce
   id173 such as dropout, batch norm, weight/bias l2
   id173, etc. in the excellent    [33]practical deep learning for
   coders    course, [34]jeremy howard advises getting rid of underfitting
   first. this means you overfit the training data sufficiently, and only
   then addressing overfitting.

31. give it time

   maybe your network needs more time to train before it starts making
   meaningful predictions. if your loss is steadily decreasing, let it
   train some more.

32. switch from train to test mode

   some frameworks have layers like batch norm, dropout, and other layers
   behave differently during training and testing. switching to the
   appropriate mode might help your network to predict properly.

33. visualize the training

     * monitor the activations, weights, and updates of each layer. make
       sure their magnitudes match. for example, the magnitude of the
       updates to the parameters (weights and biases) [35]should be 1-e3.
     * consider a visualization library like [36]tensorboard and
       [37]crayon. in a pinch, you can also print
       weights/biases/activations.
     * be on the lookout for layer activations with a mean much larger
       than 0. try batch norm or elus.
     * [38]deeplearning4j points out what to expect in histograms of
       weights and biases:

        for weights, these histograms should have an approximately gaussian
     (normal) distribution, after some time. for biases, these histograms
     will generally start at 0, and will usually end up being
     approximately gaussian (one exception to this is for lstm). keep an
     eye out for parameters that are diverging to +/- infinity. keep an
     eye out for biases that become very large. this can sometimes occur
     in the output layer for classification if the distribution of
     classes is very imbalanced.   

     * check layer updates, they should have a gaussian distribution.

34. try a different optimizer

   your choice of optimizer shouldn   t prevent your network from training
   unless you have selected particularly bad hyperparameters. however, the
   proper optimizer for a task can be helpful in getting the most training
   in the shortest amount of time. the paper which describes the algorithm
   you are using should specify the optimizer. if not, i tend to use adam
   or plain sgd with momentum.

   check this [39]excellent post by sebastian ruder to learn more about
   id119 optimizers.

35. exploding / vanishing gradients

     * check layer updates, as very large values can indicate exploding
       gradients. gradient clipping may help.
     * check layer activations. from [40]deeplearning4j comes a great
       guideline:    a good standard deviation for the activations is on the
       order of 0.5 to 2.0. significantly outside of this range may
       indicate vanishing or exploding activations.   

36. increase/decrease learning rate

   a low learning rate will cause your model to converge very slowly.

   a high learning rate will quickly decrease the loss in the beginning
   but might have a hard time finding a good solution.

   play around with your current learning rate by multiplying it by 0.1 or
   10.

37. overcoming nans

   getting a nan (non-a-number) is a much bigger issue when training id56s
   (from what i hear). some approaches to fix it:
     * decrease the learning rate, especially if you are getting nans in
       the first 100 iterations.
     * nans can arise from division by zero or natural log of zero or
       negative number.
     * russell stewart has great pointers on [41]how to deal with nans.
     * try evaluating your network layer by layer and see where the nans
       appear.
     __________________________________________________________________

   did i miss anything? is anything wrong? let me know by leaving a reply
   below.

     if you liked this article, please help others find it: hold the clap
     icon for as long as you think this article is worth it. thanks a
     lot!

   [42][1*dma0dxhij6pkn1neqc-pmq.png]
   this article was republished by kdnuggets and got a silver blog award
   for being a most read and shared article.

   iframe: [43]/media/9d841a5bf2e7a49250ea63cb232937bc?postid=4020854bd607

resources:

     [44]http://cs231n.github.io/neural-networks-3/
     [45]http://russellsstewart.com/notes/0.html
     [46]https://stackoverflow.com/questions/41488279/neural-network-alwa
     ys-predicts-the-same-class
     [47]https://deeplearning4j.org/visualization
     [48]https://www.reddit.com/r/machinelearning/comments/46b8dz/what_do
     es_debugging_a_deep_net_look_like/
     [49]https://www.researchgate.net/post/why_the_prediction_or_the_outp
     ut_of_neural_network_does_not_change_during_the_test_phase
     [50]http://book.caltech.edu/bookforum/showthread.php?t=4113
     [51]https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f
     69e56ea134
     [52]https://www.quora.com/how-do-i-debug-an-artificial-neural-networ
     k-algorithm

     * [53]machine learning
     * [54]debugging
     * [55]ai
     * [56]neural networks
     * [57]data science

   (button)
   (button)
   (button) 6.9k claps
   (button) (button) (button) 24 (button) (button)

     (button) blockedunblock (button) followfollowing
   [58]go to the profile of slav ivanov

[59]slav ivanov

   entrepreneur / hacker

     (button) follow
   [60]slav

[61]slav

   machine learning, deep learning and other types of learning.

     * (button)
       (button) 6.9k
     * (button)
     *
     *

   [62]slav
   never miss a story from slav, when you sign up for medium. [63]learn
   more
   never miss a story from slav
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.slavv.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4020854bd607
   4. https://medium.com/
   5. https://medium.com/
   6. https://blog.slavv.com/?source=avatar-lo_2yhds7dseizk-a1fee82e231
   7. https://blog.slavv.com/?source=logo-lo_2yhds7dseizk---a1fee82e231
   8. https://medium.com/m/signin?redirect=https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607&source=--------------------------nav_reg&operation=register
  10. https://blog.slavv.com/@slavivanov?source=post_header_lockup
  11. https://blog.slavv.com/@slavivanov
  12. https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#b6fb
  13. https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#678a
  14. https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#86fe
  15. https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#95eb
  16. https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#74de
  17. http://dilbert.com/strip/2014-05-07
  18. https://arxiv.org/pdf/1412.6596.pdf
  19. http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/
  20. https://stats.stackexchange.com/a/226693/30773
  21. https://arxiv.org/abs/1609.04836
  22. https://medium.com/@hengcherkeng
  23. http://cs231n.github.io/neural-networks-2/#datapre
  24. https://xkcd.com/1838/
  25. http://cs231n.github.io/neural-networks-3/#sanitycheck
  26. http://ufldl.stanford.edu/tutorial/supervised/debugginggradientchecking/
  27. http://cs231n.github.io/neural-networks-3/#gradcheck
  28. https://www.coursera.org/learn/machine-learning/lecture/y3s6r/gradient-checking
  29. http://carlvondrick.com/ihog/
  30. http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
  31. http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/he_delving_deep_into_iccv_2015_paper.pdf
  32. http://scikit-learn.org/stable/modules/grid_search.html
  33. http://course.fast.ai/
  34. https://twitter.com/jeremyphoward
  35. https://cs231n.github.io/neural-networks-3/#summary
  36. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  37. https://github.com/torrvision/crayon
  38. https://deeplearning4j.org/visualization#usingui
  39. http://ruder.io/optimizing-gradient-descent/
  40. https://deeplearning4j.org/visualization#usingui
  41. http://russellsstewart.com/notes/0.html
  42. http://www.kdnuggets.com/2017/08/top-news-week-0821-0827.html
  43. https://blog.slavv.com/media/9d841a5bf2e7a49250ea63cb232937bc?postid=4020854bd607
  44. http://cs231n.github.io/neural-networks-3/
  45. http://russellsstewart.com/notes/0.html
  46. https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class
  47. https://deeplearning4j.org/visualization
  48. https://www.reddit.com/r/machinelearning/comments/46b8dz/what_does_debugging_a_deep_net_look_like/
  49. https://www.researchgate.net/post/why_the_prediction_or_the_output_of_neural_network_does_not_change_during_the_test_phase
  50. http://book.caltech.edu/bookforum/showthread.php?t=4113
  51. https://gab41.lab41.org/some-tips-for-debugging-deep-learning-3f69e56ea134
  52. https://www.quora.com/how-do-i-debug-an-artificial-neural-network-algorithm
  53. https://blog.slavv.com/tagged/machine-learning?source=post
  54. https://blog.slavv.com/tagged/debugging?source=post
  55. https://blog.slavv.com/tagged/ai?source=post
  56. https://blog.slavv.com/tagged/neural-networks?source=post
  57. https://blog.slavv.com/tagged/data-science?source=post
  58. https://blog.slavv.com/@slavivanov?source=footer_card
  59. https://blog.slavv.com/@slavivanov
  60. https://blog.slavv.com/?source=footer_card
  61. https://blog.slavv.com/?source=footer_card
  62. https://blog.slavv.com/
  63. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  65. https://medium.com/p/4020854bd607/share/twitter
  66. https://medium.com/p/4020854bd607/share/facebook
  67. https://medium.com/p/4020854bd607/share/twitter
  68. https://medium.com/p/4020854bd607/share/facebook
