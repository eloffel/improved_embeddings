   #[1]rss

     * [2]about
     * [3]software
     * [4]demos
     * [5]blog

   [supervised-learning.jpg]    kemal   anl   & agata sasiuk

supervised learning is great     it   s data collection that   s broken

   april 2, 2017    by ines montani and matthew honnibal

   short of artificial general intelligence, we'll always need some way of
   specifying what we're trying to compute. labelled examples are a great
   way to do that, but the process is often tedious. however, the
   dissatisfaction with supervised learning is misplaced. instead of
   waiting for the unsupervised messiah to arrive, we need to fix the way
   we're collecting and reusing human knowledge.

update (january 2018)

   we wrote this post while working on [6]prodigy, our new annotation tool
   for radically efficient machine teaching. prodigy features many of the
   ideas and solutions for data collection and supervised learning
   outlined in this blog post. it's a cloud-free, downloadable tool and
   comes with powerful active learning models. for more details, see the
   [7]website or try the [8]live demo.

   most ai systems today rely on supervised learning: you provide labelled
   input and output pairs, and get a program that can perform analogous
   computation for new data. supervised learning algorithms have been
   improving quickly, leading many people to anticipate a new wave of
   entirely unsupervised algorithms: algorithms so "advanced" they can
   compute whatever you want, without you specifying what that might be.
   this is like hoping for a programming language so advanced you don't
   even need to write a program.about this blog postthis post is based on
   a talk i gave at "how ai will shape the future of work" at founders in
   copenhagen in january. you can view the slides [9]here, or watch the
   [10]video on facebook.
   this topic is something we're continuing to work on at [11]explosion
   ai. id21 and better annotation tooling are both key to our
   current plans for [12]spacy and related projects.

   supervised learning is often seen as inconvenient and expensive: you
   don't only need a lot of examples     they also need to be labelled. this
   means that at some point in the process, a human has to assign those
   labels, and they should match the labels the system should predict. in
   order to achieve meaningful results, the examples and labels need to be
   as specific to your application as possible. this is why many have
   placed their bets on unsupervised learning. if we can enable the
   computer to detect hidden structures in the training examples and come
   up with its own rules to label them, we can finally train our system on
   the knowledge of the world and stop relying on a human's input, right?

     rather than spending a month figuring out an unsupervised machine
     learning problem, just label some data for a week and train a
     classifier.

     [13]@richardsocher

   the problem is that there's any number of "structures" that an
   unsupervised algorithm might recover. sometimes the unsupervised
   algorithm will happen to produce the output you want, but other times
   it won't. if it doesn't, there's not much you can do     by definition,
   you've chosen an approach you have little control over. supervised
   learning is not the problem. the problem is how we're currently
   creating these annotations     a part of the ai process that has received
   surprisingly little innovation.

[14]how machines    learn   

   to understand what supervised learning actually means, take a look at
   this example for training a simple part-of-speech tagger     a program
   that can tell you whether each word in a sentence is a noun, verb,
   adjective, etc. the function takes as input a sequence of examples.
   each example consists of a context and its correct tag, provided by a
   human annotator. the output of the function is the weights table, w,
   which can be used to predict a tag given the context. to keep the
   example simple, the context consists of only three pieces of evidence:
   the word being tagged, and its two immediate neighbours.
part-of-speech taggerdef train_tagger(examples):
    n_tags = max(tag for features, tag in examples)
    w = defaultdict(lambda: numpy.zeros(n_tags))
    for (word, prev, next), human_tag in examples:
        scores = w[word] + w[prev] + w[next]
        guess = scores.argmax()
        if guess != human_tag:
            for feat in (word, prev, next):
                w[feat][guess] -= 1
                w[feat][human_tag] += 1
    return w

   the id88 algorithmalthough this is a simplified example, it's
   more realistic than you might think. spacy uses roughly the same
   algorithm, just with a few more features, and an additional trick to
   reduce over-fitting (parameter averaging). for an in-depth look at how
   this works, see our blog post [15]a good part-of-speech tagger in about
   200 lines of python. this implementation has since been adopted by
   [16]nltk.

   when we start our training process, the weights in w are all 0, so no
   matter what context we see, we'll judge all the tags to be equally
   likely. in other words, we don't know anything yet     we start off with
   no assumptions. to learn how we should weigh the evidence in the
   context, we take an error-driven approach. we iterate over the
   examples, get the current scores for each tag given its context and
   select the best-scoring one, i.e. the one that our theory thinks is
   most likely correct. if it matches the tag a human has assigned to it    
   great. if not, we decrease the score for the "bad" tag in this context,
   and increase the score for the "good" one, i.e. the human-assigned tag.
   this is a simple as adding and subtracting a point.

   looking at this example, it's clear that the human_tag is the most
   crucial part here. if our human data is good, we'll quickly be able to
   achieve a pretty decent accuracy on the task. but if our human data is
   bad and contains mistakes and inconsistencies, we'll end up increasing
   scores on the wrong tags, resulting in a much worse model.

[17]where human knowledge in ai really comes from

   knowledge can be extracted from all kinds of freely available sources    
   for example, you can use wikipedia's [18]disambiguation data, or
   predict sentiment from emoji on [19]reddit comments. but no matter what
   application you're building, you usually need at least some data
   specific to your problem, and this data will need to be annotated by
   humans.data problems are still the biggest problemsin our [20]the state
   of ai survey, we asked ai developers about the problems that keep them
   up at night. according to the preliminary results, the leading problems
   rated 4/5 or 5/5 were data quality (66%), data quantity (54%) and the
   resulting accuracy (65%).

   the most popular place to source large volumes of annotated data is
   [21]amazon mechanical turk, the amazon cloud of human labour. you can
   use their platform to publish survey-style "human intelligence tasks"
   (hit), which will be completed by workers from all over the world.
   while this sounds great in theory, it's often disastrous in practice.
   the workers make around $5 an hour on average, with no connection to
   the task, and interfaces reminiscent of early-2000s-style surveys.
   incentives are also completely misaligned, so you have to worry about
   being cheated by the workers     who have to worry about being cheated by
   you. it's quite ironic that our oh so progressive and world-changing ai
   gets its knowledge from... this.
   a human intelligence task on amazon mechanical turk. (images:
   [22]mturk.com, [23]depressing.org)

   so no wonder your data is bad. don't expect great data if you're boring
   the shit out of underpaid people. the thing is, none of this is news.
   our so-called start-up culture is based on the realisation that in
   order to achieve the best results, we need an engaged team that's
   passionate about their work, a motivating work environment, high
   incentives and fair pay. we know all of this. yet, when it comes to the
   absolute core of the application, the training data, all of this
   knowledge seems to go straight out of the window.

   the problems with mechanical turk are not a secret and there have been
   many [24]attempts at designing around them. but instead of "designing
   around" underpaid people doing boring work with bad incentives, data
   collection should receive the same treatment as all other human-facing
   interactions. imagine all the knowledge you'd be able to collect if you
   spent as much time on your data collection process as you did on, say,
   the user onboarding flow of your app.

[25]solution #1: ux-driven data collection with active learning

   when humans interact with machines, their experience is what decides
   about the success of the interaction. i've already talked about some of
   this in [26]my post on the importance of front-end development for ai:

     if your tools are bad, the task will be boring and frustrating, and
     as a result, the workers' input quality will be low. believing that
     you can make your annotation tools bad because labour should be
     cheap is like believing that your offer is so valuable that your
     users shouldn't care if your website is confusing and hard to use.

     [27]how front-end development can improve artificial intelligence

   the more time, clicks and effort it requires to complete a task, the
   less efficient the result. even the most subtle changes to a user
   interface can have a noticable impact, for instance on converting
   [28]visitors to users or [29]users to paying customers. there's a
   reason why tinder doesn't ask you to type in a comma-separated list of
   the full names of everyone you'd like to talk to. their card-based ui
   reduces a complex interaction to one intuitive motion: swipe left and
   [30]swipe right. it's so effective because it reduces friction between
   the user and the interface.
   a user interface that requires 3 different actions (highlight, select,
   click) vs. one binary decision using the model's predictions.

   human time and attention is precious. instead of presenting the
   annotators with a span of text that contains an entity, asking them to
   highlight it, select one of many labels from a dropdown and confirm,
   you can break the whole interaction down into a simple binary decision.
   you'll have to ask more questions to get the same information, but each
   question will be simple and focused. you'll collect more user actions,
   giving you lots of smaller pieces to learn from, and a much tighter
   feedback loop between the human and the model. you don't need to ask
   the questions using a fixed queue. as the human clicks through the
   examples, you can prioritise the questions, using the current state of
   the model. this puts the computer in charge of what it's good at    
   memory and consistency.considerationseach individual binary annotation
   is less specific than a full labelled example. this is especially true
   of the negative, "rejected" examples, which could be wrong in a number
   of different ways. however, perfect information is not required for
   training. all you need is a steady sequence of error gradients.
   an annotation workflow using active learning vs. conventional "batch
   learning".

   the above graphic shows an annotation project using this active
   learning approach. instead of asking the human to annotate a fixed
   batch of tasks, the model selects a task and presents it to the human
   for annotation. the annotated single task can then immediately adjust
   the model's internal weights and influence its choice of what to ask
   next. a simple policy is to ask what the model is least sure about,
   although a range of [31]other strategies have been explored.

[32]solution #2: id21 with general-purpose models

   regardless of the application you're building, it will always require
   some general knowledge about language and the world, from basic grammar
   to a variety of phrases, expressions and entities. think of it like
   hiring a new junior employee     you know you'll have to spend time
   teaching them about the specifics of the work and you don't expect them
   to know everything right away. but you do want them to speak a language
   and know how to use a computer. you're not going to raise them from
   birth.

   pre-trained models let you jump-start your application with general
   information, which you can then fine-tune and improve to fit your
   custom needs. with deep learning, you can even chain together multiple
   models, and adjust the whole pipeline based on the eventual,
   task-specific error. normally, the reuseable components will be the
   lowest, least abstract layers of the network. for instance, consider
   the following sketch of an intent recognition model:
   about intent recognitionintent recognition is a text categorisation
   problem that arises in many natural language dialog architectures. with
   the current interest in id70, it's become especially relevant. an
   intent recognition model should route a user's utterance to the correct
   subsystem of the application, which extracts arguments, so that the
   command can be executed.

   this model takes a user input and returns an intent, i.e. a prediction
   of what the user might want. the example input, "whats the best way to
   catalinas", could mean a lot of things. in a social app, the user might
   want to get directions to their friend catalina's house. a restaurant
   app might want to show the local fast food restaurant, catalinas. or
   maybe you're building a latin american travel app and your user wants
   to book a trip to the beach town las catalinas in puerto rico.

   to figure this out, the model computes a series of internal
   representations, which encode generalisable information, about the
   language and the world. for example, the phrase meanings (likely
   computed using a id98 or id56 layer) will let the model determine that
   "best way" is an expression that means something much more specific
   than the sum of its parts. the entity labels can take care of assigning
   the correct label to "catalinas" to figure out whether it's a person, a
   place, a geopolitical entity or something entirely different.

   but what if the intent recogniser assigns the label person to
   "catalinas", when the user was actually interested in a local fast food
   restaurant? if you own the weights of your model, you're not only able
   to fix the mistaken output, but also correct all the wrong assumptions
   that led to it. id21 is especially advantageous if you
   have full write access to your model. if you don't     for instance, if
   you're consuming a cloud api     you have to settle for less principled
   and much less effective approaches to fine-tuning.

[33]conclusion

   if you're using a saas provider and only get access via an api, your
   model will essentially stay a black box. you're not easily able to
   backpropagate to correct the model's internal representations. all you
   can do is add workarounds for your read-only model or retrain it with
   more examples. this challenges the current trends towards thin clients
   and centralised cloud computing, and is one of many reasons to bet
   against machine learning as a service.

     machine learning as a service is an idea we   ve been seeing for
     nearly 10 years and it   s been failing the whole time. the bottom
     line on why it doesn   t work: the people that know what they   re doing
     just use open source, and the people that don   t will not get
     anything to work, ever, even with apis.

     [34]bradford cross, five ai startup predictions for 2017

   supervised learning is only just getting good, and with transfer
   learning and fully differentiable models it's constantly getting
   better. it doesn't make sense to give up on controlling the model's
   output just because our annotation tooling and processes suck. there's
   definitely a problem here, but it's not the concept of supervised
   learning. at explosion ai, we're making this one of our priorities, and
   we're looking forward to sharing the results.

update (january 2018)

   we wrote this post while working on [35]prodigy, our new annotation
   tool for radically efficient machine teaching. prodigy features many of
   the ideas and solutions for data collection and supervised learning
   outlined in this blog post. it's a cloud-free, downloadable tool and
   comes with powerful active learning models. for more details, see the
   [36]website or try the [37]live demo.

   ines montani
   about the author

ines montani

   ines is a developer specialising in applications for ai technology.
   she's a core developer of the spacy natural language processing library
   and the prodigy annotation tool. before founding explosion ai, she was
   a freelance front-end developer and strategist, using her four years
   executive experience in ad sales and digital marketing.

read more

[38]introducing spacy v2.1

[39]explosion ai in 2017: our year in review

[40]introducing custom pipelines and extensions for spacy v2.0

[41]pseudo-rehearsal: a simple solution to catastrophic forgetting for nlp

[42]prodigy: a new tool for radically efficient machine teaching

[43]supervised similarity: learning symmetric relations from duplicate
question data

    about us

   explosion ai is a digital studio specialising in artificial
   intelligence and natural language processing. we   re the makers of
   spacy, the leading open-source nlp library.

    navigation

     * [44]home
     * [45]about
     * [46]software
     * [47]demos
     * [48]blog
     * [49]legal / imprint

    our software

     * [50]spacy
       industrial-strength nlp
     * [51]prodigy
       radically efficient machine teaching

   [52]see more    

references

   visible links
   1. https://explosion.ai/feed.xml
   2. https://explosion.ai/about
   3. https://explosion.ai/software
   4. https://explosion.ai/demos
   5. https://explosion.ai/blog
   6. https://prodi.gy/
   7. https://prodi.gy/
   8. https://prodi.gy/demo
   9. https://www.slideshare.net/inesmontani/teaching-ai-about-human-knowledge
  10. https://www.facebook.com/foundersas/videos/712970348885532/
  11. https://explosion.ai/about
  12. https://spacy.io/
  13. https://twitter.com/richardsocher/status/840333380130553856
  15. https://explosion.ai/blog/part-of-speech-pos-tagger-in-python
  16. http://www.nltk.org/
  18. https://en.wikipedia.org/wiki/category:disambiguation_pages
  19. https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/
  20. https://thestateofai.com/
  21. https://www.mturk.com/
  22. https://www.mturk.com/
  23. http://www.depressing.org/2015/04/finding-your-way-through-first-day-on_20.html
  24. https://www.semanticscholar.org/paper/taking-a-hit-designing-around-rejection-mistrust-mcinnis-cosley/e80bd1c31ae754ad2b83a5889a49b84bebb17822
  26. https://explosion.ai/blog/how-front-end-can-improve-ai
  27. https://explosion.ai/blog/how-front-end-can-improve-ai
  28. http://www.appcues.com/blog/5-notable-changes-slack-made-to-its-user-onboarding-experience/
  29. http://unbounce.com/conversion-rate-optimization/small-changes-increasing-conversion-rates/
  30. https://www.wired.com/2016/09/history-of-tinder-right-swipe/
  31. https://www.semanticscholar.org/paper/beyond-disagreement-based-agnostic-active-learning-zhang-chaudhuri/39ca623a1cab9001cb0858302a26bd1d228e3f77
  34. http://www.bradfordcross.com/blog/2017/3/3/five-ai-startup-predictions-for-2017
  35. https://prodi.gy/
  36. https://prodi.gy/
  37. https://prodi.gy/demo
  38. https://explosion.ai/blog/spacy-v2-1
  39. https://explosion.ai/blog/year-in-review-2017
  40. https://explosion.ai/blog/spacy-v2-pipelines-extensions
  41. https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
  42. https://explosion.ai/blog/prodigy-annotation-tool-active-learning
  43. https://explosion.ai/blog/supervised-similarity-siamese-id98
  44. https://explosion.ai/
  45. https://explosion.ai/about
  46. https://explosion.ai/software
  47. https://explosion.ai/demos
  48. https://explosion.ai/blog
  49. https://explosion.ai/legal
  50. https://spacy.io/
  51. https://prodi.gy/
  52. https://explosion.ai/software

   hidden links:
  54. https://explosion.ai/
  55. mailto:ines@explosion.ai
  56. https://twitter.com/_inesmontani
  57. https://github.com/ines
  58. https://ines.io/
  59. https://explosion.ai/blog/spacy-v2-1
  60. https://explosion.ai/blog/year-in-review-2017
  61. https://explosion.ai/blog/spacy-v2-pipelines-extensions
  62. https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting
  63. https://explosion.ai/blog/prodigy-annotation-tool-active-learning
  64. https://explosion.ai/blog/supervised-similarity-siamese-id98
  65. mailto:contact@explosion.ai
  66. https://twitter.com/explosion_ai
  67. https://github.com/explosion
  68. https://youtube.com/c/explosionai
  69. https://explosion.ai/feed
