graphical model research 

in audio, speech, and 
language processing

jeff a. bilmes

university of washington
department of ee, ssli-lab

acknowledgements

    thanks to the following people:

    chris bartels     university of washington
    ozgur cetin     university of washington
    karim filali     university of washington
    katrin kirchhoff     university of washington
    karen livescu     mit
    brian lucena     university of washington
    thomas richardson     university of washington
    geoff zweig     ibm

    slides and list of references for this tutorial available: 

http://ssli.ee.washington.edu/~bilmes

jeff  a. bilmes

gms in audio, speech, and language

1

outline
outline
i. id114 review
ii. id103 overview
iii. goals for gms in speech/language

a. explicit control
b. latent modeling (audio, speech, language)
c. observation modeling
d. structure learning

iv. toolkits and id136

jeff  a. bilmes

gms in audio, speech, and language

id114 (gms)

    id114 give us:

salad

    structure
    algorithms
    language
    approximations
    data-bases

jeff  a. bilmes

gms in audio, speech, and language

2

id114 (gms)

gms give us:
i.

structure: a method to explore the 
structure of    natural    phenomena (causal 
vs. correlated relations, properties of 
natural signals and scenes)

ii. algorithms: a set of algorithms that 

provide    efficient    probabilistic 
id136 and statistical decision making

iii. language: a mathematically formal, 
abstract, visual language with which to 
efficiently discuss and intuit families of 
probabilistic models and their properties.

jeff  a. bilmes

gms in audio, speech, and language

id114 (gms)

gms give us (cont):
iii. approximation: methods to explore 

systems of approximation and their 
implications.
i.
ii. task dependent structural approximation
iv. data-base: provide a probabilistic    data-

inferential approximation

base    and corresponding    search 
algorithms    for making queries about 
properties in such model families.

jeff  a. bilmes

gms in audio, speech, and language

3

conditional independence

    notation:  x || y | z    
zypzxp
(
|

zyxp
|
,(

=

(

)

)

|

)

   

zyx
,{
},

    many ci properties (from lauritzen 96)

- x || y | z  => y ||  x | z
- y ||  x | z and u=h(x) => y || u|z
- y ||  x | z and u=h(x) => x || y|{z,u}
- xa || yb | z => xa    || yb    | z
where a, b sets of integers, a        a, b       b

xa = {xa1, xa2,..., xan}

jeff  a. bilmes

gms in audio, speech, and language

directed gm (dgms) 
(id110s)

    when is xa || xb | xc?
    only when c d-separates a from b, i.e. if:

for all paths from a to b, there is a v on the path 
s.t. either one of the following holds:

1. either    v    or    v    and v   c
2.    v    and neither v nor any descendants are in c

    equivalent to    directed local markov property   

(ci of non-descendants given parents), plus 
others (again see lauritzen    96)

jeff  a. bilmes

gms in audio, speech, and language

4

conditional independence

    factorization (i.e., simplification)
i
(
th

xxp
(

xp
(

=

)

)

f

|

i

i
1:1
   

t
:1

   =

i

   

i

subset 
 

 )x of

t:1

    better (more parsimonious) representations of the 

    ex:

underlying problem since factors are local.
xxxxxxp
(
=)
6
xxpxxpxp
(
|
1
xxpxxxpxxp
(
)

,
5
(

,
)

,
(

x4

3
,

)

)

(

(

,

,

|

|

|

|

4

3

2

1

1

1

6

2

5

2
)
2

5

4

)

3

x2

x1

x3

x5

x6

jeff  a. bilmes

gms in audio, speech, and language

switching parents: value-specific conditional 

independence

m1
m1

f1
f1

s

c

m2

f2

p c m f f f
(

1, 1, 2, 2)

|

=

   

i

p c m f s
(
,

,

|

i

i

s   r1

s   r2
i p s r
)
(
i

   

)

=

jeff  a. bilmes

gms in audio, speech, and language

5

undirected gms

    when is xa || xb | xc?
    only when c separates a from b. i.e., if:

for all paths from a to b, there is a v on the path 
s.t. v   c

    simpler semantics than id110s.
    equivalent to    global markov property   , plus 

others (again see lauritzen    96)

jeff  a. bilmes

gms in audio, speech, and language

directed and undirected models 

represent different families

decomposable models

dgm

ugm

    the classic examples:

w

y

x

z

w

z

y

jeff  a. bilmes

gms in audio, speech, and language

6

why id114 for 

speech and language processing
    expressive but concise way to describe properties of 

families of distributions

    rapid movement from novel idea to implementation 

(with the right toolkit)

    gms encompass many existing techniques used in 
speech and language processing but gm space is 
only barely covered

    holds promise to replace the ubiquitous id48
    dynamic id110s and dynamic graphical 
models can represent important structure in    natural    
time signals such as speech/language.

jeff  a. bilmes

gms in audio, speech, and language

time signals
time signals

    where is statistical structure?

jeff  a. bilmes

gms in audio, speech, and language

7

spectrograms
spectrograms

    where is statistical structure?

jeff  a. bilmes

gms in audio, speech, and language

structure of a domain

    graphs represent properties of (auditory) objects 

from natural scenes. 

    goal: find minimal structure representing 

appropriate properties for a given task (e.g., object 
classification or asr)

jeff  a. bilmes

gms in audio, speech, and language

8

id103

?

research & development needed to move to the right.

jeff  a. bilmes

gms in audio, speech, and language

automatic speech 

recognition: broad view

front end

1:tx

back end

spoken word 

output hypotheses:
   how to recognize 

speech   

   how to wreck a 

nice beach   

deterministic 

signal processing:
ex: mel-frequency 
cepstral coefficients 

(mfccs)

length t sequence 
of feature vectors 
(so nxt matrix)
t is often (but not 
always) known

transform feature 
vectors into a string 

of words.
p w x
(

|

1:

k

t
1:

)

jeff  a. bilmes

gms in audio, speech, and language

9

ideal case, use bayes decision 

rule
) argmax pr(
=

k w
k
1:

,

(

*

k w

,

*
1:

k

w k x

,

|

1:

k

)

t
1:

=

argmax pr(

k w
k
1:

,

x w k
t
1:

,

k

1:

|

)pr(

w
1:

k

)

   bayes decision theory (see duda & hart 73)

jeff  a. bilmes

gms in audio, speech, and language

generative vs. discriminative 

   

ideal case: discriminative model

models

p w x
(

|

1:

k

)

t
1:

    too many classes for a discriminative model

    (not to mention we didn   t consider all other k   s)

    100k words, k = 10     (100k)^10 classes
)tp x
(

    generative model can help:
    use the    natural    hierarchy in speech/language:

1:

    sentences are composed of words (w)
    words (w) are composed of phones (q)
    phones (q) are composed of markov chain states (s)
    states (s) are composed of acoustic feature vector sequences (x)
    acoustic feature vector sequences (x) are composed of noisy (e.g., 

channel distorted) versions thereof (y)

jeff  a. bilmes

gms in audio, speech, and language

10

speech/language hierarchy

(time collapsed)

w

q

s

x

y

word sequences

phones     typically context-dependent, could be 
syllables, etc.

states     markov chain states, 

   clean    speech     the ideal speech signal without 
channel effects (additive & convolutional noise)

speech     as received by a microphone or your ears, 
typically contains speech + unwanted material.

jeff  a. bilmes

gms in audio, speech, and language

other possible q hidden variables
    syllables     bigger than phones, smaller than words, 

perceptually meaningful unit

    subphones (i.e.,    or 1/3 of a phone)
    context-dependent phones

    tri-phone (a phone in the context of its immediately 

preceding and following phone)

    better for temporal context and coarticulation

    most common: 3-state tri-phones

    tri-phones that force the use of three states (s values).
    others are used, ibm uses 5 contextual phones on the left 

and right.

    goal: not too many (curse of dimensionality, 

estimation problems) and not too few (accuracy).

jeff  a. bilmes

gms in audio, speech, and language

11

tri-phones example

    p(x|qt) where qt is a tri-phone
    x-y+z notation: phone y with 

    left context of x
    right context of z

    example transcription of     beat it    : sil b iy t ih t sil

    sil
    sil-b+iy
    b-iy-t        (or b-iy+dx for americans)
    iy-t+ih      (or iy-dx+ih)
    t-ih+t        (or dx-ih+t)
    ih-t+sil
    sil

    to further increase states: word internal vs. cross-

word tri-phones

jeff  a. bilmes

gms in audio, speech, and language

a generative model of speech
    key goal, find a distribution over the 
variable-length set of feature vectors. 

p x
(
t
1:

|

t

p x w k
(

,

,

1:

k

t
1:

k words

)

m    phones   

,

k

)

=

w
k
1:

   
       
           

w k q m
t
1:

w k q m s
t
t
1:
1:

m
1:

m
1:

,

,

,

,

=

=

p x
(
t
1:

,

q m w k
1:

m

,

,

,

k

1:

)

p x
(
t
1:

,

s
t
1:

,

q m w k
1:

m

,

,

,

k

1:

)

t states

jeff  a. bilmes

gms in audio, speech, and language

12

generative models of speech

p x
(
t
1:
p x
(
t
1
:

,
|

s
t
1:
s
t
1
:

q m w k
,
,
,
,
m
1:
q m
s
p
(
)
t
1:
1
:

k
,

m

1:

|

)
=
p q m w k p
(
)

)

,

,

|

1
:

m

1:

k

(

w
1:

,k

k

)

acoustic
models

phone 
models

word pronunciation 

models

language model

phone id48s

word id48s

jeff  a. bilmes

sentence id48s

gms in audio, speech, and language

solution implemented using 

search via id145

w
*

=

max

s q w
,

,

p x q s w
( ,
)

,

,

    need optimized search algorithms 
    viterbi decoding, time synchronous
    stack decoding, id67, time asynchronous
    both will heavily prune the search space, thus 
achieving a form of     approximate    id136

jeff  a. bilmes

gms in audio, speech, and language

13

in other words, asr has used 
hierarchical id48s for years,    

w1

w2

w3

w4

q1

s1

x1

q2

s2

x2

q3

s3

x3

q4

s4

x4

jeff  a. bilmes

gms in audio, speech, and language

id48
        but, in existing speech systems, all of this 

complexity gets implicitly wrapped up (flattened) 
into an id48
q1

q2

q3

q4

x1

x2

x3

x4

    number of flattened states is strongly dependent 

on language model
    bi-gram language model:
    tri-gram language model:

p w w    
(
)
i
1,
p w w w   
(
i
i
1,

|
|

i

i

)

   

2

jeff  a. bilmes

gms in audio, speech, and language

14

bi-gram language models

p w w
(
1

|

1

)

p w w
(
2

|

1

p w w
(
1

|

2

id48 1

)

id48, 2

)

p w w
(
1

m

|

)

p w w
(
2

m

|

)

p w w
1(

|

)m

p w w
2(

|

)m

p w
1(

)

p w
2(

)

p w
(

)m

id48, m

jeff  a. bilmes

p w w
(
m

|

)

m
gms in audio, speech, and language

id48 lattice with bi-gram lms

word3

word2

word1

1

p s
(
t

|

p w w
3

(

|

2

)

)

p w w
(
1

|

3

)

t

s w   
t

,

1,

jeff  a. bilmes

gms in audio, speech, and language

15

tri-gram language models

p w w w
(
1

,

|

1

1

)

w1   w1

id48 1

p w
1(

)

p w
2(

)

id48, 2

jeff  a. bilmes

1

|

)

p w w
(
1
p w w w
(
2

,

|

1

1

id48 1

)

p w w w
(
1

,

|

2

1

)

p w w
(
1

|

2

)

id48, 2

w1   w2

1

2

|

,

)

p w w w
(
2
p w w w
(
1

,

|

1

2

)

p w w w
(
1

,

|

2

2

)

p w w
(
2

|

1

)

id48 1

w2   w1
)

p w w w
(
2

,

|

1

2

p w w
(
2

|

2

)

id48, 2

w2   w2

|

p w w w
(
2
gms in audio, speech, and language

)

,

2

2

id48 lattice with tri-grams lms

[word1],word1,word3,word2,word1,word2,word3

prev -> curr

word3->word3

word3->word2

word3->word1

word2->word3

word2->word2

word2->word1

word1->word3

word1->word2

word1->word1

1

p w w w
(
3

,

|

1

2

)

p w w w
(
1

,

|

3

2

)

)

p w w w
(
1

,

|

2

3

p w w w
(
2

,

|

2

1

)

p w w w
(
1

,

|

3

1

)

t

jeff  a. bilmes

gms in audio, speech, and language

16

challenges in id103

    > 60k words, exhaustive examination of all 
words is infeasible  since                 states. 

|
    even id48 decoding is a challenge

w q s

| |
2

||

|

    clearly, large grid approach is infeasible
    pruning with a beam: try to discard unlikely 

partial hypotheses as soon as possible (without 
increasing error)

    explore word sequences in parallel (multiple 

partial hypotheses are considered at same time)

jeff  a. bilmes

gms in audio, speech, and language

tree-based lexicons

    the speech/language hierarchy can help guide 

(and reduce computation in) the decoder

ey

say

s

t

p

aw

eh

ee

eh

k

l

k

ch

l

talk

tell

speak

speech

spell

jeff  a. bilmes

gms in audio, speech, and language

17

    generative model + speech/language hierarchy allows 

the savior: parameter tying
for massive amounts of parameter tying or sharing.
    same words in difference sentences or different parts of 

same sentence are the same

    same phones (subwords) in different words or in different 

parts of same word are the same

    certain states in different phones are merged

    e.g., p(x|s=i) = p(x|s=j) for the right i and j.

    certain observation parameters (e.g., means) are shared.

    various ways to accomplish this: 
    backing off (like in language model)

    [a-b+c] model backs off to [b+c] or to [a-b] etc.

    smoothing, interpolation, and mixing
    id91 (widely used)

decision tree clustered tri-phones
both bottom up and top down id91 procedures.

jeff  a. bilmes

gms in audio, speech, and language

four main goals for 

gms in speech/language

1. explicit control: derive graph structures that 
themselves explicitly represent control constructs

   

e.g., parameter tying/sharing, state sequencing, smoothing, 
mixing, backing off, etc.

2. latent modeling: use graphs to represent latent 

information in speech/language, not normally 
represented.

3. observation modeling: represent structure over 

observations.

4. structure learning: derive structure

automatically, ideally to improve error rate while 
simultaneously minimizing computational cost.

jeff  a. bilmes

gms in audio, speech, and language

18

graph control structure approaches
    the    implicit    graph structure approach

    implementation of dependencies determine sequencing 

through time-series model

    everything is flattened, all edge implementations are random

    the    explicit    graph structure approach

    graph structure itself represents control sequence mechanism 

and parameter tying in a statistical model.

jeff  a. bilmes

gms in audio, speech, and language

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

triangle structures:

a basic explicit approach for parameter tying

end of word observation

counter

transition

phone

1

y

2

2

1

0

1

aa

aa

3

m

4

5

1

1

1

aa

hh

6

aa

  

1

observation

    structure for the word    yamaha   , note that /aa/ 

occurs in multiple places preceding different phones.
gms in audio, speech, and language

jeff  a. bilmes

19

key points

    graph explicitly represents parameter sharing
    same phone at different parts of the word are the same: 
phone /aa/ in positions 2, 4, and 6 of the word    yamaha   

    phone-dependent transition indicator variables yield 
geometric phone duration distributions for each phone
    counter variable ensures /aa/   s at different positions 

move only to correct next phone 

    some edge implementations are deterministic (green) 

and others are random (red)

    end of word observation, gives zero id203 to  

variable assignments corresponding to incomplete words.

jeff  a. bilmes

gms in audio, speech, and language

explicit bi-gram training graph structure
skip silence

...

word
counter

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

...

jeff  a. bilmes

gms in audio, speech, and language

20

explicit bi-gram training graph structure
skip silence

...

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

...

jeff  a. bilmes

gms in audio, speech, and language

bi-gram training w. pronunciation variant
skip silence

end-of-utterance 
observation=1

...

word
counter

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

word counter

word 

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

pronunciation

word transition

state counter

state transition

state

observation

...

jeff  a. bilmes

gms in audio, speech, and language

21

explicit bi-gram decoder

wordtransition is a switching parent of word. it 
switches the implementation of word(t) to either be a 
copy of word(t-1) or to invoke the bi-gram.

nodes & 
nodes & 
edges:
edges:
    red    
    red    
random
random
    green    
    green    
deterministic
deterministic
   dashed line 
   dashed line 

   
   

switching 
switching 
parent
parent
jeff  a. bilmes

...

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

gms in audio, speech, and language

explicit tri-gram decoder

previous word 

nodes & 
nodes & 
edges:
edges:
    red    
    red    
random
random
    green    
    green    
deterministic
deterministic
   dashed line 
   dashed line 

   
   

switching 
switching 
parent
parent
jeff  a. bilmes

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

gms in audio, speech, and language

...

22

from explicit control to 

latent modeling

1. so far, each graph has been essentially still an 

id48 (in disguise).

2. most edges were deterministic.
3.

in latent modeling, we move more towards 
representing and learning additional 
information in (factored) hidden space.

4. factored representations place constraints on 

what would be flattened id48 transition 
matrix parameters thereby potentially 
improving estimation quality.

jeff  a. bilmes

gms in audio, speech, and language

graphs for speech/audio transformations

(feature level)

x

y

   clean    speech

speech

y1

x1

y2

x2

y3

y4

    the data y1:4 is explained by the two (marginally) 

independent latent causes x1:4

    many techniques used:

    principle components analysis
    factor analysis
    independent component analysis
    id156 (different graph than above)

jeff  a. bilmes

gms in audio, speech, and language

23

speech/audio de-noising models

(signal level)

    latent variables x represent clean speech/sound at the 

signal/sample level for observed speech/sound y

    various forms of noise:

    convolutional: moving average, auto-regressive
    independent additive u
    structured additive v

x[n]

y[n]

u[n]

v[n]

jeff  a. bilmes

gms in audio, speech, and language

latent modeling

qt-2=qt-2

qt-1=qt-1

the hidden variable cloud

qt=qt

qt+1=qt+1

observations

1:tx
    key questions: what are the most important 

   causes    or latent explanations of the temporal 
evolution of the statistics of the vector observation 
sequence?

    how best can we factor these causes to improve 
parameter estimation, reduce computation, etc.?

jeff  a. bilmes

gms in audio, speech, and language

24

latent x modeling

qt-2=qt-2

qt-1=qt-1

the hidden variable cloud

qt=qt

qt+1=qt+1

observations

other hidden 

variables
    where x = gender, speaker cluster, speaking rate, 

noise condition, accent, dialect, pitch, formant 
frequencies, vocal tract length, etc.

    we elaborate upon latent articulatory modeling   

jeff  a. bilmes

gms in audio, speech, and language

ex: latent articulatory modeling

pictures from linguistics 001, university of pennsylvania

jeff  a. bilmes

gms in audio, speech, and language

25

latent articulatory modeling.
    definition:

    articulatory features specify complete state of vocal 

tract (directly or implicitly) at a given point in time
    features can be binary/multivalued,                      
lips

discrete/continuous, partial/complete

    they are one of the    causes    of speech, yet they are 

hidden when given only acoustic signal

tongue

    motivation:

velum
glottis

    speech described by asynchronous articulator motion rather than by phones 

with synchronous start and end times

    articulatory features concisely represent coarticulatory effects such as 

nasalization and inserted stop closures (   warmth           warmpth   )

    articulatory features can help recover information; e.g., a vowel is more 

likely to be nasalized if following nasal is deleted

    pronunciation modeling:  phones are more likely to be modified by 

articulatory change rather than replaced with entirely different phones

jeff  a. bilmes

gms in audio, speech, and language

phone-based articulatory 

graphical model

frame  i

phone

frame  i+1

phone

a1

a2

. . .

an

a1

a2

. . .

an

obs

obs

jeff  a. bilmes

gms in audio, speech, and language

26

phone-free articulatory graph

(by karen livescu)

jeff  a. bilmes

gms in audio, speech, and language

standard id38
    example: standard tri-gram
p w h
(
,
t

w w
t
t

p w
(
t

w
t

=

)

)

1
   

,

|

|

   

   

3

2

t

4tw    

3tw    

2tw    

tw1tw    

jeff  a. bilmes

gms in audio, speech, and language

27

interpolated uni-,bi-,tri-grams
p w w
(
)
t
p w w w
(
,
t

(
  
t
(
  
t

p w
(
t

2)
3)

+
+

p
p

(
  
t

=
=

1)

p

|
|

=

=

1
   

)

)

1
   

t

t

t

p w h
(
t

|

t

)

   

2

jeff  a. bilmes

gms in audio, speech, and language

conditional mixture tri-gram
p w h
(
t

p
1|
(
)
=
  
=
t
p
2 |
(
  
=
+
t
p
3|
(
  
+
=
t

w w p w
)
t
t
w w p w w
|
t
t
t
w w
|
t
t

(
(
p w w w
(
t

)
2
)
)

1
   
,
,

)
,

1
   

1
   

1
   

1
   

)

   

2

,

|

   

   

   

2

2

t

t

t

t

t

jeff  a. bilmes

gms in audio, speech, and language

28

skip bi-gram

    often there is silence between words

       fool me once <sil> shame on <sil> shame on you   

    silence might not be good predictor of next word
    but silence lexemes should be represented since 

acoustics quite different during silence.

    goal: allow silence between words, but retain 

true word predictability skipping silence regions.

    switching parents can facilitate such a model.

jeff  a. bilmes

gms in audio, speech, and language

p r w r
(
,
t

|

t

t

1
   

p w s r
(
,
t

|

t

t

)

1
   

skip bi-gram
)

t

r
   =
t
1

  
      =    
r
t
  
      
r w
=
t
t
   =
      =    
tw sil
p
      
bigram
1) pr(
=

t

=
   

if w sil
if w sil
if
if

)

w r
(
|
t
t
1
   
silence
)

tp s
(

=

s
t
s
t

=
=

1
0

jeff  a. bilmes

gms in audio, speech, and language

29

skip bi-gram with conditional 

mixtures

p
bigram

(

w r
|
t
t

1
   

)

=

p

(
  
t

=

1)

p w
(
t

)

+

p

(
  
t

=

2)

p w w
(
t

|

t

)

1
   

jeff  a. bilmes

gms in audio, speech, and language

p w w w
(
t

,

t

t

2

   

)

1
   

two switching indicator variables
w w p w w w
|
)
|
t
t
tri
w w p w w
(
t
t
2
   
w p w w
|
)
(
t
bi
t
w p w
(
t
t

p
(
  
t
p
(
  
t
p
(
  
t
p
(
  
t

1|
0 |
1|
0 |

p w w
(
t

=
=
=
=

=
+
=
+

,
,
)
)

1
   
)

)
)

t
)

t
|

1
   

1
   

1
   

1
   

1
   

1
   

(

)

1
   

,

|

   

   

2

2

t

t

t

t

t

jeff  a. bilmes

gms in audio, speech, and language

30

skip trigram

    similar to skip bi-gram, but skips over two 

previous <sil> tokens.

    p(city|<sil>,york,<sil>,new) = 

p(city|york,new)

jeff  a. bilmes

gms in audio, speech, and language

putting it together: 

mixture and skip tri-gram.

jeff  a. bilmes

gms in audio, speech, and language

31

class language model

    when number of words large (>60k), can be 

better to represent clusters/classes of words
    clusters can be grammatical or data-driven
    just an id48 (perhaps higher-order)

jeff  a. bilmes

gms in audio, speech, and language

explicit smoothing

    disjoint partition of vocabulary based on training-data 

counts:    = {unk}            
  = singletons,   =    many-tons   , unk=unknown

   
    ml distribution gives zero id203 to unk.
    goal: directed gm that represents:
w unk
=
w
   

p w
)
(

   
   =
   
   
   

p
(
)
0.5
   
ml
p w
(
0.5
)
ml
p w
(
)
ml

if
if
otherwise

    word variable is like a switching parent of itself (but of 

course can   t be)

jeff  a. bilmes

gms in audio, speech, and language

32

explicit smoothing

    introduce two hidden variables k and b and one child 

observed variables v=1.

    hidden variables are switching parents
    k = indicator of singleton vs.    many-ton   
    b = indicator of singleton vs. unknown word

    observed child v induces    reverse causal    phenomena 

via its dependency implementation
    i.e., child says    if you want me to give you non-zero 

id203, you parents had better do x   

k

b

w

v

jeff  a. bilmes

gms in audio, speech, and language

k

w

v

b

p b
(
p k
(

explicit smoothing
0) 0.5
=
0)
=
=
0
b
1 and 
b
1 and 

=
p k
(
k
if
k
if
k
if

p b
1)
(
=
=
1) 1
=    
=
p w
(
)
   
m
   
p w
(
)
   
s
   
   =
   
w unk
t
   
 

p w k b
(
, )

=
=
=

p

w

=

=

k

,

|

( )

=
=

1
0

p v
(

=

1|

w k
, )

=

{1

(

   
   =    
      

p w
(
)
ml
p
)
(
 
ml
0

if

w

   

else

p w
)
m

(

jeff  a. bilmes

p w
)
s

(

   
   =    
      

1) or (
or (

,

1) 
=
}
0)

w unk k
,
=
k
w
   
=
 
p w
)
(
ml
p
( )
 
ml
0
gms in audio, speech, and language

else

w

   

if

 

33

putting it together: class language 
model with smoothing constraints

jeff  a. bilmes

gms in audio, speech, and language

id52
    represent and find part-of-speech tags (noun, 

adjective, verb, etc.) for a string of words

    id48s for word tagging

tags

words

    discriminative models for this task

tags

words

    label bias issue and selection bias.

jeff  a. bilmes

gms in audio, speech, and language

34

factored language models

    decompose words into smaller 

morphological or class-based units (e.g., 
morphological classes, stems, roots, 
patterns, or other automatically derived 
units).

    produce probabilistic models over these 

units to attempt to improve language 
modeling accuracy and parameter 
estimation

jeff  a. bilmes

gms in audio, speech, and language

example with words, stems, 
and morphological classes
3tm    

tm1tm    

2tm    

3ts    

2ts    

1ts    

ts

3tw    
)

2tw    
|

tw1tw    
p m w w   
,
(
t

)

,

|

1

t

t

   

2

)

   

2

p w s m
(
t

,

|

t

t

p s m w w   
(
t

,

1

t

t

t

jeff  a. bilmes

gms in audio, speech, and language

35

example with words, stems, 
and morphological classes
3tm    

tm1tm    

2tm    

3ts    

2ts    

1ts    

ts

3tw    

2tw    

tw1tw    

p w w w s
(
,
t

1
   

,

|

   

2

t

t

t

jeff  a. bilmes

,

1
   

s m m
t
t
   

1
   

,

,

2

t

   

2

)

gms in audio, speech, and language

in general

3tf    
3

3tf    
2

3tf    
1

2tf    
3

2tf    
2

2tf    
1

1tf    
3

1tf    
2

1tf    
1

3

tf

2

tf

1

tf

jeff  a. bilmes

gms in audio, speech, and language

36

general factored lm

    a word is equivalent to collection of factors.
w
f   
{ } {
t

the kth factor

kf =

}k

1:

t

p w w w
(
t

    e.g., if k=3
|
=
t
=

1
   

,

2

t

t

t

t

t

t

t

t

t

t

t

t

t

3

3

1

3

2

1

   

)

t
2

,
,
,
,

,
,
,
,

|
,
,
|

f
f
f
f

f
f
f
f

f
f
f
f

2
1
   
2
1
   
2
1
   
2
1
   

1
1
   
1
1
   
1
1
   
1
1
   

p f
(
p f
(

f
,
t
f
|
t
p f
(

f
,
t
f
,
2
f
|
t
p f
(

)
3
2
   
)
3
2
   
)
3
2
   
2 )
3
t   
    goal: find appropriate conditional  independence 
statements to simplify while keeping perplexity 
and error low. 
    structure learning problem

2
2
   
2
2
   
2
2
   
2
2
   

1
2
   
1
2
   
1
2
   
1
2
   

3
1
   
3
1
   
3
1
   
3
1
   

f
f
f
f

f
f
f
f

f
f
f
f

,
,
,
,

,
,
,
,

,
,
,
,

t
3

t

t

t

t

t

t

t

t

t

t

t

t

t

t

jeff  a. bilmes

gms in audio, speech, and language

   auto-regressive    id48s
    observation is no longer independent of 

other observations given current state
    can not be represented by an id48
    one of the first id48 extensions tried in 

id103.

q1

x1

q2

x2

q3

x3

q4

x4

jeff  a. bilmes

gms in audio, speech, and language

37

observed modeling

qt-2=qt-2

qt-1=qt-1

the hidden variable cloud

qt=qt

x

these are the feature
elements that comprise

z

jeff  a. bilmes

the 

implementation of 

these edges 

determines f(z). 
could be linear bz

or non-linear

qt+1=qt+1

say, for this 

element (suppose 
we name it xti)

gms in audio, speech, and language

buried markov models (bmms)
    markov chain is    further hidden    (buried) by 
specific element-wise cross-observation edges
    switching dependencies between observation 

elements conditioned on the hidden chain.

q1:t=q1:t

q1:t=q   1:t

jeff  a. bilmes

gms in audio, speech, and language

38

switching structure
switching structure

third condition
first condition
second condition

f
e
a
t
u
r
e
 
p
o
s
i
t
i
o
n

jeff  a. bilmes

time frame ->

gms in audio, speech, and language

bmm complexity

    ar-id48(k)

q1

x1

q2

x2

q3

x3

q4

x4

    theorem: triangulation comes for free after 

moralization in an ar-id48.

    theorem: triangulated-by-moralization ar-

id48(k) has hidden clique state space size of at 
most 2. 

    therefore, bmms have same asymptotic 
complexity as id48s, but they can not be 
represented exactly via id48s.

jeff  a. bilmes

gms in audio, speech, and language

39

multi-stream buried markov 

models

q1:t=q1:t

q1:t=q   1:t

jeff  a. bilmes

gms in audio, speech, and language

buried articulator models (bams)

q1:t=q1:t

q1:t=q   1:t

jeff  a. bilmes

gms in audio, speech, and language

40

segment models as gms 

p x
(

(cid:65)
t q
(
,
1:
1:
     

x
,
t q
,
(
1:
1:
     

(cid:65)

i
, ,2)

,

(cid:34)

,

x
t q
(
,
1:
1:
     

(cid:65)

i
, ,

(cid:65)

)

i

|

q
i

, )
  

i
, ,1)

p q q
(
i

|

i

, )
( )
     

p

1
   

  

=             

q
1:
  

(cid:65)
1:
  

i

1
=

  

p x
(
t
1:

)

  

q1

q2

l1

l2

1x

2x

...

1x(cid:65)

1 1x +(cid:65)

...

x +(cid:65)

1

(cid:65)

2

......

...

q  

l  

ix    (cid:65)

...

tx

jeff  a. bilmes

gms in audio, speech, and language

discriminative structure learning
    structure is typically learned to optimize marginal 

likelihood (e.g., statistical predictability)

    when the underlying goal is classification 

(regression), discriminative structure learning 

    structure is chosen to optimize conditional 
posterior of class variable (more generally, 
conditional likelihood) rather than marginal 
likelihood.

    can still use generative models
    structure edges can    switch    depending on current 

condition

jeff  a. bilmes

gms in audio, speech, and language

41

discriminative vs. generative 

models

    p(x|c)   vs.   p(c|x)
    goals for recognition (classification) are different than for 

generative accuracy (e.g., synthesis)
    generative models natural for asr
    approach: retain generative models but train 

discriminatively
    discriminative parameter training can occur for 

parameters of generative models also (e.g., maximum 
mutual information estimation on id48s)

    discriminative structure learning.

jeff  a. bilmes

gms in audio, speech, and language

discriminative generative 

models (dg models)

=f

mxf
({

:

max
arg:)
m

mpmxp
(

(

)

|

)

=

max
arg
m

mpmxf
,(

(

)

)}

so choose the f that satisfies the above, but is as 
simple (few parameters, easy to compute) as possible.
model might no longer    generates    samples 
accurately, but discriminates well.

jeff  a. bilmes

gms in audio, speech, and language

42

visual example

jeff  a. bilmes

gms in audio, speech, and language

spatial example

point generated by real

generative model

point not generated by real

generative model

point generated by disc

generative model

point generated by disc

generative model

jeff  a. bilmes

gms in audio, speech, and language

43

structural discriminability for 
classification performance

    overall goal: model parsimony, 

    achieve same or better performance with same or 

    family of models that concentrate power only on 

fewer parameters.

what matters.

    find structures that are inherently discriminative.
    rough idea: represent the    unique    dependencies of 

each class,  do not represent    common     
dependencies for all classes that do not help 
classification.

    models should be incapable of not discriminating

    distinct from being capable of discriminating

jeff  a. bilmes

gms in audio, speech, and language

structural discriminability

object generation process:

v4

v3

v4

v3

v2
p v v v v c =
(

v1
,

,

,

|

1

2

3

4

1)

v2
p v v v v c =
(
|

v1
,

,

,

1

2

3

4

2)

object recognition process: remove non-distinct dependencies that 
are found to be superfluous for classification, for example:

v4

v3

v4

v3

v1

v2

  (
p v v v v c =

,

,

,

|

1)

1

2

3

4

v1

v2
  (
p v v v v c =

,

,

,

|

1

2

3

4

1)

jeff  a. bilmes

gms in audio, speech, and language

44

comparison of generative vs. 

generative-discriminative models.

case  type 
1 
2 
3 

generative 
id48 
discriminative 

wer  params 
32.0%  207k 
157k 
5.0% 
4.6% 
157k 

 

jeff  a. bilmes

gms in audio, speech, and language

why id48s and delta features?
    wrong can be right

    id48 observations augment feature vector 

with temporal derivative estimates

    wrong generative structure is better from a 

discriminative point of view

jeff  a. bilmes

gms in audio, speech, and language

45

i x z q
(

the ear measure
i x z
(
;
;
    ear: explaining away residual
    a way of approximating the discriminative 

   

)

)

|

quality of the edge between z and x in 
context of q.
z

x

q

    marginally independent, conditionally 

dependent

jeff  a. bilmes

gms in audio, speech, and language

visualizing dependency

    use conditional mutual information

i x y c
(

;

|

=

c

)

=    

x y
,

p x y c
(

,

|

    there are many points to compute

) log

|
(

)
|

p x y c
(
,
p x c p y c
(
|
)
x   +
t
,

j

)

parent
parent

f
e
a
t
u
r
e
 
p
o
s
i
t
i
o
n

candidate
candidate

edge
edge
t

t+  

child
child

,t ix

time frame ->

gms in audio, speech, and language

jeff  a. bilmes

46

projection visualizations

parent-lag 

plot

child-lag 

plot

d
l
i
h
c

i

iparent

lag

parent-child 

plot

jeff  a. bilmes

gms in audio, speech, and language

mi vs. ear measure 

e
v
i
t
a
n
i
m

i
r
c
s
i
d
-
n
o
n

g
a
l
 
.
s
v

 
t
n
e
r
a
p

e
v
i
t
a
n
i
m

i
r
c
s
i
d

g
a
l
 
.
s
v

 
t
n
e
r
a
p

e
v
i
t
a
n
i
m

i
r
c
s
i
d
-
n
o
n

g
a
l
 
.
s
v
d
l
i
h
c

 

e
v
i
t
a
n
i
m

i
r
c
s
i
d
-
n
o
n

d
l
i
h
c
 
.
s
v

 
t
n
e
r
a
p

e
v
i
t
a
n
i
m

i
r
c
s
i
d

g
a
l
 
.
s
v
d
l
i
h
c

 

e
v
i
t
a
n
i
m

d
l
i
h
c
 
.
s
v

i
r
c
s
i
d

 
t
n
e
r
a
p

jeff  a. bilmes

gms in audio, speech, and language

47

mi vs. ear measure 

e
v
i
t
a
n
i
m

i
r
c
s
i
d

g
a
l
 
.
s
v

 
t
n
e
r
a
p

e
v
i
t
a
n
i
m

i
r
c
s
i
d

g
a
l
 
.
s
v
d
l
i
h
c

 

e
v
i
t
a
n
i
m

d
l
i
h
c
 
.
s
v

i
r
c
s
i
d

 
t
n
e
r
a
p

e
v
i
t
a
n
i
m

i
r
c
s
i
d
-
n
o
n

g
a
l
 
.
s
v

 
t
n
e
r
a
p

e
v
i
t
a
n
i
m

i
r
c
s
i
d
-
n
o
n

g
a
l
 
.
s
v
d
l
i
h
c

 

e
v
i
t
a
n
i
m

i
r
c
s
i
d
-
n
o
n

d
l
i
h
c
 
.
s
v

 
t
n
e
r
a
p

jeff  a. bilmes

gms in audio, speech, and language

learned 

jeff  a. bilmes

gms in audio, speech, and language

48

gmtk: id114 

toolkit

a gm-based software system for speech, language, 
and time-series modeling
one system     many different underlying statistical 
models (more than an id48)
complements rather than replaces other asr and 
gm systems (e.g., htk, at&t, isip, bnt, 
bugs, hugin, etc.)
ultimately will be open-source, freely available
long-term multi-year goal: improve features, 
computational speed, and portability.

   

   

   

   
   

jeff  a. bilmes

gms in audio, speech, and language

gmtk is infrastructure

    gmtk does not solve speech and language 
processing problems, but provides tools to 
help to simplify testing modeling, and does 
so in novel ways.

    the space of possible solutions is huge, 
and its exploration has only just begun. 

jeff  a. bilmes

gms in audio, speech, and language

49

gmtk features

    textual graph language
    switching parent functionality
    linear/non-linear dependencies on observations
    arbitrary low-level parameter sharing (em/gem 

training)

    gaussian vanishing/splitting algorithm.
    decision-tree-based implementations of 

dependencies (deterministic and sparse)

    full id136, single pass decoding possible on 

smaller tasks (current version)

    sampling methods
    log space exact id136     memory o(logt)

jeff  a. bilmes

gms in audio, speech, and language

gmtk structure file for id48

frame : 0 {

variable : state {

type : discrete hidden cardinality 4000;
switchingparents : nil;
conditionalparents : nil using densecpt(   pi   );

}
variable : observation {

type : continuous observed 0:39;
switchingparents : nil;
conditionalparents : state(0) using mixgaussian mapping(   state2obs   );

}

}
frame : 1 {

variable : state {

type : discrete hidden cardinality 4000;
switchingparents : nil;
conditionalparents : state(-1) using densecpt(   transitions   );

}
variable : observation {

type : continuous observed 0:39;
switchingparents : nil;
conditionalparents : state(0) using mixgaussian mapping(   state2obs   );

}

}
jeff  a. bilmes

gms in audio, speech, and language

50

gmtk structure file for id48
    structure file defines a prologue   , chunk 
  , and epilog   . e.g., for the basic id48:

q1

x1

q2

x2

q3

x3

prologue, first
group of frames

chunk, repeated
until t frames

epilogue, last
group of frames

jeff  a. bilmes

gms in audio, speech, and language

gmtk unrolled structure

    chunk is unrolled t-size(prologue)-

size(epilog) times (if 1 frame in chunk)

q1

x1

q2

x2

q3

x3

   

qt-1

xt-1

qt

xt

prologue, first
group of frames

chunk, repeated

until t frames is obtained.

epilog, last

group of frames

jeff  a. bilmes

gms in audio, speech, and language

51

multiframe repeating chunks
prologue  

repeating chunk  

epilogue  

prologue

chunk unrolled 1 time

epilogue

   

jeff  a. bilmes

gms in audio, speech, and language

switching parents

s   r1

m1
m1

f1
f1

s

c

m2

f2

p c m f f f
(

1, 1, 2, 2)

|

=

   

i

p c m f s
(
,

,

|

i

i

s   r2
i p s r
)
(
i

   

)

=

jeff  a. bilmes

gms in audio, speech, and language

52

variable : s {

gmtk switching structure
type : discrete hidden cardinality 100;
switchingparents : nil;
conditionalparents : nil using densecpt(   pi   );

}
variable : m1 {...}
variable : f1 {...}
variable : m2 {...}
variable : f2 {...}
variable : c {

m1
m1

f1
f1

s

c

m2

f2

type : discrete hidden cardinality 30;
switchingparents : s(0) using mapping(   s-mapping   );
conditionalparents : 

m1(0),f1(0) using densecpt(   m1f1   )
| m2(0),f2(0) using densecpt(   m2f2   );

}

jeff  a. bilmes

gms in audio, speech, and language

gaussians represented as bayesian 

networks

    factor concentration matrix: k = u   du

    d = positive diagonal of conditional variances
    u = unit upper-triangular matrix

    det(u) = 1 so det(u   du) = det(d)
    gaussian becomes:

xf
)(

=

d
2
  

2/1
   

   

1
2

e

(

x

   

xduu
)'
(
  

'

   

)
  

jeff  a. bilmes

gms in audio, speech, and language

53

gaussians and directed models

the b matrix
i b d i b
   

)'

   

(

k u du

=

'

x
1
x
2
x
3
x
4

   
   
   
   
   
   
   

   
   
   
   
   
   
   

=

0
0
0
0

   
   
   
   
   
   

b
12
0
0
0

=

(

b
13
b
23
0
0

b
14
b
24
b
34
0

   
   
   
   
   
   

x
1
x
2
x
3
x
4

   
   
   
   
   
   
   

   
   
   
   
   
   
   

+

  
   
1
   
  
   
2
   
  
3
   
  
   
   
4

   
   
   
   
   
   
   

)
   a gaussian can be viewed 
as a directed graphical model
    fsicms, obtained via 
u   du factorization, provides 
the edge coefficients

x1

x2

x3

x4

f x
( )

=    

i

f x x  
(
i

|

i

)

jeff  a. bilmes

gms in audio, speech, and language

actual sparse covariance matrix

n
e
r
d
l
i
h
c

2
1
c

-
1
c
,
e

)
2
1
c

-
1
c
,
e
(
   

)
2
1
c

-
1
c
,
e
(
   
   

jeff  a. bilmes

gms in audio, speech, and language

parents

54

gmtk sharing & em/gem training

    in gmtk, gaussians are viewed as directed 

id114.

    gmtk supports arbitrary parameter 

sharing: 
    any gaussian can share its mean, variance d, 

and/or its (sparse) b matrix with others.

    normal em training leads to a circularity
    gmtk training uses a gem algorithm

(
*
  

,

*

d b

,

*

)

=

argmax
d b

,
  

,

q

(
,
  

d b
;

,

o
  

,

o

d b

,

o

)

jeff  a. bilmes

gms in audio, speech, and language

gmtk splitting/vanishing 

algorithm

    determines number gaussian components/state
   

split gaussian if it   s component id203 
(   responsibility   )  rises above a number-of-
components dependent threshold

    vanish gaussian if it   s component id203 
falls below a number-of-components dependent 
threshold

    use a splitting/vanishing schedule, one set of 

thresholds per each em training iteration.

jeff  a. bilmes

gms in audio, speech, and language

55

decision-tree implementation 

of discrete dependencies

x1

x2

q1(x1)=t

q1

q1(x1)=f

q2

q5

q3

q6

q7

q4

pa(x2)

pb(x2)

pc(x2)

pd(x2)

jeff  a. bilmes

gms in audio, speech, and language

linear and log space exact 

id136

    exact id136 o(t*s) space and time 
complexity, s = clique state space size

    log-space id136 o(log(t)*s) space at 

an extra cost of a factor of log(t) time.

    can use both linear and log space id136 

at same time (for optimal tradeoff).

    this is same idea as what has been called 

the island algorithm

jeff  a. bilmes

gms in audio, speech, and language

56

example: linear-space in id48
      
t
t
( )
(
  =
j

a b x
(
ji
t

1)

)

i

  
i

j

  
i

t
( )

=

+   
t
(

  
j

j

1)

a b x
(
ij
t

j

1
+

)

jeff  a. bilmes

gms in audio, speech, and language

example: one recursions log space
      
t
t
( )
(
  =
j

a b x
(
ji
t

1)

)

i

  
i

j

  
i

t
( )

=

+   
t
(

  
j

j

1)

a b x
(
ij
t

j

1
+

)

jeff  a. bilmes

gms in audio, speech, and language

57

example: two recursions log space
t
( )

1)

)

a b x
(
ji
t

i

      
t
(
  =
j

  
i

j

  
i

t
( )

=

+   
t
(

  
j

j

1)

a b x
(
ij
t

j

1
+

)

jeff  a. bilmes

gms in audio, speech, and language

the gmtk triangulation engine

(an anytime algorithm)

    user specifies an amount of time (2mins, 3 hours, 4 

days, 5 weeks, etc.) to spend triangulating

    user does not worry about intricacies of graph 

triangulation

    uses a    boundary algorithm    to find chunks of dbn 

to triangulate (uai   2003)

    many heuristics implemented: min-fill in, min size, 
min weight, maximum cardinality search, simulated 
annealing, exhaustive elimination, and exhaustive 
triangulation

jeff  a. bilmes

gms in audio, speech, and language

58

current status
current status

i.

system available at:
a. http://ssli.ee.washington.edu/~bilmes/gmtk
b. ~100 pages of documentation
c. book chapter on use of id114 for speech 

and language

d. jhu   2001 workshop technical report

ii. gmtk triangulation    engine    running and 

ready

jeff  a. bilmes

gms in audio, speech, and language

exact id136 in dbns

    triangulation in dbns

    standard triangulation heuristics typically poor for dbns

since they are short and wide

    slice-by-slice triangulation via elimination: severely limit 

number of elimination orders without limiting optimal 
triangulation quality

    triangulation quality is lower-bounded by size of interface to 

previous (or next) slice

    can allow interfaces to span multiple slices, which can make 

interface quality much better (uai   2003)

    use message passing order in junction tree that 

respects directed deterministic dependencies (to cut 
down on state space)

jeff  a. bilmes

gms in audio, speech, and language

59

approximate id136 in dbns
    standard approximate id136 methods

    pruning as is done performed by modern speech 

recognition systems

    variational and mean-field approaches
    loopy belief propagation
    sampling, id143ing, etc.

    all techniques for approximate id136 in 

dbns are relevant to the speech/language case 
as well.

jeff  a. bilmes

gms in audio, speech, and language

conclusions
    many models and many techniques
    we have just scratched the surface, still a 

relatively young research area.

    key challenges summary:
    explicit control structures
    structure learning
    fast id136 techniques
    identifying interesting latent variables
    structural discriminability

jeff  a. bilmes

gms in audio, speech, and language

60

the end

jeff  a. bilmes

gms in audio, speech, and language

61

