id114 in speech 

and language research

jeff a. bilmes

university of washington
department of ee, ssli-lab

tutorial philosophy
tutorial philosophy

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

detailed outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

cause and effect

    fact: at least 196 cows died in thailand, 

jan/feb 2004, as have 16 people.

    consequence: canadian officials in april 

2004 killed 19 million birds in british 
columbia (chickens, ducks, geese, etc.)
    possible cause i: original deaths due to 

avian influenza (h5n1 or bird flu)

    possible cause ii: they died of old age!

jeff  a. bilmes@

id114

cause and effect
    simple directed graphs can be used.
    directed edges go from parent (possible cause) 

to child (possible effect)

bird flu

old age

deaths

canadian action

jeff  a. bilmes@

id114

cause and effect

bird flu

old age

    quantities of interest:

deaths

canadian action

computing 
probabilities:

examples:

pr( deaths | flu and old )
pr( deaths |  old ) 
pr( bird flu | canadian action )

asking

questions:

examples:

    in general, does old age increase the chance that 
a cow has contracted bird flu (if at all)?
    if we know the action by canada occurred,  does 
having bird flu decrease the chance that it was old 
when it died?

very simple scenario, with obvious answers to questions. what happens with more 

complicated real-world problems?

jeff  a. bilmes@

id114

realistic domains

    graph represents relational structure of domain. 
    regardless of graph complexity, same set of 

algorithms used to compute desirable quantities and 
answer all questions.

b

c

    example: is pr(a|b) larger or 

smaller than pr(a|b,c)?

    graphs are natural way to 

explain complex situations on 
an intuitive visual level.

    graphs are used everywhere, 
for many different purposes. 
here they have a specific 
meaning: random variables 
and conditional independence.

id114

a

jeff  a. bilmes@

id114 (gms)

    structure
    algorithms
    language
    approximations
    data-bases

jeff  a. bilmes@

id114

id114 (gms)

gms give us:
i.

structure: a method to explore the 
structure of    natural    phenomena (causal 
vs. correlated relations, properties of 
natural signals and scenes, factorization)

ii. algorithms: a set of algorithms that 

provide    efficient    probabilistic 
id136 and statistical decision making

iii. language: a mathematically formal, 
abstract, visual language with which to 
efficiently discuss families of 
probabilistic models and their properties.

jeff  a. bilmes@

id114

id114 (gms)

gms give us (cont):
iv. approximation: methods to explore 

systems of approximation and their 
implications. e.g., what are the 
consequences of a (perhaps known to be) 
wrong assumption?

v. data-base: provide a probabilistic    data-

base    and corresponding    search 
algorithms    for making queries about 
properties in such model families.

jeff  a. bilmes@

id114

topology of id114

id114

other semantics

causal models

chain graphs

factor graphs

dependency networks

dgms

fst

zms

dbns

id48

ar

simple 
models

ugms

mrfs

id110s

mixture 
models

kalman

decision 

trees

lda

factorial id48/mixed 
memory markov models

jeff  a. bilmes@

bmms

segment models

pca

gibbs/boltzman

distributions

id114

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

three important graph types

    three (of many) types of gm

    id110s
    undirected id114
    factor graphs

    each conveys a different family
    each type of graph conveys factorization of 

id203 distributions in some way.

jeff  a. bilmes@

id114

factorization and conditional 

    factorization of id203 distributions 

independence
notation: xa is a set of 
random variable indexed 
usually (but not always) implies some form 
by the set a. so if a = 
of conditional independence.
xa = {x1, x2, x4} 
    conditional independence:

{1,2,4}, then 

x

a

      

x

|

x

c

b

    holds if and only if there are functions f() 

and g() such that:
p x
(
=

x

)

,

,

x
c

a

b

g x
(

,

x h x
) (
c

b

,

x
c

)

a

jeff  a. bilmes@

id114

id110s

    only one type of graphical model (many others exist)
    compact representation: factors of probabilities of 
children given parents.
sub-family specification:
directed acyclic graph 

earthquake

burglary

radio

alarm

(dag)

    nodes - random variables 
    edges - direct    influence   
together:
defines a unique distribution 
in a factored form

be
be
e
b
e
b
b
e

p(a | e,b)

0.9
0.1
0.2
0.8
0.9
0.1
0.01 0.99

call
instantiation: 
set of conditional 
id203 distributions
)

(

|

,

p b e a c r
(

,
jeff  a. bilmes@

,

,

)

=

p b p e p a b e p r e p c a
(
)

)

(

)

(

)

(

,

|

|

id114

id110s
x
      

x

|

    when is                             ?
    only when c d-separates a from b, i.e. if:

c

a

b

x

for all paths from a to b, there is a v on the path that is 
blocked. node v is blocked if either:

1.    v    or    v    and v   c
2.    v    and neither v nor any descendants are in c

    equivalent to    directed local markov property   

    a variable is conditionally independent of its non-

descendants given its parents

    see lauritzen    96 for other properties.
    distribution factors as product of id155 

distributions of child given its parents p(c|parents)

jeff  a. bilmes@

id114

three basic cases

    graphs encode conditional independence 

via factorization.

v1

v1

v2

v3

v2
v
2
v
2

v3
v v
|
      
3
1
v        
3

v2
v
2
v
2

v3
v v
|
      
3
1
v        
3

v1
v      
3
v v
3
1

v
2
v
        
2

|

jeff  a. bilmes@

id114

a probabilistic data-base
    what are the implicit assumptions made by a 
given statistical model? can be complicated.

,

,

p x y z a b c d e
,
(
p y d p d c e p e z p c b p b a p x a p a p z
(
)
(
=

,
(

,
)

,
,

)
(

,
|

)

(

)

(

)

(

)

(

)

|

|

|

|

|

)

is 

x

      

z y

|

???

jeff  a. bilmes@

id114

a probabilistic data-base

    gms can help illuminate the answer.

,

,

p x y z a b c d e
,
(
p y d p d c e p e z p c b p b a p x a p a p z
)
(
(
=

,
(

,
)

,
,

)
(

,
|

)

(

)

(

)

(

)

(

)

|

|

|

|

|

)

a

x

b

c

z

e

d

y

x

      

z y

|

???

no!!

jeff  a. bilmes@

id114

hidden variables

    hidden variables can introduce significant 
capabilities for probabilistic queries (called 
id136). 

observed

observed

v1

v2

v3

vvp
(
|
1

3

)

hidden

vvvp
(
,
2

|

1

3

   =

v
2
)
=

vvpvvp
(
|
1

()

|

2

2

3

)

vvp
(
|
2

3

)

jeff  a. bilmes@

id114

nodes as inputs and/or outputs
    variables in a gm can be both an input or 
an output variable at different times (unlike 
a neural network). this can be very useful. 

input

output

v1

v2

v3

output

input

v1

v2

v3

p v v
|
(
1

3

)

p v v
|
(
3

1

)

jeff  a. bilmes@

id114

undirected gms (ugms)

    when is xa || xb | xc?
    only when c separates a from b. i.e., if:

for all paths from a to b, there is a v on the path 
s.t. v   c

    simpler semantics than id110s.
    equivalent to    global markov property   , plus 

others (again see lauritzen    96)

jeff  a. bilmes@

id114

ugms (markov random fields)
    example independence:
v2
v v
{ , } { , }|
5
6

v v v
2
4

      

v1

3

v4

v3

    factorization

v5

v6

,

,

p v v v v v v
,
,
(
6
1
f v v f v v f v v f v v v
(
)
,
=
3

4
(

5
,

2
,

3
)

)

(

)

(

,

,

,

4

5

5

6

1

4

2

4

)

jeff  a. bilmes@

id114

example: undirected gms (ugms)

xp
(

)

=

1
z

      

c

x x
(

c

c

)

if gibbs:

=

exp{
   

1
z

1
xu
)}(
tk
0
c   =
x xh
(
c

)

c = cliques (completely 
connected nodes) in graph.
semantics: simple separation
xa || xb | xc if xc separates xa from xb in graph.

xu
)(

c

w

y

x

z

cliques are: { (w,x), (x,z), (z,y), (y,w) }

ugms are the same as mrfs

jeff  a. bilmes@

id114

directed and undirected models 

represent different families

decomposable models

dgm

ugm

    the classic examples:

w

y

x

z

w

z

y

jeff  a. bilmes@

id114

factor graphs

    graph represents all possible factorizations 

of a distribution.

    simplest example:

p v v v
,
(
3

,

1

2

)

=

f v v f v v f v v
,
(
1

)

(

)

(

,

2

2

3

3

,
1
v1

)

v2

v3

jeff  a. bilmes@

id114

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

dynamic id110s

(dbns)

    most appropriate for speech/language.
    dbns are id110s over time
    specified using a    rolled up    template
    in unrolled dbn, all variables sharing same 

origin in template have tied parameters.

    allows for specifying graph over arbitrary 

length series.

jeff  a. bilmes@

id114

id48

    id48s are dbns.
q2

q1

q3

x3
    corresponds to template:

x2

x1

q4

x4

note: these graphs is not

stochastic finite state 

automata, as in:

q1

x1

q2

x2

jeff  a. bilmes@

id114

dynamic id110s
    more generally, dbn specifies template to 

be unrolled:
intra-slice edges
inter-slice edges

unrolled dbn, 3 times

dbn template

jeff  a. bilmes@

id114

general probabilistic id136
xp
(
6
)

xxp
(
,
/)
6
   =
xp
)
(
6:1

x
)
|
=
6
xxp
,
(
6

xp
(
1
x6

x4

x2

1

1

)

x3

x5

p x x x x x x
,
(
6

,

,

,

,

1

2

3

4

5

)

x

5:2

exploit local structure to provide for 
efficient id136 o(r3) rather than o(r6)
(variable elimination algorithm)

p x p x
(
(
2

)

1

p x p x
(
(
2

)

1

|

|

x p x
(
1
3

)

|

x p x
(
1
4

)

|

x p x
(
2
6

)

|

x x p x
(
2
5

)

,

5

|

x
3

)

x
1

)

   

x
3

p x
(
3

|

x
1

)

p x
(
6

|

   

x
5

x x p x
(
2
5

)

,

5

|

x
3

)

   

x
4

p x
(
4

|

x

)

2

x1

   
x
2:5
   
=
   

=

x
2:5

x
2

    way in which sums are distributed into products corresponds to 

different ways of running the junction tree algorithm 
(generalization of baum welsh)

jeff  a. bilmes@

id114

moralization & triangulation, 

lead to junction tree

a

c

f

d

g

b

e

h

i

original

a

c

f

d

g

b

e

h

i

moralized

complexity of id136: 
o

cs
))(

(   

c
jeff  a. bilmes@

a

c

f

d

g

b

e

a,b,c,d

b,c,d,f

h

i
triangulated/
decomposable/
eliminatable
cr
|
   

|

cs
)(

b,e,f

e,f,h

f,d,g

f,g,i

junction tree

id114

moralization & triangulation

    moralization

    why ok? more edges, fewer independence 

assumptions, bigger family

    why needed? so ugm doesn   t violate bn 

semantics, summations include parents in cliques.

    triangulation

    why ok? fewer independencies -> bigger family
    why needed? to get a decomposable graph, where 
junction tree (with running intersection property) 
exists and message passing algorithm is correct 
(i.e., local consistency implies global consistency)

jeff  a. bilmes@

id114

id136: message passing in jt

now, all cliques equal 
the joint of their 
variables and any 
evidence (observations), 
e.g., p(a,b,c,d,obs), 
p(b,c,d,f,obs), etc.

a,b,c,d

b,c,d,f

select root 

of jt

b,e,f

f,d,g

e,f,h

1. collect evidence phase
2. distribute evidence phase

f,g,i

jeff  a. bilmes@

id114

id136 in dynamic models

    id136 procedures are equivalent to 

forming a    dynamic    junction tree

    generalizes forward/backward 

(baum/welch) procedure in id48s

    dynamic junction tree is much wider than 

higher.

jeff  a. bilmes@

id114

id136 in dynamic models

intra-slice edges
inter-slice edges

dbn template

w1

x1

y1

z1

w2

x2

y2

z2

unrolled dbn, 3 times

w1

x1

y1

z1

w2

x2

y2

z2

w3

x3

y3

z3

w4

x4

y4

z4

w5

x5

y5

z5

jeff  a. bilmes@

id114

id136 in dynamic models

intra-slice edges
inter-slice edges
moralization edges

moralized dbn

w1

x1

y1

z1

w2

x2

y2

z2

w3

x3

y3

z3

w4

x4

y4

z4

w5

x5

y5

z5

jeff  a. bilmes@

id114

id136 in dynamic models

    boundary between slices defines 

graph partitions to triangulate

w1

x1

y1

z1

w2

x2

y2

z2

w3

x3

y3

z3

dbn partition

w2

x2

y2

z2

y1

z1

intra-slice edges
inter-slice edges
moralization edges
compulsory interface edges
triangulation edges
resulting triangulated
dbn partition

w2

x2

y2

z2

y1

z1

jeff  a. bilmes@

id114

id136 in dynamic models
    each partition is stitched together to create 
what is guaranteed to be a triangulated dbn

intra-slice edges
inter-slice edges
moralization edges
compulsory interface edges
triangulation edges

resulting triangulated dbn

w1

x1

y1

z1

w2

x2

y2

z2

w3

x3

y3

z3

w4

x4

y4

z4

w5

x5

y5

z5

jeff  a. bilmes@

id114

id136 in dynamic models

    resulting junction tree (note: better ones exist, see 

bilmes&bartels uai   03 on triangulating dbns)

    junction    tree    in this case is a 

markov chain

    not dissimilar to hidden markov 
model alpha-recursion, but here 
the    cliques    oscillate between two 
forms. 

w1

x1

y1

z1

w2

x2

y2

z2

w3

x3

y3

z3

w4

x4

y4

z4

w5

x5

y5

z5

w1,x1,y1

w2,x2,y2

w3,x3,y3

w4,x4,y4

w5,x5,y5

y1,z1,w2,x2

y2,z2,w3,x3

y3,z3,w4,x4

y4,z4,w5,x5

y5,z5

jeff  a. bilmes@

id114

id136 is hard

    np complete (exponentially difficult) to perform 

id136.

    goal: find small cliques, since complexity is 

exponential in clique size (also hard, np)

    approximate id136 schemes exist for harder 

problems
    variational approaches
    sampling techniques (mcmc, gibbs, etc.) 
    loopy belief propagation (ldpc & turbo codes)
    pruning procedures

jeff  a. bilmes@

id114

   learning    id114

   

   

five scenarios for learning:
1. structure known, no hidden variables
2. structure known, hidden variables
3. structure unknown, no hidden variables
4. structure unknown, edges unknown over known 

hidden variables

5. structure unknown,unknown set of hidden variables. 
typically, we need to do 5 for 
speech/language/machine translation 
recognition.

jeff  a. bilmes@

id114

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

why id114 for 

speech and language processing
    expressive concise way to describe properties of 
families of distributions

    rapid movement from novel idea to implementation 
(with right toolkit)     all graphs utilize exactly same 
id136 algorithm! researcher concentrates on 
model and can stay focused on domain.

    gms include many standard techniques but gm space 

is hardly explored.

    dynamic id114 can represent important 
structure in    natural    time signals but ignore what is 
unimportant for a given task (example parsimony 
through structural discriminability)

jeff  a. bilmes@

id114

four main goals for 

gms in speech/language

1. explicit control: derive graph structures that 
themselves explicitly represent control constructs

   

e.g., parameter tying/sharing, state sequencing, smoothing, 
mixing, backing off, etc.

2. latent modeling: use graphs to represent latent 

information in speech/language

3. observation modeling: represent structure over 

observations.

4. structure learning: derive structure

automatically, ideally to improve error rate while 
simultaneously minimizing computational cost.

jeff  a. bilmes@

id114

graph control structure approaches
    the    implicit    graph structure approach

    implementation of dependencies determine sequencing 

through time-series model

    everything is flattened, all edge implementations are random 

but are very sparse (most but not all entries are zero)

    the    explicit    graph structure approach

    graph structure itself represents control sequence mechanism 

and parameter tying in a statistical model.

jeff  a. bilmes@

id114

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

basic triangle structures:
a basic explicit approach for parameter tying
zweig & 
russel,    99

end of word observation

counter

transition

phone

1

y

2

2

1

0

1

aa

aa

3

m

4

5

1

1

1

aa

hh

6

aa

  

1

observation

    structure for the word    yamaha   , note that /aa/ 

occurs in multiple places preceding different phones.

jeff  a. bilmes@

id114

key points

    graph explicitly represents parameter sharing
    same phone at different parts of the word are the same: 
phone /aa/ in positions 2, 4, and 6 of the word    yamaha   

    phone-dependent transition indicator variables yield 
geometric phone duration distributions for each phone
    counter variable ensures /aa/   s at different positions 

move only to correct next phone 

    some edge implementations are deterministic (green) 

and others are random (red)

    end of word observation, gives zero id203 to  

variable assignments corresponding to incomplete words.

jeff  a. bilmes@

id114

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

explicit bi-gram training graph structure

skip silence

word
counter

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

...

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

...

jeff  a. bilmes

gms in audio, speech, and language

explicit bi-gram training graph structure

skip silence

word
counter

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

...

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

...

jeff  a. bilmes@

id114

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

bi-gram training w. pronunciation variant

skip silence

word counter

word 

nodes & 
nodes & 
edge colors:
edge colors:

red    
red    
random
random
green    
green    
deterministic
deterministic

...

...

end-of-utterance 
observation=1

pronunciation

word transition

state counter

state transition

state

observation

jeff  a. bilmes@

id114

switching parents: value-specific conditional 

independence

s   r1

m1
m1

f1
f1

s

c

m2

f2

p c m f f f
(

1, 1, 2, 2)

|

=

   

i

p c m f s r p s r
(
i

   

(

,

,

|

i

i

i

)

s   r2
)
   

jeff  a. bilmes@

id114

explicit bi-gram decoder

wordtransition is a switching parent of word. it 
switches the implementation of word(t) to either be a 
copy of word(t-1) or to invoke the bi-gram
)

p w w    
(
t
1

|

t

nodes & 
nodes & 
edges:
edges:
    red    
    red    
random
random
    green    
    green    
deterministic
deterministic
   dashed line 
   dashed line 

   
   

switching 
switching 
parent
parent

jeff  a. bilmes@

...

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

id114

explicit tri-gram decoder

p w w w   
(
t
1,

|

t

t

   

2

)

previous word 

nodes & 
nodes & 
edges:
edges:
    red    
    red    
random
random
    green    
    green    
deterministic
deterministic
   dashed line 
   dashed line 

   
   

switching 
switching 
parent
parent

jeff  a. bilmes@

end-of-utterance 
observation=1

word 

word transition

state counter

state transition

state

observation

id114

...

   auto-regressive    id48s
    observation is no longer independent of 

other observations given current state
    can not be represented by an id48
    one of the first id48 extensions tried in 

id103.

q1

x1

q2

x2

q3

x3

q4

x4

jeff  a. bilmes@

id114

observed modeling

qt-2=qt-2

qt-1=qt-1

the hidden variable cloud

qt=qt

x

these are the feature
elements that comprise

z

jeff  a. bilmes@

the 

implementation of 

these edges 

determines f(z). 
could be linear bz

or non-linear

qt+1=qt+1

say, for this 

element (suppose 
we name it xti)

id114

buried markov models (bmms)
    markov chain is    further hidden    (buried) by 
specific element-wise cross-observation edges
    switching dependencies between observation 

elements conditioned on the hidden chain.

q1:t=q1:t

q1:t=q   1:t

jeff  a. bilmes@

id114

multi-stream buried markov 

models

q1:t=q1:t

q1:t=q   1:t

jeff  a. bilmes@

id114

acoustic
model

acoustic
model

...

acoustic
model

1
 
l
e
n
n
a
h
c

2

 
l

e
n
n
a
h
c

acoustic
model

acoustic
model

...

acoustic
model

observation

word transition

word 

previous word

end-of-conv.

previous word

word 

word transition

observation

c
o
n
v
e
r
s
a
t
i
o
n
a
l
 

m
o
d
e
l

jeff  a. bilmes@

i

ii

iii

id114

id100 (a digression)

state of 
the world

policy of 

agent

action taken 

by agent

world

reward

goal: maximize the sum of rewards, obtainable using normal 

forward algorithm (id145)

jeff  a. bilmes@

id114

from explicit control to 

latent modeling

1.

in latent modeling, we move more towards 
representing and learning additional 
information in (factored) hidden space.

2. factored representations place constraints on 

what would be flattened id48 transition 
matrix parameters thereby potentially 
improving estimation quality.

jeff  a. bilmes@

id114

latent modeling

qt-2=qt-2

qt-1=qt-1

the hidden variable cloud

qt=qt

qt+1=qt+1

observations

1:tx
    key questions: what are the most important 

   causes    or latent explanations of the temporal 
evolution of the statistics of the vector observation 
sequence?

    how best can we factor these causes to improve 
parameter estimation, reduce computation, etc.?

jeff  a. bilmes@

id114

latent x modeling

qt-2=qt-2

qt-1=qt-1

the hidden variable cloud

qt=qt

qt+1=qt+1

observations

other hidden 

variables
    where x = gender, speaker cluster, speaking rate, 

noise condition, accent, dialect, pitch, formant 
frequencies, vocal tract length, etc.

    we elaborate upon latent articulatory modeling   

jeff  a. bilmes@

id114

ex: latent articulatory modeling

pictures from linguistics 001, university of pennsylvania

jeff  a. bilmes@

id114

phone-free articulatory graph

(by karen livescu)

jeff  a. bilmes@

id114

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

id52
    represent and find part-of-speech tags (noun, 

adjective, verb, etc.) for a string of words

    id48s for word tagging

tags

words

    discriminative models for this task

tags

words

    label bias issue and selection bias.

jeff  a. bilmes@

id114

standard id38
    example: standard 4-gram
p w h
(
,
t

w w
t
t

p w
(
t

w
t

=

)

)

1
   

,

|

|

   

   

3

2

t

4tw    

3tw    

2tw    

tw1tw    

jeff  a. bilmes@

id114

interpolated uni-,bi-,tri-grams
p w w
(
)
t
p w w w
(
,
t

p
(
  
t
p
(
  
t

p w
(
t

2)
3)

+
+

(
  
t

=
=

1)

p

|
|

=

=

1
   

)

)

1
   

t

t

t

t

p w h
(
t

|

)

   

2

    nothing gets zero id203

jeff  a. bilmes@

id114

conditional mixture tri-gram
p w h
(
t

p
1|
(
)
=
=
  
t
p
2 |
(
  
=
+
t
p
3|
(
  
=
+
t

w w p w
)
t
t
w w p w w
|
t
t
t
w w
|
t
t

(
(
p w w w
(
t

)
2
)
)

1
   
,
,

)
,

1
   

1
   

1
   

1
   

)

   

2

,

|

   

   

   

2

2

t

t

t

t

t

jeff  a. bilmes@

id114

skip bi-gram

    often there is silence between words

       fool me once <sil> shame on <sil> shame on you   

    silence might not be good predictor of next word
    but silence lexemes should be represented since 
other graph modules might depend on them (e.g., 
acoustics, id144, meaning in silence for mt).
    goal: allow silence between words, but retain 

true word predictability skipping silence regions.

    switching parents can facilitate such a model.

jeff  a. bilmes@

id114

p r w r
(
,
t

|

t

t

1
   

p w s r
(
,
t

|

t

t

)

1
   

skip bi-gram
)

r
   =
t
1

t

  
      =    
r
t
  
      
r w
=
t
t
   =
      =    
tw sil
p
      
bigram
1) pr(
=

t

=
   

if w sil
if w sil
if
if

)

w r
(
|
t
t
1
   
silence
)

tp s
(

=

s
t
s
t

=
=

1
0

jeff  a. bilmes@

id114

skip bi-gram with conditional 

p
bigram

(

w r
|
t
t

1
   

)

mixtures
p
(
1|
=
  
t
p
(
2 |
+
  
t

=
=

1
   

w p w
)
(
t
t
w p w r
|
(
t
t

)
)

1
   

t

)

1
   

jeff  a. bilmes@

id114

p w w w
(
t

1
   

|

t

t

2

   

)

two switching parents
w w p w w w
|
,
t
t
tri
w w p w w
(
t
t
2
   
w p w w
)
|
(
t
bi
t
w p w
(
t
t

p
(
  
t
p
(
  
t
p
(
  
t
p
(
  
t

1|
0 |
1|
0 |

=
=
=
=

=
+
=
+

,
,
)
)

1
   
)

)
)

t
)

t
|

1
   

1
   

1
   

1
   

1
   

1
   

(

)

1
   

,

   

2

t

t

t

t

)

   

2

p w w
(
t

|

t

jeff  a. bilmes@

id114

skip trigram

    similar to skip bi-gram, but skips over two 

previous <sil> tokens.

    p(city|<sil>,york,<sil>,new) = 

p(city|york,new)

jeff  a. bilmes@

id114

putting it together: 

mixture and skip tri-gram.

jeff  a. bilmes@

id114

class language model

    when number of words large (>60k), can be 

better to represent clusters/classes of words
    clusters can be grammatical or data-driven
    just an id48 (perhaps higher-order)

jeff  a. bilmes@

id114

explicit smoothing

    disjoint partition of vocabulary based on training-data 

counts: (cid:62) = {unk}    (cid:54)     (cid:48)

    (cid:54) = singletons, (cid:48) =    many-tons   , unk=unknown
    ml distribution gives zero id203 to unk.
    goal: directed gm that represents and learns   :

p w
)
(

(

s)
)

   
   =
   
   
   

p
(1
)
   
  
ml
p w
)
(
  
ml
p w
)
(
ml

w unk
w

if
=
s
if
   
otherwise

    word variable is like a switching parent of itself (but of 

course can   t be, no directed cycles allowed.)

jeff  a. bilmes@

id114

explicit smoothing

    introduce two hidden variables k and b and one child 

observed variables v=1.

    hidden variables are switching parents

    k = indicator of singleton+unk (k=1) vs.    many-ton    (k=0)
    b = indicator of singleton (b=1) vs. unknown word (b=0)
    fixed observation child v induces    reverse causal    

phenomena via its dependency implementation
    i.e., child says    if you want me to give you non-zero 

id203 on this observation, you parents had better do x   

k

b

w

v

jeff  a. bilmes@

id114

singleton+unk
vs. manyton

singleton 
vs. unk

k

goal

b

w

v

p w
( )

p
(1
)
  
   
   
ml
   =
p w
( )
  
   
ml
   
p w
( )
   
ml

(

w unk
w

s)
) if
=
s
if
   
otherwise

p w k b
, )
(

|

p v
(

=

1|

w k
, )

=

{1

(

w

p w
)
m

(

   
   =    
      

p w
(
)
ml
m
p
(
)
ml
0

if

w

   

m

else

jeff  a. bilmes@

explicit smoothing
p b
(
p k
(

p b
(
p k
(
k
if
k
if
k
if

0)
=
=
  
= s
p
( )
0)
=
0
b
1 and 
b
1 and 

=
=
=

=
=

1
0

1) 1
=
=    
1) 1
=    
=
p w
)
(
   
m
   
p w
)
(
   
s
   
   =
   
w unk
t
m
   

=

k

,

=

}
1)

k

s
,
   
=

w
0) or (
=
w unk k
,
or (
p w
)
(
ml
s
p
( )
ml
0

   
   =    
      

else

w

if

   

1) 
=

s

p w
)
s

(

id114

putting it together: class language 
model with smoothing constraints

jeff  a. bilmes@

id114

factored language models

(katrin kirchhoff)
tf
3

2tf    
3

1tf    
3

3tf    
3

3tf    
2

3tf    
1

2tf    
2

2tf    
1

1tf    
2

1tf    
1

2

tf

1

tf

jeff  a. bilmes@

id114

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

switching parents

s   r1

m1
m1

f1
f1

s

c

m2

f2

p c m f f f
(

1, 1, 2, 2)

|

=

   

i

p c m f s
(
,

,

|

i

i

s   r2
i p s r
)
(
i

   

)

=

jeff  a. bilmes@

id114

   switching existence    variables

    random variables whose values determine 
the number of other variables in the graph

    can represent a random number of random 

variables

    e.g.: can represent the length of hidden 

sequences, fertility in ibm models 3-5, etc. 

jeff  a. bilmes@

id114

switching existence: parameter 
tying and connection to id48s

    parameter tying is key for implementing switching 

existence variables:  when a new variable comes 
into existence, it needs some way to obtain its 
parameters

    time homogeneous id48s are also implicitly 

based on parameter tying, i.e., all state variables 
share the same transition matrix

    probabilistic relational models also use a similar 
mechanism:  a variable number of instances of an 
object class share the same parameters

jeff  a. bilmes@

id114

mt noisy channel graphical 

model

graphs by karim filali

    goal: translate a french 

string, 
=r
f

f
m
1 =

ff
f
...21
to an english string

m

1=r
e
le

    model:  noisy channel --
fr
    id136: recover most 

er
is a noisy version of    

fr
likely     given   

er

e1

e2

hidden 
variable

em

f0

f1

fl

jeff  a. bilmes@

observed 
variable

id114

ibm models

    introduce one-to-many alignment variables a1:m     each 
french word can translate into at most one english word

e1

a1

f1

e2

a2

f2

m

el

am

fm

jeff  a. bilmes@

id114

ibm model 1

    independence 
assumptions:

existence 
variable

l

e1

f1

a1

e2

f2

a2
1

el

fm

am

switching parents

note: parameters for a1:m are tied
(i.e. position independent)

id114

jeff  a. bilmes@

a pause for questions
a pause for questions

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

outline

1. properties

a) overview and motivation
b) gm types and constructs
c) theory and practice of dynamic gms
d) explicit temporal structures

2. specific models
a) gms in speech
b) gms in language
c) gms in machine translation

3. toolkits
4. discussion/open questions

jeff  a. bilmes@

id114

gmtk: id114 

toolkit

   

   

   

   

a gm-based software system for speech, 
language, and time-series modeling
one system     many different underlying 
statistical models (more than an id48)
complements rather than replaces other 
asr and gm systems (e.g., htk, at&t, 
isip, bnt, bugs, hugin, etc.)
freely available, to be open-source

jeff  a. bilmes@

id114

gmtk features

1. textual graph language
2. switching parent functionality
3. forwards and backwards time links
4. multi-rate models with extended dbn templates.
5. linear dependencies on observations
6. arbitrary low-level parameter sharing (em/gem training)
7. gaussian vanishing/splitting algorithm.
8. decision-tree-based implementations of dependencies 

(deterministic, sparse, formula leaf nodes)

9. full id136, single pass decoding possible 
10. sampling methods
11. linear and island algorithm (o(logt)) exact id136

jeff  a. bilmes@

id114

gmtk structure file for id48
    structure file defines a prologue (cid:51), chunk 
(cid:38), and epilog (cid:40). e.g., for the basic id48:

q1

x1

q2

x2

q3

x3

prologue, first
group of frames

chunk, repeated
until t frames

epilogue, last
group of frames

jeff  a. bilmes@

id114

gmtk unrolled structure

    chunk is unrolled t-size(prologue)-

size(epilog) times (if 1 frame in chunk)

q1

x1

q2

x2

q3

x3

   

qt-1

xt-1

qt

xt

prologue, first
group of frames

chunk, repeated

until t frames is obtained.

epilog, last

group of frames

jeff  a. bilmes@

id114

multiframe repeating chunks
prologue (cid:51) repeating chunk (cid:38) epilogue (cid:40)

prologue

chunk unrolled 1 time

epilogue

   

jeff  a. bilmes@

id114

gmtk structure file for id48

frame : 0 {

variable : state {

type : discrete hidden cardinality 4000;
switchingparents : nil;
conditionalparents : nil using densecpt(   pi   );

}
variable : observation {

type : continuous observed 0:39;
switchingparents : nil;
conditionalparents : state(0) using mixgaussian mapping(   state2obs   );

}

}
frame : 1 {

variable : state {

type : discrete hidden cardinality 4000;
switchingparents : nil;
conditionalparents : state(-1) using densecpt(   transitions   );

}
variable : observation {

type : continuous observed 0:39;
switchingparents : nil;
conditionalparents : state(0) using mixgaussian mapping(   state2obs   );

}

}

jeff  a. bilmes@

id114

variable : s {

gmtk switching structure
type : discrete hidden cardinality 100;
switchingparents : nil;
conditionalparents : nil using densecpt(   pi   );

}
variable : m1 {...}
variable : f1 {...}
variable : m2 {...}
variable : f2 {...}
variable : c {

m1
m1

f1
f1

s

c

m2

f2

type : discrete hidden cardinality 30;
switchingparents : s(0) using mapping(   s-mapping   );
conditionalparents : 

m1(0),f1(0) using densecpt(   m1f1   )
| m2(0),f2(0) using densecpt(   m2f2   );

}

jeff  a. bilmes@

id114

decision-tree implementation 

of discrete dependencies

x1

x2

q1(x1)=t

q1

q1(x1)=f

q2

q5

q3

q6

q4

q7

pa(x2)

pb(x2)

pc(x2)

pd(x2)

jeff  a. bilmes@

id114

gaussians and directed models

i b d i b
   

)'

   

(

k u du

=

'

x
1
x
2
x
3
x
4

   
   
   
   
   
   
   

   
   
   
   
   
   
   

=

0
0
0
0

   
   
   
   
   
   

b
12
0
0
0

=

(

b
13
b
23
0
0

b
14
b
24
b
34
0

   
   
   
   
   
   

x
1
x
2
x
3
x
4

   
   
   
   
   
   
   

   
   
   
   
   
   
   

+

  
   
1
   
  
   
2
   
  
3
   
  
   
   
4

   
   
   
   
   
   
   

)
   a gaussian can be viewed 
as a directed graphical model
    fsicms, obtained via 
u   du factorization, provides 
the edge coefficients

x1

x2

x3

x4

f x
( )

=    

i

f x x  
(
i

|

i

)

jeff  a. bilmes@

id114

gmtk splitting/vanishing 

algorithm

    determines number gaussian components/state
   

split gaussian if it   s component id203 
(   responsibility   )  rises above a number-of-
components dependent threshold

    vanish gaussian if it   s component id203 
falls below a number-of-components dependent 
threshold

    use a splitting/vanishing schedule, one set of 

thresholds per each em training iteration.

jeff  a. bilmes@

id114

gmtk sharing & em/gem training

    in gmtk, gaussians are viewed as directed 

id114.

    gmtk supports arbitrary parameter 

sharing: 
    any gaussian can share its mean, variance d, 

and/or its (sparse) b matrix with others.

    normal em training leads to a circularity
    gmtk training uses a gem algorithm

(
*
  

,

*

d b

,

*

)

=

argmax
d b

,
  

,

q

(
,
  

d b
;

,

o
  

,

o

d b

,

o

)

jeff  a. bilmes@

id114

exact id136 in dbns

    triangulation in dbns

    standard triangulation heuristics typically poor for dbns

since they are short and wide

    slice-by-slice triangulation via elimination: severely limit 

number of elimination orders without limiting optimal 
triangulation quality

    triangulation quality is lower-bounded by size of interface to 

previous (or next) slice

    can allow interfaces to span multiple slices, which can make 

interface quality much better (   on triangulating dynamic 
id114, uai   2003,   , bilmes & bartels).
    use message passing order in junction tree that 

respects directed deterministic dependencies when 
possible (to cut down on state space)

jeff  a. bilmes@

id114

the gmtk triangulation engine

(an anytime algorithm)

    user specifies an amount of time (2mins, 3 hours, 4 

days, 5 weeks, etc.) to spend triangulating

    user need not worry about intricacies of graph 

triangulation (user concentrates on model)

    uses a    boundary algorithm    to find chunks of dbn 

to triangulate (bilmes & bartels, uai   2003)

    many different triangulation heuristics 

implemented, all hidden from user (if she so 
desires).

jeff  a. bilmes@

id114

sparse-joins in clique structures
fast way to do:

   

when   i((cid:124)) functions very sparse and very 
large.

a,b,c

   ,a.b,c

   ,b,c,d

b,c,d

d,e

   ,d,e

jeff  a. bilmes@

a,b,c,d,e,f

id114

linear and island algorithm 
(log space) exact id136

    exact id136 o(t*s) space and time 
complexity, s = clique state space size

    log-space id136 o(log(t)*s) space at 

an extra cost of a factor of log(t) time.

    can use both linear and log space id136 

at same time (for optimal tradeoff).

    this is called the island algorithm (binder 

et. al. 1997)

jeff  a. bilmes@

id114

example: linear-space in id48
      
t
t
( )
(
  =
j

a b x
(
ji
t

1)

)

i

j

  
i

  
i

t
( )

=

+   
t
(

  
j

j

1)

a b x
(
ij
t

j

1
+

)

jeff  a. bilmes@

id114

example: one recursions log space
      
t
t
( )
(
  =
j

a b x
(
ji
t

1)

)

i

j

  
i

  
i

t
( )

=

+   
t
(

  
j

j

1)

a b x
(
ij
t

j

1
+

)

jeff  a. bilmes@

id114

example: two recursions log space
t
( )

1)

)

a b x
(
ji
t

i

      
t
(
  =
j

j

  
i

  
i

t
( )

=

+   
t
(

  
j

j

1)

a b x
(
ij
t

j

1
+

)

jeff  a. bilmes@

id114

gmtk is infrastructure

    gmtk does not solve speech and language 
processing problems, but provides tools to 
help to simplify testing modeling, and does 
so in novel ways.

    the space of possible solutions is quite 
large, and its exploration has only just 
started.

jeff  a. bilmes@

id114

current status
current status

i. old version (developed by jeff bilmes & geoff 

zweig) available at:
a. http://ssli.ee.washington.edu/~bilmes/gmtk
b. ~100 pages of documentation
c. book chapter on use of id114 for speech 

and language

d. jhu   2001 workshop technical report

ii. new version running, much faster and with 

many new features. end summer   04 beta 
release.

jeff  a. bilmes@

id114

other gm toolkits

    best place to look:

http://www.ai.mit.edu/~murphyk/softwa

re/bnt/bnsoft.html contains a great 
comparison of various toolboxes.

    gmtk     optimized for speech/language and 

dbns. summer   04 version will be even 
more so.

    other tools

    htk
    at&t finite state tools

jeff  a. bilmes@

id114

discussion
discussion

ideas?

    questions?
   
    confusion?
    suggestions?
    please ask.

jeff  a. bilmes@

id114

conclusions

    id114 are very flexible!!
    with right toolkit, possible to rapidly build up a 

novel statistical idea.

    space of models is still relatively unexplored, 
young research area for speech/language/nlp.

jeff  a. bilmes@

id114

the end
thank you!

jeff  a. bilmes@

id114

conditional independence

    notation:  x || y | z    
zypzxp
(
|

zyxp
,(
|

=

(

)

)

|

)

   

zyx
,{
},

    many ci properties (from lauritzen 96)

- x || y | z  => y ||  x | z
- y ||  x | z and u=h(x) => y || u|z
- y ||  x | z and u=h(x) => x || y|{z,u}
- xa || yb | z => xa    || yb    | z
where a, b sets of integers, a        a, b       b

xa = {xa1, xa2,..., xan}

jeff  a. bilmes@

id114

