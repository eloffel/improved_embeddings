id35s

for robust natural language processing

mark steedman

with jason baldridge, c  em boszahin, ruken c  ak  c  , stephen clark,

james curran, julia hockenmaier, tom kwiatkowski, mike lewis, jong park

emily thomforde, prachya boonkwan, greg copolla, sharon goldwater,

luke zettlemoyer and others

june 2012

steedman

nasslli, austin tx

june 2012

prospectus

1

    prologue: why use id35 for nlp?
    i: id35
    ii: wide coverage parsing with id35
    iii: the statistical problem of id146
    iv: towards a robust semantics
    v: the surface id152 of

intonation and information

structure

steedman

nasslli, austin tx

june 2012

prologue: why use id35 for nlp?

2

steedman

nasslli, austin tx

june 2012

the long tail and the uncanny valley

3

    zipf   s law:

    the uncanny valley:

ignoring the long tail can engender the uncanny:

steedman

nasslli, austin tx

june 2012

frequencyfrequency ranklikingfrequency rankzin the uncanny valley

4

    trec 2005:

q77.6 name opponents who foreman defeated.
q77.7 name opponents who defeated foreman.

    a qa program (kor 2005):

opponents who
foreman defeated:
george foreman
joe frazier
ken norton
sonny
archie moore

opponents who
defeated foreman:
george foreman
joe frazier
ken norton
sonny
archie moore

steedman

nasslli, austin tx

june 2012

the problem

5

    the contribution of certain constructions to determining system acceptability

is disproportionate to their low frequency.

    this is bad news.

machine learning is very bad at acquiring systems for which crucial information
is in rare events.

steedman

nasslli, austin tx

june 2012

zthe darkling plain

6

if the distribution of event types really is a power law curve, then there is no
other side to the uncanny valley accessible to brute-force machine learning.

    we shall see that, for certain categories of parser error, up to half the error
rate is due to unseen grammatical event types (such as lexical entries), and
up to half is due to unseen model tokens for seen types (such as head word
dependencies).

    so the long tail is already hurting us badly.

steedman

nasslli, austin tx

june 2012

zranklikingwhat to do

7

    the distribution of grammatical event types isn   t a true power law, because
there is a    nite number of them, de   ned generatively, ultimately by a universal
semantics.

    in principle, we can enumerate the types.

but there are more constructions than you can shake a stick at (goldberg
1995)

    induce them from labeled data. (or get linguists to enumerate them).
    if we knew what that semantics was, we might be able to solve the model

problem as well.

but none of the existing logicist semantic formalisms will do (maccartney and
manning 2007).

steedman

nasslli, austin tx

june 2012

zzhow to do it

8

    we need a readily extensible, construction-based grammar.
    it must be robustly and e   ciently parsable with wide coverage
    it must be transparent to a    natural    semantics, supporting cheap id136.

steedman

nasslli, austin tx

june 2012

i: id35

9

steedman

nasslli, austin tx

june 2012

categorial grammar

10

    categorial grammar replaces ps rules by lexical categories and general

combinatory rules (radical lexicalization):
(1) s     np vp
vp     tv np
tv     {proved,    nds, . . .}

    categories:

(2) proved := (s\np)/np
(3) think := (s\np)/(cid:5)s

steedman

nasslli, austin tx

june 2012

categorial grammar

11

    categorial grammar replaces ps rules by lexical categories and general

combinatory rules (lexicalization):
(1) s     np vp
vp     tv np
tv     {proved,    nds, . . .}

    categories with semantic intepretations:

(2) proved := (s\np)/np : prove0
(3) think := (s\np)/(cid:5)s : think0

steedman

nasslli, austin tx

june 2012

applicative cg derivation = id18

12

    (4) marcel

np

proved
(s\np)/np
s\np

s

completeness

np

>

<

(5)

i
np

think
(s\np)/(cid:5)s

marcel

np

s\np

s

proved
(s\np)/np
s\np

completeness

np

>

<

>

s

<

steedman

nasslli, austin tx

june 2012

applicative cg derivation = id18

13

    (4) marcel

proved

completeness

np : marcel0 (s\np)/np : prove0 np : completeness0

s\np :    y.prove0completeness0y

s : prove0completeness0marcel0

>

<

(5)

i

think

marcel

np : i0 (s\np)/(cid:5)s : think0 np : marcel0

proved

completeness

(s\np)/np : prove0 np : completeness0

s\np :    y.prove0completeness0y

>

<

s : prove0completeness0marcel0

s\np : think0(prove0completeness0marcel0)

>

s : think0(prove0completeness0marcel0)i0

<

steedman

nasslli, austin tx

june 2012

14

id35 (id35)

    combinatory rules:

application :

composition :

crossed composition :

x/?y
x
x/(cid:5)y
x/(cid:5)z
x/  y
x\  z

y

>

y /(cid:5)z
y\  z

>b

>b  

x\ ?y

<
x\ (cid:5)y
x\  y

y
x
y\ (cid:5)z
x\ (cid:5)z
y /  z
x/  z

    all arguments are type-raised in the (morpho) lexicon:

type raising :

x
t/(t\x)

>t

x
t\(t/x)

<b

<b  

<t

    we omit a further family of rules based on the combinator s

steedman

nasslli, austin tx

june 2012

id35 (id35)

15

    combinatory rules:

x/?y : f y : g

application :

composition :

x: f (g) >
x/(cid:5)y : f y /(cid:5)z: g
x/(cid:5)z:    z. f (g(z)) >b
x/  y : f y\  z: g
x\  z:    z. f (g(z)) >b  
    all arguments are type-raised in the (morpho) lexicon:

crossed composition :

type raising :

x: x

t/(t\x):    f . f (x) >t

x: f (g)

y : g x\ ?y : f
<
y\ (cid:5)z: g x\ (cid:5)y : f
x\ (cid:5)z:    z. f (g(z)) <b
y /  z: g x\  y : f
x/  z:    z. f (g(z)) <b  

x: x

t\(t/x):    f . f (x) <t

    we omit a further family of rules based on the combinator s

steedman

nasslli, austin tx

june 2012

slash typing

16

    the features ?,(cid:5),   were introduced by baldridge 2002 following hepple (1987)
    they form a lattice

figure 1: id35 type hierarchy for slash modalities (baldridge and kruij    2003)
       type written as bare slash e.g.   /   means any rule can apply
    (cid:5) type e.g.   /(cid:5)   means any rule except    can apply.
       type e.g.   /     means any rule except (cid:5) can apply.
    ? type e.g.   /?   means no rule except ? can apply.

steedman

nasslli, austin tx

june 2012

   .combinatory derivation

(6)

marcel

np
s/(s\np)

s/np

>t

s

proved
(s\np)/np

completeness

np
s\(s/np)

>b

17

<t

<

(7)

marcel

np
s/(s\np)
>t

proved
(s\np)/np

completeness

np
(s\np)\((s\np)/np)
<t

s\np

s

<

>

steedman

nasslli, austin tx

june 2012

combinatory derivation

18

(6)

marcel

np : marcel0

proved

(s\np)/np : prove0

s/(s\np) :    f .f marcel0

>t

completeness

np : completeness0

s\(s/np) :    p.p completeness0

<t

s/np :    x.prove0x marcel0

>b

s : prove0completeness0marcel0

<

(7)

marcel

np : marcel0
s/(s\np)
>t
:    f .f marcel0

proved

(s\np)/np : prove0

completeness

np : completeness0
(s\np)\((s\np)/np)
<t
:    p.p completeness0

s\np :    y.prove0completeness0y

s : prove0completeness0marcel0

<

>

steedman

nasslli, austin tx

june 2012

case as type-raising

19

    the type-raising combinator t is directly related to case systems, as in latin:

(8) a.balbus ambulat. b.livia balbum amat.
   livia loves balbus.   

   balbus walks.   

c.livia balbo murum dabit.
   livia gave balbus a wall.   

    this involves the following fragment of latin lexicon:

(9)

balb+us
balb+um := s/(s/np)   p(e,(e,t))   y.p balbus0y

:= s/(s\np) :    p(e,t).p balbus0
:= (s\np)/((s\np)/np) :    p(e,(e,t))   y.p balbus0y
:= ((s\np)/np)/(((s\np)/np)/np) :    p(e,(e,(e,t)))   y   z.p balbus0yz
    even english possesses a case system in this sense. it   s just very ambiguous.
    we will sometimes schematize cased/raised forms as np   

balb+o

&c.

                                       

                                       

steedman

nasslli, austin tx

june 2012

linguistic predictions: unbounded    movement   
    the combination of type-raising and composition allows derivation to project
lexical function-argument relations onto    unbounded    constructions such as
relative clauses and coordinate structures, without transformational rules:

20

(10)

a

arrived
(s/(s\np))/n n (n\n)/(s/np) s/(s\np) (s\np)/(cid:5)s s/(s\np) (s\np)/np s\np

think

man

who

you

like

i

s/(cid:5)s

n\n

<

n
s/(s\np)

>

>b

s/np

>

s/np

>b

>b

s

>

steedman

nasslli, austin tx

june 2012

linguistic predictions: constraints on    movement   
    we also predict the following assymetry without stipulation, since we can allow

a without allowing b, but can   t allow b without also getting c:

21

(11) a.

a man who(m) [i think that]s/s [keats likes]s/np
b. *a man who(m) [i think that]s/s [likes keats]s\np
c. *[i think]s/s chapman [that]s/s [likes keats]s\np.

    in transformational (mp) terms, id35 reduces move to merge.

steedman

nasslli, austin tx

june 2012

predictions: english intonation

22

    a minimal pair of contexts and contours:

(12) q: i know who proved soundness. but who proved completeness?

a: (marcel)(proved completeness).
lh%

l+h*

h*l

(13) q: i know which result marcel predicted. but which result did marcel

prove?

a: (marcel proved )( completeness).
ll%

l+h* lh%

h*

    crossing contexts and responses yields complete incoherence.
    in transformational (mp) terms, id35 reduces intonational phrases to surface

constituents.

steedman

nasslli, austin tx

june 2012

predictions: argument-cluster coordination

    the following construction is predicted on arguments of symmetry.

23

(14) give a teacher an apple
<t

dtv tv\dtv vp\tv (x\?x)/?x tv\dtv

and

a policeman a    ower
vp\tv
<t
<b

<t

<b

<t
vp\dtv

vp\dtv
(vp\dtv)\?(vp\dtv)

>

<

vp\dtv

vp

<

   where vp = s\np; tv = (s\np)/np; dtv = ((s\np)/np)/np.

    a variant like the following cannot occur in an svo language like english:

(15) *a policeman a    ower and give a teacher an apple.

steedman

nasslli, austin tx

june 2012

24

syntax = type-raising and composition

    id35s combination of type-raising and composition yields a permuting and

rebracketing string calculus closely tuned to the needs of natural grammar.

    the argument cluster coordination construction (14) is an example of a
universal tendency for    deletion under coordination    to respect basic word
order: in all languages, if arguments are on the left of the verb then argument
clusters coordinate on the left, if arguments are to the right of the verb then
argument clusters coordinate to the right of the verb (ross 1970):

(16) svo: *so and svo svo and so
vso:*so and vso vso and so
sov: so and sov *sov and so

    in transformational (mp) terms, id35 reduces copy/delete to merge.

steedman

nasslli, austin tx

june 2012

these things are out there in the treebank

25

    full object relatives ( 570 in wsj treebank)
    reduced object relatives ( 1070 in wsj treebank)
    argument cluster coordination ( 230 in wsj treebank):

(s (np-sbj it)

(vp (md could)

(vp (vp (vb cost)

(np-1 taxpayers)
(np-2 $ 15 million))

(cc and)
(vp (np=1 bpc residents)

(np=2 $ 1 million)))))

    it could cost taxpayers 15 million and

bpc residents 1 million

steedman

nasslli, austin tx

june 2012

these things are out there (contd.)

    parasitic gaps (at least 6 in wsj treebank):

(s (np-sbj hong kong   s uneasy relationship with china)

26

(vp (md will)

(vp (vp (vb constrain)

(np (-none- *rnr*-1)))

(prn (: --)

(in though)
(vp (rb not)

(vb inhibit)
(np (-none- *rnr*-1)))

(: --))

(np-1 long-term economic growth))))

    hk   s relation with c will constrain , though not inhibit , long-term growth.

steedman

nasslli, austin tx

june 2012

a trans-context free natural language

    id35 can capture unboundedly crossed dependencies in dutch and zurich

german (examples from shieber 1985):

27

steedman

nasslli, austin tx

june 2012

... das        mer         em hans             es huus        haelfed  aastriiche  ... that   we.nom   hans.dat  the house.acc   helped       paint     ... that  we helped hans paint the house.   ...  das        mer            d   chind                  em hans          es huus        loend   haelfe  aastriiche       ... that   we.nom  the children.acc   hans.dat   the house.acc      let      help       paint     ... that we let the children help hans paint the house.   a trans context-free id35 analysis

28

(17) das

mer

em hans

es huus

that we   nom hans   dat the house   acc

np   

nom

np   

dat

np   

acc

aastriiche

h  alfed
helped

paint
((s+sub\npnom)\npdat)/  vp vp\npacc
>b  

((s+sub\npnom)\npdat)\npacc

>

(s+sub\npnom)\npdat

s+sub\npnom

>

>

s+sub

   that we helped hans paint the house   

    the following alternative word order is correctly also allowed:

(18) das mer em hans h  alfed es huus aastriiche.

the corresponding word order is disallowed in the related dutch construction.

steedman

nasslli, austin tx

june 2012

za trans context-free id35 analysis

29

(18) das

that we   nom the children   acc hans   dat the house   acc

em hans

es huus

mer

np   

nom

d0chind

np   

acc

np   

dat

np   

acc

l  ond
let

h  alfe
help

aastriiche

paint
((s+sub\npnom)\npacc)/  vp (vp\npdat)/  vp vp\npacc

(((s+sub\npnom)\npacc)\npdat)/  vp

(((s+sub\npnom)\npacc)\npdat)\npacc

>b  

>b2  

>

((s+sub\npnom)\npacc)\npdat

>

(s+sub\npnom)\npacc

>

s+sub\npnom

>

s+sub

   that we let the children help hans paint the house   

    again, other word orders are correctly allowed.

steedman

nasslli, austin tx

june 2012

on so-called    spurious    ambiguity

30

    examples like (80), and (14), embody the claim that fragments like    marcel
proved   , and    a policeman a    ower   , are constituents, comparable to    proved
completeness   .

    if    marcel proved    can be constituent in right node raising, then it can be a

constituent of a canonical transitive sentence.

    even such simple sentences are derivationally ambiguous:

steedman

nasslli, austin tx

june 2012

s : prove   completeness   marcel   a.  marcel  proved   completeness                               b.  marcel  proved  completenesss : prove   completeness   marcel   on so-called    spurious    ambiguity (contd.)

    more complex sentences are multiply ambiguous:

31

    this has been referred to (misleadingly) as    spurious    ambiguity, since all the

derivations have the same interpretation   .

    interestingly, so called    spurious    constituents include many left pre   xes.
    left pre   xes are relevant to id38 for purposes like asr.

steedman

nasslli, austin tx

june 2012

s:  s:  s:  frankie thinks that anna married      manny.frankie thinks that anna married      manny.frankie thinks that anna married      manny.a.b.c.32

parsing in the face of    spurious ambiguity   

    all grammars exhibit derivational ambiguity   even id18.
    any grammar that captures coordination at all will have the same derivational

ambiguity as id35.

    use standard table-driven parsing methods such as cky, with packed charts,

either:

    checking identity of underlying representation of table entries (steedman

2000b), rather than identity of derivation, or:

    parsing normal-form derivations (eisner 1996; hockenmaier and bisk 2010)

steedman

nasslli, austin tx

june 2012

id35 is    nearly context-free   

33

    id35 and tag are provably weakly equivalent to linear indexed grammar

(lig) vijay-shanker and weir (1994).

    hence they are not merely    mildly context sensitive    (joshi 1988), but rather

   nearly context free,    or    type 1.  9    in the extended id154.

language type

automaton

type 0: re
type 1: cs

i

lcfrs (mcf)
li/id35/tag
type 2: cf

universal turing machine

linear bound automaton (lba)
nested stack automaton(nsa)

ith-order epda

embedded pda (epda)

push-down automaton (pda)

type 3: fs

finite-state automaton (fsa)

rule-types
         

  a             
a[(i),...]       b[(i),...]  c[(i),...]  
a[(i),...]       b[(i),...]  

a[[(i),...]...]       b[[(i),...]...]  

a       

a    na b

a

exemplar

p(anbncn) (?)

a2n

anbncn . . .mn

anbncn
anbn
an

steedman

nasslli, austin tx

june 2012

processing

34

    id35 was widely expected to be completely useless for processing, because of

so-called    spurious    derivational ambiguity

    however, any theory that covers the range of grammatical phenomena covered

by id35 has the same ambiguity.

    moreover, everyone has to use a probabilistic parsing model to limit search

arising from standard ambiguity, so who cares about a little more?

    near context free power guarantees polynomial parsability hence applicability

of standard algorithms.

    id35 supports one of the fastest and most accurate wide coverage parsers
(clark and curran 2004; auli and lopez 2011) (with the bonus of capturing
unbounded dependencies.)

steedman

nasslli, austin tx

june 2012

35

cky

(19) 1. for j := 1 to n do

begin
t( j, j) := {a|a is a lexical category for a j}

2. for i := j    1 down to 0 do

begin

3. for k := i down to 0 do

begin
t(k, j) := pack{a|for all b     t(k,i),c     t(i + 1, j)
such that b c     a for some
combinatory rule in r
and admissible(b c     a)}

end
end

end

steedman

nasslli, austin tx

june 2012

cky

36

    the procedure pack packs all categories a in the chart entry t(k, j) with the
same syntactic type   a but di   erent logical forms   a into a single disjunctive
structure-sharing entry

    the boolean function admissible stands for one of a number of possible
conditions on the inclusion of a in the chart entry t(k, j) that are necessary to
keep the algorithm polynomial.

    the simplest admissibility condition is that a not already be in t(k, j). others

are normal form conditions (eisner 1996; hockenmaier and bisk 2010).

    for real-life sized examples like parsing the newspaper, such algorithms must

be statistically optimized.

steedman

nasslli, austin tx

june 2012

ii: wide-coverage parsing with id35

37

steedman

nasslli, austin tx

june 2012

human and computational nlp

38

    no handwritten grammar ever has the coverage that is needed to read the

daily newspaper.

    language is syntactically highly ambiguous and it is hard to pick the best parse.
quite ordinary sentences of the kind you read every day routinely turn out to
have hundreds and on occasion thousands of parses, albeit mostly semantically
wildly implausible ones.

    high ambiguity and long sentences break exhaustive parsers.

steedman

nasslli, austin tx

june 2012

for example:

39

       in a general way such speculation is epistemologically relevant, as suggesting
how organisms maturing and evolving in the physical environment we know
might conceivably end up discoursing of abstract objects as we do.    (quine
1960:123).

       yields the following (from abney 1996), among many other horrors:

steedman

nasslli, austin tx

june 2012

sppapabsolutevpin the physical envirmnmentnp such speculation is                                   as suggesting howmight       ap                  ptcpl                 objects as we donp                                                      vpin a general way  rc            epistemologically relevant  pp           organisms maturing and evolving     we     know                                           sconceivably end up   discoursing of abstractthe anatomy of a parser

40

    every parser can be identi   ed by three elements:

    a grammar (regular, context free, linear indexed, etc.) and an associated

automaton (finite state, push-down, nested push-down, etc.);

    a search algorithm characterized as left-to-right (etc.), bottom-up (etc.),

and the associated working memories (etc.);

    a model, to resolve ambiguity.

    the model can be used in two ways, either to actively limit the search space,

or in the case of an    all paths    parser, to rank the results.
    in wide coverage parsing, we mostly use it the former way.

steedman

nasslli, austin tx

june 2012

competence and performance

41

    linguists (chomsky 1957, passim), have always insisted on the methodological
independence of    competence    (the grammar that linguists study) and
   performance    (the mechanisms of language use).

    this makes sense: there are many more parsers than there are grammars.
    nevertheless, competence and performance must have evolved as a single
package, for what evolutionary edge does a parser without a grammar have, or
a grammar without a parser?

any theory that does not allow a one-to-one relation between the grammatical
and derivational constituency has some explaining to do.

steedman

nasslli, austin tx

june 2012

zhuman sentence processing

42

       garden path    sentences are sentences which are grammatical, but which naive

subjects fail to parse.

    example (20a) is a garden path sentence, because the ambiguous word    sent   

is analysed as a tensed verb:

(20) a. # the doctor sent for the patient died.
b. the    owers sent for the patient died.

    however (20b) is not a garden path.
    so garden path e   ects are sensitive to world knowledge (bever 1970).
    they are even sensitive to referential context: (altmann and steedman 1988)
showed that (simplifying somewhat) if a context is established with two doctors,
one of whom was sent for a patient, then the garden path e   ect is reversed.

steedman

nasslli, austin tx

june 2012

the architecture of the human sentence

processor

43

    this requires a    cascade    architecture:

steedman

nasslli, austin tx

june 2012

}.id103parsing modelthe situationsyntax & semanticsyes?yes?yes?{the  flowers  sent for the patient dieddoctorprobably!/forget it!probably!/forget it!probably!/forget it!grammar and incrementality

44

    many left pre   x substrings of sentences are typable constituents in id35, for

which alternative analyses can be compared using the parsing model

    the fact that (21a,b) involve the nonstandard constituent [the doctor sent

for]s/np, means that constituent is also available for (21c,d)

(21) a. the patient that [the doctor sent for]s/np died.

b. [the doctor sent for]s/np and [the nurse attended]s/np the patient who had complained of a pain.

c. #[the doctor sent for](
[the    owers sent for](

d.

s/np
(s/(s\np))/n
#s/np
(s/(s\np))/n

n

n

(n\n)/np

(n\n)/np

) [the patient]np dieds\np.
) [the patient]np dieds\np.

    (22) a. #[the doctor sent for the patient] s dieds\np.

b.

[the    owers sent for the patient] s/(s\np) dieds\np.

steedman

nasslli, austin tx

june 2012

45

the strict competence hypothesis

    since the spurious constitutent [#the    owers sent for]s/np is available in the
chart, so that its low id203 in comparison with the probabilities of the
unreduced components can be detected (according to some       gure of merit   
(charniak et al. 1998) discounting the future), the garden path in (20b) is
avoided, even under the following very strong assumption about the parser:

    the strict competence hypothesis: the parser only builds structures that

are licensed by the competence grammar as typable constituents.

    this is an attractive hypothesis, because it allows the competence grammar
and the performance parser/generator to evolve as a package deal, with parsing
completely transparent to grammar, as in standard bottom-up algorithms.

    but is such a simple parser possible? we need to look at some real-life parsing

programs.

steedman

nasslli, austin tx

june 2012

46

wide coverage parsing: the state of the art

    early attempts to model parse id203 simply by attaching probabilities to

rules of id18 performed poorly (bad independence assumption).

    great progress as measured by the parseval measure has been made by
combining statistical models of headword dependencies with cf grammar-
based parsing (hindle and rooth 1993; collins 1997; charniak 2000)

    however, the parseval measure is very forgiving. such parsers have until now
been based on highly overgenerating context-free covering grammars. analyses
depart in important respects from interpretable structures.

    in particular, they typically fail to represent the long-range    deep    semantic

dependencies that are involved in relative and coordinate constructions.

steedman

nasslli, austin tx

june 2012

head-dependencies as model

47

    head-dependency-based statistical parser optimization works because it

approximates an oracle using real-world knowledge.

    in fact, the knowledge- and context- based psychological oracle may be
much more like a probabilistic relational model augmented with associative
epistemological tools such as typologies and thesauri and associated with a
dynamic context model than like traditional logicist semantics and inferential
systems.

    many context-free processing techniques generalize to the    mildly context

sensitive    grammars.

    the    nearly context free    grammars such as ltag and id35   the least
expressive generalization of id18 known   have been treated by xia (1999),
hockenmaier and steedman (2002a), and clark and curran (2004).

steedman

nasslli, austin tx

june 2012

nearly context-free grammar

48

and long range dependency.

    such grammars capture the deep dependencies associated with coordination
    both phenomena are frequent in corpora, and are explicitly annotated in the
    standard treebank grammars ignore this information and fail to capture these

penn wsj corpus.

phenomena entirely.
zipf   s law says using it won   t give us much better overall numbers. (around
3% of sentences in wsj include long-range object dependencies, but those
dependencies are only a small proportion of the dependencies in those
sentences.)
    but there is a big di   erence between getting a perfect eval-b score on a

sentence including an object relative clause and interpreting it!

steedman

nasslli, austin tx

june 2012

zsupervised id35 induction by machine

    extract a id35 lexicon from the id32: hockenmaier and steedman
(2002a), hockenmaier (2003) (cf. buszkowski and penn 1990; xia 1999).

49

    this trades lexical types (500 against 48) for rules (around 3000 instantiated
binary combinatory rule types against around 12000 ps rule types) with
standard treebank grammars.

steedman

nasslli, austin tx

june 2012

mark constituents:    heads    complements    adjunctsassign categoriesthe lexiconthe treebanksnpvpnpnpsvpnp(h)(c)(h)(c)(h)npsnps\np(s\np)/npibm boughtlotusibm boughtlotusibm boughtlotusvbdvbd ibm  :=    npbought  :=    (s\np)/nplotus  :=    npsupervised id35 induction: full algorithm

50

    foreach tree t:

preprocesstree(t);
preprocessargumentcluster(t);
determineconstituenttype(t);
makebinary(t);
percolatetraces(t);
assigncategories(t);
treatargumentclusters(t);
cuttracesandunaryrules(t);

steedman

nasslli, austin tx

june 2012

id35bank: hockenmaier and steedman 2007

51

trees

in id35-bank are

the
(rightward-branching normalized) id35
derivations, and in cases like argument cluster coordination and relativisation
they depart radically from id32 structures.

    the resulting treebank is somewhat cleaner and more consistent, and is o   ered
for use in inducing grammars in other expressive formalisms. it was released in
june 2005 by the linguistic data consortium with documentation and can be
searched using t-grep.

steedman

nasslli, austin tx

june 2012

zstatistical models for wide-coverage parsers

52

    there are two kinds of statistical models:

    generative models directly represent the probabilities of the rules of the
grammar, such as the id203 of the word eat being transitive, or of it
taking a nounphrase headed by the word integer as object.

    discriminative models compute id203 for whole parses as a function
of the product of a number of weighted features, like a id88. these
features typically include those of generative models, but can be anything.

    both have been applied to id35 parsing

steedman

nasslli, austin tx

june 2012

generative models (hockenmaier)

53

    a problem: standard generative models for the local dependencies characteristic
of id18s do not immediately generalize to the reentrant dependencies
generated by these more expressive grammars (abney 1997).

    the generative model of hockenmaier and steedman 2002b only models
id203 for collins-style local dependencies (although it can recover long
range dependencies).

    hockenmaier (2003) showed that a sound full generative model is as possible

for nearly id18s as it is for id18.

    log linear models o   er another solution (clark and curran 2003, 2004, and

see below)

steedman

nasslli, austin tx

june 2012

hockenmaier 2002/2003: overall dependency

recovery

54

    hockenmaier and steedman (2002b)

model
baseline
hwdep

lexcat
87.7
92.0

lp
72.8
81.6

parseval
bp
78.3
85.5

lr
72.4
81.9

br
77.9
85.9

surface dependencies
hphsi
81.1
84.0

hi
84.3
90.1

    collins (1999) reports 90.9% for unlabeled hi    surface    dependencies.
    id35 bene   ts greatly from word-word dependencies.

(in contrast to gildea (2001)   s observations for collins    model 1)

    this parser is available on the project webpage.

steedman

nasslli, austin tx

june 2012

long range dependencies (hockenmaier 2003)

55

    extraction:

    dependencies involving subject relative pronoun

(np\np)/(s[dcl]\np): 98.5%lp, 95.4%lr (99.6%up, 98.2%ur)
    lexical cat. for embedded subject extraction (steedman 1996b)

((s[dcl]\np)/np)/(s[dcl]\np): 100.0%p, 83.3%r

    dependencies involving object relative pronoun (including es)
(np\np)/(s[dcl]/np): 66.7%lp, 58.3%lr (76.2%up, 58.3%ur)

    coordination:

    vp coordination (coordination of s[.]\np): 67.3%p, 67.0%r
    right-node-raising (coordination of (s[.]\np)/np): 73.1%p, 79.2%r

steedman

nasslli, austin tx

june 2012

56

log-linear conditional id35 parsing models

    features fi encode evidence indicating good/bad parses
    (23) p(d|s) = 1
    use standard maximum id178 techniques to train a id122    supertagger   
clark (2002) to assign id35 categories, multitagging (n     3) at over 98%
accuracy (clark and curran 2003, 2004).

z(s)e   i   i fi(d,s)

    clark and curran use a conditional log-linear model such as maximum id178

of either:

    the derived structure or parse yield;
    all derivations;
    all derivations with eisner normal form constraints.

steedman

nasslli, austin tx

june 2012

57

conditional id35 parsing models (contd.)

    discriminative estimation via the limited-memory bfgs algorithm is used to

set feature weights

    estimation is computationally expensive, particularly for    all derivations   :

    beowulf cluster allows complete id32 to be used for estimation.
    the fact that the supertagger is very accurate makes this possible.

steedman

nasslli, austin tx

june 2012

58

overall dependency recovery

clark et al. 2002
hockenmaier 2003
clark and curran 2004
hockenmaier (pos)
c&c (pos)

lp
81.9
84.3
86.6
83.1
84.8

lr
81.8
84.6
86.3
83.5
84.5

up
90.1
91.8
92.5
91.1
91.4

ur
89.9
92.2
92.1
91.5
91.0

cat
90.3
92.2
93.6
91.5
92.5

table 1: dependency evaluation on section 00 of the id32

    to maintain comparability to collins, hockenmaier (2003) did not use a
supertagger, and was forced to use beam-search. with a supertagger front-
end, the generative model might well do as well as the log-linear model. we
have yet to try this experiment.

steedman

nasslli, austin tx

june 2012

log-linear overall dependency recovery

59

    the c&c parser has state-of-the-art dependency recovery.
    the c&c parser is very fast (    30 sentences per second)
    the speed comes from highly accurate id55 which is used in an
aggressive    best-first increasing    mode (clark and curran 2004), and behaves
as an    almost parser    (bangalore and joshi 1999)

    clark and curran 2006 show that id35 all-paths almost-parsing with
supertagger-assigned categories loses only 1.3% dependency-recovery f-score
against parsing with a full dependency model

    c&c has been ported to the trec qa task (clark et al. 2004) using a
hand-supertagged question corpus, and applied to the entailment qa task
(bos et al. 2004), using automatically built logical forms.

steedman

nasslli, austin tx

june 2012

recovering deep or semantic dependencies

60

clark et al. (2004)

lexical item

which
which
which
had
had

category

(npx\npx,1)/(sdcl2/npx)
(npx\npx,1)/(sdcl2/npx)
(npx\npx,1)/(sdcl2/npx)

(sdclhad\np1)/np2)
(sdclhad\np1)/np2)

slot
2
1
1
2
2

head of arg

had

con   dence

respect

con   dence

respect

steedman

nasslli, austin tx

june 2012

respect  and  confidence     which     most      americans    previously           had61

full object relatives in section 00

    431 sentences in wsj 2-21, 20 sentences (24 object dependencies) in

section 00.

1. commonwealth edison now faces an additional court-ordered refund on its summerwinter rate di   erential collections that the illinois
appellate court has estimated at dollars.
2. mrs. hills said many of the 25 countries that she placed under varying degrees of scrutiny have made genuine progress on this
touchy issue.
x 3. it   s the petulant complaint of an impudent american whom sony hosted for a year while he was on a luce fellowship in tokyo    
to the regret of both parties.
x 4. it said the man, whom it did not name, had been found to have the disease after hospital tests.
5. democratic lt. gov. douglas wilder opened his gubernatorial battle with republican marshall coleman with an abortion
commercial produced by frank greer that analysts of every political persuasion agree was a tour de force.
6. against a shot of monticello superimposed on an american    ag, an announcer talks about the strong tradition of freedom and
individual liberty that virginians have nurtured for generations.
x 7.
that southeast asian politicians have pursued in    ts and starts for decades.
8. another was nancy yeargin, who came to greenville in 1985, full of the energy and ambitions that reformers wanted to reward.
9. mostly, she says, she wanted to prevent the damage to self-esteem that her low-ability students would su   er from doing badly on
the test.
x 10. mrs. ward says that when the cheating was discovered, she wanted to avoid the morale-damaging public disclosure that a trial
would bring.

interviews with analysts and business people in the u.s. suggest that japanese capital may produce the economic cooperation

steedman

nasslli, austin tx

june 2012

62

in cat sections where students    knowledge of two-letter consonant sounds is tested, the authors noted that scoring high

interpublic group said its television programming operations     which it expanded earlier this year     agreed to supply more than

interpublic is providing the programming in return for advertising time, which it said will be valued at more than dollars in

x 11.
concentrated on the same sounds that the test does     to the exclusion of other sounds that    fth graders should know.
x 12.
4,000 hours of original programming across europe in 1990.
13.
1990 and dollars in 1991.
x 14. mr. sherwood speculated that the leeway that sea containers has means that temple would have to substantially increase their
bid if they   re going to top us.
x 15. the japanese companies bankroll many small u.s. companies with promising products or ideas, frequently putting their money
behind projects that commercial banks won   t touch.
x 16. in investing on the basis of future transactions, a role often performed by merchant banks, trading companies can cut through the
logjam that small-company owners often face with their local commercial banks.
17. a high-balance customer that banks pine for, she didn   t give much thought to the rates she was receiving, nor to the fees she was
paying.
x 18. the events of april through june damaged the respect and con   dence which most americans previously had for the leaders of
china.
x 19. he described the situation as an escrow problem, a timing issue, which he said was rapidly recti   ed, with no losses to customers.
x 20. but rep. marge roukema (r., n.j.) instead praised the house   s acceptance of a new youth training wage, a subminimum that
gop administrations have sought for many years.

    cases of object extraction from a relative clause in 00 associated with the object relative

pronoun category (npx\npx)/(s[dcl]/npx);

    the extracted object, relative pronoun and verb are in italics; sentences marked with a x are

cases where the parser correctly recovers all object dependencies

steedman

nasslli, austin tx

june 2012

63

clark et al. (2004): full object relatives

    24 cases of extracted object in section 00:
    15/24 (62.5%) recovered with all dependencies correct (15/20 (75%) precision)
    that is, with both noun attachment and rel pronoun-verb dependency
correct   comparable to 58.3%/67% labelled recall/precision by hockenmaier
2003 and signi   cantly better than clark et al. (2002) 42% recall

    1 sentence (1) failed to parse at all (necessary category for seen verb

estimated unseen in 2-21).

    5 were incorrect because wrong category assigned to relative pronoun, of
which: in two (5, 9) this was only because again the necessary category for
a seen verb was unseen in 2-21, and one (17) was incorrect because the
pos tagger used for back-o    labeled the entirely unseen verb incorrectly

    3 incorrect only because relative clause attached to the wrong noun

steedman

nasslli, austin tx

june 2012

64

clark et al. (2004): free relatives

    14/17 (82%) recall 14/15 (93%) precision for the single dependency.
    better performance on long-range dependencies can be expected with more

features such as id157 for max ent to work on.

    other varieties of deep dependency (control,

subject relatives,

reduced

relatives) discussed in hockenmaier (2003); clark et al. (2002, 2004).

    it looks as though about half the errors arise because the lexicon is too small,

and about half because the head-dependency model is too weak.

1m words of treebank is nothing like enough data

steedman

nasslli, austin tx

june 2012

zexperiments with porting the parser

    as with all treebank grammars, almost any practical application involves

porting the parser to a di   erent grammar and model.

    for example, in ongoing experiments with open domain id53,

65

we would like to use the parser for parsing the questions.

    however, all treebank grammars including this one do appallingly badly on the
trec question database, because wsj contains almost no direct questions,
and none at all of some common patterns.

    hand-labelling data for retraining is usually not possible.
    however, semi-automatically hand-id55 a few thousand sentences and

retraining the supertagger with those included is quite practical.

    clark et al. 2004 did the 1,171 what questions from trec in a week.

steedman

nasslli, austin tx

june 2012

porting to questions: results

66

tenfold cross-validation), average length 8.6 words.

    171 what-question development set. 1000 for training (and testing using
    since the gold standard question data is only labelled to the level of lexical
    however, supertagger accuracy and sentence accuracy correlate very highly
with dependency and category recall by the parser, and we know we need
around 97% per word and 60% per sentence for the original wsj performance

category we can only evaluate to that level.

   

model

id35bank
qs
qs+id35bank

1 cat
acc

sent
acc

72.0
92.3
93.1

1.8
66.7
61.4

1.5 cats
/word
84.8
96.6
98.1

sent
acc

11.1
80.7
86.5

table 2: accuracy of supertagger on development set question data

steedman

nasslli, austin tx

june 2012

67

porting to questions: results

   

id55/
parsing method
increasing av. cats
decreasing av. cats
increasing cats (rand)
decreasing cats (rand)
baseline

cat sent what
acc
acc acc

94.6
89.7
93.4
64.0
68.5

81.8
65.3
79.4
9.4
0.0

91.2
80.0
88.2
21.2
60.6

table 3: category accuracy of parser on dev question data

    for the what object questions, per word/sentence accuracies were 90%/71%,

suggesting that they are harder than the average question.

    object dependency recall by the parser for these questions was 78%.

steedman

nasslli, austin tx

june 2012

porting to questions: results

68

    see rimell et al. 2009; rimell 2010; nivre et al. 2010 for comparisons of masy

modern parsers on recovery rates for long-range dependencies.

steedman

nasslli, austin tx

june 2012

69

applications: id35 parsers as language models

    standard technique/baseline is trigram modeling, strikingly akin to elman   s

simply recurrent networks.

    strict left-to-right parsing interpolated with trigram model does better: chelba

and jelinek (1998); roark (2001).

    immediate-head parser modeling alone does even better, even with a non-left-

to-right algorithm: charniak (2001).

steedman

nasslli, austin tx

june 2012

id35 parsers as language models

70

    id35 type-raising treats head and complement as dual: in some sense, it makes

all constructions head    rst.

    hence many left pre   xes are constituents, even in dutch/german/japanese.
    while any grammar can in principle be mapped onto a pre   x grammar with a
generative model (jelinek and la   erty 1991), id35 already is (nearly) a pre   x
grammar and probabilities for pre   x dependency structures can be derived from
the standard dependency model.

    id35 similarly o   ers a direct way to use prosodic information (steedman

2000a). (see (79) and (80), above, and cf. charniak 2001).

steedman

nasslli, austin tx

june 2012

id35 parsers as language models

71

    for example, in dutch the pre   x dat cecilia een hond een knok . . .

(   that
cecilia a dog a bone . . .    ) has (via type-raising and composition) a category
s/(((s\np)\np)\np).

    we know this, because the cluster can coordinate (ross 1970):

(24) . . . dat

[cecilia een hond een knok]s/(((s\np)\np)\np) en [henk een politieman een

bloems/(((s\np)\np)\np) [heeft gegeven]((s\np)\np)\np.

(25) henk

een politieman

een bloem

s/(s\np) (s\np)/((s\np)\np) ((s\np)\np)/(((s\np)\np)\np)

s/((s\np)\np)

>b

s/(((s\np)\np)\np)

>b

steedman

nasslli, austin tx

june 2012

id35 parsers as language models

72

    the type of this constituents tells you how to invert the dependency model to

obtain a left-to-right prediction.

    it predicts a ditransitive verbgroup and tells you all you need to know to
estimate its arg max from verbs of that class. (for example, the    give    stem
is going to come out ahead of the    sell    stem.)

    dat een hond een knok cecilia . . .

is going to make a quite di   erent

predictions.

    type-raising preserves the head dependency model relations.
    this is the real point of type-raising.

steedman

nasslli, austin tx

june 2012

73

aside: can you do this with lambek/tlg proof

nets?

    yes (baldridge and kruij    2003)   its just like making a dcg into a pdcg

with a head-dependency model. you can even use our lexicon.

    but its not clear that its the same enterprise.
    for reasons of theoretical and computational economy, it seems very odd to
relegate word order to post-hoc linear precedence (lp) rules, as in some
linear logic and proof-net -based generalizations of the lambek calculus.

steedman

nasslli, austin tx

june 2012

74

aside: can you do this with proof nets?

    first, categories already express linear order.
    second, multiple occurrences of words so treated induce factorially many

spurious derivations:

    police police police police police.
    this is the dog that worried the cat that killed the rat that ate the malt

that lay in the house that jack built.

    so string position must be part of the resource, not extra logical.

steedman

nasslli, austin tx

june 2012

75

aside: can you do this with proof nets?

    but string positions are what makes dcgs, lambek, and all linguistic grammars
and parsers linear (in both senses of the term) in the    rst place   so is that
what we are axiomatizing?

    this seems to be what is implied by talk of    compiling    linear logic grammars

into more standard formalisms like tag.

    but to anyone who cares about actually parsing, that seems to be an odd way
of thinking about it. when we   re computing arithmetic, we don   t use peano   s
axioms, or even think of our operations as    compiled    peano arithmetic.

steedman

nasslli, austin tx

june 2012

where do we go from here?

76

    this performance is still bad by human standards.
    the main obstacle is that 1m words of annotated training data is not nearly
    there are lots of words that never occur at all in the treebank at all.

enough,

    this is a problem that smoothing can help with

    but a worse problem is words that have been seen, but not with the necessary
    the only answer to this problem is to generalize the grammar and the model,

category.

using

    active learning over unreliable parser output from unlabeled data, or
    high precision low recall methods over web-scale amounts of data.

steedman

nasslli, austin tx

june 2012

moral

77

    you can have the linguistic expressivity that is needed to build interpretable
structure and parse e   ciently with wide coverage   with an automatically
induced id35 lexicon and a statistical head-dependency model

steedman

nasslli, austin tx

june 2012

iii: the statistical problem of language

acquisition

78

    id35 (id35) and surface id152

(steedman 2012b)

    id35-based induction of    semantic parsers    for geoqueries, atis, etc.

datasets (kwiatkowski et al. 2010, 2011).

    id29 as a model of child id146.
    results from using the childes    eve    dataset for learning (kwiatkowski

et al. 2012).

    incorporating information structure.
    comparisons with fodor, yang, etc. parameter setting.

steedman

nasslli, austin tx

june 2012

79

inducing semantic parsers with id35

    thompson and mooney (2003); zettlemoyer and collins (2005, 2007); wong
and mooney (2007); lu et al. (2008); b  orschinger et al. (2011); liang
et al. (2011) generalize the problem of inducing parsers from language-speci   c
treebanks like wsj to that of inducing parsers from paired sentences and
unaligned language-independent logical forms.

    the sentences might be in any language.
    the logical

forms might be database queries,    -terms,

robot action

primitives, etc.

    this is a harder problem:

in the worst case, we would have to consider all
possible pairings of all possible substrings of the sentence with all possible
subtrees of the logical form.

steedman

nasslli, austin tx

june 2012

inducing semantic parsers: geoquery

80

most of these programs invoke (english) language-speci   c assumptions.

    kwiatkowski et al. (2010, 2011) have applied a more language-general approach
inducing multilingual grammars from the geoqueries

to the problem of
database of sentence meaning pairs (thompson and mooney, 2003):

    model

    which states border states through which the mississippi traverses?
       x.   y[state(x)    state(y)    loc(mississippi river,y)    next to(x,y)]
geoquery is all about wh-dependency
is discriminative (log-linear),

learned by batch mode inside-outside
em using stochastic id119, iterated, evaluated by 10-fold cross-
validation.
learning is accelerated by inititialization with giza++ alignment between
strings and logical forms.

steedman

nasslli, austin tx

june 2012

zzzinducing id35 semantics parsers: geoquery 250
    % of unseen test sentences parsed correctly by induced grammars:

81

english
spanish
japanese
turkish

ubl-s
81.8
81.4
83.0
71.8

   -wasp luo8
72.8
79.2
76.0
66.8

75.6
80.0
81.2
68.8

this is done without the language-speci   c engineering of the other approaches.
constraints on splits are universal (e.g. atb, a-over-a, semantic-syntactic
types mapping).
    see kwiatkowski et al. (2011) for e   ect of factored lexical generalization, and

competitive results on the much harder atis travel bookings dataset.

steedman

nasslli, austin tx

june 2012

z82

ii: child and computer language development

    the child   s problem is similar to the problem of inducing a semantic parser

(siskind 1992; villavicencio 2002, 2011; buttery 2006).

    children too have unordered logical forms in a universal language of thought,

not language-speci   c ordered wsj trees.

    so they too have to work out which words (substrings) go with which
element(s) of logical form, as well as the directionality of the syntactic
categories (which are otherwise universally determined by the semantic
types of the latter).

a word may correspond to any substructure of the logical form

steedman

nasslli, austin tx

june 2012

z83

child and computer language development

    children do not seem to have to deal with a greater amount of illformedness

than that in the penn wsj treebank (macwhinney 2005).

    but they need to learn big grammars.
    they are faced with contexts which support irrelevant logical forms.
    they need to be able to recover from temporary wrong lexical assignments.
    and they need to be able to handle serious amounts of lexical ambiguity.

steedman

nasslli, austin tx

june 2012

84

the statistical problem of id146
    the task that faces the child is to learn the categorial lexicon on the basis of
exposure to (probably ambiguous, possibly somewhat noisy) sentence-meaning
pairs, given a universal combinatory projection principle, and a mapping from
semantic types to the set of all universally available lexical syntactic types.
    once the lexicon is learned, id35 will handle unbounded projection for free.

in id35, all dependencies are projective   even so-called    non-projective    ones.

steedman

nasslli, austin tx

june 2012

zinde   nitely expandable model

85

    at the start of the learning process the child does not have access to the scope

of the    nal grammar or lexicon.

    we need to model an inde   nitely expandable set of grammar rules and lexical

items.

    this is done using dirichlet priors.
    all unseen rules and lexical items are drawn from geometric base distributions.

steedman

nasslli, austin tx

june 2012

the algorithm

86

    id58 (cf. sato 2001; ho   man et al. 2010)
    incremental two-stage expectation/maximisation algorithm
    the intuition:

    compute the probabilities of all analyses of the new sentence on the basis

of the previous model.

    update the model on the basis of weighted counts of events in the new

sentence.

steedman

nasslli, austin tx

june 2012

incremental id58 em

87

    for n string-interpretation pairs {(si,ii);i = 1 . . .n}:

1. find all derivations d that map string s to i
2. for each derivation d     d, calculate its id203:

p(d|s,i;  ) = 1

z    r   d p(r|  )

3. calculate expectation of each rule r being used in all derivations d:

e(r|s,d;  ) =    d   d count(r     d)   p(d|s,i;  )

4. update model parameters with rule expectations:

   t+1 :=    t + e(r|d;  )

steedman

nasslli, austin tx

june 2012

experiment: learning from childes data
(kwiatkowski et al. 2012 this conference)

    part of the childes corpus (   eve   ) is annotated with dependency graphs.

88

    these are english-speci   c.
    we can ignore linear order and treat them as impoverished logical forms.
    in fact we automatically map them into equivalent    -terms.

steedman

nasslli, austin tx

june 2012

put      your toys    awayobjloclimitations of childes data

89

the resulting pseudo-logical forms are still partly lexically english-speci   c.

we will learn constructions in other languages that are more synthetic than
english as multi-word items.

    milou traverse la rue `a la course! (milou runs across the road!)

(26)

milou

traverselarue`alacourse

!

s/(s\np) :    p.p milou0 s\np :    y.run0(across0road0)y

s : run0(across0road0) milou0

>

childes isn   t annotated in the language of thought accessed by the child.

steedman

nasslli, austin tx

june 2012

zzzusing childes data

90

    nevertheless, we can learn any construction in any language that is less

synthetic than english.

    ranges tes jouets! (put away your toys!)

(27)

ranges

tes jouets !

s/np :    x.put0away0x you0

np : toys0

s : put0away0toys0you0

>

steedman

nasslli, austin tx

june 2012

results

91

    following alishahi and stevenson (2008), we train on chronological sections
1    n and test on n + 1. we see steady learning for both this program and
kwiatkowski et al. (2010) (the latter with the giza++ alignment initialization
turned o    and run for 10 iterations over 1    n.)
    the present program learns around a steady 8% better than the latter state-
    even with giza alignment it is around 1% better. full results in kwiatkowski
    absolute accuracy of all systems is low because we can only learn from 33%
of eve, excluding stu    like    mmmmmm    and    doggie doggie doggie!   
    the eve corpus is also a tiny proportion of what the child has to work with,

of-the-art semantic parser inducer.

et al. (2012).

so test-on-n + 1 is very brutal.

steedman

nasslli, austin tx

june 2012

analysis: learning curves

92

    the following slides show learning curves for
1. learning that verbs are svo rather than sov, vso, vos, osv, or ovs
2. learning that determiners are det-n rather than n-det;

in each case, curves are shown for learning with

1. the correct logical form alone;
2. the correct logical form plus the logical forms from the preceding and

succeeding turn, as irrelevant distractors.

3. cases with even more distractors are discussed in kwiatkowski et al. (2012).
    in the later case learning is slower, but still converges.

steedman

nasslli, austin tx

june 2012

learning svo word order

93

steedman

nasslli, austin tx

june 2012

learning determiners a, another, any

94

steedman

nasslli, austin tx

june 2012

fast mapping

95

    frequent determiners like    a    ( f = 168) are learned slowly and continuously

with high stability,

    by the time low frequency determiners like    another    ( f = 10) and    any   
( f = 2) are actually encountered, the prior on the category np/n has grown
to the point where learning may only need a few trials.

    such    fast mapping    is characteristic of later child id146 (carey

and bartlett 1978).

steedman

nasslli, austin tx

june 2012

later development

96

    the latter e   ect is all that is needed to explain the phenomenon of    syntactic
id64    (gleitman 1990), where at a later stage of development, the
child can learn lexical entries for words for which the corresponding concept is
not salient, or is even entirely lacking to the child.

    transitive verbs could in principle be assigned either of the two syntactic
categories in (29), both of which support a derivation of a di   erent logical
form supported by the same contexts:
(28) grover    ees big bird! := s :    ee0bigbird0grover0
(29) a.    ee := (s\np)/np :    x   y. f lee0xy
b.    ee := *(s/np)\np :    x   y.chase0xy

steedman

nasslli, austin tx

june 2012

97

id64    flee    against competing    chase   
    pairs of verbs which a single situation necessarily supports are relatively rare,

and one member is usually much rarer

    there is exactly one occurrence of any form of       ee    in the entire childes
corpus, in comparison to 162 occurences of in   ected forms of the verb    chase   .
    we are therefore justi   ed in assuming that situations unambiguously supporting

the correct transitive category will predominate.

    providing everything else in the sentence is known, this should be enough to
ensure that the priors for the derivation that supports the correct category
(29a) with the nonsalient or unavailable meaning will be more probable than
that with the nonstandard category (29b) with a salient meaning.

steedman

nasslli, austin tx

june 2012

98

id64 known and unknown meanings

    thus, provided the adult   s intended meaning is available, even if with low prior
id203, then the child is in a position to assign the correct hypothesis a
high id203.

    even if it is not available, the child will assign a high id203 to the correct
lexical entry, and can productively proceed to investigate its meaning further
(thomforde and steedman 2011).

(30)    why can   t you cut ice with a. smaill?   

steedman

nasslli, austin tx

june 2012

id64

99

    gleitman 1990 has described the process by which the child resolves this
contextual ambiguity as    syntactic id64,    meaning that it is the childs
knowledge of the language-speci   c grammar, as opposed to the semantics, that
guides lexical acquisition.

    however,

in present terms syntactic id64 is emergent from the

statistical model resulting from primary semantic id64.

steedman

nasslli, austin tx

june 2012

id64

100

    like the related proposals of siskind; villavicencio; zettlemoyer and collins;
piantadosi et al. and the somewhat di   erent probabilistic approach of yang
2002, this proposal considerably simpli   es the logical problem of language
acquisition:

    no    subset principle.   
    no    triggers    other than reasonably short reasonably interpretable sentences

in context, drawn from a reasonably representative sample.

    hence no    trigger ordering problem.   
    no    parameters   

we need more datasets! (commentaries? call centers?)

steedman

nasslli, austin tx

june 2012

zconclusion

101

    the theory presented here somewhat resembles the proposals of fodor 1998 as
developed in sakas and fodor (2001) and niyogi (2006), and yang (2002) in
treating the acquisition of grammar as in some sense parsing with a universal
   supergrammar   .

    however,

rather than learning over the space of all possible grammars
corresponding to all possible parameter settings, the present theory adjusts
probabilities in a model of all elements of the grammar for which there is
positive evidence from all processable utterances.

       parameters    like v2 vs.

free order are simply statements about id203

distributions over lexical types and rule types.

    nevertheless, learning is typically step-like, like parameter-setting.

steedman

nasslli, austin tx

june 2012

moral: against    parameter setting   

102

    if parameters are implicit in the rules or categories themselves, and you can
learn the rules or categories directly, why should the child (or a truly minimal
theory) bother with parameters at all?

    for the child, all-or-none parameter-setting is counterproductive, as many

languages include inconsistent items.

    consider english expressions like doggies galore!

   galore    is one of a tiny group of phrase-   nal determiner in e. (it came from
irish. the others are    a-plenty    (norman french) and    a gogo    (also fr))

steedman

nasslli, austin tx

june 2012

ziv: towards a robust semantics

103

steedman

nasslli, austin tx

june 2012

building interpretations

104

    the combinatory rules guarantee    surface compositionality    with any

compositional semantic representation.

    thus the process of interpretation building can be built into the categories and

combinatory rules, and can be done in parallel to derivation, as in (4)

    to make such a semantics wide-coverage involves specifying a semantics or
a morphological stem-based semantic schema for the 400-500 most frequent
category types (hockenmaier et al. 2004; bos et al. 2004)

    generalize categories for open-class content words.
    use 1st order logics such as drt, using    -calculus as    glue language   .
    example (bos et al. (2004): from 1953 to 1955 , 9.8 billion kent cigarettes

with the    lters were sold , the company said .

steedman

nasslli, austin tx

june 2012

_____________

_________________________________________________________________

| x1
|
|-------------| |-----------------------------------------------------------------|

| | x2 x3

105

(| company(x1) |a| say(x2)

| | agent(x2,x1)
| single(x1)
|_____________| | theme(x2,x3)

| x4

| | x6 x7 x8

____________

________________

__________________

(| card(x4)=billion |;(| filter(x5) |a| with(x4,x5)

|
x3: |------------------|

| x5
|
|------------| |----------------|

|)
|
|
|
| proposition(x3)
|
|
|
|
|
|
|)) |
|
| plural(x5) | | sell(x6)
|
|
|
|
|____________| | patient(x6,x4) |
|
|
|
| 1953(x7)
|
|
|
| single(x7)
|
|
|
| 1955(x8)
|
|
|
| single(x8)
|
|
|
| to(x7,x8)
|
|
|
| from(x6,x7)
|
|
| event(x6)
|
|
|
|________________|
|
|
| event(x2)
|_________________________________________________________________|

|
| 9.8(x4)
|
| kent(x4)
|
| cigarette(x4)
|
| plural(x4)
|__________________|

steedman

nasslli, austin tx

june 2012

the poverty of logicism

106

    parsing with c&c 2004, and feeding such logical forms to a battery of fol
theorem provers, bos and markert (2005) attained quite high precision of 76%
on the 2nd pascal rte challenge problems.

however, recall was only 4%, due to the overwhelming search costs of full fol
theorem proving.

    maccartney and manning (2007) argue that entailment must be computed
more directly, from the surface form of sentences, using id153, and
fast id136 such as modus ponens implicit in polarity marking and resources
such as id138.

it is the latter that does the real work.

steedman

nasslli, austin tx

june 2012

zzpolarity

107

    it is well-known that explicit and implicit negation systematically switches the
   upward    or    downward direction of entailment of sentences with respect to
ontology-based id136:

(31) egon walks

egon doesn   t walk   egon doesn   t walk quickly

   egon moves
0egon walks quickly

0egon doesn   t move

    s  anchez valencia (1991) and dowty (1994) point out that polarity can be

computed surface-compositionally using cg.

steedman

nasslli, austin tx

june 2012

taking scope (steedman 2012b, hereafter ts)

108

    (32) everybody loves somebody.
    (33) a.    x[person0x        y[person0y    loves0yx]])
b.    y[person0y      x[person0x     loves0yx]

    (34) an effective silencer must be    tted to every vehicle.
    appears not to allow computation of lf from the simple combinatorics of

grammatical derivation.

has motivated    quantifying in,       covert quanti   er movement,    morpholexically
unmotivated type-changing operations, and the dreaded    underspeci   cation.   

steedman

nasslli, austin tx

june 2012

zthe problem with underspeci   cation/

movement/&c.

109

the following two sentences from the rondane treebank of mrs-based
underspeci   ed logical forms respectively generate 3,960 readings all falling
into one equivalence class, and 480 readings falling into two semantically
distinct equivalence classes (koller and thater 2006):

(35) a. for travelers going to finnmark there is a bus service from oslo to alta

b. we quickly put up the tents in the lee of a small hillside and cook for the    rst

through sweden.

time in the open.

    we should stick to surface-compositionality, using nothing but the derivational
combinatorics of surface grammar to deliver all and only the attested readings.

steedman

nasslli, austin tx

june 2012

zscope alternation: the universals

110

    the universal quanti   ers every and each can invert scope in the strong sense
of binding (unboundedly) c- or lf-commanding inde   nites, subject to certrain
island conditions.

    such quanti   er    movement    appears to be subject to the same    across-the-
board    condition as wh-movement, as in examples like the following (geach
1972):

(36) every boy admires, and every girl detests, some saxophonist.

two readings, not
underspeci   cation, et.,)

four.

(another problem for covert movement,

steedman

nasslli, austin tx

june 2012

zscope (non)alternation: the existentials

    existential quanti   ers like some, a, and at least/at most/exactly three appear
able to take wide scope over unboundedly c- or lf-commanding universals, and
are not sensitive to island boundaries.

111

however, existentials in general cannot invert scope in the strong sense of
distributing over a structurally-commanding inde   nite:

maybe existentials don   t really move at all.

steedman

nasslli, austin tx

june 2012

zz112

deriving scope from grammatical combinatorics
    existentially quanti   ed nps are replaced by a generalization of standard skolem

terms.

    skolem terms are obtained by replacing all occurrences of a given existentially
quanti   ed variable by a term applying a unique functor to all variables bound
by universal quanti   ers in whose scope the existential quanti   er falls.

    such skolem terms denote dependent    narrow-scope    inde   nite individuals.
    if there are no such universal quanti   ers, then the skolem term is a constant.
    since constants behave as if they    have scope everywhere   , such terms denote

nondependent    wide-scope    speci   c-inde   nites.

steedman

nasslli, austin tx

june 2012

generalized skolem terms

113

    we generalize the notion of skolem terms by analogy to generalized quanti   ers
by packaging the restriction p (and any associated cardinality property c)
inside the functor over arguments a , together with a number n identifying the
originating np,in a term of the form sk(a )

n: p;c,

    we can usually ignore n and c
    the ambiguity of (34) can be expressed by the following two logical forms,
person0 (denoting a dependent

which di   er only in the generalized skolem terms sk(x)
or    narrow-scope    beloved) and skperson0, a skolem constant.
(37) a.    x[person0x     loves0sk(x)

person0x)]
b.    x[person0x     loves0skperson0x)]

steedman

nasslli, austin tx

june 2012

the model theory

114

we need an explicit model theory because generalized skolem terms are    rst
class citizens of the logic, rather than being derived from existentials via prenex
normal form. they need to carry information about their scope with them, to
avoid problems arising from their interaction with negation.

(38) a. some farmer owns no donkey.

b.   iowns0   iskdonkey0+skfarmer0

because of the involvement of skolem terms and their restrictors, which are
   -terms in l, we need to identify a notion of level for terms of l. object
symbols, variables, and the related pro-terms are terms of level 0.
the model theory also treats implication as   p    (p    q), rather than material
implication, because of duplication of skolem terms in donkey sentences.

steedman

nasslli, austin tx

june 2012

zzzsyntax

115

level i.

1. if a1, . . . ,an are terms whose maximum level is i, then rn(a1, . . . ,an) is a w    of
2. if x is a w    of level i then [  x] is a w    of level i.
3. if x and y are w    for which i is the higher of their respective levels, then
[x    y ], [x    y ], and [x     y ] are all w    of level i.
4. if x is a w    of level i then [   x[x]] is a w    of level i
5. if x is a w    of level i then ska

   x.x is a term of level i + 1 where a is the set
of arguments of the skolem functor sk   x.x and a is a superset of the free
variables of x other than x.

a complete formula or sentence of l is then a w    all of whose variables are
bound.

steedman

nasslli, austin tx

june 2012

semantics: preliminaries

116

object symbols, relation symbols, and pro-terms.

arguments a (and hence none in its    -term p) as saturated.

    we refer to a generalized skolem term ska
p;c with no free variables among its
    there is a basic correspondence c0 from model objects and relations to l
    if a correspondence c includes c0, but does not map any object of m to
a particular saturated generalized skolem term t, then we will speak of a
correspondence c 0 obtained by adding to c a pair ha,ti (together with all the
related pronoun pairs ha,pro0ti, ha,pro0(pro0t)i, . . . ) for some object a   m as
an    extension of c to t    and of a as the    value    named by t in c 0. we
will refer to the set of correspondences obtained by extending c to some set
of saturated generalized skolem terms in l (including the null set) as the
   extensions    of c . (that is to say that the extensions of c include c .)

    the function c    1 on the range of a correspondence c is the inverse of c .

steedman

nasslli, austin tx

june 2012

semantics

117

1. c satis   es an atomic formula r(a1, . . . ,an) in l if and only if there is an there
is an extension c 0 of c for which the terms a1, . . . ,an are all in the range of
c 0 and:
(a) the n-tuple hc 0   1(a1), . . . , c 0   1(an)i is in the relation c 0   1(r) in m;
(b) for all ai that are skolem terms of the form ska

p;c, c 0 also satis   es p(ska
p;c)
p;c whose value under c 0 is a set
object a0, there is no correspondence c 00 di   ering from c 0 only in the value
a00 named by ska
p;c) in which a00
is a proper superset of a0;

p;c that satis   es the atomic formula and p(ska

(c) for all such skolem terms of the form ska

and c(ska

p;c);

2. given two sentences y and z in l:
(a) c satis   es a sentence   y if and only if c does not satisfy y ;

steedman

nasslli, austin tx

june 2012

118

(b) c satis   es a sentence y     z if and only if c satis   es at least one of y or z;
(c) c satis   es a sentence y     z if and only if there is an extension c 0 of c to
all and only the saturated generalized skolem terms common to y and z
that are not in the range of c such that c 0 satis   es both y and z;
(d) c satis   es a sentence y     z if and only if every extension c 0 of c to all
and only the saturated generalized skolem terms common to y and z that
are not in the range of c that satis   es y also satis   es z;

3. given a well-formed formula y (x) in l, in which x and no other variable is free:
(a) c satis   es a sentence    x[y (x)] if and only if there is an extension c 0 of c
to all saturated generalized skolem terms in y (x) such that for all object
symbols a, in l c 0 satis   es y (a).

we then de   ne truth of a sentence y in a model m as follows: y is true in m
relative to a correspondence c if and only if c satis   es y .

steedman

nasslli, austin tx

june 2012

example

119

    consider a model containing six individuals: giles, george, eliza th, pedro, mo <ine,
and maxwelton. the unary relation farmer holds for giles, george, and eliza th. the
unary relation  nkey holds for pedro, mo <ine, and maxwelton. the binary relation
own holds for the pairs {giles, pedro}, {giles, mo <ine}, and {eliza th, maxwelton}.
the binary relation feed holds for the pairs {giles, pedro}, {giles, mo <ine}, and
{eliza th, maxwelton}.

    consider the correspondence c0 consisting of the following pairs:

(39) {giles, giles0}

{maxwelton, maxwelton0}
{farmer, farmer0}
{george, george0}
{eliza th, elizabeth0} { nkey, donkey0}
{pedro, pedro0}
{own, own0}
{mo <ine, modestine0}{feed, feed0}

steedman

nasslli, austin tx

june 2012

120

    consider the donkey sentence,    x[farmer0x   own0sk(x)
donkey0)x]
    by 3a, c0 satis   es this sentence if and only if for all object symbols a in l
donkey0

there is an extension of c0 to the saturated generalized skolem term sk(a)
that satis   es farmer0a    own0sk(a)

donkey0x    feed0(pro0sk(x)

donkey0a     feed0(pro0sk(a)

donkey0)a.

    by 2d, the interesting cases are a = giles0 and a = elizabeth0, and the respective
extensions by the pairs {pedro, sk(giles0)
donkey0}, and {maxwelton,
sk(elizabeth0)
}, since for all object symbols there is either no extension that
donkey0
satis   es the antecedent or all of these extensions satisfy both antecedent and
consequent, once the skolem terms are unpacked via rule 1b.

donkey0}, {mo <ine, sk(giles0)

    this is the    strong    reading of the donkey sentence, because of 2d.

steedman

nasslli, austin tx

june 2012

related approaches

121

    drt:

    like drt with generalized skolem terms as discourse referents, except:
    quanti   es over farmers rather than farmer donkey pairs, hence no proportion

problem.

    gets the strong reading for donkey sentences, unlike duplex.

    e-type pronouns:

    no covert de   nites masquerading as pronouns, hence:
    no uniqueness problem, and hence,
    no call for minimal situations, nor ensuing rami   cations concerning bishops

meeting other bishops, split antecedents etc.

steedman

nasslli, austin tx

june 2012

related approaches (contd.)

122

    other referential accounts of inde   nites:

    unlike fodor 1982, reinhart 1987, park 1995, inde   nites have only the

non-quanti   cational reading.

    unlike storage and underspeci   cation accounts, possibilities for scope-taking

are closely tied to derivation and syntactic combinatorics.

steedman

nasslli, austin tx

june 2012

123

universals are generalized quanti   ers in id35

    the universals every and each are good old-fashioned generalized quanti   er

determiners:
(40) every, each := np   

3sg/(cid:5)n3sg :    p   q   . . .   x[px     qx . . .]

    np    schematizes over all np types raised over functions of the form t|np.

   . . . schematizes over the corresponding arguments.

this is analogous to lexicalizing covert quanti   er movement to spec-of-ip/cp.
but once again there is no movement or equivalent syntactic type-lifting, only
merge, a.k.a. uni   cation of variables

steedman

nasslli, austin tx

june 2012

zexistentials not generalized quanti   ers in id35

124

all other    quanti   ers    are referential (cf. woods 1975; vanlehn 1978; webber
1978; fodor and sag 1982; park 1996).
(41) a, an, some := np   

agr/(cid:5)nagr :    p   q.q(skolem0p)

in the present theory, existentials entirely lack quanti   cational senses.

steedman

nasslli, austin tx

june 2012

zz125

inde   nites as generalized skolem terms

    we do this by making the meaning of nps underspeci   ed skolem terms of the
n : (p;c)p, (again, p is a predicate such as donkey0, corresponding
form skolem0
to the restrictor of a generalized quanti   er, c is a cardinality condition which
may be null, and n is a number unique to the originating np which we usually
suppress.)

    we then de   ne a notion of an environment for skolem terms:

(42) the environment e of an unspeci   ed skolem term t is a tuple comprising all
variables bound by a universal quanti   er or other operator in whose structural
scope t has been brought at the time of speci   cation, by the derivation so far.

steedman

nasslli, austin tx

june 2012

126

inde   nites as generalized skolem terms
    skolem term speci   cation (simpli   ed) can then be de   ned as follows:

(43) skolem speci   cation of a term t of the form skolem0

np;c in an environment e
yields a generalized skolem term ske
n,p;c, which applies a generalized skolem
functor skn,p to the tuple e , de   ned as the environment of t at the time of
speci   cation, which constitutes the arguments of the generalized skolem term.

we will suppress the number n from now on, since it usually does no work.

there is more to say about negation and polarity here   see ts.

steedman

nasslli, austin tx

june 2012

znarrow-scope saxophonist reading

127

(44)

every boy
s/(s\np3sg)

:    p.   y[boy0y     py]

admires

some saxophonist
(s\np3sg)/np (s\np)\((s\np)/np)
:    q.q(skolem0sax0)
s\np

admire0

<

: admires0(skolem0sax0)

>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    y[boy0y     admires0(skolem0sax0)y]

s :    y[boy0y     admires0sk(y)

sax0y]

    unlike fcg/tlg, the left-branching derivation allows the same logical

form.

    that has to be the case, because of the geach sentence.

steedman

nasslli, austin tx

june 2012

wide-scope saxophonist reading

128

(45)

every boy
s/(s\np3sg)

:    p.   y[boy0y     py]

admires

some saxophonist
(s\np3sg)/np (s\np)\((s\np)/np)
:    q.q(skolem0sax0)
. . . . . . . . . . . . . . . . . . .

admires0

:    q.q sksax0

s\np : admires0sksax0

<

>

s :    y[boy0y     admires0sksax0y]

    unlike fcg/tlg, the left-branching derivation allows the same logical

form.

    that has to be the case, because of the geach sentence.

steedman

nasslli, austin tx

june 2012

how universals invert scope

129

    (46)

some boy
s/(s\np3sg)

:    p.p(skolem0boy0) :    x   y.admires0xy

admires

every saxophonist
(s\np3sg)/np (s\np)\((s\np)/np)
:    q.   x[sax0x     qx]
s\np3sg :    y.   x[sax0x     admires0xy]

<

s :    x[sax0x     admires0x(skolem0boy0)]

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

>

s :    x[sax0x     admires0x sk(x)
boy0]

    the svo grammar of english means that embedded subjects in english are
correctly predicted neither to extract nor to allow universals to take scope over
their matrix subject in examples like the following (cooper 1983, farkas 2001):

steedman

nasslli, austin tx

june 2012

non-inversion of embedded subject universals

130

    (47) a. *a boy who(m) [i know that]s/(cid:5)s [admires some saxophonist]s\np

b.

[somebody knows (that)]s/(cid:5)s [every boy]s/(s\np) [admires](s\np)/np some
saxophonist.
6=    x[boy0x     know0(admire0sksaxophonist0x)sk(x)
6=    x[boy0x     know0(admire0sk(x)
saxophonist0x)sk(x)

person0]
person0]

    this sort of thing is very common in german (kayne 1998; bayer 1990, 1996;

sp)

to allow bare complement subjects to extract a quite di   erent    antecedent
governed    category (vp/np   lex,agr)/(s\npagr) must be added to the english
lexicon for know. however, every boy cannot combine with that.

steedman

nasslli, austin tx

june 2012

z131

how universals invert scope out of np modi   ers
    (48) a. some apple in every barrel was rotten.

b. someone from every city despises it/#the dump
    cf. #a city that every person from admires sincerity.
    but also cf. a city that every person from despises

(49)

was rotten
(s/(s\np))/np :    x   p.p(skolem0   y.apple0y    in0x y) np    :    p.   x[barrel0x     px] s\np : rotten0

some apple in

every barrel

s/(s\np) :    p.   x[barrel0x     p(skolem0   y.apple0y    in0x y)]

<

>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    x[barrel0x     rotten0(skolem0   y.apple0y    in0x y)]

s :    x[barrel0x     rotten0sk(x)

   y.apple0y   in0x y]

steedman

nasslli, austin tx

june 2012

inverse scope limits readings

132

this process only supports four distinct readings for the following:

(50) a. some representative of every company saw every sample.
b. every representative of some company saw every sample.

    we will return to this example below.

steedman

nasslli, austin tx

june 2012

z133

why non universals don   t invert scope

    non-universals cannot invert scope because they are not quanti   cational:

(51) a. some linguist can program in at most two programming languages.

b. most linguists speak at least three/many/exactly    ve/no/most languages.

chierchia (1995) points out that apparent exceptions like    a canadian    ag
was hanging in front of at least    ve windows,    crucially involve unaccusatives,
passives, etc.

hirschb  uller pointed out that, exceptionally, they support inversion out of vp
ellipsis. something else is going on.

steedman

nasslli, austin tx

june 2012

zzbinding and distributivity are lexicalized

    (52) a. eat a pizza:= s\nppl :    y.eat0(skolem0pizza0)y
    binding and distributivity are lexicalized via the verb (cf. link 1983, passim):

b. eat a pizza := s\nppl :    y.   w[w     y     eat0(skolem0pizza0)w]

134

(53)

three
np   
pl/npl

boys
npl
:    n   p.p(skolem0n ;    s.|s| = 3))
: boy0
np   
pl :    p.p(skolem0boy0 ;    s.|s| = 3))
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
np   

pl :    p.p(skboy0 ;    s.|s|=3))

>

ate

(s\nppl)/np

a pizza
np   

:    x   y.   z[z     y     eat0xz] :    p.p(skolem0pizza0
s\np :    y :    z[z     y     eat0(skolem0pizza0)z]

<

>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    z[z     skboy0 ;    s.|s|=3     eat0(skolem0pizza0)z]

s :    z[z     skboy0 ;    s.|s|=3     eat0sk(z)

pizza0z]

crucially, the term on the left of the implication is not a generalized skolem
term, but a bound variable z.

steedman

nasslli, austin tx

june 2012

z135

binding and distributivity are lexicalized

    compare greenlandic eskimo and chinese, in which distributivity is explicitly
morphologically marked on the verb, and english    collective-only    verbs, as in
three boys/#every boy gathered in the library.

    localizing distributivity on the verb predicts mixed readings for the following:

(54) a. three boys ate a pizza and lifted a piano.

b. three boys gathered in the bar and ate a pizza.
c. three boys met each other and ate a pizza.

as robaldo points out, this account means that exactly two boys read exactly
one book is false in a model in which there are two boys who read the same
book, and one of them read some other book.

    i claim that result is correct for natural quanti   ers.

steedman

nasslli, austin tx

june 2012

zthe donkey

136

(55)

every

farmer
(s/(s\np3sg)/(cid:5)n3sg n3sg
:    n   p.   x[nx     px]
: farmer0

who

(nagr\(cid:5)nagr)/(cid:5)(s\npagr) (s\np3sg)/np (s\np)\(s\np)/np

feeds it
s\np3sg
:    x   y.own0xy :    p.p(skolem0donkey0) :    y.feed0it0y

:    q   n   y.ny    qy

a donkey

owns

s\np3sg

:    y.ny    own0(skolem0donkey0)y

<

n3sg\n3sg :    n   y.ny    own0(skolem0donkey0)y

n3sg :    y.farmer0y    own0(skolem0donkey0)y
s/(s\np3sg) :    p.   x[farmer0x    own0(skolem0donkey0)x     px]

>

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    x[farmer0x    own0(skolem0donkey0)x     feed0it0x]

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    x[farmer0x    own0sk(x)

donkey0x     feed0it0x]
donkey0x     feed0(pro0sk(x)

donkey0)x]

s :    x[farmer0x    own0sk(x)

>

<

>

steedman

nasslli, austin tx

june 2012

137

strength sans proportion/uniqueness problems
    (56)

farmers who own a donkey

most
pl/(cid:5)npl

np   

:    x.farmer0x    own0(skolem0donkey0)x
:    n   p.p(skolem0n ; most0)
np   
pl :    p.p(skolem0(   x.farmer0x    own0(skolem0donkey0)x ; most0))
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

>

np   

pl :    p.p(sk   x.farmer0x   own0(skolem0donkey0)x ; most0)

npl

feed it
s\nppl

:    y.   z[z     y     feeds0(pronoun0it0)z]

>
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    z[z     sk   x.farmer0x   own0(skolem0donkey0)x ; most0     feeds0(pronoun0it0)z]

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    z[z     sk   x.farmer0x   own0sk(z)
s :    z[z     sk   x.farmer0x   own0sk(z)

donkey0x ; most0     feeds0(pronoun0it0)z]
donkey0x ; most0     feeds0sk(z)
donkey0z]

    most0 =    s.|s| > 0.5   |all0(   x. f armer0x    own(skolem0donkey0))|.
    quanti   es over farmers z not farmer-donkey pairs, avoiding proportion problem.
it is the distributive universal quanti   er that ensures strong reading without
recourse to devices like    minimal situations,    avoiding uniqueness problem.

steedman

nasslli, austin tx

june 2012

coordination constraints on scope alternation

    sp showed that, by contrast with distributivity, localizing quanti   cation and

skolem terms on the np disallows mixed readings:

138

    narrow-scope saxophonist reading of (36):

(57)

every boy admires and every girl detests

s/np

:    x.   y[boy0y     admires0xy]      z[girl0z     detests0xz]
:    q.q(skolem0sax0)
s :    y[boy0y     admires0(skolem0sax0)y]      z[girl0z     detests0(skolem0sax0)z]
<
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s :    y[boy0y     admires0sk(y)

sax0y]      z[girl0z     detests0sk(z)
sax0z]

some saxophonist

s\(s/np)

steedman

nasslli, austin tx

june 2012

coordination constraints on scope alternation
    the same categories also yield the wide-scope saxophonist reading of (36):

139

(58)

every boy admires and every girl detests

some saxophonist
:    x.   y[boy0y     admires0xy]      z[girl0z     detests0xz] :    q.q(skolem0sax0)
. . . . . . . . . . . . . . .
:    q.q(sksax)

s\(s/np)

s/np

s :    y[boy0y     admires0sksaxy]      z[girl0z     detests0sksaxz]

<

there are no mixed readings.

steedman

nasslli, austin tx

june 2012

zpolarity and directional entailment

140

    (59) doesn   t    := (s   \np)/(s   
        stands for the polarity of the syntactic/semantic environment, and     stands

inf\np) :    p.   p

for       , its inverse.

    crucially, this category inverts the polarity of the predicate alone.

steedman

nasslli, austin tx

june 2012

negation, polarity, and directional entailment

    s  anchez valencia (1991) and dowty (1994) point to the natural compatibility

141

of cg and polarity.

    (60)

enoch

doesn0t
(s   \np)/(s   

enoch+ :=
s   /(s   \np+)
:    p.p +enoch0

doesn0t    :=
:    p   x.   p    x
doesn0t   walk    := s   \np :    walk0
enoch+doesn0t+walk    := s+ :    walk0+enoch0

inf\np)

>

>

walk
walk    :=
inf\np
s   
:    walk0

this engenders a number of complications to the model theory of ts, notably
that skolem terms must carry polarity, and in the case it is negative, a binding
to a particular negative operator

steedman

nasslli, austin tx

june 2012

znpis &c.

142

(61) any:=(s   /(s   \np))/n   ) :    p   q.q    (skolem0p)
(s   /(s   /np))/n   ) :    p   q.q    (skolem0p)
((s   \np)\((s   \np)/np))/n   ) :    p   q.q    (skolem0p)
&c.

steedman

nasslli, austin tx

june 2012

split scope

143

    b laszczak and g  artner (2005) point out that id35 predicts split scope readings

under such an analysis:

(62)

they asked us

s/vpto   inf

:    p.ask0(p us0)   us0   they0

to review
vp   
to   inf /np vp   
: review0
vp   

no book
to   inf\(vp   
to   inf /np)
:    p   y.  p    skbook0y
to   inf :    y.  review0   skbook0y

<

s+ : ask0(  review0   skbook0+us0)+us0+they0

>

steedman

nasslli, austin tx

june 2012

split scope (contd.)

144

(63) they
s/vp
   p.p they0 :    x   y.ask0(review0x   us0)   us0y :    p   y.  p    skbook0y

vp   \(vp   /np)

asked us to review

vp   /np

no book

vp    :    y.  ask0(review0   skbook0us0)   us0y

s+ :   ask0(review0   skbook0+us0)+us0+they0

<

>

    see ts for mit publication sentences, cross linguistic di   erences, double

negation, negtive concord, &c.

steedman

nasslli, austin tx

june 2012

processing scope in id35

145

    one might expect skolem speci   cation to induce further spurious ambiguity

(64) a representative of a company saw a sample.

    the parser will have to keep track of eight distinct underspeci   ed logical forms,
representing all possible combinations of speci   cation versus nonspeci   cation
of three generalized skolem terms.

    this ambiguity too is real, and must be allowed for in any framework. for
example, if there is a dependency-inducing universal, as follows, then all eight
interpretations are semantically distinct.

(65) every exhibitor knows that a representative of a company saw a sample.

steedman

nasslli, austin tx

june 2012

son of spurious ambiguity

146

since skolem speci   cation can happen at any point in a derivation, it might
therefore appear that there is a danger of an even greater proliferation of
semantically spurious ambiguity.

steedman

nasslli, austin tx

june 2012

zsharing structure

147

    unlike related sets of traditional logical forms using traditional quanti   ers, all

eight partially speci   ed logical forms are structurally homomorphic.

    rather than maintaining a single underspeci   ed logical form as in udrt, the
multiple speci   ed readings can instead be e   ciently stored as a single packed
shared structure, which we might visualize as follows:

(cid:26) skolem0

sk

(cid:27)

   x.(representative0x    of 0(cid:26) skolem0

(cid:27)

sk

company0 x))

(cid:26) skolem0

(cid:27)

sk

(66) saw0(

sample0)(

    since unspeci   ed terms can be eliminated at the end of derivation, this ends

up as:
(67) saw0({sk}sample0)({sk}   x.(representative0x    of 0{sk}company0 x))

steedman

nasslli, austin tx

june 2012

sharing structure

148

    the related ambiguous example (68) delivers a shared structure (69), o    which

four distinct speci   ed readings can be read directly.

(cid:20)

(68) a representative of every company saw a sample.

(cid:26) sk(y)

(cid:27)

sk

(cid:26) sk(y)

(cid:27)

sk

(69)    y

company0y     saw0(

sample0)(

   x.representative0x    of 0yx)

(cid:21)

    (cf. example (50).)

steedman

nasslli, austin tx

june 2012

sharing structure

149

    the four readings are as follows:
(70) a.    y[company0y     saw0sk(y)
sample0sk(y)
   x.representative0x   of 0yx]
b.    y[company0y     saw0sk(y)
sample0sk   x.representative0x   of 0yx]
c.    y[company0y     saw0sksample0sk(y)
   x.representative0x   of 0yx]
d.    y[company0y     saw0sksample0sk   x.representative0x   of 0yx]

steedman

nasslli, austin tx

june 2012

controlling speci   cation

150

    in order to avoid duplicating speci   ed generalized skolem terms in the logical
form, we need only to add a test for nonredundant speci   cation to the
admissibility condition admissible of the algorithm for adding a new entry a to
the chart.

    such an admissibility condition can be incorporated by comparing the
environment associated with each daughter category b, c with that of a
to determine whether skolem speci   cation could possibly a   ect the structure-
sharing logical form   a by specifying all instances of a given unspeci   ed skolem
term, say skolem0

39sample0.

    in which case, the speci   cation operation is applied to its instances, and the

result is structure shared, and we iterate.

steedman

nasslli, austin tx

june 2012

example

151

    (71) every man who read a book loves every woman.
    the result consists of two packed logical forms corresponding to the two

possible scopes of the two universals with respect to each other:

(cid:27)

(cid:26) sk(x)

sk

(72) a. s :    x[man0x    read0(

b. s :    y[woman0y        x[man0x    read0(

book0)x        y[woman0y     loves0yx]]

          sk(x,y)

sk(x)
sk

         book0)x     loves0yx]]

    each of these two packed logical forms subsumes two interpretations, one
with a wide-scope skolem constant book, and another in which books are
dependent on men. the latter generates a further reading in which books are
dependent on both men and women.

steedman

nasslli, austin tx

june 2012

representatives

152

    (73) every representative of a company saw most samples.

the result reveals only four readings, not the    ve claimed by hobbs and
shieber, and by keller, and predicted by their higher-order uni   cation-based
mechanism.

    these four readings are represented by a single packed structure, repeated here,
it is therefore immediately apparent

since there is only one true quanti   er.
that they are semantically distinct.

(74) s :    x[rep0x    of 0(

comp0)x     saw0(

(cid:27)

(cid:26) sk(x)

sk

(cid:26) sk(x)

sk

(cid:27)

(samples0;most0))x]

steedman

nasslli, austin tx

june 2012

geach sentence and packing

153

forms like the following for

in unpacking logical
the geach sentence,
which has more than one occurrence of the same generalized skolem term
skolem0
39saxophonist0, we must ensure that all instances are interpreted as the
   rst, or as the second, etc. speci   ed form.

)

(

sk(y)
39
sk39

(

)

sk(z )
39
sk39

sax0)z]

(75)    y[boy0y     admires0(

sax0)y]      z[girl0z     detests0(

    this move does not compromise the competence-theoretic account of why
there are only two readings for the geach sentence. it is simply a consequence
of the use of packing in the performance representation.

steedman

nasslli, austin tx

june 2012

zgeach sentence

154

    thus, the geach sentence ends up with just two interpretations:
o
sax0)z]

sax0)y]      z[girl0z     detests0(

(76) a.    y[boy0y     admires0(

b.    y[boy0y     admires0({sk39}sax0)y]      z[girl0z     detests0({sk39}sax0)z]

sk(y)
39

sk(z)
39

n

n

o

steedman

nasslli, austin tx

june 2012

remarks

155

    most so-called quanti   ers aren   t generalized quanti   ers.

(many languages
appear to entirely lack true generalized quanti   ers   baker 1995; bittner 1994;
aoun and li 1993).

    the account combines the advantages of both drt and e-type theories with

a movement-free syntax and semantics.

    it escapes the scylla of the proportion problem and the charybdis of the
uniqueness problem, without the involvement of category ambiguity for
existentials or minimal situations.

steedman

nasslli, austin tx

june 2012

156

what about the open-class words?

    we need to get beyond the semantics of    walks    being walks0   
    hand built resources like id138, framenet, etc. are useful but incomplete.
    we need unsupervised    machine reading    (etzioni et al. 2007; mitchell et al.

2009) over web-scale corpora.

    work by harrington and clark (2009) using c&c and spreading activation

semantic networks is interesting.
    this work has only just begun.

been road tested.

ideas of    distributed semantics    have hardly

steedman

nasslli, austin tx

june 2012

conclusion

157

    scope relations are de   ned lexically at the level of logical form, and projected
onto the sentence by combinatory derivation the pure syntactic combinatorics
of id35 is the source of all and only the grammatically available readings.

    all logical-form level constraints on scope-orderings can be dispensed with   a
result related to, but more powerful than, that of pereira 1990, as extended in
dalrymple et al. 1991, shieber et al. 1996 and dalrymple et al. 1997.

    some but not all of these results transfer to other non-tg frameworks, such

as ltag, lfg, hpsg, and recent mp and drt.

    however, the interactions of scope and coordinate structure discussed here

seem to demand the speci   c syntactic combinatorics of id35.

steedman

nasslli, austin tx

june 2012

v: the surface id152

of english intonation

158

steedman

nasslli, austin tx

june 2012

outline

159

1. the four dimensions of information-structural meaning:

(a) contrast/background
(b) theme/rheme
(c) presence in common ground
(d) speaker/hearer agency

2. the surface compositionality of intonational semantics

3. conclusion:

intonation structure = information structure = derivation

structure

steedman

nasslli, austin tx

june 2012

dimensions of information-structural meaning

160

steedman

nasslli, austin tx

june 2012

161

dimension 1: contrast/background

    accents are properties of words.
    in english (and perhaps in every language), primary accents mark words as
contributing via their interpretation to contrast between the speakers actual
utterance, and various other utterances that they might have made, as in the
alternatives semantics of karttunen (1976) and rooth (1985).

(77) q: who was that lady i saw you with last night?

a: that was mywife.

h* ll%

contrast in this sense is a property of all (primary) accents   cf. vallduv     and
vilkuna (1998)    kontrast   .

steedman

nasslli, austin tx

june 2012

zaccents are not necessarily pitch-accents

162

while many english speakers (including the present one) mark the various
species of accent by pitch contour, and we accordingly use the notation of
pierrehumbert 1980 to distinguish those species, such labels do not presuppose
the use of pitch alon as a marker.

    they are abstract phonological categories re   ecting a trade-o    between a
including length, syllabic alignment and

number of artulatory dimensions,
relative height, as well as pitch (calhoun 2006, 2010).

    some speakers, including those without a larynx (such as miles davis), certain
non native speakers (sucxh as finns), and certain native speakers mark the
same distinctions without any pitch excursion.

steedman

nasslli, austin tx

june 2012

zaccents are not necessarily pitch-accents

163

steedman

nasslli, austin tx

june 2012

japanstilldoesnotletuscompetefairlyintheircountry(n?a)(an) (n) (n)kontrastbackgdbackgdkontrastkontrastbkgdkontrastthemerhemerhemerheme75500200300400pitch (hz)time (s)03.6881.85307797japansemantics of contrast

164

logical forms of all

    we follow rooth (1992) in assuming that all

linguistic
elements come in pairs (  o,  a), and steedman 2012b (hereafter ts) in
assuming that non-universals translate as generalized skolem terms, rather
than as existential generalized quanti   ers.
      a is an    alternative    logical form , in which the constants c in the    ordinary   
logical form   o corresponding to words bearing an accent have been replaced
by unique free variables of the same type   c as c, de   ning an    alternatives set   
{  a}.
    for example, the alternative semantic content of the all-rheme example (77),

that was my wife might be written as follows:

(cid:26) was sk   x.wife x   mine that

was sk   x.v  wife x   mine x that

(cid:27)

(78)

steedman

nasslli, austin tx

june 2012

an extension to the earlier model

165

    a model m for the logical language l of ts includes a correspondence c
from the objects {anna, manny, . . . } and relations {man, marry, introduce, . . . }
in m into a set of object symbols {anna,manny, . . .} (not including any
generalized skolem terms or free variables), and a set of relation symbols
{man,marry,introduce, . . . ,} in l. the function c    1 on the range of the
correspondence c is de   ned as the inverse of c . then:
1. the correspondence c satis   es a formula ra1 . . .an in which r is a relation
symbol in l and all ai are object symbols in l in the standard way, if and
only if the n-tuple hc    1(a1), . . . , c    1(an)i is in the relation c    1(r) in m.

steedman

nasslli, austin tx

june 2012

166

2. the correspondence c satis   es a formula ra1 . . .an in which in which r
is a relation symbol in l and some ai are generalized skolem terms skpi
if and only if there is an interpretation for each skolem term skpi as an
object symbol a0
i sati   es the restrictor condition p of the
skolem term skpi, and when the skolem terms skpi are replaced by the object
symbols a0

i in l such that a0

i, c satis   es ra1 . . .an.

3. the correspondence c satis   es a formula ra1 . . .an in which in which r
and/or some ai are free variables v  r and/or v  pi
if and only if there is an
interpretation for each free variable as a relation symbol r0 or an object
symbol a0
i in l such that, when the free variables are replaced by the relation
and/or object symbols a0

i, c satis   es ra1 . . .an.

    there is much more to say (not least about semantics of negation), but for
present purposes, we can assume that the rest of the model theory for ts is
much like a standard model of    rst-order predicate logic, with rules for each
connective and for the sole (universal) quanti   er.

steedman

nasslli, austin tx

june 2012

dimension 2: theme/rheme

167

    accents also de   ne information-structural role, which syntax projects onto

constituents delimited by boundary tones:

(79) q: i know emma will marry arnim. but who will marry manny?

a: (anna)(will marry manny).

h*

l+h*

lh%

(80) q: i know emma will marry arnim. but who will anna marry?

a: ( anna will marry

)( manny).

l+h*

lh% h*

ll%

    the claim: l+h* (and l*+h) mark theme (roughly, the topic or    question
under discussion   ) in english. h* (and h*+l, l*, and h+l*) mark rheme
(roughly, the comment or part that advances that discussion).

steedman

nasslli, austin tx

june 2012

the speaker de   nes the theme

168

that is not to say that information structure is uniquely determined by contexts
such as wh-questions.
speakers establish information structure by their utterance, as in the following
variant of (80):

(81) q: i know emma will marry arnim. but who will anna marry?

a:

(anna)(will marry manny).

l+h* lh%

h*

ll%

    the hearer accomodates the speaker   s consistent presupposition that anna
(as opposed to somebody else) is the theme, and marrying manny (as opposed
to someone else) is the rheme. this obviates criticism by joshi (1990) and
pulman (1997).

steedman

nasslli, austin tx

june 2012

zzcommon ground

169

    we follow stalnaker and thomason in assuming that common ground consists
in the set of propositions that a conversational participant supposes to be
mutually agreed to for the purposes of the conversation.
6= the set of propositions that all participants actually believe.

steedman

nasslli, austin tx

june 2012

ztheme and rheme

170

    in these terms, we can informally de   ne theme and rheme as follows:

    a theme is a part of the meaning of an utterance that some participant
in the conversation supposes (or fails to suppose) already to be common
ground;

    a rheme is a part of the meaning of an utterance which some participant

makes (or fails to make) common ground.

    cf. gussenhoven 1983 selection/addition
    cf. brazil 1997 referring/proclaiming.
    cf. roberts 1996; ginzburg 1996 qud; inquisitive issues; &c.

steedman

nasslli, austin tx

june 2012

unmarked themes

171

    in cases where there is only one theme, known to all participants, the theme

lacks any contrast and any accent.

    e.g., the following responses to the questions in (79) and (80) are possible:

(82) q: i know emma will marry arnim. but who will marry manny?

a: (anna)(will marry manny).

h*

ll%

(83) q: i know anna dated arnim. but who will she marry?

a: (anna will marry)( manny).

h*

ll%

    such non-contrastive themes are referred to as    unmarked    themes.

steedman

nasslli, austin tx

june 2012

unmarked themes vs. all-rheme utterances

    sentences with unmarked themes are ambiguous, and a number of information
structures (including the    all-rheme    utterance (84c) that is appropriate to the
   out-of-the-blue    context) are acoustically indistinguishable.

172

(84) a. q: what will anna do?

a: (anna will)   ( marry manny)   .

b. q: what about anna?

a: (anna)   (will marry manny)   .

h*

ll%

h*

ll%

c. q: what   s new?

a: (anna will marry manny)   .

h*

ll%

steedman

nasslli, austin tx

june 2012

all-rheme utterances with non-final accent

    in english, the following all-rheme example succeeds as an out-of-the-blue

rheme just in case phoning is a background activity of the absent mother:

173

(85) q: what   s new?

a: (your mothercalled)   .

h*

ll%

however, the possibility of such a subject-accented all-rheme utterance does
not appear in general to extend to transitive examples like the following:

(86) q: what   s new?

a: #(annamarried manny)   .

h*

ll%

steedman

nasslli, austin tx

june 2012

zdimension 3: presence in common ground

    english accents are distinguished along an orthogonal dimension of whether
their information unit is (or becomes) present in common ground (h*, l+h*,
h*+l) or not (l*, l*+h, h+l*):

174

(87) a. you put my trousersin the microwave!
ll%
b. you put my trousersin the microwave?
lh%

h*

h*

l*

l*

    in (87a), the speaker marks the proposition as being made common ground.
    in (87b), the speaker marks the proposition as not being made common

ground, thereby possibly implicating disbelief.

steedman

nasslli, austin tx

june 2012

175

dimension 4: speaker/hearer agency

    a further dimension of intonational meaning is carried by the boundaries,

rather than the accents.

    the level boundaries l, ll% and hl% mark information units as being (or
not being) supposed to be (or made to be) common ground by the speaker.
    the rising boundaries h, lh% and hh% mark information units as being (or

not being) supposed to be (or made to be) common ground by the hearer.

steedman

nasslli, austin tx

june 2012

176

generativity of intonational tunes in english

    the system relating these four dimensions of information structural meaning
can be set out as follows, where    signi   es theme, and    signi   es rheme,
while > and    signify success/failure of supposition/update over the common
ground by the speaker/hearer agents s and h.

steedman

nasslli, austin tx

june 2012

generativity of intonational tunes in english

177

>

l+h*

  
   h*, h*+l

   

l*+h

l*, h+l*

table 4: meaning elements contributed by the accents

s
l, ll%, hl%
h h, hh%, lh%

table 5: meaning element contributed by the boundaries

steedman

nasslli, austin tx

june 2012

178

semantics of theme/rheme, common ground,

and agency

    we de   ne the common ground as a (sub)model c, and the property of a
proposition holding in c as a logical modality [c]. the thematic function of
being already supposed present in common ground can then be represented as
   , and the rhematic function of being made present in common ground as   ,
de   ned as follows:1
(88)    =def    p   x.suppose([c]theme po      a     {pa}[theme a     a = po])x
(89)    =def    p   x.[c]update c po x      t[theme t    update c (po t) x

   where:
1the latter de   nition is simpli   ed here by omitting any mention of the alternative semantic value pa.

steedman

nasslli, austin tx

june 2012

179

1. p is a polymorphic variable ranging over pairs (po, pa) where po is a function
of any valency (including propositions of zero valency), and pa is a function
of the same valency that includes at least one free variable;

2. {pa} is the alternative set characterized by pa;
3. suppose can be thought of as a modal version of beaver   s (2001) fallible
presupposition operator        roughly, verify or update with respect to the
common ground c;

4. the predicate theme is assumed to be directly interpreted in the common

ground model c as a (polymorphic) property t
me.

5. update can be thought of as a fallible update predicate which fails if its
argument is not a proposition, and which either extends the common ground
model c by the denotation of a proposition p, or    nds a theme t and extends
it by the denotation of the result of applying p to t.

6. x is the agent s or h.

steedman

nasslli, austin tx

june 2012

alternative semantics for id35: accents

180

    the proper name anna bearing an h* accent has the following nominative

category, among other case-like type-raised categories:

(90) anna

h*

:=s>,  /(s>,  \np>,  ) :

    a subject bearing no accent has the following category:

(91) anna:=s  ,  /(s  ,  \np  ,  ) :

(cid:27)

(cid:26)    p.p anna
(cid:27)
(cid:26)    p.p anna

   p.p v  anna

   p.p anna

(where   o and   a are identical as here we will write them as one e.g.
   p.p anna.)

steedman

nasslli, austin tx

june 2012

181

alternative semantics for id35: boundaries

    boundaries are not properties of words or phrases, but independent string

elements in their own right.

    they bear a category which    freezes      ,   /  ,  -marked constituents as
complete information-/intonation- structural units, making them unable to
combine further with anything except similarly complete prosodic units.

    for example, the speaker-supposition- signaling ll% boundary bears the

following category:
(92) ll%:= s$  \ ?s$  ,   :    f .  (   f s)

steedman

nasslli, austin tx

june 2012

a derivation

182

(93)

anna
l    +h

married
(s\np)/np

n    f . f anna

o
>t
s   ,   /(s   ,  \np   ,   )
n    x.married x anna
:

   p.p v  anna

s   ,   /np   ,   :

:    x.   y.married xy :    f .  (   f h) :

   x.married x v  anna

>b

o
n    x.married x anna
(cid:26)    p.p manny

   x.married x v  anna

o
(cid:27)

h)

s   /np   :    (  

lh%

s$  \?s$  ,  

manny

h   

(cid:27)
<t
s>,  \(s>,  /np>,  )

(cid:26)    p.p manny

   p.p v  manny

.

ll%

s$  \?s$  ,  
:    g.  (   g s)

<

(cid:26)    p.p manny
s  \(s   /np  ) : >(  
o

n    x.married x anna

   p.p v  manny

h))

(cid:27)

<
s)

<

s   : >(  

   x.married x v  anna
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

   p.p v  manny

s)(   (  

n married manny anna

o

s :

married v  manny v  anna

   you do not suppose the question of who anna (as opposed to anyone else) married to be
common ground, i make it common ground that she married manny (as opposed to anyone else)   

steedman

nasslli, austin tx

june 2012

remarks

183

    theme/rheme marking is projected onto phrasal constituents by syntactic

derivation alone.

    it is bounded by combination of the phrase with a boundary tone.
    no independent extrasyntactic mechanism of    focus projection    is needed to

achieve the semantics of    broad focus   

steedman

nasslli, austin tx

june 2012

unmarked theme

184

    (94) prosodic phrase promotion rule (%)

s$  ,   : f    % s$   :   (   f s)

(95)

anna
>t
s/(s\np)
:    f .f anna

married

(s\np)/np :

   x.   y.married xy

:

.

manny

h   

<t

ll%

(cid:26)    p.p manny)

(cid:27)
s>,  \(s>,  /np>,  )
(cid:26)    p.p manny

s$  \?s$  ,   :
:    g.  (   g s)
(cid:27)

   p.p v  manny)

<

   p.p v  manny

s)

<

s   /np   :   (   {   x.married x anna}s)

s  \(s   /np  ) : >(  

s/np :    x.married x anna

>b

%

(cid:26)    p.p manny

(cid:27)

s   : >(  
s)(  (  (   x.married x anna)s))
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

n married manny anna

   p.p v  manny

o

s :

married v  manny anna

steedman

nasslli, austin tx

june 2012

accent-final all-rheme utterance

185

    the above contour also allows an alternative analysis as a all-rheme utterance:

(96)

anna
>t
s/(s\np)
:    f .f anna :    x.   y.married xy :

married
(s\np)/np

manny

h   

(cid:27)
<t
s>,  \(s>,  /np>,  )

(cid:26)    p.p manny

   p.p v  manny

.

ll%

s$  \?s$  ,  
:    g.  (   g s)

>b
s/np :    x.married x anna

n married manny anna

married v  manny anna

o

s>,   :

<

o

n married manny anna
n married manny anna

married v  manny anna

married v  manny anna

o

s   : >(  

s :

<

s)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

   i make it common ground that anna married manny   

steedman

nasslli, austin tx

june 2012

non-final accent all-rheme utterance

    since english unaccented verbs are also unmarked as to the theme/rheme
dimension, there is also a all-rheme analysis for intransitives like the following:

186

(97) your

mother

h   

(cid:26)    f . f (your mother)

s>,  /(s>,  \np>,  )
   f . f (your v  mother)

:

>t

(cid:27)

.

ll%

called
s\np

s$  \?s$  ,  
:    x.called x :    g.  (   g s)

>

(cid:26) called (your mother)

(cid:27)

called (your v  mother)
s   : >(  

(cid:26) called (your mother)
(cid:26) called (your mother)

called (your v  mother)

(cid:27)
(cid:27)

s :

called (your v  mother)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

<

s)

s>,   :

steedman

nasslli, austin tx

june 2012

no english non-   nal rheme-accented transitives

187

we are free to make unaccented accusatives theme-marked:
(98) manny :=(s>,  \np>,  )\((s>,  \np>,  )/np>,  ) :    p.p manny0

s>,  \(s>,   /np>,  ) :    p.p manny0

steedman

nasslli, austin tx

june 2012

zno english non-   nal rheme-accented transitives
    this category allows a derivation for (82) with an unmarked theme:

188

(99)

anna
h   

n    f . f anna

o
s>,  /(s>,  \np>,  )
:

   f . f v  anna

n    f . f anna

   f . f v  anna)

s   /(s  \np  ) : >(  

married
(s\np)/np

manny

.

ll%

(s>,  \np>,   )\((s>,  \np>,   )/np>,   ) s$  \?s$  ,  
:    g.  (   g s)

<t

:    x.   y.married xy

:    p.p manny
s>,  \np>,   :    x.married manny x

s  \np   : >(  {   x.married manny x}s)

<

<

>

>t

%
s

o
n    f . f anna

   f . f v  anna

o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

s   : >(  

s)(>(   {   x.married manny x}s))

n married manny anna

married manny v  anna

o

s :

   i make anna (as opposed to anyone else) common ground, i suppose the
question of who married manny to be common ground   

steedman

nasslli, austin tx

june 2012

non-final accent rheme disallowed

189

however, the theme-marked object category prevents an all-rheme analysis

(100)

anna
h   

n    f . f anna

o
>t
s>,  /(s>,  \np>,  )
:

   f . f v  anna

married
(s\np)/np

manny

.

ll%

(s>,  \np>,   )\((s>,  \np>,   )/np>,   ) s$  \?s$  ,  
:    g.  (   g s)

<t

:    x.   y.married xy

:    p.p manny
s>,  \np>,   :    x.married manny x

<
   

   #i make it common ground that anna (as opposed to anyone else) married
manny   

    hence the anomaly of the out-of-the-blue utterance (86) with the same

contour. (the all-theme version is ok in context.)

steedman

nasslli, austin tx

june 2012

zanother impossible non-final accent rheme

    the same observation applies to examples like the following, consistent with

ladd   s 1996 analysis of related examples (cf. steedman 2000a:119, (62)):

190

(101) q: has anna read ulysses?

a: (anna doesn   t read)   (books)   .

h*

ll%

    (101) cannot be uttered out of the blue.

steedman

nasslli, austin tx

june 2012

   anchoring    objects and superman sentences

    examples like the following can be uttered out-of-the-blue:

191

(102) a. i have to see a guy.

b. you need to talk to someone.
c. your mother called you.
d. he was reading superman to some kid.

(neeleman and szendr  oi 2004)

    such non-theme-marked objects are reminiscent of prince   s 1981    anchoring   

given modi   ers:

(103) anna married a guy i know.

(the alternatives are people, not people i know.)

steedman

nasslli, austin tx

june 2012

focusing particles such as only

(104) only := np   /np    :    np   p   . . . .npo p . . .      a     {npa}[a p . . .     (a = npo)]

192

(105)

anna

married

>t
s/(s\np)
:    f .f anna :    x.   y.married xy :    np   p.npo p      a     {pa}[a p     (a = npo)]

(s\np)/np

np   /np   

only

manny

h   

:

<t

(cid:27)
s>,  \(s>,   /np>,  )

(cid:26)    p.p manny
(cid:9)[a p     (a =    p.p manny)]
:    p.p manny      a    (cid:8)   p.p v  manny
(cid:9)[a p     (a =    p.p manny)](cid:9)s)
: >(  (cid:8)   p.p manny      a    (cid:8)   p.p v  manny
(cid:9)[a p     (a =    p.p manny)](cid:9)s)(  (  (   x.married x anna)s))

(cid:9)[a(   x.married x anna)     (a =    p.p manny)]

s>,  \(s>,   /np>,  )

s  \(s   /np   )

   p.p v  manny

>

s/np

:    x.married x anna

>b

s   /np  

%
:   (   {   x.married x anna}s)

s   : >(  (cid:8)   p.p manny      a    (cid:8)   p.p v  manny

s : married manny anna      a    (cid:8)   p.p v  manny

<
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

ll%

s$  \?s$  ,  
:    g.  (   g s)

<

   i suppose the question of who anna married to be common ground, i make it common ground
she married manny and none of the alternatives.   

steedman

nasslli, austin tx

june 2012

only and    second occurence focus   

193

    who ate only tofu?

(106)

>t

anna
h   

o
s>,   /(s>,  \np>,  )
:
o

n    f . f anna
n    f . f anna

   f . f v  anna
s   /(s  \np   )
   f . f v  anna

%

s)

: >(  

ate

only

tofu

(s\np)/np
:    x.   y.ate xy :    np   p   y.npo p y      a     pa[a p y     (a = npo)]
(s\np)\((s\np)/np)

np   /np   

:    p.   y.p tofu y      a     {   p   y.p tofu y}[a p y     a =    p   y.p tofu y]

.

ll%

<t

(s\np)\((s\np)/np) s$  \?s$  ,  
:    g.  (   g s)

:    p   y.p tofu y

>

<

:    y.ate tofu y      a     {   p   y.p tofu y}[a ate y     a =    p   y.p tofu y]

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o

:   (   {   y.ate tofu y      a     {   p   y.p tofu y}[a ate y     a =    p   y.p tofu y]}s)
s)(  (   {   y.ate tofu y      a     {   p   y.p tofu y}[a ate y     a =    p   y.p tofu y]}s))

n    f . f anna
(cid:26) ate tofu anna      a     {   p   y.p tofu y}[a ate anna     (a =    p   y.p tofu y)]

   f . f v  anna
ate tofu v  anna       a     {   p   y.p tofu y}[a ate v  anna     (a =    p   y.p tofu y)]

(cid:27)

s   : >(  

s :

<

>

s\np

s  \np  

steedman

nasslli, austin tx

june 2012

second occurrence isn   t contrastive

194

    category (104) does not require   o,  a values to be distinct.
    (106) does not evoke any alternatives to tofu, because tofu is unaccented.
    but that is because we have already disposed of the alternatives to tofu in a

previous utterance such as who ate only tofu?

    (106) is only admissible at all in such contexts, in which by de   nition such
alternatives are not only given, but also explicitly denied by the previous
utterance. so why should they be evoked?

    cf. rooth 1992 people who grow rice generally only eat rice.

the semantics of only is independent of focus, at least in the sense of contrast
with alternatives.

steedman

nasslli, austin tx

june 2012

z   nested    focus

195

    adverbial only and also

(107) only := (s\np)/$/(s\np)/$ :    p   x . . . .pox . . .      a     {pa}[a x . . .     (a = po)]

(108) also := (s\np)/$/(s\np)/$ :    p   x . . . .pox . . .      a     {pa}[a x . . .    a 6= po)]

    wold (1996) notes that rooth makes the wrong prediction for    nested focus   
examples like the following elaborated answer to the question    who did john
introduce to bill?   :

(109) a. anna only introduced sue to bill.

b. anna also only introduced sue to tom

steedman

nasslli, austin tx

june 2012

wold   s problem

196

    the available reading supported by the context is one in which anna introduced

sue and no one else to both bill and tom.

    however,

if both the second mention focus and the novel focus in the
second sentence are captured by only, (b) means (counterfactually) that anna
introduced only sue to only tom.

    because the id35 account of the projection of rheme focus (that is, accent) is
strictly via the derivation, the preferred consistent reading is correctly derived
here (brace yourself):

steedman

nasslli, austin tx

june 2012

also

only

(s\np)/(s\np)

((s\np)/pp)/((s\np)/pp)

:    p   y.poy      a     {pa}[a y    (a 6= po)]    p   z   y.pozywedge   a     {pa}[a zy     (a = po)]

anna

np   

>t

:    f .f anna

%
  (   {   f . f anna})

np   
  

197

introduced

((s\np)/pp)/np

sue

np   

:    x   z   y.introduced zxy :    g.g sue
<

(s\np)/pp

   z   y.introduce z sue y

>

to tom
h    ll%

n    h.h tom

pp   

   h.h v  tom

:

(s\np)/pp

:    z   y.introduce z sue y

      a     {   z   y.introduce z sue y}[a y     (a = (   z   y.introduce z sue y))]

(cid:26)    y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a =    y.introduce tom sue y)]

   y.introduce v  tom sue y      a     {   y.introduce v  tom sue y}[a y     (a =    y.introduce v  tom sue y)]

s>,  \np>,  

:

[a y     (a =    y.introduce v  tom sue y)]}[a y    (a 6= (   y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a =    y.introduce tom sue y)]))]

   y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a =    y.introduce tom sue y)]

      a     {   y.introduce v  tom sue y      a     {   y.introduce v  tom sue y}

>(  {   y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a = (   y.introduce tom sue y))]
      a     {   y.introduce v  tom sue y      a     {   y.introduce v  tom sue y}[a y     (a =    y.introduce v  tom sue y)]}

[a y    (a 6= (   y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a =    y.introduce tom sue y)]))]}s)

s>,  \np>,  

s  \np  

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

  (  (   f .f anna))(>(  {   y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a = (   y.introduce tom sue y))]

      a     {   y.introduce v  tom sue y      a     {   y.introduce v  tom sue y}[a y     (a =    y.introduce v  tom sue y)]}

[a y    (a 6= (   y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a =    y.introduce tom sue y)]))]}s))

introduce tom sue anna      a     {   y.introduce tom sue y}[a anna     (a = (   y.introduce tom sue y))]

      a     {   y.introduce v  tom sue y}      a     {   y.introduce v  tom sue y}[a y     (a =    y.introduce v  tom sue y)]}

[a anna    (a 6= (   y.introduce tom sue y      a     {   y.introduce tom sue y}[a y     (a =    y.introduce tom sue y)]))]}s))

s  

steedman

nasslli, austin tx

june 2012

198

steedman

nasslli, austin tx

june 2012

199

coda: intonational phrases are constituents

    the present theory makes intonation structure as de   ned by intonational
boundaries isomorphic with the top-level constituency of surface syntactic
derivational structure.

    surface derivational structure is also, as we have seen, isomorphic to coordinate

structure and the domain of relativization.

    it follows that this theory predicts the strongest possible relation between
intonation structure, information structure, coordination, and movement, as
follows (cf. steedman 1991):

    all and only those substrings that can either undergo coordination or be
extracted over can be intonational phrases and elements of information
structure, and vice versa.

steedman

nasslli, austin tx

june 2012

syntactic islands are intonational islands

200

    (110) a. *(three mathematicians)(in ten derivea lemma).
h* ll%
b. *(seymour prefers the nuts)(and bolts approach).
h* ll%

l+h* lh%

l+h*

lh%

c. *(they only asked whether i knew the woman who chairs)(the zoning board).
ll%

l+h* lh%

h*

    (111) a. *three mathematicians in ten derive a lemma and in a hundred can cook a

boiled egg.

b. *the nuts which seymour prefers and bolts approach
c. *which boards did they ask whether you knew the woman who chairs?

steedman

nasslli, austin tx

june 2012

201

coda: the real problem of id146
    if all there was to language was an encoding of propositions that the child
already has in mind, as in part iii, it is not clear why they should bother to
learn language at all, as clark (2004) points out, in defence of a pac learning
model (!).

    we know from fernald (1993) that infants are sensitive to interpersonal aspects

of intonation from a very early age.

    in english,

intonation contour

information-structural elements,

is used to convey a complex system
of
including topic/comment markers and
given/newness markers (bolinger 1965; halliday 1967; ladd 1996), and is
exuberantly used in speech by and to infants.

steedman

nasslli, austin tx

june 2012

towards a more realistic syntax and semantics
    for example, it is likely that the child   s representation of the utterance    more

doggies! is more like (112):

202

(112)

:

more doggies

h    h   
np   
+,  

(cid:26)    p.p (more0dogs0)

!

(cid:27)

ll%
x  \?x  ,  
:    g.  [s]   g

   p.p (vmore0vdogs0)
np   

(cid:26)    p.p (more0dogs0)

   p.p (vmore0vdogs0)

   : [s]  

(cid:27)

<

   mummy makes the property afforded by more dogs (as opposed to the alternatives) common
ground.   

steedman

nasslli, austin tx

june 2012

    consider the child then faced with the following, from fisher and tokura

(1996), as the next utterance (cf. steedman 1996a):

203

(113)

you

s/(s\np)
:    p.p you0 :

s/np :

like
h   

n    x   y.like0xy
n    x.like0x you0

(s\np)/np
   x   y.vlike0xy
   x.vlike0x you0

o

>b

o

l

the doggies!
x  \?x  ,  
s  \(s   /np  )
:    g.  [s]   g : [s]     q.q dogs0

ll%

s   /np   : [s]  
s   : ([s]     p.p dogs0)([s]  

<

   x.vlike0x you0

n    x.like0x you0
o
n    x.like0x you0
n like0dogs0you0

   x.vlike0x you0

o

s :

vlike0dogs0you0

o

)

<

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

   mummy supposes what property the dogs a   ord to be common ground, and
makes it common ground it   s me liking (as opposed to anything else) them.   

steedman

nasslli, austin tx

june 2012

id35 as    motherese   

204

information-structural information, directly reveals the target grammar.

    fisher points out that the l boundary after the verb makes the intonation
structure inconsistent with standard assumptions about surface constituency.
    however, this intonation structure is homomorphic to the id35 derivation,
which delivers the corresponding theme/rheme information partition directly.
    thus, here too, the availability of the full semantic interpretation, including
    in this case, since the derivation requires the use of the forward composition
rule, indexed >b, the child gets information about the id203 of applying
the composition rule to the    rst two categories to yield s\np.
    thus, the child can build the entire parsing model in parallel with learning the
    long range dependencies come for free.

grammar,

steedman

nasslli, austin tx

june 2012

coda: information structure and gapping

    i conjecture that the alternative logical form de   ned in this section is the

locus of the gap information in the english gapped construction.

205

(114)

(cid:26) married0manny0anna0

anna married manny
married0v  manny0v  anna0

(cid:27)

s :

and

(x\?x)/?x s\((s/np)/npsg) :

tom sue

(cid:26)   tv.tv sue0tom0

  tv.tv v  sue0v  tom0

(cid:27)

    this would    ll a hole in the account of gapping as constituent coordination in

steedman (1990)

steedman

nasslli, austin tx

june 2012

conclusion

206

    intonation structure is just surface syntactic derivation, a.k.a. pf.
    information structure is just logical form, a.k.a. lf.
    pf and lf are the only    interface levels   
    lf is the only structural representational level.

covert/overt move (a.k.a. copy/delete etc.) = merge = lf surface
composition

steedman

nasslli, austin tx

june 2012

z207

references

abney, steven, 1996.    statistical methods and linguistics.    in judith klavans
and philip resnik (eds.), the balancing act, cambridge, ma: mit press.
1   26.

abney, steven, 1997.    stochastic attribute-value grammars.    computational

linguistics 23:597   618.

alishahi, afra and stevenson, suzanne, 2008.    a computational model of early

argument structure acquisition.    cognitive science 32:789   834.

steedman

nasslli, austin tx

june 2012

altmann, gerry and steedman, mark, 1988.    interaction with context during

human sentence processing.    cognition 30:191   238.

208

auli, michael and lopez, adam, 2011.

   a comparison of loopy belief
propagation and id209 for integrated id35 id55 and
parsing.    in proceedings of the 49th annual meeting of the association
for computational linguistics: human language technologies. portland, or:
acl, 470   480.

baldridge, jason, 2002. lexically speci   ed derivational control in combinatory

categorial grammar. ph.d. thesis, university of edinburgh.

baldridge, jason and kruij   , geert-jan, 2003.

   multi-modal combinatory
categorial grammar.    in proceedings of 11th annual meeting of the european
association for computational linguistics. budapest, 211   218.

steedman

nasslli, austin tx

june 2012

bangalore, srinivas and joshi, aravind, 1999.    id55: an approach to

almost parsing.    computational linguistics 25:237   265.

209

beaver, david, 2001. presupposition and assertion in dynamic semantics.

stanford, ca: csli publications.

bever, thomas, 1970.    the cognitive basis for linguistic structures.    in john
hayes (ed.), cognition and the development of language, new york: wiley.
279   362.

b laszczak, joanna and g  artner, hans-martin, 2005.    intonational phrasing,

discontinuity, and the scope of negation.    syntax 8:1   22.

bolinger, dwight, 1965. forms of english. cambridge, ma: harvard university

press.

steedman

nasslli, austin tx

june 2012

b  orschinger, benjamin, jones, bevan k., and johnson, mark, 2011.    reducing
grounded learning tasks to grammatical
in proceedings of
the 2011 conference on empirical methods in natural language processing.
edinburgh: acl, 1416   1425.

id136.   

210

bos, johan, clark, stephen, steedman, mark, curran, james r., and
hockenmaier, julia, 2004.    wide-coverage semantic representations from
a id35 parser.    in proceedings of the 20th international conference on
computational linguistics, geneva. acl, 1240   1246.

bos, johan and markert, katja, 2005.

   combining shallow and deep
in proceedings of
nlp methods for recognizing id123.   
the first pascal challenge workshop on recognizing id123.
http://www.pascal-network.org/challenges/rte/: pascal, 65   68.

steedman

nasslli, austin tx

june 2012

brazil, david, 1997. the communicative value of

intonation in english.

cambridge university press, second edition.

211

buszkowski, wojciech and penn, gerald, 1990.    categorial grammars determined

from linguistic data by uni   cation.    studia logica 49:431   454.

buttery, paula, 2006. computational models for first id146.

ph.d. thesis, university of cambridge.

calhoun, sasha, 2006.

intonation and information structure in english. ph.d.

thesis, university of edinburgh.

calhoun, sasha, 2010.

   the centrality of metrical structure in signaling

information structure: a probabilistic perspective.    language 86:1   42.

steedman

nasslli, austin tx

june 2012

carey, sue and bartlett, elsa, 1978.    acquiring a single new word.    in
proceedings of the 15th stanford child language conference. stanford, papers
and reports on child language development, 15, 17   29.

212

charniak, eugene, 2000.    a maximum-id178-inspired parser.    in proceedings
of the 1st meeting of the north american chapter of the association for
computational linguistics. seattle, wa, 132   139.

charniak, eugene, 2001.    immediate-head parsing for language models.    in
proceedings of the 39th annual meeting of the association for computational
linguistics. toulouse: acl, 116   123.

charniak, eugene, goldwater, sharon, and johnson, mark, 1998.    edge-based
best-first chart parsing.    in proceedings of the 6th workshop on very large
corpora, montreal, august. 127   133.

steedman

nasslli, austin tx

june 2012

chelba, ciprian and jelinek, frederick, 1998.    exploiting syntactic structure
for id38.    in proceedings of the 36th annual meeting of
the association for computational linguistics and seventeenth international
conference on computational linguistics. montreal: coling/acl, 225   231.

213

chomsky, noam, 1957. syntactic structures. the hague: mouton.

clark, alexander, 2004.    grammatical id136 and first id146.   
in coling workshop on psycho-computational models of human language
acquisition.

clark, stephen, 2002.    a supertagger for id35.    in

proceedings of the tag+ workshop. venice, 19   24.

clark, stephen and curran, james r., 2003.    id148 for wide-

steedman

nasslli, austin tx

june 2012

coverage id35 parsing.   
methods in natural language processing. sapporo, japan, 97   104.

in proceedings of the conference on empirical

214

clark, stephen and curran, james r., 2004.    parsing the wsj using id35
and id148.    in proceedings of the 42nd annual meeting of the
association for computational linguistics. barcelona, spain: acl, 104   111.

clark, stephen and curran, james r., 2006.    partial training for a lexicalized
grammar parser.   
in proceedings of the human language technology
conference and annual meeting of the north american chapter of the
association for computational linguistics (hlt-naacl    06). new york: acl.

clark, stephen, hockenmaier, julia, and steedman, mark, 2002.    building deep
dependency structures with a wide-coverage id35 parser.    in proceedings
of the 40th annual meeting of the association for computational linguistics.
philadelphia, 327   334.

steedman

nasslli, austin tx

june 2012

clark, stephen, steedman, mark, and curran, james r., 2004.    object-extraction
and question-parsing using id35.    in proceedings of the conference on
empirical methods in natural language processing. barcelona, spain: acl,
111   118.

215

collins, michael, 1997.    three generative lexicalized models for statistical
parsing.    in proceedings of the 35th annual meeting of the association for
computational linguistics. madrid: acl, 16   23.

collins, michael, 1999. head-driven statistical models for natural language

parsing. ph.d. thesis, university of pennsylvania.

dowty, david, 1994.    the role of negative polarity and concord marking
in proceedings of the 4th conference
in natural language reasoning.   
on semantics and theoretical linguistics. rochester, ny: clc publications,
cornell university.

steedman

nasslli, austin tx

june 2012

eisner, jason, 1996.    e   cient normal-form parsing for combinatory categorial
grammar.    in proceedings of the 34th annual meeting of the association for
computational linguistics, santa cruz, ca. san francisco: morgan kaufmann,
79   86.

216

etzioni, oren, banko, michele, and cafarella, michael, 2007.    machine reading.   
in proceedings of aaai spring symposium on machine reading. menlo
park, ca: aaai press.

fernald, anne, 1993.    approval and disapproval: infant responsiveness to vocal
a   ect in familiar and unfamiliar languages.    child development 64:657   667.

fisher, cynthia and tokura, hisayo, 1996.    id144 in speech to infants: direct
and indirect acoustic cues to syntactic structure.    in james morgan and
katherine demuth (eds.), signal to syntax: id64 from speech to
grammar in early acquisition, erlbaum. 343   363.

steedman

nasslli, austin tx

june 2012

fodor, janet dean, 1998.    unambiguous triggers.    linguistic inquiry 29:1   36.

217

gildea, dan, 2001.    corpus variation and parser performance.    in proceedings
of the 2001 conference on empirical methods in natural language processing.
pittsburgh, pa, 167   202.

ginzburg, jonathan, 1996.    interrogatives: questions, facts, and dialogue.    in
shalom lappin (ed.), handbook of contemporary semantic theory, oxford:
blackwell. 385   422.

gleitman, lila, 1990.    the structural sources of verb meanings.    language

acquisition 1:1   55.

goldberg, ad`ele, 1995. constructions: a construction grammar approach to

argument structure. chicago: chicago university press.

steedman

nasslli, austin tx

june 2012

gussenhoven, carlos, 1983. on the grammar and semantics of sentence accent.

dordrecht: foris.

218

halliday, michael, 1967. intonation and grammar in british english. the hague:

mouton.

harrington, brian and clark, stephen, 2009.    asknet: creating and evaluating
large scale integrated semantic networks.    international journal of semantic
computing 2:343   364.

hepple, mark, 1987. methods for parsing combinatory grammars and the

spurious ambiguity problem. master   s thesis, university of edinburgh.

hindle, donald and rooth, mats, 1993.    structural ambiguity and lexical

relations.    computational linguistics 19:103   120.

steedman

nasslli, austin tx

june 2012

hockenmaier, julia, 2003. data and models for statistical parsing with id35.

ph.d. thesis, school of informatics, university of edinburgh.

219

hockenmaier, julia, bierner, gann, and baldridge, jason, 2004.    extending the

coverage of a id35 system.    journal of logic and computation 2:165   208.

hockenmaier,

julia and bisk, yonatan,

   normal-form parsing
for id35s with generalized composition and
type-raising.   
in proceedings of the 23nd international conference on
computational linguistics. beijing, 465   473.

2010.

hockenmaier, julia and steedman, mark, 2002a.    acquiring compact lexicalized
grammars from a cleaner treebank.    in proceedings of the third international
conference on language resources and evaluation. las palmas, spain, 1974   
1981.

steedman

nasslli, austin tx

june 2012

hockenmaier, julia and steedman, mark, 2002b.

   generative models for
statistical parsing with id35.    in proceedings of
the 40th meeting of the association for computational linguistics. philadelphia,
335   342.

220

hockenmaier, julia and steedman, mark, 2007.    id35bank: a corpus of id35
derivations and dependency structures extracted from the id32.   
computational linguistics 33:355   396.

ho   man, matthew., blei, david, and bach, francis, 2010.    online learning
for id44.    advances in neural information processing
systems 23:856   864.

jelinek, fred and la   erty, john, 1991.    computation of the id203 of initial
substring generation by stochastic context-free grammars.    computational
linguistics 17:315   323.

steedman

nasslli, austin tx

june 2012

joshi, aravind, 1988.

in david dowty, lauri
karttunen, and arnold zwicky (eds.), natural language parsing, cambridge:
cambridge university press. 206   250.

   tree-adjoining grammars.   

221

joshi, aravind, 1990.    phrase structure and intonation: comments on the
papers by marcus and steedman.    in gerry altmann (ed.), cognitive models
of speech processing, cambridge, ma: mit press. 457   482.

karttunen, lauri, 1976.    discourse referents.    in james mccawley (ed.), syntax

and semantics, academic press, volume 7. 363   385.

koller, alexander and thater, stefan, 2006.

   an improved redundancy
elimination algorithm for underspeci   ed descriptions.    in proceedings of
the international conference on computational linguistics/association for
computational linguistics. sydney: coling/acl, 409   416.

steedman

nasslli, austin tx

june 2012

kor, kian wei, 2005. improving answer precision and recall of list questions.

ph.d. thesis, edinburgh.

222

kwiatkowski, tom, steedman, mark, zettlemoyer, luke, and goldwater, sharon,
2012.    a probabilistic model of id146 from utterances and
meanings.    in proceedings of the 13th conference of the european chapter of
the acl (eacl 2012). avignon: acl, 234   244.

kwiatkowski, tom, zettlemoyer, luke, goldwater, sharon, and steedman, mark,
2010.    inducing probabilistic id35 grammars from logical form with higher-
order uni   cation.    in proceedings of the conference on empirical methods in
natural language processing. cambridge, ma: acl, 1223   1233.

kwiatkowski, tom, zettlemoyer, luke, goldwater, sharon, and steedman,
mark, 2011.    lexical generalization in id35 grammar induction for semantic

steedman

nasslli, austin tx

june 2012

parsing.    in proceedings of the conference on empirical methods in natural
language processing. edinburgh: acl, 1512   1523.

223

ladd, d. robert, 1996. intonational phonology. cambridge: cambridge university

press. 2nd edition revised 2008.

liang, percy, jordan, michael, and klein, dan, 2011.    learning dependency-
based id152.    in proceedings of the 49th annual meeting of
the association for computational linguistics: human language technologies.
portland, or: acl, 590   599.

lu, wei, ng, hwee tou, lee, wee sun, and zettlemoyer, luke s., 2008.    a
generative model for parsing natural language to meaning representations.   
in proceedings of the 2008 conference on empirical methods in natural
language processing. honolulu, hawaii: acl, 783   792.

steedman

nasslli, austin tx

june 2012

maccartney, bill and manning, christopher d., 2007.

   natural logic for
textual id136.    in proceedings of the acl-pascal workshop on textual
entailment and id141. prague: acl, 193   200.

224

macwhinney, brian, 2005.    item based constructions and the logical problem.   
in proceedings of the workshop on psychocomputational models of human
id146. connl-9. acl, 53   68.

mitchell, tom, betteridge, justin, carlson, andrew, hruschka, estevam, and
wang, richard, 2009.    populating the semantic web by macro-reading
internet text.   
in proceedings of the 8th international semantic web
conference (iswc 2009). karlsuhe: semantic web science association.

neeleman, ad. and szendr  oi, kriszta, 2004.    superman sentences.    linguistic

inquiry 35:149   159.

steedman

nasslli, austin tx

june 2012

nivre, joakim, rimell, laura, mcdonald, ryan, and g  omez rodr    guez, carlos,
2010.    evaluation of dependency parsers on unbounded dependencies.    in
proceedings of the 23rd international conference on computational linguistics.
beijing: coling/acl, 833   841.

225

niyogi, partha, 2006. computational nature of language learning and evolution.

cambridge, ma: mit press.

piantadosi, steven, goodman, noah, ellis, benjamin, and tenenbaum, joshua,
2008.    a bayesian model of the acquisition of id152.   
in proceedings of the 30th annual meeting of the cognitive science society.
washington dc: cognitive science society, 1620   1625.

pierrehumbert, janet, 1980. the phonology and id102 of english intonation.
ph.d. thesis, mit. distributed by indiana university linguistics club,
bloomington.

steedman

nasslli, austin tx

june 2012

prince, ellen, 1981.    towards a taxonomy of given-new information.    in peter

cole (ed.), radical pragmatics, new york: academic press. 223   256.

226

pulman, stephen, 1997.    higher order uni   cation and the interpretation of

focus.    linguistics and philosophy 20:73   115.

quine, willard van orman, 1960. word and object. cambridge, ma: mit press.

rimell, laura, 2010.    parser evaluation using id123 by grammatical
relation comparison.    in proceedings of the 5th international workshop on
semantic evaluation (semeval-2010). uppsala: acl, 268   271.

rimell, laura, clark, stephen, and steedman, mark, 2009.

   unbounded
dependency recovery for parser evaluation.    in proceedings of the conference
on empirical methods in natural language processing. singapore: acl, 813   
821.

steedman

nasslli, austin tx

june 2012

roark, brian, 2001.    probabilistic top-down parsing and id38.   

computational linguistics 27:249   276.

227

robaldo, livio, 2008.    skolem theory and generalized quanti   ers.    in workshop
information, and computation (wollic-08. berlin:

on logic, language,
springer, lecture notes in arti   cial intelligence 5110, 286   297.

roberts, craige, 1996.

   information structure in discourse: towards an
integrated formal theory of pragmatics.    osu working papers in linguistics
49:91   136.

rooth, mats, 1985. association with focus. ph.d. thesis, university of

massachusetts, amherst.

rooth, mats, 1992.    a theory of focus interpretation.    natural language

semantics 1:75   116.

steedman

nasslli, austin tx

june 2012

ross, john robert, 1970.

in
manfred bierwisch and karl heidolph (eds.), progress in linguistics, the
hague: mouton. 249   259.

   gapping and the order of constituents.   

228

sakas, william and fodor, janet dean, 2001.    the structural triggers learner.   
in s. bertolo (ed.), id146 and learnability, cambridge:
cambridge university press. 172   233.

s  anchez valencia, v    ctor, 1991.

studies on natural logic and categorial

grammar. ph.d. thesis, universiteit van amsterdam.

sato, maso-aki, 2001.    online model selection based on the id58.   

neural computation 13(7):1649   1681.

siskind, je   rey, 1992. naive physics, event perception, lexical semantics, and

id146. ph.d. thesis, mit.

steedman

nasslli, austin tx

june 2012

siskind, je   rey, 1996.    a computational study of cross-situational techniques

for learning word-to-meaning mappings.    cognition 61:39   91.

229

steedman, mark, 1990.    gapping as constituent coordination.    linguistics and

philosophy 13:207   263.

steedman, mark, 1991.    structure and intonation.    language 67:262   296.

steedman, mark, 1996a.    the role of id144 and semantics in the acquisition
of syntax.    in james morgan and katherine demuth (eds.), signal to syntax,
hillsdale, nj: erlbaum. 331   342.

steedman, mark, 1996b. surface structure and interpretation. linguistic inquiry

monograph 30. cambridge, ma: mit press.

steedman

nasslli, austin tx

june 2012

steedman, mark, 2000a.    information structure and the syntax-phonology

interface.    linguistic inquiry 34:649   689.

230

steedman, mark, 2000b. the syntactic process. cambridge, ma: mit press.

steedman, mark,

2007.

   information-structural semantics

for english
intonation.    in chungmin lee, matthew gordon, and daniel b  uring (eds.),
topic and focus:cross-linguistic perspectives on meaning and intonation,
dordrecht: kluwer, number 82 in studies in linguistics and philosophy. 245   
264. proceedings of the lsa workshop on topic and focus, santa barbara ca
july, 2001.

steedman, mark, 2012a.    the surface id152 of english

intonation.    submitted .

steedman

nasslli, austin tx

june 2012

steedman, mark, 2012b. taking scope: the natural semantics of quanti   ers.

cambridge, ma: mit press.

231

thomforde, emily and steedman, mark, 2011.    semi-supervised id35 lexicon
extension.    in proceedings of the conference on empirical methods in natural
language processing. acl, 1246   1256.

thompson, cynthia and mooney, raymond, 2003.    acquiring word-meaning
mappings for id139.    journal of arti   cial intelligence
research 18:1   44.

vallduv    , enric and vilkuna, maria, 1998.    on rheme and kontrast.    in peter
culicover and louise mcnally (eds.), syntax and semantics, vol. 29: the
limits of syntax, san diego: academic press. 79   108.

steedman

nasslli, austin tx

june 2012

vijay-shanker, k. and weir, david, 1994.    the equivalence of four extensions

of context-free grammar.    mathematical systems theory 27:511   546.

232

villavicencio, aline, 2002. the acquisition of a uni   cation-based generalised

categorial grammar. ph.d. thesis, university of cambridge.

villavicencio, aline, 2011.    id146 with feature-based grammars.   
in robert boyer and kirsti b  orjars (eds.), non-transformational syntax: a
guide to current models, blackwell. 404   442.

wold, dag, 1996.    long distance selective binding: the case of focus.    in

proceedings of salt. volume 6, 311   328.

wong, yuk wah and mooney, raymond, 2007.    learning synchronous grammars
for id29 with id198.    in proceedings of the 45th annual
meeting of the association for computational linguistics. acl, 960   967.

steedman

nasslli, austin tx

june 2012

xia, fei, 1999.    extracting tree-adjoining grammars from bracketed corpora.   
in proceedings of the 5th natural language processing paci   c rim symposium
(nlprs-99).

233

yang, charles, 2002. knowledge and learning in natural language. oxford:

oxford university press.

zettlemoyer, luke and collins, michael, 2005.    learning to map sentences
to logical form:
structured classi   cation with probabilistic categorial
grammars.    in proceedings of the 21st conference on uncertainty in ai
(uai). edinburgh: aaai, 658   666.

zettlemoyer, luke and collins, michael, 2007.    online learning of relaxed id35
grammars for parsing to logical form.    in proceedings of the joint conference
on empirical methods in natural language processing and computational
natural language learning (emnlp/conll). prague: acl, 678   687.

steedman

nasslli, austin tx

june 2012

