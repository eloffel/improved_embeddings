sequence   models   

noah   smith   

associate   professor   

school   of   computer   science   
carnegie   mellon   university   

nasmith@cs.cmu.edu   

lecture   outline   

1.    markov   models   
2.    hidden   markov   models   
3.    viterbi   algorithm   
4.    other   id136   algorithms   for   id48s   
5.    learning   algorithms   for   id48s   

shameless   self   promokon   

       linguis   c  structure  

predic   on  

       links   material   in   this   

lecture   to   many   
related   ideas,   including   
some   in   other   lxmls   
lectures.   

       available   in   electronic   

and   print   form.   

markov   models   

one   view   of   text   
       sequence   of   symbols   (bytes,   leqers,   
characters,   morphemes,   words,      )   
      let        denote   the   set   of   symbols.   

       lots   of   possible   sequences.      (  *   is   in   nitely   

large.)   

       id203   distribukons   over     *?   

pop   quiz   

       am   i   wearing   a   genera   ve   or   discrimina   ve   

hat   right   now?   

pop   quiz   

       generakve   models   tell   a   

       discriminakve   models   

mythical   story   to   
explain   the   data.   

focus   on   tasks   (like   
sorkng   examples).      

trivial   distribukons   over     *   
       give   id203   0   sequences   with   length   
       use   data:      with   n   examples,   give   id203   
n   1   to   each   observed   sequence,   0   to   the   rest.   

greater   than   b;   uniform   over   the   rest.   

       what   if   we   want   every  sequence   to   get   some   
id203?   
      need   a   probabiliskc   model  family   and   algorithms   

for   construckng   the   model   from   data.   

a   history   based   model   

p(start, w1, w2, . . . , wn, stop) =

  (wi | w1, w2, . . . , wi   1)

       generate   each   word   from   leb   to   right,   
condikoned   on   what   came   before   it.   

n+1!i=1

die   /   dice   

one   die   

two   dice   

start   

one   die   per   history:   

      

      

      

start   

i   

one   die   per   history:   

      

      

      

start   

i    want   

one   die   per   history:   

      

      

      

start   

i    want    a   

one   die   per   history:   

      

      

      

start   

i    want    a       ight   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to    lisbon   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to    lisbon    .   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to    lisbon    .    stop   

one   die   per   history:   

      

      

      

a   history   based   model   

n+1!i=1

p(start, w1, w2, . . . , wn, stop) =

  (wi | w1, w2, . . . , wi   1)

       generate   each   word   from   leb   to   right,   
condikoned   on   what   came   before   it.   

       very   rich   representakonal   power!   
       how   many   parameters?   
       what   is   the   id203   of   a   sentence   not   seen   

in   training   data?   

a   bag   of   words   model   

p(start, w1, w2, . . . , wn, stop) =

  (wi)

       every   word   is   independent   of   every   other   

word.   

n+1!i=1

start   

one   die:   

start   

i   

one   die:   

start   

i    want   

one   die:   

start   

i    want    a   

one   die:   

start   

i    want    a       ight   

one   die:   

start   

i    want    a       ight    to   

one   die:   

start   

i    want    a       ight    to    lisbon   

one   die:   

start   

i    want    a       ight    to    lisbon    .   

one   die:   

start   

i    want    a       ight    to    lisbon    .    stop   

one   die:   

a   bag   of   words   model   

n+1!i=1

p(start, w1, w2, . . . , wn, stop) =

  (wi)

       every   word   is   independent   of   every   other   word.   
       strong   assumpkons   mean   this   model   cannot      t   

the   data   very   closely.   

       how   many   parameters?   
       what   is   the   id203   of   a   sentence   not   seen   in   

training   data?   

first   order   markov   model   

       happy   medium?   

p(start, w1, w2, . . . , wn, stop) =

       condikon   on   the   most   recent   symbol   in   

history.   

n+1!i=1

  (wi | wi   1)

start   

one   die   per   history:   

      

      

      

start   

i   

one   die   per   history:   

      

      

      

start   

i    want   

one   die   per   history:   

      

      

      

start   

i    want    a   

one   die   per   history:   

      

      

      

start   

i    want    a       ight   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to    lisbon   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to    lisbon    .   

one   die   per   history:   

      

      

      

start   

i    want    a       ight    to    lisbon    .    stop   

one   die   per   history:   

      

      

      

first   order   markov   model   

       happy   medium?   

p(start, w1, w2, . . . , wn, stop) =

       condikon   on   the   most   recent   symbol   in   

history.   

       independence   assumpkons?   
       number   of   parameters?   
       sentences   not   seen   in   training?   

n+1!i=1

  (wi | wi   1)

mth   order   markov   models   

p(start, w1, w2, . . . , wn, stop) =

n+1!i=1

  (wi | wi   m, . . . , wi   1)

bag   of   words   

0   

mth   order   markov   

enkre   history   

m   

      

fewer   parameters   

stronger   independence   assumpkons   

richer   expressive   power   

example   

       unigram   model   eskmated   on   2.8m   words   of   

american   polikcal   blog   text.   

this trying our putting and funny
 and among it herring it obama
 but certainly foreign my
 c on byron again but from i
 i so and i chuck yeah the as but but republicans if 
this stay oh so or it mccain bush npr this with what 
and they right i while because obama

example   

       bigram   model   eskmated   on   2.8m   
words   of   american   polikcal   blog   text.   

the lack of the senator mccain hadn t keep this story 
backwards
while showering praise of the kind of gop weakness
it was mistaken for american economist anywhere in the 
white house press hounded the absence of those he s as 
a wide variety of this election day after the candidate 
b richardson was polled ri in hempstead moderated by 
the convention that he had zero wall street journal 
argues sounds like you may be the primary
but even close the bill told c e to take the obama on 
the public schools and romney
fred flinstone s see how a lick skillet road it s 
little sexist remarks

example   

       trigram   model   eskmated   on   2.8m   words   of   
american   polikcal   blog   text.   

 as i can pin them all none of them want to bet that 
any of the might be
 conservatism unleashed into the privacy rule book and 
when told about what paul
 fans organized another massive fundraising initiative 
yesterday and i don t know what the rams supposedly 
want ooh
 but she did but still victory dinner
 alone among republicans there are probably best not 
all of the fundamentalist community
 asked for an independent maverick now for 
crystallizing in one especially embarrassing

example   

       5   gram   model   eskmated   on   2.8m   
words   of   american   polikcal   blog   text.   

 he realizes fully how shallow and insincere 
conservative behavior has been he realizes that there 
is little way to change the situation
 this recent arianna huffington item about mccain 
issuing heartfelt denials of things that were actually 
true or for that matter about the shi a sunni split and 
which side iran was on would get confused about this 
any more than someone with any knowledge of us politics 
would get confused about whether neo confederates were 
likely to be supporting the socialist workers party
 at the end of the world and i m not especially 
discouraged now that newsweek shows obama leading by 
three now

example   

       100   gram   model   eskmated   on   2.8m   
words   of   american   polikcal   blog   text.   

 and it would be the work of many hands to catalogue all 
the ridiculous pronouncements made by this man since his 
long train of predictions about the middle east has been 
gaudily disastrously stupefyingly misinformed just the 
buffoon it seems for the new york times to award with a 
guest column for if you object to the nyt rewarding 
failure in quite this way then you re intolerant according 
to the times editorial page editor andrew rosenthal 
rosenthal doesn t seem to recognize that his choice of 
adjectives to describe kristol serious respected are in 
fact precisely what is at issue for those whom he 
dismisses as having a fear of opposing views

n   gram   models   

pros  

       easily   understood   

linguis2c   formalism.   

       fully   generakve.   
       algorithms:   

       calculate   id203   of   a   

sequence   

       choose   a   sequence   from   

a   set   

       training   

cons  

       obviously   inaccurate   
lingiuskc   formalism.   

       as   n   grows,   data   

sparseness   becomes   a   
problem.   
       smoothing   is   a   black   art.   

       how   to   deal   with   
unknown   words?   

n   gram   models   

pros  

       easily   understood   

linguis2c   formalism.   

       fully   generakve.   
       algorithms:   

       calculate   id203   of   a   

sequence   

       choose   a   sequence   from   

a   set   

       training   

cons  

       obviously   inaccurate   
lingiuskc   formalism.   

       as   n   grows,   data   

sparseness   becomes   a   
problem.   
       smoothing   is   a   black   art.   

       how   to   deal   with   
unknown   words?   

calculakng   the   id203      

of   a   sequence   

       let   n   be   the   length   of   the   sequence   and   m   be   

the   length   of   the   history.   
       for   every   consecukve   (m+1)   words   wi         wi+m,   
look   up   p(wi+m   |   wi         wi+m   1).   
       look   up   p(stop   |   wn   m         wn).   
       mulkply   these   quankkes   together.   

choosing   a   sequence   from   a   set   
       calculate   the   id203   of   each   sequence   in   

the   set.   

       choose   the   one   that   has   the   highest   

id203.   

training   

       maximum   likelihood   eskmakon   by   relakve   

frequencies:   

unigram   

    (w) =

bigram   

trigram   

general   

    (w | w!) =
    (w | w!w!!) =
    (w | h) =

freq(w)
#words
freq(w!w)
freq(w!)
freq(w!w!!w)
freq(w!w!!)
freq(hw)
freq(h)

note   about   current   research   

       in   the   past   few   years,      web   scale      n   gram   models   

have   become   popular.   
       more   data   always   seems   to   make   language   models   

beqer   (brants   et   al.,   2007,   inter  alia)   

       a   number   of   recent   research   e   orts   seek   to   make   
the   construckon   and   use   of   language   models   very   
e   cient.   
       runkme:      mapreduce   architectures   (e.g.,   lin   &   dyer,   
       memory:      compression   (e.g.,   hea   eld,   2011)   

2010)   

sequence   models   as   components   
       typically   we   care   about   a   sequence   together   

with   something   else.   
      analysis:      sequence   in,   predict      something   else.      
      generakon:         something   else   in,      sequence   out.   

       sequence   models   are   useful   components   in   

both  scenarios.   

noisy   channel   

channel   

x   

source   

true   y   

decoding   rule:   

sequence   model   as   source   

channel   

x   

source   

true   y   

decoding   rule:   

(jelinek,   1997)   

       speech   recognikon   
       machine   translakon   
(brown   et   al.,   1993)   
       opkcal   character   

recognikon   (kolak   and   
resnik,   2002)   
       spelling   and   

punctuakon   correckon   
(kernighan   et   al.,   
1990)   

sequence   model   as   channel   

channel   

x   

source   

true   y   

decoding   rule:   

       text   categorizakon   
      
      

language   idenk   cakon   
informakon   retrieval   
(ponte   and   crob,   1998;   
berger   and   la   erty,   
1999)   

       sentence   compression   

(knight   and   marcu,   
2002)   
       queskon   to   search   
query   (radev   et   al.,   
2001)   

it   s   hard   to   beat   n   grams!   

       they   are   very   fast   to   work   with.   they      t   the   

data   really,   really   well.   

       improvements   for   some   speci   c  problems   

follow   from:   
      task   speci   c   knowledge   
      domain   knowledge   (e.g.,   linguiskcs)   

class   based   sequence   models   

       from   brown   et   al.   (1990):   

p(start, w1, w2, . . . , wn, stop) =

  (wi | cl(wi))      (cl(wi) | cl(wi   1))
          cl      is   a   determiniskc   funckon   from   words   to   a   

n+1!i=1

smaller   set   of   classes.   
      each   word   only   gets   one   class;   known   in   advance.   
      discovered   from   data   using   a   id91   algorithm.   

start   

start    c53   

one      next   class      die   per   class:   

      

      

      

each   word   appears   on   
only   one   of   the   word   dice. 

start    c53   

i   

one   word   die   per   class:   

      

      

      

start    c53   

c23   

i   

one      next   class      die   per   class:   

      

      

      

start    c53   

c23   

i   

want   

one   word   die   per   class:   

      

      

      

start    c53   

c23   

c2   

i   

want   

one      next   class      die   per   class:   

      

      

      

start    c53   

c23   

c2   

i   

want   

a   

one   word   die   per   class:   

      

      

      

start    c53   

c23   

c2   

c5   

i   

want   

a   

one      next   class      die   per   class:   

      

      

      

start    c53   

c23   

c2   

c5   

i   

want   

a   

   ight   

one   word   die   per   class:   

      

      

      

class   based   sequence   models   

       from   brown   et   al.   (1990):   
n+1!i=1

p(start, w1, w2, . . . , wnstop) =

       independence   assumpkons?   
       number   of   parameters?   
       generalizakon   ability?   

  (wi | cl(wi))      (cl(wi) | cl(wi   1))

lecture   outline   

       markov   models   
2.    hidden   markov   models   
3.    viterbi   algorithm   
4.    other   id136   algorithms   for   id48s   
5.    learning   algorithms   for   id48s   

hidden   markov   models   

hidden   markov   model   

       a   model   over   sequences   of   symbols,   but   there   is   
missing   informakon   associated   with   each   symbol:      
its      state.      
       assume   a      nite   set   of   possible   states,     .   

p(start, s1, w1, s2, w2, . . . , sn, wnstop) =

n+1!i=1

  (wi | si)      (si | si   1)

       a   joint   model   over   the   observable   symbols   and   

their   hidden/latent/unknown   classes.   

start    c53   

one      next   class      die   per   class:   

      

      

      

the   only   change   to   the   
class   based   model   is   that   
now,   the   di   erent   word   
dice   can   share  words!  

start    c53   

i   

one   word   die   per   class:   

      

      

      

start    c53   

c23   

i   

one      next   class      die   per   class:   

      

      

      

start    c53   

c23   

i   

want   

one   word   die   per   class:   

      

      

      

start    c53   

c23   

c2   

i   

want   

one      next   class      die   per   class:   

      

      

      

start    c53   

c23   

c2   

i   

want   

a   

one   word   die   per   class:   

      

      

      

start    c53   

c23   

c2   

c5   

i   

want   

a   

one      next   class      die   per   class:   

      

      

      

start    c53   

c23   

c2   

c5   

i   

want   

a   

   ight   

one   word   die   per   class:   

      

      

      

two   equivalent   stories   

       first,   as   shown:      transikon,   emit,   transikon,   

emit,   transikon,   emit.   

       second:   

      generate   the   sequence   of   transikons.      essenkally,   

a   markov   model   on   classes.   

      stochaskcally   replace   each   class   with   a   word.      

mth   order   hidden   markov   models   
       we   can   condikon   on   a   longer   history   of   past   

states:   

  (wi | si)      (si | si   m, . . . , si   1)

p(start, s1, w1, s2, w2, . . . , sn, wnstop) =

n+1!i=1
       number   of   parameters?      
       bene   t:      longer      memory.      
       today   i   will   skck   with      rst   order   id48s.   

uses   of   id48s   in   nlp   

2000)   

       part   of   speech   tagging   (church,   1988;   brants,   
       named   enkty   recognikon   (bikel   et   al.,   1999)   and   
       text   chunking   and   shallow   parsing   (ramshaw   and   
       word   alignment   in   parallel   text   (vogel   et   al.,   

other   informakon   extrackon   tasks   

marcus,   1995)   

1996)   

       also   popular   in   computakonal   biology.   

part   of   speech   tagging   

aber   paying   the   medical   bills   ,   frances   was   nearly   broke   .   
         rb                  vbg               dt                     jj                        nns      ,               nnp               vbz               rb                        jj                  .   

       adverb   (rb)   
       verb   (vbg,   vbz,   and   others)   
       determiner   (dt)   
       adjeckve   (jj)   
       noun   (nn,   nns,   nnp,   and   others)   
       punctuakon   (.,   ,,   and   others)   

named   enkty   recognikon   

with   commander   chris   ferguson   at   the   helm   ,   

atlanks   touched   down   at   kennedy   space   center   .   

named   enkty   recognikon   

o   

i   person   

i   person   

b   person   

o    o   
with   commander   chris   ferguson   at   the   helm   ,   
i   place   
b   space   shuqle   
o   
atlanks   touched   down   at   kennedy   space   center   .   

b   place   

o    o   

i   place   

o   

o   

o   

            what   makes   this   hard?   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

word   alignment   

mr.   president   ,   noah   s   ark   was      lled   not   with   produckon   factors   ,   but   with   living   creatures.   

null                  noahs   arche   war   nicht   voller   produckonsfactoren   ,   sondern   gesch  pfe   .   

hidden   markov   model   

       a   model   over   sequences   of   symbols,   but   there   is   
missing   informakon   associated   with   each   symbol:      
its      state.      
       assume   a      nite   set   of   possible   states,     .   

p(start, s1, w1, s2, w2, . . . , sn, wnstop) =

n+1!i=1

  (wi | si)      (si | si   1)

       a   joint   model   over   the   observable   symbols   and   

their   hidden/latent/unknown   classes.   

lecture   outline   

       markov   models   
       hidden   markov   models   
3.    viterbi   algorithm   
4.    other   id136   algorithms   for   id48s   
5.    learning   algorithms   for   id48s   

algorithms   for      
hidden   markov   models   

how   to   calculate         

given   the   id48   and   a   sequence:   
1.    the   most   probable   state   sequence?   
2.    the   id203   of   the   word   sequence?   
3.    the   id203   distribukon   over   states,   for   

each   word?   

4.    minimum   risk   sequence   
given   states   and   sequences,   or   just   states:   
5.    the   parameters   of   the   id48   (     and     )?   

problem   1:         

most   likely   state   sequence   

       input:      id48   (     and     )   and   symbol   sequence   

w.   

       output:   

arg max

s

p(s | w,   ,   )

       stakskcs   view:      maximum   a  posteriori   

id136   

       computakonal   view:      discrete,   combinatorial   

opkmizakon               

example   

i   

suspect   

the       present   

forecast   

is      

pessimiskc   

cd   
nn   
nnp   
prp   

jj   
nn   
vb   
vbp   

dt   
jj   
nn   
nnp   
vbp   

jj   
nn   
rb   
vb   
vbp   

nn   
vb   
vbd   
vbn   
vbp   

nns   
vbz   

jj   

.   

.   

4   

4   

5   

5   

5   

2   

1   

1   

4,000   possible   state   sequences!   

na  ve   solukons   

       list   all   the   possibilikes   in     n.   

      correct.   
      ine   cient.   

       work   leb   to   right   and   greedily   pick   the   best   si   
at   each   point,   based   on   si   1   and   wi.   
      not   correct;   solukon   may   not   be   equal   to:   

arg max

s

p(s | w,   ,   )

interackons   

       each   word   s   label   depends   on   the   word,   and   
       but   given   adjacent  labels,   others   do   not   maqer.   

nearby   labels.   

i   

suspect   

the       present   

forecast   

is      

pessimiskc   

cd   
nn   
nnp   
prp   

jj   
nn   
vb   
vbp   

dt   
jj   
nn   
nnp   
vbp   

jj   
nn   
rb   
vb   
vbp   

nn   
vb   
vbd   
vbn   
vbp   

nns   
vbz   

jj   

.   

.   

(arrows   show   most  preferred   label   by   each   neighbor)   

base   case:      last   label   

start   

w1   

w2   

w3   

      

wn   1   

wn   

stop   

      

  1   
  2   
  3   
  4   
      
  |  |   

scoren(  ) =   (stop |   )      (wn |   )      (   | sn   1)

of   course,   we   do   not   actually   know   sn   1!   

recurrence   

       if   i   knew   the   score   of   every   sequence   s1         sn   1,   
i   could   reason   easily   about   sn.   
      but   my   decision   about   sn   would   only   depend   on   
sn   1!   
       so   i   really   only   need   to   know   the   score   of   the   

best   sequence   ending   in   each   sn   1.   

       think   of   that   as   some      precalculakon      that   

happens   before   i   think   about   sn.   

recurrence   

       assume   we   have   the   scores   for   all   pre   xes   of   

the   current   state   sequence.   
      one   score   for   each   possible   last   state   of   the   pre   x.   

scoren(  ) =   (stop |   )      (wn |   )    max

  !

  (   |   !)    scoren   1(  !)

scoren   1(  ) =   (wn   1 |   )    max
scoren   2(  ) =   (wn   2 |   )    max

  !

  !

  (   |   !)    scoren   2(  !)
  (   |   !)    scoren   3(  !)

...

...

score1(  ) =   (w1 |   )      (   | start)

recurrence   

       the   recurrence      boqoms   out      at   start.   
       this   leads   to   a   simple   algorithm   for   calculakng   

all   the   scores.   

scoren(  ) =   (stop |   )      (wn |   )    max

  !

  (   |   !)    scoren   1(  !)

scoren   1(  ) =   (wn   1 |   )    max
scoren   2(  ) =   (wn   2 |   )    max

  !

  !

  (   |   !)    scoren   2(  !)
  (   |   !)    scoren   3(  !)

...

...

score1(  ) =   (w1 |   )      (   | start)

viterbi   algorithm   (scores   only)   

       for   every        in     ,   let:   

score1(  ) =   (w1 |   )      (   | start)

       for   i   =   2   to   n         1,   for   every        in     :   

scorei(  ) =   (wi |   )    max
  !     

       for   every        in     :   
scoren(  ) =   (stop |   )      (wn |   )    max
  !     

  (   |   ")    scorei   1(  ")

  (   |   ")    scoren   1(  ")

       claim:   

max

s

p(s, w |   ,   ) = max
       

scoren(  )

exploikng   distribukvity   

max
       

scoren(  ) = max
       
= max
       
    (wn   1 |   ")    max
  !!     

  (stop |   )      (wn |   )    max
  !     
  (stop |   )      (wn |   )    max
  !     

  (  " |   "")    scoren   2(  "")

  (   |   ")    scoren   1(  ")
  (   |   ")

  (stop |   )      (wn |   )    max
  !     

  (   |   ")

= max
       
    (wn   1 |   ")    max
  !!     
    (wn   2 |   "")    max
  !!!     

  (  " |   "")
  (  "" |   """)    scoren   3(  """)

=

max

  ,  !,  !!,  !!!

  (stop |   )      (wn |   )      (   |   ")

    (wn   1 |   ")      (  " |   "")
    (wn   2 |   "")      (  "" |   """)    scoren   3(  """)

= max
s     n

n+1!i=1
p(s, w |   ,   ) = max
       

max

s

  (si | si   1)      (wi | si)
scoren(  )

i   

suspect   

the       present   

forecast   

is      

pessimiskc   

.   

1e   9   
2e   10   

3e   8   
1e   12   
1e   13   
4e   13   

3e   7   

4e   6   
1e   5   

4e   3   

6e   9   

2e   14   
3e   15   

5e   7   

4e   14   

4e   15   

cd   
dt   
jj   
nn   
nnp   
nns   
prp   
rb   
vb   
vbd   
vbn   
vbp   
vbz   

.   

3e   12   
6e   13   

4e   16   

7e   23   

1e   21   

6e   18   

2e   19   
6e   18   
4e   18   
9e   19   

2e   24   

not   quite   there   

       as   described,   this   algorithm   only   lets   us   
calculate   the   id203   of   the   best   label   
sequence.   

       it   does   not   recover   the   best   sequence!   

understanding   the   scores   

       scorei(  )   is   the   score   of   the   best   sequence   
labeling   up   through   wi,   ignoring   what   comes   
later.   

scorei(  ) = max

s1,...,si   1

p(s1, w1, s2, w2, . . . , si =   , wi)

       similar   trick   as   before:      if   i   know   what   si+1   is,   
then   i   can   use   the   scores   to   choose   si.   
       solukon:      keep   backpointers.   

i   

suspect   

the       present   

forecast   

is      

pessimiskc   

.   

1e   9   
2e   10   

3e   8   
1e   12   
1e   13   
4e   13   

3e   7   

4e   6   
1e   5   

4e   3   

6e   9   

2e   14   
3e   15   

5e   7   

4e   14   

4e   15   

cd   
dt   
jj   
nn   
nnp   
nns   
prp   
rb   
vb   
vbd   
vbn   
vbp   
vbz   

.   

3e   12   
6e   13   

4e   16   

7e   23   

1e   21   

6e   18   

2e   19   
6e   18   
4e   18   
9e   19   

2e   24   

i   

suspect   

the       present   

forecast   

is      

pessimiskc   

.   

1e   9   
2e   10   

3e   8   
1e   12   
1e   13   
4e   13   

3e   7   

4e   6   
1e   5   

4e   3   

6e   9   

2e   14   
3e   15   

5e   7   

4e   14   

4e   15   

cd   
dt   
jj   
nn   
nnp   
nns   
prp   
rb   
vb   
vbd   
vbn   
vbp   
vbz   

.   

3e   12   
6e   13   

4e   16   

7e   23   

1e   21   

6e   18   

2e   19   
6e   18   
4e   18   
9e   19   

2e   24   

viterbi   algorithm   

       for   every        in     ,   let:   

score1(  ) =   (w1 |   )      (   | start)

       for   i   =   2   to   n         1,   for   every        in     :   

scorei(  ) =   (wi |   )    max
  !     
  (   |   ")    scorei   1(  ")

bpi(  ) = arg max
  !     

  (   |   ")    scorei   1(  ")

       for   every        in     :   
scoren(  ) =   (stop |   )      (wn |   )    max
  !     

bpn(  ) = arg max
  !     

  (   |   ")    scoren   1(  ")

  (   |   ")    scoren   1(  ")

viterbi   algorithm:      backtrace   

       aber   calculakng   all   score   and   bp   values,   start   

by   choosing   sn   to   maximize   scoren.   
       then   let   sn   1   =   bpn(sn).   

       in   general,   si   1   =   bpi(si).   

another   example   

kme   

   ies   

like   

an   

10e   15   

8e   13   
6e   14   

1e   14   
8e   16   

2e   4   

2e   7   

2e   9   

4e   20   

arrow   
6e   21   
1e   19   
2e   16   
3e   16   
1e   16   
1e   19   
4e   19   
3e   18   
1e   21   
5e   22   

.   

3e   17   

dt   
in   
jj   
nn   
nnp   
vb   
vbp   
vbz   
.   
,   

another   example   

kme   

   ies   

like   

an   

10e   15   

8e   13   
6e   14   

1e   14   
8e   16   

2e   4   

2e   7   

2e   9   

4e   20   

arrow   
6e   21   
1e   19   
2e   16   
3e   16   
1e   16   
1e   19   
4e   19   
3e   18   
1e   21   
5e   22   

.   

3e   17   

dt   
in   
jj   
nn   
nnp   
vb   
vbp   
vbz   
.   
,   

general   idea:      dynamic   programming   
       use   a   table   data   structure   to   store   parkal   
quankkes   that   will   be   reused   many   kmes.   
      opkmal   substructure:      best   solukon   to   a   problem   

relies   on   best   solukons   to   its   (similar   looking)   
subproblems.   

      overlapping   subproblems:      reuse   a   small   number   

of   quankkes   many   kmes   

       examples:      viterbi,   minimum   levenshtein   

distance,   dijkstra   s   shortest   path   algorithm,         

a   di   erent   view:      best   path   

asymptokc   analysis   

memory:   
       the   table   is   n         |  |.   

runkme:   
       each   cell   in   the   table   requires   o(|  |)   

operakons.   

       total   runkme   is   o(n|  |2).   

lecture   outline   

       markov   models   
       hidden   markov   models   
       viterbi   algorithm   
4.    other   id136   algorithms   for   id48s   
5.    learning   algorithms   for   id48s   

coffee   break   

lecture   outline   

       markov   models   
       hidden   markov   models   
       viterbi   algorithm   
4.    other   id136   algorithms   for   id48s   
5.    learning   algorithms   for   id48s   

how   to   calculate         

given   the   id48   and   a   sequence:   
       the   most   probable   state   sequence?   
2.    the   id203   of   the   word   sequence?   
3.    the   id203   distribukon   over   states,   for   

each   word?   

4.    minimum   risk   sequence   
given   states   and   sequences,   or   just   states:   
5.    the   parameters   of   the   id48   (     and     )?   

problem   2:      p(w   |     ,     )         

       why   might   we   be   interested   in   this   quankty?   

      using   an   id48   as   a   language   model,   we   might   

want   to   compare   two   or   more   sequences.   

      later,   we   will   want   to   maximize   this   quankty   with   

respect   to   the   parameters        and        (learning).   

maximizing   and   summing   

most   probable   state   

total   id203   of   all   

sequence   given   words:   

max

s

p(s, w |   ,   )

combinatorial   

opkmizakon   problem,   
solvable   in   polynomial   
kme.   

state   sequences,   
together   with   words:   
p(w |   ,   )
= !s

p(s, w |   ,   )

a   very   similar   trick   

       the   sum   of   all   label   sequence   probabilikes   
breaks   down   into   the   sum   over   scores   for   
di   erent      nal   symbols.   

!        !s1...sn   1
"
= !       

p(s1 . . . sn   1  , w |   ,   )
%
  (stop |   )      (wn |   )!  "      !s1...sn   2

#$

fn(  )

"

p(s1 . . . sn   2  #, w1 . . . wn   1 |   ,   )
%

fn   1(  ")

#$

a   very   similar   trick   

       as   before,   there   is   a   recurrence.   
       here,   we   exploit   the   fact   that   mulkplicakon   

distributes   over   addikon.   
!        !s1...sn   1
"
= !       

p(s1 . . . sn   1  , w |   ,   )
%
  (stop |   )      (wn |   )!  "      !s1...sn   2

#$

fn(  )

"

p(s1 . . . sn   2  #, w1 . . . wn   1 |   ,   )
%

fn   1(  ")

#$

forward   algorithm   

       for   every        in     ,   let:   
f1(  ) =   (w1 |   )      (   | start)
       for   i   =   2   to   n         1,   for   every        in     :   
fi(  ) =   (wi |   )    !  !     
       for   every        in     :   
fn(  ) =   (stop |   )      (wn |   )    !  !     
p(w |   ,   ) = !       

fn(  )

  (   |   ")    fi   1(  ")

  (   |   ")    fn   1(  ")

a   di   erent   view:      path   sum   

a   di   erent   view:      linear   system   
       |  |   kmes   n   free   variables,   same   number   of   

equakons.   

       can   rewrite   as   a   matrix   inversion   problem!   

f1(  ) =   (w1 |   )      (   | start)
fi(  ) =   (wi |   )    !  !     
fn(  ) =   (stop |   )      (wn |   )    !  !     

  (   |   ")    fi   1(  ")

  (   |   ")    fn   1(  ")

from   forward   to   backward   
       forward   algorithm:      precomputakon   of   

parkal   sums,   from   i   =   1   to   n,   each   involving      
|  |   quankkes,   each   a   sum   over   |  |   
combinakons.   
       asymptokc   analysis   is   the   same   as   viterbi.   

       no   need   to   start   at   the   leb   and   move   right!   
       backward   algorithm   calculates   parkal   sums   

from   the   right   to   the   leb.   

backward   algorithm   

       for   every        in     ,   let:   

       for   i   =   n         1   to   2,   for   every        in     :   

bn(  ) =   (stop |   )      (wn |   )
bi(  ) =   (wi |   )    !  !     
bn(  ) =   (   | start)      (w1 |   )!  !     

       for   every        in     :   

p(w |   ,   ) = !       

b1(  )

  (  " |   )    bi+1(  ")

  (  " |   )    b2(  ")

forward   and   backward   

       two   di   erent   ways   to   rearrange   the   sums   of   

products   of   sums   of   products   of   sums   of   
products.   

       di   erent   intermediate   quankkes.   

pop   quiz   

       might   we   have   done   the   same   thing   with   the   

viterbi   algorithm?   

maximize   and   

mulkply   
operakons   

works   leb   to   right   
works   right   to   leb   

viterbi   

?   

add   and   
mulkply   
operakons   
forward   
backward   

generalizakon:      semirings   

       viterbi   and   forward   algorithms   correspond   to   

exactly   the   same   calculakons,   except   one   
maximizes   and   the   other   sums.   

       one   view:      they   are   the   same  abstract   
algorithm,   instankated   in   two   di   erent   
semirings.   

       informally,   a   semiring   is   a   set   of   values   and   

some   operakons   that   obey   certain   properkes.   

semirings,   more   formally   

       a   set   of   values,   including   a      zero      (addikve   
idenkty   and   mulkplicakve   annihilator)   and   a   
   one      (mulkplicakve   idenkty).   

       two   operakons:         plus      and      kmes.      
         plus      is   associakve   and   commutakve.   
         times      is   associakve.   

          times      distributes   over      plus.      

      this   is   what   we   have   exploited   to   get   e   cient   

algorithms   for   maximizing   and   summing!   

semirings   

set   of   values    nonnegakve   reals    nonnegakve   reals   

real   

viterbi   

   zero      
   one      
   plus      
   kmes      

0   
1   
+   
      

0   
1   

max   
      

some   other   semirings   

can   produce   the   string   at   all.   

       boolean:      use   to   determine   whether   the   id48   
       coun2ng:      use   to   determine   how   many   valid   
labelings   there   are.      
       could   be   less   than   |  |n,   if   some   transikon   and/or   
       log   real:      use   with   log   probabilikes   to   avoid   
       k   best:      use   to      nd   the   k   best   label   sequences.   
       min   cost:      used   with   levenshtein   edit   distance   

emission   probabilikes   are   zero.   

under   ow.   

and   dijkstra   s   algorithms.   

how   to   calculate         

given   the   id48   and   a   sequence:   
       the   most   probable   state   sequence?   
       the   id203   of   the   word   sequence?   
3.    the   id203   distribukon   over   states,   for   

each   word?   

4.    minimum   risk   sequence   
given   states   and   sequences,   or   just   states:   
5.    the   parameters   of   the   id48   (     and     )?   

random   variables   

       so   far   we   ve   focused   on   reasoning   about   the   

whole  sequence  of   states.   

       local   reasoning   was   in   service   of:   

      finding   the   best   s   =   s1   s2         sn   
      finding   the   total   id203   of   w,   averaging   over   

all   possible   values   of   s.   

       it   is   helpful   to   use   a   graphical   representakon   

of   all   our   random   variables.   

graphical   model   representakon   

s1   

s2   

s3   

      

sn   1   

sn   

w1   

w2   

w3   

wn   1   

wn   

       each   node   is   a   random   variable   taking   some   value.   
      

incoming   edges   to   a   r.v.   tell   what   other   r.v.s   it   
condikons   on   directly.   

p(x) = !i

p(xi | parents(xi))

graphical   model   representakon   

s1   

s2   

s3   

      

sn   1   

sn   

   

w1   

w2   

w3   

wn   1   

wn   

   

       each   node   is   a   random   variable   taking   some   value.   
      

incoming   edges   to   a   r.v.   tell   what   other   r.v.s   it   
condikons   on   directly.   

p(x) = !i

p(xi | parents(xi))

problem   1:      most   likely   s   

s1   

s2   

s3   

      

sn   1   

sn   

w1   

w2   

w3   

wn   1   

wn   

gray.   

       sequence   of   words   is   observed,   so   we   color   those   r.v.s   
       we   want   to   assign   values,   colleckvely,   to   the   states,   so   
       goal:      calculate   the   best   value   of   p   for   any   assignment   

we   color   those   red.   

to      red      r.v.s.,   respeckng      gray      evidence   r.v.s.   

problem   2:      id203   of   w   

s1   

s2   

s3   

      

sn   1   

sn   

w1   

w2   

w3   

wn   1   

wn   

color   them   blue.   

       sequence   of   words   is   observed,   so   we   color   those   r.v.s   gray.   
       we   want   to   sum   over   all   se(cid:128)ngs   of   the   state   r.v.s,   so   we   
       goal:      calculate   the   best   value   of   p   for   any   assignment   to   
   red      r.v.s.,   respeckng      gray      evidence   r.v.s.   and   summing   
over   all   possible   assignments   to      blue      r.v.s.   
       there   are   no   red   r.v.s   in   this   problem!   

problem   3:      a   single   si   

s1   

s2   

s3   

      

sn   1   

sn   

w1   

w2   

w3   

wn   1   

wn   

       goal:      calculate   the   best   value   of   p   for   any   
assignment   to      red      r.v.s.,   respeckng      gray      
evidence   r.v.s.   and   summing   over   all   possible   
assignments   to      blue      r.v.s.   

aside   

       probabiliskc   graphical   models   are   an   

extremely   useful   framework   for   nlp,   vision,   
and   other   complex   problems.   

       id48s   are   one   kind   of   graphical   model;   there   

are   many   others   you   may   have   heard   of.   
      bayesian   networks   
      markov   networks   
      factor   graphs   

quankkes   we   need   

s1   

      

si   1   

si   

si+1   

      

sn   

w1   

wi   1   

wi   

wi+1   

wn   

p(w, si |   ,   ) = !s1...si   1 !si+1...sn
= !s1...si   1 !si+1...sn
= !s1...si   1
= fi(si)    bi(si)

p(s1 . . . si . . . sn, w |   ,   )
p(s1 . . . si, w1 . . . wi |   ,   )    p(si+1 . . . sn, wi+1 . . . wn | si,   ,   )

p(s1 . . . si, w1 . . . wi |   ,   ) !si+1...sn

p(si+1 . . . sn, wi+1 . . . wn | si,   ,   )

quankkes   we   need   

s1   

      

si   1   

si   

si+1   

      

sn   

fi   

w1   

wi   1   

wi   

wi+1   

bi   

wn   

p(w, si |   ,   ) = !s1...si   1 !si+1...sn
= !s1...si   1 !si+1...sn
= !s1...si   1
= fi(si)    bi(si)

p(s1 . . . si . . . sn, w |   ,   )
p(s1 . . . si, w1 . . . wi |   ,   )    p(si+1 . . . sn, wi+1 . . . wn | si,   ,   )

p(s1 . . . si, w1 . . . wi |   ,   ) !si+1...sn

p(si+1 . . . sn, wi+1 . . . wn | si,   ,   )

distribukon   over   states   for   wi   
       run   forward   and   backward   algorithms   to   

produce   fi   and   bi.   
      if   we   only   care   about   one   state,   we   can   stop   at   i.   

       for   each        in     :   

p(si =   , w |   ,   ) = fi(  )    bi(  )
p(si =    | w,   ,   ) =

p(si =   , w |   ,   )
!  !     
p(si =   ", w |   ,   )
       note   that   the   denominator   is   p(w   |     ,     ).   

   posterior      

how   to   calculate         

given   the   id48   and   a   sequence:   
       the   most   probable   state   sequence?   
       the   id203   of   the   word   sequence?   
       the   id203   distribukon   over   states,   for   

each   word?   

4.    minimum   risk   sequence   
given   states   and   sequences,   or   just   states:   
5.    the   parameters   of   the   id48   (     and     )?   

building   on   per   word   posteriors   

algorithms   is   o(n|  |2).   

       total   runkme   for   forward   and   backward   
       once   you   have   all   f   and   b   quankkes,   you   can   
calculate   the   posteriors   for   every   word   s   label.   

  si     arg max
       

p(si =    | w,   ,   )
!
$

fi(  )  bi(  )

"#

       this   is   somekmes   called   posterior   decoding.   

posterior   decoding   

       this   approach   to   decoding   exploits   the   full   
distribukon   over   sequences   to   choose   each   
word   s   label.      for   each   i:   

  si     arg max
       

p(si =    | w,   ,   )
!
$

fi(  )  bi(  )

"#

       compare   with   map   decoding   (somekmes   called   

   viterbi      decoding   aber   the   algorithm   that   
accomplishes   it:   

  s     arg max

s

p(s | w,   ,   )

which   one   to   use?   

       they   will   not,   in   general,   give   the   same   label   

sequence.   

       somekmes   one   works   beqer,   somekmes   the   

other.   

       posterior   decoding   can   give   a   label   sequence   

that   itself   gets   zero  id203!   

       there   is   a   way   to   unify   both.   

cost   

imagine   that,   once   we   construct   our   id48,   we   are   
going   to   play   a   game.   

      
       the   id48   will   be   given   a   new   sequence   w.   
       we   must   label   w   using   the   id48.   
       our   label   sequence   s   will   be   compared   to   the   true   one,   
       depending   on   how   badly   we   do,   we   will   pay   a      ne.   
       we   want   to   minimize   the   cost.   
       without   seeing   w,   our   strategy   will   depend   on   how   the   

s*.   

cost   is   de   ned!   

all   or   nothing   cost   

       suppose   we   will   pay   1      if   we   get   the   sequence   
       otherwise   we   pay   nothing.   

wrong,   i.e.,   if   s         s*.   

       what   should   we   do?   
       if   we   trust   our   distribukon   p(w,   s   |  ,     ),   then   

we   should   use   the   most   probable   whole   
sequence   s.   
      viterbi   

hamming   cost   

that   we   label   incorrectly.      

       alternately,   suppose   we   pay   1/n         for   every   word   
       this   is   more   forgiving,   and   suggests   that   we   focus   
on   reasoning   about   each   word   without   worrying   
about   the   coherence   of   the   whole   sequence.   

       what   should   we   do?   
       if   we   trust   our   distribukon   p(w,   s   |  ,     ),   then   we   

should   use   the   most   label   for   each   word.   
       posterior   decoding   

minimum   bayes   risk   

       the   assumpkon   that   we   have   a   good   eskmated   
distribukon   p(w,   s   |  ,     )   leads   naturally   to   the   
following   decoding   rule:   

  s     arg min
ep(s|w,  ,  )[cost(s!, s)]
       pick   the   s   that   is   least   o   ensive,   in   expectakon.   

s!

       with   all   or   nothing   cost,   we   get   map/viterbi   

decoding.   

       with   hamming   cost,   we   get   posterior   decoding.   

word   wise   costs   

exploit   linearity   of   expectakon:   

       if   the   cost   funckon   is   a   sum   of   local   costs,   we   can   
costi(s!i, si)#

ep(s|w,  ,  )[cost(s!, s)] = ep(s|w,  ,  )!"i

= "i

ep(s|w,  ,  )[costi(s!i, si)]

       for   the   hamming   cost,   we   can   make   independent   
decisions   once   we   know   the   expected   local   costs.   

manipulakng   the   cost   funckon   
       suppose   we   have   an   id48   for   named   enkty   

recognikon.   
      tags   are   b,   i,   and   o.   

       if   we   really   care   about   precision   (   nding   only   

correct   named   enkkes,   at   risk   of   missing   
some),   what   cost   funckon   makes   sense?   

       what   if   we   really   care   about   recall?   

one   more   cost   funckon   

       bio   tagging   again.   
       suppose   we   assign   di   erent   costs   to   recall,   

precision,   and   boundary   errors.   

hyp.:   

b   b   

correct:   
b   b   
b   i    merge   
b   o   
recall   
i   b   
i   i    merge   
i   o   
recall   
o   b   
o   o   

recall   

b   i   
split   

recall   
split   

recall   
prec.   

i   b   

b   o   
prec.   
bound.    merge   
recall   

prec.   
bound.    merge   
recall   

prec.   

recall   

i   i   
split   

o   b   

o   o   
i   o   
prec.   
prec.   
bound.    bound.    bound.   

bound.   

split   

bound.   
bound.   
recall   

recall   

prec.   
prec.   
bound.    bound.    bound.   

prec.   

recall   

recall   

prec.   

a   more   complex   posterior   

p(sisi+1 | w,   ,   )

       use   the   same   trick   as   before,   but   pair   up   slightly   

di   erent   forward   and   backward   probabilikes:   

p(sisi+1, w |   ,   ) = !s1...si   1 !si+2...sn

p(s1 . . . sisi+1 . . . sn, w |   ,   )

p(s1 . . . si, w1 . . . wi |   ,   )      (si+1 | si)      (wi+1 | si+1)

= !s1...si   1
   !si+2...sn
= fi(si)      (si+1 | si)      (wi+1 | si+1)    bi+1(si+1)

p(si+2 . . . sn, wi+1 . . . wn | si+1,   ,   )

pairwise   posterior   

s1   

      

si   1   

si   

si+1   

      

sn   

fi   

w1   

wi   1   

wi   

wi+1   

bi+1   

wn   

p(sisi+1, w |   ,   ) = !s1...si   1 !si+2...sn

p(s1 . . . sisi+1 . . . sn, w |   ,   )

p(s1 . . . si, w1 . . . wi |   ,   )      (si+1 | si)      (wi+1 | si+1)

= !s1...si   1
   !si+2...sn
= fi(si)      (si+1 | si)      (wi+1 | si+1)    bi+1(si+1)

p(si+2 . . . sn, wi+1 . . . wn | si+1,   ,   )

a   problem   

       we   can   label   each   adjacent   pair   of   words,   but   

nothing   guarantees   that   our   labels   will   be   
consistent   with   each   other.      

pop   quiz   

1.      can   you   think   of   a   cost   funckon   such   that   

minimum   bayes   risk   decoding   can   t   be   done   in   
polynomial   kme?   

quiz   answer   (1)   

       every   word   must   get   a   di   erent   label.   

      equivalently,   we   can   use   each   label   at   most   once.   

       each   label   must   be   used   exactly   once.   

      hamiltonian   path.   

pop   quiz   

part   2:   
       we   want   to   choose   a   single  sequence   of   labels   

to   minimize   risk.   

       the   scoring   funckon   factors   into   local   scores   

(sum   of   pairwise   posteriors).   

       how   do   we   do   it?   

quiz   answer   (2)   

       pairwise   local   costs   are   kind   of   like   transikon   

probabilikes.   
      except   we   want   to   minimize   their   sum   rather   than   

maximize   their   product.   

       viterbi   algorithm,   but   with   min   cost   semiring,   

a   di   erent      transikon      score,   and   no   
   emission      score.   

pairwise   minimum   bayes   risk   

       first,   run   forward   and   backward.   
       use   posteriors   to   score   all   possible   label   pairs   

for   all   adjacent   words.   

       use   a   viterbi   like   algorithm   to      nd   the   best   

scoring,   consistent   labeling.   
      use   min   cost   semiring.   
      algorithm   scores   label   pairs   by   local   cost   kmes   

posterior   id203   (local   risk).   

how   to   calculate         

given   the   id48   and   a   sequence:   
       the   most   probable   state   sequence?   
       the   id203   of   the   word   sequence?   
       the   id203   distribukon   over   states,   for   

each   word?   

       minimum   risk   sequence   
given   states   and   sequences,   or   just   states:   
5.    the   parameters   of   the   id48   (     and     )?   

lecture   outline   

       markov   models   
       hidden   markov   models   
       viterbi   algorithm   
       other   id136   algorithms   for   id48s   
5.    learning   algorithms   for   id48s   

learning   id48s   

       typical   starkng   point:      we   have   some   data   to   
learn   from,   and   we   know   how   many   states   the   
id48   has.   
      we   may   also   have   constraints   on   the   states,   but   

assume   for   now   that   we   do   not.   

       two   main   possibilikes:   

      supervised:      we   have   complete   data:      example   
      unsupervised:      we   only   have   examples   of   w.   

pairs   (w,   s).   

supervised   learning   of   id48s   

       the   building   blocks   of   id48s   are   mulknomial   

distribukons   
        :      distribukon   over   next   state   given   current   state   
        :      distribukon   over   word   given   current   state   
       stakskcs   o   ers   us   the   maximum   likelihood   

principle:   

!    ,     "     arg max
!  ,  "

p(s, w |   ,   )

separability   of   learning   

       with   observed   data,   each   state   s   transikon   
and   emission   distribukons   can   be   eskmated   
separately   
      from   each   other   
      from   those   of   all   other   states   

       the   result   is   that   learning   is   very   simple   and   

fast.   

id113   by   relakve   frequencies   
       (i   m   skipping   the   derivakon;   it   involves   log   

likelihood,   a   lagrangian   mulkplier,   and   some   
di   erenkal   calculus.)   
    (   |   !) =

freq(    !)
freq(  !)

freq!   
w "

freq(  )

    (w |   ) =

graphical   models   view   

     

s1   

s2   

s3   

      

sn   1   

sn   

w1   

w2   

w3   

     

wn   1   

wn   

learning   with   a   prior   

       the   techniques   described   so   far   are   examples   of   
       id113   works   well   when   there   is   a   lot   of   data.   

maximum   likelihood   es2ma2on   (id113).   

       we   never   have   as   much   as   we   d   like.   

       learning   with   a   prior   is   a   way   to   inject   some   

background   knowledge   and   avoid   over   (cid:128)ng   to   
the   training   data.      

       map   learning:   

!    ,     "     arg max
!  ,  "

p(s, w |   ,   )    p(  ,   )

this   part   is   new   

priors   for   id48s   

       id48s   are   built   out   of   mulknomial   distribukons.      
       an   easy   prior   for   the   mulknomial   distribukon   is   
the   dirichlet   distribukon.   
       simplest   version:      symmetric,   non   sparse   dirichlet.   

       in   prackce:   

       choose     t   >   1   for   transikons   and     e   >   1   for   emissions.   
       before   normalizing   frequencies,   add     t         1   to   
transikon   counts   and     e         1   to   emission   counts.   
       exactly   the   same   as   addikve   smoothing   in   language   

models.   

graphical   models   view   

     

  t   

s1   

s2   

s3   

      

sn   1   

sn   

w1   

w2   

w3   

     

wn   1   

wn   

  e   

unsupervised   learning   of   id48s   
       historically   (going   back   to   the   1960s),   we   do   
not   observe   the   states,   even   during   training   
kme.   
      hence   the   name:      hidden   markov   models.   

       the   earliest   instance   of   a   parameter   

eskmakon   problem   where   the   data   are   
incomplete.   

       how   do   we   learn   only   from   w?   

we   already   have   all   the   tools!   
       stakskcs:      maximum   likelihood   eskmakon.   
      counkng   events   and   normalizing   the   counts.   
       computakon:      id136   about   posteriors.   

      forward   and   backward   algorithms.   

graphical   models   view   

     

states   are   now   missing!   

s1   

s2   

s3   

      

sn   1   

sn   

w1   

w2   

w3   

     

wn   1   

wn   

mixed   id136   

       mixed   id136   problems         where   we   want   to   
sum   out   some   random   variables   while   ge(cid:128)ng   
values   for   others         are   hard   in   general.   
       indeed,      nding   the   id113   is   an   np   hard   

problem!   

       opkmizakon   view:      we   are   opkmizing   a   non   

convex   funckon   (of   the   parameters).   

high   level   view   of   em   

       em   stands   for      expectakon   maximizakon.      

s1   

w
1   

s2   

w
2   

     

s3   

      

w
3   

     

sn
   1   

wn   1   

sn   

w
n   

s1   

w
1   

s2   

w
2   

     

s3   

      

w
3   

     

sn
   1   

wn   1   

sn   

w
n   

e   step:      infer   posterior   
distribukon   over   missing   
data.   

m   step:      maximum   
likelihood   eskmakon   with   
sob   values   for   missing   
data.   

procedural   view   of   em   
       begin   with   an   inikal   eskmate   of   the   
parameters   of   the   id48   (     and     ).   

       iterate:   

      e   step:      calculate   id203   of   each   possible   
transikon   and   each   possible   emission   at   each   
posikon.   

      m   step:      re   eskmate   parameters   to   maximize   

likelihood   of      complete      data.   

e   step   

       calculate   p(s   |   w,     ,     )   for   each   s   and   count   

each   s   proporkonal   to   its   id203.   

or   

       calculate   p(si   |   w,     ,     )   for   each   i,   and   count   
each   emission   of   wi   from   si   proporkonal   to   its   
id203.   
       calculate   p(si   si+1   |   w,     ,     )   for   each   i,   and   
count   each   transikon   from   si   to   si+1   
proporkonal   to   its   id203.   

e   step:      per   word   and   pairwise   

posteriors   

       given   w   and   current   parameters,   run   forward   
and   backward   algorithms   to   obtain,   for   each   i,   
the   posteriors:   
      p(si   |   w,     ,     )      
      p(si   si+1   |   w,     ,     )      

       think   of   these   as   soc   or   frac   onal   counts   of   

transikon   and   emission   events.   

sob   counts   from   posteriors   

p(si =   , si+1 =   ! | w,   ,   )
fi(  )      (  ! |   )      (wi+1 |   !)    bi(  !)

!freq(    !) = "i
= "i
!freq#   
w $ = "i:wi=w
= "i:wi=w
!freq(  ) = "i
= "i

p(si =    | w,   ,   )
fi(  )    bi(  )

p(si =    | w,   ,   )
p(si =    | w,   ,   )

procedural   view   of   em   
       begin   with   an   inikal   eskmate   of   the   
parameters   of   the   id48   (     and     ).   

       iterate:   

      e   step:      calculate   id203   of   each   possible   
transikon   and   each   possible   emission   at   each   
posikon.   

      m   step:      re   eskmate   parameters   to   maximize   

likelihood   of      complete      data.   

m   step:      id113   by   relakve   frequencies   
       when   we   observed   all   the   data,   we   used   hard   
freq!   
w "

freq(    !)
freq(  !)

    (   |   !) =

counts:   

freq(  )

    (w |   ) =
       now   we   do   the   same   with   sob   counts:   
    (   |   !) = !freq(  !  )
!freq(  !)
    (w |   ) = !freq"   
w #
!freq(  )

em:      assurances   

       each   iterakon   of   em   will   give   us   an   eskmate   

with   a   beqer   likelihood   than   the   last.   
       eventually   we   will   converge   to   a   local  

opkmum   (or   saddle   point).   
      we   usually   do   not   worry   about   saddle   points.   
      where   you   end   up   depends   on   where   you   start.   

the   importance   of   inikalizakon   

   hard      em   

       instead   of   using   forward   backward   to   get   
frackonal   counts,   we   can   use   viterbi   to   get   
hard   counts.   

       this   equates   to:   

s1   

w
1   

s2   

w
2   

     

s3   

      

w
3   

     

sn
   1   

wn   1   

sn   

w
n   

s1   

w
1   

s2   

w
2   

     

s3   

      

w
3   

     

sn
   1   

wn   1   

sn   

w
n   

   hard      em   

       can   be   understood   as   coordinate   ascent   on   

this   maximizakon   problem:   

max
  ,  ,s

p(s, w |   ,   )

       some   people   prefer   this.   

      faster   to   converge   (but   to   a   di   erent   solukon).   
      viterbi   algorithm   instead   of   forward   backward.   
      maybe   more   briqle.   

terminology   

       expectakon   maximizakon   (em)   is   a   very   
general   technique,   not   just   for   id48s.   
      applicable   to   any   generakve   model!   
      you   may   have   seen   it   for   mixtures   of   gaussians   or   

other   kinds   of   id91.   

      k   means   id91   is   a   kind   of   hard   em.   
       somekmes   the   id48   version   is   called      

baum   welch   training   or   forward   backward   
training.   

id48s   and      
weighted   finite   state   machines   

finite   state   machines   

from   formal   language   theory   and   theory   of   

computakon:   

       finite   set   of   states.   
       set   of   allowed   transikons   between   states.   
       nondeterminiskc   walk   among   states.   
       each   state   (alternately,   each   transikon)   

generates   a   symbol.   

id48s   are   probabiliskc   fsas   

go   from      nondeterminiskc      to      probabiliskc.      
       put   a   id203   distribukon   on   the   

transikons   out   of   each   state.   

       put   a   id203   distribukon   on   the   emissions   

from   each   state.   

powerful   generalizakon   

       if   the      nite   state   machine   reads   one   sequence   
and      transcribes      it   into   another   sequence,   we   
have   a      nite   state   transducer.   
       somekmes:         read   one   tape   and   write   one   tape.      

       we   can   make   these   probabiliskc   as   well,   in   a   lot   
       allows   composikon   of   stochaskc   relakons,   or   

of   di   erent   ways.   

chaining   together   of   string   to   string   
transformakons.   

       algorithms   for   id136   and   learning   are   similar   

to   what   we   have   seen.   

examples   of   composed   fsts   

       speech   recognikon:         

acouskc   signal         pronounced   phonemes         
canonicalized   words.   

       translakon:         

words   in   czech         morphemes   in   czech         
morphemes   in   slovak         words   in   slovak   

more   advanced   topics   

fsts   

semi   id48s)   

       feature   based   parameterizakons   of   id48s   and   
       discriminakve   versions   of   id48s   (e.g.,   crfs)   
       weakening   independence   assumpkons   (e.g.,   
       generalizing   id48s   and   fsts   to   probabiliskc   and   
weighted   context   free   grammars   (and   beyond)   to   
model   long   distance   interackons   and   reordering.   
       bayesian   id136   and   learning   
       nonparametric   priors   (e.g.,   the      in   nite      id48)   

lecture   outline   

       markov   models   
       hidden   markov   models   
       viterbi   algorithm   
       other   id136   algorithms   for   id48s   
       learning   algorithms   for   id48s   

thanks!   

