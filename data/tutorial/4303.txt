brief introduction to machine learning

without deep learning

kyunghyun cho

courant institute of mathematical sciences and

center for data science,

new york university

january 28, 2019

abstract

this is a lecture note for the course csci-ua.0473-001 (intro to machine learning) at the department of computer
science, courant institute of mathematical sciences at new york university. the content of the lecture note was
selected to    t a single 12-week-long course and to mainly serve undergraduate students majoring in computer science.
many existing materials in machine learning therefore had to be omitted.

for a more complete coverage of machine learning (with math!), the following text books are recommended in

addition to this lecture note:

       pattern recognition and machine learning    by chris bishop [2]
       machine learning: a probabilistic perspective    by kevin murphy [16]
       a course in machine learning    by hal daum  e1
for practical exercises, python scripts based on numpy and scipy are available online. they are under heavy
development and subject to frequent changes over the course (and over the years i will be spending at nyu). i
recommend you to check back frequently. again, these are not exhaustive, and for a more complete coverage on
machine learning practice, i recommend the following book:

       introduction to machine learning with python    by andreas m  uller and sarah guido
this course intentionally omits any recent advances in deep learning. this decision was made, because there
is an excellent course    deep learning    taught at the nyu center for data science by yann lecun himself. also,
every undergrad, who thinks they are interested in and passionate about machine learning, self-teaches themself deep
learning, and i found it necessary for me to focus on anything that does not look like deep learning to give them a more
balance view of machine learning. of course, i have largely failed.

1 available at http://ciml.info/.

contents

1 classi   cation

.
.

.
.

.
.

.
.

.
.

1.4.1 over   tting: generalization error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.2 validation .
1.4.3 over   tting and id173 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1 supervised learning .
1.2 id28 .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
1.3 one classi   er, many id168s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.1 classi   cation as scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.2
support vector machines: max-margin classi   er . . . . . . . . . . . . . . . . . . . . . . . .
1.4 over   tting, id173 and complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
1.5 multi-class classi   cation .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.6 what does the weight vector tell us? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.7 nonlinear classi   cation .
.
.
feature extraction .
k-nearest neighbours: fixed basis networks . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.7.1
1.7.2
1.7.3 id80s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.7.4 adaptive basis function networks or deep learning . . . . . . . . . . . . . . . . . . . . .

.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.8 further topics(cid:63)

. . .

.
.

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

2 regression

.

.

.

.

.
.

.
.

2.1 id75 .

.
2.1.1 id75 .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 recap: id203 and distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 bayesian id75 .
2.3.1 bayesian id75 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.2 bayesian supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . .
2.3.3

further topic: gaussian process regression(cid:63) . . . . .

. . . . .

. . . . .

. . .

.

.

.

3 id84 and id105

3.1 id84: problem setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 id105: problem setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
principal component anslysis: traditional derivation . . . . . . . . . . . . . . . . . . . . .
3.2.1
.
3.2.2
pca: minimum reconstruction error with orthogonality constraint . . . . . . . . . . . . .
3.2.3 non-negative id105: nmf . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
3.3 deep autoencoders: nonlinear id105 . . . . . . . . . . . . . . . . . . . . . . . . . .
3.4 id84 beyond id105 . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4.1 metric multi-dimensional scaling (mds) . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.5 further topics(cid:63)

. . .

.

.

.

.

.

.

.

.

.

.

.

2

5
5
7
9
9
10
12
12
13
14
16
18
19
19
21
23
24
26

28
28
28
29
34
34
38
39

40
40
41
42
45
47
49
50
50
51

52
52
54
54
57

59
59

4 id91

id116 id91 .

4.1
4.2 mixture of gaussians and id48 . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 mixture of gaussians .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.2 mixture of gaussians to id48 . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
.

.

.

.

.

.

.

.

.

.

5 sequential decision making

5.1 sequential decision making as a series of classi   cation . . . . . . . . . . . . . . . . . . . . . . .

.

3

exercises

the following exercise materials, based on python, numpy, scipy, autograd and scikit-learn, have been prepared by
varun d n.2 thanks, varun! they are available at https://github.com/nyu-dl/intro_to_ml_lecture_
note/tree/master/exercises:

1. id28: logistic_regression_1.ipynb,

logistic_regression_2.ipynb, mnist_classification.ipynb

2. introduction to autograd: autograd_introduction.ipynb

3. support vector machines: id166.ipynb, id166_vs_logreg.ipynb

4. id80s: rbfn.ipynb

5. k-nearest neighbour classi   er: knn1.ipynb

6. principal component analysis: pca_mnist.ipynb, pca_newsgroup20.ipynb

7. non-negative id105: nmf.ipynb, nmf_newsgroup20.ipynb

8. bayesian id75 (requires pymc3): bayesian_linear_regression_mcmc.ipynb

these are well-written, but come also with some bugs. i will    x them next year, if i happen to teach the course once
more.

2 https://www.linkedin.com/in/varundn/

4

chapter 1

classi   cation

1.1 supervised learning
in supervised learning, our goal is to build or    nd a machine m that takes as input a multi-dimensional vector x     rd
and outputs a response vector y     rd(cid:48). that is,

m : rd     rd(cid:48).

of course this cannot be done out of blue, and we    rst assume that there exists a reference design m    of such a
machine. we then re   ne our goal as to build or    nd a machine m that imitates the reference machine m    as closely as
possible. in other words, we want to make sure that for any given x, the outputs of m and m    coincide, i.e.,

m(x) = m   (x), for all x     rd.

(1.1)

this is still not enough for us to    nd m, because there are in   nitely many possible m   s through which we must search.
we must hence decide on our hypothesis set h of potential machines. this is an important decision, as it directly
in   uences the dif   culty in    nding such a machine. when your hypothesis set is not constructed well, there may not
be a machine that satis   es the criterion above.

we can state the same thing in a slightly different way. first, let us assume a function     that takes as input the

output of m   , a machine m and an input vector x, and returns how much they differ from each other, i.e.,

where r+ is a set of non-negative real numbers. as usual in our everyday life, the smaller the output of     the more
similar the outputs of m and m   . an example of such a function would be

    : rd(cid:48)    h    rd     r+,

(cid:26) 0,

   (y,m,x) =

if y = m(x)

1, otherwise

.

it is certainly possible to tailor this distance function, or a per-example cost function, for a speci   c target task. for
instance, consider an intrusion detection system m which takes as input a video frame of a store front and returns a
binary indicator, instead of a real number, whether there is a thief in front of the store (0: no and 1: yes). when there
is no thief (m   (x) = 0), it does not cost you anything when m agrees with m   , but you must pay $10 for security
dispatch if m predicted 1. when there is a thief in front of your store (m   (x) = 1), you will lose $100 if the alarm fails
to detect the thief (m(x) = 0) but will not lose any if the alarm went off. in this case, we may de   ne the per-example
cost function as

   (y,m,x) =

note that this distance is asymmetric.

          0,

10,
100,

if y = m(x)
if y = 0 and m(x) = 1
if y = 1 and m(x) = 0

.

5

given a distance function    , we can now state the supervised learning problem as    nding a machine m, with in a

given hypothesis set h, that minimizes its distance from the reference machine m    for any given input. that is,

   (m   (x),m,x)dx.

(1.2)

(cid:90)

argmin
m   h

rd

you may have noticed that these two conditions in eqs. (1.1)   (1.2) are not equivalent. if a machine m satis   es the
   rst condition, the second conditions is naturally satis   ed. the other way around however does not necessarily hold.
even then, we prefer the second condition as our ultimate goal to satisfy in machine learning. this is because we often
cannot guarantee that m    is included in the hypothesis set h. the    rst condition simple becomes impossible to satisfy
when m    /    h, but the second condition gets us a machine m that is close enough to the reference machine m   . we
prefer to have a suboptimal solution rather than having no solution.
the formulation in eq. (1.2) is however not satisfactory. why? because not every point x in the input space rd
is born equal. let us consider the previous example of a video-based intrusion detection system again. because the
camera will be installed in a    xed location pointing toward the store front, video frames will generally look similar to
each other, and will only form a very small subset of all possible video frames, unless some exotic event happens. in
this case, we would only care whether our alarm m works well for those frames showing the store front and people
entering or leaving the store. whether the distance between the reference machine and my alarm is small for a video
frame showing the earth from the outer space would not matter at all.

and, here comes id203. we will denote by px (x) the id203 (density) assigned to the input x under the
distribution x, and assume that this id203 re   ects how likely the input x is. we want to emphasize the impact of
the distance     on likely inputs (high px (x)) while ignoring the impact on unlikely inputs (low px (x)). in other words,
we weight each per-example cost with the id203 of the corresponding example. then the problem of supervised
learning becomes

argmin
m   h

rd

px (x)   (m   (x),m,x)dx = argmin
m   h

ex   x [   (m   (x),m,x)] .

(1.3)

are we done yet? no, we still need to consider one more hidden cost in order to make the description of supervised
learning more complete. this hidden cost comes from the operational cost, or complexity, of each machine m in the
hypothesis set h. it is reasonable to think that some machines are cheaper or more desirable to use than some others
are. let us denote this cost of a machine by c(m), where c : h     r+. our goal is now slightly more complicated in
that we want to    nd a machine that minimizes both the cost in eq. (1.3) and its operational cost. so, at the end, we get

(cid:90)

(cid:125)
(cid:124)
ex   x [   (m   (x),m,x)] +   c(m)

(cid:123)(cid:122)

,

r=expected cost

argmin
m   h

(1.4)

where        r+ is a coef   cient that trades off the importance between the expected distance (between m    and m) and
the operational cost of m.
in summary, supervised learning is a problem of    nding a machine m such that has bot the low expectation of the

distance between the outputs of m    and m over the input distribution and the low operational cost.

in reality
it is unfortunately impossible to solve the minimization problem in eq. (1.4) in reality. there are so many
reasons behind this, but the most important reason is the input distribution px or lack thereof. we can decide on a
distance function     ourselves based on our goal. we can decide ourselves a hypothesis set h ourselves based on our
requirements and constraints. all good, but px is not controllable in general, as it re   ects how the world is, and the
world does not care about our own requirements nor constraints.

let   s take the previous example of video-based intrusion system. our reference machine m    is a security expert
who looks at a video frame (and a person within it) and determines whether that person is an intruder. we may decide
to search over any arbitrary set of neural networks to minimize the expected loss. we have however absolutely no idea
what the precise id203 p(x) of any video frame. instead, we only observe x   s which was randomly sampled from
the input distribution by the surrounding environment. we have no access to the input distribution itself, but what
comes out of it.

we only get to observe a    nite number of such samples x   s, with which we must approximate the expected cost
in eq. (1.4). this approximation method, that is approximation based on a    nite set of samples from a id203

6

distribution, is called a monte carlo method. let us assume that we have observed n such samples: (cid:8)x1, . . . ,xn(cid:9).

then we can approximate the expected cost by

(cid:125)
(cid:124)
ex   x [   (m   (x),m,x)] +   c(m)

(cid:123)(cid:122)

expected cost

=

1
n

(cid:124)

n

   

n=1

   (m   (xn),m,xn) +   c(m)

+  ,

(1.5)

(cid:123)(cid:122)

  r=empirical cost

(cid:125)

where    is an approximation error. we will call this cost, computed using a    nite set of input vectors, an empirical
cost.

id136 we have so far talked about what is a correct way to    nd a machine m for our purpose. we concluded that
we want to    nd m by minimizing the empirical cost in eq. (1.5). this is a good start, but let   s discuss why we want
to do this    rst. there may be many reasons, but often a major complication is the expense of running the reference
machine m    or the limited access to the reference machine m   . let us hence make it more realistic by assuming that
we will have access to m    only once at the very beginning together with a set of input examples. in other words, we
are given

dtra =(cid:8)(x1,m   (x1)), . . . , (xn,m   (xn))(cid:9) ,

to which we refer as a training set. once this set is available, we can    nd m that minimizes the empirical cost from
eq. (1.5) without ever having to query the reference machine m   .
now let us think of what we would do when there is a new input x /    dtra. the most obvious thing is to use   m
  m(x). is there any other way? another way is to use all the models in the
that minimizes the empirical cost, i.e.,
hypothesis set, instead of using only one model. obviously, not all models were born equal, and we cannot give all
of them the same chance in making a decision. preferably we give a higher weight to the machine that has a lower
empirical cost, and also we want the weights to sum to 1 so that they re   ect a properly normalized proportion. thus,
let us (arbitrarily) de   ne, as an example, the weight of each model as:

where j corresponds to the empirical cost, and

  (m) =

1
z

exp (   j(m,dtra)) ,

z =    
m   h

exp (   j(m,dtra))

is a id172 constant.

with all the models and their corresponding weights, i can now think of many strategies to infer what the output
of m    given the new input x. indeed, the    rst approach we just talked about corresponds to simply taking the output
of the model that has the highest weight. perhaps, i can take the weighted average of the outputs of all the machines:

  (m)m(x),

   
m   h

(1.6)

which is equivalent to e [m(x)] under our arbitrary construction of the weights.1 we can similarly check the variance
of the prediction. perhaps i want to inspect a set of outputs from the top-k machines according to the weights.

we will mainly focus on the former approach, which is often called maximum a posteriori (map), in this course.
however, in a few of the lectures, we will also consider the latter approach in the framework of bayesian modelling.

1.2 id28
one way to build a machine that tackles binary classi   cation is to build it to output the id203 p(c|x), where
c     {   1,1}.

1 is it really arbitrary, though?

7

hypothesis set to do so, let us    rst de   ne a machine m. m takes as input a vector x     rd and returns a id203
p(c|x)     [0,1] rather than {0,1}:

m(x) =    (w(cid:62)   x),

(1.7)

where    is a sigmoid function de   ned as

   (a) =

1

1 + exp(   a)

and is bounded by 0 from below and by 1 from above. this machine in other words does not return the prediction, but
the id203 of the prediction being positive (1). that is,

naturally, our hypothesis set is now

p(c = 1|x) = m(x).

(cid:110)
m|m(x) =    (w(cid:62)   x),w     rd+1(cid:111)

,

h =

where   x = [x;1] denotes concatenating 1 at the end of the input vector as before.

distance the distance is not trivial to de   ne in this case, because the things we want to measure the distance between
are not directly comparable. one is an element in a discrete set (0 or 1), and the other is a id203. it is helpful
now to think instead about how often a machine m will agree with the reference machine m   , if we randomly sample
the prediction given its output distribution p(c|x). this is exactly equivalent to p(c = m   (x)|x). in this sense, the
distance between the reference machine m    and our machine m given an input vector x is smaller than this frequency
of m being correct is larger, and vice versa. therefore, we de   ne the distance as the negative log-id203 of the
m   s output being correct:

   (m   (x),m,x) =    log p(c = m   (x)|x)

=    (m   (x)logm(x) + (1    m   (x))log(1    m(x))),

(1.8)

where p(c = 1|x) = m(x). the latter equality comes from the de   nition of bernoulli distribution.2
with this de   nition of our distance, how do we adjust w of m to lower it? perhaps we can creatively think of a rule
to update w to minimize this distance function. it however turned out to be not too trivial with this id28
distance.3 thus, we now turn to calculus, and use the fact that the gradient of a function points to the direction toward
which its output increases (at least locally).

the gradient of the above id28 distance function with respect to the weight vector w is4

when we move the weight vector ever so slightly in the opposite direction, the id28 distance in eq. (1.26)
will decrease. that is,

   w   (m   (x),m,x) =    (m   (x)    m(x))  x.

(1.9)

w     w +    (m   (x)    m(x))   x,
where        r is a small scalar and called either a step size or learning rate.
scalar p     [0,1], and the id203 of x being c0 as 1    p. when c0 = 0 and c1 = 1, we can write the id203 of x as

2 a binary, or bernoulli, random variable x may take one of two values c0 and c1 (often 0 and 1). the id203 of x being c1 is de   ned as a

(1.10)

p(x) = px (1    p)(1    x),

and its logarithm is

log p(x) = x log p + (1    x)log(1    p).

3 it may be trivial to some who have great mathematical intuition.
4 the step-by-step derivation of this is left to you as a homework assignment.

8

coincidence?

learning with the distance function de   ned, we can write a full empirical cost as

j(w,dtra) =    

1
n

n

   

n=1

yn logm(xn) + (1    yn)log(1    m(xn)).

we assume that the operational cost, or complexity, of each m can be ignored. similarly to what we have done for
minimizing (decreasing) the distance between m and m    given a single input vector, we will use the gradient of the
whole empirical cost function to decide how we change the weight vector w. the gradient is

   wj =    

1
n

n

   

n=1

   wd(yn,m,xn),

which is simply the average of the gradients of the distances from eq. (1.9) over the whole training set.

the fact that the empirical cost function is (twice) differentiable with respect to the weight vector allows us to use
any advanced gradient-based optimization algorithm. perhaps even more importantly, we can use automatic differ-
entiation to compute the gradient of the empirical cost function automatically.5 any further discussion on advanced
optimization algorithms is out of this course   s scope, and i recommend you the following courses:

    ds-ga 3001.03: optimization and computational id202 for data science
    csci-ga.2420 numerical methods i
    csci-ga.2421 numerical methods ii

1.3 one classi   er, many id168s
1.3.1 classi   cation as scoring
let us use f as a shorthand for denoting the dot product between the weight vector w and an input vector x augmented
with an extra 1, that is f (x;w) = w(cid:62)   x. instead of y     {0,1} as a set of labels (negative and positive), we will switch
to y     {   1,1} to make later equations less cluttered. now, let us de   ne a score function6 that takes as input an input
vector x, the correct label y (returned by a reference machine m   ) and the weight vector (or you can say the machine
m itself):

s(y,x;m) = yw(cid:62)   x.

(1.11)

given any machine that performs binary classi   cation, this score function tells us whether a given input vector x
is correctly classi   ed. if the score is larger than 0, it was correctly classi   ed. otherwise, the score would be smaller
than 0. in other words, the score function de   nes a decision boundary of the machine m, which is de   ned as a set of
points at which the score is 0, i.e.,

b(m) = {x|s(m(x),x;m) = 0} .
5 some of widely-used software packages that implement automatic differentiation include
    autograd for python: https://github.com/hips/autograd
    autograd for torch: https://github.com/twitter/torch-autograd
    theano: http://deeplearning.net/software/theano/
    tensorflow: https://www.tensorflow.org/

throughout this course, we will use autograd for python for demonstration.

6 note that the term    score    has a number of different meanings. for instance, in statistics, the score is de   ned as a gradient of the log-id203,

that is

    log p(x)

    x

.

9

figure 1.1: the    gure plots three major loss
functions   0-1 loss, log loss and hinge loss   
with respect to the output of a score function
s. note that the log loss and hinge loss are
upper-bound to the 0-1 loss.

when the score function s is de   ned as a linear function of the input vector x as in eq. (1.11), the decision boundary
corresponds to a linear hyperplane. in such a case, we call the machine a linear classi   er, and if the reference machine
m    is a linear classi   er, we call this problem of classi   cation linear separable.

with this de   nition of a score function s, the problem of classi   cation is equivalent to    nding the weight vector w,
or the machine m, that positively scores each pair of an input vector and the corresponding label. in other words, our
empirical cost function for classi   cation is now

j(m,dtra) =

1
|dtra|

   

(y,x)   dtra

(cid:124)
(cid:125)
sign(   s(y,x;m))

(cid:123)(cid:122)

d0-1=0-1 loss

.

(1.12)

log loss: id28 the distance7 function of id28 in eq. (1.26) can be re-written as

dlog(y,x;m) =

1

log2

log(1 + exp(   s(y,x;m))),

(1.13)

where y     {   1,1} instead of {0,1}, and the score function s is de   ned in eq. (1.11).8 this id168 is usually
referred to as log loss.
how is this log loss related to the 0-1 loss from eq. (1.12)? as shown in fig. 1.1, the log loss is an upper-bound

of the 0-1 loss. that is,

dlog(y,x;m)     d0-1(y,x;m) for all s(y,x;m)     r.

by minimizing this upper-bound, we can indirectly minimize the 0-1 loss. of course, minimizing the upper-bound
does not necessarily minimize the 0-1 loss, but we can be certain that the 0-1 loss, or true classi   cation loss, is lower
than how far we have minimized the log loss.

1.3.2 support vector machines: max-margin classi   er
hinge loss one potential issue with the log loss in eq. (1.13) is that it is never 0:

dlog(y,x;m) > 0.

7 from here on, i will use both distance and loss to mean the same thing. this is done to make terminologies a bit more in line with how others

use.

8 homework assignment: show that eq. (1.26) and eq. (1.13) are equivalent up to a constant multiplication for binary id28.

10

   4   2024score0.00.51.01.52.02.53.00-1lossloglosshingelosswhy is this an issue? because it means that the machine m wastes its modelling capacity on pushing those examples
as far away from the decision boundary as possible even if they were already correctly classi   ed. this is unlike the
0   1 loss which ignores any correctly classi   ed example.

let us introduce another id168, called hinge loss, which is de   ned as
dhinge(y,x;m) = max(0,1    s(y,x;m)).

similarly to the log loss, the hinge loss is always greater than or equal to the zero-one loss, as can be seen from fig. 1.1.
we minimize this hinge loss and consequently minimize the 0   1 loss.9

what does this imply? it implies that minimizing the empirical cost function that is the sum of hinge losses will    nd
a solution in which all the examples are at least a unit-distance (1) away from the decision boundary (s(y,x;m) = 0).
once any example is further than a unit-distance away from and on the correct side of the decision boundary, there will
not be any penalty, i.e., zero loss. this is contrary to the log loss which penalizes even correctly classi   ed examples
unless they are in   nitely far away from and on the correct side of the boundary.

max-margin classi   er
it is time for a thought experiment. we have only two unique training examples; one posi-
tive example   x+ and one negative example   x   . we can draw a line l between these two points. any linear classi   er
perfectly classi   es these two examples into their correct classes as long as the decision boundary, or separating hyper-
plane, meets the line l connecting the two examples. because we are in the real space, there are in   nitely many such
classi   ers. among these in   nitely many classi   ers, which one should we use? should the intersection between the
separating hyperplane and the connecting line l be close to either one of those examples? or, should the intersection
be as far from both points as possible? an intuitive answer is    yes    to the latter: we want the intersection to be as far
away from both points as possible.

let us de   ne a margin    as the distance between the decision boundary (w(cid:62)   x = 0) and the nearest training example
  x, of course, (loosely) assuming that the decision boundary classi   es most of, if not all, the training examples correctly.
this assumption is necessary to ensure that there are at least one correctly classi   ed example on each side of the
decision boundary. the above thought experiment now corresponds to an idea of    nding a classi   er that has the
largest margin, i.e., a max-margin classi   er.

the distance to the nearest positive and negative examples can be respectively written down as

w(cid:62)   x+
(cid:107)w(cid:107)
w(cid:62)   x   
(cid:107)w(cid:107)
then, the margin can be de   ned in terms of these two distances as

d    =   

d+ =

,

,

   =

=

1
2 (d+ + d   )
c
(cid:107)w(cid:107)

,

(1.14)

(1.15)

where c is the unnormalized distance to the positive and negative examples from the decision boundary. these two
examples are equi-distance c away from the decision boundary, because our thought experiment earlier suggests that
the decision boundary with the maximum margin should be as far away from both of these examples as possible.10

eq. (1.14) states that the margin    is inversely proportional to the norm of the weight vector (cid:107)w(cid:107). in other words,

we should minimize the norm of the weight vector if we want to maximize the margin.

9 one major difference between this hinge loss and the log loss is that the former is not differentiable everywhere. does it mean that we cannot
use a gradient-based optimization algorithm for    nding a solution that minimizes the empirical cost function based on the hinge loss? if not, what
can we do about it? the answer is left to you as a homework assignment.

10 homework assignment: derive eq. (1.14), and explain in words the derivation.

11

support vector machines now let us put together the hinge loss based empirical cost function and the principle of
maximum margin into one optimization problem:

jid166(m,dtra) =

(cid:124)(cid:123)(cid:122)(cid:125)
c
2 (cid:107)w(cid:107)2

(a)

+

1
(cid:124)
|dtra|

   

(cid:123)(cid:122)

(y,x)   dtra
(b)

dhinge(y,x;m)
,

(cid:125)

(1.16)

where c/2 can be thought of as a id173 coef   cient. this is a cost function for so-called support vector
machines [7].

this classi   er is called a support vector machine, because at its minimum, the weight vector can be fully described

by a small set of training examples which are often referred to as support vectors. let us derive it quickly here:

where dmiscla is a set of misclassi   ed, or barely classi   ed, training examples, and

    jid166
   w = cw   

   w =

1

c|dtra|

(y,x)   dmisclas

i(yw(cid:62)x     1)yx = 0

   

(y,x)   dtra

yx,

i(a) =

if a is true
otherwise

.

1
|dtra|
   

(cid:26) 1,

0,

often, |dmiscla| (cid:28) |dtra|, and thus, a support vector machine is categorized into a family of sparse classi   ers.
1.4 over   tting, id173 and complexity
1.4.1 over   tting: generalization error
at the very beginning of this course, we have talked about two cost functions; (1) expected cost in eq. (1.4) and (2)
empirical cost in eq. (1.5).11 we loosely stated that we    nd a machine that minimizes the empirical cost   r because we
cannot compute the expected cost r, somehow hoping that the approximation error    (from eq. (1.5)) would be small.
let   s discuss this a bit more in detail here.

let us de   ne the generalization error as a difference between the empirical and expected cost functions given a

reference machine m   , a machine m and a training set dtra:

we can further de   ne its expectation as

g(m   ,m,dtra) = r(m   ,m)      r(m   ,m,dtra).
(cid:2)   r(m   ,m,d)(cid:3) ,

  g(m   ,m) = r(m   ,m)    ed   p

(1.17)

(1.18)

where p is the data distribution.

when this generalization error is large, it means that we are hugely overestimating how good the machine m is
compared to the reference machine m   . although m is not really good, i.e., the expected cost r is high, m is good
on the training set dtra, i.e., the empirical cost   r is low. this is precisely the situation we want to avoid: we do not
want to brag our machine is good when it is in fact a horrible approximation to the reference machine m   . in this
embarrassing situation, we say that the machine m is over   tting to the training data.

unlike how i said earlier, the goal of supervised learning, or machine learning in general, is therefore to search for
a machine in a hypothesis set not only to minimize the empirical cost function but also to minimize the generalization
error. in other words, we want to    nd a machine that simultaneously minimizes the empirical cost function and avoids
over   tting.

statistical learning theory, a major sub   eld of machine learning, largely focuses on computing the upper-bound of
the generalization error. the bound, which is often probabilistic, is often a function of, for instance, the dimensionality

11 note that the complexity, or operational cost, of a machine m is often not included in either of the cost functions, but this is not a problem to

include them as long as both cost functions have them.

12

of an input vector x and a hypothesis set. this allows us to understand how well we should expect our learning setting
to work, in terms of generalization error, even without testing it on actual data. awesome, but we will skip this in
this course, as the upper-bound is often too loose, and rough sample-based approximation of the generalization error
works better in practice.12

1.4.2 validation
in practice, the generalization error itself is rarely of interest. it is rather the expected cost function r of a machine m
that interests us, because we will eventually pick one m that has the least expected cost.13 but, again, we cannot really
compute the expected cost function and must resort to an approximate method. as done for training, we again use a
set dval of samples from the data distribution to estimate the expected cost, as in eq. (1.5), that is14

  rval =

1

n(cid:48)    
(y,x)   dval

   (y,m,x) +   c(m).

in order to avoid any bias in estimating the expected cost function, via this validation cost, we must use a validation
set dval independent from the training set dtra. we ensure this in practice by splitting a given training set into two
disjoint subsets, or partitions, and use them as training and validation sets.

this is how we will estimate the expected cost of a trained model m. how are we going to use it? let us consider
having more than one hypothesis set hl for m = l, . . . ,l. given a training set dtra and one of hypothesis sets hl, we
will    nd a machine ml     hl that minimizes the empirical cost function:
  r(m,dtra).

we now have a set of trained models(cid:8)m1, . . . ,ml(cid:9), and we must choose one of them as our    nal solution. in doing

ml = argmin
m   hl

so, we use the validation cost computed using a separate validation set dval which approximates the expected cost.
we choose the one with the smallest validation cost among the l trained models:

  m = argmin
ml|l=1,...,l

  rval(ml,dval).

example 1: model selection let   s take a simple example of having two hypothesis sets. one hypothesis set hlogreg
has all possible id28s, and the other set hid166 has all possible support vector machines. we will    nd one
id28 and one support vector machine from these hypothesis sets by using the learning rules we learned
earlier. we select one of these two models based not on the empirical cost function but on the validation cost function.

example 2: early stopping can this be useful even if we have a single hypothesis set? indeed. so far, for
both classi   ers we have considered learning happened iteratively. that is, we slowly evolve the parameters, or more
speci   cally the weight vector. let us denote the weight vector after the l-th update by wl, and assume that we have
applied the learning rule l-many times. suddenly i have l different classi   ers, just like what we had with l different
classi   ers earlier when there were l hypothesis sets.15 instead of taking the very last weight vector wl, we will choose

  m = argmin
ml|l=1,...,l

  rval(ml,dval),

where ml is a classi   er with its weight vector wl. this technique is often referred to as early stopping, and is a crucial
recipe in iterative learning.

12 well, the better answer is that it involves too much math..
13 though, as we discussed earlier in eq. (1.6), it may be better to keep more than one m in certain cases.
14 it is a usual practice to omit the model complexity term when computing the validation cost. we will get to why this is so shortly.
15 in some sense, we can view each of these classi   ers as coming from l different (overlapping) hypothesis sets. each hypothesis set can be

characterized as reachable in l gradient updates from the initial weight vector.

13

cross-validation often we are not given a large enough set of examples to afford dividing it into two sets, and using
only one of them to train competing models. either the training set would be too small for the empirical (training) cost
to well approximate the expected cost, or the validation set would be too small for the validation cost to be meaningful
when selecting the    nal model (or the correct hypothesis set.) in this case, we use a technique called k-fold cross
validation.

we    rst evenly split a given training set dtra into k partitions while ensuring that the statistics of all the partitions
are roughly similar, for instance, by ensuring that the label proportion (the number of positive examples to that of the
negative examples) stays same. for each hypothesis set h, we train k classi   ers, where dtra
dk
tra is used to train the k-th classi   er and dk
tra to compute its validation cost. we then get k validation costs of which
the average is our estimate of the empirical cost of the current hypothesis set. we essentially reuse each example k   1
times for training and k   1 times for validation, thereby increasing both the ef   ciency of our use of training examples
as well as the stability of our estimate. when k is set to the number of all training examples, we call it leave-one-out
cross-validation (loocv).

cross-validation is a good approach for model selection, but not usable for early-stopping.16 furthermore, when
the training set is large, it may easily become infeasible to try cross-validation, as the computational complexity
grows linearly with respect to k. it is however a recommended practice to use cross-validation whenever you have a
manageable size of training examples.

test set as soon as we use the validation set to select among multiple hypothesis sets or models, the validation cost
of the    nal model is not anymore a good estimate of its expected cost. this is largely because again of over   tting. our
choice of hypothesis set or model will agree well with the validation cost, but unavoidably the validation cost will have
its own generalization error. thus, we need yet another set of examples based on which we estimate the true expected
cost. this set of examples is often called a test set.

most importantly, the test set must be withheld throughout the whole process of learning until the very end. as
soon as any intermediate decision about learning, such as the choice of hypothesis set, is made (even subconsciously)
based on the test set, your estimate of the expected cost of the    nal model becomes biased. thus, in practice, what
you must do is to split a training set into three portions; training, validation and test partitions, in advance of anything
you do. is there an ideal split? no.

similarly to how we estimated the validation cost, it is often the case in which you do not have enough data and
cannot afford to withhold a substantial portion of it as a test set. in that case, it is also a good practice to employ the
strategy of k-fold cross-validation. in this case, it is worth noting that you need nested k-fold cross-validation. that
is, for each k-th fold from the outer cross-validation loop, you use the inner cross-validation (k iterations of training
and validation) for model selection. it is computationally expensive, as now it grows quadratically with respect to k,
i.e., o(k2), but this is the best practice to accurately estimate the expected cost of your learning algorithm given only
a small number of training examples.

1.4.3 over   tting and id173
we now know how to measure the degree of over   tting by approximately computing the difference between the
expected cost and the empirical cost. in this section, let us think of how we can use this in more detail. let us start
from the    example 1: model selection    from above.

when we select a model, the    rst question that needs to be answered is what are our hypothesis sets. an obvious
approach is to choose each hypothesis set to include all possible con   gurations of one model family, such as logistic
regression and support vector machine. instead, we can also decide on a model family, and sub-divide the gigantic
hypothesis sets into several subsets. the latter is one we will discuss further here.

16 homework assignment: why is cross-validation not a feasible strategy for early-stopping?

14

figure 1.2: training and test errors with re-
spect to the weight decay coef   cient. notice
that the test error grows back even when the
training error is 0 as the weight decay coef   -
cient decreases.

let us consider the cost function of support vector machines from eq. (1.16):

jid166(m,dtra) =

(cid:124)(cid:123)(cid:122)(cid:125)
c
2 (cid:107)w(cid:107)2

+

1
|dtra|

(a) id173

dhinge(y,x;m).

   

(y,x)   dtra

the term (a) controls how much our separating hyperplane (w(cid:62)   x = 0) may deviate from a    at line (w = 0). what

does it mean? let us now look at the gradient of the cost function:

   wjid166 = cw(cid:124)(cid:123)(cid:122)(cid:125)

(a)

+

1
|dtra|

   

(y,x)   dtra

   wdhinge(y,x;m).

the    rst term corresponds to pulling the weight vector w closer to an all-zero vector, effectively disallowing the weight
vector to move too far away from a vector with small values. the degree to which this pull toward a simple setting is
determined by the id173 coef   cient c. when c = 0, there is no force pulling the weight vector toward a small
vector, while with a large c, this pulling force is greater.

based on this observation, we can now de   ne a smaller hypothesis set hc(cid:48), which is a subset of the larger hypothesis
set of all support vector machines, as all the support vector machines reachable by minimizing the cost function of a
support vector machine when the id173 coef   cient c is set to c(cid:48). in other words, given a set of prede   ned
coef   cients {c1,c2, . . . ,cm}, we can construct as many hypothesis sets hc1, . . . ,hcm .
we    nd a support vector machine that minimizes the cost function jid166 for each of these hypothesis sets. then, as
we discussed earlier in sec. 1.4.2, we choose one of them based on the validation cost which in this case should omit
the id173 term c

two questions naturally arise here. first, why don   t we estimate the id173 coef   cient c just like we did

2(cid:107)w(cid:107)2.

with the weight vector w? in other words, is it possible to simply    nd   c such that

  c = argmin

c

jid166(m,dtra)?

it is because we are not supposed to use the training set to estimate such a id173 coef   cient c. this would be
equivalent to selecting a hypothesis set based on the training set, which we have learned not to do.

second, why do we omit this id173 term when selecting among trained support vector machines each
belonging to a different hypothesis set? because at the end of the day, what we are really interested in is how well our
classi   er classi   es unseen input vectors, which is precisely what the 0-1 loss in eq. (1.12) measures. this is however
not a universal practice, and you should choose how each hypothesis set, or the machine found within it, is scored
based on your actual constraints. for instance, if an intrusion detection system can only run a low-end processor due
to the power consumption constraint, you may have to down-weight the machines you   ve found from hypothesis sets
with high computational requirement.

15

543210123\log c0.0000.0250.0500.0750.1000.1250.1500.175error ratetraining errortest error(a) training set

(b) validation set

figure 1.3: both solutions of id28 perfectly    t the training set regardless of whether weight decay
id173 was used. however, it is clear that the model with the optimal weight decay coef   cient (blue line)
classi   es the test set better.

example
in this example there are 20 training pairs and 100 validation pairs. we search for the best id173
coef   cient, or corresponding hypothesis sets, over the set of 50 equally-spaced logc from    5 to 3. we plot how the
training and validation errors (0-1 loss) change with respect to the id173 coef   cient c (or equivalently logc)
in fig. 1.2. it is clear from the plot that as the id173 strength weakens (logc           ) the training error drops
to zero. on the other hand, the validation error decreases until logc = 0, but from there on, increases, which is a clear
evidence of over   tting.

in fig. 1.3, we see the difference between the support vector machines found using c = 0 (no id173, red
dashed line) and logc = 0 (best id173 according to the validation error, blue dashed line). the red dashed
decision boundary, which corresponds to the support vector machine without any id173, classi   es all the
training input vectors perfectly, while the blue dashed decision boundary fails to classify one training input vector near
(   0.1,1.3) correctly. however, when we consider the validation input vectors, the picture looks different. a better
machine is now the regularized support vector machine.

1.5 multi-class classi   cation
so far, we have considered a binary classi   cation only. in reality, there are often more than two categories to which
an input vector belongs. it is indeed interesting to build a machine that can tell whether an object of a particular type,
such as a dog, is in a given image, but we often want our machine to be able to classify an object in a given image
into one of many object types. that is, we want our machine to answer    what is in an image?    rather than    is a dog in
an image?    a slightly different formulation of the same question is    which of the following animals does this image
describe, dog, cat, rabbit,    sh, giraffe or tiger?    this kind of problem is a multi-class classi   cation. instead of two
possible categories as in binary classi   cation, now each input vector may belong to one of more than two categories.
let us start from id28 in sec. 1.2. we already learned that the id28 classi   er outputs a
bernoulli distribution over two possible categories   negative and positive. we extend it so that the id28
classi   er is now returning a distribution over k-many possible categories

c = {0,1, . . . ,k} .

(1.19)

first, we must decide what kind of distribution, other than bernoulli distribution, we should use. in this case of
multi-class classi   cation, we use a categorical distribution. the categorical distribution is de   ned over a set of k
events (equivalent to k categories in our case) with a set of l probabilities {p1, . . . , pk}. pk is the id203 of the
k-th event happening, or the input vector belonging to the k-th category. as usual with any other id203, those

16

1.51.00.50.00.51.01.52.02.5321012initial: 0.017x1+0.14x2+0.0=0c=0: 31.11x1+6.55x2+12.23=0c=0.08: 1.02x1+0.43x2+0.21=032101233210123probabilities are constrained to sum to 1, i.e.,

k

   

k=1

pk = 1.

now given an input vector x, we should let our new multi-class classi   er output this categorical distribution. this
is equivalent to building a machine that takes as input a vector x and outputs k probabilities that sum to 1. in order to
do so, we turn the d-dimensional input vector into a k-dimensional vector by

a = w  x,

where   x is as before [x;1]. w, to which we refer as a weight matrix, is a k    d-dimensional matrix. do you see how
it has changed from a weight vector earlier to a weight matrix now?
we have k real numbers in a. these numbers however are not constrained, meaning that their sum is not 1, and that
some of them may even be negative. let us now turn this k numbers into k probabilities of a categorical distribution.
first, we make them positive by exponentiating each of them separately. that is,

then, we force those k positive numbers to sum to 1 by

a+ = exp(a) > 0.

p =

1
   k
k=1 a+
k

a+.

(1.20)

that was easy, right? this transformation   exponentiation followed by id172    is called softmax [5].

hypothesis set this new machine, often called multinomial id28, is fully characterized by the weight
matrix w. in other words, our hypothesis set is a set of all k-by-d real-valued matrices.

distance we de   ne the distance function similarly to how we did with id28. that is, it is the negative
log-id203 of the correct category returned by the reference machine m   :

(1.21)
i used pm   (x) to denote the m   (x)-th id203 value stored in p, which is by our de   nition of the machine the
id203 of the correct category predicted by our machine. let   s expand it a bit further:

   (m   (x),m,x) =    log p(c = m   (x)|x) =    log pm   (x).

   (y   ,m,x) =    log pm   (x)
   
=    ay    + log

k

k=1

exp(ak).

gradient of the distance as we have done so with id28 and support vector machines earlier, we need
to compute the gradient of the distance function in eq. (1.21) with respect to the weight matrix.17

we will do this for each row of the weight matrix separately. first, we consider the y   -th row vector, that corre-

sponds to the correct class outputted by the reference machine:

      (y   ,m,x)

   wy   

(cid:33)

exp(ak)

(cid:32)
   
w(cid:62)y      x    log
=   
=    (1    p(c = y   |x))  x.

   
   wy   

k

k=1

(cid:32)
   
w(cid:62)y      x    log
=   
=    (0    p(c = y|x))  x.

   
   wy

k

k=1

17

similarly, we can compute the gradient of the distance function with respect to the weight vector that corresponds to
any other incorrect class y     {1, . . . ,k}\{y   }:
      (y   ,m,x)

(cid:33)

exp(ak)

   wy

17 the full derivation is left for you as a homework assignment.

we can combine them together into a single vector equation:

where

y    =

   w   (y   ,m,x) =     (y        p)   x(cid:62),

                             y   -th row

                        

0,
...,
1,
...,
0

(1.22)

is an one-hot vector corresponding to a desired output, and p is the actual output from the multinomial logistic regres-
sion from eq. (1.20).

this equation above reminds us of the learning rule of id28. see for instance eq. (1.9) as a compar-
ison. both rules (id28 and multinomial id28) have a multiplicative term in the front, and
that multiplicative term is a difference between the predicted output (or the predicted conditional distribution over the
categories given an input vector) and the desired output generated by the reference machine.

for the row vector of the weight matrix corresponding to the correct category y   , this learning rule will add the
input vector (augmented with an extra one) to the this vector so that they would align better. on the other hand, for any
other category, the learning rule will subtract the input vector instead to make them less aligned. the degree to which
the input vector is subtracted is decided based on how well the reference machine and our machine agree. learning
terminates, when the multinomial id28 puts all the id203 mass (1) to the correct class.

1.6 what does the weight vector tell us?

before we move on to more advanced topics, let us brie   y discuss about what the weight vector or matrix tells us. in
a standard setting of binary classi   cation, each component x j of an input vector x has a corresponding weight value
w j. when this associated weight value is close to 0, what does it mean? it means that this j-th component does not
matter! this is easy to verify by looking at the score function we de   ned in eq. (1.11) which can be rewritten as

s(y,x;m) = yw(cid:62)   x = y

w jx j + wd+1

.

(cid:32) d

   

j=1

(cid:33)

let   s consider the k-th component of the input vector:

which is equivalent to the equation below, if wk = 0.

(cid:33)
      

d

   
j(cid:48)=k+1

d

   
j(cid:48)=k+1

s(y,x;m) = y

w jx j + wkxk +

w j(cid:48)x j(cid:48) + wd+1

,

s(y,x;m) =y

=y

w jx j + wkxk

+
=0 if wk=0

(cid:124)(cid:123)(cid:122)(cid:125)
   
j(cid:48)=k+1

d

w jx j +

w j(cid:48)x j(cid:48) + wd+1

.

w j(cid:48)x j(cid:48) + wd+1

(cid:33)

j=1

   

(cid:32)k   1
      k   1
(cid:32)k   1

   

j=1

   

j=1

this is as if our input vector never had the k-th component to start with.

along the same line of reasoning, we can see that the magnitude of each weight value |wk| roughly corresponds to
how sensitive the output of a machine to the change in the value of the k-th component of the input vector xk. can we
make it slightly more precise by de   ning the sensitivity more carefully? indeed, we can. the sensitivity of the output

18

figure 1.4: if the problem is not linearly sep-
arable as in the case shown, a linear classi-
   er, such as id28, fails miser-
ably. this is a famous example of a exclusive-
or (xor) problem (with noise.)

of our machine, w(cid:62)   x with respect to a single input component xk is precisely the de   nition of the partial derivative of
the output with respect to the input component. that is,

(cid:33)

w jx j + wkxk +

d

   
j(cid:48)=k+1

w j(cid:48)x j(cid:48)

(cid:32)k   1

   

j=1

   w(cid:62)   w
    xk

=

=

   
    xk
    wkxk
    xk

=wk.

this de   nition of sensitivity via partial derivative will become handy in the later part of the course.

in other words, we can understand which components of the input vector are meaningful for or have high in   uence
on the output of our machine by inspecting the weight vector. for an example of inspecting the weight vector, or ma-
trix in the case of multi-class classi   cation, see https://github.com/nyu-dl/intro_to_ml_lecture_
note/blob/master/notebook/weight%20analyzer.ipynb.

1.7 nonlinear classi   cation
so far in this course, we have looked at a linear classi   er which de   nes a hyperplane (w(cid:62)   x = 0) that partitions the
input space into two partitions. clearly this type of classi   er can only solve linearly separable problems. a famous
example in which a linear classi   er fails is exclusive-or (xor) problem shown in fig. 1.4. in this section, we discuss
how to build a classi   er for problems which are not linearly separable.

1.7.1 feature extraction
we have so far assumed that an input vector x is somehow given together with data. is this assumption reasonable?
let us think of what kind of data we run into in practice. for instance, in the example of intrusion detection system
from earlier sections, the input to a machine is not a vector but a picture taken by a camera installed at the front of the
store. in the case of building a machine that categorizes a document, the input to a machine is again not a vector but a
long list of words. if we are building a machine for detecting violent scenes from a movie, our machine takes as input
a video not a single,    at vector. what all these examples suggest is that we need one more step in addition to what
we have discussed as a full pipeline of machine learning. that is the step of feature extraction, or sometimes called
feature engineering.

let us introduce another symbol x to denote the original input which could be anything from a colour image,
video clip to a social network of a person. then, the feature extraction stage can be thought of as a function    that

19

   2.0   1.5   1.0   0.50.00.51.01.5   2.5   2.0   1.5   1.0   0.50.00.51.01.52.0initial:0.9x1+0.75x2+0.0=0final:   0.047x1+   0.0006x2+0.002=0figure 1.5: the xor problem in the original coordinate system (left) is not linearly separable, but as shown in the
right panel, it become linearly separable by transforming the space (center and right). see the main text for more
details.

maps from this arbitrary original input x to a corresponding input vector x. that is,

x =    (x ).

why is this process called feature extraction? that is because the function    can be thought of as extracting d-
many characteristics of the original input x . we extract features out of a given input x and build the corresponding
input vector x     rd.
example: bag-of-words representation let us consider an example of document categorization we learned about
earlier. what is a property of a given document that largely determines the category or topic of the document? one
thing that immediately comes to our mind is the existence of category-related words. for instance, if a word    hockey   
is mentioned in the document, it is highly likely that it is a document about sports, and more speci   cally about hockey
rather than baseball. perhaps, it is also important how frequently such a word appeared in the document. if the word
   baseball    appeared ten times more than the word    baseball   , the topic of the document is likely    hockey    rather than
   baseball   , even though the word    baseball    appeared.

this observation leads us to use a so-called bag-of-words (bow) feature representation of a document for docu-
ment categorization. as the name suggests, this representation puts all the words in a given document into a bag and
counts how often each word appeared in the document, ignoring any order among those words. this is equivalent
to turning each word into a one-hot vector from eq. (4.1) and sum them into a single vector. in order to do so, we
will    rst build a vocabulary of all unique words in all the document from a training set (again, do not touch any test
document!), which is similar to building a category set from eq. (1.19). we then transform a document into a sequence
of one-hot vectors wi   s, and sum all those one-hot vector to obtain a bag-of-words vector:

x =

|x |   

i=1

wi,

where |x | denotes the length, or the number of words, of the document x . this bow vector x can be used with any
machine learning algorithm, such as any of the classi   ers we have learned so far.

linear separable feature extraction feature extraction serves two purposes. the    rst purpose is to build a    xed-
dimensional vector x from an arbitrary input x , of which an example was to build a bag-of-word vector from a

20

   101   1.5   1.0   0.50.00.51.01.5xor(original)   2   1012   2.0   1.5   1.0   0.50.00.51.01.52.0xor(rotated)0.00.51.01.52.00.000.250.500.751.001.251.501.752.00xor(rotated+abs)document of arbitrary length. the second purpose is to make a given dataset easier for a classi   er, or any other linear
machines. more speci   cally for the case of classi   ers we have learned so far, the goal of feature extraction is to make
a dataset linearly separable, even when it is not so in the original input x space.

let us go back to the example of xor problem from earlier. in its original form, the xor problem is not solvable
by a linear classi   er, because the positive and negative classes are not linear separable, meaning that there is no 1-d
hyperplane (line) that separates the examples into the positive and negative classes. the question is then: can we
somehow transform the original input   a vector in a 2-d space    so that the transformed data is linear separable?

first, let us rotate the whole space, or every single point in the space, clock-wise by 45 degrees. this can be done

by    rst de   ning a rotation matrix as

(cid:20) cos      sin  

cos  

sin  

(cid:21)

,

r(   ) =

where    is in radian (rad(45   )     0.785). we then rotate each point in the space by

xr = r(rad(45   ))x.

the xor problem after this rotation is illustrated in the center panel of fig. 1.5.

the problem is however not linearly separable yet. let us then further apply one more transformation. that is, we

will take the absolute value of each element of the resulting, rotate vector:

(cid:20)

xra =

|xr
1|
|xr
2|

(cid:21)

.

effectively, we fold the four quadrants of the rotated space into the    rst quadrant (the top-right one), and the resulting
space is now linearly separable as shown in the right panel of fig. 1.5. within this new transformed space, the xor
problem, which was not linearly separable, is now linearly separable, and we can use any of the linear classi   cation
machines we have learned earlier.

this is a great news! apparently, we can    nd a set of features   the elements in an input vector x    that turn a
problem, which is not linearly separable in its original form, into a linearly separable one. as long as we can    nd such
a transformation, or a sequence of them, we are pretty much solve any classi   cation problem.

unfortunately, it is often impossible to    nd such a transformation manually, because the original inputs or the
original feature vectors are almost always high-dimensional. in the remainder of this section, we will discuss how we
can automate such a procedure.

k-nearest neighbours: fixed basis networks

1.7.2
let us consider another transformation for the xor problem above. we start with selecting four points in the space
that correspond to

r1 = [   1,   1](cid:62)
r2 = [1,1](cid:62)
r3 = [   1,1](cid:62)
r4 = [1,   1](cid:62)

to which we refer as basis vectors. with these basis vectors, we will transform each two-dimensional input vector x
into a four-dimensional vector    (x) such that

  i(x) = exp(cid:0)

   (x    ri)2(cid:1) .

(1.23)

this is equivalent to saying that the i-th element of the transformed vector    (x) is inversely proportional to the distance
between the input vector x and the i-th basis vector.

this function is often called a (gaussian) radial basis function. this function   s output is bounded between 0 and
1. the output is closer to 1, when the input vector x is close to the basis vector r, but converges to 0 as the distance
between them grows.

21

in this newly transformed space, the xor problem is linearly separable. how do we con   rm this? we can either
train a linear classi   er, such as the ones we have learned so far in the course, or manually    nd a weight vector w     r5
that solves the problem perfectly. let us try the latter, based on the intuition we built from sec. 1.6.
we    rst observe that any input vector that is close to one of the    rst two basis vectors r1 and r2 should be classi   ed
as a positive class, and an input vector closer to either r3 or r4 should be classi   ed as a negative class. in other words,
any positive input vector would have either the    rst or second element of the transformed vector close to 1, while any
negative input vector close to 0. for the third and fourth elements, they would have a value close to 1, if the original
input vector is negative, and close to 0 otherwise.

based on this observation, we can easily notice that the following weight vector will perfectly solve the problem:18

the weight vector above can be written alternatively as

w = [1,1,   1,   1,0](cid:62) .

w =(cid:2)y1,y2,y3,y4,0(cid:3)(cid:62) ,

(1.24)

where yi is the class label (-1 or 1) of the i-th basis vector. in other words, the input vector x belongs to the class to
which the nearest basis vector belongs.

let us generalize this idea by assuming that we have k basis vectors and their corresponding labels:

(cid:8)(r1,y1), . . . , (rk,yk)(cid:9) .
   (x    r1)2(cid:1)
          exp(cid:0)
exp(cid:0)
   (x    rk)2(cid:1)
w =(cid:2)y1, . . . ,yk(cid:3) ,

...

   (x) =

each input vector x is transformed into

         

(1.25)

in the case of binary classi   cation, the optimal weight vector is given in eq. (1.24). in multi-class classi   cation,

the weight matrix w can be constructed as

where yi is the one-hot vector corresponding to the class to which the i-th basis vector belongs (see eq. (4.1).) again,
it is left for you as a homework assignment to show that this construction of the weight matrix solves the problem of
multi-class classi   cation.

suddenly the problem of    nding a linearly separable transformation has become a problem of    nding a set of good
basis vectors and their own classes. of course, now a big question is what these good basis vectors. an even bigger
question is how we know to which class each of those basis vectors belongs. after all, the whole point of classi   cation
is to    gure out this latter question.

k-nearest neighbours we can push this idea to the extreme by declaring each and every input vector in a training
set as basis vectors. furthermore, instead of building a linear classi   er in the transformed space (using all the training
input vectors as basis vectors,) we can simply use the class label of the nearest basis vector, or more conventionally
nearest neighbour.

this can be written down more formally by    rst de   ning as many basis vectors as there are training examples.

that is,

ri = xi,

where xi is the i-th input vector in a training set. then, each input vector is transformed in two stages. first, we use
the radial basis function to get    (x) as done in eq. (1.23). second, we turn the resulting vector into an one-hot vector
by setting the element with the largest value to 1 and all the others to 0:

18 it is a homework assignment for you to show that this weight vector solves the xor problem in the new space.

  (cid:48)(x) = argmax(   (x)).

22

(a) k = 1

(b) k = 5

(c) k = 20

figure 1.6: the effect of varying k in k-nearest-neighbour classi   er. we observe that the decision boundary becomes
smoother as k increases, which suggests that a large k corresponds to having a stronger id173 term.

the weight matrix is constructed as before in eq. (1.25). in practice, none of these formal steps is necessary. all that
is needed is to    nd the nearest neighbour from a training set given an arbitrary (test) vector, and return the class of the
nearest neighbour. we call this classi   er a nearest-neighbour classi   er.

the nearest-neighbour classi   er can be written down in a single equation:

argmin

(x,y)   dtra(cid:107)x    x(cid:48)(cid:107)2,

where x(cid:48) is a new input vector of which label must be found. note that it is not necessary to use the euclidean distance
(cid:107)a    b(cid:107)2, and any distance function may be used instead.
the nearest-neighbour classi   er is rarely used in practice. instead, it is more common to use its variant, called a k-
nearest-neighbour (knn) classi   er. unlike the nearest-neighbour classi   er, the knn classi   er selects k nearest input
vectors from a training set given a new input vector, and lets them vote on which category this new vector belongs to.
the nearest-neighbour classi   er is thus a special case of the knn classi   er, where k is    xed to 1.

increasing k has the effect of id173, similarly to the weight decay id173 term, or the max-margin
id173 term from support vector machines (see eq. (1.16) (a).) when k = 1, the empirical cost on a training set
is perfect by de   nition, but this nearest-neighbour classi   er is susceptible to outliers or noise. imagine a case where
one negative input vector was accidentally placed in the middle of a cluster19 of positive input vectors. the nearest-
neighbour classi   er would assign any input vector in a small region around this negative input vector to a negative
class, although it is quite clear that this training example is an outlier, or was labelled incorrectly. this behaviour,
which is a classical example of over   tting, is easily mitigated by using k > 2, as this would ignore such an outlier
training example. on the other hand, we can also think of a case where k = |dtra|, in which case any new input vector
would be assigned to a majority class, and such a knn classi   er wouldn   t be able to correctly classify any training
input vector belonging to a minority class. the latter case is considered over-regularized or under-   tted.

1.7.3 id80s
the most obvious weakness of the knn classi   er is that it requires (a) a large storage (since it must maintain the
entire training set,) and (b) a sweep through the entire training set (unless some smart indexing with an appropriate
approximation strategy is used.) this becomes more severe, as the size of a training set grows (which is precisely what
is happening everyday,) and if there is a computational constraint in run-time (such as when run on a mobile phone.)
a radial basis function (rbf) network overcomes this weakness of the knn classi   er by selecting only a small
number of basis vectors, according to memory and computational constraints. of course, as we discussed earlier, how
should we choose such basis vectors?

there are two widely-used approaches, when there is a constraint on the number of basis vectors. let this con-
straint be k. the    rst approach is rather dumb in that it uniform-randomly selects k training input vectors without
replacement:

1. uniform-randomly select an index i from {1, . . . ,|d|}
19 a cluster refers to a group of closely located input vectors.

23

2. b     b   {xi}
3. d     d\{xi}
4. go back to 1, if |b| < k.

where d is initialized with a training set dtra, and b is a set of basis vectors initialized to be an empty set. this
approach works surprisingly well, because (a) no basis vector is too far away from the training examples, and (b)
uniform-randomly sampling ensures that the selected basis vectors are evenly distributed across the region occupied by
the training examples. in this random-sampling approach, we know precisely what the correct label, or that predicted
by a reference machine, of each basis vector is. this allows us to set the weight vector, or weight matrix, exactly, as
we have done with the nearest-neighbour classi   er in eq. (1.25).

the other approach is to    nd a set of clusters of training input vectors and pick the centroids20 of these clusters
as basis vectors. in the case of the xor problem in fig. 1.5, there are four clusters centered at (1,1), (-1,1), (-1,-1)
and (1,-1), respectively. hence we would take these four centroids [1,1], [   1,1], [   1,   1] and [1,   1] as basis vectors.
this ensures that there is at least one basis vector for any group of training input vectors, and this guarantee is much
stronger than we get with the random-sampling approach. despite this nice property, we are now faced with two
additional issues.

first, how do we get those clusters? in the case of low-dimensional input vectors (e.g., 2-d or 3-d), it may be
possible for us to visually inspect the input vectors to manually select clusters. this however becomes implausible
when the dimensionality d of the input vector grows beyond 3. we then resort to automatic id91 which is another
class of algorithms in machine learning. we will learn about automatic id91 later in the course.

when we have found clusters and their centroids, we have the second question to answer. that is, how should we
set the weight vector, or matrix, without having the correct labels for these basis vectors? unlike the random-sampling
approach, or the nearest-neighbour classi   er (which is the special case of random-sampling approach with k = |dtra|),
the centroids are not necessarily included in a training set, and thereby, are without correct labels. fortunately we
already have a solution to this problem. in fact we have been learning how to answer this problem this entire semester.

let us assume that we have k basis vectors(cid:8)r1,r2, . . . ,rk(cid:9) corresponding to the centroids of k clusters. with

these basis vectors, we now transform each and every input vector in a training set into a k-dimensional vector:

               

exp(cid:0)
   (x    r1)2(cid:1)
   (x    r2)2(cid:1)
exp(cid:0)
exp(cid:0)
   (x    rk)2(cid:1)

...

                    rk.

   (x) =

this transformation of every training input vector is equivalent to building a new training set consisting of pairs of a
transformed input vector and its correct class. that is,

dtra     {(   (x1),y1), . . . , (   (xn),yn)} .

once this transformation is done, we can    nd the k + 1-dimensional weight vector, or (k + 1)    |c|-dimensional
matrix, using any of the techniques we have learned earlier, including id28, support vector machines and
multinomial id28.

in other words, we have now learned how to use the linear classi   ers we have learned so far for problems that
are not linearly separable. we    rst    nd a set of basis vectors based on which each input vector is transformed using
a radial basis function from eq. (1.23), and    t a linear classi   er on a new training set with these transformed input
vectors. a nonlinear classi   er constructed in this way is often referred to as a id80 (rbfn).
also, as both knn and rbfn use a    xed set of basis vectors, we call this family of classi   ers a    xed basis network.

1.7.4 adaptive basis function networks or deep learning
adaptive basis function networks a natural question that follows our discussion on the    xed basis network is
whether it is necessary to    x basis vectors. perhaps a more fundamental question would be how we know that those
   xed basis vectors we have selected are good for the    nal classi   cation accuracy. is it possible that there is a better

20 a centroid is the average of all the input vectors in the corresponding cluster.

24

strategy for selecting basis vectors than the ones we have discussed already? is it possible that this new strategy selects
basis vectors not based on our intuition but based on the actual classi   cation accuracy?

let us go back to id28 and consider its distance function from eq. (1.26):

   (m   (x),m,x) =    log p(c = m   (x)|x)

=    (m   (x)logm(x) + (1    m   (x))log(1    m(x))),

(1.26)

where

m(x) =    (w(cid:62)w + b)

and    is a sigmoid function.21 given this distance function, we were able to derive a learning rule for logistic
regression by computing its gradient with respect to the weight vector (and a bias scalar).

with k basis vectors, we can rewrite this distance function as

   (m   (   (x)),m,   (x)) =    (m   (   (x))logm(   (x)) + (1    m   (   (x)))log(1    m(   (x)))),

where

          exp(cid:0)
   (x    r1)2(cid:1)
   (x    rk)2(cid:1)
exp(cid:0)

...

         

   (x) =

from eq. (1.25). in this rewritten form, we notice that we can compute the gradient of the distance with respect not
only to the weight vector (w and b), but also to each and every basis vector. that is, we can compute

   rk   (m   (   (x)),m,   (x)).

let   s compute this gradient:

(cid:125)
   rk   (y   ,m,   (x)) =   (y           (w(cid:62)   (x) + b))

(cid:124)
(cid:123)(cid:122)
   rk   k(x)
=    2(y           (w(cid:62)   (x) + b))wk  k(x)(x    rk),

(cid:125)
(cid:124)
(2  k(x))(x    rk)

wk(cid:124)(cid:123)(cid:122)(cid:125)    a

(cid:123)(cid:122)

     k (x)

      
    a

where a = w(cid:62)   (x) +b.22 with this gradient, we now know how to adjust the k-th basis vector rk to reduce the distance
given a training example (x,y   ):

rk     rk          rk   (y   ,m,   (x)).

this is just like how we have learned to update the weight vector to minimize the distance earlier for the linear
classi   ers. unlike those learning rules, however, this learning rule is not easily interpretable. for instance, when
should the basis vector rk become similar to the input vector x? and, how much should it be adjusted toward or
against x? how does this value correlate with the classi   cation accuracy? despite the fact that we cannot or are not
willing to answer these questions, one thing is clear; this learning rule will adjust the k-th basis vector to reduce the
distance.

what does this imply? it implies that we can use the very same technique we learned earlier for training a linear
classi   er for automatically adapting the basis vectors as well. as long as the gradient of the distance function could
be computed with respect to any of the basis vectors, we can simultaneously update the weight vector, or matrix, and
all the basis vectors to minimize the distance, thereby maximizing the classi   cation accuracy. even better, we do not
even need to compute the gradient ourselves, but can leave this tedious job to automatic differentiation.

in practice, we use one of the two selection strategies from the previous section to initialize basis vectors rather than
   xing them. then, we can use any off-the-shelf optimization algorithm to jointly tune both the weight vector, or matrix,
and all the basis vectors to minimize the empirical cost function, using the gradient computed again automatically by
automatic differentiation algorithm. when the basis vectors are not anymore    xed, we call this type of a classi   er an
adaptive basis function network (abfn).

21 i have split the weight vector into two components; (a) the weight vector w and (b) the bias b, as this is a more standard notation in deep

learning.

22 at this point, you should already know that the full derivation is left for you as a homework assignment.

25

figure 1.7: the goal of adaptive basis net-
works is to    nd a parametrized mapping from
the original input space x     rd to another
space    (x)     rq that makes the problem lin-
early separable.

deep learning a further implication of gradient-based learning is that there is absolutely no constraint that the
feature extraction function    be a radial basis function. in fact,    can be any parametrized, differentiable function that
maps from an input vector x to its transformation. in the case of a radial basis function, for instance,    is a function
from rd to rk parametrized by a set of k basis vectors. this feature extraction function is differentiable with respect
to all the k basis vectors, and this allows us to use any gradient-based off-the-shelf optimization algorithm to train the
whole classi   er jointly.

what is a good parametric, differentiable function   ? in order to answer this question, we should think of what
we want this feature extraction function to do. we want this feature extraction function to make the problem linear
separable so that the linear classi   er acting on    (x) can do its job well. see fig. 1.7 for graphical illustration.

this can be done in two ways. first, we can make one large function    that does the perfect job in one go.
or, we can compose a series of simple feature extraction functions such that each feature extraction function makes
the problem slightly more linearly separable than it was beforehand. that is, we stack a series of feature extraction
function    1, . . .   l such that    l(         1(x)) (or    1                 l) is linearly separable, as illustrated in fig. 1.8.
what kind of simple feature extraction function should we use then, and how does the stack of such simple
feature extraction functions make the problem more linearly separable? this question requires us    nally to think
of and understand the underlying structures or properties behind a target task (classi   cation) and data. based on
the underlying structures, suitable feature extraction functions may be selected and composed into a deep feature
extraction function which is often followed by a linear classi   er. this composition of a linear classi   er and a deep
feature extraction function is jointly trained to minimize the empirical cost function using a gradient-based off-the-
shelf optimization algorithm, and the    eld in which this type of machine learning models is studied is called deep
learning.

there are many fascinating recent developments in deep learning, but they are out of the scope of this course.
for comprehensive discussion on deep learning, i highly recommend you to read a newly published text book deep
learning by ian goodfellow, yoshua bengio and aaron courville [8]. for a general overview of recent advances, see
the review article [14]. if you are a student at new york university, you can also attend the course deep learning
taught by prof. yann lecun.

1.8 further topics(cid:63)

the success of support vector machines does not necessarily come only from the use of maximum margin principle.
support vector machines are wildly popular when used together with a kernel technique, which extends a support
vector machine to handle problems that are not linearly separable. readers are recommended to read [22] by sch  olkopf
and smola for more in-depth discussion of this kernel support vector machine.

one of the most successful classi   ers in practice is a id79 classi   er [4]. a id79 classi   er
consists of many randomly generated id90. due to the lack of time, we cannot cover id90 and how
they are used to form a powerful id79 classi   er. scikit-learn implements a id79 classi   er, and i
highly recommend you to try it out.

related to the id79 classi   er are ensemble methods. this family of ensemble methods is focused on how
to combine multiple classi   ers in order to achieve better generalization performance. for more discussion, i suggest
you to read sec. 14.2   3 of [2] and/or sec. 16.2.5 and sec. 16.4.3 of [16].

26

figure 1.8: the goal of deep learning is to
stack many feature extraction functions    l   s
on top of each other to gradually transform the
problem to become linearly separable.

27

chapter 2

regression

we have so far considered a problem of classi   cation, where the output of a machine m is constrained to be a    nite
set of discrete labels/classes. in this section, we consider a regression problem in which case the machine outputs an
element from an in   nite set.1 a general setup of the problem remains largely identical to that from sec. 1.1, meaning
that it is probably a good idea to re-read that section at this point. in the context of regression, we will particularly
focus on framing the whole problem as probabilistic modelling.

2.1 id75
2.1.1 id75
as we have done with classi   cation, we will start with considering id75. in id75, our machine
m is de   ned as

m(x) = w(cid:62)   x,

where we use   x to denote the input vector with an extra 1 attached at the end. similarly to the earlier classi   cation
problems, we are given a set of training examples:

dtra = {(x1,y   1), . . . , (xn,y   n)} .

unlike classi   cation, the output y   n     rq is a q-dimensional real vector.
as should be obvious at this point, the goal of id75 is to    nd a machine, or equivalently its weight
matrix, so as to minimize the distance between the reference machine   s output and our machine   s output. the reference
machine   s outputs are given as a part of the training set.

distance function: log-likelihood functional given two vectors, one natural way to de   ne the distance between
them is a euclidean distance which is de   ned as

following our convention, we thus obtain the following distance function for id75:

(cid:107)y        y(cid:107)2
2 =

q

   

k=1

(y   k     yk)2.

   (m   (x),m,x) =

1
2(cid:107)m   (x)    m(x)(cid:107)2
2 =

1
2

where the multiplicative factor 1

2 was added to simplify the gradient.2

q

   

k=1

(y   k     yk)2,

(2.1)

1 this de   nition however is not universal, in that even when the output is from a    nite set, the problem is sometimes called regression if there

exists natural ordering of labels.

2 why does it simplify anything?

28

at this point of this course, you already know what you need to do. first, you would de   ne an empirical cost using

a training set as

  r(m,dtra) =

1
n

n

   
n=1(cid:107)y   n     m(xn)(cid:107)2

2.

then, you would compute the gradient of this empirical cost with respect to the weight matrix w (or more strictly its
   attened vector):

   w   r.

then, you would use an off-the-shelf optimization algorithm to    nd a weight matrix that minimizes the empirical cost
function. this would the    nal step, right? no! you should use validation to    nd a correct hypothesis set (or early-stop
learning) to approximately minimize the expected cost function instead of empirical cost function.

instead of going this whole pipeline once more in the setting of regression, we will consider a slightly difference
framework in which these machine learning problems, including both classi   cation and regression, could be embed-
ded.

2.2 recap: id203 and distribution

let us brie   y go over a few concepts from id203 here. they will become useful in the later discussion where a
new framework is introduced.

random variables a random variable is not a usual variable in mathematics. a usual variable is often given a
single value. when i say x = 2, the variable x is equivalent to the value 2, and it is equivalent to replacing x with 2 in
any subsequent equations that include x. this is however not true in the case of a random variable, and sometimes we
call a normal variable a deterministic variable as opposed to a random variable or a stochastic variable.

a random variable is assigned not a single value but a distribution over all possible values it could take. as an
example, consider    ipping a coin. we may declare a random variable x to denote the outcome of    ipping a coin. x
can then take one of two values     = {0 (head),1 (tail)}. because we do not know what the outcome would be, we
do not assign x to any one of these values explicitly but assign to it a distribution over these two possible choices.
a distribution is characterized by a function p that maps from one of all possible values to its corresponding
id203, and by a set of constraints on this function. in this example of coin    ipping, p :         r. there are two
constraints on this function p. first, the output of this function must be non-negative:

p(x)     0.
second, the probabilities of all possible values must sum to 1:

p(x) = 1.

   
x      

this function is called a id203 mass function.

this idea of distribution can be extended to a continuous random variable. a continuous random variable is
assigned correspondingly a continuous distribution over a continuous set    . similarly to the de   nition of a distribution
we de   ned above for a discrete random variable, we characterize a continuous distribution by a function f and a set
of constraints. unlike the earlier case, this function f does not return a id203 of a given value, but computes a
cumulative id203. that is,

i.e., what is the id203 of a random variable x being less than or equal to x?

there are two major constraints on this cumulative id203 function f.3 first, as was the case with the discrete

random variable, the value of f must be bounded, and in this case, between 0 and 1:

f(x) = p(x     x),

(2.2)

3 in addition to two more constraints that are out of scope for this course.

0     f(x)     1,   x        .

29

(a) normal distribution

(b) gamma distribution

in these two    gures, we plot both a id203 density function f , cumulative density function f, the
figure 2.1:
mean and the mode. additionally, in the case of (a) normal distribution, we coloured the area corresponding to the
deviation from the mean by one standard deviation.

second, as the name suggests, this function must be monotonically non-decreasing:

f(x +   )     f(x),

where    > 0.

of course, we want something similar to p with a continuous random variable as well in order to easily mix discrete
and continuous random variables. the de   nition of the cumulative id203 function f in eq. (2.2) suggests such a
function which acts on a single value x, rather than a set of values (i.e. {y        |y     x}):

(cid:90) x

      

f(x) =

f (x)dx.

this function f is called a id203 density function.

it is a very bad habit, but a lot of people, including myself, often refer to both the id203 mass function
and id203 density function simply as a id203. this is certainly not correct, but often helps us make our
explanation as well as equations concise, without introducing much, if any, confusion. especially, at the level of our
discussion in this course, it is almost always okay to mix them up.4

expectation, variance and other statistics when a distribution is de   ned over many possible values, or some-
times in   nitely many values as in the case of continuous random variables, it is useful to extract a small set of repre-
sentative values of such a distribution. this is often what we do in everyday life as well. for instance, when we move
to a new city, we often ask the average monthly rent of an apartment rather than a full distribution over all possible
rent prices. furthermore, we also want to know how much a usual monthly rent varies from this average monthly rent
so as not to be surprised.

first, let us de   ne what we mean by the average x by de   ning a quantity called mean. the mean of a random

variable x, which has been assigned a distribution whose id203 function is p, is de   ned as

or in the case of a continuous variable x,

xp(x),

e [x] =    
x      
(cid:90)

x      

e [x] =

xp(x)dx.

4 but again, as your instructor, i must insist you know this distinction, and will likely ask you to clarify this distinction in the    nal exam.

30

0123450.00.20.40.60.81.0normal(2,1)pdfcdfmeanmode+- std. dev.0123450.00.20.40.60.81.0gamma(2.3)pdfcdfmeanmodewhen we say an    average    of a collection of n values, what we often mean is the following:

1
n

n

   

n=1

xn.

can we somehow connect this intuitive de   nition of average and the new de   nition above?

indeed we can. this can be done by thinking of p(x) as a frequency of x being selected out of all possible values
in    . let   s say we have in   nitely many realizations of the random variable x. p(x) then corresponds to how many
there are x in this set of in   nitely many realizations of x, meaning how frequently x occurred when we observed x
in   nitely many times. in this case, multiplying x with the frequency p(x) corresponds to adding all the occurrences of
x. by doing this for all possible values of x, we end up with our everyday life de   nition of    average   .

next, let us de   ne variance. we want to know how far each realization is from the mean on average. based on

what we have discussed, it should be clear that

when x is a continuous random variable, we replace the summation     with the integral(cid:82) . the square-root of the

variance is called a standard deviation, and is often easier to understand intuitively as it is in the same scale as the
original input x.

var(x) =    
x      

(x    e [x])2 p(x).

can we generalize this notion of variance further? how about this?

momentp(x) =    
x      

(x    e [x])p p(x).

(2.3)

this is called a p-th central moment, and has turned out to have interesting properties. for instance, the 3-rd central
moment, to which we refer as skewness, indicates whether the distribution is symmetric. for instance, the normal
distribution plotted in fig. 2.1 (a) has a zero 3-rd central moment, while the gamma distribution in fig. 2.1 (b) has a
non-zero 3-rd central moment. the 4-th central moment, called kurtosis, indicates the    atness of the distribution with
respect to the normal distribution. many of these moments have fascinating use cases in machine learning, but they
are out of scope for this course.

one interesting observation about the mean is that the mean may not correspond to an actual value. as a simple
example, let us consider a distribution over a 5-star moving rating. let us assume that a priori a 5-star moving rating
obeys the following distribution:

p(1) = 0.3, p(2) = 0.2, p(3) = 0.05, p(4) = 0.15, p(5) = 0.3.

the mean is 2.95. this number is however not at all informative because of two reasons.

first, there is no rating of 2.95. in other words, this is some fantasy number that summarizes the whole distribution,
however, without corresponding to any real rating. this could be problematic, if our goal is to use this mean to make a
decision or act. for instance, consider placing a bet on predicting the rating of a newly released movie. i cannot place
my bet on 2.95 but only on one of those    ve scores. second, even if we decide to round the mean to select the nearest
integer score, we notice that this is far from being representative of the distribution. the nearest score 3 has only 5%
chance!

this latter issue encourages us to de   ne yet another metric called a mode of a distribution. a mode of a distribution

is a value of which id203 is highest:

mode(x) = argmax

x      

p(x).

as you can easily guess, a mode is often not unique, and there may be multiple modes that have the same id203.
in fact, a uniform distribution, which assigns the same id203 of each and every possible value, has as many modes
as the size of     (in the case of a continuous random variable, this would correspond to the in   nity.) in practice, we
often relax this de   nition, and consider all local maxima as modes of a distribution.

examples of these statistics with real distributions are presented in fig. 2.1.

31

distributions there are a number of widely used distributions, both discrete and continuous. as this course is an
introduction to machine learning, we will consider only a small subset of such distributions.

in the case of discrete variables, we have already learned two popular distributions. first, we used a bernoulli
distribution for id28 in sec. 1.2. bernoulli distribution is de   ned over a set of two possible values and
fully speci   ed by a single parameter   :

p(x) =   x(1      )(1    x),

where x     {0,1}. the mean of the bernoulli distribution is   , and the variance is   (1      ).
later on in sec. 1.5, we extended this bernoulli distribution to a categorical distribution to build a multi-class
id28. categorical distribution is de   ned over a set of c possible values, and is speci   ed by c-many
probabilities:

with a constraint that they are all non-negative and sum to 1:

p(x) = px,

px     0,   x     c,
   
x      

px = 1.

in the case of continuous variables, we will solely use a normal distribution, or often gaussian distribution,
throughout this course. a normal distribution is de   ned over an unbounded real vector space rd, and is fully speci   ed
by two parameters   mean vector        rd and covariance matrix        rd  d. its id203 density function is de   ned as
(2.4)

f (x) =

exp

,

where z is the id172 constant that ensures the integral of the density function is 1. that is,

(cid:18)
(cid:18)

   

(cid:19)
1
2 (x      )(cid:62)     1(x      )
(cid:19)
1
2 (x      )(cid:62)     1(x      )

dx

1
z

(cid:90)
x   rd

exp

z =
   
= (2  )d/2|  |1/2,
where |  | is the determinant of the covariance matrix.
matrix, i.e.,

in practice, and for most of cases throughout this course, we will assume that the covariance matrix is a diagonal

                        

   =

                         .

   2
1
0
...
...
0

0
   2
2
0
...
0

0
0

      
      
0
      
...
      
          2

d

in other words, we assume that each dimension of the input x is decorrelated from each other.
id203 density function simpli   es to5

in this case, the

(cid:18)

(cid:19)

f (x) =

d

   

i=1

1

   2    i

exp

1
2

1
   2
i

(xi       i)2

   

.

(2.5)

5 of course, you may now have noticed that this simpli   cation is left for you as a homework assignment.

32

(a) joint distribution

(b) (un-normalized) conditional distributions

figure 2.2:
distributions over x given select values of y .

in the left    gure, we plot the joint distribution over x and y . in the right    gure, we plot the conditional

bayes    rule let us consider having two random variables x and y . it is not too dif   cult to imagine a joint distribu-
tion over them, which could easily be done by assigning a multivariate distribution, such as the multivariate normal
distribution from above, to a pair of x and y :6

p(x = x,y = y).

this distribution computes the id203 of x and y having the values x and y respectively. we call this distribution
a joint distribution between random variables x and y .

based on this, we can further de   ne a conditional distribution. a conditional distribution considers only a subset of
all possible joint probabilities (via the joint distribution) by    xing the value of one random variable y to a pre-speci   ed
value y:

p(x,y = y).

in other words, given that the random variable y is    xed to a value y, what is the distribution over the other free random
variable x?

one thing we notice is that the above quantity p(x,y = y) is not a valid distribution as it will not sum to 1 in

general. instead, we need to normalize it by

p(x|y = y) =

p(x,y = y)
p(y = y)

to get a proper id155 p(x|y ). we often omit = y to denote that this equation holds regardless of
which value y was assigned to:

p(x|y ) =

p(x,y )
p(y )

(2.6)

from this de   nition, we can establish one interesting notion of independence. if the random variables x and y
are independent from each other (or mutually independent), then the conditional distribution over x given y should
simply be the distribution over x, and vice versa. what does this notion of independence imply? if x is independent
from y , i.e.,

6 when it is not confusing, we will use p(x,y) as a shorthand notation.

p(x|y ) = p(x),

33

32101234x32101234y321012340.000.010.020.030.040.050.060.07p(x|y=2)p(x|y=1)p(x|y=0)p(x|y=1)p(x|y=2)then

p(x|y ) = p(x) =

p(x,y )
p(y )        p(x,y ) = p(x)p(y ).

(2.7)

in fact, the converse of this statement is also true. that is, if p(x,y ) = p(x)p(y ), then x and y are mutually indepen-
dent.

these two de   nitions can easily be mixed in. consider three random variables x, y and z. a joint distribution
over all of them is p(x,y,z). a conditional distribution over x and y given z can be written as p(x,y|z). x and y
are conditionally independent from each other given z iff

from this de   nition of conditional distribution, we can compute the conditional distribution in the opposite direc-

tion:

p(x,y|z) = p(x|z)p(y|z).

(2.8)

p(y|x) =

p(x|y )p(y )

p(x)

.

(2.9)

this rule is called bayes    rule, and it is left for you as a homework assignment to verify that this rule is indeed true.
the importance of this rule will become self-evident in the following sections.

we have so far used p(x) and p(y ) together with a joint distribution p(x,y ) without establishing their relationship.

of course, the de   nition of the id155 tells us that

p(x,y ) = p(x|y )p(y ),

but this is simply a circular de   nition. instead, can we de   ne p(x) (or p(y )) on its own based on the joint distribution
without resorting to the use of conditional distribution? yes, and we do it by marginalizing out the other random
variable:

where    y is a set of all possible values for y . with this marginalization, we can rewrite the bayes    rule in eq. (2.9) as

p(x) =    
y      y

p(x,y = y),

(2.10)

p(y|x) =

p(x|y )p(y )
   y p(x,y )

.

the usefulness of these probabilistic tools   joint distribution, conditional distribution, independence, marginal-
ization and bayes    rule    will become self-evident in the following sections, when we start to introduce a so-called
bayesian approach to machine learning.

2.3 bayesian id75
2.3.1 bayesian id75
now that we have refreshed our memory on id203, let us frame what we have learned so far into this probabilistic
terms. for this, we need to de   ne two distributions; (1) likelihood or a conditional distribution, and (2) a prior
distribution.

first, let us introduce a new symbol    which we will use to denote any adjustable parameters of a machine. for
instance,    includes a weight vector in the case of linear classi   ers and id75. in the case of adaptive basis
function networks, or deep neural networks,    includes a weight vector, or matrix, as well as all the basis vectors which
may be adjusted to maximize the classi   cation accuracy. all these seemingly diverse set of adjustable parameters can
all be    attened and concatenated to form a single vector   .

second, let us slightly modify the distance function of id75 in eq. (2.1) so as to make it easier to be
integrated into a id203 framework. as our motivation speaks for itself, we want to turn the distance function to be
a id203 (density) function, similar to what we have done with id28 (see eq. (1.26).) unlike logistic

34

regression, id75 outputs a continuous value, and therefore we use a normal distribution in eq. (2.4) instead
of categorical distribution. for simplicity, we assume that the covariance    is an identity matrix:

                        

1
0
...
...
0

0
1

0
...
0

   =

                         =      1.

(cid:19)

       0
       0
...
      
...
      
       1
(cid:18)

in this case, the id203 density function of normal distribution simpli   es to

f (x) =

d

   

i=1

1
   2  

exp

1
2 (xi       i)2

   

.

(2.11)

by carefully inspecting this equation and id75 distance function in eq. (2.1), we notice that the latter
appears as it is in the former. furthermore, by replacing xi with the i-th component of a ground-truth output y   i and
   with the output of our current machine m, we notice that minimizing the id75 distance function is
equivalent to maximizing the log-id203 density of this normal distribution. that is,

argmin
m   h

   (m   (x),m,x) =argmin

m   h    log f (m   (x);    = m(x))

=argmin
m   h

q

   

i=1

1
2 (y   i       i)2 +c,

(2.12)

(2.13)

where c is a constant that does not depend on m, and the constant multiplicative factor 1
mization problem.

2 does not change the opti-

in other words, we can rewrite the distance function of id75 as a negative log-id203 of a ground-
truth output vector y    under a normal distribution whose mean is the output of the current machine m and covariance
is an identity matrix. so, we have somehow turned id75 into a probabilistic model. let us continue.

if we consider this    and an entire training set d = dtra

7 as random variables, a notion of how
likelihood p(d|   )
likely the training set, or the set of training examples, is given some   . this is precisely what id155
is (see eq. (2.6),) and we call this speci   c id155 p(d|   ) a likelihood. how does this likelihood look
like?
in this course, we have implicitly assumed that each training input vector xn (and consequently the corresponding
reference output y   n) was independently drawn from a single data distribution (see eq. (1.3) from long time ago.) then,
based on the de   nition of conditional independence in eq. (2.8), we can decompose the likelihood into

p(y   n|xn,   )p(xn|   ) =
the logarithm of this likelihood, to which we refer as log-likelihood, is then

p((xn,y   n)|   ) =

p(d|   ) =

n=1

n=1

n

   

n

   

n

   

n=1

p(y   n|xn,   )p(xn).

log p(d|   ) = c +

n

   

n=1

(cid:124)
(cid:125)
log p(y   n|xn,   )

(cid:123)(cid:122)

,

(a)

where

c =

n

   

n=1

log p(xn)

is a constant with respect to   . where have we seen (a) before?

yes, we have seen it twice already this course. we saw one when we de   ned the distance functions of logistic
regression and multi-class classi   cation, and we just saw this for id75 right above in eq. (2.12). we have

7 i will use d instead of dtra throughout the remainder of this section for brevity.

35

just had a glimpse of an interesting relationship between the likelihood (or its logarithm version, log-likelihood) and
the empirical cost function. that is, if the distance function of a model could be described in terms of a id203
function, we can de   ne an equivalent log-likelihood functional.8

prior p(   ) before we observe a training set d, or data, what kind of prior knowledge do we have about the parame-
ters of a machine? or, similarly what kind of prior information do we want to impose on the parameters? the answer
to the former question may be problem-speci   c, while that to the latter may be algorithm-speci   c. a prior distribution
is our way to impose or incorporate such prior knowledge.

unlike the likelihood, the prior distribution p(   ) is de   ned solely on the parameters    irrespective of actual data d.
one of the most widely used prior distributions is again a normal distribution often with an all-zero mean vector and
a diagonal covariance matrix, as in eq. (2.5).9 intuitively, this choice of prior distribution states that each parameter
is unlikely to deviate too much from 0, and how far it may deviate depends on the variance    2.10 mathematically, we
express this prior distribution by

   2
i
2   2
where |  | is the dimensionality of the vector   . the logarithmic form is

   2    

p(   ) =

exp

   

i=1

1

|  |   

(cid:18)

(cid:19)

,

log p(   ) =

|  |   
i=1   

   2
2   2     log   2     .
i

(2.14)

what does this remind you of?

of course, this is not the only choice, and we can freely choose a prior distribution so as to impose certain struc-
tures. for instance, if we believe the    rst and second parameters are strongly correlated with each other, we may
choose to abandon the diagonal covariance matrix and instead use a covariance matrix where the cross-correlation
term between the    rst and second parameters is a large positive value. but, we will get to this later (hopefully!)

posterior p(  |d) at the end of the day, we have two goals in machine learning. first, we want to    gure out what
the parameters of a model look like once trained. we tackled this problem earlier by minimizing the empirical cost
function (with some id173, as in sec. 1.4.3). in a probabilistic framework, however, we are rather interested
in a posterior distribution over the parameters given a training set: p(  |d).
unlike the likelihood and prior, we will not designate the form of this posterior distribution ourselves. because we
already have the likelihood and prior distribution, we can instead compute the posterior distribution from them using
bayes    rule in eq. (2.9):

(cid:44)

(cid:124) (cid:123)(cid:122) (cid:125)
p(  |d) = p(d|   )

likelihood

(cid:124)(cid:123)(cid:122)(cid:125)

p(   )
prior

(cid:124)(cid:123)(cid:122)(cid:125)

p(d)
evidence

.

clearly, we need one more distribution p(d), to which we refer as evidence. or, do we?11

similarly to the likelihood, it is common to consider the log-posterior:

log p(  |d) = log p(d|   ) + log p(   )    log p(d).

8 a functional is informally-speaking a mapping from a function to a scalar. both the distance function and log-likelihood are functional in that
the input to them is a function m. however, we can equivalently think of them as functions by considering the parameters    of m as their input.
this view is enough for the content of this course, but does not hold true in general.

9 now you see why i introduced a normal distribution only when we were discussing about common distributions.
10 notice the lack of the subscript i. we assume that each and every parameter   i has the same variance   .
11 the following two questions are left for you as homework assignments.
1. why do we call this distribution, or the id203 of data d, evidence?
2. do we need to de   ne this evidence distribution ourselves? if not, what can we do about it?

36

other than the log-evidence log p(d), let   s try to plug in what we know in the case of id75 to the right-hand
side of this log-posterior:

log p(w|dtra) =

log p(xn) +

n

(cid:124)
   

n=1

q

   

i=1

1

2 (y   n,i     [w(cid:62)xn]i)2     log   2  
(cid:125)
(cid:123)(cid:122)
=log p(d|   )

+

q

   
j=1   

d

(cid:124)
   

i=1

w2
i j

2   2     log   2    
(cid:125)
(cid:123)(cid:122)

=log p(   )

   log p(dtra).

maximum-a-posteriori (map) estimation: poor man   s bayes given this form of the log-posterior distribution,
what should we do? the    rst thing that comes to my mind is to    nd the parameter vector   , or correspondingly
the weight matrix w, that maximizes this posterior id203. that is, we want to    nd a mode of the posterior
distribution:

log p(  |d) = argmax
in the case of id75, this equation is equivalent to

     = argmax

  

  

log p(d|   ) + log p(   )    log p(d).

n

   

n=1

log p(xn) +

q

   
i=1   

1

2 (y   n,i     [w(cid:62)xn]i)2     log   2   +

d

   

i=1

q

   
j=1   

w2
i j

2   2     log   2         log p(d)

argmax

  

log p(w|dtra) =argmax

  

=argmax

  

q

   
j=1   

w2
i j

2   2     log   2         log p(d)

q

n

   

n=1
n

n=1
n

   
(cid:124)
   

n=1

d

q

1

i=1

   

2 (y   n,i     [w(cid:62)xn]i)2     log   2   +
   
i=1   
1
1
   
2   2 w2
2 (y   n,i     [w(cid:62)xn]i)2    
i=1   
c
1
(cid:125)
(cid:123)(cid:122)
(cid:125)
(cid:124)
   
2 (y   n,i     [w(cid:62)xn]i)2
2

   
(cid:123)(cid:122)
   

i=1
d

j=1
q

w2
i j,

   

   

j=1

i=1

+

i=1

d

q

q

empirical cost

id173

i j

=argmax

  

=argmin

  

where c = 1
they are not dependent on the parameters   .

   2 . notice that the log-partition functions log   2     s and the log-evidence log p(d) have been removed, as
by carefully inspecting the above equation, especially the    nal form, we notice that this is identical to the empirical
cost function of id75 augmented with a id173 term. more speci   cally, the id173 term is
the maximum margin id173 term we learned earlier from support vector machines in sec. 1.3.2. in other
words, this probabilistic formulation of id75 has resulted in a more traditional formulation of machine
learning we have discussed so far, in the terms of empirical cost function and id173.

does this mean that all we have done during the past one and a half week has been to simply    nd a different way
to end up with the exact same solution we have already learned? that would have been an awful waste of our time,
wouldn   t it?

predictive distribution p((x(cid:63),y(cid:63))|d) now we go one step further. in supervised machine learning, what is our
ultimate goal? was it to    nd the parameters that minimize the empirical cost function together with a id173
term? in some cases, yes. our goal was however to build a machine that can predict the outcome of a future, unseen
input vector as well as possible.

can we push this even further? perhaps what we want is only to know the outcome of a new, unseen input
vector. if we can do so, do we really care about speci   cally which machine (equivalently, which set of parameters)
has been used? indeed, we may want to use more than one machines to make their own predictions and return us their
(weighted) consensus. we discussed this possibility already at the very beginning of this course in eq. (1.6).

eventually, what we want to know the conditional distribution over the new example (x(cid:63),y(cid:63)) given the training

37

(a)

(b)

figure 2.3:
(a) bayesian id28, and (b) bayesian multilayer id88. both of them were done with
   ensemble samplers with af   ne invariance    [9] using python emcee available at http://dan.iel.fm/emcee/
current/.

set.12 we can do this by marginalizing out the parameters   . as we have seen earlier in eq. (2.10), we can do so by

p((x(cid:63),y(cid:63))|d)
=            p(x(cid:63),y(cid:63),  |d)
=            p(x(cid:63),y(cid:63)|   ,d)p(  |d)
=            p(x(cid:63),y(cid:63)|   )p(  |d),

marginalization
id155
conditional independence

where we assumed in the last line that the distribution over an example is independent from all the other examples
given a set of parameters.    is a set of all possible sets of parameters. examining the    nal form further, we notice that
it is the expectation of the likelihood under the posterior distribution over the parameters:
= e  |d [p(x(cid:63),y(cid:63)|   )] .

(cid:123)(cid:122)
(cid:125)
(cid:124)
p(x(cid:63),y(cid:63)|   )

(cid:124) (cid:123)(cid:122) (cid:125)
p(  |d)

p((x(cid:63),y(cid:63))|d) =    
       

posterior

likelihood
we call this resulting distribution a predictive distribution.

what does this equation imply intuitively? the answer is quite straightforward. we will let each machine,
parametrized by   , cast a vote by scoring each possible answer y given a new input vector x(cid:63). their votes are however
not equal, and the votes by those parameters, or equivalently models, which are more likely given the training data d
(i.e., higher posterior p(  |d)) would be weighted more. in other words, those machines that do better in terms of em-
pirical cost function are given higher weights than those that do worse. the term   (m) in eq. (1.6) thus corresponds
to the posterior id203 of m (equiv. to   ).

as an extra-credit homework assignment, you are asked to derive a closed-form id203 density function of
the predictive distribution in the case of id75. id75 with this derived predictive distribution is
at the heart of bayesian id75.

2.3.2 bayesian supervised learning
although we have used id75 as an example13 throughout the whole discussion on this probabilistic ap-
proach to machine learning, it should have been clear that this idea is generally applicable as long as we de   ne the
12 note that y(cid:63) would not be given together with x(cid:63), but if we know their joint distribution p(x(cid:63),y(cid:63)|d), we can get the conditional distribution
over y(cid:63) given x(cid:63) according to the de   nition in eq. (2.6). of course, in the case of supervised learning, we probably want to compute p(y(cid:63)|x(cid:63),d)
directly.

13 we call id75 under this probabilistic approach a bayesian id75.

38

   3   2   10123   3   2   10123   2.0   1.5   1.0   0.50.00.51.01.52.0   3   2   1012345following distributions:

    prior distribution over the parameters: p(   )
    likelihood distribution: p(d|   )

from these two distributions and a training set d, we can derive the posterior distribution p(  |d), and compute the
predictive distribution.
of course, this is not strictly true in that in almost all cases, we do not know how to compute the posterior
distribution and/or predictive distribution exactly. fortunately with id75, we were able to do so, but as
soon as we encounter some non-trivial prior and/or likelihood distributions, it is impossible to derive an analytical
form of either posterior or predictive distribution.

thus, to be more precise, in addition to de   ning those two distributions above, we need to be able to compute,
either exactly or approximately, the posterior distribution.14 there are two major, general approaches to this prob-
lem of computing the posterior distribution. the    rst one is a sampling-based approach by which we can generate
samples from a given distribution one at a time with an asymptotic guarantee that the collected samples are from the
given distribution. the most widely used family of sampling-based algorithms is called markov-chain monte-carlo
(mcmc). the second approach is variational id136 in which a simpler approximate posterior distribution is    tted
to the complex, but true posterior distribution. once this simpler approximate distribution is found, we use it for any
subsequent task including the computation of the predictive distribution. there are a number of other approaches, such
as message passing (or belief propagation) that could be used. unfortunately any of these approaches are far out of
scope of this course.15

instead, let me show you two examples in fig. 2.3. on the left panel is the visualization of the predictive distri-
bution of bayesian id28. the background gradient indicates the predictive id203 of y = 1 given a
training set (blue markers); red close to 1, and blue close to 0. we notice that the con   dence, which can be thought of
as how far away the predictive id203 of y = 1 is from 0.5, is lower near the decision boundary. furthermore, we
notice that the low-con   dence region grows as a new input vector is further away from the training examples.

on the right panel is the example of bayesian multi-layer regression which is the regression version of the adaptive
basis function network from sec. 1.7.4. the plot shows both training examples, the mean of the predictive distributions
as well as its standard deviation (shaded region surrounding the mean curve.) a noticeable feature is that the standard
deviation shrinks when there are many training examples nearby, and grows in the other case.

both examples were done using a speci   c mcmc algorithm to generate many samples from the posterior distri-

bution.

2.3.3 further topic: gaussian process regression(cid:63)
in id75, when the prior distribution over the parameters is gaussian and the likelihood is also gaussian, the
posterior distribution over the parameters as well as the predictive distribution are both gaussian, due to the property
of multivariate gaussian distribution. it implies that we can fully characterize the prediction of a new input example
given a training set by computing the mean and covariance of multivariate gaussian distribution. the covariance
matrix is often speci   ed by a covariance function, and a suitable choice allows us to turn the id75 into
nonid75. unfortunately, gaussian process regression is out of this course   s scope, and i refer readers to a
widely read textbook [24] by williams and rasmussen, and a great introduction tutorial16 by david mackay.

14 when i say compute a distribution, i mean that being able to (1) compute the (unnormalized) id203 and (2) compute the expectation and

the n-th central moments in eq. (2.3).

15 i recommend [2] for further discussion.
16 http://videolectures.net/gpip06_mackay_gpb/

39

chapter 3

id84 and matrix
factorization

3.1 id84: problem setup

let us assume that we are given a set of input vectors without their corresponding labels. what can we do about these
input vectors

d = {x1, . . . ,xn}?

perhaps, the    rst thing we should try is to look at all of these input vectors to see if we can    nd any interesting
regularity behind them. if they are two-dimensional vectors, we can visualize them by plotting a two-dimensional
scatter plot in which each vector is drawn as a single point. if they are three-dimensional vectors, we can still visualize
them by plotting a three-dimensional scatter plot or by plotting a contour plot. this is however not as trivial as plotting
two-dimensional points. if they are four-dimensional vectors, already we run out of any    general    way to visualize
them.

when we are presented with high-dimensional vectors, we try to reduce their dimensionality in order to (1) vi-
sualize them more easily, and (2) more ef   ciently process them.1 there is a family of machine learning techniques
dedicated to this problem of reducing the dimensionality of input vectors. applying one of these techniques is called
id84.

in id84, given a set of input vectors

d = {x1,x2, . . . ,xn}
our goal is to    nd a set of corresponding lower-dimensional vectors
  d = {z1,z2, . . . ,zn} ,

where xi     rd, z j     rq, and d (cid:29) q.
there are two major families of techniques in id84. first, there are parametric dimensionality
reduction techniques. similarly to supervised learning we have learned earlier, we obtain a machine m that either
maps from an input vector to its corresponding lower-dimensional vector

or maps from a lower-dimensional vector to its corresponding higher-dimensional vector2

m : rd     rq,

1 why is it more ef   cient to process data points if they are lower-dimensional vectors? this question is left to you as a homework assignment.
2 note that    nding either of these two mappings is equivalent when such a mapping is invertible. we will discuss this more in detail later when

we introduce a deep autoencoder.

m : rq     rd.

40

again, as we learned with supervised learning, this type of machine may be categorized into either linear or nonlinear.
in the case of linear, parametric id84, the problem can be formulated as id105. on
the other hand, nonlinear, parametric id84 is more general, and we will discuss one particular
instantiation, called a deep autoencoder later in this chapter.

the other family of id84 techniques consists of non-parametric techniques. a non-parametric
id84 technique does not provide a separate machine that could be used on a new example, but
only returns a set of lower-dimensional vector   d. these techniques are often strictly used for visualization of high-
dimensional data.

in this course, we will mainly focus on the parametric family of id84 techniques, starting
from id105 and ending with deep autoencoders. if time permits, we will study some non-parametric
id84 techniques that are widely used in the scienti   c community.

3.2 id105: problem setup

let us build a large matrix that contains all the input vectors by lining them next to each other. that is,

id105 is then a problem of (approximately) representing this data matrix x as a product of two matrices:

x = [x1;x2;      xn]     rd  n.

x     wz,

(3.1)

where z is a matrix that contains all the lower-dimensional vectors
z = [z1;z2;       ;zn] ,

and w     rd  q is a weight matrix. z is often called a code matrix, and w a dictionary matrix. by setting q to be
smaller than d, i.e., q (cid:28) d, this id105 allows us to    nd a set of lower-dimensional vectors. in other
words, id105 allows us to reduce the dimensionality of input vectors.
id105 is a widely-used, and perhaps oldest, id84 technique. it is a linear dimen-
sionality reduction technique, as evident from eq. (3.1). why is this so? because, the equation in eq. (3.1) could be
understood as a set of n equations of which one is

xi = wzi.

(3.2)

id105 is precisely a way to build a linear machine that maps from a lower-dimensional vector to its
original space rd.

eq. (3.2) reminds us of id75 in sec. 2.1. the only difference is that both z and x were known in
id75, while only one of them, x, is known in id105. this lack of input vectors, following
the terminology from id75, implies that this problem is under-speci   ed, meaning that there may be many
solutions of w and z that satisfy eq. (3.1). this is quite obvious, if we consider the simplest case of d = q = 1, in
which case the whole problem reduces to    nding w and z that satis   es

where x is known. for any solution (w,z) that satis   es this equality, there are in   nitely many other solutions (w(cid:48),z(cid:48))
such that

x = wz,

w(cid:48) = cw and z(cid:48) =

z
c ,

where c is an arbitrary real number. a similar thing happens with d,q > 1 with any invertible matrix c     rq  q,
because

x = (wc)(c   1z) = w(cc   1

)z = wz.

(cid:124)(cid:123)(cid:122)(cid:125)

=i

what this implies is that there are many different ways to solve this id105. by imposing a set of
clever constraints on the weight matrix w and/or z, we get a diverse set of linear id84 algorithms.
in the rest of this section, we investigate three such algorithms.

41

3.2.1 principal component anslysis: traditional derivation
the    rst such algorithm is called principal component analysis (pca). in pca, there are two constraints. the    rst
constraint is on z, or a set of code vectors z1, . . . ,zn. it states that each component of these code vectors must be
decorrelated from all the other components. this is equivalent to

assuming that zn   s are centered. we can write it more compactly by

n

   

n=1

z j,nzi,n = 0, for all i (cid:54)= j,

zz(cid:62) =

where

n

   

n=1

znz(cid:62)n = diag(   2

1 , . . . ,   2
q ),

   = diag(   2

1 , . . . ,   2

q ) =

                        

   2
1
0
...
...
0

0
  22
0
...
0

0
      
0
      
...
      
...
      
          2

q

                         .

the second constraint is that these variances    2

j    s are non-increasing. in other words,
   2
i        2

j , for all i < j.

with these constraints in our mind, let us consider the (scaled) covariance of the input vector matrix x which is

de   ned as

xx(cid:62)(cid:124)(cid:123)(cid:122)(cid:125)

c=covariance of x

=(wz)(wz)(cid:62)

=wzz(cid:62)(cid:124)(cid:123)(cid:122)(cid:125)

w(cid:62)
  =covariance of z
=w  w(cid:62)

since any covariance matrix is by de   nition symmetric,3 we immediately notice that this equation precisely describes
a procedure called eigendecomposition:

assuming q = d. w is then a matrix consisting of d eigenvectors of c on each column:4

c = w  w(cid:62),

where

w = [v1;v2;       ;vd] ,

cv j =    2

j v j

(3.3)

(3.4)

which is the de   nition of the j-th eigenvector, and    2
j is the corresponding j-th eigenvalue. these eigenvalues and the
corresponding eigenvectors can be computed by    rst solving the characteristic polynomial of c to    nd all eigenvalues

3 a matrix c is symmetric if and only if

c = c(cid:62).

4 we assume that n (cid:29) d, and that the subspace in which all the input vectors lie is at least q-dimensional.

42

and solving eq. (3.4) for each of the eigenvalues.5 we see that eq. (3.3) is nothing but a stack of eq. (3.4) in the
decreasing order of    2
j    s. in other words, we can compute the weight matrix w by eigendecomposition of the (scaled)
covariance matrix c of the input matrix x.

this is a good start as this selection of the weight matrix w ensures that the code vectors are decorrelated, satisfying
the    rst constraint. but, how do we compute the code matrix z? one important property of the eigenvectors is that
they are orthogonal to each other.6 since the inverse of any orthogonal matrix is its transpose, we can compute the
code matrix by

z = w(cid:62)x.

(3.5)

along the derivation, we have made one assumption that q = d. this effectively means that we have not done any
id84. how do we then use this whole procedure to reduce the dimensionality, that is q (cid:28) d? we
can do it easily by simply taking only the    rst q columns of the weight matrix, or taking only the    rst q eigenvectors.
that is,

w = [v1;v2;       ;vq] .

because the eigenvectors were sorted in the decreasing order of the eigenvalues which correspond to the variance of
each component of the code vectors, this satis   es the second constraint. of course, this breaks the equality in eq. (3.3),
but is still a good approximation, as we will see shortly.

n    n

n=1 xn

in summary, pca is done in the following steps:
1. centering: xn     xn     1
2. covariance: c = xx(cid:62)
3. eigendecomposition: c = w  w(cid:62)
4. top-q-column extraction: w     w:,1:q
5. code vectors: z = w(cid:62)x

despite the simplicity in description, this whole process is computational expensive, especially due to the computation
of the covariance matrix c. it is therefore desirable to skip computing the covariance matrix, and fortunately there is
a way to do so by resorting to singular value decomposition.

singular value decomposition (svd) decomposes a given matrix x into a product of three matrices:

(3.6)
where w     rd  d and v     rd  n are orthogonal matrices, and s     rd  d is a diagonal matrix with decreasing diagonal
entries. let us use this decomposition to compute the covariance matrix c:

x = wsv,

c =xx(cid:62)

s(cid:62)(cid:124)(cid:123)(cid:122)(cid:125)

=s

w(cid:62)

=wsv(wsv)(cid:62)

=wsvv(cid:62)(cid:124)(cid:123)(cid:122)(cid:125)

=i
=wssw(cid:62)
=w  w(cid:62).

surprised? yes, the matrix w could be precisely recovered by svd on the input matrix x without computing the
covariance matrix. because of this, it is common to directly use svd on the input matrix in practice.

5 it is out of the scope of this course to teach how to    nd eigenvalues and eigenvectors. i refer students to take, for instance, math-ua 140

id202 listed at the department of mathematics.

6 this is true only when the associated eigenvalues are positive.

43

proportion of variance explained: pv the variance of the sum of decorrelated random variables is simply the
sum of the variances of all those random variables, i.e.,

var(

q

   

i=1

zi) =

q

   

i=1

var(zi).

by considering each component of the code vectors as a random variable, this implies that the (empirical) variance
of the sum of these code components is simply the sum of the    rst q eigenvalues:    q
i . we can then compute
how much variance has been explained by the code vectors by inspecting the ratio between the variance of the code
components and the variance of the whole input:

i=1    2

pv(q) =

   q
i=1    2
i
   d
j=1    2
j

.

what does this proportion of variance explained (pv) correspond to? we will see it in the upcoming demonstration

session.

what are principal components? the principal components are the column vectors of the weight matrix w. they
are orthogonal to each other, and form a q-dimensional subspace in the original d-dimensional real space rd. the
code vector z of an input vector x is then an orthogonal projection of x onto this subspace. this projection z obtained
by eq. (3.5) then corresponds to the reconstruction in the original space by
  x = z1w1 + z2w2 +       + zqwq = wz.

(3.7)

note that

with this reconstruction, we can de   ne a reconstruction error by

zi = w(cid:62)i x.

(cid:107)x      x(cid:107)2
2.

now the question is what kind of subspace the procedure above    nds. from the    rst constraint, that the code vec-
tors are decorrelated with each other, we know that the principal components are eigenvectors of the input covariance
matrix c.7 what does the second constraint tell us about our selection of the    rst q eigenvectors?

first, we notice that we can represent any input vector as a weighted sum of the eigenvectors, similarly to eq. (3.7):

where

x = z(cid:48)1w1 + z(cid:48)2w2 +       + z(cid:48)dwd,

z(cid:48)i = w(cid:62)i x.

7 orthonormal vectors are orthogonal to each other and have a unit norm.

44

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

z(cid:48)jw j

d

   

j=q+1

the reconstruction error can then be written as8

(cid:107)x      x(cid:107)2
2 =

=

=

=

=

=

   

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) q
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) d

i=1

   

j=q+1
d

   

j=q+1

(z(cid:48)i     zi)wi +

z(cid:48)jw j

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:124)(cid:123)(cid:122)(cid:125)
z(cid:48)2
j (cid:107)w j(cid:107)2

2

=1

d

   

j=q+1

d

   

j=q+1

d

   

j=q+1

z(cid:48)2
j

w(cid:62)i xx(cid:62)(cid:124)(cid:123)(cid:122)(cid:125)

wi
c=covariance

w(cid:62)i cwi.

now this looks awfully similar to the eigendecomposition from eq. (3.3), where we learned that9

in other words, the reconstruction error equals to the sum of the eigenvalues of the discarded eigenvectors:

   = w(cid:62)cw
j = w(cid:62)j cw j, for all j = 1, . . . ,d.

          2

(cid:107)x      x(cid:107)2
2 =

d

   

j=q+1

   2
j .

thus, we minimize the reconstruction error by selecting the top-q eigenvectors according to their corresponding eigen-
values as the principal components. if we want to add one more, we add another eigenvector whose eigenvalue is
greater than equal to that of any other remaining eigenvector.

this view of pca as    nding a subspace that minimizes the reconstruction error becomes handy when we extend
it to nonlinear pca later. a deep autoencoder, which is one realization of nonlinear pca, directly and explicitly
minimizes the reconstruction error.

3.2.2 pca: minimum reconstruction error with orthogonality constraint
we have derived pca from the two constraints; (1) the elements of a code vector z are decorrelated, and (2) the
variances of the elements of a code vector are sorted in a decreasing order. we have seen that the    rst constraint led to
the orthogonality of the weight matrix, or the dictionary matrix, w. the second constraint, on the other hand, led to
the minimum reconstruction error criterion. this latter revelation tells us that the second constraint of pca is perhaps
not a criterion on its own, but rather a consequence of optimization which has been at the core of machine learning so
far in this course (except for the bayesian id75.)

here, we will re-formulate pca by starting with an optimization cost, which is similar to the empirical cost
function we have used throughout our discussions on supervised learning. the optimization cost function is de   ned
as the reconstruction error over a given training set dtra:

  r(w,z;dtra) =

n

2

   
n=1(cid:107)xn       xn(cid:107)2
=(cid:107)x    wz(cid:107)2
f .

8 note that i am looking at a single input vector x, but this equation trivially extends to, and is in fact necessary to have, multiple input vectors
x1, . . . ,xn. for brevity and simplicity, my derivation here considers a single input vector, but it is left for you as a homework assignment to extend
this derivation to use multiple input vectors.

9 it is your homework assignment to show that the equations below are true.

45

looking closely at the equation at the bottom, we in fact see that this corresponds to    nding the weight matrix w
and the code matrix z such that their product wz is close to the input matrix x, which was the main goal of matrix
factorization from the very beginning as in eq. (3.1).

this minimization alone does not however result in pca, as naive minimization would not    nd a solution that

satis   es the    rst constraint. we therefore impose that the    rst constraint be satis   es during optimization. that is,

  r(w,z)

min
w,z

subject to the constraint that

the column vectors of w are orthonormal vectors.

assuming that this constraint is always satis   ed during optimization, we can now safely replace z with w(cid:62)x, because
w(cid:62)w = i. this results in

subject to subject to the constraint that

  r(w,x;dtra) = (cid:107)x    ww(cid:62)x(cid:107)2

f

the column vectors of w are orthonormal vectors.

this alternative derivation of pca gives us a more general framework on top of which various id105
algorithms could be implemented. in this general framework, the goal is to minimize the reconstruction error (cid:107)x   
p
wz(cid:107)
f. this is natural, as our goal is to    nd the weight and code matrices whose product is approximately the input
matrix. we then need another mechanism g , or a function, that allows us to map from a given input vector x to its
corresponding code vector z, i.e. z = g(x,w). that is, we should be able to infer what the code matrix z is given the
input matrix x and the weight matrix w.10 then, we need a certain constraint on either the weight matrix w and/or
the code matrix z. in summary, a id105 algorithm, or framework, is de   ned based on the following
items:

1. cost function (almost always reconstruction error)

2. constraints on w and/or z

3. id136 mechanism g: infer z from x given w

furthermore, the appropriate choice of constraints relaxes the basic assumption we had earlier about the dimensionality
of the code vector q that it is often much smaller than d.

to reiterate, in the case of pca, the cost function is a reconstruction error de   ned in terms of euclidean distance
(or frobenius norm). the id136 mechanism is simply z = g(x) = w(cid:62)x, and there is a single constraint that w is
orthogonal.

many different id105 algorithms could be formulated under this framework. for instance, sparse

coding [17] is de   ned by

1. cost function: l2 reconstruction error
2. constraint: (cid:107)z(cid:107)0 = k, for some k > 0
3. id136 mechanism: minimization with respect to z

it is often the case with sparse coding that q (cid:29) d. independent component analysis (see, e.g., [11])is on the other hand
de   ned by

1. cost function: l2 reconstruction error
2. constraint: zi   z j, for all i (cid:54)= j
10 note that g may not be a function, but a process in which the reconstruction cost function is minimized with respect to the code matrix z

(instead of w). this is a usual practice in sparse coding.

46

3. id136 mechanism: minimization with respect to z or g(x) = uz

u is a matrix separate from mw that recovers the code vector from an input vector.     is used to denote the inde-
pendence between two random variables (see eq. (2.7).) because of different choices of id136 mechanism and
constraints, these algorithms often learn different weight matrices and code matrices even when provided with the
same input matrix x.

in the next subsection, we will consider another id105 algorithm, called non-negative matrix factor-

ization, that is capable of learning a so-called part-based representation.

3.2.3 non-negative id105: nmf
let us consider another id105 scheme called non-negative id105 (nmf, [15]). let us go
step-by-step here. what is our main objective? exactly the objective function we have used for principal component
analysis (pca):

  r(w,z) = (cid:107)x    wz(cid:107)2
f .

what kind of constraints do we want? the name itself suggests them. that is, both the weight matrix w and the code
matrix z are non-negative. of course, this naturally assumes that the input matrix x is also non-negative, because the
sum, product or their combination, of non-negative numbers would never result in negative. what this implies is that
we must ensure that the input matrix is non-negative by, for instance, subtracting the minimum value of x from x.
this is a preprocessing step that is similar to the centering step of pca.

in summary, the optimization problem of id4 is de   ned as

w,z (cid:107)x    wz(cid:107)2
argmin

f

(3.8)

subject to

wi j     0, for all i = 1, . . . ,d and j = 1, . . . ,q
zi j     0, for all i = 1, . . . ,q and j = 1, . . . ,n.

before learning how to solve this constrained optimization problem, let us discuss why nmf is an interesting case of
id105.

as we discussed earlier, the weight matrix w is often called a dictionary matrix. this dictionary matrix contains

a set of q atoms:

w = [w1;w2;       ;wq] .

these atoms are then combined according to their coef   cients, given by a code vector, to form one of the input vectors.
that is,

  x = z1w1 + z2w2 +       + zqwq = wz.

now let   s do some quick thought experiment. input vectors are human faces. what would be those atoms, when we
are constrained to only add them? in other words, those positive-valued atoms and their positive-valued coef   cients
prevent us from cancelling out the contributions of the atoms. for instance, we cannot have an atom that has three
positive noses and another atom that has two negative noses so that summing them would leave us only one nose. in
order to build a full face with nmf, we would need to have an atom for a nose, an atom for a pair of eyes, an atom for
a mouth, an atom for a pair of ears and so on. in other words, each positive-valued atom needs to contain a set of parts
of a face that do not overlap with each other, and each positive-valued coef   cient indicates how much contribution the
corresponding atom makes to the full face.

this is precisely the motivation behind the id4. lee and seung [15] showed that if you solve nmf on a set x of
(normalized) faces, the dictionary matrix w contains face parts (such as mouth, eyes, nose, and so on), and the code
matrix z captures how some of those parts are selected and combined to form full faces. this process for one face is
shown in fig. 3.1.

47

figure 3.1: a graphical illustration of how
non-negative id105 models a face
as a weighted sum of parts in a dictionary ma-
trix w. the    gure was taken from [15].

figure 3.2: a graphical illustration of how
non-negative id105 models a
document as a weighted sum of topics from
a dictionary matrix w. the    gure was taken
from [15].

let us do another thought experiment. how about modelling a document? as we did earlier in sec. 1.7.1, we
assume each document is represented as a bag of words. what would be a good dictionary of atoms in this case?
perhaps each atom should represent a topic. if a document is about ice hockey, this document would be a result of
adding multiple topics such as sports, hockey and ice. again, each of these topics would have to be exclusive, since
the non-negativity constraint prevents us from two atoms to cancel out each other. again, this was one of the examples
given in [15], as shown in fig. 3.2.

the constrained optimization problem for nmf in eq. (3.8) is not a trivial problem, and many optimization algo-
rithms have been proposed to solve nmf. since most of them are way out of the scope of this course, i will brie   y
describe a straightforward extension of gradient-descent algorithm, called projected gradient-descent algorithm. un-
like the usual id119 algorithm, the projected gradient-descent algorithm consists of two steps. the    rst step
is to update the parameters (in our case, w and z) following the negative gradient direction:

  w =w         w   r = w      (x    wz)z(cid:62),
  z =z         z   r = z      w(cid:62)(x    wz),

(3.9)
(3.10)

which is precisely the gradient-descent algorithm.

the second step (note that we have not updated the actual matrices yet) involves projecting these candidate matrices

back to a feasible region which consists of all points that satisfy the constraints. that is,

w = argmin

w   0 (cid:107)w      w(cid:107)2
z   0 (cid:107)z      z(cid:107)2

f ,

f .

z = argmin

48

in other words, we want to    nd, for instance, a weight matrix w with all non-negative values that is close to the
updated matrix   w, and the same for the code matrix z. finding such a projection is generally dif   cult, but in the case
of nmf, the projection is simply

wi j =max(0,   wi j),
zi j =max(0,   zi j).

that is, we simply cut off any negative values from both the weight matrix and code matrix. this projected gradient
algorithm is widely used for non-negative id105 in addition to the original multiplicative update rules
from [15].

we will see nmf in action during the next demonstration session.

3.3 deep autoencoders: nonlinear id105
a major limitation of id105 x     wz is that the relationship between a code vector z and its correspond-
ing input vector x is linear. given a    xed dimensionality q of a code vector, what happens if there is no good linear
map from it to the corresponding input vector? perhaps what we need is to relax this linearity assumption. that is,

x = f   (z),

where f   is a nonlinear function parametrized by a set    of parameters. we can use, for instance, a deep feature
extraction function (or deep neural network) from sec. 1.7.4.

as usual with the other id105 methods above, we    rst de   ne a cost function as a reconstruction error.

a usual one is euclidean distance as below:

  r(   ,z) =

1
n

n

   
n=1(cid:107)x    f   (z)(cid:107)2

2.

this is however not the only option. for instance, if x is a binary vector, it would be a good idea to use the negative
log-id203 under bernoulli distribution, as we did with id28 earlier (remember?)

now we need an id136 mechanism for computing z given x. in deep autoencoders, this is done by having
another deep feature extraction function g   that maps from an input vector x to its corresponding code vector z. that
is,

the question is where this new set    of parameters comes from. the answer turns out to be more straightforward. we
can simply    nd both    and    to minimize the reconstruction error. in other words,

z = g   (x).

  r(   ,   ) =

1
n

n

n=1(cid:107)x    f  (cid:124)(cid:123)(cid:122)(cid:125)
   

decode

( g  (cid:124)(cid:123)(cid:122)(cid:125)

encode

(x))(cid:107)2
2.

in this problem, we see that g   encodes an input vector x, and f   decodes the obtained code vector back into the input
vector   x. we thus call this approach a deep autoencoder [10].

it is now time to decide on the constraint. unfortunately there is not a well-agreed-upon set of constraints for
deep autoencoders. some impose sparsity on the code vector z [6], some constraint that the dimensionality of the
code vector is smaller than that of the input vector [10], or encourages the code vectors to be close to a binary vector
it is also possible to let the encoder f and decoder g share parameters in order to avoid a
by adding noise [20].
trivial solution in which f is a random mapping. it is out of this course   s scope to discuss all these constraints, and i
recommend you to read [8] and take the course deep learning taught by prof. yann lecun.

49

3.4 id84 beyond id105
3.4.1 metric multi-dimensional scaling (mds)
so far we have considered id105 for id84. a major property of such approaches
was that we assumed a linear, or nonlinear in the case of deep autoencoders, mapping from a code vector to the
corresponding input vector. that is, x     wz or x     f (z). in this section, we ask whether this is necessary.
a major disadvantage from this id105 based approach is that all the input data points must be
presented in their vector forms. what if this is not easily doable nor natural? for instance, consider embedding a graph
which consists of many nodes, or vertices, and their connections, or edges. unfortunately each node is anonymous
in the sense that there is no description attached to it. without any description, such as its absolute coordinate, it is
not easy to label each node with its vector representation. instead, we have a set of edges that de   ne the connectivity
among those nodes, and perhaps each edge comes with a weight that de   nes the similarity or distance between a pair
of nodes. what would be represented as such a graph in a real life?

let us think of a graph whose nodes correspond to all the cities in the world with their own airports. a pair of two
cities i and j is connected with an edge, if there is more than one direct    ight connection between them. the weight
of the edge is de   ned inversely-proportional to the number of direct connections between them. in this graph, we
do not have any obvious vector representation of each city, but we are only given the distances among them, de   ned
as the edge weights di j. for any non-existing edge, we will assign dmax > 1. now we want to    nd n code vectors
{z1, . . . ,zn} in a low-dimensional space rq, assuming that there are n cities.
example would be to use a euclidean distance such that

the    rst step is to de   ne a distance between each pair of code vectors. let us use f (zi,z j) for this distance. one

f (zi,z j) = (cid:107)zi     z j(cid:107)2.
(cid:32)

( f (zi,z j)    di j)2

z

   

i< j

(cid:33)1/2

given this distance function, we now de   ne a cost function of metric multidimensional scaling (mds) as

  r(z;d) =

,

(3.11)

where z is a id172 constant that determines a speci   c type of mds. we can of course set it to 1.

what does this cost function of mds do? the answer is quite straightforward. the cost function decreases as the
distance between each pair of the code vectors is close to the distance between the corresponding input data points.
the latter distance may be given arbitrarily without having to have a distance function de   ned over the input vector
space.

back to our example case earlier, i am plotting the cities according to how well connected they are to each other
in terms of direct    ight connections in fig. 3.3. this plot was generated by the metric mds using    scikit-learn    using
the data made available from openflights.11 the plot is the result of applying mds to the top-100 cities according to
the number of direct    ight connections from any other cities included in the database.

we notice some clusters of cities according to their continents, which is understandable as cities in the same
continent are likely to have more connections among them. this is however not necessarily true across the continent
boundaries, as hub cities are closely connected to each other. these hub cities are often placed on the boundaries
between two or three continents. for instance, new york and seoul are places between us and asia. similarly,
beijing is placed between europe and asia.

a major strength of mds is that it does not require us to have a precise vector representation of each input data
point. all that is necessary is a dissimilarity matrix which is a symmetric matrix of which each (i, j)-th element
indicates how distant the i-th and j-th input points are. in this sense, one can think of mds as a method of inferring
the underlying vectors given the dissimilarity measures. this strength is at the same time its major weakness. that is,
it is not trivial to    nd a code vector of a new data point which was not available at the beginning.

mds is closely related to principal component analysis (pca) we studied earlier. in pca, an input vector x is
assumed to be given by wz, where w is an orthonormal matrix. in this case, the cost function of mds in eq. (3.11)
is zero automatically, if di j is the euclidean distance between the input vectors xi and x j.
it is your homework
assignment to show that this is indeed a case.

11 http://openflights.org/data.html

50

figure 3.3: the scatter plot of cities according to how close they are in terms of the number of direct    ight connections
among them. the more direct    ights there are the closer the cities are. notice two major clusters of the cities. on the
left, we see a cluster of the cities in the united states, while the cities in europe are clustered on the right. however,
because the distances are de   ned in terms of the airline connections, we see that their arrangements do not necessarily
correspond to their geographical locations. for instance,    new york   ,    toronto    and    los angeles    are located
close to the cities in asia, likely due to many intercontinental connections to those cities. see the jupyter notebook
   mds.ipynb    for details.

3.5 further topics(cid:63)

yet another way to derive principal component analysis, or factor analysis in general, is to build a probabilistic latent
variable model. this approach is called probabilistic principal component analysis [19, 23], and is particularly inter-
esting, because it naturally allows us to perform principal component analysis even when there are missing values in
the input matrix [12, 21]. similarly to bayesian id75, we can consider both the weight and code matrices
as random variables and marginalize both of them. this results in gaussian process latent variable model and allows
us to perform nonlinear id105 [13].

student-t stochastic neighbourhood embeddings (tsne)

!!!

51

chapter 4

id91

id116 id91

4.1
id91 vs. id84 from the 10,000 feet above the ground, the problem of dimensionality
reduction was simply to    nd a set of code vectors {z1, . . . ,zn} given a set of input vectors (or a dissimilarity matrix, in
the case of multidimensional scaling,) while preserving the similarities of input vectors as well as possible in the code
vector space. as we went down closer to the ground, we notice that there are many different types of dimensionality
reduction algorithms, including principal component analysis (pca, sec. 3.2.1) and non-negative id105
(nmf, sec. 3.2.3). these algorithms were generally distinguished from each other by different types of constraints
that have been put either directly on the code vector or on the encoding or decoding functions.

let us now consider an extreme constraint on a code vector that the code vector can only be an one-hot vector. an

one-hot vector, as we learned earlier in eq. (4.1), is a vector whose elements but one are all zeros. that is,

                                         {0,1}q

                                    

0,
...,
0,
1,
0,
...,
0

z =

.

(4.1)

as we discussed earlier, an one-hot vector is equivalent to an integer index between 1 and q, and this allows us to
use this code vector as an indicator of which of the q possible bins the corresponding input vector x belongs to. in
other words, by constraining the code vector to be one-hot, we effectively transformed the problem of dimensionality
reduction into id91, where a code vector denotes which of q clusters the corresponding input vector belongs to.

id116 id91 let us start using k instead of q to denote the dimensionality of a code vector in order to be
more consistent of a traditional description of id91. that is, our goal is to cluster a given set of data points
{x1, . . . ,xn} into k groups, which is equivalent to obtaining a set of code vectors {z1, . . . ,zn} with the constraint that
each and every code vector is an one-hot vector.
as usual with the id105 algorithms we have learned earlier, our goal is to minimize the reconstruction

error:

because we constrain each code vector to be an one-hot vector, the reconstruction error can be rewritten as

  r(z,w;x) = (cid:107)x    wz(cid:107)2
f .

  r(z,w;x) =

k

   
k(cid:48)=1

   
(cid:124)
(cid:125)
j:z j,k(cid:48) =1(cid:107)wk(cid:48)     x j(cid:107)2

(cid:123)(cid:122)

within-cluster error

2

,

52

(a) cluster assignment

(b) centroid estimation

(c) convergence

figure 4.1:
(a) the cluster assignment given cluster centroids. (b) the estimation of the cluster centroids given the
cluster assignment. (c) the    nal result after iterating between the centroid estimation and the cluster assignment until
convergence.

where z j,k(cid:48) is the k(cid:48)-th element of the j-th code vector z j. in other words, the reconstruction error can be computed
separately for each k(cid:48)-th cluster, given a    xed set of code vectors z.

let us assume that we indeed have a    xed set of code vectors z(0), i.e.,

. this implies that we have
already divided the input vector set into k clusters, and we can minimize the reconstruction error by minimizing the
within-cluster reconstruction error. for each k(cid:48)-th cluster, we need to minimize
(cid:107)wk(cid:48)     x j(cid:107)2

z(0)
1 , . . . ,z(0)
n

2

(cid:110)

(cid:111)

  rk(cid:48) (z(0),w;x) =    
j:z(0)
j,k(cid:48) =1

with respect to the associated weight vector wk(cid:48). this within-cluster reconstruction error has a unique solution which
is

wk(cid:48)    

(cid:12)(cid:12)(cid:12){ j : z(0)

1
j,k(cid:48) = 1}

(cid:12)(cid:12)(cid:12)    

j:z(0)

j,k(cid:48) =1

x j.

(4.2)

in other words, the optimal associated weight vector wk(cid:48) is the arithmetic mean of all the input vectors in, or a centroid
of, the corresponding k(cid:48)-th cluster. of course, these weight vectors, or centroids, are only optimal with respect to the
current set of code vectors z(0). let us use the superscript (0) to denote that these weight vectors w(0) are optimal
with respect to z(0).

given these centroid w(0), we now adjust the code vectors in order to minimize the reconstruction error. unlike

before when we looked at each cluster separately, we look at each input vector x separately this time:

minimizing this per-input reconstruction error is equivalent to    rst computing the distance between the input vector
and each of the cluster centroids, and then selecting the cluster with the minimal distance. that is,

(4.3)

  rn(z,w(0);x) = (cid:107)xn     w(0)zn(cid:107)2
2.

zn     argmin

k(cid:48)

  rn = argmin

k(cid:48)

(cid:8)
(cid:107)xn     wk(cid:48)(cid:107)2

2

(cid:9) .

in other words, the optimal code vector zn for the input vector xn corresponds to the assignment of the input vector to
the cluster whose centroid is nearest. this is true for every input vector, and the optimal code vector set, or the cluster
assignment, z(1) is the set of cluster assignment of each input vector, given the cluster centroids w(0).

of course, now that we have found new cluster assignment z(1), the current cluster centroids w(0) are not anymore
optimal. we hence go back to the earlier step and recompute the optimal cluster centroids w(1). then, the current
cluster assignment z(1) becomes sub-optimal, and we must recompute them by assigning each input vector to the
nearest cluster z(2). this iterative process continues until both the cluster centroids and the cluster assignment stop
changing. fortunately, this algorithm is guaranteed to terminate, although there is no guarantee that the converged
solution is globally optimal.1

this algorithm is known as id116 id91, and is widely used for id91 high-dimensional input vectors.
1 a solution is globally optimal, when there exists no pair z and w that results in the reconstruction error smaller.

53

(a) solution 1

(b) solution 2

figure 4.2: depending on the initialization of the cluster centroids, id116 id91 may end up with a different
solution.

4.2 mixture of gaussians and id48
4.2.1 mixture of gaussians
probabilistic treatment of id116 id91 let   s recall some of the concepts from sec. 2.3. there was a
prior distribution which was a distribution over a random variable that we did not observe. there was a likelihood
distribution which was a distribution over a random variable that we did observe given a random variable that we did
not observe. from these two, we were able to de   ne a posterior distribution over a random variable that we did not
observe given a random variable that we did observe. let us apply these concepts to id116 id91.

in k-mean id91, we observe data points which are {x1,x2, . . . ,xn} but do not observe to which cluster each
of these input vectors belongs {z1, . . . ,zn}. in other words, we have an observed random variable x that corresponds
to the input vector and an unobserved/latent random variable z that corresponds to the assignment of the input vector.
first, we de   ne a prior distribution over the assignment by noticing that assignment z is a categorical random
variable (recall from sec. 2.2,) as it can take one of q values. this prior distribution is characterized by q probabilities
that sum to 1:

p(z = k) =   k     0,

(4.4)

where    q

k=1   k = 1.

distribution to be multi-variate gaussian:

second, we de   ne a likelihood distribution over the observed data point x given its assignment z. we assume this

for simplicity, we assume a diagonal covariance, i.e.,

p(x|z = k) = n (x|  k,  k).

                        

  k =

0
0

      
      
0
      
...
      
          2

k,d

   2
k,1
0
...
...
0

0
   2
k,2
0
...
0

54

(4.5)

                         .

with these two distributions, we can now write the joint id203 as

(cid:32)

(cid:33)
(xi       k,i)2
(cid:32)

2   2
k,i

p(x,z = k) =  k

d

   

i=1

1

   2    k,i

exp

   

=

q

   
k(cid:48)=1

1(k(cid:48) = k)  k(cid:48)

d

   

i=1

1

   2    k(cid:48),i

exp

   

(cid:33)

,

(xi       k(cid:48),i)2

2   2
k(cid:48),i

(4.6)

where 1 is an indicator function.

let us assume two conditions: (1)   k     {0,1}, and (2)    2
(xi       k,i)2

d

log p(x,z = k) =   

2

   

i=1
q

k,d = 1. these two assumptions lead to
    log   2  
(xi       k(cid:48),i)2
   

    qlog   2  

2

d

where c is a constant, z is a one-hot vector indicating k, and

=   

=   

i=1

1(k(cid:48) = k)

   
k(cid:48)=1
1
2(cid:107)x    wz(cid:107)2 +c,
(cid:104)

w =

  1;   2; . . . ,   q

(cid:105)

.

if we consider a set of datapoints {x1, . . . ,xn} rather than a single point x, this is generalized to
1
2(cid:107)x    wz(cid:107)2

   
n=1(cid:107)xn     wzn(cid:107)2 +c =    

log p(xn,zn) =    

log p(dtra) =

   

1
2

n=1

n

n

f +c(cid:48),

which is precisely the id168 we used to derive k-mean id91. this implies that id116 id91 is a
special case of the mixture of gaussians and equivalently that the mixture of gaussian generalizes id116 id91.

expectation-maximization there are two ways to train the mixture of gaussians, which are id119 and
expectation-maximization. here, we focus on the latter, as you should already know how to use id119.2

the goal of training in the mixture of gaussians is to maximize the marginal id203 of all the data points with

respect to the parameters,   k   s and   k   s. let us write this marginal log-id203 for a single data point x:3

log p(x) =log p(x,z)    log p(z|x)
p(z|x)
q(z)

=log p(x,z)    logq(z)    log p(z|x) + logq(z)
=log

p(x,z)
(cid:20)
q(z)     log
q(z)

log

=   
z
=   
z
=   
z

p(x,z)
q(z)     log
p(x,z)
q(z)

p(z|x)
q(z)
+ kl(q(cid:107)p)

q(z)log
q(z)log p(x,z) + h (q) + kl(q(cid:107)p),

(cid:21)

  n =(cid:2)  n

where h is an id178. q is a categorical distribution over z parameterized with its own parameters,   1, . . . ,  q,
such that    q
k=1   k = 1. in other words, we have just introduced an additional set of parameters for each data point,

(cid:3), in addition to   k   s,   k   s and   k   s.

1 , . . . ,  n
q

2 perhaps not surprisingly, this is left for you as a homework assignment.
3 the marginal log-id203 of the entire set of data points is simply the sum of this per-example marginal log-id203.

55

we maximize the marginal log-id203 of the dataset by maximizing the sum of the marginal log-id203
of each data point. we do so by alternating between estimating the parameters of the q distributions4 and the original
parameters. first, we notice that the kl divergence between q and p is always non-negative and is exactly zero when
q and p (which is p(z|x)) are identical. what this implies is that we can simply set the parameters of q, which is
categorical, to match the posterior distribution p(z|x) which is also categorical. because of this property, we can come
up with an analytical rule that is guaranteed to improve log p(x) by setting

  k = p(z = k|x)     exp

d

   

i=1

(xi       k,i)2

k,i     log   2    k,i

2   2

.

(4.7)

in other words, we compute how likely the data point x and its assignment to each cluster k is under the joint dis-
tribution p(x,z), and the posterior id203 of the data point belonging to the k-th cluster is proportional to this
id203. in practice, this boils down to applying softmax to [log p(x,z = 1), . . . ,log p(z,z = q)].

once q is    xed, i.e.,   k   s have been estimated for all the data points, we now re-estimate all the other parameters to
n=1 log p(xn). as the id178 is constant with respect

improve the marginal log-id203 of the entire dataset, i.e.,    n
to these parameters, this effectively correspond to maximizing

(cid:32)

(cid:32)
log  k    

(cid:33)(cid:33)

l (x) =    
z

q(z)log p(x,z) =

q

   

k=1

  k log p(x,z) =

q

   

k=1

  k

(cid:32)
log  k    

(cid:18) (xi       k,i)2

2   2
k

d

   

i=1

(cid:19)(cid:33)

    log   2    k

let   s estimate   k    rst by computing the derivative the quantity above with respect to   k,i, which leads to

    l (xn)
      k,i

=

n

   

n=1

  n
k

1
   2
k

(xn
i       k,i) = 0

(4.8)

  n
k (xn

i       k,i)) = 0

n

   

n=1
n

   

n=1
n

   

n=1

      

      

  n
k xn

i =   k,i

  n
k

n

n=1

   
  n
k
   n
n(cid:48)=1   n(cid:48)

k

xn
i

         k,i =

n

   

n=1

you can simiarly derive the update rule of    2

k,i, which is left for you as a homework assignment.

estimating   k is less trivial, as we must ensure that    q

construct the following optimization problem:

k=1   k = 1. we use the method of lagrangian multipliers and

min
  

max
  1,...,  q

n

   

n=1

q

   

k=1

  n
k log  k       

(cid:32)
1   

(cid:33)

q

   

k=1

  k

.

it is quite natural to understand how this formulation imposes the constraint. if the constraint is not met, i.e., 1   
   k   k (cid:54)= 0, we can arbitrary lower the objective function without considering how to set   k   s by setting the lagrangian
multiplier    to be either positive or negative in   nity. if the constraint is met, the multiplier has no in   uence on the
original optimization problem. we    rst solve the problem for   k:

   
     k

n

   

n=1

q

   

k=1

  n
k log  k       

1
  

n

   

n=1

  n
k .

(4.9)

q

(cid:32)
   
1   
(cid:32)

k=1

  n
k log

(cid:33)

  k

=

1
  

n

   

n=1

  n
k

1
  k

n

n=1

   
(cid:32)
1   

      

(cid:33)

  n
k        = 0          k =
(cid:33)

q

   

k=1

1
  

n

   

n=1

  n
k

,

this is plugged back into the original optimization problem:

min
  

4 we have n such distributions.

n

   

n=1

q

   

k=1

56

and we solve for the lagrangian multiplier    :

(cid:32)

(cid:33)

   
     

n

   

n=1

q

   

k=1

  n
k log

1
  

n

   

n=1

  n
k

      

(cid:32)
1   

q

   

k=1

(cid:33)

  n
k

=    

1
  

n

   

n=1

n

   

n=1

1
       1 = 0           = n.

q

  n
k

(cid:124)(cid:123)(cid:122)(cid:125)
   

k=1

=1

by plugging this back into eq. (4.9), we    nally obtain the analytical solution for the coef   cients   k   s:

  k =

1
n

n

   

n=1

  n
k .

these two stages (expectation and maximization stages) are then alternated until the parameters (both those of q

and the original ones) converge.

with a small number of modi   cations and assumptions on the parameters, we can arrive at the learning rule of
id116 id91 from the expectation-maximization algorithm for the mixture of gaussians. because of this, the
learning rule of id116 id91 is often referred to as hard expectation-maximization. it is left for you to    gure
out what these modi   cations and assumptions are as a homework assignment.

expectation-maximization: fisher   s identity at this point, you must be pretty confused by this expectation-
maximization algorithm. are we abandoning our faithful id119 algorithm?
let us instead look at the gradient of the marginal log-id203 of a data point:

   log p(x) =

=

=

p(x,z)

   p(x)
p(x)
1
      
p(x)
z
1
p(x)    
   p(x,z)
1
p(x)    
p(x,z)
=   
   log p(x,z)
p(x)
z
=   
p(z|x)   log p(x,z),
z

=

z

z

p(x,z)   log p(x,z)

which is called fisher   s identity. what this implies is that the gradient of the marginal log-id203 is equivalent
to the expected gradient of the joint log-id203 over the posterior distribution. if you think once more about
expectation-maximization, this is precisely what we did. we    rst compute the posterior distribution in eq. (4.7), and
use it to compute the expected gradient in eq. (4.8). in other words, the expectation-maximization algorithm above is
really nothing but id119 on the marginal log-id203.

4.2.2 mixture of gaussians to id48
considering the stark similarity between id116 id91 and the mixture of gaussians, it is a valid question to
ask why we had to go an extra mile to introduce the mixture of gaussians. one answer is that this approach of
probabilistic modelling allows us to naturally extend the existing model to different problems. as an example, we
consider the problem of id91 frames of a video clip in this section.

let   s set up a problem    rst. we are given a video clip x which is a sequence of video frames, x = (x1, . . . ,xn).
each frame is represented as a vector. our goal is to assign each frame to one of q clusters, just like id116 id91,
however while taking into account the context in which each frame appears. that is, we cannot simply run either k-
means id91 or train the mixture of gaussians, because those do not take into account neighbouring frames.

we start from the mixture of gaussians. the prior distribution over an assignment zn of each frame is categorical.
unlike the mixture of gaussians, this categorical distribution is not independent of assignment of other frames, but is

57

dependent on the assignment of the immediately previous frame. that is,
p(zn = k|zn   1 = j) =   k, j     0,

where    q
k=1   k, j = 1 for all j = 1, . . . ,q and this matrix    is called a transition matrix. this dependence on the
immediately previous frame gives arise to the name hidden    markov    model. the likelihood distribution stays same
as the mixture of gaussians. that is,

in addition, we need one more categorical distribution for the initial assignment:

p(xn|zn = k) = n (xn|  k,  k).

p(z0 = k) =   k     0,

where    q

k=1   k = 1.

the joint distribution is then

n

   

n=1

log p(xn|zn) + log p(zn|zn   1)    log p(z0,z1, . . . ,zn|x1, . . . ,xn)

p(x1, . . . ,xn,z0,z1, . . . ,zn) = p(z0)

n

   

n=1

p(zn|zn   1)p(xn|zn).

learning: expectation-maximization fisher   s inequality

let us write the marginal log-id203 over the observation sequence x = (x1, . . . ,xn):

log p(x1, . . . ,xn) =log p(z0) +

log p(xn|zn) + log p(zn|zn   1)    log p(z0,z1, . . . ,zn|x1, . . . ,xn)

n

   

n=1

q(z0)log p(z0) +

   
z0

tbd

maximum-a-posterior decoding tbd

58

chapter 5

sequential decision making

5.1 sequential decision making as a series of classi   cation

the material covered so far in this course has largely assumed a static world, meaning that our machines were expected
to work with a single input vector x regardless of what past nor future input vectors were. this setup covers a large
portion of use cases of machine learning in practice. consider, for instance, image id134, which is being
rolled out as we speak in many commercial social net services. this caption generator would take as input a single
image, and perhaps a bit of associated metadata, and generate an appropriate caption, regardless of which series of
images it has worked on earlier and which series of images it will work on later.

instead, let us now think of a more general setting in which a machine needs to make multiple decisions in order to
arrive at a conclusion. there are many such cases. think of, for example, driving. every moment we make a decision
on how to turn a steering wheel, whether to hit a brake or whether to hit a gas pedal. the effect, or success, of a series
of all these decisions will only be apparent at the end of a trip, based on how quickly, safely and successfully the cat
has arrived at an intended destination. furthermore, your decision affects the surrounding environment. for instance,
how you have decided to turn your steering wheel early on will affect where you end up in the future, and whether you
have hit the brake immediately in   uences the speed at which the car drives in the next moment.

this setting could be understood as running a series of classi   cation and/or regression. our machine maps an input
vector x, that is an observation of the world, to its action y. unlike the previous static-world machines, the machine in
this sequential decision making setting may optionally expose an additional auxiliary data about itself. this auxiliary
data, represented as a vector h will be used as a part of the input in the next time, along the actual action taken by the
machine in the previous step. in a more concise form,

where t is the time index. if we assume for now that ht = 0 for all t, meaning that the machine does not maintain its
own internal state, the equation simpli   es to

[yt;ht ] = m([xt;yt   1;ht   1]),

this equation reminds us of all the supervised learning methods, including classi   ers and regression models, we have
learned throughout this course; the machine tries to output a correct answer given an input vector.

yt = m([xt;yt   1]).

supervised learning considering this, what should be our    rst approach? obviously, it is supervised learning. we
assume that there exists a reference machine m    that solves this kind of sequential decision making problem (nearly)
perfectly. we let the reference machine run over and over, while collecting observations and the corresponding actions
taken by the reference machine. that is, we build a training set:

dtra =(cid:8)(x1

1,y1

1), . . . , (x1

t ,y1

t ), . . . (xn

1 ,yn

1 ), . . . , (xn

t ,yn

t )(cid:9) .

for each pair (x,y)     dtra, we can de   ne a distance function:

   (y,m,x),

59

as we have done for all the supervised learning methods so far. with this distance function, we can de   ne and minimize
an empirical cost function to    t our machine to solve a sequential decision making problem.

this approach based on supervised learning has been found to be surprisingly effective. for instance, bo-

jarski et al. [3] showed that you can train such a supervised classi   er to drive an actual car on a road.

of course, this does not mean that there is no issue with this supervised learning approach. a major issue is that
error made by the classi   er accumulates quadratically with respect to the length of an episode, where an episode is
de   ned as a single, full run of the classi   er. in order to avoid this, a number of imitation learning, or learning to
search, algorithms have been proposed (see, e.g., [18]). unfortunately those algorithms are out of this course   s scope.

id23: policy gradient
in an extreme case, there may not be an expert from which we collect
training example pairs. instead we may only receive weak supervision by a supervisor, or the environment in which
sequential decision making takes place, in the form of a scalar reward at each time step. such a reward will be arbitrary,
but eventually at the end of each episode, the sum of such rewards will be a good indicator of whether the sequence of
decisions made during the episode was good.

we start with a random classi   er m0 which as before takes as input an observation of the surrounding environment
x and outputs a decision   y. unlike supervised learning, we restrict ourselves to a probabilistic classi   er, such as logistic
regression, that outputs a conditional distribution p(y|x) over all possible decisions, or actions, y given an observation
x.
we now collect training examples using this random classi   er m0 by letting it run multiple times, i.e., multiple
t ) from which
is sampled. the classi   er performs the sampled action, and the environment, or a (weak) supervisor,

episodes. at each time step t of each m-th episode, m0 receives an observation xm
one action   ym
t
provides a reward rm

t and computes p(y|xm

t . from each episode, we then collect

((xm

1 ,   ym

1 ,rm

1 ), . . . , (xm

t ,   ym

t ,rm

t ))

which we turn into

((xm

1 ,   ym

1 ,qm

1 ), . . . , (xm

t ,   ym

t ,qm

t )).

qm
t

is the accumulated future reward de   ned as

qm

t =

t

   
t(cid:48)=t

  t(cid:48)   trm
t(cid:48) ,

and is the indicator of whether the decision   ym
and in our case of a    nite-length episode, can trivially set to 1.

t was good or not.        (0,1] is a discount factor,
why is the accumulated future reward q more important than the immediate reward r? because a decision is
not a good one even with a high immediate reward, if it eventually leads to a situation in the future in which only
low rewards could be collected. a decision is only good, if it leads to a situation in the future in which an episode
accumulates as much reward as possible.

t given an observation xm

we run the random classi   er m0 multiple times to collect as many such tuples of input vector, sampled decision

and the corresponding reward. what we get is then a training set:

d0
tra = {(x1,   y1,q1), . . . (xn,   yn,qn)} .

using this training set, we now train our next classi   er m1. how should we do this?

one thing clear from the above dataset is that not every pair of observation and the corresponding decision was
born equal. we want to make sure that the next classi   er puts a higher id203 on an associated decision y given
an observation x if and only if the pair led to a high accumulated future reward, or q. if the associated q was low, we
want to make sure that the next classi   er m1 puts a low id203. we can achieve this behaviour by modifying the
empirical cost function into

  r(m1;d0

tra) =

1
n

n

   

n=1

   (yn,m1,xn),

(5.1)

(cid:123)(cid:122)
(cid:125)
(cid:124)
(qn    vn)

advantage

60

where     is a usual distance function de   ned per classi   er. in the case of (multinomial) id28, this distance
function would be a cross-id178 loss from eq. (1.21), and it would be a simple euclidean distance in the case of
id75.

what is vn? vn is a value of the observation xn, de   ned as

(cid:34)
q(y,xn) =    
y

(cid:35)
p(y|xn,m0)q(y,xn)

,

vn = v (xn) = ey   m0(xn)

where q(y,xn) is the accumulated future reward should y was selected given an observation xn. in words, vn computes
what we believe would be the future accumulated reward given an observation xn should we have let m0 make a
decision, on average. this is contrast to qn which computes how much reward in the future has been accumulated by
selecting yn given xn.

with this in our mind, the advantage term in eq. (5.1) measure whether and how much better or worse the speci   c
choice of yn given xn was, measured as qn, with respect to the expected performance vn. if the choice led to something
better than our expected performance, we encourage our next classi   er to put more id203 mass on the choice,
and otherwise, we encourage it to put less id203 mass.

where does the v , often called a value function, come from? clearly, it is not feasible to compute the value
function exactly unless with a set of strict assumptions. it is thus a usual practice, at least recently, to train a yet
another regression model to estimate the value of an observation x by minimizing

1
n

n

   

n=1

(v (xn)    qn)2.

once we train the next classi   er m1, we can go back to the trajectory collection stage. we run m1 for multiple
episodes to collect training example tuples of which each consists of an observation, sampled action and associated
reward. we then minimize the empirical cost function in eq. (5.1) in order to obtain another classi   er m2. we continue
iterating these steps until we obtain our    nal classi   er m   .

training a machine for sequential decision making with only weak supervision is called id23,
and the speci   c approach discussed here is called (advantaged-based) policy gradient. id23 is
perhaps the most general form of machine learning. unfortunately, the course is now ending, and i will have to refer
anyone who is interested in learning more about id23 to the following materials:

1. neuro-id145 by bertsekas and tsitsiklis [1]

2. introduction to id23 by pineau: http://videolectures.net/deeplearning2016_

pineau_reinforcement_learning/

61

bibliography

[1] d. p. bertsekas and j. n. tsitsiklis. neuro-id145. in decision and control, 1995., proceedings

of the 34th ieee conference on. athena scienti   c, belmont, ma, 1996.

[2] c. m. bishop. pattern recognition and machine learning (information science and statistics). springer-verlag

new york, inc., secaucus, nj, usa, 2006.

[3] m. bojarski, d. del testa, d. dworakowski, b. firner, b. flepp, p. goyal, l. d. jackel, m. monfort, u. muller,

j. zhang, et al. end to end learning for self-driving cars. arxiv preprint arxiv:1604.07316, 2016.

[4] l. breiman. id79s. machine learning, 45(1):5   32, 2001.

[5] j. s. bridle. training stochastic model recognition algorithms as networks can lead to maximum mutual infor-
mation estimation of parameters. in d. touretzky, editor, advances in neural information processing systems 2,
pages 211   217. morgan-kaufmann, 1990.

[6] k. cho. simple sparsi   cation improves sparse denoising autoencoders in denoising highly corrupted images. in

proceedings of the 30th international conference on machine learning (icml-13), pages 432   440, 2013.

[7] c. cortes and v. vapnik. support-vector networks. machine learning, 20(3):273   297, 1995.

[8] i. goodfellow, y. bengio, and a. courville. deep learning. mit press, 2016.

[9] j. goodman and j. weare. ensemble samplers with af   ne invariance. communications in applied mathematics

and computational science, 5(1):65   80, 2010.

[10] g. e. hinton and r. r. salakhutdinov. reducing the dimensionality of data with neural networks. science,

313(5786):504   507, 2006.

[11] a. hyv  arinen, j. karhunen, and e. oja. independent component analysis, volume 46. john wiley & sons, 2004.

[12] a. ilin and t. raiko. practical approaches to principal component analysis in the presence of missing values.

journal of machine learning research, 11(jul):1957   2000, 2010.

[13] n. d. lawrence. gaussian process latent variable models for visualisation of high dimensional data. in advances

in neural information processing systems, pages 329   336, 2004.

[14] y. lecun, y. bengio, and g. hinton. deep learning. nature, 521(7553):436   444, 2015.

[15] d. d. lee and h. s. seung. algorithms for non-negative id105. in advances in neural information

processing systems, pages 556   562, 2001.

[16] k. p. murphy. machine learning: a probabilistic perspective. mit press, 2012.

[17] b. a. olshausen and d. j. field. sparse coding with an overcomplete basis set: a strategy employed by v1?

vision research, 37(23):3311   3325, 1997.

[18] s. ross, g. j. gordon, and d. bagnell. a reduction of imitation learning and id170 to no-regret

online learning. in aistats, volume 1, page 6, 2011.

62

[19] s. roweis. em algorithms for pca and spca. advances in neural information processing systems, pages 626   632,

1998.

[20] r. salakhutdinov and g. hinton. semantic hashing. international journal of approximate reasoning, 50(7):969   

978, 2009.

[21] r. salakhutdinov and a. mnih. probabilistic id105. in neural information processing systems,

volume 21, 2007.

[22] b. sch  olkopf and a. j. smola. learning with kernels: support vector machines, id173, optimization,

and beyond. the mit press, 2002.

[23] m. e. tipping and c. m. bishop. probabilistic principal component analysis. journal of the royal statistical

society: series b (statistical methodology), 61(3):611   622, 1999.

[24] c. k. williams and c. e. rasmussen. gaussian processes for machine learning. the mit press, 2(3):4, 2006.

63

