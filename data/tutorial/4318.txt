id203 and statistics

cookbook

version 0.2.4
14th may, 2017

http://statistics.zone/

copyright c(cid:13) matthias vallentin, 2017

contents

14 exponential family

16

21.5 spectral analysis . . . . . . . . . . . . . 28

1 distribution overview

1.1 discrete distributions
1.2 continuous distributions

. . . . . . . . . .
. . . . . . . .

2 id203 theory

3 random variables
3.1 transformations

. . . . . . . . . . . . .

4 expectation

5 variance

6 inequalities

7 distribution relationships

8 id203 and moment generating

functions

3
3
5

8

8
9

9

9

10

10

11

9 multivariate distributions

9.1 standard bivariate normal
9.2 bivariate normal
9.3 multivariate normal

11
. . . . . . . 11
. . . . . . . . . . . . . 11
. . . . . . . . . . . 11

10 convergence

11
10.1 law of large numbers (lln) . . . . . . 12
10.2 central limit theorem (clt)
. . . . . 12

11 statistical id136

12
11.1 point estimation . . . . . . . . . . . . . 12
11.2 normal-based con   dence interval
. . . 13
11.3 empirical distribution . . . . . . . . . . 13
11.4 statistical functionals . . . . . . . . . . 13

12 parametric id136

13
12.1 method of moments
. . . . . . . . . . . 13
12.2 maximum likelihood . . . . . . . . . . . 14
12.2.1 delta method . . . . . . . . . . . 14
. . . . . . . . . 14
12.3.1 multiparameter delta method . . 15
12.4 parametric bootstrap . . . . . . . . . . 15

12.3 multiparameter models

13 hypothesis testing

15

15 bayesian id136

16
15.1 credible intervals . . . . . . . . . . . . . 16
15.2 function of parameters . . . . . . . . . . 17
15.3 priors
. . . . . . . . . . . . . . . . . . . 17
15.3.1 conjugate priors . . . . . . . . . 17
15.4 bayesian testing . . . . . . . . . . . . . 18

16 sampling methods

18
16.1 inverse transform sampling . . . . . . . 18
16.2 the bootstrap . . . . . . . . . . . . . . 18
16.2.1 bootstrap con   dence intervals . 18
16.3 rejection sampling . . . . . . . . . . . . 19
16.4 importance sampling . . . . . . . . . . . 19

17 decision theory

19
17.1 risk . . . . . . . . . . . . . . . . . . . . 19
17.2 admissibility . . . . . . . . . . . . . . . 20
17.3 bayes rule
. . . . . . . . . . . . . . . . 20
17.4 minimax rules . . . . . . . . . . . . . . 20

18 id75

20
18.1 simple id75 . . . . . . . . 20
18.2 prediction . . . . . . . . . . . . . . . . . 21
18.3 multiple regression . . . . . . . . . . . 21
18.4 model selection . . . . . . . . . . . . . . 22

19 non-parametric function estimation

22
19.1 density estimation . . . . . . . . . . . . 22
19.1.1 histograms . . . . . . . . . . . . 23
19.1.2 kernel density estimator (kde) 23
19.2 non-parametric regression . . . . . . . 23
19.3 smoothing using orthogonal functions
24

20 stochastic processes

24
20.1 markov chains . . . . . . . . . . . . . . 24
20.2 poisson processes . . . . . . . . . . . . . 25

21 time series

25
21.1 stationary time series . . . . . . . . . . 26
21.2 estimation of correlation . . . . . . . . 26
21.3 non-stationary time series . . . . . . . 26
21.3.1 detrending . . . . . . . . . . . . 27
21.4 arima models . . . . . . . . . . . . . . 27
21.4.1 causality and invertibility . . . . 28

22 math

29
22.1 gamma function . . . . . . . . . . . . . 29
22.2 beta function . . . . . . . . . . . . . . . 29
22.3 series
. . . . . . . . . . . . . . . . . . . 29
. . . . . . . . . . . . . . 30
22.4 combinatorics

this cookbook integrates various topics in id203 theory
and statistics, based on literature [1, 6, 3] and in-class material
from courses of the statistics department at the university of
california in berkeley but also in   uenced by others [4, 5]. if you
   nd errors or have suggestions for improvements, please get in
touch at http://statistics.zone/.

1 distribution overview

1.1 discrete distributions

notation1

uniform

unif{a, . . . , b}

bernoulli

bern (p)

               0

1

fx (x)

(cid:98)x(cid:99)   a+1

x < a
a     x     b
b   a
x > b
(1     p)1   x

binomial

bin (n, p)

i1   p(n     x, x + 1)

multinomial

mult (n, p)

hypergeometric

hyp (n, m, n)

(cid:32)

      

(cid:33)

x     np

(cid:112)np(1     p)

negative binomial

nbin (r, p)

ip(r, x + 1)

geometric

geo (p)

1     (1     p)x x     n+

poisson

po (  )

     

e

x(cid:88)

i=0

  i
i!

fx (x)

i(a     x     b)
b     a + 1

px (1     p)1   x

px (1     p)n   x

(cid:32)

(cid:33)

n
x

k(cid:88)

i=1

xi = n

n!

x1! . . . xk!

(cid:1)

k

1        pxk
(cid:0)m
(cid:1)(cid:0)n   m
px1
(cid:0)n
(cid:1)
(cid:33)

n   x

n

x

(cid:32)

x + r     1

r     1

pr(1     p)x

p(1     p)x   1 x     n+

  xe     

x!

e [x]

a + b

2

p

v [x]

(b     a + 1)2     1

12

p(1     p)

mx (s)

eas     e   (b+1)s

s(b     a)
1     p + pes

np

          np1

...
npk
nm
n
1     p

r

p

1
p

  

np(1     p)

          (cid:32) np1(1     p1)    np1p2

. . .
   np2p1
nm(n     n)(n     m)

n 2(n     1)

1     p
r
p2
1     p
p2

  

1we use the notation   (s, x) and   (x) to refer to the gamma functions (see   22.1), and use b(x, y) and ix to refer to the beta functions (see   22.2).

(cid:33)

(1     p + pes)n

(cid:32) k(cid:88)

piesi

(cid:33)n

i=0

(cid:18)

p

1     (1     p)es

(cid:19)r

pes

1     (1     p)es

e  (es   1)

3

4

llllll1nabxpmfuniform (discrete)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.10.2010203040xpmfllln=40, p=0.3n=30, p=0.6n=25, p=0.9binomiallllllllllllllllllllllllllllllllll0.00.20.40.60.80.02.55.07.510.0xpmflllp=0.2p=0.5p=0.8geometriclllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.10.20.305101520xpmfllll=1l=4l=10poissonlllllllll0inin1abxcdfuniform (discrete)llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.000.250.500.751.00010203040xcdfllln=40, p=0.3n=30, p=0.6n=25, p=0.9binomiallllllllllllllllllllllllllllllllll0.20.40.60.81.00.02.55.07.510.0xcdflllp=0.2p=0.5p=0.8geometriclllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.000.250.500.751.0005101520xcdfllll=1l=4l=10poisson1.2 continuous distributions

uniform

normal

log-normal

notation

unif (a, b)

n(cid:0)  ,   2(cid:1)
lnn(cid:0)  ,   2(cid:1)

multivariate normal

mvn (  ,   )

student   s t

student(  )

chi-square

  2
k

f

exponential   

gamma   

f(d1, d2)

exp (  )

gamma (  ,   )

inverse gamma

invgamma (  ,   )

dirichlet

dir (  )

fx (x)

fx (x)

               0

x   a
b   a
1

  (x) =

1
2

+

1
2

erf

ix

1

  (k/2)

i d1x

d1x+d2

(cid:21)

  (t) dt

      

x < a
a < x < b
x > b

(cid:90) x
(cid:20) ln x          
(cid:17)
(cid:16)   

2  2

  

,

2

  
2

(cid:18) k
(cid:18) d1

2

,

x
2

(cid:19)
(cid:19)

,

d1
2

2

i(a < x < b)

b     a

(cid:26)
(cid:27)
    (x       )2
(cid:27)
    (ln x       )2

2  2

exp

(cid:26)

2  2

    1

2 (x     )t      1(x     )

(cid:19)   (  +1)/2

1 +

x2
  

xk/2   1e

   x/2

(2  )

  (x) =

   
1
2  

  

exp

   
1
2    2

2

   

x
   k/2|  |   1/2e

(cid:1)(cid:18)

  (cid:0)   +1
(cid:1)
      (cid:0)   
(cid:114)
xb(cid:0) d1

2k/2  (k/2)

2
1

d2
(d1x)d1 d
2

(d1x+d2)d1+d2

(cid:1)

2 , d1
2
   x/  
e

1     e

   x/  

1
  

  (  ,   x)

  (  )

  (cid:0)  ,   

(cid:1)

x
   (  )

    
   (  )

x     1e

     x

        1e
x

     /x

(cid:17)

    
   (  )

(cid:16)(cid:80)k
(cid:81)k

  

i=1   i

i=1    (  i)

k(cid:89)

i=1

x  i   1

i

beta

weibull

pareto

beta (  ,   )

ix(  ,   )

weibull(  , k)

pareto(xm,   )

   (x/  )k

1     e

1    (cid:16) xm

(cid:17)  

x

x     1 (1     x)     1

   (   +   )
   (  )    (  )

(cid:16) x

(cid:17)k   1

k
  

  

   (x/  )k
e

    

1 +

  2  

1 +

x     xm

  

x  
m
x  +1

x     xm

  xm
       1

   > 1

x2

m  

(       1)2(       2)

    we use the rate parameterization where    = 1

   . some textbooks use    as scale parameter instead [6].

e [x]

a + b

2

  

v [x]

(b     a)2

12

  2

e  +  2/2

(e  2     1)e2  +  2

  

0    > 1

(cid:40)   

       2
   

  

   > 2
1 <        2

mx (s)

esb     esa
s(b     a)

(cid:26)

exp

  s +

  2s2

2

(cid:27)

(cid:26)

exp

  t s +

1
2

(cid:27)

st   s

k

2k

(1     2s)

   k/2 s < 1/2

d2
d2     2

2(d1 + d2     2)
2d2
d1(d2     2)2(d2     4)

  

  
  

  2

  
  2

   > 2

(       1)2(       2)
e [xi] (1     e [xi])

i=1   i

i=1   i + 1

   +   

(   +   )2(   +    + 1)

  2

(cid:80)k
(cid:18)

  

       1

   > 1

  i(cid:80)k
(cid:18)

  

(cid:19)

1
k

    

(cid:19)

2
k

      2

(cid:32)

1
1     s

  

(s <   )

(cid:33)  

1
1     s
2(     s)  /2

  

  (  )

(s <   )

(cid:16)(cid:112)   4  s

(cid:17)

k  

(cid:32)k   1(cid:89)

1 +

   (cid:88)
   (cid:88)

k=1

n=0

   + r

   +    + r

sk
k!

(cid:16)

r=0
sn  n

n!

  

1 +

n
k

(cid:33)
(cid:17)

   > 2

  (   xms)    (     ,   xms) s < 0

5

6

llll1b-aabxpdfuniform (continuous)0.00.51.01.52.0   5.0   2.50.02.55.0xpdfm=0, s2=0.2m=0, s2=1m=0, s2=5m=   2, s2=0.5normal0.000.250.500.751.000123xpdfm=0, s2=3m=2, s2=2m=0, s2=1m=0.5, s2=1m=0.25, s2=1m=0.125, s2=1log   normal0.00.10.20.30.4   5.0   2.50.02.55.0xpdfn=1n=2n=5n=  student's t0.000.250.500.751.0002468xpdfk=1k=2k=3k=4k=5c20123012345xpdfd1=1, d2=1d1=2, d2=1d1=5, d2=2d1=100, d2=1d1=100, d2=100f0.00.51.01.52.0012345xpdfb=0.5b=1b=2.5exponential0.00.10.20.30.40.505101520xpdfa=1, b=0.5a=2, b=0.5a=3, b=0.5a=5, b=1a=9, b=2gamma01234012345xpdfa=1, b=1a=2, b=1a=3, b=1a=3, b=0.5inverse gamma0123450.000.250.500.751.00xpdfa=0.5, b=0.5a=5, b=1a=1, b=3a=2, b=2a=2, b=5beta0.00.51.01.52.00.00.51.01.52.02.5xpdfl=1, k=0.5l=1, k=1l=1, k=1.5l=1, k=5weibull012341.01.52.02.5xpdfxm=1, k=1xm=1, k=2xm=1, k=4pareto7

01abxcdfuniform (continuous)0.000.250.500.751.00   5.0   2.50.02.55.0xcdfm=0, s2=0.2m=0, s2=1m=0, s2=5m=   2, s2=0.5normal0.000.250.500.750123xcdfm=0, s2=3m=2, s2=2m=0, s2=1m=0.5, s2=1m=0.25, s2=1m=0.125, s2=1log   normal0.000.250.500.751.00   5.0   2.50.02.55.0xcdfn=1n=2n=5n=  student's t0.000.250.500.751.0002468xcdfk=1k=2k=3k=4k=5c20.000.250.500.751.00012345xcdfd1=1, d2=1d1=2, d2=1d1=5, d2=2d1=100, d2=1d1=100, d2=100f0.000.250.500.751.00012345xcdfb=0.5b=1b=2.5exponential0.000.250.500.751.0005101520xcdfa=1, b=0.5a=2, b=0.5a=3, b=0.5a=5, b=1a=9, b=2gamma0.000.250.500.751.00012345xcdfa=1, b=1a=2, b=1a=3, b=1a=3, b=0.5inverse gamma0.000.250.500.751.000.000.250.500.751.00xcdfa=0.5, b=0.5a=5, b=1a=1, b=3a=2, b=2a=2, b=5beta0.000.250.500.751.000.00.51.01.52.02.5xcdfl=1, k=0.5l=1, k=1l=1, k=1.5l=1, k=5weibull0.000.250.500.751.001.01.52.02.5xcdfxm=1, k=1xm=1, k=2xm=1, k=4pareto2 id203 theory

de   nitions

    sample space    
    outcome (point or element)           
    event a        
      -algebra a
1.         a

2. a1, a2, . . . ,    a =    (cid:83)   

i=1 ai     a

3. a     a =      a     a
    id203 distribution p

1. p [a]     0    a
2. p [   ] = 1
3. p

(cid:34)    (cid:71)

(cid:35)

   (cid:88)

p [ai]
    id203 space (   ,a, p)

ai

i=1

i=1

=

properties

    p [   ] = 0
    b =         b = (a       a)     b = (a     b)     (  a     b)
    p [  a] = 1     p [a]
    p [b] = p [a     b] + p [  a     b]
    p [   ] = 1

p [   ] = 0

      ((cid:83)
    p [(cid:83)

n an) =(cid:84)
n an] = 1     p [(cid:84)

n   an   ((cid:84)

n   an]

n an) =(cid:83)

n   an

    p [a     b] = p [a] + p [b]     p [a     b]

=    p [a     b]     p [a] + p [b]

    p [a     b] = p [a       b] + p [  a     b] + p [a     b]
    p [a       b] = p [a]     p [a     b]

continuity of probabilities

    a1     a2     . . . =    limn       p [an] = p [a] where a =(cid:83)   
    a1     a2     . . . =    limn       p [an] = p [a] where a =(cid:84)   

i=1 ai
i=1 ai

independence       

a        b        p [a     b] = p [a] p [b]

id155

p [a| b] =

p [a     b]

p [b]

p [b] > 0

law of total id203

p [b] =

n(cid:88)

i=1

p [b|ai] p [ai]

    =

n(cid:71)

i=1

ai

bayes    theorem

p [ai | b] =

p [b | ai] p [ai]

p [b | aj] p [aj]

j=1

    =

ai

inclusion-exclusion principle

(cid:80)n
(cid:12)(cid:12)(cid:12)(cid:12) =
n(cid:88)

r=1

(cid:12)(cid:12)(cid:12)(cid:12) n(cid:91)

i=1

ai

(   1)r   1 (cid:88)

i   i1<      <ir   n

(cid:12)(cid:12)(cid:12)(cid:12) r(cid:92)

j=1

i=1

n(cid:71)
(cid:12)(cid:12)(cid:12)(cid:12)

aij

3 random variables

random variable (rv)

x :         r

id203 mass function (pmf)

fx (x) = p [x = x] = p [{           : x(  ) = x}]

id203 density function (pdf)

p [a     x     b] =

(cid:90) b

a

f (x) dx

demorgan

cumulative distribution function (cdf)

fx : r     [0, 1]

fx (x) = p [x     x]

1. nondecreasing: x1 < x2 =    f (x1)     f (x2)
2. normalized: limx          = 0 and limx       = 1
3. right-continuous: limy   x f (y) = f (x)

(cid:90) b

p [a     y     b| x = x] =

fy |x (y | x)dy

a     b

a

fy |x (y | x) =

f (x, y)
fx (x)

independence

1. p [x     x, y     y] = p [x     x] p [y     y]
2. fx,y (x, y) = fx (x)fy (y)

8

3.1 transformations

transformation function

discrete

fz(z) = p [  (x) = z] = p [{x :   (x) = z}] = p(cid:2)x          1(z)(cid:3) =

z =   (x)

(cid:88)

fx (x)

x        1(z)

continuous

fz(z) = p [  (x)     z] =

f (x) dx with az = {x :   (x)     z}

special case if    strictly monotone

fz(z) = fx (     1(z))

     1(z)

(cid:12)(cid:12)(cid:12)(cid:12) = fx (x)

(cid:12)(cid:12)(cid:12)(cid:12) dx

dz

(cid:12)(cid:12)(cid:12)(cid:12) = fx (x)

1
|j|

the rule of the lazy statistician

az

(cid:90)
(cid:12)(cid:12)(cid:12)(cid:12) d

dz

(cid:90)

(cid:90)

e [ia(x)] =

e [z] =

  (x) dfx (x)

(cid:90)

a

ia(x) dfx (x) =

dfx (x) = p [x     a]

convolution

    z := x + y

fz(z) =

fx,y (x, z     x) dx

x,y    0

=

fx,y (x, z     x) dx

    z := |x     y |
    z :=

x
y

fz(z) = 2

fx,y (x, z + x) dx

fz(z) =

      

|x|fx,y (x, xz) dx

      
=

0

xfx(x)fx (x)fy (xz) dx

(cid:90)    

      

(cid:90)    
(cid:90)    
(cid:90)    

      

0

(cid:90) z

(cid:90)

xyfx,y (x, y) dfx (x) dfy (y)

(cf. jensen inequality)

x,y

    e [xy ] =
    e [  (y )] (cid:54)=   (e [x])
    p [x     y ] = 1 =    e [x]     e [y ]
    p [x = y ] = 1 =    e [x] = e [y ]
    e [x] =

p [x     x]

   (cid:88)

x=1

sample mean

x discrete

n(cid:88)

i=1

1
n

xi

  xn =

conditional expectation

(cid:90)
(cid:90)    

yf (y | x) dy

    e [y | x = x] =
    e [x] = e [e [x | y ]]
    e  (x,y ) | x=x [=]
      
    e [  (y, z)| x = x] =
    e [y + z | x] = e [y | x] + e [z | x]
    e [  (x)y | x] =   (x)e [y | x]
    e [y | x] = c =    cov [x, y ] = 0

(cid:90)    

      

  (x, y)fy |x (y | x) dx

  (y, z)f(y,z)|x (y, z | x) dy dz

5 variance
de   nition and properties

    v [x] =   2
    v

xi

(cid:34) n(cid:88)
(cid:34) n(cid:88)

i=1

    v

xi

x = e(cid:2)(x     e [x])2(cid:3) = e(cid:2)x 2(cid:3)     e [x]2
(cid:35)
n(cid:88)
(cid:35)
n(cid:88)

i(cid:54)=j
if xi        xj

v [xi] +

cov [xi, xj]

(cid:88)

v [xi]

i=1

=

=

sd[x] =(cid:112)v [x] =   x

4 expectation

de   nition and properties

(cid:90)

    e [x] =   x =

x dfx (x) =

    p [x = c] = 1 =    e [x] = c
    e [cx] = c e [x]
    e [x + y ] = e [x] + e [y ]

(cid:88)
(cid:90)

x

                                 

xfx (x)

x discrete

i=1

i=1

standard deviation

covariance

xfx (x) dx x continuous

    cov [x, y ] = e [(x     e [x])(y     e [y ])] = e [xy ]     e [x] e [y ]
    cov [x, a] = 0
    cov [x, x] = v [x]
    cov [x, y ] = cov [y, x]

9

    cov [ax, by ] = abcov [x, y ]
    cov [x + a, y + b] = cov [x, y ]
    cov

       n(cid:88)

       =

m(cid:88)

n(cid:88)

m(cid:88)

xi,

yj

cov [xi, yj]

i=1

j=1

i=1

j=1

correlation

independence

   [x, y ] =

(cid:112)v [x] v [y ]

cov [x, y ]

x        y =       [x, y ] = 0        cov [x, y ] = 0        e [xy ] = e [x] e [y ]

sample variance

s2 =

n(cid:88)

1

n     1

(xi       xn)2

conditional variance

    v [y | x] = e(cid:2)(y     e [y | x])2 | x(cid:3) = e(cid:2)y 2 | x(cid:3)     e [y | x]2

i=1

    v [y ] = e [v [y | x]] + v [e [y | x]]

6 inequalities

cauchy-schwarz

markov

chebyshev

chernoff

hoeffding

e [xy ]2     e(cid:2)x 2(cid:3) e(cid:2)y 2(cid:3)
p [  (x)     t]     e [  (x)]

t

p [|x     e [x]|     t]     v [x]
(cid:19)

(cid:18)

t2

e  

(1 +   )1+  

p [x     (1 +   )  ]    

   >    1

x1, . . . , xn independent     p [xi     [ai, bi]] = 1     1     i     n

p(cid:2)|   x     e(cid:2)   x(cid:3)|     t(cid:3)     2 exp

p(cid:2)   x     e(cid:2)   x(cid:3)     t(cid:3)     e   2nt2
(cid:80)n
i=1(bi     ai)2

(cid:26)

(cid:27)

2n2t2

t > 0

t > 0

   

jensen

e [  (x)]       (e [x])    convex

7 distribution relationships

binomial

    xi     bern (p) =    n(cid:88)

xi     bin (n, p)

i=1

    x     bin (n, p) , y     bin (m, p) =    x + y     bin (n + m, p)
    limn       bin (n, p) = po (np)
    limn       bin (n, p) = n (np, np(1     p))

(n large, p small)

(n large, p far from 0 and 1)

      

  i(cid:80)n

j=1   j

xj,

    x     nbin (r, p) . y     bin (s + r, p) =    p [x     s] = p [y     r]

negative binomial

poisson

i=1

i=1

  i

(cid:33)
       n(cid:88)

i=1 geo (p)

    x     nbin (1, p) = geo (p)

    x     nbin (r, p) =(cid:80)r
    xi     nbin (ri, p) =    (cid:80) xi     nbin ((cid:80) ri, p)
(cid:32) n(cid:88)
    xi     po (  i)     xi        xj =    n(cid:88)

xi     po

xj     bin

    xi     po (  i)     xi        xj =    xi

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)
    xi     exp (  )     xi        xj =    n(cid:88)
(cid:17)     n (0, 1)
    x     n(cid:0)  ,   2(cid:1) =    (cid:16) x     
    x     n(cid:0)  ,   2(cid:1)     z = ax + b =    z     n(cid:0)a   + b, a2  2(cid:1)
(cid:1)     xi        xj =    (cid:80)
    xi     n(cid:0)  i,   2
i   i,(cid:80)
(cid:16) b     
(cid:17)       (cid:0) a     
(cid:1)

    memoryless property: p [x > x + y | x > y] = p [x > x]

i xi     n(cid:0)(cid:80)

    p [a < x     b] =   
      (   x) = 1       (x)
    upper quantile of n (0, 1): z   =      1(1       )

xi     gamma (n,   )

  (cid:48)(x) =    x  (x)

j=1

j=1

i=1

  

  

  

i

exponential

normal

(cid:1)

i   2
i

  (cid:48)(cid:48)(x) = (x2     1)  (x)

gamma

    x     gamma (  ,   )        x/       gamma (  , 1)

    gamma (  ,   )    (cid:80)  
    xi     gamma (  i,   )     xi        xj =    (cid:80)

i=1 exp (  )

i xi     gamma ((cid:80)

i   i,   )

10

(cid:90)    

0

      (  )

     =

x     1e     x dx

beta
   

x     1(1     x)     1 =

1

b(  ,   )

    e(cid:2)x k(cid:3) =

b(   + k,   )

=
    beta (1, 1)     unif (0, 1)

b(  ,   )

  (   +   )
  (  )  (  )
   + k     1

x     1(1     x)     1

e(cid:2)x k   1(cid:3)

   +    + k     1

8 id203 and moment generating functions

    gx (t) = e(cid:2)tx(cid:3)
    mx (t) = gx (et) = e(cid:2)ext(cid:3) = e

|t| < 1

(cid:34)    (cid:88)

i=0

(cid:35)

e(cid:2)x i(cid:3)

i!

   (cid:88)

i=0

=

   ti

(xt)i

i!

    p [x = 0] = gx (0)
    p [x = 1] = g(cid:48)
x (0)
g(i)
    p [x = i] =
x (0)
i!
    e [x] = g(cid:48)
x (1   )
x (0)

    e(cid:2)x k(cid:3) = m (k)
(cid:20) x!
(cid:21)

x (1   )

    e
(x     k)!
    v [x] = g(cid:48)(cid:48)
    gx (t) = gy (t) =    x d= y

= g(k)
x (1   ) + g(cid:48)

x (1   )     (g(cid:48)

x (1   ))2

9.2 bivariate normal

let x     n(cid:0)  x,   2

x

(cid:1).
(cid:1) and y     n(cid:0)  y,   2
(cid:112)1       2
(cid:34)(cid:18) x       x
(cid:19)2     2  
(cid:18) y       y

(cid:19)2

f (x, y) =

2    x  y

+

1

y

exp

  x

  y

z =

   

(cid:26)
(cid:18) x       x

z

(cid:27)
(cid:19)(cid:18) y       y

2(1       2)

  x

  y

(cid:19)(cid:35)

conditional mean and variance

e [x | y ] = e [x] +   

v [x | y ] =   x

  x
  y

(cid:112)

1       2

(y     e [y ])

9.3 multivariate normal
covariance matrix    (precision matrix      1)

          v [x1]

...

cov [xk, x1]

         

       cov [x1, xk]
. . .
      

v [xk]

...

   =

if x     n (  ,   ),

fx (x) = (2  )   n/2 |  |   1/2 exp

(cid:26)

    1
2

(cid:27)

(x       )t      1(x       )

9 multivariate distributions

joint density

9.1 standard bivariate normal

let x, y     n (0, 1)     x        z where y =   x +(cid:112)1       2z
    x2 + y2     2  xy
(x | y = y)     n(cid:0)  y, 1       2(cid:1)

2  (cid:112)1       2
(y | x = x)     n(cid:0)  x, 1       2(cid:1)

2(1       2)

conditionals

f (x, y) =

(cid:26)

(cid:27)

and

exp

1

independence

x        y           = 0

properties

    z     n (0, 1)     x =    +   1/2z =    x     n (  ,   )
    x     n (  ,   ) =         1/2(x       )     n (0, 1)

    x     n (  ,   ) =    ax     n(cid:0)a  , a  at(cid:1)
    x     n (  ,   )     (cid:107)a(cid:107) = k =    at x     n(cid:0)at   , at   a(cid:1)

10 convergence
let {x1, x2, . . .} be a sequence of rv   s and let x be another rv. let fn denote
the cdf of xn and let f denote the cdf of x.
types of convergence

1. in distribution (weakly, in law): xn

n       fn(t) = f (t)
lim

d    x
   t where f continuous

11

2. in id203: xn

p    x
(      > 0)

3. almost surely (strongly): xn

p(cid:104)

4. in quadratic mean (l2): xn

lim
n       xn = x

lim
n       xn(  ) = x(  )

p [|xn     x| >   ] = 0

lim
n      
as    x

(cid:105)

           :

= p(cid:104)
e(cid:2)(xn     x)2(cid:3) = 0

qm    x

lim
n      

(cid:105)

relationships

p    x

d    x

p    x =    xn
p    x

    xn
    xn
    xn
    xn
    xn
    xn
    xn
    xn
    xn
    x1, . . . , xn iid     e [x] =        v [x] <              xn

qm    x =    xn
as    x =    xn
d    x     (   c     r) p [x = c] = 1 =    xn
p    y =    xn + yn
p    x     yn
p    x + y
qm    x     yn
qm    y =    xn + yn
qm    x + y
p    y =    xnyn
p    x     yn
p    x =      (xn) p      (x)
d    x =      (xn) d      (x)
qm    b        limn       e [xn] = b     limn       v [xn] = 0
qm      

p    xy

slutzky   s theorem
    xn
d    x and yn
d    x and yn
    xn
    in general: xn

p    c =    xn + yn
p    c =    xnyn
d    x and yn

d    cx

d    x + c

d    y (cid:54)=    xn + yn

d    x + y

  xn       

(cid:113)v(cid:2)   xn

(cid:3) =

   

n(   xn       )

  

zn :=

d    z

where z     n (0, 1)

p [zn     z] =   (z)

z     r

lim
n      

= 1

clt notations

continuity correction

delta method

yn     n

(cid:18)

(cid:18)
(cid:18)

zn     n (0, 1)
  2
  xn     n
n
  2
n

  xn            n

n(   xn       )     n(cid:0)0,   2(cid:1)

  ,

0,

(cid:19)
(cid:19)

   
   
n(   xn       )

    n (0, 1)

  

(cid:19)

p(cid:2)   xn     x(cid:3)       
p(cid:2)   xn     x(cid:3)     1       
(cid:19)

(cid:18) x + 1
2       
   
(cid:18) x     1
  /
2       
   
  /
(cid:18)

n

n

=      (yn)     n

  ,

  2
n

(cid:19)

  (  ), (  (cid:48)(  ))2   2
n

10.1 law of large numbers (lln)
let {x1, . . . , xn} be a sequence of iid rv   s, e [x1] =   .
weak (wlln)

p      

  xn

n        

strong (slln)

as      

  xn

n        

10.2 central limit theorem (clt)
let {x1, . . . , xn} be a sequence of iid rv   s, e [x1] =   , and v [x1] =   2.

11 statistical id136
let x1,       , xn

iid    f if not otherwise noted.

11.1 point estimation

    point estimator (cid:98)  n of    is a rv: (cid:98)  n = g(x1, . . . , xn)
    bias((cid:98)  n) = e(cid:104)(cid:98)  n
(cid:105)       
    consistency: (cid:98)  n
    sampling distribution: f ((cid:98)  n)
(cid:114)
v(cid:104)(cid:98)  n
    standard error: se((cid:98)  n) =

p      

(cid:105)

(cid:19)

12

    asymptotic normality:

((cid:98)  n       )2(cid:105)

(cid:105)
    mean squared error: mse = e(cid:104)
= bias((cid:98)  n)2 + v(cid:104)(cid:98)  n
    limn       bias((cid:98)  n) = 0     limn       se((cid:98)  n) = 0 =    (cid:98)  n is consistent
(cid:98)  n       
    slutzky   s theorem often lets us replace se((cid:98)  n) by some (weakly) consis-
tent estimator(cid:98)  n.
suppose (cid:98)  n     n(cid:16)
  ,(cid:98)se2(cid:17)
(cid:3) =   /2
and p(cid:2)   z  /2 < z < z  /2

. let z  /2 =      1(1     (  /2)), i.e., p(cid:2)z > z  /2
(cid:3) = 1        where z     n (0, 1). then

11.2 normal-based con   dence interval

d    n (0, 1)

se

cn =(cid:98)  n    z  /2(cid:98)se

11.3 empirical distribution

empirical distribution function (ecdf)

(cid:98)fn(x) =

i(xi     x) =

(cid:80)n
i=1 i(xi     x)
(cid:40)

n
1 xi     x
0 xi > x

properties (for any    xed x)

(cid:105)
    e(cid:104)(cid:98)fn
(cid:105)
    v(cid:104)(cid:98)fn
    (cid:98)fn

    mse =

= f (x)

f (x)(1     f (x))

=
f (x)(1     f (x))

n

d    0

n

p    f (x)

nonparametric 1        con   dence band for f

= 2e   2n  2

(cid:20)

p

sup

x

(cid:21)
(cid:12)(cid:12)(cid:12)f (x)     (cid:98)fn(x)
(cid:12)(cid:12)(cid:12) >   
l(x) = max{(cid:98)fn      n, 0}
u (x) = min{(cid:98)fn +  n, 1}
(cid:19)
(cid:18) 2

(cid:115)

1
2n

log

  

  =

p [l(x)     f (x)     u (x)    x]     1       

11.4 statistical functionals

    statistical functional: t (f )

    plug-in estimator for linear functional:

    plug-in estimator of    = (f ): (cid:98)  n = t ((cid:98)fn)
    linear functional: t (f ) =(cid:82)   (x) dfx (x)
n(cid:88)
  (x) d(cid:98)fn(x) =
=    t ((cid:98)fn)    z  /2(cid:98)se

(cid:90)
t ((cid:98)fn) =
t (f ),(cid:98)se2(cid:17)

1
n

i=1

    pth quantile: f    1(p) = inf{x : f (x)     p}

    often: t ((cid:98)fn)     n(cid:16)
    (cid:98)   =   xn
n(cid:88)
    (cid:98)  2 =
(cid:80)n
i=1(xi    (cid:98)  )3
    (cid:98)   =
(cid:98)  3
(cid:80)n
(cid:113)(cid:80)n
    (cid:98)   =
i=1(xi       xn)(yi       yn)
i=1(xi       xn)2

(cid:113)(cid:80)n

(xi       xn)2

n     1

i=1

1
n

i=1(yi       yn)2

1

  (xi)

12 parametric id136

let f =(cid:8)f (x;   ) :          (cid:9) be a parametric model with parameter space        rk

and parameter    = (  1, . . . ,   k).

12.1 method of moments

jth moment

jth sample moment

method of moments estimator (mom)

1
n

(cid:90)
  j(  ) = e(cid:2)x j(cid:3) =
n(cid:88)
(cid:98)  j =
  1(  ) =(cid:98)  1
  2(  ) =(cid:98)  2
  k(  ) =(cid:98)  k

... =

i=1

...

x j
i

13

dvoretzky-kiefer-wolfowitz (dkw) inequality (x1, . . . , xn     f )

xj dfx (x)

properties of the mom estimator

p      

    asymptotic normality:

    (cid:98)  n exists with id203 tending to 1
    consistency: (cid:98)  n
n((cid:98)         ) d    n (0,   )
where    = ge(cid:2)y y t(cid:3) gt , y = (x, x 2, . . . , x k)t ,
           1

g = (g1, . . . , gk) and gj =    

j (  )

   

12.2 maximum likelihood
likelihood: ln :        [0,   )

ln(  ) =

n(cid:89)

i=1

f (xi;   )

log-likelihood

(cid:96)n(  ) = log ln(  ) =

n(cid:88)

i=1

log f (xi;   )

maximum likelihood estimator (id113)

ln((cid:98)  n) = sup

  

ln(  )

score function

fisher information

s(x;   ) =

   
     

log f (x;   )

i(  ) = v   [s(x;   )]

fisher information (exponential family)

in(  ) = ni(  )

(cid:20)

(cid:21)

i(  ) = e  

       
     

s(x;   )

observed fisher information

n (  ) =        2
i obs
     2

n(cid:88)

i=1

log f (xi;   )

properties of the id113

    consistency: (cid:98)  n

p      

    asymptotic optimality (or e   ciency), i.e., smallest variance for large sam-

    equivariance: (cid:98)  n is the id113 =      ((cid:98)  n) is the id113 of   (  )
ples. if (cid:101)  n is any other estimator, the asymptotic relative e   ciency is:
1. se    (cid:112)1/in(  )
2. (cid:98)se    (cid:113)
1/in((cid:98)  n)

d    n (0, 1)

se

((cid:98)  n       )
((cid:98)  n       )(cid:98)se
are((cid:101)  n,(cid:98)  n) =

d    n (0, 1)

(cid:105)
(cid:105)     1

v(cid:104)(cid:98)  n
v(cid:104)(cid:101)  n

    asymptotic optimality

    approximately the bayes estimator

d    n (0, 1)

12.2.1 delta method

if    =   ((cid:98)  ) where    is di   erentiable and   (cid:48)(  ) (cid:54)= 0:
((cid:98)  n        )
(cid:98)se((cid:98)   )
where(cid:98)   =   ((cid:98)  ) is the id113 of    and(cid:98)se =
(cid:12)(cid:12)(cid:12)  (cid:48)((cid:98)  )
(cid:12)(cid:12)(cid:12)(cid:98)se((cid:98)  n)
let    = (  1, . . . ,   k) and (cid:98)   = ((cid:98)  1, . . . ,(cid:98)  k) be the id113.

12.3 multiparameter models

hjj =

fisher information matrix

in(  ) =    

   2(cid:96)n
     2

hjk =

   2(cid:96)n
     j     k

         

       e   [h1k]
. . .
       e   [hkk]

...

...

         e   [h11]
((cid:98)         )     n (0, jn)

e   [hk1]

under appropriate regularity conditions

14

with jn(  ) = i   1

n . further, if (cid:98)  j is the jth component of   , then

((cid:98)  j       j)
(cid:98)sej
(cid:104)(cid:98)  j,(cid:98)  k
(cid:105)

d    n (0, 1)

= jn(j, k)

where (cid:98)se2

j = jn(j, j) and cov

12.3.1 multiparameter delta method

      =

                  

     
     1
...
     
     k

let    =   (  1, . . . ,   k) and let the gradient of    be

                  
suppose      (cid:12)(cid:12)  =(cid:98)   (cid:54)= 0 and(cid:98)   =   ((cid:98)  ). then,
((cid:98)          )
(cid:98)se((cid:98)   )
(cid:114)(cid:16)(cid:98)     
(cid:98)se((cid:98)   ) =
and (cid:98)jn = jn((cid:98)  ) and (cid:98)      =      (cid:12)(cid:12)  =(cid:98)  .
sample from f (x;(cid:98)  n) instead of from (cid:98)fn, where (cid:98)  n could be the id113 or method

12.4 parametric bootstrap

(cid:16)(cid:98)     
(cid:17)

(cid:17)t (cid:98)jn

d    n (0, 1)

where

of moments estimator.

13 hypothesis testing

h0 :          0

versus

h1 :          1

de   nitions

    null hypothesis h0
    alternative hypothesis h1
    simple hypothesis    =   0
    composite hypothesis    >   0 or    <   0
    two-sided test: h0 :    =   0 versus h1 :    (cid:54)=   0
    one-sided test: h0 :          0 versus h1 :    >   0

    critical value c
    test statistic t
    rejection region r = {x : t (x) > c}
    power function   (  ) = p [x     r]
    power of a test: 1     p [type ii error] = 1        = inf
       1
    test size:    = p [type i error] = sup
       0

  (  )

  (  )

retain h0

   

h0 true
h1 true type ii error (  )

type i error (  )

reject h0
   

(power)

p-value

    p-value = sup       0
    p-value = sup       0

p   [t (x)     t (x)] = inf(cid:8)   : t (x)     r  
(cid:124)

p   [t (x (cid:63))     t (x)]

= inf(cid:8)   : t (x)     r  

(cid:123)(cid:122)

(cid:9)

(cid:125)

1   f  (t (x))

since t (x (cid:63))   f  

(cid:9)

p-value
< 0.01
0.01     0.05
0.05     0.1
> 0.1

evidence
very strong evidence against h0
strong evidence against h0
weak evidence against h0
little or no evidence against h0

wald test

    two-sided test
    reject h0 when |w| > z  /2 where w =

    p(cid:2)|w| > z  /2

(cid:3)       

(cid:98)         0(cid:98)se

    p-value = p  0 [|w| > |w|]     p [|z| > |w|] = 2  (   |w|)

likelihood ratio test

    t (x) =

sup        ln(  )
sup       0 ln(  )
      (x) = 2 log t (x) d      2

ln((cid:98)  n)
ln((cid:98)  n,0)
k(cid:88)
r   q >   (x)(cid:3)
    p-value = p  0 [  (x) >   (x)]     p(cid:2)  2

i       2
z 2

r   q where

i=1

=

k and z1, . . . , zk

iid    n (0, 1)

15

(cid:19)xj

natural form

fx (x|   ) = h(x) exp{      t(x)     a(  )}

= h(x)g(  ) exp{      t(x)}

= h(x)g(  ) exp(cid:8)  t t(x)(cid:9)

multinomial lrt

    id113: (cid:98)pn =

    t (x) =

n

(cid:18) x1
ln((cid:98)pn)
k(cid:88)

ln(p0)

xk
n

(cid:19)
(cid:18) (cid:98)pj
k(cid:89)
(cid:19)
(cid:18) (cid:98)pj

j=1

p0j

, . . . ,

=

pearson chi-square test

k(cid:88)

    t =

(xj     e [xj])2

e [xj]

j=1

    t d      2

    p-value = p(cid:2)  2

k   1

k   1 > t (x)(cid:3)

    faster d    x 2

      (x) = 2
    the approximate size    lrt rejects h0 when   (x)       2

d      2

xj log

k   1

p0j

j=1

k   1,  

where e [xj] = np0j under h0

k   1 than lrt, hence preferable for small n

independence testing

    i rows, j columns, x multinomial sample of size n = i     j

    id113s unconstrained: (cid:98)pij = xij
    id113s under h0: (cid:98)p0ij =(cid:98)pi  (cid:98)p  j = xi  
(cid:16) nxij
    lrt:    = 2(cid:80)i
(cid:80)j
(cid:80)j
    pearsonchisq: t =(cid:80)i
k  , where    = (i     1)(j     1)

    lrt and pearson d      2

xi  x  j
(xij   e[xij ])2

j=1 xij log

e[xij ]

(cid:17)

x  j
n

j=1

i=1

i=1

n

n

14 exponential family

scalar parameter

vector parameter

fx (x|   ) = h(x) exp{  (  )t (x)     a(  )}

= h(x)g(  ) exp{  (  )t (x)}

(cid:40) s(cid:88)

(cid:41)

  i(  )ti(x)     a(  )

fx (x|   ) = h(x) exp

i=1

= h(x) exp{  (  )    t (x)     a(  )}
= h(x)g(  ) exp{  (  )    t (x)}

15 bayesian id136

bayes    theorem

f (   | x) =

f (x|   )f (  )

f (xn)

f (x|   )f (  )

(cid:82) f (x|   )f (  ) d  

=

    ln(  )f (  )

de   nitions

    x n = (x1, . . . , xn)
    xn = (x1, . . . , xn)
    prior density f (  )
    likelihood f (xn |   ): joint density of the data

n(cid:89)

f (xi |   ) = ln(  )

in particular, x n iid =    f (xn |   ) =

    posterior density f (   | xn)

    normalizing constant cn = f (xn) =(cid:82) f (x|   )f (  ) d  
    posterior mean     n =(cid:82)   f (   | xn) d   =
(cid:82)   ln(  )f (  )d  
(cid:82) ln(  )f (  ) d  

    kernel: part of a density that depends on   

i=1

15.1 credible intervals

posterior interval

p [       (a, b)| xn] =

equal-tail credible interval

(cid:90) a

      

(cid:90) b
(cid:90)    

a

b

f (   | xn) d   =

f (   | xn) d   =   /2

f (   | xn) d   = 1       

highest posterior density (hpd) region rn

1. p [       rn] = 1       
2. rn = {   : f (   | xn) > k} for some k

rn is unimodal =    rn is an interval

16

15.2 function of parameters
let    =   (  ) and a = {   :   (  )       }.
posterior cdf for   

h(r | xn) = p [  (  )        | xn] =

(cid:90)

a

f (   | xn) d  

posterior density

bayesian delta method

h(   | xn) = h(cid:48)(   | xn)

   | x n     n(cid:16)

  ((cid:98)  ),(cid:98)se

(cid:12)(cid:12)(cid:12)  (cid:48)((cid:98)  )
(cid:12)(cid:12)(cid:12)(cid:17)

15.3 priors

choice

the research   s a priori knowledge   via prior elicitation

    subjective bayesianism: prior should incorporate as much detail as possible
    objective bayesianism: prior should incorporate as little detail as possible
    robust bayesianism: consider various priors and determine sensitivity of

(non-informative prior)

15.3.1 conjugate priors

continuous likelihood (subscript c denotes constant)

likelihood

conjugate prior

unif (0,   )

pareto(xm, k)

c

exp (  )

n(cid:0)  ,   2
(cid:1)
n(cid:0)  c,   2(cid:1)
n(cid:0)  ,   2(cid:1)

gamma (  ,   )

n(cid:0)  0,   2

0

(cid:1)

scaled inverse chi-
square(  ,   2
0)

normal-
inverse
scaled
gamma(  ,   ,   ,   )

mvn(  ,   c)

mvn(  0,   0)

mvn(  c,   )

inverse-
wishart(  ,   )

pareto(xmc, k) gamma (  ,   )

pareto(xm, kc)

pareto(x0, k0)

posterior hyperparameters

xi

   + n,    +

max(cid:8)x(n), xm
(cid:9) , k + n
n(cid:88)
(cid:80)n
(cid:18) 1
(cid:18)   0
(cid:19)
(cid:19)   1
(cid:18) 1
0 +(cid:80)n

i=1
i=1 xi
  2
c

n
  2
c
    2

  2
0

  2
0

+

+

/

  2
0

   + n,

i=1(xi       )2
   + n

(cid:19)

,

+

n
  2
c

n
2

,

   +
  (  x       )2
2(n +   )

0   0 + n     1   x(cid:1),

     + n  x

   + n

   +

(cid:0)     1
(cid:0)     1

n(cid:88)

,

i=1

   + n,
(xi       x)2 +

1
(cid:1)   1(cid:0)     1
2
(cid:1)   1
0 + n     1
n(cid:88)
0 + n     1
n(cid:88)

i=1

c

c

n +   ,    +

log

   + n,    +
x0, k0     kn where k0 > kn

i=1

xi
xmc

n(cid:88)

(xi       c)(xi       c)t

our id136s to changes in the prior

gamma (  c,   ) gamma (  0,   0)

  0 + n  c,   0 +

xi

types

i=1

    flat: f (  )     constant

    proper: (cid:82)    
    improper: (cid:82)    

       f (  ) d   = 1

       f (  ) d   =    

    jeffrey   s prior (transformation-invariant):

f (  )    (cid:112)i(  )

f (  )    (cid:112)det(i(  ))

    conjugate: f (  ) and f (   | xn) belong to the same parametric family

17

likelihood

conjugate prior posterior hyperparameters

discrete likelihood

bayes factor

xi,    + n     n(cid:88)
n(cid:88)
ni     n(cid:88)
n(cid:88)

xi,    +

xi

i=1

i=1

i=1

n(cid:88)
n(cid:88)

i=1

i=1

n(cid:88)
n(cid:88)

i=1

bern (p)

beta (  ,   )

bin (p)

beta (  ,   )

   +

   +

nbin (p)

beta (  ,   )

   + rn,    +

xi

po (  )

gamma (  ,   )

   +

multinomial(p) dir (  )

   +

geo (p)

beta (  ,   )

   + n,    +

i=1

i=1

xi,    + n

x(i)

n(cid:88)

xi

i=1

15.4 bayesian testing
if h0 :          0:

prior id203 p [h0] =

posterior id203 p [h0 | xn] =

(cid:90)
(cid:90)

  0

  0

f (  ) d  

f (   | xn) d  

let h0. . .hk   1 be k hypotheses. suppose        f (   | hk),
(cid:80)k
f (xn | hk)p [hk]
k=1 f (xn | hk)p [hk]

p [hk | xn] =

,

marginal likelihood

(cid:90)

  

f (xn | hi) =

f (xn |   , hi)f (   | hi) d  

posterior odds (of hi relative to hj)

p [hi | xn]
p [hj | xn]

=

f (xn | hi)
f (xn | hj)

(cid:123)(cid:122)

(cid:125)

(cid:124)

bayes factor bfij

   p [hi]
(cid:124) (cid:123)(cid:122) (cid:125)
p [hj]

prior odds

xi

evidence

log10 bf10 bf10
0     0.5
0.5     1
1     2
> 2

1     1.5 weak
1.5     10 moderate
10     100
> 100

strong
decisive

p    =

p
1   p bf10

1 + p

1   p bf10

where p = p [h1] and p    = p [h1 | xn]

16 sampling methods

16.1 inverse transform sampling

setup

    u     unif (0, 1)
    x     f
    f    1(u) = inf{x | f (x)     u}

algorithm

1. generate u     unif (0, 1)
2. compute x = f    1(u)

16.2 the bootstrap

let tn = g(x1, . . . , xn) be a statistic.
[tn].

1. estimate vf [tn] with v(cid:98)fn
2. approximate v(cid:98)fn

n,1, . . . , t    

n,b, an iid sample from

[tn] using simulation:

(a) repeat the following b times to get t    

the sampling distribution implied by (cid:98)fn
n     (cid:98)fn.
(cid:32)
b(cid:88)

i. sample uniformly x   
ii. compute t    
n = g(x   

1 , . . . , x   
1 , . . . , x   
n).

(b) then

vboot =(cid:98)v(cid:98)fn

=

1
b

n,b     1
t    
b

b=1

(cid:33)2

b(cid:88)

r=1

t    

n,r

16.2.1 bootstrap con   dence intervals

normal-based interval

pivotal interval

tn    z  /2(cid:98)seboot

1. location parameter    = t (f )

18

2. pivot rn =(cid:98)  n       

b=1

1
b

i(r   

n,b =(cid:98)     

3. let h(r) = p [rn     r] be the cdf of rn
4. let r   

n,b    (cid:98)  n. approximate h using bootstrap:
n,b     r)

   = beta sample quantile of (r   

5.      
n,b)
n,1, . . . , r   
6. r   
7. approximate 1        con   dence interval cn =

b(cid:88)
(cid:98)h(r) =
   =    sample quantile of ((cid:98)     
n,1, . . . ,(cid:98)     
(cid:17)
(cid:98)  n     (cid:98)h   1(cid:16)
(cid:98)  n     (cid:98)h   1(cid:16)   
(cid:17)
(cid:16)

(cid:16)
(cid:98)  n     r   
(cid:98)  n     r   
(cid:17)

n,b), i.e., r   

1       
2

1     /2 =

  /2 =

(cid:17)

  b =

  a,   b

  a =

=

=

2

cn =

     
  /2,      

1     /2

percentile interval

   =      
where

      (cid:98)  n
2(cid:98)  n          
2(cid:98)  n          

1     /2

  /2

16.3 rejection sampling

setup

    we can easily sample from g(  )
    we want to sample from h(  ), but it is di   cult
    we know h(  ) up to a proportional constant: h(  ) =
    envelope condition: we can    nd m > 0 such that k(  )     m g(  )      

(cid:82) k(  ) d  

k(  )

algorithm

1. draw   cand     g(  )
2. generate u     unif (0, 1)
3. accept   cand if u     k(  cand)
m g(  cand)

2. generate u     unif (0, 1)
ln((cid:98)  n)
3. accept   cand if u     ln(  cand)

16.4 importance sampling

sample from an importance function g rather than target density h.
algorithm to obtain an approximation to e [q(  )| xn]:

2. wi =

1. sample from the prior   1, . . . ,   n
   i = 1, . . . , b

(cid:80)b
ln(  i)
i=1 ln(  i)

3. e [q(  )| xn]    (cid:80)b

i=1 q(  i)wi

iid    f (  )

17 decision theory

de   nitions

    unknown quantity a   ecting our decision:          

    decision rule: synonymous for an estimator (cid:98)  
context, the action is just an estimate of   , (cid:98)  (x).
discrepancy between    and (cid:98)  , l :       a     [   k,   ).

    action a     a: possible value of the decision rule. in the estimation
    id168 l: consequences of taking action a when true state is    or

id168s

(cid:40)

k1(       a) a        < 0
k2(a       ) a            0

    squared error loss: l(  , a) = (       a)2
    linear loss: l(  , a) =
    absolute error loss: l(  , a) = |       a|
    lp loss: l(  , a) = |       a|p
    zero-one loss: l(  , a) =

(cid:40)

0 a =   
1 a (cid:54)=   

(linear loss with k1 = k2)

4. repeat until b values of   cand have been accepted

17.1 risk

example

    we can easily sample from the prior g(  ) = f (  )
    target is the posterior h(  )     k(  ) = f (xn |   )f (  )

    envelope condition: f (xn |   )     f (xn |(cid:98)  n) = ln((cid:98)  n)     m

    algorithm

1. draw   cand     f (  )

posterior risk

(frequentist) risk

r((cid:98)   | x) =
r(  ,(cid:98)  ) =

(cid:90)
(cid:90)

l(  ,(cid:98)  (x))f (   | x) d   = e  |x
l(  ,(cid:98)  (x))f (x|   ) dx = ex|  

(cid:104)
(cid:104)

(cid:105)
l(  ,(cid:98)  (x))
(cid:105)
l(  ,(cid:98)  (x))

19

bayes risk

(cid:90)(cid:90)

r(f,(cid:98)  ) =

(cid:104)
l(  ,(cid:98)  (x))f (x,   ) dx d   = e  ,x
l(  ,(cid:98)  (x))
(cid:104)ex|  
(cid:104)
(cid:105)
r(  ,(cid:98)  )
(cid:104)e  |x
(cid:105)
(cid:104)
r((cid:98)   | x)

(cid:104)
(cid:105)(cid:105)
l(  ,(cid:98)  (x)
(cid:104)
(cid:105)(cid:105)
l(  ,(cid:98)  (x)

= ex

= e  

r(f,(cid:98)  ) = e  
r(f,(cid:98)  ) = ex

(cid:105)

18 id75

de   nitions

    response variable y
    covariate x (aka predictor variable or feature)

18.1 simple id75

17.2 admissibility

    (cid:98)  (cid:48) dominates (cid:98)   if
    (cid:98)   is inadmissible if there is at least one other estimator (cid:98)  (cid:48) that dominates

      : r(  ,(cid:98)  (cid:48))     r(  ,(cid:98)  )
      : r(  ,(cid:98)  (cid:48)) < r(  ,(cid:98)  )

it. otherwise it is called admissible.

17.3 bayes rule

bayes rule (or bayes estimator)

    r(f,(cid:98)  ) = inf(cid:101)   r(f,(cid:101)  )
    (cid:98)  (x) = inf r((cid:98)   | x)    x =    r(f,(cid:98)  ) =(cid:82) r((cid:98)   | x)f (x) dx

theorems

    squared error loss: posterior mean
    absolute error loss: posterior median
    zero-one loss: posterior mode

17.4 minimax rules

  

r(  , a)

minimax rule

maximum risk

  r(a) = sup
  

  r((cid:98)  ) = sup

r(  ,(cid:98)  )
r(  ,(cid:98)  ) = inf(cid:101)  
r(  ,(cid:101)  )
(cid:98)   = bayes rule        c : r(  ,(cid:98)  ) = c
least favorable prior(cid:98)  f = bayes rule     r(  ,(cid:98)  f )     r(f,(cid:98)  f )      

  r((cid:101)  ) = inf(cid:101)  

sup

sup

  

  

yi =   0 +   1xi +  i

e [ i | xi] = 0, v [ i | xi] =   2

model

fitted line

(cid:98)r(x) = (cid:98)  0 +(cid:98)  1x
(cid:98)yi =(cid:98)r(xi)

   i = yi    (cid:98)yi = yi    (cid:16)(cid:98)  0 +(cid:98)  1xi

(cid:17)

predicted (   tted) values

residuals

residual sums of squares (rss)

least square estimates

   2
i

i=1

n(cid:88)
rss((cid:98)  0,(cid:98)  1) =
(cid:98)  t = ((cid:98)  0,(cid:98)  1)t : min(cid:98)  0,(cid:98)  1
(cid:98)  0 =   yn    (cid:98)  1   xn
(cid:80)n
(cid:98)  1 =
(cid:80)n
i=1(xi       xn)(yi       yn)
(cid:19)
(cid:18)  0
e(cid:104)(cid:98)   | x n(cid:105)
i=1(xi       xn)2
(cid:18)n   1(cid:80)n
v(cid:104)(cid:98)   | x n(cid:105)
(cid:114)(cid:80)n
(cid:98)se((cid:98)  0) = (cid:98)  
(cid:98)se((cid:98)  1) = (cid:98)  
i=1(xi     x n)2 and(cid:98)  2 = 1
p      0 and (cid:98)  1

   x n
i=1 x 2
i
n

  1
  2
ns2
x
   

p      1

i=1 x 2

n   2

   

sx

sx

=

=

n

n

(cid:19)

i    x n
1

(cid:80)n

x = n   1(cid:80)n
    consistency: (cid:98)  0

where s2
further properties:

rss

(cid:80)n
(cid:80)n
i=1 xiyi     n   xy
i     nx 2
i=1 x 2

=

i=1    2

i (unbiased estimate).

20

18.3 multiple regression

         x11

...
xn1

x =

where

likelihood

y = x   +  

             =

       x1k
...
. . .
       xnk

           1
(cid:26)

...
  k

         

...
 n

            =
          1
(cid:27)
n(cid:88)

rss

i=1

= 1     rss
tss

l(  ,   ) = (2    2)   n/2 exp

    1
2  2

rss = (y     x  )t (y     x  ) = (cid:107)y     x  (cid:107)2 =

(yi     xt

i   )2

if the (k    k) matrix x t x is invertible,

v(cid:104)(cid:98)   | x n(cid:105)

(cid:98)   = (x t x)   1x t y
(cid:98)       n(cid:0)  ,   2(x t x)   1(cid:1)

=   2(x t x)   1

estimate regression function

(cid:98)r(x) =

k(cid:88)

j=1

(cid:98)  jxj

unbiased estimate for   2

(cid:98)  2 =

n(cid:88)

i=1

   2
i

1

n     k

    = x(cid:98)       y

    asymptotic normality:

d    n (0, 1)

(cid:98)  0       0
(cid:98)se((cid:98)  0)
(cid:98)  0    z  /2(cid:98)se((cid:98)  0) and

and

(cid:98)  1       1
(cid:98)se((cid:98)  1)
(cid:98)  1    z  /2(cid:98)se((cid:98)  1)

d    n (0, 1)

    approximate 1        con   dence intervals for   0 and   1:

    wald test for h0 :   1 = 0 vs. h1 :   1 (cid:54)= 0: reject h0 if |w| > z  /2 where

w = (cid:98)  1/(cid:98)se((cid:98)  1).
(cid:80)n
i=1((cid:98)yi     y )2
(cid:80)n
i=1(yi     y )2
n(cid:89)

r2 =

f (xi, yi) =

l =

r2

likelihood

i=1

fx (xi)

i=1

n(cid:89)
n(cid:89)
n(cid:89)

i=1

i=1

l1 =

l2 =

i=1    2
i

= 1    

(cid:80)n
(cid:80)n
i=1(yi     y )2
fx (xi)    n(cid:89)
(cid:40)

i=1

(cid:88)

(cid:16)

    1
2  2

i

fy |x (yi | xi) = l1    l2

(cid:17)2(cid:41)

fy |x (yi | xi)          n exp

yi     (  0       1xi)

under the assumption of normality, the least squares estimator is also the id113
but the least squares variance estimator is not the id113.

(cid:98)  2 =

1
n

n(cid:88)

i=1

   2
i

18.2 prediction

observe x = x    of the covariate and want to predict their outcome y   .

prediction interval

(cid:105)

(cid:98)y    = (cid:98)  0 +(cid:98)  1x   
(cid:105)
= v(cid:104)(cid:98)  0
(cid:105)
v(cid:104)(cid:98)y   
+ x2   v(cid:104)(cid:98)  1
(cid:18)(cid:80)n
(cid:98)  2
n(cid:80)
n =(cid:98)  2
i=1(xi     x   )2
i(xi       x)2j
(cid:98)y       z  /2(cid:98)  n

(cid:105)

(cid:104)(cid:98)  0,(cid:98)  1
(cid:19)

+ 2x   cov

id113

+ 1

1        con   dence interval

(cid:98)   =   x

n     k
n

(cid:98)  2 =
(cid:98)  j    z  /2(cid:98)se((cid:98)  j)

  2

21

h0 :   j = 0 vs. h1 :   j (cid:54)= 0    j     j

i=1

18.4 model selection
consider predicting a new observation y     for covariates x    and let s     j
denote a subset of the covariates in the model, where |s| = k and |j| = n.
issues

    under   tting: too few covariates yields high bias
    over   tting: too many covariates yields high variance

procedure

1. assign a score to each model
2. search through all models to    nd the one with the highest score

hypothesis testing

mean squared prediction error (mspe)

prediction risk

training error

r2

i=1

i=1

r(s) =

mspei =

mspe = e(cid:104)
((cid:98)y (s)     y    )2(cid:105)
i )2(cid:105)
e(cid:104)
n(cid:88)
n(cid:88)
((cid:98)yi(s)     y    
n(cid:88)
(cid:98)rtr(s) =
((cid:98)yi(s)     yi)2
(cid:80)n
i=1((cid:98)yi(s)     y )2
tss = 1     (cid:98)rtr(s)
(cid:80)n
tss = 1    
i=1(yi     y )2
e(cid:104)(cid:98)rtr(s)
(cid:105)
(cid:105)     r(s) =    2
bias((cid:98)rtr(s)) = e(cid:104)(cid:98)rtr(s)
r2(s) = 1     n     1
n     k

(cid:104)(cid:98)yi, yi

n(cid:88)

rss
tss

< r(s)

(cid:105)

cov

i=1

i=1

r2(s) = 1     rss(s)

the training error is a downward-biased estimate of the prediction risk.

adjusted r2

mallow   s cp statistic

(cid:98)r(s) = (cid:98)rtr(s) + 2k(cid:98)  2 = lack of    t + complexity penalty

akaike information criterion (aic)

bayesian information criterion (bic)

s)     k

aic(s) = (cid:96)n((cid:98)  s,(cid:98)  2
bic(s) = (cid:96)n((cid:98)  s,(cid:98)  2

s)     k
2

log n

validation and training

(cid:98)rv (s) =

m(cid:88)

((cid:98)y    
i (s)     y    
i )2

m = |{validation data}|, often

n
4

or

n
2

leave-one-out cross-validation

(cid:98)rcv (s) =

(yi    (cid:98)y(i))2 =

(cid:33)2

(cid:32)

n(cid:88)

i=1

yi    (cid:98)yi(s)

1     uii(s)

n(cid:88)

i=1

u (s) = xs(x t

s xs)   1xs (   hat matrix   )

19 non-parametric function estimation

integrated square error (ise)

19.1 density estimation

estimate f (x), where f (x) = p [x     a] =(cid:82)
(cid:17)2
f (x)     (cid:98)fn(x)
(cid:105)

l(f,(cid:98)fn) =
r(f,(cid:98)fn) = e(cid:104)

(cid:90) (cid:16)

frequentist risk

a f (x) dx.

dx = j(h) +

(cid:90)

f 2(x) dx

b2(x) dx +

=

(cid:90)
l(f,(cid:98)fn)
b(x) = e(cid:104)(cid:98)fn(x)
(cid:105)     f (x)
v(x) = v(cid:104)(cid:98)fn(x)
(cid:105)

(cid:90)

v(x) dx

22

19.1.1 histograms

de   nitions

m

    number of bins m
    binwidth h = 1
    bin bj has   j observations

    de   ne (cid:98)pj =   j/n and pj =(cid:82)
m(cid:88)
(cid:98)pj

histogram estimator

i(x     bj)

(cid:98)fn(x) =
(cid:105)
e(cid:104)(cid:98)fn(x)
v(cid:104)(cid:98)fn(x)
(cid:105)
r((cid:98)fn, f )     h2

=

=

h

j=1
pj
h
pj(1     pj)

(cid:90)

nh2

12

1

h    =

n1/3

r   ((cid:98)fn, f )     c

n2/3

(cid:32)

1
nh

(f(cid:48)(u))2 du +

(cid:33)1/3
(cid:82) (f(cid:48)(u))2 du
(cid:18) 3
(cid:19)2/3(cid:18)(cid:90)

6

c =

4

f (u) du

bj

h    =

r   (f,(cid:98)fn) =

kde

(cid:98)fn(x) =
r(f,(cid:98)fn)     1

1
n

n(cid:88)

i=1

1
h

(cid:19)

(cid:18) x     xi
(cid:90)

h

k

(f(cid:48)(cid:48)(x))2 dx +
   1/5
c
3

(h  k)4
4
   2/5
c
1

   1/5
c
2
n1/5

c4
n4/5

c4 =

5
4

(cid:124)

(  2

k)2/5

(cid:90)

1
nh

(cid:18)(cid:90)

(cid:123)(cid:122)

c(k)

c1 =   2

k, c2 =

k 2(x) dx, c3 =

(f(cid:48)(cid:48)(x))2 dx

k 2(x) dx

(cid:90)

(cid:18)(cid:90)

(cid:19)4/5
(cid:125)

k 2(x) dx

(f(cid:48)(cid:48))2 dx

(cid:90)
(cid:19)1/5

epanechnikov kernel

(cid:40)

   
4
0

3

5(1   x2/5)

k(x) =

   

|x| <
otherwise

5

cross-validation estimate of e [j(h)]

(cid:98)jcv (h) =

(cid:90) (cid:98)f 2

n(x) dx     2
n

n(cid:88)

i=1

(cid:98)f(   i)(xi)     1

hn2

(cid:19)1/3

(f(cid:48)(u))2 du

k   (x) = k (2)(x)     2k(x)

k (2)(x) =

n(cid:88)

i=1

n(cid:88)
(cid:90)

j=1

k   (cid:18) xi     xj

h

(cid:19)

+

2
nh

k(0)

k(x     y)k(y) dy

cross-validation estimate of e [j(h)]

(cid:98)jcv (h) =

(cid:90) (cid:98)f 2

n(x) dx     2
n

n(cid:88)

i=1

(cid:98)f(   i)(xi) =

2

(n     1)h

    n + 1
(n     1)h

19.1.2 kernel density estimator (kde)

kernel k

    k(x)     0

    (cid:82) k(x) dx = 1
    (cid:82) xk(x) dx = 0
    (cid:82) x2k(x) dx       2

k > 0

m(cid:88)

j=1

(cid:98)p2

j

19.2 non-parametric regression
estimate f (x) where f (x) = e [y | x = x]. consider pairs of points
(x1, y1), . . . , (xn, yn) related by

yi = r(xi) +  i

e [ i] = 0
v [ i] =   2

k-nearest neighbor estimator

(cid:88)

(cid:98)r(x) =

1
k

i:xi   nk(x)

yi

where nk(x) = {k values of x1, . . . , xn closest to x}

23

nadaraya-watson kernel estimator

i=1

wi(x)yi

(cid:98)r(x) =

n(cid:88)
(cid:1)
k(cid:0) x   xi
(cid:16) x   xj
(cid:17)
(cid:80)n
(cid:18)(cid:90)
r((cid:98)rn, r)     h4
(cid:90)   2(cid:82) k 2(x) dx

wi(x) =

j=1 k

4

h

h

x2k 2(x) dx

+

nhf (x)

dx

    [0, 1]

(cid:19)4(cid:90) (cid:18)

(cid:19)2

dx

f(cid:48)(x)
f (x)

r(cid:48)(cid:48)(x) + 2r(cid:48)(x)

n4/5

h        c1
n1/5

r   ((cid:98)rn, r)     c2
n(cid:88)

(cid:98)jcv (h) =

i=1

cross-validation estimate of e [j(h)]

(yi    (cid:98)r(   i)(xi))2 =

n(cid:88)

i=1

(cid:32)

(yi    (cid:98)r(xi))2
(cid:16) x   xj
(cid:80)n

k(0)

j=1 k

h

1    

19.3 smoothing using orthogonal functions

approximation

r(x) =

multivariate regression

j=1

   (cid:88)

  j  j(x)     j(cid:88)
           0(x1)

y =      +   

j=1

...

  j  j(x)

         

         j (x1)
. . .
         j (xn)

...

where

  i =  i

and    =

least squares estimator

(cid:98)   = (  t   )   1  t y

  0(xn)

cross-validation estimate of e [j(h)]

    1
n

  t y

(for equally spaced observations only)

(cid:98)rcv (j) =

n(cid:88)

      yi     j(cid:88)

  j(xi)(cid:98)  j,(   i)

      2

i=1

j=1

20 stochastic processes

(cid:40){0,  1, . . .} = z discrete

[0,   )

continuous

stochastic process

{xt : t     t}

t =

    notations xt, x(t)
    state space x
    index set t

20.1 markov chains

markov chain

p [xn = x| x0, . . . , xn   1] = p [xn = x| xn   1]

   n     t, x     x

(cid:33)2

(cid:17)

transition probabilities

pij     p [xn+1 = j | xn = i]
pij(n)     p [xm+n = j | xm = i]

n-step

transition matrix p (n-step: pn)

    (i, j) element is pij
    pij > 0

    (cid:80)

i pij = 1

chapman-kolmogorov

pij(m + n) =

(cid:88)

k

pij(m)pkj(n)

pm+n = pmpn

pn = p              p = pn

marginal id203

  n = (  n(1), . . . ,   n(n )) where   i(i) = p [xn = i]
  0 (cid:44) initial distribution
  n =   0pn

24

20.2 poisson processes

poisson process

    {xt : t     [0,   )} = number of events up to and including time t
    x0 = 0
    independent increments:

   t0 <        < tn : xt1     xt0                      xtn     xtn   1

    intensity function   (t)

    p [xt+h     xt = 1] =   (t)h + o(h)
    p [xt+h     xt = 2] = o(h)

    xs+t     xs     po (m(s + t)     m(s)) where m(t) =(cid:82) t

0   (s) ds

homogeneous poisson process

  (t)        =    xt     po (  t)

   > 0

waiting times

interarrival times

wt := time at which xt occurs

(cid:19)

(cid:18)

t,

1
  

wt     gamma

st = wt+1     wt
st     exp

(cid:18) 1

(cid:19)

  

st

wt   1

wt

t

21 time series

mean function

  xt = e [xt] =

(cid:90)    

      

xft(x) dx

autocovariance function

autocorrelation function (acf)

(cid:112)v [xs] v [xt]

cov [xs, xt]

(cid:112)  (s, s)  (t, t)

  (s, t)

=

  (s, t) =

cross-covariance function (ccv)

  xy(s, t) = e [(xs       xs)(yt       yt)]

cross-correlation function (ccf)

  xy(s, t) =

(cid:112)  x(s, s)  y(t, t)

  xy(s, t)

backshift operator

di   erence operator

white noise

bk(xt) = xt   k

   d = (1     b)d

(cid:1)

iid    n(cid:0)0,   2

    wt     wn(0,   2
w)
    gaussian: wt
    e [wt] = 0
    v [wt] =   2
      w(s, t) = 0 s (cid:54)= t     s, t     t

t     t
t     t

w

random walk

    drift   

    xt =   t +(cid:80)t

    e [xt] =   t

j=1 wj

  x(s, t) = e [(xs       s)(xt       t)] = e [xsxt]       s  t

  x(t, t) = e(cid:2)(xt       t)2(cid:3) = v [xt]

mt =

symmetric moving average

k(cid:88)

j=   k

ajxt   j

where aj = a   j     0 and

k(cid:88)

j=   k

aj = 1

25

21.1 stationary time series

strictly stationary

p [xt1     c1, . . . , xtk     ck] = p [xt1+h     c1, . . . , xtk+h     ck]

weakly stationary

    e(cid:2)x2
    e(cid:2)x2

t

(cid:3) <    
(cid:3) = m

t

   t     z
   t     z

      x(s, t) =   x(s + r, t + r)

   k     n, tk, ck, h     z

   r, s, t     z

autocovariance function

      (0) = e(cid:2)(xt       )2(cid:3)

      (h) = e [(xt+h       )(xt       )]
      (0)     0
      (0)     |  (h)|
      (h) =   (   h)

autocorrelation function (acf)

(cid:112)v [xt+h] v [xt]

cov [xt+h, xt]

  x(h) =

jointly stationary time series

   h     z

(cid:112)  (t + h, t + h)  (t, t)

  (t + h, t)

=

=

  (h)
  (0)

linear process

   (cid:88)

j=      

xt =    +

  jwt   j where

|  j| <    

j=      

  xy(h) = e [(xt+h       x)(yt       y)]

  xy(h) =

  xy(h)

(cid:112)  x(0)  y(h)
   (cid:88)

   (cid:88)

j=      

sample variance

(cid:19)

(cid:18)

n(cid:88)

h=   n

1     |h|

n

  x(h)

v [  x] =

1
n

sample autocovariance function

(cid:98)  (h) =

1
n

n   h(cid:88)

(xt+h       x)(xt       x)

t=1

sample autocorrelation function

sample cross-variance function

(cid:98)  xy(h) =

(cid:98)  (h) = (cid:98)  (h)(cid:98)  (0)
n   h(cid:88)

1
n

t=1

(xt+h       x)(yt     y)

sample cross-correlation function

(cid:98)  xy(h) =

(cid:98)  xy(h)
(cid:112)(cid:98)  x(0)(cid:98)  y(0)

properties

      (cid:98)  x(h) =
      (cid:98)  xy(h) =

1   
n
1   
n

if xt is white noise

if xt or yt is white noise

  (h) =   2
w

  j+h  j

classical decomposition model

21.3 non-stationary time series

21.2 estimation of correlation

sample mean

  x =

1
n

xt

xt =   t + st + wt

      t = trend
    st = seasonal component
    wt = random noise term

26

n(cid:88)

t=1

21.3.1 detrending

moving average polynomial

least squares

1. choose trend model, e.g.,   t =   0 +   1t +   2t2

2. minimize rss to obtain trend estimate (cid:98)  t = (cid:98)  0 +(cid:98)  1t +(cid:98)  2t2

3. residuals (cid:44) noise wt

moving average

    the low-pass    lter vt is a symmetric moving average mt with aj = 1

2k+1 :

  (z) = 1 +   1z +        +   qzq

z     c       q (cid:54)= 0

moving average operator

  (b) = 1 +   1b +        +   pbp

ma (q) (moving average model order q)

xt = wt +   1wt   1 +        +   qwt   q        xt =   (b)wt

k(cid:88)

1

vt =

xt   1

(cid:80)k
i=   k wt   j     0, a linear trend function   t =   0 +   1t passes

2k + 1

i=   k

    if

1

2k+1

without distortion

di   erencing

      t =   0 +   1t =       xt =   1

21.4 arima models

autoregressive polynomial

  (z) = 1       1z                  pzp

z     c       p (cid:54)= 0

autoregressive operator

  (b) = 1       1b                  pbp

autoregressive model order p, ar (p)

xt =   1xt   1 +        +   pxt   p + wt          (b)xt = wt

k   1(cid:88)

j=0

  j(wt   j)

k      ,|  |<1

=

   (cid:88)
(cid:124)

j=0

  j(wt   j)

(cid:123)(cid:122)

(cid:125)

linear process

ar (1)

    xt =   k(xt   k) +

    e [xt] =(cid:80)   

j=0   j(e [wt   j]) = 0

      (h) = cov [xt+h, xt] =   2
      (h) =   (h)
      (h) =     (h     1) h = 1, 2, . . .

  (0) =   h

w  h
1     2

  (h) = cov [xt+h, xt] =

j=0   j  j+h

0     h     q
h > q

ma (1)

e [xt] =

  je [wt   j] = 0

j=0

  2
w
0

q(cid:88)
(cid:40)
(cid:80)q   h
               (1 +   2)  2
(cid:40)   

    2
w
0

xt = wt +   wt   1

  (h) =

w h = 0
h = 1
h > 1

  (h) =

(1+  2) h = 1
0
h > 1

arma (p, q)

xt =   1xt   1 +        +   pxt   p + wt +   1wt   1 +        +   qwt   q

partial autocorrelation function (pacf)

  (b)xt =   (b)wt

i

(cid:44) regression of xi on {xh   1, xh   2, . . . , x1}

    xh   1
      hh = corr(xh     xh   1
    e.g.,   11 = corr(x1, x0) =   (1)

, x0     xh   1

) h     2

h

0

arima (p, d, q)

   dxt = (1     b)dxt is arma (p, q)

  (b)(1     b)dxt =   (b)wt

exponentially weighted moving average (ewma)
xt = xt   1 + wt       wt   1

27

   (cid:88)

j=1

xt =

(1       )  j   1xt   j + wt when |  | < 1

  xn+1 = (1       )xn +     xn

seasonal arima

    denoted by arima (p, d, q)    (p, d, q)s
      p (bs)  (b)   d

s    dxt =    +   q(bs)  (b)wt

21.4.1 causality and invertibility

arma (p, q) is causal (future-independent)           {  j} :(cid:80)   

   (cid:88)

   (cid:88)

j=0

   (cid:88)

j=0

xt =

wt   j =   (b)wt

arma (p, q) is invertible           {  j} :(cid:80)   
j=0   j <     such that
   (cid:88)

j=0

  (b)xt =

xt   j = wt

properties

j=0

    arma (p, q) causal        roots of   (z) lie outside the unit circle

    arma (p, q) invertible        roots of   (z) lie outside the unit circle

  (z) =

  jzj =

  (z)
  (z)

|z|     1

  (z) =

  jzj =

  (z)
  (z)

|z|     1

behavior of the acf and pacf for causal and invertible arma models

ar (p)
tails o   

acf
pacf cuts o    after lag p

ma (q)

arma (p, q)

cuts o    after lag q

tails o    q

tails o   
tails o   

21.5 spectral analysis

periodic process

xt = a cos(2    t +   )

= u1 cos(2    t) + u2 sin(2    t)

    frequency index    (cycles per unit time), period 1/  
    amplitude a
    phase   
    u1 = a cos    and u2 = a sin    often normally distributed rv   s

periodic mixture

q(cid:88)

k=1

xt =

(uk1 cos(2    kt) + uk2 sin(2    kt))

j=0   j <     such that

    uk1, uk2, for k = 1, . . . , q, are independent zero-mean rv   s with variances   2

k

      (h) =(cid:80)q
      (0) = e(cid:2)x2

t

(cid:3) =(cid:80)q

k=1   2

k cos(2    kh)

k=1   2
k

spectral representation of a periodic process

=

spectral distribution function

  (h) =   2 cos(2    0h)
e   2  i  0h +

=

  2
2

e2  i  0h

e2  i  h df (  )

f (  ) =

   <      0

  2/2              <   0
  2

         0

   1/2

  2
2

(cid:90) 1/2
               0

    f (      ) = f (   1/2) = 0
    f (   ) = f (1/2) =   (0)

spectral density

   (cid:88)

    needs(cid:80)   
      (0) = v [xt] =(cid:82) 1/2

    f (  )     0
    f (  ) = f (     )
    f (  ) = f (1       )

    white noise: fw(  ) =   2

w

   1/2 f (  ) d  

  (h)e   2  i  h     1
2

           1
2

f (  ) =

h=      

h=       |  (h)| <     =      (h) =(cid:82) 1/2

   1/2 e2  i  hf (  ) d   h = 0,  1, . . .

28

    arma (p, q) ,   (b)xt =   (b)wt:

22.2 beta function

where   (z) = 1    (cid:80)p

fx(  ) =   2
w

k=1   kzk and   (z) = 1 +(cid:80)q

k=1   kzk

|  (e   2  i  )|2
|  (e   2  i  )|2

discrete fourier transform (dft)

d(  j) = n   1/2

n(cid:88)

i=1

xte   2  i  j t

fourier/fundamental frequencies

inverse dft

xt = n   1/2

d(  j)e2  i  j t

  j = j/n

n   1(cid:88)

j=0

i(j/n) = |d(j/n)|2

periodogram

scaled periodogram

4
n

(cid:32)

p (j/n) =

=

22 math

i(j/n)

n(cid:88)

t=1

2
n

(cid:90)    

22.1 gamma function

ts   1e   tdt

(cid:90)    
(cid:90) x

x

0

0

    ordinary:   (s) =
    upper incomplete:   (s, x) =
    lower incomplete:   (s, x) =
      (   + 1) =     (  )
      (n) = (n     1)!
      (0) =   (   1) =    
      (1/2) =
      (   1/2) =    2  (1/2)

n     n

   > 1

   

  

ts   1e   tdt

ts   1e   tdt

(cid:33)2

(cid:32)

n(cid:88)

t=1

2
n

(cid:33)2

xt sin(2  tj/n

xt cos(2  tj/n

+

    ordinary: b(x, y) = b(y, x) =

tx   1(1     t)y   1 dt =

  (x)  (y)
  (x + y)

ta   1(1     t)b   1 dt

(cid:90) 1

0

(cid:90) x
a+b   1(cid:88)

0

j=a

    incomplete: b(x; a, b) =
    regularized incomplete:
a,b   n

b(x; a, b)

b(a, b)

ix(a, b) =
    i0(a, b) = 0
    ix(a, b) = 1     i1   x(b, a)

=

i1(a, b) = 1

(a + b     1)!

j!(a + b     1     j)!

xj(1     x)a+b   1   j

22.3 series

finite

k=1

k=1

    n(cid:88)
    n(cid:88)
    n(cid:88)
    n(cid:88)
    n(cid:88)

k=1

k=1

k=0

k =

n(n + 1)

2

(2k     1) = n2

k2 =

k3 =

ck =

n(n + 1)(2n + 1)

(cid:18) n(n + 1)

6

(cid:19)2

2
cn+1     1
c     1

c (cid:54)= 1

binomial

n

=

(cid:19)

(cid:19)
(cid:18)r + n + 1
(cid:19)
(cid:18) n + 1
(cid:18)m + n
(cid:19)

m + 1

k=0

k=0

    n(cid:88)
    n(cid:88)
    n(cid:88)
r(cid:88)
n(cid:88)

k=0

k

k

= 2n

(cid:19)
(cid:18)n
(cid:18)r + k
(cid:18) k
(cid:19)
(cid:18)m
(cid:19)(cid:18) n
(cid:19)
(cid:18)n

m

=

k

k=0

    vandermonde   s identity:

=
    binomial theorem:

r     k

k=0

k

r

an   kbk = (a + b)n

in   nite

k=0

   (cid:88)
   (cid:88)
   (cid:88)
   (cid:88)

k=0

k=0

,

k=1

pk =

1
1     p

   (cid:88)
(cid:32)    (cid:88)
(cid:19)
(cid:18)r + k     1
(cid:18)  
(cid:19)

kpk   1 =

d
dp

k=0

k

pk = (1 + p)  

k

k=0

   

   

   

   

|p| < 1

pk =

(cid:33)

p
1     p

(cid:18) 1

(cid:19)

=

pk

d
dp
xk = (1     x)   r

1     p
r     n+

|p| < 1 ,        c

=

1

(1     p)2

|p| < 1

(cid:19)

29

22.4 combinatorics

sampling

w/o replacement

k out of n

ordered

unordered

k   1(cid:89)
(cid:19)

i=0

=

nk =

(cid:18)n

k

(n     i) =

n!

(n     k)!
n!

nk
k!

=

k!(n     k)!

[3] r. h. shumway and d. s. sto   er. time series analysis and its applications with r

examples. springer, 2006.

[4] a. steger. diskrete strukturen     band 1: kombinatorik, graphentheorie, algebra.

springer, 2001.

w/ replacement

[5] a. steger. diskrete strukturen     band 2: wahrscheinlichkeitstheorie und statistik.

springer, 2002.

(cid:18)n     1 + r

(cid:19)

r

nk

=

(cid:19)

(cid:18)n     1 + r

n     1

[6] l. wasserman. all of statistics: a concise course in statistical id136. springer, 2003.

stirling numbers, 2nd kind

(cid:26)n

(cid:27)

(cid:26)n     1
(cid:27)

= k

+

k

(cid:26)n     1
(cid:27)

k     1

(cid:26)n

(cid:27)

0

=

1     k     n

(cid:40)

1 n = 0
0

else

k

partitions

pn+k,k =

n(cid:88)

i=1

pn,i

k > n : pn,k = 0

n     1 : pn,0 = 0, p0,0 = 1

balls and urns

f : b     u

d = distinguishable,   d = indistinguishable.

|b| = n, |u| = m

f arbitrary

mn

n

(cid:18)m + n     1
(cid:19)
(cid:27)
(cid:26)n
m(cid:88)
m(cid:88)

k=1

k

pn,k

k=1

b : d, u : d

b :   d, u : d

b : d, u :   d

b :   d, u :   d

references

(cid:40)

f injective
mn m     n
0

else

(cid:18)m

(cid:19)

(cid:40)
(cid:40)

n

1 m     n
0 else
1 m     n
0 else

f surjective

m

m!

(cid:27)
(cid:26) n
(cid:18) n     1
(cid:19)
(cid:27)
(cid:26) n

m     1

m

pn,m

(cid:40)
(cid:40)
(cid:40)
(cid:40)

f bijective

n! m = n
0

else

1 m = n
0

else

1 m = n
0

else

1 m = n
0

else

[1] p. g. hoel, s. c. port, and c. j. stone. introduction to id203 theory. brooks cole,

1972.

[2] l. m. leemis and j. t. mcqueston. univariate distribution relationships. the american

statistician, 62(1):45   53, 2008.

30

.
]
2
[

n
o
t
s
e
u
q
c
m
d
n
a

s
i

m
e
e
l

y
s
e
t
r
u
o
c

,
s
p

i

h
s
n
o
i
t
a
l
e
r

n
o
i
t
u
b
i
r
t
s
i
d

e
t
a
i
r
a
v
i
n
u

31

