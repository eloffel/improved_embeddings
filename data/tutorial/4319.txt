stats 331

introduction to bayesian statistics

brendon j. brewer

this work is licensed under the creative commons attribution-sharealike

3.0 unported license. to view a copy of this license, visit

http://creativecommons.org/licenses/by-sa/3.0/deed.en gb.

contents

1 prologue

1.1 bayesian and classical statistics . . . . . . . . . . . . . . . . . . . . . . . .
1.2 this version of the notes
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 assessment

2 introduction

2.1 certainty, uncertainty and id203 . . . . . . . . . . . . . . . . . . . .

4
5
6
6

8
8

3 first examples

3.1 the bayes    box . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11
11
3.1.1 likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.1.2 finding the likelihood values . . . . . . . . . . . . . . . . . . . . . 13
3.1.3 the mechanical part . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.1.4
interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.2 bayes    rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.3 phone example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.3.1
solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
important equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

3.4

4 parameter estimation i: bayes    box

21
4.1 parameter estimation: bus example . . . . . . . . . . . . . . . . . . . . . 22
sampling distribution and likelihood . . . . . . . . . . . . . . . . . 25
4.1.1
4.1.2 what is the    data   ? . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.2 prediction in the bus problem . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.3 bayes    rule, parameter estimation version . . . . . . . . . . . . . . . . . . 27

5 parameter estimation: analytical methods

5.1
5.2 the e   ect of di   erent priors

29
          notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
31
5.2.1 prior 2: emphasising the extremes . . . . . . . . . . . . . . . . . . 32
5.2.2 prior 3: already being well informed . . . . . . . . . . . . . . . . . 32
5.2.3 the beta distribution . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.2.4 a lot of data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

. . . . . . . . . . . . . . . . . . . . . . . . .

6 summarising the posterior distribution

36
6.1 point estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
6.1.1 a very brief introduction to decision theory . . . . . . . . . . . . 38
6.1.2 absolute loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

1

contents

2

6.1.3 all-or-nothing loss . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
invariance of decisions . . . . . . . . . . . . . . . . . . . . . . . . . 40
6.1.4
41
6.1.5 computing point estimates from a bayes    box . . . . . . . . . . . .
6.1.6 computing point estimates from samples
. . . . . . . . . . . . . .
41
6.2 credible intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
6.2.1 computing credible intervals from a bayes    box . . . . . . . . . . . 42
6.2.2 computing credible intervals from samples
. . . . . . . . . . . . . 43
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

6.3 con   dence intervals

7 hypothesis testing and model selection

45
. . . . . . . . . . . . . . . . . . . . . . . . . 45
7.1 an example hypothesis test
7.2 the    testing    prior
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
7.3 some terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
7.4 hypothesis testing and the marginal likelihood . . . . . . . . . . . . . . .
51

8 id115

8.1.1

52
8.1 monte carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
summaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
8.2 multiple parameters
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
8.3 the metropolis algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
8.3.1 metropolis, stated . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
8.4 a two state problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
8.5 the steady-state distribution of a markov chain . . . . . . . . . . . . . . 59
8.6 tactile mcmc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

9 using jags

61
9.1 basic jags example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
9.2 checklist for using jags . . . . . . . . . . . . . . . . . . . . . . . . . . . 65

10 regression

67
10.1 a simple id75 problem . . . . . . . . . . . . . . . . . . . . . 67
10.2 interpretation as a bayesian question . . . . . . . . . . . . . . . . . . . . . 67
10.3 analytical solution with known variance . . . . . . . . . . . . . . . . . . 68
10.4 solution with jags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
10.5 results for    road    data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
10.6 predicting new data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
10.7 simple id75 with outliers . . . . . . . . . . . . . . . . . . . . 75
10.8 multiple id75 and id28 . . . . . . . . . . . . . 76

11 replacements for t-tests and anova

11.1 a t-test example

77
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
11.1.1 likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
11.1.2 prior 1: very vague
. . . . . . . . . . . . . . . . . . . . . . . . . . 79
11.1.3 prior 2: they might be equal!
. . . . . . . . . . . . . . . . . . . . . 79
11.1.4 prior 3: alright, they   re not equal, but they might be close . . . . . 80
11.2 one way anova . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
11.2.1 hierarchical model
. . . . . . . . . . . . . . . . . . . . . . . . . . . 83
11.2.2 mcmc e   ciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
11.2.3 an alternative parameterisation . . . . . . . . . . . . . . . . . . . 86

contents

12 acknowledgements

3

88

a r background

89
a.1 vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
a.2 lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90
a.3 functions
91
a.4 for loops . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
a.5 useful id203 distributions . . . . . . . . . . . . . . . . . . . . . . . .
91

a id203

92
a.1 the product rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
a.1.1 bayes    rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
a.2 the sum rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
a.3 random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
a.3.1 discrete random variables . . . . . . . . . . . . . . . . . . . . . . . 94
a.3.2 continuous random variables . . . . . . . . . . . . . . . . . . . . . 94
a.3.3 shorthand notation . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
a.4 useful id203 distributions . . . . . . . . . . . . . . . . . . . . . . . . 95

a rosetta stone

96

chapter 1

prologue

this course was originally developed by dr wayne stewart (formerly of the university of
auckland) and was    rst o   ered in 2009 (figure 1.1). i joined the department of statistics
in july 2012 and took over the course from him. it was good fortune for me that wayne
left the university as i arrived. if i had been able to choose which undergraduate course i
would most like to teach, it would have been this one!

wayne is a passionate bayesian1 and advocate for the inclusion of bayesian statistics in
the undergraduate statistics curriculum. i also consider myself a bayesian and agree that
this approach to statistics should form a greater part of statistics education than it does
today. while this edition of the course di   ers from wayne   s in some ways2, i hope i am
able to do the topic justice in an accessible way.

in this course we will use the following software:

    r (http://www.r-project.org/)
    jags (http://mcmc-jags.sourceforge.net/)
    the rjags package in r
    rstudio (http://www.rstudio.com/)

you will probably have used r, at least a little bit, in previous statistics courses. rstudio
is just a nice program for editing r code, and if you don   t like it, you   re welcome to use
any other text editor. jags is in a di   erent category and you probably won   t have seen
it before. jags is used to implement bayesian methods in a straightforward way, and
rjags allows us to use jags from within r. don   t worry, it   s not too di   cult to learn
and use jags! we will have a lot of practice using it in the labs.

these programs are all free and open source software. that is, they are free to use, share
and modify. they should work on virtually any operating system including the three

1bayesian statistics has a way of creating extreme enthusiasm among its users. i don   t just use bayesian

methods, i am a bayesian.

2the di   erences are mostly cosmetic. 90% of the content is the same.

4

chapter 1. prologue

5

figure 1.1: an ad for the original version of this course (then called stats 390), showing
wayne stewart with two ventriloquist dolls (tom bayes and freaky frequentist), who would
have debates about which approach to statistics is best.

most popular: microsoft windows, mac os x and gnu/linux. in previous editions of
the course, another program called winbugs was used instead of jags. unfortunately,
winbugs has not been updated for several years, and only works on microsoft windows.
therefore i switched over to jags in 2013. the di   erences between jags and winbugs
are fairly minor, but jags has the advantage of being open source and cross-platform.
all of this software is already installed on the lab computers, but if you would like to
install it on your own computer, instructions are provided on the course information sheet.

1.1 bayesian and classical statistics

throughout this course we will see many examples of bayesian analysis, and we will
sometimes compare our results with what you would get from classical or frequentist
statistics, which is the other way of doing things. you will have seen some classical
statistics methods in stats 10x and 20x (or biosci 209), and possibly other courses as
well. you may have seen and used bayes    rule before in courses such as stats 125 or 210.
bayes    rule can sometimes be used in classical statistics, but in bayesian stats it is used
all the time).

many people have di   ering views on the status of these two di   erent ways of doing
statistics.
in the past, bayesian statistics was controversial, and you had to be very
brave to admit to using it. many people were anti-bayesian! these days, instead of

chapter 1. prologue

6

bayesians and anti-bayesians, it would be more realistic to say there are bayesians and
non-bayesians, and many of the non-bayesians would be happy to use bayesian statistics
in some circumstances. the non-bayesians would say that bayesian statistics is one way of
doing things, and it is a matter of choice which one you prefer to use. most bayesian statis-
ticians think bayesian statistics is the right way to do things, and non-bayesian methods
are best thought of as either approximations (sometimes very good ones!) or alternative
methods that are only to be used when the bayesian solution would be too hard to calculate.

sometimes i may give strongly worded opinions on this issue, but there is one important
point that you should keep in mind throughout this course:

you do not have to agree with me in order to do well in stats 331!

1.2 this version of the notes

wayne stewart taught stats 331 with his own course notes. when i took over the
course, i found that our styles were very di   erent, even though we teach the same ideas.
unfortunately, it was challenging for the students to reconcile my explanations with
wayne   s. therefore i thought it would be better to have my own version of the notes.
these lecture notes are a work in progress, and do not contain everything we cover in
the course. there are many things that are important and examinable, and will be only
discussed in lectures, labs and assignments!

the plots in these notes were not produced using r, but using a di   erent plotting package
where i am more familiar with the advanced plotting features. this means that when
i give an r command for a plot, it will not produce a plot that looks exactly like the
plot that follows. however, it will give approximately the same plot, conveying the same
information. i apologise if you    nd this inconsistency distracting.

at this stage, the course notes contain the basic material of the course. some more
advanced topics will be introduced and discussed in lectures, labs and assignments.

i appreciate any feedback you may have about these notes.

1.3 assessment

the assessment for this course is broken down as follows:

    20% assignments. there will be four assignments, worth 5% each. the assignments

are not small, so please do not leave them until the last minute.

chapter 1. prologue

7

    20% midterm test (50 minutes, calculators permitted). this will be held in class, in

place of a lecture, some time just after mid semester break.

    60% final exam (two hours, calculators permitted).

chapter 2

introduction

every day, throughout our lives, we are required to believe certain things and not to
believe other things. this applies not only to the    big questions    of life, but also to trivial
matters, and everything in between. for example, this morning i boarded the bus to
university, sure that it would actually take me here and not to wellington. how did i
know the bus would not take me to wellington? well, for starters i have taken the same
bus many times before and it has always taken me to the university. another clue was
that the bus said    midtown    on it, and a bus to wellington probably would have said
wellington, and would not have stopped at a minor bus stop in suburban auckland. none
of this evidence proves that the bus would take me to university, but it does makes it very
plausible. given all these pieces of information, i feel quite certain that the bus will take
me to the city. i feel so certain about this that the possibility of an unplanned trip to
wellington never even entered my mind until i decided to write this paragraph.

somehow, our brains are very often able to accurately predict the correct answer to many
questions (e.g. the destination of a bus), even though we don   t have all the available
information that we would need to be 100% certain. we do this using our experience of
the world and our intuition, usually without much conscious attention or problem solving.
however, there are areas of study where we can   t just use our intuition to make judgments
like this. for example, most of science involves such situations. does a new treatment
work better than an old one? is the expansion of the universe really accelerating? people
tend to be interested in trying to answer questions that haven   t been answered yet, so our
attention is always on the questions where we   re not sure of the answer. this is where
statistics comes in as a tool to help us in this grey area, when we can   t be 100% certain
about things, but we still want to do the best we can with our incomplete information.

2.1 certainty, uncertainty and id203

in the above example, i said things like    i couldn   t be 100% certain   . the idea of using a
number to describe how certain you are is quite natural. for example, contestants on the
tv show    who wants to be a millionaire    often say things like    i   m 75% sure the answer

8

chapter 2.

introduction

9

is a   1.

there are some interesting things to notice about this statement. firstly, it is a subjective
statement. if someone else were in the seat trying to answer the question, she might say
the id203 that a is correct is 100%, because she knows the answer! a third person
faced with the same question might say the id203 is 25%, because he has no idea
and only knows that one of the four answers must be correct.

in bayesian statistics, the interpretation of what id203 means is that it is a description
of how certain you are that some statement, or proposition, is true. if the id203 is 1,
you are sure that the statement is true. so sure, in fact, that nothing could ever change
your mind (we will demonstrate this in class). if the id203 is 0, you are sure that the
proposition is false. if the id203 is 0.5, then you are as uncertain as you would be
about a fair coin    ip. if the id203 is 0.95, then you   re quite sure the statement is
true, but it wouldn   t be too surprising to you if you found out the statement was false.
see figure 2.1 for a graphical depiction of probabilities as degrees of certainty or plausibility.

figure 2.1: id203 can be used to describe degrees of certainty, or how plausible some
statement is. 0 and 1 are the two extremes of the scale and correspond to complete certainty.
however, probabilities are not static quantities. when you get more information, your
probabilities can change.

in bayesian statistics, probabilities are in the mind, not in the world.

it might sound like there is nothing more to bayesian statistics than just thinking about a
question and then blurting out a id203 that feels appropriate. fortunately for us,
there   s more to it than that! to see why, think about how you change your mind when
new evidence (such as a data set) becomes available. for example, you may be on    who
wants to be a millionaire?    and not know the answer to a question, so you might think
the id203 that it is a is 25%. but if you call your friend using    phone a friend   , and
your friend says,    it   s de   nitely a   , then you would be much more con   dent that it is
a! your id203 probably wouldn   t go all the way to 100% though, because there is

1this reminds me of an amusing exchange from the tv show monk. captain stottlemeyer: [about
someone electrocuting her husband] monk, are you sure? i mean, are you really sure? and don   t give me
any of that    95 percent    crap. monk: captain, i am 100% sure... that she probably killed him.

01id203somewhat sureit's false, but i could be wrongi'm very uncertain.it's a toss-upi am very sure thatit is truechapter 2.

introduction

10

always the small possibility that your friend is mistaken.

when we get new information, we should update our probabilities to take
the new information into account. bayesian methods tell us exactly how
to do this.

in this course, we will learn how to do data analysis from a bayesian point of view. so
while the discussion in this chapter might sound a bit like philosophy, we will see that
using this kind of thinking can give us new and powerful ways of solving practical data
analysis problems. the methods we will use will all have a common structure, so if you
are faced with a completely new data analysis problem one day, you will be able to design
your own analysis methods by using the bayesian framework. best of all, the methods
make sense and perform extremely well in practice!

chapter 3

first examples

we will now look at a simple example to demonstrate the basics of how bayesian statistics
works. we start with some probabilities at the beginning of the problem (these are called
prior probabilities), and how exactly these get updated when we get more information
(these updated probabilities are called posterior probabilities). to help make things more
clear, we will use a table that we will call a bayes    box to help us calculate the posterior
probabilities easily.

suppose there are two balls in a bag. we know in advance that at least one of them is
black, but we   re not sure whether they   re both black, or whether one is black and one is
white. these are the only two possibilities we will consider. to keep things concise, we
can label our two competing hypotheses. we could call them whatever we want, but i will
call them bb and bw. so, at the beginning of the problem, we know that one and only one
of the following statements/hypotheses is true:

bb: both balls are black
bw: one ball is black and the other is white.

suppose an experiment is performed to help us determine which of these two hypotheses
is true. the experimenter reaches into the bag, pulls out one of the balls, and observes its
colour. the result of this experiment is (drumroll please!):

d: the ball that was removed from the bag was black.

we will now do a bayesian analysis of this result.

3.1 the bayes    box

a bayesian analysis starts by choosing some values for the prior probabilities. we have
our two competing hypotheses bb and bw, and we need to choose some id203 values
to describe how sure we are that each of these is true. since we are talking about two
hypotheses, there will be two prior probabilities, one for bb and one for bw. for simplicity,

11

chapter 3. first examples

12

we will assume that we don   t have much of an idea which is true, and so we will use the
following prior probabilities:

p (bb) = 0.5
p (bw) = 0.5.

(3.1)
(3.2)

pay attention to the notation. the upper case p stands for id203, and if we just
write p (whatever), that means we are talking about the prior id203 of whatever.
we will see the notation for the posterior id203 shortly. note also that since the two
hypotheses are mutually exclusive (they can   t both be true) and exhaustive (one of these
is true, it can   t be some unde   ned third option). we will almost always consider mutually
exclusive and exhaustive hypotheses in this course1.

the choice of 0.5 for the two prior probabilities describes the fact that, before we did the
experiment, we were very uncertain about which of the two hypotheses was true. i will
now present a bayes    box, which lists all the hypotheses (in this case two) that might be
true, and the prior probabilities. there are some extra columns which we haven   t discussed
yet, and will be needed in order to    gure out the posterior probabilities in the    nal column.
the    rst column of a bayes    box is just the list of hypotheses we are considering. in

hypotheses prior likelihood prior    likelihood posterior

bb
bw

totals:

0.5
0.5
1

this case there are just two. if you need to construct a bayes    box for a new problem,
just think about what the possible answers to the problem are, and list them in the    rst
column. the second column lists the prior probabilities for each of the hypotheses. above,
before we did the experiment, we decided to say that there was a 50% id203 that
bb is true and a 50% id203 that bw is true, hence the 0.5 values in this column. the
prior column should always sum to 1. remember, the prior probabilities only describe our
initial uncertainty, before taking the data into account. hopefully the data will help by
changing these probabilities to something a bit more decisive.

3.1.1 likelihood

the third column is called likelihood, and this is a really important column where the
action happens. the likelihood is a quantity that will be used for calculating the posterior
probabilities. in colloquial language, likelihood is synonymous with id203. it means
the same thing. however, in statistics, likelihood is a very speci   c kind of id203. to
   ll in the third column of the bayes    box, we need to calculate two likelihoods, so you can
tell from this that the likelihood is something di   erent for each hypothesis. but what is it
exactly?

1if this does not appear to be true in a particular problem, it is usually possible to rede   ne the various

hypotheses into a set that of hypotheses that are mutually exclusive and exhaustive.

chapter 3. first examples

13

the likelihood for a hypothesis is the id203 that you would have
observed the data, if that hypothesis were true. the values can be found
by going through each hypothesis in turn, imagining it is true, and asking,
   what is the id203 of getting the data that i observed?   .

here is the bayes    box with the likelihood column    lled in. i will explain how these
numbers were calculated in a bit more detail in the next subsection. if you have taken

hypotheses prior likelihood h = prior    likelihood posterior

bb
bw

totals:

0.5
0.5
1

1
0.5

stats 210 and used the maximum likelihood method, where you    nd the value of a
parameter that maximises the likelihood function, that is the same as the likelihood we
use in this course! so you have a head start in understanding this concept.

3.1.2 finding the likelihood values

we will    rst calculate the value of the likelihood for the bb hypothesis. remember, the
data we are analysing here is that we chose one of the balls in the bag    at random   , and it
was black. the likelihood for the bb hypothesis is therefore the id203 that we would
get a black ball if bb is true.

imagine that bb is true. that means both balls are black. what is the id203 that the
experiment would result in a black ball? that   s easy     it   s 100%! so we put the number 1
in the bayes box as the likelihood for the bb hypothesis.

now imagine instead that bw is true. that would mean one ball is black and the other is
white. if this were the case and we did the experiment, what would be the id203 of
getting the black ball in the experiment? since one of the two balls is black, the chance of
choosing this one is 50%. therefore, the likelihood for the bw hypothesis is 0.5, and that   s
why i put 0.5 in the bayes    box for the likelihood for bw.

in general, the likelihood is the id203 of the data that you actually got, assuming
a particular hypothesis is true. in this example it was fairly easy to get the likelihoods
directly by asking    if this hypothesis is true, what is the id203 of getting the black
ball when we do the experiment?   . sometimes this is not so easy, and it can be helpful
to think about all possible experimental outcomes/data you might have seen     even
though ultimately, you just need to select the one that actually occurred. table 3.1 shows
an example of this process.

the fact that only the blue probabilities in table 3.1 enter the bayes    box calculation
is related to the likelihood principle, which we will discuss in lectures. note also that
in table 3.1, the probabilities for the di   erent possible data sets add to 1 within each
hypothesis, but the sum of the blue    selected    likelihood values is not 1 (it is, in fact,
meaningless).

chapter 3. first examples

14

hypotheses possible data id203

bb

bw

black ball
white ball
black ball
white ball

1
0
0.5
0.5

table 3.1: this table demonstrates a method for calculating the likelihood values, by
considering not just the data that actually occurred, but all data that might have occurred.
ultimately, it is only the id203 of the data which actually occurred that matters, so
this is highlighted in blue.

when we come to parameter estimation in later chapters, we will usually set up our problems
in this way, by considering what data sets are possible, and assigning probabilities to them.

3.1.3 the mechanical part

the third column of the bayes    box is the product of the prior probabilities and the
likelihoods, calculated by simple multiplication. the result will be called    prior times
likelihood   , but occasionally we will use the letter h for these quantities. this is the
unnormalised posterior. it does not sum to 1 as the posterior probabilities should, but it
is at least proportional to the actual posterior probabilities.

to    nd the posterior probabilities, we take the prior    likelihood column and divide
it by its sum, producing numbers that do sum to 1. this gives us the    nal posterior
probabilities, which were the goal all along. the completed bayes    box is shown below:

hypotheses prior likelihood h = prior    likelihood posterior

bb
bw

totals:

0.5
0.5
1

1
0.5

0.5
0.25
0.75

0.667
0.333

1

we can see that the posterior probabilities are not the same as the prior probabilities,
because we have more information now! the experimental result made bb a little bit more
plausible than it was before. its id203 has increased from 1/2 to 2/3.

3.1.4 interpretation

the posterior probabilities of the hypotheses are proportional to the prior probabilies
and the likelihoods. a high prior id203 will help a hypothesis have a high posterior
id203. a high likelihood value also helps. to understand what this means about
reasoning, consider the meanings of the prior and the likelihood. there are two things
that can contribute to a hypothesis being plausible:

    if the prior id203 is high. that is, the hypothesis was already plausible, before

we got the data.

    if the hypothesis predicted the data well. that is, the data was what we would have

expected to occur if the hypothesis had been true.

chapter 3. first examples

15

i hope you agree that this is all very sensible.

in class we will also study variations on this problem, considering di   erent assumptions
about the prior probabilities and how they a   ect the results, and also considering what
happens when we get more and/or di   erent data.

3.2 bayes    rule

bayes    rule is an equation from id203 theory, shown in figure 3.1. the various terms
in bayes    rule are all probabilities, but notice that there are conditional probabilities in
there. for example, the left hand side of the equation is p (a|b) and that means the
id203 of a given b. that is, it   s the id203 of a after taking into account the
information b. in other words, p (a|b) is a posterior id203, and bayes    rule tells
us how to calculate it from other probabilities. bayes    rule is true for any statements a

figure 3.1: a blue neon sign displaying bayes    rule. you can use it to calculate the
id203 of a given b, if you know the values of some other probabilities on the right
hand side. image credit: matt buck. obtained from wikimedia commons.

and b. if you took the equation in figure 3.1 and replaced a with    k  ak  ap  o will survive
beyond 2050    and b with    i had co   ee this morning   , the resulting equation would still
be true2.

it is helpful to relabel a and b in bayes    rule to give a more clear interpretation of how
the equation is to be used. in this version of bayes    rule (which is one you should commit
to memory), a has been replaced by h, and b has been replaced by d. the reason for
these letters is that you should interpret h as hypothesis and d as data. then you can
interpret bayes    rule as telling you the id203 of a hypothesis given some data, in

2it would still be true, but it would not very interesting, because whether or not i had co   ee doesn   t

tell you much about the survival prospects of endangered new zealand parrots.

chapter 3. first examples

other words, a posterior id203.

p (h|d) =

p (h)p (d|h)

p (d)

16

(3.3)

in bayesian statistics, most of the terms in bayes    rule have special names. some of them
even have more than one name, with di   erent scienti   c communities preferring di   erent
terminology. here is a list of the various terms and the names we will use for them:

    p (h|d) is the posterior id203. it describes how certain or con   dent we
are that hypothesis h is true, given that we have observed data d. calculating
posterior probabilities is the main goal of bayesian statistics!

    p (h) is the prior id203, which describes how sure we were that h was true,

before we observed the data d.

    p (d|h) is the likelihood.

id203 that you would have observed data d.

if you were to assume that h is true, this is the

    p (d) is the marginal likelihood. this is the id203 that you would have

observed data d, whether h is true or not.

since you may encounter bayesian methods outside of stats 331, i have included an
appendix called    rosetta stone    that lists some common alternative terminology.

in the above example, we did some calculations to work out the numbers in the bayes    box,
particularly the posterior probabilities, which are the ultimate goal of the calculation. what
we were actually doing in these calculations was applying bayes    rule. we actually applied
bayes    rule twice, once to compute p (bb|d) and a second time to calculate p (bw|d).
when you use a bayes    box to calculate posterior probabilities, you are
really just applying bayes    rule a lot of times: once for each hypothesis
listed in the    rst column.

3.3 phone example

this example is based on question 1 from the 2012    nal exam. i got the idea for this
question from an example in david mackay   s wonderful book    id205,
id136 and learning algorithms    (available online as a free pdf download. you   re
welcome to check it out, but it is a large book and only about 20% of the content is
relevant to this course!).

you move into a new house which has a phone installed. you can   t remember the phone
number, but you suspect it might be 555-3226 (some of you may recognise this as being
the phone number for homer simpson   s    mr plow    business). to test this hypothesis,
you carry out an experiment by picking up the phone and dialing 555-3226.

if you are correct about the phone number, you will de   nitely hear a busy signal because
you are calling yourself. if you are incorrect, the id203 of hearing a busy signal is
1/100. however, all of that is only true if you assume the phone is working, and it might
be broken! if the phone is broken, it will always give a busy signal.

chapter 3. first examples

17

when you do the experiment, the outcome (the data) is that you do actually get the busy
signal. the question asked us to consider the following four hypotheses, and to calculate
their posterior probabilities: note that the four hypotheses are mutually exclusive and

hypothesis

description

prior id203

h1
h2
h3
h4

phone is working and 555-3226 is correct
phone is working and 555-3226 is incorrect

phone is broken and 555-3226 is correct
phone is broken and 555-3226 is incorrect

0.4
0.4
0.1
0.1

table 3.2: the four hypotheses about the state of the phone and the phone number. the
prior probabilities are also given.

exhaustive. if you were to come up with hypotheses yourself,    phone is working    and
   555-3226 is correct    might spring to mind. they wouldn   t be mutually exclusive so you
couldn   t do a bayes    box with just those two, but it is possible to put these together
(using    and   ) to make the four mutually exclusive options in the table.

3.3.1 solution

we will go through the solution using a bayes    box. the four hypotheses listed in table 3.2
and their prior probabilities are given, so we can    ll out the    rst two columns of a bayes   
box right away: the next thing we need is the likelihoods. the outcome of the experiment

hypotheses prior likelihood prior    likelihood posterior

h1
h2
h3
h4

totals:

0.4
0.4
0.1
0.1
1

(the data) was the busy signal, so we need to work out p (busy signal|h) for each h in
the problem (there are four of them). let   s start (naturally!) with h1.

if we assume h1 is true, then the phone is working and 555-3226 is the correct phone
number. in that case, we would de   nitely get a busy signal because we are calling ourselves.
therefore p (busy signal|h1) = 1 is our    rst likelihood value.
next, let   s imagine that h2 is true, so the phone is working, but 555-3226 is not the right
phone number. in this case, it is given in the question that the id203 of getting a
busy signal is 1/100 or 0.01 (in reality, this would be based on some other data, or perhaps
be a totally subjective judgement). therefore p (busy signal|h2) = 0.01, and that   s our
second likelihood value.

the likelihoods for h3 and h4 are quite straightforward because they both imply the
phone is broken, and that means a busy signal is certain. therefore p (busy signal|h3) =
p (busy signal|h4) = 1. we have our four likelihoods, and can proceed to work out
everything in the bayes    box, including the main goal     the posterior probabilities! here
it is:

chapter 3. first examples

18

hypotheses prior likelihood prior    likelihood posterior
0.662
0.00662
0.166
0.166

h1
h2
h3
h4

0.01

1
1

1

0.4
0.004
0.1
0.1
0.604

0.4
0.4
0.1
0.1
1

totals:

1

to conclude this phone problem, i should admit that i actually calculated the numbers
in the bayes    box using r. my code is shown below. a lot of the code we write in labs
will look like this. obviously in the 2012 exam the students had to use their calculators
instead.

prior = c(0.4, 0.4, 0.1, 0.1) # vector of prior probs
lik = c(1, 0.01, 1, 1)
# vector of likelihoods
h = prior*lik
z = sum(h)
post = prior*lik/z
# look at all the results
print(prior)
print(lik)
print(h)
print(z)
print(post)

# sum of prior times likelihood
# normalise to get posterior

now let   s try to see if this makes sense. there are many things we could think about,
but let   s just consider the question of whether the phone is working or not. the    rst two
hypotheses correspond to the phone being in a working state. if you want to calculate
the id203 of a or b, then you can just add the probabilities if they are mutually
exclusive. the prior id203 that the phone is working is therefore:

p (phone working) = p (h1     h2)

= p (h1) + p (h2)
= 0.4 + 0.4
= 0.8.

(3.4)
(3.5)
(3.6)
(3.7)

here, i have introduced the notation    , meaning    logical or   : for any two propositions a,
b, the proposition (a     b) is true if either one of a or b is true (or both).
the posterior id203 is worked out in a similar way, but using the posterior probabilities
instead of the prior ones:

p (phone working|busy signal) = p (h1     h2|busy signal)

= p (h1|busy signal) + p (h2|busy signal)
= 0.662 + 0.00662
= 0.6689.

(3.8)
(3.9)
(3.10)
(3.11)

our id203 that the phone is working has gone down a little bit as a result of this
evidence! that makes sense to me. a busy signal is what you would expect to happen if
the phone was broken. this data doesn   t prove the phone is broken, but it does point in

chapter 3. first examples

19

that direction a little bit, and hence the id203 that the phone is working has been
reduced from 0.8 to 0.6689.

3.4

important equations

posterior probabilities are calculated using bayes    rule. for a single hypothesis h given
data d, bayes    rule is:

p (h|d) =

p (h)p (d|h)

p (d)

(3.12)

this gives the posterior id203 p (h|d) in terms of the prior id203 p (h), the
likelihood p (d|h) and the marginal likelihood p (d) in the denominator. to obtain p (h),
think about your prior beliefs (which may indicate a large amount of uncertainty, or may
already be well informed based on previous data sets). to obtain p (d|h), think about
what the experiment is doing: if h is true, what data would you expect to see and with
what probabilities?

the denominator is the id203 of obtaining the data d but without assuming that h is
either true or false. this is obtained using the sum rule. there are two ways that the data
d could occur, either via the route of h being true (this has id203 p (h)p (d|h)),
or via the route of h being false (this has id203 p (   h)p (d|   h)). these two ways are
mutually exclusive, so we can add their probabilities:

p (d) = p (h)p (d|h) + p (   h)p (d|   h).

(3.13)

bayes    rule can be applied to a whole set of hypotheses (that are mutually exclusive
and exhaustive) simultaneously. this is a more common way of using it, and it is the
way we use it when we use a bayes    box. if we applied equation 3.12 to n hypotheses
h1, h2, ..., hn , given data d, we would get the following for the posterior id203 of
each hypothesis hi (for i = 1, 2, ..., n ):

p (hi|d) =

p (hi)p (d|hi)

p (d)

(3.14)

the denominator p (d) is a single number. it does not depend on the index i. it can
again be obtained using the sum rule. there are n mutually exclusive ways that the
data d could have occurred: via h1 being true, or via h2 being true, etc. adding the
probabilities of these gives:

p (d) =

p (hi)p (d|hi).

(3.15)

which just happens to be the sum of the prior times likelihood values. if you don   t    nd
equations particularly easy to read, just remember that following the steps for making
a bayes    box is equivalent to applying bayes    rule in this form! the p (hi) values are
the prior id203 column, the p (d|hi) values are the likelihood column, and the
denominator is the sum of the prior times likelihood column. for example, the posterior

n(cid:88)

i=1

chapter 3. first examples

20

id203 for h1 (the top right entry in a bayes    box) is given by the prior id203
for h1 times the likelihood for h1, divided by the sum of prior times likelihood values.
that is, p (h1|d) = p (h1)p (d|h1)/p (d). the correspondence between the probabilities
that go in a bayes    box (in general) and the terms in the equations are given in table 3.3.

hypotheses

h1
h2
. . .

totals:

prior
p (h1)
p (h2)

. . .
1

likelihood prior    likelihood posterior
p (d|h1)
p (h1|d)
p (h2|d)
p (d|h2)

p (h1)    p (d|h1)
p (h2)    p (d|h2)

. . .

. . .

p (d)

. . .
1

table 3.3: a general bayes    box. using bayes    rule or making a bayes    box are actually
the same thing, and this table can be used to identify the terms.

chapter 4

parameter estimation i: bayes    box

one of the most important times to use bayes    rule is when you want to do parameter
estimation. parameter estimation is a fairly common situation in statistics. in fact, it is
possible to interpret almost any problem in statistics as a parameter estimation problem
and approach it in this way!

firstly, what is a parameter? one way to think of a parameter is that it is just a fancy
term for a quantity or a number that is unknown1. for example, how many people are
currently in new zealand? well, a google search suggests 4.405 million. but that does not
mean there are exactly 4,405,000 people. it could be a bit more or a bit less. maybe it is
4,405,323, or maybe it is 4,403,886. we don   t really know. we could call the true number
of people in new zealand right now   , or we could use some other letter or symbol if we
want. when talking about parameter estimation in general we often call the unknown
parameter(s)   , but in speci   c applications we will call the parameter(s) something else
more appropriate for that application.

the key is to realise that we can use the bayes    box, like in previous chapters. but now,
our list of possible hypotheses is a list of possible values for the unknown parameter.
for example, a bayes    box for the precise number of people in new zealand might look
something like the one in table 4.1.

there are a few things to note about this bayes    box. firstly, it is big, which is why i just
put a bunch of    . . .    s in there instead of making up numbers. there are lots of possible
hypotheses, each one corresponding to a possible value for   . the prior probabilities i
have put in the second column were for illustrative purposes. they needn   t necessarily
all be equal (although that is often a convenient assumption). all the stu    we   ve seen
in smaller examples of bayes    rule and/or use of a bayes    box still applies here. the
likelihoods will still be calculated by seeing how the id203 of the data depends on
the value of the unknown parameter. you still go through all the same steps, multiplying
prior times likelihood and then normalising that to get the posterior probabilities for all
of the possibilities listed. note that a set of possible values together with the probabilities
is what is commonly termed a id203 distribution. in basic bayesian problems, like in
the introductory chapters, we start with some prior probabilities and update them to get

1another use for the term parameter is any quantity that something else depends on. for example, a
normal distribution has a mean    and a standard deviation    that de   nes which normal distribution we
are talking about.    and    are then said to be parameters of the normal distribution.

21

chapter 4. parameter estimation i: bayes    box

22

likelihood prior    likelihood posterior

possible hypotheses

. . .

   = 4404999
   = 4405000
   = 4405001
   = 4405002
   = 4405003
   = 4405004
   = 4405005
   = 4405006

. . .

totals:

prior

. . .

0.000001
0.000001
0.000001
0.000001
0.000001
0.000001
0.000001
0.000001

. . .
1

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .

. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
1

table 4.1: an example of how a bayes    box may be used in a parameter estimation
situation.

posterior probabilities. in parameter estimation, we start with a prior distribution for the
unknown parameter(s) and update that to get a posterior distribution for the unknown
parameter(s).

a quantity which has a id203 associated with each possible value is
traditionally called a    random variable   . random variables have proba-
bility distributions associated with them. in bayesian stats, an unknown
parameter looks mathematically like a    random variable   , but i try to
avoid the word random itself because it usually has connotations about
something that    uctuates or varies. in bayesian statistics, the prior dis-
tribution and posterior distribution only describe our uncertainty. the
actual parameter is a single    xed number.

4.1 parameter estimation: bus example

this is a beginning example of parameter estimation from a bayesian point of view. it
shows the various features that are always present in a bayesian parameter estimation
problem. there will be a prior distribution, the likelihood, and the posterior distribution.
we will spend a lot of time on this problem but keep in mind that this is just a single
example, and certain things about this example (such as the choice of the prior and the
likelihood) are speci   c to this example only, while other things about it are very general
and will apply in all parameter estimation problems. you will see and gain experience
with di   erent problems in lectures, labs, and assignments.

after moving to auckland, i decided that i would take the bus to work each day. however,
i wasn   t very con   dent with the bus system in my new city, so for the    rst week i just
took the    rst bus that came along and was heading in the right direction, towards the city.
in the    rst week, i caught 5 morning buses. of these 5 buses, two of them took me to
the right place, while three of them took me far from work, leaving me with an extra 20
minute walk. given this information, i would like to try to infer the proportion of the

chapter 4. parameter estimation i: bayes    box

23

buses that are    good   , that would take me right to campus. let us call this fraction   
and we will infer    using the bayesian framework. we will start with a prior distribution
that describes initial uncertainty about    and update this to get the posterior distribution,
using the data that 2/5 buses i took were    good   .

first we must think about the meaning of the parameter    in our particular problem so
we can choose a sensible prior distribution. since    is, in this example, a proportion, we
know it cannot be less than 0 or greater than 1. in principle,    could be any real value
between 0 and 1. to keep things simple to begin with, we shall make an approximation
and assume that the set of possible values for    is:

{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}.

this discrete approximation means that we can use a bayes    box. the    rst things to
   ll out in the bayes    box are the possible values and the prior probabilities (the prior
distribution). for starters, let us assume that before we got the data (two successes out of
5 trials), we were very uncertain about the value of   , and this can be modelled by using a
uniform prior distribution. there are 11 possible values for    that are being considered
with our discrete approximation, so the id203 of each is 1/11 = 0.0909. the partially
complete bayes    box is given in table 4.2. note the new notation that i have put in
the column titles. we will use this notation in all of our parameter estimation examples
(although the parameter(s) and data may have di   erent symbols when    and x respectively
are not appropriate).

possible values

  
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1

totals

prior
p(  )
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909

1

likelihood prior    likelihood posterior

p(  )p(x|  )

p(  |x)

p(x|  )

1

table 4.2: starting to make a bayes    box for the bus problem. this one just has the
possible parameter values and the prior distribution.

to get the likelihoods, we need to think about the properties of our experiment.
in
particular, we should imagine that we knew the value of    and were trying to predict what
experimental outcome (data) would occur. ultimately, we want to    nd the id203 of
our actual data set (2 out of the 5 buses were    good   ), for all of our possible    values.

recall that, if there are n repetitions of a    random experiment    and the    success   
id203 is    at each repetition, then the number of    successes    x has a binomial

24

(4.1)

chapter 4. parameter estimation i: bayes    box

(cid:19)

(cid:18) n

x

p(x|  ) =

  x (1       )n   x .

distribution:

(cid:18) n

(cid:19)

= n !

x

where
x!(n   x)! . this is the id203 mass function for x (if we imagine    to
be known), hence the notation p(x|  ), read as    the id203 distribution for x given      .
since there are    ve trials (n = 5) in the bus problem, the number of successes x must
be one of 0, 1, 2, 3, 4, or 5. if    is a high number close to 1, then we would expect the
resulting value of the data (number of successes) x to be something high like 4 or 5. low
values for x would still be possible but they would have a small id203. if    is a small
number, we would expect the data to be 0, 1, or 2, with less id203 for more than
2 successes. this is just saying in words what is written precisely in equation 5.1. the
id203 distribution for the data x is plotted in figure 4.1 for three illustrative values
of the parameter   . to obtain the actual likelihood values that go into the bayes    box, we

figure 4.1: the binomial id203 distribution for the data x, for three di   erent values
of the parameter   . if    is low then we would expect to see lower values for the data. if   
is high then high values are more probable (but all values from 0 to 5 inclusive are still
possible). the actual observed value of the data was x = 2. if we focus only on the values
of the curves at x = 2, then the heights of the curves give the likelihood values for these
three illustrative values of   .

can simply substitute in the known values n = 5 and x = 2:

(cid:19)

(cid:18) 5

2

p (x = 2|  ) =

  2 (1       )5   2

= 10      2 (1       )3 .

(4.2)

(4.3)

the resulting equation depends on    only! we can go through the list of    values and get a
numerical answer for the likelihood p (x = 2|  ), which is what we need for the bayes    box.

012345possibledatax0.00.20.40.60.8id203  =0.3  =0.5  =0.8chapter 4. parameter estimation i: bayes    box

25

likelihood prior    likelihood posterior

possible values

  
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1

totals

prior
p(  )
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909
0.0909

1

0

p(x|  )
0.0729
0.2048
0.3087
0.3456
0.3125
0.2304
0.1323
0.0512
0.0081

0

p(  )p(x|  )
0.0066
0.0186
0.0281
0.0314
0.0284
0.0209
0.0120
0.0047
0.0007

0

p(  |x)
0.0437
0.1229
0.1852
0.2074
0.1875
0.1383
0.0794
0.0307
0.0049

0

0

0.1515

0
1

table 4.3: the completed bayes    box for the bus problem (using a binomial distribution to
obtain the likelihood).

the    nal steps are, as usual, to multiply the prior by the likelihood and then normalise
that to get the posterior distribution. the completed bayes    box is given in table 4.3.

there are a few interesting values in the likelihood column that should help you to
understand the concept of likelihood a bit better. look at the likelihood for    = 0: it is
zero. what does this mean? it means that if we imagine    = 0 is the true solution, the
id203 of obtaining the data that we got (x = 2 successes) would be zero. that makes
sense! if    = 0, it means none of the buses are the    good    buses, so how could i have
caught a good bus twice? the id203 of that is zero.

the likelihood for    = 1 is also zero for similar reasons. if all of the buses are good,
then having 2/5 successes is impossible. you would get 5/5 with 100% certainty. so
p (x = 2|   = 1) = 0. the likelihood is highest for    = 0.4, which just so happens to equal
2/5. this    = 0.4 predicted the data best. it does not necessarily mean that    = 0.4 is the
most probable value. that depends on the prior as well (but with a uniform prior, it does
end up being that way. as you can see in the posterior distribution column,    = 0.4 has
the highest id203 in this case).

4.1.1 sampling distribution and likelihood

as we study more examples of parameter estimation, you might notice that we always    nd
the likelihood by specifying a id203 distribution for the data given the parameters
p(x|  ), and then we substituting in the actual observed data (equations 4.1 and 4.3).
technically, only the version with the actual data set substituted in is called the likelihood.
the id203 distribution p(x|  ), which gives the id203 of other data sets that did
not occur (as well as the one that did), is sometimes called the sampling distribution. at
times, i will distinguish between the sampling distribution and the likelihood, and at other
times i might just use the word likelihood for both concepts.

chapter 4. parameter estimation i: bayes    box

26

4.1.2 what is the    data   ?

even though this example is meant to be introductory, there is a subtlety that has been
swept under the rug. notice that our data consisted of the fact that we got 2/5 successes in
the experiment. when we worked out the likelihood, we were considering the id203
of getting x = 2, but we didn   t have a id203 for n = 5. in principle, we could treat
x and n as two separate data sets. we could    rst update from the prior to the posterior
given n = 5, and then update again to take into account x as well as n . however, the
   rst update would be a bit weird. why would knowing the number of trials tell you
anything about the success id203? e   ectively, what we have done in our analysis
is assume that n = 5 is prior information that lurks in the background the whole time.
therefore our uniform prior for    already    knows    that n = 5, so we didn   t have to
consider p (n = 5|  ) in the likelihood. this subtlety usually doesn   t matter much.

4.2 prediction in the bus problem

we have now seen how to use information (data) to update from a prior distribution to a
posterior distribution when the set of possible parameter values is discrete. the posterior
distribution is the complete answer to the problem. it tells us exactly how strongly we
should believe in the various possible solutions (possible values for the unknown parameter).
however, there are other things we might want to do with this information. predicting
the future is one! it   s fun, but risky. here we will look at how prediction is done using the
bayesian framework, continuing with the bus example. to be concrete, we are interested
in the following question: what is the id203 that i will catch the right bus tomorrow?.
this is like trying to predict the result of a future experiment.

in the bayesian framework, our predictions are always in the form of probabilities or
(later) id203 distributions. they are usually calculated in three stages. first, you
pretend you actually know the true value of the parameters, and calculate the id203
based on that assumption. then, you do this for all possible values of the parameter   
(alternatively, you can calculate the id203 as a function of   ). finally, you combine
all of these probabilities in a particular way to get one    nal id203 which tells you
how con   dent you are of your prediction.

suppose we knew the true value of    was 0.3. then, we would know the id203 of
catching the right bus tomorrow is 0.3. if we knew the true value of    was 0.4, we would
say the id203 of catching the right bus tomorrow is 0.4. the problem is, we don   t
know what the true value is. we only have the posterior distribution. luckily, the sum
rule of id203 (combined with the product rule) can help us out. we are interested in
whether i will get the good bus tomorrow. there are 11 di   erent ways that can happen.
either    = 0 and i get the good bus, or    = 0.1 and i get the good bus, or    = 0.2 and i
get the good bus, and so on. these 11 ways are all mutually exclusive. that is, only one
of them can be true (since    is actually just a single number). mathematically, we can

chapter 4. parameter estimation i: bayes    box

27

obtain the posterior id203 of catching the good bus tomorrow using the sum rule:

p (good bus tomorrow|x) =

=

p(  |x)p (good bus tomorrow|  , x)

p(  |x)  

(4.4)

(4.5)

(cid:88)
(cid:88)

  

  

this says that the total id203 for a good bus tomorrow (given the data, i.e. using
the posterior distribution and not the prior distribution) is given by going through each
possible    value, working out the id203 assuming the    value you are considering
is true, multiplying by the id203 (given the data) this    value is actually true, and
summing. in this particular problem, because p (good bus tomorrow|  , x) =   , it just so
happens that the id203 for tomorrow is the expectation value of    using the posterior
distribution. to three decimal places, the result for the id203 tomorrow is 0.429.
interestingly, this is not equal to 2/5 = 0.4.

in practice, these kinds of calculations are usually done in a computer. the r code for
computing the bayes    box and the id203 for tomorrow is given below. this is very
much like many of the problems we will work on in labs.

# make a vector of possibilities (first column of the bayes    box)
theta = seq(0, 1, by=0.1)

# corresponding vector of prior probabilities
# (second column of the bayes    box)
prior = rep(1/11,11)

# likelihood. notice use of dbinom() rather than formula
# because r conveniently knows a lot of
# standard id203 distributions already
lik = dbinom(2,5,theta)

# prior times likelihood, then normalise to get posterior
h = prior*lik
post = h/sum(h)

# id203 for good bus tomorrow (prediction!)
# this happens to be the same as the posterior expectation of theta
# *in this particular problem* because the id203 of a
# good bus tomorrow given theta is just theta.
prob_tomorrow = sum(theta*post)

4.3 bayes    rule, parameter estimation version

mathematically, what we did to calculate the posterior distribution was to take the prior
distribution as a whole (the whole second column) and multiply it by the likelihood (the
whole third column) to get the unnormalised posterior, then normalise to get the    nal
posterior distribution. this can be written as follows, which we will call the    parameter

chapter 4. parameter estimation i: bayes    box

estimation    version of bayes    rule. there are three ways to write it:

p(  )p(x|  )

p(  |x) =
p(  |x)     p(  )p(x|  )

p(x)

posterior     prior    likelihood.

28

(4.6)

(4.7)
(4.8)

writing the equations in these ways is most useful when you can write the prior p(  ) and
the likelihood p(x|  ) as formulas (telling you how the values depend on    as you go through
the rows). then you can get the equation for the posterior distribution (whether it is a
discrete distribution, or a continuous one, in which case p(  ) and p(  |x) are id203
densities. we will do this in the next chapter.

the notation in equation 4.8 is very simpli   ed and concise, but is a popular kind of
notation in bayesian work. for an explanation of the relationship between this notation
and other common choices (such as p (x = x) for a discrete distribution or f (x) for a
density), see appendix a.

chapter 5

parameter estimation: analytical
methods

analytical methods are those which can be carried out with a pen and paper, or the    old
school    way before we all started using computers. there are some problems in bayesian
statistics that can be solved in this way, and we will see a few of them in this course. for
an analytical solution to be possible, the maths usually has to work out nicely, and that
doesn   t always happen, so the techniques shown here don   t always work. when they do    
great! when they don   t, that   s what mcmc (and jags) is for!

let   s look at the binomial likelihood problem again, with the familiar bus example. out of
n = 5 attempts at a    repeatable    experiment, there were x = 2 successes. from this, we
want to infer the value of   , the success id203 that applied on each trial, or the overall
fraction of buses that are good. because of its meaning, we know with 100% certainty
that    must be between 0 and 1 (inclusive).

recall that, if we knew the value of    and wanted to predict the data x (regarding n as
being known in advance), then we would use the binomial distribution:

(cid:19)

(cid:18) n

x

p(x|  ) =

  x (1       )n   x .

(5.1)

let   s use a uniform prior for   , but instead of making the discrete approximation and
using a bayes    box, let   s keep the continuous set of possibilities, that    can be any real
number between 0 and 1. because the set of possibilities is continuous, the prior and the
posterior for    will both be id203 densities. if we tried to do a bayes    box now, it
would have in   nitely many rows! the equation for our prior, a uniform id203 density
between 0 and 1, is:

(cid:26) 1, 0            1

0, otherwise

p(  ) =

(5.2)

if we keep in mind that    is between 0 and 1, and therefore remember at all times that
we are restricting our attention to        [0, 1], we can write the uniform prior much more
simply as:

p(  ) = 1.

29

(5.3)

chapter 5. parameter estimation: analytical methods

30

if you    nd the bayes    box way of thinking easier to follow than the mathematics here, you
can imagine we are making a bayes    box like in table 4.2, but with an    in   nite    number of
rows, and the equation for the prior tells us how the prior id203 varies as a function
of    as we go down through the rows (since the prior is uniform, the probabilities don   t
vary at all).

to    nd the posterior id203 density for   , we use the    parameter estimation    form of
bayes    rule:

posterior     prior    likelihood
p(  |x)     p(  )p(x|  ).

(5.4)
(5.5)

we already wrote down the equations for the prior and the likelihood, so we just need to
multiply them.

p(  |x)     p(  )p(x|  )

(cid:19)

(cid:18) n

x

    1   

  x (1       )n   x

(5.6)

(5.7)

since we are using the abbreviated form of the prior, we must remember this equation
only applies for        [0, 1]. to simplify the maths, there are some useful tricks you can
use a lot of the time when working things out analytically. notice that the    parameter
estimation    form of bayes    rule has a proportional sign in it, not an equals sign. that   s
because the prior times the likelihood can   t actually be the posterior distribution because
it is not normalised. the sum or integral is not 1. however, the equation still gives the
correct shape of the posterior id203 density function (the way it varies as a function
of   ). this is helpful because you can save ink. if there are some constant factors in your
expression for the posterior that don   t involve the parameter (in this case,   ), you can
ignore them. the proportional sign will take care of them. in this case, it means we can
forget about the pesky    n choose x    term, and just write:
p(  |x)       x (1       )n   x
      2 (1       )3 .

(5.8)
(5.9)

the    nal step i included was to substitute in the actual values of n and x instead of leaving
the symbols there. that   s it! we have the correct shape of the posterior distribution. we
can use this to plot the posterior, as you can see in figure 5.1.

5.1           notation

while it is very helpful to know the full equations for di   erent kinds of id203
distributions (both discrete and continuous), it is useful to be able to communicate about
id203 distributions in an easier manner. there is a good notation for this which we
will sometimes use in stats 331. if we want to communicate about our above analysis,
and someone wanted to know what prior distribution we used, we could do several things.
we could say    the prior for    was uniform between 0 and 1   , or we could give the formula
for the prior distribution (equation 5.2). however, a convenient shorthand in common use
is to simply write:

       uniform(0, 1)

(5.10)

chapter 5. parameter estimation: analytical methods

31

figure 5.1: the prior and the posterior for    in the bus problem, given that we had 2/5
successes. the prior is just a uniform density and this is plotted as a    at line, describing
the fact that    can be anywhere between 0 and 1 and we don   t have much of an idea. after
getting the data, the distribution changes to the posterior which is peaked at 0.4, although
there is still a pretty wide range of uncertainty.

or, even more concisely:

       u (0, 1).

(5.11)

this notation conserves ink, and is good for quick communication. it is also very similar
to the notation used in jags, which will be introduced in later chapters.

we can also write the binomial likelihood (which we used for the bus problem) in this
notation, instead of writing out the full equation (equation 5.1). we can write:

x|       binomial(n,   )

(5.12)

this says that if we knew the value of   , x would have a binomial distribution with n
trials and success id203   . we can also make this one more concise:

x     bin(n,   )

(5.13)

the di   erences here are that    binomial    has been shortened to    bin    and the    given      
part has been left out. however, we see that there is a    present on the right hand side, so
the    given       must be understood implicitly.

5.2 the e   ect of di   erent priors

we decided to do this problem with a uniform prior, because it is the obvious    rst choice
to describe    prior ignorance   . however, in principle, the prior could be di   erent. this

0.00.20.40.60.81.0possiblevalues  0.00.51.01.52.02.5id203densitypriorp(  )posteriorp(  |x)chapter 5. parameter estimation: analytical methods

32

will change the posterior distribution, and hence the conclusions. this isn   t a problem
of bayesian analysis, but a feature. data on its own doesn   t tell us exactly what should
believe. we must combine the data with all our other prior knowledge (i.e. put the data
in context) to arrive at reasoned conclusions.

in this section we will look at the e   ect of di   erent priors on the results, again focusing
on the bus problem for continuity. speci   cally, we will look at three di   erent priors: the
uniform one that we already used, and two other priors discussed below.

5.2.1 prior 2: emphasising the extremes

that    is between 0 and 0.1 is only(cid:82) 0.1

one possible criticism of the uniform prior is that there is not much id203 given to
extreme solutions. for example, according to the uniform(0, 1) prior, the prior id203
0 1 d   = 0.1. but, depending on the situation, we
might think values near zero should be more plausible1. one possible choice of prior
distribution that assigns more id203 to the extreme values (close to 0 or 1) is:

p(  )           1

2 (1       )    1

2 .

(5.14)

5.2.2 prior 3: already being well informed

here   s another scenario that we might want to describe in our prior. suppose that, before
getting this data, you weren   t ignorant at all, but already had a lot of information about
the value of the parameter. say that we already had a lot of information which suggested
the value of    was probably close to 0.5. this could be modelled by the following choice of
prior:

p(  )       100(1       )100.

(5.15)

the three priors are plotted in figure 5.2 as dotted lines. the three corresponding posterior
distributions are plotted as solid lines. the posteriors were computed by multiplying the
three priors by the likelihood and normalising. the blue curves correspond to the uniform
prior we used before, the red curves use the    emphasising the extremes    prior, and the
green curves use the    informative    prior which assumes that    is known to be close to 0.5.

there are a few interesting things to notice about this plot. firstly, the posterior dis-
tributions are basically the same for the red and blue priors (the uniform prior and the
   emphasising the extremes    prior). the main di   erence in the posterior is, as you would
expect, that the extremes are emphasised a little more. if something is more plausible
before you get the data, it   s more plausible afterwards as well.

the big di   erence is with the informative prior. here, we were already pretty con   dent
that    was close to 0.5, and the data (since it   s not very much data) hasn   t given us any

1here   s another parameter that is between 0 and 1: the proportion of households in new zealand that
keep a macaw as a pet (call that   ). i hope this number is low (it is very di   cult to take responsible care
of such a smart bird). i also think it probably is low. i would de   nitely object to a prior that implied
p (   < 0.1) = 0.1. i would want a prior that implied something like p (   < 0.1) = 0.999999.

chapter 5. parameter estimation: analytical methods

33

figure 5.2: three di   erent priors (dotted lines) and the corresponding three posteriors
(solid lines) given the bus example data. see the text for discussion of these results.

reason to doubt that, so we still think    is close to 0.5. since we already knew that   
was close to 0.5, the data are acting only to increase the precision of our estimate (i.e.
make the posterior distribution narrower). but since we had so much prior information,
the data aren   t providing much    extra    information, and the posterior looks basically the
same as the prior.

5.2.3 the beta distribution

the three priors we have used are all examples of beta distributions. the beta distributions
are a family of id203 distributions (like the normal, poisson, binomial, and so on)
which can be applied to continuous random variables known to be between 0 and 1. the
general form of a beta distribution (here written for a variable x) is:

p(x|  ,   )     x     1(1     x)     1.

(5.16)

the quantities    and    are two parameters that control the shape of the beta distribution.
since we know the variable x is between 0 and 1 with id203 1, the normalisation
constant could be found by doing an integral. then you could write the id203
distribution with an equals sign instead of a proportional sign,

p(x|  ,   ) =

=

(cid:82) 1

x     1(1     x)     1
0 x     1(1     x)     1 dx
x     1(1     x)     1

.

b(  ,   )

(5.17)

(5.18)

where b(  ,   ) (called the    beta function   ) is de   ned (usefully. . . ) as the result of doing
that very integral (it can be related to factorials too, if you   re interested). thankfully, we

0.00.20.40.60.81.0possiblevalues  024681012id203densitythreepriors,threeposteriorsprior1prior2prior3chapter 5. parameter estimation: analytical methods

34

can get away with the    proportional    version most of the time. in           notation the beta
distribution is written as:

x|  ,        beta(  ,   ).

(5.19)

again, the    given    and       can be dropped. it is implicit because they appear on the
right hand side. by identifying the terms of equation 5.16 with the form of our three
priors (equations 5.2, 5.14 and 5.15)), we see that our three priors can be written in          
notation like this:

prior 1:
prior 2:
prior 3:

(cid:1)

       beta(cid:0) 1

       beta(1, 1)
2, 1
       beta(101, 101)

2

when you work out the posterior distributions analytically and then compare them to
the formula for the beta distribution, you can see that the three posteriors are also beta
distributions! speci   cally, you get:

posterior 1:
posterior 2:
posterior 3:

       beta(3, 4)
       beta(2.5, 3.5)
       beta(103, 104)

this is    magic    that is made possible by the mathematical form of the beta prior and the
binomial likelihood2. it is not always possible to do this.

we can also derive the general solution for the posterior for    when the prior is a beta(  ,   )
distribution, the likelihood is a binomial distribution, and x successes were observed out
of n trials. the posterior is:

p(  |x)     p(  )p(x|  )

           1(1       )     1      x(1       )n   x
=     +x   1(1       )  +n   x   1

(5.20)
(5.21)
(5.22)

which can be recognised as a beta(   + x,    + n     x) distribution.
remember that in this particular problem, the id203 of a success tomorrow is simply
the expectation value (mean) of the posterior distribution for   . we can look up (or
derive) the formula for the mean of a beta distribution and    nd that if x     beta(  ,   )
then e(x) =   /(   +   ). applying this to the three posterior distributions gives:

p (good bus tomorrow|x) = 3/7
    0.429
    0.417
p (good bus tomorrow|x) = 2.5/6
p (good bus tomorrow|x) = 103/207     0.498

(using prior 1)
(using prior 2)
(using prior 3)

the result for prior 1 is laplace   s infamous    rule of succession    which i will discuss a
little bit in lectures.

2the technical term for this magic is that the beta distribution is a conjugate prior for the binomial

likelihood.

chapter 5. parameter estimation: analytical methods

35

5.2.4 a lot of data

as shown above, the choice of prior distribution has an impact on the conclusions.
sometimes it has a big impact (the results using prior 3 were pretty di   erent to the results
from priors 1 and 2), and sometimes not much impact (e.g. the results from priors 1 and 2
were pretty similar). there is a common phenomenon that happens when there is a lot
of data: the prior tends not to matter so much. imagine we did a much bigger version
of the bus experiment with n = 1000 trials, which resulted in x = 500 successes. then
the posterior distributions corresponding to the three di   erent priors are all very similar
(figure 5.3).

figure 5.3: when you have a lot of data, the results are less sensitive to the choice of
prior distribution. note that we have zoomed in and are only looking around    = 0.5: these
posterior distributions are quite narrow because there is now a lot more information about
  . the red and blue posteriors (based on priors 1 and 2) are so similar that they overlap
and look like one purple curve.

this is reassuring. note, however, that this only occurs because the three analyses used
the same likelihood. if three people have di   erent prior distributions for something and
they can   t agree on what the experiment even means, there is no guarantee they will end
up agreeing, even if there   s a large amount of data!

remember though, that when the results are sensitive to the choice of prior, that is not
a problem with the bayesian approach, but rather an important warning message: the
data aren   t very informative! then, the options are: i) think really hard about your prior
distribution and be careful when deciding what it should be, and ii) get more or better
data!

0.400.450.500.55possiblevalues  051015202530id203densitylotsofdataprior1prior2prior3chapter 6

summarising the posterior
distribution

the posterior distribution is the full answer to any bayesian problem. it gives a complete
description of our state of knowledge and our uncertainty about the value(s) of unknown
parameters. from the posterior distribution, we can calculate any id203 we want. for
example, if we had a posterior distribution p(  |x) and we wanted to know the id203
that    is greater than or equal to 100, we could do:

or

p (       100|x) =

p (       100|x) =

(cid:90)    
   (cid:88)

100

100

p(  |x) d  

p(  |x)

(6.1)

(6.2)

depending on whether the set of possible    values is continuous or discrete. we could
also work out the id203 of anything else. however, the posterior distribution is
sometimes too much information for us to think about easily. maybe a giant list of    values
and probabilities isn   t easy to digest. sometimes, we need to summarise the posterior
distribution to help us communicate our results with others. a giant bayes    box (or a
million mcmc samples of the parameter, we   ll see that later), might technically contain
everything we want, but it   s not easy to talk about.

for example, say you were trying to estimate a parameter, and a colleague asked you to
state your uncertainty about the parameter. well, your posterior distribution might be
complicated. it might have bumps and wiggles in it, or some other kind of structure. if
there were two or more unknown parameters, there might be dependence in the posterior
distribution. in some cases there might even me multiple separate peaks! figure 6.1 shows
an example of what a complicated posterior distribution might look like. if this was your
result, your colleague might not care about all the little wiggles in this plot. they just
want to know the    big picture    of your results.

the idea of summarising the posterior distribution is very closely related to the idea of
summarising a data set, which you probably encountered when you studied descriptive
statistics.

36

chapter 6. summarising the posterior distribution

37

figure 6.1: a complicated posterior distribution. when communicating with others, it
is often useful to summarise the posterior distribution with a few numbers. in this case,
something like    the parameter = 5    1    might be a useful summary.

in descriptive statistics, you often make summaries of a complex data set
(e.g. the mean and the standard deviation) so that you can communicate
about the data set in a concise way. in bayesian statistics, you often do a
similar thing, but instead of giving a concise description of the data, you
give a concise description of the posterior distribution.

6.1 point estimates

a    point estimate    refers to a single number guess for the value of a parameter. if you
have several parameters, a point estimate would be a single guess for the value of each
parameter (like a single point in a multidimensional space). if you look at the posterior
distribution plotted in figure 6.1, you can see that the true value of the parameter is
probably somewhere around 5, but with some uncertainty. if you were to provide a single
number as a guess of the parameter, you would probably say something close to 5. in
classical statistics, a single number guess is called an    estimate   , and a rule for generating
such guesses is called an    estimator   . estimates are usually written by putting a hat over
the name of the parameter. so, by looking at the plot of the posterior, you could give an
estimate like this:

     = 5.

(6.3)

but there are better things you could do than just looking at the plot, and you   ve probably
learnt some of them in previous statistics courses. here are three methods you could
use to choose a point estimate using the posterior distribution: the posterior mean (the

0246810someparameter0.00.10.20.30.40.50.60.70.8posteriorid203densitychapter 6. summarising the posterior distribution

38

expectation value of the parameter), the posterior median (the value that divides the
id203 in half), and the posterior mode (the value where the posterior distribution
has its peak). in our illustrative example, the values of these three point estimates are:

     = 4.988 (the posterior mean)
     = 4.924 (the posterior median)
     = 4.996 (the posterior mode)

(6.4)
(6.5)
(6.6)

in this example, there   s not much of a di   erence between these three methods. but in other
situations, they can be quite di   erent (this usually happens if the posterior distribution is
skewed, or has multiple modes; you may notice a strong analogy between this topic and
descriptive statistics). is there a way to say which one is the best? it turns out there is,
but that depends on what you mean by    best   .

before we move on to the formal ways of deciding what constitutes a good estimate, i would
like to mention a very common method that is easy to use. if the posterior distribution
looks even vaguely like a normal distribution, it is common to summarise it like this:

   = posterior mean    posterior standard deviation.

(6.7)

i use this kind of summary frequently in my own research.

6.1.1 a very brief introduction to decision theory

decision theory is a very important topic. in this course we will use a tiny amount of it,
just enough to solve the problem of    which point estimate is best?   . if you think about
it, this is a bit of a weird question. obviously, the best point estimate is the true value.
of course it is, how could it be otherwise? our only problem is that we can   t actually
implement this suggestion. we don   t know the true value. we only have the posterior
distribution (which is based on all the evidence we have), and we have to do the best we
can with our incomplete information. to think about which decision is best, the    rst thing
we should think about is which decisions are possible. for estimating a single parameter,
any real number is a possible guess.

the key idea in decision theory is the concept of utility, and the related concept of loss (loss
is just negative utility). utility is a numerical measure of how good it would be if a certain
outcome came true. conversely, loss is a measure of how bad it would be if a certain
outcome came true. utilities are often subjective (not unlike prior probabilities), but in
some applications utility can be more concrete. for example, in betting or investment
decisions the utility can be measured in dollars. the problem with utility is that we
have uncertainty about what is going to happen, or about what is true, so we can   t just
choose the decision that gives us the greatest utility. instead we will use our posterior
probabilities and choose the decision that gives us the maximum possible expected value of
the utility.

imagine we were estimating a parameter    and we wanted to give a point estimate     . one
idea for what the utility or loss might be is the quadratic id168, which is given by

l(  ,     ) =

.

(6.8)

(cid:17)2

(cid:16)           

chapter 6. summarising the posterior distribution

39

this expression inside the parentheses is the di   erence between our point estimate and
the true value. this formula says that if our point estimate is o    by 2, that is four times
worse than if we were o    by 1. if we were o    by 10, that is 100 times worse than if we
were o    by 1, due to the squaring in the quadratic id168 formula.

it turns out (we will prove this below) that if the id168 is quadratic, the best
estimate you can give is the posterior mean. here is the proof. the expected value of the
loss is

e

l(  ,     )

=

p(  |x)(           )2 d  

(6.9)

since we are summing (integrating) over all possible true    values, the expected loss is
only a function of our estimate     . to minimise a function of one variable, you di   erentiate
it and then set the derivative to zero. the derivative is

(cid:104)

(cid:104)

(cid:105)

(cid:105)

(cid:90)

(cid:90)
(cid:90)

=

=

(cid:90)

e

d
d    

l(  ,     )

d
d    

(           )2 d  

p(  |x)
p(  |x)2(           ) d  

     =

  p(  |x) d  .

setting this equal to zero and then solving for      gives the    nal result:

(6.10)

(6.11)

(6.12)

which is the posterior mean. some people call the posterior mean the    bayes estimate   
for this reason. i don   t like that term because i don   t think point estimates are really
bayesian. the actual output of a bayesian analysis is the posterior distribution.

note that i didn   t verify that      actually minimises the expected loss , because setting the
derivative to zero would also    nd a maximum. to make sure it really does minimise the
expected loss, you can calculate the second derivative and verify that it is positive. but
that   s not really needed. it would be pretty bizarre if the posterior mean was the worst
estimate!

6.1.2 absolute loss

sometimes, the quadratic loss/utility is not a reasonable model for the consequences of an
incorrect estimate. another plausible form for the id168 is the absolute loss. this
looks like:

l(    ,   ) = |           |.

(6.13)

with this assumption, the    badness    of an incorrect estimate is proportional to how far
the estimate is from the true value. if the estimate is twice as far from the true value, it is
twice as bad. we will not prove it (although you are welcome to derive this yourself), but
for this id168 the best estimate is the posterior median, which is the value of      for
which p (           ) = p (   >     ) = 0.5.

chapter 6. summarising the posterior distribution

40

6.1.3 all-or-nothing loss

the third kind of id168 we will look at is the    all-or-nothing    loss, also sometimes
called 0-1 loss. sometimes, you may need your estimate to be completely correct, and if
it isn   t correct, then it is irrelevant how far your estimate was from the true value. all
incorrect estimates are equally bad. the all-or-nothing loss looks like:

(cid:26)

l(    ,   ) =

     =   
0
1, otherwise.

(6.14)

if you were in this situation you would want to make your chances as high as possible,
which implies you should simply choose the most probable value of    as your point estimate
    . that is, the appropriate estimate is the posterior mode. this intuition is correct. with
all-or-nothing loss, the best estimate is the posterior mode. the three id168s we
consider in stats 331 are shown in figure 6.2.

figure 6.2: three kinds of id168, which measure how bad it is for our point estimate
     to be di   erent from the true value of the parameter   . note that the all-or-nothing loss
has a small amount of width in this plot, just so that we can clearly see the spike at            
= 0.

6.1.4 invariance of decisions

you may be wondering about the de   nitions of our id168s. for example, we de   ned
the quadratic loss as (          )2, but what if we de   ned it as 3(          )2 + 5 instead? would our
best decision change? luckily, the answer is no. the decision (estimate) which minimises
the expected value of a id168 l also minimises the expected value of a di   erent
id168 al + b, where a is any positive number and b is any other number. for the
mathematicians, the optimal decision is invariant under positive a   ne transformations of
the utility or id168. phew!

   2.0   1.5   1.0   0.50.00.51.01.52.0         0.00.51.01.52.02.53.03.54.0loss(badness)(negativeofutility)threelossfunctionsquadraticlinearall-or-nothingchapter 6. summarising the posterior distribution

41

6.1.5 computing point estimates from a bayes    box

we have just discussed three di   erent point estimates, and under what circumstances we
can consider them to be the best possible estimate we could make. now, we will look at
how to actually obtain the point estimates. the posterior mean is straightforward. it   s
the expectation value of the parameter using the posterior distribution. in r, the code is:

post_mean = sum(theta*post)

you should also know how to compute this manually from a bayes    box, using a calculator.

the posterior mode is also fairly straightforward. first, we can    nd the highest id203
in the bayes    box. then we    nd the corresponding parameter value.

highest_id203 = max(post)
post_mode = theta[post == highest_id203]

in the case of a tie, post mode might be a vector, indicating that there isn   t a single mode.

the posterior median is a little harder. we need to    nd the    value which has 50% of
the id203 to the left and 50% of the id203 to the right. note that this isn   t
precisely de   ned in some cases, particularly with discrete distributions. for example, if   
could be 1, 2, or 3, and the probabilities of these were 0.3, 0.6, and 0.1, then what is the
median? it is not entirely clear. however, if there are a large number of possibilities then
the de   nition becomes more clear.

to calculate the posterior median in r, we need to use the cumulative distribution which
is de   ned as f (t) = p (       t). if we then    nd the value of t where f (t) = 0.5, we have
found the posterior median. this isn   t always possible but we can always    nd the value of
t which makes f (t) very close to 0.5. to obtain the cumulative distribution in r you can
use the cumsum function, which calculates the cumulative sum of a vector. the posterior
vector contains the probabilities of    equalling certain values. if we want the id203
that    is less than or equal to a certain value, we sum all the probabilities up to and
including that value. the cumulative sum function achieves this. here is the code for
calculating the posterior median:

f = cumsum(post)
dist = abs(f - 0.5) # distance of the f-values from 0.5
post_median = theta[dist == min(dist)]

note that this may also produce more than one result. like the mode, the posterior
median is not always uniquely de   ned.

6.1.6 computing point estimates from samples

when we use a bayes    box (or the equivalent r commands which represent the columns
of a bayes    box as vectors), we end up with a vector of possible parameter values and
another vector containing the posterior distribution. when we use mcmc and jags, the

chapter 6. summarising the posterior distribution

42

output is di   erent. we will only have a vector of parameter values, without corresponding
probabilities. the vector of parameter values is meant to be a random sample of values
drawn from the posterior distribution. it   s like saying    here are a bunch of guesses for the
parameter   , and any region where there are a lot of guesses is considered to be a region
with high id203.

when we have samples instead of an exhaustive list of parameter values and probabilities,
the methods for computing the summaries are di   erent. for a parameter called   , the
methods for computing the summaries are given below.

# posterior mean using samples
post_mean = mean(theta)
# posterior mode using samples
# post_mode = ??? (this can   t be done easily with samples!)
# if you have a really large number of samples,
# visually finding the peak of
# a histogram can work.
# posterior median using samples
sorted = sort(theta)
post_median = sorted[0.5*length(theta)]

6.2 credible intervals

credible intervals are another useful kind of summary. they are used to make statements
like    there is a 95% id203 the parameter is between 100 and 150   . the basic idea is
to use the posterior distribution to    nd an interval [a, b] such that

p (a            b|x) =   

(6.15)

where    is some pre-de   ned id203. 95% seems to be the most popular choice. an
example of a 95% credible interval is given in figure 6.3.

note that the interval shown in figure 6.3 is not the only possible interval that would
contain 95% of the id203. however, to make the notion of a credible interval precise,
we usually use a central credible interval. a central credible interval containing an amount
of id203    will leave (1       )/2 of the id203 to its left and the same amount
(1       )/2 of the id203 to its right.

6.2.1 computing credible intervals from a bayes    box

the method for computing credible intervals is closely related to the method for computing
the posterior median. with the median, we found the value of    which has 50% of the
posterior id203 to its left and 50% to its right. to    nd the lower end of a 95%
credible interval, we    nd the    value that has 2.5% of the id203 to its left. to    nd
the upper end we    nd the value of    that has 2.5% of the posterior id203 to its right,
or 97.5% to the left.

chapter 6. summarising the posterior distribution

43

figure 6.3: a central 95% credible interval is de   ned as an interval that contains 95% of
the posterior id203, while having 2.5% of the id203 above the upper limit and
2.5% of the id203 below the lower limit. the credible interval is formed by    nding the
edges of the grey region. in this case the credible interval is [3.310, 7.056].

6.2.2 computing credible intervals from samples

if you have used mcmc and have obtained random samples from the posterior distribution,
you can    nd a credible interval in a similar way to how you would    nd the posterior median.
again, instead of    nding the 0.5 quantile of the posterior distribution you would    nd the
0.025 quantile and the 0.975 quantile (if you wanted a central 95% credible interval).

6.3 con   dence intervals

in previous stats courses you have probably come across the concept of a con   dence
interval. a con   dence interval is a concept in classical statistics that is somewhat similar
to a credible interval in bayesian statistics. when people calculate con   dence intervals,
they usually want to say they are 95% sure that the parameter is in that interval, given the
data. this is what bayesian credible intervals do, but it is not what classical con   dence
intervals do!

luckily, a lot of the time, the classical and the bayesian methods for making intervals will
actually give the same interval. but this isn   t always the case! in lectures we will study
an example (taken from an ed jaynes paper from the 70s) where the bayesian credible
interval and the classical con   dence interval give completely di   erent results. the result is
shown in figure 6.4. the key thing to note is the classical con   dence interval lies entirely
in a region where we are certain (from the data) that    cannot possibly be!

0246810someparameter0.00.10.20.30.40.50.60.70.8posteriorid203density95%2.5%2.5%chapter 6. summarising the posterior distribution

44

figure 6.4: an example of a bayesian credible interval and a frequentist con   dence interval
applied to a particular problem where they give di   erent answers. the posterior distribution
in blue shows that the parameter    is probably somewhere between 11 and 12, and values
above 12 are completely impossible. however, the entire frequentist con   dence interval lies
above    = 12.

chapter 7

hypothesis testing and model
selection

hypothesis testing (also known as model selection, particularly when it is done using
the method in section 7.4) is a very important topic that is traditionally considered a
di   erent topic from parameter estimation. however, in bayesian statistics we will see that
hypothesis testing is basically the same thing as parameter estimation! the one di   erence,
for us, will be that we will sometimes change the prior distribution a little bit.

one big advantage of bayesian statistics is that it uni   es parameter estimation and
hypothesis testing1. that   s good news, because instead of having to understand two
di   erent topics, we only have to understand one!

to see why hypothesis testing is fundamentally the same as parameter estimation, you only
need to understand how parameter estimation works from a bayesian point of view, which
we have already studied. parameter estimation is nothing more than testing a bunch of
hypotheses about the value of the parameter. for example,    = 1 vs.    = 2 vs.    = 3 and
so on. if we have their posterior probabilities, then we   ve tested them.

7.1 an example hypothesis test

suppose we were performing a bayesian parameter estimation analysis using a bayes    box.
here is an example bayes    box with made up numbers:

possible values prior likelihood prior    likelihood posterior

  
1.5
2.0
2.5
3.0

p(  )
0.25
0.25
0.25
0.25

p(x|  )
0.2
0.4
0.6
0.8

totals

1

p(  )p(x|  )

0.05
0.1
0.15
0.2
0.5

p(  |x)
0.1
0.2
0.3
0.4
1

1   uni   es    is a popular word for physicists. it means that two seemingly di   erent topics are fundamen-

tally the same, or at least closely related.

45

chapter 7. hypothesis testing and model selection

46

suppose we wanted to test the following two hypotheses about the parameter   . the    rst
hypothesis h0 is a    null hypothesis   , and the second hypothesis, h1, is an    alternative
hypothesis   .

h0 :
h1 :

   = 2
   (cid:54)= 2

(7.1)
(7.2)

in classical statistics, if you saw a question phrased in this way, you would need to come
up with a test statistic and then calculate a p-value, which tries to say something about
whether the value of the test statistic would be considered extreme, under the assumption
that h0 is true. in bayesian statistics, the only thing we need to do is calculate the
posterior id203 of h0 and the posterior id203 of h1. the posterior id203
of h0 is given by:

p (h0|x) = p (   = 2|x)

= 0.2

(7.3)
(7.4)

all we did here was look up the appropriate number in the bayes    box! the posterior
id203 of h1 is only slightly harder (but still easy) to calculate: h1 will be true if   
takes any value other than 2. therefore, the posterior id203 of h1 is

p (h1|x) = p (   = 1.5        = 2.5        = 3|x)

= p (   = 1.5|x) + p (   = 2.5|x) + p (   = 3|x)
= 0.1 + 0.3 + 0.4
= 0.8.

(7.5)
(7.6)
(7.7)
(7.8)

here we used the fact that everything in a bayes    box is mutually exclusive (only one
of the hypotheses is true) so we could add the probabilities. alternatively, you could
have just noticed that h1 is true if h0 is false. so p (h0|x) + p (h1|x) = 1, which implies
p (h1|x) = 1     p (h0|x).

7.2 the    testing    prior

here we will study a hypothesis testing example that involves a null and an alternative
hypothesis. since the bus example has been used a lot, we will now switch over to a
di   erent example.

suppose it is known that the mean systolic blood pressure in the general population is
120 mm hg, with a standard deviation of 15 mm hg (millimetres of mercury is an old
fashioned unit for pressure, even though it sounds like a unit of length). a new drug is
developed that may be helpful in reducing blood pressure. a sample of n = 100 people
(who can be considered representative of the general population) are given the drug, and
their systolic blood pressure is measured. this results in 100 blood pressure measurements
{x1, x2, ..., xn}, which will be our data. as a shorthand, i   ll sometimes write x (a bold
vector) to denote the data collectively, instead of {x1, x2, ..., xn}.
we are interested in whether the drug works. let    be the mean systolic blood pressure
that would apply in the general population if everyone was taking the drug. our goal is

chapter 7. hypothesis testing and model selection

47

to infer the value of    from the data. in classical statistics, this is sometimes phrased as a
hypothesis test between the two competing hypotheses. we will not be concerned with
the possibility that the drug has the opposite e   ect to what is intended.

h0 :    = 120 (the drug does nothing)
h1 :    < 120 (the drug reduces blood pressure)

suppose the mean of all the data values was

100(cid:88)

xi

  x =

1
n

i=1
= 115.9.

(7.9)

(7.10)

(7.11)

does this data provide evidence against h0 and in favour of h1? in classical statistics
this question would be addressed using a p-value. the p-value would be the id203 of
getting a result this extreme or a result more extreme than what is observed, assuming
that the    null hypothesis    is true. that is,

p-value = p (  x     115.9|h0).

(7.12)

in case you   re curious, the p-value in this case is 0.0031, which is usually taken to mean
that there is fairly strong evidence against h0 and in favour of h1. to calculate the p-value
i had to assume that the id203 distribution for the data values {x1, x2, ..., x100} was
a normal distribution with a known standard deviation of    = 15, and that they were
independent:

xi     n (  ,   2).

(7.13)

in bayesian statistics, p-values are not used. instead, we should think of this as a parameter
estimation problem. we can state a set of hypotheses about the value of   , and then
choose a prior distribution, update to a posterior distribution, etc. then our result will be
the posterior id203 of the null hypothesis, p (h0|x) = p (   = 120|x). this is helpful
because the posterior id203 of the null hypothesis is exactly what we want. it is a
description of how plausible the null hypothesis is given the data. it is not some other
id203 that isn   t really relevant. we can also get the posterior id203 of h1 by
summing the posterior probabilities for all other values of    apart from 120, or by using
p (h1|x) = 1     p (h0|x).
there is only one minor tweak we need to make to make bayesian id136 an appropriate
framework for solving this problem. when the null and alternative hypotheses are written
like we wrote them above, it implies that the value of    that we are calling the    null
hypothesis    is a special value that is especially plausible. to take this into account in our
bayesian analysis we need to make sure the prior distribution recognises there is a special
value of the parameter that we think is extra plausible. when we do this, we will call it a
testing prior. an example of a testing prior and the resulting bayes    box for the blood
pressure problem is given in table 7.1. the r code for calculating these results is given
below.

# parameter values
mu = seq(110, 120)

chapter 7. hypothesis testing and model selection

48

# make the testing prior
prior = rep(0.5/10, 11)
prior[11] = 0.5

# compute the likelihood for the 100 data points.
# the numbers get close to 0, so let   s use logs
log_lik = rep(0, 11)

# use a for loop to loop over all data values
# and multiply the likelihoods
for(i in 1:100)
{

log_lik = log_lik + dnorm(x[i], mean=mu, sd=15, log=true)

}

# rescale the likelihood for readability
lik = exp(log_lik - max(log_lik))*1000
#lik = lik/max(lik)*1000

# calculate the posterior
h = prior*lik
post = h/sum(h)

# the null hypothesis
post[11]

possible values prior likelihood prior    likelihood posterior

  
110
111
112
113
114
115
116
117
118
119
120
totals

p(  )
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.05
0.5
1

p(x|  )
0.44
4.83
34.12
154.64
449.33
837.13
1000.00
756.93
376.15
118.44
23.91

p(  )p(x|  )

0.02
0.24
1.71
7.73
22.47
41.86
50.00
38.30
18.81
5.92
11.96
199.01

p(  |x)
0.0001
0.0012
0.0086
0.0389
0.1129
0.2103
0.2512
0.1924
0.0945
0.0298
0.0601

1

table 7.1: an example of a testing prior for the blood pressure problem. we give more
prior id203 to the special value    = 120 because it is particularly plausible. for
readability i have rescaled the likelihoods so that the maximum is 1000. note that the
posterior id203 of h0 can simply be read o    the table.

the conclusion of our bayesian hypothesis test is that the posterior id203 of h0 is
0.0601. recall that the classical p-value was 0.0031. these numbers are very di   erent, and
there is no reason why they should be similar. the p-value might imply that the evidence

chapter 7. hypothesis testing and model selection

49

is overwhelming (if you are not experienced at interpreting p-values), but the posterior
id203 still says there   s a 6% chance the drug does nothing.

note that the calculation of the posterior distribution uses all of the data values, rather
than reducing the whole data set down to a single number (the sample mean   x). in this
particular example, reducing the whole dataset to a single number is harid113ss2. but in
di   erent situations (e.g.
if your sampling distribution or likelihood was based on the
heavy-tailed cauchy distribution instead of a normal distribution), reducing an entire data
set to a single    test statistic    can be extremely wasteful!

note also that there were some fairly arbitrary decisions made in choosing our testing
prior. we decided not to allow    > 120, but the analysis could also have allowed for
that. the discrete approximation was fairly coarse. finally, we assumed    couldn   t be
lower than 110, and had a uniform prior for all    values apart from 120. some of these
assumptions can and should be questioned when applying bayesian hypothesis testing in
practice. in figure 7.1, there are three possible ideas for what the prior should be in the
blood pressure question. they may all seem somewhat reasonable in this situation, but
could lead to di   erent conclusions.

prior 1 is basically the same as the prior in our bayes    box, although it goes down to
   = 100 and divides the possible    values more    nely. this prior says the null has a 50%
id203, and if    is not equal to 120, then it could be anything. prior 2 is similar,
but has only 30% of the prior id203 on the null hypothesis, instead of 50%, and the
shape of the prior is non-uniform for the lower values of   . this is like saying       could be
precisely 120, and if it   s not precisely 120, then it is probably at least close to 120   . in a
lot of hypothesis testing situations this would be a more accurate description of our prior
beliefs than prior 1. prior 3 isn   t really a testing prior at all (it doesn   t have a spike), but
is just a bell-shaped prior. this is like saying    alright, i would never believe    is exactly
120, but i think there   s a reasonable chance it   s close to 120. often, it would be nonsense
to think the null hypothesis is perfectly true, to an arbitrary level of accuracy. something
like prior 3 would be more appropriate. these three priors would all give di   erent results,
and the appropriate choice depends on the individual problem you are solving.

remember, if the conclusions depend sensitively on the choice of the prior
distribution, that is an important    nding. you either need to be really
careful about choosing your prior, or you need more data.

7.3 some terminology

there is some alternative terminology that is widely used and is particularly popular
in bayesian hypothesis testing (aka model selection) problems. suppose there were two
hypotheses h1 and h2, and some data x. now, h1 and h2 might be a null and alternative
hypothesis, or they might be two particular values of the parameter, or something else.

2in this problem, the sample mean is a    su   cient statistic   : deleting all of the data and using just the

mean has no consequences!

chapter 7. hypothesis testing and model selection

50

figure 7.1: three possible priors we could use for the blood pressure question. all may
seem    reasonable   , and the choice can a   ect the results (quite signi   cantly in some cases).
care should be taken when choosing the prior in hypothesis testing problems.

two repetitions of bayes    rule for these two hypotheses are:
p (h1)p(x|h1)

p (h1|x) =

p(x)

p (h2|x) =

p (h2)p(x|h2)

p(x)

these could also be written in words:

posterior =

prior    likelihood
marginal likelihood

(7.14)

(7.15)

(7.16)

dividing these two equations gives the odds form of bayes    rule, which deals with ratios of
probabilities instead of probabilities themselves.

in words, this can be written as:

p (h1|x)
p (h2|x)

=

p (h1)
p (h2)   

p(x|h1)
p(x|h2)

posterior odds = prior odds    bayes factor

(7.17)

(7.18)

sometimes people talk about odds (or odds ratios, which are the same thing) and bayes
factors instead of about prior and posterior probabilities. the odds tell us how plausible
h1 is compared to h2. for example, a posterior odds ratio of 5 means h1 is 5 times as
plausible as h2. of course, odds can be greater than 1 even though probabilities cannot
be. the bayes factor is the ratio of the likelihoods. results are often quoted as bayes
factors because that   s the part of the equation where the data is important. if you say
   the bayes factor for h1 over h2 was 10   , then whoever you   re talking to is free to apply
whatever prior odds they like, whereas if you state the posterior odds then people may
wonder what your prior odds were.

100105110115120parameter  0.00.10.20.30.40.5priorid203threedi   erentpriorsprior1prior2prior3chapter 7. hypothesis testing and model selection

51

7.4 hypothesis testing and the marginal likelihood

the bayes    factor in equation 7.17 is the ratio of likelihoods for two hypotheses h1 and h2.
if we wanted to calculate the bayes factor for h0 and h1 in the blood pressure example,
we could easily get the likelihood for h0 (it   s right there in the bayes    box). but how
would we get p(x|h1), which needs to be a single number? h1 is the statement    = 100
or    = 111 or    = 112 and so on up to    = 119.

imagine we had left    = 120 out of our bayes    box and just done parameter estimation
within the context of h1 (i.e. assuming, for argument   s sake, that h1 is true). this would
involve a reduced bayes    box with one less row in it. we would end up getting some

marginal likelihood p(x) =(cid:80) p(  )p(x|  ). the key is to realise that since we are assuming
h1 throughout, the marginal likelihood is really p(x|h1) =(cid:80) p(  |h1)p(x|  , h1), which is

exactly the thing we need to calculate the bayes factor!

all of this implies there are two mathematically equivalent ways of doing bayesian
hypothesis testing, or model selection. one is to make a big model that includes both the
null and the alternative hypothesis. the bayes    box with a testing prior accomplishes this.
in most cases this is the most convenient way to do the calculations.

the other way is to do the two analyses separately. first, do parameter estimation within
the context of h1. then, do parameter estimation within the context of h2. then, use
the marginal likelihoods as if they were likelihoods, to compare h1 vs. h2. this second
way of calculating bayes factors is most useful when the two analyses were actually done
separately by di   erent people.

chapter 8

id115

8.1 monte carlo

monte carlo is a general term for computational techniques that use random numbers.
monte carlo can be used in classical and bayesian statistics. a special kind of monte carlo
called id115 (mcmc) was one of the main reasons for the revival of
bayesian statistics in the second half of the 20th century. before mcmc became popular,
one of the major drawbacks of the bayesian approach was that some of the calculations
were too hard to do. mcmc enables us to solve a wide range of bayesian problems which
cannot be solved using analytical methods.

8.1.1 summaries

so far, we have represented our id203 distributions (prior and posterior) in a computer
by using a vector of possible parameter values and a corresponding vector of probabilities.
for example, suppose we have a single parameter    and we have worked out the posterior
distribution by using a bayes    box. this will give us a vector theta of possible    values
and a corresponding vector post containing the posterior probabilities. well, one thing
we could do is plot the posterior distribution, resulting in a plot like the one in figure 8.1.

plot(theta, post, xlab="theta", ylab="posterior id203")

if we want to obtain some summaries, we could do it like so:

post_mean = sum(theta*post)
post_sd = sqrt(sum(theta^2*post) - post_mean^2)

however, there is an alternative way of representing this posterior distribution in a
computer. it may not be immediately obvious why this is a good idea, because there
is nothing wrong with the tried and true method we have used so far. but this second
method has the advantage that it continues to work well on much bigger problems, such
as when we have more than one parameter. with more than one parameter, the    vector of
possible solutions    approach can fail very dramatically.

our new way of representing a id203 distribution in a computer will be via monte

52

chapter 8. id115

53

figure 8.1: a posterior distribution can be represented in a computer by a discrete set of
possible parameter values, and the corresponding probabilities.

carlo samples. instead of having two vectors (one of    values and one of the corresponding
probabilities), imagine we had some method to compute a random sample of    values,
drawn from the posterior distribution in figure 8.1. there would only be one vector
theta. so how would we know there is greater id203 around    = 1? well, more
elements of the theta vector would be near 1. instead of carrying around a second vector
of probabilities, we understand that more probable regions will simply contain more points.
say our vector of random samples is called theta. then we can look at the posterior
distribution by plotting a histogram of samples:

hist(theta, breaks=100)

the histogram looks something like the one in figure 8.2. we can also get our summaries,
but the code looks di   erent (it   s actually easier than before!):

post_mean = mean(theta)
post_sd = sd(theta)

because of the randomness involved in generating the theta values, the summaries aren   t
exact. for example, i know the actual posterior mean and standard deviation in this
example were both 1, but the values obtained from the monte carlo samples were 0.9604
and 1.0008, respectively. however, this doesn   t matter much because the results indicate   
is probably somewhere around 1, with an uncertainty of about 1. the error introduced by
using random samples is much smaller than the amount of uncertainty inherent in the
posterior distribution itself. for example, if i summarised the posterior distribution by
saying    = 0.9604    1.0008, for almost all practical purposes the conclusion is exactly the
same as the true version of the summaries    = 1    1.
in this discussion, we haven   t answered the question of how to actually generate random
samples of    from the posterior distribution. this is the job of id115.

the purpose of id115 is to generate random samples
of parameter values drawn from the posterior distribution. this makes it

   10   50510theta0.000.010.020.030.040.050.060.070.08posteriorid203chapter 8. id115

54

figure 8.2: the posterior distribution for a parameter    can also be represented by a
random sample of    values drawn from the posterior distribution. some    values are more
probable than others, which is encoded by certain values appearing more frequently in the
sample.

very easy to compute summaries even if you have more than one unknown
parameter.

8.2 multiple parameters

mcmc becomes extremely useful when we begin to look at bayesian models involving
more than one unknown parameter. having posterior samples makes the process of
marginalisation much easier. imagine we wanted to infer two parameters, called a and
b, from data x. bayes    rule (parameter estimation version) would give us the posterior
distribution:

p(a, b|x)     p(a, b)p(x|a, b)

(8.1)

however, what if you didn   t really care about the value of b but only really wanted to
measure a? the terminology for this is that b is a nuisance parameter: you need it to
de   ne the model, but ultimately you are not really interested in knowing its value. what
you need in this case is the marginal posterior distribution for a (that is, the posterior
distribution for a on its own, not the joint distribution with b). this can be obtained
using the sum rule. the result is:

or

p(a|x) =

p(a|x) =

p(a, b|x) db

p(a, b|x)

(8.2)

(8.3)

(cid:90)
(cid:88)

b

b

   3   2   1012345theta0510152025303540numberchapter 8. id115

55

depending on whether the set of possible b values is continuous or discrete. before mcmc,
these integrals or sums usually couldn   t be done without making certain choices purely
for mathematical convenience (e.g. choosing the prior to be a certain kind of distribution
only because it will make the maths work out, rather than it being a good description of
your prior beliefs).

samples of parameter values drawn from the posterior distribution (achieved using mcmc)
make this hard problem much easier. we no longer need to worry about mathematical
convenience. see figure 8.3 for an example showing how monte carlo sampling makes
marginalisation trivial.

figure 8.3: an example of a posterior distribution for two parameters, a and b. the left
panels show the joint posterior distribution (which has a correlation) and the marginal
posterior distribution for a, obtained by integrating over all the possible b values. the top
right panel contains random samples (points) drawn from the posterior distribution for a
and b. the only step needed to get the marginal distribution for a (lower right panel) is to
ignore the b-values of the points!

   4   2024a   4   2024bjointposteriordistribution   4   2024a   4   2024bjointposteriordistribution   4   2024a0.00.20.4id203densitymarginalposteriordistribution   4   2024a01020304050numberofsamplesmarginalposteriordistributionchapter 8. id115

56

8.3 the metropolis algorithm

the metropolis algorithm is the most basic mcmc method. the ideas behind it are
fairly simple, yet metropolis forms the basis of a large number of more advanced mcmc
methods. in stats 331 we will study the basic ideas behind how the metropolis algorithm
works, which will involve a small amount of markov chain theory. we will also look at
a small amount of r code which implements the metropolis algorithm, but for solving
practical problems it is more convenient to use the jags program1.

the metropolis algorithm was invented in the 1950s by physicists (including nicholas
metropolis, for whom the algorithm is named), who used it to do calculations in the    eld
of statistical mechanics. this intriguing    eld focuses on calculating the macroscopic (large
scale) properties of matter from knowledge of the small-scale properties. for example, if
you know water is composed of h2o molecules, you can use statistical mechanics to    gure
out what will happen if you have a lot of water molecules. for example, it will freeze at 0
degrees celsius and boil at 100 degrees celsius.

it took many decades before people started to realise the metropolis algorithm was useful
in bayesian statistics as well as statistical mechanics. the bayesian approach seemed very
elegant and useful to many people, but it could always be criticised as unworkable, because
you usually had to do di   cult or impossible integrals when solving practical problems
(to summarise the posterior, or to get rid of nuisance parameters). mcmc changed all
that, and is one of the reasons for the explosion in the popularity of bayesian statistics
beginning in the 1990s.

the basic idea of mcmc is that we want a method which will travel between di   erent
possible states (such as the possible hypotheses/parameter values in a bayesian analysis).
we want the amount of time spent in any particular state to be proportional to the
posterior id203 of the state. the computer    explores    the set of possible parameter
values, spending a lot of time in the regions with high posterior id203, and only
rarely visiting regions of low posterior id203. figure 8.4 shows an example of mcmc
applied to a problem with only two possible hypotheses or parameter values. nobody
would actually use mcmc on such a small problem, but it is helpful for explaining how
mcmc works.

8.3.1 metropolis, stated

the metropolis algorithm is given below. the    rst thing to do is start somewhere in the
   parameter space    (set of possible parameter values). you then propose to move somewhere
else. there is an acceptance id203    that determines whether to accept the proposal.
if the proposal is better (h, the prior times likelihood value is higher), then you accept
it, and the proposed state becomes the new state of the algorithm. if the proposal is
worse, you can also accept it, but the id203 of accepting is given by the ratio of the
unnormalised posterior probabilities, i.e. h(cid:48)/h. for example, if the proposed point is 1/3
as good as the current point, the acceptance id203 is 1/3. if the proposed point is
rejected, the original point remains the state of the algorithm, and gets counted again in

1jags uses a number of mcmc methods internally, including metropolis,    id150   , and

   slice sampling   , which we will not study in this course.

chapter 8. id115

57

figure 8.4: an illustration of the basic idea behind mcmc. imagine we had a bayes   
box with two possible hypotheses, and we knew the prior    likelihood values. the
amount of time the mcmc program will spend in each state is proportional to the posterior
id203 of the state. in this example, the mcmc algorithm was in state 1 three times
and in state 2 seven times. using this, we could estimate the posterior id203 of state
2 as being 0.7. this estimate would become more accurate if we ran the mcmc for more
iterations.

the results.

the metropolis algorithm works because it makes transitions towards low id203
states rare, while transitions towards high id203 states are common, because of the
acceptance id203. it is hard to move into an improbable state, so not much time will
be spent there. over time, the fraction of time spent in any given state is equal to the
posterior id203 of the corresponding hypothesis.

    start in some state   
    generate a    proposal    state   (cid:48) from a proposal distribution q (assumed symmetric

so that q(  (cid:48)|  ) = q(  |  (cid:48)))

chapter 8. id115

58

    with id203    = min(1, h(cid:48)/h), replace the current state with the proposed

state

    repeat

8.4 a two state problem

we will now study how the metropolis algorithm works on a very simple example, namely
the two-ball problem from the beginning of the notes. there are two hypotheses and the
posterior probabilities are 1/3 and 2/3. it is important to note that mcmc is not actually
needed for a problem this simple, but it is a good test case to see precisely how an mcmc
algorithm works. when we solve real data analysis problems with jags, we won   t have to
think too much about how the mcmc works, but can concentrate instead on the bayesian
statistics problem at hand.

let   s call the less probable hypothesis    state 1    and the more probable hypothesis    state
2    for the purposes of this section. what we need is a markov process that will spend 1/3
of the time in state 1 and 2/3 of the time in state 2. the metropolis algorithm described
above will do what we need. the main thing we need to compute is the acceptance
id203   ij for a proposed transition from state i to state j where i, j     {1, 2}. the
acceptance id203    for a proposed move from state i to state j is given by

  ij = min

1,

(8.4)

where hi and hj are proportional to the posterior probabilities of states i and j respectively.
if the proposal is to move to an equal or better (higher posterior id203) state (hj     hi)
then the acceptance id203 is 1. if the proposal is to move to a less probable state
then the acceptance id203 is hj/hi, the ratio of the two posterior probabilities2.

the transition id203 is the id203 of being in state j at the next iteration given
that you are in state i at the current iteration. the transition id203 is given by the
product rule:

pij = qj  ij

(8.5)

for i (cid:54)= j. the transition matrix of the markov chain is a matrix with all the di   erent pij
values in it:

(cid:18)

(cid:19)

hj
hi

(cid:21)

(cid:20) p11 p12

p21 p22

1

2    1

1

2    1

2

(cid:21)

p =

(cid:20)

p =

(8.6)

(8.7)

in our particular case, we can work out the o   -diagonal elements of p using equation 8.5:

2note that this algorithm can be used even if the marginal likelihood is unknown, because only ratios
of posterior probabilities are needed. this is useful because the marginal likelihood is sometimes very
hard to calculate in multi-parameter problems.

chapter 8. id115

59

the diagonal elements can be found by knowing the rows of p must sum to 1. we must
be in some state at the next iteration. therefore the transition matrix of our markov
chain in this two-state problem is:

(cid:20) 1

(cid:21)

1
2
3
4

p =

2
1
4

(8.8)

a markov chain with a small number of possible states can be represented graphically
using a transition diagram, as in figure 8.5.

figure 8.5: a transition diagram for a markov chain with two possible states. the states
(which correspond to hypotheses in bayesian id136) are drawn as circles labelled    1   
and    2   . when the algorithm is in a particular state (e.g. state 1), and we apply one step
of the metropolis algorithm, the id203 it will be in state 1 is p11 and the id203 it
will be in state 2 is p12. mcmc works by making it easy to move into states with high
posterior id203, and hard to move out of them.

8.5 the steady-state distribution of a markov chain

once we have the transition matrix, we can work out the steady state distribution of the
markov chain. imagine, instead of starting the mcmc from an arbitrary initial state, you
have a id203 distribution for the initial state. after applying one iteration of the
metropolis algorithm, the id203 distribution for the updated state will usually be
di   erent from the id203 distribution for the initial state.

however, one special id203 distribution, called the steady state distribution, does
not change after you apply an mcmc update. if your initial point was drawn from the
steady state distribution, then subsequent points will also be drawn from the steady state
distribution. in mcmc the steady state distribution should be the same as the posterior
distribution.

imagine we are using mcmc on our two-state problem, and our initial position is state
1 with id203 v1 and state 2 with id203 v2. what is the id203 of being
in state 1 at the next iteration? there are two ways for that to happen: by starting in
state 1 and then making a transition from 1     1, or by starting in state 2 and making a
transition from 2     1. the total id203 is then:

p (state 1 after iteration) = v1p11 + v2p21.

(8.9)

chapter 8. id115

60

similarly for state 2:

p (state 2 after iteration) = v1p12 + v2p22.

(8.10)

if v1 and v2 happened to be the steady state distribution, then these probabilities would
also be v1 and v2. this gives us two simultaneous equations

v1 = v1p11 + v2p21
v2 = v1p12 + v2p22

(8.11)
(8.12)

which can also be written in matrix form as vp = v where v = (v1, v2) and p is the
transition matrix3. these two simultaneous equations can be solved for v1 and v2. however
there is a third constraint, that is v1 + v2 = 1. since there are three equations but two
unknowns, it seems like the problem might be    over-determined   , but that is not actually
the case because the transition matrix rows are not all linearly independent. it is left
as an exercise for the reader to show the steady state distribution for our markov chain

(equation 8.8) is in fact equal to the posterior distribution, so v =(cid:0) 1

(cid:1).

3, 2

3

8.6 tactile mcmc

in class we will do    tactile mcmc   , which is an implementation of the metropolis
algorithm using coins and dice instead of the random number generators provided in
computer software such as r. this is a good way to get a feel for the    ow of mcmc
algorithms.

in stats 331, when we want to use mcmc in practice, we will use the jags program
rather than using metropolis directly. jags is a general purpose mcmc program which
allows you to solve fairly complex problems without a large amount of programming.

3mathematics students might recognise this equation, which says v is the left-eigenvector of p, with
eigenvalue 1. an alternative is to write the stationary distribution as a column vector and use pt v = v.

chapter 9

using jags

jags stands for    just another gibbs sampler   . jags is a computer program that
allows the user to implement id115 (mcmc) on fairly complicated
problems very quickly. the    id150    mentioned in the name of jags is a
particular mcmc technique that is beyond the scope of this course: however, it is not so
di   erent from the metropolis-hastings method that we do study in this course. if you are
faced with a statistics problem which you would like to solve in a bayesian way, jags
makes it very straightforward to implement a bayesian model. essentially, you just need
to tell jags the following things:

    the names of your unknown parameter(s), and their prior distributions
    the likelihood

then we simply load in the data and let it go! jags will run mcmc, automatically
choosing an appropriate starting point based on your prior, and moving the parameter
values around so the posterior distribution is well sampled. in this chapter we will see
how to use jags with a simple example, and we will see some more features of jags
when we come to study more complex examples. there are some more advanced features
of jags that we will not use in this course.

jags is not the    rst program of its kind, and it is related to many other available
programs. starting in the late 1980s, a program called bugs (bayesian id136 using
id150) was developed, and this evolved into the program winbugs. these
programs had a huge e   ect on the uptake of bayesian statistics, by dramatically reducing
the amount of work it takes to get an mcmc simulation to run. instead of having to
code up your own implementation of the metropolis algorithm or an alternative mcmc
method, all you had to do was tell winbugs what your prior and likelihood were, and it
would automatically use appropriate and sophisticated mcmc methods.

up until 2012, stats 331 was taught using winbugs. however, there are a number
of disadvantages to winbugs, so i decided to switch over to jags in 2013. the main
advantages of jags over winbugs are: i) it is open source software and works on all
operating systems, ii) it allows a more concise version of notation that can save a lot
of space and make the code easier to read and write, and iii) winbugs is not actively
developed or maintained any more. in addition, a lot of time in previous iterations of 331
was spent teaching di   erent ways of using winbugs (i.e. calling it from r, vs. using the

61

chapter 9. using jags

62

graphical interface, vs. writing a script). in 2013 we will use jags in just one way (by
calling it from r), which frees up time for us to concentrate on the stats! there is another
up to date bugs program called openbugs, but it too is only a windows program.
most of what you learn about jags can be transferred to winbugs or openbugs quite
easily, if necessary, with only minor changes.

9.1 basic jags example

since we have used it a lot already, it makes sense to look at how the bus problem looks
in jags. recall we had a single unknown parameter   , with a uniform prior between 0
and 1. we also had a binomial likelihood, which we could write as x     binomial(n,   ).
to implement this model in jags, the code looks like this:

model
{

# parameters and the priors
theta ~ dunif(0, 1)

# likelihood
x ~ dbin(theta, n)

}

as in r, comments (statements that have no e   ect but help to annotate the code) can be
written with a # symbol. the names of the distributions in jags are very similar to (but
not always exactly the same as) the names of the r functions for evaluating the id203
densities or mass functions. in this example, dunif is the uniform distribution and dbin
is the binomial distribution. one    nal thing to note is the order of the parameters in the
binomial distribution. in jags the success id203 comes    rst and the number of
trials comes second. there are some quirks with the other distributions as well, such as
the normal distribution, which we will see in later chapters.

the code for implementing a bayesian model in jags belongs inside a model{ } block. in
our example, the    rst statement inside the model is theta ~ dunif(0, 1). as you can
probably guess, theta is simply the name of our parameter. we are free to name it as we
wish, and in di   erent situations we will give the parameters di   erent names. the tilde sign
is like the           notation for id203 distributions. we are about to specify a id203
distribution that applies to theta. finally, the actual distribution is given, which is a
uniform distribution between 0 and 1. note the command used for a uniform distribution
is dunif and not uniform. our line of code theta ~ dunif(0, 1) tells jags there is a
parameter called theta and we want to use a uniform prior between 0 and 1.

the notation for the likelihood is very similar. we write the name of the data followed by
   ~    and then the distribution, in this case the binomial distribution. one annoying this
about the binomial distribution is the ordering of the parameters. usually people write
the number of trials    rst and the success id203 second, but in jags it   s the other
way around.

interestingly, the likelihood part of the code looks exactly like the prior part. so how does

chapter 9. using jags

63

jags know that x is data and not just another parameter? well, when we call jags
from r, we will pass to it an r list containing the data. there will be a value for x in this
list, which tells jags that it is a    xed and known quantity, and not another unknown
parameter like theta.

above, we speci   ed the jags model, but this isn   t the only thing we need. we also need
a way to actually run jags! the most convenient way to use jags is to call it from r,
using the r library rjags. this way, the output (samples from the posterior distribution
for the parameters) is available in r for postprocessing such as plots and smmaries. the
rjags library has many features and options, and it can be a bit overwhelming to    gure
out all of them using the documentation. therefore, i have written a template r script
called use jags.r where you can specify the data, the jags model, and some options at
the top of the    le, and you do not have to worry about all the functions for calling jags
from r.

the    rst part of use jags.r is given below. this is the part you can modify to load
di   erent data, change the model assumptions, and decide how long (for how many iterations
or steps) you would like the mcmc to run for.

the burn-in is an initial part of the mcmc run where results are not saved. this is
bene   cial because sometimes it can take a while for the mcmc to locate the regions of
high posterior id203, and if you include the initial parts of the run in you results,
you can get incorrect answers. for most of our models in stats 331, we do not need a
long burn-in period.

model = "model
{

theta ~ dunif(0, 1)
x ~ dbin(theta, n)

}
"

# the data (use na for no data)
data = list(x=2, n=5)

# variables to monitor
variable_names = c(   theta   )

# how many burn-in steps?
burn_in = 1000

# how many proper steps?
steps = 10000

# thinning?
thin = 1

the second part of use jags.r actually runs jags. you won   t need to edit this or know
much about it, but for completeness, here it is:

# no need to edit past here!!!

chapter 9. using jags

64

# just run it all and use the results list.

library(   rjags   )

# write model out to file
fileconn=file("model.temp")
writelines(model, fileconn)
close(fileconn)

if(all(is.na(data)))
{

m = jags.model(file="model.temp")

} else
{

m = jags.model(file="model.temp", data=data)

}
update(m, burn_in)
draw = jags.samples(m, steps, thin=thin, variable.names = variable_names)
# convert to a list
make_list <- function(draw)
{

results = list()
for(name in names(draw))
{

# extract "chain 1"
results[[name]] = as.array(draw[[name]][,,1])

# transpose 2d arrays
if(length(dim(results[[name]])) == 2)

results[[name]] = t(results[[name]])

}
return(results)

}
results = make_list(draw)

when this code is executed, it creates an r list called results. inside results, there
is a vector for each variable that you chose to    monitor    by listing its name in the
variable names vector. in this example there is only one parameter, so it seems obvious
we would like to monitor it. in more complex situations there may be many parameters,
and only some of them are actually interesting, the others are    nuisance parameters   . the
variable names vector allows you to choose just the parameters you really care about.
notice also the various options such as the number of steps, and the thin option. if thin
is set to 10, for example, only every 10th iteration of the mcmc will appear in the results.
this is useful for keeping the size of the results list manageable, even if you run the
mcmc for a very long time.

one of the most important things to check after running jags is a trace plot of the
parameters. a trace plot is a plot of the value of the parameter over time, as the mcmc
was running. to plot a trace plot of the mcmc run, we can simply use the following code,
for a parameter called theta. if the parameter has a di   erent name, replace theta with

chapter 9. using jags

65

the actual name of the parameter.

plot(results$theta, type=   l   )

you could look at the posterior distribution using a histogram, and you can compute
summaries using the methods discussed in chapter 6. the code for the histogram for a
parameter theta is given below.

hist(results$theta, breaks=100)

examples of a trace plot and a histogram are given in figure 9.1. trace plots are the best
diagnostic tool for seeing whether mcmc is working properly. ideally, trace plots should
look like    noise   , without strong correlations between one point and the next. if there are
strong correlations in the trace plot, the mcmc will need to be run for a longer period of
time to obtain e   ectively independent samples from the posterior distribution.

figure 9.1: the trace plot and histogram of the mcmc samples returned by jags. the
trace plot (top) is zoomed in on the    rst 500 samples, and shows the theta value moving
around as the mcmc proceeds. the histogram of theta values, sampled from the posterior
distribution, is given in the lower panel. you can compare this with figure 5.1.

9.2 checklist for using jags

when running jags using the use jags.r script provided, there are several things you
need to ensure. these are listed below.

    the data you want to analyse must be contained in an r list called data. inside
this list you should also include variables such as the size of the data set (which i
usually call n), or any other known constants that are referred to in the model.

0100200300400500iteration0.00.20.40.60.81.0theta0.00.20.40.60.81.0theta050100150200250frequencychapter 9. using jags

66

    the jags model must be correctly written inside the r string called model. in the
likelihood part of the jags model, you must ensure that the names of the data
variables match the names in your data list. in the example of this chapter, the
number of successes is called x in both the data list and in the jags model.

    the variable names vector, which lists the parameters you are interested in, can
only list parameters that actually exist in the jags model. if you try to monitor a
variable that isn   t in your model, you will get an error message.

chapter 10

regression

regression is a very important topic in statistics that is applied extremely frequently.
there are many di   erent kinds of regression, but in stats 331 we will mostly focus
on id75. this gives us a nice familiar example example to demonstrate how
bayesian statistics works and how it is di   erent from classical or frequentist statistics.
here we will study an example of a simple id75 problem taken from stats
20x.

10.1 a simple id75 problem

data were collected from a sample of 30 drivers. the age of the driver and the maximum
distance at which they could read a newly designed road sign were recorded. it is of
interest to build a simple model that can be used to predict the maximum distance at
which the sign is legible, using the age of the driver. figure 10.1 shows the data. the
purpose of simple id75 is to    nd a straight line that goes throught the data
points. the slope and intercept of the straight line are then helpful for understanding the
relationship between the variables. also, the straight line can be used to predict future
data, such as the maximum distance at which a 90-year-old person could read the sign.
the most common method used to obtain the straight line is to    nd the line (i.e. the
slope and intercept values) which    ts best by the criterion of    least squares   .

10.2

interpretation as a bayesian question

from what you now know about bayesian statistics, you might be able to come up with
some reasons why the standard least squares solution is unsatisfactory. one glaring issue
is that the data are hardly ever going to be good enough to tell us with certainty that a
particular slope and intercept are correct (the exception would be if three or more points
lie perfectly on a straight line, with no scatter). in principle, we will almost always have
uncertainty about the slope and the intercept. from a bayesian perspective, our goal is
not to    nd a point estimate for the slope and the intercept. instead we should calculate
the posterior distribution for the slope and the intercept, given the data. the posterior

67

chapter 10. regression

68

figure 10.1: the maximum distance at which a person can read a road sign vs. the age of
the person. there are n = 30 data points. you can clearly see that older people have, on
average, worse eyesight. simple id75 can be thought of as       tting a straight
line    to the data.

distribution will tell us exactly how much uncertainty we have. if we do want summaries
for convenience, we can use the posterior distribution to create the summaries, as discussed
in chapter 6.

the equation for a straight line is usually written as y = mx + b where m is the gra-
dient/slope and b is the intercept. however, for consistency with later, more complex
regression models, we will write the equation as:

y =   0 +   1x.

(10.1)

here,   0 is the y-intercept and   1 is the slope. our goal is to calculate the posterior
distribution for   0 and   1 given the data.

10.3 analytical solution with known variance

bayes    rule (parameter estimation version) tells us how to calculate the posterior distribu-
tion:

p(  |x)     p(  )p(x|  )

(10.2)

this is the generic form for parameters    and data x. in our particular case, the unknown
parameters are   0 and   1, and the data are the y values of the data points. the data also
consist of a number n of points and the x-values, but we shall assume that these on their
own provide no information about the slope and intercept (it would be a bit strange if they
did). so the x-values and the number of points n act like prior information that lurks    in
the background    of this entire analysis. the y-values are our data in the sense that we
will obtain our likelihood by writing down a id203 distribution for the y-values given
the parameters.

020406080100age(years)0100200300400500600700800distance(metres)chapter 10. regression

69

therefore, bayes    rule for this problem (i.e. with the actual names of our parameters and
data, rather than generic names) reads:

p(  0,   1|y1, y2, ..., yn )     p(  0,   1)p(y1, y2, ..., yn|  0,   1)

(10.3)

we can now say some things about bayesian id75 by working analytically. for
starters, let   s assume uniform priors for both   0 and   1, and that the prior for these two
parameters are independent. the id203 density for a uniform prior distribution can
be written simply as:

p(  0,   1)     1.

(10.4)

note that we have written proportional instead of equals. if we decided to place the limits
at -500 and 500 (say) then the actual value of the density would be 10   6. but this is just
a number and in the end, when we normalise the posterior distribution, it won   t matter.
we can even imagine making our prior    in   nitely wide   , which is called an improper prior.
in many cases simply writing p(  0,   1)     1 will not cause any problems. we are assuming
the prior id203 density is uniform over a very wide range which we will not specify.

now, on to the likelihood. there are n data points and so there are n y-values in the
dataset, called {y1, y2, ..., yn}. we can obtain the likelihood by writing down a id203
distribution for the data given the parameters, sometimes called a    sampling distribution   .
this describes our beliefs about the connection between the data and the parameters,
without which it would be impossible to learn anything from data. if we knew the true
values of   0 and   1, then we would predict the y-values to be scattered around the straight
line. speci   cally we will assume that each point departs from the straight line by an
amount  i which has a n (0,   2) id203 distribution. for now, we will assume   , the
standard deviation of the scatter, is known. in           notation, this can be written as:

(10.5)

(cid:21)

yi     n (  0 +   1xi,   2).
(cid:20)

n(cid:89)

1

     2  

exp

   

i=1

it is implied that all of the data values are independent (given the parameters). therefore
the likelihood can be written as a product of n normal densities, one for each data point:

p({y1, y2, ..., yn}|  0,   1) =

1
2  2 (yi     (  0 +   1xi))2

.

(10.6)

remember, when we combine the likelihood with the prior using bayes    rule, we can
usually ignore any constant factors which do not depend on the parameters. this allows
us to ignore the    rst part of the product, outside the exponential (since we are assuming
   is known).

p(  0,   1|y1, y2, ..., yn )     p(  0,   1)p(y1, y2, ..., yn|  0,   1)

n(cid:89)
(cid:34)

i=1

(cid:21)

(cid:20)
(cid:35)
1
2  2 (yi     (  0 +   1xi))2
n(cid:88)

   

(yi     (  0 +   1xi))2

.

i=1

exp

1
2  2

    1   

    exp

   

(10.7)

(10.8)

(10.9)

we have just found the expression for the posterior distribution for   0 and   1. this is
a distribution for two parameters (i.e.
it is bivariate). it is not easy to interpret this

chapter 10. regression

70

equation just by looking at it, but we could use it to work out the value of the posterior
id203 density for any possible values of   0 and   1

there are a few things you may notice about the posterior distribution in equation 10.9.
firstly, the way it depends on the parameters is an exponential of something involving   0
and   1 in linear and second-order ways (if you were to expand the square, you would get
terms like   0  1 and   2
0). mathematicians would call the expression inside the exponential
a quadratic form. when a id203 density can be written as the exponential of a
quadratic form, it is a normal density. therefore, the posterior distribution for   0 and   1
is a (bivariate) normal distribution.

we can obtain some more insight about this problem by inspecting the sum term inside
the exponential in the posterior distribution (equation 10.9). the sum is over all the data
points, and what is being summed is the di   erence between the data value yi and the
straight line prediction   0 +   1xi, all squared. the sum term is just the sum of squared
residuals that is minimised when solving this problem by    least squares   . in classical
least squares id75,   0 and   1 are estimated by minimising this sum of squared
residuals. because of the exp and the minus sign, the posterior distribution is telling us
that the choice of   0 and   1 that minimises the sum of squared residuals, maximises the
posterior id203 density. other values of the parameters that don   t quite minimise
the sum of squared residuals are somewhat plausible, and the form of the posterior density
tells us exactly how much less plausible they are. the take home message is summarised
below.

doing a id75 by least squares is equivalent to having a uniform
prior and a normal likelihood, and    nding the posterior mode. if you think
this is appropriate in a particular application, and you are happy with just
a point estimate, then classical least squares    tting will be    ne. otherwise,
you   d better do a bayesian analysis.

while classical regression results may come with    standard errors   , these are not the same
as a posterior distribution. a posterior distribution describes the uncertainty about the
parameters given the speci   c data set you actually have. standard errors describe how
di   erent your point estimate would be if your data set was di   erent.

10.4 solution with jags

the above analytical results made the unrealistic assumption that the standard deviation
  , of the scatter, was known. in practice,    usually needs to be estimated from the data
as well. therefore, in the bayesian framework, we should include it as an extra unknown
parameter. now we have three unknown parameters instead of two. our parameters are
now   0,   1, and   . one major advantage of mcmc is that we can increase the number of
unknown parameters without having to worry about the fact that the posterior distribution
might be hard to interpret or plot.

the data is the same as before, {y1, y2, ..., yn}. the likelihood is also the same as before:
(10.10)

yi     n (  0 +   1xi,   2).

chapter 10. regression

71

our three parameters will need priors. jags requires proper priors (i.e. we can   t have a
uniform prior over an in   nite range), but we can still make our priors very wide. instead
of using uniform distributions this time, we will use normal distributions with a mean of 0
and a large standard deviation of 1000.

for the standard deviation parameter   , we know    rstly that this cannot be negative. we
will use a    log-uniform    prior with generous lower and upper limits, to express uncertainty
about the order of magnitude of   . this prior implies things like p (1 <    < 10) =
p (10 <    < 100) = p (100 <    < 1000), which is sometimes a good description of a large
amount of uncertainty about a positive parameter. the easiest way to implement this in
jags is to actually use log(  ) as the parameter, with a uniform prior, and then de   ne
   = exp (log(  )). notice that    deterministic nodes    (quantities that are de   ned in terms
of other variables) in jags are de   ned using    <-    instead of    ~   .

in jags, the model looks like this:

model
{

# prior for all the parameters
beta0 ~ dnorm(0, 1/1000^2)
beta1 ~ dnorm(0, 1/1000^2)
log_sigma ~ dunif(-10, 10)
sigma <- exp(log_sigma)

# likelihood
for(i in 1:n)
{

y[i] ~ dnorm(beta0 + beta1*x[i], 1/sigma^2)

}

}

the    rst part de   nes the priors for the parameters. for   0 and   1, we have just chosen very
broad priors that describe vague prior knowledge. note that the standard deviations of
the priors are 1000, so we should be careful to only apply this code in situtations where we
don   t expect the intercept or slope to have an extreme value (either positive or negative).

for the standard deviation parameter   , which describes how much we expect the data
points to be scattered around the straight line, we have assigned a log-uniform prior.
the limits of    10 and 10 for log sigma imply limits of 4.5    10   5 to 22,000 for sigma, a
generous range. if we thought    might actually be outside this range, we should change
the prior to something else or risk getting strange answers.

the likelihood part of the code involves a for loop, because our data is more than just
a single number. the code inside the loop is e   ectively duplicated n times (with i = 1,
then with i = 2, etc), once for each data point. since the loop refers to a quantity n, this
must be speci   ed in the data list if you are using my use jags.r template code.

another new feature of this jags model is the normal distribution, which is called dnorm
in jags. usually a normal distribution is written n (  ,   2) where    is the mean and    is
the standard deviation (and   2 is the variance). unfortunately, in jags there is a quirk:
the    rst argument to dnorm is indeed the mean, but the second argument must be one

chapter 10. regression

72

over the variance, or one over the standard deviation squared.

10.5 results for    road    data

our jags output will contain samples from the posterior distribution for   0,   1 and   .
the    rst thing we should do is make trace plots and check that everything converged
properly. then we can make histograms of each parameter to visually inspect the (marginal)
posterior distribution for each parameter. we can also plot one parameter vs. another to
look at the joint posterior distribution for the parameters. r code for all of these is given
below.

# plot trace plots
plot(results$beta0, type=   l   , xlab=   iteration   , ylab=   beta0   )
plot(results$beta1, type=   l   , xlab=   iteration   , ylab=   beta1   )
plot(results$sigma, type=   l   , xlab=   iteration   , ylab=   sigma   )

# plot histograms
hist(results$beta0, breaks=20, xlab=   beta0   )
hist(results$beta1, breaks=20, xlab=   beta1   )
hist(results$sigma, breaks=20, xlab=   sigma   )

# plot joint posterior distribution of beta0 and beta1
plot(results$beta0, results$beta1, cex=0.1, xlab=   beta0   , ylab=   beta1   )

all of the plots for the road data are shown in figure 10.2.

with classical id75 it is usually helpful to plot the best    tting line through
the data.
in bayesian id75 our output is posterior samples for what the
parameters might be (and therefore what the line might be). a common way of displaying
the posterior is to plot many lines through the data, with the lines produced using the
posterior samples. some r code for doing this is given below. the plot produced looks
like the one in figure 10.3.

# plot the data
plot(data$x, data$y)

# make some x-values for plotting lines
x = c(0, 100)
# plot the first 30 lines from the posterior distribution
for(i in 1:30)
{

lines(x, results$beta0[i] + results$beta1[i]*x)

}

chapter 10. regression

73

figure 10.2: results (posterior samples) from the simple id75 model applied to
the road data. top left: trace plots showing the parameters moving around over time
(as the mcmc progressed). top right: histograms of the posterior samples, showing
the marginal posterior distributions for the parameters. bottom: a plot of   1 vs.   0,
showing samples from the joint posterior for these two parameters. these parameters had
independent priors, but the posterior shows a correlation. all this means is that if   0 is a
high value then   1 must be low, and vice versa.

10.6 predicting new data

one of the most important uses of regression models is for prediction. given this data,
what can we say about the value of the output variable y at some new value of the input
variable, xnew? it   s unknown, but let   s call it ynew. with our road example, we will try to
predict the maximum reading distance of a person who is xnew = 90 years old. if we knew
the parameters of the straight line (or had a good point estimate), we could extend it out
to x = 90, and compute our predicted value:

0100200300400500480500520540560580600620640660beta0traceplots0100200300400500   4.5   4.0   3.5   3.0   2.5   2.0   1.5beta10100200300400500iteration30405060708090sigma480500520540560580600620640660beta001020304050607080histograms(marginalposteriors)   4.5   4.0   3.5   3.0   2.5   2.0   1.5beta101020304050607030405060708090sigma01020304050607080480500520540560580600620640660beta0   4.5   4.0   3.5   3.0   2.5   2.0   1.5beta1jointposteriorchapter 10. regression

74

figure 10.3: the road data with credible regression lines (sampled from the posterior
distribution) overplotted.

ynew =   0 +   1xnew
=   0 +   1    90

(10.11)
(10.12)

but that   s not quite in the bayesian spirit. firstly, we don   t know the value of the
parameters, we only have the posterior distribution. secondly, we would like not just a
point estimate for ynew but a whole id203 distribution, describing all of the uncertainty
in the prediction. one simple improvement would be to state that our uncertainty about
ynew should be described by a normal distribution around the straight line, with standard
deviation   . this still isn   t quite right though, because we   d need to know the true values
of the parameters to actually do it.

in general, bayesian prediction works like so. with parameters    and data x, we can
predict    new data    x(cid:48) by calculating the    posterior predictive distribution    which is just
the id203 distribution for x(cid:48) given x:

p(x(cid:48)|x) =
=

p(  , x(cid:48)|x) d  
p(  |x)p(x(cid:48)|x,   ) d  

(10.13)

(10.14)

we    rst saw this in section 4.2. the    rst term inside the integral is the posterior,
and therefore the whole integral is an expectation value, with respect to the posterior
distribution. the second term is the id203 distribution for x(cid:48) given the parameters,
i.e. imagining that we knew the parameters.

this equation is telling us to imagine that we know the true parameter values, make a
prediction (in terms of a id203 distribution), repeat this for all possible parameter
values and then average together all the id203 distributions into one       nal    distribu-
tion. in the averaging process we should give more weight to the id203 distributions
that were based on plausible values of   .

(cid:90)
(cid:90)

020406080100age(years)0100200300400500600700800distance(metres)100posteriorsampleschapter 10. regression

75

thankfully, actually doing this is much easier than it sounds, thanks to mcmc. remarkably,
we can accomplish all this, and obtain our id203 distribution for ynew, by adding just
a single line to the jags model:

y_new ~ dnorm(beta0 + beta1*90, 1/sigma^2)

of course, to look at the posterior samples for y new, you   ll need to monitor it. the
samples of y new will be drawn from the posterior predictive distribution for the new data.
internally, jags will simulate a new value of y new at every iteration, from the speci   ed
distribution, and using its current estimates of the parameters. since the parameters are
not    xed, but explore the posterior distribution, the distribution of y new values will take
into account all of the uncertainty we have about the parameter values.

as a general rule for predicting new data in jags, the extra line(s) you   ll add to the
jags model will usually resemble the likelihood, but the variable will have a di   erent
name. the results for the road data prediction are shown in figure 10.4.

figure 10.4: samples from the posterior predictive distribution, answering the question
   what do we know about y at x = 90?   . summaries are shown in the title. if we simply
assumed the best    t line was true and applied a point estimate of    to get our uncertainty,
we would have obtained a prediction of 306.07    49.76.

10.7 simple id75 with outliers

one complication that is common in id75 (or other model-   tting) problems is
the existence of outliers. these are points that do not    t in with the general trend assumed
by the model. many methods exist for deciding how to    detect    and    remove    outliers.
from a bayesian point of view, there is not a one-size-   ts-all approach to outliers. if you
really believe in your model assumptions, an    outlier    might be your most important data
point. if you aren   t really sure of your model assumptions and are just using a convenient
default model, then your results may be misleading. in lectures and labs we will study

100150200250300350400450500ynew010203040506070numbermean=308.30,sd=58.40chapter 10. regression

76

an extension to the simple id75 model that allows for outliers by using a
heavier-tailed sampling distribution, the student-t distribution.

10.8 multiple id75 and logistic regres-

sion

we will study an example of multiple id75, and nonid75 (involving
an    interaction term   ) in lectures. we will not study a id28 explicitly, but
the special lecture on predicting sports matches is very closely related to id28.
the main di   erence between id75 is that the output variable can only be 0 or 1,
so the likelihood will usually involve the    bernoulli    distribution (a binomial distribution
with one trial).

chapter 11

replacements for t-tests and
anova

anova is a common procedure in classical statistics, and is related to the simpler idea of
a t-test. these classical tests were designed for particular kinds of problems, and in this
chapter we will study similar problems but solve them from a bayesian point of view. we
will also use these examples to discuss some issues about the choice of prior distributions
when there are more than a few parameters. when there are only a few parameters it is
usually safe to assign a vague, wide prior to describe your initial uncertainty (unless, of
course, you have more information than that). in higher dimensions, problems can arise if
you do this. one way of getting around these problems is to use a hierarchical model.

11.1 a t-test example

this example is based on one given in a 1976 article by physicist e. t. jaynes, called
   con   dence intervals vs. bayesian intervals   . this is a very strongly worded paper and
might be an interesting read for those who are interested in the battle between frequentist
and bayesian statistics when the latter was making its comeback in the second half of the
20th century. it   s also where i got the crazy con   dence interval example from.

two manufacturers, 1 and 2, both make    widgets   , and we are interested in    guring out
which manufacturer makes the best widgets (on average), as measured by their lifetime. to
determine this, we obtain 9 widgets from manufacturer 1 and 4 widgets from manufacturer
2, and measure their lifetimes, in days. the results are given below:

x1 = {41.26, 35.81, 36.01, 43.59, 37.50, 52.70, 42.43, 32.52, 56.20}
x2 = {54.97, 47.07, 57.12, 40.84}

(11.1)
(11.2)

these measurements can be summarised by the means and standard deviations, which are
42    7.48 for group 1 and 50    6.48 for group 2. the question is: given this data, is there
evidence that one of the manufacturers is better than the other, and if so, by how much?
in classical statistics the standard procedure for this situation would be a two sample
t0-test. however, before we do anything i   d like you to consider the numbers and use your
intuition: what do you think about what the evidence says?

77

chapter 11. replacements for t-tests and anova

78

an underlying assumption of a classical t-test is that the data are normally distributed
around the mean values for each group1. we may as well adopt this assumption for our
bayesian model. if we call the group 1 data points {x1
n1} and the group 2 data
points {x2

2, ..., x1

2, ..., x2

1, x1

1, x2

n1}, then the likelihood is:
x1
i     n
x2
i     n

(cid:0)  1,   2(cid:1)
(cid:0)  2,   2(cid:1)

(11.3)

where all the data points are independent given the parameters. note the assumption
that the two groups have the same underlying (   population   ) standard deviation   . this
is a popular assumption in this kind of analysis but it is not necessarily well justi   ed! we
will build our bayesian models using this assumption, but it is not that di   cult to relax it
if you want to. you could just include multiple    parameters in the model, just like how
we will include the multiple    parameters.

instead of just one model for this situation, we will study three di   erent versions. each
model will have the same likelihood as given above in equation 11.3, and the same prior
for   . however, the models will all have di   erent priors for   1 and   2. we will be able to
see that the choice of prior does in   uence the results (of course), but in ways that make
sense. which of these models is more appropriate in a practical situation would depend
on the exact situation. there is no    one size    ts all    model.

11.1.1 likelihood

to implement our model in jags, we can begin by specifying the likelihood part like so:

# sampling distribution/likelihood
for(i in 1:n1)
{

x1[i] ~ dnorm(mu1, 1/sigma^2)

}
for(i in 1:n2)
{

x2[i] ~ dnorm(mu2, 1/sigma^2)

}

we have called our data arrays x1 and x2, and we have also assumed that the sample sizes
n1 and n2 are de   ned, so our data list will need to be consistent with these choices. the
parameters we will be estimating are mu1, mu2, and sigma, so we will need to specify prior
distributions for them. in the following sections, we   ll use the same prior for sigma, so we
may as well specify that now. let   s use a log-uniform prior where    is between e   10 and
e10.

# prior for sigma
log_sigma ~ dunif(-10, 10)
sigma <- exp(log_sigma)

1strictly speaking, it   s the id203 distribution for the data given the parameters that is normal,

the data may or may not look normally distributed.

chapter 11. replacements for t-tests and anova

79

11.1.2 prior 1: very vague

the last missing ingredients to    nish the jags model are the priors for mu1 and mu2. for
our    rst model, let   s be really naive and assign super-wide uniform priors.

# prior 1: very vague
mu1 ~ dnorm(0, 1/1000^2)
mu2 ~ dnorm(0, 1/1000^2)

at    rst glance, this might seem like a fairly reasonable thing to do. in many problems, it
doesn   t make much di   erence if we just use vague priors and get on with the calculation
(as opposed to thinking really hard about the prior, and what is actually known about the
parameters).

however, this prior has a number of properties that suggest it might not be quite right:
   rstly, what is the id203 that   1 =   2? in classical t-tests, the whole point is to test
the hypothesis that the two    population means    (parameters) are equal. however, our
prior actually implies that the id203 they are equal is 0! therefore, no matter what
data we get, the posterior id203 of   1 =   2 will always be zero.

11.1.3 prior 2: they might be equal!

the problem with prior 1 is that we may think   1 might exactly equal   2, and prior 1
doesn   t allow for this. so here   s another way we might set up the prior. we   ll start by
de   ning the prior for   1 as we did before. then, when we consider   2, we need a way of
giving it a 50% id203 of equalling   1, and if not, then it should have a    bi-exponential   
distribution centered around   1. here is our solution. read it carefully and make sure
you understand what this prior does.

# first mean
mu1 ~ dnorm(0, 1/1000^2)

# prior for difference, mu2 - mu1
u ~ dunif(-1, 1)

# length of exponential prior given difference != 0
l <- 5
size_of_difference <- step(u)*(-l*log(1 - u))

# to make the difference positive or negative
c ~ dbin(0.5, 1)
difference <- (2*c - 1)*size_of_difference

# second mean
mu2 <- mu1 + difference

chapter 11. replacements for t-tests and anova

80

11.1.4 prior 3: alright, they   re not equal, but they might be

close

prior 2 is also a little bit strange, if you think about it. if we   re comparing these two
manufacturers of widgets, why would we think it is possible that the two manufacturers are
exactly equal? maybe we just think the parameters   1 and   2 are likely to be similar in
value. in other words, we shouldn   t worry so much about the prior probablity of   1 =   2,
but we should at least make sure there   s a moderate prior id203 that   1       2.
one way we could do this is by applying a normal prior to both   1 and   2 with some mean
(let   s call it the    grand mean   ) and some standard deviation (let   s call it the    diversity   ).
that way,   1 and   2 would both be likely to be somewhere around the grand mean, and
they would likely be di   erent by roughly the size of the diversity. the challenge now seems
to be the choice of appropriate values for the grand mean and the diversity. fortunately,
we don   t actually have to! what we can do instead is apply priors for them instead.

this is our    rst example of a hierarchical model. in a hierarchical model, instead of directly
assigning priors to our parameters, we imagine that we knew the values of some other
parameters (called    hyperparameters   ), and assign our prior for the parameters given the
hyperparameters. then we assign a prior for they hyperparameters as well, to complete
the model.

# hierarchical prior for the means
# hyperparameters
grand_mean ~ dnorm(0, 1/1000^2)
log_diversity ~ dunif(-10, 10)
diversity <- exp(log_diversity)

# prior for the parameters given the hyperparameters
mu1 ~ dnorm(grand_mean, 1/diversity^2)
mu2 ~ dnorm(grand_mean, 1/diversity^2)

samples (obtained using jags) of the three priors are shown in figure 11.1.

the posteriors are shown in figure 11.2. the id136s are di   erent, as you would expect,
and that   s entirely down to the choice of the prior. any summaries we make will therefore
depend on which prior we want to use.

the original question was whether manufacturer two was better, equal, or worse than
manufacturer one. we can answer that question by calculating the posterior probabilities
of   1 =   2,   1 <   2, and   1 >   2. the results are shown in table 11.1.

remember that prior 1 did not assign any id203 to the possibility of the two
parameters being equal. therefore, no possible evidence can increase make the posterior
id203 nonzero. however, according to this model, there is quite strong evidence that
  1 <   2, as the id203 changed from 0.5 to 0.946.

prior 2 did allow the two parameters to be equal, and if we use prior 2, we seem to have
found very weak evidence that they are not in fact equal. the id203 decreased from
0.5 to 0.424. according to prior 2, if   1 (cid:54)=   2, then   1 <   2 is the next most likely scenario.
however, prior 2 has an issue associated with it. our prior says that if   2 is not equal to

chapter 11. replacements for t-tests and anova

81

figure 11.1: the three di   erent priors we are trying for our bayesian equivalent of a
t-test. the    rst prior simply asserts a large amount of prior ignorance about the value of
the two parameters   1 and   2. the second is similar but applies 50% id203 to the
proposition   1 =   2. the third prior does not allow the two parameters to be exactly equal,
but enhances the id203 that they are quite similar in value.

figure 11.2: the posterior distributions, given the widget data, based on the three di   erent
priors for   1 and   2.

  1, then it is likely to be close to   1. exactly how close we expect it to be is set by the
variable l in the model. if we were to make l very large, then the data would go from
weak evidence against   1 =   2 to strong evidence for it! why does this happen? well, if
we increased l, the prior id203 that   2 and   1 are close given that they   re di   erent
is decreased. then, the hypothesis that the   s are di   erent does not predict our data as
well, since our data looks like the   s are close together. since it doesn   t predict the data
as well as before, its posterior id203 will be lower. some people think this sensitivity
to the prior is a danger of bayesian id136 (if you want, you can do a web search for
the    je   reys-lindley paradox   ), but it is behaving logically: the wider we make the prior,
the lower we make the prior id203 that   1 and   2 are close but not equal, giving the
model no choice but to believe that they   re equal. if the results are sensitive to the prior,
that   s important, and you should think about the logic of the problem to understand why.

prior 3 seems like it   s what we might want in general.
it   s often silly to think two
parameters might be exactly equal. what we really think is that there is a di   erence, and
it might be very small, or moderate or large.

   2000   1000010002000  1   2000   1000010002000  2   2000   1000010002000  1   2000   1000010002000priors   2000   1000010002000  1   2000   1000010002000204060  1204060  2204060  1204060posteriors204060  1204060chapter 11. replacements for t-tests and anova

82

prior probabilities:

prior   1 <   2   1 =   2   1 >   2
1
2
3

0.5
0.25
0.5

0.5
0.25
0.5

0
0.5
0

posterior probabilities:

prior   1 <   2   1 =   2   1 >   2
0.055
1
0.079
2
3
0.372

0.945
0.491
0.629

0.430

0

0

table 11.1: prior and posterior probabilities for three di   erent hypotheses about the two
manufacturers, based on the models with the three di   erent priors. as you can see, the
conclusions are quite sensitive to the choice of prior in this case.

11.2 one way anova

one-way anova can be considered as a generalisation of a t-test to more than two groups.
the question is usually phrased as a test of the hypothesis that the group means are the
same, versus the alternative that there is some di   erence. as we saw in the bayesian
   t-test   , it is possible (using clever tricks) to make a model that has some prior id203
that the group means are equal. however, this gets more tricky with multiple groups.
therefore we will build our one-way anova model in a similar way to the    hierarchical
model    version of the t-test model. there will be one other major di   erence, but it is a
di   erence in the way the model is coded, not a conceptual di   erence.

in the t-test section our data set was composed of measurements in two groups and
our data list contained two vectors of measurements, called x1 and x2. the sampling
distribution/likelihood part of our jags model also needed two for loops, one for each
group.
if we have many groups (in the following example we will have four), it can
get awkward having to write all those loops. therefore, when we develop our    one-way
anova    model, we will format the data di   erently by putting all measurements into a
single vector x. to make this work, we   ll need an extra vector in the dataset, which tells
us which group each data point belongs to.

we   ll use an example dataset on the masses of starlings (a type of bird). the masses
of some starlings were measured at four locations. we are interested in the di   erences
between the locations. how similar are they in terms of the average weight of starlings?
are they basically the same, radically di   erent, or something in between? a boxplot of
the data is shown in figure 11.3, which seems to show substantial di   erences between
the locations. however, only ten starlings were measured at each location, so we can   t be
absolutely sure of this, and our goal is to investigate how sure we should be.

to solve this problem in a bayesian way, we will treat it as a parameter estimation problem
with four    parameters, one for each of the locations. we will also need at least one
parameter describing the standard deviation of the starling masses at each location. for
convenience we   ll assume that   s the same across all locations, but it is straightforward to

chapter 11. replacements for t-tests and anova

83

relax this assumption later.

figure 11.3: the masses of starlings as measured at four di   erent locations. it seems as
though the mean mass varies somewhat according to the location, and our results will tell
us how plausible this is.

11.2.1 hierarchical model

our    one-way anova    model is very much the same as our    nal    t-test    model, except
for the format of the dataset. the main advantage of this model is that it generalises to
more than two groups in a very straightforward way; we no longer need to write separate
for loops for each group. as with the third    t-test    model, we are not seriously considering
the hypothesis that all of the group means (i.e. the    parameters) are exactly equal, but
we are allowing them to be quite close together, or quite distinct in value, by using the
hierarchical model structure with the diversity parameter.

model
{

# log-uniform prior for the scatter
log_sigma ~ dunif(-10, 10)
sigma <- exp(log_sigma)

# hierarchical prior for the means
# hyperparameters
grand_mean ~ dnorm(0, 1/1000^2)
log_diversity ~ dunif(-10, 10)
diversity <- exp(log_diversity)

# parameters
for(i in 1:n)

123470758085starling datalocationmass (grams)chapter 11. replacements for t-tests and anova

84

{

}

mu[i] ~ dnorm(grand_mean, 1/diversity^2)

# sampling distribution/likelihood
for(i in 1:n)
{

x[i] ~ dnorm(mu[group[i]], 1/sigma^2)

}

}

after running this model on the starling data, we can plot any results we wish.
in
figure 11.4, i have plotted a trace plot of   1, the parameter for the mean weight of
starlings at location 1. this is a healthy trace plot, although there is a strange feature near
iteration 1000 which we will discuss in the next section. figure 11.5 shows the posterior
distribution for the log diversity hyperparameter, which quanti   es how di   erent the
groups really are. our prior for this parameter was u(-10, 10), and the posterior peaks
at around 1.5, which corresponds to diversity     4.5, although there is a fair bit of
uncertainty. notice also the long tail of the posterior on the left hand side. although
we never allowed the   s to be exactly the same, we did allow them to be close (and this
corresponds to the diversity being low). the fact that some posterior samples landed
between -10 and 0 suggests there is a small id203 that the di   erences between groups
are very small, despite the fact that the data doesn   t look that way.

figure 11.4: a trace plot of   1 from a jags run on the starling data. things appear to be
mixing well, except for an odd feature near iteration 1000.

as usual, we can use our posterior samples to calculate the posterior id203 of any
hypothesis that we can think of based on the parameters. here are a couple of interesting
examples:

0200040006000800010000iteration78808284868890  1(grams)traceplotchapter 11. replacements for t-tests and anova

85

figure 11.5: the posterior distribution for log diversity.

# is mu2 really greater than mu3?
> mean(results$mu[,2] > results$mu[,3])
[1] 0.672
# is the diversity really less than 1 (log diversity less than 0)?
> mean(results$diversity < 0)
[1] 0.0272

11.2.2 mcmc e   ciency

the hierarchical    one-way anova    model given above works, but a quick look at the
trace plot suggests the mixing (how easily the mcmc algorithm is able to move around)
did have some di   culties (see figure 11.4). in this particular example the problem wasn   t
fatal, but this problem could be more severe with a di   erent data set. in some models, it
makes sense to consider the parameterisation of the model. there are actually di   erent
ways to implement exactly the same model assumptions, but in a way that helps the
e   ciency of the mcmc sampling. this is done by changing which parameters are de   ned
by    ~    and which are de   ned by    <-   , in a way that keeps the meaning of the model
intact, but forces jags to do the exploration di   erently.

let   s look at a small subset of the above model: just the hierarchical prior for the   s. here
it is:

# hierarchical prior for the means
# hyperparameters
grand_mean ~ dnorm(0, 1/1000^2)
log_diversity ~ dunif(-10, 10)

   10   8   6   4   20246log(diversity)020040060080010001200numbermarginalposteriorchapter 11. replacements for t-tests and anova

86

diversity <- exp(log_diversity)

# parameters
for(i in 1:n)
{

mu[i] ~ dnorm(grand_mean, 1/diversity^2)

}

to understand why this causes problems, we need to understand a little about how jags
works internally. jags uses two main mcmc methods, known as id150 and
slice sampling. both of these methods usually work by updating a single parameter or
hyperparameter at a time, while keeping all of the others    xed. because jags is sampling
the posterior, each parameter will tend to move about as far as it can without the new
value becoming inconsistent with the data or the (joint) prior distribution. but the above
model doesn   t just have problems exploring the posterior e   ciently, but would also have
problems exploring the prior!

for example, if the current values of grand mean and diversity are 50 and 5, and jags
is moving the parameter mu[3], it will probably move it to somewhere within the range 50
   5, roughly speaking, since the prior for mu[3] given grand mean=50 and diversity=5
is normal(50, 52). but another possibility that is (speaking loosely again) compatible with
the prior is to have grand mean=-1500, diversity=10000, and mu[3]=5600. how would
the sampler move from having mu[3]=5 to having mu[3]=5600? it certainly couldn   t do
this while diversity was still 5. somehow, diversity would have to be much greater
than 5. yet when the sampler tries to increase the value of diversity, it won   t be able to
move very far, because that would make it inconsistent with the values of the other mu
parameters!

many mcmc methods (and importantly for us, the ones used by jags) are ine   cient
when the posterior distribution has strong dependence between di   erent parameters.
unfortunately, in our one-way anova model, it   s not just the posterior that has strong
dependence, but even the prior has strong dependence!

11.2.3 an alternative parameterisation

we will now look at an alternative way of implementing the hierarchical model, that entails
exactly the same assumptions (the same prior distributions and sampling distribution),
yet has computational advantages. the alternative parameterisation is given below.

# hierarchical prior for the means
# hyperparameters
grand_mean ~ dnorm(0, 1/1000^2)
log_diversity ~ dunif(-10, 10)
diversity <- exp(log_diversity)

# parameters
for(i in 1:n)
{

n[i] ~ dnorm(0, 1)

chapter 11. replacements for t-tests and anova

87

mu[i] <- grand_mean + diversity*n[i]

}

the only di   erence between this implementation and the original is the part within the
loop. instead of de   ning the prior for the   s directly, we have de   ned di   erent parameters
called n, with standard normal priors. we then compute the mus deterministically from
the ns. in this alternative parameterisation, the prior for the ns is completely independent
of grand mean and diversity, so sampling from the prior would be extremely e   cient,
yet the implied prior for the mus is exactly the same as before. of course, the posterior
(what we actually want to sample) will still probably have dependence, but hopefully less.

running this new version of the model on the starling data gives the trace plot in
figure 11.6, which doesn   t have any strange features.

figure 11.6: a trace plot of the parameter   1 using the revised model.

0200040006000800010000iteration788082848688  1(grams)traceplotchapter 12

acknowledgements

i would like to thank michael williams (columbia), renate meyer (auckland), and geraint
lewis (sydney) for their comments. thanks also to sampath fernando (auckland) for
   nding several errors, and the other students who noticed an error.

88

appendix a

r background

this chapter describes the r programming techniques that we will need in order to do
bayesian statistics both within r and also by using jags.

a.1 vectors

we will use vectors frequently throughout the course. a vector can be thought of as a
list of numbers. in r, you can create a vector using the function c, which stands for
   concatenate   .

my_vector = c(12.4, -6.2, 4.04)

you can then examine the contents of the vector by typing its name.

> my_vector
[1] 12.40 -6.20 4.04

there are various helpful things you can do with vectors. you can get the length of a
vector (the number of elements in it) like so:

> length(my_vector)
[1] 3

you can do arithmetic with vectors too, if they   re the same length. for example:

> c(1,2,3)*c(2,5,3)
[1] 2 10 9

if you wanted, you could assign the output to a new variable:

> x = c(1,2,3)
> y = c(2,5,3)
> z = x*y
> z
[1] 2 10 9

it is important to understand how to access subsets (or    slices   ) of vectors based on which

89

appendix a. r background

90

elements satisfy a certain condition. here is an example:

> x = c(1, 2, 3, 0, 10)
> test = x > 3
> test
[1] false false false false true
> y = x[x <= 2]
> y
[1] 1 2 0
> z = sum(x[x <= 2])
> z
[1] 3

this kind of thing will be used frequently in the computational parts of the course.

a.2 lists

lists are a bit like vectors, in that they can contain a lot of information in a single variable.
but instead of just being numbers, lists can contain all sorts of things. for example,
suppose i want a variable/object in an r program to represent a person. a person can
have a name and an age. here is how to make a list:

a_person = list(name="nicole", age=21)

if you needed to extract certain elements from a list, you can do it using the $ operator in
r. for example suppose i wanted to extract the name variable from within a person. i
could do this:

> a_person$name
[1] "nicole"

and voila. when we use jags, a data set will be represented using a list. so will our
jags output.

if you have ever learned c, c++, or matlab, a list in r is basically the same thing as
a    struct    in these languages. if you have learned c++, java, or python, a list is like a
   class    but with just variables and no functions. in python,    dictionaries    are also very
similar to r lists.

a.3 functions

in this course you   ll need to be able to read and understand simple r functions, and
perhaps write a few. a function is like a machine that takes an input, does something, and
returns an output. you have probably used many built-in r functions already, like sum().

# defining a function called my_function
my_function = function(x)
{

appendix a. r background

91

# do some stuff
result = 3*x + 0.5
return(result)

}

a.4 for loops

for loops are mostly used to repeat an action many times. here is an example.

n = 100
for(i in 1:n)
{

print(i)

}

a.5 useful id203 distributions

since r is a statistics program, it knows about a lot of id203 distributions already.
so, if i wanted to use the id203 density function of a normal distribution, instead of
having to code something like this:

f = exp(-0.5*((x - mu)/sigma)**2)/(sigma*sqrt(2*pi))

i can just use the built-in function dnorm.

f = dnorm(x, mean=mu, sd=sigma)

much easier! if, instead of wanting to evaluate the pdf, i wanted to generate random
samples from a normal distribution, i could use rnorm.

# generate 1000 samples
samples = rnorm(1000, mean=50., sd=10.)

appendix a

id203

this course will use id203 theory quite a lot, but we will often use a fairly informal
notation. bayesian statistics is really just id203 theory used for a particular purpose,
to describe uncertainty. the two most important rules of id203 are given below, for
reference.

a.1 the product rule

the    rst important rule of id203 is the product rule. this tells us how to calculate
the id203 that any two propositions or hypotheses, a and b, are both true. the
id203 of a and b, will be denoted p (a, b). this can be calculated by    rst    nding
the id203 that a is true, and then multiplying by the id203 that b is true given
that a is true.

p (a, b) = p (a)p (b|a)

(a.1)

we could also have done this the other way around:    rst    nding the id203 that b is
true and then the id203 that a is true given that b is true:

p (a, b) = p (b)p (a|b).

(a.2)

when using the product rule (or any rule of id203, for that matter), you must ensure
that the statements to the right of the    given    sign (or the absence of any statements) are
consistent throughout. for example, p (a, b|c) = p (a|c)p (b|a, c) is a valid use of the
product rule, since    given c    is part of the background information in all of the terms.

you may be familiar with the idea of a tree diagram from earlier statistics courses or maybe
even high school. a tree diagram is a helpful way to work with the product rule. if you
   nd tree diagrams helpful, feel free to use them, although tree diagrams themselves will
not be examinable. an example tree diagram is given in figure a.1.

the product rule can also be applied to more than two propositions, like so:

p (a, b, c) = p (a)p (b|a)p (c|b, a).

(a.3)

92

appendix a. id203

93

figure a.1: a tree diagram.

you can also apply the product rule in a situation where there is a common statement in
the    given part    of all probabilities in the expression. for example, the following is also
valid:

p (a, b, c|d) = p (a|d)p (b|a, d)p (c|b, a, d).

(a.4)

in fact, it   s best to regard even    unconditional    probabilities such as p (a) as being
conditional on some prior information i, which is just left out to keep the notation simple.

a.1.1 bayes    rule

looking at equations a.1 and a.2, they are both equations for the same thing, p (a, b).
therefore we can equate the right hand sides. doing this gives a result known as bayes   
rule:

p (a|b) =

p (a)p (b|a)

p (b)

.

(a.5)

bayes    rule will be used extensively throughout this course. you will need to know it and
know how to use it!

a.2 the sum rule

the sum rule is the second important rule of id203. a general statement of the sum
rule is

p (a     b) = p (a) + p (b)     p (a, b).

(a.6)

where     means logical or.
the sum rule is often used to calculate the id203 of some statement a when we only
know the id203 of a conditional on some some other statement b. then we can use
the sum rule like this:

p (a) = p (a, b) + p (a,  b)

= p (b)p (a|b) + p (  b)p (a|  b).

(a.7)
(a.8)

n(cid:88)

i=1

appendix a. id203

94

where the    symbol means    not   , i.e.   b is the statement that b is false. to understand
this formula, imagine we want to know the id203 of a. there are two mutually
exclusive ways that could happen: via b being true, or via b being false. the    rst way
has id203 p (b)p (a|b), and the second way has id203 p (  b)p (a|  b).
if, instead of just two mutually exclusive and exhaustive pathways b and   b, there are
many, such as b1, ..., bn. then the sum rule takes the form

p (a) =

p (bi)p (a|bi).

(a.9)

as an exercise, you can try proving this version of the sum rule starting from the simpler
version of equation a.6.

in bayesian statistics the sum rule is most often used to calculate the marginal likelihood
p (d), and to marginalise out    nuisance parameters    from the posterior distribution. in
stats 331 we will mostly use mcmc to do the latter.

a.3 random variables

throughout this course i will use the term    id203 distribution    to refer to both
the id203 mass function for a discrete random variable, and the id203 density
function for a continuous random variable. i will also use a common shorthand notation.

a.3.1 discrete random variables

we will also see quite a lot of random variables in this course (although without using
that terminology very much, as i consider the word    random    to be worse than useless).
a discrete random variable is just a quantity x that has a countable number of possible
values. a discrete random variable has a id203 mass function that tells you the
id203 as a function of the possible values x. for example, the equation for the poisson
distribution (a useful discrete distribution) is:

p (x = x) =

  xe     

x!

(a.10)

for x     {0, 1, 2, 3, ...}. the actual random variable is named x, and x is just used so we
can write the probabilities (p (x = 0), p (x = 1), ...) as a formula.

a.3.2 continuous random variables

continuous random variables are those where the set of possibilities is continuous. for
example, with a normal distribution, technically any real value is possible. therefore
it doesn   t make sense to ask, for example, the id203 that x = 1.32. the answer
is zero because the total id203 of 1 has to be spread among an in   nite number of
possibilities. instead, we can ask the id203 that x is in some region that has a

appendix a. id203

95

nonzero size. in general, if x has a id203 density function f (x), then the id203
that x     [a, b] is:

(cid:90) b

p (a     x     b) =

f (x) dx.

a

(a.11)

note the lower case x in the id203 density function. this is analogous to the lower
case x in the id203 mass function of a discrete random variable. note that i won   t
often get you to do an integral analytically. one of the major reasons why mcmc is so
awesome is that you can get away without having to do hard integrals!

a.3.3 shorthand notation

the notation of random variables can be cumbersome. for example, consider inferring a
(discrete) parameter y from (discrete) data x. bayes    rule gives us:

p (y = y|x = x) =

p (y = y)p (x = x|y = y)

p (x = x)

.

that   s very verbose, so instead we use the shorthand:

p(y|x) =

p(y)p(x|y)

p(x)

.

(a.12)

(a.13)

in this notation we don   t distinguish between the symbol for the random variable and
the dummy variables that allow us to write the id203 distribution as a formula, we
just use the lower case for everything. despite this simpli   cation, everything still works.
just read p(y|x) as    the id203 distribution for y given x    and everything will be    ne.
astute readers may have noticed that when i gave the poisson formula (equation a.10),
   given       was implicit throughout.

to be clear when we are talking about a simple id203 and when we are talking about
the id203 distribution for a variable, i will use upper case p for the former and
lower-case p for the latter.

a.4 useful id203 distributions

in the course we will study bayesian models which involve the following discrete proba-
bility distributions: general (all probabilities given explicitly, such as in a bayes    box),
binomial, poisson, discrete uniform, negative binomial, multinomial. we may also use the
following continuous distributions: uniform, normal, cauchy, student-t, beta, log-uniform,
exponential, gamma, dirichlet.

wikipedia is an excellent resource for looking up all the properties of these distributions,
but i will also give various properties (e.g. the mean of a beta distribution) and describe
the distributions when we need them.

appendix a

rosetta stone

bayesian statistics has a lot of terminology    oating around, and sometimes di   erent
communities use di   erent terms for the same concept. this appendix lists various common
terms that are basically synonymous (they mean the same thing). i may even throw in
the di   erent terms from time to time, intentionally or unintentionally!

event, hypothesis, proposition, statement

these are all basically the same thing. a proposition is something that can be either true
or false, such as    my age is greater than 35    or    the number of aardvarks in my room
is either zero or one   . these are the things that go in our id203 statements: if we
write p (a|b), a and b are both propositions, that is, statements that may be true or
false. event is the preferred term in classical statistics.

sampling distribution, id203 model for the data, genera-
tive model, likelihood function, likelihood

this is the thing that we write as p(x|  ). sometimes it is called the sampling distribution
or a generative model because you can sometimes think of the data as having been    drawn
from    p(x|  ) but using the true value of   , which you don   t actually know. there is a
subtlety here, and that is that the word likelihood can be used to mean either p(x|  )
before x is known (in which case it is the thing you would use to predict possible data) or
after x is known. in the latter case p(x|  ) is only a function of    because x is    xed at the
observed value. the term likelihood function is often used at this point.

id203 distribution

the term id203 distribution is used to refer to either a id203 density function
(in the continuous case) or a id203 mass function (in the discrete case). the latter
gives the id203 of each particular value, whereas the former only gives a id203
if you integrate it within some region.

96

appendix a. rosetta stone

97

marginal likelihood, evidence, prior predictive id203, nor-
malising constant

this is the p(x) or p (x) term in the denominator of bayes    rule, and it is also the total of
the prior    likelihood column of a bayes    box. this is the id203 of getting the
data that you actually got, before you observed it: hence the terminology    prior predictive
id203   . it is also the thing you use to normalise the posterior distribution (make it
sum or integrate to 1), hence the term normalising constant. marginal likelihood makes
sense because it is a id203 of data (like the regular likelihood) but    marginalised   
(i.e. not caring about) the value of the parameter(s). it is also called    evidence    because
it can be used to compare di   erent models, or to    patch together    two bayes    boxes after
the fact (see the hypothesis testing chapter).

