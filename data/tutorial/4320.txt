understanding short texts

acl 2016 tutorial

zhongyuan wang (microsoft research)

haixun wang (facebook inc.)

tutorial website:
http://www.wangzhongyuan.com/tutorial/acl2016/understanding-short-texts/

outline

    part 1: challenges 
    part 2: explicit representation
    part 3. implicit representation
    part 4: conclusion

short text

    search query

    document title

    ad keyword

    caption

    anchor text

    question

    image tag

    tweet/weibo

challenges

    first, short texts contain limited context

1.08%

1.83%

(a) by traffic

2.55%

2.94%

5.45%

8.65%

14.24%

39.72%

23.53%

(b) by # of distinct queries

1 word
2 words
3 words
4 words
5 words
6 words
7 words
8 words
more than 8 words

4.45%

9.67%

3.68%

5.73%

8.87%

13.94%

19.06%

13.57%

21.06%

1 word
2 words
3 words
4 words
5 words
6 words
7 words
8 words
more than 8 words

based on bing query log between 06/01/2016 and 06/30/2016

challenges
    second,    telegraphic   : no word order, no function 

words, no capitalization,    

query    distance between sun and earth    can also be expressed as:

   
   
   
   
   

   

   

   

   

"how far" earth sun
"how far" sun
"how far" sun earth
average distance earth sun
average distance from earth 
to sun
average distance from the 
earth to the sun
distance between earth & 
sun
distance between earth and 
sun
distance between earth and 
the sun

   

   
   

   

   

   

   
   

distance from earth to the 
sun
distance from sun to earth
distance from sun to the 
earth
distance from the earth to 
the sun
distance from the sun to 
earth
distance from the sun to the 
earth
distance of earth from sun
distance between earth sun

   

   

   
   

   

   
   

   

how far away is the sun 
from earth
how far away is the sun 
from the earth
how far earth from sun
how far earth is from the 
sun
how far from earth is the 
sun
how far from earth to sun
how far from the earth to 
the sun
distance between sun and 
earth

hang li,    learning to match for natural language processing and information retrieval   

challenges
    second,    telegraphic   : no word order, no function 

words, no capitalization,    

short text 1

short text 2

china kong (actor)

china hong kong

hot dog

dog hot

term 
match

partial

yes

semantic 
match

no

no

the big apple tour 

new york tour

almost no yes

berlin

dnn tool

germany capital

no

yes

deep neural network 
tool

almost no yes

wedding band

band for wedding

why are windows so 
expensive

why are macs so 
expensive

partial

partial

no

no

challenges

    sparse, noisy, ambiguous

watch for kids

i)

ii)                   iii)

it   s not
a fair 
trade!!

short text understanding

    many applications

    search engines
    automatic id53
    online advertising
    id126s
    conversational bot
       

    traditional nlp approaches not sufficient

the big question

    humans are much powerful than machines in 

understanding short texts.

    our minds build rich models of the world and make 

strong generalizations from input data that is 
sparse, noisy, and ambiguous     in many ways far 
too limited to support the id136s we make. 

    how do we do it?

if the mind goes beyond the data given, 
another source of information must make up 
the difference.

science 331, 1279 (2011);

explicit 
(logic) 

representation

symbolic knowledge

(explicit)

implicit 

(embedding) 
representation

id65

(implicit)

explicit id99

    first, understand superlatives      tallest,       largest,    

etc.   and ordered items. so you can ask:

   who are the tallest mavericks players?   
   what are the largest cities in texas?   
   what are the largest cities in iowa by area?   

    second, have you ever wondered about a particular 
point in time? google now do a much better job of 
understanding questions with dates in them. so you 
can ask:
   what was the population of singapore in 1965?   
   what songs did taylor swift record in 2014?   
   what was the royals roster in 2013?   

    finally, google starts to understand some complex 

combinations. so google can now respond to 
questions like:
   what are some of seth gabel's father-in-law's movies?   
   what was the u.s. population when bernie sanders was born?   
   who was the u.s. president when the angels won the world series?   

http://insidesearch.blogspot.com/2015/11/the-google-app-now-understands-you.html

explicit id99

    logic representation 

    vector representation

(first-order-logic)
    freebase, google 

id13   

    esa: mapping text to wikipedia 

article titles

    conceptualization: mapping text 

to concept space

p(concept | short text)

a domain millions of concepts 
used in day to day communication

search query, anchor text
twitter, ads keywords,     

true or false

probabilistic model

explicit id99

    logic representation 

    vector representation

(first-order-logic)
    freebase, google 

id13   

pros:

    esa: mapping text to wikipedia 

article titles

    conceptualization: mapping text 

    the results are easy to understand for human beings
    easy to tune and customize

to concept space

cons:

    coverage/sparse model: can   t handle unseen 

p(concept | short text)

terms/entities/relations

    model size: usually very large

a domain millions of concepts 
used in day to day communication

search query, anchor text
twitter, ads keywords,     

true or false

probabilistic model

implicit id99: 
embedding 

cw08
(senna)

glove

deep structured semantic model (dssm)

https://code.google.com/p/id97/

input units: tri-letter
training size: ~20b clicks (bing + ie log)
vocabulary: 30k        parameter: ~10m

knet

input units: word
vocabulary: 130k

collobert, ronan, et al. "natural 
language processing (almost) from 
scratch." the journal of machine 
learning research 12 (2011): 
2493-2537.

input units: word
training size: > 42b tokens 
vocabulary: > 400k

j pennington, r socher, cd manning    glove: 
global vectors for word representation.    
emnlp 2014.

count + predict

input units: word
training size: > 100b sequence (freebase)
vocabulary: > 2m

tomas mikolov, kai chen, greg corrado, and 
jeffrey dean. efficient estimation of word 
representations in vector space. in 
proceedings of workshop at iclr, 2013. 

predict

huang, po-sen, et al. "learning deep 
structured semantic models for web 
search using clickthrough data." in cikm. 
acm, 2013.

implicit id99: 
embedding 

cw08
(senna)

glove

pros:

    dense semantic encoding
    a good representation framework
    facilitates computation (similarity measure)

https://code.google.com/p/id97/

deep structured semantic model (dssm)

cons:

    perform poorly for rare words and new words
    missing relations (e.g, isa, ispropertyof) 
    hard to tune since it   s not nature for human beings

input units: tri-letter
training size: ~20b clicks (bing + ie log)
vocabulary: 30k        parameter: ~10m

knet

input units: word
vocabulary: 130k

collobert, ronan, et al. "natural 
language processing (almost) from 
scratch." the journal of machine 
learning research 12 (2011): 
2493-2537.

input units: word
training size: > 42b tokens 
vocabulary: > 400k

j pennington, r socher, cd manning    glove: 
global vectors for word representation.    
emnlp 2014.

count + predict

input units: word
training size: > 100b sequence (freebase)
vocabulary: > 2m

tomas mikolov, kai chen, greg corrado, and 
jeffrey dean. efficient estimation of word 
representations in vector space. in 
proceedings of workshop at iclr, 2013. 

predict

huang, po-sen, et al. "learning deep 
structured semantic models for web 
search using clickthrough data." in cikm. 
acm, 2013.

implicit id99: dnn

stanford deep autoencoder for paraphrase 

detection [soucher et al. 2011]

stanford mv-id56 for id31 

[soucher et al. 2012]

facebook deeptext classifier [zhang et al. 2015]

new trend: fusion of explicit and implicit 
knowledge

    relationship
    rules of id136

teach

explicit 
(logic) 

representation

symbolic knowledge

implicit 

(embedding) 
representation

id65

learn

   

learn more similar 
rules, enrich logic 
representation 

