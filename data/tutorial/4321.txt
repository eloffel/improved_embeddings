part ii: explicit representation 
for short text understanding

zhongyuan wang (microsoft research)

haixun wang (facebook inc.)

tutorial website:
http://www.wangzhongyuan.com/tutorial/acl2016/understanding-short-texts/

what is explicit understanding?

add common sense to computing

pablo picasso

25 oct 1881

spanish

which is    kiki    and which is    bouba   ?

\                         

sound 

shape

zigzaggedness

china

brazil

india

emerging market

country

the

engineer

is eating an

apple

it company

fruit

body

smell

taste

wine

outline

    knowledge bases
    explicit representation models
    applications 

linguistic linked open data cloud

cyc

http://linghub.lider-project.eu/llod-cloud

1.    python tutorial   
2.    who was the u.s. president when the angels won the 

world series?   

linguistic, common 
sense knowledge

knowledge

knowledge

encyclopedia 
knowledge

short text

understanding

answer

(internal representation)

common sense knowledge 
vs. encyclopedia knowledge

common sense 
knowledge base

encyclopedia 
knowledge base

common sense/linguistic knowledge 

among terms

isa

ispropertyof
co-occurrence

   

typicality, basic level of 

categorization

entities
facts

dayofbirth
locatedin
spouseof

   

black or white

precision

id138*, knowitall, nell,

freebase, yago, dbpedia, google 

probase,    

id13,    

* special cases

id138 [stark et al. 1998]

brief introduction:

    id138   is a large lexical database of english. nouns, verbs, adjectives and 

adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a 
distinct concept.

statistics:

sample:

pos

unique
strings
117798
noun
11529
verb
adjective 21479
adverb
totals

4481
155287

synsets

82115
13767
18156
3621
117659

total
word-sense pairs
146312
25047
30002
5580
206941

   

s: (n) china, people's republic of china, mainland china, communist china, red china, prc, cathay (a 
communist nation that covers a vast territory in eastern asia; the most populous country in the world)

authors:

    the project began in the princeton university department of psychology, and is 

currently housed in the department of computer science.

urls:

    homepage: http://id138.princeton.edu/id138/about-id138/
    download: http://id138.princeton.edu/id138/download/

knowitall: extract high-quality knowledge
from the web [banko et al. 2007, etzioni et al. 2011]

brief introduction:

    openie distills semantic relations from

web-scale natural language texts.
    textrunner -> reverb -> open ie, 

part of knowitall

statistics:

    yielding over 5 billion extraction from 

over a billion web pages.

sample:

    from    u.s. president barack obama gave his inaugural address on january 20, 2013.   

to: (barack obama; is president of; u.s.)

(barack obama; gave; [his inaugural address, on january 20, 2013])

news:

    openie v4.1.3 has been released.

authors:

    turing center at the university of washington

urls:

    http://openie.allenai.org/
    http://reverb.cs.washington.edu/

nell: never-ending language learning [carlson et al. 2010]

brief introduction:

    nell is a research project that attempts to create a computer system 

that learns over time to read the web. since january 2010.

statistics:

    over 50 million candidate beliefs by reading the web. they are 

considered at different levels of confidence.

    out of 50 million, high confidence in 2,817,156 beliefs. 

sample:

nell: never-ending language learning

news:

   

it is continually learning facts on web. resources is publicly available.

authors:

    nell research team at cmu.

urls:

    homepage: http://rtw.ml.cmu.edu/rtw/
    download: http://rtw.ml.cmu.edu/rtw/resources

probase [wu et al. 2012]

brief introduction:

    probase is a semantic network to make machines    aware    of the mental world of 

human beings, so that machines can better understand human communication.

probase network:

   
   
   

5,401,933 unique concepts 
12,551,613 unique instances
87,603,947 isa relations

nodes:

concepts

entities

(   spanish artists   )

(   pablo picaso   )

attributes
(   birthday   )

verbs/adjectives

(   eat   ,    sweet   )

edges:

isa

(concept, entities)

ispropertyof

(attributes)

co-occurrence
(isceoof, locatedin, etc)

probase

concepts:

countries

basic watercolor techniques

celebrity wedding dress designers

probase isa error rate: <%1 @1 and <10% for random pair

authors: 

    microsoft research

urls: 

    public release: coming soon in aug./sept. 2016 
    project homepage: http://research.microsoft.com/probase/

freebase [bollacker et al. 2008]

brief introduction:

    freebase is a well-known collaborative knowledge base consisting of 

data composed mainly by its community

statistics:

    freebase contains more than 23 million entities.
    freebase contains 1.9 billion triples.
    each triple is organized as form of 

<subject>    <predicate>    <object>

    freebase is a collection of facts.
    freebase only contains nodes 

and links.

    freebase is a labeled graph.

freebase -> wiki data

news:

    freebase data was integrated into wikidata
    the freebase api will be completely shut-down on aug 31 2016, 

replaced by google id13 api

authors:

    freebase community

urls:

    homepage: http://wiki.freebase.com/wiki/main_page
    download: https://developers.google.com/freebase/
    wikidata: https://www.wikidata.org/

google id13

brief introduction:

    id13 is a knowledge base used by google to enhance its search engine's 

search results with semantic-search information gathered from a wide variety of 
sources.

statistics:

    570 million objects and more than 18 billion facts about relationships
between different objects.

sample:

authors:

    google inc.

urls:

    homepage: 

https://www.google.com/intl/es419/insidesearch/features/search/knowledge.html

yago [suchanek et al. 2007]

brief introduction:

    yago is a huge semantic knowledge base, derived from geonames id138 and 
wikipedia (10 wikipedias in different languages). 

statistics:

    more than 10 million entities.(persons, 
organizations, cities, etc)
    more than 120 million facts about entities.
    more than 35,000 classes assigned to 
entities.
    many of its facts and entities are attached 
a temporal dimension and a spatial dimension.

sample:

<albert_einstein>   <ismarriedto>   <elsa_einstein>

yago

news:

    an evaluated version of yago3 (combining information from wikipedia from different
languages) is released. [15 sep. 2015]

authors:

    max planck institute for informatics in saarbr  cken/germany and 
dbweb group at t  l  com paristech university.

urls:

    homepage: http://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/yago

    download: http://www.mpi-inf.mpg.de/departments/databases-and-

information-systems/research/yago-naga/yago/downloads/

outline

    knowledge bases
    explicit representation models
    applications 

statistics of search queries

2%

7%

1%

17%

44%

29%

(a) by traffic

pok  mon go   microsoft hololens

instance 1

instance 2

1 term
1 instance
2 terms
2 instances
3 terms
3 instances
4 terms
4 instances
5 terms
5 instances
more than 5 terms
more than 5 instances

4%

10%

7%

19%

26%

1 term
1 instance
2 terms
2 instances
3 terms
3 instances
4 terms
4 instances
5 terms
5 instances
more than 5 terms
more than 5 instances

34%

(b) by # of distinct queries

if the short text is a single instance   

    python
    microsoft
    apple
       

single instance understanding

    is this instance ambiguous?

    what are its basic-level concepts?

    what are its similar instances?

word ambiguity 
    id51: rely on dictionaries 

(id138)

take a seat on 
this chair

the chair of the 
math department

instance ambiguity
    instance sense disambiguation: extra knowledge 

needed 

i have an apple pie 
for lunch

he bought an apple ipad

?

here    apple    is a proper noun!

ambiguity [hua et al. 2016] 

    many instances are ambiguous

short text

instance

population china

glass vs. china

pear apple

microsoft apple

china

china

apple

apple

read harry potter

harry potter

watch harry potter

harry potter

sense

country

fragile item

fruit

company

book

movie

age of harry potter

harry potter

character

    intuition: ambiguous instances have multiple senses

pre-definition for ambiguity (1): 
sense [hua et al. 2016] 

    what is a sense in semantic networks?
    a sense as a hierarchy of concept clusters

region

creature

crop

food

animal

country

state

city

fruit

vegetable meat

germany

predator

pre-definition for ambiguity (2): 
concept cluster [li et al. 2013, li et al. 2015] 

    what is a concept cluster (cl)?

    cluster similar concepts into a concept cluster using k-

means like approach (k-medoids)

company

client
firm

manufacturer
corporation

large company

rival
giant

big company
local company

large corporation

international 

company

   

company

fruit

fresh fruit

juice

tropical fruit

berry

exotic fruit

seasonal fruit

fruit juice
citrus fruit
soft fruit
dry fruit
wild fruit
local fruit

   

fruit

definitions of instance ambiguity 
[hua et al. 2016] 

    3 levels of instance ambiguity

    level 0: unambiguous
    contains only 1 sense
    e.g., dog (animal), beijing (city), potato (vegetable)

    level 1: unambiguous and ambiguous both make sense

    contains 2 or more senses, but these senses are related
    e.g., google (company & search engine), french (language & 

country), truck(vehicle & public transport service)

    level 2: ambiguous 

    contains 2 or more senses, and the senses are very different from 

each other

    e.g., apple (fruit & company), jaguar(animal & company), python

(animal & language)

ambiguity score

    using top-2 senses to calculate the ambiguity score

0,
         2     
         1     

                     =

    1                                                  1,     2

,

                     = 0

                     = 1

score = 1 +

    (        2|    )
    (        1|    )

    1                                                      1,         2

,

                     = 2

* denote top-2 senses as     1 and     2, top-2 sense clusters as         1 and         2
* denote similarity of two sense clusters as the maximum similarity of their senses:

                                                 1,         2 =             {                                        (                     1,                      2)}

* for an entity     , denote the weight (popularity) of a sense          as the sum of weights of its concept 
clusters:

             |     =              |     =    

    (            |    )

                       

* for an entity     , denote the weight (popularity) of a sense cluster              as the sum of weights of its 
senses:

                       =    

    (        |    )

                       

examples

    level 0:

    california

    country; state; city; region; institution: 0.943

    fruit

   

food; product; snack; carbs; crop: 0.827

    alcohol

    substance; drug; solvent; food; addiction: 0.523

    computer

    device; product; electronics; technology; appliance: 0.537

    coffee

    beverage; product; food; crop; stimulant: 0.73

    potato

    vegetable; food; crop; carbs; product: 0.896

    bean

   

food; vegetable; crop; legume; carbs: 0.801

examples (cont.)

    level 1:

    nike, score = 0.034

company; store: 0.861

   
    brand: 0.035
   

shoe; product: 0.033

   

twitter, score = 0.035

    website; tool: 0.612
    network: 0.165
   
   

application: 0.033
company: 0.031

   

facebook, score = 0.037
    website; tool: 0.595
    network: 0.17
   
   

company: 0.053
application: 0.029

    yahoo, score = 0.38

search engine: 0.457
company; provider; account: 0.281

   
   
    website: 0.0656

    google, score = 0.507

search engine: 0.46
company; provider; organization: 0.377

   
   
    website: 0.0449

examples (cont.)

    level 2:

    jordan, score = 1.02

    country; state; company; regime: 0.92
    shoe: 0.02

    fox, score = 1.09

    animal; predator; species: 0.74
    network: 0.064
    company: 0.035
    puma, score = 1.15

    brand; company; shoe: 0.655
    species; cat: 0.116

    gold, score = 1.21

    metal; material; mineral resource; mineral:0.62
    color: 0.128

examples (cont.)

    level 2:

    soap, score = 1.22

    product; toiletry; substance: 0.49
    technology; industry standard: 0.11

    silver, score = 1.24

    metal; material; mineral resource; mineral: 0.638
    color: 0.156

    python, score = 1.29

    language: 0.667
    snake; animal; reptile; skin: 0.193

    apple, score = 1.41

    fruit; food; tree: 0.537
    company; brand: 0.271

single instance

    is this instance ambiguous?

    what are its basic-level concepts?

    what are its similar instances?

a concept view of    microsoft   

software
company

company

international 

company

technology 

leader

largest desktop os vendor

basic-level conceptualization (blc)

[rosch et al. 1976]

basic-level 
conceptualization

software company

company

   

   

largest desktop os vendor

kfc

bmw

microsoft

how to make blc?

    naive approaches

    typicality: an important measure for understanding the 

relationship between an object and its concept

    pointwise mutual information (pmi): a common 

measure of the strength of association between two 
terms

naive approach 1: typicality

bird

country

usa

seychelles

robin

penguin

   robin    is a more typical bird than a    penguin   

p(robin|bird) > p(penguin|bird)

   usa    is a more typical country than    seychelles   

p(usa|country) > p(seychelles|country)

using typicality for blc

    associate each isa relationship (     is     ) with typicality scores 

               and               

               =

         ,     
         

    (    |    ) =

         ,     
    (    )

    p(e|c) indicates how typical (or popular) e is in the given concept c
    p(c|e) indicates how typical (or popular) the concept c is given e

    however,

company

largest desktop os vendor

high typicality p(c|e)

high typicality p(e|c)

microsoft

naive approach 2: pmi

[manning and schutze 1999] 

    pointwise mutual information (pmi), is a measure of 

association used in id205 and statistics

    consider using the pmi between concept c and instance e to 

find the basic-level concepts as follows:

            (    ,     ) = log

    (    ,     )

    (    )    (    )

= log    (    |    )     log    (    )

    however, 

    in basic level of categorization, we are interested in finding a 

concept for a given e, which means p(e) is a constant

    thus, ranking by pmi(e, c) is the same as ranking by p(e|c)

using rep(e, c) for blc [wang et al. 2015b]

    the measure                  ,      =     (    |    )         (    |    ) means:

given e, the c should be its typical 
concept (shortest distance)

given c, the e should be its typical 
instance (shortest distance)

a process of finding concept nodes having 

shortest expected distance with e

    (with pmi) if we take the logarithm of our scoring function, we get:

    (    ,     )
    (    )

   

    (    ,     )
    (    )

= log

    (    ,     )2
    (    )    (    )

=                  ,      + log          ,     

log                 ,      = log                       (    |    ) = log
=             2

    (with commute time) the commute time between an instance e and a 

concept c is:

                (    ,     ) =    
    =1

   

    
(2    )             (    ,     ) =    
    =1

   
2                      ,      +    
    =    +1

2                      ,     

    

          =1

(2    )             (    ,     ) + 2(     + 1)     (1           =1

    

        (    ,     )) = 4     2                 (    ,     )

evaluations on different measures for blc

ndcg
no smoothing
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-3
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-4
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-5
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-6
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-7
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)

1
0.516
0.725
0.599
0.297
0.401
0.758

0.374
0.484
0.692
0.297
0.703
0.621

0.407
0.648
0.747
0.297
0.791
0.758

0.429
0.725
0.813
0.297
0.709
0.791

0.516
0.725
0.791
0.297
0.495
0.758

0.516
0.725
0.670
0.297
0.423
0.758

2
0.531
0.664
0.597
0.380
0.386
0.771

0.414
0.511
0.652
0.380
0.697
0.580

0.430
0.604
0.777
0.380
0.795
0.714

0.465
0.647
0.779
0.380
0.728
0.787

0.510
0.655
0.766
0.380
0.516
0.784

0.531
0.664
0.655
0.380
0.421
0.771

3
0.519
0.652
0.579
0.409
0.396
0.745

0.441
0.509
0.607
0.409
0.704
0.554

0.458
0.579
0.761
0.409
0.802
0.711

0.478
0.642
0.778
0.409
0.735
0.762

0.515
0.651
0.732
0.409
0.520
0.767

0.519
0.652
0.633
0.409
0.415
0.745

5
0.531
0.660
0.554
0.422
0.398
0.723

0.448
0.502
0.603
0.422
0.681
0.561

0.462
0.575
0.737
0.422
0.767
0.689

0.501
0.642
0.765
0.422
0.722
0.739

0.526
0.654
0.728
0.422
0.508
0.755

0.530
0.658
0.604
0.422
0.407
0.725

10
0.562
0.628
0.540
0.438
0.401
0.656

0.473
0.519
0.585
0.438
0.637
0.554

0.492
0.578
0.700
0.438
0.738
0.653

0.517
0.627
0.730
0.438
0.702
0.707

0.546
0.641
0.673
0.438
0.512
0.691

0.562
0.630
0.575
0.438
0.414
0.663

15
0.574
0.631
0.539
0.446
0.410
0.647

0.481
0.525
0.585
0.446
0.628
0.555

0.503
0.576
0.685
0.446
0.729
0.636

0.528
0.624
0.723
0.446
0.696
0.703

0.563
0.631
0.659
0.446
0.521
0.686

0.571
0.631
0.570
0.446
0.424
0.661

20
0.594
0.646
0.549
0.461
0.428
0.661

0.495
0.533
0.592
0.460
0.626
0.559

0.512
0.590
0.688
0.461
0.724
0.653

0.545
0.638
0.729
0.461
0.703
0.706

0.579
0.649
0.668
0.461
0.540
0.694

0.592
0.647
0.581
0.461
0.438
0.668

precision

no smoothing
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=0.001
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=0.0001
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-5
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-6
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)
smoothing=1e-7
mi(e)
pmi3(e)
npmi(e)
typicality p(c|e)
typicality p(e|c)
rep(e)

1
0.769
0.885
0.692
0.462
0.500
0.846

0.577
0.731
0.923
0.462
0.885
0.846

0.615
0.846
0.885
0.462
0.885
0.923

0.615
0.885
0.885
0.462
0.769
0.885

0.769
0.885
0.885
0.462
0.538
0.846

0.769
0.885
0.769
0.462
0.500
0.846

2
0.692
0.769
0.692
0.577
0.462
0.865

0.615
0.673
0.827
0.577
0.865
0.731

0.615
0.731
0.904
0.577
0.904
0.846

0.635
0.769
0.846
0.577
0.808
0.904

0.673
0.769
0.846
0.577
0.615
0.885

0.692
0.769
0.750
0.577
0.481
0.865

3
0.705
0.756
0.667
0.603
0.526
0.872

0.628
0.692
0.769
0.603
0.872
0.718

0.654
0.731
0.885
0.603
0.910
0.833

0.667
0.744
0.872
0.603
0.846
0.872

0.705
0.756
0.821
0.603
0.615
0.897

0.705
0.756
0.718
0.603
0.526
0.872

5
0.685
0.800
0.638
0.577
0.523
0.862

0.600
0.654
0.746
0.577
0.831
0.723

0.608
0.715
0.869
0.577
0.877
0.815

0.662
0.777
0.869
0.577
0.823
0.862

0.677
0.785
0.815
0.577
0.615
0.877

0.685
0.792
0.700
0.577
0.523
0.854

10
0.719
0.754
0.627
0.569
0.523
0.758

0.612
0.669
0.731
0.569
0.785
0.700

0.635
0.723
0.823
0.569
0.831
0.781

0.677
0.758
0.831
0.569
0.808
0.812

0.700
0.773
0.750
0.569
0.608
0.788

0.719
0.758
0.650
0.569
0.531
0.765

15
0.705
0.733
0.610
0.564
0.510
0.731

0.605
0.644
0.695
0.564
0.741
0.669

0.628
0.685
0.777
0.564
0.813
0.736

0.656
0.731
0.810
0.564
0.782
0.800

0.692
0.726
0.726
0.564
0.613
0.777

0.703
0.736
0.641
0.564
0.523
0.749

20
0.690
0.721
0.610
0.556
0.521
0.719

0.592
0.623
0.671
0.554
0.704
0.638

0.612
0.677
0.752
0.556
0.777
0.719

0.646
0.710
0.787
0.556
0.765
0.767

0.679
0.723
0.719
0.556
0.615
0.765

0.688
0.725
0.633
0.556
0.523
0.733

single instance

    is this instance ambiguous?

    what are its basic-level concepts?

    what are its similar instances?

what is the semantic similarity?
    are the following instance pairs similar?

    <apple, microsoft>

    <apple, pear>

    <apple, fruit>

    <apple, food>

    <apple, ipad>

    <car, journey>

approaches on term similarity 

    categories of approaches for semantic similarity

    string based approach
    knowledge based approach

    use preexisting thesauri, taxonomy or encyclopedia, such as 

id138;

    corpus based approach

    use contexts of terms extracted from web pages, web search 

snippets or other text repositories;

    embedding based approach

    will introduce in detail in    part 3: implicit understanding   

79

approaches on term similarity (2)
    categories

80

knowledge based approaches (id138)corpus based approachespath length/lexical chain-basedinformation content-basedgraph learning algorithm basedsnippet search basedrada 1989resnik1995jcn1997lin1998s  nch2011agirre2010alvarez2007string based approacheshuntray, 2005hirst1998do2009bol2011chen2006state-of-the-art approachesban 2002term similarity using semantic 
networks [li et al. 2013, li et al. 2015] 
    framework

step 1: type checking

step 2: context 

representation
(vector) 

step 3: context similarity

83

term pairs <t1, t2>type checkingconcept pairsentity pairsentity-distribution context collectionconcept-distribution context collectionconcept-entity pairsconcept collection for the entity term t1similarity evaluationcosine(t(t1), t(t2))for each pair<t2,cx>context vector t(t1) and t(t2)get max{sim(t2,cx)} for <t1, t2>endendconcept id91cluster context vector cx(t1) and cy(t2)similarity evaluationmax(x,y) {cosine(cx(t1), cy(t2))}endconcept id91for each cluster ci(t1)select top k concept,  namely {cx}an example [li et al. 2013, li et al. 2015] 

for example:
<banana, pear>

step 1: type checking

step 2: context 

representation
(vector) 

step 3: context similarity

88

<banana, pear>entity pairstype checkingconcept context collectionsimilarity evaluation, cosine(t(t1), t(t2)): 0.916examples

term 1

lunch

tiger

car

television

term 2

dinner

jaguar

plane

radio

technology company microsoft

high impact sport

competitive sport

employer

fruit

travel 

music

large corporation

green pepper

meal

lunch

alcoholic beverage

sports equipment

company

table tennis

similarity

0.9987

0.9792

0.9711

0.9465

0.8208

0.8155

0.5353

0.2949

0.0426

0.0116

0.0314

0.0003

http://adapt.seiee.sjtu.edu.cn/similarity/simcompleteresults.pdf

96

statistics of search queries

2%

7%

1%

17%

44%

29%

(a) by traffic

pok  mon go   microsoft hololens

instance 1

instance 2

1 term
1 instance
2 terms
2 instances
3 terms
3 instances
4 terms
4 instances
5 terms
5 instances
more than 5 terms
more than 5 instances

4%

10%

7%

19%

26%

1 term
1 instance
2 terms
2 instances
3 terms
3 instances
4 terms
4 instances
5 terms
5 instances
more than 5 terms
more than 5 instances

34%

(b) by # of distinct queries

if the short text has context for the instance   

    python tutorial
    dangerous python
    moon earth distance
       

short text understanding

    how to segment this short text?

    what does this short text mean (its intent, senses, 

or concepts)?

    what are the relations among terms in the short 

text?

    how to calculate the similarity between short 

texts?

supervised segmentation [bergsma et al. 2007]

    problem: divide query into semantic units

example query:

two man power saw

[two man] [power saw]
[two] [man] [power saw]
[two] [man power] [saw]

    approach: turn segmentation into position-based 

binary classification

input: a query and its positions

output: the decision for making segmentation at each position

supervised segmentation

    features

    decision boundary features
    statistical features

e.g. indicators: the
pos tags in query: is
position features: forward/backward

mutual information between 
left and right parts

    context features
    dependency features

context information

bank loan amortization schedule

depend

female

bus  driver

supervised segmentation

    segmentation overview

input query: two man power saw

two

man

power

saw

learning features

id166

classifier

output: segmentation decision for each position (yes/no)

unsupervised segmentation [tan et al. 2008]

    unsupervised learning for query segmentation

id203 of generated segmentation s for query q

              =          1 p     2|    1     p              1    2                1

    (        )

       
               

unigram model

segments

valid segment boundary: if and only if the pointwise mutual information 
between the two segments resulting from the split is negative:

                 ,         +1 = log

        ([        ,         +1])

                               (        +1)

< 0

example:

log

        (["            ", "                "])

        ("             ")              ("                ")

> 0

new york

times

subscription

    1

    2

no segment boundary here

unsupervised segmentation

    find top k segmentations: id145

words in a query:

input: query      1    2             , concept id203 distribution

output: top k segmentations with highest likehood

    using em optimization on the fly

exploit click-through [li et al. 2011]

    motivation

    probabilistic query segmentation

input query:  bank of america online banking

output top-3 segmentation:
{[bank of america] [online banking], 0.502}, 
{bank of america online banking], 0.428}, 
{[bank of ] [ america] [online banking], 0.001}

    use click-through data

click data

query

q -> {url} -> d 

document

exploit click-through

    segmentation model

an interpolated model:

global info

global info

click-through
info

query

[credit card] [bank of america]

click-through 
info

clicked html documents

1. bank of america credit cards contact 
us overview
2. secured visa credit card from bank of 
america
3.    credit cards overview find the right 
bank of america credit card for you

short text understanding

    how to segment this short text?

    what does this short text mean (its intent, senses, 

or concepts)?

    what are the relations among terms in the short 

text?

    how to calculate the similarity between short 

texts?

sense changes with different context 

watch harry potter

read harry potter

age harry potter

harry potter walkthrough

movie

book

character

game

entity recognition in query [guo et al. 2009]

    motivation

detect named entity in a short text and categorize it

single-named-entity query

triple <e, t, c>

ambiguous term

class of entity

context terms

example:

harry potter walkthrough

(   harry potter   ,    # walkthrough   ,    game   )

term

context

class

entity recognition in query

    probabilistic generative model

goal: given a query q, find triple <e, t, c>  maximize the id203 

objective: given query q, find:

id203 to generate triple:

assume context only depends on class

e.g.    walkthrough    only depends 
on game, instead of happy potter

the problem then becomes how to estimate pr(e), pr(c|e) and pr(t|c)

entity recognition in query

    id203 estimation by learning

learning objective: 

challenge: difficult as well as 
time consuming to manually 
assign class labels to named 
entities in queries

new learning problem: 

build training set      = {(        ,         )}, 
view          as a hidden variable

solved with topic model ws-lda

      n1iiii)c,t,p(emax                         n1iiin1iin1iiic)|)p(te|p(c)p(emax)t,p(emax csignal from click [pantel et al. 2012]

    motivation

predict entity type in web search

entity

entity type

context

click

generative model

user intent

query type distribution 

(73 types)

signal from click

    joint model for prediction

for each query

pick type

  

t

entity distribution

  

t

pick entity

e

pick intent
i

distribution over types

intent distribution

  

t

  

k

pick click

word distribution

host distribution

pick context words

n

2

c

q

  

k

telegraphic query interpretation 
[sawant et al. 2013, joshi et al. 2014]

    entity-seeking telegraphic queries

query

germany 

capital

result 
entity

berlin

    interpretation = segmentation + annotation

accuracy

recall

knowledge base

large corpus

joint interpretation and ranking 
[sawant et al. 2013, joshi et al. 2014]

    overview

telegraphic query

annotated corpus

generative model

discriminative model

e1
e2
e3

output

two models for interpretation and ranking

joint interpretation and ranking 
[sawant et al. 2013]

    generative model

based on probabilistic language models

e

san diego padres

type

context

t

major league 
baseball team

padres have been to two world 
series, losing in 1984 and 1998

model

   

type hint : 

baseball , team

switch
z

   

model

context matchers : 

lost , 1998, world series

q losing team baseball world series 1998
losing team baseball world series 1998

borrow from u. sawant (2013)

joint interpretation and ranking 
[sawant et al. 2013]

    discriminative model

based on max-margin discriminative learning

correct entity

incorrect entity

san_diego_padres

1998_world_series

losing team baseball world 

losing team baseball world 

series 1998

losing team baseball world 

series 1998
(baseball team)
series 1998
(baseball team)
(t = baseball team)

losing team baseball world 

losing team baseball world 

series 1998

losing team baseball world 

series 1998
(series)
series 1998
(series)
(t = series)

telegraphic query interpretation 

[joshi et al. 2014]
    queries seek answer entities (e2)
    contain (query) entities (e1) , target types (t2), 

relations (r), and selectors (s).

query

e1

r

dave 
navarro 
first band

dave navarro

band

dave navarro

-

t2

band

band

spider 
automobile 
company

spider

automobile 
company

automobile 
company

s

first

first

-

automobile

company

company

spider

borrow from m. joshi (2014)

improved generative model

    generative model

[sawant et al. 2013] 

[joshi et al. 2014]

consider e1
(in q) and r

improved discriminative model

    discriminative model

[sawant et al. 2013] 

[joshi et al. 2014]

consider e1
(in q) and r

understand short texts with a multi-
tiered model [hua et al. 2015 (icde best paper)] 

    input: a short text
    output: semantic interpretation
    three steps in understanding a short text

wanna watch eagles band

watch[verb] eagles[entity](band) band[concept]

wanna watch eagles band

watch[verb] eagles[entity](band) band[concept]

step 1: text segmentation     divide into 
a sequence of terms in vocabulary

step 3: concept labeling     infer the best 
concept of each entity within context

watch eagles band

watch[verb] eagles[entity] band[concept]

step 2: type detection     determine the 
best type of each term

text segmentation
    observations:

    mutual exclusion     terms containing the same word mutually exclude 

each other

    mutual reinforcement     related terms mutually reinforce each other

    build a candidate term graph (ctg)

   vacation april in paris   

   watch harry potter   

april in paris

2/3

harry potter

2/3

1/3

april

0.005

0.047

1/3

paris

1/3

harry

0.092

1/3

0.053

potter

0.029

0.041

0.014

0.018

vacation

1/3

watch

1/3

find best segmentation

    best segmentation= sub-graph in ctg which

    is a complete graph (clique)

    no mutual exclusion

is a segmentation

    has 100% word coverage

    except for stopwords

    has the largest average edge weight

best segmentation

april in paris

2/3

harry potter

2/3

1/3

april

0.005

0.047

1/3

paris

1/3

harry

0.092

1/3

0.053

potter

0.029

0.041

0.014

0.018

vacation

1/3

watch

1/3

find best segmentation

    best segmentation= sub-graph in ctg which

    is a complete graph (clique)

    no mutual exclusion

maximal clique

    has 100% word coverage

    except for stopwords

    has the largest average edge weight

best segmentation

april in paris

2/3

harry potter

2/3

1/3

april

0.005

0.047

1/3

paris

1/3

harry

0.092

1/3

0.053

potter

0.029

0.041

0.014

0.018

vacation

1/3

watch

1/3

type detection

    pairwise model

    find the best typed-term for each term, so that the 

maximum spanning tree of the resulting sub-graph between 
typed-terms has the largest weight

watch[c]

watch[e]

watch[v]

watch

movie[c]

movie[e]

movie

free[adj]

free[v]

free

concept labeling

    entity disambiguation is the most important task of 

concept labeling

    filter/re-rank of the original concept cluster vector

watch harry potter

read harry potter

movie

book

    weighted-vote

    the final score of each concept cluster is a combination
of its original score and the support from context using 
concept co-occurrence

example of entity disambiguation
[hua et al. 2015 (icde best paper), hua et al. 2016] 

co-occurrence 

network

short 
text

   ipad apple   

ipad

apple

is-a

is-a

parsing 

term id91 
by isa

concept filtering 
by co-occurrence

head/modifier 
analysis

concept 
orthogonalization

conceptualization

c1, p1 
c2, p2 
c3, p3 
   

concept 
vector

device

   

product

   

co-occur

fruit

   

food

   

company

   

product

   

filtering

semantic network

device

   

product

   

brand

   

company

   

mining lexical relationships

[wang et al. 2015b] 

    lexical knowledge represented by the probabilities

watch   harry   potter

                                                         

                                                                  

product

                                         

book

                                                                      

   

              

verb

                                             ,                 

movie

               =
              ,      =                                 

   

e: instance
t: term
c: concept
z: role

              ,     

   

understanding queries [wang et al. 2015b] 
    goal: to rank the concepts and find:
    (    |    ,     )

arg max

query

    

all possible 
segmentations

the offline semantic network

random walk with restart [sun et al., 2005]

on the online subgraph

short text understanding

    how to segment this short text?

    what does this short text mean (its intent, senses, 

or concepts)?

    what are the relations among terms in the short 

text?

    how to calculate the similarity between short 

texts?

head, modifier, and constraint 
detection in short texts [wang et al. 2014b] 
    example:    popular smart cover iphone 5s   

    definition: 

    head acts to name the general (semantic) category to which the 

whole short text belongs. usually, the head is the intent of the 
short text.

       smart cover   : intent of the query

    constraints distinguish this member from other members of the 

same category. 

       iphone 5s   : limit the type of the head

    non-constraint modifiers (a.k.a. pure modifiers) are subjective

modifiers which can be dropped without changing intent

       popular   : subjective, can be neglected

non-constraint modifiers mining: 
construct modifier networks

edges form a 
modifier network

concept hierarchy tree in 
   country    domain

modifier network in 
   country    domain
in this case,    large    and 
   top    are pure modifiers

countryasian countrydeveloped countrywestern countryasiandevelopedwesternwestern developed countrytop western countrylargelargetoptopwesternlarge asian countrylarge developed countrytop developed countrycountryasianwesterndevelopedlargetopnon-constraint modifiers mining: 
betweenness centrality
    betweenness centrality is a measure of a node's centrality in 

a network

    betweennes of node v is defined as:

    where              is the total number of shortest paths from node s to node  t and              (    ) is the number of 

those paths that pass through v

    id172 & aggregation

    for a pure modifier, it should have low betweenness centrality 

aggregation score pms(t)

head-constraints mining [wang et al. 2014b]

    a term can be a head sometimes, and be a 

constraint in some other cases

    e.g. 

seattle  hotel

seattle  hotel  job

constraint

head

constraint

constraint

head

head-constraints mining: 
acquiring concept patterns

building concept pattern dictionary: 

get entity pairs 
from query log

conceptualization

query logs

cover for iphone 6s,
battery for sony a7r
wicked on broadway

extract patterns

a for b, a of b,
a with b, a in b,
a on b, a at b    

entity 1/head 

entity 2/constraint

concept patterns 

for each 

prepositions

entity1

concept1,1

concept1,2

concept1,3

concept1,4

concept2,1

concept2,2

concept2,3

entity2

concept pattern 
dictionary

(concept1,1, concept2,1), (concept1,1, concept2,2)
(concept1,1, concept2,3)   

why concepts can   t be too 
general
    it may cause too many concept pattern conflicts: 

can   t distinguish head and modifier for general 
concept pairs

derived concept pattern

device

head

supporting entity pairs

iphone 4

modem

wireless router

iphone 4

head

derived concept pattern

company

supporting entity pairs

amazon books

netflix

skype

netflix

modifier

company

verizon

comcast

comcast

tmobile

modifier

device

kindle

touchpad

windows phone

ps3

conflict

why concepts can   t be too 
specific
    it may generate concepts with little coverage

   

device

device

device

device

   

   

largest desktop os vendor

largest software development company

largest global corporation

latest windows and office provider

   

    concept regresses to entity

    large storage space: up to (million * million) patterns

basic-level conceptualization (blc) is 

a good choice [wang et al. 2015b]

top concept patterns

cluster size

sum of cluster score

head;constraint;score

615
296
153
70
22
34
42
16
32
18
20
27
19
25
16
18
14
11
29
23
15
18
22
19
14
18
13
9
16

21146.91
7752.357
3466.804
1182.59
1010.993
948.9159
899.2995
742.1599
710.403
669.2376
644.4603
599.4205
591.3545
584.8804
546.5424
480.9389
473.0792
453.6199
435.0954
399.4886
386.065
378.213
372.2948
340.8953
330.5753
321.4226
318.0272
316.973
314.875

breed;state;3572.98460224501

game;platform;627.403476771856
accessory;vehicle;533.93705094809
browser;platform;132.612807637391
requirement;school;271.407526294823

drug;disease;154.602405333541

cosmetic;skin condition;81.4659415003929

job;city;279.03732555528

accessory;phone;246.513830851194
software;platform;210.126322725878

test;disease;239.774028397537
clothes;breed;98.773996282851
penalty;crime;200.544192793488

tax;state;240.081818612579

sauce;meat;183.592863621553

credit card;country;142.919087972152

food;holiday;145.54140330924
mod;game;257.163856882439

garment;sport;47.1533326845442

career information;professional;73.2726483731257

song;instrument;128.189481818135

bait;fish;78.0426514113169

study guide;book;50.8339765053921
plugins;browser;55.0326072627126

recipe;meat;88.2779863422951

currency;country;110.825444188352

lens;camera;186.081673263957

decoration;holiday;130.055844126533

food;animal;73.38544366514

game

game

platform

device

video game

platform

game

game

console game pad

gaming platform

detection

game (head)

platform (modifier)

angry birds

android

angry birds

ios

angry birds

windows 10

   

   

head modifier relationship

    train a classifier on

(head-embedding, modifier-embedding)

    training data: 

    positive (head, modifier)
    negative (modifier, head)

    precision >= 0.9, recall >= 0.9

    disadvantage: not interpretable

syntactic parsing based on hm

    information is incomplete:

    preposition, and other function words
    within a noun compound: el capitan, macbook pro

    why not train a parser for web queries?

syntactic parsing of short texts

[sun et al. emnlp 2016]

    syntactic structures are valuable for short text 

understanding

    examples:

challenges: short texts lack 
grammatical signals
    lack function words, word order

       toys queries    has ambiguous intent

       distance earth moon    has clear intent

    many equivalent forms:    earth moon distance   ,    earth 

distance moon   ,    

challenges: syntactic parsing of 
queries

    no standard

    no ground-truth

why is syntactic parsing of queries even a 
legitimate problem?

derive syntax from semantics

[sun et al. 2016]

    query    thai food houston   

    clicked sentence:

    project dependency to the query:

a treebank for short texts

    given query     

    given        s clicked sentence {    }

    parse each     

    project dependency from      to     

    aggregate dependencies 

algorithm of projection

result examples

results

    random queries:

queryparser uas: 0.83, las: 0.75
stanford          uas: 0.72, las: 0.64

    queries with no function words:

queryparser uas: 0.82, las: 0.73
stanford           uas: 0.70, las: 0.61

    queries with function words:

queryparser uas: 0.90, las: 0.85
stanford           uas: 0.86, las: 0.80

short text understanding

    how to segment this short text?

    what does this short text mean (its intent, senses, 

or concepts)?

    what are the relations among terms in the short 

text?

    how to calculate the similarity between short 

texts?

short text similarity using word 
embedding[kenter and rijke 2015]
    measuring similarity between two short texts and 

sentences

    basic idea: word-by-word comparison using  

embedding vector

short text similarity using word 
embedding[kenter and rijke 2015]
    use saliency-weighted semantic graph to computer 

similarity

features acquired:

    bins of all edges
    bins of max edges

similarity measurement: 

inspired by bm25

                (        ,         ) =

   

            (    )    

               

short texts

term

)
            (    ,         )     (    1 + 1

            (    ,         ) +     1     (1          +         

|        |
                

   

semantic 
similarity

from the concept view

from the concept view [wang et al. 2015a] 

co-occurrence 

network

short 
text 1

short 
text 2

parsing 

term id91 
by isa

concept filtering 
by co-occurrence

head/modifier 
analysis

concept 
orthogonalization

concept vector 1:

[(c1, score1), (c2, score2)   ]

bags of concepts

similarity

conceptualization

concept vector 2:

[(c1   , score1   ), (c2   , score2   )   ]

semantic network

outline

    knowledge bases
    explicit representation models
    applications

applications

    explicit short text understanding benefit lot of 

application scenarios:

    ads/search semantic match
    definition mining
    query recommendation 
    web table understanding 
    semantic search
       

ads keyword selection [wang et al. 2015a]

ads keyword selection [wang et al. 2015a]

6.00%

5.00%

4.00%

3.00%

2.00%

1.00%

0.00%

0.60%

0.50%

0.40%

0.30%

0.20%

0.10%

0.00%

decile 4

decile 5

decile 6

decile 7

decile 8

decile 9

decile 10

mainline ads

sidebar ads

decile 4

decile 5

decile 6

decile 7

decile 8

decile 9

decile 10

definition mining [hao et al. 2016] 

    definition scenarios: search engines, qna, etc. 

    why conceptualization is useful for definition mining?

    examples:    what is emphysema   

answer 1:

emphysema is a disease largely associated with smoking and strikes about 2 
million americans each year.

    this sentence has the form of definition
    embedding is helpful to some extent, but it also return high similarity 

score for (emphysema, disease) and (emphysema, smoking) 

answer 2:

emphysema is an incurable, progressive lung disease that primarily affects 
smokers and causes shortness of breath and difficulty breathing.

    conceptualization can provide strong semantics
    contextual embedding can also provide semantic similarity beyond is-a

definition mining [hao et al. 2016] 

concept based short text classification 
and ranking [wang et al. 2014a] 

offlineofflineonlineonlineoriginal short text:justin bieber graduates   knowledge baseconceptualiztionconcept vectorentity extractioncandidates generationclassification & rankingmodel learningmodel learningconcept weightingmodelmodel nmodel iconcept model concept model class 1class nclass itrainingdata<music , score>concept based short text classification and 
ranking [wang et al. 2014a] 

concept space

article titles/tags
in this category

category

tv

        

        

concept based short text classification and 
ranking [wang et al. 2014a] 

category

tv

music

movie

   

concept space

        

        

   

concept based short text classification and 
ranking [wang et al. 2014a] 

category

tv

music

movie

   

concept space

        

        

        

        

query

   

precision performance on each category 
[wang et al. 2014a] 

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3

n
o
i
s
i
c
e
r
p

bocstc

lm_ch

movie
money
music
tv

0.71
0.97
0.97
0.96

0.91
0.95
0.90
0.46

id166

0.84
0.54
0.88
0.92

vsm_cosi

ne
0.81
0.57
0.73
0.56

lm_d

entity_esa

0.72
0.52
0.68
0.51

0.56
0.74
0.58
0.55

examples [wang et al. 2014a] 

table understanding [wang et al. 2012a] 

semantic search [wang et al. 2012b] 

references

   

   

   

   

   

   

   

[ stark et al. 1998 ] michael m. stark, and richard f. riesenfeld, id138: an electronic lexical 
database. proceedings of 11th eurographics workshop on rendering. 1998. 

[ banko et al. 2007 ]  michele banko, michael j cafarella, stephen soderland, matt broadhead and oren 
etzioni, id10 from the web  in ijcai 2007. 

[ etzioni et al. 2011 ] etzioni, oren, anthony fader, janara christensen, stephen soderland, and 
mausam mausam. "id10: the second generation." in ijcai, vol. 11, pp. 3-10. 
2011.

[carlson et al. 2010 ] a. carlson, j. betteridge, b. kisiel, b. settles, e.r. hruschka jr. and t.m. mitchell, 
toward an architecture for never-ending language learning. in proceedings of the conference on 
artificial intelligence (aaai), 2010.

[ wu et al. 2012 ]   wentao wu, hongsong li, haixun wang, and kenny zhu, probase: a probabilistic 
taxonomy for text understanding. in acm international conference on management of data 
(sigmod), may 2012. 

[ bollacker et al. 2008 ]  kurt bollacker, colin evans, praveen paritosh, tim sturge, jamine taylor, 
freebase: a collaboratively created graph database for structuring human knowledge</i>  in sigmod 
2008. 

[ auer et al. 2007 ] s  ren auer, christian bizer, georgi kobilarov, jens lehmann, richard cyganiak, 
zachary g. ives, dbpedia: a nucleus for a web of open data. in  iswc/aswc 2007.

references

   

   

   

   

   

   

   

[ suchanek et al. 2007 ]   fabian m. suchanek, gjergji kasneci, gerhard weikum, yago: a core of 
semantic knowledge in www 2007. 

[ wu et al. 2015 ] sen wu, ce zhang, christopher de sa, jaeho shin, feiran wang, and c. r  , 
incremental knowledge base construction using deepdive,  in vldb 2015

[ navigli et al. 2012 ] r. navigli and s. ponzetto. babelnet: the automatic construction, evaluation and 
application of a wide-coverage multilingual semantic network. in artificial intelligence, 2012. 

[ nastase et al. 2010 ]  vivi nastase, michael strube, benjamin b  rschinger, c  cilia zirn, and anas
elghafari, wikinet: a very large scale multi-lingual concept network  in lrec, 2010. 

[ speer et al. 2013 ] robert speer, and havasi catherine, conceptnet 5: a large semantic network for 
relational knowledge, the people   s web meets nlp.  springer berlin heidelberg, 2013. 

[ hua et al. 2016 ] wen hua, zhongyuan wang, haixun wang, kai zheng and xiaofang zhou. 
   understand short texts by harvesting and analyzing semantic knowledge   .  ieee transactions on 
knowledge and data engineering (tkde), 2016. 

[ hua et al. 2015 ] wen hua, zhongyuan wang, haixun wang, kai zheng, and xiaofang zhou, short text 
understanding through lexical-semantic analysis. in international conference on data engineering 
(icde), april 2015. 

references

   

   

   

   

   

   

   

[ li et al. 2013 ]  peipei li, haixun wang, kenny q. zhu, zhongyuan wang, and xindong wu. computing 
term similarity by large probabilistic isa knowledge.    in acm international conference on information 
and knowledge management (cikm), 2013.

[ li et al. 2015 ]  peipei li, haixun wang, kenny q. zhu, zhongyuan wang, xue-gang hu, and xindong
wu: a large probabilistic semantic network based approach to compute term similarity.   in ieee 
transactions on knowledge and data engineering (tkde), 27(10): 2604-2617, 2015. 

[ rosch et al. 1976 ] eleanor rosch, carolyn b mervis, wayne d gray, david m johnson, and penny 
boyesbraem. basic objects in natural categories. cognitive psychology, 8(3):382   439, 1976.

[ manning and schutze 1999 ] christopher d manning and hinrich schutze. foundations of statistical 
natural language processing. in volume 999. mit press, 1999. 

[ wang et al. 2015b ] zhongyuan wang, kejun zhao, haixun wang, xiaofeng meng, and ji-rong wen, 
query understanding through knowledge-based conceptualization  in ijcai, july 2015. 

[ bergsma et al. 2007 ]shane bergsma, qin iris wang. learning noun phrase query segmentation. in 
emnlp-conll 2007: 819-826. 

[ tan et al. 2008 ] bin tan, fuchun peng. unsupervised query segmentation using generative language 
models and wikipedia. in www 2008: 347-356.

references

   

   

   

   

   

   

   

[ li et al. 2011 ]  yanen li, bo-june paul hsu, chengxiang zhai, kuansan wang. unsupervised query 
segmentation using clickthrough for information retrieval. in sigir 2011: 285-294. 

[ guo et al. 2009 ]  jiafeng guo, gu xu, xueqi cheng, hang li. id39 in query. in 
sigir 2009: 267-274. 

[ pantel et al. 2012 ] patrick pantel, thomas lin, michael gamon. mining entity types from query logs 
via user intent modeling. in acl 2012: 563-571.

[ joshi et al. 2014 ]  mandar joshi, uma sawant, soumen chakrabarti. id13 and corpus 
driven segmentation and answer id136 for telegraphic entity-seeking queries. in emnlp 2014: 
1104-1114. 

[ sawant et al. 2013 ]  uma sawant, soumen chakrabarti. learning joint query interpretation and 
response ranking. in www 2013: 1099-1110. 

[ wang et al. 2014b ]  zhongyuan wang, haixun wang, and zhirui hu, head, modifier, and constraint 
detection in short texts in international conference on data engineering (icde), 2014.

[ sun et al. 2016 ]  xiangyan sun, haixun wang, yanghua xiao, zhongyuan wang, syntactic parsing of 
web queries  in emnlp 2016. 

references

   

   

   

   

   

   

[ kenter and rijke 2015 ] tom kenter and maarten de rijke. short text similarity with id27s
in cikm, 2015. 

[ wang et al. 2015a ]  zhongyuan wang, haixun wang, ji-rong wen, and yanghua xiao, an id136 
approach to basic level of categorization  in cikm, october 2015. 

[ hao et al. 2016 ]  zehui hao, zhongyuan wang, xiaofeng meng and jun yan, combining language 
model with conceptualization for definition ranking. msr-technical report, 2016. 

[ wang et al. 2014a ]  fang wang, zhongyuan wang, zhoujun li, and ji-rong wen, concept-based short 
text classification and ranking in cikm, 2014.

[ wang et al. 2012a ]  jingjing wang, haixun wang, zhongyuan wang, and kenny zhu, understanding 
tables on the web.  in international conference on conceptual modeling, october 2012. 

[ wang et al. 2012b ]  yue wang, hongsong li, haixun wang, and kenny zhu,toward topic search on the 
web.  in international conference on conceptual modeling, october 2012. 

