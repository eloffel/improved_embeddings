part iii. implicit representation 
for short text understanding

zhongyuan wang (microsoft research)

haixun wang (facebook inc.)

tutorial website:
http://www.wangzhongyuan.com/tutorial/acl2016/understanding-short-texts/

   implicit    model

    goal: 

    a distributed representation of a short text that captures its 

semantics.

    why? 

    to solve the sparsity problem

    representation readily used as features in downstream models

short text vs. phrase embedding

    there   s a lot of work on embedding phrases.

    a short text (e.g., a web query) is often not well 

formed

    e.g., no word order, no functional words

    a short text (e.g., a web query) is often more 

expressive

    e.g.,    distance earth moon   

applications

http://www.theverge.com/2015/10/26/9614836/google-search-ai-rankbrain

rankbrain

    a huge vocabulary

    contains every possible token

    query, doc title, doc url representation

    average id27

    architecture:

    3     4 hidden layers

    data

    months of search log data

the core problem (for the rest of us)

    what is the objective function used in training the 

representation?

    does the optimal solution force the representation 

to capture the full semantics?

traditional representation of text

    bag-of-words (bow) model: text (such as a sentence or a 

document) is represented as a bag (multiset) of words, 
disregarding grammar and word order but keeping 
multiplicity.

1.
2.

john likes to watch movie, mary likes movie too.
john also likes to watch football games.

the sentences are represented by two 10-entry vectors;
(1)
(2)

[1,2,1,1,2,0,0,0,1,1]
[1,1,1,1,0,1,1,1,0,0]

    disadvantages: no word order. matrix is sparse.

assumption: distributional hypothesis

    distributional hypothesis: words that are used and 

occur in the same contexts tend to purport similar 
meaning (wikipedia).

    e.g. paris is the capital of france.

    in this assumption,    paris    will be close in semantic 

space with    london    , which would also be 
surrounded by    capital of    and country   s name.

    based on this assumption, researchers proposed 

many models to learn the text representations from 
corpus.  

neural network language model (bengio et al. 2003)

statistical model

       (    1

    
    ) =    
    =1

       (        |    1

       1

   

assuming a word is determined by 
its previous words. 

two words with same previous 
words will share similar semantics.

yoshua bengio, r  jean ducharme ,pascal vincent, christian jauvin    a neural probabilistic language model    journal of machine 
learning research 3 (2003) 1137   1155

recurrent neural net language model (mikolov, 2012)

output values:

    (    ) =     (uw(    ) + w    (         1)

)

)
    (    ) =     (v    (    )

          :                                                                     

y      :                                                                                                                                                             

s      :                                             
u,v,w:                                                                                  

    generate much more meaningful text than id165 models 

    the sparse history h is projected into some continuous 
low-dimensional space, where similar histories get clustered 

id97tor model (mikolov et al. 2013)

    the id97 projects words in a shallow layer 

structure. 

maximize

   

(    ,    )       

   
               

log    (    |        

   

    directly learn the representation of words using 

context words

    optimizing the objective function in whole corpus.

efficient estimation of word representations in vector space.  mikolov et al 2013 

id97tor model (mikolov et al. 2013)

cbow

    given the word, predicting the context
    faster to train than the skip-gram,
better accuracy for the frequent words

skip-gram

    given the context, predicting the word
    works well with small training data,
represents well even rare words or phrases

glove: global vectors for word representation 
(pennington et al. 2014)

    constructing the word-word co-occurrence matrix of whole corpus.  

    inspired by lsa, using id105 to produce word 

representation.

id168:

        =    
    ,    

    (                     

   

                     log            

2

x-ij is the count of if j-th word occurs, the occurrence of i-th word. w are word vectors. 
minimize id168.

global vectors for word representation,  pennington et al, 2014

glove: global vectors for word representation 
(pennington et al. 2014)

    glove vs id97

word analogy task, e.g.

king     man + woman = queen

two variants of 
id97

beyond words

id27 is a great success.

phrase and sentence embedding is much harder:

    sparsity: from atomic symbols to compositional 

structures

    ground truth: from syntactic context to semantic 

similarity

composition methods

- algebraic composition

- composition tied with syntax (dependency tree of 

phrase / sentences)

averaging

    expand vocabulary to include ngrams

    otherwise go with bag of unigrams.

   a cat is being chased by a dog in yard   

    but a    jade elephant    is not an    elephant   

12?nsentencevvvvn            linear transformation

         =     (    ,     ), where

    ,      are embedding of uni-grams     ,     

     is a composition function

    common composition model: linear transformation

    training data: unigram and bigram embeddings

recursive auto-encoder with dynamic pooling

    recursive auto-encoder

    from bottom to top, leaves to root. 

    after parsing, important components in sentence will trend to get on 

higher level.

parent node

pre-trained 

word 

vector as 

input.

word 
vector

child node

child node

non-linear activation function

     =     (        [    1;     2] +     

)

    1:     2 is the concatenation of two word vectors

recursive auto-encoder with dynamic pooling

    dynamic pooling

    the sentences are not fixed-size. using 

pooling to map them into fix-sized 
vector.

    using fixed-size matrix as input of 
neural network or other classifiers.

example of the dynamic min-
pooling layer finding the 
smallest number in a pooling 
window region of the original 
similarity matrix s.

recursive auto-encoder with dynamic pooling [socher et 
al. 2011]

    using dependency parser to transform sequence to tree structure, which 

retains syntactical info

    using dynamic pooling to map varied-size sentence to a fixed-size form

most time, the para2vec model or traditional id56/lstm doesn   t consider 
the syntactical information of sentences.

from

parsing

to

sequential model

tree-like model

richard socher, eric h. huang, jeffrey pennington, andrew y. ng, christopher d. manning:    dynamic pooling and unfolding 
recursive autoencoders for paraphrase detection.    nips 2011: 801-809

id56 encoder-decoder

(cho et al. 2014)

    create a reversible sentence representation.

    the representation can be reconstructed to an actual sentence form which 

is reasonable and novel. 

kyunghyun cho, bart van merrienboer,   aglar g  l  ehre, dzmitry bahdanau, fethi bougares, holger schwenk, yoshua bengio:    learning phrase 
representations using id56 encoder-decoder for id151.    emnlp 2014: 1724-1734

id56 encoder-decoder

(cho et al. 2014)

    the conditional distribution of next symbol.

    (        |           1,            2,     ,     1,     ) =     (   <    >,            1,     

)

    add a summary(constant) symbol, it will hold

the semantics of sentence.

   <    > =     (   <       1>,            1,     

)

    for long sentences, adding hidden unit to 

remember/forget memory.

id56 encoder-decoder
(cho et al. 2014)

small section of the id167 of the phrase representation 

id56 for composition [socher et al 2011]

f = tanh is a standard element-wise nonlinearity

w is shared

mv-id56 [socher et al. 2012]

    each composition function depends on the actual 

words being combined.

    represent every word and phrase as both a vector and 

a matrix.

recursive neural tensor network [socher et al. 2013]

    number of parameters is very large for mv-id56 

mv-id56: need to train a new 
parameter for each leaf node

use tensor: unified parameter 
for all nodes

recursive neural tensor network [socher et al. 2013]

    interpret each slice of the tensor as capturing a 

specific type of composition

assign label to each node via:

recursive neural tensor network

    target : id31

sentence: there are slow and repetitive parts, 
but it has just enough spice to keep it interesting

capture 
construction
x but y

demo: http://nlp.stanford.edu:8080/sentiment/rntndemo.html

cvg (compositional vector grammars) 
[socher et al. 2013]

    task: represent phrase and categories

    pid18: capture discrete categorization of phrases
    id56: capture fine-grained syntactic and compositional-

semantic information

    parse and represent phrases as vector

an example of cvg tree

parsing with compositional vector grammars, socher et al 2013

cvg

    weights at each node are conditionally dependent 

on categories of the child constituents

    combined with syntactically untied id56

normal id56

su-id56

replicated weight matrix

depends on syntactic categories 
of its children

phrases & sentences

    composition based approaches

    algebraic composition not powerful enough
    syntactic composition requires parsing

    non-composition based approaches

    translation based approaches
    extend id97 to sentences, phrases
    ground truth: search log, dictionary, image

sequence to sequence translation

    last node    remembers    the semantics of the input 

sentence

    not feasible for embedding web queries

phrase translation model  [gao et al 2013]

the quality of a phrase translation is judged implicitly through the translation quality 
(id7) of the sentences that contain the phrase pair.

learning semantic representations for the phrase translation model, gao et al 2013

phrase translation model

the core is the bag-of-words approach

web query translation model

    training data

use smt

clicked 

sentence (en)

click log

sentence (fr)

alignment

query (en)

train smt

query (fr)

    train an nn translation model on query(en) and 

query(fr) pair

doc2vec (quoc le et al 2014)

    7distributed 

representations of 
sentences and

distributed representations of sentences and documents, quoc le et al. 2014

lda vs. doc2vec

similar topics to    machine learning    returned by lda and doc2vec

skip thought vectors (kiros et al 2015)

given a tuple             1,         ,         +1 of contiguous sentences, with          the i-th sentence of a 
book, the sentence          is encoded and tries to reconstruct the previous sentence 
           1and next sentence         +1. 

in this example, the input is the sentence triplet i got back home. i could see the cat on 
the steps. this was strange. unattached arrows are connected to the encoder output. 
colors indicate which components share parameters. <eos> is the end of sentence 
token.

skip thought vector
(ryan kiros et al. 2015)

    semantic relatedness:

gt is ground truth relatedness, pred is prediction by trained model.

skip thought vector for phrases

query of s in place of the 
original, clicked sentence s

translation vs. syntactic context

    different property of representation

    different perplexity

    different applications 

phrase embedding: using a multi-label 
classifier

bce (binary cross id178)

hidden layer

hidden layer

last hidden layer as
query embedding

explicit 
concepts

query 

embedding

language 
models

other lexical 

signals

phrase embedding: using a dictionary

lexical semantics

as bridge

giraffe

noun

target: word vector

phrasal semantics

goal: from word representation to 
phrase and  sentence 
representation

a tall, long-necked, spotted -
ruminant, giraffa camelopardalis,
of africa: the tallest living
quadruped animal.

the representation of definition should be closed with 
defining word vector. 

felix hill, kyunghyun cho, anna korhonen, yoshua bengio:
learning to understand phrases by embedding the dictionary. tacl 4: 17-30 (2016)

phrase embedding: using a dictionary

model:

pre-trained input representation

input

neural language model

optimize

objective function

definitions

   control consisting of a mechanical 
device for controlling fluid flow   

   when you like one thing more than 
another thing    

.
 
 
.
 
 
.

id97 as each word   s representation

recurrent neural networks

or

bag-of-words

words

   valve   

   prefer   

: input phrase embedding

: pre-trained embedding of defining word

: randomly selected word from vocabulary

max(0,cos((),)cos((),))cccrmmid166sv      cscvrv1()tttauvwvb               1tttaawv         phrase embedding: using a dictionary

    application: reverse dictionaries

    given a test description, definition, or question, all models produce a 

ranking of possible word answers based on the proximity of their 
representations of the input phrase and all possible output words. 

query

   an activity that requires 
strength 
and determination   

input

trained nlm models

map

vector representation

   exercise   

output

closest vector

look up

pre-trained word vector space

12[,,,]nxxxphrase embedding: using a dictionary

    application: crossword id53

    given the absence of a knowledge base or web-scale information in 

our architecture, they narrow the scope of the task by focusing on 
general knowledge crossword questions

test set

description

word

long (150 char)

   french poet and key figure in the development of symbolism   

baudelaire

short (120 char)

   devil devotee   

single-word (30 char)

   culpability   

satanist

guilt

+ several constrains to reduce the target space

learning to understand phrases by embedding the dictionary (felix hill et al. 2016)

phrase embedding: using images

a deep visual-semantic embedding model, nips 2013

zero-shot learning through cross-modal transfer, nips 2013

caption: a girl in a blue shirt is on a swing

keywords: girl, blue  shirt, swing

phrase embedding: using images

    (image, query)

    but image maps to multiple queries

    (*image*, girl)
    (*image*, blue shirt)
    (*image*, swing)

    the image places unnecessary constraint on the 3 queries.

query embedding: using clicked data

query side: shanghai hotel

(ctr data indicates the semantic relation
between query side and document side)

document side:    shanghai hotels 
accommodation hotel in shanghai discount 
and reservation   

basic lstm architecture for sentence embedding  

deep sentence embedding using long short-term 
memory networks, palangi et al 2016

latent semantic model with convolutional-pooling 
structure 
(yelong shen, et al. 2014)

search engine microsoft office excel

search engine welcome to the apartment office

query examples on internet

what   s the meaning of office ?

traditional method: bag-of-words

contextual information

office 1 = office 2

word sequence + convolutional-pooling structure

office 1 

office 2

low-dimentional, semantic vector representations 
for search queries and web document

shen, yelong, et al. "a latent semantic model with convolutional-pooling structure for information retrieval." proceedings of the 23rd acm 
international conference on conference on information and knowledge management. acm, 2014.

   latent semantic model with convolutional-pooling 
structure 
(yelong shen, et al. 2014)

    models:

the clsm maps a variable-length word 
sequence to a low-dimensional vector 
in a latent semantic space. 

latent semantic model with convolutional-pooling 
structure 

(yelong shen, et al. 2014)
    models:

# is word boundary symbol

concatenating

letter-trigram based word-id165 representation

word trigram vector

convolution operation

microsoft office excel could allow remote code execution
welcome to the apartment office
online body fat percentage calculator
online auto body repair estimates
vitamin a the health benefits given by carrots

bold words win max operation

max pooling

variable length sequence of feature vectors

latent semantic model with convolutional-pooling 
structure 

(yelong shen, et al. 2014)
    models:

    latent semantic vector representations 

v is the global feature vector after max pooling, ws is the semantic projection 
matrix, and y is the vector representation of the input query.

using cosine similarity to measure relatedness between queries and documents

tanh()sywv      (,)cosine(,)=||||||||tqdqdqdyyrqdyyyy   summary

    bag of words not powerful enough unless we have huge amount of 

high quality pairs.

    web queries are not phrases.  simple composition or phrase 

translation does not work for web queries.

    sentiment, classification as targets are not powerful enough to 

capture full semantics.

    translation is a better target, as it forces the representation to 

contain the full semantics.

conclusion

for short text understanding:

    short text   s understanding is still hard because the complexity meaning of 
combining the word in short text, the absence of certain context and syntactical
structure.

    there are not very suitable embedding approaches. but just like hamid   s work, 
we can incorporate some external data to help do the similarity measurement. 

    id27 can be a good feature but not the only feature, we can utilize 
more nlp tools such as pos or entity recognition to do disambiguation. 

reference

[ bengio et al. 2003 ]  yoshua bengio, r  jean ducharme ,pascal vincent, christian jauvin. a neural 
probabilistic language model.  in  journal of machine learning research 3 (2003) 1137   1155.

[ mikolov et al. 2013a ]  mikolov, tomas, chen, kai, corrado, greg, and dean, jeffrey. efficient 
estimation of word representations in vector space.  corr abs/1301.3781 (2013) 

[ mikolov et al. 2013b ]  mikolov, tomas, et al. distributed representations of words and phrases and 
their compositionality.in nips, 2013. 

[ pennington et al. 2014 ]   j pennington, r socher, cd manning. glove: global vectors for word 
representation.in emnlp 2014, 1532-1543. 

[ socher et al. 2011 ]   richard socher, eric h. huang, jeffrey pennington, andrew y. ng, christopher 
d. manning. dynamic pooling and unfolding recursive autoencoders for paraphrase detection. in 
nips 2011: 801-809

[ cho et al. 2014 ]  kyunghyun cho, bart van merrienboer,   aglar g  l  ehre, dzmitry bahdanau, fethi
bougares, holger schwenk, yoshua bengio. learning phrase representations using id56 encoder-
decoder for id151.in emnlp 2014: 1724-1734

[ gao et al. 2013]  jianfeng gao, xiaodong he, wen-tau yih, li deng:learning semantic 
representations for the phrase translation model.  corr abs/1312.0482 (2013)

reference

[ quoc et al. 2014 ]  quoc v. le, tomas mikolov: distributed representations of sentences and 
documents.in icml 2014: 1188-1196. 

[ ryan kiros et al. 2015 ]  kiros, ryan, et al. skip-thought vectors. in nips 2015.

[ felix hill et al. 2016 ]  felix hill, kyunghyun cho, anna korhonen, yoshua bengio. learning to 
understand phrases by embedding the dictionary.in tacl 4: 17-30 (2016) 

[ hamid et al. 2016 ]  hamid palangi, li deng, yelong shen, jianfeng gao, xiaodong he, jianshu chen, 
xinying song, rabab k. ward. deep sentence embedding using id137: 
analysis and application to information retrieval. ieee/acm trans. audio, speech & language 
processing 24(4): 694-707 (2016)

[ shen, et al. 2014]   shen et al. a latent semantic model with convolutional-pooling structure for 
information retrieval. in cikm 2014.

[ socher et al. 2012 ] r. socher, b. huval, c. manning and a. ng. semantic compositionality through 
recursive matrix-vector spaces.  in emnlp 2012.

[ socher et al. 2013a ] r. socher, j. bauer, c. manning and a. ng. parsing with compositional vector 
grammars. in acl 2013.

[ socher et al. 2013b ] r. socher, a. perelygin, j. wu, j. chuang, c. manning, a. ng and c. potts. 
recursive deep models for semantic compositionality over a sentiment treebank. in emnlp 2013.

