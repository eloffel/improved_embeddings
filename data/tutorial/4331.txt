memory networks 
for language understanding
antoine bordes - facebook ai research
lxmls     lisbon july 28, 2016
bots?

end-to-end dialog agents
we believe a true dialog agent should:
be able to combine all its knowledge and reason to fulfill complex tasks
handle long open-ended conversations involving effectively tracking and predicting dialog and world states
be able to learn (new tasks) and acquire knowledge via conversation and reading (and observing the world in multimodal scenarios).
our directions: 
machine learning end-to-end systems.
creation (and release) of datasets for training/evaluating those.
memory networks (weston et al., iclr15; sukhbaatar et al., nips15)
class of models that combine large memory with learning component that can read and write to it.
incorporates reasoning with attention over memory (ram).
most ml has limited memory which is more-or-less all that   s needed for    low level    tasks e.g. id164.
our motivation: long-term memory is required to read a story and then e.g. answer questions about it.

similarly, it   s also required for dialog: to remember previous dialog (short- and long-term), and respond.
babi tasks (weston et al., iclr16)
set of 20 tasks testing basic reasoning capabilities for qa from stories
short stories are generated from a simulation
easy to interpret results / test a broad range of properties





useful to foster innovation: cited ~100 times
john dropped the milk.
john took the milk there.
sandra went to the bathroom.
john moved to the hallway.
mary went to the bedroom.
where is the milk ?	hallway
the suitcase is bigger than the chest.
the box is bigger than the chocolate.
the chest is bigger than the chocolate.
the chest fits inside the container.
the chest fits inside the box.
does the suitcase fit in the chocolate? 	no
task 3: two supporting facts
task 18: size reasoning


memory module
controller module
input
memory networks (weston et al., iclr15)
addressing = max => hard attention
requires explicit supervision of attention during training
only feasible for simple tasks



memory module
controller module
input
end-to-end memory networks (sukhbaatar et al., nips15)

memory module
dot product
memory networks on babi







memories
(m1, m2, m3, m4,    )
m4: john moved to the hallway.
m3: sandra went to the bathroom.
m2: john took the milk there.
m1: john dropped the milk.
john
john
milk
hallway
office
   
hallway
m5: mary went back to the bedroom.
john dropped the milk.
john took the milk there.
sandra went to the bathroom.
john moved to the hallway.
mary went back to the bedroom.
where is the milk?


weakly supervised
training on 1k stories
supervised supp. facts
dashboard
attention during memory lookups
samples from toy qa tasks
20 babi tasks
related memory models   
(published before or ~same time as original paper)
id56search (bahdanau et al.) for machine translation
can be seen as a memory network where memory goes back only one sentence (writes embedding for each word). 
at prediction time, reads memory and performs a soft max to find best alignment (most useful words). 1 hop only.
generating sequences with id56s (graves,    13)
also does alignment with previous sentence to generate handwriting (so id56 knows what letter it   s currently on).
id63s (graves et al., 14) [on arxiv just 5 days after memnns!]
has read and write operations over memory to perform tasks (e.g. copy, sort, associative recall).
128 memory slots in experiments; content addressing computes a score for each slot     slow for large memory?
earlier work by (das    92), (schmidhuber et al., 93), discern (miikkulainen,    90) and others...



learning of basic algorithms using reasoning, attention, memory (ram)      
(e.g. addition, multiplication, sorting)
methods include adding stacks and addressable memory to id56s:
   neural net architectures for temporal sequence processing    m. mozer. 
   id63s    a. graves, g. wayne, i. danihelka.
   inferring algorithmic patterns with stack augmented recurrent nets    a. joulin, t. mikolov
   learning to transduce with unbounded memory    e. grefenstette et al.
   neural programmer-interpreters    s. reed, n. de freitas.
   id23 turing machine    w. zaremba and i. sutskever.
   learning simple algorithms from examples    w. zaremba, t. mikolov, a. joulin, r. fergus 
   the neural gpu and the neural ram machine    i. sutskever.



how about on real data? in other conditions?
toy ai tasks are important for developing innovative methods.
but they do not give all the answers.

how do these models work in real/different conditions?
story understanding (children   s book, news articles)
open id53 (knowledge bases, wikipedia)
dialog (synthetic dialog, ubuntu)
story understanding

children   s books test (cbt) (hill et al., iclr16)
story understanding dataset based on 118 children books from project gutenberg

memory networks on cbt
memories format?
sentence: whole sentences 
   (as in the babi tasks)
word: 1 word at a time 
   (id38 style)
words window: store windows made through the story 
   (convolution style)
different word types / different models



id53 on new   s articles
we evaluate our models on the data from:
   teaching machines to read and comprehend   
karl moritz hermann, tom     ko  isk  , edward grefenstette, lasse espeholt, will kay, mustafa suleyman, phil blunsom

latest fresh results

our best results:  
   qaid98: 69.4    cbt-ne: 66.6   cbt-cn: 63.0
text understanding with the attention sum reader network. kadlec et al.	(4 mar    16)     qaid98: 75.4    cbt-ne: 71.0   cbt-cn: 68.9                 
iterative alternating neural attention for machine reading. sordoni et al. 	(7 jun    16)     qaid98: 76.1    cbt-ne: 72.0    cbt-cn: 71.0
natural language comprehension with the epireader. trischler et al. 	(7 jun    16)          qaid98: 74.0    cbt-ne: 71.8    cbt-cn: 70.6
gated-attention readers for text comprehension. dhingra et al.		(5 jun    16)                     qaid98: 77.4    cbt-ne: 71.9    cbt-cn: 69.0             
open id53

open-domain id53
what year was the movie blade runner released? 
can you describe blade runner in a few words? 
in blade runner, who built the replicants?
1982
a dystopian and noir movie
???
kbs can suffer from missing information and fixed schemas
answer questions on any topic
information extraction
wikipedia entry: blade runner
blade runner is a 1982 american neo-noir dystopian science fiction film directed by ridley scott and starring harrison ford, rutger hauer, sean young, and edward james olmos. the screenplay, written by hampton fancher and david peoples, is a modified film adaptation of the 1968 novel    do androids dream of electric sheep?    by philip k. dick. the film depicts a dystopian los angeles in november 2019 in which genetically engineered replicants, which are visually indistinguishable from adult humans, are manufactured by the powerful tyrell corporation as well as by other    mega-corporations    around the world   

what year was the movie blade runner released? 
can you describe blade runner in a few words? 
in blade runner, who built the replicants?
1982
a dystopian and noir movie
tyrell corporation

[replicants, manufactured_by, tyrell coporation ] 
???
ie is not an easy problem!
id53 directly from text
wikipedia entry: blade runner
blade runner is a 1982 american neo-noir dystopian science fiction film directed by ridley scott and starring harrison ford, rutger hauer, sean young, and edward james olmos. the screenplay, written by hampton fancher and david peoples, is a modified film adaptation of the 1968 novel    do androids dream of electric sheep?    by philip k. dick. the film depicts a dystopian los angeles in november 2019 in which genetically engineered replicants, which are visually indistinguishable from adult humans, are manufactured by the powerful tyrell corporation as well as by other    mega-corporations    around the world   

what year was the movie blade runner released? 
can you describe blade runner in a few words? 
in blade runner, who built the replicants?
1982
a dystopian and noir movie
tyrell corporation
much more information than in kb
but qa is harder
movieqa (miller et al., arxiv16)
hypothesis: systems answering from text directly must be on par with systems using kbs for questions whose answers are in kbs.

movieqa: a new analysis tool for qa
a set of 100k question -- answer pairs (based on simplequestions)
3 knowledge sources:
a kb based on omdb
raw text extracted from wikipedia
an imperfect kb made by an ie system ran on the wikipedia articles
answers to all questions are in the kb and in the wikipedia text.

memory networks for qa from kb (bordes et al., arxiv15)




what year was the movie blade runner released? 
[blade runner, written_by, philip k. dick]
[blade runner, release_year, 1982]
[blade runner, directed_by, ridley scott]
tron
1982
police
tom cruise
   
1982
[   ] [   ] [   ]
memory networks for qa from text (hill et al., iclr16)




what year was the movie blade runner released? 
tron
1982
police
tom cruise
   
1982
wikipedia entry: blade runner
blade runner is a 1982 american neo-noir dystopian science fiction film directed by ridley scott and starring harrison ford, rutger hauer, sean young, and edward james olmos. the screenplay, written by hampton fancher and david peoples, is a modified film adaptation of the 1968 novel    do androids dream of electric sheep?    by philip k. dick. the film depicts a dystopian los angeles in november 2019 in which genetically engineered replicants, which are visually indistinguishable from adult humans, are manufactured by the powerful tyrell corporation as well as by other    mega-corporations    around the world   

written by h. fancher and david peoples
directed by ridley scott and starring
is a 1982 american neo-noir
[   ] [   ] [   ]



memory networks on movieqa
no knowledge
(embeddings)
standard qa 
system on kb
54.4%
93.5%

24%
memory networks
response accuracy (%)
structuring memories
structure in the symbolic memories
parts of the memories match questions where others encode response




prior knowledge on the task
which wikipedia page do the windows come from?
which knowledge source do memories have been extracted from?
[blade runner, release_year, 1982]
[blade runner, directed_by, ridley scott]
directed by ridley scott and starring
is a 1982 american neo-noir
key-value memory networks on kb




what year was the movie blade runner released? 
[blade runner, written_by] / philip k. dick
[blade runner, release_year] / 1982
[blade runner, directed_by] / ridley scott
tron
1982
police
tom cruise
   
1982
[   ] [   ] [   ]
key-value memory networks on text




what year was the movie blade runner released? 
tron
1982
police
tom cruise
1982
wikipedia entry: blade runner
blade runner is a 1982 american neo-noir dystopian science fiction film directed by ridley scott and starring harrison ford, rutger hauer, sean young, and edward james olmos. the screenplay, written by hampton fancher and david peoples, is a modified film adaptation of the 1968 novel    do androids dream of electric sheep?    by philip k. dick. the film depicts a dystopian los angeles in november 2019 in which genetically engineered replicants, which are visually indistinguishable from adult humans, are manufactured by the powerful tyrell corporation as well as by other    mega-corporations    around the world   

written by h. fancher and d. p. / h. fancher
directed by r. scott and starring / r. scott
is a 1982 american neo-noir / 1982
[   ] [   ] [   ]



is a 1982 american neo-noir / blade runner
directed by r. scott and starring / blade runner
written by h. fancher and d. p. / blade runner
   
results on movieqa
no knowledge
(embeddings)
standard qa 
system on kb
54.4%
93.5%

17%
memory networks
key-value memory networks
response accuracy (%)
synthetic documents
kb: [flags of our fathers, directed_by, clint eastwood]
one template: clint eastwood directed flags of our fathers
all templates: flags of our fathers was directed by clint eastwood.
one template + coref.: flags of our fathers came out in 2006. clint eastwood directed it.
one template + conjunctions: flags of our fathers is in english and clint eastwood directed flags of our fathers.
all templates + coref. + conj.: flags of our fathers is a famous film. ryan phillippe, jesse bradford, adam beach, and john benjamin hickey are the actors in it and clint eastwood is the person who directed it.
wikipedia: the film adaptation flags of our fathers, which opened in the u.s. on october 20, 2006, was directed by clint eastwood and produced by steven spielberg, with a screenplay written by william broyles, jr. and paul haggis.

difficulty
synthetic documents analysis
key-value memory networks
response accuracy (%)



wikiqa (yang et al., emnlp15)
qa benchmark in the answer selection setting
key-value memories ->  (window, sentence)
q: how are glacier caves formed ?       
a: a glacier cave is a cave formed within the ice of a glacier 
training size is very small (~1k examples):
id27s pre-trained on wikipedia and fixed 
dropout id173

dialog

how about on dialog data? 
everything we showed so far was q&a potentially with long-term context.
we have also built a movie dialog dataset (dodge et al., iclr16)
closed, but large, domain about movies (75k entities, 3.5m ex).
ask facts about movies?
ask for opinions (recommendations) about movies?
dialog combining facts and opinions?
general chit-chat about movies (statements not questions)?

and    combination of all above in one end-to-end model.


combines qa with dialog tasks (dodge et al., iclr16)
ubuntu data (lowe et al. mcgill,    15)
dialog dataset: ubuntu irc channel logs, users ask questions about issues they are having with ubuntu and get answers by other users.
best current results:    sentence pair scoring: towards unified framework for text comprehension    (baudis et al., 2016) => id56-id98 combo: 67.2

goal-oriented dialog (bordes et al., arxiv16)
babi tasks for goal-oriented dialog
6 tasks around restaurant booking
involve manipulate language and kb symbols
next steps

variants of the class   
some options and extensions:
representation of inputs and memories could use all kinds of encodings:  bag of words,  id56 style reading at word or character level, etc. 
different possibilities for output module: e.g. multi-class classifier or uses an id56 to output sentences. 
if the memory is huge (e.g. wikipedia) we need to organize the memories. solution: hash the memories to store in buckets (topics). then, memory addressing and reading doesn   t operate on all memories.
if the memory is full, there could be a way of removing one it thinks is most useless; i.e. it ``forgets       somehow. that would require a scoring function of the utility of each memory..
conclusion
(key-value) memory networks: promising model for jointly using symbolic and continuous systems 
can be trained end-to-end through id26 + sgd
provide a great flexibility on how to design memories

babi, cbt, movieqa, etc.: new tools for developing learning algorithms
training and evaluation sets of reasonable sizes
designed to ease interpretation
open research
papers:
key-value memory networks: http://arxiv.org/abs/1606.03126 
memory networks: http://arxiv.org/abs/1410.3916
end-to-end memory networks: http://arxiv.org/abs/1503.08895
babi tasks: http://arxiv.org/abs/1502.05698
children   s books test: http://arxiv.org/abs/1511.023701
large-scale qa with memory networks: http://arxiv.org/abs/1506.02075
evaluating pre-requisite qualities of id71: http://arxiv.org/abs/1511.06931 
dialog babi tasks: http://arxiv.org/abs/1605.07683 
dialog-based language learning: http://arxiv.org/abs/1604.06045 
data: fb.ai/babi (7 datasets including babi tasks, cbt and movieqa)
code:
memory networks: https://github.com/facebook/memnn
babi tasks generator: https://github.com/facebook/babi-tasks



memnn
q&a
dialog
repeval @ acl 2016
https://sites.google.com/site/repevalacl16/
