hankel-based methods for latent-state 

id170  

	
   

ariadna	
   qua*oni	
   

xerox	
   research	
   centre	
   europe	
   

collaborators:	
   r.	
   bailly,	
   b.	
   balle,	
   x.carreras,	
   a.	
   globerson,	
   f.	
   luque,	
   a.	
   mayo	
   

	
   

lxmls	
   2014	
   

 id170 

latent state models 

latent state models 

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

   classic spectral 

learning algorithms    

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

   classic spectral 

learning algorithms    

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

   classic spectral 

learning algorithms    

   different 

interpretation of 
spectral trick    

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

weighted automata (wfa) 

examples of functions computed by wfas 

id48s 

examples of functions computed by wfas 

fsts 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

forward-backward mappings 

example, for id48s: 

forward-backward mappings 

example, for id48s: 

forward-backward mappings 

example, for id48s: 

forward-backward mappings 

example, for id48s: 

forward-backward mappings 

example, for id48s: 

consequences: 

forward-backward mappings 

example, for id48s: 

consequences: 

forward-backward mappings 

example, for id48s: 

consequences: 

forward-backward mappings 

example, for id48s: 

consequences: 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

the hankel matrix 

hankel matrix of a wfa 

hankel matrix of a wfa 

hankel matrix of a wfa 

hankel matrix of a wfa 

hankel matrix of a wfa 

hankel matrix of a wfa 

hankel matrix of a wfa 

hankel matrix of a wfa 

low-rank hankel and operators 

low-rank hankel and operators 

low-rank hankel and operators 

spectral method 

spectral method 

spectral method 

spectral method 

experiment: pos-tag sequence models 

experiment: pos-tag sequence models 

classical spectral: summary 

classical spectral: summary 

learning setting: 
       considers stochastic functions  
       assumes there is a model that generates sequences 
       we can sample from this model 

classical spectral: summary 

learning setting: 
       considers stochastic functions  
       assumes there is a model that generates sequences 
       we can sample from this model 

unknown 

pwfa 

classical spectral: summary 

learning setting: 
       considers stochastic functions  
       assumes there is a model that generates sequences 
       we can sample from this model 

unknown 

pwfa 

samples 

classical spectral: summary 

learning setting: 
       considers stochastic functions  
       assumes there is a model that generates sequences 
       we can sample from this model 

unknown 

pwfa 

samples 

estimate 
hankel 

 typically 

involves estimating 

expectations 

classical spectral: summary 

learning setting: 
       considers stochastic functions  
       assumes there is a model that generates sequences 
       we can sample from this model 

unknown 

pwfa 

samples 

estimate 
hankel 

algebraic  
operations 

 typically 

involves estimating 

expectations 

model 

parameters 

classical spectral: summary 

learning setting: 
       considers stochastic functions  
       assumes there is a model that generates sequences 
       we can sample from this model 

unknown 

pwfa 

samples 

estimate 
hankel 

algebraic  
operations 

nice theoretical 

guarantees 

  

 typically 

involves estimating 

expectations 

model 

parameters 

classical spectral: limitations 

classical spectral: limitations 

       necessary statistics are hard to estimate  
      

in some cases we can not directly estimate the hankel from samples 
 

 eg. learning a function that is not a distribution 

                     learning a pid18 when we don   t have derivations 
                     learning transducers when we don   t have alignments   

classical spectral: limitations 

       necessary statistics are hard to estimate  
      

in some cases we can not directly estimate the hankel from samples: 
 

 eg. learning a function that is not a distribution 

                     learning a pid18 when we don   t have derivations 
                     learning transducers when we don   t have alignments   

it would be better not to have to assume that 

samples are generated by some model !! 

classical spectral: limitations 

       necessary statistics are hard to estimate  
      

in some cases we can not directly estimate the hankel from samples: 
 

 eg. learning a function that is not a distribution 

                     learning a pid18 when we don   t have derivations 
                     learning transducers when we don   t have alignments   

it would be better not to have to assume that 

samples are generated by some model !! 

training samples 

g(x) 

g 

   most similar    

f 

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

outline 

       spectral learning of wfa 
 
       hankel-based optimization  

  for id170 

sequence tagging 

new id173 for factorized linear models 

new id173 for factorized linear models 
feature vector 

feature function 

otherwise 

parameter vector 

new id173 for factorized linear models 
feature vector 

feature function 

otherwise 

parameter vector 

scoring function 

new id173 for factorized linear models 
feature vector 

feature function 

otherwise 

parameter vector 

scoring function 

think of parameter 
as a vector 

id173 

new id173 for factorized linear models 
feature vector 

feature function 

otherwise 

parameter vector 

scoring function 

think of parameter 
as a vector 

id173 

directly from multiclass prediction 

new id173 for factorized linear models 
feature vector 

feature function 

otherwise 

parameter vector 

scoring function 

think of parameter 
as a vector 

id173 

directly from multiclass prediction 

think of parameter 
as a matrix 

spectral 
id173 

new id173 for factorized linear models 
feature vector 

feature function 

otherwise 

parameter vector 

scoring function 

think of parameter 
as a vector 

id173 

directly from multiclass prediction 

think of parameter 
as a matrix 

spectral 
id173 

specifically designed for id170  

spectral trick: 

 change of representation 

spectral trick: 

 change of representation 

learning 

parameters of 
latent-state 

models 

spectral trick: 

 change of representation 

problem 
reduction 

learning 

parameters of 
latent-state 

models 

solving 

systems of 
polynomial 
equations 

spectral trick: 

 change of representation 

problem 
reduction 

problem 
reduction 

learning 

parameters of 
latent-state 

models 

solving 

systems of 
polynomial 
equations 

finding a low-
rank matrix 
under linear 
constraints 

spectral trick: 

 change of representation 

problem 
reduction 

problem 
reduction 

problem 
relaxation 

learning 

parameters of 
latent-state 

models 

solving 

systems of 
polynomial 
equations 

finding a low-
rank matrix 
under linear 
constraints 

finding a low 
nuclear-norm 
matrix under 

linear 

constraints  

spectral trick: 

change of representation 

problem 
reduction 

problem 
reduction 

problem 
relaxation 

learning 

parameters of 
latent-state 

models 

solving 

systems of 
polynomial 
equations 

finding a low-
rank matrix 
under linear 
constraints 

finding a low 
nuclear-norm 
matrix under 

what are we 
loosing??  

linear 

constraints  

thanks! 

