natural language understanding

in a continuous space

karl-moritz hermann, nal kalchbrenner, edward grefenstette,

and phil blunsom

phil.blunsom@cs.ox.ac.uk

features and nlp

twenty years ago id148 freed us from the shackles of
simple multinomial parametrisations, but imposed the tyranny of
feature engineering.

2/38

features and nlp

distributed/neural models allow us to learn shallow features for our
classi   ers, capturing simple correlations between inputs.

3/38

features and nlp

 fully connected

layer

k-max pooling

(k=3)

folding

convolution

wide
(m=2)

dynamic
k-max pooling
 (k= f(s) =5)

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

game's the same, just got more    erce

deep learning allows us to learn hierarchical generalisations.
something that is proving rather useful for vision, speech, and now
nlp...

4/38

outline

1 distributed representations in id152

2 from vector space id152 to mt

distributed representations in id152

5/38

how to represent meaning in nlp

we can represent words using a number of approaches

    characters
    pos tags
    grammatical roles
    id39
    collocation and distributional representations
    task-speci   c features

all of these representations can be encoded in vectors. some of
these representations capture meaning.

distributed representations in id152

6/38

a harder problem: paraphrase detection

q: do two sentences (roughly) mean the same?

   he enjoys jazz music           he likes listening to jazz    ?

a: use a distributional representation to    nd out?

distributed representations in id152

7/38

a harder problem: paraphrase detection

q: do two sentences (roughly) mean the same?

   he enjoys jazz music           he likes listening to jazz    ?

a: use a distributional representation to    nd out?

most representations not sensible on the sentence level

    characters ?
    pos tags ?
    grammatical roles ?
    id39 ?
    collocation and distributional representations ?
    task-speci   c features ?

distributed representations in id152

7/38

why can   t we extract hierarchical features?

the curse of dimensionality
as the dimensionality of a representation increases, learning
becomes less and less viable due to sparsity.

dimensionality for collocation

    one word per entry: size of dictionary (small)
    one sentence per entry: number of possible sentences

(in   nite)

) we need a di   erent method for representing sentences

distributed representations in id152

8/38

what is deep learning

deep learning for language
learning a hierarchy of features, where higher levels of abstraction
are derived from lower levels.

distributed representations in id152

9/38

a door, a roof, a window: it   s a house

,

0.1
0.5
0.1

0.2
0.3
0.4

0.4
0.7
0.3

0.5
0.3
0.8

distributed representations in id152

10/38

composition

lots of possible ways to compose vectors

    addition
    multiplication
    kronecker product
    tensor magic
    matrix-vector multiplication
    ...

requirements

not commutative
encode its parts?
more than parts?

mary likes john 6= john likes mary
magic carpet     magic + carpet
memory lane 6= memory + lane

distributed representations in id152

11/38

autoencoders

we want to ensure that the joint representation captures the
meaning of its parts. we can achieve this by autoencoding our
data at each step:

for this to work, our autoencoder minimizes an objective function
over inputs xi , i 2 n and their reconstructions x0i :

j =

distributed representations in id152

1
2

nxi   x0i   xi  2

12/38

recursive autoencoders (rae)

we still want to learn how to represent a full sentence (or house).
to do this, we chain autoencoders to create a recursive structure.

we use a composition function
g (w     input + bias)
g is a non-linearity (tanh, sigm)
w is a weight matrix
b is a bias

distributed representations in id152

13/38

a di   erent task: paraphrase detection

q: do two sentences (roughly) mean the same?

   he enjoys jazz music           he likes listening to jazz    ?

a: use deep learning to    nd out!

distributed representations in id152

14/38

other applications: stick a label on top

1. combine label and
reconstruction error

e (n, l,    ) =

xn2n

erec (n,    ) + elbl (vn, l,    )

erec (n,    ) =

elbl (v , l,    ) =

2

1

2   [xnkyn]   rn   

1
2 kl   vk2

2. strong results for a number
of tasks:
id31
paraphrase detection
image search
...

distributed representations in id152

15/38

convolution sentence models

deep learning is suppose to learn the features for us, so can we do
away with all this structural engineering and forget about latent
parse trees?

distributed representations in id152

16/38

convolution sentence models

open

the

pod

bay

doors hal

distributed representations in id152

17/38

convolution sentence models

m = 2

open

the

pod

bay

doors hal

distributed representations in id152

17/38

convolution sentence models

m = 3

m = 2

open

the

pod

bay

doors hal

distributed representations in id152

17/38

convolution sentence models

m = 3

m = 2

open

the

pod

bay

doors hal

distributed representations in id152

17/38

a csm for dialogue act tagging

a: my favourite show is masterpiece theatre.

statement-non-opinion

a: do you like it by any chance?

b: oh yes!

a: you do!

b: yes, very much.

a: well, wouldn't you know.

b: as a matter of fact, i prefer public television.

b: and, uh, i have, particularly enjoy english 
comedies.

yes-no-question

yes-answers

declarative yes-no-q

yes-answers

exclamation

statement-non-opinion

statement-non-opinion

distributed representations in id152

18/38

a csm for dialogue act tagging

dave: hello hal, do 
you read me hal?

hal: af   rmative, dave, 
i read you.

dave: open the pod bay 
doors, hal.

hal: i'm sorry, dave, 
i'm afraid i can't do that.

distributed representations in id152

19/38

a csm for dialogue act tagging

dave: hello hal, do 
you read me hal?

hal: af   rmative, dave, 
i read you.

dave: open the pod bay 
doors, hal.

hal: i'm sorry, dave, 
i'm afraid i can't do that.

distributed representations in id152

19/38

a csm for dialogue act tagging

dave: hello hal, do 
you read me hal?

hal: af   rmative, dave, 
i read you.

dave: open the pod bay 
doors, hal.

hal: i'm sorry, dave, 
i'm afraid i can't do that.

distributed representations in id152

19/38

a csm for dialogue act tagging

dave: hello hal, do 
you read me hal?

hal: af   rmative, dave, 
i read you.

dave: open the pod bay 
doors, hal.

hal: i'm sorry, dave, 
i'm afraid i can't do that.

s4

hdave

i

x3

ohal

p4

h4

hi = g(ixi 1 + hi 1hi 1 + ssi)
pi = softmax(oihi)

distributed representations in id152

19/38

a csm for dialogue act tagging

dave: hello hal, do 
you read me hal?

hal: af   rmative, dave, 
i read you.

dave: open the pod bay 
doors, hal.

hal: i'm sorry, dave, 
i'm afraid i can't do that.

s4

hdave

i

x3

ohal

p4

h4

hi = g(ixi 1 + hi 1hi 1 + ssi)
pi = softmax(oihi)

state of the art results while allowing online processing of dialogue.

distributed representations in id152

19/38

convolution sentence models: id53

?x : have-population-of(vancouver, x)

what

is

the population of vancouver ?

competitive with a template based approach with lots of hand
engineered features.

distributed representations in id152

20/38

convolution sentence models

 projected
sentence 
matrix
(s=7)

 the cat sat on the red mat

distributed representations in id152

21/38

convolution sentence models

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

 the cat sat on the red mat

distributed representations in id152

21/38

convolution sentence models

dynamic
k-max pooling
 (k= f(s) =5)

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

distributed representations in id152

21/38

 the cat sat on the red mat

convolution sentence models

convolution

wide
(m=2)

dynamic
k-max pooling
 (k= f(s) =5)

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

distributed representations in id152

21/38

 the cat sat on the red mat

convolution sentence models

folding

convolution

wide
(m=2)

dynamic
k-max pooling
 (k= f(s) =5)

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

distributed representations in id152

21/38

 the cat sat on the red mat

convolution sentence models

k-max pooling

(k=3)

folding

convolution

wide
(m=2)

dynamic
k-max pooling
 (k= f(s) =5)

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

distributed representations in id152

21/38

 the cat sat on the red mat

convolution sentence models

 fully connected

layer

k-max pooling

(k=3)

folding

convolution

wide
(m=2)

dynamic
k-max pooling
 (k= f(s) =5)

convolution

wide
(m=3)

 projected
sentence 
matrix
(s=7)

distributed representations in id152

21/38

 the cat sat on the red mat

small sentiment task

sentiment prediction on the stanford movie reviews dataset.

distributed representations in id152

22/38

large sentiment task

accuracy on the larger twitter sentiment dataset.

distributed representations in id152

23/38

question classi   cation task

hier

classi   er

maxent

maxent

features
unigram, pos, head chunks
ne, semantic relations
unigram, bigram, trigram
pos, chunks, ne, supertags
id35 parser, id138
unigram, bigram, trigram
pos, wh-word, head word
word shape, parser
hypernyms, id138
unigram, pos, wh-word
head word, parser
hypernyms, id138
60 hand-coded rules
max-tdnn unsupervised vectors
unsupervised vectors
nbow
did98
unsupervised vectors

id166

acc. (%)

91.0

92.6

93.6

95.0

84.4
88.2
93.0

accuracy (%)

classi   er
id166
binb
maxent
max-tdnn
nbow
did98

table 3: accuracy on the twitter sentiment
dataset. the three non-neural classi   ers are based
on unigram and bigram features; the results are re-
ported from (go et al., 2009).

function. at training time we apply dropout to the
penultimate layer after the last tanh non-linearity
(hinton et al., 2012).

six-way question classi   cation on the trec questions dataset, e.g.

input: how far is it from denver to aspen ?

table 2: accuracy of six-way question classi   ca-
tion on the trec questions dataset. the second
column details the external features used in the
various approaches. the    rst four results are re-
spectively from li and roth (2002), blunsom et al.
(2006), huang et al. (2008) and silva et al. (2011).

number

output:

we see that the did98 signi   cantly outper-
forms the other neural and non-neural models.
the nbow performs similarly to the non-neural
id165 based classi   ers. the max-tdnn per-
forms worse than the nbow likely due to the ex-
cessive pooling of the max pooling operation; the
latter discards most of the sentiment features of the

24/38

distributed representations in id152

feature: not only . . . but also

    not      only          manufactured    ,               but    also          so        
    while    not           all             transitions     to     are           so        
    s       not           there           yet                                  but       
    not      all           transitions     to              are    so            ,         
    may      not           be              new             ,      but           australian
    feels    not           only            manufactured    ,      but           also      
    s       not           merely          unwatchable     ,      but           also      
    land     than          crash           ,               but    ultimately    serving   
    least    surprising    ,               it              is     still         ultimately
    great    bond          movie           ,               but    it            is        

distributed representations in id152

25/38

feature: as . . . as . . . as

    as           predictable    and          as     lowbrow    as          the    
    as           lively         and          as     fun        as          it     
    ,            confusing      spectacle    ,      one        that        may    
    that         hinges         on           its    casting    ,           and    
    cinematic    high           crime        ,      one        that        brings 
    as           an             athlete      as     well       as          an     
    its          audience       and          its    source     material    .      
    ,            and            lane         as     vincent    ,           the    
    as           lo             fi           as     the        special     effects
    age          story          restraint    as     well       as          warmth 

distributed representations in id152

26/38

feature: positivity

    startling      film       that        gives           you          a           fascinating 
    well           written    ,           nicely          acted        and         beautifully 
    best           from       his         large           cast         in          beautifully 
    strong         ,          credible    performances    from         the         whole       
    compelling     journey    ...         and             ``           his         best        
    be             a          joyful      or              at           least       fascinating 
    throughout     is         daring      ,               inventive    and         impressive  
    enjoyable      film       for         the             family       ,           amusing     
    originality    it         makes       up              for          in          intelligence
    charming       ,          quirky      and             paced        scottish    comedy      

distributed representations in id152

27/38

outline

1 distributed representations in id152

2 from vector space id152 to mt

from vector space id152 to mt

28/38

generalisation in mt

(cid:2)

(cid:1)

   

   

   

                 

from vector space id152 to mt

29/38

generalisation in mt

id198

generalisation

(cid:2)

(cid:1)

   

   

   

                 

from vector space id152 to mt

29/38

generalisation in mt

i 'd like a glass of white wine , please .

generation

id198

generalisation

(cid:2)

(cid:1)

   

   

   

                 

from vector space id152 to mt

29/38

generalisation in mt

i 'd like a glass of white wine , please .

generation

generalisation

(cid:2)

(cid:1)

   

   

   

                 

formal logical representations are very hard to learn from data. let
us optimistically assume a vector space and see how we go.

from vector space id152 to mt

29/38

generation

a simple distributed representation language model:

r(wn-2)

r(wn-1)

pn

c2

x

+

c1

x

=

pn = cn 2r(wn 2) + cn 1r(wn 1)
p(wn|wn 1, wn 2) / exp (r(wn)t pn)

this is referred to as a log-bilinear model.

from vector space id152 to mt

30/38

generation

a simple distributed representation language model:

r(wn-2)

r(wn-1)

pn

c2

x

+

c1

x

=

pn = cn 2r(wn 2) + cn 1r(wn 1)
p(wn|wn 1, wn 2) / exp (r(wn)t  (pn))

adding a non-linearity gives a version of what is often called a
neural, or continuous space, lm.

from vector space id152 to mt

30/38

conditional generation

r(tn-2)

r(tn-1)

pn

c2

x

+

x

=

cn

c1

+

csm

s(s1)

s(s2)

s(s3)

s(s4)

s(s5)

s(s6)

s(s7) s(s8)

pn = cn 2r(tn 2) + cn 1r(tn 1) + csm(n, s)
p(tn|tn 1, tn 2, s) / exp (r(tn)t  (pn))

from vector space id152 to mt

31/38

conditional generation: a naive first model

r(tn-2)

r(tn-1)

pn

c2

x

+

x

=

cn

c1

+

=

+

+

+

+

+

+

+

s(s1)

s(s2)

s(s3)

s(s4)

s(s5)

s(s6)

s(s7) s(s8)

pn = c2r(tn 2) + c1r(tn 1) +
p(tn|tn 1, tn 2, s) / exp (r(tn)t  (pn))

s(sj )

|s|xj=1

from vector space id152 to mt

32/38

conditional generation: a naive first model

      

      

      

      

   

   

(cid:1)

?

from vector space id152 to mt

33/38

conditional generation: a naive first model

      

      

      

      

   

   

(cid:1)

?

from vector space id152 to mt

33/38

conditional generation: a naive first model

=

+

+

+

+

+

+

+

      

      

      

      

   

   

(cid:1)

?

from vector space id152 to mt

33/38

conditional generation: a naive first model

may i have a wake-up call at seven tomorrow morning ?

clm

=

+

+

+

+

+

+

+

      

      

      

      

   

   

(cid:1)

?

from vector space id152 to mt

33/38

conditional generation: a naive first model

where 's the currency exchange office ?

clm

=

+

+

+

+

(cid:5)(cid:3) (cid:1)(cid:4)(cid:2)    

      

?

from vector space id152 to mt

34/38

conditional generation: a naive first model

i 'd like a glass of white wine , please .

clm

=

+

+

+

+

+

+

+

(cid:2)

(cid:1)

   

   

   

   

         

   

from vector space id152 to mt

34/38

conditional generation: a naive first model

i 'm going to los angeles this afternoon .

clm

=

+

+

+

+

+

                 (cid:1)           (cid:2)

   

from vector space id152 to mt

34/38

conditional generation: a naive first model

i 'd like to have a room under thirty dollars a night .

clm

=

+

+

+

+

+

+

+

+

+

+

           

                     

      

       (cid:1)    

from vector space id152 to mt

34/38

conditional generation: a naive first model

i 'd like to have a room under thirty dollars a night .

clm

=

+

+

+

+

+

+

+

+

+

+

           

                     

      

       (cid:1)    

rough gloss
i would like a night thirty dollars under room.

from vector space id152 to mt

34/38

conditional generation: a naive first model

i 'd like to have a room under thirty dollars a night .

clm

=

+

+

+

+

+

+

+

+

+

+

           

                     

      

       (cid:1)    

google translate
i want a late thirties under $   s room.

from vector space id152 to mt

34/38

conditional generation: a naive first model

you have to do something about it .

clm

=

+

+

+

+

+

+

+

+

+

   

       (cid:2)(cid:3)

   

(cid:4)

   

    (cid:1)(cid:5)

   

   

from vector space id152 to mt

34/38

conditional generation: a naive first model

i can n't urinate .

clm

=

+

+

+

+

+

+

+

+

+

   

       (cid:2)(cid:3)

   

(cid:4)

   

    (cid:1)(cid:5)

   

   

from vector space id152 to mt

34/38

conditional generaton: small test dataset

chinese (zh) ! english (en)
cdec (state-of-the-art mt)

direct (naive bag of words source)
direct (convolution p(en|zh))
noisy channel (convolution p(zh|en)p(en))
noisy channel     direct

test 1

test 2

50.1

30.8
44.6
50.1
51.0

58.9

33.2
50.4
51.8
55.2

id7 score results on a small chinese ! english translation task.

from vector space id152 to mt

35/38

summary

advantages

    unsupervised features extraction alleviates domain and

language dependencies.
    very compact models.
    distributed representations for words naturally include

morphological properties.

    the conditional generation framework easily permits additional

context such as dialogue and domain level vectors.

challenges

    better conditioning on sentence position for long sentences,

and all the other things this model does not capture!

    handling rare and unknown words.

from vector space id152 to mt

36/38

references

compositional morphology for word representations and language modelling
botha and blunsom. icml 2014

multilingual models for compositional distributed semantics
hermann and blunsom. acl 2014

a convolutional neural network for modelling sentences
kalchbrenner, grefenstette, and blunsom. acl 2014

recurrent continuous translation models
kalchbrenner and blunsom. emnlp 2013

the role of syntax in vector space models of id152
hermann and blunsom. acl 2013

from vector space id152 to mt

37/38

computational linguistics at the university of oxford

we are growing!

postdoctoral and dphil studentships are available working in

machine learning and computational linguistics

http://www.clg.ox.ac.uk

from vector space id152 to mt

38/38

