lxmls - lab guide

july 19, 2011

day 0

basic tutorials

in this class we will introduce several fundamental concepts needed further
ahead. we start with an introduction to python, the programming language we
will use in the lab sessions. afterwards, we present several notions on proba-
bility theory and id202. finally, we focus on numerical optimization.
the goal of this class is to give you the basic knowledge for you to under-
stand the following lectures. we will not enter in too much detail in any of the
topics.

0.1 python

0.1.1 running python
you can access and run python interactively, simply by running the python
command. alternatively, you can save your program to a    le and run python
on it:

python yourfile.py

in these lab sessions, we are going to be using python in interactive mode
several times. the standard python interface is not very friendly, though.
ipython, which stands for interactive python, is an improved python shell. it
saves your command history between sessions, has basic auto-complete, and
has internal support for interacting with graphs through matplotlib.

ipython is also designed to facilitate running parallel code of clusters of

machines, but we will not make use of that functionality.

to run ipython, simply run ipython on your command line. for interac-
tive numeric use, the -pylab    ag imports numpy and matplotlib for you and
sets up interactive graphs:

ipython -pylab

1

help and documentation

there are several ways to get help on ipython:

    adding a question mark to the end of a function or variable and pressing
enter brings up associated documentation. unfortunately, not all pack-
ages are well documented. numpy and matplotlib (the two libraries we
will extensively use in the lab sessions) are pleasant exceptions;

    help(   print   ) gets the online documentation for the print keyword;

    help(), enters the help system.

when at the help system, type q to exit.
for more information on ipython (p  erez and granger, 2007), check the web-

site: http://ipython.scipy.org/moin/

pro   ling

if you are interested in checking the performance of your program, you can use
the command %prun in ipython (this is an ipython-only feature). for example:

1 def myfunction(x):

...

3

%prun myfunction(22)

exiting

exit ipython by typing exit() or quit() (or typing ctrl-d).

0.1.2 python by example
the    rst program of every programmer in every new language prints    hello,
world!   . in python, this simply reads:

in[]: print "hello, world!"

2 out[]: hello, world!

2

data structures

in python, you can create lists of items with the following syntax:

countries = ['portugal','spain','united kingdom']

a string should be surrounded with apostrophes (   ). you can access a list

with the following:

    len(l), which returns the number of items in l;

    l[i], which returns the item at index i (the    rst item has index 0);

    l[i:j], which returns a new list, containing the items between i and j.

exercise 0.1 use l[i:j] to return the countries in the iberian peninsula.

loops

a loop allows a certain section of code to be repeated a certain number of times.
the loops continue until a stop condition is reached. for instance, when a vari-
able has reached a certain value or when the list you are iterating has reached
its end. in python you have while and for loops.

the following two programs output exactly the same: the even numbers

from 2 to 8.

1 i = 2

while i < 10:

3

2

print i
i += 2

for i in range(2,10,2):

print i

the range function is built int to python and it creates lists containing

arithmetic progressions.

exercise 0.2 david, john, allysson and anne are four of your colleagues in the sum-
mer course. create a python program to greet all of them. the output should be

3

hello, david!

2 hello, john!

hello, allysson!

4 hello, anne!

note that you have around 100 colleagues. you should use the data structures you
have just learned to minimize the lines of code you are using in this exercise.

control flow

the if statement allows to control the    ow of your program. the next pro-
gram makes a greeting that depends on the time of the day.

if hour < 12:

print 'good morning!'

elif hour >= 12 and hour < 20:

print 'good afternoon!'

else:

print 'good evening!'

functions

a function is a block of code that can be reused to perform a similar action. the
following is a function in python.

def greet(hour):

if hour < 12:

print 'good morning!'

elif hour >= 12 and hour < 20:

print 'good afternoon!'

else:

print 'good evening!'

2

4

6

2

4

6

if you call the function greet with different hours of the day, the program

will greet you accordingly.

exercise 0.3 note that the previous code allows the hour to be less than 0 or more
than 24. change the code in order to indicate that the hour given as input is invalid.
your output should be something like:

1 greet(50)

invalid hour: it should be between 0 and 24.

4

3 greet(-5)

invalid hour: it should be between 0 and 24.

indentation

in python, indentation is important. this is how python differentiates between
nested and non-nested blocks of commands. for instance, consider the follow-
ing code and its output:

a=1

2 while a <= 3:

print a
a += 1

4

1
2 2
3

exercise 0.4 can you predict the output of the following code:

1 a=1

while a <= 3:

3

print a

a += 1

0.1.3 plotting in python - matplolib
matplotlib is a plotting library for python. it supports 2d and 3d plots of vari-
ous forms. it can show them interactively or save them to a    le (several output
formats are supported).

import numpy as np

2 import matplotlib.pyplot as plt

4 x = np.linspace(-4, 4, 1000)

6 plt.plot(x, x**2*np.cos(x**2))

plt.savefig("simple.pdf")

5

exercise 0.5 try running the following on ipython, which will introduce you to some
of the basic numeric and plotting operations.

1 # this will import the numpy library

# and give it the np abbreviation

3 import numpy as np

5 # this will import the plotting library

import matplotlib.pyplot as plt

7

# linspace will return 1000 points,

9 # evenly spaced between -4 and +4

x = np.linspace(-4, 4, 1000)

11

# y[i] = x[i]**2

13 y = x**2

15 # plot using a red line ('r')

plt.plot(x, y, 'r')

17

# arange returns points ranging from -4 to +4

19 # (the upper argument is excluded!)

ints = np.arange(-4,5)

21

# we plot these on top of the previous plot

23 # using blue circles (o means a little circle)

plt.plot(ints, ints**2, 'bo')

25

# you may notice that the plot is tight around the line

27 # set the display limits to see better

plt.xlim(-4.5,4.5)

29 plt.ylim(-1,17)

0.1.4 numpy
numpy is a library needed for scienti   c computing with python.

multidimensional arrays

the main object of numpy is the multidimensional array. a multidimensional
array is a table with all elements of the same type and can have several dimen-
sions.

numpy provides various functions to access and manipulate multidimen-
sional arrays. in one dimensional arrays, you can index, slice, and iterate as
you can with lists. in a two dimensional array m, you can use perform these
operations along several dimensions.

6

    m[i,j], to access the item in the ith row and j-th column;

    m[i:j,:], to get the all the rows between the i-th and j-th;

    m[:,i], to get the i-th column of m.

again, as it happened with the lists, the    rst item of every column and

every row has index 0.

1 import numpy as np

a = np.array([

3

5

[1,2,3],
[2,3,4],
[4,5,6]])

7 a[0,:] # this is [1,2,3]

a[0] # this is [1,2,3] as well

9

11

13

a[:,0] # this is [1,2,4]

a[1:,0] # this is [ [2], [4] ]. why?

# because it is the same as a[1:n,0] where n is the

size of the array.

mathematical operations

there are many helpful functions in numpy. for basic mathematical opera-
tions, we have np.log, np.exp, np.cos,. . . with the expected meaning. these
operate both on single arguments and on arrays (where they will behave ele-
ment wise).

1 import matplotlib.pyplot as plt

import numpy as np

3

x = np.linspace(0, 4 * np.pi, 1000)

5 c = np.cos(x)
s = np.sin(x)

7

plt.plot(x, c)
9 plt.plot(x, s)

other functions take a whole array and compute a single value from it. for
example, np.sum, np.mean,. . . these are available as both free functions and
as methods on arrays.

7

1 import numpy as np

3 a = np.arange(100)

print np.mean(a)

5 print a.mean()

7 c = np.cos(a)
print c.ptp()

exercise 0.6 run the above example and lookup the ptp function/method (use the ?
functionality in ipython).

exercise 0.7 consider the following approximation to compute an integral

(cid:90) 1

0

f (x)dx     999   

i=0

f (i/1000)

1000

.

use numpy to implement this for f (x) = x2. you should not need to use any

loops. the exact value is 1/3. how close is the approximation?

0.2 id203 theory

id203 is the mathematical language for quantifying uncertainty. the sam-
ple space x is the set of possible outcomes of an experiment. events are subsets
of x.

example 0.1 (discrete space) let h denote    heads    and t denote    tails.    if we toss
a coin twice, then x = {hh, ht, th, tt}. the event that the    rst toss is heads is
a = {hh, ht}.

sample space can also be continuous (eg., x = r). the union of events a

and b is de   ned as a(cid:83) b = {       x |        a            b}. if a1, . . ., an is a
ai = {       x |        ai for at least one i}. we say that
sequence of sets then
a1, . . ., an are disjoint or mutually exclusive if ai     aj =     whenever i (cid:54)= j.
we want to assign a real number p(a) to every event a, called the proba-
bility of a. we also call p a id203 distribution or id203 measure.

n(cid:83)

i=1

8

de   nition 0.1 a function p that assigns a real number p(a) to each event
a is a id203 distribution or a id203 measure if it satis   es
the three following axioms:
axiom 1: p(a)     0 for every a
axiom 2: p(x) = 1
axiom 3: if a1, . . ., an are disjoint then

(cid:33)

(cid:32) n(cid:91)

i=1

p

ai

=

n   

i=1

p(ai).

one can derive many properties of p from these axioms:

p(   )
a     b     p(a)     p(b)

=

0

0     p(a)     1

p(a(cid:48))
1     p(a)
p(a     b)
p(a) + p(b)     p(a     b)
a     b =        p (a     b) = p(a) + p(b).

=
=

(a    is the complement of a)

an important case is when events are independent, this is also a usual ap-
proximation which lends several practical advantages to the computation of
joint id203.

de   nition 0.2 two events a and b are independent if

p(ab) = p(a)p(b)

(1)

often denoted as a        b. a set of events {ai : i     i} is independent if

      (cid:92)

i   j

       =    

i   j

ai

p(ai)

p

for every    nite subset j of i.

for events a and b, where p(b) > 0, id155 of a given b

has occurred is de   ned as

(2)
events a and b are independent if and only if p(a|b) = p(a). this follows

.

p(a|b) =

p(ab)
p(b)

from the de   nitions of independence and id155.

9

a preliminary result that forms the basis for the famous bayes    theorem is
the law of total id203 which states that if a1, . . . , ak is a partition of x,
then for any event b,

p(b) =

k   

i=1

p(b|ai)p(ai).

(3)

using equations 2 and 3, one can derive the famous bayes    theorem.

theorem 0.1 (bayes    theorem) let a1, . . . ak be a partition of x such that
p(ai) > 0 for each i. if p(b) > 0 then, for each i = 1, . . . , k,

p(ai|b) =

p(b|ai)p(ai)
   j p(b|aj)p(aj)

.

(4)

remark 0.1 p(ai) is called the prior id203 of a and p(ai|b) is the poste-
rior id203 of a.

remark 0.2 in bayesian statistical id136, bayes    theorem is used to compute the
estimates of parameters for distributions from data; where, prior is the initial belief
about the parameters, likelihood is the distribution function of the parameter (usually
trained from data) and posterior is the updated belief about the parameters.

0.2.1 id203 distribution functions
a random variable is a mapping x : x     r that assigns a real number x(  )
to each outcome   . given a random variable x, an important function called
the cumulative distributive function (or distribution function) is de   ned as:
de   nition 0.3 the cumulative distribution function cdf fx : r     [0, 1] of a
random variable x is de   ned by fx(x) = p(x     x).

the cdf is important because it captures the complete information about
the random variable. the cdf is right-continuous, non-decreasing and is nor-
malized (limx         f(x) = 0 and limx      f(x) = 1).

example 0.2 (discrete cdf) flip a fair coin twice and let x be the random variable
indicating the number of heads. then p(x = 0) = p(x = 2) = 1/4 and p(x =
1) = 1/2. the distribution function is

                     

fx(x) =

0
1/4
3/4
1

x < 0

0     x < 1
1     x < 2
x     2.

10

de   nition 0.4 x is discrete if it takes countable many values {x1, x2, . . .}. we de   ne
the id203 function or id203 mass function for x by

de   nition 0.5 a random variable x is continuous if there exists a function
fx such that fx     0 for all x,

fx(x)dx = 1 and for every a     b

fx(x) = p(x = x).

   (cid:82)

      

b(cid:90)

a

x(cid:90)

      

p(a < x < b) =

fx(x)dx.

(5)

the function fx is called the id203 density function (pdf). we have
that

fx(x) =

fx(t)dt

and fx(x) = f

(cid:48)
x(x) at all points x at which fx is differentiable.

a discussion of a few important distributions and related properties:

0.2.2 bernoulli
the bernoulli distribution is a discrete id203 distribution that takes 1
with the success id203 p and 0 with the failure id203 q = 1     p. a
single bernoulli trial is parametrized with the success id203 p, and the
input k     {0, 1} (1=success, 0=failure), and can be expressed as

f (k; p) = pkq1   k = pk(1     p)1   k

0.2.3 binomial
the id203 distribution for the number of successes in n bernoulli trials
is called a binomial distribution, which is also a discrete distribution. the
binomial distribution can be expressed as exactly j successes is

(cid:19)

(cid:18) n

j

(cid:19)

(cid:18) n

j

f (j, n; p) =

pjqn   j =

pj(1     p)n   j

where n is the number of bernoulli trails with id203 p of success on each
trial.

11

0.2.4 categorical
the categorical distribution (often con   ated with the multinomial distribu-
tion, in    elds such as natural language processing), is another generalization
of the bernoulli distribution, allowing the de   nition of a set of possible out-
comes, rather than simply the events    success    and    failure    de   ned in the
bernoulli distribution. considering a set of outcomes indexed from 1 to n, the
distribution takes the form of

f (xi; p1, ..., pn) = pi.

where parameters p1, ..., pn is the set with the occurrence id203 of each
outcome. note that we must ensure that    n
i=1 pi = 1, so we can set pn =
   n   1
i=1 pi = 1.

0.2.5 multinomial
the multinomial distribution is a generalization of the binomial distribution
and the categorical distribution, since it considers multiple outcomes, as the
categorial distribution, and multiple trials, as in the binomial distribution.
considering a set of outcomes indexed from 1 to n, the vector x1, ..., xn, where
xi indicates the number of times the event with index i occurs, follows the
multinomial distribution

f (x1, ..., xn; p1, ..., pn) =

n!

x1!...xn!

px1
1 ...pxn
n .

where parameters p1, ..., pn is the occurrence id203 of the respective out-
come.

0.2.6 gaussian distribution
a very important theorem in id203 theory called the central limit theo-
rem states that, under very general conditions, if we sum a very large number
of mutually independent random variables, then the distribution of the sum
can be closely approximated by a certain speci   c continuous density called the
normal (or gaussian) density. the normal density function with parameters   
and    is de   ned as follows:

fx(x) =

1   
2    

e   (x     )2/2  2,         < x <    .

fig. 1, compares a plot of normal density for the cases    = 0 and    = 1,

and    = 0 and    = 2.

12

figure 1: normal density for two sets of parameter values.

0.2.7 conjugate priors
de   nition 0.6 let f = { fx(x|s), s     x} be a class of likelihood functions; let p be a
class of id203 (density or mass) functions; if, for any x, any ps(s)     p, and any
fx(x|s)     f, the resulting a posteriori id203 function ps(s|x) = fx(x|s)ps(s)
is still in p, then p is called a conjugate family, or a family of conjugate priors, for
f.

0.2.8 id113
until now, we assumed that, for every distribution, the parameters    are known
and are used we calculate p(x|  ). there are some cases where the values of the
parameters are easy to infer, such as the id203 p getting a head using
a fair coin, used on a bernoulli or binomial distribution. however, in many
problems, these values are complex to de   ne, and it is more viable to estimate
the parameters using the data x. for instance, in the example above with the
coin toss, if the coin is somehow tampered to have a biased behavior, rather
than examining the dynamics of the structure of the coin to infer a parameter
for p, a person could simply throw the coin n times and count the number of
heads h and set p = h
n . in doing this, the person is using the data x to estimate
  .
with this in mind, we will know generalize this process. we de   ne the
id203 p(  |x), which is id203 of the parameter   , given the data x.
this id203 is called likelihood l(  |x) and measures how well the pa-
rameter    models the data x. this can be de   ned in terms of the distribution f
as

l(  |x1, ..., xn) =

f (xi|  )

n   

i=1

13

where x1, ..., xn are iid samples.

to understand this concept better, we go back to the tampered coin example
again. suppose that we throw the coin 5 times and get the sequence [1,1,1,1,1]
(1=head, 0=tail). using the bernoulli distribution 0.2.2 f to model this problem,
we get the following likelihood values:

    l(0, x) = f (1, 0)5 = 05 = 0

    l(0.2, x) = f (1, 0.2)5 = 0.25 = 0.00032

    l(0.4, x) = f (1, 0.4)5 = 0.45 = 0.01024

    l(0.6, x) = f (1, 0.6)5 = 0.65 = 0.07776

    l(0.8, x) = f (1, 0.8)5 = 0.85 = 0.32768

    l(1, x) = f (1, 1)5 = 15 = 1

if we get the sequence [1,0,1,1,0] instead, the likelihood values would be:
    l(0, x) = f (1, 0)3 f (0, 0)2 = 03    12 = 0
    l(0.2, x) = f (1, 0.2)3 f (0, 0.2)2 = 0.23    0.82 = 0.00512
    l(0.4, x) = f (1, 0.4)3 f (0, 0.4)2 = 0.43    0.62 = 0.02304
    l(0.6, x) = f (1, 0.6)3 f (0, 0.6)2 = 0.63    0.42 = 0.03456
    l(0.8, x) = f (1, 0.8)3 f (0, 0.8)2 = 0.83    0.22 = 0.02048
    l(1, x) = f (1, 1)5 = 13    02 = 0
we can that likelihood is highest when the distribution f with parameter p
is the best    t for the observed samples. thus, the best estimate for p according
to x would the value where l(p, x) is highest.

the value of the parameter    with the highest likelihood is called maximum

likelihood estimate(id113) and is de   ned as

    id113 = argmax  

l(  |x)

finding this for our example is relatively easy, since we can simply derivate

the likelihood function to    nd the absolute maximum. for the sequence [1,0,1,1,0],
the likelihood would be given as

l(p, x) = f (1, p)3 f (0, p)2 = p3(1     p)2

and the id113 estimate would be given by:

  l(p, x)

  p

= 0

14

which resolves into

pid113 = 0.6

exercise 0.8 over the next couple of exercises we will make use of the galton dataset,
a dataset of heights of fathers and sons from the 1877 paper that    rst discussed the
   regression to the mean    phenomenon.

    use the load() function in the galton.py    le to load the dataset.

    what are the mean height and standard deviation of all the people in the sample?

what is the mean height of the fathers and of the sons?

    plot a histogram of all the heights (you might want to use the plt.hist func-

tion and the ravel method on arrays).

    plot the height of the father versus the height of the son.

    you should notice that there are several points that are exactly the same (e.g.,
there are 21 pairs with the values 68.5 and 70.2). use the ? command in ipython
to read the documentation for the numpy.random.rand function and add
random jitter (i.e., move the point a little bit) to the points before displaying
them. does your impression of the data change?

0.3 essential id202

id202 provides a compact way of representing and representing on
sets of linear equations.

4x1    5x2 =    13
   2x1 +3x2 = 9

this is a system of linear equations in 2 variables. in matrix notation we

can write the system more compactly as

with

a =

ax = b

(cid:21)

(cid:20) 4
5   2

3

, b =

(cid:20)    13

(cid:21)

9

0.3.1 notation
we use the following notation:

    by a     rm  n, we denote a matrix with m rows and n columns, where

the entries of a are real numbers.

15

    by x     rn, we denote a vector with n entries. a vector can also be
thought of as a matrix with n rows and 1 column, known as a column
vector. a row vector     a matrix with 1 row and n columns is denoted as
xt, the transpose of x.

    the ith element of a vector x is denoted xi

                .

               

x1
x2
...
xn

x =

exercise 0.9 in the rest of the school we will represent both matrices and vectors as
numpy arrays. you can create arrays in different ways, one possible way is to create
an array of zeros.

import numpy as np

2 m = 3
n = 2

4 a = np.zeros([m,n])

print a

6 [[ 0.
[ 0.
8 [ 0.

0.]
0.]
0.]]

you can check the shape and the data type of your array using the following com-

mands:

print a.shape

2 (3, 2)

print a.dtype.name

4 float64

this shows you that    a    is an 3*2 arrays of type    oat64. by default, arrays contain
64 bit    oating point numbers. you can specify the particular array type by using the
keyword dtype.

a = np.zeros([m,n],dtype=int)

2 print a.dtype

int64

(on your computer, particularly if you have an older computer, int might denote
32 bits integers).

16

there are many other ways to create arrays, you can create arrays from lists of

numbers:

1 a = np.array([[2,3],[3,4]])

print a

3 [[2 3]

[3 4]]

there are many more ways to create arrays in numpy and we will get to see them

as we progress in the classes.

0.3.2 some matrix operations and properties

    product of two matrices a     rm  n and b     rn  p is the matrix c =

ab     rm  p, where

cij =

n   

k=1

aikbkj.

exercise 0.10 you can multiply two matrix by looping over both indexes and
multiplying the individual entries.

a = np.array([[2,3],[3,4]])
2 b = np.array([[1,1],[1,1]])

a_dim1, a_dim2 = a.shape
4 b_dim1, b_dim2 = b.shape

c = np.zeros([a_dim1,b_dim2])

6 for i in xrange(a_dim1):

for j in xrange(b_dim2):

8

10 print c

for k in xrange(a_dim2):

c[i,j] += a[i,k]*b[j,k]

this is, however, cumbersome and inef   cient. numpy supports matrix multipli-
cation with the dot function:

d = np.dot(a,b)

2 print d

important note: with numpy, you must use dot to get id127,
the expression a * b denotes element-wise multiplication.

    id127 is associative: (ab)c = a(bc).

17

    id127 is distributive: a(b + c) = ab + ac.
    id127 is (generally) not commutative : ab (cid:54)= ba.
    given two vectors x, y     rn the product xty, called inner product or dot

product

xty     r =(cid:2) x1

(cid:3)

x2

. . .

xn

a = np.array([1,2])
2 b = np.array([1,1])

np.dot(a,b)

                =

               

y1
y2
...
yn

n   

i=1

xiyi.

    given vectors x     rm and y     rn, the outer product xyt     rm  n is a

matrix whose entries are given by (xyt)ij = xiyj,

xyt     rm  n =

               (cid:2) y1

               

x1
x2
...
xn

(cid:3) =

y2

. . .

yn

               

                .

x1y1
x2y1
...
xmy1

x1y2
x2y2
...
xmy2

. . .
. . .
...
. . .

x1yn
x2yn
...
xmyn

1 np.outer(a,b)

array([[1, 1],

3

[2, 2]])

    the identity matrix, denoted i     rn  n, is a square matrix with ones on

the diagonal and zeros everywhere else. that is,

(cid:26) 1

0

iij =

i = j
i (cid:54)= j

it has the property that for all a     rm  n, ai = a = ia.

1 i = np.eye(2)

x = np.array([2.3, 3.4])

3

print i

18

5 print np.dot(i,x)

7 [[ 1.,
[ 0.,

0.],
1.]]

9 [2.3, 3.4]

    a diagonal matrix is a matrix where all non-diagonal elements are 0.

    the transpose of a matrix results from          ipping    the rows and columns.
given a matrix a     rm  n, the transpose at     rn  m is the n    m matrix
whose entries are given by (at)ij = aji.
also, (at)t = a;
(ab)t = bt at;
in numpy, you can access the transpose of a matrix as the t attribute:

(a + b)t = at + bt

1 a = np.array([ [1, 2], [3, 4] ])

print a.t

    a square matrix a     rn  n is symmetric if a = at.
    the trace of a square matrix a     rn  n is the sum of the diagonal ele-

ments, tr(a) =

n
   
i=1

aii

0.3.3 norms
the norm of a vector is informally the measure of the    length    of the vector.
the commonly used euclidean or (cid:96)2 norm is given by

(cid:115) n   

i=1

x2
i .

(cid:107)x(cid:107)2 =

    more generally, the (cid:96)p norm of a vector x     rn, where p     1 is de   ned

as

(cid:33)1/p

.

|xi|p

(cid:32) n   

i=1

(cid:107)x(cid:107)p =

note: (cid:96)1 norm : (cid:107)x(cid:107)1 =

|xi|

n
   
i=1

(cid:96)    norm : (cid:107)x(cid:107)    = maxi|xi| .

19

0.3.4 linear independence and rank
a set of vectors {x1, x2, . . . , xn}     rm is said to be (linearly) independent if
no vector can be represented as a linear combination of the remaining vectors.
conversely, if one vector belonging to the set can be represented as a linear
combination of the remaining vectors, then the vectors are said to be linearly
dependent. that is, if

n   1   
for some scalar values   1, . . . ,   n   1     r.

xn =

i=1

  ixi

    the rank of a matrix is the number of linearly independent columns.
    for a     rm  n, rank(a)     min(m, n). if rank(a) =min(m, n), then a is

said to be full rank.

    for a     rm  n, rank(a)=rank(at).
    for a     rm  n, b     rn  p, rank(ab)     min(rank(a),rank(b)).
    for a, b     rm  n, rank(a + b)     rank(a) + rank(b).
    two vectors x, y     rn orthogonal if xty = 0. a square matrix u    
rn  n is orthogonal if all its columns are orthogonal to each other and are
normalized ((cid:107)x(cid:107)2 = 1), it follows that

utu = i = uut.

0.4 numerical optimization

most problems in machine learning require minimization/maximization of
functions (likelihoods, risk, energy, id178, etc.,).

x    = arg min

x

f (x)

in a few special cases, we can solve this minimization problem analytically
in closed form (solving for optimal x    in    x f (x   ) = 0), but in most cases such
in this section we will cover some basic notions
a solutions does not exist.
of numerical optimization. the goal is to provide the intuitions behind the
methods that will be used in the rest of the school. there are plenty of good
textbooks in the subject that you can consult for more information (nocedal
and wright, 1999; bertsekas et al., 1995; boyd and vandenberghe, 2004).

the most common way to solve the problems when no closed form solu-
tion is available is to resort to an iterative algorithm. in this section, we will
see some of these iterative optimization techniques. these iterative algorithms

20

compute a sequence of points x(0), x(1), . . .     domain( f ) such that hopefully
xt = x   . such a sequence is called the minimizing sequence for the problem.

0.4.1 convex functions
one important concept of the function f (x) is if it is convex function (in the
shape of a bowl) or a non-convex-function. figures 2 and 3 show an example
of a convex and a non-convex function. convex functions are particular useful
since you are guarantee that the minimizing sequence converges to the true
global minimum of the function, while in non-convex functions you can only
guarantee to reach a local minimum.

figure 2: illustration of a convex function. the line segment between any two
points on the graph lies entirely above the curve.

figure 3:
secting the curve.

illustration of a non-convex function. note the line segment inter-

intuitively, imagine dropping a ball on either side of figure 2, the ball will
role to the bottom of the bowl independently from where it is dropped. this is
the main bene   ts of a convex functions. on the other hand if you drop a ball
from the left side of figure 3 it will reach a different position than if you drop
a ball from its right side. moreover, dropping it from the left side will lead you
to a much better place than if you drop the ball from the right side. this is
the main problem with non-convex function, there are no guarantees about the
quality of the local minimum you    nd.

21

function f (x) derivative     f
   x
x2
xn
log(x)
exp(x)
1
x

2x
nxn   1
1
x
exp(x)
    1
x2

table 1: some derivative examples

more formally, some concepts to understand about convex functions are:
a line segment between points x1 and x2: contains all points such that

x =   x1 + (1       )x2

where 0            1.

a convex set contains the line segment between any two points in the set

x1, x2     c,

0            1       x1 + (1       )x2     c

a function f : rn     r is a convex function if the domain of f is a convex

set and

f (  x + (1       )y)        f (x) + (1       ) f (y)

for all x, y     domain of f , 0            1

0.4.2 derivative and gradient
the derivative of a function is a measure of how the function varies with its
input variables. given an interval [a, b] one can compute how the function
varies within that interval by calculating the average of the function in that
interval.

f (a)     f (b)

a     b

(6)

the derivative can be seen as the limit as the interval goes to zero, and it gives
us the slope of the function at that point.

    f
   x

= lim
h=0

f (x + h)     f (x)

h

(7)

the school.

table 1 shows derivatives of some functions that we will be using during
an important rule of derivation is the chain. consider h = f     g, and u =

g(x), then:

22

   h
   x

=

    f
   u

      g
   x

(8)

example 0.3 consider the function h(x) = exp(x2).
h(x) = f (g(x)) = f (u) = exp(u), where u = g(x) = x2.
   x =     f
   h

   x = exp(u)    2x = exp(x2)    2x

   u       u

example 0.4 consider the function f (x) = x2 and its derivative     f
   x . look at the
derivative of that function at points [-2,0,2], draw the tangent to the graph in that
point.
   x (   2) =    4,     f
    f
for example, the tangent equation for x =    2 is y =    4x     b, where b = f (   2)
the following code plots the function and the derivatives on those points using

   x (0) = 0, and     f

   x (2) = 4

matplotlib (see figure 4).

a = np.arange(-5,5,0.01)

2 f_x = np.power(a,2)

plt.plot(a,f_x)

4

plt.xlim(-5,5)

6 plt.ylim(-5,15)

8 k= np.array([-2,0,2])
plt.plot(k,k**2,"bo")

10 for i in k:

plt.plot(a, (2*i)*a - (i**2))

the gradient of a function is a generalization of the derivative concept we
just saw before, for several dimensions. lets assume we have a function f (x)
where x     r2 so can be seen as a pair x = x1, x2 then the gradient measures
the slope of the function in both directions.    x f (x) = [     f
   x1

,     f
   x2

].

0.4.3 gradient based methods
gradient based methods are probably the most common methods used for
   nding the minimizing sequence for a given function. the methods use in
this class will make use of the function value f (x) as well as the gradient of
the function    x f (x). the simplest method is the id119 method, an
unconstrained    rst-order optimization algorithm.
the intuition of this method is as follows: you start at a given point x0 and
compute the gradient at that point    x0 f (x). you then take a step of length    on
the direction of the negative gradient to    nd a new point: x1 = x0          x0 f (x).

23

illustration of the gradient of the function f (x2) at three different
figure 4:
points x = [   2, 0.2]. note that at point x = 0 the gradient is zero which corre-
sponds to the minimum of the function.

you proceed until the algorithm until you have reached a minimum (local or
global). recall from the previous subsection that you can identify the mini-
mum by testing if the norm of the gradient is zero |    f (x)| = 0.

there are several practical concerns even with this basic algorithm to ensure
both that the algorithm converges (reaches the minimum) and that it does so in
fast way (by fast we mean the number of function and gradient evaluations).

    step size    a    rst question is how to    nd the step length   . one condi-
tion is that eta should guarantee suf   cient decrease in the function value.
we will not cover these methods here but the most common ones are
backtracking line search or the wolf line search (nocedal and wright,
1999).

    descent direction a second problem is that using the negative gradi-
ent as direction can lead to a very slow convergence. different methods
that change the descent direction by multiplying the gradient by a ma-
trix    have been proposed that guarantee a faster convergence. two no-
table methods are the conjugate gradient (cg) and the limited memory
quasi id77s(lbfgs) (nocedal and wright, 1999).

    stopping criteria finally, it will normally not be possible to reach full
convergence either because it will be too slow, or because of numerical is-
sues (computers cannot perform exact arithmetic). so normally we need
to de   ne a stopping criteria for the algorithm. three common criteria
(that are normally used together) are: a maximum number of iterations;
the gradient norm be smaller than a given threshold |    f (x)|       1, or

24

the normalized difference in the function value be smaller than a given
threshold

| f (xt)    f (xt   1)|
max(| f (xt)|,| f (xt   1)|)       2

algorithm 1 shows the general gradient based algorithm. note that for the
simple id119 algorithm    is the identity matrix and the descent di-
rection is just the negative gradient of the function |         f (x)|. figure 5 shows
an illustration of the id119 algorithm.

algorithm 1 id119
1: given a starting point x0, i = 0
2: repeat
3: compute step size   
4: compute descent direction   
5: xi+1     xi +         f (xi)
6: i     i + 1
7: until stopping criterion is satis   ed.

figure 5:
illustration of id119. the blue circles correspond to the
function values at different points, while the red lines correspond to steps taken
in the negative gradient direction.

exercise 0.11 consider the function f (x) = (x + 2)2     16 exp(cid:0)   (x     2)2(cid:1). make

a function that computes the function value given x.

25

def get_y(x):

2

value =
return value

pow((x+2),2) - 16*math.exp(-x*(x-2))

draw a plot around x     [   8, 8].

1 x = np.arange(-8,8,0.001)

y = map(lambda l: get_y(l),x)

3 plot(x,y)

calculate the derivative of the function f (x), implement the function get grad(x).

1 def get_grad(x):

return (2*x+4)-16*(-2*x + 4)*np.exp(-((x-2)**2))

use the method id119 to    nd the minimum of this function. convince
yourself that the code is doing the proper thing. look at the constants we de   ned.
note, that we are using a simple approach to pick the step size (always have the value
step size) which is not necessarily correct.

def gradient_descent(start_x,func,grad):

2

4

6

8

10

12

14

16

18

20

# precision of the solution
prec = 0.0001
#use a fixed small step size
step_size = 0.1
#max iterations
max_iter = 100
x_new = start_x
res = []
for i in xrange(max_iter):

x_old = x_new
#use beta egual to -1 for id119
x_new = x_old - step_size * get_grad(x_new)
f_x_new = get_y(x_new)
f_x_old = get_y(x_old)
res.append([x_new,f_x_new])
if(abs(f_x_new -f_x_old) < prec):

print "change in function values to small, leaving"
return np.array(res)

print "exceeded maximum number of iterations, leaving"
return np.array(res)

26

run the id119 algorithm starting from x0 =    8 and plot the minimiz-

ing sequence.

1 x_0 = -8

res = gradient_descent(x_0,get_y,get_grad)

3 plot(res[:,0],res[:,1],'+')

function f (x) = (x + 2)2     16 exp(cid:0)   (x     2)2(cid:1). the function is represented

figure 6: example of running id119 starting on point x0 =    8 for

in blue, while the points of the minimizing sequence are displayed as green
squares.

figure 6 shows the resulting minimizing sequence. note that the algorithm con-
verged to a minimum, but since the function is not convex it converged only to a local
minimum.

now try the same exercise starting from the initial point x0 = 8.

1 x_0 = 8

res = gradient_descent(x_0,get_y,get_grad)

3 plot(res[:,0],res[:,1],'+')

27

function f (x) = (x + 2)2     16 exp(cid:0)   (x     2)2(cid:1). the function is represented

figure 7: example of running id119 starting on point x0 = 8 for

in blue, while the points of the minimizing sequence are displayed as green
squares.

figure 7 shows the resulting minimizing sequence. note that now the algorithm
converged to the global minimum. however, note that to get to the global minimum
the sequence of points jumped from one side of the minimum to the other. this is a
consequence of using a wrong step size (in this case too large).

repeat the previous exercise changing both the values of the step-size and the pre-

cision. what do you observe.

during this school we will rely on the numerical optimization methods pro-
vided by scipy (scienti   c computing library in python), which are very ef   cient
implementations.

28

