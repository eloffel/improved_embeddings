day 1

classi   cation

this day will serve as an introduction to machine learning. we recall some fun-
damental concepts about decision theory and classi   cation. we also present
some widely used models and algorithms and try to provide the main mo-
tivation behind them. there are several textbooks that provide a thorough
description of some of the concepts introduced here: for example, mitchell
(1997),duda et al. (2001), sch  olkopf and smola (2002), joachims (2002), bishop
(2006), manning et al. (2008), to name just a few. the concepts that we intro-
duce in this chapter will be revisited in later chapters, where the same algo-
rithms and models will be adapted to structured inputs and outputs. for now,
we concern only with multi-class classi   cation (with just a few classes).

1.1 notation

in what follows, we denote by x our input set (also called observation set), and
by y our output set. we will make no assumptions about the set x, which can
be continuous or discrete. in this lecture, we consider classi   cation problems,
where y = {c1, . . . , ck} is a    nite set, consisting of k classes (also called labels).
for example, x can be a set of documents in natural language, and y a set of
topics, the goal being to assign a topic to each document.

we use upper-case letters for denoting random variables, and lower-case

letters for value assignments to those variables: for example,

    x is a random variable taking values on x,

    y is a random variable taking values on y,
    x     x and y     y are particular values for x and y.

we consider events such as x = x, y = y, etc. throughout, we use modi   ed
notation and let p(y) denote the id203 associated with the event y = y

29

(instead of writing py(y = y)). joint and conditional probabilities are denoted
respectively as p(x, y) (cid:44) px,y(x = x     y = y) and p(x|y) (cid:44) px|y(x = x | y =
y). from the laws of probabilities:

p(x, y) = p(y|x)p(x),

(1.1)

for all x     x and y     y.

quantities that are predicted or estimated from the data will be appended
a hat-symbol: for example, estimations of the probabilities above are denoted
as   p(y),   p(x, y) and   p(y|x); and a prediction of an output will be denoted   y.

we assume that a training dataset d is provided which consists of input-

output pairs (called examples or instances):

d = {(x1, y1), . . . , (xm, ym)}     x    y.

(1.2)

the goal of (supervised) machine learning is to use d to learn a function h
(called a classi   er) that maps from x to y: this way, given a new instance x     x
(test example), the machine makes a prediction   y by evaluating h on x, i.e.,
  y = h(x).

1.2 generative classi   ers: na    ve bayes

if we knew the true distribution p(x, y), the best possible classi   er (bayes op-
timal) would be one which predicts according to

=   

  y = arg max
y   y
arg max
y   y
= arg max
y   y

p(y|x) = arg max
y   y

p(x, y)
p(x)

p(x, y)
p(y)p(x|y),

(1.3)

where in     we used the fact that p(x) is constant with respect to y. the proba-
bility distributions p(y) and p(x|y) are respectively called the class prior and
the class conditionals. figure 1.2 shows an example of the bayes optimal deci-
sion boundary for a toy example. generative models assume data are gen-
erated according to the following generative story (independently for each
m = 1, . . . , m):

1. a class ym     p(y) is drawn from the class prior distribution;
2. an input xm     p(x|y = ym) is drawn from the corresponding class

conditional.

training a generative model amounts to estimating these probabilities using the
dataset d, yielding estimates   p(y) and   p(x|y).

30

figure 1.1: example of a dataset. the input set consists in points in the real
plane, x = r2, and the output set consists of two classes (red and blue). train-
ing points are represented as squares, while test points are represented as cir-
cles.

31

figure 1.2: example of a dataset together with the corresponding bayes op-
timal decision boundary. the input set consists in points in the real plane,
x = r, and the output set consists of two classes (red and blue). training
points are represented as squares, while test points are represented as circles.

32

at test time, given a new input x     x, a prediction is made according to

  y = arg max
y   y

  p(y)   p(x|y).

(1.4)

we are left with two important problems:

1. how should the distributions   p(y) and   p(x|y) be    de   ned    (i.e., what
kind of independence assumptions should they state, or how should they
factor?)

2. how should parameters be estimated from the training data d?

the    rst problem strongly depends on the application at hand. quite often,
there is a natural decomposition of the input variable x into j components,

x = (x1, . . . , xj).

(1.5)

the na    ve bayes method makes the following assumption: x1, . . . , xj are con-
ditionally independent given the class. mathematically, this means that

p(x|y) =

p(xj|y).

j   

j=1

(1.6)

note that this independence assumption greatly reduces the number of pa-
rameters to be estimated (degrees of freedom) from o(exp(j)) to o(j), hence
estimation of   p(y) and   p(x|y) becomes much simpler, as we shall see. it also
makes the overall computation much more ef   cient (in particular for large j)
and it decreases the risk of over   tting the data. on the other hand, if the as-
sumption is over-simplistic it may increase the risk of under-   tting.

the maximum likelihood criterion aims to maximize the id203 of the
training sample, assuming it was generated iid. this id203 (call it p(d))
factorizes as

p(d) =

=

m   

m=1

m   

m=1

p(xm, ym)

p(ym)

j   

j=1

p(xm

j |ym).

(1.7)

1.2.1 example: 2-d gaussians
we    rst illustrate the na    ve bayes assumption with a toy example. suppose
that x = r2 and y = {1, 2}. assume that each class-conditional is a two-
dimensional gaussian distribution with    xed covariance, i.e., p(x1, x2|y =
y) = n(  y,   y).

33

according to the na    ve bayes assumption,   p(x1, x2|y) =   p(x1|y)   p(x2|y)
(remark: this is equivalent to assuming that the   y are diagonal!). for sim-
plicity, we also assume that the two classes have unit variance. then, we
have   p(x1|y = y) = n(  y1, 1.0) and   p(x2|y = y) = n(  y2, 1.0) (figure
1.1 shows an example a dataset of two gaussians with unit variance. where
  y1 = [   1,   1] and   y1 = [1, 1]. figure 1.2 shows the same example but where
both gaussian have    = 0.5, together with the bayes optimal decision bound-
ary). the parameters that need to be estimated are the class-conditional means
  11,   12,   21,   22 and the class priors   p(y = 1) and   p(y = 2). given a training
sample d = {(x1, y1), . . . , (xm, ym)}, denote by i1     {1, . . . , m} the indices of
those instances belonging to class 1, and by i2     {1, . . . , m} the indices of the
ones that belong to class 2. the maximum likelihood estimates of the quantities
above are:

1

  p(y = 1) =
|i1|    
m   i1
|i2|    
m   i2

1

  11 =

  21 =

,

|i1|
m
xm
1 ,

  12 =

1

  p(y = 2) =
|i1|    
m   i1
|i2|    
m   i2

1

|i2|
m
xm
2

xm
2 .

xm
1 ,

  22 =

(1.8)

in words: the class priors    estimates are their relative frequencies, and the class-
conditional means    estimates are the sample means.

exercise 1.1 start by importing all the libraries necessary for this lab through
the following preamble:

1 import sys

sys.path.append("readers/" )

3 sys.path.append("classifiers/" )

5 import simple_data_set as sds

import linear_classifier as lcc

7 import gaussian_naive_bayes as gnbc

import naive_bayes as nb

now, generate a training and a test dataset like in the previous example, each
with m = 100 points, 50 of each class. assume the following class-conditionals:
p(x|y = 1)     n((   1,   1),   2i) and p(x|y = 2)     n((1, 1), i), for
   = 1.0. to do this, run the following command from the code directory:

sd = sds.simpledataset(nr_examples=100, g1 = [[-1,-1],1],

g2 = [[1,1],1], balance=0.5, split=[0.5, 0, 0.5])

34

you can visualize your data and see the bayes optimal surface boundary by typ-
ing:

1 fig,axis = sd.plot_data()

now, run na    ve bayes on this dataset. to do that, use the class gaussiannaivebayes,
which is de   ned in the    le gaussiannaivebayes.py under the classi   ca-
tion directory. report your estimates, as well as training set and testing set
accuracies:

1 gnb = gnbc.gaussiannaivebayes()

params_nb_sd = gnb.train(sd.train_x, sd.train_y)

3

print "estimated means"

5 print gnb.means

print "estimated priors"

7 print gnb.prior

y_pred_train = gnb.test(sd.train_x,params_nb_sd)

9 acc_train = gnb.evaluate(sd.train_y, y_pred_train)

y_pred_test = gnb.test(sd.test_x,params_nb_sd)

11 acc_test = gnb.evaluate(sd.test_y, y_pred_test)

print "gaussian naive bayes simple dataset accuracy train:

%f test: %f"%(acc_train,acc_test)

to visualize the surface boundary estimated by na    ve bayes, type:

fig,axis = sd.add_line(fig,axis,params_nb_sd,"naive bayes",

"red")

do not worry for now about why the surface boundaries look the way they look.
this is going to be the subject of   1.3.
repeat the exercise above for different values of   2, different balances, and differ-
ent sample sizes. what do you observe?

1.2.2 example: multinomial model for document classi   ca-

tion

we now consider a more realistic scenario where the na    ve bayes classi   er may
be applied. suppose that the task is document classi   cation: x is the set of all
possible documents, and y = {c1, . . . , ck} is a set of topics for those documents.
let v = {w1, . . . , wj} be the vocabulary, i.e., the set of words that occur in some
document.

35

a very popular id194 is through a    bag-of-words   : each
document is seen as a multiset of words along with their frequencies; word or-
dering is ignored. we are going to see that this is equivalent to a na    ve bayes
assumption with the multinomial model.1 we associate to each class a multino-
mial distribution, which ignores word ordering, but takes into consideration
the frequency with which each word appears in a document. for simplicity,
we assume that all documents have the same length l.2 each document x is
assumed to have been generated as follows. first, a class y is generated accord-
ing to p(y). then, x is generated by sequentially picking words from v with
replacement. each word wj is picked with id203 p(wj|y). for example,
the id203 of generating a document x = wj1 . . . wjl (i.e., a sequence of l
words   tokens   wj1, . . . , wjl) is
l   

p(x|y) =

p(wj|y)nj(x),

p(wjl|y) =

j   

(1.9)

l=1

j=1

where nj(x) is the number of occurrences of word wj in document x.

hence, the assumption is that word occurrences (tokens) are independent
given the class. the parameters that need to be estimated are   p(c1), . . . ,   p(ck),
and   p(wj|ck) for j = 1, . . . , j and k = 1, . . . , k. given a training sample d =
{(x1, y1), . . . , (xm, ym)}, denote by ik the indices of those instances belonging
to the kth class. the maximum likelihood estimates of the quantities above are:

  p(ck) =

|ik|
m

,

  p(wj|ck) =

   m   ik nj(xm)

|ik|

.

(1.10)

in words: the class priors    estimates are their relative frequencies (as before),
and the class-conditional word probabilities are the relative frequencies of those
words across documents with that class.

exercise 1.2 in this exercise we will use the the amazon id31 data
(blitzer et al., 2007), where the goal is to classify text documents as expressing a pos-
itive or negative sentiment (i.e., a classi   cation problem with two labels). we are
going to focus on book reviews. to load the data, type:

1 import sentiment_reader as srs

import naive_bayes as nb

3

scr = srs.sentimentcorpus("books")

1another popular model for documents is the bernoulli model, which only looks at the pres-
ence/absence of a word in a document, rather than word frequency. see manning et al. (2008);
mccallum and nigam (1998) for further information.

2we can get rid of this assumption by de   ning a distribution on the document length. every-

thing stays the same if that distribution is uniform up to a maximum document length.

36

this will load the data in a bag-of-words representation where rare words (occurring
less than 5 times in the training data) are removed.

1. create a    le multinomialnaivebayes.py and implement the na    ve bayes
with the multinomial model in a new class multinomialnaivebayes. (hint:
look at the implementation of gaussiannaivebayes for inspiration).

2. run na    ve bayes with the multinomial model on the amazon dataset (sentiment

classi   cation) and report results both for training and testing:

import multinomial_naive_bayes as mnb

2

mnb = mnb.multinomialnaivebayes()

4 params_nb_sc = mnb.train(scr.train_x,scr.train_y)
y_pred_train = mnb.test(scr.train_x,params_nb_sc)

6 acc_train = mnb.evaluate(scr.train_y, y_pred_train)

y_pred_test = mnb.test(scr.test_x,params_nb_sc)

8 acc_test = mnb.evaluate(scr.test_y, y_pred_test)

print "multinomial naive bayes amazon sentiment accuracy

train: %f test: %f"%(acc_train,acc_test)

3. observe that words that were not observed at training time cause problems at
test time. why? to solve this problem, apply a simple add-one smoothing
technique: replace the expression in eq. 1.10 for the estimation of the conditional
probabilities by

  p(wj|ck) =

1 +    m   ik nj(xm)

j + |ik|

.

where j is the number of distinct words.
this is a widely used smoothing strategy which has a bayesian interpretation: it
corresponds to choosing a uniform prior for the word distribution on both classes,
and to replace the maximum likelihood criterion by a maximum a posteriori
approach. this is a form of id173, preventing the model from over-
   tting on the training data. see e.g. manning and sch  utze (1999); manning
et al. (2008) for more information. report the new accuracies.

1.3 features and linear classi   ers

in the previous section, we assumed a particular representation for the input
objects x     x: points in a 2d euclidean space (in the gaussian example) and
bag-of-words representations of text documents (in the sentiment data exam-
ple).

the methods discussed in this lecture are also applicable to a wide range of
problems, regardless of the intricacies of our input objects. it is useful to think

37

about each x     x as an abstract object, which is subject to a set of descriptions
or measurements, which are called features. a feature is simply a real number
that describes the value of some property of x. for instance in the toy examples
described above you can thing of x as a set of points and the features to be its
2d coordinates. let g1(x), . . . , gj(x) be j features of x. we call the vector

(1.11)
a feature vector representation of x. the map g : x     rj is called a feature
mapping.

g(x) = (g1(x), . . . , gj(x))

in nlp applications, features are often binary-valued and result from eval-

uating propositions such as:

(cid:26) 1,
(cid:26) 1,
(cid:26) 1,

0, otherwise.

0, otherwise.

0, otherwise.

g1(x) (cid:44)

g2(x) (cid:44)

g3(x) (cid:44)

if sentence x contains the word ronaldo

if all words in sentence x are capitalized

if x contains any of the words amazing, excellent or :-)

in this example, the feature vector representation of the sentence x

ronaldo kicked the ball and scored an amazing goal!

would be g(x) = (1, 0, 1).
in multi-class learning problems, rather than associating features only with
the input objects, it is useful to consider joint feature mappings f : x    y     rd.
in that case, the joint feature vector f (x, y) can be seen as a collection of joint
input-output measurements. for example:

(cid:26) 1,
(cid:26) 1,

f1(x, y) (cid:44)

f2(x, y) (cid:44)

0, otherwise.

0, otherwise.

if x contains ronaldo, and topic y is sport

if x contains ronaldo, and topic y is politics

a very simple form of de   ning a joint feature mapping which is often em-
ployed is via:

f (x, y) (cid:44) g(x)     ey

(cid:124)(cid:123)(cid:122)(cid:125)

yth slot

= (0, . . . , 0, g(x)

, 0, . . . , 0)

(1.17)

where g(x)     rj is a input feature vector,     is the kronecker product ([a    
b]ij = aibj) and ey     rk, with [ey]c = 1 iff y = c, and 0 otherwise. hence

38

(1.12)

(1.13)

(1.14)

(1.15)

(1.16)

f (x, y)     rd with d = jk.

linear classi   ers are very popular in natural language processing applica-

tions. they make their decision based on the rule:
w    f (x, y).

  y = arg max
y   y

(1.18)

where

    w     rd is a weight vector;
    f (x, y)     rd is a feature vector;
    w    f (x, y) =    d

d=1 wd fd(x, y) is the inner product between w and f (x, y).
hence, each feature fd(x, y) has a weight wd and, for each class y     y, a score
is computed by linearly combining all the weighted features. all these scores
are compared, and a prediction is made by choosing the class with the largest
score.

remark 1.1 with the design above (eq. 1.17), and decomposing the weight vector as
w = (wc1, . . . , wck ), we have that

w    f (x, y) = wy    g(x).

(1.19)
in words: each class y     y gets its own weight vector wy, and one de   nes a input fea-
ture vector g(x) that only looks at the input x     x. this representation is very useful
when features only depend on input x since it allows a more compact representation.
note that the number of features is normally very large.

remark 1.2 the multinomial na    ve bayes classi   er described in the previous section
is an instance of a linear classi   er (in fact, so is the 2-d gaussian bayes classi   er   
try to show this). recall that the na    ve bayes classi   er predicts according to   y =
arg maxy   y   p(y)   p(x|y). taking logs, in the multinomial model for document classi-
   cation this is equivalent to:

where

  y = arg max
y   y

log   p(y) + log   p(x|y)

= arg max
y   y

= arg max
y   y

log   p(y) +
wy    g(x),

nj(x) log   p(wj|y)

j   

j=1

wy = (cid:0)by, log   p(w1|y), . . . , log   p(wj|y)(cid:1)

by = log   p(y)

(1.20)

g(x) = (1, n1(x), . . . , nj(x)).

(1.21)

39

algorithm 2 averaged id88
1: input: dataset d, number of rounds r
2: initialize t = 0, wt = 0
3: for r = 1 to r do
4: ds = shuf   e(d)
for i = 1 to m do
5:
6:
7:
8:

m = ds(i)
t = t + 1
take training pair (xm, ym) and predict using the current model:

  y     arg max
y(cid:48)   y

wt    f (xm, y(cid:48))

update the model: wt+1     wt + f (xm, ym)     f (xm,   y)

end for

9:
10:
11: end for
12: output: the averaged model   w     1

t    t

i=1 wi

hence, the multinomial model yields a prediction rule of the form

  y = arg max
y   y

wy    g(x).

(1.22)

exercise 1.3 show that the gaussian na    ve bayes classi   er with shared and given
variance is also a linear classi   er, and derive the formulas for wy, by. you should obtain
the formulas that are implemented in the train method of gaussiannaivebayes.
look again at the decision boundary that you have found in exercise 1.1 and com-

pare it with the bayes optimal classi   er.

1.4 online algorithms: id88 and mira

id88

1.4.1
perhaps the oldest algorithm to train a linear classi   er is the id88 (rosen-
blatt, 1958), which we depict as alg. 2.3

the id88 algorithm works as follows: at each round, it takes an in-
put datum, and uses the current model to make a prediction. if the prediction
is correct, nothing happens. otherwise, the model is corrected by adding the
feature vector w.r.t. the correct output and subtracting the feature vector w.r.t.
the predicted (wrong) output. then, we proceed to the next round. alg. 2 is
remarkably simple; yet it often reaches a very good performance, often bet-

3actually, we are showing a more robust variant of the id88, which averages the weight

vector as a post-processing step.

40

ter than the na    ve bayes model, and usually not much worse than maximum
id178 models or id166s (which will be described in the next section).
a weight vector w de   nes a separating hyperplane if it classi   es all the train-
ing data correctly, i.e., if ym = arg maxy   y w    f (xm, y) hold for m = 1, . . . , m. a
dataset d is separable if such a weight vector exists (in general, w is not unique).
a very important property of the id88 algorithm is the following: if d
is separable, then the number of mistakes made by the id88 algorithm is
   nite. this means that under this assumption, the id88 will eventually
reach a separating hyperplane w.

there are other variants of the id88 (e.g., with id173) which

we omit for brevity.

exercise 1.4 we provide an implementation of the id88 algorithm in the class
id88 (   le id88.py).

1. run the id88 algorithm on the simple dataset previously generated and

report its train and test set accuracy:

1 import id88 as percc

3 perc = percc.id88()

params_perc_sd = perc.train(sd.train_x,sd.train_y)

5 y_pred_train = perc.test(sd.train_x,params_perc_sd)
acc_train = perc.evaluate(sd.train_y, y_pred_train)

7 y_pred_test = perc.test(sd.test_x,params_perc_sd)

acc_test = perc.evaluate(sd.test_y, y_pred_test)

9 print "id88 simple dataset accuracy train: %f test: %

f"%(acc_train,acc_test)

2. plot the decision boundary found:

1 fig,axis = sd.add_line(fig,axis,params_perc_sd,"id88"

,"blue")

change the code to save the intermediate weight vectors, and plot them every
   ve iterations. what do you observe?

3. run the id88 algorithm on the amazon dataset.

1.4.2 margin infused relaxed algorithm (mira)
the mira algorithm (crammer and singer, 2002; crammer et al., 2006) has
achieved very good performance in nlp problems. at each round t, mira
updates the weight vector by solving the following optimization problem:

41

wt+1     arg min

w,  

   +   

2 (cid:107)w     wt(cid:107)2

(1.23)

       0,

s.t. w    f (xm, ym)     w    f (xm,   y) + 1       

(1.24)
(1.25)
where   y = arg maxy(cid:48)   y wt    f (xm, y(cid:48)) is the prediction using the model with
weight vector wt. by inspecting eq. 1.23 we see that mira attempts to achieve
a tradeoff between conservativeness (penalizing large changes from the previous
2 (cid:107)w     wt(cid:107)2) and correctness (by requiring, through
weight vector via the term   
the constraints, that the new model wt+1    separates    the true output from the
prediction with a margin (although slack        0 is allowed).4 note that, if the
prediction is correct (   y = ym) the solution of the problem eq. 1.23 leaves the
weight vector unchanged (wt+1 = wt). this quadratic programming problem
has a closed form solution:5

with

  t = min

   1,

  

wt+1     wt +   t( f (xm, ym)     f (xm,   y)),
(cid:26)

wt    f (xm,   y)     wt    f (xm, ym) +   (   y, ym)

(cid:107) f (xm, ym)     f (xm,   y)(cid:107)2

(cid:27)

,

where    : y    y     r+ is a non-negative cost function, such that   (   y, y) is the
cost incurred by predicting   y when the true output is y; we assume   (y, y) = 0
for all y     y. for simplicity, we focus here on the 0/1-cost (but keep in mind
that other cost functions are possible):

(cid:26) 1

  (   y, y) =

if   y (cid:54)= y

0 otherwise.

(1.26)

mira is depicted in alg. 3. for other variants of mira, see crammer et al.

(2006).

exercise 1.5 implement the mira algorithm (hint: use the id88 algorithm as
a starting point and modify it as necessary). do this by creating a    le mira.py and
implement class mira. then, repeat the id88 exercise now using mira, for
several values of   :

1 import mira as mirac

3 mira = mirac.mira()

mira.regularizer = 1.0 # this is lambda

4the intuition for this large margin separation is the same for support vector machines, which

will be discussed in   1.5.2.

5note that the id88 updates are identical, except that we always have   t = 1.

42

algorithm 3 mira
1: input: dataset d, parameter   , number of rounds r
2: initialize t = 0, wt = 0
3: for r = 1 to r do
4: ds = shuf   e(d)
for i = 1 to m do
5:
6:
7:
8:

m = ds(i)
t = t + 1
take training pair (xm, ym) and predict using the current model:

  y     arg max
y(cid:48)   y

wt    f (xm, y(cid:48))

9:

10:

(cid:111)
compute loss: (cid:96)t = wt    f (xm,   y)     wt    f (xm, ym) +   (   y, ym)
compute stepsize:   t = min
update the model: wt+1     wt +   t( f (xm, ym)     f (xm,   y))

(cid:107) f (xm,ym)    f (xm,   y)(cid:107)2

     1,

(cid:110)

(cid:96)t

end for

11:
12:
13: end for
14: output: the averaged model   w     1

t    t

i=1 wi

5 params_mira_sd = mira.train(sd.train_x,sd.train_y)

y_pred_train = mira.test(sd.train_x,params_mira_sd)
7 acc_train = mira.evaluate(sd.train_y, y_pred_train)

y_pred_test = mira.test(sd.test_x,params_mira_sd)

9 acc_test = mira.evaluate(sd.test_y, y_pred_test)

print "mira simple dataset accuracy train: %f test: %f"%(

acc_train,acc_test)

11 fig,axis = sd.add_line(fig,axis,params_mira_sd,"mira","green")

13 params_mira_sc = mira.train(scr.train_x,scr.train_y)
y_pred_train = mira.test(scr.train_x,params_mira_sc)
15 acc_train = mira.evaluate(scr.train_y, y_pred_train)

y_pred_test = mira.test(scr.test_x,params_mira_sc)

17 acc_test = mira.evaluate(scr.test_y, y_pred_test)

print "mira amazon sentiment accuracy train: %f test: %f"%(

acc_train,acc_test)

compare the results achieved and separating hiperplanes found.

43

1.5 discriminative classi   ers: maximum id178

and support vector machines

unlike the na    ve bayes classi   er, the algorithms described in the last section
(id88 and mira) directly focus on    nding a separating hyperplane to
discriminate among the classes, rather than attempting to model the probabil-
ity p(x, y) that generates the data. this kind of methods are called discrimina-
tive (by opposition to the generative ones). this section presents two important
discriminative classi   ers, with widespread use in nlp applications: maximum
id178 and support vector machines.

1.5.1 maximum id178 classi   ers
the notion of id178 in the context of id205 (shannon, 1948)
is one of the most signi   cant advances in mathematics in the twentieth cen-
tury. the principle of maximum id178 (which appears under different names,
such as    maximum mutual information    or    minimum kullback-leibler diver-
gence   ) plays a fundamental role in many methods in statistics and machine
learning (jaynes, 1982). for an excellent textbook on id205, we
recommend cover et al. (1991). the basic rationale is that choosing the model
with the highest id178 (subject to constraints that depend on the observed
data) corresponds to making the fewest possible assumptions regarding what
was unobserved, trying to making uncertainty about the model as large as pos-
sible. for example, if we have a dice and want to estimate the id203
of its outcomes, the distribution with the highest id178 would be the uni-
form distribution (each outcome having of id203 a 1/6). now suppose
that we partition the set of possible outcomes in two groups and are only told
about how many times outcomes on each of the groups have occurred. if we
know that outcomes {1, 2, 3} occurred 10 times in total, and {4, 5, 6} occurred
30 times in total, then the principle of maximum id178 would lead us to es-
timate p(1) = p(2) = p(3) = 1/12 and p(1) = p(2) = p(3) = 1/4 (i.e.,
outcomes would be uniform within each of the two groups).

for an introduction of maximum id178 models, along with pointers to
the literature, see http://www.cs.cmu.edu/  aberger/maxent.html. a
fundamental result is that the maximum id178 distribution pw(y|x) under
   rst moment matching constraints (which mean that feature expectations under
m    m ey   pw [ f (xm, y)] must match the observed relative fre-
that distribution 1
m    m f (xm, ym)) is a log-linear model. the dual of that optimization
quencies 1
problem is that of maximizing likelihood in a log-linear model (in the binary
case, called id28 model).

44

the maximum id178 distribution6 has the following parametric form:

pw(y|x) =

exp(w    f (x, y))

z(w, x)

the denominator in eq. 1.27 is called the partition function:

z(w, x) =    
y(cid:48)   y

exp(w    f (x, y(cid:48))).

(1.27)

(1.28)

an important property of the partition function is that the gradient of its loga-
rithm equals the feature expectations:

   w log z(w, x) = ew[ f (x, y)]

=    
y(cid:48)   y

pw(y(cid:48)|x) f (x, y(cid:48)).

(1.29)

maximum id178 models are trained discriminatively: this means that, in-
stead of maximizing the joint likelihood pw(x1, . . . , xm, y1, . . . , ym) (like gener-
ative approaches, such as na    ve bayes, do), one maximizes directly the condi-
tional likelihood pw(y1, . . . , ym|x1, . . . , xm). the rationale is that one does not
need to worry about modeling the input variables if all we want is an accu-
rate estimate of p(y|x), which is what matters for prediction. the average
conditional log-likelihood is:

l(w; d) =

=

=

=

1
m
1
m

1
m

1
m

log pw(y1, . . . , ym|x1, . . . , xm)

log

m   

m=1

pw(ym|xm)

log pw(ym|xm)

(w    f (xm, ym)     log z(w, xm)) .

(1.30)

m   

m=1

m   

m=1

we try to    nd the parameters w that maximize the log-likelihood l(w; d); to
avoid over   tting, we add a id173 term that penalizes values of w that
have a high magnitude. the optimization problem becomes:

  w = arg max

w

= arg min

w

l(w; d)       
2
   l(w; d) +

(cid:107)w(cid:107)2
(cid:107)w(cid:107)2.

  
2

(1.31)

6also called a log-linear model, a boltzmann distribution, or an exponential family of distribu-

tions.

45

here we use the squared l2-norm as the regularizer,7 but other norms are pos-
sible. the scalar        0 controls the amount of id173. unlike the na    ve
bayes examples, this optimization problem does not have a closed form solu-
tion in general; hence we need to resort to numerical optimization (see section
2 (cid:107)w(cid:107)2 be the objective function in eq. 1.31.
??). let f  (w; d) =    l(w; d) +   
this function is convex, which implies that a local optimum of eq. 1.31 is also
a global optimum. f  (w; d) is also differentiable: its gradient is

   wf  (w; d) =

=

1
m

1
m

m   

m=1

m   

m=1

(    f (xm, ym) +    w log z(w, xm)) +   w

(    f (xm, ym) + ew[ f (xm, y)]) +   w.

(1.32)

a batch gradient method to optimize eq. 1.31 is shown in alg. 4. essentially,
alg. 4 iterates through the following updates until convergence:

wt+1     wt       t   wf  (wt; d)
m   
1
m

= (1         t)wt +   t

m=1

( f (xm, ym)     ew[ f (xm, y)]) .

(1.33)

convergence is ensured for suitable stepsizes   t. monotonic decrease of the
objective value can also be ensured if   t is chosen with a suitable line search
method, such as armijo   s rule (nocedal and wright, 1999). in practice, more
sophisticated methods exist for optimizing eq. 1.31, such as conjugate gradient
or l-bfgs. the latter is an example of a quasi-id77, which only
requires gradient information, but use past gradients to try to construct second
order (hessian) approximations.

in large-scale problems (very large m) batch methods are slow, online or
stochastic optimization make attractive alternative methods. stochastic gradi-
ent methods make    noisy    gradient updates by considering only a single in-
stance at the time. the resulting algorithm is shown as alg. 5. at each round
t, an instance m(t) is chosen, either randomly (stochastic variant) or by cycling
through the dataset (online variant). the stepsize sequence must decrease with
t: typically,   t =   0t      for some   0 > 0 and        [1, 2], tuned in a development
partition or with cross-validation.

exercise 1.6 we provide an implementation of the l-bfgs algorithm for training
maximum id178 models in the class maxent batch, as well as an implementation
of the sgd algorithm in the class maxent online.

1. train a maximum id178 model using l-bfgs on the simple data set (try
7in a bayesian perspective, this corresponds to choosing independent gaussian priors p(wd)    

n(0; 1/  2) for each dimension of the weight vector.

46

algorithm 4 batch id119 for maximum id178
1: input: d,   , number of rounds t,
learning rate sequence (  t)t=1,...,t

2: initialize w1 = 0
3: for t = 1 to t do
4:
5:

for m = 1 to m do

take training pair (xm, ym) and compute conditional probabilities us-
ing the current model, for each y(cid:48)     y:

pwt (y(cid:48)|xm) =

exp(wt    f (xm, y(cid:48)))

z(w, x)

6:

7:
8:
9:

compute the feature vector expectation:

ew[ f (xm, y)] =    
y(cid:48)   y

pwt (y(cid:48)|xm) f (xm, y(cid:48))

end for
choose the stepsize   t using, e.g., armijo   s rule
update the model:

wt+1     (1         t)wt +   tm   1

( f (xm, ym)     ew[ f (xm, y)])

m   

m=1

10: end for
11: output:   w     wt+1

different values of   ). compare the results with the previous methods. plot the
decision boundary.

import max_ent_batch as mebc

2

me_lbfgs = mebc.maxent_batch()

4 me_lbfgs.regularizer = 1.0

params_meb_sd = me_lbfgs.train(sd.train_x,sd.train_y)

6 y_pred_train = me_lbfgs.test(sd.train_x,params_meb_sd)

acc_train = me_lbfgs.evaluate(sd.train_y, y_pred_train)

8 y_pred_test = me_lbfgs.test(sd.test_x,params_meb_sd)
acc_test = me_lbfgs.evaluate(sd.test_y, y_pred_test)

10 print "max-ent batch simple dataset accuracy train: %f test

: %f"%(acc_train,acc_test)

12 fig,axis = sd.add_line(fig,axis,params_meb_sd,"max-ent-

batch","orange")

47

algorithm 5 sgd for maximum id178
1: input: d,   , number of rounds t,
learning rate sequence (  t)t=1,...,t

2: initialize w1 = 0
3: for t = 1 to t do
4:
5:

choose m = m(t) randomly
take training pair (xm, ym) and compute conditional probabilities using
the current model, for each y(cid:48)     y:
pwt (y(cid:48)|xm) =

exp(wt    f (xm, y(cid:48)))

z(w, x)

6:

compute the feature vector expectation:

ew[ f (xm, y)] =    
y(cid:48)   y

pwt (y(cid:48)|xm) f (xm, y(cid:48))

7:

update the model:

wt+1     (1         t)wt +   t ( f (xm, ym)     ew[ f (xm, y)])

8: end for
9: output:   w     wt+1

2. train a maximum id178 model using l-bfgs, on the amazon dataset (try
different values of   ) and report training and test set accuracy. what do you
observe?

params_meb_sc = me_lbfgs.train(scr.train_x,scr.train_y)
2 y_pred_train = me_lbfgs.test(scr.train_x,params_meb_sc)

acc_train = me_lbfgs.evaluate(scr.train_y, y_pred_train)

4 y_pred_test = me_lbfgs.test(scr.test_x,params_meb_sc)
acc_test = me_lbfgs.evaluate(scr.test_y, y_pred_test)

6 print "max-ent batch amazon sentiment accuracy train: %f

test: %f"%(acc_train,acc_test)

3. now,    x    = 1.0 and train with sgd (you might try to adjust the initial step).
compare the objective values obtained during training with those obtained with
l-bfgs. what do you observe?

import max_ent_online as meoc

2

me_sgd = meoc.maxent_online()

4 me_sgd.regularizer = 1.0

48

params_meo_sc = me_sgd.train(scr.train_x,scr.train_y)
6 y_pred_train = me_sgd.test(scr.train_x,params_meo_sc)

acc_train = me_sgd.evaluate(scr.train_y, y_pred_train)

8 y_pred_test = me_sgd.test(scr.test_x,params_meo_sc)
acc_test = me_sgd.evaluate(scr.test_y, y_pred_test)

10 print "max-ent online amazon sentiment accuracy train: %f

test: %f"%(acc_train,acc_test)

1.5.2 support vector machines
support vector machines are also a discriminative approach, but they are not a
probabilistic model at all. the basic idea is that, if the goal is to accurately
predict outputs (according to some cost function), we should focus on that
goal in the    rst place, rather than trying to estimate a id203 distribution
(p(y|x) or p(x, y)), which is a more dif   cult problem. as vapnik (1995) puts
it,    do not solve an estimation problem of interest by solving a more general
(harder) problem as an intermediate step.   

we next describe the primal problem associated with multi-class support
vector machines (crammer and singer, 2002), which is of primary interest in
natural language processing. there is a signi   cant amount of literature about
kernel methods (sch  olkopf and smola, 2002; shawe-taylor and cristianini,
2004) mostly focused on the dual formulation. we will not discuss non-linear
kernels or this dual formulation here.8
consider   (y(cid:48), y) as a non-negative cost function. for simplicity, we focus
here on the 0/1-cost de   ned by equation 1.26 (but keep in mind that other cost
functions are possible). the hinge loss9 is the function

(cid:96)(w; x, y) = max
y(cid:48)   y

w    f (x, y(cid:48))     w    f (x, y) +   (y(cid:48), y).

(1.34)

note that the objective of eq. 1.34 becomes zero when y(cid:48) = y. hence, we al-
ways have (cid:96)(w; x, y)     0. moreover, if    is the 0/1 cost, we have (cid:96)(w; x, y) = 0
if and only if the weight vector is such that the model makes a correct predic-
tion with a margin greater than 1: i.e., w    f (x, y)     w    f (x, y(cid:48)) + 1 for all y(cid:48) (cid:54)= y.
otherwise, a positive loss is incurred.

8the main reason why we prefer to discuss the primal formulation with linear kernels is that the
resulting algorithms run in linear time (or less), while known kernel-based methods are quadratic
with respect to m. in large-scale problems (large m) the former are thus more appealing.
9the hinge loss for the 0/1 cost is sometimes de   ned as (cid:96)(w; x, y) = max{0, maxy(cid:48)(cid:54)=y w   
f (x, y(cid:48))     w    f (x, y) + 1}. given our de   nition of   (   y, y), note that the two de   nitons are equiva-
lent.

49

support vector machines (id166) tackle the following optimization problem:

  w = arg min

w

m   

m=1

(cid:96)(w; xm, ym) +

(cid:107)w(cid:107)2,

  
2

(1.35)

where we also use the squared l2-norm as the regularizer. for the 0/1-cost,
the problem in eq. 1.35 is equivalent to:

arg min

w,  

2 (cid:107)w(cid:107)2
s.t. w    f (xm, ym)     w    f (xm,   ym) + 1       m,

m=1   m +   

   m

(1.36)
   m,   ym     y \ {ym}.(1.37)

geometrically, we are trying to choose the linear classi   er that yields the largest
possible separation margin, while we allow some violations, penalizing the
amount of slack via extra variables   1, . . . ,   m.

problem 1.35 does not have a closed form solution. moreover, unlike max-
imum id178 models, here the objective function is non-differentiable, hence
smooth optimization is not possible. however, it is still convex, which ensures
that any local optimum is the global optimum. despite not being differen-
tiable, we can still de   ne a subgradient of the objective function (which gener-
alizes the concept of gradient), which enables us to apply subgradient-based
methods. a stochastic subgradient algorithm for solving eq. 1.35 is illustrated
as alg. 6. the similarity with maximum id178 models (alg. 5) is striking:
the only difference is that, instead of computing the feature vector expectation
using the current model, we compute the feature vector associated with the
cost-augmented prediction using the current model.

a variant of this algorithm was proposed by shalev-shwartz et al. (2007)
under the name pegasos, with excellent properties in large-scale settings. other
algorithms and software packages for training id166s that have become popu-
lar are id166light (http://id166light.joachims.org) and libid166 (http:
//www.csie.ntu.edu.tw/  cjlin/libid166/), which allow non-linear ker-
nels. these will generally be more suitable for smaller datasets, where high
accuracy optimization can be obtained without much computational effort.

remark 1.3 note the similarity between the stochastic (sub-)gradient algorithms (algs. 5   
6) and the online algorithms seen above (id88 and mira).

exercise 1.7 implement the id166 primal algorithm (hint: look at the models imple-
mented earlier, you should only need to change a few lines of code). do this by creating
a    le id166.py and implement class id166. then, repeat the maxent exercise now using
id166s, for several values of   :

import id166 as id166c

2

id166 = id166c.id166()

50

algorithm 6 stochastic subid119 for id166s
1: input: d,   , number of rounds t,
learning rate sequence (  t)t=1,...,t

2: initialize w1 = 0
3: for t = 1 to t do
4:
5:

choose m = m(t) randomly
take training pair (xm, ym) and compute the    cost-augmented predic-
tion    under the current model:

  y = arg max

y(cid:48)   y

wt    f (xm, y(cid:48))     wt    f (xm, ym) +   (y(cid:48), y)

6:

update the model:

wt+1     (1         t)wt +   t ( f (xm, ym)     f (xm,   y))

7: end for
8: output:   w     wt+1

4 id166.regularizer = 1.0 # this is lambda

params_id166_sd = id166.train(sd.train_x,sd.train_y)

6 y_pred_train = id166.test(sd.train_x,params_id166_sd)

acc_train = id166.evaluate(sd.train_y, y_pred_train)

8 y_pred_test = id166.test(sd.test_x,params_id166_sd)
acc_test = id166.evaluate(sd.test_y, y_pred_test)

10 print "id166 online simple dataset accuracy train: %f test: %f"%(

acc_train,acc_test)

12 fig,axis = sd.add_line(fig,axis,params_id166_sd,"id166","orange")

14 params_id166_sc = id166.train(scr.train_x,scr.train_y)
y_pred_train = id166.test(scr.train_x,params_id166_sc)

16 acc_train = id166.evaluate(scr.train_y, y_pred_train)

y_pred_test = id166.test(scr.test_x,params_id166_sc)
18 acc_test = id166.evaluate(scr.test_y, y_pred_test)

print "id166 online amazon sentiment accuracy train: %f test: %f"

%(acc_train,acc_test)

compare the results achieved and separating hiperplanes found.

1.6 comparison

table 1.6 provides a high-level comparison among the different models dis-
cussed in this chapter.

51

naive bayes

id88

mira maxent

id166s

generative/discriminative
performance if true model
not in the hipothesis class
performance if features overlap
training
hyperparameters to tune

g
bad

fair

closed form
1 (smoothing)

d

fair (may

not converge)

good
easy

0

d

good

good
easy

1

d

good

good
fair

1

d

good

good
fair

1

table 1.1: comparison among different models.

exercise 1.8

    using the simple data set run the different models varying some
characteristics of the data, number of points, variance (hence separability), class
balance. use function xx which receives a dataset and plots all decisions bound-
aries and accuracies. what can you say about the methods when the amount of
data increases? what about when the classes become too unbalanced.

1.7 final remarks

some implementations of the discussed algorithms are available on the web:

    id166light: http://id166light.joachims.org

    libid166: http://www.csie.ntu.edu.tw/  cjlin/libid166/

    maximum id178: http://homepages.inf.ed.ac.uk/lzhang10/

maxent_toolkit.html

    mallet: http://mallet.cs.umass.edu/.

52

