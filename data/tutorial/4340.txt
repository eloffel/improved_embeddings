day 3

learning structured
predictors

in this class, we will continue to focus on sequence classi   cation, but instead of
following a generative approach (like in the previous chapter) we move towards
discriminative approaches.

table 3.1 shows how the models for classi   cation have counterparts in se-
quential classi   cation.
in fact, in the last chapter we discussed the hidden
markov model, which can be seen as a generalization of the na    ve bayes model
for sequences. in this chapter, we will see a generalization of the id88
algorithm for sequence problems (yielding the structured id88, collins
2002) and a generalization of maximum id178 model for sequences (yielding
id49, lafferty et al. 2001). note that both these gener-
alizations are not speci   c for sequences and can be applied to a wide range of
models (we will see in tomorrow   s lecture how these methods can be applied
to parsing). although we will not cover all the methods described in chap-
ter 1, bear in mind that all of those have a structured counterpart. it should
be intuitive after this lecture how those methods could be extended to struc-
tured problems, given the id88 example. before we explain the partic-
ular methods, the next section will talk a bit about feature representation for

classi   cation

na    ve bayes 1.2

sequences

generative

id48 2.1

discriminative

id88 1.4.1
maximum id178 1.5.1 id49 3.3

structured id88 3.2

table 3.1: summary of the methods that we will be covering this lecture.

74

condition
yi = l & t = 0
yi = l & yi   1 = m
yi = l & yi   1 = m & t = n final transition features
  xi = a & yi = l

name
initial state
transition features

observation features

table 3.2:
id48 model.

idfeatures feature set. this set replicates the features used by the

sequences.

= p  (   y|  x) =       f (  x,   y)

arg max

  y

(3.1)

as in the previous section,   y is a sequence so the maximization is over an
exponential number of objects, making it intractable. again we will assume a
   rst order markov independence assumption, and so the features will decom-
pose as the model. so equation 3.1 can be written as:

arg max

  y

=    
n

   
  y

      f (n, yn,   xn) +    

n

   
yn   y

      f (n, yn, yn   1,   xn)

(3.2)

3.1

feature extraction

in this section we will de   ne two simple feature sets. the    rst one will only use
identity features, and will mimic the features used by the id48 model from
the previous section. this will allow to directly compare the performance of a
generative vs a discriminative approach. note that although not required, all
the features we will use in this section are binary features (0-1), indicating the
presence or absence of a given condition.

example 3.1 simple id feature set containing the same features as an id48 model.

0 ms./noun nf: id:ms.::noun init_tag:noun

2 1 haag/noun nf: id:haag::noun ef: prev_tag:noun::noun

2 plays/verb nf: id:plays::verb ef: prev_tag:noun::verb

4 3 elianti/noun nf: id:elianti::noun ef: prev_tag:verb::noun

4 ./. nf: id:.::. ef: last_prev_tag:noun::.

table 3.2 depicts the features that are implicit in the id48, which was the
subject of the previous chapter. these features are indicators of initial,    nal,
observation and transition events. the fact that we were using a generative

75

condition
yi = l & t = 0
yi = l & yi   1 = m
yi = l & yi   1 = m & t = n
  xi = a & yi = l
  xi = a & a is uppercased & yi = l
  xi = a & a contains digit & yi = l
  xi = a & a contains hyphen & yi = l
  xi = a & a[0..i]   i     [1, 2, 3] yi = l
  xi = a & a[n   i..n]   i     [1, 2, 3] & yi = l

name
initial state
transition features
final transition features
observation features
uppercase features
digit features
hyphen features
pre   x features
suf   x features

table 3.3: extended feature set. some features in this set could not be included
in the id48 model.

model has forced us (in some sense) to make strong independence assump-
tions. however, since we now move to a discriminative approach, where we
model p(   y|  x) rather than p(  x,   y), we are not tied anymore to some of these
assumptions. in particular:

    we may use    overlapping    features, e.g., features that    re simultaneously
for many instances. for example, we can use a feature for a word and
another for pre   xes and suf   xes of that word. this would lead to an
awkward model if we wanted to insist on a generative approach.

    we may use features that depend arbitrarily on the entire input sequence
  x. on the other hand, we still need to resort to    local    features with re-
spect to the outputs (e.g. looking only at consecutive state pairs), other-
wise decoding algorithms will become more expensive.

table 3.3 shows examples of features that are traditionally used in id52
with discriminative models. of course, we could have much more complex
features, looking arbitrarily to the input sequence. we are not going to have
them in this exercise only for performance reasons (to have less features and
smaller caches).

example 3.2

1 0 ms./noun nf: id:ms.::noun uppercased::noun suffix:.::noun

suffix:s.::noun prefix:m::noun prefix:ms::noun init_tag:noun

1 haag/noun nf: id:haag::noun uppercased::noun suffix:g::noun

suffix:ag::noun suffix:aag::noun prefix:h::noun prefix:ha::
noun prefix:haa::noun rare::noun ef: prev_tag:noun::noun

3 2 plays/verb nf: id:plays::verb ef: prev_tag:noun::verb

3 elianti/noun nf: id:elianti::noun uppercased::noun suffix:i::
noun suffix:ti::noun suffix:nti::noun prefix:e::noun prefix:
el::noun prefix:eli::noun rare::noun ef: prev_tag:verb::noun

5 4 ./. nf: id:.::. ef: last_prev_tag:noun::.

76

we consider two kinds of features: node features, which form a vector f n(  x, yi),

and edge features, which form a vector f e(  x, yi, yi   1).1 these feature vectors will
receive parameter vectors   n and   e. similarly as in the previous chapter, we
consider:

    node potentials. these are scores for a state at a particular position. they

are given by

  v(  x, yi) = exp(  v    f v(  x, yi)).

(3.3)

    edge potentials. these are scores for the transitions. they are given by

  e(  x, yi, yi   1) = exp(  e    f e(  x, yi, yi   1)).

(3.4)
let    = (  n,   e). the id155 p(   y|  x) is then de   ned as

(cid:32)

   
i

follows:

p(   y|  x) =

=

1

z(  ,   x)

1

z(  , x)

where

exp

   
i

  v    f v(  xi, yi) +    

  e    f e(  xi, yi, yi   1)

i
  v(  xi, yi)  e(  xi, yi, yi   1),

(cid:33)

(3.5)

(3.6)

(3.7)

z(  , x) =    
y   y

   
i

  v(  xi, yi)  e(  xi, yi, yi   1)

is the partition function.

there are three important problems that need to be solved:

1. given   x, computing the most likely output sequence   y (the one which

maximizes p(   y|  x).

2. compute the posterior marginals p(yi|  x) at each position i.
3. compute the partition function.

interestingly, all these problems can be solved by using the same algorithms
(just changing the potentials) that were already implemented for id48s: the
viterbi algorithm (for 1) and the forward-backward algorithm (for 2   3).

1to make things simpler, we will assume later on that edge features do not depend on the input

  x   but they could, without changing at all the decoding algorithm.

77

algorithm 10 averaged structured id88
1: input: dataset d, number of rounds t
2: initialize w1 = 0
3: for t = 1 to t do
4:
5:

choose m = m(t) randomly
take training pair (  xm,   ym) and predict using the current model:

    y     arg max

  y(cid:48)

wt    f (  xm,   y(cid:48))

update the model: wt+1     wt + f (  xm,   ym)     f (  xm,     y)

6:
7: end for
8: output: the averaged model   w     1

t    t

t=1 wt

3.2 structured id88

the structured id88 (collins, 2002), namely its averaged version is a very
simple algorithm that relies on viterbi decoding and very simple additive up-
dates. in practice this algorithm is very easy to implement and behaves re-
markably well in a variety of problems. these two characteristics make the
structured id88 algorithm a natural    rst choice to try and test a new
problem or a new feature set.

recall what you learned from   1.4.1 on the id88 algorithm and com-

pare it against the structured id88 (algorithm 10).

there are only two differences:

    instead of    nding arg maxy(cid:48)   y for a given variable, it    nds the arg max  y,
the best sequence. we can do this by using the viterbi algorithm with the
node and edge potentials (actually, the log of those potentials) de   ned in
eqs. 3.3   3.4, along with the assumption that the features decompose as
the model, as explained in the previous section.

    instead of updating the features for the entire y(cid:48) (in this case   y) we update

the features only at the positions were the labels are different.

exercise 3.1 in this exercise you will test the structured id88 algorithm using
different feature sets for part-of-speech tagging.

start by loading the corpus and creating an idfeature class. next initialize the

id88 and train the algorithm.

1 import sys

sys.path.append("readers/" )

3 sys.path.append("sequences/" )

5 import pos_corpus as pcc

78

import id_feature as idfc

import structured_id88 as spc

7

9

11 posc = pcc.postagcorpus("en",max_sent_len=15,train_sents=1000,

dev_sents=200,test_sents=200)

id_f = idfc.idfeatures(posc)

13 id_f.build_features()

sp = spc.structuredpercetron(posc,id_f)

15 sp.nr_rounds = 20

sp.train_supervised(posc.train.seq_list)

17

epoch: 0 accuracy: 0.617797
19 epoch: 1 accuracy: 0.797775
epoch: 2 accuracy: 0.864115
21 epoch: 3 accuracy: 0.901794
epoch: 4 accuracy: 0.925644
23 epoch: 5 accuracy: 0.932659
epoch: 6 accuracy: 0.938872
25 epoch: 7 accuracy: 0.946087
epoch: 8 accuracy: 0.949193
27 epoch: 9 accuracy: 0.950696

epoch: 10 accuracy: 0.952701
29 epoch: 11 accuracy: 0.952600
epoch: 12 accuracy: 0.956910
31 epoch: 13 accuracy: 0.956108
epoch: 14 accuracy: 0.956408
33 epoch: 15 accuracy: 0.958413
epoch: 16 accuracy: 0.957110
35 epoch: 17 accuracy: 0.959014
epoch: 18 accuracy: 0.959315
37 epoch: 19 accuracy: 0.960216

now evaluate the learned model on both the development and test set.

1 pred_train = sp.viterbi_decode_corpus(posc.train.seq_list)

pred_dev = sp.viterbi_decode_corpus(posc.dev.seq_list)

3 pred_test = sp.viterbi_decode_corpus(posc.test.seq_list)

eval_train = sp.evaluate_corpus(posc.train.seq_list,pred_train)

5 eval_dev = sp.evaluate_corpus(posc.dev.seq_list,pred_dev)

eval_test = sp.evaluate_corpus(posc.test.seq_list,pred_test)

7 print "structured percetron - id features accuracy train: %.3f

dev: %.3f test: %.3f"%(eval_train,eval_dev,eval_test)

9 out[]: structured percetron - id features accuracy train: 0.867

dev: 0.831 test: 0.790

79

compare with the results achieved with the id48 model.

1 best smoothing 1.000000 --

test set accuracy: posterior decode

0.809, viterbi decode: 0.777

even using a similar feature set the id88 yields better results. per-
form some error analysis and    gure out what are the main errors the percep-
tron is making. compare them with the errors made by the id48 model.
(hint: use the methods developed in the previous lecture to help you with
the error analysis).

exercise 3.2 repeat the previous exercise using the extended feature set. compare the
results.

1 import extended_feature as exfc

3 ex_f = exfc.extendedfeatures(posc)

ex_f.build_features()

5 sp = spc.structuredpercetron(posc,ex_f)

sp.nr_rounds = 20

7 sp.train_supervised(posc.train.seq_list)

9 epoch: 0 accuracy: 0.638741
epoch: 1 accuracy: 0.807596
11 epoch: 2 accuracy: 0.876541
epoch: 3 accuracy: 0.907406
13 epoch: 4 accuracy: 0.921836
epoch: 5 accuracy: 0.939974
15 epoch: 6 accuracy: 0.940575
epoch: 7 accuracy: 0.948893
17 epoch: 8 accuracy: 0.948893
epoch: 9 accuracy: 0.950095

19 epoch: 10 accuracy: 0.954404
epoch: 11 accuracy: 0.957110
21 epoch: 12 accuracy: 0.954605
epoch: 13 accuracy: 0.956910
23 epoch: 14 accuracy: 0.956509
epoch: 15 accuracy: 0.956609
25 epoch: 16 accuracy: 0.958012
epoch: 17 accuracy: 0.959014
27 epoch: 18 accuracy: 0.957411
epoch: 19 accuracy: 0.958413

29

pred_train = sp.viterbi_decode_corpus(posc.train.seq_list)

31 pred_dev = sp.viterbi_decode_corpus(posc.dev.seq_list)

pred_test = sp.viterbi_decode_corpus(posc.test.seq_list)

33

80

eval_train = sp.evaluate_corpus(posc.train.seq_list,pred_train)

35 eval_dev = sp.evaluate_corpus(posc.dev.seq_list,pred_dev)

eval_test = sp.evaluate_corpus(posc.test.seq_list,pred_test)

37

39

print "structured percetron - extended features accuracy train:

%.3f

dev: %.3f test:

%.3f"%(eval_train,eval_dev,

eval_test)

structured percetron - extended features accuracy train: 0.946

dev: 0.868 test:

0.840

compare the errors obtained with the two different feature sets. perform some
feature analysis, what errors were correct by using more features? can you think of
other features to use to solve the errors found?

3.3 id49

id49 (crf) (lafferty et al., 2001) can be seen as an ex-
tension of the maximum id178 (me) models to structured problems.2

crfs are globally normalized models: the id203 of a given sentence is
given by equation 3.5. going from a maximum id178 model (in multi-class
classi   cation) to a crf mimics the transition discussed above from id88
to structured id88:

    instead of    nding the posterior marginal p(y(cid:48)|x) for a given variable, it
   nds the posterior marginals for all factors (nodes and edges), p(   yi|  x)
and p(   yi,   yi   1|  x). we can compute this quantities by using the forward-
backward algorithm with the node and edge potentials de   ned in eqs. 3.3   
3.4, along with the assumption that the features decompose as the model,
as explained in the previous section.

    the features are updated factor wise (i.e., for each node and edge).
algorithm 11 shows the pseudo code to optimize a crf with a batch gra-
dient method (in the exercise, we will use a quasi-id77, l-bfgs).
again, we can also take an online approach to optimization, but here we will
stick with the batch one.

exercise 3.3 repeat exercises 3.1   3.2 using a crf model instead of the id88
algorithm. report the results.

here is the code for the simple feature set:

2an earlier, less successful, attempt to perform such an extension was via maximum id178
markov models (memm) (mccallum et al., 2000). there each factor (a node or edge) is a locally
normalized maximum id178 model. a shortcoming of memms is its so-called labeling bias (bot-
tou, 1991), which makes them biased towards states with few successor states (see lafferty et al.
(2001) for more information).

81

algorithm 11 batch id119 for id49
1: input: d,   , number of rounds t, learning rate sequence (  t)t=1,...,t
2: initialize   1 = 0
3: for t = 1 to t do
4:
5:

take training pair (xm, ym) and compute conditional probabilities us-
ing the current model, for each   y:

for m = 1 to m do

(cid:32)

(cid:33)

p  t (   y|  x) =

1

z(  t,   x)

exp

   
i

v    f v(  x, yi) +    
  t

i

e    f e(  x, yi, yi   1)
  t

6:

7:
8:
9:

compute the feature vector expectation:

e  t [ f (  xm,   ym)] =    

  y

p  t (   ym|  xm) f (  xm,   ym)

end for
choose the stepsize   t using, e.g., armijo   s rule
update the model:

  t+1     (1         t)  t +   tm   1

m   

m=1

10: end for
11: output:            t+1

(cid:0) f (  xm,   ym)     e  t [ f (  xm,   ym)](cid:1)

1 import crf_batch as crfc

posc = pcc.postagcorpus("en",max_sent_len=15,train_sents=1000,

dev_sents=200,test_sents=200)

3 id_f = idfc.idfeatures(posc)

id_f.build_features()

5

7 crf = crfc.crf_batch(posc,id_f)

crf.train_supervised(posc.train.seq_list)

9

pred_train = crf.viterbi_decode_corpus(posc.train.seq_list)

11 pred_dev = crf.viterbi_decode_corpus(posc.dev.seq_list)

pred_test = crf.viterbi_decode_corpus(posc.test.seq_list)

13

eval_train = crf.evaluate_corpus(posc.train.seq_list,pred_train

)

15 eval_dev = crf.evaluate_corpus(posc.dev.seq_list,pred_dev)

eval_test = crf.evaluate_corpus(posc.test.seq_list,pred_test)

17

82

print "crf - id features accuracy train: %.3f dev: %.3f test:

%.3f"%(eval_train,eval_dev,eval_test)

19 out[]: crf - id features accuracy train: 0.920 dev: 0.863 test:

0.830

here is the code for the extended feature set:

1

posc = pcc.postagcorpus("en",max_sent_len=15,train_sents=1000,

dev_sents=200,test_sents=200)

3 ex_f = exfc.extendedfeatures(posc)

ex_f.build_features()

5

7 crf = crfc.crf_batch(posc,ex_f)

crf.train_supervised(posc.train.seq_list)

9

pred_train = crf.viterbi_decode_corpus(posc.train.seq_list)

11 pred_dev = crf.viterbi_decode_corpus(posc.dev.seq_list)

pred_test = crf.viterbi_decode_corpus(posc.test.seq_list)

13

eval_train = crf.evaluate_corpus(posc.train.seq_list,pred_train

)

15 eval_dev = crf.evaluate_corpus(posc.dev.seq_list,pred_dev)

eval_test = crf.evaluate_corpus(posc.test.seq_list,pred_test)

17

19

print "crf - extended features accuracy train: %.3f dev: %.3f

test: %.3f"%(eval_train,eval_dev,eval_test)

out[]: crf - extended features accuracy train: 0.924 dev: 0.872

test: 0.831

83

