day 4

syntax and parsing

in this lab we will implement some exercises related with parsing.

4.1 phrase-based parsing

4.1.1 id18s
let t be an alphabet (i.e., a    nite set of symbols), and denote by t    its kleene
closure, i.e., the in   nite set of strings produced with those symbols:

t    =         t     t2     . . .

a language l is a subset of t   . the    complexity    of a language l can be accessed
in terms of how hard it is to construct a machine (an automaton) capable of
distinguishing the words in l from the elements of t    which are not in l.1 if l
is    nite, a very simple automaton can be built which just memorizes the strings
in l. the next simplest case is that of regular languages, which are recognizable
by    nite state machines. these are the languages that can be expressed by regular
expressions. an example (where t = {a, b}) is the language l = {abnaan | n    
n}, which corresponds to the regular expression ab    a+. id48
(studied in previous lectures) can be seen as a stochastic version of    nite state
machines.

a step higher in the hierarchy of languages leads to context-free languages,
which are more complex than regular languages. these are languages that are
generated by context-free grammars, and recognizable by push-down automata
(which are slightly more complex than    nite state machines). in this section
we describe context-free grammars and how they can be made probabilistic.

1we recommend the classic book by hopcroft et al. (1979) for a thorough introduction on the

subject of automata theory and formal languages.

84

this will yield models that are more powerful than id48,
and are specially amenable for modeling the syntax of natural languages.2

a context-free grammar (id18) is a tuple g = (cid:104)n, t, r, s(cid:105) where:

1. n is a    nite set of non-terminal symbols. elements of n are denoted by
upper case letters (a, b, c, . . .). each non-terminal symbol is a syntactic
category: it represents a different type of phrase or clause in the sentence.

2. t is a    nite set of terminal symbols (disjoint from n). elements of t are de-
noted by lower case letters (a, b, c, . . .). each terminal symbol is a surface
word: terminal symbols make up the actual content of sentences. the set
t is called the alphabet of the language de   ned by the grammar g.

3. r is a set of production rules, i.e., a    nite relation from n to (n     t)   . g
is said to be in chomsky normal form (cnf) if production rules in r are
either of the form a     bc or a     a.

4. s is a start symbol, used to represent the whole sentence. it must be an

element of n.

any id18 can be transformed to be in cnf without loosing any expressive
power in terms of the language it generates. hence, we henceforth assume that
g is in cnf without loss of generality.

to see how id18s can model the syntax of natural languages, consider the

following sentence,

she enjoys the summer school.

along with a grammar (in cnf) with the following production rules:

s --> np vp
np --> det n
np --> she
vp --> v np
v --> enjoys
det --> the
nbar --> n n
n --> summer
n --> school

2this does not mean that natural languages are context free. there is an immense body of
work on grammar formalisms that relax the    context-free    assumption, and those formalisms
have been endowed with a probabilistic framework as well. examples are: lexical functional gram-
mars, head-driven phrase structured grammars, combinatorial categorial grammars, tree adjoining
grammars, etc. some of these formalisms are mildly context sensitive, a relaxation of the    context-
free    assumption which still allows polynomial parsing algorithms. there is also equivalence in
expressive power among several of these formalisms.

85

with this grammar, we may derive the following parse tree:

np

she

s

v

enjoys

vp

det

the

np

nbar

n

n

summer

school

4.1.2 ambiguity
a fundamental characteristic of natural languages is ambiguity. for example,
consider the sentence

the man sees the boy in the park with a telescope.

for which all the following parse trees are plausible interpretations:

1. the boy is in a park and he has a telescope:

np

det

n

the

man

s

v

sees

vp

np

np

pp

np

pp

p

np

det

n

the

boy

p

in

np

with

det

n

det

n

the

park

a

telescope

2. the boy is in a park, and the man sees him using a telescope as an instru-

ment:

86

s

np

det

n

the

man

v

sees

vp

np

pp

np

pp

p

np

det

n

the

boy

p

in

np

with

det

n

det

n

the

park

a

telescope

3. the man is in the park and he has a telescope, through which he sees a

boy somewhere:

s

np

det

n

the

man

vp

v

sees

np

det

n

the

boy

pp

pp

p

in

np

p

np

det

n

the

park

with

det

n

a

telescope

the ambiguity is caused by the several places to each the prepositional
phrase could be attached. this kind of syntactical ambiguity (pp-attachment)
is one of the most frequent in natural language.

4.1.3 id140
a probabilistic context-free grammar is a tuple g   = (cid:104)n, t, r, s,   (cid:105), where (cid:104)n, t, r, s(cid:105)
is a id18 and    is a vector of parameters, one per each production rule in r. as-

87

suming that the grammar is in cnf, each rule of the kind z     xy is endowed
a id155

  z   xy = p  (xy|z),

and each unary rule of the kind z     w is endowed with a conditional proba-
bility

  z   w = p  (w|z).

for these conditional probabilities to be well de   ned, the entries of    must be
non-negative and need to normalize properly for each z     n:

   
x,y   n

  z   xy +    
w   t

  z   w = 1.

let s be a string and t a parse tree derivation for s. for each r     r, let nr(t, s)
be the number of times production rule r appears in the derivation. according
to this generative model, the joint id203 of t and s factors as the product
of the conditional probabilities above:

p(t, s) =    
r   r

nr(t,s)
r

.

  

for example, for the sentence above (she enjoys the summer school) this proba-
bility would be

p(t, s) = p(np vp|s)    p(she|np)    p(v np|vp)    p(enjoys|v)

  p(det nbar|np)    p(the|det)    p(n n|nbar)
  p(summer|n)    p(school|n).

(4.1)

when a sentence is ambiguous, the most likely parse tree can be obtained by
maximizing the id155 p(t|s); this quantity is proportional to
p(t, s) and therefore the latter quantity can be maximized. the number of pos-
sible parse trees, however, grows exponentially with the sentence length, ren-
dering a direct maximization intractable. fortunately, a generalization of the
viterbi algorithm exists which uses id145 to carry out this
computation. this is the subject of the next section.

4.1.4 the cky parsing algorithm
one of the most widely algorithm for parsing natural language sentences is
the cocke-kasami-younger (cky) algorithm. given a grammar in cnf with
|r| production rules, its runtime complexity for parsing a sentence of length
n is o(n3|r|). we present here a simple extension of the cky algorithm that

88

  (i, i, z) =   z   wi

for each production rule r     r of the form z     wi do

algorithm 12 cky algorithm
1: input: probabilistic id18 g   in cnf, sentence s = w1 . . . wn
2:
3: {initialization}
4: for i = 1 to n do
5:
6:
7:
8: end for
9:
10: {induction}
11: for i = 2 to n do {i is length of span}
12:
for each non-terminal z     n do
13:
14:

for j = 1 to n     i + 1 do {j is start of span}

end for

set partial id203:
  (j, j + i     1, z) = max
x,y
j<k<j+i

  (j, k     1, x)      (k, j + i     2, y)      z   xy

15:

store backpointer:
  (j, j + i     1, z) = arg max
j<k<j+i

x,y

  (j, k     1, x)      (k, j + i     2, y)      z   xy

end for

end for

16:
17:
18: end for
19:
20: {termination}
21: p(s,   t) =   (1, n, s)
22: backtrack through    to obtain most likely parse tree   t

obtains the most likely parse tree of a sentence, along with its id203.3 this
is displayed in alg. 12.

exercise 4.1 in this simple exercise, you will see the cky algorithm in action. there
is a javascript applet that illustrates how cky works (in its non-probabilistic form). go
to http://www.diotavelli.net/people/void/demos/cky.html, and
observe carefully the several steps taken by the algorithm. write down a small grammar
in cnf that yields multiple parses for the ambiguous sentence the man saw the boy
in the park with a telescope, and run the demo for this particular sentence. what

3similarly, the forward-backward algorithm for computing posterior marginals in sequence
models can be extended for context-free parsing. it takes the name inside-outside algorithm. see
manning and sch   utze (1999) for details.

89

would happen in the probabilistic form of cky?

4.1.5 learning the grammar
there is an immense body of work on grammar induction using probabilis-
tic models (see e.g., manning and sch   utze (1999) and the references therein,
as well as the most recent works of klein and manning (2002); smith and
eisner (2005b); cohen et al. (2008)): this is the problem of learning the pa-
rameters of a grammar from plain sentences only. this can be done in an
em fashion (like in sequence models), except that the forward-backward al-
gorithm is replaced by inside-outside. unfortunately, the performance of un-
supervised parsers is far from good, at present days. much better results have
been produced by supervised systems, which, however, require expensive an-
notation in the form of treebanks: this is a corpus of sentences annotated with
their corresponding syntactic trees. the following is an example of an anno-
tated sentence in one of the most widely used treebanks, the id32
(http://www.cis.upenn.edu/  treebank/):

( (s

(np-sbj (nnp bell) (nnp industries) (nnp inc.) )
(vp (vbd increased)

(np (prp$ its) (nn quarterly) )
(pp-dir (to to)

(np (cd 10) (nns cents) ))

(pp-dir (in from)

(np

(np (cd seven) (nns cents) )
(np-adv (dt a) (nn share) ))))

(. .) ))

exercise 4.2 this exercise will show you that real-world sentences can have compli-
cated syntactic structures. there is a parse tree visualizer in http://www.ark.
cs.cmu.edu/treeviz/. go to your local data/treebanks folder and open
the    le ptb excerpt.txt. copy a few sentences from there, and examine their
parse trees in the visualizer.

a treebank makes possible to learn a parser in a supervised fashion. the
simplest way is via a generative approach. instead of counting transition and
observation events of an id48 (as we did for sequence models), we now need
to count production rules and symbol occurrences to estimate the parameters
of a probabilistic context-free grammar. while performance would be much
better than that of unsupervised parsers, it would still be rather poor. the
reason is that the model we have described so far is oversimplistic: it makes too
strong independence assumptions. in this case, the markovian assumptions
are:

90

1. each terminal symbol w in some position i is independent of everything
else given that it was derived by the rule z     w (i.e., given its parent z);
2. each pair of non-terminal symbols x and y spanning positions i to j,
with split point k, is independent of everything else given that they were
derived by the rule z     xy (i.e., given their parent z).

the next section describes some model re   nements that complicate the
problem of parameter estimation, but usually allow for a dramatic improve-
ment on the quality of the parser.

4.1.6 model re   nements
a number of re   nements has been made that yield more accurate parsers. we
mention just a few:

parent annotation. this strategy splits each non-terminal symbol in the gram-
mar (e.g. z) by annotating it with all its possible parents (e.g. creates
nodes zx, zy, . . . every time production rules like x     z  , x       z,
y     z  , or y       z exist in the original grammar). this increases the
vertical markovian length of the model, hence weakening the indepen-
dence assumptions. parent annotation was initiated by johnson (1998)
and carried on in the unlexicalized parsers of klein and manning (2003)
and follow-up works.

lexicalization. a particular weakness of pid18s is that they ignore word con-
text for interior nodes in the parse tree. yet, this context is relevant in
determining the production rules that should be invoked in the deriva-
tion. a way of overcoming this limitation is by lexicalizing parse trees,
i.e., annotating each phrase node with the lexical item (word) which gov-
this is called the head word of the phrase. fig. 4.1
erns that phrase:
shows an example of a lexicalized parse tree. to account for lexicaliza-
tion, each non-terminal symbol in the grammar (e.g. z) is split into many
symbols, each annotated with a word that may govern that phrase (e.g.
zw1, zw2, . . .). this greatly increases the size of the grammar, but it has a
signi   cant impact in performance. a string of work involving lexicalized
pid18s includes magerman (1995); charniak (1997); collins (1999).

discriminative models. similarly as in sequence models (where it was shown
how to move from an id48 to a crf), we may abandon our genera-
tive model and consider a discriminative one. an advantage of doing
that is that it becomes much easier to adopt non-local input features (i.e.,
features that depend arbitrarily on the surface string), for example the
kind of features obtained via lexicalization, and much more. the cky
parsing algorithm can still be used for decoding, provided the feature
vector decompose according to non-terminal symbols and production rules.

91

np (she)

she

s (enjoys)

v (enjoys)

enjoys

vp (enjoys)

np (school)

det (the)

nbar (school)

the

n (summer)

n (school)

summer

school

figure 4.1: a lexicalized parse tree for the sentence she enjoys the summer school.

in this case, productions and non-terminals will have a score which does
not correspond to a log-id203; the partition function and the poste-
rior marginals can be computed with the inside-outside algorithm. see
taskar et al. (2004) for an application of structured id166s to parsing, and
finkel et al. (2008) for an application of crfs.

latent variables. splitting the variables in the grammar by introducing latent
variables appears as an alternative to lexicalization and parent annota-
tion. there is a string of work concerning latent variable grammars, ei-
ther for the generative and discriminative cases: petrov and klein (2007,
2008a,b). some related work also considers coarse-to-   ne parsing, which
iteratively applies more and more re   ned models: charniak et al. (2006).

history-based parsers. finally, there is a totally different line of work which
models parsers as a sequence of greedy shift-reduce decisions made by a
push-down automaton (ratnaparkhi, 1999; henderson, 2003). when dis-
criminative models are used, arbitrary conditioning can be done on past
decisions made by the automaton, allowing to include features that are
dif   cult to handle by the other parsers. this comes at the price of greed-
iness in the decisions taken, which implies suboptimality in maximizing
the desired objective function.

92

figure 4.2: a dependency tree for the sentence she enjoys the summer school.
note the additional dummy root symbol (*) which is included for convenience.

4.2 id33

4.2.1 motivation
consider again the sentence

she enjoys the summer school.

along with the lexicalized parse tree displayed in fig. 4.1. if we drop the phrase
constituents and keep only the head words, the parse tree would become:

enjoys

she

school

the

summer

this representation is called a dependency tree; it can be alternatively repre-
sented as shown in fig. 4.2. dependency trees retain the lexical relationships
involved in lexicalized phrase-based parse trees. however, they drop phrasal
constituents, which render non-terminal nodes unnecessary. this has compu-
tational advantages (no grammar constant is involved in the complexity of the
parsing algorithms) as well as design advantages (no grammar is necessary,
and treebank annotations are way simpler, since no internal constituents need
to be annotated). it also shifts the focus from internal syntactic structures and
generative grammars (chomsky, 1965) to lexical and transformational gram-
mars (tesni`ere, 1959; hudson, 1984; mel  cuk, 1988; covington, 1990). arcs con-
necting words are called dependency links or dependency arcs. in a arc (cid:104)h, m(cid:105), the
source word h is called the head and the target word m is called the modi   er.

4.2.2 projective and non-projective parsing
dependency trees constructed using the method just described (i.e., lexicaliza-
tion of context-free phrase-based trees) always satisfy the following properties:

1. each word (excluding the dummy root symbol) has exactly one parent.

2. the dummy root symbol has no parents.

3. there are no cycles.

93

 * sheenjoysthesummerschoolfigure 4.3: a non-projective parse tree.

4. the dummy root symbol has exactly one child.
5. all arcs are projective. this means that for any arc (cid:104)h, m(cid:105), all words in its
span (i.e., all words lying between h and m) are descendents from h (i.e.
there is a directed path of dependency links from h to such word).

conditions 1   3 ensure that the set of dependency links form a well-formed
tree, rooted in the dummy symbol, which spans all the words of the sentence.
condition 4 requires that there is a single link departing from the root. finally,
a tree satisfying condition 5 is said projective: it implies that arcs cannot cross
(e.g., we cannot have arcs (cid:104)h, m(cid:105) and (cid:104)h(cid:48), m(cid:48)(cid:105) such that h < h(cid:48) < m < m(cid:48)).

in many languages (e.g., those which have free-order) we would like to re-
lax the assumption that all trees must be projective. even in languages which
have    xed word order (such as english) there are syntactic phenomena which
are awkward to characterize using projective trees arising from the context-free
assumption. usually, such phenomena are characterized with additional lin-
guistic artifacts (e.g., traces, wh-movement, etc.). an example is the sentence
(extracted from the id32)

we learned a lesson in 1987 about volatility.

there, the prepositional phrase in 1987 should be attached to the verb phrase
headed by learned (since this is when we learned the lesson), but the other
prepositional phrase about volatility should be attached to the noun phrase
headed by lesson (since the lesson was about volatility). to explain such phe-
nomenon, context-free grammars need to use additional machinery which al-
lows words to be scrambled (in this case, via a movement transformation and
the consequent insertion of a trace). in the dependency-based formalism, we
can get rid of all those artifacts altogether by allowing non-projective parse trees.
these are trees that satisfy conditions 1   3 above, but not necessarily conditions
4 or 5.4 the dependency tree in fig. 4.3 is non-projective: note that the arc
(cid:104)lesson, about(cid:105) is not projective.

4it is also common to impose conditions 1   4, in which case the tree need not be projective, but
it must have a single link departing from the root. the algorithms to be described below can be
adapted for this case.

94

 * welearnedalessonin1987aboutvolatilitywe end this section by mentioning that dependency trees can have their
arcs labeled, to provide more detailed syntactic information. for example, the
arc (cid:104)enjoys, she(cid:105) could be labeled as subj to denote that the modi   er she has a
subject function, and the arc (cid:104)enjoys, school(cid:105) could be labeled as obj to denote
that the modi   er school has an object function. for simplicity, we resort to unla-
beled trees, which just convey the backbone structure. the cope with the labels,
one can use either a joint model that infers the backbone and labels altogether,
or to have a two-stage approach that    rst gets the backbone structure, and then
the arc labels.

4.2.3 algorithms for projective id33
we now turn our attention to algorithms for obtaining a dependency parse tree.
we start by considering a simple kind of models which are called arc-factored.
these models assign a score s  (h, m) to each possible arc (cid:104)h, m(cid:105) connecting a
pair of words; they then score a particular dependency tree t by summing over
the individual scores of the arcs that are present in the tree:

score  (t) =    
(cid:104)h,m(cid:105)   t

s  (h, m).

as usual, from the point of view of the parsing algorithm, it does not matter
whether the scores come from a generative or discriminative approach, and
which features were used to compute the scores. the three important id136
tasks are:

1. obtain the tree with the largest score,

  t = arg max

t

score  (t).

2. compute the partition function (for a log-linear model),

z(s  ) =    

t

exp(score  (t)).

3. compute the posterior marginals for all the possible arcs (which for a

log-linear model is the gradient of the log-partition function),

p  ((cid:104)h, m(cid:105)     t) =

    log z(s  )
   s  (h, m)

.

exercise 4.3 in projective id33 using arc-factored models, the three
tasks above can be solved in time o(n3). sketch how the most likely dependency tree
can be computed by    adapting    the cky algorithm. (hint: note that the cky algo-
rithm builds larger spans by combining smaller spans, and multiplies their weights by

95

the weight of the corresponding production rule. in id33, each    span   
is not represented by a constituent, but rather by the position of its lexical head. con-
vince yourself that this can only be either the leftmost or the rightmost position, and
work out how the two spans can be combined.)

the instantiation of the cky algorithm for projective id33 is called
eisner   s algorithm (eisner, 1996). analogously, the partition function and the poste-
rior marginals can be computed by adapting the inside-outside algorithm.

4.2.4 algorithms for non-projective id33
we turn our attention to non-projective id33. in that case, ef   -
cient solutions also exist for the three problems above; interestingly, they are
based in combinatorial algorithms which are not related at all with dynamic
programming:

    the    rst problem corresponds to    nding the maximum weighted directed
spanning tree in a directed graph. this problem is well known in combi-
natorics and can be solved in o(n3) using chu-liu-edmonds    algorithm
(chu and liu, 1965; edmonds, 1967).5 this has    rst been noted by mc-
donald et al. (2005).

    the second and third problems can be solved by invoking another im-
portant result in combinatorics, the matrix-tree theorem (tutte, 1984). this
fact has been noticed independently by smith and smith (2007); koo et al.
(2007); mcdonald and satta (2007). the cost is that of computing a de-
terminant and inverting a matrix, which can be done in time o(n3). the
procedure is as follows. we    rst consider the directed weighted graph
formed by including all the possible dependency links (cid:104)h, m(cid:105) (including
the ones departing from the dummy root symbol, for which h = 0 by
convention), along with weights given by exp(s  (h, m)), and compute its
(n + 1)-by-(n + 1) laplacian matrix l whose entries are:

lhm =

h(cid:48)=0 exp(s  (h(cid:48), m)),
    exp(s  (h, m))),

if h = m,
otherwise.

(4.2)

(cid:26)    n

denote by r = (rm)m=1,...,n the    rst row of this matrix, transposed, with
the    rst entry removed, i.e., ri (cid:44)     exp(s  (0, m))). furthermore, denote
by   l the (0, 0)-minor of l, i.e., the matrix obtained from l by removing
the    rst row and column. consider the determinant det   l and the inverse
matrix [   l   1]. then:

    the partition function is given by

(4.3)
5there is a asymptotically faster algorithm by tarjan (1977) which solves the same problem in

z(s  ) = det   l;

o(n2).

96

    the posterior marginals are given by

(cid:26) exp(s  (h, m))    ([   l   1]mm     [   l   1]mh)

exp(s  (0, m))    [   l   1]mm

p  ((cid:104)h, m(cid:105)     t) =

if h (cid:54)= 0
otherwise.
(4.4)

exercise 4.4 in this exercise you are going to experiment with arc-factored non-projective
dependency parsers.

the conll-x and conll 2008 shared task datasets (buchholz and marsi, 2006;
surdeanu et al., 2008) contain dependency treebanks for 14 languages. in this lab, we
are going to experiment with the portuguese and english datasets. we preprocessed
those datasets to exclude all sentences with more than 15 words; this yielded the    les:

    data/deppars/portuguese train.conll,

    data/deppars/portuguese test.conll,

    data/deppars/english train.conll,

    data/deppars/english test.conll.

1. after importing all the necessary libraries, load the portuguese dataset:

import sys

2 sys.path.append("parsing/" )

4 import dependency_parser as depp

6 dp = depp.dependencyparser()

dp.read_data("portuguese")

observe the statistics which are plotted. how many features are there in total?

2. we will now have a close look on the features that can be used in the parser. ex-
amine the    le code/parsing/dependency features.py. the follow-
ing method takes a sentence and computes a vector of features for each possible
arc (cid:104)h, m(cid:105):
1 def create_arc_features(self, instance, h, m, add=false):

'''creates features for arc h-->m.'''

we grouped the features in several subsets, so that we can conduct some ablation
experiments:

    basic features that look only at the parts-of-speech of the words that can be

connected by an arc;

97

    lexical features that also look at these words themselves;
    distance features that look at the length and direction of the dependency

link (i.e., distance between the two words);

    contextual features that look at the context (part-of-speech tags) of the

words surrounding h and m.

in the default con   guration, only the basic features are enabled. the total num-
ber of features is the quantity observed in the previous question. with this con-
   guration, train the parser by running 10 epochs of the structured id88
algorithm:

dp.train_id88(10)

2 dp.test()

what is the accuracy obtained in the test set? (note: the plotted accuracy is the
fraction of words whose parent was correctly predicted.)

3. repeat the previous exercise by subsequently enabling the lexical, distance and

contextual features:

dp.features.use_lexical = true

2 dp.read_data("portuguese")

dp.train_id88(10)

4 dp.test()

6 dp.features.use_distance = true

dp.read_data("portuguese")

8 dp.train_id88(10)

dp.test()

10

dp.features.use_contextual = true

12 dp.read_data("portuguese")

dp.train_id88(10)

14 dp.test()

for each con   guration, write down the number of features and test set accura-
cies. observe the improvements obtained when more features were added. feel
free to engineer new features!

4. which of the three important id136 tasks discussed above (computing the
most likely tree, computing the partition function, and computing the marginals)
need to be performed in the structured id88 algorithm? what about a max-
imum id178 classi   er, with stochastic id119? check your answers
by looking at the following two methods in code/dependency parser.py:

98

def train_id88(self, n_epochs):

2 ...

4 def train_crf_sgd(self, n_epochs, sigma, eta0 = 0.001):

...

repeat the last exercise by training a maximum id178 classi   er, with stochastic
id119, using    = 0.01 and a initial stepsize of   0 = 0.1:

1 dp.train_crf_sgd(10, 0.01, 0.1)

dp.test()

compare the results with those obtained by the id88 algorithm.

5. train a parser for english using your favourite learning algorithm:

dp.read_data("english")
2 dp.train_id88(10)

dp.test()

the predicted trees are placed in the    le data/deppars/english test.conll.pred.
to get a sense of which errors are being made, you can check the sentences that
differ from the gold standard (data/deppars/english test.conll) and
visualize those sentences, e.g., in http://www.ark.cs.cmu.edu/treeviz/.

4.2.5 model re   nements
a number of re   nements has been made that yield more accurate dependency
parsers. we mention just a few:

sibling and grandparent features. the arc-factored assumption fails to cap-
ture correlations between pairs of arcs. the id145 algo-
rithms for the projective case can be extended (at some additional cost) to
handle features that look at consecutive sibling arcs on the same side of
the head (i.e., pairs of arcs of the form (cid:104)h, m(cid:105) and (cid:104)h, s(cid:105) with h < m < s or
h > m > s, such that no arc (cid:104)h, r(cid:105) exists with r between m and s. this has
been done by eisner and satta (1999). similarly, grandparents can also be
accommodated with similar extensions (carreras, 2007). these are called
   second-order models.   
for the non-projective case, however, any extension beyond the arc-factored
model becomes np-hard (mcdonald and satta, 2007). yet, approximate

99

algorithms have been proposed to handle    second-order models    that
seem to work well: a greedy method (mcdonald et al., 2006), loopy belief
propagation (smith and eisner, 2008), a id135 relaxation
(martins et al., 2009), and a id209 method (koo et al., 2010).

third-order models. for the projective case, third order models have also been

considered (koo and collins, 2010)

transition-based parsers. like in the phrase-based case, there is a totally dif-
ferent line of work which models parsers as a sequence of greedy shift-
reduce decisions (nivre et al., 2006; huang and sagae, 2010). these parsers
seem to be very fast (expected linear time) and only slightly less accurate
than the state-of-the-art. solutions have been worked out for the non-
projective case also (nivre, 2009).

100

