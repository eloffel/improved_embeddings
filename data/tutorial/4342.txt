day 5

unsupervised learning

in this class we will address the problem of unsupervised learning of linguistic
structures, namely parts-of-speech. in this setting we are not given any labeled
data.
instead, all we get to see is a set of natural language sentences. the
underlying question is:

can we learn something from raw text?

this task is particularly challenging since the process by which linguistic
structures are generated is not always clear and even when it is, it is normally
too complex to be formally expressed. nevertheless, unsupervised learning
has been applied to a wide range of natural language processing tasks, such
as: part-of-speech induction (sch   utze, 1995; merialdo, 1994; clark, 2003), de-
pendency grammar induction (klein and manning, 2004; smith and eisner,
2006), constituency grammar induction (klein and manning, 2004), statistical
word alignments (brown et al., 1993) and id2 (charniak and
elsner, 2009), just to name a few.

different motivations have pushed research in this area. from both a lin-
guistic and cognitive point of view, unsupervised learning is useful as a tool to
study id146. from a machine learning point of view, unsuper-
vised learning is a fertile ground for testing new learning methods, where sig-
ni   cant improvements can yet be made. from a more pragmatic perspective,
unsupervised learning is required since annotated corpora is a scarce resource
for different reasons. independently of the reason, unsupervised learning is an
increasing active    eld of research.
a    rst problem with unsupervised learning, since we don   t observe any
labeled data (i.e., the training set is now d = {x1, . . . , xm}), is that most of
the methods studied so far (id88, mira, id166s) cannot be used since we
cannot compare the true output with the predicted output. note also that a
direct minimization of the complete negative log-likelihood of the data, log p  (d),

101

is very challenging, since it would require marginalizing out (i.e., summing
over) all possible hidden variables:

log p  (d) =

m   

m=1

log    
y   y

p  (xm, y).

(5.1)

note also that the objective above is non-convex even for a linear model: hence,
it may have local minima, which makes optimization much more dif   cult.

another observation is that normally we are restricted to generative mod-
els, with some remarkable exceptions (smith and eisner, 2005a), since the ob-
jective of discriminative models when no labels are observed are meaningless
(   ym p(ym|xm) = 1); this rules out, for instance, maximum id178 classi   ers.
the most common optimization method in the presence of hidden (latent)
variables is the expectation maximization (em) algorithm. note that this al-
gorithm is a generic optimization routine that does not depend on a particular
model. the next section will explain the em algorithm. on section 5.2 we will
apply the em algorithm to the task of part-of-speech induction, where one is
given raw text and a number of clusters and the task is to cluster words that
behave similarly in a grammatical sense.

5.1 expectation maximization algorithm

given a particular model p  (  x,   y) and a training corpus x of d sentences   x1 . . .   xd,
training seeks model parameters    that minimize the negative log-likelihood of
the corpus:

negative log likelihood : l(  ) =(cid:98)e[    log p  (  x)] =(cid:98)e[    log    
where (cid:98)e[ f (  x)] = 1

(5.2)
i=1 f (  xi) denotes the empirical average of a function f

p  (  x,   y)],

  y

d    d
over the training corpus.

because of the hidden variables   y, the likelihood term contains a sum over
all possible hidden structures inside of a logarithm, which makes this quantity
hard to compute.

the most common minimization algorithm to    t the model parameters in
the presence of hidden variables is the expectation maximization (em) algo-
rithm.

the em procedure can be thought of intuitively in the following way. if
we observe the hidden variables    values for all sentences in the corpus, then
we could easily compute the maximum likelihood value of the parameters as
described in section 2.2. on the other hand, if we had the model parameters we
could label data using the model, and collect the suf   cient statistics described
in section 2.2. since we are working in an unsupervised setting, we never

102

get to observe the hidden state sequence. instead, given a training set x =
{  x1 . . .   xd}, we will need to collect suf   cient statistics, or expected counts that
represent the expected number of times that each hidden variable is expected to
be used with the current parameters setting. these suf   cient statistics will then
be used during learning as fake observations of the hidden variables. using the
node and edge posterior distributions described in equations 2.17 and 2.18, the
suf   cient statistics can be computed by the following formulas:

initial counts :

ic(yl) =

d   

d=1

  1(yl);

final counts :

f c(yn, yn   1) =

d=1

d   
n   1   

d   

d=1

i=1

  n   1(yl, ym);

  i(yl, ym);

transition counts :

tc(yl, ym) =

state counts :

sc(vq, ym) =

d   

n   

d=1

i=1,xi=vq

  i(ym).

(5.3)

(5.4)

(5.5)

(5.6)

compare the previous equations with the ones described in section 2.2 for
the same quantities. the main difference is that while in the presence of su-
pervised data you sum the observed events, when you have no label data you
sum the posterior probabilities of each event. if these probabilities were such
that the id203 mass was around single events then both equations will
produce the same result.

the em procedure starts with an initial guess for the parameters   0 at time
t = 0. the algorithm iterates for t iterations until it converges to a local minima
of the negative log likelihood, and each iteration is divided into two steps:

the    rst step -    e step    (expectation) - computes the posteriors for the hid-
den variables p  (   y |   x), given the current parameter values   t and the
observed variables. in the case of the id48 this requires only to run the
fb algorithm.

the second step -    m step    (maximization) - uses p  (   y |   x) to    softly    ll in   
the values of the hidden variables   y, and collects the suf   cient statistics,
initial counts (eq: 5.3), transition counts (eq: 5.5) and state counts (eq:
5.6) and uses those counts to estimate maximum likelihood parameters
  t+1 as described in section 2.2.

the em algorithm is guaranteed to converge to a local minimum of l(  )
under mild conditions. note that we are not committing to the best assign-
ment of the hidden variables, but summing the occurrences of each parameter
weighed by the posterior id203 of all possible assignments. this modular

103

split into two intuitive and straightforward steps accounts for the vast popu-
larity of em.
more formally, em minimizes l(  ) via block-coordinate descent on an up-
per bound f(q,   ) using an auxiliary distribution over the latent variables q(   y |
  x):
l(  ) = (cid:98)e
= (cid:98)e
= (cid:98)e

q(   y |   x)     p  (  x,   y)
(cid:35)
q(   y |   x)

    log    
  y
    log    
  y

q(   y |   x) log

q(   y |   x) log

p  (  x,   y)
q(   y |   x)

   (cid:98)e

(cid:34)
(cid:34)
(cid:34)

       
  y

= f(q,   ),

p  (  x,   y)

(5.8)

(5.7)

(5.9)

(cid:35)

(cid:35)

(cid:34)

(cid:35)

q(   y |   x)
p  (  x,   y)

   
  y

where we have multiplied and divided the p  (  x,   y) by the same quantity q(   y |
  x), and the lower bound comes from applying jensen inequality (equation 5.8).
f(q,   ) is normally referred to as the energy function, which comes from the
physics    eld and refers to the energy of a given system that we want to mini-
mize.

em upper bound : l(  )     f(q,   ) =(cid:98)e

(cid:34)

q(   y |   x) log

   
  y

q(   y |   x)
p  (  x,   y)

.

(5.10)

(cid:35)

the alternating e and m steps at iteration t + 1 can be seen as minimizing the
energy function    rst with respect to q(   y |   x) and then with respect to   :
(cid:35)

qt+1(   y |   x) = arg min
q(   y|  x)

kl(q(   y |   x) || p  t (   y |   x)) = p  t (   y |   x);
(5.11)

f(q,   t) = arg min
q(   y|  x)

(cid:34)

qt+1(   y |   x) log p  (  x,   y)

;

(5.12)

  t+1 = arg min

f(qt+1,   ) = arg max

  

  

(cid:98)e

   
  y

e :

m :

where kl(q||p) = eq[log q(  )
p(  ) ] is the id181. the kl term
in the e-step results from dropping all terms from the energy function that
are constant for a set   , in this case the likelihood of the observation sequence
p  (  x):

104

q(   y |   x) log

   
  y

q(   y |   x)
p  (  x,   y)

=    
  y

q(   y |   x) log q(   y |   x)        
  y

q(   y |   x) log p  (  x,   y)

(5.13)
q(   y |   x) log p  (  x)p  (   y |   x)
(5.14)

=    
  y

q(   y |   x) log q(   y |   x)        
  y

q(   y |   x)
p  (   y |   x)

q(   y |   x) log

    log p  (  x)
=    
  y
= kl(q(   y |   x)||p  (   y |   x))     log p  (  x).

(5.15)

(5.16)

algorithm 13 presents the pseudo code for the em algorithm. note that
this algorithm is agnostic of a particular model, it only requires the model to
implement a common interface.

algorithm 13 em algorithm.
1: input: dataset d, an initialized model
2: for t = 1 to t do
3: model.clear counts()
for seq     d do
4:
5:
6:
7:
end for
8:
9: m-step:
10: model.update params(counts)
11: end for

e-step:
posteriors,likelihood =model.compute posteriors(seq)
model.update counts(seq,posteriors)

one important thing to note in algorithm 13 is that for the id48 model
we already have all the model pieces we require.
in fact the only method
we don   t have yet implemented from previous classes is the method to up-
date counts(posteriors).

exercise 5.1 implement the method update counts(seq,posteriors).

1 def update_counts(self,seq,posteriors):

use the method you de   ned previously to check the count tables to check if this

method is correct. use a corpus with only one sentence to make the test simpler.

1 in []: run readers/pos_corpus.py

in []: posc = postagcorpus("en",max_sent_len=15,train_sents=1,

dev_sents=0,test_sents=0)

105

3 in []: run sequences/id48.py

in []: id48 = id48(posc)

5 in []: id48.train_supervised(posc.train,smoothing=0.1)

in []: id48.clear_counts()

7 in []: posteriors,likelihood = id48.get_posteriors(posc.train.

seq_list[0])

in []: id48.update_counts(posc.train.seq_list[0],posteriors)

9 in []: id48.sanity_check_counts(posc.train)

if you pass this test, then you have all the pieces to implement the em algorithm.
look at the code for em algorithm in    le sequences/em.py and check it for yourself.

def train(self,seq_list,nr_iter=10,smoothing=0,evaluate=

true):
if(evaluate):

### evaluate accuracy at initial iteration
pred = self.model.viterbi_decode_corpus(seq_list.

seq_list)

acc = self.model.evaluate_corpus(seq_list.seq_list,

pred)

for t in xrange(1,nr_iter):

#e-step
total_likelihood = 0
self.model.clear_counts(smoothing)
for seq in seq_list.seq_list:

posteriors,likelihood = self.model.

get_posteriors(seq)

self.model.update_counts(seq,posteriors)
total_likelihood += likelihood

print "iter: %i - log likelihood %f"%(t,-1*math.log

(total_likelihood))

#m-step
self.model.update_params()

if(evaluate):

### evaluate accuracy at this iteration
pred = self.model.viterbi_decode_corpus(

seq_list.seq_list)

acc = self.model.evaluate_corpus(seq_list.

seq_list,pred)

print "iter: %i acc %f"%(t,acc)

1

3

5

7

9

11

13

15

17

19

21

106

5.2 part of speech induction

in this section we present the part-of-speech induction task. part-of-speech
tags are pre-requisite for many text applications. the task of part-of-speech
tagging where one is given a labeled training set of words and respective tags
is a well studied task with several methods achieving high prediction quality,
as we saw in chapters 2 and 3.

on the other hand the task of part-of-speech induction where one does not
have access to a labeled corpus is a much harder task with a huge space for
improvement. in this case, we are given only the raw text along with sentence
boundaries and a prede   ned number of clusters we can use. this problem
can be seen as a id91 problem. we want to cluster words that behave
grammatically in the same way on the same cluster. this is a much harder
problem.

formally, the problem setting is the following: we are given a training set
x =   x1 . . .   xd of d training examples, where each example   x = x1 . . . xn is a
sentence of n words, whose values v are taken from a vocabulary v of possible
word types. we are also given the set of clusters y that we are allowed to
use. the hidden structure   y = y1 . . . yn corresponds to a sequence of cluster
assignments for each individual word, such that yn = yl with yl     y.

depending on the task at hand we can pick an arbitrary number of clusters.
if the goal is to test how well our method can recover the true pos tags then
we should use the same number of clusters as pos tags. on the other hand, if
the task is to extract features to be used by other methods we can use a much
bigger number of clusters (e.g. 200) to capture correlations not captured by pos
tags, like lexical af   nity.

note, however that nothing is said about the identity of each cluster. the
model has no preference in assigning cluster 1 to nouns vs cluster 2 to nouns.
given this non-identi   ability several metrics have been proposed for evalu-
ation (reichart and rappoport, 2009; haghighi and klein, 2006; meil  a, 2007;
rosenberg and hirschberg, 2007). in this class we will use a common and sim-
ple metric called 1-many, which maps each cluster to majority pos tag that it
contains (see figure 5.1 for an example).

exercise 5.2 run the em algorithm for part of speech induction:

in []: run readers/pos_corpus.py

2 in []: posc = postagcorpus("en",max_sent_len=15,train_sents=

1000,dev_sents=0,test_sents=0)

in []: run sequences/id48.py

4 in []: id48 = id48(posc)

in []: id48.initialize_radom()

6 in []: run sequences/em.py

in []: em = em(posc,id48)

8 in []: em.train(posc.train,nr_iter=20)

107

figure 5.1: confusion matrix example. each cluster is a column. the best
tag in each column is represented under the column (1-many) mapping. each
color represents a true pos tag.

out []: init acc 0.335505

10 out []: iter: 1 - log likelihood 16.071708

out []: iter: 1 acc 0.361960

12 out []: iter: 2 - log likelihood 11.212829

out []: iter: 2 acc 0.381000

14 out []: iter: 3 - log likelihood 11.091918

out []: iter: 3 acc 0.387013

16 out []: iter: 4 - log likelihood 10.751445

out []: iter: 4 acc 0.391222

18 out []: iter: 5 - log likelihood 10.046576

out []: iter: 5 acc 0.390420

20 out []: iter: 6 - log likelihood 9.055178

out []: iter: 6 acc 0.391723

22 out []: iter: 7 - log likelihood 8.109925

out []: iter: 7 acc 0.390420

24 out []: iter: 8 - log likelihood 7.497388

out []: iter: 8 acc 0.390520

26 out []: iter: 9 - log likelihood 7.225907

out []: iter: 9 acc 0.393827

28 out []: iter: 10 - log likelihood 7.127711

out []: iter: 10 acc 0.398236

30 out []: iter: 11 - log likelihood 7.105954

out []: iter: 11 acc 0.404449

32 out []: iter: 12 - log likelihood 7.111193

108

out []: iter: 12 acc 0.406654

34 out []: iter: 13 - log likelihood 7.041794

out []: iter: 13 acc 0.411264

36 out []: iter: 14 - log likelihood 6.958736

out []: iter: 14 acc 0.408558

38 out []: iter: 15 - log likelihood 6.828692

out []: iter: 15 acc 0.407656

40 out []: iter: 16 - log likelihood 6.693052

out []: iter: 16 acc 0.403848

42 out []: iter: 17 - log likelihood 6.670297

out []: iter: 17 acc 0.405451

44 out []: iter: 18 - log likelihood 6.684892

out []: iter: 18 acc 0.408658

46 out []: iter: 19 - log likelihood 6.706640

out []: iter: 19 acc 0.412166

note: your results may not be the same as in this example since we are using a random
start, but the trend should be the same. also note that in some iterations the likelihood
does not go down because of some rounding errors, however the general trend is that
likelihood decreases over iterations.

in the previous exercise we used an id48 to do part-of-speech induction
using 12 clusters (by omission the id48 uses as number of hidden states the
one provided by the corpus). a    rst observation is that the log-likelihood is
always increasing as expected. another observation is that the accuracy goes
up from 33% to 41%. note that normally you will run this algorithm for 200
iterations, we stopped earlier for time constraints. another observations is that
the accuracy is not monotonic increasing, this is because the likelihood is not
a perfect proxy for the accuracy. in fact all that likelihood is measuring are
co-occurrences of words in the corpus; it has no idea of pos tags. the fact we
are improving derives from the fact that language is not random but follows
some speci   c hidden patterns. in fact this patterns are what true pos-tags try
to capture. a    nal observation is that the performance is really bad compared
to the supervised scenario, so there is a lot of space for improvement. the
actual state of the art is around 71% for fully unsupervised (grac  a, 2010; berg-
kirkpatrick et al., 2010) and 80% (das and petrov, 2011) using parallel data and
information from labels in the other language.

looking at figure 5.1 shows the confusion matrix for this particular exam-
ple. a    rst observation is that most clusters are mapped to nouns, verbs or
punctuation. this is a none fact since there are many more nouns and verbs
than any other tags. since maximum likelihood prefers probabilities to be uni-
form (imagine two parameters. in one setting both have value 0.5 so the like-
lihood will be 0.5*0.5 = 0.25, while in the other case one as 0.1 and 0.9 so the
maximum likelihood is 0.09). several approaches have been proposed to ad-
dress this problem under moving towards a bayesian setting or using posterior

109

id173 (johnson, 2007; grac  a et al., 2009) more about this later today.
part-of-speech induction is a very active    eld of research, in fact in the last two
acl conferences (association for computational linguistics) the short paper
award (2010) and the best paper award (2011) were about this topic (lamar
et al., 2010; das and petrov, 2011).

110

bibliography

berg-kirkpatrick, t., bouchard-c  ot  e, a., denero, j., and klein, d. (2010). pain-

less unsupervised learning with features. in proc. naacl.

bertsekas, d., homer, m., logan, d., and patek, s. (1995). nonlinear program-

ming. athena scienti   c.

bishop, c. (2006). pattern recognition and machine learning, volume 4. springer

new york.

blitzer, j., dredze, m., and pereira, f. (2007). biographies, bollywood, boom-
boxes and blenders: id20 for sentiment classi   cation. in an-
nual meeting-association for computational linguistics, volume 45, page 440.

bottou, l. (1991). une approche theorique de l   apprentissage connexionniste: ap-

plications a la reconnaissance de la parole. phd thesis.

boyd, s. and vandenberghe, l. (2004). id76. cambridge univ

pr.

brown, p. f., pietra, s. a. d., pietra, v. j. d., and mercer, r. l. (1993). the
mathematics of id151: parameter estimation. com-
putational linguistics, 19(2):263   311.

buchholz, s. and marsi, e. (2006). conll-x shared task on multilingual de-

pendency parsing. in proc. of conll.

carreras, x. (2007). experiments with a higher-order projective dependency

parser. in proc. of conll.

charniak, e. (1997). statistical parsing with a context-free grammar and word
in proceedings of the national conference on arti   cial intelligence,

statistics.
pages 598   603. citeseer.

charniak, e. and elsner, m. (2009). em works for pronoun id2.
in proceedings of the 12th conference of the european chapter of the association
for computational linguistics, pages 148   156. association for computational
linguistics.

111

charniak, e., johnson, m., elsner, m., austerweil, j., ellis, d., haxton, i., hill,
c., shrivaths, r., moore, j., pozar, m., et al. (2006). multilevel coarse-to-   ne
pid18 parsing. in proceedings of the main conference on human language technol-
ogy conference of the north american chapter of the association of computational
linguistics, pages 168   175. association for computational linguistics.

chomsky, n. (1965). aspects of the theory of syntax, volume 119. the mit press.

chu, y. j. and liu, t. h. (1965). on the shortest arborescence of a directed

graph. science sinica, 14:1396   1400.

clark, a. (2003). combining distributional and morphological information for

part of speech induction. in proc. eacl.

cohen, s., gimpel, k., and smith, n. (2008). logistic normal priors for unsu-

pervised probabilistic grammar induction. in in nips. citeseer.

collins, m. (1999). head-driven statistical models for natural language parsing. phd

thesis, university of pennsylvania.

collins, m. (2002). discriminative training methods for hidden markov mod-
els: theory and experiments with id88 algorithms. in proceedings of the
acl-02 conference on empirical methods in natural language processing-volume
10, pages 1   8. association for computational linguistics.

cover, t., thomas, j., wiley, j., et al. (1991). elements of id205, vol-

ume 6. wiley online library.

covington, m. (1990). parsing discontinuous constituents in dependency

grammar. computational linguistics, 16(4):234   236.

crammer, k., dekel, o., keshet, j., shalev-shwartz, s., and singer, y. (2006).

online passive-aggressive algorithms. jmlr, 7:551   585.

crammer, k. and singer, y. (2002). on the algorithmic implementation of multi-
class kernel-based vector machines. the journal of machine learning research,
2:265   292.

das, d. and petrov, s. (2011). unsupervised part-of-speech tagging with bilin-
gual graph-based projections. in proceedings of the 49th annual meeting of the
association for computational linguistics: human language technologies, pages
600   609, portland, oregon, usa. association for computational linguistics.

duda, r., hart, p., and stork, d. (2001). pattern classi   cation, volume 2. wiley

new york.

edmonds, j. (1967). optimum branchings.

bureau of standards, 71b:233   240.

journal of research of the national

112

eisner, j. (1996). three new probabilistic models for id33: an
exploration. in proceedings of the 16th conference on computational linguistics-
volume 1, pages 340   345. association for computational linguistics.

eisner, j. and satta, g. (1999). ef   cient parsing for bilexical context-free gram-

mars and head automaton grammars. in proc. of acl.

finkel, j., kleeman, a., and manning, c. (2008). ef   cient, feature-based, con-

ditional random    eld parsing. proceedings of acl-08: hlt, pages 959   967.

grac  a, j. (2010). posterior id173 framework: learning tractable models
with intractable constraints. phd thesis, universidade t  ecnica de lisboa, in-
stituto superior t  ecnico.

grac  a, j., ganchev, k., pereira, f., and taskar, b. (2009). parameter vs. posterior

sparisty in latent variable models. in proc. nips.

haghighi, a. and klein, d. (2006). prototype-driven learning for sequence

models. in proc. htl-naacl. acl.

henderson, j. (2003). inducing history representations for broad coverage sta-
in proceedings of the 2003 conference of the north american
tistical parsing.
chapter of the association for computational linguistics on human language
technology-volume 1, pages 24   31. association for computational linguis-
tics.

hopcroft, j., motwani, r., and ullman, j. (1979). introduction to automata theory,

languages, and computation, volume 3. addison-wesley reading, ma.

huang, l. and sagae, k. (2010). id145 for linear-time incre-

mental parsing. in proc. of acl, pages 1077   1086.

hudson, r. (1984). word grammar. blackwell oxford.

jaynes, e. (1982). on the rationale of maximum-id178 methods. proceedings

of the ieee, 70(9):939   952.

joachims, t. (2002). learning to classify text using support vector machines:

methods, theory and algorithms. kluwer academic publishers.

johnson, m. (1998). pid18 models of linguistic tree representations. computa-

tional linguistics, 24(4):613   632.

johnson, m. (2007). why doesn   t em    nd good id48 pos-taggers. in in proc.

emnlp-conll.

klein, d. and manning, c. (2002). a generative constituent-context model for
improved grammar induction. in proceedings of the 40th annual meeting on
association for computational linguistics, pages 128   135. association for com-
putational linguistics.

113

klein, d. and manning, c. (2003). accurate unlexicalized parsing. in proceed-
ings of the 41st annual meeting on association for computational linguistics-
volume 1, pages 423   430. association for computational linguistics.

klein, d. and manning, c. (2004). corpus-based induction of syntactic struc-

ture: models of dependency and constituency. in proc. acl.

koo, t. and collins, m. (2010). ef   cient third-order dependency parsers. in

proc. of acl, pages 1   11.

koo, t., globerson, a., carreras, x., and collins, m. (2007). structured predic-

tion models via the matrix-tree theorem. in proc. emnlp.

koo, t., rush, a. m., collins, m., jaakkola, t., and sontag, d. (2010). dual
decomposition for parsing with non-projective head automata. in emnlp.

lafferty, j., mccallum, a., and pereira, f. (2001). conditional random    elds:
probabilistic models for segmenting and labeling sequence data. in procs. of
icml, pages 282   289.

lamar, m., maron, y., johnson, m., and bienenstock, e. (2010). svd and clus-
tering for unsupervised id52. in proceedings of the acl 2010 confer-
ence: short papers, pages 215   219, uppsala, sweden. association for compu-
tational linguistics.

magerman, d. (1995). statistical decision-tree models for parsing.

in pro-
ceedings of the 33rd annual meeting on association for computational linguistics,
pages 276   283. association for computational linguistics.

manning, c., raghavan, p., and sch   utze, h. (2008). introduction to information

retrieval, volume 1. cambridge university press cambridge, uk.

manning, c. and sch   utze, h. (1999). foundations of statistical natural language

processing, volume 59. mit press.

marcus, m., marcinkiewicz, m., and santorini, b. (1993). building a large an-
notated corpus of english: the id32. computational linguistics,
19(2):313   330.

martins, a. f. t., smith, n. a., and xing, e. p. (2009). concise integer linear
programming formulations for id33. in proc. of acl-ijcnlp.

mccallum, a., freitag, d., and pereira, f. (2000). maximum id178 markov
in proceedings of the
models for information extraction and segmentation.
seventeenth international conference on machine learning, pages 591   598. cite-
seer.

114

mccallum, a. and nigam, k. (1998). a comparison of event models for naive
bayes text classi   cation. in aaai-98 workshop on learning for text categoriza-
tion, volume 752, pages 41   48. citeseer.

mcdonald, r., lerman, k., and pereira, f. (2006). multilingual dependency

analysis with a two-stage discriminative parser. in proc. of conll.

mcdonald, r. and satta, g. (2007). on the complexity of non-projective data-

driven id33. in proc. of iwpt.

mcdonald, r. t., pereira, f., ribarov, k., and hajic, j. (2005). non-projective
id33 using spanning tree algorithms. in proc. of hlt-emnlp.

meil  a, m. (2007). comparing id91s   an information based distance. j.

multivar. anal., 98(5):873   895.

mel  cuk, i. (1988). dependency syntax: theory and practice. state university of

new york press.

merialdo, b. (1994). tagging english text with a probabilistic model. computa-

tional linguistics, 20(2):155   171.

mitchell, t. (1997). machine learning.

nivre, j. (2009). non-projective id33 in expected linear time. in
proceedings of the joint conference of the 47th annual meeting of the acl and the
4th international joint conference on natural language processing of the afnlp:
volume 1-volume 1, pages 351   359. association for computational linguis-
tics.

nivre, j., hall, j., nilsson, j., eryi  git, g., and marinov, s. (2006). labeled
in

pseudo-projective id33 with support vector machines.
procs. of conll.

nocedal, j. and wright, s. (1999). numerical optimization. springer verlag.

p  erez, f. and granger, b. e. (2007). ipython: a system for interactive scienti   c

computing. comput. sci. eng., 9(3):21   29.

petrov, s. and klein, d. (2007). improved id136 for unlexicalized parsing.
in human language technologies 2007: the conference of the north american
chapter of the association for computational linguistics; proceedings of the main
conference, pages 404   411, rochester, new york. association for computa-
tional linguistics.

petrov, s. and klein, d. (2008a). discriminative log-linear grammars with latent
variables. in platt, j., koller, d., singer, y., and roweis, s., editors, advances
in neural information processing systems 20 (nips), pages 1153   1160, cam-
bridge, ma. mit press.

115

petrov, s. and klein, d. (2008b). sparse multi-scale grammars for discrimina-
tive latent variable parsing. in proceedings of the 2008 conference on empirical
methods in natural language processing, pages 867   876, honolulu, hawaii.
association for computational linguistics.

rabiner, l. (1989). a tutorial on id48 and selected applica-

tions in id103. in proc. ieee, 77(2):257   286.

ratnaparkhi, a. (1999). learning to parse natural language with maximum

id178 models. machine learning, 34(1):151   175.

reichart, r. and rappoport, a. (2009). the nvi id91 evaluation measure.

in proc. conll.

rosenberg, a. and hirschberg, j. (2007). v-measure: a conditional id178-
in emnlp-conll, pages 410   

based external cluster evaluation measure.
420.

rosenblatt, f. (1958). the id88: a probabilistic model for information

storage and organization in the brain. psychological review, 65(6):386.

sch  olkopf, b. and smola, a. j. (2002). learning with kernels. the mit press,

cambridge, ma.

sch   utze, h. (1995). distributional part-of-speech tagging. in proceedings of the
seventh conference on european chapter of the association for computational lin-
guistics, pages 141   148. morgan kaufmann publishers inc.

shalev-shwartz, s., singer, y., and srebro, n. (2007). pegasos: primal estimated

sub-gradient solver for id166. in icml.

shannon, c. (1948). a mathematical theory of communication. bell syst. tech.

journ., 27(379):623.

shawe-taylor, j. and cristianini, n. (2004). kernel methods for pattern analysis.

cup.

smith, d. a. and eisner, j. (2008). id33 by belief propagation.

in proc. of emnlp.

smith, d. a. and smith, n. a. (2007). probabilistic models of nonprojective

dependency trees. in proc. emnlp-conll.

smith, n. and eisner, j. (2005a). contrastive estimation: training log-linear

models on unlabeled data. in proc. acl. acl.

smith, n. and eisner, j. (2005b). guiding unsupervised grammar induction us-
ing contrastive estimation. in proc. of ijcai workshop on grammatical id136
applications. citeseer.

116

smith, n. a. and eisner, j. (2006). annealing structural bias in multilingual
weighted grammar induction. in acl-44: proceedings of the 21st international
conference on computational linguistics and the 44th annual meeting of the as-
sociation for computational linguistics, pages 569   576, morristown, nj, usa.
association for computational linguistics.

surdeanu, m., johansson, r., meyers, a., m`arquez, l., and nivre, j. (2008). the
conll-2008 shared task on joint parsing of syntactic and semantic depen-
dencies. proc. of conll.

tarjan, r. (1977). finding optimum branchings. networks, 7(1):25   36.

taskar, b., klein, d., collins, m., koller, d., and manning, c. (2004). max-

margin parsing. in proc. emnlp, pages 1   8.

tesni`ere, l. (1959). el  ements de syntaxe structurale. libraire c. klincksieck.

tutte, w. (1984). id207. addison-wesley, reading, ma.

vapnik, n. v. (1995). the nature of statistical learning theory. springer-verlag,

new york.

117

