learning with big data i: 

divide and conquer 

chris dyer 

lti

28   july   2014         lxmls	

lti

ablc

no   data   like   more   data!	

training instances 

(banko and brill, acl 2001) 
(brants et al., emnlp 2007) 

problem	

moore   s law (~ power/cpu)

#(t) ~ n0    2t/(2 years)

~

hard disk capacity

#(t) ~ m0    3.2t/(2 years)

~

we can represent more data than we 
can centrally process ... and this will 

only get worse in the future.

solution	
       partition training data into subsets 

o    core concept: data shards 
o    algorithms that work primarily    inside    shards and communicate minimally 
o    nodes is a graph, edges represent communication pathways 

       alternative solutions 

o    reduce data points by instance selection (e.g. core sets) 
o    id84 / compression 

       related problems 

o    large numbers of dimensions (horizontal partitioning) 

       model is too big to fit in memory 

o    large numbers of related tasks 

       every gmail user has his own opinion about what is spam 

outline	

       mapreduce 
       design patterns for mapreduce 

       batch learning algorithms on mapreduce 
       distributed online algorithms 

mapreduce 
cheap commodity clusters 

+ simple, distributed programming models  
= data-intensive computing for all 

divide   and   conquer	

w1	

   worker

   	

r1	

   work   	

w2	

   worker

   	

r2	

   result   	

w3	

   worker

   	

r3	

partition 

combine 

it   s   a   bit   more   complex   	

fundamental issues 

scheduling, data distribution, synchronization, 
inter-process communication, robustness, fault 
tolerance,     

different programming models 
shared memory 

message passing 

y 
r
o
m
e
m

p1  p2  p3  p4  p5 

p1  p2  p3  p4  p5 

different programming constructs 

mutexes, conditional variables, barriers,     
masters/slaves, producers/consumers, work queues,     

architectural issues 

flynn   s taxonomy (simd, mimd, etc.), 
network typology, bisection bandwidth 
uma vs. numa, cache coherence  

common problems 

livelock, deadlock, data starvation, priority inversion    
dining philosophers, sleeping barbers, cigarette smokers,     

the reality: programmer shoulders the burden 
of managing concurrency    

source: ricardo guimar  es herrmann 

typical   problem	

map 
      
iterate over a large number of records 
       extract something of interest from each 
       shuffle and sort intermediate results 
r e d u c e  
       aggregate intermediate results 
       generate final output 

key idea: functional abstraction for these two operations 

map 

fold 

f 

f 

f 

f 

f 

map 

g 

g 

g 

g 

g 

reduce 

mapreduce	

       programmers specify two functions: 

map (k, v)     <k   , v   >* 
reduce (k   , v   )     <k   , v   >* 
o    all values with the same key are reduced together 

       usually, programmers also specify: 
partition (k   , number of partitions )     partition for k    
o    often a simple hash of the key, e.g. hash(k   ) mod n 
o    allows reduce operations for different keys in parallel 
combine(k   ,v   )     <k   ,v   > 
o       mini-reducers    that run in memory after the map phase 
o    optimizes to reduce network traffic & disk writes 
implementations: 
o    google has a proprietary implementation in c++ 
o    hadoop is an open source implementation in java 

      

k1 

v1 

k2 

v2 

k3 

v3 

k4 

v4 

k5 

v5 

k6 

v6 

map	

map	

map	

map	

a  1 

c 

2 

b 
7 
shu   e   and   sort:   aggregate   values   by   keys	

3 

6 

a 

5 

2 

b 

c 

c 

c 

9 

a 

1  5 

b 

2  7 

c 

2  3  6  9 

reduce	

reduce	

reduce	

r1  s1 

r2  s2 

r3  s3 

mapreduce   runtime	

o    assigns workers to map and reduce tasks 

       handles scheduling 
       handles    data distribution    
o    moves the process to the data 
       handles synchronization 
       handles faults 
       everything happens on top of a distributed fs (later) 

o    gathers, sorts, and shuffles intermediate data 

o    detects worker failures and restarts 

   hello   world   :   word   

count	

map(string input_key, string input_value): 
     // input_key: document name 
     // input_value: document contents 
     for each word w in input_values: 
          emitintermediate(w, "1"); 
 
reduce(string key, iterator intermediate_values): 
     // key: a word, same for input and output 
     // intermediate_values: a list of counts 
     int result = 0; 
     for each v in intermediate_values: 
          result += parseint(v); 
          emit(asstring(result)); 
 

(1) fork 

user 
program 

(1) fork 

master 

(1) fork 

(2) assign map 

(2) assign reduce 

worker 

(3) read 

worker 

(4) local write 

(5) remote read 

worker 

(6) write 

worker 

output 
file 0 

output 
file 1 

worker 

map 
phase 

intermediate files 

(on local disk) 

reduce 
phase 

output 
files 

split 0 
split 1 
split 2 
split 3 
split 4 

input 
files 

redrawn from dean and ghemawat (osdi 2004) 

how   do   we   get   data   to   the   

workers?	

nas 

compute nodes 

san 

what   s the problem here? 

distributed   file   system	
       don   t move data to workers    move workers to the 

data! 
o    store data on the local disks for nodes in the cluster 
o    start up the workers on the node that has the data local 

       why? 

o    not enough ram to hold all the data in memory 
o    disk access is slow, disk throughput is good 

       a distributed file system is the answer 

o    gfs (google file system) 
o    hdfs for hadoop (= gfs clone) 

gfs:   assumptions	
       commodity hardware over    exotic    hardware 
       high component failure rates 

o   

inexpensive commodity components fail all the time 

          modest    number of huge files 
       files are write-once, mostly appended to 

o    perhaps concurrently 

       large streaming reads over random access 
       high sustained throughput over low latency 

gfs slides adapted from material by dean et al. 

gfs:   design   decisions	

       files stored as chunks 
       reliability through replication 

o    fixed size (64mb) 

o    each chunk replicated across 3+ chunkservers 

       single master to coordinate access, keep 

metadata 
o    simple centralized management 

       no data caching 

o    little benefit due to large data sets, streaming reads 

       simplify the api 

o    push some of the issues onto the client 

application 
gsf client 

(file name, chunk index) 

(chunk handle, chunk location) 

gfs master 
file namespace 

/foo/bar 
chunk 2ef0 

(chunk handle, byte range) 

chunk data 

instructions to chunkserver 

chunkserver state 

gfs chunkserver 
linux file system 

gfs chunkserver 
linux file system 

    

    

redrawn from ghemawat et al. (sosp 2003) 

master   s   responsibilities	
       metadata storage 
       namespace management/locking 
       periodic communication with chunkservers 
       chunk creation, replication, rebalancing 
       garbage collection 

questions? 

mapreduce    killer app   : 

graph algorithms 

graph   algorithms:   topics	
      

introduction to graph algorithms and graph 
representations 

       single source shortest path (sssp) problem 

o    refresher: dijkstra   s algorithm 
o    breadth-first search with mapreduce 

       id95 

what   s   a   graph?	

       g = (v,e), where 

o    v represents the set of vertices (nodes) 
o    e represents the set of edges (links) 
o    both vertices and edges may contain additional information 
       different types of graphs: 

o    directed vs. undirected edges 
o    presence or absence of cycles 
o        

some   graph   problems	

       finding shortest paths 
       finding minimum spanning trees 

o    routing internet traffic and ups trucks 

o    telco laying down fiber 
       finding max flow 

o    airline scheduling 
identify    special    nodes and communities 
o    breaking up terrorist cells, spread of swine/avian/    flu 

      
       bipartite matching 
o    monster.com, match.com 

       and of course... id95 

representing   graphs	

       g = (v, e) 
       two common representations 

o    a poor representation for computational purposes 

o    adjacency matrix 
o    adjacency list 

adjacency   matrices	

represent a graph as an n x n square matrix m 

o    n = |v| 
o    mij = 1 means a link from node i to j 

1  2  3  4 
1  0  1  0  1 
2  1  0  1  1 
3  1  0  0  0 
4  1  0  1  0 

2	

1	

3	

4	

adjacency   lists	

take adjacency matrices    and throw away all the 

zeros 

1  2  3  4 
1  0  1  0  1 
2  1  0  1  1 
3  1  0  0  0 
4  1  0  1  0 

1: 2, 4 
2: 1, 3, 4 
3: 1 
4: 1, 3 

adjacency   lists:   critique	
       advantages: 

o    much more compact representation 
o    easy to compute over outlinks 
o    graph structure can be broken up and distributed 

       disadvantages: 

o    much more difficult to compute over inlinks 

single   source   shortest   

path	

       problem: find shortest path from a source node to 

one or more target nodes 

       first, a refresher: dijkstra   s algorithm 

dijkstra   s   algorithm   

example	

0	

10 

5 

   	

1 

   	

2 

3 

9 

4 

6 

7 

2 

   	

   	

example from clr 

dijkstra   s   algorithm   

example	

0	

10 

5 

10	

1 

   	

2 

3 

9 

4 

6 

7 

2 

   	

5	

example from clr 

dijkstra   s   algorithm   

example	

0	

10 

5 

8	

1 

14	

2 

3 

9 

4 

6 

7 

2 

7	

5	

example from clr 

dijkstra   s   algorithm   

example	

0	

10 

5 

8	

1 

13	

2 

3 

9 

4 

6 

7 

2 

7	

5	

example from clr 

dijkstra   s   algorithm   

example	

0	

10 

5 

8	

1 

9	

2 

3 

9 

4 

6 

7 

2 

7	

5	

example from clr 

dijkstra   s   algorithm   

example	

0	

10 

5 

8	

1 

9	

2 

3 

9 

4 

6 

7 

2 

7	

5	

example from clr 

single   source   shortest   

path	

       problem: find shortest path from a source node to 

one or more target nodes 

       single processor machine: dijkstra   s algorithm 
       mapreduce: parallel breadth-first search (bfs) 

finding   the   shortest   path	
       first, consider equal edge weights 
       solution to the problem can be defined inductively 
       here   s the intuition: 
o    distanceto(startnode) = 0 
o    for all nodes n directly reachable from startnode,  

distanceto(n) = 1 

o    for all nodes n reachable from some other set of nodes s, distanceto(n) = 

1 + min(distanceto(m), m     s) 

from   intuition   to   

algorithm	

       a map task receives 

o    key: node n 
o    value: d (distance from start), points-to (list of nodes reachable from n) 

          p     points-to: emit (p, d+1) 
       the reduce task gathers possible distances to a 

given p and selects the minimum one 

multiple   iterations   

needed	

       this mapreduce task advances the    known 

frontier    by one hop 
o    subsequent iterations include more reachable nodes as frontier advances 
o    multiple iterations are needed to explore entire graph 
o    feed output back into the same mapreduce task 

       preserving graph structure: 

o    problem: where did the points-to list go? 
o    solution: mapper emits (n, points-to) as well 

visualizing   parallel   bfs	

3	

1	

2	

2	

3	

2	

3	

4	

3	

4	

termination	

       does the algorithm ever terminate? 

o    eventually, all nodes will be discovered, all edges will be considered (in a 

connected graph) 

       when do we stop? 

weighted   edges	

       now add positive weights to the edges 
       simple change: points-to list in map task includes a 

weight w for each pointed-to node 
o    emit (p, d+wp) instead of (p, d+1) for each node p 

       does this ever terminate? 

o    yes! eventually, no better distances will be found. when distance is the 

o    mapper should emit (n, d) to ensure that    current distance    is carried into 

same, we stop 

the reducer 

comparison   to   dijkstra	

       dijkstra   s algorithm is more efficient  

o    at any step it only pursues edges from the minimum-cost path inside the 

frontier 

       mapreduce explores all paths in parallel 

o    divide and conquer 
o    throw more hardware at the problem 

general   approach	
       mapreduce is adept at manipulating graphs 
       graph algorithms with for mapreduce: 

o    store graphs as adjacency lists 

o    each map task receives a node and its outlinks 
o    map task compute some function of the link structure, emits value with 

target as the key 

      

o    reduce task collects keys (target nodes) and aggregates 
iterate multiple mapreduce cycles until some 
termination condition 
o    remember to    pass    graph structure from one iteration to next 

random   walks   over   the   

web	

       model: 

o    user starts at a random web page 
o    user randomly clicks on links, surfing from page to page 

       id95 = the amount of time that will be spent 

on any given page 

id95:   de   ned	

given page x with in-bound links t1   tn, where 

o    c(t) is the out-degree of t 
o       is id203 of random jump 
o    n is the total number of nodes in the graph 

xpr
)(

      =
  
      
      

1
n

n

   

1
=

i

tpr
)(
i
tc
)(
i

      
1(
   +      
      

)
  

t1	

x	

t2	

    

tn	

computing   id95	

       properties of id95 
o    can be computed iteratively 
o    effects at each iteration is local 

       sketch of algorithm: 
o    start with seed pri values 
o    each page distributes pri    credit    to all pages it links to 
o    each target page adds up    credit    from multiple in-bound links to 

compute pri+1 
iterate until values converge 

o   

id95   in   mapreduce	

map: distribute id95    credit    to link targets 

reduce: gather up id95    credit    from multiple sources 
to compute new id95 value 

iterate until 
convergence 

... 

id95:   issues	

      

is id95 guaranteed to converge? how 
quickly? 

       what is the    correct    value of    , and how sensitive is 

the algorithm to it? 

       what about dangling links? 
       how do you know when to stop? 

graph   algorithms   in   

mapreduce	

       general approach 

o    store graphs as adjacency lists (node, points-to, points-to    ) 
o    mappers receive (node, points-to*) tuples 
o    map task computes some function of the link structure 
o    output key is usually the target node in the adjacency list representation 
o    mapper typically outputs the graph structure as well 
iterate multiple mapreduce cycles until some 
convergence criterion is met 

      

questions? 

mapreduce algorithm design 

managing   dependencies	
       remember: mappers run in isolation 
o    you have no idea in what order the mappers run 
o    you have no idea on what node the mappers run 
o    you have no idea when each mapper finishes 

       tools for synchronization: 

o    ability to hold state in reducer across multiple key-value pairs 
o    sorting function for keys 
o    partitioner 
o    cleverly-constructed data structures 

motivating   example	
       term co-occurrence matrix for a text collection 

o    m = n x n matrix (n = vocabulary size) 
o    mij: number of times i and j co-occur in some context  

(for concreteness, let   s say context = sentence) 

       why? 

o    distributional profiles as a way of measuring semantic distance 
o    semantic distance useful for many language processing tasks 

   you shall know a word by the company it keeps    (firth, 1957) 

e.g., mohammad and hirst (emnlp, 2006) 

mapreduce:   large   
counting   problems	
       term co-occurrence matrix for a text collection 
= specific instance of a large counting problem 
o    a large event space (number of terms) 
o    a large number of events (the collection itself) 
o    goal: keep track of interesting statistics about the events 

       basic approach 

o    mappers generate partial counts 
o    reducers aggregate partial counts 

how do we aggregate partial counts efficiently? 

first   try:      pairs   	

       each mapper takes a sentence: 

o    generate all co-occurring term pairs 
o    for all pairs, emit (a, b)     count 

       reducers sums up counts associated with these 

pairs 

       use combiners! 

note: in these slides, we donate a key-value pair as k     v 

   pairs      analysis	

       advantages 
       disadvantages 

o    easy to implement, easy to understand 

o    lots of pairs to sort and shuffle around (upper bound?) 

another   try:      stripes   	

idea: group together pairs into an associative array 

      

(a, b)     1  
(a, c)     2  
(a, d)     5  
(a, e)     3  
(a, f)     2  

a     { b: 1, c: 2, d: 5, e: 3, f: 2 } 

       each mapper takes a sentence: 

o    generate all co-occurring term pairs 
o    for each term, emit a     { b: countb, c: countc, d: countd     } 

       reducers perform element-wise sum of associative 

arrays 

+ 

a     { b: 1,         d: 5, e: 3 } 
a     { b: 1, c: 2, d: 2,         f: 2 } 
a     { b: 2, c: 2, d: 7, e: 3, f: 2 } 

   stripes      analysis	

       advantages 

o    far less sorting and shuffling of key-value pairs 
o    can make better use of combiners 

       disadvantages 

o    more difficult to implement 
o    underlying object is more heavyweight 
o    fundamental limitation in terms of size of event space 

cluster size: 38 cores 
data source: associated press worldstream (apw) of the english gigaword corpus (v3), 
which contains 2.27 million documents (1.8 gb compressed, 5.7 gb uncompressed) 

relative   frequency   

estimates	

       how do we compute relative frequencies from 

counts? 

|

)

=

abp
(
)

ba
,
a
(
)

count
(
count

count
(
count
   
       why do we want to do this? 
       how do we do this with mapreduce? 

ba
)
,
ba
(
,

)'

=

b

'

p(b|a):      pairs      	

(a, *)     32  
(a, b1)     3  
(a, b2)     12  
(a, b3)     7 
(a, b4)     1  
    

reducer holds this value in memory 

(a, b1)     3 / 32  
(a, b2)     12 / 32 
(a, b3)     7 / 32 
(a, b4)     1 / 32 
    

       for this to work: 

o    must emit extra (a, *) for every bn in mapper 
o    must make sure all a   s get sent to same reducer (use partitioner) 
o    must make sure (a, *) comes first (define sort order) 
o    must hold state in reducer across different key-value pairs 

p(b|a):      stripes      	

a      {b1:3, b2 :12, b3 :7, b4 :1,     } 

       easy! 

o    one pass to compute (a, *) 
o    another pass to directly compute f(b|a) 

synchronization   in   

mapreduce	

       approach 1: turn synchronization into an ordering 

problem 
o    sort keys into correct order of computation 
o    partition key space so that each reducer gets the appropriate set of 

partial results 

o    hold state in reducer across multiple key-value pairs to perform 

computation 
illustrated by the    pairs    approach 

o   

       approach 2: construct data structures that    bring 

the pieces together    
o    each reducer receives all the data it needs to complete the computation 
o   

illustrated by the    stripes    approach 

issues   and   tradeo   s	

       number of key-value pairs 

o    object creation overhead 
o    time for sorting and shuffling pairs across the network 
o   

in hadoop, every object emitted from a mapper is written to disk 

       size of each key-value pair 

o    de/serialization overhead 

       combiners make a big difference! 

o    ram vs. disk and network 
o    arrange data to maximize opportunities to aggregate partial results 

batch learning algorithms in mr 

batch   learning   
algorithms	

       expectation maximization 

o    gaussian mixtures / id116 
o    forward-backward learning for id48s 

       gradient-ascent based learning 

o    computing (gradient, objective) using mapreduce 
o    optimization questions 

k-        means   on   mr	

       until convergence 

o    distribute parameters (centroids) to all worker nodes 
o    mappers 

       assign labels to each data point 
       produce a (label, point) output 

o    reducers 

       compute centroids for each label 
       can be as many reducers as cardinality of label set 

       every id116 iteration is one map-reduction 

em   assumptions	

       e-step 

       m-step 

o    compute posterior distribution over latent variable 
o   

in the case of id116, that   s the class label, given the data and the 
parameters 

o    solving an optimization problem given the posteriors over latent variables 
o    id116, id48s, pid18s: analytic solutions 
o    not always the case 

em   algorithms   in   

mapreduce	

e step (mappers) 

compute the posterior distribution over the latent variables y and 
the current parameters 

   (t)

e[y] = p(y | x,    (t))

m step (reducer) 

   (t+1)   arg max

    l(x, e[y])

em   algorithms   in   

mapreduce	

e step (mappers) 

compute the posterior distribution over the latent variables y and 
the current parameters 

   (t)

e[y] = p(y | x,    (t))

cluster labels 

data points 

m step (reducer) 

   (t+1)   arg max

    l(x, e[y])

em   algorithms   in   

mapreduce	

e step (mappers) 

compute the posterior distribution over the latent variables y and 
the current parameters 

   (t)

e[y] = p(y | x,    (t))

state sequence 

words 

m step (reducer) 

   (t+1)   arg max

    l(x, e[y])

em   algorithms   in   

mapreduce	

e step 

compute the expected log likelihood with respect to the 
conditional distribution of the latent variables with respect to the 
observed data. 

e[y] = p(y | x,    (t))

expectations are just sums of function evaluation over an event 
times that event   s id203: perfect for mapreduce! 

mappers compute model likelihood given small pieces of the 
training data (scale em to large data sets!) 

em   algorithms   in   

mapreduce	

m step 

   (t+1)   arg max

    l(x, e[y])

the solution to this problem depends on the parameterization 
used. for id48s, pid18s with multinomial parameterizations, this is 
just computing the relative frequency. 

       easy! 

o    one pass to compute (a, *) 
o    another pass to directly compute f(b|a) 

a      {b1:3, b2 :12, b3 :7, b4 :1,     } 

challenges	

       each iteration of em is one mapreduce job 
       mappers require the current model parameters 

o    certain models may be very large 
o    optimization: any particular piece of the training data probably depends 

on only a small subset of these parameters 

       reducers may aggregate data from many 

mappers 
o    optimization: make smart use of combiners! 

log-        linear   models	

       nlp   s favorite discriminative model: 

       applied successfully to classificiation, id52, 

parsing, mt, id40, named entity 
recognition, lm    
o    make use of millions of features (hi   s) 
o    features may overlap 
o    global optimum easily reachable, assuming no latent variables 

exponential   models   in   

mapreduce	

       training is usually done to maximize likelihood 

(minimize negative llh), using first-order methods 
o    need an objective and gradient with respect to the parameterizes that 

we want to optimize 

exponential   models   in   

mapreduce	

       how do we compute these in mapreduce? 

as seen with em: expectations map nicely onto the mr paradigm. 

each mapper computes two quantities: the llh of a 
training instance <x,y> under the current model and the 
contribution to the gradient. 

exponential   models   in   

mapreduce	

       what about reducers? 

the objective is a single value     make sure to use a combiner! 

the gradient is as large as the feature space     but may be quite 
sparse.  make use of sparse vector representations! 

exponential   models   in   

mapreduce	

       after one mr pair, we have an objective and 

gradient 

       run some optimization algorithm 

o    lbfgs, id119, etc    
       check for convergence 
      

if not, re-run mr to compute a new objective and 
gradient 

challenges	

       each iteration of training is one mapreduce job 
       mappers require the current model parameters 
       reducers may aggregate data from many 

mappers 

       optimization algorithm (lbfgs for example) may 

require the full gradient 
o    this is okay for millions of features 
o    what about billions? 
o       or trillions? 

case study:  

id151 

statistical   machine   

translation	

       conceptually simple: 

(translation from foreign f into english e) 

  
e

=

arg

epefp
(
)(

max
       difficult in practice! 
       phrase-based machine translation (pbmt) : 

)|

e

o    break up source sentence into little pieces (phrases) 
o    translate each phrase individually 

dyer et al. (third acl workshop on mt, 2008) 

maria 

mary 

no 

not 

did not 

no 

dio 

una 

bofetada 

give 

a 

slap 

a slap 

slap 

did not give 

a 

to 

by 

la 

the 

bruja 

verde 

witch 

green 

green witch 

to the 

to 

the 

slap 

the witch 

example from koehn (2006) 

mt   architecture	

training data 

word alignment 

phrase extraction 

i saw the small table 
vi la mesa peque  a 

parallel sentences 

he sat at the table 
the service was good 
target-language text 

(vi, i saw) 
(la mesa peque  a, the small table) 
    

language   

model	

translation   

model	

decoder	

m  s de vino, por favor! 
foreign input sentence 

more wine, please! 
english output sentence 

the   data   bo]leneck	

mt   architecture	

there are mapreduce implementations of 
these two components! 

training data 

word alignment 

phrase extraction 

i saw the small table 
vi la mesa peque  a 

parallel sentences 

he sat at the table 
the service was good 
target-language text 

(vi, i saw) 
(la mesa peque  a, the small table) 
    

language   

model	

translation   

model	

decoder	

maria no daba una bofetada a la bruja verde 

foreign input sentence 

mary did not slap the green witch 

english output sentence 

alignment   with   id48s	

mary deu um tapa a bruxa verde 

1 

2 

3 

4 

5 

6 

7 

mary 

slapped 

the 

green  witch 

vogel et al. (1996) 

alignment   with   id48s	

mary deu um tapa a bruxa verde 

1 

2 

3 

4 

5 

6 

7 

1	

4	

5	

7	

6	

mary 

slapped 

the 

green  witch 

alignment   with   id48s	
       emission parameters: translation probabilities 
       states: words in source sentence 
       transition probabilities: id203 of jumping +1, +2, 

       alternative parameterization of state probabilities: 

independent of previous alignment decision, dependent only on global 
position in sentence (   model 2   ) 

       this is still state-of-the-art in machine translation 
       how many parameters are there? 

+3, -1, -2, etc. 

o    uniform (   model 1   ) 
o   

o    many other models    

id48   alignment:   giza	

single-core commodity server 

id48   alignment:   

mapreduce	

single-core commodity server 

38 processor cluster 

id48   alignment:   

mapreduce	

38 processor cluster 

1/38 single-core commodity server 

online learning algorithms in mr 

online   learning   algorithms	
       mapreduce is a batch processing system 

o    associativity used to factor large computations into subproblems that can 

be solved 

o    once job is running, processes are independent 

       online algorithms 

o    great deal of research currently in ml 
o    theory: strong convergence, mistake guarantees 
o    practice: rapid convergence 
o    good (empirical) performance on nonconvex problems 
o    problems 

       parameter updates made sequentially after each training instance 
       theoretical analysis rely on sequential model of computation 
       how do we parallelize this? 

review:   id88	

collins (2002), rosenblatt (1957) 

review:   id88	

collins (2002), rosenblatt (1957) 

assume   separability	

algorithm   1:   parameter   mixing	
inspired by the averaged id88, let   s run p 
id88s on shards of the data 

      

       return the average parameters 

return average parameters from p runs 

mcdonald, hall, & mann (naacl, 2010) 

full data 

shard 1 

shard 2 

processor 1 

processor 2 

shard 1 

shard 2 

a]empt   1:   parameter   mixing	

       unfortunately    
       theorem (proof by counterexample). for any training set 

t separable by margin   ,  parammix does not 
necessarily return a separating hyperplane. 

serial	
1/p   -           serial	
parammix	

id88	
85.8	
75.3	
81.5	

avg.   id88	
88.8	
76.6	
81.6	

researchers at carnegie mellon university in pittsburgh, 
pa, have put together an ios app, drawafriend,     

algorithm   2:   iterative   mixing	
       rather than mixing parameters just once, mix after 

each epoch (pass through a shard) 

       redistribute parameters and use them as a starting 

point on next epoch 

algorithm   2:   analysis	
       theorem. the weighted number of mistakes has is 

bound (like the sequential id88) by 
o    the worst-case number of epochs (through the full data) is the same as 

the sequential id88 

o    with non-uniform mixing, it is possible to obtain a speedup of p 

serial	
1/p   -           serial	
parammix	
iterparammix	

id88	
85.8	
75.3	
81.5	
87.9	

avg.   id88	
88.8	
76.6	
81.6	
88.1	

algorithm   2:   empirical	

summary	

       two approaches to learning 

o    batch: process the entire dataset and make a single update 
o    online: sequential updates 

       online learning is hard to realize in a distributed 

(parallelized) environment 
o    parameter mixing approaches offer a theoretically sound solution with 

practical performance guarantees 

o    generalizations to log-loss and mira loss have also been explored 
o    conceptually some workers have    stale parameters    that eventually 

(after a number of synchronizations) reflect the true state of the learner 

       other challenges (next couple of days) 

o    large numbers of parameters (randomized representations) 
o    large numbers of related tasks (multitask learning) 

obrigado!	

