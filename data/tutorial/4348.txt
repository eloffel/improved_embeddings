deep	
   learning	
   

lxmls	
   2015	
   

lisbon,	
   portugal	
   

	
   	
   

	
   

lisbon	
   machine	
   learning	
   summer	
   school	
   

yoshua	
   bengio	
   	
   
july	
   23,	
   2015	
   

	
   

outline of the tutorial 
1.    representa5on	
   learning,	
   mo5va5ons	
   	
   
2.    why	
   does	
   deep	
   learning	
   work	
   so	
   well?	
   
3.    algorithms:	
   backprop,	
   convnets,	
   id56s	
   
4.    unsupervised	
   &	
   genera5ve	
   learning	
   
5.    ajen5on	
   mechanisms	
   

upcoming	
   mit	
   press	
   book:	
      deep	
   learning   	
   http://www.iro.umontreal.ca/~bengioy/dlbook/ 
for	
   dras	
   chapters	
   (in	
   prepara5on)	
   

breakthrough 
      	
   deep	
   learning:	
   machine	
   
learning	
   algorithms	
   based	
   on	
   
learning	
   mulaple	
   levels	
   of	
   
representaaon	
   /	
   abstracaon.	
   

amazing	
   improvements	
   in	
   error	
   rate	
   in	
   object	
   recogni5on,	
   object	
   
detec5on,	
   speech	
   recogni5on,	
   and	
   more	
   recently,	
   in	
   natural	
   
language	
   processing	
   /	
   understanding	
   

	
   

3	
   

ongoing progress: combining vision 
and natural language understanding  
       recurrent	
   nets	
   genera5ng	
   credible	
   sentences,	
   even	
   bejer	
   if	
   

condi5onally:	
   
       machine	
   transla5on	
   
       image	
   2	
   text	
   

xu	
   et	
   al,	
   icml   2015	
   

initial breakthrough in 2006 

canadian	
   iniaaave:	
   cifar	
   

       ability	
   to	
   train	
   deep	
   architectures	
   by	
   

using	
   layer-     wise	
   unsupervised	
   
learning,	
   whereas	
   previous	
   purely	
   
supervised	
   ajempts	
   had	
   failed	
   

       unsupervised	
   feature	
   learners:	
   

      
      
      

rbms	
   
auto-     encoder	
   variants	
   
sparse	
   coding	
   variants	
   

toronto 
hinton 

bengio 
montr  al 

le cun 
new york 

5	
   

2010-2012: breakthrough in speech 
recognition ! in androids by 2012 

100%	
   

10%	
   

4%	
   

2%	
   

1%	
   

according	
   to	
   microsos:	
   

deep	
   learning	
   

1990	
   

2000	
   

2010	
   

breakthrough in id161: 
2012-2015 
       gpus	
   +	
   10x	
   more	
   data	
   

       1000	
   object	
   categories,	
   	
   
       facebook:	
   millions	
   of	
   faces	
   
       2015:	
   

7	
   

deep learning in the news 

researcher dreams up machines  
that learn without humans 
06.27.13 

scientists see promise in 
deep-learning programs 
john markoff 
november 23, 2012 

google	
   taps	
   u	
   
of	
   t	
   professor	
   
to	
   teach	
   
context	
   to	
   
computers	
   
03.11.13	
   
8	
   

it companies are racing into  
deep learning 

why is          

deep learning 
working so well? 

10	
   

automating 
feature discovery 

output

mapping 
from 
features

output

output

output

mapping 
from 
features

mapping 
from 
features

most 
complex 
features

hand-
designed 
program

hand-
designed 
features

features

simplest 
features

input

input

input

input

rule-based
systems

classic
machine
learning

representation

learning

deep
learning

11	
   

learning multiple levels 
of representation 

(lee,	
   largman,	
   pham	
   &	
   ng,	
   nips	
   2009)	
   
(lee,	
   grosse,	
   ranganath	
   &	
   ng,	
   icml	
   2009)	
   	
   
successive	
   model	
   layers	
   learn	
   deeper	
   intermediate	
   representa5ons	
   

	
   

layer	
   3	
   

high-     level	
   

linguis5c	
   representa5ons	
   

parts	
   combine	
   
to	
   form	
   objects	
   

layer	
   2	
   

12	
   
prior:	
   underlying	
   factors	
   &	
   concepts	
   compactly	
   expressed	
   w/	
   mulaple	
   levels	
   of	
   abstracaon	
   
	
   

layer	
   1	
   

google image search: 
different object types represented in the 
same space 

google:	
   
s.	
   bengio,	
   j.	
   
weston	
   &	
   n.	
   
usunier	
   
(ijcai	
   2011,	
   
nips   2010,	
   
jmlr	
   2010,	
   
mlj	
   2010)	
   

machine learning, ai 
& no free lunch 

       three	
   key	
   ingredients	
   for	
   ml	
   towards	
   ai	
   

1.    lots	
   &	
   lots	
   of	
   data	
   
2.    very	
      exible	
   models	
   
3.    powerful	
   priors	
   that	
   can	
   defeat	
   the	
   curse	
   of	
   

dimensionality	
   

14	
   

ultimate goals 
       ai	
   
       needs	
   knowledge	
   
       needs	
   learning	
   

	
   

	
   

	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

(involves	
   priors	
   +	
   op#miza#on/search)	
   

	
   

(guessing	
   where	
   id203	
   mass	
   concentrates)	
   

       needs	
   generalizaaon	
   
       needs	
   ways	
   to	
      ght	
   the	
   curse	
   of	
   dimensionality	
   
(exponen5ally	
   many	
   con   gura5ons	
   of	
   the	
   variables	
   to	
   consider)	
   
       needs	
   disentangling	
   the	
   underlying	
   explanatory	
   factors	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   

	
   

(making	
   sense	
   of	
   the	
   data)	
   

15	
   

ml 101. what we are fighting against:  
the curse of dimensionality 

	
   	
   	
   to	
   generalize	
   locally,	
   
need	
   representa5ve	
   
examples	
   for	
   all	
   
relevant	
   varia5ons!	
   

	
   
classical	
   solu5on:	
   hope	
   
for	
   a	
   smooth	
   enough	
   
target	
   func5on,	
   or	
   
make	
   it	
   smooth	
   by	
   
handcrasing	
   good	
   
features	
   /	
   kernel	
   

not dimensionality so much as 
number of variations 

       theorem:	
   gaussian	
   kernel	
   machines	
   need	
   at	
   least	
   k	
   examples	
   
to	
   learn	
   a	
   func5on	
   that	
   has	
   2k	
   zero-     crossings	
   along	
   some	
   line	
   

(bengio, dellalleau & le roux 2007) 

	
   
	
   
	
   
	
   
	
   
       theorem:	
   for	
   a	
   gaussian	
   kernel	
   machine	
   to	
   learn	
   some	
   
maximally	
   varying	
   func5ons	
   	
   over	
   d	
   inputs	
   requires	
   o(2d)	
   
examples	
   

	
   

why id165s have poor generalization 
       for	
      xed	
   n,	
   the	
   func5on	
   p(next	
   word	
   |	
   last	
   n-     1	
   words)	
   is	
   learned	
   

purely	
   from	
   the	
   instances	
   of	
   the	
   speci   c	
   n-     tuples	
   associated	
   
with	
   each	
   possible	
   (n-     1)-     word	
   context.	
   no	
   generaliza5on	
   to	
   
other	
   sequences	
   of	
   n	
   words	
   and	
   no	
   cross-     generaliza5on	
   
between	
   di   erent	
   n-     tuples!	
   

       with	
   back-     o   	
   /	
   smoothing	
   models,	
   there	
   is	
   some	
   (limited)	
   

generaliza5on	
   arising	
   from	
   shorter	
   n-     grams,	
   for	
   which	
   there	
   is	
   
more	
   data,	
   at	
   the	
   price	
   of	
   less	
   speci   c	
   predic5ons.	
   

the	
   

no	
   sharing,	
   where	
   lots	
   
would	
   be	
   possible	
   

cat	
   

dog	
   

18	
   

sat	
   

on	
   

sat	
   

is	
   

barks	
   

   	
   

putting id203 mass where 
structure is plausible 

       empirical	
   distribu5on:	
   mass	
   at	
   

training	
   examples	
   

       smoothness:	
   spread	
   mass	
   around	
   
      
       guess	
   some	
      structure   	
   and	
   

insu   cient	
   

generalize	
   accordingly	
   

19	
   

bypassing the curse of 
dimensionality 

we	
   need	
   to	
   build	
   composi5onality	
   into	
   our	
   ml	
   models	
   	
   

just	
   as	
   human	
   languages	
   exploit	
   composi5onality	
   to	
   give	
   
representa5ons	
   and	
   meanings	
   to	
   complex	
   ideas	
   

exploi5ng	
   composi5onality	
   gives	
   an	
   exponen5al	
   gain	
   in	
   
representa5onal	
   power	
   

distributed	
   representa5ons	
   /	
   embeddings:	
   feature	
   learning	
   
deep	
   architecture:	
   mul5ple	
   levels	
   of	
   feature	
   learning	
   

prior:	
   composi5onality	
   is	
   useful	
   to	
   describe	
   the	
   
world	
   around	
   us	
   e   ciently	
   

20	
   

	
   

non-distributed representations 

id91	
   

       id91,	
   n-     grams,	
   nearest-     
neighbors,	
   rbf	
   id166s,	
   local	
   
non-     parametric	
   density	
   
es5ma5on	
   &	
   predic5on,	
   
decision	
   trees,	
   etc.	
   

       parameters	
   for	
   each	
   
dis5nguishable	
   region	
   

       #	
   of	
   disanguishable	
   regions	
   
is	
   linear	
   in	
   #	
   of	
   parameters	
   

     	
   no	
   non-     trivial	
   generaliza5on	
   to	
   regions	
   without	
   examples	
   

21	
   

the need for distributed 
representations 
       factor	
   models,	
   pca,	
   rbms,	
   
neural	
   nets,	
   sparse	
   coding,	
   
deep	
   learning,	
   etc.	
   

mul5-     	
   

id91	
   

       each	
   parameter	
   in   uences	
   
many	
   regions,	
   not	
   just	
   local	
   
neighbors	
   
       #	
   of	
   disanguishable	
   regions	
   
grows	
   almost	
   exponenaally	
   
with	
   #	
   of	
   parameters	
   
       generalize	
   non-     locally	
   
to	
   never-     seen	
   regions	
   

22	
   

non-     mutually	
   
exclusive	
   features/
ajributes	
   create	
   a	
   
combinatorially	
   large	
   
set	
   of	
   dis5nguiable	
   
con   gura5ons	
   

c1	
   

c2	
   

c3	
   

input	
   

classical symbolic ai vs 
representation learning 
       two	
   symbols	
   are	
   equally	
   far	
   from	
   each	
   other	
   
       concepts	
   are	
   not	
   represented	
   by	
   symbols	
   in	
   our	
   

brain,	
   but	
   by	
   pajerns	
   of	
   ac5va5on	
   	
   

	
   (connec:onism,	
   1980   s)	
   

geo   rey	
   hinton	
   

output	
   units	
   

hidden	
   units	
   

input	
   
units	
   

23	
   

cat	
   	
   

dog	
   	
   

person	
   	
   

david	
   rumelhart	
   

neural language models: fighting one 
exponential by another one! 
      

(bengio	
   et	
   al	
   nips   2000)	
   

i   th output = p(w(t)  = i | context)

output

. . .

softmax

. . .

exponen5ally	
   large	
   set	
   of	
   
generaliza5ons:	
   seman5cally	
   close	
   
sequences	
   

most  computation here

. . .

tanh

. . .

r(w1)

r(w2)

r(w3)

r(w4)

r(w5)

r(w6)

c(w(t   n+1))
. . .

c(w(t   2)) c(w(t   1))

. . .

. . .

. . .

table
look   up
in

c

matrix

c

shared parameters
across words

w1 w2 w3 w4 w5 w6

index for w(t   n+1)

index for w(t   2)

index for w(t   1)

input sequence

24	
   

exponen5ally	
   large	
   set	
   of	
   possible	
   contexts	
   

neural id27s: visualization 
directions = learned attributes 

25	
   

analogical representations for free 
(mikolov et al, iclr 2013) 
       seman5c	
   rela5ons	
   appear	
   as	
   linear	
   rela5onships	
   in	
   the	
   space	
   of	
   

learned	
   representa5ons	
   

       king	
      	
   queen	
      	
   	
   man	
      	
   woman	
   
       paris	
      	
   france	
   +	
   italy	
      	
   rome	
   

france	
   

italy	
   

paris	
   

rome	
   

26	
   

the next challenge: rich semantic 
representations for word sequences 

      

impressive	
   progress	
   in	
   
capturing	
   word	
   seman5cs	
   
easier	
   learning:	
   non-     parametric	
   
(table	
   look-     up)	
   

       op5miza5on	
   challenge	
   for	
   

mapping	
   sequences	
   to	
   rich	
   &	
   
complete	
   representa5ons	
   
       good	
   test	
   case:	
   machine	
   

transla5on	
   with	
   auto-     encoder	
   
framework	
   

27	
   

a semantic challenge: 
end-to-end machine translation 
       classical	
   machine	
   transla5on:	
   several	
   models	
   separately	
   trained	
   
by	
   max.	
   likelihood,	
   brought	
   together	
   with	
   logis5c	
   regression	
   on	
   
top,	
   based	
   on	
   n-     grams	
   

       neural	
   language	
   models	
   already	
   shown	
   to	
   outperform	
   n-     gram	
   

models	
   in	
   terms	
   of	
   generaliza5on	
   power	
   

       why	
   not	
   train	
   a	
   neural	
   transla5on	
   model	
   end-     to-     end	
   to	
   es5mate	
   

p(target	
   sentence	
   |	
   source	
   sentence)?	
   	
   

28	
   

encoder-decoder framework 
      

intermediate	
   representa5on	
   of	
   meaning	
   	
   
=	
      universal	
   representa5on   	
   

       encoder:	
   from	
   word	
   sequence	
   to	
   sentence	
   representa5on	
   
       decoder:	
   from	
   representa5on	
   to	
   word	
   sequence	
   distribu5on	
   

english	
   sentence	
   

english	
   sentence	
   

	
   

a
t
a
d
	
   
t
x
e
t
i
b
	
   
r
o
f

english	
   
decoder	
   

french	
   
encoder	
   

french	
   sentence	
   

	
   

a
t
a
d

	
   
l

a
u
g
n

i
l
i

n
u
	
   
r
o
f

english	
   
decoder	
   

english	
   
encoder	
   

english	
   sentence	
   

(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)

(cid:5)(cid:4)(cid:6)

(cid:5)(cid:3)

(cid:5)(cid:2)

(cid:7)

(cid:1)(cid:2)

(cid:1)(cid:3)

(cid:1)(cid:4)

(cid:13)(cid:14)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)

29	
   

the depth prior can be exponentially  
advantageous 

theore5cal	
   arguments:	
   

2 layers of 

logic gates 
formal neurons 
rbf units 

= universal approximator 

rbms & auto-encoders = universal approximator 

theorems on advantage of depth: 
(hastad et al 86 & 91, bengio et al 2007, 
bengio & delalleau 2011, braverman 2011, 
pascanu et al 2014, montufar et al nips 2014) 

1	
    2	
    3	
   

   	
   

some functions compactly 
represented with k layers may 
require exponential size with 2 
layers 

   	
   

1	
    2	
    3	
   

n	
   

2n 

subroutine1 includes 
subsub1 code and 
subsub2 code and 
subsubsub1 code 

subroutine2 includes 
subsub2 code and 
subsub3 code and 
subsubsub3 code and     

main 

   shallow    computer program 

subsubsub1 

subsubsub2 

subsubsub3 

subsub1 

subsub2 

subsub3 

sub1 

sub2 

sub3 

main 

   deep    computer program 

sharing components in a deep 
architecture 

polynomial	
   expressed	
   with	
   shared	
   components:	
   advantage	
   of	
   
depth	
   may	
   grow	
   exponen5ally	
   	
   
	
   

sum-     product	
   
network	
   

theorems	
   in	
   	
   
(bengio	
   &	
   delalleau,	
   alt	
   2011;	
   
delalleau	
   &	
   bengio	
   nips	
   2011)	
   

new theoretical result: 
expressiveness of deep nets with 
piecewise-linear activation fns 

(pascanu,	
   montufar,	
   cho	
   &	
   bengio;	
   iclr	
   2014)	
   
(montufar,	
   pascanu,	
   cho	
   &	
   bengio;	
   nips	
   2014)	
   
deeper	
   nets	
   with	
   rec5   er/maxout	
   units	
   are	
   exponen5ally	
   more	
   
expressive	
   than	
   shallow	
   ones	
   (1	
   hidden	
   layer)	
   because	
   they	
   can	
   split	
   
the	
   input	
   space	
   in	
   many	
   more	
   (not-     independent)	
   linear	
   regions,	
   with	
   
constraints,	
   e.g.,	
   with	
   abs	
   units,	
   each	
   unit	
   creates	
   mirror	
   responses,	
   
folding	
   the	
   input	
   space:	
   	
   

	
   
	
   

34	
   

a myth is being debunked: local 
minima in neural nets  
! convexity is not needed 

(pascanu,	
   dauphin,	
   ganguli,	
   bengio,	
   arxiv	
   may	
   2014):	
   on	
   the	
   
saddle	
   point	
   problem	
   for	
   non-     convex	
   op:miza:on	
   
(dauphin,	
   pascanu,	
   gulcehre,	
   cho,	
   ganguli,	
   bengio,	
   nips   	
   2014):	
   
iden:fying	
   and	
   akacking	
   the	
   saddle	
   point	
   problem	
   in	
   high-     
dimensional	
   non-     convex	
   op:miza:on	
   	
   
(choromanska,	
   hena   ,	
   mathieu,	
   ben	
   arous	
   &	
   lecun	
   2014):	
   the	
   
loss	
   surface	
   of	
   mul:layer	
   nets	
   

      

      

      

35	
   

saddle points 

       local	
   minima	
   dominate	
   in	
   low-     d,	
   but	
   

saddle	
   points	
   dominate	
   in	
   high-     d	
   
       most	
   local	
   minima	
   are	
   close	
   to	
   the	
   

bojom	
   (global	
   minimum	
   error)	
   

36	
   

saddle points during training 
       oscilla5ng	
   between	
   two	
   behaviors:	
   
       slowly	
   approaching	
   a	
   saddle	
   point	
   
       escaping	
   it	
   

37	
   

low index critical points 

choromanska	
   et	
   al	
   &	
   lecun	
   2014,	
      the	
   loss	
   surface	
   of	
   mul:layer	
   nets   	
   
shows	
   that	
   deep	
   rec5   er	
   nets	
   are	
   analogous	
   to	
   spherical	
   spin-     glass	
   models	
   
the	
   low-     index	
   cri5cal	
   points	
   of	
   large	
   models	
   concentrate	
   in	
   a	
   band	
   just	
   
above	
   the	
   global	
   minimum	
   

38	
   

saddle-free optimization  
(pascanu, dauphin, ganguli, bengio 2014) 
       saddle	
   points	
   are	
   attractive	
   for	
   newton   s	
   method	
   
       replace	
   eigenvalues	
     	
   of	
   hessian	
   by	
   |  |	
   
       jus5   ed	
   as	
   a	
   par5cular	
   trust	
   region	
   method	
   

advantage	
   increases	
   
with	
   dimensionality	
   

39	
   

how do humans generalize 
from very few examples? 
       they	
   transfer	
   knowledge	
   from	
   previous	
   learning:	
   

       representa5ons	
   
      

explanatory	
   factors	
   

       previous	
   learning	
   from:	
   unlabeled	
   data	
   	
   

	
   

	
   

	
   	
   	
   	
   	
   

	
   +	
   labels	
   for	
   other	
   tasks	
   

       prior:	
   shared	
   underlying	
   explanatory	
   factors,	
   in	
   

paracular	
   between	
   p(x)	
   and	
   p(y|x)	
   	
   

40	
   

	
   

id72 

       generalizing	
   bejer	
   to	
   new	
   tasks	
   
(tens	
   of	
   thousands!)	
   is	
   crucial	
   to	
   
approach	
   ai	
   

task 1  
output y1 
task	
   a	
   

task 2 
output y2 
task	
   b	
   

task 3  
output y3 
task	
   c	
   

       deep	
   architectures	
   learn	
   good	
   

intermediate	
   representa5ons	
   that	
   
can	
   be	
   shared	
   across	
   tasks	
   

	
   	
   	
   	
   	
   (collobert	
   &	
   weston	
   icml	
   2008,	
   
	
   	
   	
   	
   	
   bengio	
   et	
   al	
   aistats	
   2011)	
   
       good	
   representa5ons	
   that	
   

disentangle	
   underlying	
   factors	
   of	
   
varia5on	
   make	
   sense	
   for	
   many	
   tasks	
   
because	
   each	
   task	
   concerns	
   a	
   
subset	
   of	
   the	
   factors	
   
prior:	
   shared	
   underlying	
   explanatory	
   factors	
   between	
   tasks	
   	
   
	
   

e.g.	
   dic5onary,	
   with	
   intermediate	
   
concepts	
   re-     used	
   across	
   many	
   de   ni5ons	
   

raw input x 

41	
   

sharing statistical strength by semi-
supervised learning 
       hypothesis:	
   p(x)	
   shares	
   structure	
   with	
   p(y|x)	
   

purely	
   
supervised	
   

semi-     	
   
supervised	
   

42	
   

algorithms 

43	
   

simple chain rule 

44	
   

multiple paths chain rule 

45	
   

multiple paths chain rule - general 

   	
   

46	
   

chain rule in flow graph 

flow	
   graph:	
   any	
   directed	
   acyclic	
   graph	
   

	
   node	
   =	
   computa5on	
   result	
   
	
   arc	
   =	
   computa5on	
   dependency	
   

	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   	
   =	
   successors	
   of	
   	
   

   	
   

   	
   

   	
   

47	
   

back-prop in multi-layer net 

   	
   
   	
   

48	
   

error

forward-
prop in 
multi-layer 
net 

out

h2

h1

   	
   
   	
   

w3

w2

w1

49	
   

error

backprop in 
multi-layer 
net: 
 
how outputs 
could change 
to make 
error smaller 

out

h2

h1

   	
   
   	
   

w3

w2

w1

50	
   

error

backprop in 
multi-layer 
net: 
 
how h2 could 
change to 
make error 
smaller 

out

h2

h1

   	
   
   	
   

w3

w2

w1

51	
   

error

backprop in 
multi-layer 
net: 
 
how h1 could 
change to 
make error 
smaller 

out

h2

h1

   	
   
   	
   

w3

w2

w1

52	
   

error

backprop in 
multi-layer 
net: 
 
how w1 could 
change to 
make error 
smaller 

out

h2

h1

   	
   
   	
   

w3

w2

w1

53	
   

back-prop in general flow graph 

single	
   scalar	
   output	
   

1.    fprop:	
   visit	
   nodes	
   in	
   topo-     sort	
   order	
   	
   
2.    bprop:	
   

-         compute	
   value	
   of	
   node	
   given	
   predecessors	
   

	
   -     	
   ini5alize	
   output	
   gradient	
   =	
   1	
   	
   
	
   -     	
   visit	
   nodes	
   in	
   reverse	
   order:	
   

	
   compute	
   gradient	
   wrt	
   each	
   node	
   using	
   	
   

	
   	
   	
   	
   	
   	
    	
   gradient	
   wrt	
   successors	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   

	
   	
   =	
   successors	
   of	
   	
   

   	
   

   	
   

   	
   

54	
   

back-prop in recurrent & recursive 
nets 
       replicate	
   a	
   

zt   1	
   

zt+1	
   

zt	
   

parameterized	
   func5on	
   
over	
   di   erent	
   5me	
   
steps	
   or	
   nodes	
   of	
   a	
   dag	
   	
   

       output	
   state	
   at	
   one	
   

5me-     step	
   /	
   node	
   is	
   used	
   
as	
   input	
   for	
   another	
   
5me-     step	
   /	
   node	
   

xt   1	
   

xt	
   

xt+1	
   

s

vp

vp

np

np

a	
   small	
   
crowd

quietly	
   
enters

det.

adj.

a	
   small	
   crowd	
   
quietly	
   enters	
   
the	
   historic	
   

church
semantic	
   	
   
representations
np

n.

55	
   

the

historic

church

id26 through structure 
       id136	
        	
   discrete	
   choices	
   	
   

       (e.g.,	
   shortest	
   path	
   in	
   id48,	
   best	
   output	
   con   gura5on	
   in	
   crf)	
   

       e.g.	
   max	
   over	
   con   gura5ons	
   or	
   sum	
   weighted	
   by	
   posterior	
   
       the	
   loss	
   to	
   be	
   op5mized	
   depends	
   on	
   these	
   choices	
   
       the	
   id136	
   opera5ons	
   are	
      ow	
   graph	
   nodes	
   
       if	
   con5nuous,	
   can	
   perform	
   stochas5c	
   gradient	
   descent	
   

       max(a,b)	
   is	
   con5nuous.	
   

56	
   

automatic differentiation 

       the	
   gradient	
   computa5on	
   can	
   
be	
   automa5cally	
   inferred	
   from	
   
the	
   symbolic	
   expression	
   of	
   the	
   
fprop.	
   

       each	
   node	
   type	
   needs	
   to	
   know	
   
how	
   to	
   compute	
   its	
   output	
   and	
   
how	
   to	
   compute	
   the	
   gradient	
   
wrt	
   its	
   inputs	
   given	
   the	
   
gradient	
   wrt	
   its	
   output.	
   

       easy	
   and	
   fast	
   prototyping	
   

57	
   

machine learning 101 

f   
   

       family	
   of	
   func5ons	
   
       tunable	
   parameters	
   
       examples	
   z	
   ~	
   unknown	
   data	
   genera5ng	
   distribu5on	
   p(z)	
   
       loss	
   l	
   maps	
   z	
   and	
   	
   	
   	
   	
   	
   	
   	
   	
   to	
   a	
   scalar	
   	
   	
   	
   
       regularizer	
   r	
   (typically	
   on	
   depends	
   on	
   	
   	
   	
   	
   	
   but	
   possibly	
   also	
   on	
   z)	
   
       training	
   criterion:	
   

f   

   

c(   ) = averagez   datasetl(f   , z) + r(   , z)

       approximate	
   minimiza5on	
   algorithm	
   to	
   search	
   for	
   good	
   	
   
   
       supervised	
   learning:	
   

       z=(x,y)	
   and	
   	
   	
   

l = l(f   (x), y )

58	
   

p (y |x)

  log p (y |x)

log-likelihood for neural nets 
       es5ma5ng	
   a	
   condi5onal	
   id203	
   
       parametrize	
   it	
   by	
   
       loss	
   =	
   	
   
       e.g.	
   gaussian	
   y,	
   
  
	
   	
   	
   	
   	
   typically	
   only	
   	
   	
   	
   	
   	
   	
   	
   is	
   the	
   network	
   output,	
   depends	
   on	
   x	
   
	
   	
   	
   	
   	
   equivalent	
   to	
   mse	
   criterion:	
   	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   loss	
   =	
   
       e.g.	
   mul5noulli	
   y	
   for	
   classi   ca5on,	
   	
   
	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   loss	
   =	
   	
   

p (y |x) = p (y |! = f   (x))
! = (  ,  )

!i = p (y = i|x) = f   ,i(x) = softmaxi(a(x))

  log p (y |x) = log   + ||f   (x)   y ||2/ 2

  log !y =   log f   ,y (x)

59	
   

multiple output variables 
      
  log p (y |x) =   log p (y1, . . . yk|x) =   logyi
p (yi|x) =  xi
       likelihood	
   if	
   some	
   yi   s	
   are	
   missing:	
   just	
   ignore	
   those	
   losses	
   

if	
   they	
   are	
   condi5onally	
   independent	
   (given	
   x),	
   the	
   individual	
   
predic5on	
   losses	
   add	
   up:	
   

log p (yi|x)

if	
   not	
   condi5onally	
   independent,	
   need	
   to	
   capture	
   the	
   
condi5onal	
   joint	
   distribu5on	
   
       example:	
   output	
   =	
   image,	
   sentence,	
   tree,	
   etc.	
   
p (y1, . . . yk|x)
       similar	
   to	
   unsupervised	
   learning	
   problem	
   of	
   capturing	
   joint	
   
       exact	
   likelihood	
   may	
   similarly	
   be	
   intractable,	
   depending	
   on	
   
model	
   

      

60	
   

deep supervised neural nets, rectifiers 
       now	
   can	
   train	
   them	
   even	
   without	
   

unsupervised	
   pre-     training:	
   	
   
be]er	
   iniaalizaaon	
   and	
   non-     
lineariaes	
   (rec5   ers,	
   maxout),	
   
	
   
	
   
	
   generalize	
   well	
   with	
   large	
   labeled	
   
sets	
   and	
   regularizers	
   (dropout)	
   

(glorot	
   &	
   bengio	
   aistats	
   2011;	
   
goodfellow	
   et	
   al	
   icml	
   2013	
   )	
   

       unsupervised	
   pre-     training:	
   	
   

rare	
   classes,	
   transfer,	
   smaller	
   
labeled	
   sets,	
   or	
   as	
   extra	
   
regularizer.	
   

61	
   

recurrent neural networks 
       selec5vely	
   summarize	
   an	
   input	
   sequence	
   in	
   a	
      xed-     size	
   state	
   

vector	
   via	
   a	
   recursive	
   update	
   

f   

unfold

s

x

62	
   

st 1

st

st+1

f   

f   

f   

xt 1

xt

xt+1

recurrent neural networks 
       can	
   produce	
   an	
   output	
   at	
   each	
   5me	
   step:	
   unfolding	
   the	
   graph	
   

tells	
   us	
   how	
   to	
   back-     prop	
   through	
   5me.	
   
o
ot 1

v w
s

u
x

63	
   

unfold

v

w

st 1
w

u
xt 1

ot

v

st

u
xt

ot+1

v

st+1

w

w

u
xt+1

generative id56s 
       an	
   id56	
   can	
   represent	
   a	
   fully-     connected	
   directed	
   genera5ve	
   

model:	
   every	
   variable	
   predicted	
   from	
   all	
   previous	
   ones.	
   

lt 1

lt

lt+1

v

w

ot 1
st 1
w

ot

v

st

ot+1

v

st+1

w

w

u
xt 1

u
xt

64	
   

u
xt+1

xt+2

temporal & spatial inputs: 
convolutional & recurrent nets 
      
       sharing	
   weights	
   across	
   5me/space	
   (transla5on	
   equivariance)	
   
       pooling	
   (transla5on	
   invariance,	
   cross-     channel	
   pooling	
   for	
   learned	
   invariances)	
   

local	
   connec5vity	
   across	
   5me/space	
   

zt-     1	
   

zt	
   

zt+1	
   

xt-     1	
   

xt	
   

xt+1	
   

recurrent	
   nets	
   (id56s)	
   can	
   summarize	
   
informa5on	
   from	
   the	
   past	
   
bidirec5onal	
   id56s	
   also	
   summarize	
   
informa5on	
   from	
   the	
   future	
   

65	
   

convolution = sparse connectivity + 
parameter sharing 

s1

s2

s3

s4

s5

s1

s2

s3

s4

s5

x1

x2

x3

x4

x5

x1

x2

x3

x4

x5

sparse	
   

dense	
   

s1

s2

s3

s4

s5

s1

s2

s3

s4

s5

x1

x2

x3

x4

x5

x1

x2

x3

x4

x5

shared	
   

not	
   shared	
   

66	
   

pooling layers 
       aggregate	
   to	
   achieve	
   local	
   invariance	
   
0.3

0.2

1.

1.

1.

...

...

...

1.

...

0.1

1.

0.2

0.1

...

...

0.3

0.1

1.

1.

1.

0.2

...

...

max-     pooling	
   

e   ect	
   of	
   transla5on	
   

       subsampling	
   to	
   reduce	
   temporal/spa5al	
   scale	
   and	
   computa5on	
   

1.

1.

0.2

0.1

0.1

0.1

0.0

0.2

0.1

67	
   

multiple convolutions: feature maps 

68	
   

alternating convolutions & pooling 
      

inspired	
   by	
   visual	
   cortex,	
   idea	
   from	
   fukushima   s	
   neocognitron,	
   
combined	
   with	
   back-     prop	
   and	
   developped	
   by	
   lecun	
   since	
   1989	
   

increasing	
   number	
   of	
   features,	
   decreasing	
   spa5al	
   resolu5on	
   

      
       top	
   layers	
   are	
   fully	
   connected	
   	
   

krizhevsky,	
   sutskever	
   &	
   hinton	
   2012	
   
breakthrough	
   in	
   object	
   recogni5on	
   

69	
   

googlenet:	
   22	
   layers,	
   intermediate	
   targets	
   

convoluaon	
   
pooling	
   
so_max	
   
other	
   

unsupervised or     
semi-supervised deep 

learning &    
generative deep 
learning 

the next challenge: 
unsupervised learning 

       recent	
   progress	
   mostly	
   in	
   supervised	
   dl	
   
       real	
   technical	
   challenges	
   for	
   unsupervised	
   dl	
   
       poten5al	
   bene   ts:	
   

       exploit	
   tons	
   of	
   unlabeled	
   data	
   
       answer	
   new	
   ques5ons	
   about	
   the	
   variables	
   observed	
   
       regularizer	
      	
   transfer	
   learning	
      	
   domain	
   adapta5on	
   
       easier	
   op5miza5on	
   (local	
   training	
   signal)	
   
       structured	
   outputs	
   

72	
   

unsupervised and id21 
challenge + id21 
challenge: deep learning 1st place 

raw	
   data	
   

icml   2011	
   
workshop	
   on	
   
unsup.	
   &	
   
transfer	
   learning	
   

1	
   layer	
   

2	
   layers	
   

3	
   layers	
   

4	
   layers	
   

nips   2011	
   
transfer	
   
learning	
   
challenge	
   	
   
paper:	
   
icml   2012	
   

why latent factors & unsupervised 
representation learning? because of 
causality. 

      

if	
   ys	
   of	
   interest	
   are	
   among	
   the	
   causal	
   factors	
   of	
   x,	
   then	
   

p (y |x) =

p (x|y )p (y )

p (x)

is	
   5ed	
   to	
   p(x)	
   and	
   p(x|y),	
   and	
   p(x)	
   is	
   de   ned	
   in	
   terms	
   of	
   p(x|y),	
   i.e.	
   
       the	
   best	
   possible	
   model	
   of	
   x	
   (unsupervised	
   learning)	
   must	
   

involve	
   y	
   as	
   a	
   latent	
   factor,	
   implicitly	
   or	
   explicitly.	
   

       representa5on	
   learning	
   seeks	
   the	
   latent	
   variables	
   h	
   that	
   

explain	
   the	
   varia5ons	
   of	
   x,	
   making	
   it	
   likely	
   to	
   also	
   uncover	
   y.	
   

	
   	
   

74	
   

invariance and disentangling 
       invariant	
   features	
   

       which	
   invariances?	
   

       alterna5ve:	
   learning	
   to	
   disentangle	
   factors	
   

       good	
   disentangling	
        	
   	
   

	
   avoid	
   the	
   curse	
   of	
   dimensionality	
   

75	
   

emergence of disentangling 

      

      

76	
   

(goodfellow	
   et	
   al.	
   2009):	
   sparse	
   auto-     encoders	
   trained	
   
on	
   images	
   	
   
       some	
   higher-     level	
   features	
   more	
   invariant	
   to	
   
geometric	
   factors	
   of	
   varia5on	
   	
   

(glorot	
   et	
   al.	
   2011):	
   sparse	
   rec5   ed	
   denoising	
   auto-     
encoders	
   trained	
   on	
   bags	
   of	
   words	
   for	
   sen5ment	
   
analysis	
   
       di   erent	
   features	
   specialize	
   on	
   di   erent	
   aspects	
   
(domain,	
   sen5ment)	
   

why?	
   

manifold learning =  

 representation learning 

tangent directions

tangent plane

data on a curved manifold

77	
   

non-parametric manifold learning: 
hopeless without powerful enough priors 

manifolds	
   es5mated	
   out	
   of	
   the	
   
neighborhood	
   graph:	
   	
   
	
   -     	
   node	
   =	
   example	
   
	
   -     	
   arc	
   =	
   near	
   neighbor	
   

ai-     related	
   data	
   manifolds	
   have	
   too	
   many	
   
twists	
   and	
   turns,	
   not	
   enough	
   examples	
   
to	
   cover	
   all	
   the	
   ups	
   &	
   downs	
   &	
   twists	
   

78	
   

auto-encoders learn salient 
variations, like a non-linear pca 

keep	
   varia5ons	
   along	
   manifold.	
   

       minimizing	
   reconstruc5on	
   error	
   forces	
   to	
   
       regularizer	
   wants	
   to	
   throw	
   away	
   all	
   
       with	
   both:	
   keep	
   only	
   sensi5vity	
   to	
   

varia5ons.	
   

varia5ons	
   on	
   the	
   manifold.	
   

79	
   

input	
   point	
   

tangents	
   

mnist	
   

80	
   

input	
   point	
   

tangents	
   

mnist	
   tangents	
   

81	
   

learned tangent prop:  
the manifold tangent classifier 

	
   
makes	
   classi   er	
   f(x)	
   insensi5ve	
   to	
   
varia5ons	
   on	
   manifold	
   at	
   x	
   
	
   
tangent	
   plane	
   characterized	
   by	
   dh(x)/dx	
   
	
   
	
   
	
   
(rifai	
   et	
   al	
   nips   2012)	
   

class	
   1	
   
manifold	
   

df/dx	
   

dh/dx	
   

class	
   2	
   
manifold	
   

bypassing id172 constants 
with generative black boxes 
      

instead	
   of	
   parametrizing	
   p(x),	
   
parametrize	
   a	
   machine	
   which	
   
generates	
   samples	
   

	
   (goodfellow	
   et	
   al,	
   nips	
   2014,	
   
genera5ve	
   adversarial	
   nets)	
   for	
   the	
   
case	
   of	
   ancestral	
   sampling	
   in	
   a	
   deep	
   
genera5ve	
   net.	
   varia5onal	
   auto-     
encoders	
   are	
   closely	
   related.	
   

(bengio	
   et	
   al,	
   icml	
   2014,	
   genera5ve	
   
stochas5c	
   networks),	
   learning	
   the	
   
transi5on	
   operator	
   of	
   a	
   markov	
   chain	
   
that	
   generates	
   the	
   data.	
   

      

      

83	
   

random	
   
numbers	
   

parameters	
   

generated	
   
samples	
   

random	
   
numbers	
   

parameters	
   

previous	
   state	
   

generated	
   
samples	
   

	
   next	
   state	
   

generated	
   
samples	
   

generated	
   
samples	
   

auto-encoders 

p(x|h)	
   

reconstruc,on!r!

decoder.g!

encoder.f!

p(h)	
   

q(h|x)	
   

x	
   

84	
   

probabilisac	
   criterion:	
   
	
   
reconstruc5on	
   log-     likelihood	
   =	
   
	
   
	
   -     	
   log	
   p(x	
   |	
   h)	
   

denoising	
   auto-     encoder:	
   
during	
   training,	
   input	
   is	
   corrupted	
   
stochas5cally,	
   and	
   auto-     encoder	
   must	
   
learn	
   to	
   guess	
   the	
   distribu5on	
   of	
   the	
   
missing	
   informa5on.	
   

code!h!

input!x!

denoising auto-encoder 
       learns	
   a	
   vector	
      eld	
   poin5ng	
   towards	
   

higher	
   id203	
   direc5on	
   (alain	
   &	
   bengio	
   2013)	
   
reconstruction(x)   x !  2 @ log p(x)
       some	
   daes	
   correspond	
   to	
   a	
   kind	
   of	
   
gaussian	
   rbm	
   with	
   regularized	
   score	
   
matching	
   (vincent	
   2011)	
   
	
   	
   	
   	
   	
   [equivalent	
   when	
   noise     0]	
   

corrupted input 

@x

prior:	
   examples	
   
concentrate	
   near	
   a	
   
lower	
   dimensional	
   
   manifold   	
   	
   

corrupted input 

regularized auto-encoders learn a 
vector field that estimates a 
gradient field  (alain	
   &	
   bengio	
   iclr	
   2013)	
   

86	
   

denoising auto-encoder markov chain 

corrupt	
   

~	
   
xt	
   

denoise	
   

~	
   
xt+1	
   

~	
   
xt+2	
   

xt	
   

xt+1	
   

xt+2	
   

87	
   

denoising auto-encoders learn a 
markov chain transition distribution 

(bengio	
   et	
   al	
   nips	
   2013)	
   

88	
   

space-filling in representation-space 
       deeper	
   representaaons	
   "	
   abstracaons	
   "	
   disentangling	
   
       manifolds	
   are	
   expanded	
   and	
      a]ened	
   

x-     space	
   

pixel	
   space	
   

9   s	
   manifold	
    3   s	
   manifold	
   

representa5on	
   space	
   

9   s	
   manifold	
    3   s	
   manifold	
   

linear	
   interpola5on	
   at	
   layer	
   2	
   

3   s	
   manifold	
   

h-     space	
   

9   s	
   manifold	
   

linear	
   interpola5on	
   at	
   layer	
   1	
   

linear	
   interpola5on	
   in	
   pixel	
   space	
   

extracting structure by gradual 
disentangling and manifold unfolding 
(bengio 2014, arxiv 1407.7906)  

	
   

each	
   level	
   transforms	
   the	
   
data	
   into	
   a	
   representa5on	
   in	
   
which	
   it	
   is	
   easier	
   to	
   model,	
   
unfolding	
   it	
   more,	
   
contrac5ng	
   the	
   noise	
   
dimensions	
   and	
   mapping	
   the	
   
signal	
   dimensions	
   to	
   a	
   
factorized	
   (uniform-     like)	
   
distribu5on.	
   
	
   
min kl(q(x, h)||p (x, h))
	
   
for	
   each	
   intermediate	
   level	
   h	
   
90	
   

e
s
i
o
n

p(hl)	
   

q(hl)	
   

signal	
   
gl	
   

fl	
   

   	
   

q(h2|h1)	
   

f2	
   

g2	
   

p(h2|h1)	
   

q(h1)	
   

q(h1|x)	
   

f1	
   

p(h1)	
   

g1	
   

p(x|h1)	
   

q(x)	
   

draw: the latest variant of 
variational auto-encoder 

draw: a recurrent neural network for image generation

(gregor	
   et	
   al	
   of	
   google	
   deepmind,	
   arxiv	
   1502.04623,	
   2015)	
   	
   

       even	
   for	
   a	
   sta5c	
   input,	
   the	
   encoder	
   and	
   decoder	
   are	
   now	
   

karolg@google.com
danihelka@google.com
gravesa@google.com
wierstra@google.com

recurrent	
   nets,	
   which	
   gradually	
   add	
   elements	
   to	
   the	
   answer,	
   
and	
   use	
   an	
   ajen5on	
   mechanism	
   to	
   choose	
   where	
   to	
   do	
   so.	
   
ct  

draw: a recurrent neural network for image generation

write

write

. . .

ct

this paper introduces the deep recurrent atten-
tive writer (draw) neural network architecture
for image generation. draw networks combine
a novel spatial attention mechanism that mimics
the foveation of the human eye, with a sequential
variational auto-encoding framework that allows
for the iterative construction of complex images.
the system substantially improves on the state
of the art for generative models on mnist, and,
when trained on the street view house numbers
dataset, it generates images that cannot be distin-

quence of partial glimpses, or foveations, than by a sin-
gle sweep through the entire image (larochelle & hinton,
2010; denil et al., 2012; tang et al., 2013; ranzato, 2014;
zheng et al., 2014; mnih et al., 2014; ba et al., 2014; ser-
manet et al., 2014). the main challenge faced by sequential
id12 is learning where to look, which can be
addressed with id23 techniques such as
policy gradients (mnih et al., 2014). the attention model in
draw, however, is fully differentiable, making it possible
to train with standard id26. in this sense it re-
sembles the selective read and write operations developed
for the id63 (graves et al., 2014).
the following section de   nes the draw architecture,
along with the id168 used for training and the pro-
cedure for image generation. section 3 presents the selec-
tive attention model and shows how it is applied to read-

figure 1. a trained draw network generating mnist dig-
its. each row shows successive stages in the generation of a sin-
gle digit. note how the lines composing the digits appear to be
   drawn    by the network. the red rectangle delimits the area at-
tended to by the network at each time-step, with the focal preci-

91	
   

a person asked to draw, paint or otherwise recreate a visual
scene will naturally do so in a sequential, iterative fashion,
reassessing their handiwork after each modi   cation. rough
outlines are gradually replaced by precise forms, lines are
sharpened, darkened or erased, shapes are altered, and the
   nal picture emerges. most approaches to automatic im-

p (x|z)
decoder
fnn
z

ct 1
hdec
t 1

sample

encoder
fnn

henc
t 1

decoder
id56

sample

encoder
id56

read

decoder
id56

sample
q(zt+1|x, z1:t)
encoder
id56

read

decoding
(generative model)
encoding
(id136)

figure 2. left: conventional variational auto-encoder. dur-
ing generation, a sample z is drawn from a prior p (z) and passed
through the feedforward decoder network to compute the proba-
bility of the input p (x|z) given the sample. during id136 the
input x is passed to the encoder network, producing an approx-

timexztzt+1p(x|z1:t)xq(zt|x,z1:t 1)xq(z|x)task
100     100 mnist classi   cation
mnist model
svhn model
cifar model

draw samples of svhn images: the 
drawing process 

#glimpses lstm #h
256
256
800
400

8
64
32
64

#z read size write size

-
100
100
200

92	
   

figure 12. generated cifar images. the rightmost column

draw: a recurrent neural network for image generation

draw samples of svhn images: 
generated samples vs training nearest 
neighbor 

nearest	
   training	
   
example	
   for	
   last	
   
column	
   of	
   samples	
   

93	
   

figure 9. generated svhn images. the rightmost column

id3 

       don   t write a formula for p(x), just learn to sample 

directly.	


       no markov chain	

       no variational bound	

       how? by playing a game.	


deep learning workshop, icml 2015 --- ian goodfellow 

94 

adversarial nets framework 
       a game between two players:	


1.    discriminator d 	

2.    generator g	


       d tries to discriminate between: 	


-    a sample from the data distribution. 	

-    and a sample from the generator g.	


       g tries to    trick    d by generating samples that are 

hard for d to distinguish from data.	


deep learning workshop, icml 2015 --- ian goodfellow 

95 

adversarial nets framework 

deep learning workshop, icml 2015 --- ian goodfellow 

96 

zero-sum game 

      

minimax value function:	


discriminator pushes 

up	


discriminator   s ability to 
recognize data as being real 	


generator pushes 

down	


discriminator   s	


ability to recognize generator 

samples as being fake	


deep learning workshop, icml 2015 --- ian goodfellow 

97 

visualization of model samples  

mnist	


tfd	


cifar-10 (fully connected)	


cifar-10 (convolutional)	


deep learning workshop, icml 2015 --- ian goodfellow 

98 

learned 2-d manifold of mnist 

deep learning workshop, icml 2015 --- ian goodfellow 

99 

visualization of model trajectories  

mnist digit dataset	


toronto face dataset 
(tfd)	

deep learning workshop, icml 2015 --- ian goodfellow 

100 

visualization of model trajectories  

cifar-10 
(convolutional)	


deep learning workshop, icml 2015 --- ian goodfellow 

101 

laplacian pyramid 

deep learning workshop, icml 2015 --- ian goodfellow 

102 

(denton + chintala, et al 2015)	


lapgan results 

       40% of samples mistaken by humans for real photos 

deep learning workshop, icml 2015 --- ian goodfellow 

103 

(denton + chintala, et al 2015)	


attention-based neural machine 
translation 
      
      

(bahdanau,	
   cho	
   &	
   bengio,	
   arxiv	
   sept.	
   2014)	
   
(jean,	
   cho,	
   memisevic	
   &	
   bengio,	
   arxiv	
   dec.	
   2014)	
   

related	
   to	
   earlier	
   graves	
   2013	
   for	
   genera5ng	
   handwri5ng	
   

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)
e ui

l
p
m
a
s
s

 
d
r
o
w

t
n
e
r
r
u
c
e
r

e zi

t
a
t
s

m

s
i
n
a
h
c
e

m

n
o
i
t
n
e
t
t

a

n
o
i
t
a
t
o
n
n
a

s
r
o
t
c
e
v

104	
   

attention 
       weight

+

aj(cid:1) =1

a

j

hj

e = (economic, growth, has, slowed, down, in, recent, years, .)

applying an attention mechanism to 
 
- translation 
 
- speech  
 
- images 
 
- video 
 
- memory 

105	
   

encoder-decoder framework 
      

intermediate	
   representa5on	
   of	
   meaning	
   	
   
=	
      universal	
   representa5on   	
   

       encoder:	
   from	
   word	
   sequence	
   to	
   sentence	
   representa5on	
   
       decoder:	
   from	
   representa5on	
   to	
   word	
   sequence	
   distribu5on	
   

english	
   sentence	
   

english	
   sentence	
   

	
   

a
t
a
d
	
   
t
x
e
t
i
b
	
   
r
o
f

english	
   
decoder	
   

french	
   
encoder	
   

french	
   sentence	
   

	
   

a
t
a
d

	
   
l

a
u
g
n

i
l
i

n
u
	
   
r
o
f

english	
   
decoder	
   

english	
   
encoder	
   

english	
   sentence	
   

(cid:8)(cid:9)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)

(cid:5)(cid:4)(cid:6)

(cid:5)(cid:3)

(cid:5)(cid:2)

(cid:7)

(cid:1)(cid:2)

(cid:1)(cid:3)

(cid:1)(cid:4)

(cid:13)(cid:14)(cid:7)(cid:10)(cid:11)(cid:9)(cid:12)

106	
   

encoder & decoder id56 
       need	
   to	
   use	
   gated	
   id56	
   such	
   as	
   lstm	
   or	
   gru	
   

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

ui

ip

e
l
p
m
a
s
s
d
r
o
w

 

y
t
i
l
i
b
a
b
o
r
p
d
r
o
w

 

t
n
e
r
r
u
c
e
r

ez i

t
a
t
s

t
n
e
r
r
u
c
e
r

e hi

t
a
t
s

si

n
o
i
t
a
t
n
e
s
e
r
p
e
r
d
r
o
w

 

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
c

wi

g
n
i
d
o
c
 
k

-
f
o
-
1

107	
   

e = (economic, growth, has, slowed, down, in, recent, years, .)

d
e
c
o
d
e
r

vanilla	
   
architecture	
   

e
n
c
o
d
e
r

bidirectional id56 for input side 
       following	
   alex	
   graves   	
   work	
   on	
   handwri5ng	
   

hi

l
a
n
o
i
t
c
e
r
i
d
i
b

t
n
e
r
r
u
c
e
r

e
t
a
t
s

si

n
o
i
t
a
t
n
e
s
e
r
p
e
r
 
d
r
o
w

e
c
a
p
s
-
s
u
o
u
n
i
t
n
o
c

wi

g
n
i
d
o
c
 
k

-
f
o
-
1

e = (economic, growth, has, slowed, down, in, recent, years, .)

108	
   

attention mechanism for deep learning 
       consider	
   an	
   input	
   (or	
   intermediate)	
   sequence	
   or	
   image	
   
       consider	
   an	
   upper	
   level	
   representa5on,	
   which	
   can	
   choose	
   

  	
   where	
   to	
   look	
     ,	
   by	
   assigning	
   a	
   weight	
   or	
   id203	
   to	
   each	
   
input	
   posi5on,	
   as	
   produced	
   by	
   an	
   mlp,	
   applied	
   at	
   each	
   posi5on	
   

sosmax	
   over	
   lower	
   	
   
loca5ons	
   condi5oned	
   
on	
   context	
   at	
   lower	
   and	
   
higher	
   loca5ons	
   	
   

109	
   

higher-     level	
   

lower-     level	
   

improvements over pure ae model 

30

25

20

15

10

e
r
o
c
s
u
e
l
b

5

0

0

id56search-50
id56search-30
id56enc-50
id56enc-30

10

20

30

40

50

60

figure 2: the id7 scores
of the generated translations
on the test set with respect
to the lengths of the sen-
tences. the results are on
the full
test set which in-
cludes sentences having un-
known words to the models.

sentence length

       id56enc:	
   encode	
   whole	
   sentence	
   
       id56search:	
   predict	
   alignment	
   
       id7	
   score	
   on	
   full	
   test	
   set	
   (including	
   unk)	
   

3.3.2 alignment model
the alignment model should be designed considering that the model needs to be evaluated tx    
ty times for each sentence pair of lengths tx and ty. in order to avoid the potential issue with
computation, we use a single-layer multilayer id88 such that

110	
   

a(si 1, hj) = va tanh (wasi 1 + uahj) ,

where wa 2 rn,n, ua 2 rn,2n and va 2 rn are the weight matrices. since uahj does not depend
on i, we can pre-compute it in advance to minimize the computational cost. a similar trick was

paying 
attention to 
selected parts 
of the image 
while uttering 
words 

(xu	
   et	
   al,	
   arxiv	
   jan.	
   2015,	
   icml	
   2015)	
   

111	
   

speaking about what one sees 

112	
   

show, attend and tell: neural 
image id134 with 
visual attention 

neural image id134 with visual attention

results	
   from	
   (xu	
   et	
   al,	
   arxiv	
   jan.	
   2015,	
   
icml	
   2015)	
   

table 1. id7-1,2,3,4/meteor metrics compared to other methods,     indicates a different split, (   ) indicates an unknown metric,  
indicates the authors kindly provided missing metrics by personal communication,     indicates an ensemble, a indicates using alexnet

dataset

flickr8k

flickr30k

coco

model

google nic(vinyals et al., 2014)      
log bilinear (kiros et al., 2014a) 

soft-attention
hard-attention
google nic       
log bilinear
soft-attention
hard-attention

google nic       
log bilinear 
soft-attention
hard-attention

cmu/ms research (chen & zitnick, 2014)a

ms research (fang et al., 2014)   a

bid56 (karpathy & li, 2014) 

id7

b-2
41
42.4
44.8
45.7
42.3
38
43.4
43.9
   
   
45.1
46.1
48.9
49.2
50.4

b-3
27
27.7
29.9
31.4
27.7
25.4
28.8
29.6
   
   
30.4
32.9
34.4
34.4
35.7

b-1
63
65.6
67
67
66.3
60.0
66.7
66.9
   
   
64.2
66.6
70.8
70.7
71.8

b-4 meteor
   
17.7
19.5
21.3
18.3
17.1
19.1
19.9
   
   
20.3
24.6
24.3
24.3
25.0

   
17.31
18.93
20.30
   
16.88
18.49
18.46
20.41
20.71
   
   
20.03
23.90
23.04

113	
   

randomly sample a length and retrieve a mini-batch of size
64 of that length. we found that this greatly improved con-
vergence speed with no noticeable diminishment in perfor-
mance. on our largest dataset (ms coco), our soft atten-

5.1. data
we report results on the popular flickr8k and flickr30k
dataset which has 8,000 and 30,000 images respectively

the good 

114	
   

and the bad 

115	
   

attention through time for video 
id134 
      

(yao	
   et	
   al	
   arxiv	
   1502.08029,	
   2015)	
   video	
   descrip:on	
   genera:on	
   
incorpora:ng	
   spa:o-     temporal	
   features	
   and	
   a	
   sod-     aken:on	
   
mechanism	
   

       ajen5on	
   can	
   be	
   focused	
   
	
   	
   	
   	
   	
   temporally,	
   i.e.,	
   selec5ng	
   
	
   	
   	
   	
   	
   input	
   frames	
   

    

   

 

       

 

   

 

   

 

features-extraction 

soft-attention 

a 

man 

 

   
caption  

generation 

116	
   

attention through time for video 
id134 (yao et al 2015) 

       ajen5on	
   is	
   focused	
   at	
   

appropriate	
   frames	
   
depending	
   on	
   which	
   
word	
   is	
   generated.	
   

117	
   

attention through time for video 
id134 (yao et al 2015) 
       sos-     ajen5on	
   worked	
   best	
   in	
   this	
   se(cid:130)ng	
   

figure 3. a visualization of where the soft-attentional model    looks at    in a video, while generating the captions (captions included on
the left). each word is mapped into a vector of     in equ. (6). only bars in the same row are comparable, and their height re   ects the
magnitude of    . the model is able to focus its attention on different frames of the video when generating different words in the caption.
best viewed with zooming-in on pdf.

model

feature

non-attention gnet
soft-attention gnet

gnet+3dconvnon-att

gnet+3dconvatt

id7
3
3.4
4.3
3.0
3.1

2
9.2
10.4
7.7
8.2

1
32.0
33.6
31.0
28.2

4
1.2
1.8
1.2
1.3

mb
0.3
0.7
0.3
0.7

meteor

perplexity

4.43
5.73
4.05
5.6

88.28
84.41
66.63
65.44

table 2. attention and 3d-conv performances evaluation on dvs. blue 1-4, multiblue (mb), meteor and perplexity metrics are reported.

have encountered in this dataset is that its captions cover a
much wider domain, rending this task challenging for both
non-attention and id12. according to table 2,
by comparing on perplexity, the id12 improve
consistently upon non-id12. given the same
type of model, using gnet+3dconv. features also steadily

generated	
   
cap5ons	
   

improves upon using gnet features alone. in fact, using
id12 offers about 20 improvement on perplex-
ity upon non-id12. with the same model type,
using the combined features also results better id7 and
meteor, while the effect on id7 and meteor score across
model types is less obvious.

118	
   

attention mechanisms for memory 
access 
       neural	
   turing	
   machines	
   (graves	
   et	
   al	
   2014)	
   
       and	
   memory	
   networks	
   (weston	
   et	
   al	
   2014)	
   
       use	
   a	
   form	
   of	
   ajen5on	
   mechanism	
   to	
   
control	
   the	
   read	
   and	
   write	
   access	
   into	
   a	
   
memory	
   

write	
   

       the	
   ajen5on	
   mechanism	
   outputs	
   a	
   sosmax	
   

over	
   memory	
   loca5ons	
   

       for	
   e   ciency,	
   the	
   sosmax	
   should	
   be	
   sparse	
   
(mostly	
   0   s),	
   e.g.	
   maybe	
   using	
   a	
   hash-     table	
   
formula5on.	
   

read	
   

119	
   

sparse access memory for long-term 
dependencies 
       whereas	
   lstm	
   memories	
   always	
   decay	
   exponen5ally	
   (even	
   if	
   
slowly),	
   a	
   mental	
   state	
   stored	
   in	
   an	
   external	
   memory	
   can	
   stay	
   
for	
   arbitrarily	
   long	
   dura5ons,	
   un5l	
   evoked	
   for	
   read	
   or	
   write.	
   

       need	
   to	
   replace	
   the	
   sos	
   gater	
   or	
   sosmax	
   ajen5on	
   by	
   hard	
   one	
   

that	
   is	
   0	
   most	
   of	
   the	
   5me,	
   and	
   yet	
   for	
   which	
   training	
   works	
   
(again,	
   may	
   use	
   noisy	
   decisions	
   and/or	
   reinforce).	
   

       di   erent	
     	
   threads	
     	
   can	
   run	
   in	
   parallel	
   if	
   we	
   view	
   the	
   memory	
   

as	
   an	
   associa5ve	
   one.	
   

passive	
   copy	
   

access	
   

120	
   

deep learning challenges 
(bengio, arxiv 1305.0445 deep learning 
of representations: looking forward) 
       computa5onal	
   scaling	
   
       op5miza5on	
   &	
   under   (cid:130)ng	
   
       intractable	
   marginaliza5on,	
   approximate	
   
id136	
   &	
   sampling	
   
       disentangling	
   factors	
   of	
   varia5on	
   
       reasoning	
   &	
   one-     shot	
   learning	
   of	
   facts	
   

121	
   

learning multiple levels of 
abstraction 
       the	
   big	
   payo   	
   of	
   deep	
   learning	
   is	
   to	
   allow	
   learning	
   

higher	
   levels	
   of	
   abstrac5on	
   

       higher-     level	
   abstrac5ons	
   disentangle	
   the	
   factors	
   of	
   

varia5on,	
   which	
   allows	
   much	
   easier	
   generaliza5on	
   and	
   
transfer	
   

122	
   

conclusions 

       distributed	
   representaaons:	
   	
   

       prior	
   that	
   can	
   buy	
   exponen5al	
   gain	
   in	
   generaliza5on	
   

       deep	
   composiaon	
   of	
   non-     lineariaes:	
   	
   

       prior	
   that	
   can	
   buy	
   exponen5al	
   gain	
   in	
   generaliza5on	
   

       both	
   yield	
   non-     local	
   generalizaaon	
   
       strong	
   evidence	
   that	
   local	
   minima	
   are	
   not	
   an	
   issue,	
   saddle	
   points	
   
       auto-     encoders	
   capture	
   the	
   data	
   generaang	
   distribuaon	
   

       gradient	
   of	
   the	
   energy	
   
       markov	
   chain	
   genera5ng	
   an	
   es5mator	
   of	
   the	
   dgd	
   
       can	
   be	
   generalized	
   to	
   deep	
   genera5ve	
   models	
   

123	
   

mila: montreal institute for learning algorithms 

