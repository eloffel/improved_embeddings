modeling sequential data 
with recurrent networks

chris dyer
deepmind 

carnegie mellon university

lxmls 2016

july 28, 2016

outline: part i

    neural networks as feature inducers 
    recurrent neural networks 

    application: language models 

    learning challenges and solutions 

    vanishing gradients 
    long short-term memories 

    id149 

    break

outline: part ii

    id56 performance tuning and implementation tricks 

    bidirectional id56s 

    application: better word representations 

    sequence to sequence transduction with id56s 

    applications: machine translation & image id134 

    sequences as matrices and attention 

    application: machine translation

feature induction

  y = wx + b

1
m

mxi=1

2

||  yi   yi||2

f =
in id75, the goal is to learn w and b such that   
f is minimized for a dataset d consisting of m training 
instances. an engineer must select/design x carefully.

feature induction

  y = wx + b

1
m

mxi=1

2

||  yi   yi||2

f =
in id75, the goal is to learn w and b such that   
f is minimized for a dataset d consisting of m training 
instances. an engineer must select/design x carefully.

   nonid75   

h = g(vx + c)
  y = wh + b
use    naive features    x and learn their transformations 
(conjunctions, nonlinear transformation, etc.) into h.

feature induction

h = g(vx + c)
  y = wh + b
    what functions can this parametric form compute? 

    if h is big enough (i.e., enough dimensions), it can 

represent any vector-valued function to any degree of 
precision 

    this is a much more powerful regression model! 
    you can think of h as    induced features    in a linear classi   er 

    the network did the job of a feature engineer

feature induction

h = g(vx + c)
  y = wh + b
    what functions can this parametric form compute? 

    if h is big enough (i.e., enough dimensions), it can 

represent any vector-valued function to any degree of 
precision 

    this is a much more powerful regression model! 
    you can think of h as    induced features    in a linear classi   er 

    the network did the job of a feature engineer

recurrent neural networks

    lots of interesting data is sequential in nature 

    words in sentences 

    dna 

    stock market returns 

        

    how do we represent an arbitrarily long history?

    we will train neural networks to build a representation of these 

arbitrarily big sequences

recurrent neural networks

    lots of interesting data is sequential in nature 

    words in sentences 

    dna 

    stock market returns 

        

    how do we represent an arbitrarily long history?

    we will train neural networks to build a representation of these 

arbitrarily big sequences

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

  y

h

x

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

ht = g(vxt + uht 1 + c)
  yt = wht + b

recurrent nn

  y

h

x

  yt

ht

xt

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

recurrent nn

ht = g(vxt + uht 1 + c)
ht = g(v[xt; ht 1] + c)
  yt = wht + b

  y

h

x

  yt

ht

xt

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

recurrent nn

ht = g(vxt + uht 1 + c)
ht = g(v[xt; ht 1] + c)
  yt = wht + b

ht 1

  y

h

x

  yt

ht

xt

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

how do we train the id56   s parameters?

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

y1

cost1

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

f

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

recurrent neural networks

f

    the unrolled graph is a well-formed (dag) 
computation graph   we can run backprop 

    parameters are tied across time, derivatives are 

aggregated across all time steps  

    this is historically called    id26 

through time    (bptt)

parameter tying

ht = g(vxt + uht 1 + c)
  yt = wht + b

f

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

parameter tying

ht = g(vxt + uht 1 + c)
  yt = wht + b

f

y1

cost1

  y1

h1

x1

h0

u

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

parameter tying

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

  y1

h1

x1

h0

u

parameter tying

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

  y1

h1

x1

h0

u

@f
@u

=

4xt=1

@ht
@u

@f
@ht

parameter tying

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

  y1

h1

x1

h0

u

4xt=1

=

@ht
@u

@f
@ht

@f
@u
parameter tying also came up when learning the    lters   
in convolutional networks (and in the transition matrices   
for id48s!).

parameter tying

    why do we want to tie parameters? 

    reduce the number of parameters to be learned  

    deal with arbitrarily long sequences 

    what if we always have short sequences? 

    maybe you might untie parameters, then. but you 

wouldn   t have an id56 anymore!

what else can we do?

ht = g(vxt + uht 1 + c)
  yt = wht + b

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

f

y4

cost4

  y4

h4

x4

y3

cost3

  y3

h3

x3

   read and summarize   

ht = g(vxt + uht 1 + c)
  y = wh|x| + b
summarize a sequence into a single vector.   
(this will be useful later   )

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

view 2: recursive de   nition

    recall how to construct a list recursively:   
base case   
     [] is a list (the empty list)   
       
induction   
     [t | h] where t is a list and h is an atom is a list 

    id56s de   ne functions that compute representations recursively according 

to this de   nition of a list. 
    de   ne (learn) a representation of the base cases 

    learn a representation of the inductive step 

    anything you can construct recursively, you can obtain an    embedding    of 

with neural networks using this general strategy

view 2: recursive de   nition

    recall how to construct a list recursively:   
base case   
     [] is a list (the empty list)   
       
induction   
     [t | h] where t is a list and h is an atom is a list 

    id56s de   ne functions that compute representations recursively according 

to this de   nition of a list. 
    de   ne (learn) a representation of the base cases 

    learn a representation of the inductive step 

    anything you can construct recursively, you can obtain an    embedding    of 

with neural networks using this general strategy

view 2: recursive de   nition

    recall how to construct a list recursively:   
base case    
     [] is a list (the empty list)   
       
induction   
     [t | h] where t is a list and h is an atom is a list 

    id56s de   ne functions that compute representations recursively according 

to this de   nition of a list. 
    de   ne (learn) a representation of the base case 

    learn a representation of the inductive step 

    anything you can construct recursively, you can obtain an 

   embedding    of with neural networks using this general strategy

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?w

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?b

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?b

p(e) =p(e1)   

p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
      

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?b

p(e) =p(e1)   

p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
      

 istories are sequences of words   
h

example: language model

example: language model

h0

h1

x1

<s>

example: language model

  p1

softmax

h0

h1

x1

<s>

example: language model
p(tom | hsi)

tom

   

  p1

softmax

h0

h1

x1

<s>

example: language model
p(tom | hsi)

tom

   

  p1

softmax

h0

h1

x1

<s>

x2

example: language model
p(tom | hsi)

tom

   

  p1

softmax

softmax

h2

x2

h0

h1

x1

<s>

example: language model
p(tom | hsi)

   p(likes | hsi, tom)

tom

   

  p1

likes

   

softmax

softmax

h0

h1

x1

<s>

h2

x2

example: language model
p(tom | hsi)

   p(likes | hsi, tom)

   p(beer | hsi, tom, likes)

tom

   

  p1

likes

   

beer

   

softmax

softmax

softmax

h0

h1

x1

<s>

h2

x2

h3

x3

example: language model
p(tom | hsi)

   p(likes | hsi, tom)

   p(beer | hsi, tom, likes)

   p(h/si | hsi, tom, likes, beer)

beer

</s>

   

   

tom

   

  p1

likes

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

h0

h1

x1

<s>

language model training

tom

   

  p1

likes

   

beer

   

</s>

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

h0

h1

x1

<s>

language model training

likes

cost2

beer

cost3

</s>

cost4

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

h1

x1

<s>

  p1

h0

language model training

likes

cost2

beer

cost3

</s>

cost4

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

{log loss/   
cross id178

  p1

h0

h1

x1

<s>

language model training

likes

cost2

f

</s>

cost4

beer

cost3

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

{log loss/   
cross id178

  p1

h0

h1

x1

<s>

language model training

likes

cost2

f

</s>

cost4

beer

cost3

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

{log loss/   
cross id178

  p1

h0

h1

x1

<s>

id56 language models

    unlike markov (id165) models, id56s never forget 

    however we will see they might have trouble learning to 

use their memories (more soon   ) 

    algorithms 

    sample a sequence from the id203 distribution 

de   ned by the id56 

    train the id56 to minimize cross id178 (aka id113) 
    what about: what is the most probable sequence?

questions?

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z

t=2

q4

@ht
@ht 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@ht 11a @   y

=0@
|x|yt=2

@f
@h1

@h|x|

@f
@   y

@ht

training challenges
z

vxt + uht 1 + c)

}|

{

zt

ht = g(
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

what happens to gradients as you go back   
in time?
@f
@f

@ht 11a @   y

=0@
|x|yt=2

@f
@h1

@h|x|

@f
@   y

@ht

y

f

  y

h4

x4

training challenges
z

vxt + uht 1 + c)

}|

{

zt

ht = g(
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@   y

@f
@h1

@ht
@zt

@zt

@ht 11a @   y

@h|x|

=0@
|x|yt=2

@f
@f

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

@f
@h1

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

@f
@h1

@ht
@zt

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

@f
@h1

@ht
@zt
@zt
@ht 1

?

= u

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

@f
@h1

@ht
@zt
@zt
@ht 1

= u

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

= u

=

@ht
@zt

@zt
@ht 1

= diag(g0(zt))u

@f
@h1

@ht
@zt
@zt
@ht 1
@ht
@ht 1

zt

vxt + uht 1 + c)

ht = g(
  y = wh|x| + b
@ht
@zt

training challenges
z
=0@
|x|yt=2
=0@
|x|yt=2

}|
{
@ht 11a @   y
diag(g0(zt))u1a @   y

@f
@f
@f
@   y

@f
@f

@h|x|

@h|x|

@f
@   y

@zt

@f
@h1

@f
@h1

zt

vxt + uht 1 + c)

ht = g(
  y = wh|x| + b
@ht
@zt

training challenges
z
=0@
|x|yt=2
=0@
|x|yt=2

@f
@f
@f
@   y
three cases: largest eigenvalue is   
exactly 1; gradient propagation is stable   
<1; gradient vanishes (exponential decay) 
>1; gradient explodes (exponential growth)

}|
{
@ht 11a @   y
diag(g0(zt))u1a @   y

@f
@f

@h|x|

@h|x|

@f
@   y

@zt

@f
@h1

@f
@h1

vanishing gradients

    in practice, the spectral radius of u is small, and gradients vanish 
    in practice, this means that long-range dependencies are dif   cult to learn 

(although in theory they are learnable) 

    solutions 

    better optimizers (second order methods, approximate second order 

methods) 

    id172 to keep the gradient norms stable across time 
    clever initialization so that you at least start with good spectra (e.g., 

start with random orthonormal matrices) 

    alternative parameterizations: lstms and grus

alternative id56s

    long short-term memories (lstms; hochreiter and 

schmidthuber, 1997) 

    id149 (grus; cho et al., 2014) 
    intuition instead of multiplying across time (which 
leads to exponential growth), we want the error to 
be constant. 
    what is a function whose jacobian has a 

spectral radius of exactly i: the identity function

memory cells

ct = ct 1 + f (xt)

c1

x1

i

c2

x2

i

c3

x3

i

c4

x4

h0

memory cells

ct = ct 1 + f (xt)

f (v) = tanh(wv + b)

c1

x1

i

c2

x2

i

c3

x3

i

c4

x4

h0

memory cells

ct = ct 1 + f (xt)
ht = g(ct)

f (v) = tanh(wv + b)

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h4

c4

x4

h0

memory cells

ct = ct 1 + f (xt)
ht = g(ct)

f (v) = tanh(wv + b)

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ct 1 + f (xt)
ht = g(ct)

note:

f (v) = tanh(wv + b)

@ct
@ct 1

= i

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ct 1 + f ([xt; ht 1])
ht = g(ct)

@ct
@ct 1

= i + "

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ct 1 + f ([xt; ht 1])
ht = g(ct)

   almost constant   

@ct
@ct 1

= i + "

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ft   ct 1 + it   f ([xt; ht 1])
ht = g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))

   forget gate   
   input gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

memory cells

ct = ft   ct 1 + it   f ([xt; ht 1])
ht = g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))

   forget gate   
   input gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

memory cells

ct = ft   ct 1 + it   f ([xt; ht 1])
ht = g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))

   forget gate   
   input gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

   forget gate   
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

   forget gate   
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm variant
ct = (1   it)   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

ft = 1   it
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm variant
ct = (1   it)   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

ft = 1   it
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

another visualization

figure credit: christopher olah

another visualization

figure credit: christopher olah

another visualization

forget some of the past

figure credit: christopher olah

another visualization

forget some of the past

add new memories

figure credit: christopher olah

id149 

(grus)

ht = (1   zt)   ht 1 + zt     ht
zt =  (fz([ht 1; xt]))
rt =  (fr([ht 1; xt]))
  ht = f ([rt   ht 1; xt]))

h1

x1

h0

   

h2

x2

   

h3

x3

   

f

  y

h4

x4

summary

    better gradient propagation is possible when you use 

additive rather than multiplicative/highly non-linear 
recurrent dynamics 

id56
lstm
gru

ht = f ([xt; ht 1])
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = (1   zt)   ht 1 + zt   f ([xt; rt   ht 1])

    recurrent architectures are an active area of research, 

requires a mix of mathematical analysis, creativity, 
problem-speci   c knowledge 
    (lstms are hard to beat though!)

summary

    better gradient propagation is possible when you use 

additive rather than multiplicative/highly non-linear 
recurrent dynamics 

id56
lstm
gru

ht = f ([xt; ht 1])
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = (1   zt)   ht 1 + zt   f ([xt; rt   ht 1])

    recurrent architectures are an active area of research, 

requires a mix of mathematical analysis, creativity, 
problem-speci   c knowledge 
    (lstms are hard to beat though!)

summary

    better gradient propagation is possible when you use 

additive rather than multiplicative/highly non-linear 
recurrent dynamics 

id56
lstm
gru

ht = f ([xt; ht 1])
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = (1   zt)   ht 1 + zt   f ([xt; rt   ht 1])

    recurrent architectures are an active area of research, 

requires a mix of mathematical analysis, creativity, 
problem-speci   c knowledge 
    (lstms are hard to beat though!)

questions?

break?

a few tricks of the trade

    depth 

    dropout 

    implementation tricks

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

  y

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

  y

x1

x2

x3

x4

does depth matter?

    yes, it helps 

    it seems to play a less signi   cant role in text than in audio/visual processing 

    h1: more transformation of the input is required for asr, image recognition, etc., 
than for common text applications (word vectors become customized to be    good 
inputs    to id56s whereas you   re stuck with what nature gives you for speech/vision) 

    h2: less effort has been made to    nd good architectures (id56s are expensive to 

train; have been widely used for less long) 

    h3: back prop through time + depth is hard and we need better optimizers 

    many other possibilities    

    2-8 layers seems to be standard 

    input    skip    connections are used often but by no means universally

dropout and deep lstms

    applying dropout layers requires some care

  y

x1

x2

x3

x4

dropout and deep lstms

    apply dropout between layers, but not on the 

recurrent connections

dropout

dropout

dropout

x1

dropout

dropout

dropout

x2

dropout

dropout

dropout

x3

  y

dropout

dropout

dropout

dropout

x4

implementation details

    for speed

    use diagonal matrices instead of full matrices (esp. for gates) 

    concatenate parameter matrices for all gates and do a single matrix-

vector(/matrix) multiplication 

    use optimized implementations (from nvidia) 
    use grus or reduced-gate variant of lstms 

    for learning speed and performance

    initialize so that the bias on the forget gate is large (intuitively: at the 

beginning of training, the signal from the past is unreliable) 

    use random orthogonal matrices to initialize the square matrices

implementation details: 

minibatching

    gpu hardware is 

    pretty fast for elementwise operations (io bound- can   t get enough data 

through the gpu) 

    very fast for matrix-id127 (usually compute bound - the 

gpu will work at 100% capacity, and gpu cores are fast) 

    id56s, lstms, grus all consist of 

    lots of elementwise operations (addition, multiplication, nonlinearities,    ) 

    lots of matrix-vector products 

    minibatching: convert many matrix-vector products into a single matrix-

id127

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

anything wrong here?

minibatching

    the challenge with working with mini batches of 

sequences is     sequences are of different lengths 

    this usually means you bucket training instances 

based on similar lengths, and pad with 0   s 

    be careful when padding not to back propagate a 

non-zero value! 

    manual minibatching convinces me that this is the era 

of assembly language programming for neural 
networks. make the future an easier place to program!

questions?

bidirectional id56s

    we can read a sequence from left to right to obtain 

a representation 

    or we can read it from right to left 

    or we can read it from both and combine the 

representations

id27 models

car

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

memorize

c

a

r

generalize

id27 models

0

      

      

      
car

0 1 0       

0

   
start

c

a

r

   
stop

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

   
start

c

a

r

   
stop

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

   
start

c

a

r

   
stop

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

   
start

c

a

r

   
stop

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

   
start

c

a

r

   
stop

memorize

generalize

id27 models

0

      

      

      
car

0 1 0       

0

   
start

c

a

r

   
stop

memorize

generalize

id38   
charlstm > word lookup

analytic

agglutinative

fusional

(
(

english

turkish

german

portuguese

catalan

ppl   
words
59.4

ppl   
chars
57.4

  

-2.0

|  |   

|  |   
chars
words
4.3m 0.18m

44.0

59.1

46.2

35.3

32.9

-11.1

5.7m 0.17m

43.0

-16.1

6.3m 0.18m

40.9

34.9

-5.3

-0.4

4.2m 0.18m

4.3m 0.18m

id38   
charlstm > word lookup

analytic

agglutinative

fusional

(
(

english

turkish

german

portuguese

catalan

ppl   
words
59.4

ppl   
chars
57.4

  

-2.0

|  |   

|  |   
chars
words
4.3m 0.18m

44.0

59.1

46.2

35.3

32.9

-11.1

5.7m 0.17m

43.0

-16.1

6.3m 0.18m

40.9

34.9

-5.3

-0.4

4.2m 0.18m

4.3m 0.18m

id38   
charlstm > word lookup

analytic

agglutinative

fusional

(
(

english

turkish

german

portuguese

catalan

ppl   
words
59.4

ppl   
chars
57.4

  

-2.0

|  |   

|  |   
chars
words
4.3m 0.18m

44.0

59.1

46.2

35.3

32.9

-11.1

5.7m 0.17m

43.0

-16.1

6.3m 0.18m

40.9

34.9

-5.3

-0.4

4.2m 0.18m

4.3m 0.18m

id38   
charlstm > word lookup

analytic

agglutinative

fusional

(
(

english

turkish

german

portuguese

catalan

ppl   
words
59.4

ppl   
chars
57.4

  

-2.0

|  |   

|  |   
chars
words
4.3m 0.18m

44.0

59.1

46.2

35.3

32.9

-11.1

5.7m 0.17m

43.0

-16.1

6.3m 0.18m

40.9

34.9

-5.3

-0.4

4.2m 0.18m

4.3m 0.18m

id38   
word similarities

increased

john

noahshire

phding

reduced

richard nottinghamshire

mixing

improved george

bucharest

 modelling

expected

james

saxony

styling

decreased robert

johannesburg

blaming

targeted

edward gloucestershire

christening

id38   
word similarities

increased

john

noahshire

phding

reduced

richard nottinghamshire

mixing

improved george

bucharest

 modelling

expected

james

saxony

styling

decreased robert

johannesburg

blaming

targeted

edward gloucestershire

christening

questions?

recurrent neural networks (id56s)

c = id56(x)

0

ht = f (ht 1, xt)

x =

start
start

x1

x2

x3

x4

what is a vector representation of a sequence    ?
x

recurrent neural networks (id56s)

c = id56(x)

0

ht = f (ht 1, xt)

x =

start
start

x1

x2

x3

x4

what is a vector representation of a sequence    ?
x

recurrent neural networks (id56s)

c = id56(x)

0

ht = f (ht 1, xt)

x =

start
start

x1

x2

x3

x4

what is a vector representation of a sequence    ?
x

recurrent neural networks (id56s)

c = id56(x)

0

ht = f (ht 1, xt)

x =

start
start

x1

x2

x3

x4

what is a vector representation of a sequence    ?
x

recurrent neural networks (id56s)

c = id56(x)

0

ht = f (ht 1, xt)

x =

start
start

x1

x2

x3

x4

what is a vector representation of a sequence    ?
x

recurrent neural networks (id56s)

c = id56(x)

0

ht = f (ht 1, xt)

c

x =

start
start

x1

x2

x3

x4

what is a vector representation of a sequence    ?
x

id56 encoder-decoders

c = id56(x)

y | c     id56lm(c)

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

c = id56(x)

y | c     id56lm(c)

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

c = id56(x)

y | c     id56lm(c)

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

c = id56(x)

y | c     id56lm(c)

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

c = id56(x)

y | c     id56lm(c)

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

c = id56(x)

y | c     id56lm(c)

c

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

start

         

c = id56(x)

y | c     id56lm(c)

c

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

beginnings

start

         

c = id56(x)

y | c     id56lm(c)

c

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

beginnings

are

start

         

c = id56(x)

y | c     id56lm(c)

c

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

beginnings

are

dif   cult

start

         

c = id56(x)

y | c     id56lm(c)

c

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

id56 encoder-decoders

beginnings

are

dif   cult

stop

         

start

         

c = id56(x)

y | c     id56lm(c)

c

aller anfang

ist

schwer

stop

        

what is the id203 of a sequence           ?
y | x
cho et al. (2014); sutskever et al. (2014)

i   m

hungry

</s>

ich

habe

hunger

</s>

<s>

i   m

hungry

i   m

hungry

</s>

ich

habe

hunger

</s>

<s>

i   m

hungry

sutskever et al. (2014)

ensembles of nns

did not work well

    sutskever noticed that their single models 
    but by combining n independently trained 
models and obtaining a    consensus   , the 
performance could be improved a lot

    this is called ensembling.

encode anything as a 

vector!

encode anything as a 

vector!

encode anything as a 

vector!

a

   

  p1

spooky

   

old

   

house

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

h1

x1

<s>

limitations
    a possible conceptual problem
    sentences have unbounded lengths
    vectors have    nite capacity
    a possible practical problem
    distance between    translations    and their 

sources are distant- can lstms learn 
this?

two goals

    represent a source sentence as a matrix 

    generate a target sentence from a matrix 

    these two steps are: 

    an algorithm for neural mt 
    a way of introducing attention

sentences as matrices

    problem with the    xed-size vector model in translation 

(maybe in images?) 

    sentences are of different sizes but vectors are of 

the same size 

    solution: use matrices instead 

    fixed number of rows, but number of columns 

depends on the number of words 

    usually |f| = #cols

sentences as matrices

ich m  ochte ein bier

sentences as matrices

ich m  ochte ein bier

mach   s gut

sentences as matrices

ich m  ochte ein bier

mach   s gut

die wahrheiten der menschen sind die unwiderlegbaren irrt  umer

sentences as matrices

ich m  ochte ein bier

mach   s gut

die wahrheiten der menschen sind die unwiderlegbaren irrt  umer

question: how do we build these matrices?

with concatenation

    each word type is represented by an n-dimensional 

vector 

    take all of the vectors for the sentence and 

concatenate them into a matrix 

    simplest possible model 

    so simple, no one has bothered to publish how 

well/badly it works!

x1

x2

x3

x4

ich m  chte ein

bier

fi = xi

x1

x2

x3

x4

ich m  chte ein

bier

fi = xi

x1

x2

x3

x4

ich m  chte ein

bier

f 2 rn   |f|

ich m  ochte ein bier

with convolutional nets

    apply convolutional networks to transform the naive 

concatenated matrix to obtain a context-dependent matrix 

    closely related to the    rst    modern    neural translation 

model proposed (kalchbrenner et al., 2013) 
    no one has been using convnets lately in mt (including 
kalchbrenner et al, who are using bilstms these days) 

    note: convnets usually have a    pooling    operation at the 
top level that results in a    xed-sized representation. for 
sentences, it is probably good to leave this out.

x1

x2

x3

x4

ich m  chte ein

bier

x1

x2

x3

x4

ich m  chte ein

bier

   

filter 1

x1

x2

x3

x4

ich m  chte ein

bier

   

   

filter 1

filter 2

x1

x2

x3

x4

ich m  chte ein

bier

   

   

filter 1

filter 2

x1

x2

x3

x4

f 2 rf (n)   g(|f|)

ich m  ochte ein bier

ich m  chte ein

bier

with bidirectional id56s

    by far the most widely used matrix representation, due to 

bahdanau et al (2015)  

    one column per word 

    each column (word) has two halves concatenated together: 

    a    forward representation   , i.e., a word and its left context 

    a    reverse representation   , i.e., a word and its right context 
    implementation: bidirectional id56s (grus or lstms) to read f 

from left to right and right to left, concatenate representations

x1

x2

x3

x4

ich m  chte ein

bier

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

  h 1

  h 2

  h 3

  h 4

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

f 2 r2n   |f|

x1

x2

x3

x4

ich m  ochte ein bier

ich m  chte ein

bier

where are we in 2016?

    there are lots of ways to construct f 

    very little (published?) work comparing them 
    there are many more undiscovered things out there 

    convolutions are particularly interesting and under-explored 
    syntactic information could help 
    my intuition is simpler/faster models will work well for the 

matrix encoding part   context dependencies are limited in 
language. 

    try something with phrase types instead of word types?

generation from matrices

    we have a matrix f representing the input, now we need to generate from it 
    bahdanau et al. (2015) were the    rst to propose using attention for translating from matrix-

encoded sentences 

    high-level idea 

    generate the output sentence word by word using an id56 
    at each output position t, the id56 receives two inputs (in addition to any recurrent inputs) 

    a    xed-size vector embedding of the previously generated output symbol et-1 
    a    xed-size vector encoding a    view    of the input matrix 

    how do we get a    xed-size vector from a matrix that changes over time? 

    bahdanau et al: do a weighted sum of the columns of f (i.e., words) based on how 

important they are at the current time step. (i.e., just a matrix-vector product fat) 

    the weighting of the input columns at each time-step (at) is called attention

0

recall id56s   

0

    

recall id56s   

0

    

recall id56s   

i'd

    

0

recall id56s   

i'd

0

    

i'd

recall id56s   

i'd

like

0

    

i'd

recall id56s   

0

    

0

    

ich m  ochte ein bier

0

    

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

0

    

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

    

0

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

like

0

z

}|

{

    

i'd

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

0

z

}|

{

    

i'd

like

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

0

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

stopstop

0

z

}|

{

    

i'd

like

a

beer

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

attention

    how do we know what to attend to at each time-

step? 

    that is, how do we compute     ?at

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every word: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every word: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every word: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every word: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every word: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(called     in the paper)

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)
ct = fat

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = tanh (wf + rt) v

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = v> tanh(wf + rt)

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = v> tanh(wf + rt)

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

putting it all together

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
while et 6= h/si :

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention)

(        is a learned embedding of    )
et 1
et
(    and    are learned parameters)
p

b

putting it all together

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
while et 6= h/si :

doesn   t depend on output decisions

}(compute attention)

(        is a learned embedding of    )
et 1
et
(    and    are learned parameters)
p

b

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
x = wf
while et 6= h/si :

x

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention)

(        is a learned embedding of    )
et
et 1
(    and    are learned parameters)
p

b

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
x = wf
while et 6= h/si :

t = t + 1
rt = vst 1
ut = v> tanh(x + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention)

(        is a learned embedding of    )
et
et 1
(    and    are learned parameters)
p

b

summary

    attention is closely related to    pooling    operations in convnets (and other 

architectures) 

    bahdanau   s attention model seems to only cares about    content    

    no obvious bias in favor of diagonals, short jumps, fertility, etc. 

    some work has begun to add other    structural    biases (luong et al., 2015; 

cohn et al., 2016), but there are lots more opportunities 

    attention is similar to alignment, but there are important differences  

    alignment makes stochastic but hard decisions. even if the alignment 

id203 distribution is       at   , the model picks one word or phrase at a time 

    attention is    soft    (you add together all the words). big difference between 

      at    and    peaked    attention weights

attention and translation

    cho   s question: does a translator read and memorize 
the input sentence/document and then generate the 
output? 

    compressing the entire input sentence into a vector 

basically says    memorize the sentence    

    common sense experience says translators refer 

back and forth to the input. (also backed up by eye-
tracking studies) 

    should humans be a model for machines?

questions?

