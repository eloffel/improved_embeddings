teaching machines to read and comprehend

karl moritz hermann, tomas kocisky, edward grefenstette,

lasse espeholt, will kay, mustafa suleyman, lei yu,

and phil blunsom

pblunsom@google.com

features and nlp

twenty years ago id148 allowed greater freedom to
model correlations than simple multinomial parametrisations, but
imposed the need for feature engineering.

features and nlp

distributed/neural models allow us to learn shallow features for our
classi   ers, capturing simple correlations between inputs.

deep learning and nlp

deep learning should allow us to learn hierarchical generalisations.

k-max pooling(k=3) fully connectedlayerfoldingwideconvolution(m=2)dynamick-max pooling (k= f(s) =5) projectedsentence matrix(s=7)wideconvolution(m=3)game's the same, just got more    ercedeep learning and nlp: question answer selection

beyond classi   cation, deep models for embedding sentences have
seen increasing success.

when did james dean die?generalisationgeneralisationin 1955, actor james dean was killed in a two-car collision near cholame, calif.deep learning and nlp: question answer selection

recurrent neural networks provide a very practical tool for
sentence embedding.

gin1955,actorwaskilledinjamesdeanatwo-carcollisionnearcholame,calif.whendidjamesdeandie?deep learning for nlp: machine translation

we can even view translation as encoding and decoding sentences.

(cid:2)(cid:1)         i 'd like a glass of white wine , please .generation               generalisationdeep learning for nlp: machine translation

recurrent neural networks again perform surprisingly well.

leschiensaimentlesos|||dogslovebonesdogslovebones</s>source sequencetarget sequencenlp at google deepmind

small steps towards nlu:

    reading and understanding text,
    connecting natural language, action,
and id136 in real environments.

supervised reading comprehension

to achieve our aim of training supervised machine learning models
for machine reading and comprehension, we must    rst    nd data.

supervised reading comprehension: mctest

document

james the turtle was always getting in trouble. sometimes he   d reach into the
freezer and empty out all the food. other times he   d sled on the deck and get
a splinter. his aunt jane tried as hard as she could to keep him out of trouble,
but he was sneaky and got into lots of trouble behind her back.
one day, james thought he would go into town and see what kind of trouble he
could get into. he went to the grocery store and pulled all the pudding o    the
shelves and ate two jars. then he walked to the fast food restaurant and
ordered 15 bags of fries. he didn   t pay, and instead headed home.
. . .

question

where did james go after he went to the grocery store?

1 his deck

2 his freezer

3 a fast food restaurant

4 his room

supervised reading comprehension: fb synthetic

synthetic example from the facebook data set

an alternative to real language is to generate scripts from a
synthetic grammar.

towardsai-completequestionanswering:asetofprerequisitetoytasksachieve100%accuracy.wetriedtochoosetasksthatarenaturaltoahumanreader,andnobackgroundinareassuchasformalsemantics,machinelearning,logicorknowledgerepresentationisrequiredforanadulttosolvethem.thedataitselfisproducedusingasimplesimulationofcharactersandobjectsmovingaroundandinteractinginlo-cations,describedinsection4.thesimulationallowsustogeneratedatainmanydifferentscenariouswherethetruelabelsareknownbygroundingtothesimulation.foreachtask,wedescribeitbygivingasmallsampleofthedatasetincludingstatements,questionsandthetruelabels(inred).3.1.basicfactoidqawithsinglesupportingfactour   rsttaskconsistsofquestionswhereasinglesupport-ingfactthathasbeenpreviouslygivenprovidestheanswer.we   rsttestoneofthesimplestcasesofthis,byaskingforthelocationofaperson.asmallsampleofthetaskisthus:johnisintheplayground.bobisintheof   ce.whereisjohn?a:playgroundthiskindofsyntheticdatawasalreadyusedin(westonetal.,2014).itcanbeconsideredthesimplestcaseofsomerealworldqadatasetssuchasin(faderetal.,2013).3.2.factoidqawithtwosupportingfactsahardertaskistoanswerquestionswheretwosupportingstatementshavetobechainedtoanswerthequestion:johnisintheplayground.bobisintheof   ce.johnpickedupthefootball.bobwenttothekitchen.whereisthefootball?a:playgroundforexample,toanswerthequestionwhereisthefootball?,bothjohnpickedupthefootballandjohnisintheplay-groundaresupportingfacts.again,thiskindoftaskwasalreadyusedin(westonetal.,2014).notethat,toshowthedif   cultyofthesetasksforalearningmachinewithnootherknowledgewecanshuf   ethelettersofthealphabetandproduceequivalentdatasets:sbdmipimvduyonrckblms.abfipimvdubhhigu.sbdmyigauslyvduhbbvfnoo.abfzumvvbvduaivgdum.mdukuipvduhbbvfnoo?a:yonrckblmswecanalsousethesimulationtogenerateotherlanguagesotherthanenglish.wethusproducedthesamesetoftasksinhindi,e.g.forthistask:manojgendhlekaraaya.manojgusalkhaneymeinchalagaya.gendhissamaykahanhai?a:gusalkhanamanojdaftargaya.priyabagicheygayi.gendhabkahanhai?a:daftar3.3.factoidqawiththreesupportingfactssimilarly,onecanmakeataskwiththreesupportingfacts:johnpickeduptheapple.johnwenttotheof   ce.johnwenttothekitchen.johndroppedtheapple.wherewastheapplebeforethekitchen?a:of   cethe   rstthreestatementsareallrequiredtoanswerthis.3.4.twoargumentrelations:subjectvs.objecttoanswerquestionstheabilitytodifferentiateandrecog-nizesubjectsandobjectsiscrucial.weconsiderheretheextremecasewheresentencesfeaturere-orderedwords,i.e.abag-of-wordswillnotwork:theof   ceisnorthofthebedroom.thebedroomisnorthofthebathroom.whatisnorthofthebedroom?a:of   cewhatisthebedroomnorthof?a:bathroomnotethatthetwoquestionsabovehaveexactlythesamewords,butinadifferentorder,anddifferentanswers.3.5.threeargumentrelationssimilarly,sometimesoneneedstodifferentiatethreesepa-ratearguments,suchasinthefollowingtask:marygavethecaketofred.fredgavethecaketobill.jeffwasgiventhemilkbybill.whogavethecaketofred?a:marywhodidfredgivethecaketo?a:billwhatdidjeffreceive?a:milkwhogavethemilk?a:billthelastquestionispotentiallythehardestforalearnerasthe   rsttwocanbeansweredbyprovidingtheactorthatisnotmentionedinthequestion.3.6.yes/noquestionsthistasktests,onsomeofthesimplestquestionspossible(speci   cally,oneswithasinglesupportingfact)theabilityofamodeltoanswertrue/falsetypequestions:johnisintheplayground.danielpicksupthemilk.isjohnintheclassroom?a:nodoesdanielhavethemilk?a:yessupervised reading comprehension

the id98 and dailymail websites provide paraphrase summary
sentences for each full news story.

supervised reading comprehension

id98 article:

document the bbc producer allegedly struck by jeremy clarkson will not

press charges against the    top gear    host, his lawyer said
friday. clarkson, who hosted one of the most-watched
television shows in the world, was dropped by the bbc
wednesday after an internal investigation by the british
broadcaster found he had subjected producer oisin tymon    to
an unprovoked physical and verbal attack.    . . .

query producer x will not press charges against jeremy clarkson, his

lawyer says.

answer oisin tymon

we formulate cloze style queries from the story paraphrases.

supervised reading comprehension

from the daily mail:

    the hi-tech bra that helps you beat breast x;
    could saccharin help beat x ?;
    can    sh oils help    ght prostate x ?

an ngram language model would correctly predict (x = cancer ),
regardless of the document, simply because this is a frequently
cured entity in the daily mail corpus.

supervised reading comprehension

mnist example generation:

we generate quasi-synthetic examples from the original
document-query pairs, obtaining exponentially more training
examples by anonymising and permuting the mentioned entities.

supervised reading comprehension

original version

anonymised version

context

the bbc producer allegedly struck by jeremy
clarkson will not press charges against the    top
gear    host, his lawyer said friday. clarkson, who
hosted one of the most-watched television shows in
the world, was dropped by the bbc wednesday after
an internal investigation by the british broadcaster
found he had subjected producer oisin tymon    to an
unprovoked physical and verbal attack.    . . .

the ent381 producer allegedly struck by ent212 will
not press charges against the     ent153     host , his
lawyer said friday . ent212 , who hosted one of the
most - watched television shows in the world , was
dropped by the ent381 wednesday after an internal
investigation by the ent180 broadcaster found he
had subjected producer ent193     to an unprovoked
physical and verbal attack .     . . .

query

producer x will not press charges against jeremy
clarkson, his lawyer says.

producer x will not press charges against ent212 , his
lawyer says .

answer

oisin tymon

ent193

original and anonymised version of a data point from the daily
mail validation set. the anonymised entity markers are constantly
permuted during training and testing.

data set statistics

id98

daily mail

train valid test

train valid

test

95
# months
108k
# documents
438k
# queries
456
max # entities
avg # entities
30
avg tokens/doc 780
vocab size

1
1k
4k

1
56
1k 195k
3k 838k
424
41

1
11k
55k
250
190 398
32
45
30
809 773 1044 1061 1066

1
12k
61k
247
45

125k

275k

articles were collected from april 2007 for id98 and june 2010 for the daily
mail, until the end of april 2015. validation data is from march, test data
from april 2015.

question di   culty

category

sentences
2    3

1

simple
lexical
coref
coref/lex
complex
unanswerable

12
14
0
10
8

2
0
8
8
8
10

0
0
2
4
14

distribution (in percent) of queries over category and number of
context sentences required to answer them based on a subset of
the id98 validation data.

frequency baselines (accuracy)

id98

daily mail

valid

test

valid

maximum frequency 26.3
exclusive frequency
30.8

27.9
32.6

22.5
27.3

test

22.7
27.7

a simple baseline is to always predict the entity appearing most
often in the document. a re   nement of this is to exclude entities
in the query.

frame semantic matching

a stronger benchmark using a state-of-the-art frame semantic
parser and rules with an increasing recall/precision trade-o   :

strategy

pattern     q

pattern     d

example (cloze / context)

1
2
3
4
5
6

exact match
be.01.v match
correct frame
permuted frame
matching entity
back-o    strategy

(p, v , y )
(p, be.01.v, y )
(p, v , y )
(p, v , y )
(p, v , y )
pick the most frequent entity from the context that doesn   t appear in the query

x loves suse / kim loves suse
x is president / mike is president
x won oscar / tom won academy award
x met suse / suse met tom
x likes candy / tom loves candy

(x, v , y )
(x, be.01.v, y )
(x, v , z)
(y , v , x)
(x, z , y )

x denotes the entity proposed as answer, v is a fully quali   ed
propbank frame (e.g. give.01.v ). strategies are ordered by
precedence and answers determined accordingly.

frame semantic matching

id98

daily mail

valid

test

valid

maximum frequency
26.3
exclusive frequency
30.8
frame-semantic model 32.2

27.9
32.6
33.0

22.5
27.3
30.7

test

22.7
27.7
31.1

failure modes:

    the propbank parser has poor coverage with many relations

not picked up as they do not adhere to the default
predicate-argument structure.

    the frame-semantic approach does not trivially scale to

situations where several frames are required to answer a query.

word distance benchmark

consider the query    tom hanks is friends with x   s manager,
scooter brown    where the document states    ... turns out he is
good friends with scooter brown, manager for carly rae jepson.   

the frame-semantic parser fails to pickup the friendship or
management relations when parsing the query.

word distance benchmark

word distance benchmark:

    align the placeholder of the cloze form question with each

possible entity in the context document,

    calculate a distance measure between the question and the

context around the aligned entity,

    sum the distances of every word in q to its nearest aligned

word in d.

alignment is de   ned by matching words either directly or as
aligned by the coreference system.

word distance benchmark

id98

daily mail

valid

test

valid

26.3
maximum frequency
exclusive frequency
30.8
frame-semantic model 32.2
word distance model
46.2

27.9
32.6
33.0
46.9

22.5
27.3
30.7
55.6

test

22.7
27.7
31.1
54.8

this benchmark is robust to small mismatches between the query
and answer, correctly solving most instances where the query is
generated from a highlight which in turn closely matches a
sentence in the context document.

reading via encoding

use neural encoding models for estimating the id203 of word
type a from document d answering query q:
p(a|d, q)     exp (w (a)g (d, q)) ,

s.t. a     d.

where w (a) indexes row a of weight matrix w and function
g (d, q) returns a vector embedding of a document and query pair.

deep lstm reader

(cid:48)

(cid:48)

x

we employ a deep lstm cell with skip connections,

(t, k) = x(t)||y

(t, k     1),
(t, k) + wkhi h(t     1, k) + wkci c(t     1, k) + bki
(cid:48)
f (t, k) =    (wkxf x(t) + wkhf h(t     1, k) + wkcf c(t     1, k) + bkf ) ,

i(t, k) =   (cid:0)wkxi x
c(t, k) = f (t, k)c(t     1, k) + i(t, k) tanh(cid:0)wkxc x
o(t, k) =   (cid:0)wkxox

(t, k) + wkhoh(t     1, k) + wkcoc(t, k) + bko

(cid:1) ,

(cid:48)

(cid:48)

(cid:1) ,

(t, k) + wkhc h(t     1, k) + bkc

(cid:1) ,

y

h(t, k) = o(t, k) tanh (c(t, k)) ,
(cid:48)

(t, k) = wky h(t, k) + bky ,
y (t) = y

(t, 1)|| . . .||y

(t, k ),

(cid:48)

(cid:48)

where || indicates vector concatenation h(t, k) is the hidden state for layer k at
time t, and i, f , o are the input, forget, and output gates respectively.

g lstm(d, q) = y (|d| + |q|)

with input x(t) the concatenation of d and q separated by the delimiter |||.

deep lstm reader

marywenttoxvisitedenglandengland|||gdeep lstm reader

marywenttoxvisitedenglandengland|||gdeep lstm reader

id98

daily mail

valid

test

valid

26.3
maximum frequency
exclusive frequency
30.8
frame-semantic model 32.2
word distance model
46.2

27.9
32.6
33.0
46.9

22.5
27.3
30.7
55.6

deep lstm reader

49.0

49.9

57.1

test

22.7
27.7
31.1
54.8

57.3

given the di   cult of its task, the deep lstm reader performs
very strongly.

the attentive reader

denote the outputs of a bidirectional lstm as       y (t) and       y (t). form two

encodings, one for the query and one for each token in the document,

u =       yq (|q|) ||       yq (1),

yd (t) =       yd (t) ||       yd (t).

the representation r of the document d is formed by a weighted sum of the
token vectors. the weights are interpreted as the model   s attention,

m(t) = tanh (wymyd (t) + wumu) ,
s(t)     exp (w

(cid:124)
ms m(t)) ,

r = yd s.

de   ne the joint document and query embedding via a non-linear combination:

g ar(d, q) = tanh (wrg r + wug u) .

the attentive reader

rs(1)y(1)s(3)y(3)s(2)y(2)ugs(4)y(4)marywenttoxvisitedenglandenglandthe attentive reader

id98

daily mail

valid

test

valid

26.3
maximum frequency
exclusive frequency
30.8
frame-semantic model 32.2
word distance model
46.2

deep lstm reader
uniform attention1
attentive reader

49.0
31.1
56.5

27.9
32.6
33.0
46.9

49.9
33.6
58.9

22.5
27.3
30.7
55.6

57.1
31.0
64.5

test

22.7
27.7
31.1
54.8

57.3
31.7
63.7

the attention variables e   ectively address the deep lstm
reader   s inability to focus on part of the document.

1the uniform attention baseline sets all m(t) parameters to be equal.

attentive reader training

models were trained using asynchronous minibatch stochastic
id119 (rmsprop) on approximately 25 gpus.

the attentive reader: predicted: ent49, correct: ent49

the attentive reader: predicted: ent27, correct: ent27

the attentive reader: predicted: ent85, correct: ent37

the attentive reader: predicted: ent24, correct: ent2

the impatient reader

at each token i of the query q compute a representation vector

r (i) using the bidirectional embedding yq(i) =       yq (i) ||       yq (i):
m(i, t) = tanh (wdmyd (t) + wrmr (i     1) + wqmyq(i)) , 1     i     |q|,
s(i, t)     exp (w
r (0) = r0,

(cid:124)
ms m(i, t)) ,
r (i) = y

1     i     |q|.

(cid:124)
d s(i),

the joint document query representation for prediction is,

g ir(d, q) = tanh (wrg r (|q|) + wqg u) .

the impatient reader

rurmarywenttoxvisitedenglandenglandrgthe impatient reader

id98

daily mail

valid

test

valid

26.3
maximum frequency
exclusive frequency
30.8
frame-semantic model 32.2
word distance model
46.2

deep lstm reader
uniform attention
attentive reader
impatient reader

49.0
31.1
56.5
57.0

27.9
32.6
33.0
46.9

22.5
27.3
30.7
55.6

57.1
49.9
31.0
33.6
58.9
64.5
60.6 64.8

test

22.7
27.7
31.1
54.8

57.3
31.7
63.7
63.9

the impatient reader comes out on top, but only marginally.

id12 precision@recall

precision@recall for the id12 on the id98 validation data.

conclusion

summary

    supervised machine reading is a viable research direction with

the available data,

    lstm based recurrent networks constantly surprise with their

ability to encode dependencies in sequences,

    attention is a very e   ective and    exible modelling technique.

future directions

    more and better data, corpus querying, and cross document

queries,

    recurrent networks incorporating long term and working

memory are well suited to nlu task.

google deepmind and oxford university

