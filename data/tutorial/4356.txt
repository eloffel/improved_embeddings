introduction to machine learning:

linear learners1

lisbon machine learning school, 2016

stefan riezler

computational linguistics & scienti   c computing

heidelberg university, germany

riezler@cl.uni-heidelberg.de

1including (lots of) material from shay cohen and ryan mcdonald

introduction to machine learning

1(124)

modeling the frog   s perceptual system

introduction

introduction to machine learning

2(124)

modeling the frog   s perceptual system

introduction

(cid:73) [lettvin et al. 1959] show that the frog   s perceptual system

constructs reality by four separate operations:

(cid:73) contrast detection: presence of sharp boundary?
(cid:73) convexity detection: how curved and how big is object?
(cid:73) movement detection: is object moving?
(cid:73) dimming speed: how fast does object obstruct light?

(cid:73) the frog   s goal: capture any object of the size of an insect or

worm providing it moves like one.

introduction to machine learning

3(124)

modeling the frog   s perceptual system

introduction

(cid:73) [lettvin et al. 1959] show that the frog   s perceptual system

constructs reality by four separate operations:

(cid:73) contrast detection: presence of sharp boundary?
(cid:73) convexity detection: how curved and how big is object?
(cid:73) movement detection: is object moving?
(cid:73) dimming speed: how fast does object obstruct light?

(cid:73) the frog   s goal: capture any object of the size of an insect or

worm providing it moves like one.

(cid:73) can we build a model of this perceptual system and learn to

capture the right objects?

introduction to machine learning

3(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

introduction to machine learning

4(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) =
, p(-) =
(cid:73) p(convex = small|-) =
p(speed = small|-) =
p(convex = small|+) =
p(speed = small|+) =

, p(convex = med|-) =
, p(speed = med|-) =
, p(convex = med|+) =
, p(speed = med|+) =

, p(convex = large|-) =

, p(speed = large|- ) =

, p(convex = large|+) =

, p(speed = large|+ ) =

introduction to machine learning

4(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 8/14, p(-) = 6/14
(cid:73) p(convex = small|-) =
p(speed = small|-) =
p(convex = small|+) =
p(speed = small|+) =

, p(convex = med|-) =
, p(speed = med|-) =
, p(convex = med|+) =
, p(speed = med|+) =

, p(convex = large|-) =

, p(speed = large|- ) =

, p(convex = large|+) =

, p(speed = large|+ ) =

introduction to machine learning

4(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 8/14, p(-) = 6/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

introduction to machine learning

4(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 8/14, p(-) = 6/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) =
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) =

introduction to machine learning

4(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 8/14, p(-) = 6/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) = 8/14    1/8    3/8 = 0.027
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) =

introduction to machine learning

4(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 8/14, p(-) = 6/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) = 8/14    1/8    3/8 = 0.027
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) = 6/14    2/6    1/6 = 0.024

introduction to machine learning

4(124)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 8/14, p(-) = 6/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) = 8/14    1/8    3/8 = 0.027
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) = 6/14    2/6    1/6 = 0.024
(cid:73) inedible: p(convex = med, speed = med, label = -) > p(convex = med, speed = med, label = +)!

introduction to machine learning

4(124)

machine learning is a frog   s world

introduction

(cid:73) machine learning problems can be seen as problems of

function estimation where

(cid:73) our models are based on a combined feature representation of

inputs and outputs

(cid:73) similar to the frog whose world is constructed by

four-dimensional feature vector based on detection operations

introduction to machine learning

5(124)

machine learning is a frog   s world

introduction

(cid:73) machine learning problems can be seen as problems of

function estimation where

(cid:73) our models are based on a combined feature representation of

inputs and outputs

(cid:73) similar to the frog whose world is constructed by

four-dimensional feature vector based on detection operations

(cid:73) learning of parameter weights is done by optimizing    t of

model to training data

(cid:73) frog uses binary classi   cation into edible/inedible objects as

supervision signals for learning

introduction to machine learning

5(124)

machine learning is a frog   s world

introduction

(cid:73) machine learning problems can be seen as problems of

function estimation where

(cid:73) our models are based on a combined feature representation of

inputs and outputs

(cid:73) similar to the frog whose world is constructed by

four-dimensional feature vector based on detection operations

(cid:73) learning of parameter weights is done by optimizing    t of

model to training data

(cid:73) frog uses binary classi   cation into edible/inedible objects as

supervision signals for learning

(cid:73) the model used in the frog   s perception example is called

naive bayes: it measures compatibility of inputs to outputs by
a linear model and optimizes parameters by linear optimization

introduction to machine learning

5(124)

lecture outline: linear learners for nlp

introduction

(cid:73) preliminaries

(cid:73) data: input/output, assumptions
(cid:73) feature representations
(cid:73) linear models

(cid:73) linear learners
(cid:73) naive bayes
(cid:73) generative versus discriminative
(cid:73) id28
(cid:73) id88
(cid:73) large-margin learners (id166s)

(cid:73) id173
(cid:73) online learning
(cid:73) non-linear models

introduction to machine learning

6(124)

inputs and outputs

preliminaries

(cid:73) input: x     x

(cid:73) e.g., document or sentence with some words x = w1 . . . wn

(cid:73) output: y     y

(cid:73) e.g., document class, translation, parse tree

(cid:73) input/output pair: (x, y)     x    y

(cid:73) e.g., a document x and its class label y,
(cid:73) a source sentence x and its translation y,
(cid:73) a sentence x and its parse tree y

introduction to machine learning

7(124)

feature representations

(cid:73) we assume a mapping from input x to a high dimensional

preliminaries

feature vector

(cid:73)   (x) : x     rm

(cid:73) for many cases, more convenient to have mapping from

input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) under certain assumptions, these are equivalent
(cid:73) most papers in nlp use   (x, y)

introduction to machine learning

8(124)

feature representations

(cid:73) we assume a mapping from input x to a high dimensional

preliminaries

feature vector

(cid:73)   (x) : x     rm

(cid:73) for many cases, more convenient to have mapping from

input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) under certain assumptions, these are equivalent
(cid:73) most papers in nlp use   (x, y)
(cid:73) (was?) not so common in nlp:        rm (but see word
(cid:73) more common:   i     {1, . . . , fi}, fi     n+ (categorical)
(cid:73) very common:        {0, 1}m (binary)

embeddings)

introduction to machine learning

8(124)

feature representations

(cid:73) we assume a mapping from input x to a high dimensional

preliminaries

feature vector

(cid:73)   (x) : x     rm

(cid:73) for many cases, more convenient to have mapping from

input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) under certain assumptions, these are equivalent
(cid:73) most papers in nlp use   (x, y)
(cid:73) (was?) not so common in nlp:        rm (but see word
(cid:73) more common:   i     {1, . . . , fi}, fi     n+ (categorical)
(cid:73) very common:        {0, 1}m (binary)
(cid:73) for any vector v     rm, let vj be the j th value

embeddings)

introduction to machine learning

8(124)

examples

preliminaries

(cid:73) x is a document and y is a label

          1 if x contains the word    interest   

and y =      nancial   

0 otherwise

  j (x, y) =

we expect this feature to have a positive weight,    interest    is
a positive indicator for the label       nancial   

introduction to machine learning

9(124)

examples

preliminaries

  j (x, y) = % of words in x containing punctuation and y =   scienti   c   

punctuation symbols - positive indicator or negative indicator for
scienti   c articles?

introduction to machine learning

10(124)

examples

preliminaries

(cid:73) x is a word and y is a part-of-speech tag

(cid:26) 1 if x =    bank    and y = verb

  j (x, y) =

0 otherwise

what weight would it get?

introduction to machine learning

11(124)

examples

preliminaries

(cid:73) x is a source sentence and y is translation

  j (x, y) =

and    are there    present in y

          1 if    y a-t-il    present in x
          1 if    y a-t-il    present in x

0 otherwise

0 otherwise

  k (x, y) =

and    are there any    present in y

which phrase indicator should be preferred?

introduction to machine learning

12(124)

examples

preliminaries

note: label y includes sentence x

introduction to machine learning

13(124)

linear models

(cid:73) linear model: de   nes a discriminant function that is based

linear models

on linear combination of features and weights
        (x, y)

f (x;   ) = arg max

y   y

= arg max

y   y

m(cid:88)

j=0

  j      j (x, y)

introduction to machine learning

14(124)

linear models

(cid:73) linear model: de   nes a discriminant function that is based

linear models

on linear combination of features and weights
        (x, y)

f (x;   ) = arg max

y   y

= arg max

y   y

m(cid:88)

j=0

  j      j (x, y)

(cid:73) let        rm be a high dimensional weight vector
(cid:73) assume that    is known

(cid:73) multiclass classi   cation: y = {0, 1, . . . , n}
        (x, y)

y = arg max

y   y

(cid:73) binary classi   cation just a special case of multiclass

introduction to machine learning

14(124)

binary linear model
   de   nes a hyperplane (line in 2 dimensions) that divides all
points:

linear models

introduction to machine learning

15(124)

12-2-112-2-1points along linehave scores of 0multiclass linear model

de   nes regions of space. visualization di   cult.

linear models

(cid:73) + are all points (x, y) where + = arg maxy         (x, y)

introduction to machine learning

16(124)

separability

linear models

(cid:73) a set of points is separable, if there exists a    such that

classi   cation is perfect

separable

not separable

introduction to machine learning

17(124)

supervised learning

linear learners

we now have a way to make decisions... if we have a weight vector
  . but where do we get this   ?

(cid:73) input:

(cid:73) i.i.d. (independent and identically distributed) training

examples t = {(xt, yt)}|t |

t=1

(cid:73) feature representation   

introduction to machine learning

18(124)

supervised learning

linear learners

we now have a way to make decisions... if we have a weight vector
  . but where do we get this   ?

(cid:73) input:

(cid:73) i.i.d. (independent and identically distributed) training

examples t = {(xt, yt)}|t |

t=1

(cid:73) feature representation   

(cid:73) output:    that maximizes an objective function on the

training set

(cid:73)    = arg maxl(t ;   )
(cid:73) equivalently minimize:    = arg min   l(t ;   )

introduction to machine learning

18(124)

objective functions

linear learners

(cid:73) l(t ;   )    (cid:80)

(cid:73) ideally we can decompose l by training pairs (x, y)

(cid:73) loss is a function that measures some value correlated with

(x,y)   t loss((x, y);   )

errors of parameters    on instance (x, y)

introduction to machine learning

19(124)

objective functions

linear learners

(cid:73) l(t ;   )    (cid:80)

(cid:73) ideally we can decompose l by training pairs (x, y)

(cid:73) loss is a function that measures some value correlated with

(x,y)   t loss((x, y);   )

errors of parameters    on instance (x, y)

(cid:73) example:

(cid:73) y     {1,   1}, f (x;   ) is the prediction we make for x using   
(cid:73) 0-1 id168: loss((x, y);   ) =

if f (x;   ) = y ,
else

(cid:26) 0

1

introduction to machine learning

19(124)

id76

linear learners

(cid:73) a function is convex if its graph lies on or below the line

segment connecting any two points on the graph
f (  x+  y)       f (x)+  f (y) for all   ,        0,   +   = 1 (1)

(cid:73) for linear models we have equality in (1)

introduction to machine learning

20(124)

id76

linear learners

(cid:73) optimization problem is de   ned as problem of    nding a point

that minimizes our objective function (maximization is
minimization of    f (x))

(cid:73) limit attention to convex (or even linear) functions
(cid:73) find point at which gradient of f is 0

(cid:73) slope of the gradient is non-decreasing as one moves away

from minimum, 0 at minimum

(cid:73) in order to    nd minimum, follow opposite direction of gradient
(cid:73) for convex functions, this will lead to the single global

minimum

introduction to machine learning

21(124)

naive bayes

naive bayes

introduction to machine learning

22(124)

naive bayes

naive bayes

(cid:73) probabilistic decision model:

arg max

y

p(y|x)     arg max

p(y)p(x|y)

y

(cid:73) uses bayes rule:

p(y|x) =

p(y)p(x|y)

p(x)

for    xed x

(cid:73) generative model since p(y)p(x|y) = p(x, y) is a joint

id203

(cid:73) because we model a distribution that can randomly generate

outputs and inputs, not just outputs

introduction to machine learning

23(124)

naive bayes

naivety of naive bayes

(cid:73) we need to decide on the structure of p(x, y)
(cid:73) p(x|y) = p(  (x)|y) = p(  1(x), . . . ,   m(x)|y)

p(  1(x), . . . ,   m(x)|y) =(cid:81)

naive bayes assumption
(conditional independence)

i p(  i (x)|y)

p(x, y) = p(y)p(  1(x), . . . ,   m(x)|y) = p(y)

m(cid:89)

i=1

p(  i (x)|y)

introduction to machine learning

24(124)

naive bayes     learning

naive bayes

(cid:73) input: t = {(xt, yt)}|t |
(cid:73) let   i (x)     {1, . . . , fi}
(cid:73) parameters p = {p(y), p(  i (x)|y)}

t=1

(cid:73) both p(y) and p(  i (x)|y) are multinomials

introduction to machine learning

25(124)

naive bayes

id113

(cid:73) what   s left? de   ning an objective l(t )

(cid:73) p plays the role of   

(cid:73) what objective to use?

(cid:73) objective: id113 (id113)

|t |(cid:89)

(cid:32)

|t |(cid:89)

m(cid:89)

l(t ) =

p(xt, yt) =

p(yt)

p(  i (xt)|yt)

(cid:33)

t=1

t=1

i=1

introduction to machine learning

26(124)

naive bayes     learning
id113 has closed form solution

p = arg max

p

naive bayes

(cid:33)

p(  i (xt)|yt)

|t |(cid:89)

t=1

m(cid:89)

i=1

p(yt)

(cid:32)
(cid:80)|t |
(cid:80)|t |
(cid:26) 1

|t |

p(y) =

(cid:80)|t |

t=1[[yt = y]]

p(  i (x)|y) =

t=1[[  i (xt) =   i (x) and yt = y]]

t=1[[yt = y]]

where [[p]] =

if p is true,
0 otherwise.

thus, these are just normalized counts over events in t

introduction to machine learning

27(124)

naive bayes

naive bayes document classi   cation

(cid:73) doc 1: y1 = sports,    hockey is fast   
(cid:73) doc 2: y2 = politics,    politicians talk fast   
(cid:73) doc 3: y3 = politics,    washington is sleazy   

(cid:73)   0(x) = 1 if doc has word    hockey   , 0 else
(cid:73)   1(x) = 1 if doc has word    is   , 0 else
(cid:73)   2(x) = 1 if doc has word    fast   , 0 else
(cid:73)   3(x) = 1 if doc has word    politicians   , 0 else
(cid:73)   4(x) = 1 if doc has word    talk   , 0 else
(cid:73)   5(x) = 1 if doc has word    washington   , 0 else
(cid:73)   6(x) = 1 if doc has word    sleazy   , 0 else

your turn? what is p(sports)? what is p(  0(x) = 1|politics)?

introduction to machine learning

28(124)

naive bayes

naive bayes document classi   cation

(cid:73) doc 1: y1 = sports,    hockey is fast   
(cid:73) doc 2: y2 = politics,    politicians talk fast   
(cid:73) doc 3: y3 = politics,    washington is sleazy   

(cid:73)   0(x) = 1 if doc has word    hockey   , 0 else
(cid:73)   1(x) = 1 if doc has word    is   , 0 else
(cid:73)   2(x) = 1 if doc has word    fast   , 0 else
(cid:73)   3(x) = 1 if doc has word    politicians   , 0 else
(cid:73)   4(x) = 1 if doc has word    talk   , 0 else
(cid:73)   5(x) = 1 if doc has word    washington   , 0 else
(cid:73)   6(x) = 1 if doc has word    sleazy   , 0 else

your turn? what is p(sports)? 1/3 what is
p(  0(x) = 1|politics)?

introduction to machine learning

29(124)

naive bayes

naive bayes document classi   cation

(cid:73) doc 1: y1 = sports,    hockey is fast   
(cid:73) doc 2: y2 = politics,    politicians talk fast   
(cid:73) doc 3: y3 = politics,    washington is sleazy   

(cid:73)   0(x) = 1 if doc has word    hockey   , 0 else
(cid:73)   1(x) = 1 if doc has word    is   , 0 else
(cid:73)   2(x) = 1 if doc has word    fast   , 0 else
(cid:73)   3(x) = 1 if doc has word    politicians   , 0 else
(cid:73)   4(x) = 1 if doc has word    talk   , 0 else
(cid:73)   5(x) = 1 if doc has word    washington   , 0 else
(cid:73)   6(x) = 1 if doc has word    sleazy   , 0 else

your turn? what is p(sports)? 1/3 what is
p(  0(x) = 1|politics)? 0

introduction to machine learning

30(124)

deriving id113

naive bayes

(cid:32)

|t |(cid:89)

t=1

m(cid:89)

i=1

p(yt)

(cid:33)

p(  i (xt)|yt)

p = arg max

p

introduction to machine learning

31(124)

deriving id113

naive bayes

p = arg max

p(  i (xt)|yt)

= arg max

log p(yt) +

log p(  i (xt)|yt)

m(cid:88)

i=1

(cid:33)

|t |(cid:88)

m(cid:88)

t=1

i=1

(cid:33)

m(cid:89)

i=1

p

p

t=1

(cid:32)
(cid:32)

p(yt)

|t |(cid:89)
|t |(cid:88)
|t |(cid:88)
y p(y) = 1,(cid:80)fi

t=1

t=1

such that(cid:80)

= arg max

p(y)

log p(yt) + arg max
p(  i (x)|y)

log p(  i (xt)|yt)

j=1 p(  i (x) = j|y) = 1, p(  )     0

introduction to machine learning

32(124)

deriving id113

log p(yt) + arg max
p(  i (x)|y)

|t |(cid:88)
|t |(cid:88)
v count(v ) log p(v ), s.t.,(cid:80)

t=1

t=1

(cid:80)

p = arg max

p(y)

arg maxp

both optimizations are of the form

naive bayes

m(cid:88)

i=1

log p(  i (xt)|yt)

v p(v ) = 1, p(v )     0

where v is event in t , e.g. (yt = y) or
(  i (xt) =   i (x) and yt = y)

introduction to machine learning

33(124)

naive bayes

deriving id113

arg maxp

(cid:80)
v count(v ) log p(v )
v p(v ) = 1, p(v )     0

s.t.,(cid:80)
(cid:80)
v count(v ) log p(v )        ((cid:80)

introduce lagrangian multiplier   , optimization becomes
v p(v )     1)

arg maxp,  

(cid:73) derivative:

(cid:73) set to zero:

(cid:73) final solution:

introduction to machine learning

34(124)

naive bayes

deriving id113

arg maxp

(cid:80)
v count(v ) log p(v )
v p(v ) = 1, p(v )     0

s.t.,(cid:80)
v count(v ) log p(v )        ((cid:80)
(cid:80)

introduce lagrangian multiplier   , optimization becomes
v p(v )     1)

arg maxp,  

(cid:73) derivative w.r.t p(v ) is

count(v)

p(v )       

count(v)

  

(cid:73) setting this to zero p(v ) =

(cid:73) combine with(cid:80)

p(v ) =

(cid:80)

count(v)
v(cid:48) count(v(cid:48))

v p(v ) = 1. p(v )     0, then

introduction to machine learning

35(124)

naive bayes

deriving id113

reinstantiate events v in t :

p(y) =

(cid:80)|t |

t=1[[yt = y]]

(cid:80)|t |
(cid:80)|t |

|t |

t=1[[yt = y]]

p(  i (x)|y) =

t=1[[  i (xt) =   i (x) and yt = y]]

introduction to machine learning

36(124)

naive bayes is a linear model

naive bayes

(cid:73) let   y = log p(y),    y     y
(cid:73) let     i (x),y = log p(  i (x)|y),    y     y,   i (x)     {1, . . . , fi}
(cid:73) let    be set of all       and      ,   

arg max

y

p(y|  (x))     arg max

y

p(  (x), y) = arg max

p(y)

y

i=1

m(cid:89)

p(  i (x)|y) =

where           {0, 1},   i,j (x) = [[  i (x) = j]],   y(cid:48) (y) = [[y = y(cid:48)]]

introduction to machine learning

37(124)

naive bayes is a linear model

naive bayes

(cid:73) let   y = log p(y),    y     y
(cid:73) let     i (x),y = log p(  i (x)|y),    y     y,   i (x)     {1, . . . , fi}
(cid:73) let    be set of all       and      ,   

arg max

y

p(y|  (x))     arg max

y

= arg max

y

= arg max

y

= arg max

y

p(  (x), y) = arg max

p(y)

p(  i (x)|y)

m(cid:89)

m(cid:88)

i=1

y

i=1

log p(  i (x)|y)

log p(y) +

m(cid:88)

  y +

    i (x),y

i=1

  y  y(cid:48) (y) +

(cid:88)

y(cid:48)

m(cid:88)

fi(cid:88)

    i (x),y  i,j (x)

i=1

j=1

where           {0, 1},   i,j (x) = [[  i (x) = j]],   y(cid:48) (y) = [[y = y(cid:48)]]

introduction to machine learning

38(124)

naive bayes

smoothing

(cid:73) doc 1: y1 = sports,    hockey is fast   
(cid:73) doc 2: y2 = politics,    politicians talk fast   
(cid:73) doc 3: y3 = politics,    washington is sleazy   

(cid:73) new doc:    washington hockey is fast   
(cid:73) both    sports    and    politics    have probabilities of 0

(cid:73) smoothing aims to assign a small amount of id203 to

unseen events

(cid:73) e.g., additive/laplacian smoothing
=    p(v ) =

p(v ) =

(cid:80)

count(v )
v(cid:48) count(v(cid:48))

(cid:80)

count(v ) +   
v(cid:48) (count(v(cid:48)) +   )

introduction to machine learning

39(124)

naive bayes

discriminative versus generative

(cid:73) generative models attempt to model inputs and outputs

(cid:73) e.g., naive bayes = id113 of joint distribution p(x, y)
(cid:73) statistical model must explain generation of input

(cid:73) occam   s razor:    among competing hypotheses, the one with

the fewest assumptions should be selected   

(cid:73) discriminative models

(cid:73) use l that directly optimizes p(y|x) (or something related)
(cid:73) id28     id113 of p(y|x)
(cid:73) id88 and id166s     minimize classi   cation error
(cid:73) generative and discriminative models use p(y|x) for

prediction

(cid:73) di   er only on what distribution they use to set   

introduction to machine learning

40(124)

id28

id28

introduction to machine learning

41(124)

id28

id28

de   ne a id155:

p(y|x) =

e      (x,y)

zx

,

where zx =

(cid:88)

y(cid:48)   y

e      (x,y(cid:48))

note: still a linear model

arg max

y

p(y|x) = arg max

y

= arg max

y

= arg max

y

e      (x,y)

zx

e      (x,y)
        (x, y)

introduction to machine learning

42(124)

id28

id28

p(y|x) =

e      (x,y)

zx

(cid:73) q: how do we learn weights   
(cid:73) a: set weights to maximize log-likelihood of training data:

l(t ;   )

   = arg max

  

= arg max

|t |(cid:89)

  

t=1

p(yt|xt) = arg max

  

|t |(cid:88)

t=1

log p(yt|xt)

(cid:73) in a nutshell we set the weights    so that we assign as much
id203 to the correct label y for each x in the training set

introduction to machine learning

43(124)

id28

p(y|x) =

e      (x,y)

zx

,

   = arg max

|t |(cid:88)

  

t=1

where zx =

log p(yt|xt) (*)

id28

(cid:88)

y(cid:48)   y

e      (x,y(cid:48))

(cid:73) the objective function (*) is concave
(cid:73) therefore there is a global maximum
(cid:73) no closed form solution, but lots of numerical techniques

(cid:73) gradient methods (gradient ascent, conjugate gradient,

iterative scaling)

(cid:73) id77s (limited-memory quasi-newton)

introduction to machine learning

44(124)

gradient ascent

id28

introduction to machine learning

45(124)

gradient ascent

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 log(cid:0)e      (xt ,yt )/zx

(cid:73) want to    nd arg max   l(t ;   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

id28

(cid:1)

  i =   i   1 +   (cid:79)l(t ;   i   1)
(cid:73)    > 0 and set so that l(t ;   i ) > l(t ;   i   1)
(cid:73) (cid:79)l(t ;   ) is gradient of l w.r.t.   
l(t ;   ),    

(cid:73) a gradient is all partial derivatives over variables wi
(cid:73) i.e., (cid:79)l(t ;   ) = (    
l(t ;   ), . . . ,    
     m

     0

     1

l(t ;   ))

(cid:73) gradient ascent will always    nd    to maximize l

introduction to machine learning

46(124)

id119

(cid:73) let l(t ;   ) =    (cid:80)|t |

t=1 log(cid:0)e      (xt ,yt )/zx

(cid:73) want to    nd arg min   l(t ;   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

id28

(cid:1)

  i =   i   1       (cid:79)l(t ;   i   1)
(cid:73)    > 0 and set so that l(t ;   i ) < l(t ;   i   1)
(cid:73) (cid:79)l(t ;   ) is gradient of l w.r.t.   
l(t ;   ),    

(cid:73) a gradient is all partial derivatives over variables wi
(cid:73) i.e., (cid:79)l(t ;   ) = (    
l(t ;   ), . . . ,    
     m
(cid:73) id119 will always    nd    to minimize l

     0

     1

l(t ;   ))

introduction to machine learning

47(124)

the partial derivatives

id28

(cid:73) need to    nd all partial derivatives

l(t ;   )

   
     i

l(t ;   ) =

=

=

t

(cid:88)
(cid:88)
(cid:88)

t

t

log p(yt|xt)

(cid:80)
e      (xt ,yt )
y(cid:48)   y e      (xt ,y(cid:48))
(cid:80)
j   j    j (xt ,yt )

e

zxt

log

log

introduction to machine learning

48(124)

partial derivatives - some reminders

id28

(cid:73) we always assume log is the natural logarithm loge

1.

2.

3.

4.

   

   x log f = 1

f

   
   x f

   

   x ef = ef    

(cid:80)
t ft =(cid:80)

   x f

t

f
g =

   x f   f    
g    

g 2

   
   x

   
   x

   
   x ft
   x g

introduction to machine learning

49(124)

the partial derivatives

id28

l(t ;   ) =

   
     i

introduction to machine learning

50(124)

the partial derivatives (1)

id28

l(t ;   ) =

   
     i

=

=

   
     i

(cid:88)
(cid:88)

t

(cid:80)
(cid:80)

e

e

(cid:88)

t

   
     i

log

log

j   j    j (xt ,yt )

zxt

j   j    j (xt ,yt )

zxt

(cid:80)
j   j    j (xt ,yt )

zxt

e

)(

   
     i

(

e

t

(cid:80)
j   j    j (xt ,yt )

zxt

)

introduction to machine learning

51(124)

the partial derivatives

(cid:80)
j   j     j (xt ,yt )

zxt

=

now,

e

   
     i

id28

introduction to machine learning

52(124)

the partial derivatives (2)
now,

(cid:80)
j   j    j (xt ,yt )

e

   
     i

zxt

(cid:80)

e

zxt

   
     i

(cid:80)

j   j    j (xt ,yt )     e
z 2
xt

(cid:80)
j   j    j (xt ,yt )    
     i

(cid:80)

zxt e

j   j    j (xt ,yt )  i (xt , yt )     e

j   j    j (xt ,yt )    
     i

zxt

id28

zxt

=

=

=

=

(cid:80)
j   j    j (xt ,yt )
(cid:80)
j   j    j (xt ,yt )

z 2
xt

e

e

z 2
xt

z 2
xt

(zxt   i (xt , yt )        
     i

zxt )

(zxt   i (xt , yt )

(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

e

(cid:48)

))

because

   
     i

zxt =

   
     i

(cid:88)

y(cid:48)   y

(cid:80)

e

j   j    j (xt ,y(cid:48)) =

j   j    j (xt ,y(cid:48))  i (xt , y

(cid:48)

)

    (cid:88)
(cid:88)
(cid:80)

y(cid:48)   y

e

y(cid:48)   y

introduction to machine learning

53(124)

the partial derivatives

id28

introduction to machine learning

54(124)

id28

the partial derivatives (3)
from (2),

(cid:80)
j   j    j (xt ,yt )

(cid:80)
j   j    j (xt ,yt )

e

   
     i

zxt

(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

(cid:48)

))

=

(

e

e

zxt

z 2
xt

(cid:80)

y(cid:48)   y

(zxt   i (xt , yt )

    (cid:88)
(cid:80)
(zxt   i (xt , yt )     (cid:88)
(cid:88)
  i (xt , yt )    (cid:88)
(cid:80)
j   j    j (xt ,y(cid:48))
(cid:88)
  i (xt , yt )    (cid:88)

j   j    j (xt ,yt ) )(

   
     i

1
zxt

y(cid:48)   y

y(cid:48)   y

p(y

zxt

e

e

e

t

t

(cid:88)
(cid:88)
(cid:88)
(cid:88)

t

t

t

t

y(cid:48)   y

(cid:48)|xt )  i (xt , y

(cid:48)

)

j   j    j (xt ,yt )
zxt
(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

e

)

(cid:48)

)))

  i (xt , y

(cid:48)

)

sub this in (1),

l(t ;   ) =

   
     i

=

=

=

introduction to machine learning

55(124)

finally!!!

id28

(cid:73) after all that,

l(t ;   ) =

   
     i

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

(cid:73) and the gradient is:
(cid:79)l(t ;   ) = (

   

l(t ;   ),

     0

l(t ;   ), . . . ,

   

     1

l(t ;   ))

   

     m

(cid:73) so we can now use gradient ascent to    nd   !!

introduction to machine learning

56(124)

id28 summary

(cid:73) de   ne id155

id28

(cid:73) set weights to maximize log-likelihood of training data:

p(y|x) =

e      (x,y)

zx

(cid:88)

   = arg max

  

t

log p(yt|xt)

(cid:73) can    nd the gradient and run gradient ascent (or any

gradient-based optimization algorithm)

l(t ;   ) =

   
     i

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

introduction to machine learning

57(124)

id28

id28 = maximum id178

(cid:73) well-known equivalence
(cid:73) max ent: maximize id178 subject to constraints on

features: p = arg maxp h(p) under constraints

(cid:73) empirical feature counts must equal expected counts

(cid:73) quick intuition

(cid:73) partial derivative in id28

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

l(t ;   ) =

   
     i

(cid:73) first term is empirical feature counts and second term is

expected counts

(cid:73) derivative set to zero maximizes function
(cid:73) therefore when both counts are equivalent, we optimize the

id28 objective!

introduction to machine learning

58(124)

id88

id88

introduction to machine learning

59(124)

id88

id88

|t |(cid:88)
|t |(cid:88)

t=1

(cid:73) choose a    that minimizes error

l(t ;   ) =

1     [[yt = arg max

y

   = arg min

  

t=1

[[p]] =

(cid:73) this is a 0-1 id168

1     [[yt = arg max

(cid:26) 1 p is true

y

0 otherwise

        (xt, y)]]

        (xt, y)]]

(cid:73) we will see later how to formalize the id88 error

function as hinge-loss

introduction to machine learning

60(124)

id88 learning algorithm

id88

t=1

for n : 1..n

training data: t = {(xt, yt)}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.

return   i

for t : 1..t
let y(cid:48) = arg maxy(cid:48)   (i)      (xt, y(cid:48))
if y(cid:48) (cid:54)= yt

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))
i = i + 1

introduction to machine learning

61(124)

id88: separability and margin

id88

(cid:73) given an training instance (xt, yt), de   ne:
(cid:73)   yt = y     {yt}
(cid:73) i.e.,   yt is the set of incorrect labels for xt

(cid:73) a training set t is separable with margin    > 0 if there exists

a vector u with (cid:107)u(cid:107) = 1 such that:

u      (xt, yt)     u      (xt, y(cid:48))       

(2)

for all y(cid:48)       yt and ||u|| =

(cid:113)(cid:80)

j u2
j

(cid:73) assumption: the training set is separable with margin   

introduction to machine learning

62(124)

id88: main theorem

id88

(cid:73) theorem: for any training set separable with a margin of   ,

the following holds for the id88 algorithm:

mistakes made during training     r 2
  2

where r     ||  (xt, yt)       (xt, y(cid:48))|| for all (xt, yt)     t and
y(cid:48)       yt

(cid:73) thus, after a    nite number of training iterations, the error on

the training set will converge to zero

(cid:73) let   s prove it! (proof taken from [collins 2002])

introduction to machine learning

63(124)

id88

id88 learning algorithm
training data: t = {(xt , yt )}|t |

(cid:73) lower bound:

t=1

1.
2.
3.
4.
5.
6.
7.
8.

  (0) = 0; i = 0
for n : 1..n

for t : 1..t

let y(cid:48) = arg maxy(cid:48)   (i)      (xt , y(cid:48))
if y(cid:48) (cid:54)= yt
  (i+1) =   (i) +   (xt , yt )       (xt , y(cid:48))

i = i + 1

return   i

  (k   1) are weights before k th error
suppose k th error made at (xt , yt )
y(cid:48) = arg maxy(cid:48)   (k   1)      (xt , y(cid:48))
y(cid:48) (cid:54)= yt
  (k) =
  (k   1) +   (xt , yt )       (xt , y(cid:48))

introduction to machine learning

64(124)

id88

id88 learning algorithm
training data: t = {(xt , yt )}|t |

(cid:73) lower bound:

1.
2.
3.
4.
5.
6.
7.
8.

t=1

for t : 1..t

  (0) = 0; i = 0
for n : 1..n

let y(cid:48) = arg maxy(cid:48)   (i)      (xt , y(cid:48))
if y(cid:48) (cid:54)= yt
  (i+1) =   (i) +   (xt , yt )       (xt , y(cid:48))

  (k   1) are weights before k th error
suppose k th error made at (xt , yt )
y(cid:48) = arg maxy(cid:48)   (k   1)      (xt , y(cid:48))
y(cid:48) (cid:54)= yt
  (k) =
  (k   1) +   (xt , yt )       (xt , y(cid:48))
return   i
u      (k) = u      (k   1) + u    (  (xt , yt )       (xt , y(cid:48)))     u      (k   1) +   , by (2)
since   (0) = 0 and u      (0) = 0, for all k: u      (k)     k  , by induction on k
since u      (k)     ||u||    ||  (k)||, by the law of cosines, and ||u|| = 1, then
||  (k)||     k  

i = i + 1

introduction to machine learning

64(124)

id88

id88 learning algorithm
training data: t = {(xt , yt )}|t |

(cid:73) lower bound:

1.
2.
3.
4.
5.
6.
7.
8.

t=1

i = i + 1

for t : 1..t

  (0) = 0; i = 0
for n : 1..n

let y(cid:48) = arg maxy(cid:48)   (i)      (xt , y(cid:48))
if y(cid:48) (cid:54)= yt
  (i+1) =   (i) +   (xt , yt )       (xt , y(cid:48))

  (k   1) are weights before k th error
suppose k th error made at (xt , yt )
y(cid:48) = arg maxy(cid:48)   (k   1)      (xt , y(cid:48))
y(cid:48) (cid:54)= yt
  (k) =
  (k   1) +   (xt , yt )       (xt , y(cid:48))
return   i
u      (k) = u      (k   1) + u    (  (xt , yt )       (xt , y(cid:48)))     u      (k   1) +   , by (2)
since   (0) = 0 and u      (0) = 0, for all k: u      (k)     k  , by induction on k
since u      (k)     ||u||    ||  (k)||, by the law of cosines, and ||u|| = 1, then
||  (k)||     k  
(cid:73) upper bound:

||  (k)||2 = ||  (k   1)||2 + ||  (xt , yt )       (xt , y(cid:48))||2 + 2  (k   1)    (  (xt , yt )       (xt , y(cid:48)))
||  (k)||2     ||  (k   1)||2 + r 2, since r     ||  (xt , yt )       (xt , y(cid:48))||

and   (k   1)      (xt , yt )       (k   1)      (xt , y(cid:48))     0

    kr 2 for all k, by induction on k

introduction to machine learning

64(124)

id88 learning algorithm

id88

(cid:73) we have just shown that ||  (k)||     k   and ||  (k)||2     kr 2

(cid:73) therefore,

k 2  2     ||  (k)||2     kr 2

(cid:73) and solving for k

k     r 2
  2

(cid:73) therefore the number of errors is bounded!

introduction to machine learning

65(124)

id88 summary

id88

(cid:73) learns parameters of a linear model by minimizing error
(cid:73) guaranteed to    nd a    in a    nite amount of time
(cid:73) id88 is an example of an online learning algorithm
(cid:73)    is updated based on a single training instance in isolation

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))

introduction to machine learning

66(124)

averaged id88

id88

t=1

for n : 1..n

training data: t = {(xt, yt)}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
6.
7.
8.

i   (i)(cid:1) / (n    t )

return(cid:0)(cid:80)

  (i+1) =   (i)

i = i + 1

else

for t : 1..t
let y(cid:48) = arg maxy(cid:48)   (i)      (xt, y(cid:48))
if y(cid:48) (cid:54)= yt

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))

introduction to machine learning

67(124)

margin

training

testing

id88

denote the
value of the
margin by   

introduction to machine learning

68(124)

maximizing margin

id88

(cid:73) for a training set t
(cid:73) margin of a weight vector    is smallest    such that

        (xt, yt)             (xt, y(cid:48))       
(cid:73) for every training instance (xt, yt)     t , y(cid:48)       yt

introduction to machine learning

69(124)

maximizing margin

id88

(cid:73) intuitively maximizing margin makes sense
(cid:73) more importantly, generalization error to unseen test data is

proportional to the inverse of the margin

     

r 2

  2    |t |

(cid:73) id88: we have shown that:

(cid:73) if a training set is separable by some margin, the id88

will    nd a    that separates the data

(cid:73) however, the id88 does not pick    to maximize the

margin!

introduction to machine learning

70(124)

support vector machines

support vector machines (id166s)

introduction to machine learning

71(124)

support vector machines

maximizing margin

let    > 0

such that:

max
||  ||=1

  

        (xt, yt)             (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

(cid:73) note: algorithm still minimizes error if data is separable
(cid:73) ||  || is bound since scaling trivially produces larger margin
  (        (xt, yt)             (xt, y(cid:48)))         , for some        1

introduction to machine learning

72(124)

max margin = min norm

support vector machines

let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

introduction to machine learning

73(124)

support vector machines

max margin = min norm
let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

?

introduction to machine learning

74(124)

support vector machines

max margin = min norm
let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

?

min norm (step 1):

max

u

1
||u|| = min
u

||u||

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

introduction to machine learning

75(124)

max margin = min norm
let    > 0

support vector machines

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

?

min norm (step 2):

||u||

min
u

such that:
  u    (xt, yt)     u    (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

introduction to machine learning

76(124)

max margin = min norm
let    > 0

support vector machines

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

?

min norm (step 2):

||u||

min
u

such that:
u    (xt, yt)   u    (xt, y(cid:48))     1

   (xt, yt)     t
and y(cid:48)       yt

introduction to machine learning

77(124)

support vector machines

max margin = min norm
let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

?

min norm (step 3):

min
u

||u||2

1
2

such that:
u    (xt, yt)   u    (xt, y(cid:48))     1

   (xt, yt)     t
and y(cid:48)       yt

introduction to machine learning

78(124)

support vector machines

max margin = min norm

let    > 0

max margin:

min norm:

max
||  ||=1

  

min
u

||u||2

1
2

such that:
      (xt, yt)         (xt, y(cid:48))       

such that:
u    (xt, yt)   u    (xt, y(cid:48))     1

   (xt, yt)     t
and y(cid:48)       yt

   (xt, yt)     t
and y(cid:48)       yt

(cid:73) intuition: instead of    xing ||  || we    x the margin    = 1

introduction to machine learning

79(124)

support vector machines

support vector machines
what if data is not separable? (original problem: will not satisfy
the constraints!)

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)             (xt, y(cid:48))     1       t and   t     0

   (xt, yt)     t and y(cid:48)       yt

  t: slack variable representing amount of constraint violation
if data is separable, optimal solution has   i = 0,    i
c balances focus on margin (c < 1

2 ) and on error (c > 1
2 )

introduction to machine learning

80(124)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)             (xt, y(cid:48))     1       t
where   t     0 and    (xt, yt)     t and y(cid:48)       yt

(cid:73) computing the dual form results in a quadratic programming

problem     a well-known id76 problem
[boyd and vandenberghe 2004]

(cid:73) can we have representation of this objective that allows more

direct optimization?

introduction to machine learning

81(124)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)     max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))     1       t

introduction to machine learning

81(124)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

(cid:124)

        (xt, y(cid:48))             (xt, yt)

negated margin for example

(cid:123)(cid:122)

(cid:125)

introduction to machine learning

81(124)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 +

  
2

|t |(cid:88)

t=1

  t

   =

1
c

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

(cid:124)

        (xt, y(cid:48))             (xt, yt)

negated margin for example

(cid:123)(cid:122)

(cid:125)

introduction to machine learning

81(124)

support vector machines

support vector machines

  t     1 + max
y(cid:48)(cid:54)=yt

(cid:124)

(cid:123)(cid:122)

        (xt, y(cid:48))             (xt, yt)

(cid:125)

negated margin for example

(cid:73) if (cid:107)  (cid:107) classi   es (xt, yt) with margin 1, penalty   t = 0
(cid:73) otherwise:   t = 1 + maxy(cid:48)(cid:54)=yt         (xt, y(cid:48))             (xt, yt)
(cid:73) that means that in the end   t will be:

  t = max{0, 1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt)}

introduction to machine learning

82(124)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2+

  
2

|t |(cid:88)

t=1

  t s.t.   t     1+ max
y(cid:48)(cid:54)=yt

      (xt, y(cid:48))         (xt, yt)

   = arg min

l(t ;   ) = arg min

loss((xt, yt);   ) +

  
2

hinge loss

|t |(cid:88)

  

t=1

= arg min

  

max (0, 1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt))

||  ||2

  
2

(cid:73) we will see later how to do e   cient optimization of hinge loss

introduction to machine learning

83(124)

  

       |t |(cid:88)

t=1

||  ||2

       +

support vector machines

summary

what we have covered

(cid:73) linear learners
(cid:73) naive bayes
(cid:73) id28
(cid:73) id88
(cid:73) support vector machines

what is next

(cid:73) id173
(cid:73) online learning
(cid:73) non-linear models

introduction to machine learning

84(124)

id173

id173

introduction to machine learning

85(124)

fit of a model

id173

(cid:73) two sources of error:

(cid:73) bias error, measures how well the hypothesis class    ts the

space we are trying to model

(cid:73) variance error, measures sensitivity to training set selection
(cid:73) want to balance these two things

introduction to machine learning

86(124)

over   tting

id173

(cid:73) early in lecture we made assumption data was i.i.d.
(cid:73) rarely is this true

(cid:73) e.g., syntactic analyzers typically trained on 40,000 sentences

from early 1990s wsj news text

(cid:73) even more common: t is very small
(cid:73) this leads to over   tting

(cid:73) e.g.:    fake    is never a verb in wsj treebank (only adjective)

(cid:73) high weight on      (x, y) = 1 if x=fake and y=adjective   
(cid:73) of course: leads to high log-likelihood / low error

(cid:73) other features might be more indicative
(cid:73) adjacent word identities:    he wants to x his death        x=verb

introduction to machine learning

87(124)

id173

id173

(cid:73) in practice, we regularize models to prevent over   tting

l(t ;   )       r(  )

arg max

  

(cid:73) where r(  ) is the id173 function
(cid:73)    controls how much to regularize
(cid:73) common functions

(cid:73) l2: r(  )     (cid:107)  (cid:107)2 = (cid:107)  (cid:107) =(cid:112)(cid:80)
(cid:73) l0: r(  )     (cid:107)  (cid:107)0 =(cid:80)

i   2

(cid:73) approximate with l1: r(  )     (cid:107)  (cid:107)1 =(cid:80)

(cid:73) non-convex

i |  i|

i [[  i > 0]]     zero weights desired

i     smaller weights desired

introduction to machine learning

88(124)

id28 with l2 id173

id173

|t |(cid:88)

(cid:16)

(cid:73) perhaps most common learner in nlp

l(t ;   )       r(  ) =

log

e      (xt ,yt )/zx

t=1

(cid:73) what are the new partial derivatives?

   
   wi

l(t ;   )        
   wi

(cid:73) we know    
   wi
(cid:73) just need    
   wi

l(t ;   )
2(cid:107)  (cid:107)2 =    

  

   wi

  
2

(cid:16)(cid:112)(cid:80)

  r(  )
(cid:17)2

i   2
i

=    
   wi

(cid:17)       

2

(cid:107)  (cid:107)2

(cid:80)

i   2

i =     i

  
2

introduction to machine learning

89(124)

support vector machines

id173

(cid:73) id166 in hinge-loss formulation: l2 id173 corresponds

to margin maximization!

l(t ;   ) +   r(  )

   = arg min

  

= arg min

  

= arg min

  

= arg min

t=1

|t |(cid:88)
|t |(cid:88)
|t |(cid:88)

t=1

  

t=1

loss((xt , yt );   ) +   r(  )

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +   r(  )

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

introduction to machine learning

90(124)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

introduction to machine learning

91(124)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

id166s/hinge-loss: max (0, 1 + maxy(cid:54)=yt (        (xt , y)             (xt , yt )))

|t |(cid:88)

   = arg min

  

t=1

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

introduction to machine learning

91(124)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

id166s/hinge-loss: max (0, 1 + maxy(cid:54)=yt (        (xt , y)             (xt , yt )))

|t |(cid:88)

   = arg min

max (0, 1 + max
y(cid:54)=yt

  

id28/log-loss:     log (cid:0)e      (xt ,yt )/zx

t=1

(cid:1)

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

|t |(cid:88)

(cid:16)

    log

e      (xt ,yt )/zx

(cid:17)

+

(cid:107)  (cid:107)2

  
2

   = arg min

  

t=1

introduction to machine learning

91(124)

generalized linear learners

   = arg min

  

l(t ;   ) +   r(  ) = arg min

  

t=1

id173

loss((xt , yt );   ) +   r(  )

|t |(cid:88)

introduction to machine learning

92(124)

which learner to use?

id173

(cid:73) trial and error

(cid:73) training time available

(cid:73) choice of features is often more important

introduction to machine learning

93(124)

online learning

online learning

introduction to machine learning

94(124)

online vs. batch learning

online learning

batch(t );

(cid:73) for 1 . . . n

(cid:73)        update(t ;   )

(cid:73) return   

online(t );

(cid:73) for 1 . . . n

(cid:73) for (xt, yt)     t

(cid:73)        update((xt , yt );   )

(cid:73) end for

(cid:73) end for
(cid:73) return   

e.g., id166s, logistic regres-
sion, naive bayes

e.g., id88

   =    +   (xt, yt)       (xt, y)

introduction to machine learning

95(124)

batch id119

online learning

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

  i =   i   1       (cid:79)l(t ;   i   1)

|t |(cid:88)

=   i   1    

  (cid:79)loss((xt, yt);   i   1)

(cid:73)    > 0 and set so that l(t ;   i ) < l(t ;   i   1)

t=1

introduction to machine learning

96(124)

stochastic id119

online learning

(cid:73) stochastic id119 (sgd)

(cid:73) approximate batch gradient (cid:79)l(t ;   ) with stochastic

gradient (cid:79)loss((xt, yt);   )

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence
(cid:73) sample (xt , yt )     t
(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   i   1)

//    stochastic   

(cid:73) return   

introduction to machine learning

97(124)

sgd over finite training data

online learning

(cid:73) nlp practice: cycling over    nite data set

(cid:73) set   0 = o m
(cid:73) for 1 . . . n

(cid:73) for (xt , yt )     t
(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   i   1)

(cid:73) return   

introduction to machine learning

98(124)

online id28

online learning

(cid:73) stochastic id119 (sgd)
(cid:73) loss((xt, yt);   ) = log-loss

(cid:73) (cid:79)loss((xt, yt);   ) = (cid:79)(cid:0)    log (cid:0)e      (xt ,yt )/zxt
(cid:1)(cid:1)
(cid:32)
(cid:79)(cid:16)    log
  (xt, yt)    (cid:88)

(cid:73) from id28 section:

e      (xt ,yt )/zxt

(cid:17)(cid:17)

=    

(cid:16)

(cid:33)

p(y|x)  (xt, y)

(cid:73) plus id173 term (if part of model)

y

introduction to machine learning

99(124)

online learning

(cid:19)

        (xt, y)             (xt, yt))

online id166s

(cid:73) stochastic id119 (sgd)
(cid:73) loss((xt, yt);   ) = hinge-loss

(cid:18)

(cid:79)loss((xt, yt);   ) = (cid:79)

max (0, 1 + max
y(cid:54)=yt

(cid:73) subgradient is:

max (0, 1 + max
y(cid:54)=yt

(cid:19)

        (xt, y)             (xt, yt))

(cid:18)
(cid:40)

(cid:79)

=

if         (xt, yt)     maxy         (xt, y)     1
0,
  (xt, y)       (xt, yt), otherwise, where y = maxy         (xt, y)

(cid:73) plus id173 term (required for id166s)

introduction to machine learning

100(124)

id88 and hinge-loss

online learning

id166 subgradient update looks like id88 update

  i =   i   1       

if         (xt , yt )     maxy         (xt , y)     1
0,
  (xt , y)       (xt , yt ), otherwise, where y = maxy         (xt , y)

(cid:40)

id88

  i =   i   1       

(cid:40)

if         (xt , yt )     maxy         (xt , y)     0
0,
  (xt , y)       (xt , yt ), otherwise, where y = maxy         (xt , y)

where    = 1, note   (xt , y)       (xt , yt ) not   (xt , yt )       (xt , y) since           (descent)

id88 = sgd with no-margin hinge-loss

max (0, 1+ max
y(cid:54)=yt

        (xt, y)             (xt, yt))

introduction to machine learning

101(124)

online vs. batch learning

online learning

(cid:73) online algorithms

(cid:73) each update step relies only on the derivative for a single

randomly chosen example

(cid:73) computational cost of one step is 1/t compared to batch
(cid:73) easier to implement

(cid:73) larger variance since each gradient is di   erent

(cid:73) variance slows down convergence
(cid:73) requires    ne-tuning of decaying learning rate

(cid:73) batch algorithms

(cid:73) higher cost of averaging gradients over t for each update

(cid:73) implementation more complex
(cid:73) less    ne-tuning, e.g., allows constant learning rates
(cid:73) faster convergence

introduction to machine learning

102(124)

variance-reduced online learning

online learning

(cid:73) sgd update extended by velocity vector v weighted by

momentum coe   cient 0        < 1 [polyak 1964]:

(cid:73)

where

  i+1 =   i       (cid:79)loss((xt, yt);   i ) +   vi

vi =   i       i   1

(cid:73) momentum accelerates learning if gradients are aligned along
same direction, and restricts changes when successive gradient
are opposite of each other

(cid:73) general direction of gradient reinforced, perpendicular

directions    ltered out

(cid:73) best of both worlds: e   cient and e   ective!

introduction to machine learning

103(124)

online-to-batch conversion

online learning

(cid:73) classical online learning:

(cid:73) data are given as an in   nite sequence of input examples
(cid:73) model makes prediction on next example in sequence
(cid:73) online error is averaged over predictions after each update

(cid:73) standard nlp applications:

(cid:73) finite set of training data, prediction on new batch of test data
(cid:73) online learning applied by cycling over    nite data
(cid:73) online-to-batch conversion: which model to use at test time?

(cid:73) last model? random model? best model on heldout set?

introduction to machine learning

104(124)

online-to-batch conversion by averaging

online learning

(cid:73) averaged id88

(cid:73)      =(cid:0)(cid:80)

i   (i)(cid:1) / (n    t )

(cid:73) use weight vector averaged over online updates for prediction
(cid:73) how does the id88 mistake bound carry over to batch?
(cid:73) let mk be number of mistakes made during online learning,

then with id203 of at least 1       :

e[loss((x, y);     ]     mk +

(cid:114) 2

ln

1
  

k

(cid:73) = generalization bound based on online performance

[cesa-bianchi et al. 2004]

(cid:73) can be applied to all online learners with convex losses

introduction to machine learning

105(124)

summary

quick summary

introduction to machine learning

106(124)

linear learners

summary

(cid:73) naive bayes, id88, id28 and id166s
(cid:73) generative vs. discriminative
(cid:73) objective functions and id168s

(cid:73) log-loss, min error and hinge loss
(cid:73) generalized linear learners

(cid:73) id173
(cid:73) online vs. batch learning

introduction to machine learning

107(124)

non-linear models

non-linear models

introduction to machine learning

108(124)

non-linear models

(cid:73) some data sets require more than a linear decision boundary

to be correctly modeled

(cid:73) decision boundary is no longer a hyperplane in the feature

non-linear models

space

(cid:73) a lot of models out there
(cid:73) k-nearest neighbours
(cid:73) id90
(cid:73) neural networks
(cid:73) kernels

introduction to machine learning

109(124)

kernels

(cid:73) a kernel is a similarity function between two points that is
symmetric and positive semi-de   nite, which we denote by:

non-linear models

k (xt, xr )     r
(cid:73) let m be a n    n matrix such that ...

mt,r = k (xt, xr )

(cid:73) ... for any n points. called the gram matrix.
(cid:73) symmetric:

k (xt, xr ) = k (xr , xt)

(cid:73) positive de   nite: positivity on diagonal

k (x, x)     0 forall x with equality only for x = 0

introduction to machine learning

110(124)

kernels

non-linear models

(cid:73) mercer   s theorem: for any kernel k , there exists an   , in

some rd , such that:

k (xt, xr ) =   (xt)      (xr )

(cid:73) since our features are over pairs (x, y), we will write kernels

over pairs

k ((xt, yt), (xr , yr )) =   (xt, yt)      (xr , yr )

introduction to machine learning

111(124)

kernel trick: general overview

non-linear models

(cid:73) de   ne a kernel, and do not explicitly use dot product between

vectors, only kernel calculations

(cid:73) in some high-dimensional space, this corresponds to dot

product

(cid:73) in that space, the decision boundary is linear, but in the

original space, we now have a non-linear decision boundary

(cid:73) let   s do it for the id88!

introduction to machine learning

112(124)

non-linear models

t=1

for n : 1..n

kernel trick     id88 algorithm
training data: t = {(xt , yt )}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.
(cid:73) each feature function   (xt, yt) is added and   (xt, y) is

for t : 1..t
let y = arg maxy   (i)      (xt , y)
if y (cid:54)= yt

  (i+1) =   (i) +   (xt , yt )       (xt , y)
i = i + 1

return   i

subtracted to    say   y,t times

(cid:73)   y,t is the # of times during learning label y is predicted for

example t

(cid:73) thus,

   =

(cid:88)

  y,t[  (xt, yt)       (xt, y)]

t,y

introduction to machine learning

113(124)

kernel trick     id88 algorithm

non-linear models

(cid:73) we can re-write the argmax function as:

y    = arg maxy      (i)      (x, y   )

=

=

=

(cid:73) we can then re-write the id88 algorithm strictly with

kernels

introduction to machine learning

114(124)

kernel trick     id88 algorithm

(cid:73) we can re-write the argmax function as:

non-linear models

  (i)      (x, y   )

y    = arg max

y   

= arg max

y   

= arg max

y   

= arg max

y   

(cid:88)
(cid:88)
(cid:88)

t,y

t,y

t,y

  y,t[  (xt, yt)       (xt, y)]      (x, y   )

  y,t[  (xt, yt)      (xt, y   )       (xt, y)      (x, y   )]

  y,t[k ((xt, yt), (xt, y   ))     k ((xt, y), (x, y   ))]

(cid:73) we can then re-write the id88 algorithm strictly with

kernels

introduction to machine learning

115(124)

non-linear models

kernel trick     id88 algorithm
training data: t = {(xt , yt )}|t |
1.
2.
3.
4.
5.
6.

let y    = arg maxy   (cid:80)

   y, t set   y,t = 0
for n : 1..n

for t : 1..t
if y    (cid:54)= yt

  y   ,t =   y   ,t + 1

t=1

t,y   y,t [k ((xt , yt ), (xt , y   ))     k ((xt , y), (xt , y   ))]

(cid:73) given a new instance x

(cid:88)

t,y

y    = arg max

y   

  y,t[k ((xt, yt), (x, y   ))   k ((xt, y), (x, y   ))]

(cid:73) but it seems like we have just complicated things???

introduction to machine learning

116(124)

non-linear models

kernels = tractable non-linearity

(cid:73) a linear model in a higher dimensional feature space is a

non-linear model in the original space

(cid:73) computing a non-linear kernel is often better computationally

than calculating the corresponding dot product in the high
dimensional feature space

(cid:73) thus, kernels allow us to e   ciently learn non-linear models by

id76

introduction to machine learning

117(124)

linear learners in high dimension

non-linear models

introduction to machine learning

118(124)

non-linear models

example: polynomial kernel

(cid:73)   (x)     rm , d     2
(cid:73) k (xt, xs ) = (  (xt)      (xs ) + 1)d
(cid:73) o(m) to calculate for any d!!

(cid:73) but in the original feature space (primal space)
(cid:73) consider d = 2, m = 2, and   (xt) = [xt,1, xt,2]

(  (xt )      (xs ) + 1)2 = ([xt,1, xt,2]    [xs,1, xs,2] + 1)2

= (xt,1xs,1 + xt,2xs,2 + 1)2
= (xt,1xs,1)2 + (xt,2xs,2)2 + 2(xt,1xs,1) + 2(xt,2xs,2)

+2(xt,1xt,2xs,1xs,2) + (1)2

which equals:
   

[(xt,1)2, (xt,2)2,

(cid:124)

   

2xt,2,

   

2xt,1,

(cid:123)(cid:122)

2xt,1xt,2, 1]

[(xs,1)2, (xs,2)2,

2xs,1,

2xs,1xs,2, 1]

   

   

2xs,2,

   

(cid:123)(cid:122)

(cid:125)

(cid:125)

  

(cid:124)

feature vector in high-dimensional space

feature vector in high-dimensional space

introduction to machine learning

119(124)

non-linear models

popular kernels

(cid:73) polynomial kernel

k (xt, xs ) = (  (xt)      (xs ) + 1)d
(cid:73) gaussian radial basis kernel (in   nite feature space

representation!)

k (xt, xs ) = exp(

   ||  (xt)       (xs )||2

2  

)

(cid:73) string kernels
(cid:73) tree kernels

introduction to machine learning

120(124)

kernels summary

non-linear models

(cid:73) can turn a linear model into a non-linear model
(cid:73) kernels project feature space to higher dimensions

(cid:73) sometimes exponentially larger
(cid:73) sometimes an in   nite space!

(cid:73) can    kernelize    algorithms to make them non-linear
(cid:73) id76 methods still applicable to learn

parameters

introduction to machine learning

121(124)

kernels for large training sets

(cid:73) exact kernel methods depend polynomially on the number of

training examples - infeasible for large datasets

(cid:73) alternative: explicit randomized feature map

non-linear models

[rahimi and recht 2007]

(cid:73) shallow neural network by random fourier transformation:

(cid:73) random weights from input to hidden units
(cid:73) cosine as transfer function
(cid:73) linear learning of weights from hidden to output units

introduction to machine learning

122(124)

wrap up and questions

wrap up and time for questions

introduction to machine learning

123(124)

wrap up and questions

summary
basic principles of machine learning:

(cid:73) to do learning, we set up an objective function that tells the

   t of the model to the data

(cid:73) we optimize with respect to the model (weights, id203

model, etc.)

(cid:73) can do it in a batch or online (preferred!) fashion

what model to use?

(cid:73) one example of a model: linear model
(cid:73) can kernelize/randomize these models to get non-linear

models

(cid:73) id76 applicable for both types of model

introduction to machine learning

124(124)

references

further reading
(cid:73) introductory example:
(cid:73) j. y. lettvin, h. r. maturana, w. s. mcculloch, and w. h. pitts. 1959.

what the frog   s eye tells the frog   s brain. proc. inst. radio engr., 47:1940   1951.

(cid:73) naive bayes:
(cid:73) pedro domingos and michael pazzani. 1997.

on the optimality of the simple bayesian classi   er under zero-one loss. machine
learning, (29):103   130.

(cid:73) id28:
(cid:73) bradley efron. 1975.

the e   ciency of id28 compared to normal discriminant analysis.
journal of the american statistical association, 70(352):892   898.

(cid:73) adam l. berger, vincent j. della pietra, and stephen a. della pietra. 1996.
a maximum id178 approach to natural language processing. computational
linguistics, 22(1):39   71.

(cid:73) stefan riezler, detlef prescher, jonas kuhn, and mark johnson. 2000.

introduction to machine learning

124(124)

lexicalized stochastic modeling of constraint-based grammars using log-linear
measures and em training. in proceedings of the 38th annual meeting of the
association for computational linguistics (acl   00), hong kong.

references

(cid:73) id88:
(cid:73) albert b.j. noviko   . 1962.

on convergence proofs on id88s. symposium on the mathematical theory of
automata, 12:615   622.

(cid:73) yoav freund and robert e. schapire. 1999.

large margin classi   cation using the id88 algorithm. journal of machine
learning research, 37:277   296.

(cid:73) michael collins. 2002.

discriminative training methods for id48: theory and experiments
with id88 algorithms. in proceedings of the conference on empirical
methods in natural language processing (emnlp   02), philadelphia, pa.

(cid:73) id166:
(cid:73) vladimir n. vapnik. 1998.

statistical learning theory. wiley.

(cid:73) olivier chapelle. 2007.

introduction to machine learning

124(124)

references

training a support vector machine in the primal. neural computation,
19(5):1155   1178.

(cid:73) ben taskar, carlos guestrin, and daphne koller. 2003.

max-margin markov networks. in advances in neural information processing
systems 17 (nips   03), vancouver, canada.

(cid:73) ioannis tsochantaridis, thomas hofmann, thorsten joachims, and yasemin altun.

2004.
support vector machine learning for interdependent and structured output spaces.
in proceedings of the 21st international conference on machine learning
(icml   04), ban   , canada.
(cid:73) kernels and id173:
(cid:73) bernhard sch  olkopf and alexander j. smola. 2002.

learning with kernels. support vector machines, id173, optimization, and
beyond. the mit press.

(cid:73) ali rahimi and ben recht. 2007.

random features for large-scale kernel machines. in advances in neural
information processing systems (nips), vancouver, b.c., canada.

introduction to machine learning

124(124)

references

(cid:73) zhiyun lu, dong guo, alireza bagheri garakani, kuan liu, avner may, aurelien
bellet, linxi fan, michael collins, brian kingsbury, michael picheny, and fei sha.
2016.
a comparison between deep neural nets and kernel acoustic models for speech
recognition. in ieee international conference on acoustics, speech, and signal
processing (icassp).

(cid:73) convex and non-id76:
(cid:73) yurii nesterov. 2004.

introductory lectures on id76: a basic course. springer.

(cid:73) stephen boyd and lieven vandenberghe. 2004.

id76. cambridge university press.
(cid:73) dimitri p. bertsekas and john n. tsitsiklis. 1996.
neuro-id145. athena scienti   c.

(cid:73) online/stochastic optimization:
(cid:73) herbert robbins and sutton monro. 1951.

a stochastic approximation method. annals of mathematical statistics,
22(3):400   407.

(cid:73) boris t. polyak. 1964.

introduction to machine learning

124(124)

references

some methods of speeding up the convergence of iteration methods. ussr
computational mathematics and mathematical physics, 4(5):1     17.

(cid:73) nicol`o cesa-bianchi, alex conconi, and claudio gentile. 2004.

on the generalization ablility of on-line learning algorithms. ieee transactons on
id205, 50(9):2050   2057.

(cid:73) nicolas leroux, mark schmidt, and francis bach. 2012.

a stochastic gradient method with an exponential convergence rate for    nite
training sets. in advances in neural information processing systems (nips), lake
tahoe, ca.

(cid:73) ilya sutskever, james martens, george e. dahl, and geo   rey e. hinton. 2013.

on the importance of initialization and momentum in deep learning. in proceedings
of international conference on machine learning (icml), pages 1139   1147.

introduction to machine learning

124(124)

