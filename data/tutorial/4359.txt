syntax and parsing ii 

id33

slav petrov     google 

thanks to: 

dan klein, ryan mcdonald, alexander rush, joakim nivre,    

greg durrett, david weiss 

lisbon machine learning school 2016

id33

root

nsubj

prep

dobj

det

pobj

verb

pron
noun
they     solved     the   problem     with    statistics

noun

adp

det

conll format

root
su

vc

obj1

mod

punct

  * cathy zag hen wild zwaaien . 
   0    1     2     3    4        5      6

http://ilk.uvt.nl/conll/

(non-)projectivity

    crossing arcs needed to account for non-

projective constructions 

    fairly rare in english but can be common 

in other languages (e.g. czech): 

formal conditions

styles of id33

    transition-based (tr)  
    fast, greedy, linear time 
    trained for greedy search 
    id125

id136 algorithms 

    graph-based (gr)  

    slower, exhaustive, dynamic 

programming id136 
algorithms 

    higher-order factorizations

y
c
a
r
u
c
c
a

greedy tr

o(n)

k-b est tr
( k   n
o

)

1st-order gr

3rd-order gr

o(n4)

2nd-order gr

o(n3)

[nivre et al.    03-   11]

o(n3)

time

[mcdonald et al.    05-   06]

arc-factored models

graph-based parsing

    assumes that scores factor over the tree 
    arc-factored models 
    score(tree) =     edges

i washed dishes with detergent  =     i washed 
                                 
                                                           washed dishes 

                                                           washed with  

                                                           with detergent

+
+
+

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

dependency representation

representation

*

as

mcgwire

neared

,

fans

went

wild

heads

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

modi   ers

graph-based parsing

arc-factored projective parsing

arc-factored projective parsing

eisner algorithm

eisner first-order rules

eisner first-order parsing

 

+

m

h

r

r + 1

m

 

+

e

h

m

m

e

h

h

in practice also left arc version

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner first-order parsing

first-order parsing

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

eisner algorithm pseudo code

maximum spanning trees (msts)

can use mst algorithms for nonprojective parsing!

chu-liu-edmonds

chu-liu-edmonds

find cycle and contract

recalculate edge weights

theorem

final mst

chu-liu-edmonds pseudocode

chu-liu-edmonds pseudocode

arc weights

arc feature ideas for f(i,j,k)

    identities of the words wi and wj and the label lk 
    part-of-speech tags of the words wi and wj and the label lk 
    part-of-speech of words surrounding and between wi and wj 
    number of words between wi and wj , and their orientation 
    combinations of the above

first-order feature calculation

first-order feature computation

*

as

mcgwire

neared

,

fans

went

wild

*

as

mcgwire

neared

,

fans

went

wild

a

m

n

e

s

e

,

f

a

w

w

i

l

n

d

t

c

a

g

r

e

d

w

i

r

e

n

s

[went]

[verb]

[went, as]

[verb, in]

[adj, *, adp]

[nns, vbd, adp]

[nns, adp, nnp]

[adp, left, 5]

[jj, *, in]

[noun, verb, in]

[noun, in, noun]

[vbd]

[as]

[vbd, adp]

[vbd, as, adp]

[vbd, *, adp]

[nns, vbd, *]

[nns, vbd, nnp]

[verb, as, in]

[verb, *, in]

[noun, verb, *]

[noun, verb, noun]

[as]

[in]

[went, verb]

[went, as, adp]

[vbd, adj, adp]

[adj, adp, nnp]

[went, left, 5]

[went, as, in]

[verb, jj, in]

[jj, in, noun]

[went, left, 5]

[adp]

[went, vbd]

[as, in]

[went, vbd, adp]

[vbd, adj, *]

[went]

[as, adp]

[went, as]

[went, vbd, as]

[nns, *, adp]

[vbd, adp, nnp]

[vbd, adj, nnp]

[vbd, left, 5]

[went, verb, in]

[verb, jj, *]

[as, left, 5]

[went, verb, as]

[noun, *, in]

[verb, in, noun]

[verb, jj, noun]

[verb, left, 5]

[as, left, 5]

[in, left, 5]

[went, vbd, as, adp]

[vbd, adj, *, adp]

[nns, vbd, *, adp]

[vbd, adj, adp, nnp]

[nns, vbd, adp, nnp]

[went, vbd, left, 5]

[as, adp, left, 5]

[went, as, left, 5]

[vbd, adp, left, 5]

[went, verb, as, in]

[went, verb, left, 5]

[verb, jj, *, in]

[noun, verb, *, in]

[verb, jj, in, noun]

[noun, verb, in, noun]

[as, in, left, 5]

[went, as, left, 5]

[verb, in, left, 5]

[vbd, as, adp, left, 5]

[went, as, adp, left, 5]

[went, vbd, adp, left, 5]

[went, vbd, as, left, 5]

[adj, *, adp, left, 5]

[vbd, *, adp, left, 5]

[vbd, adj, adp, left, 5]

[vbd, adj, *, left, 5]

[nns, *, adp, left, 5]

[nns, vbd, adp, left, 5]

[nns, vbd, *, left, 5]

[adj, adp, nnp, left, 5]

[vbd, adp, nnp, left, 5]

[vbd, adj, nnp, left, 5]

[nns, adp, nnp, left, 5]

[nns, vbd, nnp, left, 5]

[verb, as, in, left, 5]

[went, as, in, left, 5]

[went, verb, in, left, 5]

[went, verb, as, left, 5]

[jj, *, in, left, 5]

[verb, *, in, left, 5]

[verb, jj, in, left, 5]

[verb, jj, *, left, 5]

[noun, *, in, left, 5]

[noun, verb, in, left, 5]

(structured) id88

transition based id33

    process sentence left to right 

    different transition strategies available 
    delay decisions by pushing on stack   

    arc-standard transition strategy [nivre    03] 

 
 

initial configuration: ([],[0,   ,n],[])   
terminal configuration: ([0],[],a) 

(cid:15482) ([  |i],  ,a) 

,[i|

 
  
],a) 
 
 

shift: (
  
left-arc (label): ([
|i|j],b,a) 
right-arc (label): ([
|i|j],b,a) 

  

  

(cid:15482) ([  |j],b,a   {j,l,i}) 
(cid:15482) ([  |i],b,a   {i,l,j})

arc-standard example

    stack

    buffer

i

booked

a

flight

to

lisbon

shift

i

booked

a

flight

to

lisbon

arc-standard example

    stack

i

    buffer

booked

a

flight

to

lisbon

shift

i

booked

a

flight

to

lisbon

arc-standard example

    stack

booked

i

    buffer

a

flight

to

lisbon

left-arc   

nsubj

i

booked

a

flight

to

lisbon

arc-standard example

    stack

    buffer

 i        booked

a

flight

to

lisbon

shift

nsubj

i

booked

a

flight

to

lisbon

arc-standard example

    stack

a

 i        booked

    buffer

flight

to

lisbon

shift

nsubj

i

booked

a

flight

to

lisbon

arc-standard example

    buffer

to

lisbon

    stack

flight

a

 i        booked

left-arc   

det

nsubj

i

booked

a

flight

to

lisbon

arc-standard example

    stack

    buffer

to

lisbon

 a          flight

 i        booked

shift

nsubj

det

i

booked

a

flight

to

lisbon

arc-standard example

    buffer

lisbon

    stack

to

 a          flight

 i        booked

shift

nsubj

det

i

booked

a

flight

to

lisbon

arc-standard example

    buffer

    stack

lisbon

to

 a          flight

 i        booked

right-arc   

pobj

nsubj

det

i

booked

a

flight

to

lisbon

arc-standard example

    stack

    buffer

 to        lisbon

 a          flight

 i        booked

right-arc   

prep

nsubj

det

pobj

i

booked

a

flight

to

lisbon

arc-standard example

    stack

    buffer

a flight to lisbon

 i        booked

right-arc   

dobj

nsubj

det

prep

pobj

i

booked

a

flight

to

lisbon

arc-standard example

    stack

    buffer

i booked a flight to lisbon

dobj

nsubj

det

prep

pobj

i

booked

a

flight

to

lisbon

features

    stack

    buffer

to

lisbon

 a          flight

 i        booked

shift

right-arc?

left-arc?

stack top word =    flight    
stack top pos tag =    noun    
buffer front word =    to    
child of stack top word =    a    
....

features zpar parser

# from single words
pair { stack.tag stack.word }
stack { word tag }
pair { input.tag input.word }
input { word tag }
pair { input(1).tag input(1).word }
input(1) { word tag }
pair { input(2).tag input(2).word }
input(2) { word tag }

# from word pairs
quad { stack.tag stack.word input.tag input.word }
triple { stack.tag stack.word input.word }
triple { stack.word input.tag input.word }
triple { stack.tag stack.word input.tag }
triple { stack.tag input.tag input.word }
pair { stack.word input.word }
pair { stack.tag input.tag }
pair { input.tag input(1).tag }

# from word triples
triple { input.tag input(1).tag input(2).tag }
triple { stack.tag input.tag input(1).tag }
triple { stack.head(1).tag stack.tag input.tag }
triple { stack.tag stack.child(-1).tag input.tag }
triple { stack.tag stack.child(1).tag input.tag }
triple { stack.tag input.tag input.child(-1).tag }

# distance
pair { stack.distance stack.word }
pair { stack.distance stack.tag }
pair { stack.distance input.word }
pair { stack.distance input.tag }
triple { stack.distance stack.word input.word }
triple { stack.distance stack.tag input.tag }

# valency
pair { stack.word stack.valence(-1) }
pair { stack.word stack.valence(1) }
pair { stack.tag stack.valence(-1) }
pair { stack.tag stack.valence(1) }
pair { input.word input.valence(-1) }
pair { input.tag input.valence(-1) }

# unigrams
stack.head(1) {word tag}
stack.label
stack.child(-1) {word tag label}
stack.child(1) {word tag label}
input.child(-1) {word tag label}

# third order
stack.head(1).head(1) {word tag}
stack.head(1).label
stack.child(-1).sibling(1) {word tag label}
stack.child(1).sibling(-1) {word tag label}
input.child(-1).sibling(1) {word tag label}
triple { stack.tag stack.child(-1).tag stack.child(-1).sibling(1).tag }
triple { stack.tag stack.child(1).tag stack.child(1).sibling(-1).tag }
triple { stack.tag stack.head(1).tag stack.head(1).head(1).tag }
triple { input.tag input.child(-1).tag input.child(-1).sibling(1).tag }

# label set
pair { stack.tag stack.child(-1).label }
triple { stack.tag stack.child(-1).label stack.child(-1).sibling(1).label }
quad { stack.tag stack.child(-1).label stack.child(-1).sibling(1).label stack.child(-1).sibling(2).label }
pair { stack.tag stack.child(1).label }
triple { stack.tag stack.child(1).label stack.child(1).sibling(-1).label }
quad { stack.tag stack.child(1).label stack.child(1).sibling(-1).label stack.child(1).sibling(-2).label }
pair { input.tag input.child(-1).label }
triple { input.tag input.child(-1).label input.child(-1).sibling(1).label }
quad { input.tag input.child(-1).label input.child(-1).sibling(1).label input.child(-1).sibling(2).label }

id166 / structured id88 hyperparameters

    id173 
    id168 
    hand-crafted features

neural network transition based parser

[chen & manning    14] and [weiss et al.    15]

softmax

hidden layer

words

pos

labels

   

embedding layer

f0 = 1 [buffer0-word =    to   ]
f1 = 1 [buffer1-word =    bilbao   ]
f2 = 1 [buffer0-pos =    in   ]
f3 = 1 [stack0-label =    pobj   ]

      

atomic inputs

neural network transition based parser

 [weiss et al.    15]

softmax

hidden layer

words

pos

labels

   

embedding layer

f0 = 1 [buffer0-word =    to   ]
f1 = 1 [buffer1-word =    bilbao   ]
f2 = 1 [buffer0-pos =    in   ]
f3 = 1 [stack0-label =    pobj   ]

      

atomic inputs

neural network transition based parser

 [weiss et al.    15]

softmax

hidden layer

words

pos

labels

   

embedding layer

f0 = 1 [buffer0-word =    to   ]
f1 = 1 [buffer1-word =    bilbao   ]
f2 = 1 [buffer0-pos =    in   ]
f3 = 1 [stack0-label =    pobj   ]

      

atomic inputs

neural network transition based parser

 [weiss et al.    15]

softmax

hidden layer 2

hidden layer

1

words

pos

labels

   

embedding layer

f0 = 1 [buffer0-word =    to   ]
f1 = 1 [buffer1-word =    bilbao   ]
f2 = 1 [buffer0-pos =    in   ]
f3 = 1 [stack0-label =    pobj   ]

      

atomic inputs

neural network transition based parser

. wfs

 [weiss et al.    15]

words

pos

labels

   

f0 = 1 [buffer0-word =    to   ]
f1 = 1 [buffer1-word =    bilbao   ]
f2 = 1 [buffer0-pos =    in   ]
f3 = 1 [stack0-label =    pobj   ]

      

structured
id88

english results (wsj 23)

las beam

method

3rd-order graph-based (zm2014)
transition-based linear (zn2011)

uas
93.22 91.02
93.00 90.95
91.80 89.60
92.58 90.54
nn deeper network (weiss et al., 2015) 93.19 91.18

nn baseline (chen & manning, 2014)
nn better sgd (weiss et al., 2015)

-
32
1
1
1

nn hyperparameters

    id173 
    id168

nn hyperparameters

    id173 
    id168
    dimensions 
    activation function 
    initialization 
    adagrad 
    dropout

nn hyperparameters

    id173 
    id168
    dimensions 
    activation function 
    initialization 
    adagrad 
    dropout
    mini-batch size 
    initial learning rate 
    learning rate schedule 
    momentum 

    stopping time 
    parameter averaging 

nn hyperparameters

optimization matters! 

use random restarts, grid  
pick best using holdout data 

tune: wsj s24 
dev: wsj s22 
test: wsj s23

random restarts: how much variance?

92.7

92.6

92.5

92.4

92.3

92.2

92.1

t

e
s
 
v
e
d
 
j
s
w
n
o

 

 
)

%

(
 

s
a
u

92

 

91.2

variance of networks on tuning/dev set
pretrained 200x200
pretrained 200
200x200
200

 

2nd hidden layer +    
pre training increases 

correlation

92

91.4

91.8
uas (%) on wsj tune set

91.6

effect of embedding dimensions

word tuning on wsj (tune set, dpos,dlabels=32)

 

92

91.5

91

90.5

90

)

%

(
 

s
a
u

pretrained 200x200
pretrained 200
200x200
200

4

8

16

32

64

128

id27 dimension (dwords)

89.5

 
1

2

effect of embedding dimensions

pos/label tuning on wsj (tune set, dwords=64)

 

92

91.5

91

)

%

(
 

s
a
u

90.5

 
1

pretrained 200x200
pretrained 200
200x200
200

2

pos/label embedding dimension (dpos,dlabels)

4

8

16

32

the importance of search

 [weiss et al.    15, andor et al.    16]

globally normalized models
e[
e[

]

]

[andor et al.    16]

    crf objective 
    full id26 training 

    novel proof (label bias): 

    globally normalized models are strictly more 

expressing than locally normalized models

no lookahead parsing

94.0 with 3 token lookahead 

    stack

    buffer

local

(beam=1)

local

 (beam=16)

global

 (beam=16)

95.0

90.0

85.0

80.0

75.0

)

%

(
 
s
a
u

93.681.477.0english results (wsj 23)

las beam

method

3rd-order graph-based (zm2014)
transition-based linear (zn2011)

nn baseline (chen & manning, 2014)
nn better sgd (weiss et al., 2015)

uas
93.22 91.02
93.00 90.95
91.80 89.60
92.58 90.54
nn deeper network (weiss et al., 2015) 93.19 91.18
93.99 92.05
94.61 92.79
95.01 92.97
93.20 90.90
92.83    

nn crf semi-supervised (andor et al.)

nn id88 (weiss et al., 2015)

contrastive nn (zhou et al., 2015)

nn crf (andor et al., 2016)

s-lstm (dyer et al., 2015)

-
32
1
1
1
8
32
32
1
100

tri-training

[zhou et al.    05, li et al.    14]

zpar

parser

berkeley

parser

uas 89.96 
las 87.26 

uas 89.84 
las 87.21 

uas 96.35 
las 95.02 

~40%   
agreement

english out-of-domain results

    train on wsj + web treebank +  questionbank 
    evaluate on web

)

%

(
 
s
a
u

90

89.25

88.5

87.75

87

3rd order graph (zm2014)
transition-based nn (b=1)

transition-based linear (zn 2011, b=32)
transition-based nn (b=8)

supervised

semi-supervised

multilingual results

tensor-based graph (lei et al. '14
transition-based nn (weiss et al. '15)

3rd-order graph (zhang & mcdonald '14)
transition-based crf (andor et al. 16)

95.0

90.0

85.0

80.0

catalan

chinese

czech

english german japanese spanish
no tri-training data
with morph features

)

%

(
 
s
a
u

syntaxnet and parsey mcparseface

syntaxnet and parsey 

    add screenshots once released

lstms vs syntaxnet

lstms

syntaxnet

accuracy

efficiency

end-to-end

recurrence

+
-
++
+

++
+
-
-

(yet)

(yet)

universal dependencies

stanford universal++ dependencies

interset++ morphological features

google universal++ pos tags

http://universaldependencies.github.io/docs/

universal dependencies

summary

    constituency parsing 

    cky algorithm 
    lexicalized grammars 
    latent variable grammars 
    conditional random field parsing 
    neural network representations 

    id33 

    eisner algorithm 
    maximum spanning tree algorithm 
    transition based parsing 
    neural network representations 

