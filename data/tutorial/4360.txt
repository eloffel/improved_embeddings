introduction to machine learning

linear classi   ers

lisbon machine learning school, 2014

ryan mcdonald

google inc., london

e-mail: ryanmcd@google.com

introduction to machine learning

1(107)

linear classi   ers

introduction

(cid:73) go onto acl anthology
(cid:73) search for:    naive bayes   ,    maximum id178   ,    logistic

regression   ,    id166   ,    id88   
(cid:73) do the same on google scholar

(cid:73)    maximum id178    &    nlp    9,000 hits, 240 before 2000
(cid:73)    id166    &    nlp    11,000 hits, 556 before 2000
(cid:73)    id88    &    nlp   , 3,000 hits, 147 before 2000

(cid:73) all are examples of linear classi   ers
(cid:73) all have become tools in any nlp/cl researchers tool-box in

past 15 years

(cid:73) arguably the most important tool

introduction to machine learning

2(107)

introduction

experiment

(cid:73) document 1     label: 0; words: (cid:63) (cid:5)    
(cid:73) document 2     label: 0; words: (cid:63)     (cid:52)
(cid:73) document 3     label: 1; words: (cid:63) (cid:52)    
(cid:73) document 4     label: 1; words: (cid:5) (cid:52)    

introduction to machine learning

3(107)

introduction

experiment

(cid:73) document 1     label: 0; words: (cid:63) (cid:5)    
(cid:73) document 2     label: 0; words: (cid:63)     (cid:52)
(cid:73) document 3     label: 1; words: (cid:63) (cid:52)    
(cid:73) document 4     label: 1; words: (cid:5) (cid:52)    
(cid:73) new document     words: (cid:63) (cid:5)    ; label ?

introduction to machine learning

3(107)

introduction

experiment

(cid:73) document 1     label: 0; words: (cid:63) (cid:5)    
(cid:73) document 2     label: 0; words: (cid:63)     (cid:52)
(cid:73) document 3     label: 1; words: (cid:63) (cid:52)    
(cid:73) document 4     label: 1; words: (cid:5) (cid:52)    
(cid:73) new document     words: (cid:63) (cid:5)    ; label ?
(cid:73) new document     words: (cid:63) (cid:5)    ; label ?

introduction to machine learning

3(107)

introduction

experiment

(cid:73) document 1     label: 0; words: (cid:63) (cid:5)    
(cid:73) document 2     label: 0; words: (cid:63)     (cid:52)
(cid:73) document 3     label: 1; words: (cid:63) (cid:52)    
(cid:73) document 4     label: 1; words: (cid:5) (cid:52)    
(cid:73) new document     words: (cid:63) (cid:5)    ; label ?
(cid:73) new document     words: (cid:63) (cid:5)    ; label ?
(cid:73) new document     words: (cid:63) (cid:52)    ; label ?

introduction to machine learning

3(107)

introduction

experiment

(cid:73) document 1     label: 0; words: (cid:63) (cid:5)    
(cid:73) document 2     label: 0; words: (cid:63)     (cid:52)
(cid:73) document 3     label: 1; words: (cid:63) (cid:52)    
(cid:73) document 4     label: 1; words: (cid:5) (cid:52)    
(cid:73) new document     words: (cid:63) (cid:5)    ; label ?
(cid:73) new document     words: (cid:63) (cid:5)    ; label ?
(cid:73) new document     words: (cid:63) (cid:52)    ; label ?

why can we do this?

introduction to machine learning

3(107)

introduction

experiment

(cid:73) document 1     label: 0; words: (cid:63) (cid:5)    
(cid:73) document 2     label: 0; words: (cid:63)     (cid:52)
(cid:73) document 3     label: 1; words: (cid:63) (cid:52)    
(cid:73) document 4     label: 1; words: (cid:5) (cid:52)    
(cid:73) new document     words: (cid:63) (cid:5)    ; label 0

label 0

label 1

p(0|(cid:63)) = count((cid:63) and 0)
p(0|(cid:5)) = count((cid:5) and 0)
p(0|   ) = count(    and 0)

count((cid:63))
count((cid:5))
count(   )

= 2

3 = 0.67 vs. p(1|(cid:63)) = count((cid:63) and 1)
count((cid:63))
2 = 0.5 vs. p(1|(cid:5)) = count((cid:5) and 1)
= 1
count((cid:5))
1 = 1.0 vs. p(1|   ) = count(    and 1)
count(   )

= 1

= 1

= 1

3 = 0.33
2 = 0.5
1 = 0.0

= 0

introduction to machine learning

4(107)

introduction

experiment

(cid:73) document 1     label: 0; words: (cid:63) (cid:5)    
(cid:73) document 2     label: 0; words: (cid:63)     (cid:52)
(cid:73) document 3     label: 1; words: (cid:63) (cid:52)    
(cid:73) document 4     label: 1; words: (cid:5) (cid:52)    
(cid:73) new document     words: (cid:63) (cid:52)    ; label ?

label 0

label 1

p(0|(cid:63)) = count((cid:63) and 0)
p(0|(cid:52)) = count((cid:52) and 0)
p(0|   ) = count(    and 0)

count((cid:63))
count((cid:52))
count(   )

= 2

3 = 0.67 vs. p(1|(cid:63)) = count((cid:63) and 1)
count((cid:63))
3 = 0.33 vs. p(1|(cid:52)) = count((cid:52) and 1)
count((cid:52))
2 = 0.5 vs. p(1|   ) = count(    and 1)
= 1
count(   )

= 1

= 1

3 = 0.33
= 2

3 = 0.67

= 1

2 = 0.5

introduction to machine learning

4(107)

machine learning

introduction

(cid:73) machine learning is well motivated counting
(cid:73) typically, machine learning models

1. de   ne a model/distribution of interest
2. make some assumptions if needed
3. count!!

(cid:73) model: p(label|doc) = p(label|word1, . . . wordn)

(cid:73) prediction for new doc = arg maxlabel p(label|doc)

(cid:73) assumption: p(label|word1, . . . , wordn) = 1
(cid:73) count (as in example)

n

(cid:80)

i p(label|wordi )

introduction to machine learning

5(107)

introduction

lecture outline

(cid:73) preliminaries

(cid:73) data: input/output, assumptions
(cid:73) feature representations
(cid:73) linear classi   ers and decision boundaries

(cid:73) classi   ers

(cid:73) naive bayes
(cid:73) generative versus discriminative
(cid:73) logistic-regression
(cid:73) id88
(cid:73) large-margin classi   ers (id166s)

(cid:73) id173
(cid:73) online learning
(cid:73) non-linear classi   ers

introduction to machine learning

6(107)

inputs and outputs

preliminaries

(cid:73) input: x     x

(cid:73) e.g., document or sentence with some words x = w1 . . . wn, or

a series of previous actions

(cid:73) output: y     y

(cid:73) e.g., parse tree, document class, part-of-speech tags,

word-sense

(cid:73) input/output pair: (x, y)     x    y
(cid:73) e.g., a document x and its label y
(cid:73) sometimes x is explicit in y, e.g., a parse tree y will contain

the sentence x

introduction to machine learning

7(107)

general goal

preliminaries

when given a new input x predict the correct output y

but we need to formulate this computationally!

introduction to machine learning

8(107)

feature representations

(cid:73) we assume a mapping from input x to a high dimensional

preliminaries

feature vector

(cid:73)   (x) : x     rm

(cid:73) for many cases, more convenient to have mapping from

input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) under certain assumptions, these are equivalent
(cid:73) most papers in nlp use   (x, y)

introduction to machine learning

9(107)

feature representations

(cid:73) we assume a mapping from input x to a high dimensional

preliminaries

feature vector

(cid:73)   (x) : x     rm

(cid:73) for many cases, more convenient to have mapping from

input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) under certain assumptions, these are equivalent
(cid:73) most papers in nlp use   (x, y)
(cid:73) not common in nlp:        rm
(cid:73) more common:   i     {1, . . . , fi}, fi     n+ (categorical)
(cid:73) very common:        {0, 1}m (binary)

introduction to machine learning

9(107)

feature representations

(cid:73) we assume a mapping from input x to a high dimensional

preliminaries

feature vector

(cid:73)   (x) : x     rm

(cid:73) for many cases, more convenient to have mapping from

input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) under certain assumptions, these are equivalent
(cid:73) most papers in nlp use   (x, y)
(cid:73) not common in nlp:        rm
(cid:73) more common:   i     {1, . . . , fi}, fi     n+ (categorical)
(cid:73) very common:        {0, 1}m (binary)
(cid:73) for any vector v     rm, let vj be the j th value

introduction to machine learning

9(107)

examples

preliminaries

(cid:73) x is a document and y is a label

  j (x, y) =

0 otherwise

and y =      nancial   

          1 if x contains the word    interest   
(cid:26) 1 if x =    bank    and y = verb

  j (x, y) = % of words in x with punctuation and y =   scienti   c   

(cid:73) x is a word and y is a part-of-speech tag

  j (x, y) =

0 otherwise

introduction to machine learning

10(107)

example 2

preliminaries

(cid:73) x is a name, y is a label classifying the name

0

          1
          1
          1
          1

0

0

0

  0(x, y) =

  4(x, y) =

if x contains    george   
and y =    person   
otherwise

  1(x, y) =

if x contains    washington   
and y =    person   
otherwise

  5(x, y) =

  2(x, y) =

  3(x, y) =

if x contains    bridge   
and y =    person   
otherwise

if x contains    general   
and y =    person   
otherwise

  6(x, y) =

  7(x, y) =

0

          1
          1
          1
          1

0

0

0

if x contains    george   
and y =    object   
otherwise

if x contains    washington   
and y =    object   
otherwise

if x contains    bridge   
and y =    object   
otherwise

if x contains    general   
and y =    object   
otherwise

(cid:73) x=general george washington, y=person       (x, y) = [1 1 0 1 0 0 0 0]
(cid:73) x=george washington bridge, y=object       (x, y) = [0 0 0 0 1 1 1 0]
(cid:73) x=george washington george, y=object       (x, y) = [0 0 0 0 1 1 0 0]

introduction to machine learning

11(107)

block feature vectors

preliminaries

(cid:73) x=general george washington, y=person       (x, y) = [1 1 0 1 0 0 0 0]
(cid:73) x=general george washington, y=object       (x, y) = [0 0 0 0 1 1 0 1]
(cid:73) x=george washington bridge, y=object       (x, y) = [0 0 0 0 1 1 1 0]
(cid:73) x=george washington george, y=object       (x, y) = [0 0 0 0 1 1 0 0]

(cid:73) each equal size block of the feature vector corresponds to one

label

(cid:73) non-zero values allowed only in one block

introduction to machine learning

12(107)

feature representations -   (x)

preliminaries

(cid:73) instead of   (x, y) : x    y     rm over input/outputs (x, y)
(cid:73) let   (x) : x     rm(cid:48)

(e.g.,m(cid:48) = m/|y|)

(cid:73) i.e., feature representation only over inputs x

(cid:73) equivalent when   (x, y) =   (x)    y

(cid:73) advantages: can make math cleaner, e.g., binary

classi   cation; can use less parameters.

(cid:73) disadvantages: no complex features over properties of labels

introduction to machine learning

13(107)

feature representations -   (x) vs.   (x, y)

preliminaries

(cid:73)   (x, y)

(cid:73) x=general george washington, y=person       (x, y) = [1 1 0 1 0 0 0 0]
(cid:73) x=general george washington, y=object       (x, y) = [0 0 0 0 1 1 0 1]

(cid:73)   (x)

(cid:73) x=general george washington       (x) = [1 1 0 1]

(cid:73) di   erent ways of representing same thing
(cid:73) can deterministically map from   (x) to   (x, y) given y

introduction to machine learning

14(107)

linear classi   ers

(cid:73) linear classi   er: score (or id203) of a particular

linear classi   ers

classi   cation is based on a linear combination of features and
their weights

(cid:73) let        rm be a high dimensional weight vector
(cid:73) assume that    is known

(cid:73) multiclass classi   cation: y = {0, 1, . . . , n}

        (x, y)

y = arg max

y

= arg max

m(cid:88)

y

j=0

  j      j (x, y)

(cid:73) binary classi   cation just a special case of multiclass

introduction to machine learning

15(107)

linear classi   ers

linear classi   ers       (x)

(cid:73) de   ne |y| parameter vectors   y     rm(cid:48)

(cid:73) i.e., one parameter vector per output class y

(cid:73) classi   cation

y = arg max

y

  y      (x)

introduction to machine learning

16(107)

linear classi   ers

linear classi   ers       (x)

(cid:73) de   ne |y| parameter vectors   y     rm(cid:48)

(cid:73) i.e., one parameter vector per output class y

(cid:73) classi   cation

y = arg max

y

  y      (x)

(cid:73)   (x, y)

(cid:73) x=general george washington, y=person       (x, y) = [1 1 0 1 0 0 0 0]
(cid:73) x=general george washington, y=object       (x, y) = [0 0 0 0 1 1 0 1]
(cid:73) single        r8

(cid:73)   (x)

(cid:73) x=general george washington       (x) = [1 1 0 1]
(cid:73) two parameter vectors   0     r4,   1     r4

introduction to machine learning

16(107)

linear classi   ers - bias terms

linear classi   ers

(cid:73) often linear classi   ers presented as

m(cid:88)

  j      j (x, y) + by

y = arg max

y

j=0
(cid:73) where b is a bias or o   set term
(cid:73) sometimes this is folded into   

x=general george washington, y=person       (x, y) = [1 1 0 1 1 0 0 0 0 0]
x=general george washington, y=object       (x, y) = [0 0 0 0 0 1 1 0 1 1]

(cid:26) 1 y =   person   

  4(x, y) =

0

otherwise

  9(x, y) =

(cid:26) 1 y =   object   

0

otherwise

(cid:73)   4 and   9 are now the bias terms for the labels

introduction to machine learning

17(107)

binary linear classi   er
let   s say    = (1,   1) and by = 1,    y
then    is a line (generally a hyperplane) that divides all points:

linear classi   ers

introduction to machine learning

18(107)

12-2-112-2-1points along linehave scores of 0binary linear classi   er - block features

  (x, y) = [v , 0] or [0, v ] in block features

linear classi   ers

introduction to machine learning

19(107)

12-2-112-2-1points along linehave scores of 0multiclass linear classi   er

de   nes regions of space. visualization di   cult.

linear classi   ers

(cid:73) i.e., + are all points (x, y) where + = arg maxy         (x, y)

introduction to machine learning

20(107)

linear classi   ers

separability

(cid:73) a set of points is separable, if there exists a    such that

classi   cation is perfect

separable

not separable

(cid:73) this can also be de   ned mathematically (and we will shortly)

introduction to machine learning

21(107)

machine learning        nding   

linear classi   ers

(cid:73) supervised learning
(cid:73) input: training examples t = {(xt, yt)}|t |
(cid:73) input: feature representation   
(cid:73) output:    that maximizes some important function on the

t=1

training set

(cid:73)    = arg maxl(t ;   )

introduction to machine learning

22(107)

machine learning        nding   

linear classi   ers

(cid:73) supervised learning
(cid:73) input: training examples t = {(xt, yt)}|t |
(cid:73) input: feature representation   
(cid:73) output:    that maximizes some important function on the

t=1

training set

(cid:73)    = arg maxl(t ;   )

(cid:73) equivalently minimize:    = arg min   l(t ;   )

introduction to machine learning

22(107)

objective functions

linear classi   ers

(cid:73) l(  ) is called the objective function
(cid:73) usually we can decompose l by training pairs (x, y)

(cid:73) l(t ;   )    (cid:80)

(x,y)   t loss((x, y);   )

(cid:73) loss is a function that measures some value correlated with

errors of parameters    on instance (x, y)

(cid:73) de   ning l(  ) and loss is core of linear classi   ers in machine

learning

introduction to machine learning

23(107)

supervised learning     assumptions

linear classi   ers

(cid:73) assumption: (xt, yt) are sampled i.i.d.

(cid:73) i.i.d. = independent and identically distributed
(cid:73) independent = each sample independent of the other
(cid:73) identically = each sample from same id203 distribution

(cid:73) sometimes assumption: the training data is separable

(cid:73) needed to prove convergence for id88
(cid:73) not needed in practice

introduction to machine learning

24(107)

naive bayes

naive bayes

introduction to machine learning

25(107)

probabilistic models

naive bayes

(cid:73) for a moment, forget linear classi   ers and parameter vectors   

(cid:73) let   s assume our goal is to model the id155

of output labels y given inputs x (or   (x))

(cid:73) i.e., p(y|x)
(cid:73) if we can de   ne this distribution, then classi   cation becomes

(cid:73) arg maxy p(y|x)

introduction to machine learning

26(107)

bayes rule

(cid:73) one way to model p(y|x) is through bayes rule:

naive bayes

p(y)p(x|y)

p(y|x) =

p(x)
p(y|x)     arg max

p(y)p(x|y)

arg max

y

y
(cid:73) since x is    xed
(cid:73) p(y)p(x|y) = p(x, y): a joint id203
(cid:73) modeling the joint input-output distribution is at the core of

generative models

(cid:73) because we model a distribution that can randomly generate

outputs and inputs, not just outputs

(cid:73) more on this later

introduction to machine learning

27(107)

naive bayes

naive bayes (nb)

(cid:73) use   (x)     rm instead of   (x, y)
(cid:73) p(x|y) = p(  (x)|y) = p(  1(x), . . . ,   m(x)|y)

p(  1(x), . . . ,   m(x)|y) =(cid:81)

naive bayes assumption
(conditional independence)

p(y)p(  1(x), . . . ,   m(x)|y) = p(y)

i p(  i (x)|y)
m(cid:89)

p(  i (x)|y)

i=1

introduction to machine learning

28(107)

naive bayes

naive bayes     learning
(cid:73) input: t = {(xt, yt)}|t |
(cid:73) let   i (x)     {1, . . . , fi}     categorical; common in nlp
(cid:73) parameters p = {p(y), p(  i (x)|y)}

t=1

(cid:73) both p(y) and p(  i (x)|y) are multinomials

(cid:73) objective: id113 (id113)

l(t ) =

p(xt, yt) =

p(yt)

p(  i (xt)|yt)

|t |(cid:89)

|t |(cid:89)

t=1

p = arg max

p

(cid:32)

|t |(cid:89)
(cid:32)

t=1

m(cid:89)

m(cid:89)

i=1

p(yt)

p(  i (xt)|yt)

t=1

i=1

(cid:33)

(cid:33)

introduction to machine learning

29(107)

naive bayes

naive bayes     learning
id113 has closed form solution!! (more later)

p = arg max

p

(cid:33)

p(  i (xt)|yt)

|t |(cid:89)

t=1

m(cid:89)

i=1

p(yt)

(cid:32)
(cid:80)|t |
(cid:80)|t |

p(y) =

(cid:80)|t |

t=1[[yt = y]]

|t |

p(  i (x)|y) =

t=1[[  i (xt) =   i (x) and yt = y]]

t=1[[yt = y]]

[[x ]] is the identity function for property x

thus, these are just normalized counts over events in t

introduction to machine learning

30(107)

naive bayes

naive bayes example

(cid:73)   i (x)     0, 1,    i
(cid:73) doc 1: y1 = 0,   0(x1) = 1,   1(x1) = 1
(cid:73) doc 2: y2 = 0,   0(x2) = 0,   1(x2) = 1
(cid:73) doc 3: y3 = 1,   0(x3) = 1,   1(x3) = 0

(cid:73) two label parameters p(y = 0), p(y = 1)
(cid:73) eight feature parameters

(cid:73) 2 (labels) * 2 (features) * 2 (feature values)
(cid:73) e.g., y = 0 and   0(x) = 1: p(  0(x) = 1|y = 0)

(cid:73) p(y = 0) = 2/3, p(y = 1) = 1/3
(cid:73) p(  0(x) = 1|y = 0) = 1/2, p(  1(x) = 0|y = 1) = 1/1

introduction to machine learning

31(107)

naive bayes document classi   cation

naive bayes

(cid:73) doc 1: y1 = sports,    hockey is fast   
(cid:73) doc 2: y2 = politics,    politicians talk fast   
(cid:73) doc 3: y3 = politics,    washington is sleazy   

(cid:73)   0(x) = 1 i    doc has word    hockey   , 0 o.w.
(cid:73)   1(x) = 1 i    doc has word    is   , 0 o.w.
(cid:73)   2(x) = 1 i    doc has word    fast   , 0 o.w.
(cid:73)   3(x) = 1 i    doc has word    politicians   , 0 o.w.
(cid:73)   4(x) = 1 i    doc has word    talk   , 0 o.w.
(cid:73)   5(x) = 1 i    doc has word    washington   , 0 o.w.
(cid:73)   6(x) = 1 i    doc has word    sleazy   , 0 o.w.

introduction to machine learning

32(107)

naive bayes

deriving id113

p = arg max

p(  i (xt)|yt)

= arg max

log p(yt) +

log p(  i (xt)|yt)

m(cid:88)

i=1

(cid:33)

|t |(cid:88)

m(cid:88)

t=1

i=1

(cid:33)

m(cid:89)

i=1

p

p

t=1

(cid:32)
(cid:32)

p(yt)

|t |(cid:89)
|t |(cid:88)
|t |(cid:88)
y p(y) = 1,(cid:80)fi

t=1

t=1

such that(cid:80)

= arg max

p(y)

log p(yt) + arg max
p(  i (x)|y)

log p(  i (xt)|yt)

j=1 p(  i (x) = j|y) = 1, p(  )     0

introduction to machine learning

33(107)

both optimizations are of the form

deriving id113

p = arg max

p(y)

arg maxp

for example:

arg max

p(y)

naive bayes

m(cid:88)

i=1

log p(  i (xt)|yt)

t=1

t=1

log p(yt) + arg max
p(  i (x)|y)

|t |(cid:88)
|t |(cid:88)
(cid:80)
v count(v ) log p(v ), s.t.,(cid:80)
|t |(cid:88)
(cid:88)
such that(cid:80)

log p(yt) = arg max

p(y)

t=1

y

y p(y) = 1, p(y)     0

v p(v ) = 1, p(v )     0

count(y,t ) log p(y)

introduction to machine learning

34(107)

naive bayes

deriving id113

arg maxp

(cid:80)
v count(v ) log p(v )
v p(v ) = 1, p(v )     0

s.t.,(cid:80)
v count(v ) log p(v )        ((cid:80)
(cid:80)

introduce lagrangian multiplier   , optimization becomes
v p(v )     1)

arg maxp,  

derivative w.r.t p(v ) is

count(v)

p(v )       

setting this to zero p(v ) =

count(v)

  

v p(v ) = 1. p(v )     0, then p(v ) =

combine with(cid:80)

(cid:80)

count(v)
v(cid:48) count(v(cid:48))

introduction to machine learning

35(107)

naive bayes

put it together

(cid:32)

p(yt)

|t |(cid:89)

t=1

(cid:33)

p(  i (xt)|yt)

m(cid:89)
|t |(cid:88)

i=1

m(cid:88)

p = arg max

p

|t |(cid:88)

= arg max

p(y)

t=1

log p(yt) + arg max
p(  i (x)|y)

t=1

i=1

log p(  i (xt)|yt)

p(y) =

(cid:80)|t |

t=1[[yt = y]]

(cid:80)|t |
(cid:80)|t |

|t |

t=1[[yt = y]]

p(  i (x)|y) =

t=1[[  i (xt) =   i (x) and yt = y]]

introduction to machine learning

36(107)

naive bayes

nb is a linear classi   er
(cid:73) let   y = log p(y),    y     y
(cid:73) let     i (x),y = log p(  i (x)|y),    y     y,   i (x)     {1, . . . , fi}
(cid:73) let    be set of all       and      ,   

arg max

y

p(y|  (x))     arg max

y

= arg max

y

= arg max

y

= arg max

y

p(  (x), y) = arg max

p(y)

p(  i (x)|y)

m(cid:89)

m(cid:88)

i=1

y

i=1

log p(  i (x)|y)

log p(y) +

m(cid:88)

  y +

    i (x),y

i=1

  y  y(cid:48) (y) +

(cid:88)

y(cid:48)

m(cid:88)

fi(cid:88)

    i (x),y  i,j (x)

i=1

j=1

where           {0, 1},   i,j (x) = [[  i (x) = j]],   y(cid:48) (y) = [[y = y(cid:48)]]

introduction to machine learning

37(107)

naive bayes

smoothing

(cid:73) doc 1: y1 = sports,    hockey is fast   
(cid:73) doc 2: y2 = politics,    politicians talk fast   
(cid:73) doc 3: y3 = politics,    washington is sleazy   

(cid:73) new doc:    washington hockey is fast   
(cid:73) both    sports    and    politics    have probabilities of 0

(cid:73) smoothing aims to assign a small amount of id203 to

unseen events

(cid:73) e.g., additive/laplacian smoothing
=    p(v ) =

p(v ) =

(cid:80)

count(v )
v(cid:48) count(v(cid:48))

(cid:80)

count(v ) +   
v(cid:48) (count(v(cid:48)) +   )

introduction to machine learning

38(107)

discriminative versus generative

naive bayes

(cid:73) generative models attempt to model inputs and outputs

(cid:73) e.g., nb = id113 of joint distribution p(x, y)
(cid:73) statistical model must explain generation of input

(cid:73) ocam   s razor: why model input?
(cid:73) discriminative models

(cid:73) use l that directly optimizes p(y|x) (or something related)
(cid:73) id28     id113 of p(y|x)
(cid:73) id88 and id166s     minimize classi   cation error
(cid:73) generative and discriminative models use p(y|x) for

prediction

(cid:73) di   er only on what distribution they use to set   

introduction to machine learning

39(107)

id28

id28

introduction to machine learning

40(107)

id28

id28

de   ne a id155:

p(y|x) =

e      (x,y)

zx

,

where zx =

(cid:88)

y(cid:48)   y

e      (x,y(cid:48))

note: still a linear classi   er

arg max

y

p(y|x) = arg max

y

= arg max

y

= arg max

y

e      (x,y)

zx

e      (x,y)
        (x, y)

introduction to machine learning

41(107)

id28

id28

p(y|x) =

e      (x,y)

zx

(cid:73) q: how do we learn weights   
(cid:73) a: set weights to maximize log-likelihood of training data:

l(t ;   )

   = arg max

  

= arg max

|t |(cid:89)

  

t=1

p(yt|xt) = arg max

  

|t |(cid:88)

t=1

log p(yt|xt)

(cid:73) in a nut shell we set the weights    so that we assign as much
id203 to the correct label y for each x in the training set

introduction to machine learning

42(107)

id28

p(y|x) =

e      (x,y)

zx

,

   = arg max

|t |(cid:88)

  

t=1

where zx =

log p(yt|xt) (*)

id28

(cid:88)

y(cid:48)   y

e      (x,y(cid:48))

(cid:73) the objective function (*) is concave (take the 2nd derivative)
(cid:73) therefore there is a global maximum
(cid:73) no closed form solution, but lots of numerical techniques

(cid:73) gradient methods (gradient ascent, conjugate gradient,

iterative scaling)

(cid:73) id77s (limited-memory quasi-newton)

introduction to machine learning

43(107)

gradient ascent

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 log(cid:0)e      (xt ,yt )/zx

(cid:73) want to    nd arg max   l(t ;   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

id28

(cid:1)

  i =   i   1 +   (cid:79)l(t ;   i   1)
(cid:73)    > 0 and set so that l(t ;   i ) > l(t ;   i   1)
(cid:73) (cid:79)l(t ;   ) is gradient of l w.r.t.   
l(t ;   ),    

(cid:73) a gradient is all partial derivatives over variables wi
(cid:73) i.e., (cid:79)l(t ;   ) = (    
l(t ;   ), . . . ,    
     m

     0

     1

l(t ;   ))

(cid:73) gradient ascent will always    nd    to maximize l

introduction to machine learning

44(107)

id119

(cid:73) let l(t ;   ) =    (cid:80)|t |

t=1 log(cid:0)e      (xt ,yt )/zx

(cid:73) want to    nd arg min   l(t ;   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

id28

(cid:1)

  i =   i   1       (cid:79)l(t ;   i   1)
(cid:73)    > 0 and set so that l(t ;   i ) < l(t ;   i   1)
(cid:73) (cid:79)l(t ;   ) is gradient of l w.r.t.   
l(t ;   ),    

(cid:73) a gradient is all partial derivatives over variables wi
(cid:73) i.e., (cid:79)l(t ;   ) = (    
l(t ;   ), . . . ,    
     m

     0

     1

l(t ;   ))

(cid:73) gradient ascent will always    nd    to minimize l

introduction to machine learning

45(107)

the partial derivatives

id28

(cid:73) need to    nd all partial derivatives

l(t ;   )

   
     i

l(t ;   ) =

=

=

t

(cid:88)
(cid:88)
(cid:88)

t

t

log p(yt|xt)

(cid:80)
e      (xt ,yt )
y(cid:48)   y e      (xt ,y(cid:48))
(cid:80)
j   j    j (xt ,yt )

e

zxt

log

log

introduction to machine learning

46(107)

partial derivatives - some reminders

id28

(cid:73) we always assume log is the natural logarithm loge

1.

2.

3.

4.

   

   x log f = 1

f

   
   x f

   

   x ef = ef    

(cid:80)
t ft =(cid:80)

   x f

t

f
g =

   x f   f    
g    

g 2

   
   x

   
   x

   
   x ft
   x g

introduction to machine learning

47(107)

the partial derivatives

id28

l(t ;   ) =

   
     i

=

=

   
     i

(cid:88)
(cid:88)

t

(cid:80)
(cid:80)

e

e

(cid:88)

t

   
     i

log

log

j   j    j (xt ,yt )

zxt

j   j    j (xt ,yt )

zxt

(cid:80)
j   j    j (xt ,yt )

zxt

e

)(

   
     i

(

e

t

(cid:80)
j   j    j (xt ,yt )

zxt

)

introduction to machine learning

48(107)

the partial derivatives
(cid:80)
now,

(cid:80)
j   j    j (xt ,yt )

e

zxt

e

   
     i

zxt

   
     i

(cid:80)

j   j    j (xt ,yt )     e
z 2
xt

(cid:80)
j   j    j (xt ,yt )    
     i

(cid:80)

zxt e

j   j    j (xt ,yt )  i (xt , yt )     e

j   j    j (xt ,yt )    
     i

zxt

id28

zxt

=

=

=

=

(cid:80)
j   j    j (xt ,yt )
(cid:80)
j   j    j (xt ,yt )

z 2
xt

e

e

z 2
xt

z 2
xt

(zxt   i (xt , yt )        
     i

zxt )

(zxt   i (xt , yt )

(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

e

(cid:48)

))

because

   
     i

zxt =

   
     i

(cid:88)

y(cid:48)   y

(cid:80)

e

j   j    j (xt ,y(cid:48)) =

j   j    j (xt ,y(cid:48))  i (xt , y

(cid:48)

)

    (cid:88)
(cid:88)
(cid:80)

y(cid:48)   y

e

y(cid:48)   y

introduction to machine learning

49(107)

id28

the partial derivatives
from before,

(cid:80)
j   j    j (xt ,yt )

(cid:80)
j   j    j (xt ,yt )

e

   
     i

zxt

(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

(cid:48)

))

=

(

e

e

zxt

z 2
xt

(cid:80)

y(cid:48)   y

(zxt   i (xt , yt )

    (cid:88)
(cid:80)
(zxt   i (xt , yt )     (cid:88)
(cid:88)
  i (xt , yt )    (cid:88)
(cid:80)
j   j    j (xt ,y(cid:48))
  i (xt , yt )    (cid:88)
(cid:88)

j   j    j (xt ,yt ) )(

   
     i

1
zxt

y(cid:48)   y

y(cid:48)   y

p(y

zxt

e

e

e

t

t

(cid:88)
(cid:88)
(cid:88)
(cid:88)

t

t

t

t

y(cid:48)   y

(cid:48)|xt )  i (xt , y

(cid:48)

)

j   j    j (xt ,yt )
zxt
(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

e

)

(cid:48)

)))

  i (xt , y

(cid:48)

)

sub this in,

l(t ;   ) =

   
     i

=

=

=

introduction to machine learning

50(107)

finally!!!

id28

(cid:73) after all that,

l(t ;   ) =

   
     i

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

(cid:73) and the gradient is:
(cid:79)l(t ;   ) = (

   

l(t ;   ),

     0

l(t ;   ), . . . ,

   

     1

l(t ;   ))

   

     m

(cid:73) so we can now use gradient assent to    nd   !!

introduction to machine learning

51(107)

id28 summary

(cid:73) de   ne id155

id28

(cid:73) set weights to maximize log-likelihood of training data:

p(y|x) =

e      (x,y)

zx

(cid:88)

   = arg max

  

t

log p(yt|xt)

(cid:73) can    nd the gradient and run gradient ascent (or any

gradient-based optimization algorithm)

l(t ;   ) =

   
     i

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

introduction to machine learning

52(107)

id28

id28 = maximum id178

(cid:73) well known equivalence
(cid:73) max ent: maximize id178 subject to constraints on features

(cid:73) empirical feature counts must equal expected counts

(cid:73) quick intuition

(cid:73) partial derivative in id28

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

l(t ;   ) =

   
     i

(cid:73) first term is empirical feature counts and second term is

expected counts

(cid:73) derivative set to zero maximizes function
(cid:73) therefore when both counts are equivalent, we optimize the

id28 objective!

introduction to machine learning

53(107)

id88

id88

introduction to machine learning

54(107)

id88

id88

|t |(cid:88)
|t |(cid:88)

t=1

(cid:73) choose a    that minimizes error

l(t ;   ) =

1     [[yt = arg max

y

   = arg min

  

t=1

[[p]] =

(cid:73) this is a 0-1 id168

1     [[yt = arg max

(cid:26) 1 p is true

y

0 otherwise

        (xt, y)]]

        (xt, y)]]

(cid:73) when minimizing error people tend to use hinge-loss
(cid:73) we   ll get back to this

introduction to machine learning

55(107)

id88

aside: min error versus max log-likelihood

(cid:73) highly related but not identical
(cid:73) example: consider a training set t with 1001 points

1000    (xi , y = 0) = [   1, 1, 0, 0]
1    (x1001, y = 1) = [0, 0, 3, 1]

for

i = 1 . . . 1000

(cid:73) now consider    = [   1, 0, 1, 0]
(cid:73) error in this case is 0     so    minimizes error

[   1, 0, 1, 0]    [   1, 1, 0, 0] = 1 > [   1, 0, 1, 0]    [0, 0,   1, 1] =    1
[   1, 0, 1, 0]    [0, 0, 3, 1] = 3 > [   1, 0, 1, 0]    [3, 1, 0, 0] =    3

(cid:73) however, log-likelihood = -126.9 (omit calculation)

introduction to machine learning

56(107)

id88

aside: min error versus max log-likelihood

(cid:73) highly related but not identical
(cid:73) example: consider a training set t with 1001 points

1000    (xi , y = 0) = [   1, 1, 0, 0]
1    (x1001, y = 1) = [0, 0, 3, 1]

for

i = 1 . . . 1000

(cid:73) now consider    = [   1, 7, 1, 0]
(cid:73) error in this case is 1     so    does not minimizes error

[   1, 7, 1, 0]    [   1, 1, 0, 0] = 8 > [   1, 7, 1, 0]    [   1, 1, 0, 0] =    1

[   1, 7, 1, 0]    [0, 0, 3, 1] = 3 < [   1, 7, 1, 0]    [3, 1, 0, 0] = 4

(cid:73) however, log-likelihood = -1.4
(cid:73) better log-likelihood and worse error

introduction to machine learning

57(107)

aside: min error versus max log-likelihood

id88

(cid:73) max likelihood (cid:54)= min error
(cid:73) max likelihood pushes as much id203 on correct labeling

of training instance

(cid:73) even at the cost of mislabeling a few examples

(cid:73) min error forces all training instances to be correctly classi   ed

(cid:73) often not possible
(cid:73) ways of regularizing model to allow sacri   cing some errors for

better predictions on more examples

introduction to machine learning

58(107)

id88 learning algorithm

id88

t=1

for n : 1..n

training data: t = {(xt, yt)}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.

return   i

for t : 1..t
let y(cid:48) = arg maxy(cid:48)   (i)      (xt, y(cid:48))
if y(cid:48) (cid:54)= yt

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))
i = i + 1

introduction to machine learning

59(107)

id88: separability and margin

id88

(cid:73) given an training instance (xt, yt), de   ne:
(cid:73)   yt = y     {yt}
(cid:73) i.e.,   yt is the set of incorrect labels for xt

(cid:73) a training set t is separable with margin    > 0 if there exists

a vector u with (cid:107)u(cid:107) = 1 such that:

u      (xt, yt)     u      (xt, y(cid:48))       

(cid:113)(cid:80)

j u2
j

for all y(cid:48)       yt and ||u|| =

(cid:73) assumption: the training set is separable with margin   

introduction to machine learning

60(107)

id88: main theorem

id88

(cid:73) theorem: for any training set separable with a margin of   ,

the following holds for the id88 algorithm:

mistakes made during training     r 2
  2

where r     ||  (xt, yt)       (xt, y(cid:48))|| for all (xt, yt)     t and
y(cid:48)       yt

(cid:73) thus, after a    nite number of training iterations, the error on

the training set will converge to zero

(cid:73) let   s prove it! (proof taken from collins    02)

introduction to machine learning

61(107)

id88

id88 learning algorithm
training data: t = {(xt , yt )}|t |

t=1

mistake

  (0) = 0; i = 0
for n : 1..n

for t : 1..t

let y(cid:48) = arg maxy(cid:48)   (i)      (xt , y(cid:48))
if y(cid:48) (cid:54)= yt
  (i+1) =   (i) +   (xt , yt )       (xt , y(cid:48))

i = i + 1

1.
2.
3.
4.
5.
6.
7.
8.

(cid:73)   (k   1) are the weights before k th

tth example, (xt , yt )

(cid:73) suppose k th mistake made at the
(cid:73) y(cid:48) = arg maxy(cid:48)   (k   1)      (xt , y(cid:48))
(cid:73) y(cid:48) (cid:54)= yt
(cid:73)   (k) =

return   i

  (k   1) +   (xt , yt )       (xt , y(cid:48))
(cid:73) now: u      (k) = u      (k   1) + u    (  (xt , yt )       (xt , y(cid:48)))     u      (k   1) +   
(cid:73) now:   (0) = 0 and u      (0) = 0, by induction on k, u      (k)     k  
(cid:73) now: since u      (k)     ||u||    ||  (k)|| and ||u|| = 1 then ||  (k)||     k  
(cid:73) now:

||  (k)||2 = ||  (k   1)||2 + ||  (xt , yt )       (xt , y(cid:48))||2 + 2  (k   1)    (  (xt , yt )       (xt , y(cid:48)))
||  (k)||2     ||  (k   1)||2 + r 2

(since r     ||  (xt , yt )       (xt , y(cid:48))||
and   (k   1)      (xt , yt )       (k   1)      (xt , y(cid:48))     0)

introduction to machine learning

62(107)

id88 learning algorithm

id88

(cid:73) we have just shown that ||  (k)||     k   and

||  (k)||2     ||  (k   1)||2 + r 2

(cid:73) by induction on k and since   (0) = 0 and ||  (0)||2 = 0

(cid:73) therefore,

(cid:73) and solving for k

||  (k)||2     kr 2

k 2  2     ||  (k)||2     kr 2

k     r 2
  2

(cid:73) therefore the number of errors is bounded!

introduction to machine learning

63(107)

id88 summary

id88

(cid:73) learns a linear classi   er that minimizes error
(cid:73) guaranteed to    nd a    in a    nite amount of time
(cid:73) id88 is an example of an online learning algorithm
(cid:73)    is updated based on a single training instance in isolation

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))

introduction to machine learning

64(107)

averaged id88

id88

t=1

for n : 1..n

training data: t = {(xt, yt)}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
6.
7.
8.

i   (i)(cid:1) / (n    t )

return(cid:0)(cid:80)

  (i+1) =   (i)

i = i + 1

else

for t : 1..t
let y(cid:48) = arg maxy(cid:48)   (i)      (xt, y(cid:48))
if y(cid:48) (cid:54)= yt

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))

introduction to machine learning

65(107)

margin

training

testing

id88

denote the
value of the
margin by   

introduction to machine learning

66(107)

maximizing margin

id88

(cid:73) for a training set t
(cid:73) margin of a weight vector    is smallest    such that

        (xt, yt)             (xt, y(cid:48))       
(cid:73) for every training instance (xt, yt)     t , y(cid:48)       yt

introduction to machine learning

67(107)

maximizing margin

id88

(cid:73) intuitively maximizing margin makes sense
(cid:73) more importantly, generalization error to unseen test data is

proportional to the inverse of the margin

     

r 2

  2    |t |

(cid:73) id88: we have shown that:

(cid:73) if a training set is separable by some margin, the id88

will    nd a    that separates the data

(cid:73) however, the id88 does not pick    to maximize the

margin!

introduction to machine learning

68(107)

support vector machines

support vector machines (id166s)

introduction to machine learning

69(107)

support vector machines

maximizing margin

let    > 0

such that:

max
||  ||   1

  

        (xt, yt)             (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

(cid:73) note: algorithm still minimizes error if data is seperable
(cid:73) ||  || is bound since scaling trivially produces larger margin
  (        (xt, yt)             (xt, y(cid:48)))         , for some        1

introduction to machine learning

70(107)

max margin = min norm

support vector machines

let    > 0

max margin:

max
||  ||   1

  

min norm:

min
  

||  ||2

1
2

such that:
=
      (xt, yt)         (xt, y(cid:48))       

such that:
      (xt, yt)         (xt, y(cid:48))     1

   (xt, yt)     t
and y(cid:48)       yt

   (xt, yt)     t
and y(cid:48)       yt

(cid:73) instead of    xing ||  || we    x the margin    = 1

introduction to machine learning

71(107)

support vector machines

max margin = min norm

max margin:

min norm:

max

||  ||   1

  

min
  

||  ||2

1

2

such that:

=

such that:

        (xt , yt )             (xt , y

(cid:48)

)       

        (xt , yt )             (xt , y

(cid:48)

)     1

   (xt , yt )     t
(cid:48)       yt

and y

   (xt , yt )     t
(cid:48)       yt

and y

(cid:73) let   s say min norm solution ||  || =   
(cid:73) now say original objective is max||  ||        
(cid:73) we know that    must be 1

(cid:73) or we would have found smaller ||  || in min norm solution
(cid:73) |  ||     1 in max margin formulation is an arbitrary scaling choice

introduction to machine learning

72(107)

support vector machines

support vector machines

such that:

   = arg min

  

||  ||2

1
2

        (xt, yt)             (xt, y(cid:48))     1

   (xt, yt)     t and y(cid:48)       yt

(cid:73) quadratic programming problem     a well known convex

optimization problem

(cid:73) can be solved with many techniques [nocedal and wright 1999]

introduction to machine learning

73(107)

support vector machines

support vector machines

what if data is not separable?

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)             (xt, y(cid:48))     1       t and   t     0

   (xt, yt)     t and y(cid:48)       yt

  t: trade-o    between margin per example and (cid:107)  (cid:107)
larger c = more examples correctly classi   ed
if data is separable, optimal solution has   i = 0,    i

introduction to machine learning

74(107)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)             (xt, y(cid:48))     1       t

introduction to machine learning

75(107)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)     max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))     1       t

introduction to machine learning

75(107)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt)

introduction to machine learning

75(107)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 +

  
2

|t |(cid:88)

t=1

  t

   =

1
c

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt)

introduction to machine learning

75(107)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 +

  
2

|t |(cid:88)

t=1

  t

   =

1
c

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt)

if (cid:107)  (cid:107) classi   es (xt, yt) with margin 1, penalty   t = 0

otherwise penalty   t = 1 + maxy(cid:48)(cid:54)=yt         (xt, y(cid:48))             (xt, yt)

introduction to machine learning

75(107)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 +

  
2

|t |(cid:88)

t=1

  t

   =

1
c

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt)

if (cid:107)  (cid:107) classi   es (xt, yt) with margin 1, penalty   t = 0

otherwise penalty   t = 1 + maxy(cid:48)(cid:54)=yt         (xt, y(cid:48))             (xt, yt)
hinge loss:

loss((xt , yt );   ) = max (0, 1 + maxy(cid:48)(cid:54)=yt         (xt , y(cid:48))             (xt , yt ))

introduction to machine learning

75(107)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 +

  
2

|t |(cid:88)

t=1

  t

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt)

hinge loss equivalent

|t |(cid:88)

   = arg min

l(t ;   ) = arg min

loss((xt, yt);   ) +

  
2

  

t=1

= arg min

  

max (0, 1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt))

  

       |t |(cid:88)

t=1

||  ||2

       +

||  ||2

  
2

76(107)

introduction to machine learning

support vector machines

summary

what we have covered

(cid:73) linear classi   ers
(cid:73) naive bayes
(cid:73) id28
(cid:73) id88
(cid:73) support vector machines

what is next

(cid:73) id173
(cid:73) online learning
(cid:73) non-linear classi   ers

introduction to machine learning

77(107)

id173

id173

introduction to machine learning

78(107)

over   tting

id173

(cid:73) early in lecture we made assumption data was i.i.d.
(cid:73) rarely is this true

(cid:73) e.g., syntactic analyzers typically trained on 40,000 sentences

from early 1990s wsj news text

(cid:73) even more common: t is very small
(cid:73) this leads to over   tting

(cid:73) e.g.:    fake    is never a verb in wsj treebank (only adjective)

(cid:73) high weight on      (x, y) = 1 if x=fake and y=adjective   
(cid:73) of course: leads to high log-likelihood / low error

(cid:73) other features might be more indicative
(cid:73) adjacent word identities:    he wants to x his death        x=verb

introduction to machine learning

79(107)

id173

id173

(cid:73) in practice, we regularize models to prevent over   tting

l(t ;   )       r(  )

arg max

  

(cid:73) where r(  ) is the id173 function
(cid:73)    controls how much to regularize
(cid:73) common functions

(cid:73) l2: r(  )     (cid:107)  (cid:107)2 = (cid:107)  (cid:107) =(cid:112)(cid:80)
(cid:73) l0: r(  )     (cid:107)  (cid:107)0 =(cid:80)

i   2

(cid:73) approximate with l1: r(  )     (cid:107)  (cid:107)1 =(cid:80)

(cid:73) non-convex

i |  i|

i [[  i > 0]]     zero weights desired

i     smaller weights desired

introduction to machine learning

80(107)

id28 with l2 id173

id173

|t |(cid:88)

(cid:16)

(cid:73) perhaps most common classi   er in nlp

l(t ;   )       r(  ) =

log

e      (xt ,yt )/zx

t=1

(cid:73) what are the new partial derivatives?

   
   wi

l(t ;   )        
   wi

(cid:73) we know    
   wi
(cid:73) just need    
   wi

l(t ;   )
2(cid:107)  (cid:107)2 =    

  

   wi

  
2

(cid:16)(cid:112)(cid:80)

  r(  )
(cid:17)2

i   2
i

=    
   wi

(cid:17)       

2

(cid:107)  (cid:107)2

(cid:80)

i   2

i =     i

  
2

introduction to machine learning

81(107)

support vector machines

id173

hinge-loss formulation: l2 id173 already happening!

l(t ;   ) +   r(  )

   = arg min

  

= arg min

  

= arg min

  

= arg min

t=1

|t |(cid:88)
|t |(cid:88)
|t |(cid:88)

t=1

  

t=1

loss((xt , yt );   ) +   r(  )

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +   r(  )

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

    id166 optimization    

introduction to machine learning

82(107)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

introduction to machine learning

83(107)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

id166s/hinge-loss: max (0, 1 + maxy(cid:54)=yt (        (xt , y)             (xt , yt )))

|t |(cid:88)

   = arg min

  

t=1

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

introduction to machine learning

83(107)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

id166s/hinge-loss: max (0, 1 + maxy(cid:54)=yt (        (xt , y)             (xt , yt )))

|t |(cid:88)

   = arg min

max (0, 1 + max
y(cid:54)=yt

  

id28/log-loss:     log (cid:0)e      (xt ,yt )/zx

t=1

(cid:1)

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

|t |(cid:88)

(cid:16)

    log

e      (xt ,yt )/zx

(cid:17)

+

(cid:107)  (cid:107)2

  
2

   = arg min

  

t=1

introduction to machine learning

83(107)

generalized linear classi   ers

   = arg min

  

l(t ;   ) +   r(  ) = arg min

  

t=1

id173

loss((xt , yt );   ) +   r(  )

|t |(cid:88)

introduction to machine learning

84(107)

online learning

online learning

introduction to machine learning

85(107)

online vs. batch learning

online learning

batch(t );

(cid:73) for 1 . . . n

(cid:73)        update(t ;   )

(cid:73) return   

online(t );

(cid:73) for 1 . . . n

(cid:73) for (xt, yt)     t

(cid:73)        update((xt , yt );   )

(cid:73) end for

(cid:73) end for
(cid:73) return   

e.g., id166s, logistic regres-
sion, nb

e.g., id88

   =    +   (xt, yt)       (xt, y)

introduction to machine learning

86(107)

online vs. batch learning

online learning

(cid:73) online algorithms

(cid:73) tend to converge more quickly
(cid:73) often easier to implement
(cid:73) require more hyperparameter tuning (exception id88)
(cid:73) more unstable convergence

(cid:73) batch algorithms

(cid:73) tend to converge more slowly
(cid:73) implementation more complex (quad prog, lbfgs)
(cid:73) typically more robust to hyperparameters
(cid:73) more stable convergence

introduction to machine learning

87(107)

id119 reminder

online learning

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

|t |(cid:88)

  i =   i   1     (cid:79)l(t ;   i   1) =   i   1   

  (cid:79)loss((xt, yt);   i   1)

(cid:73)    > 0 and set so that l(t ;   i ) < l(t ;   i   1)

t=1

introduction to machine learning

88(107)

id119 reminder

online learning

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

|t |(cid:88)

t=1

  i =   i   1     (cid:79)l(t ;   i   1) =   i   1   

  (cid:79)loss((xt, yt);   i   1)

(cid:73)    > 0 and set so that l(t ;   i ) < l(t ;   i   1)
(cid:73) stochastic id119 (sgd)

(cid:73) approximate (cid:79)l(t ;   ) with single (cid:79)loss((xt, yt);   )

introduction to machine learning

88(107)

online learning

stochastic id119

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence
(cid:73) sample (xt, yt)     t

(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   )

(cid:73) return   

//    stochastic   

introduction to machine learning

89(107)

online learning

stochastic id119

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence
(cid:73) sample (xt, yt)     t

(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   )

//    stochastic   

(cid:73) return   

in practice

(cid:73) set   0 = o m
(cid:73) for 1 . . . n

(cid:73) for (xt, yt)     t

(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   )

(cid:73) return   

introduction to machine learning

89(107)

online learning

stochastic id119

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence
(cid:73) sample (xt, yt)     t

(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   )

//    stochastic   

(cid:73) return   

in practice

need to solve (cid:79)loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) for 1 . . . n

(cid:73) for (xt, yt)     t

(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   )

(cid:73) return   

introduction to machine learning

89(107)

online id28

online learning

(cid:73) stochastic id119 (sgd)
(cid:73) loss((xt, yt);   ) = log-loss

(cid:73) (cid:79)loss((xt, yt);   ) = (cid:79)(cid:0)    log (cid:0)e      (xt ,yt )/zxt
(cid:1)(cid:1)
(cid:32)
(cid:79)(cid:16)    log
  (xt, yt)    (cid:88)

(cid:73) from id28 section:

e      (xt ,yt )/zxt

(cid:17)(cid:17)

=    

(cid:16)

(cid:33)

p(y|x)  (xt, y)

(cid:73) plus id173 term (if part of model)

y

introduction to machine learning

90(107)

online learning

(cid:19)

        (xt, y)             (xt, yt))

online id166s

(cid:73) stochastic id119 (sgd)
(cid:73) loss((xt, yt);   ) = hinge-loss

(cid:18)

(cid:79)loss((xt, yt);   ) = (cid:79)

max (0, 1 + max
y(cid:54)=yt

(cid:73) subgradient is:

max (0, 1 + max
y(cid:54)=yt

(cid:19)

        (xt, y)             (xt, yt))

(cid:18)
(cid:40)

(cid:79)

=

if         (xt, yt)     maxy         (xt, y)     1
0,
  (xt, y)       (xt, yt), otherwise, where y = maxy         (xt, y)

(cid:73) plus id173 term (required for id166s)

introduction to machine learning

91(107)

id88 and hinge-loss

online learning

id166 subgradient update looks like id88 update

  i =   i   1       

if         (xt , yt )     maxy         (xt , y)     1
0,
  (xt , y)       (xt , yt ), otherwise, where y = maxy         (xt , y)

(cid:40)

id88

  i =   i   1       

(cid:40)

if         (xt , yt )     maxy         (xt , y)     0
0,
  (xt , y)       (xt , yt ), otherwise, where y = maxy         (xt , y)

where    = 1, note   (xt , y)       (xt , yt ) not   (xt , yt )       (xt , y) since           (descent)

id88 = sgd with no-margin hinge-loss

max (0, 1+ max
y(cid:54)=yt

        (xt, y)             (xt, yt))

introduction to machine learning

92(107)

margin infused relaxed algorithm (mira)

online learning

batch (id166s):

min

||  ||2

1
2

such that:
        (xt , yt )             (xt , y(cid:48))     1

online (mira):
training data: t = {(xt , yt )}|t |
1.   (0) = 0; i = 0
2.
3.
4.

for n : 1..n

for t : 1..t

t=1

(cid:13)(cid:13)  *       (i)(cid:13)(cid:13)

  (i+1) = arg min  *
such that:
        (xt , yt )             (xt , y(cid:48))     1
   y(cid:48)       yt

   (xt , yt )     t and y(cid:48)       yt
(cid:73) mira has much smaller optimizations with only |   yt|

return   i

i = i + 1

5.
6.

constraints

introduction to machine learning

93(107)

summary

quick summary

introduction to machine learning

94(107)

linear classi   ers

summary

(cid:73) naive bayes, id88, id28 and id166s
(cid:73) generative vs. discriminative
(cid:73) objective functions and id168s

(cid:73) log-loss, min error and hinge loss
(cid:73) generalized linear classi   ers

(cid:73) id173
(cid:73) online vs. batch learning

introduction to machine learning

95(107)

non-linear classi   ers

non-linear classi   ers

introduction to machine learning

96(107)

non-linear classi   ers

(cid:73) some data sets require more than a linear classi   er to be

non-linear classi   ers

correctly modeled

(cid:73) a lot of models out there
(cid:73) k-nearest neighbours
(cid:73) id90
(cid:73) kernels
(cid:73) neural networks

introduction to machine learning

97(107)

kernels

(cid:73) a kernel is a similarity function between two points that is
symmetric and positive semi-de   nite, which we denote by:

non-linear classi   ers

  (xt, xr )     r
(cid:73) let m be a n    n matrix such that ...

mt,r =   (xt, xr )

(cid:73) ... for any n points. called the gram matrix.
(cid:73) symmetric:

  (xt, xr ) =   (xr , xt)

(cid:73) positive de   nite: for all non-zero v
vmvt     0

introduction to machine learning

98(107)

kernels

non-linear classi   ers

(cid:73) mercer   s theorem: for any kernal   , there exists an   , such

that:

  (xt, xr ) =   (xt)      (xr )

(cid:73) since our features are over pairs (x, y), we will write kernels

over pairs

  ((xt, yt), (xr , yr )) =   (xt, yt)      (xr , yr )

introduction to machine learning

99(107)

non-linear classi   ers

t=1

for n : 1..n

kernel trick     id88 algorithm
training data: t = {(xt , yt )}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.
(cid:73) each feature function   (xt, yt) is added and   (xt, y) is

for t : 1..t
let y = arg maxy   (i)      (xt , y)
if y (cid:54)= yt

  (i+1) =   (i) +   (xt , yt )       (xt , y)
i = i + 1

return   i

subtracted to    say   y,t times

(cid:73)   y,t is the # of times during learning label y is predicted for

example t

(cid:73) thus,

   =

(cid:88)

  y,t[  (xt, yt)       (xt, y)]

t,y

introduction to machine learning

100(107)

kernel trick     id88 algorithm

(cid:73) we can re-write the argmax function as:

non-linear classi   ers

  (i)      (xt, y   )

y    = arg max

y   

= arg max

y   

= arg max

y   

= arg max

y   

(cid:88)
(cid:88)
(cid:88)

t,y

t,y

t,y

  y,t[  (xt, yt)       (xt, y)]      (xt, y   )

  y,t[  (xt, yt)      (xt, y   )       (xt, y)      (xt, y   )]

  y,t[  ((xt, yt), (xt, y   ))       ((xt, y), (xt, y   ))]

(cid:73) we can then re-write the id88 algorithm strictly with

kernels

introduction to machine learning

101(107)

non-linear classi   ers

kernel trick     id88 algorithm
training data: t = {(xt , yt )}|t |
1.
2.
3.
4.
5.
6.

let y    = arg maxy   (cid:80)

   y, t set   y,t = 0
for n : 1..n

for t : 1..t
if y    (cid:54)= yt

  y   ,t =   y   ,t + 1

t=1

t,y   y,t [  ((xt , yt ), (xt , y   ))       ((xt , y), (xt , y   ))]

(cid:73) given a new instance x

(cid:88)

t,y

y    = arg max

y   

  y,t[  ((xt, yt), (x, y   ))     ((xt, y), (x, y   ))]

(cid:73) but it seems like we have just complicated things???

introduction to machine learning

102(107)

kernels = tractable non-linearity

non-linear classi   ers

(cid:73) a linear classi   er in a higher dimensional feature space is a

non-linear classi   er in the original space

(cid:73) computing a non-linear kernel is often better computationally

than calculating the corresponding dot product in the high
dimension feature space

(cid:73) thus, kernels allow us to e   ciently learn non-linear classi   ers

introduction to machine learning

103(107)

linear classi   ers in high dimension

non-linear classi   ers

introduction to machine learning

104(107)

non-linear classi   ers

example: polynomial kernel

(cid:73)   (x)     rm , d     2
(cid:73)   (xt, xs ) = (  (xt)      (xs ) + 1)d
(cid:73) o(m) to calculate for any d!!

(cid:73) but in the original feature space (primal space)
(cid:73) consider d = 2, m = 2, and   (xt) = [xt,1, xt,2]

(  (xt )      (xs ) + 1)2 = ([xt,1, xt,2]    [xs,1, xs,2] + 1)2

= (xt,1xs,1 + xt,2xs,2 + 1)2
= (xt,1xs,1)2 + (xt,2xs,2)2 + 2(xt,1xs,1) + 2(xt,2xs,2)

+2(xt,1xt,2xs,1xs,2) + (1)2

   

2xt,2,

   

2xt,1xt,2, 1]

  

[(xs,1)2, (xs,2)2,

   

2xs,1,

   

2xs,2,

   

2xs,1xs,2, 1]

which equals:
   

[(xt,1)2, (xt,2)2,

2xt,1,

introduction to machine learning

105(107)

non-linear classi   ers

popular kernels

(cid:73) polynomial kernel

  (xt, xs ) = (  (xt)      (xs ) + 1)d
(cid:73) gaussian radial basis kernel (in   nite feature space

representation!)

  (xt, xs ) = exp(

   ||  (xt)       (xs )||2

2  

)

(cid:73) string kernels [lodhi et al. 2002, collins and du   y 2002]
(cid:73) tree kernels [collins and du   y 2002]

introduction to machine learning

106(107)

kernels summary

non-linear classi   ers

(cid:73) can turn a linear classi   er into a non-linear classi   er
(cid:73) kernels project feature space to higher dimensions

(cid:73) sometimes exponentially larger
(cid:73) sometimes an in   nite space!

(cid:73) can    kernalize    algorithms to make them non-linear

introduction to machine learning

107(107)

references and further reading

references and further reading
(cid:73) a. l. berger, s. a. della pietra, and v. j. della pietra. 1996.

a maximum id178 approach to natural language processing. computational
linguistics, 22(1).

(cid:73) c.t. chu, s.k. kim, y.a. lin, y.y. yu, g. bradski, a.y. ng, and k. olukotun.

2007.
map-reduce for machine learning on multicore. in advances in neural information
processing systems.

(cid:73) m. collins and n. du   y. 2002.

new ranking algorithms for parsing and tagging: kernels over discrete structures,
and the voted id88. in proc. acl.

(cid:73) m. collins. 2002.

discriminative training methods for id48: theory and
experiments with id88 algorithms. in proc. emnlp.

(cid:73) k. crammer and y. singer. 2001.

on the algorithmic implementation of multiclass kernel based vector machines.
jmlr.

(cid:73) k. crammer and y. singer. 2003.

ultraconservative online algorithms for multiclass problems. jmlr.

introduction to machine learning

107(107)

references and further reading

(cid:73) k. crammer, o. dekel, s. shalev-shwartz, and y. singer. 2003.

online passive aggressive algorithms. in proc. nips.

(cid:73) k. crammer, o. dekel, j. keshat, s. shalev-shwartz, and y. singer. 2006.

online passive aggressive algorithms. jmlr.

(cid:73) y. freund and r.e. schapire. 1999.

large margin classi   cation using the id88 algorithm. machine learning,
37(3):277   296.

(cid:73) t. joachims. 2002.

learning to classify text using support vector machines. kluwer.

(cid:73) j. la   erty, a. mccallum, and f. pereira. 2001.

conditional random    elds: probabilistic models for segmenting and labeling
sequence data. in proc. icml.

(cid:73) h. lodhi, c. saunders, j. shawe-taylor, and n. cristianini. 2002.

classi   cation with string kernels. journal of machine learning research.
(cid:73) g. mann, r. mcdonald, m. mohri, n. silberman, and d. walker. 2009.

e   cient large-scale distributed training of conditional maximum id178 models. in
advances in neural information processing systems.

(cid:73) a. mccallum, d. freitag, and f. pereira. 2000.

introduction to machine learning

107(107)

references and further reading

maximum id178 markov models for information extraction and segmentation. in
proc. icml.

(cid:73) r. mcdonald, k. crammer, and f. pereira. 2005.

online large-margin training of dependency parsers. in proc. acl.

(cid:73) k.r. m  uller, s. mika, g. r  atsch, k. tsuda, and b. sch  olkopf. 2001.

an introduction to kernel-based learning algorithms. ieee neural networks,
12(2):181   201.

(cid:73) j nocedal and sj wright. 1999.

numerical optimization, volume 2. springer new york.

(cid:73) f. sha and f. pereira. 2003.

id66 with conditional random    elds. in proc. hlt/naacl, pages
213   220.

(cid:73) c. sutton and a. mccallum. 2006.

an introduction to conditional random    elds for relational learning. in l. getoor
and b. taskar, editors, introduction to statistical relational learning. mit press.

(cid:73) b. taskar, c. guestrin, and d. koller. 2003.
max-margin markov networks. in proc. nips.

(cid:73) b. taskar. 2004.

introduction to machine learning

107(107)

references and further reading

learning id170 models: a large margin approach. ph.d. thesis,
stanford.

(cid:73) i. tsochantaridis, t. hofmann, t. joachims, and y. altun. 2004.

support vector learning for interdependent and structured output spaces. in proc.
icml.

(cid:73) t. zhang. 2004.

solving large scale linear prediction problems using stochastic id119
algorithms. in proceedings of the twenty-   rst international conference on machine
learning.

introduction to machine learning

107(107)

