sequence	
   models

noah	
   smith

computer	
   science	
   &	
   engineering

university	
   of	
   washington

nasmith@cs.washington.edu

lecture	
   outline

1. markov	
   models
2. hidden	
   markov	
   models
3. viterbi	
   algorithm
4. other	
   id136	
   algorithms	
   for	
   id48s
5. learning	
   algorithms	
   for	
   id48s

shameless	
   self-     promotion

    linguistic	
   structure	
   

prediction (2011)

    links	
   material	
   in	
   this	
   

lecture	
   to	
   many	
   
related	
   ideas,	
   including	
   
some	
   in	
   other	
   lxmls
lectures.

    available	
   in	
   electronic	
   

and	
   print	
   form.

markov	
   models

one	
   view	
   of	
   text

    sequence	
   of	
   symbols	
   (bytes,	
   letters,	
   

characters,	
   morphemes,	
   words,	
      )
    let	
     	
   denote	
   the	
   set	
   of	
   symbols.

    lots	
   of	
   possible	
   sequences.	
   	
   (  *	
   is	
   infinitely	
   

large.)

    id203	
   distributions	
   over	
     *?

pop	
   quiz

    am	
   i	
   wearing	
   a	
   generative or	
   discriminative

hat	
   right	
   now?

pop	
   quiz

    generative	
   models	
   tell	
   a	
   

mythical	
   story	
   to	
   
explain	
   the	
   data.

    discriminative	
   models	
   

focus	
   on	
   tasks	
   (like	
   
sorting	
   examples).	
   

trivial	
   distributions	
   over	
     *

    give	
   id203	
   0	
   to	
   sequences	
   with	
   length	
   

greater	
   than	
   b;	
   uniform	
   over	
   the	
   rest.

    use	
   data:	
   	
   with	
   n	
   examples,	
   give	
   id203	
   
n-     1 to	
   each	
   observed	
   sequence,	
   0	
   to	
   the	
   rest.

    what	
   if	
   we	
   want	
   every	
   sequence	
   to	
   get	
   some	
   

id203?
    need	
   a	
   probabilistic	
   model	
   family and	
   algorithms	
   

for	
   constructing	
   the	
   model	
   from	
   data.

a	
   history-     based	
   model

p(start, w1, w2, . . . , wn, stop) =

 (wi | w1, w2, . . . , wi 1)

    generate	
   each	
   word	
   from	
   left	
   to	
   right,	
   

conditioned	
   on	
   what	
   came	
   before	
   it.

n+1 i=1

die	
   /	
   dice

one	
   die

two	
   dice

start

one	
   die	
   per	
   history:

   

   

   

start

i

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   start

start

i want

one	
   die	
   per	
   history:

   

history	
   =	
   start i

   

   

start

i want

a

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   start i	
   want

start

i want

a flight

one	
   die	
   per	
   history:

   

   
   
history	
   =	
   start i	
   want	
   a

start

i want

a flight

to

one	
   die	
   per	
   history:

   

history	
   =	
   start i	
   want	
   a	
   flight

   

   

start

i want

a flight

to lisbon

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   start i	
   want	
   a	
   flight	
   to

start

i want

a flight

to lisbon .

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   start i	
   want	
   a	
   flight	
   to	
   lisbon

start

i want

a flight

to lisbon . stop

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   start i	
   want	
   a	
   flight	
   to	
   lisbon	
   .

a	
   history-     based	
   model

n+1 i=1

p(start, w1, w2, . . . , wn, stop) =

 (wi | w1, w2, . . . , wi 1)

    generate	
   each	
   word	
   from	
   left	
   to	
   right,	
   

conditioned	
   on	
   what	
   came	
   before	
   it.

    very	
   rich	
   representational	
   power!
    how	
   many	
   parameters?
    what	
   is	
   the	
   id203	
   of	
   a	
   sentence	
   not	
   seen	
   

in	
   training	
   data?

a	
   bag	
   of	
   words	
   model
n+1 i=1

p(start, w1, w2, . . . , wn, stop) =

 (wi)

    every	
   word	
   is	
   independent	
   of	
   every	
   other	
   

word.

start

one	
   die:

start

i

one	
   die:

start

i want

one	
   die:

start

i want

a

one	
   die:

start

i want

a flight

one	
   die:

start

i want

a flight

to

one	
   die:

start

i want

a flight

to lisbon

one	
   die:

start

i want

a flight

to lisbon .

one	
   die:

start

i want

a flight

to lisbon . stop

one	
   die:

a	
   bag	
   of	
   words	
   model
n+1 i=1

p(start, w1, w2, . . . , wn, stop) =

 (wi)

    every	
   word	
   is	
   independent	
   of	
   every	
   other	
   word.
    strong	
   assumptions	
   mean	
   this	
   model	
   cannot	
   fit	
   

the	
   data	
   very	
   closely.

    how	
   many	
   parameters?
    what	
   is	
   the	
   id203	
   of	
   a	
   sentence	
   not	
   seen	
   in	
   

training	
   data?

first	
   order	
   markov	
   model

    happy	
   medium?

p(start, w1, w2, . . . , wn, stop) =

    condition	
   on	
   the	
   most	
   recent	
   symbol	
   in	
   

history.

n+1 i=1

 (wi | wi 1)

start

one	
   die	
   per	
   history:

   

   

   

start

i

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   start

start

i want

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   i

start

i want

a

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   want

start

i want

a flight

one	
   die	
   per	
   history:

   

history	
   =	
   a

   

   

start

i want

a flight

to

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   flight

start

i want

a flight

to lisbon

one	
   die	
   per	
   history:

   

history	
   =	
   to

   

   

start

i want

a flight

to lisbon .

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   lisbon

start

i want

a flight

to lisbon . stop

one	
   die	
   per	
   history:

   

   

   

history	
   =	
   .

first	
   order	
   markov	
   model

    happy	
   medium?

p(start, w1, w2, . . . , wn, stop) =

    condition	
   on	
   the	
   most	
   recent	
   symbol	
   in	
   

history.

    independence	
   assumptions?
    number	
   of	
   parameters?
    sentences	
   not	
   seen	
   in	
   training?

n+1 i=1

 (wi | wi 1)

mth order	
   markov	
   models

p(start, w1, w2, . . . , wn, stop) =

n+1 i=1

 (wi | wi m, . . . , wi 1)

bag	
   of	
   words
0

mth order	
   markov

entire	
   history

m

   

fewer	
   parameters

stronger	
   independence	
   assumptions

richer	
   expressive	
   power

example

    unigram	
   model	
   estimated	
   on	
   2.8m	
   words	
   of	
   

american	
   political	
   blog	
   text.

this trying our putting and funny
and among it herring it obama
but certainly foreign my
c on byron again but from i
i so and i chuck yeah the as but but republicans if 
this stay oh so or it mccain bush npr this with what 
and they right i while because obama

example

    bigram	
   model	
   estimated	
   on	
   2.8m	
   
words	
   of	
   american	
   political	
   blog	
   text.

the lack of the senator mccain hadn t keep this story 
backwards
while showering praise of the kind of gop weakness
it was mistaken for american economist anywhere in the 
white house press hounded the absence of those he s as 
a wide variety of this election day after the candidate 
b richardson was polled ri in hempstead moderated by 
the convention that he had zero wall street journal 
argues sounds like you may be the primary
but even close the bill told c e to take the obama on 
the public schools and romney
fred flinstone s see how a lick skillet road it s 
little sexist remarks

example

    trigram	
   model	
   estimated	
   on	
   2.8m	
   words	
   of	
   

american	
   political	
   blog	
   text.

as i can pin them all none of them want to bet that 
any of the might be
conservatism unleashed into the privacy rule book and 
when told about what paul
fans organized another massive fundraising initiative 
yesterday and i don t know what the rams supposedly 
want ooh
but she did but still victory dinner
alone among republicans there are probably best not 
all of the fundamentalist community
asked for an independent maverick now for 
crystallizing in one especially embarrassing

example

    5-     gram	
   model	
   estimated	
   on	
   2.8m	
   
words	
   of	
   american	
   political	
   blog	
   text.

he realizes fully how shallow and insincere 
conservative behavior has been he realizes that there 
is little way to change the situation
this recent arianna huffington item about mccain
issuing heartfelt denials of things that were actually 
true or for that matter about the shia sunni split and 
which side iran was on would get confused about this 
any more than someone with any knowledge of us politics 
would get confused about whether neo confederates were 
likely to be supporting the socialist workers party
at the end of the world and i m not especially 
discouraged now that newsweek shows obama leading by 
three now

example

    100-     gram	
   model	
   estimated	
   on	
   2.8m	
   
words	
   of	
   american	
   political	
   blog	
   text.

and it would be the work of many hands to catalogue  all 
the ridiculous  pronouncements  made by this man since his 
long train of predictions  about the middle east has been 
gaudily  disastrously  stupefyingly  misinformed  just the 
buffoon  it seems for the new york times to award with a 
guest column for if you object to the nyt rewarding 
failure  in quite this way then you re intolerant  according 
to the times editorial  page editor andrew  rosenthal 
rosenthal  doesn t seem to recognize  that his choice  of 
adjectives  to describe  kristol  serious  respected  are in 
fact precisely  what is at issue for those whom he 
dismisses  as having  a fear of opposing  views

n-     gram	
   models

pros

    easily	
   understood	
   

linguistic	
   formalism.

    fully	
   generative.
    algorithms:

    calculate	
   id203	
   of	
   a	
   

sequence

    choose	
   a	
   sequence	
   from	
   

a	
   set

    training

cons

    obviously	
   inaccurate	
   
linguistic	
   formalism.

    as	
   n	
   grows,	
   data	
   

sparseness	
   becomes	
   a	
   
problem.
    smoothing	
   is	
   a	
   black	
   art.

    how	
   to	
   deal	
   with	
   
unknown	
   words?

n-     gram	
   models

pros

    easily	
   understood	
   

linguistic	
   formalism.

    fully	
   generative.
    algorithms:

    calculate	
   id203	
   of	
   a	
   

sequence

    choose	
   a	
   sequence	
   from	
   

a	
   set

    training

cons

    obviously	
   inaccurate	
   
linguistic	
   formalism.

    as	
   n	
   grows,	
   data	
   

sparseness	
   becomes	
   a	
   
problem.
    smoothing	
   is	
   a	
   black	
   art.

    how	
   to	
   deal	
   with	
   
unknown	
   words?

calculating	
   the	
   id203	
   

of	
   a	
   sequence

    let	
   n	
   be	
   the	
   length	
   of	
   the	
   sequence	
   and	
   m	
   be	
   

the	
   length	
   of	
   the	
   history.

    for	
   every	
   consecutive	
   (m+1)	
   words	
   wi    	
   wi+m,	
   

look	
   up	
   p(wi+m |	
   wi    	
   wi+m-     1).
    look	
   up	
   p(stop	
   |	
   wn-     m    	
   wn).
    multiply	
   these	
   quantities	
   together.

choosing	
   a	
   sequence	
   from	
   a	
   set

    calculate	
   the	
   id203	
   of	
   each	
   sequence	
   in	
   

the	
   set.

    choose	
   the	
   one	
   that	
   has	
   the	
   highest	
   

id203.

training

    maximum	
   likelihood	
   estimation	
   by	
   relative	
   

frequencies:

unigram

bigram

trigram

general

   (w) =

   (w | w ) =
   (w | w w  ) =
   (w | h) =

freq(w)
#words
freq(w w)
freq(w )
freq(w w  w)
freq(w w  )
freq(hw)
freq(h)

note	
   about	
   current	
   research

    more	
   data	
   always	
   seem	
   to	
   make	
   language	
   models	
   better	
   

       web-     scale   	
   n-     gram	
   models
(brants et	
   al.,	
   2007,	
   inter	
   alia)

    efficient	
   implementations

    runtime:	
   	
   mapreducearchitectures	
   (e.g.,	
   lin	
   &	
   dyer,	
   

2010)

    memory:	
   	
   compression	
   (e.g.,	
   heafield,	
   2011)

    neural	
   language	
   models	
   (e.g.,	
   mikolov et	
   al.,	
   2010)
    use	
   continuous	
   word	
   representations	
   and	
   recurrent	
   
functions	
   (e.g.,	
      lstm   )	
   to	
   define	
   the	
   id203	
   of	
   the	
   
next	
   word.

    n	
   need	
   not	
   be	
   fixed!

sequence	
   models	
   as	
   components

    typically	
   we	
   care	
   about	
   a	
   sequence	
   together	
   

with	
   something	
   else.
    analysis:	
   	
   sequence	
   in,	
   predict	
      something	
   else.   
    generation:	
   	
      something	
   else	
   in,   	
   sequence	
   out.

    sequence	
   models	
   are	
   useful	
   components	
   in	
   

both	
   scenarios.

noisy	
   channel

channel

x

source

true	
   y

decoding	
   rule:

sequence	
   model	
   as	
   source

source

true	
   y

channel

desired	
   output	
   sequence

decoding	
   rule:

x
observed
sequence

    speech	
   recognition	
   

(jelinek,	
   1997)

    machine	
   translation	
   
(brown	
   et	
   al.,	
   1993)

    optical	
   character	
   

recognition	
   (kolak and	
   
resnik,	
   2002)
    spelling	
   and	
   

punctuation	
   correction	
   
(kernighan	
   et	
   al.,	
   
1990)

sequence	
   model	
   as	
   channel

source

true	
   y

desired	
   output

decoding	
   rule:

channel

x
observed
sequence

   
   
   

text	
   categorization
language	
   identification
information	
   retrieval	
   
(ponte	
   and	
   croft,	
   1998;	
   
berger	
   and	
   lafferty,	
   
1999)

    sentence	
   compression	
   

(knight	
   and	
   marcu,	
   
2002)

    question	
   to	
   search	
   query	
   

(radev et	
   al.,	
   2001)

sequence	
   model	
   as	
   a	
   direct	
   model

recent	
   models	
   of	
      
    dialog
    machine	
   translation

p(y | x)

y

desired	
   output

x

observed	
   input
(not	
   modeled!)

decoding	
   rule:

it   s	
   hard	
   to	
   beat	
   n-     grams!

    they	
   are	
   very	
   fast	
   to	
   work	
   with.	
   they	
   fit	
   the	
   

data	
   really,	
   really	
   well.

    improvements	
   for	
   some specific	
   problems

follow	
   from:
    task-     specific	
   knowledge
    domain	
   knowledge	
   (e.g.,	
   linguistics)

class-     based	
   sequence	
   models

    from	
   brown	
   et	
   al.	
   (1990):

n+1yi=1

p(start, w1, w2, . . . , wn, stop) =

   (wi | cl(wi))      (cl(wi) | cl(wi 1))
       cl   	
   is	
   a	
   deterministic	
   function	
   from	
   words	
   to	
   a	
   

smaller	
   set	
   of	
   classes.
    each	
   word	
   only	
   gets	
   one	
   class;	
   known	
   in	
   advance.
    discovered	
   from	
   data	
   using	
   a	
   id91	
   algorithm.

start

start c53

one	
      next	
   class   	
   die	
   per	
   class:

   

   

   

history	
   =	
   start

each	
   word	
   appears	
   on	
   
only	
   one	
   of	
   the	
   word	
   dice.

start c53

i

one	
   word	
   die	
   per	
   class:

   

   

   

class	
   =	
   c53

start c53

c23

i

one	
      next	
   class   	
   die	
   per	
   class:

   

   

   

history	
   =	
   c53

start c53

c23

i

want

one	
   word	
   die	
   per	
   class:

   

   

   

class	
   =	
   c23

start c53

c23

c2

i

want

one	
      next	
   class   	
   die	
   per	
   class:

   

history	
   =	
   c23

   

   

start c53

c23

c2

i

want

a

one	
   word	
   die	
   per	
   class:

   

class	
   =	
   c2

   

   

start c53

c23

c2

c5

i

want

a

one	
      next	
   class   	
   die	
   per	
   class:

   

   

   

history	
   =	
   c2

start c53

c23

c2

c5

i

want

a

flight

one	
   word	
   die	
   per	
   class:

   

   

   

class	
   =	
   c5

class-     based	
   sequence	
   models

    from	
   brown	
   et	
   al.	
   (1990):
n+1 i=1

p(start, w1, w2, . . . , wnstop) =

    independence	
   assumptions?
    number	
   of	
   parameters?
    generalization	
   ability?

   (wi | cl(wi))    (cl(wi) | cl(wi 1))

lecture	
   outline

      markov	
   models
2. hidden	
   markov	
   models
3. viterbi	
   algorithm
4. other	
   id136	
   algorithms	
   for	
   id48s
5. learning	
   algorithms	
   for	
   id48s

hidden	
   markov	
   models

hidden	
   markov	
   model

    a	
   model	
   over	
   sequences	
   of	
   symbols,	
   but	
   there	
   is	
   

missing	
   information	
   associated	
   with	
   each	
   symbol:	
   	
   
its	
      state.   
    assume	
   a	
   finite	
   set	
   of	
   possible	
   states,	
     .

p(start, s1, w1, s2, w2, . . . , sn, wnstop) =

n+1 i=1

   (wi | si)    (si | si 1)

    a	
   joint model	
   over	
   the	
   observable	
   symbols	
   and	
   

their	
   hidden/latent/unknown	
   classes.

start c53

one	
      next	
   class   	
   die	
   per	
   class:

   

   

   

history	
   =	
   start

start c53

i

one	
   word	
   die	
   per	
   class:

   

the	
   only	
   change	
   to	
   the	
   
class-     based	
   model	
   is	
   that	
   
now,	
   the	
   different	
   dice	
   
can	
   share	
   words!

   

   

class	
   =	
   c53

start c53

c23

i

one	
      next	
   class   	
   die	
   per	
   class:

   

   

   

history	
   =	
   c53

start c53

c23

i

want

one	
   word	
   die	
   per	
   class:

   

class	
   =	
   c23

   

   

start c53

c23

c2

i

want

one	
      next	
   class   	
   die	
   per	
   class:

   

   

   

history	
   =	
   c23

start c53

c23

c2

i

want

a

one	
   word	
   die	
   per	
   class:

   

   

   

class	
   =	
   c2

start c53

c23

c2

c5

i

want

a

one	
      next	
   class   	
   die	
   per	
   class:

   

   

   

history	
   =	
   c2

start c53

c23

c2

c5

i

want

a

flight

one	
   word	
   die	
   per	
   class:

   

   

   

class	
   =	
   c5

two	
   equivalent	
   stories

    first,	
   as	
   shown:	
   	
   transition,	
   emit,	
   transition,	
   

emit,	
   transition,	
   emit.

    second:

    generate	
   the	
   sequence	
   of	
   transitions.	
   	
   essentially,	
   

a	
   markov	
   model	
   on	
   classes.

    stochastically	
   replace	
   each	
   class	
   with	
   a	
   word.	
   

mth order	
   hidden	
   markov	
   models

    we	
   can	
   condition	
   on	
   a	
   longer	
   history	
   of	
   past	
   

states:

p(start, s1, w1, s2, w2, . . . , sn, wnstop) =

n+1 i=1

   (wi | si)    (si | si m, . . . , si 1)

    number	
   of	
   parameters?	
   
    benefit:	
   	
   longer	
      memory.   
    today	
   i	
   will	
   stick	
   with	
   first-     order	
   id48s.

uses	
   of	
   id48s	
   in	
   nlp

    part-     of-     speech	
   tagging	
   (church,	
   1988;	
   brants,	
   

2000)

    named	
   entity	
   recognition	
   (bikel et	
   al.,	
   1999)	
   and	
   

other	
   information	
   extraction	
   tasks

    text	
   chunking	
   and	
   shallow	
   parsing	
   (ramshaw and	
   

    word	
   alignment	
   in	
   parallel	
   text	
   (vogel	
   et	
   al.,	
   

marcus,	
   1995)

1996)

    also	
   popular	
   in	
   computational	
   biology	
   and	
   central	
   

to	
   speech	
   recognition.

part	
   of	
   speech	
   tagging

after	
   paying	
   the	
   medical	
   bills	
   ,	
   frances	
   was	
   nearly	
   broke	
   .
.

nns ,	
   	
   	
   	
   	
   nnp

vbg

vbz

dt

rb

rb

jj

jj

    adverb	
   (rb)
    verb	
   (vbg,	
   vbz,	
   and	
   others)
    determiner	
   (dt)
    adjective	
   (jj)
    noun	
   (nn,	
   nns,	
   nnp,	
   and	
   others)
    punctuation	
   (.,	
   ,,	
   and	
   others)

named	
   entity	
   recognition

with	
   commander	
   chris	
   ferguson	
   at	
   the	
   helm	
   ,	
   

atlantis	
   touched	
   down	
   at	
   kennedy	
   space	
   center	
   .

named	
   entity	
   recognition

o

i-     person

i-     person

b-     person

o
with	
   commander	
   chris	
   ferguson	
   at	
   the	
   helm	
   ,	
   
i-     place
b-     space-     shuttle
o
atlantis	
   touched	
   down	
   at	
   kennedy	
   space	
   center	
   .

b-     place

i-     place

o

o

o

o

o

o

    what	
   makes	
   this	
   hard?

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

word	
   alignment

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
   filled	
   not	
   with	
   production	
   factors	
   ,	
   but	
   with	
   living	
   creatures.

null

noahs	
   arche	
   war	
   nicht	
   voller	
   productionsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .

hidden	
   markov	
   model

    a	
   model	
   over	
   sequences	
   of	
   symbols,	
   but	
   there	
   is	
   

missing	
   information	
   associated	
   with	
   each	
   symbol:	
   	
   
its	
      state.   
    assume	
   a	
   finite	
   set	
   of	
   possible	
   states,	
     .

p(start, s1, w1, s2, w2, . . . , sn, wnstop) =

n+1 i=1

   (wi | si)    (si | si 1)

    a	
   joint model	
   over	
   the	
   observable	
   symbols	
   and	
   

their	
   hidden/latent/unknown	
   classes.

lecture	
   outline

      markov	
   models
      hidden	
   markov	
   models
3. viterbi	
   algorithm
4. other	
   id136	
   algorithms	
   for	
   id48s
5. learning	
   algorithms	
   for	
   id48s

algorithms	
   for	
   
hidden	
   markov	
   models

how	
   to	
   calculate	
      

given	
   the	
   id48	
   and	
   a	
   sequence:
1. the	
   most	
   probable	
   state	
   sequence?
2. the	
   id203	
   of	
   the	
   word	
   sequence?
3. the	
   id203	
   distribution	
   over	
   states,	
   for	
   

each	
   word?

4. minimum	
   risk	
   sequence
given	
   states	
   and	
   sequences,	
   or	
   just	
   states:
5. the	
   parameters	
   of	
   the	
   id48	
   (   and	
     )?

problem	
   1:	
   	
   

most	
   likely	
   state	
   sequence

    input:	
   	
   id48	
   (   and	
     )	
   and	
   symbol	
   sequence	
   

w.

    output:

arg max

s

p(s | w,  ,    )

    statistics	
   view:	
   	
   maximum	
   a	
   posteriori

id136

    computational	
   view:	
   	
   discrete,	
   combinatorial	
   

optimization	
   	
   	
   	
   

example

i

suspect

the	
   

present

forecast

is	
   

pessimistic

cd

nn

nnp

prp

jj

nn

vb

vbp

dt

jj

nn

nnp

vbp

4

4

5

jj

nn

rb

vb

vbp

5

nns

vbz

nn

vb

vbd

vbn

vbp

5

2

jj

1

.

.

1

4,000	
   possible	
    state	
   sequences!

na  ve	
   solutions

    list	
   all	
   the	
   possibilities	
   in	
     n.

    correct.
    inefficient.

    work	
   left	
   to	
   right	
   and	
   greedily	
   pick	
   the	
   best	
   si

at	
   each	
   point,	
   based	
   on	
   si-     1 and	
   wi.
    not	
   correct;	
   solution	
   may	
   not	
   be	
   equal	
   to:

arg max
    but	
   fast!

s

p(s | w,  ,    )

interactions

    each	
   word   s	
   label	
   depends	
   on	
   the	
   word,	
   and	
   

nearby	
   labels.

    but	
   given	
   adjacent	
   labels,	
   others	
   do	
   not	
   matter.

i

suspect

the	
   

present

forecast

is	
   

pessimistic

cd

nn

nnp

prp

jj

nn

vb

vbp

dt

jj

nn

nnp

vbp

jj

nn

rb

vb

vbp

nn

vb

vbd

vbn

vbp

nns

vbz

jj

(arrows	
   show	
   most	
   preferred label	
   by	
   each	
   neighbor)

.

.

base	
   case:	
   	
   last	
   label

start

w1

w2

w3

   

wn-     1

wn

stop

   

  1
  2
  3
  4
   
  |  |

scoren(   ) =  (stop |    )      (wn |    )    (    | sn 1)

of	
   course,	
   we	
   do	
   not	
   actually	
   know	
   sn-     1!

recurrence

    if	
   i	
   knew	
   the	
   score	
   of	
   every	
   sequence	
   s1    	
   sn-     1,	
   

i	
   could	
   reason	
   easily	
   about	
   sn.
    but	
   my	
   decision	
   about	
   sn would	
   only	
   depend	
   on	
   

sn-     1!

    so	
   i	
   really	
   only	
   need	
   to	
   know	
   the	
   score	
   of	
   the	
   

one	
   best sequence	
   ending	
   in	
   each sn-     1.

    think	
   of	
   that	
   as	
   some	
      precalculation   	
   that	
   

happens	
   before	
   i	
   think	
   about	
   sn.

recurrence

    assume	
   we	
   have	
   the	
   scores	
   for	
   all	
   prefixes	
   of	
   

the	
   current	
   state.
    one	
   score	
   for	
   each	
   possible	
   last-     state	
   of	
   the	
   prefix.

scoren(   ) =  (stop |    )      (wn |    )   max

  

 (    |       )   scoren 1(      )

scoren 1(   ) =    (wn 1 |    )   max
scoren 2(   ) =    (wn 2 |    )   max

  

  

 (    |       )   scoren 2(      )
 (    |       )   scoren 3(      )

...

...

score1(   ) =    (w1 |    )    (    | start)

recurrence

    the	
   recurrence	
      bottoms	
   out   	
   at	
   start.
    this	
   leads	
   to	
   a	
   simple	
   algorithm	
   for	
   calculating	
   

all	
   the	
   scores.

scoren(   ) =  (stop |    )      (wn |    )   max

  

 (    |       )   scoren 1(      )

scoren 1(   ) =    (wn 1 |    )   max
scoren 2(   ) =    (wn 2 |    )   max

  

  

 (    |       )   scoren 2(      )
 (    |       )   scoren 3(      )

...

...

score1(   ) =    (w1 |    )    (    | start)

viterbi	
   algorithm	
   (scores	
   only)

    for	
   every	
     	
   in	
     ,	
   let:

score1(   ) =    (w1 |    )    (    | start)

    for	
   i	
   =	
   2	
   to	
   n	
       1,	
   for	
   every	
     	
   in	
     :

scorei(   ) =    (wi |    )   max
      

    for	
   every	
     	
   in	
     :
scoren(   ) =  (stop |    )      (wn |    )   max
      

 (    |       )   scorei 1(      )

 (    |       )   scoren 1(      )

    claim:

max

s

p(s, w |  ,    ) = max
   

scoren( )

exploiting	
   distributivity

max
     

scoren(   ) = max
     
= max
     
    (wn 1 |       )   max
       

 (stop |    )      (wn |    )   max
      
 (stop |    )      (wn |    )   max
      

 (       |          )   scoren 2(         )

 (    |       )   scoren 1(      )
 (    |       )

 (stop |    )      (wn |    )   max
      

 (    |       )

= max
     
    (wn 1 |       )   max
       
    (wn 2 |          )   max
        

 (       |          )
 (          |             )   scoren 3(            )

=

max

 ,  ,   ,    

 (stop |    )      (wn |    )    (    |       )

    (wn 1 |       )    (       |          )
    (wn 2 |          )    (          |             )   scoren 3(            )

= max
s    n

n+1 i=1
p(s, w |  ,    ) = max
   

max

s

 (si | si 1)      (wi | si)
scoren( )

i

3e-     7

4e-     6
1e-     5

4e-     3

cd

dt

jj

nn

nnp

nns

prp

rb

vb

vbd

vbn

vbp

vbz
.

suspect

the	
   

present

forecast

is	
   

pessimistic

.

3e-     12
6e-     13

4e-     16

7e-     23

3e-     8
1e-     12
1e-     13
4e-     13

1e-     9
2e-     10

6e-     9

2e-     14
3e-     15

5e-     7

4e-     14

4e-     15

1e-     21

6e-     18

2e-     19
6e-     18
4e-     18
9e-     19

1

2

3

4

5

6

7

2e-     24

8

not	
   quite	
   there

    as	
   described,	
   this	
   algorithm	
   only	
   lets	
   us	
   
calculate	
   the	
   id203 of	
   the	
   best	
   label	
   
sequence.

    it	
   does	
   not	
   recover	
   the	
   best	
   sequence!

understanding	
   the	
   scores

    scorei(  )	
   is	
   the	
   score	
   of	
   the	
   best	
   sequence	
   

labeling	
   up	
   through	
   wi,	
   ignoring	
   what	
   comes	
   
later.
scorei( ) = max

p(s1, w1, s2, w2, . . . , si =  , wi)

s1,...,si 1

    similar	
   trick	
   as	
   before:	
   	
   if	
   i	
   know	
   what	
   si+1 is,	
   

then	
   i	
   can	
   use	
   the	
   scores	
   to	
   choose	
   si.

    solution:	
   	
   keep	
   backpointers.

suspect

the	
   

present

forecast

is	
   

pessimistic

.

i

3e-     7

4e-     6
1e-     5

4e-     3

cd
dt
jj
nn
nnp
nns
prp
rb
vb
vbd
vbn
vbp
vbz
.

3e-     8
1e-     12
1e-     13
4e-     13

1e-     9
2e-     10

6e-     9

2e-     14
3e-     15

5e-     7

4e-     14

4e-     15

3e-     12
6e-     13

4e-     16

7e-     23

1e-     21

6e-     18

2e-     19
6e-     18
4e-     18
9e-     19

2e-     24

suspect

the	
   

present

forecast

is	
   

pessimistic

.

i

3e-     7

4e-     6
1e-     5

4e-     3

cd
dt
jj
nn
nnp
nns
prp
rb
vb
vbd
vbn
vbp
vbz
.

3e-     8
1e-     12
1e-     13
4e-     13

1e-     9
2e-     10

6e-     9

2e-     14
3e-     15

5e-     7

4e-     14

4e-     15

3e-     12
6e-     13

4e-     16

7e-     23

1e-     21

6e-     18

2e-     19
6e-     18
4e-     18
9e-     19

2e-     24

viterbi	
   algorithm

    for	
   every	
     	
   in	
     ,	
   let:

score1(   ) =    (w1 |    )    (    | start)

    for	
   i	
   =	
   2	
   to	
   n	
       1,	
   for	
   every	
     	
   in	
     :

scorei(   ) =    (wi |    )   max
      
 (    |       )   scorei 1(      )

 (    |       )   scorei 1(      )

bpi(   ) = arg max
      

    for	
   every	
     	
   in	
     :
scoren(   ) =  (stop |    )      (wn |    )   max
      

bpn(   ) = arg max
      

 (    |       )   scoren 1(      )

 (    |       )   scoren 1(      )

viterbi	
   algorithm:	
   	
   backtrace

    after	
   calculating	
   all	
   score	
   and	
   bp	
   values,	
   start	
   

by	
   choosing	
   sn to	
   maximize	
   scoren.

    then	
   let	
   sn-     1 =	
   bpn(sn).

    in	
   general,	
   si-     1 =	
   bpi(si).

another	
   example

time

flies

like

an

10e-     15

8e-     13
6e-     14

1e-     14
8e-     16

2e-     4

2e-     7

2e-     9

4e-     20

arrow
6e-     21
1e-     19
2e-     16
3e-     16
1e-     16
1e-     19
4e-     19
3e-     18
1e-     21
5e-     22

.

3e-     17

dt

in

jj

nn

nnp

vb

vbp

vbz
.
,

another	
   example

time

flies

like

an

10e-     15

8e-     13
6e-     14

1e-     14
8e-     16

2e-     4

2e-     7

2e-     9

4e-     20

arrow
6e-     21
1e-     19
2e-     16
3e-     16
1e-     16
1e-     19
4e-     19
3e-     18
1e-     21
5e-     22

.

3e-     17

dt

in

jj

nn

nnp

vb

vbp

vbz
.
,

general	
   idea:	
   	
   dynamic	
   programming

    use	
   a	
   table	
   data	
   structure	
   to	
   store	
   partial	
   
quantities	
   that	
   will	
   be	
   reused	
   many	
   times.
    optimal	
   substructure:	
   	
   best	
   solution	
   to	
   a	
   problem	
   

relies	
   on	
   best	
   solutions	
   to	
   its	
   (similar-     looking)	
   
subproblems.

    overlapping	
   subproblems:	
   	
   reuse	
   a	
   small	
   number	
   

of	
   quantities	
   many	
   times

    examples:	
   	
   viterbi,	
   minimum	
   levenshtein	
   

distance,	
   dijkstra   s	
   shortest	
   path	
   algorithm,	
      

a	
   different	
   view:	
   	
   best	
   path

asymptotic	
   analysis

    the	
   table	
   is	
   n	
       |  |.

memory:

runtime:
    each	
   cell	
   in	
   the	
   table	
   requires	
   o(|  |)	
   

operations.

    total	
   runtime	
   is	
   o(n|  |2).

lecture	
   outline

      markov	
   models
      hidden	
   markov	
   models
      viterbi	
   algorithm
4. other	
   id136	
   algorithms	
   for	
   id48s
5. learning	
   algorithms	
   for	
   id48s

coffee	
   break

viterbi	
   algorithm	
   (recap)

    for	
   every	
     	
   in	
     ,	
   let:

score1(   ) =    (w1 |    )    (    | start)

    for	
   i	
   =	
   2	
   to	
   n	
       1,	
   for	
   every	
     	
   in	
     :

scorei(   ) =    (wi |    )   max
      
 (    |       )   scorei 1(      )

 (    |       )   scorei 1(      )

bpi(   ) = arg max
      

    for	
   every	
     	
   in	
     :
scoren(   ) =  (stop |    )      (wn |    )   max
      

bpn(   ) = arg max
      

 (    |       )   scoren 1(      )

 (    |       )   scoren 1(      )

example

time

flies

like

an

10e-     15

8e-     13
6e-     14

1e-     14
8e-     16

2e-     4

2e-     7

2e-     9

4e-     20

dt

in

jj

nn

nnp

vb

vbp

vbz
.
,

arrow
6e-     21
1e-     19
2e-     16
3e-     16
1e-     16
1e-     19
4e-     19
3e-     18
1e-     21
5e-     22

.

3e-     17

example

time

flies

like

an

10e-     15

8e-     13
6e-     14

1e-     14
8e-     16

2e-     4

2e-     7

2e-     9

4e-     20

dt

in

jj

nn

nnp

vb

vbp

vbz
.
,

arrow
6e-     21
1e-     19
2e-     16
3e-     16
1e-     16
1e-     19
4e-     19
3e-     18
1e-     21
5e-     22

.

3e-     17

lecture	
   outline

      markov	
   models
      hidden	
   markov	
   models
      viterbi	
   algorithm
4. other	
   id136	
   algorithms	
   for	
   id48s
5. learning	
   algorithms	
   for	
   id48s

how	
   to	
   calculate	
      

given	
   the	
   id48	
   and	
   a	
   sequence:
      the	
   most	
   probable	
   state	
   sequence?
2. the	
   id203	
   of	
   the	
   word	
   sequence?
3. the	
   id203	
   distribution	
   over	
   states,	
   for	
   

each	
   word?

4. minimum	
   risk	
   sequence
given	
   states	
   and	
   sequences,	
   or	
   just	
   states:
5. the	
   parameters	
   of	
   the	
   id48	
   (   and	
     )?

problem	
   2:	
   	
   p(w |	
     ,	
     )

    why	
   might	
   we	
   be	
   interested	
   in	
   this	
   quantity?
    using	
   an	
   id48	
   as	
   a	
   language	
   model,	
   we	
   might	
   

want	
   to	
   compare	
   two	
   or	
   more	
   sequences.

    later,	
   we	
   will	
   want	
   to	
   maximize this	
   quantity	
   with	
   

respect	
   to	
   the	
   parameters	
      and	
      (learning).

maximizing	
   and	
   summing

most	
   probable	
   state	
   

sequence	
   given	
   words:

max

s

p(s, w |  ,    )

combinatorial	
   

optimization	
   problem,	
   
solvable	
   in	
   polynomial	
   
time.

total	
   id203	
   of	
   all	
   

state	
   sequences,	
   
together	
   with	
   words:
p(w |  ,    )
=  s

p(s, w |  ,    )

a	
   very	
   similar	
   trick

    the	
   sum	
   of	
   all	
   label-     sequence	
   probabilities	
   
breaks	
   down	
   into	
   the	
   sum	
   over	
   scores	
   for	
   
different	
   final	
   symbols.

        s1...sn 1
   
=       

p(s1 . . . sn 1   , w |  ,    )
   
 (stop |    )      (wn |    )           s1...sn 2

      

fn( )

   

p(s1 . . . sn 2      , w1 . . . wn 1 |  ,    )
   

fn 1(    )

      

a	
   very	
   similar	
   trick

    as	
   before,	
   there	
   is	
   a	
   recurrence.
    here,	
   we	
   exploit	
   the	
   fact	
   that	
   multiplication	
   

distributes	
   over	
   addition.
        s1...sn 1
   
=       

p(s1 . . . sn 1   , w |  ,    )
   
 (stop |    )      (wn |    )           s1...sn 2

      

fn( )

p(s1 . . . sn 2      , w1 . . . wn 1 |  ,    )
   

fn 1(    )

      

   

forward	
   algorithm

    for	
   every	
     	
   in	
     ,	
   let:
f1(   ) =    (w1 |    )    (    | start)
    for	
   i	
   =	
   2	
   to	
   n	
       1,	
   for	
   every	
     	
   in	
     :
fi(   ) =    (wi |    )          
    for	
   every	
     	
   in	
     :
fn(   ) =  (stop |    )      (wn |    )          
p(w |  ,    ) =     

fn( )

 (    |       )   fi 1(      )

 (    |       )   fn 1(      )

a	
   different	
   view:	
   	
   path	
   sum

a	
   different	
   view:	
   	
   linear	
   system

    |  |	
   times	
   n	
   free	
   variables,	
   same	
   number	
   of	
   

equations.

    can	
   rewrite	
   as	
   a	
   matrix	
   inversion	
   problem!

f1(   ) =    (w1 |    )    (    | start)
fi(   ) =    (wi |    )          
fn(   ) =  (stop |    )      (wn |    )          

 (    |       )   fi 1(      )

 (    |       )   fn 1(      )

from	
   forward	
   to	
   backward

    forward	
   algorithm:	
   	
   precomputation	
   of	
   

partial	
   sums,	
   from	
   i	
   =	
   1	
   to	
   n,	
   each	
   involving	
   
|  |	
   quantities,	
   each	
   a	
   sum	
   over	
   |  |	
   
combinations.
    asymptotic	
   analysis	
   is	
   the	
   same	
   as	
   viterbi.

    no	
   need	
   to	
   start	
   at	
   the	
   left	
   and	
   move	
   right!
    backward	
   algorithm calculates	
   partial	
   sums	
   

from	
   the	
   right	
   to	
   the	
   left.

backward	
   algorithm

    for	
   every	
     	
   in	
     ,	
   let:

    for	
   i	
   =	
   n	
       1	
   to	
   2,	
   for	
   every	
     	
   in	
     :

bn(   ) =  (stop |    )      (wn |    )
bi(   ) =    (wi |    )          
b1( ) =  (  | start)        (w1 |  )x 02   

    for	
   every	
     	
   in	
     :

 (     |    )   bi+1(    )

 ( 0 |  )     b2( 0)

p(w |  ,    ) =     

b1( )

forward	
   and	
   backward

    two	
   different	
   ways	
   to	
   rearrange	
   the	
   sums	
   of	
   

products	
   of	
   sums	
   of	
   products	
   of	
   sums	
   of	
   
products.

    different	
   intermediate	
   quantities.

pop	
   quiz

    might	
   we	
   have	
   done	
   the	
   same	
   thing	
   with	
   the	
   

viterbi	
   algorithm?

maximize and	
   

multiply	
   
operations

works	
   left	
   to	
   right
works	
   right	
   to	
   left

viterbi

?

add and	
   
multiply	
   
operations
forward
backward

generalization:	
   	
   semirings

    viterbi	
   and	
   forward	
   algorithms	
   correspond	
   to	
   

exactly	
   the	
   same	
   calculations,	
   except	
   one	
   
maximizes	
   and	
   the	
   other	
   sums.

    one	
   view:	
   	
   they	
   are	
   the	
   same	
   abstract	
   
algorithm,	
   instantiated	
   in	
   two	
   different	
   
semirings.

    informally,	
   a	
   semiring	
   is	
   a	
   set	
   of	
   values	
   and	
   

some	
   operations	
   that	
   obey	
   certain	
   properties.

semirings,	
   more	
   formally

    a	
   set	
   of	
   values,	
   including	
   a	
      zero   	
   (additive	
   

identity	
   and	
   multiplicative	
   annihilator)	
   and	
   a	
   
   one   	
   (multiplicative	
   identity).

    two	
   operations:	
   	
      plus   	
   and	
      times.   
       plus   	
   is	
   associative	
   and	
   commutative.
       times   	
   is	
   associative.

       times   	
   distributes	
   over	
      plus.   

    this	
   is	
   what	
   we	
   have	
   exploited	
   to	
   get	
   efficient	
   

algorithms	
   for	
   maximizing	
   and	
   summing!

semirings

real

viterbi

nonnegative reals

nonnegative	
   reals

0
1
+

   

0
1
max

   

set	
   of	
   values
   zero   
   one   
   plus   
   times   

some	
   other	
   semirings

    boolean:	
   	
   use	
   to	
   determine	
   whether	
   the	
   id48	
   

can	
   produce	
   the	
   string	
   at	
   all.

    counting:	
   	
   use	
   to	
   determine	
   how	
   many	
   valid	
   

labelings	
   there	
   are.	
   
    could	
   be	
   less	
   than	
   |  |n,	
   if	
   some	
   transition	
   and/or	
   

emission	
   probabilities	
   are	
   zero.

    log-     real:	
   	
   use	
   with	
   log-     probabilities	
   to	
   avoid	
   

underflow.

    k-     best:	
   	
   use	
   to	
   find	
   the	
   k	
   best	
   label	
   sequences.
    min-     cost:	
   	
   used	
   with	
   levenshtein	
   edit	
   distance	
   

and	
   dijkstra   s	
   algorithms.

how	
   to	
   calculate	
      

given	
   the	
   id48	
   and	
   a	
   sequence:
      the	
   most	
   probable	
   state	
   sequence?
      the	
   id203	
   of	
   the	
   word	
   sequence?
3. the	
   id203	
   distribution	
   over	
   states,	
   for	
   

each	
   word?

4. minimum	
   risk	
   sequence
given	
   states	
   and	
   sequences,	
   or	
   just	
   states:
5. the	
   parameters	
   of	
   the	
   id48	
   (   and	
     )?

random	
   variables

    so	
   far	
   we   ve	
   focused	
   on	
   reasoning	
   about	
   the	
   

whole	
   sequence	
   of	
   states.

    local	
   reasoning	
   was	
   in	
   service	
   of:

    finding	
   the	
   best	
   s =	
   s1 s2    	
   sn
    finding	
   the	
   total	
   id203	
   of	
   w,	
   averaging	
   over	
   

all	
   possible	
   values	
   of	
   s.

    it	
   is	
   helpful	
   to	
   use	
   a	
   graphical	
   representation	
   

of	
   all	
   our	
   random	
   variables.

graphical	
   model	
   representation

s1

w1

s2

w2

s3

   

sn-     1

w3

wn-     1

sn

wn

    each	
   node	
   is	
   a	
   random	
   variable	
   taking	
   some	
   value.
    incoming	
   edges	
   to	
   a	
   r.v.	
   tell	
   what	
   other	
   r.v.s	
   it	
   

conditions	
   on	
   directly.

p(x) =  i

p(xi | parents(xi))

graphical	
   model	
   representation

s1

w1

s2

w2

s3

   

sn-     1

w3

wn-     1

sn

wn

  

  

    each	
   node	
   is	
   a	
   random	
   variable	
   taking	
   some	
   value.
    incoming	
   edges	
   to	
   a	
   r.v.	
   tell	
   what	
   other	
   r.v.s	
   it	
   

conditions	
   on	
   directly.

p(x) =  i

p(xi | parents(xi))

problem	
   1:	
   	
   most	
   likely	
   s

s1

w1

s2

w2

s3

   

sn-     1

w3

wn-     1

sn

wn

gray.

    sequence	
   of	
   words	
   is	
   observed,	
   so	
   we	
   color	
   those	
   r.v.s	
   

    we	
   want	
   to	
   assign	
   values,	
   collectively,	
   to	
   the	
   states,	
   so	
   

we	
   color	
   those	
   red.

    goal:	
   	
   calculate	
   the	
   best	
   value	
   of	
   p	
   for	
   any	
   assignment	
   

to	
      red   	
   r.v.s.,	
   respecting	
      gray   	
   evidence	
   r.v.s.

problem	
   2:	
   	
   id203	
   of	
   w

s1

w1

s2

w2

s3

   

sn-     1

w3

wn-     1

sn

wn

    sequence	
   of	
   words	
   is	
   observed,	
   so	
   we	
   color	
   those	
   r.v.s gray.
    we	
   want	
   to	
   sum	
   over	
   all	
   settings	
   of	
   the	
   state	
   r.v.s,	
   so	
   we	
   

color	
   them	
   blue.

    goal:	
   	
   calculate	
   the	
   best	
   value	
   of	
   p	
   for	
   any	
   assignment	
   to	
   
   red   	
   r.v.s,	
   respecting	
      gray   	
   evidence	
   r.v.s and	
   summing	
   
over	
   all	
   possible	
   assignments	
   to	
      blue   	
   r.v.s.
    there	
   are	
   no	
   red	
   r.v.s in	
   this	
   problem!

problem	
   3:	
   	
   a	
   single	
   si

s1

w1

s2

w2

s3

   

sn-     1

w3

wn-     1

sn

wn

    goal:	
   	
   calculate	
   the	
   best	
   value	
   of	
   p	
   for	
   any	
   
assignment	
   to	
      red   	
   r.v.s.,	
   respecting	
      gray   	
   
evidence	
   r.v.s.	
   and	
   summing	
   over	
   all	
   possible	
   
assignments	
   to	
      blue   	
   r.v.s.

aside

    probabilistic	
   graphical	
   models	
   are	
   an	
   

extremely	
   useful	
   framework	
   for	
   machine	
   
learning	
   in	
   nlp	
   and	
   other	
   complex	
   problems.
    id48s	
   are	
   one	
   kind	
   of	
   graphical	
   model;	
   there	
   

are	
   many	
   others	
   you	
   may	
   have	
   heard	
   of.
    bayesian	
   networks
    markov	
   networks
    factor	
   graphs

quantities	
   we	
   need

s1

   

si-     1

w1

wi-     1

si

wi

si+1

   

wi+1

sn

wn

p(w, si |  ,    ) =  s1...si 1  si+1...sn
=  s1...si 1  si+1...sn
=  s1...si 1
= fi(si)   bi(si)

p(s1 . . . si . . . sn, w |  ,    )
p(s1 . . . si, w1 . . . wi |  ,    )   p(si+1 . . . sn, wi+1 . . . wn | si,  ,    )

p(s1 . . . si, w1 . . . wi |  ,    )  si+1...sn

p(si+1 . . . sn, wi+1 . . . wn | si,  ,    )

quantities	
   we	
   need

s1

   

si-     1

fi

w1

wi-     1

si

wi

si+1

   

bi

wi+1

sn

wn

p(w, si |  ,    ) =  s1...si 1  si+1...sn
=  s1...si 1  si+1...sn
=  s1...si 1
= fi(si)   bi(si)

p(s1 . . . si . . . sn, w |  ,    )
p(s1 . . . si, w1 . . . wi |  ,    )   p(si+1 . . . sn, wi+1 . . . wn | si,  ,    )

p(s1 . . . si, w1 . . . wi |  ,    )  si+1...sn

p(si+1 . . . sn, wi+1 . . . wn | si,  ,    )

distribution	
   over	
   states	
   for	
   wi
    run	
   forward	
   and	
   backward	
   algorithms	
   to	
   

produce	
   fi and	
   bi.
    if	
   we	
   only	
   care	
   about	
   one	
   state,	
   we	
   can	
   stop	
   at	
   i.

    for	
   each	
     	
   in	
     :

p(si =  , w |  ,    ) = fi( )   bi( )
p(si =   | w,  ,    ) =

p(si =  , w |  ,    )
       
p(si =   , w |  ,    )
    note	
   that	
   the	
   denominator	
   is	
   p(w |	
     ,   ).

   posterior   

how	
   to	
   calculate	
      

given	
   the	
   id48	
   and	
   a	
   sequence:
      the	
   most	
   probable	
   state	
   sequence?
      the	
   id203	
   of	
   the	
   word	
   sequence?
      the	
   id203	
   distribution	
   over	
   states,	
   for	
   

each	
   word?

4. minimum	
   risk	
   sequence
given	
   states	
   and	
   sequences,	
   or	
   just	
   states:
5. the	
   parameters	
   of	
   the	
   id48	
   (   and	
     )?

building	
   on	
   per-     word	
   posteriors

    total	
   runtime	
   for	
   forward	
   and	
   backward	
   

algorithms	
   is	
   o(n|  |2).

    once	
   you	
   have	
   all	
   f	
   and	
   b	
   quantities,	
   you	
   can	
   
calculate	
   the	
   posteriors	
   for	
   every	
   word   s	
   label.

  si   arg max
     

p(si =   | w,  ,    )
   
   

fi( ) bi( )

    

    this	
   is	
   sometimes	
   called	
   posterior	
   decoding.

posterior	
   decoding

    this	
   approach	
   to	
   decoding	
   exploits	
   the	
   full	
   
distribution	
   over	
   sequences	
   to	
   choose	
   each	
   
word   s	
   label.	
   	
   for	
   each	
   i:

  si   arg max
     

p(si =   | w,  ,    )
   
   

fi( ) bi( )

    

    compare	
   with	
   map	
   decoding	
   (sometimes	
   called	
   

   viterbi   	
   decoding	
   after	
   the	
   algorithm	
   that	
   
accomplishes	
   it:

  s   arg max

s

p(s | w,  ,    )

which	
   one	
   to	
   use?

    they	
   will	
   not,	
   in	
   general,	
   give	
   the	
   same	
   label	
   

sequence.

    sometimes	
   one	
   works	
   better,	
   sometimes	
   the	
   

other.

    posterior	
   decoding	
   can	
   give	
   a	
   label	
   sequence	
   

that	
   itself	
   gets	
   zero	
   id203!

    there	
   is	
   a	
   way	
   to	
   unify	
   both.

cost

    imagine	
   that,	
   once	
   we	
   construct	
   our	
   id48,	
   we	
   are	
   

going	
   to	
   play	
   a	
   game.

    the	
   id48	
   will	
   be	
   given	
   a	
   new	
   sequence	
   w.
    we	
   must	
   label	
   w using	
   the	
   id48.
    our	
   label	
   sequence	
   s will	
   be	
   compared	
   to	
   the	
   true	
   one,	
   

    depending	
   on	
   how	
   badly	
   we	
   do,	
   we	
   will	
   pay	
   a	
   fine.
    we	
   want	
   to	
   minimize	
   the	
   cost.
    without	
   seeing	
   w,	
   our	
   strategy	
   will	
   depend	
   on	
   how	
   the	
   

cost	
   is	
   defined!

s*.

all-     or-     nothing	
   cost

    suppose	
   we	
   will	
   pay	
   1    if	
   we	
   get	
   the	
   sequence	
   

wrong,	
   i.e.,	
   if	
   s    	
   s*.

    otherwise	
   we	
   pay	
   nothing.

    what	
   should	
   we	
   do?
    if	
   we	
   trust	
   our	
   distribution	
   p(w,	
   s |  ,   ),	
   then	
   

we	
   should	
   use	
   the	
   most	
   probable	
   whole-     
sequence	
   s.
    viterbi

hamming	
   cost

    alternately,	
   suppose	
   we	
   pay	
   0.10	
       for	
   every	
   

word that	
   we	
   label	
   incorrectly.	
   

    this	
   is	
   more	
   forgiving,	
   and	
   suggests	
   that	
   we	
   focus	
   
on	
   reasoning	
   about	
   each	
   word	
   without	
   worrying	
   
about	
   the	
   coherence	
   of	
   the	
   whole	
   sequence.

    what	
   should	
   we	
   do?
    if	
   we	
   trust	
   our	
   distribution	
   p(w,	
   s |  ,   ),	
   then	
   we	
   

should	
   use	
   the	
   most	
   label	
   for	
   each	
   word.
    posterior	
   decoding

minimum	
   bayes	
   risk

    the	
   assumption	
   that	
   we	
   have	
   a	
   good	
   estimated	
   
distribution	
   p(w,	
   s |  ,   ) leads	
   naturally	
   to	
   the	
   
following	
   decoding	
   rule:
  s   arg min
ep(s|w, ,   )[cost(s , s)]
    pick	
   the	
   s that	
   is	
   least	
   offensive,	
   in	
   expectation.

s 

    with	
   all-     or-     nothing	
   cost,	
   we	
   get	
   map/viterbi	
   

decoding.

    with	
   hamming	
   cost,	
   we	
   get	
   posterior	
   decoding.

word-     wise	
   costs

    if	
   the	
   cost	
   function	
   is	
   a	
   sum	
   of	
   local	
   costs,	
   we	
   can	
   

exploit	
   linearity	
   of	
   expectation:

ep(s|w, ,   )[cost(s , s)] = ep(s|w, ,   )    i

costi(s i, si)   

=    i

ep(s|w, ,   )[costi(s i, si)]

    for	
   the	
   hamming	
   cost,	
   we	
   can	
   make	
   independent	
   
decisions	
   once	
   we	
   know	
   the	
   expected	
   local	
   costs.

manipulating	
   the	
   cost	
   function

    suppose	
   we	
   have	
   an	
   id48	
   for	
   named	
   entity	
   

recognition.
    tags	
   are	
   b,	
   i,	
   and	
   o.

    if	
   we	
   really	
   care	
   about	
   precision	
   (finding	
   only	
   

correct	
   named	
   entities,	
   at	
   risk	
   of	
   missing	
   
some),	
   what	
   cost	
   function	
   makes	
   sense?

    what	
   if	
   we	
   really	
   care	
   about	
   recall?

one	
   more	
   cost	
   function

    bio	
   tagging	
   again.
    suppose	
   we	
   assign	
   different	
   costs	
   to	
   recall,	
   

precision,	
   and	
   boundary errors.

hyp.:

b-     b

correct:
b-     b
b-     i merge
b-     o
recall
i-     b
i-     i merge
i-     o
recall
o-     b
o-     o

recall

b-     i
split

recall
split

recall
prec.

i-     b

b-     o
prec.
bound. merge
recall

prec.
bound. merge
recall

prec.

recall

i-     i
split

bound.
split

bound.
bound.
recall

i-     o
prec.
bound.

prec.
bound.

prec.

o-     b

bound.
recall

bound.
recall

recall

o-     o
prec.
bound.

prec.
bound.

prec.

a	
   more	
   complex	
   posterior

p(sisi+1 | w,  ,    )

    use	
   the	
   same	
   trick	
   as	
   before,	
   but	
   pair	
   up	
   slightly	
   

different	
   forward	
   and	
   backward	
   probabilities:

p(sisi+1, w |  ,    ) =  s1...si 1  si+2...sn

p(s1 . . . sisi+1 . . . sn, w |  ,    )

p(s1 . . . si, w1 . . . wi |  ,    )    (si+1 | si)      (wi+1 | si+1)

=  s1...si 1
   si+2...sn
= fi(si)    (si+1 | si)      (wi+1 | si+1)   bi+1(si+1)

p(si+2 . . . sn, wi+1 . . . wn | si+1,  ,    )

pairwise	
   posterior

s1

   

si-     1

fi

w1

wi-     1

si

wi

si+1

   

bi+1

wi+1

sn

wn

p(sisi+1, w |  ,    ) =  s1...si 1  si+2...sn

p(s1 . . . sisi+1 . . . sn, w |  ,    )

p(s1 . . . si, w1 . . . wi |  ,    )    (si+1 | si)      (wi+1 | si+1)

=  s1...si 1
   si+2...sn
= fi(si)    (si+1 | si)      (wi+1 | si+1)   bi+1(si+1)

p(si+2 . . . sn, wi+1 . . . wn | si+1,  ,    )

a	
   problem

    we	
   can	
   label	
   each	
   adjacent	
   pair	
   of	
   words,	
   but	
   

nothing	
   guarantees	
   that	
   our	
   labels	
   will	
   be	
   
consistent	
   with	
   each	
   other.	
   

pop	
   quiz

1.	
   	
   can	
   you	
   think	
   of	
   a	
   cost	
   function	
   such	
   that	
   

minimum	
   bayes	
   risk	
   decoding	
   can   t be	
   done	
   in	
   
polynomial	
   time?

quiz	
   answer	
   (1)

    every	
   word	
   must	
   get	
   a	
   different	
   label.

    equivalently,	
   we	
   can	
   use	
   each	
   label	
   at	
   most	
   once.

    each	
   label	
   must	
   be	
   used	
   exactly	
   once.

    hamiltonian	
   path.

pop	
   quiz

part	
   2:
    we	
   want	
   to	
   choose	
   a	
   single	
   sequence	
   of	
   labels	
   

to	
   minimize	
   risk.

    the	
   scoring	
   function	
   factors	
   into	
   local	
   scores	
   

(sum	
   of	
   pairwise	
   posteriors).

    how	
   do	
   we	
   do	
   it?

quiz	
   answer	
   (2)

    pairwise	
   local	
   costs	
   are	
   kind	
   of	
   like	
   transition	
   

probabilities.
    except	
   we	
   want	
   to	
   minimize their	
   sum rather	
   than	
   

maximize	
   their	
   product.

    viterbi	
   algorithm,	
   but	
   with	
   min-     cost	
   semiring,	
   

a	
   different	
      transition   	
   score,	
   and	
   no	
   
   emission   	
   score.

pairwise	
   minimum	
   bayes	
   risk

    first,	
   run	
   forward	
   and	
   backward.
    use	
   posteriors	
   to	
   score	
   all	
   possible	
   label-     pairs	
   

for	
   all	
   adjacent	
   words.

    use	
   a	
   viterbi-     like	
   algorithm	
   to	
   find	
   the	
   best-     

scoring,	
   consistent	
   labeling.
    use	
   min-     cost	
   semiring.
    algorithm	
   scores	
   label	
   pairs	
   by	
   local	
   cost	
   times	
   

posterior	
   id203	
   (local	
   risk).

how	
   to	
   calculate	
      

given	
   the	
   id48	
   and	
   a	
   sequence:
      the	
   most	
   probable	
   state	
   sequence?
      the	
   id203	
   of	
   the	
   word	
   sequence?
      the	
   id203	
   distribution	
   over	
   states,	
   for	
   

each	
   word?

      minimum	
   risk	
   sequence
given	
   states	
   and	
   sequences,	
   or	
   just	
   states:
5. the	
   parameters	
   of	
   the	
   id48	
   (   and	
     )?

lecture	
   outline

      markov	
   models
      hidden	
   markov	
   models
      viterbi	
   algorithm
      other	
   id136	
   algorithms	
   for	
   id48s
5. learning	
   algorithms	
   for	
   id48s

learning	
   id48s

    typical	
   starting	
   point:	
   	
   we	
   have	
   some	
   data	
   to	
   

learn	
   from,	
   and	
   we	
   know	
   how	
   many	
   states	
   the	
   
id48	
   has.
    we	
   may	
   also	
   have	
   constraints	
   on	
   the	
   states,	
   but	
   

assume	
   for	
   now	
   that	
   we	
   do	
   not.

    two	
   main	
   possibilities:

    supervised:	
   	
   we	
   have	
   complete data:	
   	
   example	
   

pairs	
   (w,	
   s).

    unsupervised:	
   	
   we	
   only	
   have	
   examples	
   of	
   w.

supervised	
   learning	
   of	
   id48s

    the	
   building	
   blocks	
   of	
   id48s	
   are	
   multinomial	
   

distributions
      :	
   	
   distribution	
   over	
   next	
   state	
   given	
   current	
   state
      :	
   	
   distribution	
   over	
   word	
   given	
   current	
   state
    statistics	
   offers	
   us	
   the	
   maximum	
   likelihood

principle:

      ,            arg max
  ,      

p(s, w |  ,    )

separability	
   of	
   learning

    with	
   observed	
   data,	
   each	
   state   s	
   transition	
   
and	
   emission	
   distributions	
   can	
   be	
   estimated	
   
separately
    from	
   each	
   other
    from	
   those	
   of	
   all	
   other	
   states

    the	
   result	
   is	
   that	
   learning	
   is	
   very	
   simple	
   and	
   

fast.

id113	
   by	
   relative	
   frequencies

    (i   m	
   skipping	
   the	
   derivation;	
   it	
   involves	
   log	
   

likelihood,	
   a	
   lagrangian	
   multiplier,	
   and	
   some	
   
differential	
   calculus.)

   (  |  0) =

   (w |    ) =

freq( 0 )
freq( )

freq     
w    

freq(   )

graphical	
   models	
   view

s1

w1

s2

w2

  

s3

w3

  

   

sn-     1

wn-     1

sn

wn

learning	
   with	
   a	
   prior

    the	
   techniques	
   described	
   so	
   far	
   are	
   examples	
   of	
   

maximum	
   likelihood	
   estimation	
   (id113).

    id113	
   works	
   well	
   when	
   there	
   is	
   a	
   lot	
   of	
   data.

    we	
   never	
   have	
   as	
   much	
   as	
   we   d	
   like.

    learning	
   with	
   a	
   prior	
   is	
   a	
   way	
   to	
   inject	
   some	
   

background	
   knowledge	
   and	
   avoid	
   overfitting	
   to	
   
the	
   training	
   data.	
   

    map	
   learning:

      ,              arg max
  ,      

p(s, w |  ,    )   p( ,    )

this	
   part	
   is	
   new

priors	
   for	
   id48s

    id48s	
   are	
   built	
   out	
   of	
   multinomial	
   distributions.	
   
    an	
   easy	
   prior	
   for	
   the	
   multinomial	
   distribution	
   is	
   

the	
   dirichlet	
   distribution.
    simplest	
   version:	
   	
   symmetric, non-     sparse	
   dirichlet.

    in	
   practice:

    choose	
     t >	
   1	
   for	
   transitions	
   and	
     e >	
   1	
   for	
   emissions.
    before	
   normalizing	
   frequencies,	
   add	
     t     1	
   to	
   

transition	
   counts	
   and	
     e     1	
   to	
   emission	
   counts.

    exactly	
   the	
   same	
   as	
   additive	
   smoothing	
   in	
   language	
   

models.

graphical	
   models	
   view

s1

w1

s2

w2

  

s3

w3

  

  t

   

sn-     1

wn-     1

  e

sn

wn

unsupervised	
   learning	
   of	
   id48s

    historically	
   (going	
   back	
   to	
   the	
   1960s),	
   we	
   do	
   
not	
   observe	
   the	
   states,	
   even	
   during	
   training	
   
time.
    hence	
   the	
   name:	
   	
   hidden markov	
   models.

    the	
   earliest	
   instance	
   of	
   a	
   parameter	
   

estimation	
   problem	
   where	
   the	
   data	
   are	
   
incomplete.

    how	
   do	
   we	
   learn	
   only	
   from	
   w?

we	
   already	
   have	
   all	
   the	
   tools!

    statistics:	
   	
   maximum	
   likelihood	
   estimation.
    counting	
   events	
   and	
   normalizing	
   the	
   counts.
    computation:	
   	
   id136	
   about	
   posteriors.

    forward	
   and	
   backward	
   algorithms.

graphical	
   models	
   view

s1

w1

s2

w2

  

s3

w3

  

states	
   are	
   now	
   missing!

   

sn-     1

wn-     1

sn

wn

mixed	
   id136

    mixed	
   id136	
   problems	
       where	
   we	
   want	
   to	
   
sum	
   out	
   some	
   random	
   variables	
   while	
   getting	
   
values	
   for	
   others	
       are	
   hard	
   in	
   general.
    indeed,	
   finding	
   the	
   id113	
   is	
   an	
   np-     hard	
   

problem!

    optimization	
   view:	
   	
   we	
   are	
   optimizing	
   a	
   non-     

convex	
   function	
   (of	
   the	
   parameters).

high-     level	
   view	
   of	
   em

    em	
   stands	
   for	
      expectation	
   maximization.   

s1

w
1

s2

w
2

  

s3

w
3

  

   

sn
-     1

wn-     1

sn

w
n

s1

w
1

s2

w
2

  

s3

w
3

  

   

sn
-     1

wn-     1

sn

w
n

e	
   step:	
   	
   infer	
   posterior	
   
distribution	
   over	
   missing	
   
data.

m	
   step:	
   	
   maximum	
   
likelihood	
   estimation	
   with	
   
soft	
   values	
   for	
   missing	
   
data.

procedural	
   view	
   of	
   em

    begin	
   with	
   an	
   initial	
   estimate	
   of	
   the	
   

parameters	
   of	
   the	
   id48	
   (   and	
     ).

    iterate:

    e	
   step:	
   	
   calculate	
   id203	
   of	
   each	
   possible	
   
transition	
   and	
   each	
   possible	
   emission	
   at	
   each	
   
position.

    m	
   step:	
   	
   re-     estimate	
   parameters	
   to	
   maximize	
   

likelihood	
   of	
      complete   	
   data.

e	
   step

or

    calculate	
   p(s |	
   w,	
     ,	
     )	
   for	
   each	
   s and	
   count	
   

each	
   s proportional	
   to	
   its	
   id203.

    calculate	
   p(si |	
   w,	
     ,	
     )	
   for	
   each	
   i,	
   and	
   count	
   
each	
   emission	
   of	
   wi from	
   si proportional	
   to	
   its	
   
id203.

    calculate	
   p(si si+1 |	
   w,	
     ,	
     )	
   for	
   each	
   i,	
   and	
   

count	
   each	
   transition	
   from	
   si to	
   si+1
proportional	
   to	
   its	
   id203.

e	
   step:	
   	
   per-     word	
   and	
   pairwise	
   

posteriors

    given	
   w and	
   current	
   parameters,	
   run	
   forward	
   
and	
   backward	
   algorithms	
   to	
   obtain,	
   for	
   each	
   i,	
   
the	
   posteriors:
    p(si |	
   w,	
     ,	
     )	
   
    p(si si+1 |	
   w,	
     ,	
     )	
   

    think	
   of	
   these	
   as	
   soft or	
   fractional counts	
   of	
   

transition	
   and	
   emission	
   events.

soft	
   counts	
   from	
   posteriors
   freq(       ) =    i
p(si =    , si+1 =      | w,  ,    )
=    i
fi(   )    (     |    )      (wi+1 |     )   bi(    )
   freq     
w     =    i:wi=w
=    i:wi=w
   freq(   ) =    i
=    i

p(si =     | w,  ,    )
fi(   )   bi(   )

p(si =     | w,  ,    )
p(si =     | w,  ,    )

procedural	
   view	
   of	
   em

    begin	
   with	
   an	
   initial	
   estimate	
   of	
   the	
   

parameters	
   of	
   the	
   id48	
   (   and	
     ).

    iterate:

    e	
   step:	
   	
   calculate	
   id203	
   of	
   each	
   possible	
   
transition	
   and	
   each	
   possible	
   emission	
   at	
   each	
   
position.

    m	
   step:	
   	
   re-     estimate	
   parameters	
   to	
   maximize	
   

likelihood	
   of	
      complete   	
   data.

m	
   step:	
   	
   id113	
   by	
   relative	
   frequencies

    when	
   we	
   observed	
   all	
   the	
   data,	
   we	
   used	
   hard	
   

counts:

   (    |     ) =

freq(       )
freq(    )

freq     
w    

freq(   )

    now	
   we	
   do	
   the	
   same	
   with	
   soft	
   counts:

   (w |    ) =
   (    |     ) =    freq(       )
   freq(    )
     (w |    ) =    freq     
w    
   freq(   )

em:	
   	
   assurances

    each	
   iteration	
   of	
   em	
   will	
   give	
   us	
   an	
   estimate	
   

with	
   a	
   better	
   likelihood	
   than	
   the	
   last.
    eventually	
   we	
   will	
   converge	
   to	
   a	
   local	
   

optimum	
   (or	
   saddle	
   point).
    we	
   usually	
   do	
   not	
   worry	
   about	
   saddle	
   points.
    where	
   you	
   end	
   up	
   depends	
   on	
   where	
   you	
   start.

the	
   importance	
   of	
   initialization

   hard   	
   em

    instead	
   of	
   using	
   forward-     backward	
   to	
   get	
   
fractional	
   counts,	
   we	
   can	
   use	
   viterbi	
   to	
   get	
   
hard	
   counts.

    this	
   equates	
   to:

s1

w
1

s2

w
2

  

s3

w
3

  

   

sn
-     1

wn-     1

sn

w
n

s1

w
1

s2

w
2

  

s3

w
3

  

   

sn
-     1

wn-     1

sn

w
n

   hard   	
   em

    can	
   be	
   understood	
   as	
   coordinate	
   ascent	
   on	
   

this	
   maximization	
   problem:

max
 ,   ,s

p(s, w |  ,    )

    some	
   people	
   prefer	
   this.

    faster	
   to	
   converge	
   (but	
   to	
   a	
   different	
   solution).
    viterbi	
   algorithm	
   instead	
   of	
   forward-     backward.
    maybe	
   more	
   brittle.

terminology

    expectation-     maximization	
   (em)	
   is	
   a	
   very	
   

general	
   technique,	
   not	
   just	
   for	
   id48s.
    applicable	
   to	
   any generative	
   model!
    you	
   may	
   have	
   seen	
   it	
   for	
   mixtures	
   of	
   gaussians	
   or	
   

other	
   kinds	
   of	
   id91.

    k-     means	
   id91	
   is	
   a	
   kind	
   of	
   hard	
   em.
    sometimes	
   the	
   id48	
   version	
   is	
   called	
   

baum-     welch training	
   or	
   forward-     backward
training.

id48s	
   and	
   
weighted	
   finite-     state	
   machines

finite-     state	
   machines

from	
   formal	
   language	
   theory	
   and	
   theory	
   of	
   

computation:

    finite	
   set	
   of	
   states.
    set	
   of	
   allowed	
   transitions	
   between	
   states.
    nondeterministic	
   walk	
   among	
   states.
    each	
   state	
   (alternately,	
   each	
   transition)	
   

generates	
   a	
   symbol.

id48s	
   are	
   probabilistic	
   fsas

go	
   from	
      nondeterministic   	
   to	
      probabilistic.   
    put	
   a	
   id203	
   distribution	
   on	
   the	
   

transitions	
   out	
   of	
   each	
   state.

    put	
   a	
   id203	
   distribution	
   on	
   the	
   emissions	
   

from	
   each	
   state.

powerful	
   generalization

    if	
   the	
   finite-     state	
   machine	
   reads	
   one	
   sequence	
   
and	
      transcribes   	
   it	
   into	
   another	
   sequence,	
   we	
   
have	
   a finite-     state	
   transducer.
    sometimes:	
   	
      read	
   one	
   tape	
   and	
   write	
   one	
   tape.   

    we	
   can	
   make	
   these	
   probabilistic	
   as	
   well,	
   in	
   a	
   lot	
   

of	
   different	
   ways.

    allows	
   composition	
   of	
   stochastic	
   relations,	
   or	
   

chaining	
   together	
   of	
   string-     to-     string	
   
transformations.

    algorithms	
   for	
   id136	
   and	
   learning	
   are	
   similar	
   

to	
   what	
   we	
   have	
   seen.

examples	
   of	
   composed	
   fsts

    speech	
   recognition:	
   	
   

acoustic	
   signal	
      	
   pronounced	
   phonemes	
      	
   
canonicalized	
   words.

    translation:	
   	
   

words	
   in	
   czech	
      	
   morphemes	
   in	
   czech	
      	
   
morphemes	
   in	
   slovak	
      	
   words	
   in	
   slovak

more	
   advanced	
   topics

    feature-     based	
   parameterizations	
   of	
   id48s	
   and	
   fsts

    representation	
   learning	
   (spectral,	
   neural,	
   ...)

    discriminative	
   versions	
   of	
   id48s	
   (e.g.,	
   crfs,	
   id195)
    weakening	
   independence	
   assumptions	
   (e.g.,	
   semi-     

id48s)

    method	
   of	
   moments	
   estimation	
   (   spectral   )
    generalizing	
   id48s	
   and	
   fsts	
   to	
   probabilistic	
   and	
   
weighted	
   context-     free	
   grammars	
   (and	
   beyond)	
   to	
   
model	
   long-     distance	
   interactions	
   and	
   reordering.

    bayesian	
   id136	
   and	
   learning
    nonparametric	
   priors	
   (e.g.,	
   the	
      infinite   	
   id48)

lecture	
   outline

      markov	
   models
      hidden	
   markov	
   models
      viterbi	
   algorithm
      other	
   id136	
   algorithms	
   for	
   id48s
      learning	
   algorithms	
   for	
   id48s

thanks!

