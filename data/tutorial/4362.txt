introduction

algorithms

random sharding

natural tasks

conclusion

id72

from large-scale high-dimensional data

stefan riezler   

(joint work with patrick simianer    and chris dyer   )

    department of computational linguistics, heidelberg university, germany
    language technologies institute, carnegie mellon university, pittsburgh, pa

1 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

big data

    data can be characterized as big by

    large size of training set,
    high dimensionality of feature representation of data.

    not all datasets advertised as    large    meet both requirements
(e.g. learning-to-rank challenges at yahoo! and microsoft
work on hundreds of features for tens of thousands of queries)
    our application scenario is id151

(smt), using billions of features and training examples.

2 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

big data

    data can be characterized as big by

    large size of training set,
    high dimensionality of feature representation of data.

    not all datasets advertised as    large    meet both requirements
(e.g. learning-to-rank challenges at yahoo! and microsoft
work on hundreds of features for tens of thousands of queries)
    our application scenario is id151

(smt), using billions of features and training examples.

2 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

big data

    data can be characterized as big by

    large size of training set,
    high dimensionality of feature representation of data.

    not all datasets advertised as    large    meet both requirements
(e.g. learning-to-rank challenges at yahoo! and microsoft
work on hundreds of features for tens of thousands of queries)
    our application scenario is id151

(smt), using billions of features and training examples.

2 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

large scale learning

    learning problem is large scale if
http://hunch.net/?p=330, 2008),

    training data cannot be stored in ram (langford on
    time constraint requires that algorithms scale at worst linearly

with number of examples (bottou & bousquet nips   07).

    solutions:

(bottou & le cun nips   04),

representation (langford et al. jmlr   09),

    online learning for linear scaling in training sample size
    combined with feature selection for memory ef   cient feature
    combined with parallelization and averaging for parallel
acceleration and reduced variance at asymptotic online
learning guarantees (zinkevich et al. nips   10) .
    we add another dimension: id72.

3 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

large scale learning

    learning problem is large scale if
http://hunch.net/?p=330, 2008),

    training data cannot be stored in ram (langford on
    time constraint requires that algorithms scale at worst linearly

with number of examples (bottou & bousquet nips   07).

    solutions:

(bottou & le cun nips   04),

representation (langford et al. jmlr   09),

    online learning for linear scaling in training sample size
    combined with feature selection for memory ef   cient feature
    combined with parallelization and averaging for parallel
acceleration and reduced variance at asymptotic online
learning guarantees (zinkevich et al. nips   10) .
    we add another dimension: id72.

3 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

large scale learning

    learning problem is large scale if
http://hunch.net/?p=330, 2008),

    training data cannot be stored in ram (langford on
    time constraint requires that algorithms scale at worst linearly

with number of examples (bottou & bousquet nips   07).

    solutions:

(bottou & le cun nips   04),

representation (langford et al. jmlr   09),

    online learning for linear scaling in training sample size
    combined with feature selection for memory ef   cient feature
    combined with parallelization and averaging for parallel
acceleration and reduced variance at asymptotic online
learning guarantees (zinkevich et al. nips   10) .
    we add another dimension: id72.

3 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

id72

simultaneously from data belonging to different tasks.

    goal: a number of statistical models need to be estimated
    examples:

    ocr of handwritten characters from different writers: exploit
    ltr from search engine query logs from different countries:

commonalities on pixel- or stroke-level shared between writers.

some queries are country-speci   c (   football   ), most preference
rankings are shared across countries.

    idea:

    learn a shared model that takes advantage of commonalities

among tasks, without neglecting individual knowledge.

4 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

id72

simultaneously from data belonging to different tasks.

    goal: a number of statistical models need to be estimated
    examples:

    ocr of handwritten characters from different writers: exploit
    ltr from search engine query logs from different countries:

commonalities on pixel- or stroke-level shared between writers.

some queries are country-speci   c (   football   ), most preference
rankings are shared across countries.

    idea:

    learn a shared model that takes advantage of commonalities

among tasks, without neglecting individual knowledge.

4 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

id72

simultaneously from data belonging to different tasks.

    goal: a number of statistical models need to be estimated
    examples:

    ocr of handwritten characters from different writers: exploit
    ltr from search engine query logs from different countries:

commonalities on pixel- or stroke-level shared between writers.

some queries are country-speci   c (   football   ), most preference
rankings are shared across countries.

    idea:

    learn a shared model that takes advantage of commonalities

among tasks, without neglecting individual knowledge.

4 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our application: learning from big data in smt

    machine learning theory and practice suggests bene   ts from
using expressive feature representations and from tuning
on large training samples.

    discriminative training in smt has mostly been content with
tuning small sets of dense features on small development
data (och naacl   03).

    notable exceptions using larger feature and training sets:
    liang et al. acl   06: 1.5m features, 67k parallel sentences.
    tillmann and zhang acl   06: 35m feats, 230k sents.
    blunsom et al. acl   08: 7.8m feats, 100k sents.
    simianer, riezler, dyer acl   12: 4.7m feats, 1.6m sents.
    flanigan, dyer, carbonell naacl   13: 28.8m feats, 1m sents.

5 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our application: learning from big data in smt

    machine learning theory and practice suggests bene   ts from
using expressive feature representations and from tuning
on large training samples.

    discriminative training in smt has mostly been content with
tuning small sets of dense features on small development
data (och naacl   03).

    notable exceptions using larger feature and training sets:
    liang et al. acl   06: 1.5m features, 67k parallel sentences.
    tillmann and zhang acl   06: 35m feats, 230k sents.
    blunsom et al. acl   08: 7.8m feats, 100k sents.
    simianer, riezler, dyer acl   12: 4.7m feats, 1.6m sents.
    flanigan, dyer, carbonell naacl   13: 28.8m feats, 1m sents.

5 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our application: learning from big data in smt

    machine learning theory and practice suggests bene   ts from
using expressive feature representations and from tuning
on large training samples.

    discriminative training in smt has mostly been content with
tuning small sets of dense features on small development
data (och naacl   03).

    notable exceptions using larger feature and training sets:
    liang et al. acl   06: 1.5m features, 67k parallel sentences.
    tillmann and zhang acl   06: 35m feats, 230k sents.
    blunsom et al. acl   08: 7.8m feats, 100k sents.
    simianer, riezler, dyer acl   12: 4.7m feats, 1.6m sents.
    flanigan, dyer, carbonell naacl   13: 28.8m feats, 1m sents.

5 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our approach: multi-task distributed sgd

    distribute work and share information!

    online learning via stochastic id119 optimization.
    distributed learning using hadoop/mapreduce or
    feature selection via (cid:96)1/(cid:96)2 block norm id173 on

sungridengine.

features across multiple tasks.

    pooling baseline:

    concatenate data from all tasks into one big pool.
    becomes infeasible very quickly.
    independent modeling baseline :

    independent training of task speci   c models.
    does not share any knowledge across tasks.

6 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our approach: multi-task distributed sgd

    distribute work and share information!

    online learning via stochastic id119 optimization.
    distributed learning using hadoop/mapreduce or
    feature selection via (cid:96)1/(cid:96)2 block norm id173 on

sungridengine.

features across multiple tasks.

    pooling baseline:

    concatenate data from all tasks into one big pool.
    becomes infeasible very quickly.
    independent modeling baseline :

    independent training of task speci   c models.
    does not share any knowledge across tasks.

6 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our approach: multi-task distributed sgd

    distribute work and share information!

    online learning via stochastic id119 optimization.
    distributed learning using hadoop/mapreduce or
    feature selection via (cid:96)1/(cid:96)2 block norm id173 on

sungridengine.

features across multiple tasks.

    pooling baseline:

    concatenate data from all tasks into one big pool.
    becomes infeasible very quickly.
    independent modeling baseline :

    independent training of task speci   c models.
    does not share any knowledge across tasks.

6 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our approach: multi-task distributed sgd

    distribute work and share information!

    online learning via stochastic id119 optimization.
    distributed learning using hadoop/mapreduce or
    feature selection via (cid:96)1/(cid:96)2 block norm id173 on

sungridengine.

features across multiple tasks.

    pooling baseline:

    concatenate data from all tasks into one big pool.
    becomes infeasible very quickly.
    independent modeling baseline :

    independent training of task speci   c models.
    does not share any knowledge across tasks.

6 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our approach: multi-task distributed sgd

    distribute work and share information!

    online learning via stochastic id119 optimization.
    distributed learning using hadoop/mapreduce or
    feature selection via (cid:96)1/(cid:96)2 block norm id173 on

sungridengine.

features across multiple tasks.

    pooling baseline:

    concatenate data from all tasks into one big pool.
    becomes infeasible very quickly.
    independent modeling baseline :

    independent training of task speci   c models.
    does not share any knowledge across tasks.

6 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

our approach: multi-task distributed sgd

    distribute work and share information!

    online learning via stochastic id119 optimization.
    distributed learning using hadoop/mapreduce or
    feature selection via (cid:96)1/(cid:96)2 block norm id173 on

sungridengine.

features across multiple tasks.

    pooling baseline:

    concatenate data from all tasks into one big pool.
    becomes infeasible very quickly.
    independent modeling baseline :

    independent training of task speci   c models.
    does not share any knowledge across tasks.

6 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

related work

    online learning:

    we deploy pairwise ranking id88 (shen & joshi
    and margin id88 (collobert & bengio icml   04).

jmlr   05)

    distributed learning:

    without feature selection, our algorithm reduces to iterative
    which itself is related to id112 (breiman jmlr   96) if shards

mixing (mcdonald et al. naacl   10),

are treated as random samples.

7 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

related work

    online learning:

    we deploy pairwise ranking id88 (shen & joshi
    and margin id88 (collobert & bengio icml   04).

jmlr   05)

    distributed learning:

    without feature selection, our algorithm reduces to iterative
    which itself is related to id112 (breiman jmlr   96) if shards

mixing (mcdonald et al. naacl   10),

are treated as random samples.

7 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

related work

    (cid:96)1/(cid:96)2 id173:

    related to group-lasso approaches which use mixed norms
(yuan & lin jrss   06), hierarchical norms (zhao et al. annals
stats   09), structured norms (martins et al. emnlp   11).

    difference: norms and proximity operators are applied to

groups of features in single regression or classi   cation task    
id72 groups features orthogonally by tasks.

    closest relation to obozinski et al. statcomput   10: our

algorithm is weight-based backward feature elimination variant
of their gradient-based forward feature selection algorithm.

8 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ol framework: pairwise ranking id88

    preference pairs xj = (x(1)
x(2)
j w.r.t. sentence-wise id7 (nakov et al. coling   12).

) where x(1)

is ordered above

, x(2)

j

j

j

    hinge loss-type objective

j

j     x(2)

where   xj = x(1)
vector, and (cid:104)  ,  (cid:105) denotes the standard vector dot product.
    ranking id88 by stochastic subid119:

lj (w) = (   (cid:104)w,   xj (cid:105))+
, (a)+ = max(0, a) , w     ird is a weight
(cid:40)
     xj
0 else.

if (cid:104)w,   xj(cid:105)     0,

   lj (w) =

9 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ol framework: pairwise ranking id88

    preference pairs xj = (x(1)
x(2)
j w.r.t. sentence-wise id7 (nakov et al. coling   12).

) where x(1)

is ordered above

, x(2)

j

j

j

    hinge loss-type objective

j

j     x(2)

where   xj = x(1)
vector, and (cid:104)  ,  (cid:105) denotes the standard vector dot product.
    ranking id88 by stochastic subid119:

lj (w) = (   (cid:104)w,   xj (cid:105))+
, (a)+ = max(0, a) , w     ird is a weight
(cid:40)
     xj
0 else.

if (cid:104)w,   xj(cid:105)     0,

   lj (w) =

9 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ol framework: pairwise ranking id88

    preference pairs xj = (x(1)
x(2)
j w.r.t. sentence-wise id7 (nakov et al. coling   12).

) where x(1)

is ordered above

, x(2)

j

j

j

    hinge loss-type objective

j

j     x(2)

where   xj = x(1)
vector, and (cid:104)  ,  (cid:105) denotes the standard vector dot product.
    ranking id88 by stochastic subid119:

lj (w) = (   (cid:104)w,   xj (cid:105))+
, (a)+ = max(0, a) , w     ird is a weight
(cid:40)
     xj
0 else.

if (cid:104)w,   xj(cid:105)     0,

   lj (w) =

9 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ol framework: margin id88

    hinge loss-type objective

    stochastic subid119:

lj (w) = (1     (cid:104)w,   xj (cid:105))+
(cid:40)
     xj
0 else.

if (cid:104)w,   xj(cid:105) < 1,

   lj (w) =

    margin term controls capacity, but results in more updates.
    collobert & bengio (icml   04) argue that this justi   es not using
an explicit id173 (as for example in an sgd version of
the id166 (shalev-shwartz et al. icml   07)).

10 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ol framework: margin id88

    hinge loss-type objective

    stochastic subid119:

lj (w) = (1     (cid:104)w,   xj (cid:105))+
(cid:40)
     xj
0 else.

if (cid:104)w,   xj(cid:105) < 1,

   lj (w) =

    margin term controls capacity, but results in more updates.
    collobert & bengio (icml   04) argue that this justi   es not using
an explicit id173 (as for example in an sgd version of
the id166 (shalev-shwartz et al. icml   07)).

10 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ol framework: margin id88

    hinge loss-type objective

    stochastic subid119:

lj (w) = (1     (cid:104)w,   xj (cid:105))+
(cid:40)
     xj
0 else.

if (cid:104)w,   xj(cid:105) < 1,

   lj (w) =

    margin term controls capacity, but results in more updates.
    collobert & bengio (icml   04) argue that this justi   es not using
an explicit id173 (as for example in an sgd version of
the id166 (shalev-shwartz et al. icml   07)).

10 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

mtl framework: (cid:96)1/(cid:96)2 block norm id173

    data points {(xzn, yzn), z = 1, . . . , z , n = 1, . . . , nz},
sampled from pz on x    y (z = task; n = data point).
    objective:

(cid:88)

min
w

z,n

ln(wz) +   ||w||1,2

    where w = (wd

z )z,d is a z -by-d matrix w = (wd

z )z,d of

d-dimensional row vectors wz and z -dimensional column
vectors wd of weights associated with feature d across tasks.

    weighted (cid:96)1/(cid:96)2 norm:

  ||w||1,2 =   

d(cid:88)
d=1||wd||2

    each (cid:96)2 norm of a weight column wd represents the relevance

of the corresponding feature across tasks.

11 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

mtl framework: (cid:96)1/(cid:96)2 block norm id173

    data points {(xzn, yzn), z = 1, . . . , z , n = 1, . . . , nz},
sampled from pz on x    y (z = task; n = data point).
    objective:

(cid:88)

min
w

z,n

ln(wz) +   ||w||1,2

    where w = (wd

z )z,d is a z -by-d matrix w = (wd

z )z,d of

d-dimensional row vectors wz and z -dimensional column
vectors wd of weights associated with feature d across tasks.

    weighted (cid:96)1/(cid:96)2 norm:

  ||w||1,2 =   

d(cid:88)
d=1||wd||2

    each (cid:96)2 norm of a weight column wd represents the relevance

of the corresponding feature across tasks.

11 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

mtl framework: (cid:96)1/(cid:96)2 block norm id173

    data points {(xzn, yzn), z = 1, . . . , z , n = 1, . . . , nz},
sampled from pz on x    y (z = task; n = data point).
    objective:

(cid:88)

min
w

z,n

ln(wz) +   ||w||1,2

    where w = (wd

z )z,d is a z -by-d matrix w = (wd

z )z,d of

d-dimensional row vectors wz and z -dimensional column
vectors wd of weights associated with feature d across tasks.

    weighted (cid:96)1/(cid:96)2 norm:

  ||w||1,2 =   

d(cid:88)
d=1||wd||2

    each (cid:96)2 norm of a weight column wd represents the relevance

of the corresponding feature across tasks.

11 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

(cid:96)1/(cid:96)2 id173 explained

be 0 and others to have high weights across tasks.

    (cid:96)1 sum of (cid:96)2 norms encourages several feature columns wd to
    algorithm idea:

    contribution to loss reduction must outweigh regularizer
penalty in order to activate feature by non-zero weight.
    weight-based feature elimination criterion:

if ||wd||2       , set w[z][d] = 0,   z.

    implementation by threshold on k features or by threshold   .

12 / 33

w1w2w3w4w5w1w2w3w4w5wz1[64000][64000]wz2[00300][30000]wz3[00023][23000]column   2norm:6432375000   1sum:   18   12introduction

algorithms

random sharding

natural tasks

conclusion

(cid:96)1/(cid:96)2 id173 explained

be 0 and others to have high weights across tasks.

    (cid:96)1 sum of (cid:96)2 norms encourages several feature columns wd to
    algorithm idea:

    contribution to loss reduction must outweigh regularizer
penalty in order to activate feature by non-zero weight.
    weight-based feature elimination criterion:

if ||wd||2       , set w[z][d] = 0,   z.

    implementation by threshold on k features or by threshold   .

12 / 33

w1w2w3w4w5w1w2w3w4w5wz1[64000][64000]wz2[00300][30000]wz3[00023][23000]column   2norm:6432375000   1sum:   18   12introduction

algorithms

random sharding

natural tasks

conclusion

id72 algorithm

algorithm 1 multi-task distributed sgd

get data for z tasks, each including s sentences;
distribute to machines.
initialize v     0.
for epochs t     0 . . . t     1: do

perform task-speci   c learning

for all tasks z     {1 . . . z}: parallel do
end for
stack weights w     [w1,t,s,0| . . . |wz ,t,s,0]t
perform (cid:96)1/(cid:96)2 id173

end for
return v

13 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

implementation as feature selection algorithm

algorithm 2 multi-task distributed sgd

get data for z tasks, each including s sentences;
distribute to machines.
initialize v     0.
for epochs t     0 . . . t     1: do

for all tasks z     {1 . . . z}: parallel do

wz,t,0,0     v
for all sentences i     {0 . . . s     1}: do

decode ith input with wz,t,i,0.
for all pairs j     {0 . . . p     1}: do

wz,t,i,j+1     wz,t,i,j          lj (wz,t,i,j )

end for
wz,t,i+1,0     wz,t,i,p

end for

end for
stack weights w     [w1,t,s,0| . . . |wz ,t,s,0]t
select top k feature columns of w by (cid:96)2 norm
for k     1 . . . k do

z(cid:80)

z=1

v[k] = 1
z

end for

end for
return v

w[z][k]

14 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

implementation as adaptive path-following algorithm

algorithm 3 path-following multi-task distributed sgd
get data for z tasks, each including s sentences; distribute to machines.
initialize v     0;   0,   min,  .
for epochs t     0 . . . t     1: do

perform task-speci   c learning

for all tasks z     {1 . . . z}: parallel do
end for
stack weights w     [w1,t,s,0| . . . |wz ,t,s,0]t
for feature columns d     {1 . . . d} in w: do

if ||wd||2       t then
else

v[d] = 0

v[d] = 1
z

z(cid:80)

z=1

end if

end for
set   t+1 = min{  t ,
if   t+1 <   min then

break

end if

end for
return v

w[z][d]

(cid:80)
z,i,j (lz,i,j (vt   1)   lz,i,j (vt ))

 

}

15 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

smt using synchronous context-free grammars

(1) x     x1 hat x2 versprochen; x1 promised x2
(2) x     x1 hat mir x2 versprochen;
(3) x     x1 versprach x2; x1 promised x2

x1 promised me x2

    hierarchical phrase-based translation (chiang cl   07),

formalizes translation rules as productions of synchronous
context-free grammar (sid18).

    features in discriminative training:
examples: rule (1), (2) and (3)

    rule identi   ers for sid18 productions
    rule id165 features in source and target
examples:    x hat   ,    hat x   ,    x versprochen   
    rule shape features

examples: (nt, term   , nt, term   ; nt, term   , nt) for (1), (2);
(nt, term   , nt; nt, term   , nt) for rule (3).

16 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

smt using synchronous context-free grammars

(1) x     x1 hat x2 versprochen; x1 promised x2
(2) x     x1 hat mir x2 versprochen;
(3) x     x1 versprach x2; x1 promised x2

x1 promised me x2

    hierarchical phrase-based translation (chiang cl   07),

formalizes translation rules as productions of synchronous
context-free grammar (sid18).

    features in discriminative training:
examples: rule (1), (2) and (3)

    rule identi   ers for sid18 productions
    rule id165 features in source and target
examples:    x hat   ,    hat x   ,    x versprochen   
    rule shape features

examples: (nt, term   , nt, term   ; nt, term   , nt) for (1), (2);
(nt, term   , nt; nt, term   , nt) for rule (3).

16 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

experiment i: random sharding on large parallel data

    idea: take advantage of inherent ef   ciency (and

effectiveness) of id72.

    de   ne tasks as random shards of data,
    either by sharding once or by re-sharding after each epoch.

    advantage:
sharding.

    hadoop/mapreduce framework offers parallelization by data
    feature selection by (cid:96)1/(cid:96)2 block norm id173 on shards

iteratively cuts feature space to feasible size.

    see simianer, riezler, dyer acl   12.

17 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

experiment i: random sharding on large parallel data

    idea: take advantage of inherent ef   ciency (and

effectiveness) of id72.

    de   ne tasks as random shards of data,
    either by sharding once or by re-sharding after each epoch.

    advantage:
sharding.

    hadoop/mapreduce framework offers parallelization by data
    feature selection by (cid:96)1/(cid:96)2 block norm id173 on shards

iteratively cuts feature space to feasible size.

    see simianer, riezler, dyer acl   12.

17 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

experiment i: random sharding on large parallel data

    idea: take advantage of inherent ef   ciency (and

effectiveness) of id72.

    de   ne tasks as random shards of data,
    either by sharding once or by re-sharding after each epoch.

    advantage:
sharding.

    hadoop/mapreduce framework offers parallelization by data
    feature selection by (cid:96)1/(cid:96)2 block norm id173 on shards

iteratively cuts feature space to feasible size.

    see simianer, riezler, dyer acl   12.

17 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

data

18 / 33

newscommentary(nc)train-nclm-train-ncdev-ncdevtest-nctest-ncsentences132,753180,657105710642007tokensde3,530,907   27,78228,41553,989tokensen3,293,3634,394,42826,09826,21950,443rulecount14,350,552(1g)   2,322,9122,320,2643,274,771europarl(ep)train-eplm-train-epdev-epdevtest-eptest-epsentences1,655,2382,015,440200020002000tokensde45,293,925   57,72356,78359,297tokensen45,374,64954,728,78658,82558,10060,240rulecount203,552,525(31.5g)   17,738,76317,682,17618,273,078newscrawl(crawl)dev-crawltest-crawl10test-crawl11sentences205124893003tokensde49,84864,30176,193tokensen49,76761,92574,753rulecount9,404,33911,307,30412,561,636introduction

algorithms

random sharding

natural tasks

conclusion

smt setup

    cdec (dyer et al. acl   10) framework for decoding and

induction of sid18s.

    sid18 per-sentence grammars are stored on disk instead of
in memory (lopez emnlp   07), extracted by leave-one-out
(zollmann and sima   an jacl   05) for training-set tuning.

    scale:

corresponding to dev set size.

    data are split into shards holding about 1,000 sentences,
    on hadoop/mapreduce cluster for 300 parallel jobs this
    5m active features without feature selection on ep data set.

required 2,290 shards for ep data set.

19 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

smt setup

    cdec (dyer et al. acl   10) framework for decoding and

induction of sid18s.

    sid18 per-sentence grammars are stored on disk instead of
in memory (lopez emnlp   07), extracted by leave-one-out
(zollmann and sima   an jacl   05) for training-set tuning.

    scale:

corresponding to dev set size.

    data are split into shards holding about 1,000 sentences,
    on hadoop/mapreduce cluster for 300 parallel jobs this
    5m active features without feature selection on ep data set.

required 2,290 shards for ep data set.

19 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

smt setup

    cdec (dyer et al. acl   10) framework for decoding and

induction of sid18s.

    sid18 per-sentence grammars are stored on disk instead of
in memory (lopez emnlp   07), extracted by leave-one-out
(zollmann and sima   an jacl   05) for training-set tuning.

    scale:

corresponding to dev set size.

    data are split into shards holding about 1,000 sentences,
    on hadoop/mapreduce cluster for 300 parallel jobs this
    5m active features without feature selection on ep data set.

required 2,290 shards for ep data set.

19 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

results on news commentary (nc) data

algorithm

tuning set

features

#features

test-nc

single-task sgd

dev-nc
dev-nc

default

+id,ng,shape

multi-task sgd

train-nc

+id,ng,shape

12
180k

100k

28.0
28.15

28.81

    scaling from 12 to 180k features on dev set does not help.
    scaling to full feature- and training-set does help (+0.8 id7).
    statistical signi   cance assessed by approximate

randomization (noreen   89).

20 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

results on europarl (ep) and news crawl (crawl) data

algorithm

tuning set

features

#features

test-ep

single-task sgd

multi-task sgd

dev-ep
dev-ep
train-ep

default

+id,ng,shape
+id,ng,shape

12
300k
100k

26.42
28.37
28.62

alg.

tuning set

features

#feats

test-crawl10

test-crawl11

st

mt

dev-crawl
dev-crawl
train-ep

default

+id,ng,shape
+id,ng,shape

12
300k
100k

15.39
17.8
19.12

14.43
16.83
17.33

    scaling up feature sets helps even for dev-set tuning.
    on large scale tuning set only multi-task sgd is feasible.
    additional gains of 0.5 to 1.3 id7 by scaling to large tuning

set on out-of-domain news crawl test data.

21 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

experiments ii: random vs. natural tasks

    research question:

id173 technique on random shards.

    as shown, id72 can be used as general
    can id72 bene   t from natural task structure in
the data, where shared and individual knowledge is properly
balanced?

    see simianer & riezler wmt   13.

22 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

experiments ii: random vs. natural tasks

    research question:

id173 technique on random shards.

    as shown, id72 can be used as general
    can id72 bene   t from natural task structure in
the data, where shared and individual knowledge is properly
balanced?

    see simianer & riezler wmt   13.

22 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

experiments ii: random vs. natural tasks

    research question:

id173 technique on random shards.

    as shown, id72 can be used as general
    can id72 bene   t from natural task structure in
the data, where shared and individual knowledge is properly
balanced?

    see simianer & riezler wmt   13.

22 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

data

a
b
c
d
e
f

g
h

human necessities
performing operations, transporting
chemistry, metallurgy
textiles, paper
fixed constructions
mechanical engineering, lighting,
heating, weapons
physics
electricity

    international patent classi   cation (ipc) categorizes patents

hierarchically into eight sections, 120 classes, 600
subclasses, down to 70,000 subgroups at the leaf level.

    typically, a patent belongs to more than one section, with one

section chosen as main classi   cation.

    eight top classes/sections used to de   ne natural tasks.

23 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

smt and learning setup

    sid18 framework using sparse local features (as above).
    learning algorithms:

    baselines:

    mert (kumar et al. acl   09)
    single-task id88 w/ and w/o (cid:96)1 id173 with clipping
    single-task margin id88 (collobert & bengio icml   04).

(carpenter 2008)

    multi-task tuning using standard and margin id88.
    tuning methods with random components (mert, random

(re)sharding) were repeated 3 times and id7 scores
averaged.

24 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

smt and learning setup

    sid18 framework using sparse local features (as above).
    learning algorithms:

    baselines:

    mert (kumar et al. acl   09)
    single-task id88 w/ and w/o (cid:96)1 id173 with clipping
    single-task margin id88 (collobert & bengio icml   04).

(carpenter 2008)

    multi-task tuning using standard and margin id88.
    tuning methods with random components (mert, random

(re)sharding) were repeated 3 times and id7 scores
averaged.

24 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

train/dev/test splits

    1.2m parallel sentences from patent domain for training1.
    development and test sets of 2,000 sentences from each of

sections a to h for independent tuning and testing.
    pooled development and test sets containing 2,000

sentences with all sections evenly represented.

    pooled-cat development set for tuning on concatenation of

data from all sections.

1http://www.cl.uni-heidelberg.de/statnlpgroup/pattr

25 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

train/dev/test splits

    1.2m parallel sentences from patent domain for training1.
    development and test sets of 2,000 sentences from each of

sections a to h for independent tuning and testing.
    pooled development and test sets containing 2,000

sentences with all sections evenly represented.

    pooled-cat development set for tuning on concatenation of

data from all sections.

1http://www.cl.uni-heidelberg.de/statnlpgroup/pattr

25 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

mert baseline w/ 12 dense features

    neither tuning on pooled or pooled-cat improves over indep..
    x   {0,1,2}id7 denotes statistical signi   cance of pairwise test.

26 / 33

single-tasktuningindep.0pooled1pooled-cat2pooledtest   51.1851.22a54.920255.27055.17b51.5351.480151.69c1256.31255.9055.74d49.94050.33050.26e149.1948.97149.13f1251.2651.0251.12g149.6149.4449.55h49.3849.500149.67averagetest51.5251.4951.54introduction

algorithms

random sharding

natural tasks

conclusion

single-task id88 w/ (cid:96)1 id173

    improvements over mert, mostly on pooled-cat tuning set.
    1.5m features make serial tuning on pooled-cat infeasible.
    over   tting effect on small pooled data.

27 / 33

single-tasktuningindep.0pooled1pooled-cat2pooledtest   50.75152.08a155.1154.320155.94b152.6150.84152.57c56.1856.110156.75d150.6849.480151.22e150.2748.69150.01f151.6850.71151.95g149.9049.060150.51h150.4849.16150.53averagetest52.1151.0552.44modelsize430,092.5457,4281,574,259introduction

algorithms

random sharding

natural tasks

conclusion

single- and multi-task id88

    multi-task tuning improves id7 over all single-task runs.
    also more ef   cient due to iterative feature selection.
    difference between natural and random tasks inconclusive.

28 / 33

single-tasktuningmulti-tasktuningindep.0pooled1pooled-cat2ipc3sharding4resharding5pooledtest   51.33151.771252.561252.541252.60a54.7954.760155.3101256.3501256.2201256.21b1252.4551.30152.1901252.78012352.9801252.96c256.6256.65156.120124557.7601257.3001257.44d150.7549.88150.630124551.5401251.3301251.20e149.7049.230149.9201250.5101250.5201250.38f151.6051.09151.7101252.2801252.4301252.32g149.5049.060149.9701250.8401250.8801250.74h149.7749.500150.6401251.1601251.0701251.10averagetest51.9051.4252.0652.9052.8452.79modelsize366,869.4448,3591,478,049100,000100,000100,000introduction

algorithms

random sharding

natural tasks

conclusion

single- and multi-task margin id88

    single-task runs beat standard id88 w/ and w/o (cid:96)1.
    id173 by margin and id72 adds up.
    best result is nearly 2 id7 points better than mert.

29 / 33

single-tasktuningmulti-tasktuningindep.0pooled1pooled-cat2ipc3sharding4resharding5pooledtest   51.33152.581252.981252.951252.99a156.0955.33155.920124556.7801256.6201256.53b152.4551.59152.4401253.3101253.3501253.21c157.2056.850157.540157.46157.42157.43d150.5150.180151.380124552.14012551.8201251.66e150.2749.360150.72012451.1301250.8901251.02f152.0651.200152.610124553.0701252.8001252.87g150.0049.580150.900124551.3601251.1901251.11h150.5749.800151.3201251.5701251.620151.47averagetest52.3951.7452.8553.3553.2153.16modelsize423,731.5484,4831,697,398100,000100,000100,000introduction

algorithms

random sharding

natural tasks

conclusion

conclusion

    id72 for smt is ef   cient due to online learning,

parallelization and feature selection,

    but also effective in terms of id7 improvements over

single-task learning.

id173.

    id72 is adaptive due to path-following in
    question: can task de   nition be adapted to problem as

well?

    natural task de   nition show nominal (not statistically
    future work: optimize id91 of ipc subclasses for

signi   cant) advantage.

id72 in smt.

30 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

conclusion

    id72 for smt is ef   cient due to online learning,

parallelization and feature selection,

    but also effective in terms of id7 improvements over

single-task learning.

id173.

    id72 is adaptive due to path-following in
    question: can task de   nition be adapted to problem as

well?

    natural task de   nition show nominal (not statistically
    future work: optimize id91 of ipc subclasses for

signi   cant) advantage.

id72 in smt.

30 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

conclusion

    id72 for smt is ef   cient due to online learning,

parallelization and feature selection,

    but also effective in terms of id7 improvements over

single-task learning.

id173.

    id72 is adaptive due to path-following in
    question: can task de   nition be adapted to problem as

well?

    natural task de   nition show nominal (not statistically
    future work: optimize id91 of ipc subclasses for

signi   cant) advantage.

id72 in smt.

30 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

conclusion

    id72 for smt is ef   cient due to online learning,

parallelization and feature selection,

    but also effective in terms of id7 improvements over

single-task learning.

id173.

    id72 is adaptive due to path-following in
    question: can task de   nition be adapted to problem as

well?

    natural task de   nition show nominal (not statistically
    future work: optimize id91 of ipc subclasses for

signi   cant) advantage.

id72 in smt.

30 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

conclusion

    id72 for smt is ef   cient due to online learning,

parallelization and feature selection,

    but also effective in terms of id7 improvements over

single-task learning.

id173.

    id72 is adaptive due to path-following in
    question: can task de   nition be adapted to problem as

well?

    natural task de   nition show nominal (not statistically
    future work: optimize id91 of ipc subclasses for

signi   cant) advantage.

id72 in smt.

30 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ipc

ipc: 8 sections, 120 classes, 600 subclasses, 70,000 subgroups:

is there a natural or useful task de   nition for multi-task smt?

31 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

code

    dtrain code is part of cdec:

https://github.com/redpony/cdec.

32 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

thanks for your attention!

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

phil blunsom, trevor cohn, and miles osborne.
a discriminative latent variable model for statistical machine
translation.
in proceedings of the 46th annual meeting of the association
for computational linguistics: human language technologies
(acl-hlt   08), columbus, oh, 2008.
leon bottou and olivier bousquet.
large scale online learning.
in proceedings of the 18th annual conference on neural
information processing system (nips   04), vancouver,
canada, 2004.
leon bottou and olivier bousquet.
the tradeoffs of large scale learning.
in proceedings of the 21st annual conference on neural
information processing system (nips   07), vancouver,
canada, 2007.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

leo breiman.
id112 predictors.
machine learning, 24:123   140, 1996.
bob carpenter.
lazy sparse stochastic id119 for regularized
multinomial id28.
technical report, alias-i, 2008.
david chiang.
hierarchical phrase-based translation.
computational linguistics, 33(2), 2007.
ronan collobert and samy bengio.
links between id88s, mlps, and id166s.
in proceedings of the 21st international conference on
machine learning (icml   04), banff, canada, 2004.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

chris dyer, adam lopez, juri ganitkevitch, jonathan weese,
ferhan ture, phil blunsom, hendra setiawan, vladimir
eidelman, and philip resnik.
cdec: a decoder, alignment, and learning framework for
   nite-state and context-free translation models.
in proceedings of the acl 2010 system demonstrations,
uppsala, sweden, 2010.
jeffrey flanigan, chris dyer, and jaime carbonell.
large-scale discriminative training for statistical machine
translation using held-out line search.
in proceedings of the 2013 conference of the north american
chapter of the association for computational linguistics:
human language technologies (naacl-hlt   13), atlanta, ga,
2013.
shankar kumar, wolfgang macherey, chris dyer, and franz
och.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

ef   cient minimum error rate training and minimum bayes-risk
decoding for translation hypergraphs and lattices.
in proceedings of the 47th annual meeting of the association
for computational linguistics and the 4th ijcnlp of the
afnlp (acl-ijcnlp   09, suntec, singapore, 2009.
john langford, lihong li, and tong zhang.
sparse online learning via truncated gradient.
journal of machine learning research, 10:777   801, 2009.
percy liang, alexandre bouchard-c  ot  e, dan klein, and ben
taskar.
an end-to-end discriminative approach to machine translation.
in proceedings of the joint conference of the international
committee on computational linguistics and the association
for computational linguistics (coling-acl   06), sydney,
australia, 2006.
adam lopez.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

hierarchical phrase-based translation with suf   x arrays.
in proceedings of the joint conference on empirical methods
in natural language processing and computational natural
language learning (emnlp-conll 2007), prague, czech
republic, 2007.
andr  e f.t. martins, noah a. smith, pedro m.q.aguiar, and
m  ario a.t. figueiredo.
structured sparsity in id170.
in proceedings of the 2011 conference on empirical methods
in natural language processing, edinburgh, scotland, 2011.
ryan mcdonald, keith hall, and gideon mann.
distributed training strategies for the structured id88.
in proceedings of human language technologies: the 11th
annual conference of the north american chapter of the
association for computational linguistics (naacl-hlt   10),
los angeles, ca, 2010.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

preslav nakov, francisco guzm  an, and stephan vogel.
optimizing for sentence-level id7+1 yields short translations.
in proceedings of the 24th international conference on
computational linguistics (coling 2012), bombay, india,
2012.
eric w. noreen.
computer intensive methods for testing hypotheses. an
introduction.
wiley, new york, 1989.
guillaume obozinski, ben taskar, and michael i. jordan.
joint covariate selection and joint subspace selection for
multiple classi   cation problems.
statistics and computing, 20:231   252, 2010.
franz josef och.
minimum error rate training in id151.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

in proceedings of the human language technology
conference and the 3rd meeting of the north american
chapter of the association for computational linguistics
(hlt-naacl   03), edmonton, cananda, 2003.
shai shalev-shwartz, yoram singer, and nathan srebro.
pegasos: primal estimated sub-gradient solver for id166.
in proceedings of the 24th international conference on
machine learning (icml   07), corvallis, or, 2007.
libin shen and aravind k. joshi.
ranking and reranking with id88.
journal of machine learning research, 60(1-3):73   96, 2005.
patrick simianer and stefan riezler.
id72 for improved discriminative training in smt.
in proceedings of the acl 2013 eighth workshop on
id151 (wmt   13), so   a, bulgaria,
2013.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

patrick simianer, stefan riezler, and chris dyer.
joint feature selection in distributed stochastic learning for
large-scale discriminative training in smt.
in proceedings of the 50th annual meeting of the association
for computational linguistics (acl 2012), jeju, korea, 2012.
christoph tillmann and tong zhang.
a discriminative global training algorithm for statistical mt.
in proceedings of the joint conference of the international
committee on computational linguistics and the association
for computational linguistics (coling-acl   06), sydney,
australia, 2006.
ming yuan and yi lin.
model selection and estimation in regression with grouped
variables.
j.r.statist.soc.b, 68(1):49   67, 2006.
peng zhao, guilherme rocha, and bin yu.

33 / 33

introduction

algorithms

random sharding

natural tasks

conclusion

the composite absolute penalties family for grouped and
hierarchical variable selection.
the annals of statistics, 37(6a):3468   3497, 2009.
martin a. zinkevich, markus weimer, alex smola, and lihong
li.
parallelized stochastic id119.
in proceedings of the 24th annual conference on neural
information processing sytems (nips   10), vancouver,
canada, 2010.
andreas zollmann and khalil sima   an.
a consistent and ef   cient estimator for data-oriented parsing.
journal of automata, languages and combinatorics,
10(2/3):367   388, 2005.

33 / 33

