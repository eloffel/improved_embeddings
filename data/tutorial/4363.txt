learning structured predictors

xavier carreras

talp research center

universitat polit`ecnica de catalunya

supervised (structured) prediction

(cid:73) learning to predict: given training data

(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y that works well on unseen inputs x

(cid:73) non-id170: outputs y are atomic

(cid:73) binary prediction: y     {   1, +1}
(cid:73) multiclass prediction: y     {1, 2, . . . , l}

(cid:73) id170: outputs y are structured

(cid:73) sequence prediction: y are sequences
(cid:73) parsing: y are trees
(cid:73) . . .

id39

y per
x

jim bought

-

qnt
300

-

shares

org

org
-
of acme corp.

-
in

time
2006

id39

y per
x

jim bought

-

qnt
300

-

shares

org

org
-
of acme corp.

-
in

time
2006

y per
x jack

per

-

london went

loc
-
to paris

per

y
x paris hilton went

-

per

-
to

loc

london

part-of-speech tagging

y nnp nnp vbz nnp .
x ms.
.

elianti

haag

plays

syntactic parsing

root

vc

p

loc

obj

nmod

sbj

tmp

nmod

pmod

name

(cid:63) unesco

is

now holding

its

biennial meetings

in new york

.

x are sentences

y are syntactic dependency trees

machine translation

(galley et al 2006)

x are sentences in chinese

y are sentences in english aligned to x

rulesmaydescribethetransformationofdoesnotintone...pasinfrench.aparticularinstancemaylooklikethis:vp(aux(does),rb(not),x0:vb)   ne,x0,paslhs(ri)canbeanyarbitrarysyntaxtreefragment.itsleavesareeitherlexicalized(e.g.does)orvari-ables(x0,x1,etc).rhs(ri)isrepresentedasase-quenceoftarget-languagewordsandvariables.nowwegiveabriefoverviewofhowsuchtransformationalrulesareacquiredautomaticallyinghkm.1infigure1,the(  ,f,a)tripleisrep-resentedasadirectedgraphg(edgesgoingdown-ward),withnodistinctionbetweenedgesof  andalignments.eachnodeofthegraphislabeledwithitsspanandcomplementspan(thelatterinitalicinthe   gure).thespanofanodenisde   nedbytheindicesofthe   rstandlastwordinfthatarereachablefromn.thecomplementspanofnistheunionofthespansofallnodesn   ingthatareneitherdescendantsnorancestorsofn.nodesofgwhosespansandcomplementspansarenon-overlappingformthefrontiersetf   g.whatisparticularlyinterestingaboutthefron-tierset?foranyfrontierofgraphgcontainingagivennoden   f,spansonthatfrontierde-   neanorderingbetweennandeachotherfrontiernoden   .forexample,thespanofvp[4-5]eitherprecedesorfollows,butneveroverlapsthespanofanynoden   onanygraphfrontier.thispropertydoesnotholdfornodesoutsideoff.forinstance,pp[4-5]andvbg[4]aretwonodesofthesamegraphfrontier,buttheycannotbeorderedbecauseoftheiroverlappingspans.thepurposeofxrsrulesinthisframeworkistoorderconstituentsalongsensiblefrontiersing,andallfrontierscontainingunde   nedorderings,asbetweenpp[4-5]andvbg[4],mustbedisre-gardedduringruleextraction.toensurethatxrsrulesarepreventedfromattemptingtore-orderanysuchpairofconstituents,theserulesarede-signedinsuchawaythatvariablesintheirlhscanonlymatchnodesofthefrontierset.rulesthatsatisfythispropertyaresaidtobeinducedbyg.2forexample,rule(d)intable1isvalidaccord-ingtoghkm,sincethespanscorrespondingto1notethatweuseaslightlydifferentterminology.2speci   cally,anxrsruleriisextractedfromgbytakingasubtree       aslhs(ri),appendingavariabletoeachleafnodeof  thatisinternalto  ,addingthosevariablestorhs(ri),orderingtheminaccordancetoa,andifnecessaryinsertinganywordofftoensurethatrhs(ri)isasequenceofcontiguousspans(e.g.,[4-5][6][7-8]forrule(f)intable1).!"#!$%&''()'''&'&''($%*!""#$%&''()#"!'(*$&)!!"#$%"&"!"&"!"&##"&$%&!"'$&'!"&'!"&(!"%$("&)!")#%"*"&'&$%&!"'$&'&(!"%$+("&&&'%(!"%$("&$&'%(!"*$("&'&'%&!"*$&$&!%&!"#$&(#%)!!!"#$%&'()*+,""#$%$&$'&($)*+(,-$.%/0'*.,/%+'1)*230'140.*+$++5-figure1:spansandcomplement-spansdeterminewhatrulesareextracted.constituentsingrayaremembersofthefrontierset;aminimalruleisextractedfromeachofthem.(a)s(x0:np,x1:vp,x2:.)   x0,x1,x2(b)np(x0:dt,cd(7),nns(people))   x0,7(c)dt(these)   (d)vp(x0:vbp,x1:np)   x0,x1(e)vbp(include)   (f)np(x0:np,x1:vp)   x1,,x0(g)np(x0:nns)   x0(h)nns(astronauts)   ,(i)vp(vbg(coming),pp(in(from),x0:np))   ,x0(j)np(x0:nnp)   x0(k)nnp(france)   (l).(.)   .table1:aminimalderivationcorrespondingtofigure1.itsrhsconstituents(vbp[3]andnp[4-8])donotoverlap.conversely,np(x0:dt,x1:cd:,x2:nns)isnotthelhsofanyruleextractiblefromg,sinceitsfrontierconstituentscd[2]andnns[2]haveoverlappingspans.3finally,theghkmproce-dureproducesasinglederivationfromg,whichisshownintable1.theconcerninghkmwastoextractminimalrules,whereasoursistoextractrulesofanyarbi-trarysize.minimalrulesde   nedovergarethosethatcannotbedecomposedintosimplerrulesin-ducedbythesamegraphg,e.g.,allrulesinta-ble1.wecallminimaladerivationthatonlycon-tainsminimalrules.conversely,acomposedruleresultsfromthecompositionoftwoormoremin-imalrules,e.g.,rule(b)and(c)composeinto:np(dt(these),cd(7),nns(people))   ,73itisgenerallyreasonabletoalsorequirethattherootnoflhs(ri)bepartoff,becausenoruleinducedbygcancomposewithriatn,duetotherestrictionsimposedontheextractionprocedure,andriwouldn   tbepartofanyvalidderivation.id164

(kumar and hebert 2003)

x are images

y are grids labeled with object types

id164

(kumar and hebert 2003)

x are images

y are grids labeled with object types

today   s goals

(cid:73) introduce basic tools for structure prediction

(cid:73) we will restrict to sequence prediction

(cid:73) understand what tools we can use from standard classi   cation
(cid:73) learning paradigms and algorithms, in essence, work here too
(cid:73) however, computations behind algorithms are prohibitive

(cid:73) understand what tools we can use from existing formalisms for

structured data

(cid:73) we will use id136 algorithms for tractable computations
(cid:73) e.g., algorithms for id48s (viterbi, forward-backward) will play a

major role in today   s methods

today   s goals

(cid:73) introduce basic tools for structure prediction

(cid:73) we will restrict to sequence prediction

(cid:73) understand what tools we can use from standard classi   cation
(cid:73) learning paradigms and algorithms, in essence, work here too
(cid:73) however, computations behind algorithms are prohibitive

(cid:73) understand what tools we can use from existing formalisms for

structured data

(cid:73) we will use id136 algorithms for tractable computations
(cid:73) e.g., algorithms for id48s (viterbi, forward-backward) will play a

major role in today   s methods

sequence prediction

y
x

per
jack

per

-

london went

loc
-
to paris

sequence prediction

(cid:73) x = x1x2 . . . xn are input sequences, xi     x
(cid:73) y = y1y2 . . . yn are output sequences, yi     {1, . . . , l}

(cid:73) goal: given training data

(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y that works well on unseen inputs x

(cid:73) what is the form of our prediction model?

approach 1: local classi   ers

?

jack

london went

to paris

decompose the sequence into n classi   cation problems:

(cid:73) a classi   er predicts individual labels at each position

  yi =

argmax

l     {loc, per, -}

w    f (x, i, l)

(cid:73) f (x, i, l) represents an assignment of label l for xi
(cid:73) w is a vector of parameters, has a weight for each feature of f

(cid:73) use standard classi   cation methods to learn w

(cid:73) at test time, predict the best sequence by

a simple concatenation of the best label for each position

approach 1: local classi   ers

?

jack

london went

to paris

decompose the sequence into n classi   cation problems:

(cid:73) a classi   er predicts individual labels at each position

  yi =

argmax

l     {loc, per, -}

w    f (x, i, l)

(cid:73) f (x, i, l) represents an assignment of label l for xi
(cid:73) w is a vector of parameters, has a weight for each feature of f

(cid:73) use standard classi   cation methods to learn w

(cid:73) at test time, predict the best sequence by

a simple concatenation of the best label for each position

indicator features

(cid:73) f (x, i, l) is a vector of d features representing label l for xi

( f1(x, i, l), . . . , fj(x, i, l), . . . , fd(x, i, l) )

(cid:73) what   s in a feature fj(x, i, l)?

(cid:73) anything we can compute using x and i and l
(cid:73) anything that indicates whether l is (not) a good label for xi
(cid:73) indicator features: binary-valued features looking at a single simple

property

fj(x, i, l) =(cid:26) 1
fk(x, i, l) =(cid:26) 1

0

0

if xi =london and l =loc
otherwise
if xi+1 =went and l =loc
otherwise

more features for ne recognition

per

jack

london went

to paris

in practice, construct f (x, i, l) by . . .

(cid:73) de   ne a number of simple patterns of x and i

(cid:73) current word xi
(cid:73) is xi capitalized?
(cid:73) xi has digits?
(cid:73) pre   xes/su   xes of size 1, 2, 3, . . .
(cid:73) is xi a known location?
(cid:73) is xi a known person?

(cid:73) next word
(cid:73) previous word
(cid:73) current and next words

together

(cid:73) other combinations

(cid:73) generate features by combining patterns with label identities l

more features for ne recognition

per
jack

per

-

london went

to paris

in practice, construct f (x, i, l) by . . .

(cid:73) de   ne a number of simple patterns of x and i

(cid:73) current word xi
(cid:73) is xi capitalized?
(cid:73) xi has digits?
(cid:73) pre   xes/su   xes of size 1, 2, 3, . . .
(cid:73) is xi a known location?
(cid:73) is xi a known person?

(cid:73) next word
(cid:73) previous word
(cid:73) current and next words

together

(cid:73) other combinations

(cid:73) generate features by combining patterns with label identities l
main limitation: features can   t capture interactions between labels!

approach 2: id48 for sequence prediction

  per

per

tper,per

per

-

oper, london

jack

london

went

-

to

loc

paris

(cid:73) de   ne an id48 were each label is a state
(cid:73) model parameters:

(cid:73)   l : id203 of starting with label l
(cid:73) tl,l(cid:48): id203 of transitioning from l to l(cid:48)
(cid:73) ol,x: id203 of generating symbol x given label l

(cid:73) predictions:

p(x, y) =   y1oy1,x1(cid:89)i>1

tyi   1,yioyi,xi

(cid:73) learning: relative counts + smoothing
(cid:73) prediction: viterbi algorithm

approach 2: representation in id48

  per

per

tper,per

per

-

oper, london

jack

london

went

-

to

loc

paris

(cid:73) label interactions are captured in the transition parameters
(cid:73) but interactions between labels and input symbols are quite

limited!

(cid:73) only oyi,xi = p(xi | yi)
(cid:73) not clear how to exploit patterns such as:
(cid:73) capitalization, digits
(cid:73) pre   xes and su   xes
(cid:73) next word, previous word
(cid:73) combinations of these with label transitions

(cid:73) why? id48 independence assumptions:

given label yi, token xi is independent of anything else

approach 2: representation in id48

  per

per

tper,per

per

-

oper, london

jack

london

went

-

to

loc

paris

(cid:73) label interactions are captured in the transition parameters
(cid:73) but interactions between labels and input symbols are quite

limited!

(cid:73) only oyi,xi = p(xi | yi)
(cid:73) not clear how to exploit patterns such as:
(cid:73) capitalization, digits
(cid:73) pre   xes and su   xes
(cid:73) next word, previous word
(cid:73) combinations of these with label transitions

(cid:73) why? id48 independence assumptions:

given label yi, token xi is independent of anything else

local classi   ers vs. id48

local classifiers

id48

(cid:73) form:

(cid:73) form:

w    f (x, i, l)

(cid:73) learning: standard classi   ers
(cid:73) prediction: independent for

each xi

(cid:73) advantage: feature-rich
(cid:73) drawback: no label

interactions

  y1oy1,x1(cid:89)i>1

tyi   1,yioyi,xi

(cid:73) learning: relative counts
(cid:73) prediction: viterbi
(cid:73) advantage: label interactions
(cid:73) drawback: no    ne-grained

features

approach 3: global sequence predictors

y:
x:

per
jack

per

-

london went

loc
-
to paris

learn a single classi   er from x     y

predict(x1:n) = argmax
y   y n

w    f (x, y)

next questions: . . .

(cid:73) how do we represent entire sequences in f (x, y)?
(cid:73) there are exponentially-many sequences y for a given x,

how do we solve the argmax problem?

approach 3: global sequence predictors

y:
x:

per
jack

per

-

london went

loc
-
to paris

learn a single classi   er from x     y

predict(x1:n) = argmax
y   y n

w    f (x, y)

next questions: . . .

(cid:73) how do we represent entire sequences in f (x, y)?
(cid:73) there are exponentially-many sequences y for a given x,

how do we solve the argmax problem?

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

bigram indicator features

1
y per
x jack

2

per

3
-

london went

5

4
loc
-
to paris

(cid:73) indicator features:

fj(x, i, yi   1, yi) =         

1 if xi =   london    and

yi   1 = per and yi = per

0 otherwise

e.g., fj(x, 2, per, per) = 1,

fj(x, 3, per, -) = 0

more bigram indicator features

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f1(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f2(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f3(. . .) = 1 i    xi   1    /(in|to|at)/ and xi    /[a-z]/ and yi = loc
f4(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f5(. . .) = 1 i    yi = per and first-names(xi) = 1

more bigram indicator features

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f1(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f2(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f3(. . .) = 1 i    xi   1    /(in|to|at)/ and xi    /[a-z]/ and yi = loc
f4(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f5(. . .) = 1 i    yi = per and first-names(xi) = 1

more bigram indicator features

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f1(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f2(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f3(. . .) = 1 i    xi   1    /(in|to|at)/ and xi    /[a-z]/ and yi = loc
f4(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f5(. . .) = 1 i    yi = per and first-names(xi) = 1

more bigram indicator features

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f1(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f2(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f3(. . .) = 1 i    xi   1    /(in|to|at)/ and xi    /[a-z]/ and yi = loc
f4(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f5(. . .) = 1 i    yi = per and first-names(xi) = 1

more bigram indicator features

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f1(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f2(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f3(. . .) = 1 i    xi   1    /(in|to|at)/ and xi    /[a-z]/ and yi = loc
f4(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f5(. . .) = 1 i    yi = per and first-names(xi) = 1

representations factored at bigrams

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) f (x, i, yi   1, yi)

(cid:73) a d-dimensional feature vector of a label bigram at i
(cid:73) each dimension is typically a boolean indicator (0 or 1)

(cid:73) f (x, y) =(cid:80)n

i=1 f (x, i, yi   1, yi)

(cid:73) a d-dimensional feature vector of the entire y
(cid:73) aggregated representation by summing bigram feature vectors
(cid:73) each dimension is now a count of a feature pattern

linear sequence prediction

where

f (x, y) =

predict(x1:n) = argmax
y   y n

w    f (x, y)

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) note the linearity of the expression:

w    f (x, y) = w   

=

n(cid:88)i=1

n(cid:88)i=1

f (x, i, yi   1, yi)

w    f (x, i, yi   1, yi)

(cid:73) next questions:

(cid:73) how do we solve the argmax problem?
(cid:73) how do we learn w?

linear sequence prediction

where

f (x, y) =

predict(x1:n) = argmax
y   y n

w    f (x, y)

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) note the linearity of the expression:

w    f (x, y) = w   

=

n(cid:88)i=1

n(cid:88)i=1

f (x, i, yi   1, yi)

w    f (x, i, yi   1, yi)

(cid:73) next questions:

(cid:73) how do we solve the argmax problem?
(cid:73) how do we learn w?

linear sequence prediction

where

f (x, y) =

predict(x1:n) = argmax
y   y n

w    f (x, y)

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) note the linearity of the expression:

w    f (x, y) = w   

=

n(cid:88)i=1

n(cid:88)i=1

f (x, i, yi   1, yi)

w    f (x, i, yi   1, yi)

(cid:73) next questions:

(cid:73) how do we solve the argmax problem?
(cid:73) how do we learn w?

predicting with factored sequence models

(cid:73) consider a    xed w. given x1:n    nd:

argmax
y   y n

n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) we can use the viterbi algorithm, takes o(n|y|2)

(cid:73) intuition: output sequences that share bigrams will share scores

. . .

i     2

i     1

best subsequence with yi   1 = per

best subsequence with yi   1 = loc

c ,

o

l

,

i

( x ,

w   f

i

i + 1 . . .

r

)

e

p

best subsequence with yi = per

best subsequence with yi = loc

best subsequence with yi   1 =    

best subsequence with yi =    

intuition for viterbi

(cid:73) consider a    xed x1:n
(cid:73) assume we have the best sub-sequences up to position i     1

i     1
1
best subsequence with yi   1 = per

. . .

i

best subsequence with yi   1 = loc

best subsequence with yi   1 =    

(cid:73) what is the best sequence up to position i with yi =loc?

intuition for viterbi

(cid:73) consider a    xed x1:n
(cid:73) assume we have the best sub-sequences up to position i     1

i     1
1
best subsequence with yi   1 = per

. . .

best subsequence with yi   1 = loc

best subsequence with yi   1 =    

i

w  f(x,i,per,loc

)

w  f (x,i,loc, loc)

l o c

)

,

   

,

i

( x ,

w   f

(cid:73) what is the best sequence up to position i with yi =loc?

viterbi for linear factored predictors

  y = argmax

y   y n

n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) de   nition: score of optimal sequence for x1:i ending with a     y

  i(a) = max

y   y i:yi=a

i(cid:88)j=1

w    f (x, j, yj   1, yj)

(cid:73) use the following recursions, for all a     y:

  1(a) = w    f (x, 1, y0 = null, a)
  i(a) = max
b   y

  i   1(b) + w    f (x, i, b, a)

(cid:73) the optimal score for x is maxa   y   n(a)
(cid:73) the optimal sequence   y can be recovered through pointers

linear factored sequence prediction

predict(x1:n) = argmax
y   y n

w    f (x, y)

(cid:73) factored representation, e.g. based on bigrams
(cid:73) flexible, arbitrary features of full x and the factors
(cid:73) e   cient prediction using viterbi
(cid:73) next, learning w:

(cid:73) maximum-id178 markov models (local)
(cid:73) id49 (global)
(cid:73) structured id88 (global)
(cid:73) structured id166 (global)

id148

for sequence prediction

y
x

per
jack

per

-

london went

loc
-
to paris

id148 for sequence prediction

(cid:73) model the conditional distribution:

pr(y | x; w) =

exp{w    f (x, y)}

z(x; w)

where

   
(cid:73) x = x1x2 . . . xn     x
    and y = {1, . . . , l}
(cid:73) y = y1y2 . . . yn     y
(cid:73) f (x, y) represents x and y with d features
(cid:73) w     rd are the parameters of the model
(cid:73) z(x; w) is a normalizer called the partition function

z(x; w) = (cid:88)z   y   

exp{w    f (x, z)}

(cid:73) to predict the best sequence

predict(x1:n) = argmax
y   y n

pr(y|x)

id148: name

(cid:73) let   s take the log of the id155:

log pr(y | x; w) = log

exp{w    f (x, y)}

z(x; w)

= w    f (x, y)     log(cid:88)y

= w    f (x, y)     log z(x; w)

exp{w    f (x, y)}

(cid:73) partition function: z(x; w) =(cid:80)y exp{w    f (x, y)}

(cid:73) log z(x; w) is a constant for a    xed x
(cid:73) in the log space, computations are linear,

i.e., we model log-probabilities using a linear predictor

making predictions with id148

(cid:73) for tractability, assume f (x, y) decomposes into bigrams:

f (x1:n, y1:n) =

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) given w, given x1:n,    nd:

argmax

y1:n

pr(y1:n|x1:n; w) = amax

y

= amax

y

= amax

y

(cid:73) we can use the viterbi algorithm

exp{(cid:80)n
exp(cid:40) n(cid:88)i=1
n(cid:88)i=1

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

w    f (x, i, yi   1, yi)(cid:41)

w    f (x, i, yi   1, yi)

making predictions with id148

(cid:73) for tractability, assume f (x, y) decomposes into bigrams:

f (x1:n, y1:n) =

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) given w, given x1:n,    nd:

argmax

y1:n

pr(y1:n|x1:n; w) = amax

y

= amax

y

= amax

y

(cid:73) we can use the viterbi algorithm

exp{(cid:80)n
exp(cid:40) n(cid:88)i=1
n(cid:88)i=1

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

w    f (x, i, yi   1, yi)(cid:41)

w    f (x, i, yi   1, yi)

parameter estimation in id148

pr(y | x; w) =

exp{w    f (x, y)}

z(x; w)

how to estimate w given training data?

two approaches:

(cid:73) memms: assume that pr(y | x; w) decomposes
(cid:73) crfs: assume that f (x, y) decomposes

parameter estimation in id148

pr(y | x; w) =

exp{w    f (x, y)}

z(x; w)

how to estimate w given training data?

two approaches:

(cid:73) memms: assume that pr(y | x; w) decomposes
(cid:73) crfs: assume that f (x, y) decomposes

maximum id178 markov models (memms)
(mccallum, freitag, pereira    00)

(cid:73) similarly to id48s:

pr(y1:n | x1:n) = pr(y1 | x1:n)    pr(y2:n | x1:n, y1)

= pr(y1 | x1:n)   

pr(yi|x1:n, y1:i   1)

= pr(y1|x1:n)   

(cid:73) assumption under memms:

pr(yi|x1:n, yi   1)

pr(yi|x1:n, y1:i   1) = pr(yi|x1:n, yi   1)

n(cid:89)i=2
n(cid:89)i=2

parameter estimation in memms

(cid:73) decompose sequential problem:

pr(y1:n | x1:n) = pr(y1 | x1:n)   

n(cid:89)i=2

pr(yi|x1:n, i, yi   1)

(cid:73) learn local log-linear distributions (i.e. maxent)

pr(y | x, i, y(cid:48)) =

exp{w    f (x, i, y(cid:48), y)}

z(x, i, y(cid:48))

where

(cid:73) x is an input sequencem
(cid:73) y and y(cid:48) are tags
(cid:73) f (x, i, y(cid:48), y) is a feature vector of x, the position to be tagged, the

previous tag and the current tag

(cid:73) sequence learning reduced to multi-class id28

id49
(la   erty, mccallum, pereira 2001)

(cid:73) log-linear model of the conditional distribution:

pr(y|x; w) =

exp{w    f (x, y)}

z(x)

where

(cid:73) x = x1x2 . . . xn     x
(cid:73) y = y1y2 . . . yn     y
(cid:73) f (x, y) is a feature vector of x and y
(cid:73) w are model parameters

   
    and y = {1, . . . , l}

(cid:73) to predict the best sequence

  y = argmax

   

y   y

pr(y|x)

(cid:73) assumption in crf (for tractability):
f (x, y) decomposes into factors

crfs as factored id148

(cid:73) for tractability, f (x, y) needs to decompose.

for bigram factorizations:

f (x1:n, y1:n) =

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) the model form is:

where

pr(y|x1:n; w) =

=

z(x1:n, w) = (cid:88)z   y n

exp{w    f (x, y)}

z(x, w)

exp{(cid:80)n
exp(cid:40) n(cid:88)i=1

i=1 w    f (x, i, yi   1, yi)}

z(x, w)

w    f (x, i, zi   1, zi)(cid:41)

parameter estimation in crfs

(cid:73) given a training set

(cid:110)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:111) ,

estimate w

(cid:73) de   ne the conditional log-likelihood of the data:

l(w) =

m(cid:88)k=1

log pr(y(k)|x(k); w)

(cid:73) l(w) measures how well w explains the data. a good value for w

will give a high value for pr(y(k)|x(k); w) for all k = 1 . . . m.

(cid:73) we want w that maximizes l(w)

learning the parameters of a crf

(cid:73) we pose it as a id76 problem
(cid:73) find:

w    = argmax
w   rd

l(w)    

  
2||w||2

where

(cid:73) the    rst term is the log-likelihood of the data
(cid:73) the second term is a id173 term, it penalizes solutions with

large norm (similar to norm-minimization in id166)

(cid:73)    is a parameter to control the trade-o    between    tting the data

and model complexity

learning the parameters of a crf

(cid:73) find

w    = argmax
w   rd

l(w)    

  
2||w||2

(cid:73) in general there is no analytical solution to this optimization
(cid:73) we use iterative techniques, i.e. gradient-based optimization

1. initialize w = 0
2. take derivatives of l(w)       
3. move w in steps proportional to the gradient
4. repeat steps 2 and 3 until convergence

2||w||2, compute gradient

(cid:73) fast and scalable algorithms exist

computing the gradient in crfs

m(cid:88)k=1
m(cid:88)k=1 (cid:88)y   y
n(cid:88)i=1

   

   l(w)
   wj

=

1
m

fj(x(k), y(k))

   

where

pr(y|x(k); w) fj(x(k), y)

f (x, y) =

fj(x, i, yi   1, yi)

(cid:73) first term: observed mean feature value
(cid:73) second term: expected feature value under current w
(cid:73) in the optimal, observed = expected

computing the gradient in crfs

(cid:73) the    rst term is easy to compute, by counting explicitly

1
m

m(cid:88)k=1(cid:88)i

fj(x, i, y(k)

i   1, y(k)

i

)

(cid:73) the second term is more involved,

m(cid:88)k=1 (cid:88)y   y

   

pr(y|x(k); w)(cid:88)i

fj(x(k), i, yi   1, yi)

because it sums over all sequences y     y   

computing the gradient in crfs

(cid:73) for an example (x(k), y(k)):

fj(x(k), i, yi   1, yi) =

  k
i (a, b)fj(x(k), i, a, b)

(cid:88)y   y n

where

n(cid:88)i=1
pr(y|x(k); w)
n(cid:88)i=1 (cid:88)a,b   y
(cid:88)y   y n : yi   1=a, yi=b

  k
i (a, b) =

pr(y|x(k); w)

(cid:73) the quantities   k

i can be computed e   ciently in o(nl2) using the

forward-backward algorithm

forward-backward for crfs

, 1     i     n; a, b     y

(cid:73) de   nition: forward and backward quantities

(cid:73) assume    xed x. calculate in o(n|y|2)
pr(y|x; w)

  i(a, b) =

(cid:88)y   y n:yi   1=a,yi=b
exp(cid:110)(cid:80)i
(cid:88)yi:n   y (n   i+1):yi=b

  i(a) = (cid:88)y1:i   y i:yi=a

  i(b) =

j=1 w    f (x, j, yj   1, yj)(cid:111)
exp(cid:110)(cid:80)n

j=i+1 w    f (x, j, yj   1, yj)(cid:111)

(cid:73) z =(cid:80)a   n(a)
(cid:73)   i(a, b) = {  i   1(a)     exp{w    f (x, i, a, b)}       i(b)     z   1}
(cid:73) similarly to viterbi,   i(a) and   i(b) can be computed e   ciently in

a recursive manner

forward-backward for crfs

, 1     i     n; a, b     y

(cid:73) de   nition: forward and backward quantities

(cid:73) assume    xed x. calculate in o(n|y|2)
pr(y|x; w)

  i(a, b) =

(cid:88)y   y n:yi   1=a,yi=b
exp(cid:110)(cid:80)i
(cid:88)yi:n   y (n   i+1):yi=b

  i(a) = (cid:88)y1:i   y i:yi=a

  i(b) =

j=1 w    f (x, j, yj   1, yj)(cid:111)
exp(cid:110)(cid:80)n

j=i+1 w    f (x, j, yj   1, yj)(cid:111)

(cid:73) z =(cid:80)a   n(a)
(cid:73)   i(a, b) = {  i   1(a)     exp{w    f (x, i, a, b)}       i(b)     z   1}
(cid:73) similarly to viterbi,   i(a) and   i(b) can be computed e   ciently in

a recursive manner

crfs: summary so far

(cid:73) id148 for sequence prediction, pr(y|x; w)
(cid:73) computations factorize on label bigrams
(cid:73) model form:

argmax

y   y

    (cid:88)i

w    f (x, i, yi   1, yi)

(cid:73) prediction: uses viterbi (from id48s)
(cid:73) parameter estimation:

(cid:73) gradient-based methods, in practice l-bfgs
(cid:73) computation of gradient uses forward-backward (from id48s)

crfs: summary so far

(cid:73) id148 for sequence prediction, pr(y|x; w)
(cid:73) computations factorize on label bigrams
(cid:73) model form:

argmax

y   y

    (cid:88)i

w    f (x, i, yi   1, yi)

(cid:73) prediction: uses viterbi (from id48s)
(cid:73) parameter estimation:

(cid:73) gradient-based methods, in practice l-bfgs
(cid:73) computation of gradient uses forward-backward (from id48s)

(cid:73) next question: memms or crfs? id48s or crfs?

memms and crfs

memms: pr(y | x) =

exp{w    f (x, i, yi   1, yi)}

z(x, i, yi   1; w)

crfs: pr(y | x) =

i=1 w    f (x, i, yi   1, yi)}

z(x)

n(cid:89)i=1
exp{(cid:80)n

(cid:73) memms locally normalized; crfs globally normalized
(cid:73) memm assume that pr(yi | x1:n, y1:i   1) = pr(yi | x1:n, yi   1)
(cid:73) both exploit the same factorization, i.e. same features
(cid:73) same computations to compute argmaxy pr(y | x)
(cid:73) memms are cheaper to train
(cid:73) crfs are easier to extend to other structures (next lecture)

id48s for sequence prediction

(cid:73) x are the observations, y are the hidden states
(cid:73) id48s model the joint distributon pr(x, y)
(cid:73) parameters: (assume x = {1, . . . , k} and y = {1, . . . , l})

(cid:73) model form

(cid:73)        rl,   a = pr(y1 = a)
(cid:73) t     rl  l, ta,b = pr(yi = b|yi   1 = a)
(cid:73) o     rl  k, oa,c = pr(xi = c|yi = a)
n(cid:89)i=2

pr(x, y) =   y1oy1,x1

tyi   1,yioyi,xi

(cid:73) parameter estimation: maximum likelihood by counting events and

normalizing

id48s and crfs

i=2 log(tyi   1,yioyi,xi)

(cid:73) in id48s:

(cid:73) in crfs:   y = amaxy(cid:80)i w    f (x, i, yi   1, yi)
  y = amaxy   y1oy1,x1(cid:81)n
= amaxy log(  y1oy1,x1) +(cid:80)n

i=2 tyi   1,yioyi,xi

(cid:73) an id48 can be    ported    into a crf by setting:

fj(x, i, y, y(cid:48))
i = 1 & y(cid:48) = a

i > 1 & y = a & y(cid:48) = b

y(cid:48) = a & xi = c

wj

log(  a)
log(ta,b)
log(oa,b)

(cid:73) hence, id48 parameters     crf parameters

id48s and crfs: main di   erences

(cid:73) representation:

(cid:73) id48    features    are tied to the generative process.
(cid:73) crf features are very    exible. they can look at the whole input x

paired with a label bigram (y, y(cid:48)).

(cid:73) in practice, for prediction tasks,    good    discriminative features can

improve accuracy a lot.

(cid:73) parameter estimation:

(cid:73) id48s focus on explaining the data, both x and y.
(cid:73) crfs focus on the mapping from x to y.
(cid:73) a priori, it is hard to say which paradigm is better.
(cid:73) same dilemma as naive bayes vs. maximum id178.

id170

id88, id166s, crfs

learning structured predictors

(cid:73) goal: given training data

(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y with small error on unseen inputs

(cid:73) in a crf:

argmax

   

y   y

p (y|x; w) =

=

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

exp{(cid:80)n
n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) to predict new values, z(x; w) is not relevant
(cid:73) parameter estimation: w is set to maximize likelihood

(cid:73) can we learn w more directly, focusing on errors?

learning structured predictors

(cid:73) goal: given training data

(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y with small error on unseen inputs

(cid:73) in a crf:

argmax

   

y   y

p (y|x; w) =

=

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

exp{(cid:80)n
n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) to predict new values, z(x; w) is not relevant
(cid:73) parameter estimation: w is set to maximize likelihood

(cid:73) can we learn w more directly, focusing on errors?

the structured id88
(collins, 2002)

(cid:73) set w = 0
(cid:73) for t = 1 . . . t

(cid:73) for each training example (x, y)

1. compute z = argmaxz
2.

if z (cid:54)= y

i=1 f (x, i, zi   1, zi)

(cid:88)

i

f (x, i, yi   1, yi)    

f (x, i, zi   1, zi)

(cid:80)n

(cid:88)

i

w     w +

(cid:73) return w

the structured id88 + averaging
(freund and schapire, 1998) (collins 2002)

(cid:73) set w = 0, wa = 0
(cid:73) for t = 1 . . . t

(cid:73) for each training example (x, y)

1. compute z = argmaxz
2.

if z (cid:54)= y

w     w +

3. wa = wa + w

(cid:88)

i

(cid:80)n

i=1 f (x, i, zi   1, zi)

(cid:88)

i

f (x, i, yi   1, yi)    

f (x, i, zi   1, zi)

(cid:73) return wa/mt , where m is the number of training examples

id88 updates: example

y per
per
z
x jack

per
loc

-
-

london went

loc
-
loc
-
to paris

(cid:73) let y be the correct output for x.
(cid:73) say we predict z instead, under our current w
(cid:73) the update is:

g = f (x, y)     f (x, z)

f (x, i, yi   1, yi)    (cid:88)i

= (cid:88)i
= f (x, 2, per, per)     f (x, 2, per, loc)
+ f (x, 3, per, -)     f (x, 3, loc, -)

f (x, i, zi   1, zi)

(cid:73) id88 updates are typically very sparse

properties of the id88

(cid:73) online algorithm. often much more e   cient than    batch   

algorithms

(cid:73) if the data is separable, it will converge to parameter values with 0

errors

(cid:73) number of errors before convergence is related to a de   nition of

margin. can also relate margin to generalization properties

(cid:73) in practice:

1. averaging improves performance a lot
2. typically reaches a good solution after only a few (say 5) iterations

over the training set

3. often performs nearly as well as crfs, or id166s

averaged id88 convergence

iteration accuracy

1
2
3
4
5
6
7
8
9
10
11
12
. . .

90.79
91.20
91.32
91.47
91.58
91.78
91.76
91.82
91.88
91.91
91.92
91.96

(results on validation set for a parsing task)

margin-based id170

(cid:73) let f (x, y) =(cid:80)n

(cid:73) model: argmaxy   y

i=1 f (x, i, yi   1, yi)

    w    f (x, y)

(cid:73) consider an example (x(k), y(k)):

   y (cid:54)= y(k)

: w    f (x(k), y(k)) < w    f (x(k), y) =    error

(cid:73) let y(cid:48) = argmaxy   y

   :y(cid:54)=y(k) w    f (x(k), y)
de   ne   k = w    (f (x(k), y(k))     f (x(k), y(cid:48)))

(cid:73) the quantity   k is a notion of margin on example k:

  k > 0        no mistakes in the example
high   k        high con   dence

margin-based id170

(cid:73) let f (x, y) =(cid:80)n

(cid:73) model: argmaxy   y

i=1 f (x, i, yi   1, yi)

    w    f (x, y)

(cid:73) consider an example (x(k), y(k)):

   y (cid:54)= y(k)

: w    f (x(k), y(k)) < w    f (x(k), y) =    error

(cid:73) let y(cid:48) = argmaxy   y

   :y(cid:54)=y(k) w    f (x(k), y)
de   ne   k = w    (f (x(k), y(k))     f (x(k), y(cid:48)))

(cid:73) the quantity   k is a notion of margin on example k:

  k > 0        no mistakes in the example
high   k        high con   dence

margin-based id170

(cid:73) let f (x, y) =(cid:80)n

(cid:73) model: argmaxy   y

i=1 f (x, i, yi   1, yi)

    w    f (x, y)

(cid:73) consider an example (x(k), y(k)):

   y (cid:54)= y(k)

: w    f (x(k), y(k)) < w    f (x(k), y) =    error

(cid:73) let y(cid:48) = argmaxy   y

   :y(cid:54)=y(k) w    f (x(k), y)
de   ne   k = w    (f (x(k), y(k))     f (x(k), y(cid:48)))

(cid:73) the quantity   k is a notion of margin on example k:

  k > 0        no mistakes in the example
high   k        high con   dence

mistake-augmented margins
(taskar et al, 2004)

x(k)
y(k)
y(cid:48)
y(cid:48)(cid:48)
y(cid:48)(cid:48)(cid:48)

jack
per
per
per

-

london went

per
loc

-
-

to
-
-
-

per

paris
loc
loc

-
-

-
-
-

per

(cid:73) def: e(y, y(cid:48)) =(cid:80)n

i=1[yi (cid:54)= y(cid:48)i]

e.g., e(y(k), y(k)) = 0, e(y(k), y(cid:48)) = 1, e(y(k), y(cid:48)(cid:48)(cid:48)) = 5

(cid:73) we want a w such that

   y (cid:54)= y(k)

: w    f (x(k), y(k)) > w    f (x(k), y) + e(y(k), y)

(the higher the error of y, the larger the separation should be)

structured hinge loss

(cid:73) de   ne a mistake-augmented margin

  k,y =w    f (x(k), y(k))     w    f (x(k), y)     e(y(k), y)
  k = min
y(cid:54)=y(k)

  k,y

(cid:73) de   ne id168 on example k as:

l(w, x(k), y(k)) = max

y   y   (cid:110)e(y(k), y)     w    f (x(k), y(k))     w    f (x(k), y)(cid:111)

(cid:73) leads to an id166 for id170
(cid:73) given a training set,    nd:

argmin
w   rd

m(cid:88)k=1

l(w, x(k), y(k)) +

  
2(cid:107)w(cid:107)2

regularized loss minimization

find:

(cid:73) given a training set(cid:8)(x(1), y(1)), . . . , (x(m), y(m))(cid:9) .
  
2(cid:107)w(cid:107)2

l(w, x(k), y(k)) +

argmin
w   rd

m(cid:88)k=1

(cid:73) two common id168s l(w, x(k), y(k)) :

(cid:73) log-likelihood loss (crfs)

(cid:73) hinge loss (id166s)

    log p (y(k) | x(k); w)

max

y   y   (cid:16)e(y(k), y)     w    (f (x(k), y(k))     f (x(k), y))(cid:17)

learning structure predictors: summary so far

(cid:73) linear models for sequence prediction

argmax

y   y

    (cid:88)i

w    f (x, i, yi   1, yi)

(cid:73) computations factorize on label bigrams

(cid:73) decoding: using viterbi
(cid:73) marginals: using forward-backward

(cid:73) parameter estimation:

(cid:73) id88, log-likelihood, id166s
(cid:73) extensions from classi   cation to the structured case
(cid:73) optimization methods:

(cid:73) stochastic (sub)gradient methods (lecun et al 98)

(shalev-shwartz et al. 07)

(cid:73) exponentiated gradient (collins et al 08)
(cid:73) id166 struct (tsochantaridis et al. 04)
(cid:73) structured mira (mcdonald et al 05)

generic structure prediction

sequence prediction, beyond bigrams

(cid:73) it is easy to extend the scope of features to k-grams

f (x, i, yi   k+1:i   1, yi)

(cid:73) in general, think of state   i remembering relevant history

(cid:73)   i = yi   1 for bigrams
(cid:73)   i = yi   k+1:i   1 for k-grams
(cid:73)   i can be the state at time i of a deterministic automaton

generating y

(cid:73) the structured predictor is

argmax

y   y

    (cid:88)i

w    f (x, i,   i, yi)

(cid:73) viterbi and forward-backward extend naturally, in o(nlk)

dependency structures

(cid:73) directed arcs represent dependencies between a head word and a

modi   er word.

(cid:73) e.g.:

movie modi   es saw,
john modi   es saw,
today modi   es saw

dependencystructureslikedtoday*johnsawamoviethathe!directedarcsrepresentdependenciesbetweenaheadwordandamodi   erword.!e.g.:moviemodi   essaw,johnmodi   essaw,todaymodi   essawid33: arc-factored models
(mcdonald et al. 2005)

(cid:73) parse trees decompose into single dependencies (cid:104)h, m(cid:105)

argmax

y   y(x) (cid:88)(cid:104)h,m(cid:105)   y

w    f (x, h, m)

(cid:73) some features:

f1(x, h, m) = [    saw           movie    ]
f2(x, h, m) = [ distance = +2 ]

(cid:73) tractable id136 algorithms exist (tomorrow   s lecture)

dependencyparsing:arc-factoredmodels(mcdonaldetal.2005)likedtoday*johnsawamoviethathe!parsetreesdecomposeintosingledependencies!h,m"argmaxy   y(x)!"h,m#   yw  f(x,h,m)!somefeatures:f1(x,h,m)=[   saw         movie   ]f2(x,h,m)=[distance=+2]!tractableid136algorithmsexist(tomorrow   slecture)linear id170

(cid:73) sequence prediction (bigram factorization)

(cid:73) id33 (arc-factored)

argmax

y   y(x) (cid:88)i
y   y(x) (cid:88)(cid:104)h,m(cid:105)   y

argmax

w    f (x, i, yi   1, yi)

w    f (x, h, m)

(cid:73) in general, we can enumerate parts r     y

argmax

y   y(x) (cid:88)r   y

w    f (x, r)

linear id170 framework

(cid:73) abstract models of structures

(cid:73) input domain x , output domain y
(cid:73) a choice of factorization, r     y
(cid:73) features: f (x, r)     rd

(cid:73) the linear prediction model, with w     rd

argmax

y   y(x) (cid:88)r   y

w    f (x, r)

(cid:73) generic algorithms for id88, crf, id166

(cid:73) require tractable id136 algorithms

