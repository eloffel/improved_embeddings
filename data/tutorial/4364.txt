introduction

smt

evaluation

success stories

conclusions

id151

lucia specia

l.specia@sheffield.ac.uk

lxmls 2015
18 july 2015

id151

1 / 71

introduction

smt

evaluation

success stories

conclusions

outline

1

introduction

2 smt

3 evaluation

4 success stories

5 conclusions

some slides from wilker aziz, kevin knight, philipp koehn, adam lopez

id151

2 / 71

introduction

smt

evaluation

success stories

conclusions

outline

1

introduction

2 smt

3 evaluation

4 success stories

5 conclusions

id151

3 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s
increasingly popular since 1990: statistical approaches

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s
increasingly popular since 1990: statistical approaches
software toolkits to build translation systems from data,
e.g. moses, cdec

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s
increasingly popular since 1990: statistical approaches
software toolkits to build translation systems from data,
e.g. moses, cdec
availability of large collections of data, e.g. europarl,
taus data

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s
increasingly popular since 1990: statistical approaches
software toolkits to build translation systems from data,
e.g. moses, cdec
availability of large collections of data, e.g. europarl,
taus data
more processing power

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s
increasingly popular since 1990: statistical approaches
software toolkits to build translation systems from data,
e.g. moses, cdec
availability of large collections of data, e.g. europarl,
taus data
more processing power
increasing demand for (cheap) translations - google: 1
billion translations requests/day for 200 million users

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s
increasingly popular since 1990: statistical approaches
software toolkits to build translation systems from data,
e.g. moses, cdec
availability of large collections of data, e.g. europarl,
taus data
more processing power
increasing demand for (cheap) translations - google: 1
billion translations requests/day for 200 million users
funding for research worldwide

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

introduction

mt has been around since the early 1950s
increasingly popular since 1990: statistical approaches
software toolkits to build translation systems from data,
e.g. moses, cdec
availability of large collections of data, e.g. europarl,
taus data
more processing power
increasing demand for (cheap) translations - google: 1
billion translations requests/day for 200 million users
funding for research worldwide
exciting time for mt!

id151

4 / 71

introduction

smt

evaluation

success stories

conclusions

the task of machine translation (mt)

the boy ate an apple

o menino comeu uma ma  c  a

id151

5 / 71

                            ,                                          however  ,  the  sky  remained  clear  under  the  strong  north  wind  .introduction

smt

evaluation

success stories

conclusions

the task of machine translation (mt)

the boy ate an apple

o menino comeu uma ma  c  a

but

he said that the bottle    oated into the cave

? dijo que la botella entro a la cueva    otando

id151

5 / 71

                            ,                                          however  ,  the  sky  remained  clear  under  the  strong  north  wind  .introduction

smt

evaluation

success stories

conclusions

the task of machine translation (mt)

the boy ate an apple

o menino comeu uma ma  c  a

but

he said that the bottle    oated into the cave

? dijo que la botella entro a la cueva    otando

id151

5 / 71

                            ,                                          however  ,  the  sky  remained  clear  under  the  strong  north  wind  .introduction

smt

evaluation

success stories

conclusions

challenges in mt

lexical ambiguity
syntactic ambiguity
pronoun resolution
structural divergences

id151

6 / 71

introduction

smt

evaluation

success stories

conclusions

challenges in mt

lexical ambiguity
syntactic ambiguity
pronoun resolution
structural divergences
idioms
e.g. he    nally kicked the bucket at the hospital

    ele    nalmente bateu as botas no hospital

multi-word expressions
e.g. do take the long waiting list for organ donation in this
    considere a longa lista de espera para doa  c  ao de

country into account

  org  aos neste pa    s

id151

6 / 71

introduction

smt

evaluation

success stories

conclusions

outline

1

introduction

2 smt

3 evaluation

4 success stories

5 conclusions

id151

7 / 71

introduction

smt

evaluation

success stories

conclusions

id151

id151 (smt):    learn    how to
generate translations from data

formalised early 1990s by ibm, but idea is much older:

warren weaver (1949)
when i look at an article in russian, i say:    this is really
written in english, but it has been coded in some strange
symbols. i will now proceed to decode.   

id151

8 / 71

introduction

smt

evaluation

success stories

conclusions

id151

id151 (smt):    learn    how to
generate translations from data

formalised early 1990s by ibm, but idea is much older:

warren weaver (1949)
when i look at an article in russian, i say:    this is really
written in english, but it has been coded in some strange
symbols. i will now proceed to decode.   

inspired by wwii code-breaking, and shannon   s
id205
approach was not feasible with early computers

id151

8 / 71

introduction

smt

evaluation

success stories

conclusions

id87

id87

idea developed to model communication (shannon)

e.g. communication over an imperfect phone line

want to recover original message (here word) on basis of
distorted signal received (here noisy word)

id151

9 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

output depends probabilistically on input
to translate french (f ) into english (e ):

given a french sentence f , search for english sentence e    
that maximises p(e|f )

id151

10 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

find english sentence that maximizes p(e|f ), i.e.

id151

11 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

find english sentence that maximizes p(e|f ), i.e.

e     = argmax

p(e|f )

e

id151

11 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

find english sentence that maximizes p(e|f ), i.e.

e     = argmax

e

= argmax

e

p(e|f )
p(e )  p(f|e )

p(f )

bayes rule

id151

11 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

find english sentence that maximizes p(e|f ), i.e.

e     = argmax

e

= argmax

e

p(e|f )
p(e )  p(f|e )

p(f )

bayes rule

p(f ) constant across di   erent e , so:

id151

11 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

find english sentence that maximizes p(e|f ), i.e.

e     = argmax

e

= argmax

e

p(e|f )
p(e )  p(f|e )

p(f )

bayes rule

p(f ) constant across di   erent e , so:

e     = argmax

p(e )    p(f|e ) drop p(f)

e

id151

11 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

e     = argmax

p(e|f ) = argmax

p(e )    p(f|e )

e

e

decomposition of p(e|f ) to p(e )    p(f|e ) breaks the
problem in two parts:

p(f|e ) worries about picking e words that were likely
used to generate f     faithfulness
p(e ) worries about picking e words that are likely to be
said in english and that    t together        uency

id151

12 / 71

introduction

smt

evaluation

success stories

conclusions

id87 & smt

e     = argmax

p(e|f ) = argmax

p(e )    p(f|e )

e

e

decomposition of p(e|f ) to p(e )    p(f|e ) breaks the
problem in two parts:

p(f|e ) worries about picking e words that were likely
used to generate f     faithfulness
p(e ) worries about picking e words that are likely to be
said in english and that    t together        uency

p(e ) and p(f|e ) can be trained independently: more
reliable model

id151

12 / 71

introduction

main components for translation (f (cid:55)    e )

success stories

evaluation

smt

conclusions

translation model (tm): p(f|e )

faithfulness: tms created from (large) parallel texts

id151

13 / 71

introduction

main components for translation (f (cid:55)    e )

success stories

evaluation

smt

conclusions

translation model (tm): p(f|e )

faithfulness: tms created from (large) parallel texts

language model (lm): p(e )

fluency: lms created from large (   uent) target
language texts

id151

13 / 71

introduction

main components for translation (f (cid:55)    e )

success stories

evaluation

smt

conclusions

translation model (tm): p(f|e )

faithfulness: tms created from (large) parallel texts

language model (lm): p(e )

fluency: lms created from large (   uent) target
language texts

a decoder: (argmax)

search algorithm to    nd e   

id151

13 / 71

introduction

learning translation models - p(f|e )

success stories

evaluation

smt

conclusions

requires a sentence-aligned bilingual corpus

e.g. european/canadian/hong kong parliaments,
subtitles, bible, web crawl

id151

14 / 71

introduction

learning translation models - p(f|e )

success stories

evaluation

smt

conclusions

requires a sentence-aligned bilingual corpus

e.g. european/canadian/hong kong parliaments,
subtitles, bible, web crawl

can we estimate p(f|e ) from entire sentences?

id151

14 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

id151

15 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

english
a home
a garden
i arrived home

id151

15 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words are a good
starting point
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

the same word happens in source 1 and 3

english
a home
a garden
i arrived home

id151

16 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

could we expect the same in the target side?

english
a home
a garden
i arrived home

id151

17 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

english
a home
a garden
i arrived home

useh = home

id151

18 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

what about the contexts?

english
a home
a garden
i arrived home

id151

19 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

english
a home
a garden
i arrived home

we can align them: yiguo = i; yiguo = arrived; uh = a

id151

20 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

english
a home
a garden
i arrived home

reuse this knowledge to align more sentences: uh = a

id151

21 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

break sentences into smaller units: words
learn translation probabilities by word aligning a
sentence-aligned corpus:

zenish
uh useh
uh jejje
yiguo useh

english
a home
a garden
i arrived home

and the context again: jejje = garden

id151

22 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

word-alignment:

identify correspondences between two languages at the
word level
basis for word-based smt,    rst step for other approaches
alignment learned via expectation maximization (em)

start with all alternative word alignments as equally
likely
observe across sentences that zenish useh often links to
english home

increase id203 of this word pair aligning
knock-on e   ect: update alignment of other words

iteratively redistribute probabilities, until they identify
most likely links for each word (convergence)

id151

23 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

word alignment commonly done using ibm models 1-5

id151

24 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

word alignment commonly done using ibm models 1-5

ibm 1 is a straightforward application of em, including
the alignment to null token (deletion)

finds translation probabilities for words in isolation,
regardless of their position in parallel sentence

id151

24 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

word alignment commonly done using ibm models 1-5

ibm 1 is a straightforward application of em, including
the alignment to null token (deletion)

finds translation probabilities for words in isolation,
regardless of their position in parallel sentence

ibm 2-5 improve these distributions by considering:
position of words in target sentence are related to
position of words in source sentence (distortion model)
some source words may be translated into multiple
target words (fertility of the words)
position of a target word may be related to position of
neighbouring words (relative distortion model)

id151

24 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

ibm1 produces a probabilistic dictionary based on entire
parallel corpus:

uh
uh
uh
useh
useh
useh
jejje
jejjje
yiguo
yiguo

a
home
garden
a
home
i arrived
a
garden
i arrived
home

0.90
0.05
0.05
0.03
0.95
0.02
0.30
0.70
0.80
0.20

higher models estimate other probabilities: fertility,
position, etc.

id151

25 / 71

introduction

smt

evaluation

success stories

conclusions

learning translation models - word-based smt

at translation (decoding) time
for a new sentence to translate, take the set of translations
that jointly maximise the whole translation id203

e     = argmax

p(f|e )

e

what about the    uency in the target language?

id151

26 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

language model: p(e )

e     = argmax

p(f|e )    p(e )

e

di   erent translation options and di   erent word orders are
possible, some are more likely to happen in e

id151

27 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

language model: p(e )

e     = argmax

p(f|e )    p(e )

e

di   erent translation options and di   erent word orders are
possible, some are more likely to happen in e
p(e ) = id203 of strings e based on relative
frequencies in a large corpus of language e

id151

27 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

e.g.:

given the new sentence:    yiguo la ta jejje   
assume new dictionary entries: la = at; ta = the
translation model could generate many possible
translations, e.g.:

i arrived at the a
i arrived at the garden
home at the a
home at the garden
the a at i arrived
...

score each of them according to p(e)

id151

28 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

e.g.:

given the new sentence:    yiguo la ta jejje   
assume new dictionary entries: la = at; ta = the
translation model could generate many possible
translations, e.g.:

i arrived at the a
i arrived at the garden
home at the a
home at the garden
the a at i arrived
...

score each of them according to p(e)

id151

28 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

e.g.:

given the new sentence:    yiguo la ta jejje   
assume new dictionary entries: la = at; ta = the
translation model could generate many possible
translations, e.g.:

i arrived at the a
i arrived at the garden
home at the a
home at the garden
the a at i arrived
...

score each of them according to p(e)

id151

28 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

p(e ) = p(e1, e2, e3,       , en)

= p(e1)p(e2|e1)    p(e3|e1, e2)       p(en|e1,       , en   1)
di   cult to have reliable estimates for whole sentences    
break it down into smaller sequences: id165s

markov assumption: only the previous n-1 words
matter for predicting a word. for trigram models, n = 3
(cid:39) p(e1)    p(e2|e1)    p(e3|e1, e2)   
p(e4|e2, e3)       p(en|en   2, en   1)

id151

29 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

relative frequencies to compute these probabilities. e.g.
trigrams:
p(e3|e1, e2) = count(e1e2e3)
p(garden|at, the) = count(at the garden)

count(e1e2)

count(at the)

id151

30 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

relative frequencies to compute these probabilities. e.g.
trigrams:
p(e3|e1, e2) = count(e1e2e3)
p(garden|at, the) = count(at the garden)

count(e1e2)

count(at the)

for candidate: i arrived at the garden
p(i|start)    p(arrived|start, i )    p(at|i , arrived)   
p(the|arrived, at)    p(garden|at, the)    p(end|garden, the)

id151

30 / 71

introduction

smt

evaluation

success stories

conclusions

learning language models - word-based smt

relative frequencies to compute these probabilities. e.g.
trigrams:
p(e3|e1, e2) = count(e1e2e3)
p(garden|at, the) = count(at the garden)

count(e1e2)

count(at the)

for candidate: i arrived at the garden
p(i|start)    p(arrived|start, i )    p(at|i , arrived)   
p(the|arrived, at)    p(garden|at, the)    p(end|garden, the)

smoothing, back-o    models, etc. to improve over relative
counts

id151

30 / 71

introduction

smt

evaluation

success stories

conclusions

word-based smt     limitations

di   cult to word-align, and hence learn a tm, for
languages with di   erent words orders

considering all possible word orders     too costly, too
noisy
poor reordering model

id151

31 / 71

introduction

smt

evaluation

success stories

conclusions

word-based smt     limitations

di   cult to word-align, and hence learn a tm, for
languages with di   erent words orders

considering all possible word orders     too costly, too
noisy
poor reordering model

fertility/n-m alignments: some languages may have
di   erent notions of what counts as a word

donaydampfshi   ahrtsgesellschaftskapitaenskajuetenschluesseloch

the keyhole of the door of the cabin of the captain of a steamship
company operating on the danube

id151

31 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

it seems to1 me2     me1 parece2

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

it seems to1 me2     me1 parece2

je1 ne vais pas2 `a la3 maison4     i1 am not going2 to the3 house4

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

it seems to1 me2     me1 parece2

je1 ne vais pas2 `a la3 maison4     i1 am not going2 to the3 house4

eu1 sinto saudade de voc  e2     i1 miss you2

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

it seems to1 me2     me1 parece2

je1 ne vais pas2 `a la3 maison4     i1 am not going2 to the3 house4

eu1 sinto saudade de voc  e2     i1 miss you2

i1 miss you2     eu1 sinto sua falta2

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

it seems to1 me2     me1 parece2

je1 ne vais pas2 `a la3 maison4     i1 am not going2 to the3 house4

eu1 sinto saudade de voc  e2     i1 miss you2

i1 miss you2     eu1 sinto sua falta2

natuerlich1 hat2 john3 spass am4 spiel 5     of course1 john2 has3 fun

with the4 game5

more intuitive and reliable alignments

account for reordering within the phrases
phrases can still be reordered

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

it seems to1 me2     me1 parece2

je1 ne vais pas2 `a la3 maison4     i1 am not going2 to the3 house4

eu1 sinto saudade de voc  e2     i1 miss you2

i1 miss you2     eu1 sinto sua falta2

natuerlich1 hat2 john3 spass am4 spiel 5     of course1 john2 has3 fun

with the4 game5

more intuitive and reliable alignments

account for reordering within the phrases
phrases can still be reordered

can we directly estimate phrase translation id203
distribution from a parallel corpus using em?

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt

most popular approach since early 2000s

no voy1 a la2 casa3     i am not going1 to the2 house3

it seems to1 me2     me1 parece2

je1 ne vais pas2 `a la3 maison4     i1 am not going2 to the3 house4

eu1 sinto saudade de voc  e2     i1 miss you2

i1 miss you2     eu1 sinto sua falta2

natuerlich1 hat2 john3 spass am4 spiel 5     of course1 john2 has3 fun

with the4 game5

more intuitive and reliable alignments

account for reordering within the phrases
phrases can still be reordered

can we directly estimate phrase translation id203
distribution from a parallel corpus using em?

too many possible phrase pairs

id151

32 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - phrases from word alignments

extract phrase pairs that are consistent with wa

phrase: sequence of tokens, not linguistically motivated
wa produced by ibm models, like before, only once

reanudaci  on

del

per    odo

de

sesiones

resumption

of
the

session

id151

33 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - phrases from word alignments

extract phrase pairs that are consistent with wa

phrase: sequence of tokens, not linguistically motivated
wa produced by ibm models, like before, only once

reanudaci  on

del

per    odo

de

sesiones

resumption

of
the

session

resumption     reanudaci  on

2 of the     del

session     per    odo de sesiones

1

3

id151

33 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - phrases from word alignments

extract phrase pairs that are consistent with wa

phrase: sequence of tokens, not linguistically motivated
wa produced by ibm models, like before, only once

reanudaci  on

del

per    odo

de

sesiones

resumption

of
the

session

id151

34 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - phrases from word alignments

extract phrase pairs that are consistent with wa

phrase: sequence of tokens, not linguistically motivated
wa produced by ibm models, like before, only once

reanudaci  on

del

per    odo

de

sesiones

resumption

of
the

session

1

resumption of the     reanudaci  on del

2 of the session     del per    odo de sesiones

3

resumption of the session     reanudaci  on del per    odo de
sesiones

id151

34 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - phrases from word alignments

extract phrase pairs that are consistent with wa

phrase: sequence of tokens, not linguistically motivated
wa produced by ibm models, like before, only once

reanudaci  on

del

per    odo

de

sesiones

resumption

of
the

session

1

resumption of the     reanudaci  on del

2 of the session     del per    odo de sesiones

3

resumption of the session     reanudaci  on del per    odo de
sesiones

id151

35 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - phrase probabilities

1 extract phrase pairs from word aligned parallel corpus
2 extract counts of those phrases from large parallel

corpus (id113):

   

  (  f |  e) =

count(  f ,   e)
count(  e)

3 store phrases and their probabilities in a phrase table

probabilistic dictionary of phrases

id151

36 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - weighing components

e     = argmax

p(f|e )    p(e )

e

rewriting equation for phrases:
ps. lm (p(e )) remains the same: computed for id165s
e    = argmax

(cid:81)i
i=1   (  fi|  ei )   (cid:81)|e|

i=1 p(ei|e1        ei   1)

e

which component is more important?

p(f|e ) or p(e ) ?

id151

37 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - weighing components

e     = argmax

p(f|e )    p(e )

e

rewriting equation for phrases:
ps. lm (p(e )) remains the same: computed for id165s
e    = argmax

(cid:81)i
i=1   (  fi|  ei )   (cid:81)|e|

i=1 p(ei|e1        ei   1)

e

which component is more important?

p(f|e ) or p(e ) ?

depends on size/quality of corpus, language-pair, etc.
in generative model: components equally important

id151

37 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

weigh components for a given task (parallel corpus):

e    = argmax

[

e

  (  fi|  ei )       

p(ei|e1        ei   1)  lm ]

applying log (simpler to compute):

e    = argmax

exp[

    

e

log   (  fi|  ei ) +

  lm

log p(ei|e1        ei   1)]

id151

38 / 71

i=1

i(cid:89)
|e|(cid:89)

i=1

i=1

i(cid:88)
|e|(cid:88)

i=1

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

model

exp(cid:80)n

i=1   i hi (f, e)

e    = argmax

e

id151

39 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

model

exp(cid:80)n

i=1   i hi (f, e)

e    = argmax

e

components

1 plm
2   

id151

39 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

model

exp(cid:80)n

i=1   i hi (f, e)

e    = argmax

e

components

weights

1 plm
2   

1   lm
2     

id151

39 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

model

exp(cid:80)n

i=1   i hi (f, e)

e    = argmax

e

components

weights

feature functions

1 plm
2   

1   lm
2     

1 h1 = logplm
2 h2 = log   

id151

39 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

model

exp(cid:80)n

i=1   i hi (f, e)

e    = argmax

e

components

weights

feature functions

1 plm
2   

bene   ts

1   lm
2     

1 h1 = logplm
2 h2 = log   

1 extensible
2 weights can be tuned, i.e., learned from examples

id151

39 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

common additional components h(f, e):

direct phrase translation probabilities:   (  e|  f ) extracted
just like   (  f |  e)
distance-based phrase reordering:
d(starti     endi   1     1), for every phrase   (  fi|  ei )
exponential decaying cost function d(x) =   |x|
x = starti     endi   1     1: is there a gap in the translation
between two source phrases?

phrase penalty: constant    for each phrase produced;
   < 1 to favour fewer, but longer phrases (more    uent)

id151

40 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

common additional components h(f, e):

direct phrase translation probabilities:   (  e|  f ) extracted
just like   (  f |  e)
distance-based phrase reordering:
d(starti     endi   1     1), for every phrase   (  fi|  ei )
exponential decaying cost function d(x) =   |x|
x = starti     endi   1     1: is there a gap in the translation
between two source phrases?

phrase penalty: constant    for each phrase produced;
   < 1 to favour fewer, but longer phrases (more    uent)
etc:     15 popular components/features

id151

40 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - linear model

decoder remains similar, now with weights associated to
components

discriminative model: learn    weights such as to
minimise error in small corpus

id151

41 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - decoding

(cid:80)n

i=1   i hi (f, e)

e    = argmax

e

search problem:    nding the best scoring translation
according to the model

translation is build in sequence (left to right)
input words may be covered out of sequence (allow for
reordering)

id151

42 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - decoding

all phrases matching source words selected from phrase
table. e.g.:

j   
i
me

ai

have
has

yeux
les
eyes
the
them eye

noirs
black
dark

i have
i am
i did
i had
i have

black eyes

eyes

the eyes

espresso
somber

some

black eyes

black eyes
black eyes

i have

.
.
,
!
.
.
.
.
.

decoder selects phrases whose combination (in a given
order) yields the highest score acc to the linear model

id151

43 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - decoding

incrementally construct translation hypotheses by trying out
several possibilities:

generating target words in sequence, from left to right
computing the (so far) overall log-linear model score
for each hypothesis
pruning search space via heuristics. e.g:

distortion limit - at most 4 positions di   erent from
source order
keep only partial hypothesis that are promising, e.g.
model score is close to that of the best partial
hypothesis so far

id151

44 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - decoding

incrementally construct translation hypotheses by trying out
several possibilities:

generating target words in sequence, from left to right
computing the (so far) overall log-linear model score
for each hypothesis
pruning search space via heuristics. e.g:

distortion limit - at most 4 positions di   erent from
source order
keep only partial hypothesis that are promising, e.g.
model score is close to that of the best partial
hypothesis so far

approximate search, e.g. stack-based id125

id151

44 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - decoding

search space

hypothesis

covered source words
target (output) words
model score

id151

45 / 71

introduction

smt

evaluation

success stories

conclusions

phrase-based smt - tuning parameters

apply decoder with uniform weights to produce an
n-best list of translations (e.g. top 1,000 translations)
er geht ja nicht nach hause     he does not go home

rank
1
2
3
4
5
6
7
8
9
10

translation
it is not under house
he is not under house
it is not a home
it is not to go home
it is not for house
he is not to go home
he does not home
it is not packing
he is not packing
he is not for home

  (  e|  f )
-9.93
-7.40
-12.74
-10.34
-17.25
-10.95
-11.84
-10.63
-8.10
-13.52

  (  f |  e)
-19.00
-16.33
-19.29
-20.87
-20.43
-18.20
-16.98
-17.65
-14.98
-17.09

lex(  e|  f , a)
-5.08
-5.01
-5.08
-4.38
-4.90
-4.85
-3.67
-5.08
-5.01
-6.22

lex(  f |  e, a) wp
-5
-5
-5
-6
-5
-6
-4
-4
-4
-5

-8.22
-8.15
-8.42
-13.11
-6.90
-13.04
-8.76
-9.89
-9.82
-7.82

plm
-32.22
-34.50
-28.49
-32.53
-31.75
-35.79
-32.64
-32.26
-34.55
-36.70

error
0.8
0.6
0.6
0.8
0.8
0.6
0.2
0.8
0.6
0.4

iteratively adapt weights to re-rank n-best translations,
e.g. to make 7 appear at the top
error acc. to evaluation metric (id7) against ref
translation
mert, pro, mira...

id151

46 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical and syntax-based smt

pbsmt has trouble with long-distance reorderings
alternative approaches to bring structure and linguistic
knowledge into the transfer rules of phrase table

id151

47 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical and syntax-based smt

pbsmt has trouble with long-distance reorderings
alternative approaches to bring structure and linguistic
knowledge into the transfer rules of phrase table

id151

47 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - motivation

introduce structure into phrase-based smt models to deal
with long-distance reordering

n
e
d
n
e
h
c
e
r
p
s
t
n
e

n
e
g
n
u
k
r
e
m
n
a

i

n
e
g
d
n
  a
h
s
u
a

e
d
r
e
w

n
e
n
h
i

h
c
i

e
i
d

i
shall
be
passing
on
to
you
some
comments

id151

48 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - motivation

introduce structure into phrase-based smt models to deal
with long-distance reordering

n
e
d
n
e
h
c
e
r
p
s
t
n
e

n
e
g
n
u
k
r
e
m
n
a

i

n
e
g
d
n
  a
h
s
u
a

e
d
r
e
w

n
e
n
h
i

h
c
i

e
i
d

i
shall
be
passing
on
to
you
some
comments

how can we get a phrase for
shall be passing on?

id151

49 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - motivation

introduce structure into phrase-based smt models to deal
with long-distance reordering

e
d
r
e
w

n
e
n
h
i

h
c
i

e
i
d

x
x

x

i
shall
be
passing
on
to
you
some
comments

n
e
d
n
e
h
c
e
r
p
s
t
n
e

i

n
e
g
d
n
  a
h
s
u
a

n
e
g
n
u
k
r
e
m
n
a

x

how can we get a phrase for
shall be passing on?

we cannot, unless we get to
you some comments along

id151

50 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - motivation

introduce structure into phrase-based smt models to deal
with long-distance reordering

n
e
d
n
e
h
c
e
r
p
s
t
n
e

n
e
g
n
u
k
r
e
m
n
a

i

n
e
g
d
n
  a
h
s
u
a

e
d
r
e
w

n
e
n
h
i

h
c
i

e
i
d

i
shall
be
passing
on
to
you
some
comments

how can we get a phrase for
shall be passing on?

we cannot, unless we get to
you some comments along

unless we replace all those
words by a variable

id151

51 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - motivation

shall be passing on to you some comments

(cid:108)

werde ihnen die entsprechenden anmerkungen

aush  andigen

shall be passing on to you some comments

(cid:108)

werde ihnen die entsprechenden anmerkungen

aush  andigen

shall be passing on x

(cid:108)

werde x aush  andigen

id151

52 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - basics

learnt from word-aligned parallel corpora in the same way as
before

id151

53 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - basics

learnt from word-aligned parallel corpora in the same way as
before

based on the fact that language has recursive structures
phrases within other phrases treated as nonterminals:
replaced by x
no linguistic constraints added - yet, some structure

id151

53 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - basics

shall be passing on to you some comments

(cid:108)

werde ihnen die entsprechenden anmerkungen

aush  andigen

shall be passing on x1 some comments

(cid:108)

werde x1 die entsprechenden anmerkungen

aush  andigen

shall be passing on x1 x2

(cid:108)

werde x1 x2 aush  andigen

id151

54 / 71

introduction

smt

evaluation

success stories

conclusions

hierarchical smt - phrase-table

[x ]     shall be passing on x1 x2 | werde x1 x2 aush  andigen
[x ]     shall be passing on x3 | werde x3 aush  andigen
[x ]     to you | ihnen
[x ]     some comments | die entsprechenden anmerkungen
[x ]     to you some comments | ihnen die entsprechenden
anmerkungen

learn a bilingual (synchronous) set of context-free rules on
how to translate f   e

id151

55 / 71

introduction

smt

evaluation

success stories

conclusions

syntax-based smt

id187 are not very informative (xs) and
su   er from an exponential number of rules
syntax-based smt: uses linguistic categorise to label
nodes
syntactic parser at pre-processing time for at least one
language (the other could have xs)
learn a bilingual (synchronous) set of linguistically
informed context-free rules on how to translate f   e
rules extracted acc to word-alignment, constrained by
heuristics for syntax

id151

56 / 71

introduction

smt

evaluation

success stories

conclusions

syntax-based smt

prp

j   

s

vb

ai

s

vp

prp

vp

np

i

vb

np

dt

nn

jj

les

yeux

noirs

have

jj

nn

black

eyes

j   

ai

les

yeux

noirs

i
have
black
eyes

standard constraints on rule construction:

single nonterminal on the left

consistent with word-alignment

nonterminals on the right must align
one-to-one

id151

57 / 71

introduction

smt

evaluation

success stories

conclusions

syntax-based smt

grammar
prp     j    | i
jj     noirs | black
np     les yeux jj | jj eyes
vp     ai np | have np
s     prp vp | prp vp

decoding becomes a (probabilistic) parsing problem!

id151

58 / 71

introduction

smt

evaluation

success stories

conclusions

outline

1

introduction

2 smt

3 evaluation

4 success stories

5 conclusions

id151

59 / 71

introduction

smt

evaluation

success stories

conclusions

mt id74

for developers/researchers:

measure progress over time
compare mt systems
tune model parameters

quality =
close to human translation

id165 matching between system output and one or
more reference (human) translations

id151

60 / 71

introduction

smt

evaluation

success stories

conclusions

mt id74

id7: bilingual evaluation understudy

most widely used metric
matching of id165s between mt and ref: rewards
same words in equal order

id151

61 / 71

introduction

smt

evaluation

success stories

conclusions

mt id74

id7: bilingual evaluation understudy

most widely used metric
matching of id165s between mt and ref: rewards
same words in equal order

ref: the iraqi weapons are to be handed over to the army

within two weeks

mt: in two weeks iraq   s weapons will give to the army

1-gram precision: 6/10
2-gram precision: 3/9
3-gram precision: 3/8

(cid:16)(cid:81)3
id7 =(cid:0) 6

(cid:17) 1
(cid:1) 1
n=1 pn
10     3
9     2

id7 =

8

3

3 = 0.368

id151

61 / 71

introduction

smt

evaluation

success stories

conclusions

outline

1

introduction

2 smt

3 evaluation

4 success stories

5 conclusions

id151

62 / 71

introduction

smt

evaluation

success stories

conclusions

interest in mt beyond academia

government: u.s. (intelligence purposes); eu
(societal/political/commercial purposes):

eu spends more than 300me on human translation /
year: 23 o   cial languages (253 language pairs) in 2011

end-users: google translate, bing translator, etc.

language industry: considerable savings with mt

customised smt: kantaid4, asiaonline, etc.

id151

63 / 71

introduction

smt

evaluation

success stories

conclusions

commercial interest in mt

autodesk: productivity test (post-editing of mt) [aut11]

2-day translation vs post-editing, 37 participants
in-house moses (autodesk data: software)
time spent on each segment

id151

64 / 71

introduction

smt

evaluation

success stories

conclusions

commercial interest in mt

intel - user satisfaction, unedited mt

translation is good if customer can solve problem

id151

65 / 71

introduction

smt

evaluation

success stories

conclusions

commercial interest in mt

intel - user satisfaction, unedited mt

translation is good if customer can solve problem
mt for customer support websites [int10]

overall customer satisfaction: 75% for english   chinese

id151

65 / 71

introduction

smt

evaluation

success stories

conclusions

commercial interest in mt

intel - user satisfaction, unedited mt

translation is good if customer can solve problem
mt for customer support websites [int10]

overall customer satisfaction: 75% for english   chinese
95% reduction in cost
project cycle from 10 days to 1 day
customers in china using mt texts were more satis   ed
with support than natives using original texts (68%)!

id151

65 / 71

introduction

smt

evaluation

success stories

conclusions

other uses of mt+pe

wipo: mt for patent translation

patentscope: customised moses

https://www3.wipo.int/patentscope/translate/translate.jsf

id151

66 / 71

introduction

smt

evaluation

success stories

conclusions

other uses of mt+pe

wipo: mt for patent translation

patentscope: customised moses

https://www3.wipo.int/patentscope/translate/translate.jsf

united nations uses same work   ow as wipo

id151

66 / 71

introduction

smt

evaluation

success stories

conclusions

other uses of mt+pe

wipo: mt for patent translation

patentscope: customised moses

https://www3.wipo.int/patentscope/translate/translate.jsf

united nations uses same work   ow as wipo
european commission

customised moses for all eu languages [ec-13]
technology free of charge to any pa in an eu country, or
in an eu institution or agency

http://ec.europa.eu/dgs/translation/translationresources/machine_translation/index_en.htm

id151

66 / 71

introduction

smt

evaluation

success stories

conclusions

mt for communication

skype translator

microsoft   s speech to speech translation
pipeline:

data cleaning
pre-processing (id39)
id103
smt
speech generation

strong interaction component

https://www.microsoft.com/translator/skype.aspx

id151

67 / 71

introduction

smt

evaluation

success stories

conclusions

outline

1

introduction

2 smt

3 evaluation

4 success stories

5 conclusions

id151

68 / 71

introduction

smt

evaluation

success stories

conclusions

conclusions

(cheap) translation is in high demand, which cannot be
supplied by human translators
mt quality is good for some languages, types of texts,
applications

id151

69 / 71

introduction

smt

evaluation

success stories

conclusions

conclusions

(cheap) translation is in high demand, which cannot be
supplied by human translators
mt quality is good for some languages, types of texts,
applications
popular approaches:

most language pairs: phrase-based smt is su   cient
language pairs with long-distance reordering:
syntax-based smt does better

id151

69 / 71

introduction

smt

evaluation

success stories

conclusions

conclusions

(cheap) translation is in high demand, which cannot be
supplied by human translators
mt quality is good for some languages, types of texts,
applications
popular approaches:

most language pairs: phrase-based smt is su   cient
language pairs with long-distance reordering:
syntax-based smt does better

possible improvements from:

more information: linguistic (local and global),
contextual
better methods: fully discriminative, deciphering,
dnns, exact search

id151

69 / 71

introduction

smt

evaluation

success stories

conclusions

conclusions

(cheap) translation is in high demand, which cannot be
supplied by human translators
mt quality is good for some languages, types of texts,
applications
popular approaches:

most language pairs: phrase-based smt is su   cient
language pairs with long-distance reordering:
syntax-based smt does better

possible improvements from:

more information: linguistic (local and global),
contextual
better methods: fully discriminative, deciphering,
dnns, exact search

soa?

id151

69 / 71

introduction

smt

evaluation

success stories

conclusions

conclusions

(cheap) translation is in high demand, which cannot be
supplied by human translators
mt quality is good for some languages, types of texts,
applications
popular approaches:

most language pairs: phrase-based smt is su   cient
language pairs with long-distance reordering:
syntax-based smt does better

possible improvements from:

more information: linguistic (local and global),
contextual
better methods: fully discriminative, deciphering,
dnns, exact search

soa? dnns are a promising direction to better model
context, long-distance dependencies

id151

69 / 71

introduction

smt

evaluation

success stories

conclusions

id151

lucia specia

l.specia@sheffield.ac.uk

lxmls 2015
18 july 2015

id151

70 / 71

introduction

smt

evaluation

success stories

conclusions

references i

autodesk.
translation and post-editing productivity.
in http: // translate. autodesk. com/ productivity. html , 2011.

machine translation.
in anabela pereira, editor, languages and translation, http: // ec. europa. eu/ dgs/ translation/
publications/ magazines/ languagestranslation/ documents/ issue_ 06_ en. pdf . directorate-general
for translation, 2013.

intel.
being streetwise with machine translation in an enterprise neighborhood.
in http: // mtmarathon2010. info/ jec2010_ burgett_ slides. pptx , 2010.

id151

71 / 71

