unsupervised and cross-lingual induction    
of semantic representations	


ivan titov 
 
joint work with alex klementiev, 
mike kozhevnikov,  ashutosh modi and binod bhattarai  
 

why semantic representations? 
 
id53 about knowledge in a collection of biomedical publications: 

 

question:

 what does cyclosporin a suppress? 

answer: 

 expression of egr-2 

sentence: 

 as with egr-3 , expression of egr-2 was blocked by cyclosporin a . 

 

question: 

 what inhibits tnf-alpha? 

answer: 

 il -10 

sentence: 

  our previous studies in human monocytes have demonstrated that interleukin ( il ) -10 
inhibits lipopolysaccharide ( lps ) -stimulated production of inflammatory cytokines , il-1 
beta , il-6 , il-8 , and tumor necrosis factor alpha by blocking gene transcription . 

we need to abstract away from speci   c syntactic and lexical realizations	


2 

why cross-lingual semantic representations?  

}      

}      

improvements for individual languages  
}       crosslingual learning has been successful in syntax [kuhn, 2004; snyder et. al., 

2009; mcdonald et al., 2011] and morphology [snyder and barzilay, 2008] 

crosslingual (unknown) regularities 
provide a signal for learning 

}       should be even more beneficial for inducing semantics, as semantics is generally 

better preserved in translation 

can encode directly to drive learning: e.g. 
one-to-one correspondences between 
semantic representations 

induced semantic relationships across multiple languages 
}      

immediately useful for multilingual problems such as machine translation, 
multilingual web search,  annotation projection across languages,     

3 

outline  

joint induction of frames and roles 

induction of events and their participants 
}       unsupervised models of semantic roles
  
}      
}       cross-lingual extension and comparison with projection and transfer 
induction of semantic representations of words (and phrases) 
}       cross-lingual induction as id72 
}       evaluation (document classification,  lexicon induction) 

}      

}      

 
 

4 

representing events and their participants 

}       a semantic frame [fillmore 1968] is a conceptual structure describing 

a situation, object, or event along with associated properties and 
participants 

 
}       example:   closure / opening frame 
            jack opened the lock with a paper clip 
     semantic roles (aka frame elements): 

       agent     an initiator/doer in the event [who?] 
       patient  -  an affected entity  [to whom / to what?] 
       instrument     the entity manipulated to accomplish the goal 

other roles for closure/opening frame: beneficiary, fastener, degree, 
circumstances, manipulator, portal,     

 

5 

syntax-semantics interface 

}       though syntactic and lexical representations are often predictive of the 

predicate argument structure, this relation is far from trivial: 
(1) john broke the window                          (4)  john busted the window 

(2) the window broke                                 (5)  the window was destroyed by john  

(3) the window was broken by john             (6)  john tore down the window 

  

          

semantic roles: 

alternations 

        agent     an initiator/doer in the event [who?] 

       patient  -  an affected entity  [to whom / to what?] 

the same relation is 
encoded by different 
predicates (incl.  a 
multiword expression) 

 

 

supervised learning of semantic representations is challenging: 
datasets provide low coverage, are domain-specific and available 

only for a few languages 

6 

our task 
}       semantics is encoded by semantic dependency graphs [johansson, 2008] 

wearer

clothing

mary

wore

a

dress

wearing

}       arguments often evoke their own frames 
clothing
 

wearer

creator

for simplicity we assume that all of 
them evoke frames 

style

mary
person

wore

an

wearing

evening
occasion

dress

garment

from cardin 
brand

}       arguments and predicates often expressed by multiword expressions 

 

speaker

message

created entity

 

peter the great

gave an order

to 

build 

the 

castle 

person

request

construction

buildings

induce these representations automatically from unannotated texts	


7 

our task 
}       semantics is encoded by semantic dependency graphs [johansson, 2008] 

12

mary

wore
 573

5

a

dress

}       arguments often evoke their own frames 
 

12

5

for simplicity we assume that all of 
them evoke frames 

21

7 

mary
1121

wore
573

an

evening

445

dress
897

from cardin 
1621

}       arguments and predicates often expressed by multiword expressions 

 

9

3

11

 

peter the great

gave an order

to 

1121

621

build 
2395

the 

castle 
3

induce these representations automatically from unannotated texts	


8 

induction of frame-semantic information 

}       the semantic induction task involves 3 sub-tasks 

}       construction of a transformed syntactic dependency graph (~ argument identification) 

 

 

peter the great

gave     

an order     

 to

 build

 a

wooden

fortified

castle

9 

induction of frame-semantic information 

}       the semantic induction task involves 3 sub-tasks 

}       construction of a transformed syntactic dependency graph (~ argument identification) 
}      

induction of frames (and clusters of arguments) 

 

peter the great

gave     

an order     

 to

 build

person

request

construction

 a

wooden
material

fortified

being_protected

castle
buildings

10 

induction of frame-semantic information 
handled with a simple 
heuristic or a simple 
classifier 

}       the semantic induction task involves 3 sub-tasks 

}       construction of a transformed syntactic dependency graph (~ argument identification) 
}      

induction of frames (and clusters of arguments) 

}       role induction 

 

 

we model these sub-tasks jointly 
within our bayesian model  

speaker

message

created entity

material

type

peter the great

gave     

an order     

 to

 build

person

request

construction

 a

wooden
material

fortified

being_protected

castle
buildings

different from much of previous work where each 

subtask is tackled in isolation	


11 

induction of semantic roles:  definition 
         though after argument and semantic class identification and we know where 

arguments are, we do not know their semantic roles 

         the step can be regarded as id91 of argument occurrences for a given 

semantic class 

john

taught

teaching

linguistics

to the

students

dave

taught

the students

machine learning

teaching

the 

attendants

were

taught

how to fly

teaching

 

 
 

12 

induction of semantic roles:  definition 
         though after argument and semantic class identification and we know where 

arguments are, we do not know their semantic roles 

         the step can be regarded as id91 of argument occurrences for a given 

semantic class 

role 1

john

taught

teaching

role 2

role 3

linguistics

to the

students

we need to    color    them 

role 1

role 3

role 2

dave

taught

the students

machine learning

teaching

role 3

role 2

the 

attendants

were

taught

how to fly

teaching

         the search space is huge     in realistic datasets frequents semantic classes appear 
13 

tens of thousands times 
 

role labeling as id91 of argument keys 

[lang and lapata, 2011b, titov and klementiev, 2011] 

}       associate argument occurrences with syntactic signatures or argument keys 

}       will include simple syntactic cues such as verb voice and position relative to predicate  

role 1

mary

role 1

role 2

active:right:pmod_up 

climbed

up

mont ventoux

role 2

active:right:obj 

mary

climbed

mont ventoux

we assume the 
automatic syntactic 
analyses are available 

purity of around 90% 

}       argument keys are designed to map to a single semantic role as much as possible (for an 

individual predicate) 

all occurrences with the same key are 
automatically in the same cluster 

 

instead of id91 argument occurrences, the method clusters their argument keys	


}       here, we would cluster active:right:obj and active:right:pmod_up together 

14 

a bayesian model for role labeling 

[titov and klementiev, 2012a] 

}      

idea: propose a generative model for inducing argument clusters 
}       clusters are of argument keys, not argument occurrences 

 
}       learning signals: 

}       selection preferences 

i.e. distribution of argument 
fillers is sparse for every role 

}       duplicate roles are unlikely to occur.  e.g. this id91 is a bad idea: 

john taught students math	


gb-criterion 

}       syntax is predictive of roles 

}       how can we encode these signals in a generative story? 

15 

a bayesian model for role labeling 

[titov and klementiev, 2012a] 

f or each predicate p = 1, 2,       :
f or each occurrence l of p :
f or every role r 2 bp :
if [n     u nif (0, 1)] = 1 :
genargument(p, r)
while [n      p,r] = 1 :

genargument(p, r)

at least one 
argument 

draw first 
argument 

continue 
generation 

draw more 
arguments 

decide on arg 
key id91 

for each predicate p = 1, 2, . . . :
bp     crp (   )

passive:left:obj
passive:left:obj
passive:left:obj

passive:right:sbj

role 1
role 1
role 1
role 1

role 3

the
the

window
window

was
was
was
was
was

opened
opened
opened
opened
opened

by the wind

genargument(p, r)

kp,r     u nif (1, . . . ,|r|)
xp,r        p,r

for each predicate p = 1, 2, . . . :

for each role r 2 bp:

   p,r     dp ( , h (a))
 p,r     beta(   0,    1)

draw argument 
key 

draw argument 
filler 

16 

propbank (conll 08) 

[titov and klementiev, 2012a] 

 
id91 f1, harmonic mean of purity and 
collocation 
 

gold syntax 

predicted syntax 

 
 

85#

syntactic baseline 

80#

75#

70#

syntf#

graphpart#

splitmerge#

bayes#

bayes##
(coupled)#

previous 
approaches 

85#

80#

75#

70#

syntf#

graphpart#

splitmerge#

bayes#

bayes##
(coupled)#

17 

a bayesian model for role labeling 

[titov and klementiev, 2012a] 

}       the approaches we discussed induce roles for each predicate independently 
}       these id91s define permissible alternations 
}       but many alternations are shared across verbs 

or changes in the syntactic 
realizations of the argument 
structure of the verb 

john gave the book to mary 	

mike threw the ball to me	


vs 
vs

	

john gave mary the book	

	

mike threw me the ball	


dative alternation 

}       can we share this information across verbs? 

18 

a bayesian model for role labeling  

[titov and klementiev, 2012a] 

}      

idea: keep track of how likely a pair of argument keys should be clustered 
}       define a similarity matrix (or similarity graph) 

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

pass:left:sbj

pass:left:sbj

similarity score between 
pass:left:sbj and 
act:right:obj 

19 

a bayesian model for role labeling 

[titov and klementiev, 2012a] 

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

...

...

...

...

pass:left:sbj

pass:left:sbj

open 

overtake 

20 

a bayesian model for role labeling 

[titov and klementiev, 2012a] 

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

...

...

...

...

pass:left:sbj

pass:left:sbj

open 

overtake 

21 

a bayesian model for role labeling 

[titov and klementiev, 2012a] 

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

...

...

...

...

pass:left:sbj

pass:left:sbj

open 

overtake 

22 

a formal way to encode this: dd-crp 
}       can use crp to define a prior on the partition of argument keys: 

}       the first customer (argument key) sits the first table (role) 
}       m-th customer sits at a table according to: 
 
 
 
 
 

p(previously occupied table k|fm 1,    ) / nk
p(next unoccupied table|fm 1,    ) /    
3

5

7

2

4

. . . 

state of the restaurant 
once m-1 customers 
are seated 

encodes rich-get-richer 
dynamics but not much 
more than that 

1

6

}       an extension is distance-dependent crp (dd-crp): 
}       m-th customer chooses a customer to sit with according to: 
 
p(di   erent customer j|d,    ) / dm,j

p(itself|d,    ) /    

entire similarity graph 

similarity between 
customers m and j 

1

2

3

4

5

6

7

23 

sharing roles 

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

pass:left:sbj

}       similarity graph d to couples distinct but similar id91s of argument keys across 

predicates 
}       vertices are argument keys 
}       weights are similarity scores for each pair of argument keys 

}       we treat d as a latent random variable drawn from a prior over weighted graphs 

}       first drawn from a prior 
}       used to generate each of the id91s for every predicate 

}       we induce d automatically within the model 

}       this is in contrast to all the previous work on dd-crp where similarities were used to encode 

prior knowledge 

24 

propbank (conll 08) 

[titov and klementiev, 2012a] 

 
 

85#

syntactic baseline 

gold syntax 

predicted syntax 

80#

75#

70#

syntf#

graphpart#

splitmerge#

bayes#

bayes##
(coupled)#

previous 
approaches 

85#

80#

75#

70#

syntf#

graphpart#

splitmerge#

bayes#

bayes##
(coupled)#

25 

[titov and klementiev, 2012a] 

qualitative 
looking into induced graph encoding    priors    over id91 arguments 
keys, the most highly ranked pairs encode (or partially encode) 
 
}       passivization 
}       near-equivalence of subordinating conjunctions and prepositions 

encoded as (active:right:obj_if, 
active:right:obj_whether) 

}       e.g., whether and if 

}       benefactive alternation 

martha carved a doll for the baby 
martha carved the baby a doll 

}       dativization 

i gave the book to mary 
i gave mary the book 

}       recovery of unnecessary splits introduced by argument keys 

 
 

 

26 

a bayesian model for role labeling 

[titov and klementiev, 2012a] 

pass:left:sbj

supervised data  

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

...

...

...

...

pass:left:sbj

pass:left:sbj

open 

overtake 

27 

a bayesian model for role labeling 

[titov and klementiev, 2012b] 

pass:left:sbj

supervised data  

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:rig ht:o bj

act:left:sbj

pass:rig ht:lgs-by

...

pass:left:sbj

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

act:right:obj

act:left:sbj

pass:right:lgs-by

...

...

...

...

...

...

...

...

...

...

...

pass:left:sbj

pass:left:sbj

open 

overtake 

28 

propbank (conll 09) 

[titov and klementiev, 2012b] 

l

 
)
s
e
o
r
 
l
l
a
(
 
1
f
 
g
n
i
r
e
t
s
u
c

l

92#

91#

90#

89#

88#

87#

86#

85#

84#

83#

82#

bayes#(semisupervised)#
bayes#
supervised#
syntf#

50#

100#

200#

400#

800#

1600#

3200#

number of annotated sentences 

29 

propbank (conll 09) 

[titov and klementiev, 2012b] 

 
)
5
g
r
a
-
2
g
r
a

(
 
1
f
 
g
n
i
r
e
t
s
u
c

l

92#

90#

88#

86#

84#

82#

80#

50#

bayes#(semisupervised)#
bayes#
supervised#
syntf#

100#

200#

400#

800#

1600#

3200#

number of annotated sentences 

30 

outline  

joint induction of frames and roles 

induction of events and their participants 
}       unsupervised models of semantic roles
  
}      
}       cross-lingual extension and comparison with projection and transfer 
induction of semantic representations of words and phrases 
}       cross-lingual induction as id72 
}       evaluation (document classification,  lexicon induction) 

}      

}      

 
 

31 

induction of frames  / semantic classes 

 

 
        

        

speaker

message

created entity

peter the great

gave an order

to 

build 

the 

castle 

person

request

construction

buildings

induction of frames and induction of argument clusters     the same task 
         we will refer to both of them as semantic classes 
induction of semantic classes involves: 
         id91 of lexemes with similar meaning 

         break, bust,  destroy  should be clustered together 

         detection of multi-word expression, i.e. expressions which are not (sufficiently) 

compositional   
         these includes idiomatic expressions, terminology, proper nouns,     

         e.g.,  hold a victory over,   red herring  

 

later, they can be clustered with atomic ones. 
e.g., win +  held a victory over 

32 

generalization of the role induction model 

[titov and klementiev, 2011] 

}       the model can be generalized for joint induction of predicate-argument 

structure of an entire sentence 
}       start with a (transformed) syntactic dependency graph (~ argument identification) 

 

 

peter the great

gave     

an order     

 to

 build

 a

wooden

fortified

castle

33 

generalization of the role induction model 

[titov and klementiev, 2011] 

}       the model can be generalized for joint induction of predicate-argument 

structure of an entire sentence 
}       start with a (transformed) syntactic dependency graph (~ argument identification) 
}       predict decomposition and labeling of its parts 

}      

}      

 

 

label on nodes are frames (or semantic classes of arguments) 

labels on edges are roles (frame elements) 

speaker

message

created entity

material

type

peter the great

gave     

an order     

 to

 build

person

request

construction

 a

wooden
material

fortified

being_protected

castle
buildings

34 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

35 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

request

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

36 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

gave     an order     

request

draw synt/lex 
realization 

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

{ we use hierarchical dirichlet processes to 
represent distributions over tree fragments } 

37 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

speaker

gave     an order     

request

draw synt/lex 
realization 

at least one 
argument 

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

38 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

draw synt/lex 
realization 

at least one 
argument 

draw first 
argument 

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

speaker

gave     an order     

request

genargument(c, t)

ac,t      c,t
c0c,t      c,t
gensemclass(c0c,t)

39 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

draw synt/lex 
realization 

at least one 
argument 

draw first 
argument 

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

active:left:sbj

speaker

gave     an order     

request

genargument(c, t)

ac,t      c,t
c0c,t      c,t
gensemclass(c0c,t)

draw argument 
key 

40 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

draw synt/lex 
realization 

at least one 
argument 

draw first 
argument 

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

active:left:sbj

speaker

gave     an order     

person

request

genargument(c, t)

ac,t      c,t
c0c,t      c,t
gensemclass(c0c,t)

draw argument 
key 

draw semantic 
class for arg 

41 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

active:left:sbj

speaker

peter the great

gave     

an order     

person

request

draw synt/lex 
realization 

at least one 
argument 

draw first 
argument 

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

genargument(c, t)

ac,t      c,t
c0c,t      c,t
gensemclass(c0c,t)

draw argument 
key 

draw semantic 
class for arg 

recurse 

42 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

active:left:sbj

speaker

peter the great

gave     

an order     

person

request

draw synt/lex 
realization 

at least one 
argument 

draw first 
argument 

continue 
generation 

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

genargument(c, t)

ac,t      c,t
c0c,t      c,t
gensemclass(c0c,t)

draw argument 
key 

draw semantic 
class for arg 

recurse 

43 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

draw synt/lex 
realization 

at least one 
argument 

draw first 
argument 

continue 
generation 

draw more 
arguments 

active:left:sbj active:right:obj

speaker

message

peter the great

gave     

an order     

 to

person

request

genargument(c, t)

ac,t      c,t
c0c,t      c,t
gensemclass(c0c,t)

 build
constr.

draw argument 
key 

draw semantic 
class for arg 

recurse 

44 

the joint model 

draw semantic 
class for root 

f or each sentence :

croot      root
gensemclass(croot)

gensemclass(c)

s      c
f or each role t = 1, . . . , t :
if [n      c,t] = 1 :

genargument(c, t)
while [n      +

c,t] = 1 :

genargument(c, t)

draw synt/lex 
realization 

at least one 
argument 

draw first 
argument 

continue 
generation 

draw more 
arguments 

active:left:sbj active:right:obj

speaker

message

active:right:obj
created entity

-:left:nmod

type

peter the great

gave     

an order     

 to

person

request

genargument(c, t)

ac,t      c,t
c0c,t      c,t
gensemclass(c0c,t)

 a

 build
constr.

fortified
protected

castle

buildings

draw argument 
key 

draw semantic 
class for arg 

recurse 

45 

id136 
}      
}       we use a metropolis-hastings split-merge sampler with the following 

id136 is challenging as the search space is huge 

i=1 = arg max

{   mi}n

{mi}n

p (mi, xi|   )p (   )d   

i=1 z nyi=1

types of moves (   relabelings   ) 
}       role-syntax alignment 

}       choose  a new id91 of argument keys for a frame 

}       split     merge 

break + bust 

}       merge 2 semantic classes  together or split one class in two 

}       compose-decompose 

held + a victory = held a victory 
}       compose fragments of syntactic tree to form a new realization or split a 

fragment 

 

the similarity graph is also periodically updated 

46 

application-based evaluation 
id53 about knowledge in a corpus of biomedical abstracts 

}       dataset: 1999 biomedical abstracts from the genia corpus (kim et al, 2003) 
}       examples of induced semantic classes: 

class 
1 

variations 
motif, sequence, regulatory element, response element, element, dna sequence 

2 

3 

4 

5 

6 

7 

8 

9 

blood cells 

roughly    cause 
change position 
on a scale    frame 

donor, individual, subject  

important, essential, critical 

dose, concentration 

activation, transcriptional activation, transactivation 

b cell, t lymphocyte, thymocyte, b lymphocyte, t cell, t-cell line,  human lymphocyte, t-
lymphocyte 

indicate, reveal, document, suggest, demonstrate  

augment, abolish, inhibit, convert, cause, abrogate, modulate, block, decrease, reduce, 
diminish, suppress, up-regulate, impair, reverse, enhance  

 confirm, assess, examine, study, evaluate, test, resolve, determine, investigate  

10 

nf-kappab, nf-kappa b, nfkappab, nf-kb  

47 

application-based evaluation 
id53 about knowledge in a corpus of biomedical abstracts 

[poon and domingos, 2008] 

}       example questions and answers: 

 

question:

 what does cyclosporin a suppress? 

answer: 

 expression of egr-2 

sentence: 

 as with egr-3 , expression of egr-2 was blocked by cyclosporin a . 

 

question: 

 what inhibits tnf-alpha? 

answer: 

 il -10 

sentence: 

  our previous studies in human monocytes have demonstrated that interleukin ( il ) -10 
inhibits lipopolysaccharide ( lps ) -stimulated production of inflammatory cytokines , il-1 
beta , il-6 , il-8 , and tumor necrosis factor alpha by blocking gene transcription . 

48 

application-based evaluation 
id53 about knowledge in a corpus of biomedical abstracts 

more than 55% of mistakes are due 
to over coarse id91 in 3 
semantic classes (antonymy / 
hyponymy) 

key word 
matching 

information extraction models 

poon and domingos 

this work 

49 

application-based evaluation 
id53 about knowledge in a corpus of biomedical abstracts 

more than 55% of mistakes are due 
to over coarse id91 in 3 
semantic classes (antonymy / 
hyponymy) 

see also our results on framenet [modi et al., ils naacl 12]  
 

key word 
matching 

information extraction models 

poon and domingos 

this work 

50 

outline  

joint induction of frames and roles 

induction of events and their participants 
}       unsupervised models of semantic roles
  
}      
}       cross-lingual extension and comparison with projection and transfer 
induction of semantic representations of words and phrases 
}       cross-lingual induction as id72 
}       evaluation (document classification,  lexicon induction) 

}      

}      

 
 

51 

crosslingual induction of semantic roles 

}       we have additional multilingual resources: texts translated in multiple 

languages (parallel data) 
}       parliament proceedings, books, etc. 
}       can use standard machine translation techniques to induce word alignments 

 

peter

beschuldigte

mary

einen diebstahl zu planen

peter

blamed

planning a theft

on mary

}       we use aligned data and induce semantics jointly in multiple languages 

}       only during learning,  we apply them to monolingual sentences 

52 

crosslingual induction of semantic roles 
}       consider an example blame alternation 

[titov and klementiev,  acl 2012] 

cognizer

reason

peter

blamed

mary

for

planning

a theft

evaluee

cognizer

evaluee

reason

peter

blamed

planning a theft

on mary

}       learning the corresponding linking is not trivial 

}       selectional preferences for all roles are not very restrictive 
}       selectional restricutions for cognizer and evaluee are overlapping 
 

53 

crosslingual induction of semantic roles 
}       consider an example blame alternation 

[titov and klementiev,  acl 2012] 

role a

role b

peter

blamed

mary

for

planning

a theft

role c

role a

role c

role b

king james

blamed

the coup

on guy fawkes

}       learning the corresponding linking is not trivial 

}       selectional preferences for all roles are not very restrictive 
}       selectional restricutions for cognizer and evaluee are overlapping 

54 

crosslingual induction of semantic roles 

cognizer

reason

peter

blamed

mary

for

planning

a theft

evaluee

cognizer

evaluee

reason

peter

blamed

planning a theft

on mary

}       however, the alternation does not transfer to german 

}       both forms are likely to have the same translation 

 

cognizer

reason

evaluee

peter

beschuldigte

mary

einen diebstahl zu planen

55 

crosslingual induction of semantic roles 
}       we want induced roles for aligned sentences to be consistent 

}       favoring one-to-one mapping between aligned roles in both languages 

role 1

role 2

peter

beschuldigte

role 3

mary

role 1

role 2

role 3

einen diebstahl zu planen

k  nig jacob

beschuldigte

guy fawkes

   einen coup zu planen

peter

blamed

mary

for

planning

a theft

king james

blamed

the coup

on guy fawkes

role a

role c

role b

standard mt 
alignments 

role a

role b

role c

56 

crosslingual induction of semantic roles 
}       we want induced roles for aligned sentences to be consistent 

}       favoring one-to-one mapping between aligned roles in both languages 

role 1

role 2

peter

beschuldigte

role 3

mary

role 1

role 2

role 3

einen diebstahl zu planen

k  nig jacob

beschuldigte

guy fawkes

   einen coup zu planen

peter

blamed

mary

for

planning

a theft

king james

blamed

the coup

on guy fawkes

role a

role c

role b

consistent roles:   
a to 1 

role a

role b

role c

57 

crosslingual induction of semantic roles 
}       we want induced roles for aligned sentences to be consistent 

}       favoring one-to-one mapping between aligned roles in both languages 

role 1

role 2

peter

beschuldigte

role 3

mary

role 1

role 2

role 3

einen diebstahl zu planen

k  nig jacob

beschuldigte

guy fawkes

   einen coup zu planen

peter

blamed

mary

for

planning

a theft

king james

blamed

the coup

on guy fawkes

role a

role c

role b

role b

role c

role a

consistent roles:   
a to 1 
b to 2 

58 

crosslingual induction of semantic roles 
}       we want induced roles for aligned sentences to be consistent 

}       favoring one-to-one mapping between aligned roles in both languages 

role 1

role 2

peter

beschuldigte

role 3

mary

role 1

role 2

role 3

einen diebstahl zu planen

k  nig jacob

beschuldigte

guy fawkes

   einen coup zu planen

peter

blamed

mary

for

planning

a theft

king james

blamed

the coup

on guy fawkes

role a

role c

role b

role a

role b

role c

consistent roles:   
a to 1 
b to 2 
c to 3 

should be favored 

59 

crosslingual induction of semantic roles 
}       we want induced roles for aligned sentences to be consistent 

}       favoring one-to-one mapping between aligned roles in both languages 

 

role 1

role 2

peter

beschuldigte

role 3

mary

einen diebstahl zu planen

k  nig jacob

beschuldigte

guy fawkes

   einen coup zu planen

role 1

role 2

role 3

peter

blamed

mary

for

planning

a theft

king james

blamed

the coup

on guy fawkes

role a

role c

role b

not as good: 
a to 1 
b to 2 or 3 
c to 3 or 2 

role c

role b

role a

should be penalized 

60 

crosslingual induction of semantic roles 
}       we want induced roles for aligned sentences to be consistent 

}       favoring one-to-one mapping between aligned roles in both languages 

role 1

role 2

peter

beschuldigte

role 3

mary

role 1

role 2

role 3

einen diebstahl zu planen

k  nig jacob

beschuldigte

guy fawkes

   einen coup zu planen

peter

blamed

mary

for

planning

a theft

king james

blamed

the coup

on guy fawkes

role a

role c

role b

role a

role b

role c

}       in our example: roles induced for german will be transferred to english 

resulting in perfect accuracy on both languages 

}       model extension (see titov and klementiev [acl  2012]): 

recall the 

dipanjan's talk 

}      

formulated as posterior id173 [ganchev et al., 10, mccallum et al, 08]. 
61 

crosslingual semantic role induction 

}       experimental setup: 

induced jointly in two languages for predicates aligned in parallel data 

}      
}       parallel data is used only to constrain the model to get fair comparison 

90#

85#

80#

75#

70#

crosslingual (english/german) 

2% improvement for 
german, little for english 

english#
german#

monobayes#

mul1bayes#

syntf#

62 

do we need unsupervised induction? 
}       recall the dipanjan's talk on saturday: 

crosslingual projection and (forms of) model transfer substantially 
outperform unsupervised induction of syntax / pos tags 

 

 

63 

do we need unsupervised induction? 
}       recall the dipanjan's talk on saturday: 

 

crosslingual projection and (forms of) model transfer substantially 
outperform unsupervised induction of syntax / pos tags 

}       annotation projection: 

}       project annotation from the source language to the target language 

[pado and lapata, 2005; johansson and nugues, 2006;  pado and pitel, 
2007; tonelli and pianta, 2008,   ]  

theme

departing

peter and mary left.

peter und auch maria  gingen.

departing

 

64 

do we need unsupervised induction? 
}       recall the dipanjan's talk on saturday: 

 

crosslingual projection and (forms of) model transfer substantially 
outperform unsupervised induction of syntax / pos tags 

}       annotation projection: 

}       project annotation from the source language to the target language 

[pado and lapata, 2005; johansson and nugues, 2006;  pado and pitel, 
2007; tonelli and pianta, 2008,   ]  

theme

departing

peter and mary left.

peter und auch maria  gingen.

theme

departing

 

65 

do we need unsupervised induction? 
}       recall the dipanjan's talk on saturday: 

 

crosslingual projection and (forms of) model transfer substantially 
outperform unsupervised induction of syntax / pos tags 

}       annotation projection: 

}       project annotation from the source language to the target language 

[pado and lapata, 2005; johansson and nugues, 2006;  pado and pitel, 
2007; tonelli and pianta, 2008,   ]  

}       model transfer: 

}       apply a source srl model to the target language (maybe with some 

adaptation) 

[kozhevnikov and titov, 2013;  kozhevnikov and titov, 2014]  

66 

induction vs. transfer 

 

90 

85 

80 

75 

70 

[kozhevnikov and titov, 2013]  

induction 
transfer 

results for projection 
not included but are 

lower 

en-->zh 

zh-->en 

en-->cz 

en-->fr 

the situation is quite different from the one for syntax / pos tags 

}       why? 

}       divergences in semantic formalism across languages 
}       semantics is more tied to lexical information so harder even for supervised 

methods 

67 

 

 

outline  

joint induction of frames and roles 

induction of events and their participants 
}       unsupervised models of semantic roles
  
}      
}       cross-lingual extension and comparison with projection and transfer 
induction of semantic representations of words (and phrases) 
}       cross-lingual induction as id72 
}       evaluation (document classification,  lexicon induction) 

}      

}      

 
 

68 

why not id91 as before? 

id91 

distributed  (= latent features) 

}       cluster words into (hierarchical) 

clusters 

}       words defined by cluster 

prototypes 

how to choose 
granularity? 

many incompatible ways 
to cluster are often 
possible 

sector

market

economy

stock

president

king

prince

minister

}       dense embedding 

can encode different 
levels of granularity 

can encode multiple 
incompatible id91s 
(or multiple senses) 

easier to deal with 
compositionality 
(generalizing to phrases) 

technology

energy

steel

oil

69 

summary of our approach 

sector

president

stahl

economy

market

technology

prince

telekommunikation

verk  ufer

energy

minister

markt

steel

au  enminister

stock

pr  sidenten

pr  sident

sektor

fonds

  l

oil

king
benzin

70 

summary of our approach 
}       use cheap monolingual data to induce the representation within each language 

president

king

prince

minister

sector

market

economy

stock

markt

fonds

verk  ufer

sektor

technology

energy

steel

oil

telekommunikation

  l
stahl

benzin

au  enminister

pr  sident

pr  sidenten

71 

summary of our approach 
}       while using parallel data to bias representations to be similar for translated words 

president

king

prince

minister

sector

market

economy

stock

markt

fonds

verk  ufer

sektor

technology

energy

steel

oil

telekommunikation

  l
stahl

benzin

au  enminister

pr  sident

pr  sidenten

72 

summary of our approach 
}       semantically similar words are    close    to one another irrespective of language 

sector

president

king
pr  sident

prince

au  enminister

minister

pr  sidenten

market

verk  ufer

sektor
stock

economy
markt

fonds

energy

technology

benzin
oil
stahl
  l

steel
telekommunikation

}       treat it as multitask learning (mtl) 

}       treat words as individual tasks 
}       task relatedness is derived from co-occurrence statistics in bilingual parallel data  

73 

background: multitask learning 

}       we consider a particular mtl setup [cavallanti et al. (2010)] 
}       consider k related tasks with a labeled dataset for each task k 
}       learns a classifier (parameterized by                      ) for each task 
}       minimizes the following objective: 

vk, k 2 [1, k]

 

l(v) =

kxk=1

matrix a defines 
inter-task similarity 

1
2

vt (a     im)v

l(k)(vk) +

objectives for each individual task (e.g., 
likelihoods of each dataset) 

regularizer prefers    similar    
parameters for related tasks   

74 

what do we take from mlt? 

(klementiev, titov, bhattarai, coling 2012) 

idea: frame crosslingual distributed representation induction as 
id72 

}       we treat words in both languages as individual tasks 

}       for each word, we learn a representations 

ci 2r d

}       a will be defined by how often words align in parallel data 
}       we will take the multitask regularizer part of the objective 

l(c,    ) =

2xl=1

 

 

l(l)(c,    l) +

1
2

ct (a     im)c

id168 for a 
dataset in every language 

favors similar representations for 
frequently aligned words 

}       applicable to any distributed representation induction set-up 

}       we use neural probabilistic language model (bengio et al, 2003) 

75 

how to encode relatedness? 

}       how can we encode prior knowledge of task (= word) relatedness into a? 

}       represent tasks with an undirected weighted graph h:  

 
}       the graph laplacian l is defined as: 

task 

i

s(i,j)

j

degree of relatedness 
(e.g., how frequently 2 
words are aligned) 

li,j(h) =8<:

 

}      

p(i,k)2e s(i, k)

 s(i, j)
0

if i = j
if (i, j) 2 e
otherwise

interaction matrix is then defined as  
}       a-1 (crucial in learning) encodes the degree of relatedness between the tasks 
}       a is invertible (l is positive semi-definite) 

a = i + l

76 

qualitative evaluation 

77 

(klementiev, titov, bhattarai, coling 2012) 

crosslingual document classification 
}       use distributed representations to train a classifier in one language (l1) 
}       apply to the other language (l2) with no additional training (distribreps) 
}       baselines: 

no training data in l2!!! 

}       train in l1, gloss test documents from l2 to l1 (glossed) 
}       train in l1, translate (phrase-based mt) test documents in l2 to l1 (mt) 

   

0
8

)

%

(
 
y
c
a
r
u
c
c
a
n
o

 

i
t

a
c
i
f
i
s
s
a
c

l

0
7

0
6

0
5

0
4

   

   

   

   

   

100

200

1000

500
2000
training documents

   

distribreps
mt
glossed
majority class
10000

5000

)

%

(
 
y
c
a
r
u
c
c
a
n
o

 

i
t

a
c
i
f
i
s
s
a
c

l

5
7

0
7

5
6

0
6

5
5

0
5

5
4

0
4

   

   

   

   

   

   

100

200

1000

500
2000
training documents

   

distribreps
mt
glossed
majority class
10000

5000

train: en, test: de 

train: de, test: en 

78 

(klementiev, titov, bhattarai, coling 2012) 

crosslingual document classification 
}       use distributed representations to train a classifier in one language (l1) 
}       apply to the other language (l2) with no additional training (distribreps) 
}       baselines: 

no training data in l2!!! 

}       train in l1, gloss test documents from l2 to l1 (glossed) 
}       train in l1, translate (phrase-based mt) test documents in l2 to l1 (mt) 

   

   

   

   

0
8

)

%

(
 
y
c
a
r
u
c
c
a
n
o

 

i
t

a
c
i
f
i
s
s
a
c

l

0
7

0
6

0
5

0
4

100

200

1000

500
2000
training documents

5
7

0
7

a
c
i
f
i
s
s
a
c

l

   

   

   

   

   

   

   

)

%

see also more recent results of  hermann 
and blunsom (2013);  lauly,  boulanger and 
larochelle (2014);  zou, socher, cher and 
manning (2013) 

(
 
y
c
a
r
u
c
c
a
n
o

5
5

0
6

5
6

i
t

 

   

   

distribreps
mt
glossed
majority class
10000

5000

0
5

5
4

0
4

100

200

1000

500
2000
training documents

   

distribreps
mt
glossed
majority class
10000

5000

train: en, test: de 

train: de, test: en 

79 

conclusions 
 
}       we believe that unsupervised induction and its semi-supervised 

extensions are a very promising direction 
 

}       crosslingual learning 

}       enforcing agreement using parallel data 

}       ongoing work: beyond frames semantics: 

}       learning how events are organized in more complex activities (frermann et 

al., 2014; modi and titov, 2014)  

 
}       many questions remaining 

}       more expressive models of alternations; 
}       going beyond sentences; 

       
this work is partially supported by a google research award.   also thanks to manfred pinkal,  
alexis palmer,  ryan mcdonald, caroline sporleder 
 

80 

(some) references 

b. o   connor. 2013. learning frames from text with an unsupervised latent variable model. cmu tr. 
d. das,  n. schneider, d. chen and n. smith.  2010. probabilistic frame-id29. naacl. 
l. frermann, i. titov and m. pinkal.  2014. a hierarchical bayesian model for unsupervised induction of script knowledge.  eacl. 
grenager and c. manning.  2006. unsupervised discovery of a statistical verb lexicon.  emnlp. 
d. kawahara, d. peterson, o. popescu, m. palmer. 2014. inducing example-based semantic frames from a massive amount of verb uses. eacl. 
a. klementiev, i. titov and b. bhattarai. 2012. inducing crosslingual distributed representations of words. coling. 
m. kozhevnikov and i. titov.  2013. crosslingual transfer of semantic role models.  acl.  
m. kozhevnikov and i. titov. 2014. crosslingual model transfer using feature representation projection. acl short. 
j. lang and m. lapata. 2010. unsupervised induction of semantic roles. acl. 
j. lang and m. lapata.  2011b. unsupervised semantic role induction via split-merge id91.  acl,. 
j. lang and m. lapata.  2011a. unsupervised semantic role induction with graph partitioning. emnlp. 
a. modi,  i. titov and a. klementiev, 2012. unsupervised induction of frame-semantic representations.  ils workshop,  naacl. 
s. pado and m. lapata.  2005. cross-linguistic projection of role-semantic information. emnlp. 
s. pado and m. lapata.  2008. crosslingual annotation projection for semantic roles. journal of artificial intelligence research. 
s. pado and m. lapata.  2006. optimal constituent alignment with edge covers for semantic projection.  acl. 
s. pado and g. pitel. 2007.  annotation pr  cise du fran  ais en s  mantique de r  les par projection cross-linguistique. taln. a. palmer and c. 
sporleder.  2010. evaluating framenet-style id29: the role of coverage gaps in framenet. coling. 
l. van deer plas, p. merlo, and j. henderson.  2011. scaling up automatic cross-lingual semantic role annotation. acl. 
h. poon and p.  domingos. 2008.  unsupervised id29. emnlp. 
i. titov and a. klementiev. 2011.  a bayesian model for unsupervised id29.  acl. 
i. titov and a. klementiev.   2012a.  a bayesian approach to unsupervised semantic role induction. eacl. 
i. titov and a. klementiev.  2012b. crosslingual induction of semantic roles,  acl.   
i. titov and a. klementiev.   2012c. semi-supervised id14: approaching from an unsupervised perspective. coling. 
 

81 

