   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]stats and bots
     * [9]data science
     * [10]analytics
     * [11]startups
     * [12]bots
     * [13]design
     * [14]subscribe
     * [15]     cube.js framework
     __________________________________________________________________

machine learning translation and the google translate algorithm

the basic principles of machine translation engines

   [16]go to the profile of daniil korbut
   [17]daniil korbut (button) blockedunblock (button) followfollowing
   aug 1, 2017
   [0*aoe3erun08zcfn1s.]
   [18]google machine translation

   every day we use different technologies without even knowing how
   exactly they work. in fact, it   s not very easy to understand engines
   powered by machine learning. the [19]statsbot team wants to make
   machine learning clear by telling data stories in this blog. today,
   we   ve decided to explore machine translators and explain how the google
   translate algorithm works.
     __________________________________________________________________

   years ago, it was very time consuming to translate the text from an
   unknown language. using simple vocabularies with word-for-word
   translation was hard for two reasons: 1) the reader had to know the
   grammar rules and 2) needed to keep in mind all language versions while
   translating the whole sentence.

   now, we don   t need to struggle so much    we can translate phrases,
   sentences, and even large texts just by putting them in google
   translate. but most people don   t actually care how the engine of
   machine learning translation works. this post is for those who do care.

deep learning translation problems

   if the google translate engine tried to kept the translations for even
   short sentences, it wouldn   t work because of the huge number of
   possible variations. the best idea can be to teach the computer sets of
   grammar rules and translate the sentences according to them. if only it
   were as easy as it sounds.

   if you have ever tried learning a foreign language, you know that there
   are always a lot of exceptions to rules. when we try to capture all
   these rules, exceptions and exceptions to the exceptions in the
   program, the quality of translation breaks down.

     modern machine translation systems use a different approach: they
     allocate the rules from text by analyzing a huge set of documents.

   creating your own simple machine translator would be [20]a great
   project for any data science resume.
   [21]data scientist resume projects
   machine learning problems set to build a data scientist cv without work
   experienceblog.statsbot.co

   let   s try to investigate what hides in the    black boxes    that we call
   machine translators. deep neural networks can achieve excellent results
   in very complicated tasks (speech/visual object recognition), but
   despite their flexibility, they can be applied only for tasks where the
   input and target have fixed dimensionality.

recurrent neural networks

   here is where id137 (lstms) come into play,
   helping us to work with sequences whose length we can   t know a priori.

   lstms are a special kind of recurrent neural network (id56), capable of
   learning long-term dependencies. all id56s look like a chain of
   repeating modules.
   [0*ciugyhtowpmudvhy.]
   [22]unrolled recurrent neural network

   so the lstm transmits data from module to module and, for example, for
   generating ht we use not only xt, but all previous input values x. to
   learn more about structure and mathematical models of lstm, you can
   read the great article    [23]understanding id137.   

bidirectional id56s

   our next step is id182 (bid56s). what
   a bid56 does, is split the neurons of a regular id56 into two directions.
   one direction is for positive time, or forward states. the other
   direction is for negative time, or backward states. the output of these
   two states are not connected to inputs of the opposite direction
   states.
   [1*bacdupa0mgvio2ijluotba.png]
   [24]id182

   to understand why bid56s can work better than a simple id56, imagine that
   we have a sentence of 9 words and we want to predict the 5th word. we
   can make it know either only the first 4 words, or the first 4 words
   and last 4 words. of course, the quality in the second case would be
   better.

sequence to sequence

   now we   re ready to move to sequence to sequence models (also called
   id195). the basic id195 model consist of two id56s: an encoder
   network that processes the input and a decoder network that generates
   the output.
   [0*jf7kcjstpru7iffl.]
   [25]sequence to sequence model

   finally, we can make our first machine translator!

   however, let   s think about one trick. google translate [26]currently
   supports 103 languages, so we should have 103x102 different models for
   each pair of languages. of course, the quality of these models varies
   according to the popularity of languages and the amount of documents
   needed for training this network. the best that we can do is to make
   one nn to take any language as input and translate into any language.

google translate

   that very idea was [27]realized by google engineers at the end of 2016.
   architecture of nn was build on the id195 model, which we have
   already studied.

   the only exception is that between the encoder and decoder there are 8
   layers of lstm-id56 that have residual connections between layers with
   some tweaks for accuracy and speed. if you want to go deeper with that,
   take a look at the article [28]google   s id4
   system.

     the main thing about this approach is that now the google translate
     algorithm uses only one system instead of a huge set for every pair
     of languages.

   the system requires a    token    at the beginning of the input sentence
   which specifies the language you   re trying to translate the phrase
   into.

   this improves translation quality and enables translations even between
   two languages which the system hasn   t seen yet, a method termed
      zero-shot translation.   

what means better translation?

   when we   re talking about improvements and better results from google
   translate algorithms, how can we correctly evaluate that the first
   candidate for translation is better than the second?

   it   s not a trivial problem, because for some commonly used sentences we
   have the sets of reference translations from the professional
   translators, that have, of course, some differences.

   there are a lot of approaches that partly solve this problem, but the
   most popular and effective metric is [29]id7 (bilingual evaluation
   understudy). imagine, we have two candidates from machine translators:

     candidate 1: statsbot makes it easy for companies to closely monitor
     data from various analytical platforms via natural language.

     candidate 2: statsbot uses natural language to accurately analyze
     businesses    metrics from different analytical platforms.

   [0*x_ghrjsw9evfi4hw.]

   although they have the same meaning they differ in quality and have
   different structure.

   let   s look at two human translations:

     reference 1: statsbot helps companies closely monitor their data
     from different analytical platforms via natural language.

     reference 2: statsbot allows companies to carefully monitor data
     from various analytics platforms by using natural language.

   obviously, candidate 1 is better, sharing more words and phrases
   compared to candidate 2. this is a key idea of the simple id7
   approach. we can compare [30]id165s of the candidate with id165s of
   the reference translation and count the number of matches (independent
   from their position). we use only id165 precisions, because
   calculating recall is difficult with multiple refs and the result is
   the geometric average of id165 scores.
     __________________________________________________________________

   now you can evaluate the complex engine of machine learning
   translation. next time when you translate something with google
   translate, imagine how many millions of documents it analyzed before
   giving you the best language version.

   iframe: [31]/media/02231cd5403151a2a463e36cc3b88462?postid=96f0ed8f19e4

you   d also like:

   [32]id126 algorithms
   main existing recommendation engines and how they
   workblog.statsbot.co
   [33]text classifier algorithms in machine learning
   key text classification algorithms with use cases and
   tutorialsblog.statsbot.co
   [34]time series anomaly detection algorithms
   the current state of anomaly detection techniques in plain
   languageblog.statsbot.co

     * [35]machine learning
     * [36]machine translation
     * [37]google translate
     * [38]data science
     * [39]neural networks

   (button)
   (button)
   (button) 642 claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [40]go to the profile of daniil korbut

[41]daniil korbut

   deep learning research engineer at insilico medicine

     (button) follow
   [42]stats and bots

[43]stats and bots

   data stories on machine learning and analytics. from statsbot   s makers.

     * (button)
       (button) 642
     * (button)
     *
     *

   [44]stats and bots
   never miss a story from stats and bots, when you sign up for medium.
   [45]learn more
   never miss a story from stats and bots
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.statsbot.co/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/96f0ed8f19e4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.statsbot.co/machine-learning-translation-96f0ed8f19e4&source=--------------------------nav_reg&operation=register
   8. https://blog.statsbot.co/?source=logo-lo_yqbzjjsu91a9---cfc9f21a543a
   9. https://blog.statsbot.co/datascience/home
  10. https://blog.statsbot.co/analytics/home
  11. https://blog.statsbot.co/startups/home
  12. https://blog.statsbot.co/bots/home
  13. https://blog.statsbot.co/design/home
  14. https://blog.statsbot.co/statsbot-digest-b0d7372f842a
  15. https://cube.dev/
  16. https://blog.statsbot.co/@daniilkorbut?source=post_header_lockup
  17. https://blog.statsbot.co/@daniilkorbut
  18. https://research.googleblog.com/2016/11/zero-shot-translation-with-googles.html
  19. http://statsbot.co/?utm_source=blog&utm_medium=article&utm_campaign=machine_translation
  20. https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6?utm_source=blog&utm_medium=article&utm_campaign=machine_translation
  21. https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6
  22. http://colah.github.io/posts/2015-08-understanding-lstms/
  23. http://colah.github.io/posts/2015-08-understanding-lstms/
  24. https://www.semanticscholar.org/paper/hybrid-speech-recognition-with-deep-bidirectional-graves-jaitly/5807664af8e63d5207f59fb263c9e7bd3673be79
  25. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  26. https://www.newscientist.com/article/2114748-google-translate-ai-invents-its-own-language-to-translate-with/
  27. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  28. https://arxiv.org/abs/1609.08144
  29. https://en.wikipedia.org/wiki/id7
  30. https://en.wikipedia.org/wiki/id165
  31. https://blog.statsbot.co/media/02231cd5403151a2a463e36cc3b88462?postid=96f0ed8f19e4
  32. https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3
  33. https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278
  34. https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2
  35. https://blog.statsbot.co/tagged/machine-learning?source=post
  36. https://blog.statsbot.co/tagged/machine-translation?source=post
  37. https://blog.statsbot.co/tagged/google-translate?source=post
  38. https://blog.statsbot.co/tagged/data-science?source=post
  39. https://blog.statsbot.co/tagged/neural-networks?source=post
  40. https://blog.statsbot.co/@daniilkorbut?source=footer_card
  41. https://blog.statsbot.co/@daniilkorbut
  42. https://blog.statsbot.co/?source=footer_card
  43. https://blog.statsbot.co/?source=footer_card
  44. https://blog.statsbot.co/
  45. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  47. https://blog.statsbot.co/data-scientist-resume-projects-806a74388ae6
  48. https://blog.statsbot.co/recommendation-system-algorithms-ba67f39ac9a3
  49. https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278
  50. https://blog.statsbot.co/time-series-anomaly-detection-algorithms-1cef5519aef2
  51. https://medium.com/p/96f0ed8f19e4/share/twitter
  52. https://medium.com/p/96f0ed8f19e4/share/facebook
  53. https://medium.com/p/96f0ed8f19e4/share/twitter
  54. https://medium.com/p/96f0ed8f19e4/share/facebook
