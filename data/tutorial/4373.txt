   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a non-nlp application of id97

   [16]go to the profile of kwyk
   [17]kwyk (button) blockedunblock (button) followfollowing
   jul 31, 2017

   when using machine learning to solve a problem, having the right data
   is crucial. unfortunately, raw data is often    unclean    and
   unstructured. [18]natural language processing (nlp) practitioners are
   familiar with this issue as all of their data is textual. and because
   most of machine learning algorithms can   t accept raw strings as inputs,
   id27 methods are used to transform the data before feeding it
   to a learning algorithm. but this is not the only scenario where
   textual data arises, it can also take the form of categorical features
   in standard non-nlp tasks. in fact, many of us struggle with the
   processing of these kinds of features, so are id27 of any use
   in this case ?

   this article aims to show how we were able to use id97 ([19]2013,
   mikolov et al.), a id27 technique, to convert a categorical
   feature with a high number of modalities into a smaller set of
   easier-to-use numerical features. these features were not only easier
   to use but also successfully learned relationships between the several
   modalities similar to how classic id27s do with language.
     __________________________________________________________________

id97

     you shall know a word by the company it keeps ([20]firth, j. r.
     1957:11)

   the above is exactly what id97 seeks to do : it tries to determine
   the meaning of a word by analyzing its neighboring words (also called
   context). the algorithm exists in two flavors cbow and skip-gram. given
   a set of sentences (also called corpus) the model loops on the words of
   each sentence and either tries to use the current word of to predict
   its neighbors (its context), in which case the method is called
      skip-gram   , or it uses each of these contexts to predict the current
   word, in which case the method is called    continuous bag of words   
   (cbow). the limit on the number of words in each context is determined
   by a parameter called    window size   .
   [1*ej3wl_gjs1gsdwjhm3ochg.png]
   both id97 architectures. the current word is w(t) and
   w(t-2)..w(t+2) are context words. (mikolov et al. 2013)

   so if we choose for example the skip-gram method, id97 then
   consists of using a shallow neural network, i.e. a neural network of
   only one hidden layer, to learn the id27. the network first
   initializes randomly its weights then iteratively adapt these during
   training to minimize the error it makes when using words to predict
   their contexts. after a hopefully successful training, the word
   embedding for each word is obtained by multiplying the network   s weight
   matrix by the word   s [21]one-hot vector.

     note : besides allowing for a numerical representation of textual
     data, the resulting embedding also learn interesting
     [22]relationships between words and can be used to answer questions
     such as : king is to queen as father is to    ?

   for more details on id97 you can have a look at this [23]stanford
   lecture or this [24]tutorial by tensorflow.

application

   at [25]kwyk we provide math exercises online. teachers assign homework
   to their students and each time an exercise is done some data is
   stored. then, we use the collected data to evaluate the students    level
   and give them tailored review exercises to help them progress. for each
   exercise that is answered we store a list of identifiers that help us
   tell : what is the answered exercise ?, who is the student ? , what is
   the chapter ?    in addition to that, we store a score value that is
   either (0) or (1) depending on the student success. to evaluate the
   students    levels we then simply have to predict this score value and
   get success probabilities from our classifier.

   as you can see, a lot of our features are categorical. usually, when
   the number of modalities is small enough, one can simply transform a
   categorical feature with (n) modalities into (n-1) dummy variables then
   use that for training. but when the number of modalities is in the many
   thousands         as it is the case for some of our features         relying on
   dummy variables becomes inefficient and impracticable.

   in order to address this issue our idea is to use id97 to transform
   categorical features into a relatively small number of usable
   continuous features using a little trick. to illustrate, let   s consider
      exercise_id   , a categorical feature telling us which exercise was
   answered. in order to be able to use id97 we have to provide a
   corpus, a set of sentences to feed to the algorithm. but the raw
   feature         a list of ids         isn   t a corpus per se : the order is
   completely random and closer ids don   t carry any information about
   their neighbors. our trick consists of considering each homework given
   by a teacher as a    sentence   , a coherent list exercise ids. as a
   result, ids are naturally gathered by levels, chapters    and id97
   can start learning exercise embedding directly on that.

   indeed, thanks to these artificial sentences we were able to use
   id97 and get beautiful results :
   [1*wecky4udm8conpntqqihzg.png]
   exercise embedding (3 main components of pca) colored by level. 6e, 5e,
   4e, 3e, 2e, 1e and tm are the french equivalents of the 6th, 7th, 8th,
   9th, 10th, 11th and 12th grades in the us.

   as we can see, the resulting embedding has a structure. in fact, the
   3d-projected cloud of exercises is spiral-shaped with exercises of
   higher levels following directly those of previous levels. this means
   that the embedding successfully learned to distinguish exercises of
   different school levels and regrouped similar exercises together. but
   that   s not all, using a [26]non-linear id84
   technique we were able to reduce the whole embedding into a single real
   valued variable with the same characteristics. in other terms, we
   obtained an exercise complexity feature that is minimum for 6th grade
   exercises and grows as the exercises get more and more complex until it
   is maximum for 12th grade exercises.

   moreover, the embedding also learned relationships between exercises
   just like mikolov   s did with english words :
   [1*dvai95vhymqfyqawpvutuq.png]

   the diagram above shows some examples of the relationships our
   embedding was able to learn. so if we were to ask the question    an
   exercise of number addition is to an exercise of number subtraction as
   an exercise of time addition is to     ?    the embedding gives us the
   answer    an exercise of time subtraction   . concretely, this means that
   if we take the difference embedding[substract(numbers)] -
   embedding[add(numbers)] and add it to the embedding of an exercise
   where students are asked to add time values (hours, minutes    ) then the
   closest embedding is one of an exercise that consists of subtracting
   time values.
     __________________________________________________________________

conclusion

   all in all, id27 techniques are useful to transform textual
   data into real valued vectors which can then be plugged easily into a
   machine learning algorithm. despite being principally used for nlp
   applications such as machine translation, we showed that these
   techniques also have their place for categorical feature processing by
   giving the example of a particular feature we use at kwyk. but in order
   to be able to apply a technique such as id97, one has to build a
   corpus         i.e. a set of sentences where labels are arranged so that a
   context is implicitly created. in our example, we used homework given
   on the website to create    sentences    of exercises and learn an exercise
   embedding. as a result we were able to get new numeric features that
   successfully learned the relationships between exercises and are then
   more useful than the bunch of labels they originated from.

   credits go to christophe gabard, one of our developers at kwyk, for
   having the idea of applying id97 to process categorical features.

   author : [27]hicham el boukkouri

     * [28]machine learning
     * [29]deep learning
     * [30]nlp
     * [31]data science
     * [32]education

   (button)
   (button)
   (button) 822 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of kwyk

[34]kwyk

   startup company in education

     (button) follow
   [35]towards data science

[36]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 822
     * (button)
     *
     *

   [37]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [38]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c637e35d3668
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-non-nlp-application-of-id97-c637e35d3668&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-non-nlp-application-of-id97-c637e35d3668&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_sonwcqhzxs1y---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@kwykedu?source=post_header_lockup
  17. https://towardsdatascience.com/@kwykedu
  18. https://en.wikipedia.org/wiki/natural_language_processing
  19. https://arxiv.org/pdf/1301.3781.pdf
  20. https://en.wikipedia.org/wiki/john_rupert_firth
  21. https://www.quora.com/what-is-one-hot-encoding-and-when-is-it-used-in-data-science
  22. http://www.aclweb.org/anthology/n13-1090
  23. https://www.youtube.com/watch?v=eribwqs9p38
  24. https://www.tensorflow.org/tutorials/id97
  25. https://www.kwyk.fr/
  26. https://en.wikipedia.org/wiki/nonlinear_dimensionality_reduction#locally-linear_embedding
  27. https://www.linkedin.com/in/hichamelboukkouri
  28. https://towardsdatascience.com/tagged/machine-learning?source=post
  29. https://towardsdatascience.com/tagged/deep-learning?source=post
  30. https://towardsdatascience.com/tagged/nlp?source=post
  31. https://towardsdatascience.com/tagged/data-science?source=post
  32. https://towardsdatascience.com/tagged/education?source=post
  33. https://towardsdatascience.com/@kwykedu?source=footer_card
  34. https://towardsdatascience.com/@kwykedu
  35. https://towardsdatascience.com/?source=footer_card
  36. https://towardsdatascience.com/?source=footer_card
  37. https://towardsdatascience.com/
  38. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  40. https://medium.com/p/c637e35d3668/share/twitter
  41. https://medium.com/p/c637e35d3668/share/facebook
  42. https://medium.com/p/c637e35d3668/share/twitter
  43. https://medium.com/p/c637e35d3668/share/facebook
