   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

introducing vectorflow

a lightweight neural network library for sparse data

   [9]go to the profile of netflix technology blog
   [10]netflix technology blog (button) blockedunblock (button)
   followfollowing
   aug 2, 2017

   by [11]beno  t rostykus
   [12][1*z8on3jupcirtridzg_ykew.png]

introduction

   with the deluge of deep learning libraries and software innovation in
   the field over the last few years, it is an exciting time to be working
   on machine learning problems. most of the libraries available evolved
   from fairly specialized computational code for large dense problems
   such as image classification into general frameworks for
   neural-network-based models offering marginal support for sparse
   models.

   at netflix, our machine learning scientists deal with a wide variety of
   problems across a broad spectrum of areas: from [13]tailoring tv and
   movie recommendations to your taste to [14]optimizing encoding
   algorithms. a subset of our problems involve dealing with extremely
   sparse data; the total dimensionality of the problem at hand can easily
   reach tens of millions of features, even though every observation may
   only have a handful of non-zero entries. for these cases, we felt the
   need for a minimalist library that is specifically optimized for
   training shallow feedforward neural nets on sparse data in a
   single-machine, multi-core environment. we wanted something small and
   easy to hack, so we built [15]vectorflow, one of the many tools our
   machine learning scientists use.

design considerations

     * agility. we want our data scientists to easily run and iterate on
       their models in total autonomy. so we wrote vectorflow in [16]d, a
       modern systems language with a very gentle learning curve. thanks
       to its fast compilers and functional programming features, it
       offers a python-like experience for beginners but with typically
       multiple orders of magnitude of performance gain at run-time, while
       enabling seasoned developers to leverage an excellent templating
       engine, compile-time functionalities and lower-level features (c
       interface, inline assembler, manual memory management,
       auto-vectorization,    ). vectorflow does not have any third-party
       dependencies, which eases its deployment. it offers a
       callback-based api to easily plug-in custom id168s for
       training.
     * sparse-aware. designing the library for sparse data and shallow
       architectures implies that the runtime bottleneck will tend to be
       io: there will be relatively few operations to run per row,
       contrary, for example, to a convolutional layer on a large dense
       matrix. vectorflow avoids, wherever possible, copying or allocating
       any memory during both the forward and backward passes, with each
       layer referencing the data it needs from its parents and children.
       matrix-vector operations have both sparse and dense
       implementations, the latter ones being simd-vectorized. vectorflow
       also offers a way to run a sparse id26 when dealing with
       sparse output gradients.
     * io agnostic. if you are io bound, by definition the trainer will
       run only as fast as your io layer. vectorflow enforces very loose
       requirements on the underlying data schema (merely to provide an
       iterator of rows with a    features    attribute) so that one can write
       efficient data adapters based on the data source and avoid any
       pre-processing or data conversion steps while sticking with the
       same programming language. this allows you to move the code to the
       data, not the opposite.
     * single-machine. distributed systems are hard to debug and introduce
       fixed costs such as job scheduling. implementing distributed
       optimization of a novel machine learning technique is even harder.
       this is why we created an efficient solution in a single machine
       setting, lowering iteration time of modeling without sacrificing
       scalability for small to medium scale problems (100 million rows).
       we opted for generic asynchronous sgd solvers using [17]hogwild as
       a lock-free strategy to distribute the load over the cores with no
       communication cost. this works for most linear or shallow net
       models as long as the data is sufficiently sparse, and avoids
       having to think about the distributed aspect of the algorithm,
       since everything works as in a non-distributed case from a user
       perspective.

applications

   a few months after the project   s inception, we   ve seen a wide variety
   of use cases for the library and multiple research projects and
   production systems are now using vectorflow for problems as diverse as
   causal id136, survival regression, density estimation or ranking
   algorithms for recommendation. in fact, we   re testing using vectorflow
   to power part of the netflix home page experience. it is also included
   in the default toolbox installed on basic instances used by netflix
   machine learning practitioners.

   as an example, we investigate the performance of the library on a
   marketing problem netflix faces related to promoting our originals. in
   this case, we want to perform weighted id113
   with a survival [18]exponential distribution. to implement this, the
   custom callback function passed to vectorflow is:
   [1*nxt9mfcsbqzeqnbbtys5mw.png]

   using this callback for training, we can easily compare 3 models:
     * model 1: linear model on a tiny set of sparse features (~500
       parameters to learn)
     * model 2: linear model on a larger sparse set of features (1m
       parameters to learn)
     * model 3: shallow neural network on a sparse set of features (10m
       parameters to learn), trained on twice the data

   [1*d0-_6qmmxj_3ruyvb-xe8w.png]
   [1*3jsmv11zk8wccpnpc2rm4g.png]

   the data source is a hive table stored on s3 using the columnar data
   format [19]parquet and we train directly against this data by streaming
   it to a c4.4xlarge instance and building in-memory the training set
   which we learn from.

   the results are as follows:
   [1*ow9lo3hpz-qwwg_m7ddzng.png]

   both decompression and feature encoding happen on a single thread so
   there is room for improvement, but the end-to-end runtime demonstrates
   that there is no need for a distributed solution for medium-scale
   sparse datasets and shallow architectures. notice that the training
   time scales somewhat linearly with the sparsity of the data as well as
   the number of rows. one reason preventing linear scalability is that
   cpu memory hierarchy will create cache invalidation when multiple
   asynchronous sgd threads access the same weights, hence breaking the
   theoretical results of hogwild if the model parameters access pattern
   is not sparse enough (see [20]this paper for details).

   future work

   in the future, we plan to broaden the possible topologies supported
   beyond simple linear, polynomial or feedforward architectures, develop
   more specialized layers (such as recurrent cells) and explore new
   parallelism strategies while maintaining the    minimalist    philosophy of
   [21]vectorflow.

     * [22]machine learning
     * [23]neural networks
     * [24]data science

   (button)
   (button)
   (button) 944 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of netflix technology blog

[26]netflix technology blog

   learn more about how netflix designs, builds, and operates our systems
   and engineering organizations

     * (button)
       (button) 944
     * (button)
     *
     *

   [27]go to the profile of netflix technology blog
   never miss a story from netflix technology blog, when you sign up for
   medium. [28]learn more
   never miss a story from netflix technology blog
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/fe10d7f126b8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@netflixtechblog/introducing-vectorflow-fe10d7f126b8&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@netflixtechblog/introducing-vectorflow-fe10d7f126b8&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@netflixtechblog?source=post_header_lockup
  10. https://medium.com/@netflixtechblog
  11. https://www.linkedin.com/in/benoitrostykus/en
  12. http://www.github.com/netflix/vectorflow
  13. https://medium.com/netflix-techblog/recommending-for-the-world-8da8cbcf051b
  14. https://medium.com/netflix-techblog/per-title-encode-optimization-7e99442b62a2
  15. http://www.github.com/netflix/vectorflow
  16. http://dlang.org/
  17. https://arxiv.org/abs/1106.5730
  18. https://en.wikipedia.org/wiki/exponential_distribution
  19. https://parquet.apache.org/
  20. http://web.stanford.edu/~cdesa/papers/isca2017_buckwild.pdf
  21. http://www.github.com/netflix/vectorflow
  22. https://medium.com/tag/machine-learning?source=post
  23. https://medium.com/tag/neural-networks?source=post
  24. https://medium.com/tag/data-science?source=post
  25. https://medium.com/@netflixtechblog?source=footer_card
  26. https://medium.com/@netflixtechblog
  27. https://medium.com/@netflixtechblog
  28. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  30. https://medium.com/p/fe10d7f126b8/share/twitter
  31. https://medium.com/p/fe10d7f126b8/share/facebook
  32. https://medium.com/p/fe10d7f126b8/share/twitter
  33. https://medium.com/p/fe10d7f126b8/share/facebook
