         an end to end implementation of a machine learning pipeline

                                spandan madan

                 visual computing group, harvard university

        computer science and artificial intelligence laboratory, mit

                           [1]link to github repo

section 1. introduction[2]  

background[3]  

   in the fall of 2016, i was a teaching fellow (harvard's version of ta)
   for the graduate class on "advanced topics in data science (cs209/109)"
   at harvard university. i was in-charge of designing the class project
   given to the students, and this tutorial has been built on top of the
   project i designed for the class.

why write yet another tutorial on machine learning and deep learning?[4]  

   as a researcher on id161, i come across new blogs and
   tutorials on ml (machine learning) every day. however, most of them are
   just focussing on introducing the syntax and the terminology relavant
   to the field. for example - a 15 minute tutorial on tensorflow using
   mnist dataset, or a 10 minute intro to deep learning in keras on
   id163.

   while people are able to copy paste and run the code in these tutorials
   and feel that working in ml is really not that hard, it doesn't help
   them at all in using ml for their own purposes. for example, they never
   introduce you to how you can run the same algorithm on your own
   dataset. or, how do you get the dataset if you want to solve a problem.
   or, which algorithms do you use - conventional ml, or deep learning?
   how do you evaluate your models performance? how do you write your own
   model, as opposed to choosing a ready made architecture? all these form
   fundamental steps in any machine learning pipeline, and it is these
   steps that take most of our time as ml practitioners.

   this tutorial breaks down the whole pipeline, and leads the reader
   through it step by step in an hope to empower you to actually use ml,
   and not just feel that it was not too hard. needless to say, this will
   take much longer than 15-30 minutes. i believe a weekend would be a
   good enough estimate.

about the author[5]  

   i am [6]spandan madan, a graduate student at harvard university working
   on id161. my research work is supervised collaboratively by
   professor hanspeter pfister at harvard, and professor aude oliva at
   mit. my current research focusses on using id161 and natural
   language techniques in tandem to build systems capable of reasoning
   using text and visual elements simultaneusly.

section 2. project outline : multi-modal genre classification for movies[7]  

wow, that title sounds like a handful, right? let's break it down step by
step.[8]  

q.1. what do we mean by classification?[9]  

   in machine learning, the task of classification means to use the
   available data to learn a function which can assign a category to a
   data point. for example, assign a genre to a movie, like "romantic
   comedy", "action", "thriller". another example could be automatically
   assigning a category to news articles, like "sports" and "politics".

more formally[10]  

given:[11]  

     * a data point $x_i$
     * a set of categories $y_1,y_2...y_n$ that $x_i$ can belong to.

task :[12]  

   predict the correct category $y_k$ for a new data point $x_k$ not
   present in the given dataset.

problem :[13]  

   we don't know how the $x$ and $y$ are related mathematically.

assumption :[14]  

   we assume there exists a function $f$ relating $x$ and $y$ i.e.
   $f(x_i)=y_i$

approach :[15]  

   since $f$ is not known, we learn a function $g$, which approximates
   $f$.

important consideration :[16]  

     * if $f(x_i)=g(x_i)=y_i$ for all $x_i$, then the two functions $f$
       and $g$ are exactly equal. needless to say, this won't
       realistically ever happen, and we'll only be able to approximate
       the true function $f$ using $g$. this means, sometimes the
       prediction $g(x_i)$ will not be correct. and essentially, our whole
       goal is to find a $g$ which makes a really low number of such
       errors. that's basically all that we're trying to do.
     * for the sake of completeness, i should mention that this is a
       specific kind of learning problem which we call "supervised
       learning". also, the idea that $g$ approximates $f$ well for data
       not present in our dataset is called "generalization". it is
       absolutely paramount that our model generalizes, or else all our
       claims will only be true about data we already have and our
       predictions will not be correct.
     * we will look into generalization a little bit more a little ahead
       in the tutorial.
     * finally, there are several other kinds, but supervised learning is
       the most popular and well studied kind.

q.2. what's multi-modal classification then?[17]  

   in the machine learning community, the term multi-modal is used to
   refer to multiple kinds of data. for example, consider a youtube video.
   it can be thought to contain 3 different modalities -
     * the video frames (visual modality)
     * the audio clip of what's being spoken (audio modality)
     * some videos also come with the transcription of the words spoken in
       the form of subtitles (textual modality)

   consider, that i'm interested in classifying a song on youtube as pop
   or rock. you can use any of the above 3 modalities to predict the genre
   - the video, the song itself, or the lyrics. but, needless to say, you
   can predict it much better if you could use all three simultaneously.
   this is what we mean by multi-modal classification.

for this project, we will be using visual and textual data to classify movie
genres.[18]  

project outline[19]  

     * scraping a dataset : the first step is to build a rich data set. we
       will collect textual and visual data for each movie.
     * data pre-processing
     * non-deep machine learning models</b> : probabilistic and max-margin
       classifiers.
     * intuitive theory behind deep learning
     * deep models for visual data
     * deep models for text
     * potential extensions
     * food for thought

section 3. building your very own dataset.[20]  

   for any machine learning algorithm to work, it is imperative that we
   collect data which is "representative". now, let's take a moment to
   discuss what the word representative mean.

what data is good data? or what do you mean by data being
"representative"?[21]  

   let's look at this from first principles. mathematically, the premise
   of machine learning (to be precise, the strand of machine learning
   we'll be working with here) is that given input variable x, and an
   output variable y, if there is a function such that g(x)=y, then if g
   is unknown, we can "learn" a function f which approximates g. at the
   very heart, its not at all different from what you may have earlier
   studied as "curve fitting". for example, if you're trying to predict
   someone's movie preferences then x can be information about the
   person's gender, age, nationality and so on, while y can be the genre
   they most like to listen to!

   let's do a thought experiment. consider the same example - i'm trying
   to predict people's movie preferences. i walk into a classroom today,
   and collect information about some students and their movie
   preferences. now, i use that data to build a model. how well do you
   think i can predict my father's movie preferences? the answer is -
   probably not very well. why? intuitively, there was probably no one in
   the classroom who was my father's age. my model can tell me that as
   people go from age 18 to 30, they have a higher preference for
   documentaries over superhero movies. but does this trend continue at
   55? probably, they may start liking family dramas more. probably they
   don't. in a nutshell, we cannot say with certainty, as our data tells
   us nothing about it. so, if the task was to make predictions about
   anyone's movie preferences, then the data collected from just
   undergraduates is not representative.

   now, let's see why this makes sense mathematically. look at the graph
   below.

   [contour.png]

     fig.1: plot of a function we are trying to approximate([22]source)

   if we consider that the variable plotted on the vertical axis is $y$,
   and the values of the 2 variables on the horizontal axes make the input
   vector $x$, then, our hope is that we are able to find a function $f$
   which can approximate the function plotted here. if all the data i
   collect is such that $x_1$ belongs to (80,100) and $x_2$ belongs to
   (80,100), the learned function will only be able to learn the
   "yellow-green dipping bellow" part of the function. our function will
   never be able to predict the behavior in the "red" regions of the true
   function. so, in order to be able to learn a good function, we need
   data sampled from a diverse set of values of $x_1$ and x2. that would
   be representative data to learn this contour.

   therefore, we want to collect data which is representative of all
   possible movies that we want to make predictions about. or else (which
   is often the case), we need to be aware of the limitations of the model
   we have trained, and the predictions we can make with confidence. the
   easiest way to do this is to only make predictions about the domain of
   data we collected the training data from. for example, in our case, let
   us start by assuming that our model will predict genres for only
   english movies. now, the task is to collect data about a diverse
   collection of movies.

   so how do we get this data then? neither google, nor any university has
   released such a dataset. we want to collect visual and textual data
   about these movies. the simple answer is to scrape it from the internet
   to build our own dataset. for the purpose of this project, we will use
   movie posters as our visual data, and movie plots as textual data.
   using these, we will build a model that can predict movie genres!

we will be scraping data from 2 different movie sources - imdb and tmdb[23]  

imdb:http://www.imdb.com/

   for those unaware, imdb is the primary source of information about
   movies on the internet. it is immensely rich with posters, reviews,
   synopsis, ratings and many other information on every movie. we will
   use this as our primary data source.

tmdb:https://www.themoviedb.org/

   tmdb, or the movie database, is an open source version of imdb, with a
   free to use api that can be used to collect information. you do need an
   api key, but it can be obtained for free by just making a request after
   making a free account.

note -[24]  

   imdb gives some information for free through the api, but doesn't
   release other information about movies. here, we will keep it legal and
   only use information given to us for free and legally. however,
   scraping does reside on the moral fence, so to say. people often scrape
   data which isn't exactly publicly available for use from websites.
   in [2]:
import urllib2
import requests
import json
import imdb
import time
import itertools
import wget
import os
import tmdbsimple as tmdb
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
import pickle

here is a broad outline of technical steps to be done for data
collection[25]  

     * sign up for tmdb (themoviedatabase.org), and set up api to scrape
       movie posters for above movies.
     * set up and work with tmdb to get movie information from their
       database
     * do the same for imdb
     * compare the entries of imdb and tmdb for a movie
     * get a listing and information of a few movies
     * think and ponder over the potential challenges that may come our
       way, and think about interesting questions we can answer given the
       api's we have in our hands.
     * get data from the tmdb

   let's go over each one of these one by one.

signing up for tmdb and getting set up for getting movie metadata.[26]  

     * step 1. head over to [tmdb.org]
       ([27]https://www.themoviedb.org/?language=en) and create a new
       account there by signing up.
     * step 2. click on your account icon on the top right, then from drop
       down menu select "settings".
     * step 3. on the settings page, you will see the option "api" on the
       left pane. click on that.
     * step 4. apply for a new developer key. fill out the form as
       required. the fields "application name" and "application url" are
       not important. fill anything there.
     * step 5. it should generate a new api key for you and you should
       also receive a mail.

   now that you have the api key for tmdb, you can query using tmdb.
   remember, it allows only 40 queries per 10 seconds.

   an easy way to respect this is to just have a call to time.sleep(1)
   after each iteration. this is also being very nice to the server.

   if you want to try and maximize your throughput you can embed every
   tmdb request in a nested try except block. if the first try fails, the
   second try first uses python's sleep function to give it a little rest,
   and then try again to make a request. something like this -
try:
    search.movie(query=movie) #an api request
except:
    try:
        time.sleep(10) #sleep for a bit, to give api requests a rest.
        search.movie(query=<i>movie_name</i>) #make second api request
    except:
        print "failed second attempt too, check if there's any error in request"

using tmdb using the obtained api key to get movie information[28]  

   i have made these functions which make things easy. basically, i'm
   making use of a library called tmdbsimple which makes tmdb using even
   easier. this library was installed at the time of setup.

   however, if you want to avoid the library, it is also easy enough to
   load the api output directly into a dictionary like this without using
   tmdbsimple:
url = 'https://api.themoviedb.org/3/movie/1581?api_key=' + api_key
data = urllib2.urlopen(url).read()

# create dictionary from json
datadict = json.loads(data)

   in [5]:
# set here the path where you want the scraped folders to be saved!
poster_folder='posters_final/'
if poster_folder.split('/')[0] in os.listdir('./'):
    print('folder already exists')
else:
    os.mkdir('./'+poster_folder)

folder already exists

   in [14]:
poster_folder

   out[14]:
'posters_final/'

   in [15]:
# for the purpose of this example, i will be working with the 1999 sci-fi movie
- "the matrix"!

api_key = '' #enter your own api key here to run the code below.
# generate your own api key as explained above :)


tmdb.api_key = api_key #this sets the api key setting for the tmdb object
search = tmdb.search() #this instantiates a tmdb "search" object which allows yo
ur to search for the movie

# these functions take in a string movie name i.e. like "the matrix" or "interst
ellar"
# what they return is pretty much clear in the name - poster, id , info or genre
 of the movie!
def grab_poster_tmdb(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.movies(id)
    posterp=movie.info()['poster_path']
    title=movie.info()['original_title']
    url='image.tmdb.org/t/p/original'+posterp
    title='_'.join(title.split(' '))
    strcmd='wget -o '+poster_folder+title+'.jpg '+url
    os.system(strcmd)

def get_movie_id_tmdb(movie):
    response = search.movie(query=movie)
    movie_id=response['results'][0]['id']
    return movie_id

def get_movie_info_tmdb(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.movies(id)
    info=movie.info()
    return info

def get_movie_genres_tmdb(movie):
    response = search.movie(query=movie)
    id=response['results'][0]['id']
    movie = tmdb.movies(id)
    genres=movie.info()['genres']
    return genres

   while the above functions have been made to make it easy to get genres,
   posters and id, all the information that can be accessed can be seen by
   calling the function get_movie_info() as shown below
   in [16]:
print get_movie_genres_tmdb("the matrix")

[{u'id': 28, u'name': u'action'}, {u'id': 878, u'name': u'science fiction'}]

   in [17]:
info=get_movie_info_tmdb("the matrix")
print "all the movie information from tmdb gets stored in a dictionary with the
following keys for easy access -"
info.keys()

all the movie information from tmdb gets stored in a dictionary with the followi
ng keys for easy access -

   out[17]:
[u'poster_path',
 u'production_countries',
 u'revenue',
 u'overview',
 u'video',
 u'id',
 u'genres',
 u'title',
 u'tagline',
 u'vote_count',
 u'homepage',
 u'belongs_to_collection',
 u'original_language',
 u'status',
 u'spoken_languages',
 u'imdb_id',
 u'adult',
 u'backdrop_path',
 u'production_companies',
 u'release_date',
 u'popularity',
 u'original_title',
 u'budget',
 u'vote_average',
 u'runtime']

   so, to get the tagline of the movie we can use the above dictionary key
   -
   in [18]:
info=get_movie_info_tmdb("the matrix")
print info['tagline']

welcome to the real world.

getting movie information from imdb[29]  

   now that we know how to get information from tmdb, here's how we can
   get information about the same movie from imdb. this makes it possible
   for us to combine more information, and get a richer dataset. i urge
   you to try and see what dataset you can make, and go above and beyond
   the basic things i've done in this tutorial. due to the differences
   between the two datasets, you will have to do some cleaning, however
   both of these datasets are extremely clean and it will be minimal.
   in [19]:
# create the imdb object that will be used to access the imdb's database.
imbd_object = imdb.imdb() # by default access the web.

# search for a movie (get a list of movie objects).
results = imbd_object.search_movie('the matrix')

# as this returns a list of all movies containing the word "the matrix", we pick
 the first element
movie = results[0]

imbd_object.update(movie)

print "all the information we can get about this movie from imdb-"
movie.keys()

all the information we can get about this movie from imdb-

   out[19]:
[u'music department',
 'sound crew',
 'camera and electrical department',
 u'distributors',
 'rating',
 'runtimes',
 'costume designer',
 'make up',
 'year',
 'production design',
 'miscellaneous crew',
 'color info',
 u'casting department',
 'languages',
 'votes',
 'producer',
 'title',
 'mpaa',
 'assistant director',
 'writer',
 'production manager',
 'casting director',
 'visual effects',
 'top 250 rank',
 'set decoration',
 'editor',
 'certificates',
 u'costume department',
 'country codes',
 'language codes',
 'cover url',
 u'special effects department',
 'special effects companies',
 'sound mix',
 u'location management',
 'genres',
 'director',
 'stunt performer',
 'miscellaneous companies',
 'cinematographer',
 'art direction',
 'akas',
 'aspect ratio',
 u'production companies',
 'kind',
 u'art department',
 'countries',
 u'transportation department',
 'plot outline',
 'plot',
 'cast',
 u'animation department',
 'original music',
 u'editorial department',
 'canonical title',
 'long imdb title',
 'long imdb canonical title',
 'smart canonical title',
 'smart long imdb canonical title',
 'full-size cover url']

   in [20]:
print "the genres associated with the movie are - ",movie['genres']

the genres associated with the movie are -  [u'action', u'sci-fi']

a small comparison of imdb and tmdb[30]  

   now that we have both systems running, let's do a very short comparison
   for the same movie?
   in [21]:
print "the genres for the matrix pulled from imdb are -",movie['genres']
print "the genres for the matrix pulled from tmdb are -",get_movie_genres_tmdb("
the matrix")

the genres for the matrix pulled from imdb are - [u'action', u'sci-fi']
the genres for the matrix pulled from tmdb are - [{u'id': 28, u'name': u'action'
}, {u'id': 878, u'name': u'science fiction'}]

   as we can see, both the systems are correct, but the way they package
   information is different. tmdb calls it "science fiction" and has an id
   for every genre. while imdb calls it "sci-fi". thus, it is important to
   keep track of these things when making use of both the datasets
   simultaneously.

   now that we know how to scrape information for one movie, let's take a
   bigger step towards scraping multiple movies?

working with multiple movies : obtaining top 20 movies from tmdb[31]  

   we first instantiate an object that inherits from class movies from
   tmdb. then we use the popular() class method (i.e. function) to get top
   movies. to get more than one page of results, the optional page
   argument lets us see movies from any specified page number.
   in [22]:
all_movies=tmdb.movies()
top_movies=all_movies.popular()

# this is a dictionary, and to access results we use the key 'results' which ret
urns info on 20 movies
print(len(top_movies['results']))
top20_movs=top_movies['results']

20

   let's look at one of these movies. it's the same format as above, as we
   had information on the movie "the matrix", as you can see below. it's a
   dictionary which can be queried for specific information on that movie
   in [23]:
first_movie=top20_movs[0]
print "here is all the information you can get on this movie - "
print first_movie
print "\n\nthe title of the first movie is - ", first_movie['title']

here is all the information you can get on this movie -
{u'poster_path': u'/twqifoyuwletmmasngho7xbjett.jpg', u'title': u'beauty and the
 beast', u'overview': u"a live-action adaptation of disney's version of the clas
sic 'beauty and the beast' tale of a cursed prince and a beautiful young woman w
ho helps him break the spell.", u'release_date': u'2017-03-16', u'popularity': 2
36.720585, u'original_title': u'beauty and the beast', u'backdrop_path': u'/6auw
e0gsl69wmtswwexsormivwu.jpg', u'vote_count': 3801, u'video': false, u'adult': fa
lse, u'vote_average': 6.8, u'genre_ids': [10751, 14, 10749], u'id': 321612, u'or
iginal_language': u'en'}


the title of the first movie is -  beauty and the beast

   let's print out top 5 movie's titles!
   in [24]:
for i in range(len(top20_movs)):
    mov=top20_movs[i]
    title=mov['title']
    print title
    if i==4:
        break

beauty and the beast
wonder woman
despicable me 3
spider-man: homecoming
logan

yes, i know. i'm a little upset too seeing beauty and the beast above logan
in the list![32]  

   moving on, we can get their genres the same way.
   in [25]:
for i in range(len(top20_movs)):
    mov=top20_movs[i]
    genres=mov['genre_ids']
    print genres
    if i==4:
        break

[10751, 14, 10749]
[28, 12, 14]
[12, 16, 35, 10751]
[28, 12, 878]
[28, 18, 878]

   so, tmdb doesn't want to make your job as easy as you thought. why
   these random numbers? want to see their genre names? well, there's the
   genre() class for it. let's get this done!
   in [26]:
# create a tmdb genre object!
genres=tmdb.genres()
# the list() method of the genres() class returns a listing of all genres in the
 form of a dictionary.
list_of_genres=genres.list()['genres']

   let's convert this list into a nice dictionary to look up genre names
   from genre ids!
   in [27]:
genre_id_to_name={}
for i in range(len(list_of_genres)):
    genre_id=list_of_genres[i]['id']
    genre_name=list_of_genres[i]['name']
    genre_id_to_name[genre_id]=genre_name

   now, let's re-print the genres of top 20 movies?
   in [28]:
for i in range(len(top20_movs)):
    mov=top20_movs[i]
    title=mov['title']
    genre_ids=mov['genre_ids']
    genre_names=[]
    for id in genre_ids:
        genre_name=genre_id_to_name[id]
        genre_names.append(genre_name)
    print title,genre_names
    if i==4:
        break

beauty and the beast [u'family', u'fantasy', u'romance']
wonder woman [u'action', u'adventure', u'fantasy']
despicable me 3 [u'adventure', u'animation', u'comedy', u'family']
spider-man: homecoming [u'action', u'adventure', u'science fiction']
logan [u'action', u'drama', u'science fiction']

section 4 - building a dataset to work with : let's take a look at the top
1000 movies from the database[33]  

   making use of the same api as before, we will just pull results from
   the top 50 pages. as mentioned earlier, the "page" attribute of the
   command top_movies=all_movies.popular() can be used for this purpose.

   please note: some of the code below will store the data into python
   "pickle" files so that it can be ready directly from memory, as opposed
   to being downloaded every time. once done, you should comment out any
   code which generated an object that was pickled and is no longer
   needed.
   in [29]:
all_movies=tmdb.movies()
top_movies=all_movies.popular()

# this is a dictionary, and to access results we use the key 'results' which ret
urns info on 20 movies
len(top_movies['results'])
top20_movs=top_movies['results']

   in [30]:
# comment out this cell once the data is saved into pickle file.
all_movies=tmdb.movies()
top1000_movies=[]
print('pulling movie list, please wait...')
for i in range(1,51):
    if i%15==0:
        time.sleep(7)
    movies_on_this_page=all_movies.popular(page=i)['results']
    top1000_movies.extend(movies_on_this_page)
len(top1000_movies)
f3=open('movie_list.pckl','wb')
pickle.dump(top1000_movies,f3)
f3.close()
print('done!')

   in [31]:
f3=open('movie_list.pckl','rb')
top1000_movies=pickle.load(f3)
f3.close()

pairwise analysis of movie genres[34]  

   as our dataset is multi label, simply looking at the ditribution of
   genres is not sufficient. it might be beneficial to see which genres
   co-occur, as it might shed some light on inherent biases in our
   dataset. for example, it would make sense if romance and comedy occur
   together more often than documentary and comedy. such inherent biases
   tell us that the underlying population we are sampling from itself is
   skewed and not balanced. we may then take steps to account for such
   problems. even if we don't take such steps, it is important to be aware
   that we are making the assumption that an unbalanced dataset is not
   hurting our performance and if need be, we can come back to address
   this assumption. good old scientific method, eh?

   so for the top 1000 movies let's do some pairwise analysis for genre
   distributions. our main purpose is to see which genres occur together
   in the same movie. so, we first define a function which takes a list
   and makes all possible pairs from it. then, we pull the list of genres
   for a movie and run this function on the list of genres to get all
   pairs of genres which occur together
   in [32]:
# this function just generates all possible pairs of movies
def list2pairs(l):
    # itertools.combinations(l,2) makes all pairs of length 2 from list l.
    pairs = list(itertools.combinations(l, 2))
    # then the one item pairs, as duplicate pairs aren't accounted for by iterto
ols
    for i in l:
        pairs.append([i,i])
    return pairs

   as mentioned, now we will pull genres for each movie, and use above
   function to count occurrences of when two genres occurred together
   in [33]:
# get all genre lists pairs from all movies
allpairs = []
for movie in top1000_movies:
    allpairs.extend(list2pairs(movie['genre_ids']))

nr_ids = np.unique(allpairs)
visgrid = np.zeros((len(nr_ids), len(nr_ids)))
for p in allpairs:
    visgrid[np.argwhere(nr_ids==p[0]), np.argwhere(nr_ids==p[1])]+=1
    if p[1] != p[0]:
        visgrid[np.argwhere(nr_ids==p[1]), np.argwhere(nr_ids==p[0])]+=1

   let's take a look at the structure we just made. it is a 19x19
   structure, as shown below. also, see that we had 19 genres. needless to
   say, this structure counts the number of simultaneous occurrences of
   genres in same movie.
   in [34]:
print visgrid.shape
print len(genre_id_to_name.keys())

(19, 19)
19

   in [35]:
annot_lookup = []
for i in xrange(len(nr_ids)):
    annot_lookup.append(genre_id_to_name[nr_ids[i]])

sns.heatmap(visgrid, xticklabels=annot_lookup, yticklabels=annot_lookup)

   out[35]:
<matplotlib.axes._subplots.axessubplot at 0x111aa6290>

   [eaoeiz2fakma aaaasuvork5cyii= ]

   the above image shows how often the genres occur together, as a heatmap

   important thing to notice in the above plot is the diagonal. the
   diagonal corresponds to self-pairs, i.e. number of times a genre, say
   drama occurred with drama. which is basically just a count of the total
   times that genre occurred!

   as we can see there are a lot of dramas in the data set, it is also a
   very unspecific label. there are nearly no documentaries or tv movies.
   horror is a very distinct label, and romance is also not too widely
   spread.

   to account for this unbalanced data, there are multiple things we can
   try to explore what interesting relationships can be found.

delving deeper into co-occurrence of genres[35]  

   what we want to do now is to look for nice groups of genres that
   co-occur, and see if it makes sense to us logically? intuitively
   speaking, wouldn't it be fun if we saw nice boxes on the above plot -
   boxes of high intensity i.e. genres that occur together and don't occur
   much with other genres. in some ways, that would isolate the
   co-occurrence of some genres, and heighten the co-occurrence of others.

   while the data may not show that directly, we can play with the numbers
   to see if that's possible. the technique used for that is called
   biid91.
   in [36]:
from sklearn.cluster import spectralcoid91

   in [37]:
model = spectralcoid91(n_clusters=5)
model.fit(visgrid)

fit_data = visgrid[np.argsort(model.row_labels_)]
fit_data = fit_data[:, np.argsort(model.column_labels_)]

annot_lookup_sorted = []
for i in np.argsort(model.row_labels_):
    annot_lookup_sorted.append(genre_id_to_name[nr_ids[i]])

sns.heatmap(fit_data, xticklabels=annot_lookup_sorted, yticklabels=annot_lookup_
sorted, annot=false)
plt.title("after biid91; rearranged to show biclusters")

plt.show()

   [yziyv4iiomfhesuk5n25oy4tgyki2kv4awevmuj6b4xa88q6oyymelmnkkztcat
   gro5jptjzdkzozgnucatywsgrjzcmuwmkxka2qhlmplmzmj8p9lnc3dnqtsyaaaaaelftks
   uqmcc ]

   looking at the above figure, "boxes" or groups of movie genres
   automatically emerge!

   intuitively - crime, sci-fi, mystery, action, horror, drama, thriller,
   etc co-occur. and, romance, fantasy, family, music, adventure, etc
   co-occur.

   that makes a lot of intuitive sense, right?

   one challenge is the broad range of the drama genre. it makes the two
   clusters highly overlapping. if we merge it together with action
   thriller, etc. we will end up with nearly all movies just having that
   label.

   based on playing around with the stuff above, we can sort the data into
   the following genre categories - "drama, action, sciencefiction,
   exciting(thriller, crime, mystery), uplifting(adventure, fantasy,
   animation, comedy, romance, family), horror, history"

   note: that this categorization is subjective and by no means the only
   right solution. one could also just stay with the original labels and
   only exclude the ones with not enough data. such tricks are important
   to balance the dataset, it allows us to increase or decrease the
   strength of certain signals, making it possible to improve our
   id136s :)

interesting questions[36]  

   this really should be a place for you to get creative and hopefully
   come up with better questions than me.

   here are some of my thoughts:
     * which actors are bound to a genre, and which can easily hop genres?
     * is there a trend in genre popularity over the years?
     * can you use sound tracks to identify the genre of a movie?
     * are top romance actors higher paid than top action actors?
     * if you look at release date vs popularity score, which movie genres
       have a longer shelf life?

   ideas to explore specifically for feature correlations:
     * are title length correlated with movie genre?
     * are movie posters darker for horror than for romance end comedy?
     * are some genres specifically released more often at a certain time
       of year?
     * is the rpg rating correlated with the genre?

based on this new category set, we will now pull posters from tmdb as our
training data![37]  

   in [41]:
# done before, reading from pickle file now to maintain consistency of data!
# we now sample 100 movies per genre. problem is that the sorting is by popular
movies, so they will overlap.
# need to exclude movies that were already sampled.
movies = []
baseyear = 2017

print('starting pulling movies from tmdb. if you want to debug, uncomment the pr
int command. this will take a while, please wait...')
done_ids=[]
for g_id in nr_ids:
    #print('pulling movies for genre id '+g_id)
    baseyear -= 1
    for page in xrange(1,6,1):
        time.sleep(0.5)

        url = 'https://api.themoviedb.org/3/discover/movie?api_key=' + api_key
        url += '&language=en-us&sort_by=popularity.desc&year=' + str(baseyear)
        url += '&with_genres=' + str(g_id) + '&page=' + str(page)

        data = urllib2.urlopen(url).read()

        datadict = json.loads(data)
        movies.extend(datadict["results"])
    done_ids.append(str(g_id))
print("pulled movies for genres - "+','.join(done_ids))

starting pulling movies from tmdb. if you want to debug, uncomment the print com
mand. this will take a while, please wait...
pulled movies for genres - 12

   in [42]:
# f6=open("movies_for_posters",'wb')
# pickle.dump(movies,f6)
# f6.close()

   in [43]:
f6=open("movies_for_posters",'rb')
movies=pickle.load(f6)
f6.close()

   let's remove any duplicates that we have in the list of movies
   in [44]:
movie_ids = [m['id'] for m in movies]
print "originally we had ",len(movie_ids)," movies"
movie_ids=np.unique(movie_ids)
print len(movie_ids)
seen_before=[]
no_duplicate_movies=[]
for i in range(len(movies)):
    movie=movies[i]
    id=movie['id']
    if id in seen_before:
        continue
#         print "seen before"
    else:
        seen_before.append(id)
        no_duplicate_movies.append(movie)
print "after removing duplicates we have ",len(no_duplicate_movies), " movies"

originally we had  1670  movies
1608
after removing duplicates we have  1608  movies

   also, let's remove movies for which we have no posters!
   in [45]:
poster_movies=[]
counter=0
movies_no_poster=[]
print("total movies : ",len(movies))
print("started downloading posters...")
for movie in movies:
    id=movie['id']
    title=movie['title']
    if counter==1:
        print('downloaded first. code is working fine. please wait, this will ta
ke quite some time...')
    if counter%300==0 and counter!=0:
        print "done with ",counter," movies!"
        print "trying to get poster for ",title
    try:
        grab_poster_tmdb(title)
        poster_movies.append(movie)
    except:
        try:
            time.sleep(7)
            grab_poster_tmdb(title)
            poster_movies.append(movie)
        except:
            movies_no_poster.append(movie)
    counter+=1
print("done with all the posters!")

('total movies : ', 1670)
started downloading posters...
downloaded first. code is working fine. please wait, this will take quite some t
ime...
done with  300  movies!
trying to get poster for  gravity
done with  600  movies!
trying to get poster for  zombieland
done with  900  movies!
trying to get poster for  the substitute
done with  1200  movies!
trying to get poster for  decoys
done with  1500  movies!
trying to get poster for  lost and delirious
done with all the posters!

   in [46]:
print len(movies_no_poster)
print len(poster_movies)

170
1500

   in [47]:
# f=open('poster_movies.pckl','w')
# pickle.dump(poster_movies,f)
# f.close()

   in [48]:
f=open('poster_movies.pckl','r')
poster_movies=pickle.load(f)
f.close()

   in [49]:
# f=open('no_poster_movies.pckl','w')
# pickle.dump(movies_no_poster,f)
# f.close()

   in [50]:
f=open('no_poster_movies.pckl','r')
movies_no_poster=pickle.load(f)
f.close()

congratulations, we are done scraping![38]  

building a dataset out of the scraped information![39]  

   this task is simple, but extremely important. it's basically what will
   set the stage for the whole project. given that you have the freedom to
   cast their own project within the framework i am providing, there are
   many decisions that you must make to finalize your own version of the
   project.

   as we are working on a classification problem, we need to make two
   decisions given the data at hand -
     * what do we want to predict, i.e. what's our y?
     * what features to use for predicting this y, i.e. what x should we
       use?

   there are many different options possible, and it comes down to you to
   decide what's most exciting. i will be picking my own version for the
   example, but it is imperative that you think this through, and come up
   with a version which excites you!

   as an example, here are some possible ways to frame y, while still
   sticking to the problem of genre prediction -
     * assume every movie can have multiple genres, and then it becomes a
       multi-label classification problem. for example, a movie can be
       action, horror and adventure simultaneously. thus, every movie can
       be more than one genre.
     * make clusters of genres as we did in milestone 1 using
       biid91, and then every movie can have only 1 genre. this way,
       the problem becomes a simpler, multi-class problem. for example, a
       movie could have the class - uplifting (refer milestone 1), or
       horror or history. no movie get's more than one class.

   for the purposes of this implementation, i'm going with the first case
   explained above - i.e. a multi-label classification problem.

   similarly, for designing our input features i.e. x, you may pick any
   features you think make sense, for example, the director of a movie may
   be a good predictor for genre. or, they may choose any features they
   design using algorithms like pca. given the richness of imdb, tmdb and
   alternate sources like wikipedia, there is a plethora of options
   available. be creative here!

   another important thing to note is that in doing so, we must also make
   many more small implementation decisions on the way. for example, what
   genres are we going to include? what movies are we going to include?
   all these are open ended!

my implementation[40]  

   implementation decisions made -
     * the problem is framed here as a multi-label problem explained
       above.
     * we will try to predict multiple genres associated with a movie.
       this will be our y.
     * we will use 2 different kinds of x - text and images.
     * for the text part - input features being used to predict the genre
       is a form of the movie's plot available from tmdb using the
       property 'overview'. this will be our x.
     * for the image part - we will use the scraped poster images as our
       x.

   note : we will first look at some conventional machine learning models,
   which were popular before the recent rise of neural networks and deep
   learning. for the poster image to genre prediction, i have avoided
   using this for the reason that conventional ml models are simply not
   used anymore without using deep learning for feature extraction (all
   discussed in detail ahead, don't be scared by the jargon). for the
   movie overview to genre prediction problem we will look at both
   conventional models and deep learning models.

   now, let's build our x and y!

   first, let's identify movies that have overviews. next few steps are
   going to be a good example on why data cleaning is important!
   in [51]:
movies_with_overviews=[]
for i in range(len(no_duplicate_movies)):
    movie=no_duplicate_movies[i]
    id=movie['id']
    overview=movie['overview']

    if len(overview)==0:
        continue
    else:
        movies_with_overviews.append(movie)

len(movies_with_overviews)

   out[51]:
1595

   now let's store the genre's for these movies in a list that we will
   later transform into a binarized vector.

   binarized vector representation is a very common and important way data
   is stored/represented in ml. essentially, it's a way to reduce a
   categorical variable with n possible values to n binary indicator
   variables. what does that mean? for example, let [(1,3),(4)] be the
   list saying that sample a has two labels 1 and 3, and sample b has one
   label 4. for every sample, for every possible label, the representation
   is simply 1 if it has that label, and 0 if it doesn't have that label.
   so the binarized version of the above list will be -
[(1,0,1,0]),
(0,0,0,1])]

   in [52]:
# genres=np.zeros((len(top1000_movies),3))
genres=[]
all_ids=[]
for i in range(len(movies_with_overviews)):
    movie=movies_with_overviews[i]
    id=movie['id']
    genre_ids=movie['genre_ids']
    genres.append(genre_ids)
    all_ids.extend(genre_ids)

   in [53]:
from sklearn.preprocessing import multilabelbinarizer
mlb=multilabelbinarizer()
y=mlb.fit_transform(genres)

   in [54]:
genres[1]

   out[54]:
[28, 12, 35, 10749]

   in [55]:
print y.shape
print np.sum(y, axis=0)

(1595, 20)
[327 234 220 645 222 438 404 138  45 440 233 133 242 196 135 232 256  80
  23  25]

   in [62]:
len(list_of_genres)

   out[62]:
19

   this is interesting. we started with only 19 genre labels if you
   remember. but the shape for y is 1666,20 while it should be 1666,19 as
   there are only 19 genres? let's explore.

   let's find genre ids that are not present in our original list of
   genres!
   in [63]:
# create a tmdb genre object!
genres=tmdb.genres()
# the list() method of the genres() class returns a listing of all genres in the
 form of a dictionary.
list_of_genres=genres.list()['genres']
genre_id_to_name={}
for i in range(len(list_of_genres)):
    genre_id=list_of_genres[i]['id']
    genre_name=list_of_genres[i]['name']
    genre_id_to_name[genre_id]=genre_name

   in [64]:
for i in set(all_ids):
    if i not in genre_id_to_name.keys():
        print i

10769

   well, this genre id wasn't given to us by tmdb when we asked it for all
   possible genres. how do we go about this now? we can either neglect all
   samples that have this genre. but if you look up you'll see there's too
   many of these samples. so, i googled more and went into their
   documentation and found that this id corresponds to the genre
   "foreign". so, we add it to the dictionary of genre names ourselves.
   such problems are ubiquitous in machine learning, and it is up to us to
   diagnose and correct them. we must always make a decision about what to
   keep, how to store data and so on.
   in [65]:
genre_id_to_name[10769]="foreign" #adding it to the dictionary

   in [66]:
len(genre_id_to_name.keys())

   out[66]:
20

   now, we turn to building the x matrix i.e. the input features! as
   described earlier, we will be using the overview of movies as our input
   vector! let's look at a movie's overview for example!
   in [67]:
sample_movie=movies_with_overviews[5]
sample_overview=sample_movie['overview']
sample_title=sample_movie['title']
print "the overview for the movie",sample_title," is - \n\n"
print sample_overview

the overview for the movie doctor strange  is -


after his career is destroyed, a brilliant but arrogant surgeon gets a new lease
 on life when a sorcerer takes him under his wing and trains him to defend the w
orld against evil.

so, how do we store this movie overview in a matrix?[41]  

do we just store the whole string? we know that we need to work with numbers,
but this is all text. what do we do?![42]  

   the way we will be storing the x matrix is called a "bag of words"
   representation. the basic idea of this representation in our context is
   that we can think of all the distinct words that are possible in the
   movies' reviews as a distinct object. and then every movie overview can
   be thought as a "bag" containing a bunch of these possible objects.

   for example, in the case of zootopia the movie above - the "bag"
   contains the words ("determined", "to", "prove", "herself"......"the",
   "mystery"). we make such lists for all movie overviews. finally, we
   binarize again like we did above for y. scikit-learn makes our job easy
   here by simply using a function countvectorizer() because this
   representation is so often used in machine learning.

   what this means is that, for all the movies that we have the data on,
   we will first count all the unique words. say, there's 30,000 unique
   words. then we can represent every movie overview as a 30000x1 vector,
   where each position in the vector corresponds to the presence or
   absence of a particular word. if the word corresponding to that
   position is present in the overview, that position will have 1,
   otherwise it will be 0.

   ex - if our vocabular was 4 words - "i","am","a","good","boy", then the
   representation for the sentence "i am a boy" would be [1 1 1 0 1], and
   for the sentence "i am good" would be [1 1 0 1 0].
   in [68]:
from sklearn.feature_extraction.text import countvectorizer
import re

   in [69]:
content=[]
for i in range(len(movies_with_overviews)):
    movie=movies_with_overviews[i]
    id=movie['id']
    overview=movie['overview']
    overview=overview.replace(',','')
    overview=overview.replace('.','')
    content.append(overview)

   in [70]:
print content[0]
print len(content)

a teenager finds himself transported to an island where he must help protect a g
roup of orphans with special powers from creatures intent on destroying them
1595

are all words equally important?[43]  

at the cost of sounding "animal farm" inspired, i would say not all words are
equally important.[44]  

   for example, let's consider the overview for the matrix -
   in [75]:
get_movie_info_tmdb('the matrix')['overview']

   out[75]:
u'set in the 22nd century, the matrix tells the story of a computer hacker who j
oins a group of underground insurgents fighting the vast and powerful computers
who now rule the earth.'

   for "the matrix" a word like "computer" is a stronger indicators of it
   being a sci-fi movie, than words like "who" or "powerful" or "vast".
   one way computer scientists working with natural language tackled this
   problem in the past (and it is still used very popularly) is what we
   call tf-idf i.e. term frequence, inverse document frequency. the basic
   idea here is that words that are strongly indicative of the content of
   a single document (every movie overview is a document in our case) are
   words that occur very frequently in that document, and very
   infrequently in all other documents. for example, "computer" occurs
   twice here but probably will not in most other movie overviews. hence,
   it is indicative. on the other hand, generic words like "a","and","the"
   will occur very often in all documents. hence, they are not indicative.

   so, can we use this information to reduce our insanely high 30,000
   dimensional vector representation to a smaller, more handle-able
   number? but first up, why should we even care? the answer is probably
   one of the most used phrases in ml - "the curse of dimensionality".

the curse of dimensionality[45]  

this section is strongly borrowing from one of the greatest [46]ml papers
i've ever read.[47]  

   this expression was coined by bellman in 1961 to refer to the fact that
   many algorithms that work fine in low dimensions become intractable
   when the input is high-dimensional. the reason for them not working in
   high dimensions is very strongly linked to what we discussed earlier -
   having a representative dataset. consider this, you have a function $f$
   dependent only one dependent variable $x$, and $x$ can only integer
   values from 1 to 100. since it's one dimensional, it can be plotted on
   a line. to get a representative sample, you'd need to sample something
   like - $f(1),f(20),f(40),f(60),f(80),f(100)$

   now, let's increase the dimensionality i.e. number of dependent
   variables and see what happens. say, we have 2 variables $x_1$ and
   $x_2$, same possible as before - integers between 1 and 100. now,
   instead of a line, we'll have a plane with $x_1$ and $x_2$ on the two
   axes. the interesting bit is that instead of 100 possible values of
   dependent variables like before, we now have 100,000 possible values!
   basically, we can make 100x100 table of possible values of $x_1$ and
   $x_2$. wow, that increased exponentially. not just figuratively, but
   mathematically exponentially. needless to say, to cover 5% of the space
   like we did before, we'd need to sample $f$ at 5000 values.

   for 3 variables, it would be 100,000,000, and we'd need to sample at
   500,000 points. that's already more than the number of data points we
   have for most training problems we will ever come across.

   basically, as the dimensionality (number of features) of the examples
   grows, because a fixed-size training set covers a dwindling fraction of
   the input space. even with a moderate dimension of 100 and a huge
   training set of a trillion examples, the latter covers only a fraction
   of about $10^{   18}$ of the input space. this is what makes machine
   learning both necessary and hard.

   so, yes, if some words are unimportant, we want to get rid of them and
   reduce the dimensionality of our x matrix. and the way we will do it is
   using tf-idf to identify un-important words. python let's us do this
   with just one line of code (and this is why you should spend more time
   reading maths, than coding!)
   in [76]:
# the min_df paramter makes sure we exclude words that only occur very rarely
# the default also is to exclude any words that occur in every movie description
vectorize=countvectorizer(max_df=0.95, min_df=0.005)
x=vectorize.fit_transform(content)

   we are excluding all words that occur in too many or too few documents,
   as these are very unlikely to be discriminative. words that only occur
   in one document most probably are names, and words that occur in nearly
   all documents are probably stop words. note that the values here were
   not tuned using a validation set. they are just guesses. it is ok to
   do, because we didn't evaluate the performance of these parameters. in
   a strict case, for example for a publication, it would be better to
   tune these as well.
   in [77]:
x.shape

   out[77]:
(1595, 1365)

   so, each movie's overview gets represented by a 1x1365 dimensional
   vector.

   now, we are ready for the kill. our data is cleaned, hypothesis is set
   (overview can predict movie genre), and the feature/output vectors are
   prepped. let's train some models!
   in [78]:
import pickle
f4=open('x.pckl','wb')
f5=open('y.pckl','wb')
pickle.dump(x,f4)
pickle.dump(y,f5)
f6=open('genredict.pckl','wb')
pickle.dump(genre_id_to_name,f6)
f4.close()
f5.close()
f6.close()

congratulations, we have our data set ready![48]  

   a note : as we are building our own dataset, and i didn't want you to
   spend all your time waiting for poster image downloads to finish, i am
   working with an extremely small dataset. that is why, the results we
   will see for the deep learning portion will not be spectacular as
   compared to conventional machine learning methods. if you want to see
   the real power, you should spend some more time scraping something of
   the order of 100,000 images, as opposed to 1000 odd like i am doing
   here. quoting the paper i mentioned above - more data beats a cleverer
   algorithm.

as the ta, i saw that most teams working on the project had data of the order
of 100,000 movies. so, if you want to extract the power of these models,
consider scraping a larger dataset than me.[49]  

section 5 - non-deep, conventional ml models with above data[50]  

   here is a layout of what we will be doing -
     * we will implement two different models
     * we will decide a performance metric i.e. a quantitative method to
       be sure about how well difference models are doing.
     * discussion of the differences between the models, their strengths,
       weaknesses, etc.

   as discussed earlier, there are a lot of implementation decisions to be
   made. between feature engineering, hyper-parameter tuning, model
   selection and how interpretable do you want your model to be (read :
   bayesian vs non-bayesian approaches) a lot is to be decided. for
   example, some of these models could be:
     * generalized linear models
     * id166
     * shallow (1 layer, i.e. not deep) neural network
     * id79
     * boosting
     * decision tree

   or go more bayesian:
     * naive bayes
     * linear or quadratic discriminant analysis
     * bayesian id187

   the list is endless, and not all models will make sense for the kind of
   problem you have framed for yourself. think about which model best fits
   for your purpose.

   for our purposes here, i will be showing the example of 2 very simple
   models, one picked from each category above -
    1. id166
    2. multinomial naive bayes

   a quick overview of the whole pipeline coming below:
     * a little bit of feature engineering
     * 2 different models
     * id74 chosen
     * model comparisons

let's start with some feature engineering.[51]  

   engineering the right features depends on 2 key ideas. firstly, what is
   it that you are trying to solve? for example, if you want to guess my
   music preferences and you try to train a super awesome model while
   giving it what my height is as input features, you're going to have no
   luck. on the other hand, giving it my spotify playlist will solve the
   problem with any model. so, context of the problem plays a role.

   second, you can only represent based on the data at hand. meaning, if
   you didn't have access to my spotify playlist, but to my facebook
   statuses - you know all my statuses about harvard may not be useful.
   but if you represent me as my facebook statuses which are youtube
   links, that would also solve the problem. so, availability of data at
   hand is the second factor.

a nice way to think of it is to think that you start with the problem at
hand, but design features constrained by the data you have available. if you
have many independent features that each correlate well with the class,
learning is easy. on the other hand, if the class is a very complex function
of the features, you may not be able to learn it.[52]  

   in the context of this problem, we would like to predict the genre of a
   movie. what we have access to - movie overviews, which are text
   descriptions of the movie plot. the hypothesis makes sense, overview is
   a short description of the story and the story is clearly important in
   assigning genres to movies.

   so, let's improve our features by playing with the words in the
   overviews in our data. one interesting way to go back to what we
   discussed earlier - tf-idf. we originally used it to filter words, but
   we can also assign the tf-idf values as "importance" values to words,
   as opposed to treating them all equally. tf-idf simply tries to
   identify the assign a weightage to each word in the bag of words.

   once again, the way it works is - most movie descriptions have the word
   "the" in it. obviously, it doesn't tell you anything special about it.
   so weightage should be inversely proportional to how many movies have
   the word in their description. this is the idf part.

   on the other hand, for the movie interstellar, if the description has
   the word space 5 times, and wormhole 2 times, then it's probably more
   about space than about wormhole. thus, space should have a high
   weightage. this is the tf part.

   we simply use tf-idf to assign weightage to every word in the bag of
   words. which makes sense, right? :)
   in [79]:
from sklearn.feature_extraction.text import tfidftransformer

   in [80]:
tfidf_transformer = tfidftransformer()
x_tfidf = tfidf_transformer.fit_transform(x)
x_tfidf.shape

   out[80]:
(1595, 1365)

   let's divide our x and y matrices into train and test split. we train
   the model on the train split, and report the performance on the test
   split. think of this like the questions you do in the problem sets v/s
   the exam. of course, they are both (assumed to be) from the same
   population of questions. and doing well on problem sets is a good
   indicator that you'll do well in exams, but really, you must test
   before you can make any claims about you knowing the subject.
   in [81]:
msk = np.random.rand(x_tfidf.shape[0]) < 0.8

   in [82]:
x_train_tfidf=x_tfidf[msk]
x_test_tfidf=x_tfidf[~msk]
y_train=y[msk]
y_test=y[~msk]
positions=range(len(movies_with_overviews))
# print positions
test_movies=np.asarray(positions)[~msk]
# test_movies

   in [83]:
from sklearn.multiclass import onevsrestclassifier
from sklearn.id166 import svc
from sklearn.model_selection import gridsearchcv
from sklearn.metrics import f1_score
from sklearn.metrics import make_scorer
from sklearn.metrics import classification_report

   in [98]:
parameters = {'kernel':['linear'], 'c':[0.01, 0.1, 1.0]}
gridcv = gridsearchcv(svc(class_weight='balanced'), parameters, scoring=make_sco
rer(f1_score, average='micro'))
classif = onevsrestclassifier(gridcv)

classif.fit(x_train_tfidf, y_train)

   out[98]:
onevsrestclassifier(estimator=gridsearchcv(cv=none, error_score='raise',
       estimator=svc(c=1.0, cache_size=200, class_weight='balanced', coef0=0.0,
  decision_function_shape=none, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, id203=false, random_state=none, shrinking=true,
  tol=0.001, verbose=false),...refit=true, return_train_score=true,
       scoring=make_scorer(f1_score, average=micro), verbose=0),
          n_jobs=1)

   in [99]:
predstfidf=classif.predict(x_test_tfidf)

print classification_report(y_test, predstfidf, target_names=genre_names)

                 precision    recall  f1-score   support

      adventure       0.42      0.57      0.48        56
        fantasy       0.46      0.67      0.55        45
      animation       0.27      0.42      0.33        31
          drama       0.60      0.57      0.58       132
         horror       0.00      0.00      0.00        41
         action       0.49      0.67      0.57        70
         comedy       0.40      0.53      0.46        77
        history       0.40      0.37      0.38        27
        western       0.33      0.14      0.20         7
       thriller       0.26      1.00      0.41        76
          crime       0.46      0.48      0.47        46
    documentary       0.61      0.67      0.64        21
science fiction       0.12      1.00      0.22        36
        mystery       0.23      0.37      0.29        35
          music       0.95      0.59      0.73        34
        romance       0.35      0.50      0.41        46
         family       0.32      0.38      0.35        42
            war       0.27      0.44      0.33         9
        foreign       0.00      0.00      0.00         4
       tv movie       0.00      0.00      0.00         5

    avg / total       0.40      0.56      0.44       840


   as you can see, the performance is by and large poorer for movies which
   are less represented like war and animation, and better for categories
   like drama.

   numbers aside, let's look at our model's predictions for a small sample
   of movies from our test set.
   in [101]:
genre_list=sorted(list(genre_id_to_name.keys()))

   in [102]:
predictions=[]
for i in range(x_test_tfidf.shape[0]):
    pred_genres=[]
    movie_label_scores=predstfidf[i]
#     print movie_label_scores
    for j in range(20):
        #print j
        if movie_label_scores[j]!=0:
            genre=genre_id_to_name[genre_list[j]]
            pred_genres.append(genre)
    predictions.append(pred_genres)

   in [103]:
import pickle
f=open('classifer_svc','wb')
pickle.dump(classif,f)
f.close()

   in [115]:
for i in range(x_test_tfidf.shape[0]):
    if i%50==0 and i!=0:
        print 'movie: ',movies_with_overviews[i]['title'],'\tprediction: ',','.j
oin(predictions[i])

movie:  the walk        prediction:  adventure,fantasy,animation,action,thriller
,science fiction
movie:  cinderella      prediction:  adventure,fantasy,action,thriller,science f
iction
movie:  liza, the fox-fairy     prediction:  drama,thriller,science fiction,roma
nce,war
movie:  the polar express       prediction:  adventure,action,thriller,science f
iction,family
movie:  patema inverted         prediction:  thriller,science fiction,music

   let's try our second model? the naive bayes model.
   in [116]:
from sklearn.naive_bayes import multinomialnb
classifnb = onevsrestclassifier(multinomialnb())
classifnb.fit(x[msk].toarray(), y_train)
predsnb=classifnb.predict(x[~msk].toarray())

   in [117]:
import pickle
f2=open('classifer_nb','wb')
pickle.dump(classifnb,f2)
f2.close()

   in [118]:
predictionsnb=[]
for i in range(x_test_tfidf.shape[0]):
    pred_genres=[]
    movie_label_scores=predsnb[i]
    for j in range(20):
        #print j
        if movie_label_scores[j]!=0:
            genre=genre_id_to_name[genre_list[j]]
            pred_genres.append(genre)
    predictionsnb.append(pred_genres)

   in [120]:
for i in range(x_test_tfidf.shape[0]):
    if i%50==0 and i!=0:
        print 'movie: ',movies_with_overviews[i]['title'],'\tprediction: ',','.j
oin(predictionsnb[i])

movie:  the walk        prediction:  adventure,fantasy,animation,science fiction
movie:  cinderella      prediction:  adventure,action,science fiction
movie:  liza, the fox-fairy     prediction:  drama,romance
movie:  the polar express       prediction:  science fiction
movie:  patema inverted         prediction:  documentary,music

   as can be seen above, the results seem promising, but how do we really
   compare the two models? we need to quantify our performance so that we
   can say which one's better. takes us back to what we discussed right in
   the beginning - we're learning a function $g$ which can approximate the
   original unknown function $f$. for some values of $x_i$, the
   predictions will be wrong for sure, and we want to minimize it.

   for multi label systems, we often keep track of performance using
   "precision" and "recall". these are standard metrics, and you can
   google to read up more about them if you're new to these terms.

id74[53]  

   we will use the standard precision recall metrics for evaluating our
   system.
   in [46]:
def precision_recall(gt,preds):
    tp=0
    fp=0
    fn=0
    for t in gt:
        if t in preds:
            tp+=1
        else:
            fn+=1
    for p in preds:
        if p not in gt:
            fp+=1
    if tp+fp==0:
        precision=0
    else:
        precision=tp/float(tp+fp)
    if tp+fn==0:
        recall=0
    else:
        recall=tp/float(tp+fn)
    return precision,recall

   in [122]:
precs=[]
recs=[]
for i in range(len(test_movies)):
    if i%1==0:
        pos=test_movies[i]
        test_movie=movies_with_overviews[pos]
        gtids=test_movie['genre_ids']
        gt=[]
        for g in gtids:
            g_name=genre_id_to_name[g]
            gt.append(g_name)
#         print predictions[i],movies_with_overviews[i]['title'],gt
        a,b=precision_recall(gt,predictions[i])
        precs.append(a)
        recs.append(b)

print np.mean(np.asarray(precs)),np.mean(np.asarray(recs))

0.33085149314 0.570960451977

   in [123]:
precs=[]
recs=[]
for i in range(len(test_movies)):
    if i%1==0:
        pos=test_movies[i]
        test_movie=movies_with_overviews[pos]
        gtids=test_movie['genre_ids']
        gt=[]
        for g in gtids:
            g_name=genre_id_to_name[g]
            gt.append(g_name)
#         print predictions[i],movies_with_overviews[i]['title'],gt
        a,b=precision_recall(gt,predictionsnb[i])
        precs.append(a)
        recs.append(b)

print np.mean(np.asarray(precs)),np.mean(np.asarray(recs))

0.48893866021 0.549604519774

   the average precision and recall scores for our samples are pretty
   good! models seem to be working! also, we can see that the naive bayes
   performs outperforms id166. i strongly suggest you to go read about
   multinomial bayes and think about why it works so well for "document
   classification", which is very similar to our case as every movie
   overview can be thought of as a document we are assigning labels to.

section 6 - deep learning : an intuitive overview[54]  

   the above results were good, but it's time to bring out the big guns.
   so first and foremost, let's get a very short idea about what's deep
   learning. this is for peope who don't have background in this - it's
   high level and gives just the intuition.

   as described above, the two most immportant concepts in doing good
   classification (or regression) are to 1) use the right representation
   which captures the right information about the data which is relavant
   to the problem at hand 2) using the right model which has the
   capability of making sense of the representation fed to it.

   while for the second part we have complicated and powerful models that
   we have studied at length, we don't seem to have a principled,
   mathematical way of doing the first part - i.e. representation. what we
   did above was to see "what makes sense", and go from there. that is not
   a good approach for complex data/ complex problems. is there some way
   to automate this? deep learning, does just this.

   to just emphasize the importance of representation in the complex tasks
   we usually attempt with deep learning, let me talk about the original
   problem which made it famous. the paper is often reffered to as the
   "id163 challenge paper", and it was basically working on object
   recognition in images. let's try to think about an algorithm that tries
   to detect a chair.

if i ask you to "define" a chair, how would you? - something with 4
legs?[55]  

   [chair1.png]

       all are chairs, none with 4 legs. (pic credit: zoya bylinskii)

how about some surface that we sit on then?[56]  

   [chair2.png]

       all are surfaces we sit on, none are chairs. (pic credit: zoya
                                 bylinskii)

   clearly, these definitions won't work and we need something more
   complicated. sadly, we can't come up with a simple text rule that our
   computer can search for! and we take a more principled approach.

   the "deep" in the deep learning comes from the fact that it was
   conventionally applied to neural networks. neural networks, as we all
   know, are structures organized in layers. layers of computations. why
   do we need layers? because these layers can be seen as sub-tasks that
   we do in the complicated task of identifying a chair. it can be thought
   as a heirarchical break down of a complicated job into smalled
   sub-tasks.

   mathematically, each layer acts like a space transformation which takes
   the pixel values to a high dimensional space. when we start out, every
   pixel in the image is given equal importance in our matrix. with each
   layer, convolution operations give some parts more importance, and some
   lesser importance. in doing so, we transform our images to a space in
   which similar looking objects/object parts are closer (we are basically
   learning this space transformation in deep learning, nothing else)

   what exactly was learnt by these neural networks is hard to know, and
   an active area of research. but one very crude way to visualize what it
   does is to think like - it starts by learning very generic features in
   the first layer. something as simple as vertical and horizontal lines.
   in the next layer, it learns that if you combine the vectors
   representing vertical and horizontal vectors in different ratios, you
   can make all possible slanted lines. next layer learns to combine lines
   to form curves - say, something like the outline of a face. these
   curves come together to form 3d objects. and so on. building
   sub-modules, combining them in the right way which can give it
   semantics.

   so, in a nutshell, the first few layers of a "deep" network learn the
   right representation of the data, given the problem (which is
   mathematically described by your objective function trying to minimize
   difference between ground truth and predicted labels). the last layer
   simply looks how close or far apart things are in this high dimensional
   space.

   hence, we can give any kind of data a high dimensional representation
   using neural networks. below we will see high dimensional
   representations of both words in overviews (text) and posters (image).
   let's get started with the posters i.e. extracting visual features from
   posters using deep learning.

section 7 - deep learning for predicting genre from poster[57]  

   once again, we must make an implementation decision. this time, it has
   more to do with how much time are we willing to spend in return for
   added accuracy. we are going to use here a technique that is commonly
   referred to as pre-training in machine learning literature.

   instead of me trying to re-invent the wheel here, i am going to borrow
   this short section on pre-training from stanford university's lecture
   on [58]id98. to quote -

   ''in practice, very few people train an entire convolutional network
   from scratch (with random initialization), because it is relatively
   rare to have a dataset of sufficient size. instead, it is common to
   pretrain a convnet on a very large dataset (e.g. id163, which
   contains 1.2 million images with 1000 categories), and then use the
   convnet either as an initialization or a fixed feature extractor for
   the task of interest. ''

   there are three broad ways in which id21 or pre-training
   can be done. (the 2 concepts are different and to understand the
   difference clearly, i suggest you read the linked lecture thoroughly).
   the way we are going to about it is by using a pre-trained, released
   convnet as feature extractor. take a convnet pretrained on id163 (a
   popular id164 dataset), remove the last fully-connected
   layer. after removing the last layer, what we have is just another
   neural network i.e. a stack of space tranformations. but, originally
   the output of this stack can be pumped into a single layer which can
   classify the image into categories like car, dog, cat and so on.

   what this means, is that in the space this stack transforms the images
   to, all images which contain a "dog" are closer to each other, and all
   images containing a "cat" are closer. thus, it is a meaningful space
   where images with similar objects are closer.

   think about it, now if we pump our posters through this stack, it will
   embed them in a space where posters which contain similar objects are
   closer. this is a very meaningful feature engineering method! while
   this may not be ideal for genre prediction, it might be quite
   meaningful. for example, all posters with a gun or a car are probably
   action. while a smiling couple would point to romance or drama. the
   alternative would be to train the id98 from scratch which is fairly
   computationally intensive and involves a lot of tricks to get the id98
   training to converge to the optimal space tranformation.

   this way, we can start off with something strong, and then build on
   top. we pump our images through the pre-trained network to extract the
   visual features from the posters. then, using these features as
   descriptors for the image, and genres as the labels, we train a simpler
   neural network from scratch which learns to do simply classification on
   this dataset. these 2 steps are exactly what we are going to do for
   predicting genres from movie posters.

deep learning to extract visual features from posters[59]  

   the basic problem here we are answering is that can we use the posters
   to predict genre. first check - does this hypothesis make sense? yes.
   because that's what graphic designers do for a living. they leave
   visual cues to semantics. they make sure that when we look at the
   poster of a horror movie, we know it's not a happy image. things like
   that. can our deep learning system infer such subtleties? let's find
   out!

   for visual features, either we can train a deep neural network
   ourselves from scratch, or we can use a pre-trained one made available
   to us from the visual geometry group at oxford university, one of the
   most popular methods. this is called the vgg-net. or as they call it,
   we will extract the vgg features of an image. mathematically, as
   mentioned, it's just a space transformation in the form of layers. so,
   we simply need to perform this chain of transformations on our image,
   right? keras is a library that makes it very easy for us to do this.
   some other common ones are tensorflow and pytorch. while the latter two
   are very powerful and customizable and used more often in practice,
   keras makes it easy to prototype by keeping the syntax simple.

   we will be working with keras to keep things simple in code, so that we
   can spend more time understanding and less time coding. some common
   ways people refer to this step are - "getting the vgg features of an
   image", or "forward propogating the image through vgg and chopping off
   the last layer". in keras, this is as easy as writing 4 lines.
   in [3]:
# loading the list of movies we had downloaded posters for eariler -
f=open('poster_movies.pckl','r')
poster_movies=pickle.load(f)
f.close()

   in [4]:
from keras.applications.vgg16 import vgg16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np
import pickle
model = vgg16(weights='id163', include_top=false)

using tensorflow backend.

   in [467]:
allnames=os.listdir(poster_folder)
imnames=[j for j in allnames if j.endswith('.jpg')]
feature_list=[]
genre_list=[]
file_order=[]
print "starting extracting vgg features for scraped images. this will take time,
 please be patient..."
print "total images = ",len(imnames)
failed_files=[]
succesful_files=[]
i=0
for mov in poster_movies:
    i+=1
    mov_name=mov['original_title']
    mov_name1=mov_name.replace(':','/')
    poster_name=mov_name.replace(' ','_')+'.jpg'
    if poster_name in imnames:
        img_path=poster_folder+poster_name
        try:
            img = image.load_img(img_path, target_size=(224, 224))
            succesful_files.append(imname)
            x = image.img_to_array(img)
            x = np.expand_dims(x, axis=0)
            x = preprocess_input(x)
            features = model.predict(x)
            file_order.append(img_path)
            feature_list.append(features)
            genre_list.append(mov['genre_ids'])
            if np.max(np.asarray(feature_list))==0.0:
                print('problematic',i)
            if i%250==0 or i==1:
                print "working on image : ",i
        except:
            failed_files.append(imname)
            continue

    else:
        continue
print "done with all features, please pickle for future use!"

starting extracting vgg features for scraped images. this will take time, please
 be patient...
total images =  1347
working on image :  250
working on image :  500
working on image :  750
working on image :  1000
working on image :  1250
working on image :  1500
done with all features, please pickle for future use!

   in [476]:
len(genre_list)

   out[476]:
1317

   in [477]:
len(feature_list)

   out[477]:
1317

   in [ ]:


   in [472]:
# reading from pickle below, this code is not to be run.
list_pickled=(feature_list,file_order,failed_files,succesful_files,genre_list)
f=open('posters_new_features.pckl','wb')
pickle.dump(list_pickled,f)
f.close()
print("features dumped to pickle file")

features dumped to pickle file

   in [5]:
f7=open('posters_new_features.pckl','rb')
list_pickled=pickle.load(f7)
f7.close()
# (feature_list2,file_order2)=list_pickled

training a simple neural network model using these vgg features.[60]  

   in [6]:
(feature_list,files,failed,succesful,genre_list)=list_pickled

   let's first get the labels on our 1342 samples first! as image download
   fails on a few instances, the best way to work with the right model is
   to read the poster names downloaded, and working from there. these
   posters cannot be uploaded to github as they are too large, and so are
   being downloaded and read from my local computer. if you do re-do it,
   you might have to check and edit the paths in the code to make sure it
   runs.
   in [7]:
(a,b,c,d)=feature_list[0].shape
feature_size=a*b*c*d

   this looks odd, why are we re-running the loop we ran above again
   below? the reason is simple, the most important thing to know about
   numpy is that using vstack() and hstack() are highly sub-optimal. numpy
   arrays when created, a fixed size is allocated in the memory and when
   we stack, a new one is copied and created in a new location. this makes
   the code really, really slow. the best way to do it (and this remains
   the same with matlab matrices if you work with them), is to create a
   numpy array of zeros, and over-write it row by row. the above code was
   just to see what size numpy array we will need!

   the final movie poster set for which we have all the information we
   need, is 1265 movies. in the above code we are making an x numpy array
   containing the visual features of one image per row. so, the vgg
   features are reshaped to be in the shape (1,25088) and we finally
   obtain a matrix of shape (1265,25088)
   in [8]:
np_features=np.zeros((len(feature_list),feature_size))
for i in range(len(feature_list)):
    feat=feature_list[i]
    reshaped_feat=feat.reshape(1,-1)
    np_features[i]=reshaped_feat

   in [9]:
# np_features[-1]

   in [10]:
x=np_features

   in [11]:
from sklearn.preprocessing import multilabelbinarizer
mlb=multilabelbinarizer()
y=mlb.fit_transform(genre_list)

   in [12]:
y.shape

   out[12]:
(1317, 20)

   our binarized y numpy array contains the binarized labels corresponding
   to the genre ids of the 1277 movies
   in [503]:
visual_problem_data=(x,y)
f8=open('visual_problem_data_clean.pckl','wb')
pickle.dump(visual_problem_data,f8)
f8.close()

   in [504]:
f8=open('visual_problem_data_clean.pckl','rb')
visual_features=pickle.load(f8)
f8.close()

   in [505]:
(x,y)=visual_features

   in [506]:
x.shape

   out[506]:
(1317, 25088)

   in [507]:
mask = np.random.rand(len(x)) < 0.8

   in [508]:
x_train=x[mask]
x_test=x[~mask]
y_train=y[mask]
y_test=y[~mask]

   in [510]:
x_test.shape

   out[510]:
(264, 25088)

   now, we create our own keras neural network to use the vgg features and
   then classify movie genres. keras makes this super easy.

   neural network architectures have gotten complex over the years. but
   the simplest ones contain very standard computations organized in
   layers, as described above. given the popularity of some of these,
   keras makes it as easy as writing out the names of these operations in
   a sequential order. this way you can make a network while completely
   avoiding the mathematics (highly recommended spending more time on the
   math though)

   sequential() allows us to make models the follow this sequential order
   of layers. different kinds of layers like dense, conv2d etc can be
   used, and many id180 like relu, linear etc are also
   available.

important question : why do we need id180?[61]  

copy pasting the answer i wrote for this question on [62]quora feel free to
leave comments there.[63]  

   ""sometimes, we tend to get lost in the jargon and confuse things
   easily, so the best way to go about this is getting back to our basics.

   don   t forget what the original premise of machine learning (and thus
   deep learning) is - if the input and output are related by a function
   y=f(x), then if we have x, there is no way to exactly know f unless we
   know the process itself. however, machine learning gives you the
   ability to approximate f with a function g, and the process of trying
   out multiple candidates to identify the function g best approximating f
   is called machine learning.

   ok, that was machine learning, and how is deep learning different? deep
   learning simply tries to expand the possible kind of functions that can
   be approximated using the above mentioned machine learning paradigm.
   roughly speaking, if the previous model could learn say 10,000 kinds of
   functions, now it will be able to learn say 100,000 kinds (in actuality
   both are infinite spaces but one is larger than the other, because
   maths is cool that ways.)

   if you want to know the mathematics of it, go read about vc dimension
   and how more layers in a network affect it. but i will avoid the
   mathematics here and rely on your intuition to believe me when i say
   that not all data can be classified correctly into categories using a
   linear function. so, we need our deep learning model to be able to
   approximate more complex functions than just a linear function.

   now, let   s come to your non linearity bit. imagine a linear function
   y=2x+3, and another one y=4x+7. what happens if i pool them and take an
   average? i get another linear function y= 3x+5. so instead of doing
   those two computations separately and then averaging it out, i could
   have just used the single linear function y=3x+5. obviously, this logic
   holds good if i have more than 2 such linear functions. this is exactly
   what will happen if you don   t have have non-linearities in your nodes,
   and also what others have written in their answers.

   it simply follows from the definition of a linear function -

   (i) if you take two linear functions, and

   (ii)take a linear combination of them (which is how we combine the
   outputs of multiple nodes of a network)

   you are bound to get a linear function because
   f(x)+g(x)=mx+b+nx+c=(m+n)x+(b+c)= say h(x).

   and you could in essence replace your whole network by a simple matrix
   transformation which accounts for all linear combinations and
   up/downsamplings.

   in a nutshell, you   ll only be trying to learn a linear approximation
   for original function f relating the input and the output. which as we
   discussed above, is not always the best approximation. adding
   non-linearities ensures that you can learn more complex functions by
   approximating every non-linear function as a linear combination of a
   large number of non-linear functions.

   still new to the field, so if there   s something wrong here please
   comment below! hope it helps""

let's train our model then, using the features we extracted from vgg net[64]  

   the model we will use has just 1 hidden layer between the vgg features
   and the final output layer. the simplest neural network you can get. an
   image goes into this network with the dimensions (1,25088), the first
   layer's output is 1024 dimensional. this hidden layer output undergoes
   a pointwise relu activation. this output gets transformed into the
   output layer of 20 dimensions. it goes through a sigmoid.

   the sigmoid, or the squashing function as it is often called, is a
   function which squashes numbers between 0 and 1. what are you reminded
   of when you think of numebers between 0 and 1? right, id203.

   by squashing the score of each of the 20 output labels between 0 and 1,
   sigmoid lets us interpret their scores as probabilities. then, we can
   just pick the classes with the top 3 or 5 id203 scores as the
   predicted genres for the movie poster! simple!
   in [412]:
# y_train[115]

   in [549]:
from keras.models import sequential
from keras.layers import dense, activation
from keras import optimizers
model_visual = sequential([
    dense(1024, input_shape=(25088,)),
    activation('relu'),
    dense(256),
    activation('relu'),
    dense(20),
    activation('sigmoid'),
])
opt = optimizers.rmsprop(lr=0.0001, decay=1e-6)

#sgd = optimizers.sgd(lr=0.05, decay=1e-6, momentum=0.4, nesterov=false)
model_visual.compile(optimizer=opt,
              loss='binary_crossid178',
              metrics=['accuracy'])

   we train the model using the fit() function. the parameters it takes
   are - training features and training labels, epochs, batch_size and
   verbose.

   simplest one - verbose. 0="dont print anything as you work", 1="inform
   me as you go".

   often the data set is too large to be loaded into the ram. so, we load
   data in batches. for batch_size=32 and epochs=10, the model starts
   loading rows from x in batches of 32 everytime it calculates the loss
   and updates the model. it keeps on going till it has covered all the
   samples 10 times.

   so, the no. of times model is updated = (total samples/batch size) *
   (epochs)
   in [550]:
model_visual.fit(x_train, y_train, epochs=10, batch_size=64,verbose=1)

epoch 1/10
1053/1053 [==============================] - 7s - loss: 2.3120 - acc: 0.8160

epoch 2/10
1053/1053 [==============================] - 6s - loss: 1.6045 - acc: 0.8724

epoch 3/10
1053/1053 [==============================] - 7s - loss: 1.1555 - acc: 0.9078

epoch 4/10
1053/1053 [==============================] - 7s - loss: 0.9149 - acc: 0.9264

epoch 5/10
1053/1053 [==============================] - 6s - loss: 0.7455 - acc: 0.9420

epoch 6/10
1053/1053 [==============================] - 6s - loss: 0.6218 - acc: 0.9522

epoch 7/10
1053/1053 [==============================] - 7s - loss: 0.5393 - acc: 0.9623

epoch 8/10
1053/1053 [==============================] - 7s - loss: 0.5104 - acc: 0.9645

epoch 9/10
1053/1053 [==============================] - 6s - loss: 0.4923 - acc: 0.9644

epoch 10/10
1053/1053 [==============================] - 6s - loss: 0.4475 - acc: 0.9648


   out[550]:
<keras.callbacks.history at 0x262226650>

   in [551]:
model_visual.fit(x_train, y_train, epochs=50, batch_size=64,verbose=0)

   out[551]:
<keras.callbacks.history at 0x2630d8cd0>

   for the first 10 epochs i trained the model in a verbose fashion to
   show you what's happening. after that, in the below cell you can see i
   turned off the verbosity to keep the code cleaner.
   in [552]:
y_preds=model_visual.predict(x_test)

   in [553]:
sum(sum(y_preds))

   out[553]:
438.26369654744053

let's look at some of our predictions?[65]  

   in [554]:
f6=open('genredict.pckl','rb')
genre_id_to_name=pickle.load(f6)
f6.close()

   in [555]:
sum(y_preds[1])

   out[555]:
2.0173444656837738

   in [556]:
sum(y_preds[2])

   out[556]:
2.0

   in [557]:
genre_list=sorted(list(genre_id_to_name.keys()))

   in [558]:
precs=[]
recs=[]
for i in range(len(y_preds)):
    row=y_preds[i]
    gt_genres=y_test[i]
    gt_genre_names=[]
    for j in range(20):
        if gt_genres[j]==1:
            gt_genre_names.append(genre_id_to_name[genre_list[j]])
    top_3=np.argsort(row)[-3:]
    predicted_genres=[]
    for genre in top_3:
        predicted_genres.append(genre_id_to_name[genre_list[genre]])
    (precision,recall)=precision_recall(gt_genre_names,predicted_genres)
    precs.append(precision)
    recs.append(recall)
    if i%50==0:
        print "predicted: ",','.join(predicted_genres)," actual: ",','.join(gt_g
enre_names)

predicted:  adventure,science fiction,action  actual:  adventure,action,comedy,r
omance
predicted:  romance,science fiction,adventure  actual:  drama
predicted:  adventure,fantasy,action  actual:  fantasy,drama,action,western,thri
ller
predicted:  documentary,science fiction,action  actual:  drama,action,thriller,s
cience fiction
predicted:  thriller,action,science fiction  actual:  horror,action,thriller,mys
tery
predicted:  comedy,animation,family  actual:  animation,comedy,family

   in [559]:
print np.mean(np.asarray(precs)),np.mean(np.asarray(recs))

0.489898989899 0.500220959596

   so, even with just the poster i.e. visual features we are able to make
   great predictions! sure, text outperforms the visual features, but the
   important thing is that it still work. in more complicated models, we
   can combine the two to make even better predictions. that is precisely
   what i work on in my research.

   these models were trained on cpu's, and a simple 1 layer model was used
   to show that there is a lot of information in this data that the models
   can extract. with a larger dataset, and more training i was able to
   bring these numbers to as high as 70%, which is the similar to textual
   features. some teams in my class outperformed this even more. more data
   is the first thing you should try if you want better results. then, you
   can start playing with training on gpus, learning rate schedules and
   other hyperparameters. finally, you can consider using resnet, a much
   more powerful neural network model than vgg. all of these can be tried
   once you have a working knowledge of machine learning.

section 8 - deep learning to get textual features[66]  

   let's do the same thing as above with text now?

   we will use an off the shelf representation for words - id97 model.
   just like vggnet before, this is a model made available to get a
   meaningful representation. as the total number of words is small, we
   don't even need to forward propagate our sample through a network. even
   that has been done for us, and the result is stored in the form of a
   dictionary. we can simply look up the word in the dictionary and get
   the id97 features for the word.

   you can download the dictionary from here -
   [67]https://drive.google.com/file/d/0b7xkcwpi5kdynlnuttlss21pqmm/edit
   download it to the directory of this tutorial i.e. in the same folder
   as this ipython notebook.
   in [226]:
from gensim import models
# model2 = models.id97.load_id97_format('googlenews-vectors-negative300.
bin', binary=true)
model2 = models.keyedvectors.load_id97_format('googlenews-vectors-negative30
0.bin', binary=true)

   now, we can simply look up for a word in the above loaded model. for
   example, to get the id97 representation of the word "king" we just
   do - model2['king']
   in [227]:
print model2['king'].shape
print model2['dog'].shape

(300,)
(300,)

   this way, we can represent the words in our overviews using this
   id97 model. and then, we can use that as our x representations. so,
   instead of count of words, we are using a representation which is based
   on the semantic representation of the word. mathematically, each word
   went from 3-4 dimensional (the length) to 300 dimensions!

   for the same set of movies above, let's try and predict the genres from
   the deep representation of their overviews!
   in [228]:
len(final_movies_set)

   out[228]:
1265

   in [231]:
from nltk.tokenize import regexptokenizer
from stop_words import get_stop_words
tokenizer = regexptokenizer(r'\w+')

# create english stop words list
en_stop = get_stop_words('en')

   in [232]:
movie_mean_wordvec=np.zeros((len(final_movies_set),300))
movie_mean_wordvec.shape

   out[232]:
(1265, 300)

   text needs some pre-processing before we can train the model. the only
   preprocessing we do here is - we delete commonly occurring words which
   we know are not informative about the genre. think of it as the clutter
   in some sense. these words are often removed and are referred to as
   "stop words". you can look them up online. these include simple words
   like "a", "and", "but", "how", "or" and so on. they can be easily
   removed using the python package nltk.

   from the above dataset, movies with overviews which contain only stop
   words, or movies with overviews containing no words with id97
   representation are neglected. others are used to build our mean
   id97 representation. simply, put for every movie overview -
     * take movie overview
     * throw out stop words
     * for non stop words:
          + if in id97 - take it's id97 representation which is
            300 dimensional
          + if not - throw word
     * for each movie, calculate the arithmetic mean of the 300
       dimensional vector representations for all words in the overview
       which weren't thrown out

   this mean becomes the 300 dimensional representation for the movie. for
   all movies, these are stored in a numpy array. so the x matrix becomes
   (1263,300). and, y is (1263,20) i.e. binarized 20 genres, as before

   why do we take the arithmetic mean? if you feel that we should have
   kept all the words separately - then you're thinking correct, but sadly
   we're limited by the way current day neural networks work. i will not
   mull over this for the fear of stressing too much on an otherwise
   irrelevant detail. but if you're interested, read this awesome paper -
   [68]https://jiajunwu.com/papers/dmil_cvpr.pdf
   in [241]:
genres=[]
rows_to_delete=[]
for i in range(len(final_movies_set)):
    mov=final_movies_set[i]
    movie_genres=mov['genre_ids']
    genres.append(movie_genres)
    overview=mov['overview']
    tokens = tokenizer.tokenize(overview)
    stopped_tokens = [k for k in tokens if not k in en_stop]
    count_in_vocab=0
    s=0
    if len(stopped_tokens)==0:
        rows_to_delete.append(i)
        genres.pop(-1)
#         print overview
#         print "sample ",i,"had no nonstops"
    else:
        for tok in stopped_tokens:
            if tok.lower() in model2.vocab:
                count_in_vocab+=1
                s+=model2[tok.lower()]
        if count_in_vocab!=0:
            movie_mean_wordvec[i]=s/float(count_in_vocab)
        else:
            rows_to_delete.append(i)
            genres.pop(-1)
#             print overview
#             print "sample ",i,"had no id97"

   in [242]:
len(genres)

   out[242]:
1261

   in [ ]:


   in [243]:
mask2=[]
for row in range(len(movie_mean_wordvec)):
    if row in rows_to_delete:
        mask2.append(false)
    else:
        mask2.append(true)

   in [244]:
x=movie_mean_wordvec[mask2]

   in [245]:
x.shape

   out[245]:
(1261, 300)

   in [246]:
y=mlb.fit_transform(genres)

   in [247]:
y.shape

   out[247]:
(1261, 20)

   in [248]:
textual_features=(x,y)
f9=open('textual_features.pckl','wb')
pickle.dump(textual_features,f9)
f9.close()

   in [249]:
# textual_features=(x,y)
f9=open('textual_features.pckl','rb')
textual_features=pickle.load(f9)
f9.close()

   in [250]:
(x,y)=textual_features

   in [251]:
x.shape

   out[251]:
(1261, 300)

   in [252]:
y.shape

   out[252]:
(1261, 20)

   in [253]:
mask_text=np.random.rand(len(x))<0.8

   in [254]:
x_train=x[mask_text]
y_train=y[mask_text]
x_test=x[~mask_text]
y_test=y[~mask_text]

   once again, we use a very similar, super simple architecture as before.
   in [258]:
from keras.models import sequential
from keras.layers import dense, activation

model_textual = sequential([
    dense(300, input_shape=(300,)),
    activation('relu'),
    dense(20),
    activation('softmax'),
])

model_textual.compile(optimizer='rmsprop',
              loss='binary_crossid178',
              metrics=['accuracy'])

   in [259]:
model_textual.fit(x_train, y_train, epochs=10, batch_size=500)

epoch 1/10
982/982 [==============================] - 0s - loss: 0.4819 - acc: 0.8520
epoch 2/10
982/982 [==============================] - 0s - loss: 0.4642 - acc: 0.8520
epoch 3/10
982/982 [==============================] - 0s - loss: 0.4527 - acc: 0.8520
epoch 4/10
982/982 [==============================] - 0s - loss: 0.4456 - acc: 0.8520
epoch 5/10
982/982 [==============================] - 0s - loss: 0.4407 - acc: 0.8520
epoch 6/10
982/982 [==============================] - 0s - loss: 0.4367 - acc: 0.8520
epoch 7/10
982/982 [==============================] - 0s - loss: 0.4332 - acc: 0.8520
epoch 8/10
982/982 [==============================] - 0s - loss: 0.4295 - acc: 0.8520
epoch 9/10
982/982 [==============================] - 0s - loss: 0.4260 - acc: 0.8520
epoch 10/10
982/982 [==============================] - 0s - loss: 0.4227 - acc: 0.8520

   out[259]:
<keras.callbacks.history at 0x4e27e3850>

   in [260]:
model_textual.fit(x_train, y_train, epochs=10000, batch_size=500,verbose=0)

   out[260]:
<keras.callbacks.history at 0x4e27e3a10>

   in [261]:
score = model_textual.evaluate(x_test, y_test, batch_size=249)

249/279 [=========================>....] - eta: 0s

   in [262]:
print("%s: %.2f%%" % (model_textual.metrics_names[1], score[1]*100))

acc: 86.52%

   in [263]:
y_preds=model_textual.predict(x_test)

   in [264]:
genre_list.append(10769)

   in [266]:
print "our predictions for the movies are - \n"
precs=[]
recs=[]
for i in range(len(y_preds)):
    row=y_preds[i]
    gt_genres=y_test[i]
    gt_genre_names=[]
    for j in range(20):
        if gt_genres[j]==1:
            gt_genre_names.append(genre_id_to_name[genre_list[j]])
    top_3=np.argsort(row)[-3:]
    predicted_genres=[]
    for genre in top_3:
        predicted_genres.append(genre_id_to_name[genre_list[genre]])
    (precision,recall)=precision_recall(gt_genre_names,predicted_genres)
    precs.append(precision)
    recs.append(recall)
    if i%50==0:
        print "predicted: ",predicted_genres," actual: ",gt_genre_names

our predictions for the movies are -

predicted:  [u'science fiction', u'action', u'adventure']  actual:  [u'adventure
', u'action', u'comedy', u'romance']
predicted:  [u'thriller', u'crime', u'mystery']  actual:  [u'drama', u'thriller'
, u'science fiction']
predicted:  [u'action', u'crime', u'thriller']  actual:  [u'adventure', u'action
', u'comedy', u'thriller', u'crime']
predicted:  [u'family', u'horror', u'comedy']  actual:  [u'horror', u'action', u
'thriller']
predicted:  [u'crime', u'thriller', u'drama']  actual:  [u'action', u'science fi
ction']
predicted:  [u'drama', u'thriller', u'mystery']  actual:  [u'drama', u'thriller'
, u'mystery', u'romance']

   in [267]:
print np.mean(np.asarray(precs)),np.mean(np.asarray(recs))

0.519713261649 0.563918757467

   even without much tuning of the above model, these results are able to
   beat our previous results.

   note - i got accuracies as high as 78% when doing classification using
   plots scraped from wikipedia. the large amount of information was very
   suitable for movie genre classification with a deep model. strongly
   suggest you to try playing around with architectures.

section 9 - upcoming tutorials and acknowledgements[69]  

   congrats! this is the end of our pilot project! needless to say, a lot
   of the above content may be new to you, or may be things that you know
   very well. if it's the former, i hope this tutorial would have helped
   you. if it is the latter and you think i wrote something incorrect or
   that my understanding can be improved, feel free to create a github
   issue so that i can correct it!

   writing tutorials can take a lot of time, but it is a great learning
   experience. i am currently working on a tutorial focussing on word
   embeddings, which will explore id97 and other id27s in
   detail. while it will take some time to be up, i will post a link to
   it's repository on the readme for this project so that interested
   readers can find it.

   i would like to thank a few of my friends who had an indispensible role
   to play in me making this tutorial. firstly, professor hanspeter
   pfister and verena kaynig at harvard, who helped guide this
   tutorial/project and scope it. secondly, my friends sahil loomba and
   matthew tancik for their suggestions and editing the material and the
   presentation of the storyline. thirdly, zoya bylinskii at mit for
   constantly motivating me to put in my effort into this tutorial.
   finally, all others who helped me feel confident enough to take up this
   task and to see it till the end. thanks all of you!
   in [ ]:

references

   1. https://github.com/spandan-madan/deeplearningproject
   2. https://spandan-madan.github.io/deeplearningproject/#section-1.-introduction
   3. https://spandan-madan.github.io/deeplearningproject/#background
   4. https://spandan-madan.github.io/deeplearningproject/#why-write-yet-another-tutorial-on-machine-learning-and-deep-learning?
   5. https://spandan-madan.github.io/deeplearningproject/#about-the-author
   6. http://spandanmadan.com/
   7. https://spandan-madan.github.io/deeplearningproject/#section-2.-project-outline-:-multi-modal-genre-classification-for-movies
   8. https://spandan-madan.github.io/deeplearningproject/#wow,-that-title-sounds-like-a-handful,-right?-let's-break-it-down-step-by-step.
   9. https://spandan-madan.github.io/deeplearningproject/#q.1.-what-do-we-mean-by-classification?
  10. https://spandan-madan.github.io/deeplearningproject/#more-formally
  11. https://spandan-madan.github.io/deeplearningproject/#given:
  12. https://spandan-madan.github.io/deeplearningproject/#task-:
  13. https://spandan-madan.github.io/deeplearningproject/#problem-:
  14. https://spandan-madan.github.io/deeplearningproject/#assumption-:
  15. https://spandan-madan.github.io/deeplearningproject/#approach-:
  16. https://spandan-madan.github.io/deeplearningproject/#important-consideration-:
  17. https://spandan-madan.github.io/deeplearningproject/#q.2.-what's-multi-modal-classification-then?
  18. https://spandan-madan.github.io/deeplearningproject/#for-this-project,-we-will-be-using-visual-and-textual-data-to-classify-movie-genres.
  19. https://spandan-madan.github.io/deeplearningproject/#project-outline
  20. https://spandan-madan.github.io/deeplearningproject/#section-3.-building-your-very-own-dataset.
  21. https://spandan-madan.github.io/deeplearningproject/#what-data-is-good-data?-or-what-do-you-mean-by-data-being-"representative"?
  22. http://www.jzy3d.org/js/slider/images/contourplotsdemo.png
  23. https://spandan-madan.github.io/deeplearningproject/#we-will-be-scraping-data-from-2-different-movie-sources---imdb-and-tmdb
  24. https://spandan-madan.github.io/deeplearningproject/#note--
  25. https://spandan-madan.github.io/deeplearningproject/#here-is-a-broad-outline-of-technical-steps-to-be-done-for-data-collection
  26. https://spandan-madan.github.io/deeplearningproject/#signing-up-for-tmdb-and-getting-set-up-for-getting-movie-metadata.
  27. https://www.themoviedb.org/?language=en
  28. https://spandan-madan.github.io/deeplearningproject/#using-tmdb-using-the-obtained-api-key-to-get-movie-information
  29. https://spandan-madan.github.io/deeplearningproject/#getting-movie-information-from-imdb
  30. https://spandan-madan.github.io/deeplearningproject/#a-small-comparison-of-imdb-and-tmdb
  31. https://spandan-madan.github.io/deeplearningproject/#working-with-multiple-movies-:-obtaining-top-20-movies-from-tmdb
  32. https://spandan-madan.github.io/deeplearningproject/#yes,-i-know.-i'm-a-little-upset-too-seeing-beauty-and-the-beast-above-logan-in-the-list!
  33. https://spandan-madan.github.io/deeplearningproject/#section-4---building-a-dataset-to-work-with-:-let's-take-a-look-at-the-top-1000-movies-from-the-database
  34. https://spandan-madan.github.io/deeplearningproject/#pairwise-analysis-of-movie-genres
  35. https://spandan-madan.github.io/deeplearningproject/#delving-deeper-into-co-occurrence-of-genres
  36. https://spandan-madan.github.io/deeplearningproject/#interesting-questions
  37. https://spandan-madan.github.io/deeplearningproject/#based-on-this-new-category-set,-we-will-now-pull-posters-from-tmdb-as-our-training-data!
  38. https://spandan-madan.github.io/deeplearningproject/#congratulations,-we-are-done-scraping!
  39. https://spandan-madan.github.io/deeplearningproject/#building-a-dataset-out-of-the-scraped-information!
  40. https://spandan-madan.github.io/deeplearningproject/#my-implementation
  41. https://spandan-madan.github.io/deeplearningproject/#so,-how-do-we-store-this-movie-overview-in-a-matrix?
  42. https://spandan-madan.github.io/deeplearningproject/#do-we-just-store-the-whole-string?-we-know-that-we-need-to-work-with-numbers,-but-this-is-all-text.-what-do-we-do?!
  43. https://spandan-madan.github.io/deeplearningproject/#are-all-words-equally-important?
  44. https://spandan-madan.github.io/deeplearningproject/#at-the-cost-of-sounding-"animal-farm"-inspired,-i-would-say-not-all-words-are-equally-important.
  45. https://spandan-madan.github.io/deeplearningproject/#the-curse-of-dimensionality
  46. https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf
  47. https://spandan-madan.github.io/deeplearningproject/#this-section-is-strongly-borrowing-from-one-of-the-greatest-ml-papers-i've-ever-read.
  48. https://spandan-madan.github.io/deeplearningproject/#congratulations,-we-have-our-data-set-ready!
  49. https://spandan-madan.github.io/deeplearningproject/#as-the-ta,-i-saw-that-most-teams-working-on-the-project-had-data-of-the-order-of-100,000-movies.-so,-if-you-want-to-extract-the-power-of-these-models,-consider-scraping-a-larger-dataset-than-me.
  50. https://spandan-madan.github.io/deeplearningproject/#section-5---non-deep,-conventional-ml-models-with-above-data
  51. https://spandan-madan.github.io/deeplearningproject/#let's-start-with-some-feature-engineering.
  52. https://spandan-madan.github.io/deeplearningproject/#a-nice-way-to-think-of-it-is-to-think-that-you-start-with-the-problem-at-hand,-but-design-features-constrained-by-the-data-you-have-available.-if-you-have-many-independent-features-that-each-correlate-well-with-the-class,-learning-is-easy.-on-the-other-hand,-if-the-class-is-a-very-complex-function-of-the-features,-you-may-not-be-able-to-learn-it.
  53. https://spandan-madan.github.io/deeplearningproject/#evaluation-metrics
  54. https://spandan-madan.github.io/deeplearningproject/#section-6---deep-learning-:-an-intuitive-overview
  55. https://spandan-madan.github.io/deeplearningproject/#if-i-ask-you-to-"define"-a-chair,-how-would-you?---something-with-4-legs?
  56. https://spandan-madan.github.io/deeplearningproject/#how-about-some-surface-that-we-sit-on-then?
  57. https://spandan-madan.github.io/deeplearningproject/#section-7---deep-learning-for-predicting-genre-from-poster
  58. http://cs231n.github.io/transfer-learning/
  59. https://spandan-madan.github.io/deeplearningproject/#deep-learning-to-extract-visual-features-from-posters
  60. https://spandan-madan.github.io/deeplearningproject/#training-a-simple-neural-network-model-using-these-vgg-features.
  61. https://spandan-madan.github.io/deeplearningproject/#important-question-:-why-do-we-need-activation-functions?
  62. https://www.quora.com/why-do-neural-networks-need-an-activation-function/answer/spandan-madan?srid=5ydm
  63. https://spandan-madan.github.io/deeplearningproject/#copy-pasting-the-answer-i-wrote-for-this-question-on-quora-feel-free-to-leave-comments-there.
  64. https://spandan-madan.github.io/deeplearningproject/#let's-train-our-model-then,-using-the-features-we-extracted-from-vgg-net
  65. https://spandan-madan.github.io/deeplearningproject/#let's-look-at-some-of-our-predictions?
  66. https://spandan-madan.github.io/deeplearningproject/#section-8---deep-learning-to-get-textual-features
  67. https://drive.google.com/file/d/0b7xkcwpi5kdynlnuttlss21pqmm/edit
  68. https://jiajunwu.com/papers/dmil_cvpr.pdf
  69. https://spandan-madan.github.io/deeplearningproject/#section-9---upcoming-tutorials-and-acknowledgements
