   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

summarizing tweets in a disaster

   [16]go to the profile of gabriel tseng
   [17]gabriel tseng (button) blockedunblock (button) followfollowing
   aug 13, 2017
   [1*y2g8m1ksi3be0qxo8xipkg.jpeg]
   map of the nepal   s second earthquake   s intensity, credit:
   [18]https://www.washingtonpost.com/graphics/world/nepal-earthquake/

   on april 25th 2015, just before noon, nepal experienced an earthquake
   of magnitude [19]7.8 on the moment magnitude scale. the earthquake
   ripped through kathmandu valley, and a series of aftershocks leveled
   entire villages.

   immediately after the earthquake, volunteers from around the world were
   instrumental in guiding emergency operations, using [20]satellite
   imagery to identify infrastructure destruction throughout the region.

   however, people on the ground in nepal were also generating tremendous
   amounts of information which could be of use to rescue operations,
   albeit less directly: on twitter. between april 25th and may 28th,
   [21]33,610 tweets were tweeted by people in nepal. these tweets were
   full of useful information, but 33,610 tweets is simply too many for a
   rescue operation to comb through.

   this is the motivation for this project:

   can i take a large body of tweets, and from them extract a useful,
   short summary which could have been useful to rescue operations on the
   ground?
     __________________________________________________________________

contents:

   0. getting the tweets
    1. finding all the useful tweets, using content words and tf-idf
       scores
    2. picking the best tweets to make a short summary
    3. conclusion
     __________________________________________________________________

0. getting the tweets

   [22]link to code

   i obtained my tweets from    [23]twitter as a lifeline: human-annotated
   twitter corpora for nlp of crisis-related messages   ; this project took
   thousands of tweets from crises, and labelled them into 8 different
   categories (eg.    displaced people and evacuations   , or    sympathy and
   emotional support   ).

   however, since a rescue team wouldn   t be able to label the tweets, i
   only used the dataset for the tweets themselves, not for the labels.

   twitter   s policy says that only tweet ids, not the tweets themselves,
   can be saved         this way, if a user removes their tweets, then they are
   not stored elsewhere. i therefore had to use twitter   s api to obtain
   the tweet contents from this corpus. this required me to register an
   app:
   [1*xnrbaabqgbfgazapkbok2w.png]
   i am now an app developer!

   i then used [24]twython to download the tweets, and was on my way!

1. extracting situational tweets

   [25]link to code

   it was immediately clear that not all tweets would be equally useful to
   rescue teams. for instance,
@gurmeetramrahim: #msghelpearthquakevictims shocked!!!hearing earthquake in nepa
l n some parts of india. i pray to god to save his child  

   contains no information which is useful to rescue teams, especially
   compared to:
mea opens 24 hour control room for queries regarding the nepal #earthquake.
numbers:
+91 11 2301 2113
+91 11 2301 4104
+91 11 2301 7905

   useful tweets are categorized as situational tweets, and may contain
   status updates, or immediately useful information (such as numbers of
   nearby hospitals).

   non situational tweets contain (for example) sentiments, opinions or
   event analyses. these do not immediately help rescue efforts.

   before i could begin summarizing tweets, i needed to separate the
   situational tweets from non-situational ones. i did this in two ways: i
   first manually isolated characteristics (1.1         content words) of tweets
   which contribute to their usefulness. i then used a document analysis
   tool called tf-idf (1.2) to find words which are significant to this
   particular event and group of tweets.

1.1. content words

   in their [26]2015 paper, rudra et. al identified three classes of terms
   which provide important information during disasters: numerals, nouns
   and main verbs. words which fell into these classes are called content
   words. i found this too general to differentiate tweets, and defined
   two classes of my own:
    1. numerals (eg. number of casualties, important phone numbers)
    2. [27]entities (eg. places, dates, events, organisations, etc.)

   [28]spacy (a [29]natural language processing library, which
   automatically analyzes and extracts information from text) is a very
   useful tool to identify content words; when spacy [30]tokenizes text,
   it adds a lot of additional information to the tokens, such as whether
   it is an entity (and if it is, what type of entity it is), what its
   part of speech is (i.e. is it a noun? a verb? a numeral?), or even the
   token   s sentiment.
   [1*sldsez8dzi4yfsz4cvzjra.png]
   spacy   s awesome [31]entity recognizer

   i used spacy to tokenize the tweets. this meant breaking the tweets
   down into their components (primarily words, but also punctuation and
   numbers), and turning these components into tokens. for instance,
: over 110 killed in earthquake: nepal home ministry (pti)

   becomes
[:, over, 110, killed, in, earthquake, :, nepal, home, ministry, (, pti, )]

   the power of spacy is that these tokens are full of additional
   information; for example, i can find out the part of speech of all of
   these tokens:
[u'punct', u'adp', u'num', u'verb', u'adp', u'noun', u'punct', u'propn', u'propn
', u'propn', u'punct', u'propn', u'punct']

   another very useful attribute of a token is its entity type; spacy can
   tell me that kathmandu is a city, or that 25 april is a date. i
   included a token as a content word if its entity type (token.ent_type_)
   was:
    1. norp: nationality or religious or political group
    2. facility: buildings, airports, highways, bridges, etc.
    3. org: companies, agencies, institutions, etc.
    4. gpe: countries, cities, states.
    5. loc: non-gpe locations, mountain ranges, bodies of water.
    6. event: named hurricanes, battles, wars, sports events, etc.
    7. date: absolute or relative dates or periods.
    8. time: times smaller than a day.

   i also included tokens if their part of speech ( token.pos_ ) marked
   them as a number, or if they were one of a list of key words (eg.
      killed   ,    injured   ,    hurt   ).

   this classification broadly allowed me to start sorting my tweets;
   situational tweets contain more content words, while non-situational
   tweets contain less. for instance,
@timesofindia: #earthquake | helpline numbers of the indian embassy in nepal:\r+
9779581107021\r\r+9779851135141'

   is a highly useful tweet, and as expected, many content words are
   extracted:
[the, indian, embassy, nepal, 977, 9581107021, 977, 9851135141]

   on the other hand,
pray for #nepal where a powerful earthquake has struck. may the almighty grant t
hem ease to face this calamity with utmost s    _

   contains very little situational information, and the only content word
   extracted from it is [nepal] .

1.2. tf-idf scores

   a drawback of content words is that they fail to capture any
   information about the words themselves. for instance, for this
   disaster, the word nepal is going to be a strong indicator of whether
   or not a tweet is situational, but its not weighted any differently
   than any other content words right now.

   it is possible to introduce such a weighting, using [32]term
   frequency         inverse document frequency (tf-idf) scores.

   despite its long name, the logic behind a tf-idf score is
   straightforward:

     words which occur fairly frequently in a body of documents are
     probably more important, but if they occur too often, then they are
     too general to be helpful.

   basically, i wanted to score the word    nepal    highly (and it should
   occur once, in many tweets), but not the word    the    (which should occur
   many times, in many tweets).

   mathematically, the tf-idf score for some word t can be described as
   [1*aemhos21zcnfytvbmsjtqg.png]

   where c is the average number of times the word t appears in a
   document, n is the total number of documents, and n is the number of
   documents in which the word t appears.

   [33]textacy, a library built on top of [34]spacy, made it super easy
   for me to assign tf-idf scores to the words in the tweets:
word:morning -- tf-idf score:6.45446604904
word:bishnu -- tf-idf score:8.06390396147
word:nagarkot -- tf-idf score:12.2359876248
word:search -- tf-idf score:6.35915586923
word:kathmandu -- tf-idf score:5.27350744616

   now, if i picked only the tweets which had lots of content words, or
   even only the tweets with lots of content words with high tf-idf
   scores, i would still end up with far too many tweets for rescue teams
   to realistically find useful.

   what i want to do is find a summary of tweets which is short, but which
   also encompasses the most words with the highest tf-idf scores
   possible.

2. content word-based tweet summarization

   to generate something which would be useful to rescue teams, i need to
   generate something which is short (and therefore quick to read). it
   also needs to contain information which would be of use to rescue
   teams         so the summary needs to be full of content words with high
   tf-idf scores.

   i can easily define this as an equation to solve for, with constraints:

   equation: maximize the total score of the content words in my summary.

   constraint 1: the summary must be shorter than 150 words.

   constraint 2: if i pick a content word to be in my summary, then i must
   pick some tweet which contains that content word to be in my summary.

   constraint 3: if i pick some tweet to be in my summary, then all the
   content words in that tweet must be included.

   i need to solve the equation, subject to the constraints. the variables
   i am solving for (whether a content word is in the summary or not) are
   integers. specifically, the choice is binary         1 if the word is
   included, 0 if it is not.

   this approach of solving some equation (maximizing the score of content
   words) subject to some constraints, with integer variables, is known as
   [35]integer id135 (ilp).

   using ilp, i can define mathematically what i wrote above as maximizing
   [1*y5jdhfiuooacsx14-vvk2q.png]

   where x and y are arrays of 1s and 0s, depending on whether or not
   tweet i is selected and content word j is selected, and score(j) is
   content word j   s tf-idf score. the constraints are defined as
   [1*nuyrh6h6qnplqkyvxezk6w.png]
   see my [36]jupyter notebook for more info on what these mean

   using [37]pymathproj to optimize this ilp problem yielded the following
   summary:
1. tv: 2 dead, 100 injured in bangladesh from nepal quake: dhaka, bangladesh (ap
)        a tv r...
-------------
2. : earthquake helpline at the indian embassy in kathmandu-+977 98511 07021, +9
77 98511 35141
-------------
3. +91 11 2301 7905om no for nepal #earthquake +91 11 2301 2113
-------------
4. tremors felt in west bengal after 7.9 magnitude earthquake in nepal
-------------
5. this mobile app may help these people to predict this powerfull m7.9 earthqua
ke
-------------
6. 5.00 earthquake occured 31km nnw of nagarkot, nepal at 09:30 utc! #earthquake
 #nagarkot
-------------
7. earthquake in bihar up punjab haryana delhi ncr and nepal
-------------
8. : whole himalayan region is becoming non stable. two yrs back uttrakhand, the
n kashmir now nepal n north east. even tibet is    _
-------------
9. wellingtonhere nepal's home ministry says at least 71 people killed in the ea
rthquake: nepal'...  wellingtonhere
-------------
10. 934s of major earthquake-s in nepal:
-------------
11. historic dharahara tower collapses in kathmandu after quake | latest news &a
mp; updates at daily...
-------------
12. powerful quake near nepal capital causes widespread damage, avalanche near e
verest base camp

   comparing this to random tweets:
   [1*0ot2ecdodnrqxzet6byq2q.png]
   a comparison of the tweets selected by cowts to a random selection from
   the database.

   there definitely is some noise, but its not bad! the summary is
   especially good at providing locational information, such as describing
   where the epicenter is and which areas are impacted by the earthquake.

3. conclusions

   if i was to do this project again, i would use a [38]twitter specific
   tokenizer. spacy   s tokenizer was actually pretty poor at tokenizing the
   data, because of all the abbreviations and twitter specific lingo. i
   would also have cleaned the data more, since misspellings and
   abbreviations may also have hurt the performance of the tokenizer.

   overall, it was awesome to experiment with methods other than [39]word
   embeddings to compare and quantify textual data. also, the speed of
   this method makes me confident it could be implemented by rescue teams;
   in particular, if smaller subsets of tweets were used (eg. daily
   tweets, instead of tweets covering the whole event), it could be very
   useful!

3.1 further reading

   this project involved implementing a variety of different papers; here
   are some cool papers to further explore twitter summaries in disasters:

   [40]twitter as a lifeline: human-annotated twitter corpora for nlp of
   crisis-related messages

   [41]natural language processing to the rescue? extracting    situational
   awareness    tweets during mass emergency

   [42]summarizing situational tweets in a crisis scenario (this post
   essentially implemented this paper)

update         using nltk   s twitter tokenizer

   [43]link to code

   i repeated the exercise using nltk instead of spacy. this allowed me to
   tokenize the tweets using nltk   s [44]twitter specific tokenizer,
   yielding the following summary (compared to the spacy output):
   [1*h-1ckwunf6trvfnutckswq.png]
   ndrf: india   s [45]natural disaster response force

   because the content words are defined differently, its difficult to
   quantitatively compare the two outputs, but it is worth noting that
   nltk   s entity recognition system (i used [46]stanford   s ner) was
   significantly slower than spacy   s.
     __________________________________________________________________

   i (try to) build an abstractive summary using these chosen tweets in
   part 2:
   [47]summarizing tweets in a disaster (part ii)
   its may 2015, and rescue teams are working to rebuild nepal following
   the april earthquake (and its aftershocks). can   medium.com

     * [48]data science
     * [49]twitter
     * [50]natural disasters
     * [51]nlp
     * [52]towards data science

   (button)
   (button)
   (button) 323 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [53]go to the profile of gabriel tseng

[54]gabriel tseng

     (button) follow
   [55]towards data science

[56]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 323
     * (button)
     *
     *

   [57]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [58]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e6b355a41732
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/summarizing-tweets-in-a-disaster-e6b355a41732&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/summarizing-tweets-in-a-disaster-e6b355a41732&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_eltshcp86ccn---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@gabrieltseng?source=post_header_lockup
  17. https://towardsdatascience.com/@gabrieltseng
  18. https://www.washingtonpost.com/graphics/world/nepal-earthquake/
  19. https://earthquake.usgs.gov/earthquakes/eventpage/us20002926#executive
  20. http://news.nationalgeographic.com/2015/05/150501-nepal-crisis-mapping-disaster-relief-earthquake/
  21. https://www.int-arch-photogramm-remote-sens-spatial-inf-sci.net/xli-b2/567/2016/isprs-archives-xli-b2-567-2016.pdf
  22. https://github.com/gabrieltseng/learningdatascience/blob/master/natural_language_processing/twitter_disasters/1 - tweets from ids.ipynb
  23. https://arxiv.org/abs/1605.05894
  24. https://twython.readthedocs.io/en/latest/
  25. https://github.com/gabrieltseng/learningdatascience/blob/master/natural_language_processing/twitter_disasters/spacy/2 - tweet summarization.ipynb
  26. http://dl.acm.org/citation.cfm?id=2806485
  27. https://spacy.io/docs/usage/entity-recognition
  28. https://spacy.io/
  29. https://www.wikiwand.com/en/natural_language_processing
  30. https://nlp.stanford.edu/ir-book/html/htid113dition/id121-1.html
  31. https://spacy.io/docs/usage/entity-recognition
  32. https://www.wikiwand.com/en/tf   idf
  33. http://textacy.readthedocs.io/en/latest/api_reference.html
  34. https://spacy.io/docs/api/token
  35. https://www.wikiwand.com/en/integer_programming
  36. https://github.com/gabrieltseng/learningdatascience/blob/master/natural_language_processing/twitterdisasters/2 - cowts.ipynb
  37. http://pymprog.sourceforge.net/
  38. http://www.nltk.org/howto/twitter.html
  39. https://www.wikiwand.com/en/word_embedding
  40. https://arxiv.org/abs/1605.05894
  41. https://www.aaai.org/ocs/index.php/icwsm/icwsm11/paper/view/2834
  42. http://dl.acm.org/citation.cfm?id=2914600
  43. https://github.com/gabrieltseng/learningdatascience/blob/master/natural_language_processing/twitterdisasters/nltk/2 - tweet summarization.ipynb
  44. http://www.nltk.org/_modules/nltk/tokenize/casual.html#tweettokenizer
  45. https://www.wikiwand.com/en/national_disaster_response_force
  46. https://nlp.stanford.edu/software/crf-ner.html
  47. https://medium.com/towards-data-science/summarizing-tweets-in-a-disaster-part-ii-67db021d378d
  48. https://towardsdatascience.com/tagged/data-science?source=post
  49. https://towardsdatascience.com/tagged/twitter?source=post
  50. https://towardsdatascience.com/tagged/natural-disasters?source=post
  51. https://towardsdatascience.com/tagged/nlp?source=post
  52. https://towardsdatascience.com/tagged/towards-data-science?source=post
  53. https://towardsdatascience.com/@gabrieltseng?source=footer_card
  54. https://towardsdatascience.com/@gabrieltseng
  55. https://towardsdatascience.com/?source=footer_card
  56. https://towardsdatascience.com/?source=footer_card
  57. https://towardsdatascience.com/
  58. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  60. https://medium.com/towards-data-science/summarizing-tweets-in-a-disaster-part-ii-67db021d378d
  61. https://medium.com/p/e6b355a41732/share/twitter
  62. https://medium.com/p/e6b355a41732/share/facebook
  63. https://medium.com/p/e6b355a41732/share/twitter
  64. https://medium.com/p/e6b355a41732/share/facebook
