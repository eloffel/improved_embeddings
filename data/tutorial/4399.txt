   #[1]github [2]recent commits to effectivetensorflow:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]375
     * [35]star [36]8,264
     * [37]fork [38]909

[39]vahidk/[40]effectivetensorflow

   [41]code [42]issues 1 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   tensorflow 1.x and 2.x tutorials and best practices.
   [47]https://twitter.com/vahidk
   [48]tensorflow [49]neural-network [50]deep-learning
   [51]machine-learning [52]ebook
     * [53]117 commits
     * [54]3 branches
     * [55]0 releases
     * [56]9 contributors

   branch: master (button) new pull request
   [57]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/v
   [58]download zip

downloading...

   want to be notified of new releases in vahidk/effectivetensorflow?
   [59]sign in [60]sign up

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [63]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [64]download the github extension for visual studio
   and try again.

   (button) go back
   [65]@vahidk
   [66]vahidk [67]remove reference to depreacted api.
   latest commit [68]d998bef mar 17, 2019
   [69]permalink
   type       name      latest commit message  commit time
        failed to load latest commit information.
        [70]code
        [71].gitignore
        [72].gitmodules [73]moved framework.  aug 19, 2017
        [74]readme.md

readme.md

effective tensorflow

table of contents

part i: tensorflow fundamentals

    1. [75]tensorflow basics
    2. [76]understanding static and dynamic shapes
    3. [77]scopes and when to use them
    4. [78]broadcasting the good and the ugly
    5. [79]feeding data to tensorflow
    6. [80]take advantage of the overloaded operators
    7. [81]understanding order of execution and control dependencies
    8. [82]control flow operations: conditionals and loops
    9. [83]prototyping kernels and advanced visualization with python ops
   10. [84]multi-gpu processing with data parallelism
   11. [85]debugging tensorflow models
   12. [86]numerical stability in tensorflow
   13. [87]building a neural network training framework with learn api

part ii: tensorflow cookbook

    1. [88]get shape
    2. [89]batch gather
    3. [90]id125
    4. [91]merge
    5. [92]id178
    6. [93]kl-divergence
    7. [94]make parallel
    8. [95]leaky relu
    9. [96]batch id172
     __________________________________________________________________

   we updated the guide to follow the newly released tensorflow 2.x api.
   click [97]here for v1 branch, and [98]here for v2 branch.

   we aim to gradually expand this series by adding new articles and keep
   the content up to date with the latest releases of tensorflow api. if
   you have suggestions on how to improve this series or find the
   explanations ambiguous, feel free to create an issue, send patches, or
   reach out by email.

   we encourage you to also check out the accompanied neural network
   training framework built on top of tf.estimator api. the [99]framework
   can be downloaded separately:
git clone https://github.com/vahidk/tensorflowframework.git

part i: tensorflow fundamentals

tensorflow basics

   the most striking difference between tensorflow and other numerical
   computation libraries such as numpy is that operations in tensorflow
   are symbolic. this is a powerful concept that allows tensorflow to do
   all sort of things (e.g. automatic differentiation) that are not
   possible with imperative libraries such as numpy. but it also comes at
   the cost of making it harder to grasp. our attempt here is to demystify
   tensorflow and provide some guidelines and best practices for more
   effective use of tensorflow.

   let's start with a simple example, we want to multiply two random
   matrices. first we look at an implementation done in numpy:
import numpy as np

x = np.random.normal(size=[10, 10])
y = np.random.normal(size=[10, 10])
z = np.dot(x, y)

print(z)

   now we perform the exact same computation this time in tensorflow:
import tensorflow as tf

x = tf.random_normal([10, 10])
y = tf.random_normal([10, 10])
z = tf.matmul(x, y)

sess = tf.session()
z_val = sess.run(z)

print(z_val)

   unlike numpy that immediately performs the computation and produces the
   result, tensorflow only gives us a handle (of type tensor) to a node in
   the graph that represents the result. if we try printing the value of z
   directly, we get something like this:
tensor("matmul:0", shape=(10, 10), dtype=float32)

   since both the inputs have a fully defined shape, tensorflow is able to
   infer the shape of the tensor as well as its type. in order to compute
   the value of the tensor we need to create a session and evaluate it
   using session.run() method.
     __________________________________________________________________

   tip: when using jupyter notebook make sure to call
   tf.reset_default_graph() at the beginning to clear the symbolic graph
   before defining new nodes.
     __________________________________________________________________

   to understand how powerful symbolic computation can be let's have a
   look at another example. assume that we have samples from a curve (say
   f(x) = 5x^2 + 3) and we want to estimate f(x) based on these samples.
   we define a parametric function g(x, w) = w0 x^2 + w1 x + w2, which is
   a function of the input x and latent parameters w, our goal is then to
   find the latent parameters such that g(x, w)     f(x). this can be done
   by minimizing the following id168: l(w) =     (f(x) - g(x, w))^2.
   although there's a closed form solution for this simple problem, we opt
   to use a more general approach that can be applied to any arbitrary
   differentiable function, and that is using stochastic id119.
   we simply compute the average gradient of l(w) with respect to w over a
   set of sample points and move in the opposite direction.

   here's how it can be done in tensorflow:
import numpy as np
import tensorflow as tf

# placeholders are used to feed values from python to tensorflow ops. we define
# two placeholders, one for input feature x, and one for output y.
x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)

# assuming we know that the desired function is a polynomial of 2nd degree, we
# allocate a vector of size 3 to hold the coefficients. the variable will be
# automatically initialized with random noise.
w = tf.get_variable("w", shape=[3, 1])

# we define yhat to be our estimate of y.
f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)
yhat = tf.squeeze(tf.matmul(f, w), 1)

# the loss is defined to be the l2 distance between our estimate of y and its
# true value. we also added a shrinkage term, to ensure the resulting weights
# would be small.
loss = tf.nn.l2_loss(yhat - y) + 0.1 * tf.nn.l2_loss(w)

# we use the adam optimizer with learning rate set to 0.1 to minimize the loss.
train_op = tf.train.adamoptimizer(0.1).minimize(loss)

def generate_data():
    x_val = np.random.uniform(-10.0, 10.0, size=100)
    y_val = 5 * np.square(x_val) + 3
    return x_val, y_val

sess = tf.session()
# since we are using variables we first need to initialize them.
sess.run(tf.global_variables_initializer())
for _ in range(1000):
    x_val, y_val = generate_data()
    _, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})
    print(loss_val)
print(sess.run([w]))

   by running this piece of code you should see a result close to this:
[4.9924135, 0.00040895029, 3.4504161]

   which is a relatively close approximation to our parameters.

   this is just tip of the iceberg for what tensorflow can do. many
   problems such as optimizing large neural networks with millions of
   parameters can be implemented efficiently in tensorflow in just a few
   lines of code. tensorflow takes care of scaling across multiple
   devices, and threads, and supports a variety of platforms.

understanding static and dynamic shapes

   tensors in tensorflow have a static shape attribute which is determined
   during graph construction. the static shape may be underspecified. for
   example we might define a tensor of shape [none, 128]:
import tensorflow as tf

a = tf.placeholder(tf.float32, [none, 128])

   this means that the first dimension can be of any size and will be
   determined dynamically during session.run(). you can query the static
   shape of a tensor as follows:
static_shape = a.shape.as_list()  # returns [none, 128]

   to get the dynamic shape of the tensor you can call tf.shape op, which
   returns a tensor representing the shape of the given tensor:
dynamic_shape = tf.shape(a)

   the static shape of a tensor can be set with tensor.set_shape() method:
a.set_shape([32, 128])  # static shape of a is [32, 128]
a.set_shape([none, 128])  # first dimension of a is determined dynamically

   you can reshape a given tensor dynamically using tf.reshape function:
a =  tf.reshape(a, [32, 128])

   it can be convenient to have a function that returns the static shape
   when available and dynamic shape when it's not. the following utility
   function does just that:
def get_shape(tensor):
  static_shape = tensor.shape.as_list()
  dynamic_shape = tf.unstack(tf.shape(tensor))
  dims = [s[1] if s[0] is none else s[0]
          for s in zip(static_shape, dynamic_shape)]
  return dims

   now imagine we want to convert a tensor of rank 3 to a tensor of rank 2
   by collapsing the second and third dimensions into one. we can use our
   get_shape() function to do that:
b = tf.placeholder(tf.float32, [none, 10, 32])
shape = get_shape(b)
b = tf.reshape(b, [shape[0], shape[1] * shape[2]])

   note that this works whether the shapes are statically specified or
   not.

   in fact we can write a general purpose reshape function to collapse any
   list of dimensions:
import tensorflow as tf
import numpy as np

def reshape(tensor, dims_list):
  shape = get_shape(tensor)
  dims_prod = []
  for dims in dims_list:
    if isinstance(dims, int):
      dims_prod.append(shape[dims])
    elif all([isinstance(shape[d], int) for d in dims]):
      dims_prod.append(np.prod([shape[d] for d in dims]))
    else:
      dims_prod.append(tf.reduce_prod([shape[d] for d in dims]))
  tensor = tf.reshape(tensor, dims_prod)
  return tensor

   then collapsing the second dimension becomes very easy:
b = tf.placeholder(tf.float32, [none, 10, 32])
b = reshape(b, [0, [1, 2]])

scopes and when to use them

   variables and tensors in tensorflow have a name attribute that is used
   to identify them in the symbolic graph. if you don't specify a name
   when creating a variable or a tensor, tensorflow automatically assigns
   a name for you:
a = tf.constant(1)
print(a.name)  # prints "const:0"

b = tf.variable(1)
print(b.name)  # prints "variable:0"

   you can overwrite the default name by explicitly specifying it:
a = tf.constant(1, name="a")
print(a.name)  # prints "a:0"

b = tf.variable(1, name="b")
print(b.name)  # prints "b:0"

   tensorflow introduces two different context managers to alter the name
   of tensors and variables. the first is tf.name_scope:
with tf.name_scope("scope"):
  a = tf.constant(1, name="a")
  print(a.name)  # prints "scope/a:0"

  b = tf.variable(1, name="b")
  print(b.name)  # prints "scope/b:0"

  c = tf.get_variable(name="c", shape=[])
  print(c.name)  # prints "c:0"

   note that there are two ways to define new variables in tensorflow, by
   creating a tf.variable object or by calling tf.get_variable. calling
   tf.get_variable with a new name results in creating a new variable, but
   if a variable with the same name exists it will raise a valueerror
   exception, telling us that re-declaring a variable is not allowed.

   tf.name_scope affects the name of tensors and variables created with
   tf.variable, but doesn't impact the variables created with
   tf.get_variable.

   unlike tf.name_scope, tf.variable_scope modifies the name of variables
   created with tf.get_variable as well:
with tf.variable_scope("scope"):
  a = tf.constant(1, name="a")
  print(a.name)  # prints "scope/a:0"

  b = tf.variable(1, name="b")
  print(b.name)  # prints "scope/b:0"

  c = tf.get_variable(name="c", shape=[])
  print(c.name)  # prints "scope/c:0"

with tf.variable_scope("scope"):
  a1 = tf.get_variable(name="a", shape=[])
  a2 = tf.get_variable(name="a", shape=[])  # disallowed

   but what if we actually want to reuse a previously declared variable?
   variable scopes also provide the functionality to do that:
with tf.variable_scope("scope"):
  a1 = tf.get_variable(name="a", shape=[])
with tf.variable_scope("scope", reuse=true):
  a2 = tf.get_variable(name="a", shape=[])  # ok

   this becomes handy for example when using built-in neural network
   layers:
with tf.variable_scope('my_scope'):
  features1 = tf.layers.conv2d(image1, filters=32, kernel_size=3)
# use the same convolution weights to process the second image:
with tf.variable_scope('my_scope', reuse=true):
  features2 = tf.layers.conv2d(image2, filters=32, kernel_size=3)

   alternatively you can set reuse to tf.auto_reuse which tells tensorflow
   to create a new variable if a variable with the same name doesn't
   exist, and reuse otherwise:
with tf.variable_scope("scope", reuse=tf.auto_reuse):
  features1 = tf.layers.conv2d(image1, filters=32, kernel_size=3)

with tf.variable_scope("scope", reuse=tf.auto_reuse):
  features2 = tf.layers.conv2d(image2, filters=32, kernel_size=3)

   if you want to do lots of variable sharing keeping track of when to
   define new variables and when to reuse them can be cumbersome and error
   prone. tf.auto_reuse simplifies this task but adds the risk of sharing
   variables that weren't supposed to be shared. tensorflow templates are
   another way of tackling the same problem without this risk:
conv3x32 = tf.make_template("conv3x32", lambda x: tf.layers.conv2d(x, 32, 3))
features1 = conv3x32(image1)
features2 = conv3x32(image2)  # will reuse the convolution weights.

   you can turn any function to a tensorflow template. upon the first call
   to a template, the variables defined inside the function would be
   declared and in the consecutive invocations they would automatically
   get reused.

broadcasting the good and the ugly

   tensorflow supports broadcasting elementwise operations. normally when
   you want to perform operations like addition and multiplication, you
   need to make sure that shapes of the operands match, e.g. you can   t add
   a tensor of shape [3, 2] to a tensor of shape [3, 4]. but there   s a
   special case and that   s when you have a singular dimension. tensorflow
   implicitly tiles the tensor across its singular dimensions to match the
   shape of the other operand. so it   s valid to add a tensor of shape [3,
   2] to a tensor of shape [3, 1]
import tensorflow as tf

a = tf.constant([[1., 2.], [3., 4.]])
b = tf.constant([[1.], [2.]])
# c = a + tf.tile(b, [1, 2])
c = a + b

   broadcasting allows us to perform implicit tiling which makes the code
   shorter, and more memory efficient, since we don   t need to store the
   result of the tiling operation. one neat place that this can be used is
   when combining features of varying length. in order to concatenate
   features of varying length we commonly tile the input tensors,
   concatenate the result and apply some nonlinearity. this is a common
   pattern across a variety of neural network architectures:
a = tf.random_uniform([5, 3, 5])
b = tf.random_uniform([5, 1, 6])

# concat a and b and apply nonlinearity
tiled_b = tf.tile(b, [1, 3, 1])
c = tf.concat([a, tiled_b], 2)
d = tf.layers.dense(c, 10, activation=tf.nn.relu)

   but this can be done more efficiently with broadcasting. we use the
   fact that f(m(x + y)) is equal to f(mx + my). so we can do the linear
   operations separately and use broadcasting to do implicit
   concatenation:
pa = tf.layers.dense(a, 10)
pb = tf.layers.dense(b, 10)
d = tf.nn.relu(pa + pb)

   in fact this piece of code is pretty general and can be applied to
   tensors of arbitrary shape as long as broadcasting between tensors is
   possible:
def merge(a, b, units, activation=tf.nn.relu):
    pa = tf.layers.dense(a, units)
    pb = tf.layers.dense(b, units)
    c = pa + pb
    if activation is not none:
        c = activation(c)
    return c

   a slightly more general form of this function is [100]included in the
   cookbook.

   so far we discussed the good part of broadcasting. but what   s the ugly
   part you may ask? implicit assumptions almost always make debugging
   harder to do. consider the following example:
a = tf.constant([[1.], [2.]])
b = tf.constant([1., 2.])
c = tf.reduce_sum(a + b)

   what do you think the value of c would be after evaluation? if you
   guessed 6, that   s wrong. it   s going to be 12. this is because when rank
   of two tensors don   t match, tensorflow automatically expands the first
   dimension of the tensor with lower rank before the elementwise
   operation, so the result of addition would be [[2, 3], [3, 4]], and the
   reducing over all parameters would give us 12.

   the way to avoid this problem is to be as explicit as possible. had we
   specified which dimension we would want to reduce across, catching this
   bug would have been much easier:
a = tf.constant([[1.], [2.]])
b = tf.constant([1., 2.])
c = tf.reduce_sum(a + b, 0)

   here the value of c would be [5, 7], and we immediately would guess
   based on the shape of the result that there   s something wrong. a
   general rule of thumb is to always specify the dimensions in reduction
   operations and when using tf.squeeze.

feeding data to tensorflow

   tensorflow is designed to work efficiently with large amount of data.
   so it's important not to starve your tensorflow model in order to
   maximize its performance. there are various ways that you can feed your
   data to tensorflow.

constants

   the simplest approach is to embed the data in your graph as a constant:
import tensorflow as tf
import numpy as np

actual_data = np.random.normal(size=[100])

data = tf.constant(actual_data)

   this approach can be very efficient, but it's not very flexible. one
   problem with this approach is that, in order to use your model with
   another dataset you have to rewrite the graph. also, you have to load
   all of your data at once and keep it in memory which would only work
   with small datasets.

placeholders

   using placeholders solves both of these problems:
import tensorflow as tf
import numpy as np

data = tf.placeholder(tf.float32)

prediction = tf.square(data) + 1

actual_data = np.random.normal(size=[100])

tf.session().run(prediction, feed_dict={data: actual_data})

   placeholder operator returns a tensor whose value is fetched through
   the feed_dict argument in session.run function. note that running
   session.run without feeding the value of data in this case will result
   in an error.

python ops

   another approach to feed the data to tensorflow is by using python ops:
def py_input_fn():
    actual_data = np.random.normal(size=[100])
    return actual_data

data = tf.py_func(py_input_fn, [], (tf.float32))

   python ops allow you to convert a regular python function to a
   tensorflow operation.

dataset api

   the recommended way of reading the data in tensorflow however is
   through the dataset api:
actual_data = np.random.normal(size=[100])
dataset = tf.data.dataset.from_tensor_slices(actual_data)
data = dataset.make_one_shot_iterator().get_next()

   if you need to read your data from file, it may be more efficient to
   write it in tfrecord format and use tfrecorddataset to read it:
dataset = tf.data.tfrecorddataset(path_to_data)

   see the [101]official docs for an example of how to write your dataset
   in tfrecord format.

   dataset api allows you to make efficient data processing pipelines
   easily. for example this is how we process our data in the accompanied
   framework (see [102]trainer.py):
dataset = ...
dataset = dataset.cache()
if mode == tf.estimator.modekeys.train:
    dataset = dataset.repeat()
    dataset = dataset.shuffle(batch_size * 5)
dataset = dataset.map(parse, num_threads=8)
dataset = dataset.batch(batch_size)

   after reading the data, we use dataset.cache method to cache it into
   memory for improved efficiency. during the training mode, we repeat the
   dataset indefinitely. this allows us to process the whole dataset many
   times. we also shuffle the dataset to get batches with different sample
   distributions. next, we use the dataset.map function to perform
   preprocessing on raw records and convert the data to a usable format
   for the model. we then create batches of samples by calling
   dataset.batch.

take advantage of the overloaded operators

   just like numpy, tensorflow overloads a number of python operators to
   make building graphs easier and the code more readable.

   the slicing op is one of the overloaded operators that can make
   indexing tensors very easy:
z = x[begin:end]  # z = tf.slice(x, [begin], [end-begin])

   be very careful when using this op though. the slicing op is very
   inefficient and often better avoided, especially when the number of
   slices is high. to understand how inefficient this op can be let's look
   at an example. we want to manually perform reduction across the rows of
   a matrix:
import tensorflow as tf
import time

x = tf.random_uniform([500, 10])

z = tf.zeros([10])
for i in range(500):
    z += x[i]

sess = tf.session()
start = time.time()
sess.run(z)
print("took %f seconds." % (time.time() - start))

   on my macbook pro, this took 2.67 seconds to run! the reason is that we
   are calling the slice op 500 times, which is going to be very slow to
   run. a better choice would have been to use tf.unstack op to slice the
   matrix into a list of vectors all at once:
z = tf.zeros([10])
for x_i in tf.unstack(x):
    z += x_i

   this took 0.18 seconds. of course, the right way to do this simple
   reduction is to use tf.reduce_sum op:
z = tf.reduce_sum(x, axis=0)

   this took 0.008 seconds, which is 300x faster than the original
   implementation.

   tensorflow also overloads a range of arithmetic and logical operators:
z = -x  # z = tf.negative(x)
z = x + y  # z = tf.add(x, y)
z = x - y  # z = tf.subtract(x, y)
z = x * y  # z = tf.mul(x, y)
z = x / y  # z = tf.div(x, y)
z = x // y  # z = tf.floordiv(x, y)
z = x % y  # z = tf.mod(x, y)
z = x ** y  # z = tf.pow(x, y)
z = x @ y  # z = tf.matmul(x, y)
z = x > y  # z = tf.greater(x, y)
z = x >= y  # z = tf.greater_equal(x, y)
z = x < y  # z = tf.less(x, y)
z = x <= y  # z = tf.less_equal(x, y)
z = abs(x)  # z = tf.abs(x)
z = x & y  # z = tf.logical_and(x, y)
z = x | y  # z = tf.logical_or(x, y)
z = x ^ y  # z = tf.logical_xor(x, y)
z = ~x  # z = tf.logical_not(x)

   you can also use the augmented version of these ops. for example x += y
   and x **= 2 are also valid.

   note that python doesn't allow overloading "and", "or", and "not"
   keywords.

   tensorflow also doesn't allow using tensors as booleans, as it may be
   error prone:
x = tf.constant(1.)
if x:  # this will raise a typeerror error
    ...

   you can either use tf.cond(x, ...) if you want to check the value of
   the tensor, or use "if x is none" to check the value of the variable.

   other operators that aren't supported are equal (==) and not equal (!=)
   operators which are overloaded in numpy but not in tensorflow. use the
   function versions instead which are tf.equal and tf.not_equal.

understanding order of execution and control dependencies

   as we discussed in the first item, tensorflow doesn't immediately run
   the operations that are defined but rather creates corresponding nodes
   in a graph that can be evaluated with session.run() method. this also
   enables tensorflow to do optimizations at run time to determine the
   optimal order of execution and possible trimming of unused nodes. if
   you only have tf.tensors in your graph you don't need to worry about
   dependencies but you most probably have tf.variables too, and
   tf.variables make things much more difficult. my advice to is to only
   use variables if tensors don't do the job. this might not make a lot of
   sense to you now, so let's start with an example.
import tensorflow as tf

a = tf.constant(1)
b = tf.constant(2)
a = a + b

tf.session().run(a)

   evaluating "a" will return the value 3 as expected. note that here we
   are creating 3 tensors, two constant tensors and another tensor that
   stores the result of the addition. note that you can't overwrite the
   value of a tensor. if you want to modify it you have to create a new
   tensor. as we did here.
     __________________________________________________________________

   tip: if you don't define a new graph, tensorflow automatically creates
   a graph for you by default. you can use tf.get_default_graph() to get a
   handle to the graph. you can then inspect the graph, for example by
   printing all its tensors:
print(tf.contrib.graph_editor.get_tensors(tf.get_default_graph()))
     __________________________________________________________________

   unlike tensors, variables can be updated. so let's see how we may use
   variables to do the same thing:
a = tf.variable(1)
b = tf.constant(2)
assign = tf.assign(a, a + b)

sess = tf.session()
sess.run(tf.global_variables_initializer())
print(sess.run(assign))

   again, we get 3 as expected. note that tf.assign returns a tensor
   representing the value of the assignment. so far everything seemed to
   be fine, but let's look at a slightly more complicated example:
a = tf.variable(1)
b = tf.constant(2)
c = a + b

assign = tf.assign(a, 5)

sess = tf.session()
for i in range(10):
    sess.run(tf.global_variables_initializer())
    print(sess.run([assign, c]))

   note that the tensor c here won't have a deterministic value. this
   value might be 3 or 7 depending on whether addition or assignment gets
   executed first.

   you should note that the order that you define ops in your code doesn't
   matter to tensorflow runtime. the only thing that matters is the
   control dependencies. control dependencies for tensors are
   straightforward. every time you use a tensor in an operation that op
   will define an implicit dependency to that tensor. but things get
   complicated with variables because they can take many values.

   when dealing with variables, you may need to explicitly define
   dependencies using tf.control_dependencies() as follows:
a = tf.variable(1)
b = tf.constant(2)
c = a + b

with tf.control_dependencies([c]):
    assign = tf.assign(a, 5)

sess = tf.session()
for i in range(10):
    sess.run(tf.global_variables_initializer())
    print(sess.run([assign, c]))

   this will make sure that the assign op will be called after the
   addition.

control flow operations: conditionals and loops

   when building complex models such as recurrent neural networks you may
   need to control the flow of operations through conditionals and loops.
   in this section we introduce a number of commonly used control flow
   ops.

   let's assume you want to decide whether to multiply to or add two given
   tensors based on a predicate. this can be simply implemented with
   tf.cond which acts as a python "if" function:
a = tf.constant(1)
b = tf.constant(2)

p = tf.constant(true)

x = tf.cond(p, lambda: a + b, lambda: a * b)

print(tf.session().run(x))

   since the predicate is true in this case, the output would be the
   result of the addition, which is 3.

   most of the times when using tensorflow you are using large tensors and
   want to perform operations in batch. a related conditional operation is
   tf.where, which like tf.cond takes a predicate, but selects the output
   based on the condition in batch.
a = tf.constant([1, 1])
b = tf.constant([2, 2])

p = tf.constant([true, false])

x = tf.where(p, a + b, a * b)

print(tf.session().run(x))

   this will return [3, 2].

   another widely used control flow operation is tf.while_loop. it allows
   building dynamic loops in tensorflow that operate on sequences of
   variable length. let's see how we can generate fibonacci sequence with
   tf.while_loops:
n = tf.constant(5)

def cond(i, a, b):
    return i < n

def body(i, a, b):
    return i + 1, b, a + b

i, a, b = tf.while_loop(cond, body, (2, 1, 1))

print(tf.session().run(b))

   this will print 5. tf.while_loops takes a condition function, and a
   loop body function, in addition to initial values for loop variables.
   these loop variables are then updated by multiple calls to the body
   function until the condition returns false.

   now imagine we want to keep the whole series of fibonacci sequence. we
   may update our body to keep a record of the history of current values:
n = tf.constant(5)

def cond(i, a, b, c):
    return i < n

def body(i, a, b, c):
    return i + 1, b, a + b, tf.concat([c, [a + b]], 0)

i, a, b, c = tf.while_loop(cond, body, (2, 1, 1, tf.constant([1, 1])))

print(tf.session().run(c))

   now if you try running this, tensorflow will complain that the shape of
   the the fourth loop variable is changing. so you must make that
   explicit that it's intentional:
i, a, b, c = tf.while_loop(
    cond, body, (2, 1, 1, tf.constant([1, 1])),
    shape_invariants=(tf.tensorshape([]),
                      tf.tensorshape([]),
                      tf.tensorshape([]),
                      tf.tensorshape([none])))

   this is not only getting ugly, but is also somewhat inefficient. note
   that we are building a lot of intermediary tensors that we don't use.
   tensorflow has a better solution for this kind of growing arrays. meet
   tf.tensorarray. let's do the same thing this time with tensor arrays:
n = tf.constant(5)

c = tf.tensorarray(tf.int32, n)
c = c.write(0, 1)
c = c.write(1, 1)

def cond(i, a, b, c):
    return i < n

def body(i, a, b, c):
    c = c.write(i, a + b)
    return i + 1, b, a + b, c

i, a, b, c = tf.while_loop(cond, body, (2, 1, 1, c))

c = c.stack()

print(tf.session().run(c))

   tensorflow while loops and tensor arrays are essential tools for
   building complex recurrent neural networks. as an exercise try
   implementing [103]id125 using tf.while_loops. can you make it
   more efficient with tensor arrays?

prototyping kernels and advanced visualization with python ops

   operation kernels in tensorflow are entirely written in c++ for
   efficiency. but writing a tensorflow kernel in c++ can be quite a pain.
   so, before spending hours implementing your kernel you may want to
   prototype something quickly, however inefficient. with tf.py_func() you
   can turn any piece of python code to a tensorflow operation.

   for example this is how you can implement a simple relu nonlinearity
   kernel in tensorflow as a python op:
import numpy as np
import tensorflow as tf
import uuid

def relu(inputs):
    # define the op in python
    def _relu(x):
        return np.maximum(x, 0.)

    # define the op's gradient in python
    def _relu_grad(x):
        return np.float32(x > 0)

    # an adapter that defines a gradient op compatible with tensorflow
    def _relu_grad_op(op, grad):
        x = op.inputs[0]
        x_grad = grad * tf.py_func(_relu_grad, [x], tf.float32)
        return x_grad

    # register the gradient with a unique id
    grad_name = "myrelugrad_" + str(uuid.uu ())
    tf.registergradient(grad_name)(_relu_grad_op)

    # override the gradient of the custom op
    g = tf.get_default_graph()
    with g.gradient_override_map({"pyfunc": grad_name}):
        output = tf.py_func(_relu, [inputs], tf.float32)
    return output

   to verify that the gradients are correct you can use tensorflow's
   gradient checker:
x = tf.random_normal([10])
y = relu(x * x)

with tf.session():
    diff = tf.test.compute_gradient_error(x, [10], y, [10])
    print(diff)

   compute_gradient_error() computes the gradient numerically and returns
   the difference with the provided gradient. what we want is a very low
   difference.

   note that this implementation is pretty inefficient, and is only useful
   for prototyping, since the python code is not parallelizable and won't
   run on gpu. once you verified your idea, you definitely would want to
   write it as a c++ kernel.

   in practice we commonly use python ops to do visualization on
   tensorboard. consider the case that you are building an image
   classification model and want to visualize your model predictions
   during training. tensorflow allows visualizing images with
   tf.summary.image() function:
image = tf.placeholder(tf.float32)
tf.summary.image("image", image)

   but this only visualizes the input image. in order to visualize the
   predictions you have to find a way to add annotations to the image
   which may be almost impossible with existing ops. an easier way to do
   this is to do the drawing in python, and wrap it in a python op:
import io
import matplotlib.pyplot as plt
import numpy as np
import pil
import tensorflow as tf

def visualize_labeled_images(images, labels, max_outputs=3, name="image"):
    def _visualize_image(image, label):
        # do the actual drawing in python
        fig = plt.figure(figsize=(3, 3), dpi=80)
        ax = fig.add_subplot(111)
        ax.imshow(image[::-1,...])
        ax.text(0, 0, str(label),
          horizontalalignment="left",
          verticalalignment="top")
        fig.canvas.draw()

        # write the plot as a memory file.
        buf = io.bytesio()
        data = fig.savefig(buf, format="png")
        buf.seek(0)

        # read the image and convert to numpy array
        img = pil.image.open(buf)
        return np.array(img.getdata()).reshape(img.size[0], img.size[1], -1)

    def _visualize_images(images, labels):
        # only display the given number of examples in the batch
        outputs = []
        for i in range(max_outputs):
            output = _visualize_image(images[i], labels[i])
            outputs.append(output)
        return np.array(outputs, dtype=np.uint8)

    # run the python op.
    figs = tf.py_func(_visualize_images, [images, labels], tf.uint8)
    return tf.summary.image(name, figs)

   note that since summaries are usually only evaluated once in a while
   (not per step), this implementation may be used in practice without
   worrying about efficiency.

multi-gpu processing with data parallelism

   if you write your software in a language like c++ for a single cpu
   core, making it run on multiple gpus in parallel would require
   rewriting the software from scratch. but this is not the case with
   tensorflow. because of its symbolic nature, tensorflow can hide all
   that complexity, making it effortless to scale your program across many
   cpus and gpus.

   let's start with the simple example of adding two vectors on cpu:
import tensorflow as tf

with tf.device(tf.devicespec(device_type="cpu", device_index=0)):
   a = tf.random_uniform([1000, 100])
   b = tf.random_uniform([1000, 100])
   c = a + b

tf.session().run(c)

   the same thing can as simply be done on gpu:
with tf.device(tf.devicespec(device_type="gpu", device_index=0)):
    a = tf.random_uniform([1000, 100])
    b = tf.random_uniform([1000, 100])
    c = a + b

   but what if we have two gpus and want to utilize both? to do that, we
   can split the data and use a separate gpu for processing each half:
split_a = tf.split(a, 2)
split_b = tf.split(b, 2)

split_c = []
for i in range(2):
    with tf.device(tf.devicespec(device_type="gpu", device_index=i)):
        split_c.append(split_a[i] + split_b[i])

c = tf.concat(split_c, axis=0)

   let's rewrite this in a more general form so that we can replace
   addition with any other set of operations:
def make_parallel(fn, num_gpus, **kwargs):
    in_splits = {}
    for k, v in kwargs.items():
        in_splits[k] = tf.split(v, num_gpus)

    out_split = []
    for i in range(num_gpus):
        with tf.device(tf.devicespec(device_type="gpu", device_index=i)):
            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.auto_reuse)
:
                out_split.append(fn(**{k : v[i] for k, v in in_splits.items()}))

    return tf.concat(out_split, axis=0)


def model(a, b):
    return a + b

c = make_parallel(model, 2, a=a, b=b)

   you can replace the model with any function that takes a set of tensors
   as input and returns a tensor as result with the condition that both
   the input and output are in batch. note that we also added a variable
   scope and set the reuse to true. this makes sure that we use the same
   variables for processing both splits. this is something that will
   become handy in our next example.

   let's look at a slightly more practical example. we want to train a
   neural network on multiple gpus. during training we not only need to
   compute the forward pass but also need to compute the backward pass
   (the gradients). but how can we parallelize the gradient computation?
   this turns out to be pretty easy.

   recall from the first item that we wanted to fit a second degree
   polynomial to a set of samples. we reorganized the code a bit to have
   the bulk of the operations in the model function:
import numpy as np
import tensorflow as tf

def model(x, y):
    w = tf.get_variable("w", shape=[3, 1])

    f = tf.stack([tf.square(x), x, tf.ones_like(x)], 1)
    yhat = tf.squeeze(tf.matmul(f, w), 1)

    loss = tf.square(yhat - y)
    return loss

x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)

loss = model(x, y)

train_op = tf.train.adamoptimizer(0.1).minimize(
    tf.reduce_mean(loss))

def generate_data():
    x_val = np.random.uniform(-10.0, 10.0, size=100)
    y_val = 5 * np.square(x_val) + 3
    return x_val, y_val

sess = tf.session()
sess.run(tf.global_variables_initializer())
for _ in range(1000):
    x_val, y_val = generate_data()
    _, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})

_, loss_val = sess.run([train_op, loss], {x: x_val, y: y_val})
print(sess.run(tf.contrib.framework.get_variables_by_name("w")))

   now let's use make_parallel that we just wrote to parallelize this. we
   only need to change two lines of code from the above code:
loss = make_parallel(model, 2, x=x, y=y)

train_op = tf.train.adamoptimizer(0.1).minimize(
    tf.reduce_mean(loss),
    colocate_gradients_with_ops=true)

   the only thing that we need to change to parallelize id26 of
   gradients is to set the colocate_gradients_with_ops flag to true. this
   ensures that gradient ops run on the same device as the original op.

debugging tensorflow models

   symbolic nature of tensorflow makes it relatively more difficult to
   debug tensorflow code compared to regular python code. here we
   introduce a number of tools included with tensorflow that make
   debugging much easier.

   probably the most common error one can make when using tensorflow is
   passing tensors of wrong shape to ops. many tensorflow ops can operate
   on tensors of different ranks and shapes. this can be convenient when
   using the api, but may lead to extra headache when things go wrong.

   for example, consider the tf.matmul op, it can multiply two matrices:
a = tf.random_uniform([2, 3])
b = tf.random_uniform([3, 4])
c = tf.matmul(a, b)  # c is a tensor of shape [2, 4]

   but the same function also does batch id127:
a = tf.random_uniform([10, 2, 3])
b = tf.random_uniform([10, 3, 4])
tf.matmul(a, b)  # c is a tensor of shape [10, 2, 4]

   another example that we talked about before in the [104]broadcasting
   section is add operation which supports broadcasting:
a = tf.constant([[1.], [2.]])
b = tf.constant([1., 2.])
c = a + b  # c is a tensor of shape [2, 2]

validating your tensors with tf.assert* ops

   one way to reduce the chance of unwanted behavior is to explicitly
   verify the rank or shape of intermediate tensors with tf.assert* ops.
a = tf.constant([[1.], [2.]])
b = tf.constant([1., 2.])
check_a = tf.assert_rank(a, 1)  # this will raise an invalidargumenterror except
ion
check_b = tf.assert_rank(b, 1)
with tf.control_dependencies([check_a, check_b]):
    c = a + b  # c is a tensor of shape [2, 2]

   remember that assertion nodes like other operations are part of the
   graph and if not evaluated would get pruned during session.run(). so
   make sure to create explicit dependencies to assertion ops, to force
   tensorflow to execute them.

   you can also use assertions to validate the value of tensors at
   runtime:
check_pos = tf.assert_positive(a)

   see the official docs for a [105]full list of assertion ops.

logging tensor values with tf.print

   another useful built-in function for debugging is tf.print which logs
   the given tensors to the standard error:
input_copy = tf.print(input, tensors_to_print_list)

   note that tf.print returns a copy of its first argument as output. one
   way to force tf.print to run is to pass its output to another op that
   gets executed. for example if we want to print the value of tensors a
   and b before adding them we could do something like this:
a = ...
b = ...
a = tf.print(a, [a, b])
c = a + b

   alternatively we could manually define a control dependency.

check your gradients with tf.compute_gradient_error

   not all the operations in tensorflow come with gradients, and it's easy
   to unintentionally build graphs for which tensorflow can not compute
   the gradients.

   let's look at an example:
import tensorflow as tf

def non_differentiable_softmax_id178(logits):
    probs = tf.nn.softmax(logits)
    return tf.nn.softmax_cross_id178_with_logits(labels=probs, logits=logits)

w = tf.get_variable("w", shape=[5])
y = -non_differentiable_softmax_id178(w)

opt = tf.train.adamoptimizer()
train_op = opt.minimize(y)

sess = tf.session()
sess.run(tf.global_variables_initializer())
for i in range(10000):
    sess.run(train_op)

print(sess.run(tf.nn.softmax(w)))

   we are using tf.nn.softmax_cross_id178_with_logits to define id178
   over a categorical distribution. we then use adam optimizer to find the
   weights with maximum id178. if you have passed a course on
   id205, you would know that uniform distribution contains
   maximum id178. so you would expect for the result to be [0.2, 0.2,
   0.2, 0.2, 0.2]. but if you run this you may get unexpected results like
   this:
[ 0.34081486  0.24287023  0.23465775  0.08935683  0.09230034]

   it turns out tf.nn.softmax_cross_id178_with_logits has undefined
   gradients with respect to labels! but how may we spot this if we didn't
   know?

   fortunately for us tensorflow comes with a numerical differentiator
   that can be used to find symbolic gradient errors. let's see how we can
   use it:
with tf.session():
    diff = tf.test.compute_gradient_error(w, [5], y, [])
    print(diff)

   if you run this, you would see that the difference between the
   numerical and symbolic gradients are pretty high (0.06 - 0.1 in my
   tries).

   now let's fix our function with a differentiable version of the id178
   and check again:
import tensorflow as tf
import numpy as np

def softmax_id178(logits, dim=-1):
    plogp = tf.nn.softmax(logits, dim) * tf.nn.log_softmax(logits, dim)
    return -tf.reduce_sum(plogp, dim)

w = tf.get_variable("w", shape=[5])
y = -softmax_id178(w)

print(w.get_shape())
print(y.get_shape())

with tf.session() as sess:
    diff = tf.test.compute_gradient_error(w, [5], y, [])
    print(diff)

   the difference should be ~0.0001 which looks much better.

   now if you run the optimizer again with the correct version you can see
   the final weights would be:
[ 0.2  0.2  0.2  0.2  0.2]

   which are exactly what we wanted.

   [106]tensorflow summaries, and [107]tfdbg (tensorflow debugger) are
   other tools that can be used for debugging. please refer to the
   official docs to learn more.

numerical stability in tensorflow

   when using any numerical computation library such as numpy or
   tensorflow, it's important to note that writing mathematically correct
   code doesn't necessarily lead to correct results. you also need to make
   sure that the computations are stable.

   let's start with a simple example. from primary school we know that x *
   y / y is equal to x for any non zero value of x. but let's see if
   that's always true in practice:
import numpy as np

x = np.float32(1)

y = np.float32(1e-50)  # y would be stored as zero
z = x * y / y

print(z)  # prints nan

   the reason for the incorrect result is that y is simply too small for
   float32 type. a similar problem occurs when y is too large:
y = np.float32(1e39)  # y would be stored as inf
z = x * y / y

print(z)  # prints 0

   the smallest positive value that float32 type can represent is
   1.4013e-45 and anything below that would be stored as zero. also, any
   number beyond 3.40282e+38, would be stored as inf.
print(np.nextafter(np.float32(0), np.float32(1)))  # prints 1.4013e-45
print(np.finfo(np.float32).max)  # print 3.40282e+38

   to make sure that your computations are stable, you want to avoid
   values with small or very large absolute value. this may sound very
   obvious, but these kind of problems can become extremely hard to debug
   especially when doing id119 in tensorflow. this is because
   you not only need to make sure that all the values in the forward pass
   are within the valid range of your data types, but also you need to
   make sure of the same for the backward pass (during gradient
   computation).

   let's look at a real example. we want to compute the softmax over a
   vector of logits. a naive implementation would look something like
   this:
import tensorflow as tf

def unstable_softmax(logits):
    exp = tf.exp(logits)
    return exp / tf.reduce_sum(exp)

tf.session().run(unstable_softmax([1000., 0.]))  # prints [ nan, 0.]

   note that computing the exponential of logits for relatively small
   numbers results to gigantic results that are out of float32 range. the
   largest valid logit for our naive softmax implementation is
   ln(3.40282e+38) = 88.7, anything beyond that leads to a nan outcome.

   but how can we make this more stable? the solution is rather simple.
   it's easy to see that exp(x - c) /     exp(x - c) = exp(x) /     exp(x).
   therefore we can subtract any constant from the logits and the result
   would remain the same. we choose this constant to be the maximum of
   logits. this way the domain of the exponential function would be
   limited to [-inf, 0], and consequently its range would be [0.0, 1.0]
   which is desirable:
import tensorflow as tf

def softmax(logits):
    exp = tf.exp(logits - tf.reduce_max(logits))
    return exp / tf.reduce_sum(exp)

tf.session().run(softmax([1000., 0.]))  # prints [ 1., 0.]

   let's look at a more complicated case. consider we have a
   classification problem. we use the softmax function to produce
   probabilities from our logits. we then define our id168 to be
   the cross id178 between our predictions and the labels. recall that
   cross id178 for a categorical distribution can be simply defined as
   xe(p, q) = -    p_i log(q_i). so a naive implementation of the cross
   id178 would look like this:
def unstable_softmax_cross_id178(labels, logits):
    logits = tf.log(softmax(logits))
    return -tf.reduce_sum(labels * logits)

labels = tf.constant([0.5, 0.5])
logits = tf.constant([1000., 0.])

xe = unstable_softmax_cross_id178(labels, logits)

print(tf.session().run(xe))  # prints inf

   note that in this implementation as the softmax output approaches zero,
   the log's output approaches infinity which causes instability in our
   computation. we can rewrite this by expanding the softmax and doing
   some simplifications:
def softmax_cross_id178(labels, logits):
    scaled_logits = logits - tf.reduce_max(logits)
    normalized_logits = scaled_logits - tf.reduce_logsumexp(scaled_logits)
    return -tf.reduce_sum(labels * normalized_logits)

labels = tf.constant([0.5, 0.5])
logits = tf.constant([1000., 0.])

xe = softmax_cross_id178(labels, logits)

print(tf.session().run(xe))  # prints 500.0

   we can also verify that the gradients are also computed correctly:
g = tf.gradients(xe, logits)
print(tf.session().run(g))  # prints [0.5, -0.5]

   which is correct.

   let me remind again that extra care must be taken when doing gradient
   descent to make sure that the range of your functions as well as the
   gradients for each layer are within a valid range. exponential and
   logarithmic functions when used naively are especially problematic
   because they can map small numbers to enormous ones and the other way
   around.

building a neural network training framework with learn api

   for simplicity, in most of the examples here we manually create
   sessions and we don't care about saving and loading checkpoints but
   this is not how we usually do things in practice. you most probably
   want to use the learn api to take care of session management and
   logging. we provide a simple but practical [108]framework for training
   neural networks using tensorflow. in this item we explain how this
   framework works.

   when experimenting with neural network models you usually have a
   training/test split. you want to train your model on the training set,
   and once in a while evaluate it on test set and compute some metrics.
   you also need to store the model parameters as a checkpoint, and
   ideally you want to be able to stop and resume training. tensorflow's
   learn api is designed to make this job easier, letting us focus on
   developing the actual model.

   the most basic way of using tf.learn api is to use tf.estimator object
   directly. you need to define a model function that defines a loss
   function, a train op, one or a set of predictions, and optionally a set
   of metric ops for evaluation:
import tensorflow as tf

def model_fn(features, labels, mode, params):
    predictions = ...
    loss = ...
    train_op = ...
    metric_ops = ...
    return tf.estimator.estimatorspec(
        mode=mode,
        predictions=predictions,
        loss=loss,
        train_op=train_op,
        eval_metric_ops=metric_ops)

params = ...
run_config = tf.estimator.runconfig(model_dir=flags.output_dir)
estimator = tf.estimator.estimator(
    model_fn=model_fn, config=run_config, params=params)

   to train the model you would then simply call estimator.train()
   function while providing an input function to read the data:
estimator.train(input_fn=input_fn, max_steps=...)

   and to evaluate the model, simply call estimator.evaluate():
estimator.evaluate(input_fn=input_fn)

   the input function returns two tensors (or dictionaries of tensors)
   providing the features and labels to be passed to the model:
def input_fn():
    features = ...
    labels = ...
    return features, labels

   see [109]mnist.py for an example of how to read your data with the
   dataset api. to learn about various ways of reading your data in
   tensorflow refer to [110]this item.

   the framework also comes with a simple convolutional network classifier
   in [111]alexnet.py that includes an example model.

   and that's it! this is all you need to get started with tensorflow
   learn api. i recommend to have a look at the framework [112]source code
   and see the official python api to learn more about the learn api.

part ii: tensorflow cookbook

   this section includes implementation of a set of common operations in
   tensorflow.

get shape

def get_shape(tensor):
  """returns static shape if available and dynamic shape otherwise."""
  static_shape = tensor.shape.as_list()
  dynamic_shape = tf.unstack(tf.shape(tensor))
  dims = [s[1] if s[0] is none else s[0]
          for s in zip(static_shape, dynamic_shape)]
  return dims

batch gather

def batch_gather(tensor, indices):
  """gather in batch from a tensor of arbitrary size.

  in pseudocode this module will produce the following:
  output[i] = tf.gather(tensor[i], indices[i])

  args:
    tensor: tensor of arbitrary size.
    indices: vector of indices.
  returns:
    output: a tensor of gathered values.
  """
  shape = get_shape(tensor)
  flat_first = tf.reshape(tensor, [shape[0] * shape[1]] + shape[2:])
  indices = tf.convert_to_tensor(indices)
  offset_shape = [shape[0]] + [1] * (indices.shape.ndims - 1)
  offset = tf.reshape(tf.range(shape[0]) * shape[1], offset_shape)
  output = tf.gather(flat_first, indices + offset)
  return output

id125

import tensorflow as tf

def id56_beam_search(update_fn, initial_state, sequence_length, beam_width,
                    begin_token_id, end_token_id, name="id56"):
  """beam-search decoder for recurrent models.

  args:
    update_fn: function to compute the next state and logits given the current
               state and ids.
    initial_state: recurrent model states.
    sequence_length: length of the generated sequence.
    beam_width: beam width.
    begin_token_id: begin token id.
    end_token_id: end token id.
    name: scope of the variables.
  returns:
    ids: output indices.
    logprobs: output log probabilities probabilities.
  """
  batch_size = initial_state.shape.as_list()[0]

  state = tf.tile(tf.expand_dims(initial_state, axis=1), [1, beam_width, 1])

  sel_sum_logprobs = tf.log([[1.] + [0.] * (beam_width - 1)])

  ids = tf.tile([[begin_token_id]], [batch_size, beam_width])
  sel_ids = tf.zeros([batch_size, beam_width, 0], dtype=ids.dtype)

  mask = tf.ones([batch_size, beam_width], dtype=tf.float32)

  for i in range(sequence_length):
    with tf.variable_scope(name, reuse=true if i > 0 else none):

      state, logits = update_fn(state, ids)
      logits = tf.nn.log_softmax(logits)

      sum_logprobs = (
          tf.expand_dims(sel_sum_logprobs, axis=2) +
          (logits * tf.expand_dims(mask, axis=2)))

      num_classes = logits.shape.as_list()[-1]

      sel_sum_logprobs, indices = tf.nn.top_k(
          tf.reshape(sum_logprobs, [batch_size, num_classes * beam_width]),
          k=beam_width)

      ids = indices % num_classes

      beam_ids = indices // num_classes

      state = batch_gather(state, beam_ids)

      sel_ids = tf.concat([batch_gather(sel_ids, beam_ids),
                           tf.expand_dims(ids, axis=2)], axis=2)

      mask = (batch_gather(mask, beam_ids) *
              tf.to_float(tf.not_equal(ids, end_token_id)))

  return sel_ids, sel_sum_logprobs

merge

import tensorflow as tf

def merge(tensors, units, activation=tf.nn.relu, name=none, **kwargs):
  """merge features with broadcasting support.

  this operation concatenates multiple features of varying length and applies
  non-linear transformation to the outcome.

  example:
    a = tf.zeros([m, 1, d1])
    b = tf.zeros([1, n, d2])
    c = merge([a, b], d3)  # shape of c would be [m, n, d3].

  args:
    tensors: a list of tensor with the same rank.
    units: number of units in the projection function.
  """
  with tf.variable_scope(name, default_name="merge"):
    # apply linear projection to input tensors.
    projs = []
    for i, tensor in enumerate(tensors):
      proj = tf.layers.dense(
          tensor, units,
          name="proj_%d" % i,
          **kwargs)
      projs.append(proj)

    # compute sum of tensors.
    result = projs.pop()
    for proj in projs:
      result = result + proj

    # apply nonlinearity.
    if activation:
      result = activation(result)
  return result

id178

import tensorflow as tf

def softmax_id178(logits, dim=-1):
  """compute id178 over specified dimensions."""
  plogp = tf.nn.softmax(logits, dim) * tf.nn.log_softmax(logits, dim)
  return -tf.reduce_sum(plogp, dim)

kl-divergence

def gaussian_kl(q, p=(0., 0.)):
  """computes kl divergence between two isotropic gaussian distributions.

  to ensure numerical stability, this op uses mu, log(sigma^2) to represent
  the distribution. if q is not provided, it's assumed to be unit gaussian.

  args:
    q: a tuple (mu, log(sigma^2)) representing a multi-variatie gaussian.
    p: a tuple (mu, log(sigma^2)) representing a multi-variatie gaussian.
  returns:
    a tensor representing kl(q, p).
  """
  mu1, log_sigma1_sq = q
  mu2, log_sigma2_sq = p
  return tf.reduce_sum(
    0.5 * (log_sigma2_sq - log_sigma1_sq +
           tf.exp(log_sigma1_sq - log_sigma2_sq) +
           tf.square(mu1 - mu2) / tf.exp(log_sigma2_sq) -
           1), axis=-1)

make parallel

def make_parallel(fn, num_gpus, **kwargs):
  """parallelize given model on multiple gpu devices.

  args:
    fn: arbitrary function that takes a set of input tensors and outputs a
        single tensor. first dimension of inputs and output tensor are assumed
        to be batch dimension.
    num_gpus: number of gpu devices.
    **kwargs: keyword arguments to be passed to the model.
  returns:
    a tensor corresponding to the model output.
  """
  in_splits = {}
  for k, v in kwargs.items():
    in_splits[k] = tf.split(v, num_gpus)

  out_split = []
  for i in range(num_gpus):
    with tf.device(tf.devicespec(device_type="gpu", device_index=i)):
      with tf.variable_scope(tf.get_variable_scope(), reuse=tf.auto_reuse):
        out_split.append(fn(**{k : v[i] for k, v in in_splits.items()}))

  return tf.concat(out_split, axis=0)

leaky relu

def leaky_relu(tensor, alpha=0.1):
    """computes the leaky rectified linear activation."""
    return tf.maximum(tensor, alpha * tensor)

batch id172

def batch_id172(tensor, training=false, epsilon=0.001, momentum=0.9,
                        fused_batch_norm=false, name=none):
  """performs batch id172 on given 4-d tensor.

  the features are assumed to be in nhwc format. noe that you need to
  run update_ops in order for this function to perform correctly, e.g.:

  with tf.control_dependencies(tf.get_collection(tf.graphkeys.update_ops)):
    train_op = optimizer.minimize(loss)

  based on: https://arxiv.org/abs/1502.03167
  """
  with tf.variable_scope(name, default_name="batch_id172"):
    channels = tensor.shape.as_list()[-1]
    axes = list(range(tensor.shape.ndims - 1))

    beta = tf.get_variable(
      'beta', channels, initializer=tf.zeros_initializer())
    gamma = tf.get_variable(
      'gamma', channels, initializer=tf.ones_initializer())

    avg_mean = tf.get_variable(
      "avg_mean", channels, initializer=tf.zeros_initializer(),
      trainable=false)
    avg_variance = tf.get_variable(
      "avg_variance", channels, initializer=tf.ones_initializer(),
      trainable=false)

    if training:
      if fused_batch_norm:
        mean, variance = none, none
      else:
        mean, variance = tf.nn.moments(tensor, axes=axes)
    else:
      mean, variance = avg_mean, avg_variance

    if fused_batch_norm:
      tensor, mean, variance = tf.nn.fused_batch_norm(
        tensor, scale=gamma, offset=beta, mean=mean, variance=variance,
        epsilon=epsilon, is_training=training)
    else:
      tensor = tf.nn.batch_id172(
        tensor, mean, variance, beta, gamma, epsilon)

    if training:
      update_mean = tf.assign(
        avg_mean, avg_mean * momentum + mean * (1.0 - momentum))
      update_variance = tf.assign(
        avg_variance, avg_variance * momentum + variance * (1.0 - momentum))

      tf.add_to_collection(tf.graphkeys.update_ops, update_mean)
      tf.add_to_collection(tf.graphkeys.update_ops, update_variance)

  return tensor

     *    2019 github, inc.
     * [113]terms
     * [114]privacy
     * [115]security
     * [116]status
     * [117]help

     * [118]contact github
     * [119]pricing
     * [120]api
     * [121]training
     * [122]blog
     * [123]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [124]reload to refresh your
   session. you signed out in another tab or window. [125]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/vahidk/effectivetensorflow/commits/master.atom
   3. https://github.com/vahidk/effectivetensorflow#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/vahidk/effectivetensorflow
  32. https://github.com/join
  33. https://github.com/login?return_to=/vahidk/effectivetensorflow
  34. https://github.com/vahidk/effectivetensorflow/watchers
  35. https://github.com/login?return_to=/vahidk/effectivetensorflow
  36. https://github.com/vahidk/effectivetensorflow/stargazers
  37. https://github.com/login?return_to=/vahidk/effectivetensorflow
  38. https://github.com/vahidk/effectivetensorflow/network/members
  39. https://github.com/vahidk
  40. https://github.com/vahidk/effectivetensorflow
  41. https://github.com/vahidk/effectivetensorflow
  42. https://github.com/vahidk/effectivetensorflow/issues
  43. https://github.com/vahidk/effectivetensorflow/pulls
  44. https://github.com/vahidk/effectivetensorflow/projects
  45. https://github.com/vahidk/effectivetensorflow/pulse
  46. https://github.com/join?source=prompt-code
  47. https://twitter.com/vahidk
  48. https://github.com/topics/tensorflow
  49. https://github.com/topics/neural-network
  50. https://github.com/topics/deep-learning
  51. https://github.com/topics/machine-learning
  52. https://github.com/topics/ebook
  53. https://github.com/vahidk/effectivetensorflow/commits/master
  54. https://github.com/vahidk/effectivetensorflow/branches
  55. https://github.com/vahidk/effectivetensorflow/releases
  56. https://github.com/vahidk/effectivetensorflow/graphs/contributors
  57. https://github.com/vahidk/effectivetensorflow/find/master
  58. https://github.com/vahidk/effectivetensorflow/archive/master.zip
  59. https://github.com/login?return_to=https://github.com/vahidk/effectivetensorflow
  60. https://github.com/join?return_to=/vahidk/effectivetensorflow
  61. https://desktop.github.com/
  62. https://desktop.github.com/
  63. https://developer.apple.com/xcode/
  64. https://visualstudio.github.com/
  65. https://github.com/vahidk
  66. https://github.com/vahidk/effectivetensorflow/commits?author=vahidk
  67. https://github.com/vahidk/effectivetensorflow/commit/d998bef4512a28b22e42477b6d9bfdeacab6b0e6
  68. https://github.com/vahidk/effectivetensorflow/commit/d998bef4512a28b22e42477b6d9bfdeacab6b0e6
  69. https://github.com/vahidk/effectivetensorflow/tree/d998bef4512a28b22e42477b6d9bfdeacab6b0e6
  70. https://github.com/vahidk/effectivetensorflow/tree/master/code
  71. https://github.com/vahidk/effectivetensorflow/blob/master/.gitignore
  72. https://github.com/vahidk/effectivetensorflow/blob/master/.gitmodules
  73. https://github.com/vahidk/effectivetensorflow/commit/7b940f7bd33c3406644333411c452ab8251bf473
  74. https://github.com/vahidk/effectivetensorflow/blob/master/readme.md
  75. https://github.com/vahidk/effectivetensorflow#basics
  76. https://github.com/vahidk/effectivetensorflow#shapes
  77. https://github.com/vahidk/effectivetensorflow#scopes
  78. https://github.com/vahidk/effectivetensorflow#broadcast
  79. https://github.com/vahidk/effectivetensorflow#data
  80. https://github.com/vahidk/effectivetensorflow#overloaded_ops
  81. https://github.com/vahidk/effectivetensorflow#control_deps
  82. https://github.com/vahidk/effectivetensorflow#control_flow
  83. https://github.com/vahidk/effectivetensorflow#python_ops
  84. https://github.com/vahidk/effectivetensorflow#multi_gpu
  85. https://github.com/vahidk/effectivetensorflow#debug
  86. https://github.com/vahidk/effectivetensorflow#stable
  87. https://github.com/vahidk/effectivetensorflow#tf_learn
  88. https://github.com/vahidk/effectivetensorflow#get_shape
  89. https://github.com/vahidk/effectivetensorflow#batch_gather
  90. https://github.com/vahidk/effectivetensorflow#beam_search
  91. https://github.com/vahidk/effectivetensorflow#merge
  92. https://github.com/vahidk/effectivetensorflow#id178
  93. https://github.com/vahidk/effectivetensorflow#kld
  94. https://github.com/vahidk/effectivetensorflow#make_parallel
  95. https://github.com/vahidk/effectivetensorflow#leaky_relu
  96. https://github.com/vahidk/effectivetensorflow#batch_norm
  97. https://github.com/vahidk/effectivetensorflow/tree/v1
  98. https://github.com/vahidk/effectivetensorflow/tree/v2
  99. https://github.com/vahidk/tensorflowframework
 100. https://github.com/vahidk/effectivetensorflow#merge
 101. https://www.tensorflow.org/api_guides/python/reading_data#reading_from_files
 102. https://github.com/vahidk/tensorflowframework/blob/master/trainer.py
 103. https://en.wikipedia.org/wiki/beam_search
 104. https://github.com/vahidk/effectivetensorflow#broadcast
 105. https://www.tensorflow.org/api_guides/python/check_ops
 106. https://www.tensorflow.org/api_guides/python/summary
 107. https://www.tensorflow.org/api_guides/python/tfdbg
 108. https://github.com/vahidk/tensorflowframework/tree/master
 109. https://github.com/vahidk/tensorflowframework/blob/master/dataset/mnist.py
 110. https://github.com/vahidk/effectivetensorflow#data
 111. https://github.com/vahidk/tensorflowframework/blob/master/model/alexnet.py
 112. https://github.com/vahidk/tensorflowframework
 113. https://github.com/site/terms
 114. https://github.com/site/privacy
 115. https://github.com/security
 116. https://githubstatus.com/
 117. https://help.github.com/
 118. https://github.com/contact
 119. https://github.com/pricing
 120. https://developer.github.com/
 121. https://training.github.com/
 122. https://github.blog/
 123. https://github.com/about
 124. https://github.com/vahidk/effectivetensorflow
 125. https://github.com/vahidk/effectivetensorflow

   hidden links:
 127. https://github.com/
 128. https://github.com/vahidk/effectivetensorflow
 129. https://github.com/vahidk/effectivetensorflow
 130. https://github.com/vahidk/effectivetensorflow
 131. https://help.github.com/articles/which-remote-url-should-i-use
 132. https://github.com/vahidk/effectivetensorflow#effective-tensorflow
 133. https://github.com/vahidk/effectivetensorflow#table-of-contents
 134. https://github.com/vahidk/effectivetensorflow#part-i-tensorflow-fundamentals
 135. https://github.com/vahidk/effectivetensorflow#part-ii-tensorflow-cookbook
 136. https://github.com/vahidk/effectivetensorflow#part-i-tensorflow-fundamentals-1
 137. https://github.com/vahidk/effectivetensorflow#tensorflow-basics
 138. https://github.com/vahidk/effectivetensorflow#understanding-static-and-dynamic-shapes
 139. https://github.com/vahidk/effectivetensorflow#scopes-and-when-to-use-them
 140. https://github.com/vahidk/effectivetensorflow#broadcasting-the-good-and-the-ugly
 141. https://github.com/vahidk/effectivetensorflow#feeding-data-to-tensorflow
 142. https://github.com/vahidk/effectivetensorflow#constants
 143. https://github.com/vahidk/effectivetensorflow#placeholders
 144. https://github.com/vahidk/effectivetensorflow#python-ops
 145. https://github.com/vahidk/effectivetensorflow#dataset-api
 146. https://github.com/vahidk/effectivetensorflow#take-advantage-of-the-overloaded-operators
 147. https://github.com/vahidk/effectivetensorflow#understanding-order-of-execution-and-control-dependencies
 148. https://github.com/vahidk/effectivetensorflow#control-flow-operations-conditionals-and-loops
 149. https://github.com/vahidk/effectivetensorflow#prototyping-kernels-and-advanced-visualization-with-python-ops
 150. https://github.com/vahidk/effectivetensorflow#multi-gpu-processing-with-data-parallelism
 151. https://github.com/vahidk/effectivetensorflow#debugging-tensorflow-models
 152. https://github.com/vahidk/effectivetensorflow#validating-your-tensors-with-tfassert-ops
 153. https://github.com/vahidk/effectivetensorflow#logging-tensor-values-with-tfprint
 154. https://github.com/vahidk/effectivetensorflow#check-your-gradients-with-tfcompute_gradient_error
 155. https://github.com/vahidk/effectivetensorflow#numerical-stability-in-tensorflow
 156. https://github.com/vahidk/effectivetensorflow#building-a-neural-network-training-framework-with-learn-api
 157. https://github.com/vahidk/effectivetensorflow#part-ii-tensorflow-cookbook-1
 158. https://github.com/vahidk/effectivetensorflow#get-shape-
 159. https://github.com/vahidk/effectivetensorflow#batch-gather-
 160. https://github.com/vahidk/effectivetensorflow#beam-search-
 161. https://github.com/vahidk/effectivetensorflow#merge-
 162. https://github.com/vahidk/effectivetensorflow#id178-
 163. https://github.com/vahidk/effectivetensorflow#kl-divergence-
 164. https://github.com/vahidk/effectivetensorflow#make-parallel-
 165. https://github.com/vahidk/effectivetensorflow#leaky-relu-
 166. https://github.com/vahidk/effectivetensorflow#batch-id172-
 167. https://github.com/
