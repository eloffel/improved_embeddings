   #[1]publisher

   [2]christopher bourez's blog
   subscribe to the newsletter !

deep learning tutorial on caffe technology : basic commands, python and c++
code.

   sep 4, 2015
   [ins: :ins]

   update! : my [3]fast image annotation tool for caffe has just been
   released ! have a look !

   [4]caffe is certainly one of the best frameworks for deep learning, if
   not the best.

   let   s try to put things into order, in order to get a good tutorial :).

caffe

install

   first install caffe following my tutorials on [5]ubuntu or [6]mac os
   with python layers activated and pycaffe path correctly set export
   pythonpath=~/technologies/caffe/python/:$pythonpath.

launch the python shell

   in the ipython shell in your caffe repository, load the different
   libraries :
import numpy as np
import matplotlib.pyplot as plt
from pil import image
import caffe

   set the computation mode cpu
caffe.set_mode_cpu()

   or gpu
caffe.set_device(0)
caffe.set_mode_gpu()

define a network model

   let   s create first a very simple model with a single convolution
   composed of 3 convolutional neurons, with kernel of size 5x5 and stride
   of 1 :

   simple network

   this net will produce 3 output maps from an input map.

   the output map for a convolution given receptive field size has a
   dimension given by the following equation :
output = (input - kernel_size) / stride + 1

   create a first file conv.prototxt describing the neuron network :
name: "convolution"
input: "data"
input_dim: 1
input_dim: 1
input_dim: 100
input_dim: 100
layer {
  name: "conv"
  type: "convolution"
  bottom: "data"
  top: "conv"
  convolution_param {
    num_output: 3
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}

   with one layer, a convolution, from the [7]catalog of available layers

   load the net
net = caffe.net('conv.prototxt', caffe.test)

   the names of input layers of the net are given by print net.inputs.

   the net contains two ordered dictionaries
     * net.blobs for input data and its propagation in the layers :
       net.blobs['data'] contains input data, an array of shape (1, 1,
       100, 100) net.blobs['conv'] contains computed data in layer    conv   
       (1, 3, 96, 96)
       initialiazed with zeros.
       to print the infos,
  [(k, v.data.shape) for k, v in net.blobs.items()]

     * net.params a vector of blobs for weight and bias parameters
       net.params['conv'][0] contains the weight parameters, an array of
       shape (3, 1, 5, 5) net.params['conv'][1] contains the bias
       parameters, an array of shape (3,)
       initialiazed with    weight_filler    and    bias_filler    algorithms.
       to print the infos :
  [(k, v[0].data.shape, v[1].data.shape) for k, v in net.params.items()]

   blobs are memory abstraction objects (with execution depending on the
   mode), and data is contained in the field data as an array :
print net.blobs['conv'].data.shape

   to draw the network, a siid113 python command :
python python/draw_net.py examples/net_surgery/conv.prototxt my_net.png
open my_net.png

   this will print the following image :

   simple network

compute the net output on an image as input

   let   s load a gray image of size 1x360x480 (channel x height x width)
   into the previous net :

   gray cat

   we need to reshape the data blob (1, 1, 100, 100) to the new size (1,
   1, 360, 480) to fit the image :
im = np.array(image.open('examples/images/cat_gray.jpg'))
im_input = im[np.newaxis, np.newaxis, :, :]
net.blobs['data'].reshape(*im_input.shape)
net.blobs['data'].data[...] = im_input

   let   s compute the blobs given this input
net.forward()

   now net.blobs['conv'] is filled with data, and the 3 pictures inside
   each of the 3 neurons (net.blobs['conv'].data[0,i]) can be plotted
   easily.

   to save the net parameters net.params, just call :
net.save('mymodel.caffemodel')

load pretrained parameters to classify an image

   in the previous net, weight and bias params have been initialiazed
   randomly.

   it is possible to load trained parameters and in this case, the result
   of the net will produce a classification.

   many trained models can be downloaded from the community in the
   [8]caffe model zoo, such as car classification, flower classification,
   digit classification   

   model informations are written in github gist format. the parameters
   are saved in a .caffemodel file specified in the gist. to download the
   model :
./scripts/download_model_from_gist.sh <gist_id>
./scripts/download_model_binary.py <dirname>

   where is the gist directory (by default the gist is saved in the
   *models* directory).

   let   s download the caffenet model and the labels corresponding to the
   classes :
./scripts/download_model_binary.py models/bvlc_reference_caffenet
./data/ilsvrc12/get_ilsvrc_aux.sh

#have a look at the model
python python/draw_net.py models/bvlc_reference_caffenet/deploy.prototxt caffene
t.png
open caffenet.png

   caffenet model

   this model has been trained on processed images, so you need to
   preprocess the image with a preprocessor, before saving it in the blob.

   cat

   that is, in the python shell :
#load the model
net = caffe.net('models/bvlc_reference_caffenet/deploy.prototxt',
                'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemod
el',
                caffe.test)

# load input and configure preprocessing
transformer = caffe.io.transformer({'data': net.blobs['data'].data.shape})
transformer.set_mean('data', np.load('python/caffe/id163/ilsvrc_2012_mean.npy
').mean(1).mean(1))
transformer.set_transpose('data', (2,0,1))
transformer.set_channel_swap('data', (2,1,0))
transformer.set_raw_scale('data', 255.0)

#note we can change the batch size on-the-fly
#since we classify only one image, we change batch size from 10 to 1
net.blobs['data'].reshape(1,3,227,227)

#load the image in the data layer
im = caffe.io.load_image('examples/images/cat.jpg')
net.blobs['data'].data[...] = transformer.preprocess('data', im)

#compute
out = net.forward()

# other possibility : out = net.forward_all(data=np.asarray([transformer.preproc
ess('data', im)]))

#predicted predicted class
print out['prob'].argmax()

#print predicted labels
labels = np.loadtxt("data/ilsvrc12/synset_words.txt", str, delimiter='\t')
top_k = net.blobs['prob'].data[0].flatten().argsort()[-1:-6:-1]
print labels[top_k]

   it will print you the top classes detected for the images.

   go further : [9]create a classification map with net surgery to insert
   a trained model into an extended model where convolutions will be
   innerproducts spatially

   classification map

learn : solve the params on training data

   it is now time to create your own model, and training the parameters on
   training data.

   to train a network, you need
     * its model definition, as seen previously
     * a second protobuf file, the solver file, describing the parameters
       for the stochastic gradient.

   for example, the caffenet solver :
net: "models/bvlc_reference_caffenet/train_val.prototxt"
test_iter: 1000
test_interval: 1000
base_lr: 0.01
lr_policy: "step"
gamma: 0.1
stepsize: 100000
display: 20
max_iter: 450000
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "models/bvlc_reference_caffenet/caffenet_train"
solver_mode: gpu

   usually, you define a train net, for training, with training data, and
   a test set, for validation. either you can define the train and test
   nets in the prototxt solver file
train_net: "examples/hdf5_classification/nonlinear_auto_train.prototxt"
test_net: "examples/hdf5_classification/nonlinear_auto_test.prototxt"

   or you can also specify only one prototxt file, adding an include phase
   statement for the layers that have to be different in training and
   testing phases, such as input data :
layer {
  name: "data"
  type: "data"
  top: "data"
  top: "label"
  include {
    phase: train
  }
  data_param {
    source: "examples/id163/ilsvrc12_train_lmdb"
    batch_size: 256
    backend: lmdb
  }
}
layer {
  name: "data"
  type: "data"
  top: "data"
  top: "label"
  top: "label"
  include {
    phase: test
  }
  data_param {
    source: "examples/id163/ilsvrc12_val_lmdb"
    batch_size: 50
    backend: lmdb
  }
}

   data can also be set directly in python.

   load the solver in python
solver = caffe.get_solver('models/bvlc_reference_caffenet/solver.prototxt')

   by default it is the sgd solver. it   s possible to specify another
   solver_type in the prototxt solver file ([10]adagrad or nesterov). it   s
   also possible to load directly
solver = caffe.sgdsolver('models/bvlc_reference_caffenet/solver.prototxt')

   but be careful since sgdsolver will use sgdsolver whatever is
   configured in the prototxt file    so it is less reliable.

   now, it   s time to begin to see if everything works well and to fill the
   layers in a forward propagation in the net (computation of
   net.blobs[k].data from input layer until the loss layer) :
solver.net.forward()  # train net
solver.test_nets[0].forward()  # test net (there can be more than one)

   for the computation of the gradients (computation of the
   net.blobs[k].diff and net.params[k][j].diff from the loss layer until
   input layer) :
solver.net.backward()

   to launch one step of the id119, that is a forward
   propagation, a backward propagation and the update of the net params
   given the gradients (update of the net.params[k][j].data) :
solver.step(1)

   to run the full id119, that is the max_iter steps :
solver.solve()

input data, train and test set

   in order to learn a model, you usually set a training set and a test
   set.

   the different input layer can be :
     *    data    : for data saved in a lmdb database, such as before
     *    imagedata    : for data in a txt file listing all the files
  layer {
    name: "data"
    type: "imagedata"
    top: "data"
    top: "label"
    transform_param {
      mirror: false
      crop_size: 227
      mean_file: "data/ilsvrc12/id163_mean.binaryproto"
    }
    image_data_param {
      source: "examples/_temp/file_list.txt"
      batch_size: 50
      new_height: 256
      new_width: 256
    }
  }

     *    hdf5data    for data saved in hdf5 files
  layer {
    name: "data"
    type: "hdf5data"
    top: "data"
    top: "label"
    hdf5_data_param {
      source: "examples/hdf5_classification/data/train.txt"
      batch_size: 10
    }
  }

compute accuracy of the model on the test data

   once solved,
accuracy = 0
batch_size = solver.test_nets[0].blobs['data'].num
test_iters = int(len(xt) / batch_size)
for i in range(test_iters):
    solver.test_nets[0].forward()
    accuracy += solver.test_nets[0].blobs['accuracy'].data
accuracy /= test_iters

print("accuracy: {:.3f}".format(accuracy))

   the higher the accuracy, the better !

parameter sharing

   [11]parameter sharing between siamese networks

recurrent neural nets in caffe

   have a look at my [12]tutorial about recurrent neural nets in caffe.

spatial transformer layers

   [13]my tutorial about improving classification with spatial transformer
   layers

caffe in python

define a model in python

   it is also possible to define the net model directly in python, and
   save it to a prototxt files. here are the commands :
from caffe import layers as l
from caffe import params as p

def lenet(lmdb, batch_size):
    # our version of lenet: a series of linear and simple nonlinear transformati
ons
    n = caffe.netspec()
    n.data, n.label = l.data(batch_size=batch_size, backend=p.data.lmdb, source=
lmdb,
                             transform_param=dict(scale=1./255), ntop=2)
    n.conv1 = l.convolution(n.data, kernel_size=5, num_output=20, weight_filler=
dict(type='xavier'))
    n.pool1 = l.pooling(n.conv1, kernel_size=2, stride=2, pool=p.pooling.max)
    n.conv2 = l.convolution(n.pool1, kernel_size=5, num_output=50, weight_filler
=dict(type='xavier'))
    n.pool2 = l.pooling(n.conv2, kernel_size=2, stride=2, pool=p.pooling.max)
    n.ip1 = l.innerproduct(n.pool2, num_output=500, weight_filler=dict(type='xav
ier'))
    n.relu1 = l.relu(n.ip1, in_place=true)
    n.ip2 = l.innerproduct(n.relu1, num_output=10, weight_filler=dict(type='xavi
er'))
    n.loss = l.softmaxwithloss(n.ip2, n.label)
    return n.to_proto()

with open('examples/mnist/lenet_auto_train.prototxt', 'w') as f:
    f.write(str(lenet('examples/mnist/mnist_train_lmdb', 64)))

with open('examples/mnist/lenet_auto_test.prototxt', 'w') as f:
    f.write(str(lenet('examples/mnist/mnist_test_lmdb', 100)))


   will produce the prototxt file :
layer {
  name: "data"
  type: "data"
  top: "data"
  top: "label"
  transform_param {
    scale: 0.00392156862745
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: lmdb
  }
}
layer {
  name: "conv1"
  type: "convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 20
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool1"
  type: "pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: max
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 50
    kernel_size: 5
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "pool2"
  type: "pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: max
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "innerproduct"
  bottom: "pool2"
  top: "ip1"
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "relu"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "innerproduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "loss"
  type: "softmaxwithloss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}

create your custom python layer

   let   s create a layer to add a value.

   add a custom python layer to your conv.prototxt file :
layer {
  name: 'mypythonlayer'
  type: 'python'
  top: 'output'
  bottom: 'conv'
  python_param {
    module: 'mypythonlayer'
    layer: 'mylayer'
    param_str: "'num': 21"
  }
}

   and create a mypythonlayer.py file that has to to be in the current
   directory or in the pythonpath :
import caffe
import numpy as np
import yaml

class mylayer(caffe.layer):

    def setup(self, bottom, top):
        self.num = yaml.load(self.param_str)["num"]
        print "parameter num : ", self.num

    def reshape(self, bottom, top):
        pass

    def forward(self, bottom, top):
        top[0].reshape(*bottom[0].shape)
        top[0].data[...] = bottom[0].data + self.num

    def backward(self, top, propagate_down, bottom):
        pass

   this layer will simply add a value
net = caffe.net('conv.prototxt',caffe.test)
im = np.array(image.open('cat_gray.jpg'))
im_input = im[np.newaxis, np.newaxis, :, :]
net.blobs['data'].reshape(*im_input.shape)
net.blobs['data'].data[...] = im_input
net.forward()

caffe in c++

the blob

   the blob ([14]blob.hpp and [15]blob.cpp) is a wrapper to manage memory
   independently of cpu/gpu choice, using [16]syncedmemory class, and has
   a few functions like arrays in python, both for the data and the
   computed gradient (diff) arrays contained in the blob.

   to initiate a blob :
blob(const vector<int>& shape)

   methods on the blob :
     * shape() and shape_string() to get the shape, or shape(i) to get the
       size of the i-th dimension, or shapeequals() to compare shape
       equality
     * reshape(const vector<int>& shape) or reshapelike(const blob& other)
       another blob
     * count() the number of elements (shape(0)*shape(1)*...)
     * offset() to get the c++ index in the array
     * copyfrom() to copy the blob
     * data_at() and diff_at()
     * asum_data() and asum_diff() their l1 norm
     * sumsq_data() and sumsq_diff() their l1 norm
     * scale_data() and scale_diff() to multiply the data by a factor
     * update() to update data array given diff array

   to access the data in the blob, from the cpu code :
const dtype* cpu_data();
dtype* mutable_cpu_data();
const dtype* cpu_diff();
dtype* mutable_cpu_diff();

   and from the gpu code :
const dtype* gpu_data();
dtype* mutable_gpu_data();
const dtype* gpu_diff();
dtype* mutable_gpu_diff();

   data transfer between gpu and cpu will be dealt automatically.

   caffe provides abstraction methods to deal with data :
     * caffe_set() and caffe_gpu_set() to initialize the data with a value
     * caffe_add_scalar() and caffe_gpu_add_scalar() to add a scalar to
       data
     * caffe_axpy() and caffe_gpu_axpy() for
     * caffe_scal() and caffe_gpu_scal() for
     * caffe_cpu_sign() and caffe_gpu_sign() for
     * caffe_cpu_axpby() and caffe_cpu_axpby for
     * caffe_copy() to deep copy
     * caffe_cpu_gemm() and caffe_gpu_gemm() for id127
     * caffe_gpu_atomic_add() when you need to update a value in an atomic
       way (such as requests in acid databases but for gpu threads in this
       case)

       and so on.

the layer

   a layer, such as the [17]softmaxwithloss layer, will need a few
   functions working with arguments top blobs and bottom blobs :
     * forward_cpu or forward_gpu
     * backward_cpu or backward_gpu
     * reshape
     * optionaly : layersetup, to set non-standard fields

   the [18]layer_factory is a set of helper functions to get the right
   layer implementation according to the engine (caffe or cudnn).

   png
   [ins: :ins]

christopher bourez's blog

     * christopher bourez's blog
     * [19]christopher.bourezatgmail.com

     * [20]christopher5106
     * [21]christopher5106

   disrupting sasu. christopher bourez

references

   visible links
   1. https://plus.google.com/115479606248104388213
   2. http://christopher5106.github.io/
   3. https://github.com/christopher5106/fastannotationtool
   4. http://caffe.berkeleyvision.org/
   5. http://christopher5106.github.io/big/data/2015/07/16/deep-learning-install-caffe-cudnn-cuda-for-digits-python-on-ubuntu-14-04.html
   6. http://christopher5106.github.io/big/data/2015/07/16/deep-learning-install-caffe-cudnn-cuda-for-digits-python-on-mac-osx.html
   7. http://caffe.berkeleyvision.org/tutorial/layers.html
   8. https://github.com/bvlc/caffe/wiki/model-zoo
   9. http://nbviewer.ipython.org/github/bvlc/caffe/blob/master/examples/net_surgery.ipynb
  10. http://caffe.berkeleyvision.org/tutorial/solver.html
  11. http://caffe.berkeleyvision.org/gathered/examples/siamese.html
  12. http://christopher5106.github.io/deep/learning/2016/06/07/recurrent-neural-net-with-caffe.html
  13. http://christopher5106.github.io/big/data/2016/04/18/spatial-transformer-layers-caffe-tensorflow.html
  14. https://github.com/bvlc/caffe/blob/master/include/caffe/blob.hpp
  15. https://github.com/bvlc/caffe/blob/master/src/caffe/blob.cpp
  16. https://github.com/bvlc/caffe/blob/master/src/caffe/syncedmem.cpp
  17. https://github.com/bvlc/caffe/blob/master/src/caffe/layers/softmax_loss_layer.cpp
  18. https://github.com/bvlc/caffe/blob/master/src/caffe/layer_factory.cpp
  19. mailto:christopher.bourezatgmail.com
  20. https://github.com/christopher5106
  21. https://twitter.com/christopher5106

   hidden links:
  23. http://christopher5106.github.io/deep/learning/2015/09/04/deep-learning-tutorial-on-caffe-technology.html
  24. http://christopher5106.github.io/big/data/2016/04/18/spatial-transformer-layers-caffe-tensorflow.html
