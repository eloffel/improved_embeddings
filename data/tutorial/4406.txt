   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

deepmind   s relational reasoning networks         demystified

   [14]go to the profile of harshvardhan gupta
   [15]harshvardhan gupta (button) blockedunblock (button) followfollowing
   jul 3, 2017
   [1*4cwm6vrbh8sgld5ii7zaha.jpeg]

     this article won the [16]kdnuggets silver award, and was also the
     [17]most viral post of august 2017

   every time deepmind publishes a new paper, there is frenzied media
   coverage around it. often you will read phrases that are often
   misleading. for example, its new paper on relational reasoning networks
   has [18]futurism reporting it like

     deepmind develops a neural network that can make sense of objects
     around it.

   this is not only misleading, but it also makes the everyday non phd
   person intimidated. in this post i will go through the paper in an
   attempt to explain this new architecture in simple terms.

   you can find the original paper [19]here.

   this article assumes some basic knowledge about neural networks.

how this article is structured

   i will follow the paper   s structure as much as possible. i will add my
   own bits to simply the material.

what is relational reasoning?

   in its simplest form, relational reasoning is learning to understand
   relations between different objects(ideas). this is considered an
   essential characteristic of intelligence. the authors have included a
   helpful infographic to explain what it is
   [1*ohiimzo5xcw0mny-ngeytw.png]
   figure1.0 the model has to look at objects of different
   shape/size/color, and be able to answer questions that are related
   between multiple such objects.

relational networks

   the authors have presented a neural network that is made to inherently
   capture relations(e.g. convolutional neural networks are made to
   capture properties of images). they presented an architecture that is
   defined like so :
   [1*zlhudydaqjvxgpp46mknla.png]
   equation1.0 definition of relational networks

explained

   the relational network for o (o is the set of objects you want to learn
   relations of) is a function f  .

   g   is another function that takes two objects :oi , and oj. the output
   of g   is the    relation    that we are concerned about.

      i,j means , calculate g   for all possible pairs of objects, and then
   sum them up.

neural networks and functions

   it is easy to forget this when learning about neural networks,
   backprop ,etc. but a neural network is in fact a single mathematical
   function! therefore, the function that i described in equation 1.0 is a
   neural network!. more precisely , there are two neural networks:
    1. g  , which calculates relations between a pair of objects
    2. f  , which takes in the sum of all g  , and calculates the final
       output of the model

   both g   , and f   are multi layer id88s in the simplest case.

relational neural networks are flexible

   the authors present relational neural network as a module. it can
   accept encoded objects and learn relations from them, but more
   importantly, they can be plugged into convolutional neural networks ,
   and long short term memory networks (lstm).

   the convolutional network can be used to learn the objects using
   images. this makes it far more useful for applications because
   reasoning on an image is more useful than reasoning on an array of user
   defined objects.

   the lstms along with id27s can be used to understand the
   meaning of the query that the model has been asked. this is again ,
   more useful because the model can now accept an english sentence
   instead of encoded arrays.

   the authors have presented a way to combine relational networks,
   convolutional networks , and lstms to construct an end to end neural
   network that can learn relations between objects.
   [1*fzkj-lid166cgkptowfbwuna.png]
   figure 2.0 an end to end relational reasoning neural network.

figure 2.0 explanation

   the image is passed through a standard convolutional neural
   network(id98), which can extract features of that image in k filters.
   the    object    for the relational network is a vector of features of each
   point in the grid. e.g. one    object    is the yellow vector.

   the question is passed through an lstm , which produces a feature
   vector of that question. this is roughly the    idea    of that question.

   this modifies the original equation 1.0 slightly. it adds another term
   which makes it
   [1*r0loc7tarhvhq9s7upt_fw.png]
   equation1.0 relational network conditioned using lstm

   notice the extra q in equation 1.0. that q is the final state of the
   lstm. the relations are now conditioned using q.

   after that, the    object    from the id98 and the vector from the lstm are
   used to train the relational network. each object pair is taken, along
   with the question vector from the lstm, and those are used as inputs
   for g  (which is a neural network).

   the outputs of g   are then summed up , and used as inputs to f  (which
   is another neural network). f   is then optimsed on the answer to the
   question.

benchmarks

   the authors demonstrate the effectiveness of this model on several
   datasets. i will go through one of them (and in my opinion the most
   notable)         clevr dataset.

   the clevr dataset consists of images of objects of different
   shapes,sizes and color. the model is asked questions about these images
   like:

     is the cube the same material as the cylinder?

   [1*uik1obh10jbtrmnqa9t0vq.png]
   figure 3.0 the types of objects(top),and the positioning scheme
   (centre&bottom)

   the authors point out that other systems are far behind their own model
   in terms of accuracy. this is because relational networks are designed
   to capture relations.

   their model achieves an unprecedented 96% + accuracy, as compared to a
   mere 75% (using stacked id12)
   [1*s7y3r_1kv-njftzd-lxk9a.png]
   figure3.1 comparison between different architectures on the clevr
   dataset using pixels(i.e. not matrix encoded)

conclusion

   relational networks are extremely adept at learning relations. they do
   so in a data efficient manner. they are also flexible and can be used
   as a drop in solution when using id98   s, lstms, or both.

   this post was about debunking the    ai has taken over    hype caused by
   very large publications, and giving some perspective on what the
   current state of the art is.

p.s.

   if you notice any errors, or would like any modifications, please let
   me know through responses. your suggestions are welcome.

   if you liked the article, please recommend it to others by tapping the
       button.
   [20][1*0hqoaabq7xgpt-oyngiubg.png]
   [21][1*vgw1jka6hgnvwztsfmlnpg.png]
   [22][1*gkbpq1ruui0fvk2um_i4tq.png]

     [23]hacker noon is how hackers start their afternoons. we   re a part
     of the [24]@ami family. we are now [25]accepting submissions and
     happy to [26]discuss advertising & sponsorship opportunities.

     if you enjoyed this story, we recommend reading our [27]latest tech
     stories and [28]trending tech stories. until next time, don   t take
     the realities of the world for granted!

   [1*35tcjopcvq6lbb3i6wegqw.jpeg]

     * [29]machine learning
     * [30]artificial intelligence
     * [31]deep learning
     * [32]data science

   (button)
   (button)
   (button) 455 claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [33]go to the profile of harshvardhan gupta

[34]harshvardhan gupta

   stupid and hungry

     (button) follow
   [35]hacker noon

[36]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 455
     * (button)
     *
     *

   [37]hacker noon
   never miss a story from hacker noon, when you sign up for medium.
   [38]learn more
   never miss a story from hacker noon
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b593e408b643
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_lnbiu7gxwrhu---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@harshsayshi?source=post_header_lockup
  15. https://hackernoon.com/@harshsayshi
  16. http://www.kdnuggets.com/2017/08/deepmind-relational-reasoning-networksdemystified.html
  17. http://www.kdnuggets.com/2017/09/top-stories-2017-aug.html
  18. https://futurism.com/deepmind-develops-a-neural-network-that-can-make-sense-of-objects-around-it/
  19. https://arxiv.org/pdf/1706.01427.pdf
  20. http://bit.ly/hackernoonfb
  21. https://goo.gl/k7xybx
  22. https://goo.gl/4ofytp
  23. http://bit.ly/hackernoon
  24. http://bit.ly/atamiatami
  25. http://bit.ly/hackernoonsubmission
  26. mailto:partners@amipublications.com
  27. http://bit.ly/hackernoonlatestt
  28. https://hackernoon.com/trending
  29. https://hackernoon.com/tagged/machine-learning?source=post
  30. https://hackernoon.com/tagged/artificial-intelligence?source=post
  31. https://hackernoon.com/tagged/deep-learning?source=post
  32. https://hackernoon.com/tagged/data-science?source=post
  33. https://hackernoon.com/@harshsayshi?source=footer_card
  34. https://hackernoon.com/@harshsayshi
  35. https://hackernoon.com/?source=footer_card
  36. https://hackernoon.com/?source=footer_card
  37. https://hackernoon.com/
  38. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  40. https://medium.com/p/b593e408b643/share/twitter
  41. https://medium.com/p/b593e408b643/share/facebook
  42. https://medium.com/p/b593e408b643/share/twitter
  43. https://medium.com/p/b593e408b643/share/facebook
