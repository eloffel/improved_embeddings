   #[1]rss feed for the berkeley artificial intelligence research blog

   [2][bair_logo.png]

   [3]subscribe [4]about [5]archive [6]bair

captioning novel objects in images

   subhashini venugopalan and lisa anne hendricks    aug 8, 2017

   given an image, humans can easily infer the salient entities in it, and
   describe the scene effectively, such as, where objects are located (in
   a forest or in a kitchen?), what attributes an object has (brown or
   white?), and, importantly, how objects interact with other objects in a
   scene (running in a field, or being held by a person etc.). the task of
   visual description aims to develop visual systems that generate
   contextual descriptions about objects in images. visual description is
   challenging because it requires recognizing not only objects (bear),
   but other visual elements, such as actions (standing) and attributes
   (brown), and constructing a fluent sentence describing how objects,
   actions, and attributes are related in an image (such as the brown bear
   is standing on a rock in the forest).

current state of visual description

   brown bear in forest anteater in forest

   [7]lrcn [donahue et al.    15]: a brown bear standing on top of a lush
   green field.
   [8]ms captionbot [tran et al.    16]: a large brown bear walking through
   a forest.

   [9]lrcn [donahue et al.    15]: a black bear is standing in the grass.
   [10]ms captionbot [tran et al.    16]: a bear that is eating some grass.

   descriptions generated by existing captioners on two images. on the
   left is an image of an object (bear) that is present in training data.
   on the right is an object (anteater) that the model hasn't seen in
   training.

   current visual description or image captioning models work quite well,
   but they can only describe objects seen in existing image captioning
   training datasets, and they require a large number of training examples
   to generate good captions. to learn how to describe an object like
      jackal    or    anteater    in context, most description models require many
   examples of jackal or anteater images with corresponding descriptions.
   however, current visual description datasets, like [11]mscoco, do not
   include descriptions about all objects. in contrast, recent works in
   object recognition through convolutional neural networks (id98s) can
   recognize hundreds of categories of objects. while object recognition
   models can recognize jackals and anteaters, description models cannot
   compose sentences to describe these animals correctly in context. in
   our work, we overcome this problem by building visual description
   systems which can describe new objects without pairs of images and
   sentences about these objects.

the task: describing novel objects

   here we define our task more formally. given a dataset consisting of
   pairs of images and descriptions (paired image-sentence data, e.g.
   [12]mscoco) as well as images with object labels but no descriptions
   (unpaired image data, such as [13]id163) we wish to learn how to
   describe objects unseen in paired image-sentence data. to do this we
   must build a model which can recognize different visual constituents
   (e.g., jackal, brown, standing, and field) and compose these in novel
   ways to form a coherent description. below we describe the core
   components of our description model.

   the novel visual description task

   we aim to describe diverse objects which do not have training images
   with captions.

using external sources of data

   in order to generate captions about diverse categories of objects
   outside the image-caption training data, we take advantage of external
   data sources. specifically, we use id163 images with object labels
   as the unpaired image data source and sentences from unannotated text
   corpora such as wikipedia as our text data source. these are used to
   train our visual recognition id98 and language model respectively.

   train effectively on external resources
   train effectively on external resources

capture semantic similarity

   we want to be able to describe unseen objects (e.g. from id163) that
   are similar to objects that have been seen in the paired image-sentence
   training data. we use dense id27s to achieve this. word
   embeddings are dense high dimensional representations of words where
   words with similar meaning are closer in the embedding space.

   in our previous work, called    deep compositional captioning (dcc)    [1]
   we first train a caption model on mscoco paired image caption dataset.
   then to describe novel objects, for each novel object (such as an
   okapi) we use id27s to identify an object that   s most similar
   amongst the objects in the mscoco dataset (in this case zebra). we then
   transfer (copy) the parameters learned by the model from the seen
   object to the unseen object (i.e. copy weights in the network
   corresponding to zebra to those corresponding to okapi).

novel object captioning

   while the dcc model is able to describe several unseen object
   categories, copying parameters from one object to another can create
   sentences with grammatical artifacts. e.g. for the object    racket    the
   model copies weights from    tennis   , which results in sentences such as
      a man playing racket on court   . in our more recent work [2], we
   incorporate the embeddings directly within our language model.
   specifically, we use [14]glove embeddings in the input and output of
   our language model. this implicitly enables the model to capture
   semantic similarity when describing unseen objects. this enables our
   model to generate sentences such as    a tennis player swinging a racket
   at a ball   . additionally, incorporating the embeddings directly within
   the network makes our model end-to-end trainable.

   dense id27s to capture image similarity
   incorporate dense id27s in the language model to capture
   semantic similarity.

caption model and forgetting in neural networks.

   we combine the outputs of the visual network and language model to a
   caption model. this model is similar to existing caption models which
   are also pre-trained on id163. however, we observed that although
   the model is pre-trained on id163, when the model is trained / tuned
   on the coco image-caption dataset it tends to forget what it has seen
   before. the problem of forgetting in neural networks has also been
   observed by [15]researchers at montreal as well as [16]google deepmind
   amongst others. in our work, we resolve this problem of forgetting
   using a joint training strategy.

   joint training to overcome forgetting
   share parameters and train jointly on different data/tasks to overcome
   "forgetting"

   specifically, our network has three components: a visual recognition
   network, a caption model, and a language model. all three components
   share parameters and are jointly trained. during training, each batch
   of inputs contains some images with labels, a different set of images
   and captions, and some plain sentences. these three inputs train the
   different components of the network. since the parameters are shared
   between the three components, the network is jointly trained to
   recognize objects in images, caption images and generate sentences.
   this joint training helps the network overcome the problem of
   forgetting, and enables the model to generate descriptions for many
   novel object categories.

what   s next?

   one of the most common errors in our model comes from not recognizing
   objects, and one way to mitigate this is to use better visual features.
   another common error comes from generating sentences which are not
   fluent (a cat and a cat on a bed) or may not appeal to    common sense   
   (e.g.    a woman is playing gymnastics    is not particularly correct since
   one doesn   t    play    gymnastics). it would be interesting to develop
   solutions that can overcome these issues.

   while in this work, we proposes joint training as a strategy to
   overcome the problem of forgetting, it might not always be possible to
   train on lots of different tasks and datasets. a different way to
   approach the problem would be to build a model that can learn to
   compose descriptions based on visual information and object labels.
   such a model should also be able to integrate objects on the fly i.e.
   currently we pre-train our model on a select set of objects, we should
   also think about how we can incrementally train our model on new data
   about some new concepts. solving some of these problems can help
   develop better and more robust visual description models.

   [[17]links to more examples]

   [[18]links to trained models and code]

examples

   examples
     __________________________________________________________________

   this blog post is based on the following research papers:

   [1] l. a. hendricks, s. venugopalan, m. rohrbach, r. mooney, k. saenko,
   and t. darrell. deep compositional captioning: describing novel object
   categories without paired training data. in cvpr, 2016.

   [2] s. venugopalan, l. a. hendricks, m. rohrbach, r. mooney, k. saenko,
   and t. darrell. captioning images with diverse objects. in cvpr, 2017.
   subscribe to our [19]rss feed.
   spread the word:

comments

   please enable javascript to view the [20]comments powered by disqus.

references

   visible links
   1. https://bair.berkeley.edu/blog/feed.xml
   2. https://bair.berkeley.edu/blog/
   3. https://bair.berkeley.edu/blog/subscribe/
   4. https://bair.berkeley.edu/blog/about/
   5. https://bair.berkeley.edu/blog/archive/
   6. http://bair.berkeley.edu/
   7. http://jeffdonahue.com/lrcn/
   8. http://captionbot.ai/
   9. http://jeffdonahue.com/lrcn/
  10. http://captionbot.ai/
  11. https://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/mscoco.org
  12. https://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/mscoco.org
  13. http://www.image-net.org/
  14. https://nlp.stanford.edu/projects/glove/
  15. https://arxiv.org/abs/1312.6211
  16. https://arxiv.org/abs/1612.00796
  17. https://vsubhashini.github.io/noc_examples.html
  18. http://vsubhashini.github.io/noc.html#code
  19. https://bair.berkeley.edu/blog/feed.xml
  20. http://disqus.com/?ref_noscript

   hidden links:
  22. https://facebook.com/sharer.php?u=http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/
  23. https://twitter.com/intent/tweet?text=captioning%20novel%20objects%20in%20images&url=http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/
  24. https://plus.google.com/share?url=http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/
  25. http://www.linkedin.com/sharearticle?url=http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/&title=captioning%20novel%20objects%20in%20images
  26. http://reddit.com/submit?url=http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/&title=captioning%20novel%20objects%20in%20images
  27. https://news.ycombinator.com/submitlink?u=http://bair.berkeley.edu/blog/2017/08/08/novel-object-captioning/&t=captioning%20novel%20objects%20in%20images
