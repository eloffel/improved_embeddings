   #[1]tombone's id161 blog - atom [2]tombone's id161
   blog - rss [3]publisher

tombone's id161 blog

   deep learning, id161, and the algorithms that are shaping the
   future of artificial intelligence.

wednesday, may 16, 2018

[4]deepfakes: ai-powered deception machines

   driven by id161 and deep learning techniques, a new wave of
   imaging attacks has recently emerged which allows anyone to easily
   create highly realistic "fake" videos. these false videos are known as
   deep fakes. while highly entertaining at times, deepfakes can be used
   to perturb society and some would argue that the pre-shock has already
   begun. a rogue deepfake which goes viral can spread misinformation
   across the internet like wildfire.

     "the ability to effortlessly create visually plausible editing of
     faces in videos has the potential to severely undermine trust in any
     form of digital communication. "

     --r  ssler et al. faceforensics [3]

   because deepfakes contain a unique combination of realism and novelty,
   they are more difficult to detect on social networks as compared to
   traditional "bad" content like pornography and copyrighted movies.
   video hashing might work for finding duplicates or copyright-infringing
   content, but not good enough for deepfakes. to fight face-manipulating
   deepfake ai, one needs an even stronger ai.
   as today's deepfakes are based on deep learning, and deep learning
   tools like tensorflow and pytorch are accessible to anybody with a
   modern gpu, such face manipulation tools are particularly disruptive.
   the democratization of artificial intelligence has brought us near
   infinite use-cases. from the deepdream phenomenon of 2015 to the deep
   style transfer art apps of 2016, 2018 is the year of the deepfake.
   today's id161 technology allows a hobbyist to create a deep
   fake video of just about any person they want performing any action
   they want, in a matter of hours, using commodity computer hardware.

                                              [5][mind.png]
   fig 1. deepfakes generate "false impressions" which are attacks on the
                                 human mind.

   what is a deep fake?
   a deep fake is a video generated from a modern id161
   puppeteering face-swap algorithm which can be used to generate a video
   of target person x performing target action a, usually given a video of
   another person y performing action a. the underlying system learns two
   face models, one of target person x, and of for person y, the person in
   the original video. it then learns a mapping between the two faces,
   which can be used to create the resulting "fake" video. techniques for
   facial reenactment have been pioneered by movie studios for driving
   character animations from real actors' faces, but these techniques are
   now emerging as deep learning-based software packages, letting the deep
   convolutional neural networks do most of the work during model
   training.
   consider the following collage of faces. can you guess which ones are
   real and which ones are deepfakes?
   [6][faceforensics.png]
   fig 2. can you tell which faces are real and which ones are fake?
   figure from face forensics[3]
   it is not so easy to tell which image is modified and which one is
   unadulterated. and if you do a little bit of searching for deepfakes
   (warning, unless you are careful, you will encounter lots of
   pornographic content) you notice that the faces in those videos look
   very realistic.
   how are deep fakes made?
   while there are conceptually many different ways to make deep fakes,
   today we'll focus on two key underlying techniques: face detection from
   videos, and deep learning for creating frame alignments between source
   face x and target face y.
   a lot of this research started with the face2face work [1] presented at
   cvpr 2016. this paper was a modernization of the group's earlier
   siggraph paper and focused a lot more on the id161 details.
   at this time the tools were good enough to create siggraph-quality
   videos, but it took a lot of work to put together a facial reenactment
   rig. in addition, the underlying algorithms did not use any deep
   learning, so a lot of domain-knowledge (i.e., face modeling expertise)
   went into making these algorithms work robustly. the tum/stanford guys
   filed their real-time facial reenactment patent in 2016 [4], and have
   more recently worked on faceforensics[3] to detect such manipulated
   imagery.

   iframe: [7]https://www.youtube.com/embed/ohmajjtcpnk

   fig3. face2face technique from 2016. it is 2018 now, so just imagine
   how much better this works now!
   in addition to the face2face guys (who have now a handful of similarly
   themed papers), it is interesting to note that a lot of key early ideas
   in face puppeteering were pioneered by [8]ira kemelmacher-shlizerman
   who is now a id161 and graphics assistant professor at
   university of washington. she worked on early face puppeteering
   technology for the 2010 paper being john malkovich, continued with the
   photobios work, and later founded dreambit (based on a siggraph 2016
   paper), which was acquired by facebook. :-)
   [9][ira_early_deep_fake.png]
   fig 4. ira's early work on face swapping in 2010. see the being john
   malkovich paper[2].
   take a look at ira's dreambit video, which shows some high-quality
   "entertainment" value out of rapidly produced non-malicious deepfakes!

   iframe: [10]https://www.youtube.com/embed/millfk1rwhk

   fig 5. ira's dreambit system. lets her imagine herself in different
   eras, with different hairstyles, etc.
   the origin of ira's dreambit system is the transfiguring portraits
   siggraph 2016 paper[6]. what's important to note is that this is 2016
   and we're starting to see some use of deep learning. the transfiguring
   portraits work used a big mix of features, using some id98 features
   computed from early caffe networks. it is not an entirely easy-to-use
   system at this point, but good enough to make siggraph videos, take a
   one minute to generate other cool outputs, and definitely cool enough
   for facebook to acquire.
   [11][transfiguring_portraits_deepfake.png]
   fig 6. transfiguring portraits. the system used lots of features, but
   deep learning-based id98 features are starting to show up.
   fighting against deepfakes
   there are now published algorithms which try to battle deepfakes by
   determining if faces/videos are fake or not. faceforensics[3]
   introduces a large deepfake dataset based on their earlier face2face
   work. this dataset contains both real and "fake" face2face output
   videos. more importantly, the new dataset is big enough to train a deep
   learning system to determine if an image is counterfeit. in addition,
   they are able to both 1.) determine which pixels have likely been
   manipulated, and 2.) perform a deep cleanup stage to make even better
   deepfakes.
   [12][deepmask_deepfake_detection.png]
   fig 7. the "fakeness" masks in faceforensics[3] are based on
   xceptionnet
   another fake detection approach, this time from a berkeley ai research
   group called image splice detection, focuses on detecting where an
   image was spliced to create a fake composite image. this allows them to
   determine which part of the image was likely "photoshopped" and the
   technique is not specific to faces. and because this is a 2018 paper,
   it should not be a surprise that this kind of work is all based on deep
   learning techniques.
   [13][efros_fake_news.png]
   fig 8. fighting fake news: image splice detection[5]. response maps are
   aggregated to determine the combined id203 mask.[5]
   from the fighting fake news paper,

     "as new advances in id161 and image-editing emerge, there
     is an increasingly urgent need for effective visual forensics
     methods. we see our approach, which successfully detects
     manipulations without seeing examples of manipulated images, as
     being an initial step toward building general-purpose forensics
     tools."

   concluding remarks
   the early deepfake tools were pioneered in the early 2010s and were
   producing siggraph-quality results by 2015. it was only a matter of
   years until deepfake generators became publicly available. 2018's
   deepfake generators, being written on top of open-source deep learning
   libraries, are much easier to use than the researchy systems from only
   a few years back. today, just about any hobbyist with minimal computer
   programming knowledge and a gpu can build their own deepfakes.
   just as deep fakes are getting better, id3
   are showing more promise for photorealistic image generation. it is
   likely that we will soon see lots of exciting new work on both the
   generative side (deep fake generation) and the discriminative side
   (deep fake detection and image forensics) which incorporate more and
   more ideas from the machine learning community.
   references
   [1] justus thies, michael zollh  fer, marc stamminger, christian
   theobalt, and matthias nie  ner. "[14]face2face: real-time face capture
   and reenactment of rgb videos." in id161 and pattern
   recognition (cvpr), 2016 ieee conference on, pp. 2387-2395. ieee, 2016.
   [2] ira kemelmacher-shlizerman, aditya sankar, eli shechtman, and
   steven m. seitz. "[15]being john malkovich." in european conference on
   id161, pp. 341-353. springer, 2010.
   [3] andreas r  ssler, davide cozzolino, luisa verdoliva, christian
   riess, justus thies, and matthias nie  ner. "[16]faceforensics: a
   large-scale video dataset for forgery detection in human faces." arxiv
   preprint arxiv:1803.09179, 2018.
   [4] christian theobalt, michael zollh  fer, marc stamminger, justus
   thies, matthias nie  ner. real-time expression transfer for facial
   reenactment invention. 2018/3/8. application number 15256710
   [5] minyoung huh, andrew liu, andrew owens, alexei a. efros,
   "[17]fighting fake news: image splice detection via learned
   self-consistency." arxiv preprint arxiv:1805.04096, 2018
   [6] ira kemelmacher-shlizerman, "[18]transfiguring portraits." acm
   transactions on graphics (tog), 35(4), p.94. 2016
   posted by [19]unknown at [20]wednesday, may 16, 2018 [21]no comments:
   [22]email this[23]blogthis![24]share to twitter[25]share to
   facebook[26]share to pinterest
   labels: [27]alyosha efros, [28]cvpr, [29]deepfake, [30]descartes,
   [31]face detection, [32]face transfer, [33]face2face, [34]fake news,
   [35]gans, [36]ira kemelmacher-shlizerman, [37]justus thies,
   [38]matthias niessner, [39]realism, [40]siggraph, [41]snapchat,
   [42]truth, [43]visual forgery

friday, december 16, 2016

[44]nuts and bolts of building deep learning applications: ng @ nips2016

   you might go to a cutting-edge machine learning research conference
   like nips hoping to find some mathematical insight that will help you
   take your deep learning system's performance to the next level.
   unfortunately, as andrew ng reiterated to a live crowd of 1,000+
   attendees this past monday, there is no secret ai equation that will
   let you escape your machine learning woes. all you need is some rigor,
   and much of what ng covered is his remarkable nips 2016 presentation
   titled "the nuts and bolts of building applications using deep
   learning" is not rocket science. today we'll dissect the lecture and
   ng's key takeaways. let's begin.
   [45][nuts_and_bolts_andrew_ng.png]
   figure 1. andrew ng delivers a powerful message at nips 2016.
   andrew ng and the lecture
   andrew ng's lecture at nips 2016 in barcelona was phenomenal -- truly
   one of the best presentations i have seen in a long time. in a
   juxtaposition of two influential presentation styles, the ceo-style and
   the professor-style, andrew ng mesmerized the audience for two hours.
   andrew ng's wisdom from managing large scale ai projects at baidu,
   google, and stanford really shows. in his talk, ng spoke to the
   audience and discussed one of they key challenges facing most of the
   nips audience -- how do you make your deep learning systems better?
   rather than showing off new research findings from his cutting-edge
   projects, andrew ng presented a simple recipe for analyzing and
   debugging today's large scale systems. with no need for equations, a
   handful of diagrams, and several checklists, andrew ng delivered a
   two-whiteboards-in-front-of-a-video-camera lecture, something you would
   expect at a group research meeting. however, ng made sure to not delve
   into research-y areas, likely to make your brain fire on all cylinders,
   but making you and your company very little dollars in the foreseeable
   future.
   money-making deep learning vs idea-generating deep learning
   andrew ng highlighted the fact that while nips is a research
   conference, many of the newly generated ideas are simply ideas, not yet
   battle-tested vehicles for converting mathematical acumen into dollars.
   the bread and butter of money-making deep learning is supervised
   learning with recurrent neural networks such as lstms in second place.
   research areas such as id3 (gans), deep
   id23 (deep rl), and just about anything branding
   itself as unsupervised learning, are simply research, with a capital r.
   these ideas are likely to influence the next 10 years of deep learning
   research, so it is wise to focus on publishing and tinkering if you
   really love such open-ended research endeavours. applied deep learning
   research is much more about taming your problem (understanding the
   inputs and outputs), casting the problem as a supervised learning
   problem, and hammering it with ample data and ample experiments.

     "it takes surprisingly long time to grok bias and variance deeply,
     but people that understand bias and variance deeply are often able
     to drive very rapid progress."

     --andrew ng

   the 5-step method of building better systems
   most issues in applied deep learning come from a training-data /
   testing-data mismatch. in some scenarios this issue just doesn't come
   up, but you'd be surprised how often applied machine learning projects
   use training data (which is easy to collect and annotate) that is
   different from the target application. andrew ng's discussion is
   centered around the basic idea of id160. you want a
   classifier with a good ability to fit the data (low bias is good) that
   also generalizes to unseen examples (low variance is good). too often,
   applied machine learning projects running as scale forget this critical
   dichotomy. here are the four numbers you should always report:
     * training set error
     * testing set error
     * dev (aka validation) set error
     * train-dev (aka train-val) set error

   andrew ng suggests following the following recipe:
   [46][nuts-and-bolts-checklist.png]
   figure 2. andrew ng's "applied bias-variance for deep learning
   flowchart"
   for building better deep learning systems.
   take all of your data, split it into 60% for training and 40% for
   testing. use half of the test set for evaluation purposes only, and the
   other half for development (aka validation). now take the training set,
   leave out a little chunk, and call it the training-dev data. this 4-way
   split isn't always necessary, but consider the worse case where you
   start with two separate sets of data, and not just one: a large set of
   training data and a smaller set of test data. you'll still want to
   split the testing into validation and testing, but also consider
   leaving out a small chunk of the training data for the
   training-validation. by reporting the data on the training set vs the
   training-validation set, you measure the "variance."
   [47][bias-variance-andrew-ng.png]
   figure 3. human-level vs training vs training-dev vs dev vs test.
   taken from andrew ng's 2016 talk.
   in addition to these four accuracies, you might want to report the
   human-level accuracy, for a total of 5 quantities to report. the
   difference between human-level and training set performance is the
   bias. the difference between the training set and the training-dev set
   is the variance. the difference between the training-dev and dev sets
   is the train-test mismatch, which is much more common in real-world
   applications that you'd think. and finally, the difference between the
   dev and test sets measures how overfitting.
   nowhere in andrew ng's presentation does he mention how to use
   unsupervised learning, but he does include a brief discussion about
   "synthesis." such synthesis ideas are all about blending pre-existing
   data or using a rendering engine to augment your training set.
   conclusion
   if you want to lose weight, gain muscle, and improve your overall
   physical appearance, there is no magical protein shake and no magical
   bicep-building exercise. the fundamentals such as reduced caloric
   intake, getting adequate sleep, cardiovascular exercise, and core
   strength exercises like squats and bench presses will get you there. in
   this sense, fitness is just like machine learning -- there is no secret
   sauce. i guess that makes andrew ng the arnold schwarzenegger of
   machine learning.
   what you are most likely missing in your life is the rigor of reporting
   a handful of useful numbers such as performance on the 4 main data
   splits (see figure 3). analyzing these numbers will let you know if you
   need more data or better models, and will ultimately let you hone in
   your expertise on the conceptual bottleneck in your system (see figure
   2).
   with a prolific research track record that never ceases to amaze, we
   all know andrew ng as one hell of an applied machine learning
   researcher. but the new andrew ng is not just another data-nerd. his
   personality is bigger than ever -- more confident, more entertaining,
   and his experience with a large number of academic and industrial
   projects makes him much wiser. with enlightening lectures as "the nuts
   and bolts of building applications with deep learning" andrew ng is
   likely to be an individual whose future keynotes you might not want to
   miss.
   appendix
   you can watch a september 27th, 2016 version of the [48]andrew ng nuts
   and bolts of applying deep learning lecture on youtube, which he
   delivered at the deep learning school. if you are working on machine
   learning problems in a startup, then definitely give the video a watch.
   i will update the video link once/if the newer nips 2016 version shows
   up online.
   you can also check out [49]kevin zakka's blog post for ample
   illustrations and writeup corresponding to andrew ng's entire talk.
   posted by [50]unknown at [51]friday, december 16, 2016 [52]no comments:
   [53]email this[54]blogthis![55]share to twitter[56]share to
   facebook[57]share to pinterest
   labels: [58]advice, [59]andrew ng, [60]bias-variance, [61]deep
   learning, [62]google, [63]machine learning, [64]nips 2016,
   [65]research, [66]supervised learning, [67]synthesis

friday, june 17, 2016

[68]making deep networks probabilistic via test-time dropout

   in quantum mechanics, heisenberg's uncertainty principle states that
   there is a fundamental limit to how well one can measure a particle's
   position and momentum. in the context of machine learning systems, a
   similar principle has emerged, but relating interpretability and
   performance. by using a manually wired or shallow machine learning
   model, you'll have no problem understanding the moving pieces, but you
   will seldom be happy with the results. or you can use a black-box deep
   neural network and enjoy the model's exceptional performance. today
   we'll see one simple and effective trick to make our deep black boxes a
   bit more intelligible. the trick allows us to convert neural network
   outputs into probabilities, with no cost to performance, and minimal
   computational overhead.
   [69][interpretable_vs_deep_neural_networks.png]
   interpretability vs performance: deep neural networks perform well on
   most id161 tasks, yet they are notoriously difficult to
   interpret.
   the desire to understand deep neural networks has triggered a flurry of
   research into neural network visualization, but in practice we are
   often forced to treat deep learning systems as black-boxes. (see my
   recent [70]deep learning trends @ iclr 2016 post for an overview of
   recent neural network visualization techniques.) but just because we
   can't grok the inner-workings of our favorite deep models, it doesn't
   mean we can't ask more out of our deep learning systems.

     there exists a simple trick for upgrading black-box neural network
     outputs into id203 distributions.

   the probabilistic approach provides confidences, or "uncertainty"
   measures, alongside predictions and can make almost any deep learning
   systems into a smarter one. for robotic applications or any kind of
   software that must make decisions based on the output of a deep
   learning system, being able to provide meaningful uncertainties is a
   true game-changer.

                       [71][brain_zap_neural_network_dropout.jpg]
      applying dropout to your deep neural network is like occasionally
                             zapping your brain

   the key ingredient is dropout, an anti-overfitting deep learning trick
   handed down from hinton himself (krizhevsky's pioneering 2012 paper).
   dropout sets some of the weights to zero during training, reducing
   feature co-adaptation, thus improving generalization.

     without dropout, it is too easy to make a moderately deep network
     attain 100% accuracy on the training set.

   the accepted knowledge is that an un-regularized network (one without
   dropout) is too good at memorizing the training set. for a great
   introductory machine learning video lecture on dropout, i highly
   recommend you watch hugo larochelle's lecture on dropout for deep
   learning.

            iframe: [72]https://www.youtube.com/embed/uckpdam8cni

   geoff hinton's dropout lecture, also a great introduction, focuses on
   interpreting dropout as an ensemble method. if you're looking for new
   research ideas in the dropout space, a thorough understanding of
   hinton's interpretation is a must.

            iframe: [73]https://www.youtube.com/embed/g3kuvhx9gdy

   but while dropout is typically used at training-time, today we'll
   highlight the keen observation that dropout used at test-time is one of
   the simplest ways to turn raw neural network outputs into id203
   distributions. not only does this probabilistic "free upgrade" often
   improve classification results, it provides a meaningful notion of
   uncertainty, something typically missing in deep learning systems.

     the idea is quite simple: to estimate the predictive mean and
     predictive uncertainty, simply collect the results of stochastic
     forward passes through the model using dropout.

how to use dropout: 2016 edition

    1. start with a moderately sized network
    2. increase your network size with dropout turned off until you
       perfectly fit your data
    3. then, train with dropout turned on
    4. at test-time, turn on dropout and run the network t times to get t
       samples
    5. the mean of the samples is your output and the variance is your
       measure of uncertainty

   remember that drawing more samples will increase computation time
   during testing unless you're clever about re-using partial computations
   in the network. please note that if you're only using dropout near the
   end of your network, you can reuse most of the computations. if you're
   not happy with the uncertainty estimates, consider adding more layers
   of dropout at test-time. since you'll already have a pre-trained
   network, experimenting with test-time dropout layers is easy.

bayesian convolutional neural networks

   to be truly bayesian about a deep network's parameters, we wouldn't
   learn a single set of parameters w, we would infer a distribution over
   weights given the data, p(w|x,y). training is already quite expensive,
   requiring large datasets and expensive gpus.

     bayesian learning algorithms can in theory provide much better
     parameter estimates for convnets and i'm sure some of our friends at
     google are working on this already.

   but today we aren't going to talk about such full bayesian deep
   learning systems, only systems that "upgrade" the model prediction y to
   p(y|x,w). in other words, only the network outputs gain a probabilistic
   interpretation.
   an excellent deep learning id161 system which uses test-time
   dropout comes from a recent university of cambridge technique called
   segnet. the segnet approach introduced an encoder-decoder framework for
   dense semantic segmentation. more recently, segnet includes a bayesian
   extension that uses dropout at test-time for providing uncertainty
   estimates. because the system provides a dense per-pixel labeling, the
   confidences can be visualized as per-pixel heatmaps. segmentation
   system is not performing well? just look at the confidence heatmaps!

                [74][bayesian_segnet_uncertainty_dropout.png]

   bayesian segnet. a fully convolutional neural network architecture
   which provides

   per-pixel class uncertainty estimates using dropout.

   the bayesian segnet authors tested different strategies for dropout
   placement and determined that a handful of dropout layers near the
   encoder-decoder bottleneck is better than simply using dropout near the
   output layer. interestingly, bayesian segnet improves the accuracy over
   vanilla segnet. their confidence maps shown high uncertainty near
   object boundaries, but different test-time dropout schemes could
   provide a more diverse set of uncertainty estimates.
   [75]bayesian segnet: model uncertainty in deep convolutional
   encoder-decoder architectures for scene understanding alex kendall,
   vijay badrinarayanan, roberto cipolla, in arxiv:1511.02680, november
   2015. [[76]project page with videos]
   confidences are quite useful for evaluation purposes, because instead
   of providing a single average result across all pixels in all images,
   we can sort the pixels and/or images by the overall confidence in
   prediction. when evaluation the top 10% most confident pixels, we
   should expect significantly higher performance. for example, the
   bayesian segnet approach achieves 75.4% global accuracy on the sun rgbd
   dataset, and an astonishing 97.6% on most confident 10% of the test-set
   [personal communication with bayesian segnet authors]. this kind of
   sort-by-confidence evaluation was popularized by the pascal voc object
   detection challenge, where precision/recall curves were the norm.
   unfortunately, as the research community moved towards large-scale
   classification, the notion of confidence was pushed aside. until now.

theoretical bayesian deep learning

   deep networks that model uncertainty are truly meaningful machine
   learning systems. it ends up that we don't really have to understand
   how a deep network's neurons process image features to trust the system
   to make decisions. as long as the model provides uncertainty estimates,
   we'll know when the model is struggling. this is particularly important
   when your network is given inputs that are far from the training data.

                     [77][gaussian_process_confidence_values.png]
       the gaussian process: a machine learning approach with built-in
                            uncertainty modeling

   in a recent icml 2016 paper, [78]yarin gal and [79]zoubin
   ghahramani develop a new theoretical framework casting dropout training
   in deep neural networks as approximate bayesian id136 in deep
   gaussian processes. gal's paper gives a complete theoretical treatment
   of the link between gaussian processes and dropout, and develops the
   tools necessary to represent uncertainty in deep learning. they show
   that a neural network with arbitrary depth and non-linearities, with
   dropout applied before every weight layer, is mathematically equivalent
   to an approximation to the probabilistic deep gaussian process. i have
   yet to see researchers use dropout between every layer, so the
   discrepancy between theory and practice suggests that more research is
   necessary.
   [80]dropout as a bayesian approximation: representing model uncertainty
   in deep learning yarin gal, zoubin ghahramani, in icml. june 2016.
   [[81]appendix with relationship to gaussian processes]
   [82]a theoretically grounded application of dropout in recurrent neural
   networks yarin gal, in arxiv:1512.05287. may 2016.
   [83]what my deep model doesn't know. yarin gal. blog post. july 2015
   [84]homoscedastic and heteroscedastic regression with dropout
   uncertainty. yarin gal. blog post. february 2016.

                                        [85][black_box.png]
     test-time dropout is used to provide uncertainty estimates for deep
                              learning systems.

   in conclusion, maybe we can never get both interpretability and
   performance when it comes to deep learning systems. but, we can all
   agree that providing confidences, or uncertainty estimates, alongside
   predictions is always a good idea. dropout, the very single
   id173 trick used to battle overfitting in deep models, shows
   up, yet again. sometimes all you need is to add some random variations
   to your input, and average the results over many trials. dropout lets
   you not only wiggle the network inputs but the entire architecture.
   i do wonder what yann lecun thinks about bayesian convnets... last i
   heard, he was allergic to sampling.
   related posts
   [86]deep learning vs probabilistic id114 vs logic april 2015
   [87]deep learning trends @ iclr 2016 june 2016
   posted by [88]unknown at [89]friday, june 17, 2016 [90]no comments:
   [91]email this[92]blogthis![93]share to twitter[94]share to
   facebook[95]share to pinterest
   labels: [96]arxiv, [97]bayesian, [98]confidence, [99]deep learning,
   [100]dropout, [101]geoff hinton, [102]hugo larochelle, [103]icml,
   [104]papers, [105]segnet, [106]uncertainty, [107]yarin gal

wednesday, june 01, 2016

[108]deep learning trends @ iclr 2016

   started by the youngest members of the deep learning mafia [1],
   namely [109]yann lecun and [110]yoshua bengio, the iclr conference is
   quickly becoming a strong contender for the single most important venue
   in the deep learning space. more intimate than nips and less
   benchmark-driven than cvpr, the world of iclr is arxiv-based and moves
   fast.
   [111][deep_learning_machine_learning_conference_iclr_2016.png]
   today's post is all about iclr 2016. i   ll highlight new strategies for
   building deeper and more powerful neural networks, ideas for
   compressing big networks into smaller ones, as well as techniques for
   building    deep learning calculators.    a host of new artificial
   intelligence problems is being hit hard with the newest wave of deep
   learning techniques, and from a id161 point of view, there's
   no doubt that deep convolutional neural networks are today's "master
   algorithm" for dealing with perceptual data.
   deep powwow in paradise? [112]iclr 2016 was held in puerto rico.
   whether you're working in robotics, augmented reality, or dealing with
   a id161-related problem, the following summary of iclr
   research trends will give you a taste of what's possible on top of
   today's deep learning stack. consider today's blog post a reading group
   conversation-starter.
   part i: iclr vs cvpr
   part ii: iclr 2016 deep learning trends
   part iii: quo vadis deep learning?

part i: iclr vs cvpr

   last month's international conference of learning representations,
   known briefly as iclr 2016, and commonly pronounced as    eye-clear,   
   could more appropriately be called the international conference on deep
   learning. the iclr 2016 conference was held may 2nd-4th 2016 in lovely
   puerto rico. this year was the 4th installment of the conference -- the
   first was in 2013 and it was initially so small that it had to be
   co-located with another conference. because it was started by none
   other than the deep learning mafia, it should be no surprise that just
   about everybody at the conference was studying and/or applying deep
   learning methods. convolutional neural networks (which dominate image
   recognition tasks) were all over the place, with lstms and other
   recurrent neural networks (used to model sequences and build "deep
   learning calculators") in second place. most of my own research
   conference experiences come from cvpr (id161 and pattern
   recognition), and i've been a regular cvpr attendee since 2004.
   compared to iclr, cvpr has a somewhat colder, more-emprical feel. to
   describe the difference between iclr and cvpr, yan lecun, quoting
   [113]raquel urtasun (who got the original saying from [114]sanja
   fidler), put it best on facebook.
   cvpr: what can deep nets do for me?
   iclr: what can i do for deep nets?
   the iclr 2016 conference was my first official powwow that truly felt
   like a close-knit "let's share knowledge" event. 3 days of the main
   conference, plenty of evening networking events, and no workshops. with
   a total attendance of about 500, iclr is about 1/4 the size of cvpr. in
   fact, cvpr 2004 in d.c. was my first conference ever, and cvprs are
   infamous for their packed poster sessions, multiple sessions, and
   enough workshops/tutorials to make cvprs last an entire week. at the
   end of cvpr, you'll have a research hangover and will need a few days
   to recuperate. i prefer the size and length of iclr.
   cvpr and nips, like many other top-tier conferences heavily utilizing
   machine learning techniques, have grown to gargantuan sizes, and paper
   acceptance rates at these mega conferences are close to 20%. it not
   necessarily true that the research papers at iclr were any more
   half-baked than some cvpr papers, but the amount of experimental
   validation for an iclr paper makes it a different kind of beast than
   cvpr. cvpr   s main focus is to produce papers that are
      state-of-the-art    and this essentially means you have to run your
   algorithm on a benchmark and beat last season   s leading technique.
   iclr   s main focus it to highlight new and promising techniques in the
   analysis and design of deep convolutional neural networks,
   initialization schemes for such models, and the training algorithms to
   learn such models from raw data.
   deep learning is learning representations
   yann lecun and yoshua bengio started this conference in 2013 because
   there was a need to a new, small, high-quality venue with an explicit
   focus on deep methods. why is the conference called    learning
   representations?    because the typical deep neural networks that are
   trained in an end-to-end fashion actually learn such intermediate
   representations. traditional shallow methods are based on
   manually-engineered features on top of a trainable classifier, but deep
   methods learn a network of layers which learns those highly-desired
   features as well as the classifier. so what do you get when you blur
   the line between features and classifiers? you get representation
   learning. and this is what deep learning is all about.
   iclr publishing model: arxiv or bust
   at iclr, papers get posted on arxiv directly. and if you had any doubts
   that arxiv is just about the single awesomest thing to hit the research
   publication model since the gutenberg press, let the success of iclr be
   one more data point towards enlightenment. iclr has essentially
   bypassed the old-fashioned publishing model where some third party like
   elsevier says    you can publish with us and we   ll put our logo on your
   papers and then charge regular people $30 for each paper they want to
   read.    sorry elsevier, research doesn   t work that way. most research
   papers aren   t good enough to be worth $30 for a copy. it is the entire
   body of academic research that provides true value, for which a single
   paper just a mere door. you see, elsevier, if you actually gave the
   world an exceptional research paper search engine, together with the
   ability to have 10-20 papers printed on decent quality paper for a
   $30/month subscription, then you would make a killing on researchers
   and i would endorse such a subscription. so iclr, rightfully so, just
   said fuck it, we   ll use arxiv as the method for disseminating our
   ideas. all future research conferences should use arxiv to disseminate
   papers. anybody can download the papers, see when newer versions with
   corrections are posted, and they can print their own physical copies.
   but be warned: deep learning moves so fast, that you   ve gotta be
   hitting refresh or arxiv on a weekly basis or you   ll be schooled by
   some grad students in canada.
   attendees of iclr
   google deepmind and facebook   s fair constituted a large portion of the
   attendees. a lot of startups, researchers from the googleplex, twitter,
   nvidia, and startups such as clarifai and magic leap. overall a very
   young and vibrant crowd, and a very solid representation by super-smart
   28-35 year olds.

part ii: deep learning themes @ iclr 2016

   incorporating structure into deep learning
   [115]raquel urtasun from the university of toronto gave a talk about
   incorporating structure in deep learning. see [116]raquel's keynote
   video here. many ideas from structure learning and id114
   were presented in her keynote. raquel   s id161 focus makes her
   work stand out, and she additionally showed some recent research
   snapshots from her upcoming cvpr 2016 work.
   [117][tutorialcvpr15.jpg]
   raquel gave a wonderful [118]3d indoor understanding tutorial at last
   year's cvpr 2015.
   one of raquel's strengths is her strong command of geometry, and her
   work covers both learning-based methods as well as multiple-view
   geometry. i strongly recommend keeping a close look at her upcoming
   research ideas. below are two bleeding edge papers from raquel's group
   -- the first one focuses on soccer field localization from a broadcast
   of such a game using branch and bound id136 in a mrf.
   [119][soccer_field.png]
   raquel's new work. soccer field localization from single image.
   homayounfar et al, 2016.
   [120]soccer field localization from a single image. [121]namdar
   homayounfar, sanja fidler, raquel urtasun. in arxiv:1604.02715.
   the second upcoming paper from raquel's group is on using deep learning
   for dense optical flow, in the spirit of [122]flownet, which i
   discussed in my [123]iccv 2015 hottest papers blog post. the technique
   is built on the observation that the scene is typically composed of a
   static background, as well as a relatively small number of traffic
   participants which move rigidly in 3d. the dense optical flow technique
   is applied to autonomous driving.
   [124][optical_flow_raquel.png]
   [125]deep semantic matching for optical flow. min bai, wenjie luo,
   kaustav kundu, raquel urtasun. in arxiv:1604.01827.
   id23
   [126]sergey levine gave an excellent keynote on deep reinforcement
   learning and its application to robotics[3]. see [127]sergey's keynote
   video here. this kind of work is still the future, and there was very
   little robotics-related research in the main conference. it might not
   be surprising, because having an assembly of robotic arms is not cheap,
   and such gear is simply not present in most grad student research labs.
   most iclr work is pure software and some math theory, so a single gpu
   is all that is needed to start with a typical deep learning pipeline.
   [128][robotarms.png]
   an army of robot arms jointly learning to grasp somewhere inside
   google.
   take a look at the following interesting work which shows what
   alex [129]krizhevsky, the author of the legendary 2012 alexnet paper
   which rocked the world of object recognition, is currently doing. and
   it has to do with deep learning for robotics, currently at google.
   [130]learning hand-eye coordination for robotic grasping with deep
   learning and large-scale data collection sergey levine, peter pastor,
   alex krizhevsky, deirdre quillen. in arxiv:1603.02199.
   for those of you who want to learn more about id23,
   perhaps it is time to check out [131]andrej karpathy's deep
   id23: pong from pixels tutorial. one thing is for
   sure: when it comes to deep id23, openai is all-in.
   compressing networks
   [132][hyhk-utsvznbxzc_ub-rqrgyoghf4ia5dl4cuxspugwoyhxdd4yo9ckxsym7mdfxl
   gqg=w300]
   model compression: the winzip of
   neural nets?
   while nvidia might be today   s king of deep learning hardware, i can   t
   help the feeling that there is a new player lurking in the shadows. you
   see, gpu-based mining of bitcoin didn   t last very long once people
   realized the economic value of owning bitcoins. bitcoin very quickly
   transitioned into specialized fpga hardware for running the underlying
   bitcoin computations, and the fpgas of deep learning are right around
   the corner. will nvidia remain the king? i see a fork in nvidia's
   future. you can continue producing hardware which pleases both gamers
   and machine learning researchers, or you can specialize. there is a
   plethora of interesting companies like nervana systems, movidius, and
   most importantly google, that don   t want to rely on power-hungry
   heatboxes known as gpus, especially when it comes to scaling already
   trained deep learning models. just take a look at [133]fathom by
   movidius or the [134]google tpu.
   but the world has already seen the economic value of deep nets, and the
      software    side of deep nets isn't waiting for the fpgas of neural
   nets. the software version of compressing neural networks is a very
   trendy topic. you basically want to take a beefy neural network and
   compress it down into smaller, more efficient model. binarizing the
   weights is one such strategy. student-teacher networks where a smaller
   network is trained to mimic the larger network are already here. and
   don   t be surprised if within the next year we   ll see 1mb sized networks
   performing at the level of oxford   s vggnet on the id163 1000-way
   classification task.
   [135][deep_compress.png]
   summary from iclr 2016's deep compression paper by han et al.
   this year's iclr brought a slew of compression papers, the three which
   stood out are listed below.
   [136]deep compression: compressing deep neural networks with pruning,
   trained quantization and huffman coding. song han, huizi mao, and bill
   dally. in iclr 2016. this paper won the best paper award. see han give
   the [137]deep compression talk.
   [138]neural networks with few multiplications. zhouhan lin, matthieu
   courbariaux, roland memisevic, yoshua bengio. in iclr 2016.
   [139]8-bit approximations for parallelism in deep learning. tim
   dettmers. in iclr 2016.
   unsupervised learning
   philip isola presented a very efrosian paper on using siamese networks
   defined on patches to learn a patch similarity function in an
   unsupervised way. this patch-patch similarity function was used to
   create a local similarity graph defined over an image which can be used
   to discover the extent of objects. this reminds me of the object
   discovery line of research started by alyosha efros and the mit group,
   where the basic idea is to abstain from using class labels in learning
   a similarity function.
   [140][isola.png]
   isola et al: a siamese network has shared weights and can be used for
   learning embeddings or "similarity functions."

   [141]learning visual groups from co-occurrences in space and time
   [142]phillip isola, daniel zoran, dilip krishnan, edward h. adelson. in
   iclr 2016.
   [143][isola2.png]
   isola et al: visual groupings applied to image patches, frames of a
   video, and a large scene dataset.
   initializing networks: and why batchnorm matters
   getting a neural network up and running is more difficult than it
   seems. several papers in iclr 2016 suggested new ways of initializing
   networks. but practically speaking, deep net initialization is
      essentially solved.    initialization seems to be an area of research
   that truly became more of a    science    than an    art    once researchers
   introduced [144]batchnorm into their neural networks. batchnorm is the
   butter of deep learning -- add it to everything and everything will
   taste better. but this wasn   t always the case!
   in the early days, researchers had lots of problems with constructing
   an initial set of weights of a deep neural network such that the back
   propagation could learn anything. in fact, one of the reasons why the
   neural networks of the 90s died as a research program, is precisely
   because it was well-known that a handful of top researchers knew how to
   tune their networks so that they could start automatically learning
   from data, but the other research didn   t know all of the right
   initialization tricks. it was as if the    black magic    inside the 90s
   nns was just too intense. at some point, convex methods and kernel id166s
   because the tools of choice     with no need to initialize in a convex
   optimization setting, for almost a decade (1995 to 2005) researchers
   just ran away from deep methods. once 2006 hit, deep architectures were
   working again with hinton   s magical deep id82s and
   unsupervised pretraining. unsupervised pretaining didn   t last long, as
   researchers got gpus and found that once your data set is large enough
   (think ~2 million images in id163), that simple discriminative
   back-propagation does work. random weight initialization strategies and
   cleverly tuned learning rates were quickly shared amongst researchers
   once 100s of them jumped on the id163 dataset. people started
   sharing code, and wonderful things happened!
   but designing new neural networks for new problems was still
   problematic -- one wouldn't know exactly the best way to set multiple
   learning rates and random initialization magnitudes. but researchers
   got to work, and a handful of solid hackers from google found out that
   the key problem was that poorly initialized networks were having a hard
   time flowing information through the networks. it   s as if layer n was
   producing activations in one range and the subsequent layers were
   expecting information to be of another order of magnitude. so szegedy
   and ioffe from google proposed a simple    trick    to whiten the flow of
   data as it passes through the network. their trick, called    batchnorm   
   involves using a id172 layer after each convolutional and/or
   fully-connected layer in a deep network. this id172 layer
   whitens the data by subtracting a mean and dividing by a standard
   deviation, thus producing roughly gaussian numbers as information flows
   through the network. so simple, yet so sweet. the idea of whitening
   data is so prevalent in all of machine learning, that it   s silly that
   it took deep learning researchers so long to re-discover the trick in
   the context of deep nets.
   [145]data-dependent initializations of convolutional neural networks
   philipp kr  henb  hl, carl doersch, jeff donahue, trevor darrell. in iclr
   2016. carl doersch, a fellow cmu phd, is going to deepmind, so there
   goes another point for deepmind.
   backprop tricks
   injecting noise into the gradient seems to work. and this reminds me of
   the common grad student dilemma where you fix a bug in your gradient
   calculation, and your learning algorithm does worse. you see, when you
   were computing the derivative on the white board, you probably made a
   silly mistake like messing up a coefficient that balances two terms or
   forgetting an additive / multiplicative term somewhere.  however, with
   a high id203, your    buggy gradient    was actually correlated with
   the true    gradient   . and in many scenarios, a quantity correlated with
   the true gradient is better than the true gradient.  it is a certain
   form of id173 that hasn   t been adequately addressed in the
   research community. what kinds of    buggy gradients    are actually good
   for learning? and is there a space of    buggy gradients    that are
   cheaper to compute than    true gradients   ? these    fastgrad    methods
   could speed up training deep networks, at least for the first several
   epochs. maybe by iclr 2017 somebody will decide to pursue this research
   track.
   [146][noise.png]
   [147]adding gradient noise improves learning for very deep networks.
   arvind neelakantan, luke vilnis, quoc v. le, ilya sutskever, lukasz
   kaiser, karol kurach, james martens. in iclr 2016.
   [148]robust convolutional neural networks under adversarial noise
   jonghoon jin, aysegul dundar, eugenio culurciello. in iclr 2016.
   attention: focusing computations
   attention-based methods are all about treating different "interesting"
   areas with more care than the "boring" areas. not all pixels are equal,
   and people are able to quickly focus on the interesting bits of a
   static picture. iclr 2016's most interesting "attention" paper was the
   dynamic capacity networks paper from [149]aaron courville's group at
   the university of montreal. [150]hugo larochelle, another key
   researcher with strong ties to the deep learning mafia, is now a
   research scientist at twitter.
   [151][dcn.png]
   [152]dynamic capacity networks amjad almahairi, nicolas ballas, tim
   cooijmans, yin zheng, hugo larochelle, aaron courville. in iclr 2016.
   the    resnet trick   : going mega deep because it's mega fun
   we saw some new papers on the new    resnet    trick which emerged within
   the last few months in the deep learning community. the resnet trick is
   the    residual net    trick that gives us a rule for creating a deep stack
   of layers. because each residual layer essentially learns to either
   pass the raw data through or mix in some combination of a non-linear
   transformation, the flow of information is much smoother. this    control
   of flow    that comes with residual blocks, lets you build vgg-style
   networks that are quite deep.
   [153]inception-v4, inception-resnet and the impact of residual
   connections on learning christian szegedy, sergey ioffe, vincent
   vanhoucke. in iclr 2016.
   [154][rir.png]
   [155]resnet in resnet: generalizing residual architectures sasha targ,
   diogo almeida, kevin lyman. in iclr 2016.
   deep metric learning and learning subcategories
   a great paper, presented by manohar paluri of facebook, focused on a
   new way to think about deep metric learning. the paper is    metric
   learning with adaptive density discrimination    and reminds me of my own
   research from cmu. their key idea can be distilled to the
      anti-category    argument. basically, you build into your algorithm the
   intuition that not all elements of a category c1 should collapse into a
   single unique representation. due to the visual variety within a
   category, you only make the assumption that an element x of category c
   is going to be similar to a subset of other cs, and not all of them. in
   their paper, they make the assumption that all members of category c
   belong to a set of latent subcategories, and em-like learning
   alternates between finding subcategory assignments and updating the
   distance metric. during my phd, we took this idea even further and
   build exemplar-id166s which were the smallest possible subcategories with
   a single positive    exemplar    member.
   manohar started his research as a member of the fair team, which
   focuses more on r&d work, but metric learning ideas are very
   product-focused, and the paper is a great example of a technology that
   seems to be "product-ready." i envision dozens of facebook products
   that can benefit from such data-derived adaptive deep distance metrics.
   [156][magnet.png]
   [157]metric learning with adaptive density discrimination. oren rippel,
   manohar paluri, piotr dollar, lubomir bourdev. in iclr 2016.
   deep learning calculators
   lstms, deep id63s, and what i call    deep learning
   calculators    were big at the conference. some people say,    just because
   you can use deep learning to build a calculator, it doesn   t mean you
   should." and for some people, deep learning is the
   holy-grail-titan-power-hammer, and everything that can be described
   with words should be built using deep learning components.
   nevertheless, it's an exciting time for deep turing machines.
   the winner of the best paper award was the paper, neural
   programmer-interpreters by scott reed and nando de freitas. an
   interesting way to blend deep learning with the theory of computation.
   if you   re wondering what it would look like to use deep learning to
   learn quicksort, then check out their paper. and it seems like scott
   reed is going to google deepmind, so you can tell where they   re placing
   their bets.
   [158][npi.png]
   [159]neural programmer-interpreters. scott reed, nando de freitas. in
   iclr 2016.
   another interesting paper by some openai guys is    neural random-access
   machines    which is going to be another fan favorite for those who love
   deep learning calculators.
   [160][nram.png]
   [161]neural random-access machines. karol kurach, marcin andrychowicz,
   ilya sutskever. in iclr 2016.
   id161 applications
   boundary detection is a common id161 task, where the goal is
   to predict boundaries between objects. cv folks have been using image
   pyramids, or multi-level processing, for quite some time. check out the
   following deep boundary paper which aggregates information across
   multiple spatial resolutions.
   [162][segmentation.png]
   [163]pushing the boundaries of boundary detection using deep learning
   iasonas kokkinos, in iclr 2016.
   a great application for id56s is to "unfold" an image into multiple
   layers. in the context of id164, the goal is to decompose an
   image into its parts. the following figure explains it best, but if
   you've been wondering where to use id56s in your id161
   pipeline, check out their paper.
   [164][decompnet.png]
   [165]learning to decompose for id164 and instance
   segmentation eunbyung park, alexander c. berg. in iclr 2016.
   dilated convolutions are a "trick" which allows you to increase your
   network's receptive field size and scene segmentation is one of the
   best application domains for such dilations.
   [166][dilated_convolutions.png]
   [167]multi-scale context aggregation by dilated convolutions fisher yu,
   vladlen koltun. in iclr 2016.
   visualizing networks
   two of the best    visualization    papers were    do neural networks learn
   the same thing?    by
   [168]jason yosinski (now going to [169]geometric intelligence, inc.)
   and    visualizing and understanding recurrent networks    presented by
   andrej karpathy (now going to [170]openai). yosinski presented his work
   on studying what happens when you learn two different networks using
   different initializations. do the nets learn the same thing? i remember
   a great conversation with jason about figuring out if the neurons in
   network a can be represented as linear combinations of network b, and
   his visualizations helped make the case. andrej   s visualizations of
   recurrent networks are best consumed in presentation/blog form[2]. for
   those of you that haven   t yet seen andrej   s analysis of recurrent nets
   on hacker news, check it out [171]here.

   [172][jason.png]
   [173]convergent learning: do different neural networks learn the same
   representations? yixuan li, jason yosinski, jeff clune, hod lipson,
   john hopcroft. in iclr 2016. see [174]yosinski's video here.
   [175][karpathy.png]
   [176]visualizing and understanding recurrent networks andrej karpathy,
   justin johnson, li fei-fei. in iclr 2016.
   do deep convolutional nets really need to be deep (or even
   convolutional)?
   [177][caruana.png]
   figure from do nets have to be deep?
   this was the key question asked in the paper presented by rich caruana.
   (dr. caruana is now at microsoft, but i remember meeting him at cornell
   eleven years ago) their papers' two key results which are quite
   meaningful if you sit back and think about them. first, there is
   something truly special about convolutional layers that when applied to
   images, they are significantly better than using solely fully connected
   layers -- there   s something about the 2d structure of images and the 2d
   structures of filters that makes convolutional layers get a lot of
   value out of their parameters. secondly, we now have teacher-student
   training algorithms which you can use to have a shallower network
      mimic    the teacher   s responses on a large dataset. these shallower
   networks are able to learn much better using a teacher and in fact,
   such shallow networks produce inferior results when the are trained on
   the teacher   s training set.  so it seems you get go [data to megadeep],
   and [megadeep to minideep], but you cannot directly go from [data to
   minideep].
   [178]do deep convolutional nets really need to be deep (or even
   convolutional)? gregor urban, krzysztof j. geras, samira ebrahimi
   kahou, ozlem aslan, shengjie wang, rich caruana, abdelrahman mohamed,
   matthai philipose, matt richardson. in iclr 2016.
   another interesting idea on the [megadeep to minideep] and [minideep to
   megadeep] front,
   [179][net2deepernet.png]
   [180]net2net: accelerating learning via knowledge transfer tianqi chen,
   ian goodfellow, jonathon shlens. in iclr 2016.
   id38 with lstms
   there was also considerable focus on methods that deal with large
   bodies of text. chris dyer (who is supposedly also going to deepmind),
   gave a keynote asking the question    should model architecture reflect
   linguistic structure?    see [181]chris dyer's keynote video here. some
   of his key take-aways from comparing word-level embedding vs
   character-level embeddings is that for different languages, different
   methods work better.  for languages which have a rich syntax,
   character-level encodings outperform word-level encodings.
   [182]improved transition-based parsing by modeling characters instead
   of words with lstms miguel ballesteros, chris dyer, noah a. smith. in
   proceedings of emnlp 2015.
   an interesting approach, with a great presentation by ivan vendrov, was
      order-embeddings of images and language" by ivan vendrov, ryan kiros,
   sanja fidler, and raquel urtasun which showed a great intuitive
   coordinate-system-y way for thinking about concepts. i really love
   these coordinate system analogies and i   m all for new ways of thinking
   about classical problems.
   [183][order.png]

   [184]order-embeddings of images and language ivan vendrov, ryan kiros,
   sanja fidler, raquel urtasun. in iclr 2016. [185]see video here.
   training-free methods: brain-dead applications of id98s to image
   matching
   these techniques use the activation maps of deep neural networks
   trained on an id163 classification task for other important computer
   vision tasks. these techniques employ clever ways of matching image
   regions and from the following iclr paper, are applied to smart image
   retrieval.
   [186][mac.png]
   [187]particular object retrieval with integral max-pooling of id98
   activations. giorgos tolias, ronan sicre, herv   j  gou. in iclr 2016.
   this reminds me of the rss 2015 paper which uses convnets to match
   landmarks for a relocalization-like slam task.
   [188][rss.png]
   [189]place recognition with convnet landmarks: viewpoint-robust,
   condition-robust, training-free. niko sunderhauf, sareh shirazi, adam
   jacobson, feras dayoub, edward pepperell, ben upcroft, and michael
   milford. in rss 2015.
   gaussian processes and auto encoders
   gaussian processes used to be quite popular at nips, sometimes used for
   vision problems, but mostly    forgotten    in the era of deep learning.
   vaes or variational auto encoders used to be much more popular when
   pertaining was the only way to train deep neural nets. however, with
   new techniques like adversarial networks, people keep revisiting auto
   encoders, because we still    hope    that something as simple as an
   encoder / decoder network should give us the unsupervised learning
   power we all seek, deep down inside. vaes got quite a lot of action but
   didn't make the cut for today's blog post.
   geometric methods
   overall, very little content pertaining to the sfm / slam side of the
   vision problem was present at iclr 2016. this kind of work is very
   common at cvpr, and it's a bit of a surprise that there wasn't a lot of
   robotics work at iclr. it should be noted that the techniques used in
   sfm/slam are more based on multiple-view geometry and id202
   than the data-driven deep learning of today.
   perhaps a better venue for robotics and deep learning will be the june
   2016 workshop titled [190]are the sceptics right? limits and potentials
   of deep learning in robotics. this workshop is being held at rss 2016,
   one of the world's leading robotics conferences.

part iii: quo vadis deep learning?

   neural net compression is going to be big -- real-world applications
   demand it. the algos guys aren't going to wait for tpu and vpus to
   become mainstream. deep nets which can look at a picture and tell you
   what   s going on are going to be inside every single device which has a
   camera. in fact, i don   t see any reason why all cameras by 2020 won   t
   be able to produce a high-quality rgb image as well as a neural network
   response vector. new image formats will even have such    deep
   interpretation vectors    directly saved alongside the image. and it's
   all going to be a neural net, in one shape or another.
   openai had a strong presence at iclr 2016, and i feel like every week a
   new phd joins openai. google deepmind and facebook fair had a large
   number of papers. google demoed a real-time version of deep-learning
   based style transfer using tensorflow. microsoft is no longer king of
   research. startups were giving out little toys -- clarifai even gave
   out free sandals. graduates with well-tuned deep learning skills will
   continue being in high-demand, but once the next generation of
   ai-driven startups emerge, it is only those willing to transfer their
   academic skills into a product world-facing focus, aka the upcoming
   wave of deep entrepreneurs, that will make serious $$$.
   research-wise, arxiv is a big productivity booster. hopefully, now you
   know where to place your future deep learning research bets, have
   enough new insights to breath some inspiration into your favorite
   research problem, and you've gotten a taste of where the top
   researchers are heading. i encourage you to turn off your computer and
   have a white-board conversation with your colleagues about deep
   learning. grab a friend, teach him some tricks.
   i'll see you all at cvpr 2016. until then, keep learning.

related computervisionblog.com blog posts

   [191]why your lab needs a reading group. may 2012
   [192]iccv 2015: 21 hottest research papers december 2015
   [193]deep down the rabbit hole: cvpr 2015 and beyond june 2015
   [194]the deep learning gold rush of 2015 november 2015
   [195]deep learning vs machine learning vs pattern recognition march
   2015
   [196]deep learning vs probabilistic id114 april 2015
   [197]future of real-time slam and "deep learning vs slam" january 2016

relevant outside links

   [1] [198]welcome to the ai conspiracy: the 'canadian mafia' behind
   tech's latest craze @ <re/code>
   [2] [199]the unreasonable effectiveness of recurrent neural networks @
   andrej karpathy's blog
   [3] [200]deep learning for robots: learning from large-scale
   interaction. @ google research blog
   posted by [201]unknown at [202]wednesday, june 01, 2016 [203]15
   comments:
   [204]email this[205]blogthis![206]share to twitter[207]share to
   facebook[208]share to pinterest
   labels: [209]deep calculators, [210]deep compression, [211]deep
   learning, [212]deepmind, [213]facebook, [214]google, [215]iclr,
   [216]karpathy, [217]lstm, [218]metric learning, [219]openai,
   [220]resnet, [221]id56s, [222]robotics, [223]tensorflow, [224]urtasun,
   [225]visualization, [226]yann lecun, [227]yoshua bengio

wednesday, january 13, 2016

[228]the future of real-time slam and deep learning vs slam

   last month's international conference of id161 (iccv) was
   [229]full of deep learning techniques, but before we declare an all-out
   convnet victory, let's see how the other "non-learning" geometric side
   of id161 is doing.  simultaneous localization and mapping, or
   slam, is arguably one of the most important algorithms in robotics,
   with pioneering work done by both id161 and robotics research
   communities.  today i'll be summarizing my key points from
   iccv's [230]future of real-time slam workshop, which was held on the
   last day of the conference (december 18th, 2015).
   today's post contains a brief introduction to slam, a detailed
   description of what happened at the workshop (with summaries of all 7
   talks), and some take-home messages from the deep learning-focused
   panel discussion at the end of the session.
   [231][slammies2.png]
   slam visualizations. can you identify any of these slam algorithms?

part i: why slam matters

   visual slam algorithms are able to simultaneously build 3d maps of the
   world while tracking the location and orientation of the camera
   (hand-held or head-mounted for ar or mounted on a robot). slam
   algorithms are complementary to convnets and deep learning: slam
   focuses on geometric problems and deep learning is the master of
   perception (recognition) problems. if you want a robot to go towards
   your refrigerator without hitting a wall, use slam. if you want the
   robot to identify the items inside your fridge, use convnets.
   [232][structurefrommotion.png]
   basics of sfm/slam: from point observation and intrinsic camera
   parameters, the 3d structure of a scene is computed from the estimated
   motion of the camera. for details, see [233]openmvg website.
   slam is a real-time version of structure from motion (sfm). visual slam
   or vision-based slam is a camera-only variant of slam which forgoes
   expensive laser sensors and inertial measurement units (imus).
   monocular slam uses a single camera while non-monocular slam typically
   uses a pre-calibrated fixed-baseline stereo camera rig. slam is prime
   example of a what is called a "geometric method" in id161. in
   fact, cmu's robotics institute splits the graduate level computer
   vision curriculum into a [234]learning-based methods in vision course
   and a separate [235]geometry-based methods in vision course.
   structure from motion vs visual slam
   structure from motion (sfm) and slam are solving a very similar
   problem, but while sfm is traditionally performed in an offline
   fashion, slam has been slowly moving towards the low-power / real-time
   / single rgb camera mode of operation. many of the today   s top experts
   in structure from motion work for some of the world   s biggest tech
   companies, helping make maps better. successful mapping products like
   google maps could not have been built without intimate knowledge of
   multiple-view geometry, sfm, and slam.  a typical sfm problem is the
   following: given a large collection of photos of a single outdoor
   structure (like the colliseum), construct a 3d model of the structure
   and determine the camera's poses. the image collection is processed in
   an offline setting, and large reconstructions can take anywhere between
   hours and days.
   [236][colosseum.jpg]
   sfm software: [237]bundler is one of the most successful sfm open
   source libraries
   here are some popular sfm-related software libraries:
     * [238]bundler, an open-source structure from motion toolkit
     * [239]libceres, a non-linear least squares minimizer (useful for
       bundle adjustment problems)
     * andrew zisserman's [240]multiple-view geometry matlab functions

   visual slam vs autonomous driving
   while self-driving cars are one of the most important applications of
   slam, according to andrew davison, one of the workshop organizers, slam
   for autonomous vehicles deserves its own research track. (and as we'll
   see, none of the workshop presenters talked about self-driving cars).
   for many years to come it will make sense to continue studying slam
   from a research perspective, independent of any single holy-grail
   application. while there are just too many system-level details and
   tricks involved with autonomous vehicles, research-grade slam systems
   require very little more than a webcam, knowledge of algorithms, and
   elbow grease. as a research topic, visual slam is much friendlier to
   thousands of early-stage phd students who   ll first need years of in-lab
   experience with slam before even starting to think about expensive
   robotic platforms such as self-driving cars.
   [241][1948541]
   google's self-driving car's perception system. from ieee spectrum's
   "[242]how google's self-driving car works"
   related: march 2015 blog post, [243]mobileye's quest to put deep
   learning inside every new car.
   related: [244]one way google's cars localize themselves

part ii: the future of real-time slam

   now it's time to officially summarize and comment on the presentations
   from the future of real-time slam workshop. [245]andrew davison started
   the day with an excellent historical overview of slam called [246]15
   years of vision-based slam, and his slides have good content for an
   introductory robotics course.
   for those of you who don   t know andy, he is the one and only professor
   andrew davison of imperial college london.  most known for his 2003
   monoslam system, he was one of the first to show how to build slam
   systems from a single    monocular    camera at a time when just everybody
   thought you needed a stereo    binocular    camera rig. more recently, his
   work has influenced the trajectory of companies such as dyson and the
   capabilities of their robotic systems (e.g., [247]the brand new
   dyson360).
   i remember professor davidson from the visual slam tutorial he gave at
   the bmvc conference back in [248]2007. surprisingly very little has
   changed in slam compared to the rest of the machine-learning heavy work
   being done at the main vision conferences. in the past 8 years, object
   recognition has undergone 2-3 mini revolutions, while today's slam
   systems don't look much different than they did 8 years ago. the best
   way to see the progress of slam is to take a look at the most
   successful and memorable systems. in davison   s workshop introduction
   talk, he discussed some of these exemplary systems which were produced
   by the research community over the last 10-15 years:
     * monoslam
     * ptam
     * fab-map
     * dtam
     * kinectfusion

   davison vs horn: the next chapter in robot vision
   davison also mentioned that he is working on a new robot vision book,
   which should be an exciting treat for researchers in id161,
   robotics, and artificial intelligence. the last [249]robot vision book
   was written by b.k. horn (1986), and it   s about time for an updated
   take on robot vision.
   [250][robotvision-01.png]
   a new robot vision book?
   while i   ll gladly read a tome that focuses on the philosophy of robot
   vision, personally i would like the book to focus on practical
   algorithms for robot vision, like the excellent [251]multiple view
   geometry book by hartley and zissermann or [252]probabilistic robotics
   by thrun, burgard, and fox. a "cookbook" of visual slam problems would
   be a welcome addition to any serious vision researcher's collection.
   related: davison's [253]15-years of vision-based slam slides
   talk 1: christian kerl on continuous trajectories in slam
   the first talk, by [254]christian kerl, presented a dense tracking
   method to estimate a continuous-time trajectory. the key observation is
   that most slam systems estimate camera poses at a discrete number of
   time steps (either they key frames which are spaced several seconds
   apart, or the individual frames which are spaced approximately 1/25s
   apart).
   [255][kerl.png]
   continuous trajectories vs discrete time points. slam/sfm usually uses
   discrete time points, but why not go continuous?
   much of kerl   s talk was focused on undoing the damage of rolling
   shutter cameras, and the system demo   ed by kerl paid meticulous
   attention to modeling and removing these adverse rolling shutter
   effects.
   [256][shutter.png]
   undoing the damage of rolling shutter in visual slam.
   related: kerl's [257]dense continous-time tracking and mapping slides.
   related: dense continuous-time tracking and mapping with rolling
   shutter rgb-d cameras (c. kerl, j. stueckler, d. cremers), in ieee
   international conference on id161 (iccv), 2015. [[258]pdf]
   talk 2: semi-dense direct slam by jakob engel
   lsd-slam came out at eccv 2014 and is one of my favorite slam systems
   today! [259]jakob engel was there to present his system and show the
   crowd some of the coolest slam visualizations in town. lsd-slam is an
   acronym for large-scale direct monocular slam. lsd-slam is an important
   system for slam researchers because it does not use corners or any
   other local features. direct tracking is performed by image-to-image
   alignment using a coarse-to-fine algorithm with a robust huber loss.
   this is quite different than the feature-based systems out there. depth
   estimation uses an inverse depth parametrization (like many other slam
   systems) and uses a large number or relatively small baseline image
   pairs. rather than relying on image features, the algorithms is
   effectively performing    texture tracking   . global mapping is performed
   by creating and solving a pose graph "bundle adjustment" optimization
   problem, and all of this works in real-time. the method is semi-dense
   because it only estimates depth at pixels solely near image boundaries.
   lsd-slam output is denser than traditional features, but not fully
   dense like kinect-style rgbd slam.

                                         [260][lsd-slam.png]
    lsd-slam in action: [261]lsd-slam generates both a camera trajectory
      and a semi-dense 3d scene reconstruction. this approach works in
     real-time, does not use feature points as primitives, and performs
                      direct image-to-image alignment.

   engel gave us an overview of the original lsd-slam system as well as a
   handful of new results, extending their initial system to more creative
   applications and to more interesting deployments. (see paper citations
   below)
   related: [262]lsd-slam open-source code on github [263]lsd-slam project
   webpage
   related: lsd-slam: large-scale direct monocular slam (j. engel, t.
   sch  ps, d. cremers), in european conference on id161 (eccv),
   2014. [[264]pdf] [youtube [265]video]
   an extension to lsd-slam, omni lsd-slam was created by the observation
   that the pinhole model does not allow for a large field of view. this
   work was presented at iros 2015 (caruso is first author) and allows a
   large field of view (ideally more than 180 degrees). from engel   s
   presentation it was pretty clear that you can perform ballerina-like
   motions (extreme rotations) while walking around your office and
   holding the camera. this is one of those worst-case scenarios for
   narrow field of view slam, yet works quite well in omni lsd-slam.
   [266][omni.png]
   omnidirectional lsd-slam model. see engel's [267]semi-dense direct
   slam presentation slides.
   related: large-scale direct slam for omnidirectional cameras (d.
   caruso, j. engel, d. cremers), in international conference on
   intelligent robots and systems (iros), 2015.  [[268]pdf]
   [youtube [269]video]
   stereo lsd-slam is an extension of lsd-slam to a binocular camera rig.
   this helps in getting the absolute scale, initialization is
   instantaneous, and there are no issues with strong rotation. while
   monocular slam is very exciting from an academic point of view, if your
   robot is a 30,000$ car or 10,000$ drone prototype, you should have a
   good reason to not use a two+ camera rig. stereo lsd-slam performs
   quite competitively on slam benchmarks.
   [270][stereo-lsd.png]
   stereo lsd-slam. excellent results on kitti vehicle-slam dataset.
   stereo lsd-slam is quite practical, optimizes a pose graph in se(3),
   and includes a correction for auto exposure. the goal of auto-exposure
   correcting is to make the error function invariant to affine lighting
   changes. the underlying parameters of the color-space affine transform
   are estimated during matching, but thrown away to estimate the
   image-to-image error. from engel's talk, outliers (often caused by
   over-exposed image pixels) tend to be a problem, and much care needs to
   be taken to care of their effects.
   related: large-scale direct slam with stereo cameras (j. engel, j.
   stueckler, d. cremers), in international conference on intelligent
   robots and systems (iros), 2015.  [[271]pdf] [youtube [272]video]
   later in his presentation, engel gave us a sneak peak on new research
   about integrating both stereo and inertial sensors. for details, you   ll
   have to keep hitting refresh on arxiv or talk to usenko/engel in
   person. on the applications side, engel's presentation included updated
   videos of an autonomous quadrotor driven by lsd-slam. the flight starts
   with an up-down motion to get the scale estimate and a free-space
   octomap is used to estimate the free-space so that the quadrotor can
   navigate space on its own. stay tuned for an official publication...
   [273][quadrotor.png]
   quadrotor running stereo lsd-slam.
   see[274] engel's quadrotor youtube video from 2012.
   the story of lsd-slam is also the story of feature-based vs
   direct-methods and engel gave both sides of the debate a fair
   treatment. feature-based methods are engineered to work on top of
   harris-like corners, while direct methods use the entire image for
   alignment. feature-based methods are faster (as of 2015), but direct
   methods are good for parallelism. outliers can be retroactively removed
   from feature-based systems, while direct methods are less flexible
   w.r.t. outliners. rolling shutter is a bigger problem for direct
   methods and it makes sense to use a global shutter or a rolling shutter
   model (see kerl   s work). feature-based methods require making decisions
   using incomplete information, but direct methods can use much more
   information. feature-based methods have no need for good initialization
   and direct-based methods need some clever tricks for initialization.
   there is only about 4 years of research on direct methods and 20+ on
   sparse methods. engel is optimistic that direct methods will one day
   rise to the top, and so am i.
   [275][feature-vs-direct.png]
   feature-based vs direct methods of building slam systems. slide from
   engel's talk.
   at the end of engel's presentation, davison asked about semantic
   segmentation and engel wondered whether semantic segmentation can be
   performed directly on semi-dense "near-image-boundary" data.  however,
   my personal opinion is that there are better ways to apply semantic
   segmentation to lsd-like slam systems. semi-dense slam can focus on
   geometric information near boundaries, while object recognition can
   focus on reliable semantics away from the same boundaries, potentially
   creating a hybrid geometric/semantic interpretation of the image.
   related: engel's [276]semi-dense direct slam presentation slides
   talk 3: sattler on the challenges of large-scale localization and
   mapping
   [277]torsten sattler gave a talk on large-scale localization and
   mapping. the motivation for this work is to perform 6-dof localization
   inside an existing map, especially for mobile localization. one of the
   key points in the talk was that when you are using traditional
   feature-based methods, storing your descriptors soon becomes very
   costly. techniques such as visual vocabularies (remember product
   quantization?) can significantly reduce memory overhead, and with
   clever optimization at some point storing descriptors no longer becomes
   the memory bottleneck.
   another important take-home message from sattler   s talk is that the
   number of inliers is not actually a good confidence measure for camera
   pose estimation.  when the feature point are all concentrated in a
   single part of the image, camera localization can be kilometers away! a
   better measure of confidence is the    effective inlier count    which
   looks at the area spanned by the inliers as a fraction of total image
   area.  what you really want is feature matches from all over the image
       if the information is spread out across the image you get a much
   better pose estimate.
   sattler   s take on the future of real-time slam is the following: we
   should focus on compact map representations, we should get better at
   understanding camera pose estimate confidences (like down-weighing
   features from trees), we should work on more challenging scenes (such
   as worlds with planar structures and nighttime localization against
   daytime maps).
   [278][mobileloc.png]
   mobile localisation: sattler's key problem is localizing yourself
   inside a large city with a single smartphone picture
   related: scalable 6-dof localization on mobile devices. sven
   middelberg, torsten sattler, ole untzelmann, leif kobbelt. in eccv
   2014. [[279]pdf]
   related: torsten sattler 's [280]the challenges of large-scale
   localisation and mapping slides
   talk 4: mur-artal on feature-based vs direct-methods
   ra  l mur-artal, the creator of orb-slam, dedicated his entire
   presentation to the feature-based vs direct-method debate in slam and
   he's definitely on the feature-based side. orb-slam is available as an
   open-source slam package and it is hard to beat. during his evaluation
   of orb-slam vs ptam it seems that ptam actually fails quite often (at
   least on the tum rgb-d benchmark). lsd-slam errors are also much higher
   on the tum rgb-d benchmark than expected.
   [281][types-of-slam.jpg]
   feature-based slam vs direct slam. see mur-artal's [282]should we still
   do sparse feature based slam? presentation slides
   related: mur-artal's [283]should we still do sparse-feature based
   slam? slides
   related: monocular orb-slam r. mur-artal, j. m. m. montiel and j. d.
   tardos. a versatile and accurate monocular slam system. ieee
   transactions on robotics. 2015 [[284]pdf]
   related: [285]orb-slam open-source code on github, [286]project website
   talk 5: project tango and visual loop-closure for image-2-image
   constraints
   simply put, [287]google's project tango is the world' first attempt at
   commercializing slam. simon lynen from google zurich (formerly eth
   zurich) came to the workshop with a tango live demo (on a tablet) and a
   presentation on what's new in the world of tango. in case you don't
   already know, google wants to put slam capabilities into the next
   generation of android devices.
   [288][google-project-tango-3d-mapping-video.jpeg]
   google's project tango needs no introduction.
   the project tango presentation discussed a new way of doing loop
   closure by finding certain patters in the image-to-image matching
   matrix. this comes from the    placeless place recognition    work. they
   also do online bundle adjustment w/ vision-based loop closure.
   [289][placeless.png]
   loop closure inside a project tango? lynen et al's [290]placeless place
   recognition. the image-to-image matrix reveals a new way to look for
   loop-closure. see the algorithm in action in this [291]youtube video.
   the project tango folks are also working on combing multiple
   crowd-sourced maps at google, where the goals to combine multiple
   mini-maps created by different people using tango-equipped devices.
   simon showed a video of mountain bike trail tracking which is actually
   quite difficult in practice. the idea is to go down a mountain bike
   trail using a tango device and create a map, then the follow-up goal is
   to have a separate person go down the trail. this currently
      semi-works    when there are a few hours between the map building and
   the tracking step, but won   t work across weeks/months/etc.
   during the tango-related discussion, richard newcombe pointed out that
   the    features    used by project tango are quite primitive w.r.t. getting
   a deeper understanding of the environment, and it appears that project
   tango-like methods won't work on outdoor scenes where the world is
   plagued by non-rigidity, massive illumination changes, etc.  so are we
   to expect different systems being designed for outdoor systems or will
   project tango be an indoor mapping device?
   related: [292]placeless place recognition. lynen, s. ; bosse, m. ;
   furgale, p. ; siegwart, r. in 3dv 2014.
   related: [293]google i/o talk from may 29, 2015 about tango
   talk 6: elasticfusion is denseslam without a pose-graph
   elasticfusion is a dense slam technique which requires a rgbd sensor
   like the kinect. 2-3 minutes to obtain a high-quality 3d scan of a
   single room is pretty cool. a pose-graph is used behind the scenes of
   many (if not most) slam systems, and this technique has a different
   (map-centric) approach. the approach focuses on building a map, but the
   trick is that the map is deformable, hence the name elasticfusion. the
      fusion    part of the algorithm is in homage to kinectfusion which was
   one of the first high quality kinect-based reconstruction pipelines.
   also surfels are used as the underlying primitives.
   [294][kintinuous.png]
   image from kintinuous, an early version of whelan's elastic fusion.
   recovering light sources: we were given a sneak peak at new unpublished
   work from imperial college london / dyson robotics lab. the idea is
   that detecting the light source direction and detecting specularities,
   you can improve 3d reconstruction results. cool videos of recovering
   light source locations which work for up to 4 separate lights.
   related: [295]map-centric slam with elasticfusion presentation slides
   related: [296]elasticfusion: dense slam without a pose graph. whelan,
   thomas and leutenegger, stefan and salas-moreno, renato f and glocker,
   ben and davison, andrew j. in rss 2015.
   talk 7: richard newcombe   s dynamicfusion
   richard newcombe's (whose recently formed company was acquired by
   oculus), was the last presenter.  it's really cool to see the person
   behind [297]dtam, [298]kinectfusion, and [299]dynamicfusion now working
   in the vr space.
   [300][dynamicfusion.png]
   newcombe's [301]dynamic fusion algorithm. the technique won the
   prestigious cvpr 2015 best paper award, and to see it in action just
   take a look at the authors' [302]dynamicfusion youtube video.
   related: [303]dynamicfusion: reconstruction and tracking of non-rigid
   scenes in real-time, richard a. newcombe, dieter fox, steven m. seitz.
   in cvpr 2015. [[304]pdf] [best-paper winner]
   related: [305]slam++: simultaneous localisation and mapping at the
   level of objects renato f. salas-moreno, richard a. newcombe, hauke
   strasdat, paul h. j. kelly and andrew j. davison (cvpr 2013)
   related: [306]kinectfusion: real-time dense surface mapping and
   tracking richard a. newcombe shahram izadi,otmar hilliges, david
   molyneaux, david kim, andrew j. davison, pushmeet kohli, jamie shotton,
   steve hodges, andrew fitzgibbon (ismar 2011, best paper award!)
   workshop demos
   during the demo sessions (held in the middle of the workshop), many of
   the presenter showed off their slam systems in action. many of these
   systems are available as open-source (free for non-commercial use?)
   packages, so if you   re interested in real-time slam, downloading the
   code is worth a shot. however, the one demo which stood out was andrew
   davison   s showcase of his monoslam system from 2004. andy had to revive
   his 15-year old laptop (which was running redhat linux) to show off his
   original system, running on the original hardware. if the computer
   vision community is going to oneway decide on a    retro-vision    demo
   session, i   m just going to go ahead and nominate andy for the
   best-paper prize, right now.
   [307][img_0500.jpg]
   andry's retro-vision slam setup (pictured on december 18th, 2015)
   it was interesting to watch the slam system experts wave their usb
   cameras around, showing their systems build 3d maps of the desk-sized
   area around their laptops.  if you carefully look at the way these
   experts move the camera around (i.e., smooth circular motions), you can
   almost tell how long a person has been working with slam. when the
   non-experts hold the camera, id203 of tracking failure is
   significantly higher.
   i had the pleasure of speaking with andy during the demo session, and i
   was curious which line of work (in the past 15 years) surprised him the
   most. his reply was that ptam, which showed how to perform real-time
   bundle adjustment, surprised him the most. the ptam system was
   essentially a monoslam++ system, but the significantly improved
   tracking results were due to taking a heavyweight algorithm (bundle
   adjustment) and making it real-time     something which andy did not
   believe was possible in the early 2000s.

part iii: deep learning vs slam

   the slam panel discussion was a lot of fun. before we jump to the
   important deep learning vs slam discussion, i should mention that each
   of the workshop presenters agreed that semantics are necessary to build
   bigger and better slam systems. there were lots of interesting
   mini-conversations about future directions. during the debates,
   [308]marc pollefeys (a well-known researcher in sfm and multiple-view
   geometry) reminded everybody that robotics is the killer application of
   slam and suggested we keep an eye on the prize. this is quite
   surprising since slam was traditionally applied to robotics problems,
   but the lack of robotics success in the last few decades (google
   robotics?) has shifted the focus of slam away from robots and towards
   large-scale map building (ala google maps) and augmented reality.
   nobody at this workshop talked about robots.
   integrating semantic information into slam
   there was a lot of interest in incorporating semantics into today   s
   top-performing slam systems. when it comes to semantics, the slam
   community is unfortunately stuck in the world of bags-of-visual-words,
   and doesn't have new ideas on how to integrate semantic information
   into their systems. on the other end, we   re now seeing real-time
   semantic segmentation demos (based on convnets) popping up at
   cvpr/iccv/eccv, and in my opinion slam needs deep learning as much as
   the other way around.
   [309][semantics.png]
   integrating semantics into slam is often talk about, but it is easier
   said than done.
   figure 6.9 (page 142) from moreno's phd thesis: [310]dense semantic
   slam
   "will end-to-end learning dominate slam?"
   towards the end of the slam workshop panel, [311]dr. zeeshan zia asked
   a question which startled the entire room and led to a memorable,
   energy-filled discussion. you should have seen the look on the panel   s
   faces. it was a bunch of geometers being thrown a fireball of deep
   learning. their facial expressions suggest both bewilderment, anger,
   and disgust. "how dare you question us?" they were thinking. and it is
   only during these fleeting moments that we can truly appreciate the
   conference experience. zia's question was essentially: will end-to-end
   learning soon replace the mostly manual labor involved in building
   today   s slam systems?.
   zia's question is very important because end-to-end trainable systems
   have been slowly creeping up on many advanced computer science
   problems, and there's no reason to believe slam will be an exception. a
   handful of the presenters pointed out that current slam systems rely on
   too much geometry for a pure deep-learning based slam system to make
   sense -- we should use learning to make the point descriptors better,
   but leave the geometry alone. just because you can use deep learning to
   make a calculator, it doesn't mean you should.
   [312][convnet_lecun_stereo.png]
   [313]learning stereo similarity functions via convnets, by yan lecun
   and collaborators.
   while many of the panel speakers responded with a somewhat affirmative
   "no", it was newcombe which surprisingly championed what the marriage
   of deep learning and slam might look like.
   newcombe's proposal: use slam to fuel deep learning
   although newcombe didn   t provide much evidence or ideas on how deep
   learning might help slam, he provided a clear path on how slam might
   help deep learning.  think of all those maps that we've built using
   large-scale slam and all those correspondences that these systems
   provide     isn   t that a clear path for building terascale image-image
   "association" datasets which should be able to help deep learning? the
   basic idea is that today's slam systems are large-scale "correspondence
   engines" which can be used to generate large-scale datasets, precisely
   what needs to be fed into a deep convnet.
   concluding remarks
   there is quite a large disconnect between the kind of work done at the
   mainstream iccv conference (heavy on machine learning) and the kind of
   work presented at the real-time slam workshop (heavy on geometric
   methods like bundle adjustment). the mainstream id161
   community has witnessed several mini-revolutions within the past decade
   (e.g., dalal-triggs, dpm, id163, convnets, r-id98) while the slam
   systems of today don   t look very different than they did 8 years ago.
   the kinect sensor has probably been the single largest game changer in
   slam, but the fundamental algorithms remain intact.
   [314][134992626.jpg]
   integrating semantic information: the next frontier in visual slam.
   brain image from [315]arwen wallington's blog post.
   today   s slam systems help machines geometrically understand the
   immediate world (i.e., build associations in a local coordinate system)
   while today   s deep learning systems help machines reason categorically
   (i.e., build associations across distinct object instances). in
   conclusion, i share newcombe and davison excitement in visual slam, as
   vision-based algorithms are going to turn augmented and virtual reality
   into billion dollar markets. however, we should not forget to keep our
   eyes on the "trillion-dollar" market, the one that's going to redefine
   what it means to "work" -- namely robotics. the day of robot slam will
   come soon.
   posted by [316]unknown at [317]wednesday, january 13, 2016 [318]27
   comments:
   [319]email this[320]blogthis![321]share to twitter[322]share to
   facebook[323]share to pinterest
   labels: [324]andrew davison, [325]bundle adjustment, [326]dtam,
   [327]dynamicfusion, [328]iccv 2015, [329]jakob engel,
   [330]kinectfusion, [331]lsd-slam, [332]marc pollefeys, [333]pose,
   [334]ptam, [335]real-time, [336]richard newcombe, [337]robotics,
   [338]segmentation, [339]sfm, [340]slam, [341]workshop, [342]zisserman

   [343]older posts [344]home

   subscribe to: [345]posts (atom)

popular posts

     * [346]deep learning vs machine learning vs pattern recognition
       lets take a close look at three related terms (deep learning vs
       machine learning vs pattern recognition), and see how they relate
       to some o...
     * [347]the future of real-time slam and deep learning vs slam
       last month's international conference of id161 (iccv) was
       full of deep learning  techniques, but before we declare an
       all-out...
     * [348]from feature descriptors to deep learning: 20 years of
       id161
       we all know that deep convolutional neural networks have produced
       some stellar results on id164 and recognition benchmarks
       in th...
     * [349]deep learning trends @ iclr 2016
       started by the youngest members of the deep learning mafia [1],
       namely  yann lecun and yoshua bengio , the iclr conference is
       quickly becom...
     * [350]iccv 2015: twenty one hottest research papers
       "geometry vs recognition" becomes convnet-for-x id161
       used to be cleanly separated into two schools: geometry and rec...
     * [351]deep learning vs probabilistic id114 vs logic
       today, let's take a look at three paradigms   that have shaped the
       field of artificial intelligence in the last 50 years: logic ,
       probab...
     * [352]can a person-specific face recognition algorithm be used to
       determine a person's race?
       it's a valid question: can a person-specific face recognition
       algorithm be used to determine a person's race? i trained two
       separa...

recent posts

   [353]recent posts widget your browser does not support javascript!

links

     * [354]tomasz @ mit research homepage
     * [355]tomasz @ google scholar citations
     * [356]tomasz @ github open-source code

blog archive

     * [357]     [358]2018 (1)
          + [359]     [360]may (1)
               o [361]deepfakes: ai-powered deception machines

     * [362]     [363]2016 (4)
          + [364]     [365]december (1)
          + [366]     [367]june (2)
          + [368]     [369]january (1)

     * [370]     [371]2015 (12)
          + [372]     [373]december (1)
          + [374]     [375]november (1)
          + [376]     [377]june (1)
          + [378]     [379]may (2)
          + [380]     [381]april (3)
          + [382]     [383]march (3)
          + [384]     [385]january (1)

     * [386]     [387]2014 (8)
          + [388]     [389]november (1)
          + [390]     [391]october (1)
          + [392]     [393]january (6)

     * [394]     [395]2013 (13)
          + [396]     [397]december (5)
          + [398]     [399]october (1)
          + [400]     [401]september (1)
          + [402]     [403]july (1)
          + [404]     [405]june (3)
          + [406]     [407]april (2)

     * [408]     [409]2012 (11)
          + [410]     [411]july (1)
          + [412]     [413]june (4)
          + [414]     [415]may (1)
          + [416]     [417]april (2)
          + [418]     [419]march (1)
          + [420]     [421]january (2)

     * [422]     [423]2011 (31)
          + [424]     [425]december (4)
          + [426]     [427]november (3)
          + [428]     [429]october (3)
          + [430]     [431]september (3)
          + [432]     [433]august (5)
          + [434]     [435]july (3)
          + [436]     [437]june (2)
          + [438]     [439]april (1)
          + [440]     [441]march (5)
          + [442]     [443]january (2)

     * [444]     [445]2010 (24)
          + [446]     [447]december (1)
          + [448]     [449]november (2)
          + [450]     [451]august (2)
          + [452]     [453]june (5)
          + [454]     [455]may (2)
          + [456]     [457]april (3)
          + [458]     [459]march (3)
          + [460]     [461]february (1)
          + [462]     [463]january (5)

     * [464]     [465]2009 (29)
          + [466]     [467]december (2)
          + [468]     [469]november (4)
          + [470]     [471]october (3)
          + [472]     [473]september (1)
          + [474]     [475]august (3)
          + [476]     [477]july (3)
          + [478]     [479]june (4)
          + [480]     [481]march (5)
          + [482]     [483]february (2)
          + [484]     [485]january (2)

     * [486]     [487]2008 (23)
          + [488]     [489]december (2)
          + [490]     [491]november (3)
          + [492]     [493]october (1)
          + [494]     [495]september (1)
          + [496]     [497]august (1)
          + [498]     [499]july (3)
          + [500]     [501]june (3)
          + [502]     [503]may (2)
          + [504]     [505]april (3)
          + [506]     [507]march (2)
          + [508]     [509]february (2)

     * [510]     [511]2007 (11)
          + [512]     [513]september (1)
          + [514]     [515]august (2)
          + [516]     [517]july (1)
          + [518]     [519]june (1)
          + [520]     [521]april (1)
          + [522]     [523]march (1)
          + [524]     [525]february (1)
          + [526]     [527]january (3)

     * [528]     [529]2006 (58)
          + [530]     [531]december (3)
          + [532]     [533]november (1)
          + [534]     [535]october (1)
          + [536]     [537]september (4)
          + [538]     [539]august (3)
          + [540]     [541]july (1)
          + [542]     [543]june (4)
          + [544]     [545]may (5)
          + [546]     [547]april (7)
          + [548]     [549]march (8)
          + [550]     [551]february (8)
          + [552]     [553]january (13)

     * [554]     [555]2005 (62)
          + [556]     [557]december (12)
          + [558]     [559]november (12)
          + [560]     [561]october (10)
          + [562]     [563]september (15)
          + [564]     [565]august (13)

labels

     * [566]3d recognition
     * [567]abhinav gupta
     * [568]antonio torralba
     * [569]artificial intelligence
     * [570]cognitive science
     * [571]id161
     * [572]cvpr
     * [573]deep learning
     * [574]entrepreneurship
     * [575]future directions
     * [576]id114
     * [577]iccv
     * [578]image understanding
     * [579]matlab
     * [580]mit
     * [581]nips
     * [582]object recognition
     * [583]philosophy
     * [584]programming
     * [585]psychology
     * [586]scene understanding
     * [587]segmentation
     * [588]startups
     * [589]id166
     * [590]training
     * [591]visual memex
     * [592]vmx

   [593]page hit counter

follow by email

   ____________________ submit

subscribe to

   [arrow_dropdown.gif] posts
   [594][subscribe-netvibes.png] [595][subscribe-yahoo.png]
   [596][icon_feed12.png] atom
   [arrow_dropdown.gif] posts
   [arrow_dropdown.gif] all comments
   [597][subscribe-netvibes.png] [598][subscribe-yahoo.png]
   [599][icon_feed12.png] atom
   [arrow_dropdown.gif] all comments
   awesome inc. theme. powered by [600]blogger.

references

   visible links
   1. http://www.computervisionblog.com/feeds/posts/default
   2. http://www.computervisionblog.com/feeds/posts/default?alt=rss
   3. https://plus.google.com/107912691630546731185
   4. http://www.computervisionblog.com/2018/05/deepfakes-ai-powered-deception-machines.html
   5. https://2.bp.blogspot.com/-hya207hkaf4/wvvxnzxxtyi/aaaaaaaap1o/o1mguy4eotarbrocgmsu2-r_7ute5vtzqclcbgas/s1600/mind.png
   6. https://1.bp.blogspot.com/-yov9hezskzo/wvvd_o8kcsi/aaaaaaaap2a/8plia-dppdgmz6ncgp-ghsojmzg-cp12wclcbgas/s1600/faceforensics.png
   7. https://www.youtube.com/embed/ohmajjtcpnk
   8. https://homes.cs.washington.edu/~kemelmi/
   9. https://4.bp.blogspot.com/-zswrtgewhj8/wvvbi_w2lni/aaaaaaaap10/a_sptqou7asj7og3q-zvcsflelydm326wclcbgas/s1600/ira_early_deep_fake.png
  10. https://www.youtube.com/embed/millfk1rwhk
  11. https://2.bp.blogspot.com/-pljf2nhuuws/wvvuoynn5yi/aaaaaaaap20/d0m63ydr0zgt1ynvzadrft8yijfl23j0gclcbgas/s1600/transfiguring_portraits_deepfake.png
  12. https://4.bp.blogspot.com/-uzchrqaoi5a/wvwagqmurpi/aaaaaaaap3u/1wxhg6gbzsubrsuvclngsdj6rkqgdsw5aclcbgas/s1600/deepmask_deepfake_detection.png
  13. https://2.bp.blogspot.com/--5bawkzajdq/wvvizia5xdi/aaaaaaaap2y/4o2ibdtktfgajh3ud76yyryjnmabzku1qclcbgas/s1600/efros_fake_news.png
  14. https://web.stanford.edu/~zollhoef/papers/cvpr2016_face2face/paper.pdf
  15. http://grail.cs.washington.edu/projects/malkovich/
  16. https://arxiv.org/abs/1803.09179
  17. https://arxiv.org/abs/1805.04096
  18. https://homes.cs.washington.edu/~kemelmi/transfiguring_portraits_kemelmacher_siggraph2016.pdf
  19. https://plus.google.com/107912691630546731185
  20. http://www.computervisionblog.com/2018/05/deepfakes-ai-powered-deception-machines.html
  21. http://www.computervisionblog.com/2018/05/deepfakes-ai-powered-deception-machines.html#comment-form
  22. https://www.blogger.com/share-post.g?blogid=15418143&postid=7872786804655838317&target=email
  23. https://www.blogger.com/share-post.g?blogid=15418143&postid=7872786804655838317&target=blog
  24. https://www.blogger.com/share-post.g?blogid=15418143&postid=7872786804655838317&target=twitter
  25. https://www.blogger.com/share-post.g?blogid=15418143&postid=7872786804655838317&target=facebook
  26. https://www.blogger.com/share-post.g?blogid=15418143&postid=7872786804655838317&target=pinterest
  27. http://www.computervisionblog.com/search/label/alyosha efros
  28. http://www.computervisionblog.com/search/label/cvpr
  29. http://www.computervisionblog.com/search/label/deepfake
  30. http://www.computervisionblog.com/search/label/descartes
  31. http://www.computervisionblog.com/search/label/face detection
  32. http://www.computervisionblog.com/search/label/face transfer
  33. http://www.computervisionblog.com/search/label/face2face
  34. http://www.computervisionblog.com/search/label/fake news
  35. http://www.computervisionblog.com/search/label/gans
  36. http://www.computervisionblog.com/search/label/ira kemelmacher-shlizerman
  37. http://www.computervisionblog.com/search/label/justus thies
  38. http://www.computervisionblog.com/search/label/matthias niessner
  39. http://www.computervisionblog.com/search/label/realism
  40. http://www.computervisionblog.com/search/label/siggraph
  41. http://www.computervisionblog.com/search/label/snapchat
  42. http://www.computervisionblog.com/search/label/truth
  43. http://www.computervisionblog.com/search/label/visual forgery
  44. http://www.computervisionblog.com/2016/12/nuts-and-bolts-of-building-deep.html
  45. https://3.bp.blogspot.com/-ua-shnpwkcq/wfnia2-wvgi/aaaaaaaapsm/l4k304x-7dwhnehj7rplxnrxkczap6psqclcb/s1600/nuts_and_bolts_andrew_ng.png
  46. https://3.bp.blogspot.com/-duzbndyddga/wfntni0dcni/aaaaaaaapsc/ahuvdxl6ehagwed6ixgabqobk5qm_w05qclcb/s1600/nuts-and-bolts-checklist.png
  47. https://2.bp.blogspot.com/-kgcfhetvscc/wfnydtfcwwi/aaaaaaaapss/zt-8hxy0b6ahskqlw8evc2ymh7lol2pyqclcb/s1600/bias-variance-andrew-ng.png
  48. https://www.youtube.com/watch?v=f1ka6a13s9i
  49. https://kevinzakka.github.io/2016/09/26/applying-deep-learning/
  50. https://plus.google.com/107912691630546731185
  51. http://www.computervisionblog.com/2016/12/nuts-and-bolts-of-building-deep.html
  52. http://www.computervisionblog.com/2016/12/nuts-and-bolts-of-building-deep.html#comment-form
  53. https://www.blogger.com/share-post.g?blogid=15418143&postid=6547994887448818346&target=email
  54. https://www.blogger.com/share-post.g?blogid=15418143&postid=6547994887448818346&target=blog
  55. https://www.blogger.com/share-post.g?blogid=15418143&postid=6547994887448818346&target=twitter
  56. https://www.blogger.com/share-post.g?blogid=15418143&postid=6547994887448818346&target=facebook
  57. https://www.blogger.com/share-post.g?blogid=15418143&postid=6547994887448818346&target=pinterest
  58. http://www.computervisionblog.com/search/label/advice
  59. http://www.computervisionblog.com/search/label/andrew ng
  60. http://www.computervisionblog.com/search/label/bias-variance
  61. http://www.computervisionblog.com/search/label/deep learning
  62. http://www.computervisionblog.com/search/label/google
  63. http://www.computervisionblog.com/search/label/machine learning
  64. http://www.computervisionblog.com/search/label/nips 2016
  65. http://www.computervisionblog.com/search/label/research
  66. http://www.computervisionblog.com/search/label/supervised learning
  67. http://www.computervisionblog.com/search/label/synthesis
  68. http://www.computervisionblog.com/2016/06/making-deep-networks-probabilistic-via.html
  69. https://3.bp.blogspot.com/-ngrizxbkvr8/v2pfikundfi/aaaaaaaaozc/k91sxofkdskqvdgaltmcuyjckh33pbsjaclcb/s1600/interpretable_vs_deep_neural_networks.png
  70. http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html
  71. https://2.bp.blogspot.com/-t4hhbhjbvfe/v2jmj2nj_gi/aaaaaaaaoy0/_x5qrkosx447h-cmbsc1chx1nhb2ccitqclcb/s1600/brain_zap_neural_network_dropout.jpg
  72. https://www.youtube.com/embed/uckpdam8cni
  73. https://www.youtube.com/embed/g3kuvhx9gdy
  74. https://1.bp.blogspot.com/-dlcqj0a4h9i/v2cjfjikgti/aaaaaaaaowq/tmggcbukd7sy1leqsxfx6llhmqmb8ljpaclcb/s1600/bayesian_segnet_uncertainty_dropout.png
  75. https://arxiv.org/abs/1511.02680
  76. http://mi.eng.cam.ac.uk/projects/segnet/
  77. https://2.bp.blogspot.com/-y_qxwwrrcug/v2erx6carwi/aaaaaaaaoxk/uxssll_sssu8txodmli7q_rno0ehft9awclcb/s1600/gaussian_process_confidence_values.png
  78. http://mlg.eng.cam.ac.uk/yarin/
  79. http://mlg.eng.cam.ac.uk/zoubin/
  80. https://arxiv.org/abs/1506.02142
  81. https://arxiv.org/abs/1506.02157
  82. https://arxiv.org/abs/1512.05287
  83. http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html
  84. https://github.com/yaringal/heteroscedasticdropoutuncertainty
  85. https://4.bp.blogspot.com/-vwollbvk8da/v2j4ekoi9ai/aaaaaaaaoze/y4ayokkyzn0fvwjbssxz-fpnahtlpiavaclcb/s1600/black_box.png
  86. http://www.computervisionblog.com/2015/04/deep-learning-vs-probabilistic.html
  87. http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html
  88. https://plus.google.com/107912691630546731185
  89. http://www.computervisionblog.com/2016/06/making-deep-networks-probabilistic-via.html
  90. http://www.computervisionblog.com/2016/06/making-deep-networks-probabilistic-via.html#comment-form
  91. https://www.blogger.com/share-post.g?blogid=15418143&postid=8839595873640006183&target=email
  92. https://www.blogger.com/share-post.g?blogid=15418143&postid=8839595873640006183&target=blog
  93. https://www.blogger.com/share-post.g?blogid=15418143&postid=8839595873640006183&target=twitter
  94. https://www.blogger.com/share-post.g?blogid=15418143&postid=8839595873640006183&target=facebook
  95. https://www.blogger.com/share-post.g?blogid=15418143&postid=8839595873640006183&target=pinterest
  96. http://www.computervisionblog.com/search/label/arxiv
  97. http://www.computervisionblog.com/search/label/bayesian
  98. http://www.computervisionblog.com/search/label/confidence
  99. http://www.computervisionblog.com/search/label/deep learning
 100. http://www.computervisionblog.com/search/label/dropout
 101. http://www.computervisionblog.com/search/label/geoff hinton
 102. http://www.computervisionblog.com/search/label/hugo larochelle
 103. http://www.computervisionblog.com/search/label/icml
 104. http://www.computervisionblog.com/search/label/papers
 105. http://www.computervisionblog.com/search/label/segnet
 106. http://www.computervisionblog.com/search/label/uncertainty
 107. http://www.computervisionblog.com/search/label/yarin gal
 108. http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html
 109. http://yann.lecun.com/
 110. http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html
 111. https://3.bp.blogspot.com/-kvgtfewiq-0/v06d4m-6i7i/aaaaaaaaoqq/txxsfstfdpw8y3h9q3sirbbrvhxry5vfwclcb/s1600/deep_learning_machine_learning_conference_iclr_2016.png
 112. http://www.iclr.cc/doku.php?id=iclr2016:main
 113. http://www.cs.toronto.edu/~urtasun/
 114. http://www.cs.utoronto.ca/~fidler/
 115. http://www.cs.toronto.edu/~urtasun/
 116. http://videolectures.net/iclr2016_urtasun_incoporating_structure/
 117. http://www.cs.toronto.edu/~fidler/courses/tutorialcvpr15.jpg
 118. http://www.cs.toronto.edu/~fidler/3dscenetutorialcvpr15.html
 119. https://4.bp.blogspot.com/-tnt9yh6q1fk/vz4rwm1usdi/aaaaaaaaopk/nzzwdima0i8qqxmi8mzo69_y1de03j55wclcb/s1600/soccer_field.png
 120. http://arxiv.org/abs/1604.02715
 121. http://www.cs.toronto.edu/~namdar/
 122. http://arxiv.org/abs/1504.06852
 123. http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html
 124. https://3.bp.blogspot.com/-d2rhi4k0tkq/vz4stflnjhi/aaaaaaaaops/r03t3zfjroio_sl6ojupb0gv9jw6fvjqwclcb/s1600/optical_flow_raquel.png
 125. http://arxiv.org/abs/1604.01827
 126. http://homes.cs.washington.edu/~svlevine/
 127. http://videolectures.net/iclr2016_levine_deep_learning/
 128. https://1.bp.blogspot.com/-q3zcsqgstqy/vzusf9ydc0i/aaaaaaaaoos/cavlflniqsg2m7jqhpcg9mqv3erpuhytgclcb/s1600/robotarms.png
 129. https://www.cs.toronto.edu/~kriz/
 130. http://arxiv.org/abs/1603.02199
 131. http://karpathy.github.io/2016/05/31/rl/
 132. https://3.bp.blogspot.com/hyhk-utsvznbxzc_ub-rqrgyoghf4ia5dl4cuxspugwoyhxdd4yo9ckxsym7mdfxlgqg=w300
 133. http://www.movidius.com/solutions/machine-vision-algorithms/machine-learning
 134. https://cloudplatform.googleblog.com/2016/05/google-supercharges-machine-learning-tasks-with-custom-chip.html
 135. https://4.bp.blogspot.com/-h73q0mjpdzq/vzuh6aglbti/aaaaaaaaonq/df8uqn343zwre_vpwfnl0ccb_eqtzmmegclcb/s1600/deep_compress.png
 136. http://arxiv.org/abs/1510.00149
 137. http://videolectures.net/iclr2016_han_deep_compression/
 138. https://arxiv.org/abs/1510.03009
 139. http://arxiv.org/abs/1511.04561
 140. https://3.bp.blogspot.com/-8a73vkcm2wi/vzuefhxpkwi/aaaaaaaaomk/iu50qdsogx8hejfw_bumpdwdhsyybaoqqclcb/s1600/isola.png
 141. http://arxiv.org/pdf/1511.06811.pdf
 142. http://web.mit.edu/phillipi/
 143. https://1.bp.blogspot.com/-almdlo_icya/vzufbnybjvi/aaaaaaaaoms/nd7_ylebiwew22ppdfo1cupzil0puuaugclcb/s1600/isola2.png
 144. http://arxiv.org/abs/1502.03167
 145. https://arxiv.org/abs/1511.06856
 146. https://3.bp.blogspot.com/-anwn04qz7go/vzuhqd9oyai/aaaaaaaaone/wdc-pg1xrcqpn_bmvxalzx-f8y_mezqpgclcb/s1600/noise.png
 147. http://arxiv.org/abs/1511.06807
 148. http://arxiv.org/abs/1511.06306
 149. https://aaroncourville.wordpress.com/
 150. http://www.dmi.usherb.ca/~larocheh/index_en.html
 151. https://4.bp.blogspot.com/-x8a-5ffhjak/vz4p6mugm7i/aaaaaaaaopy/dhguts6alv8ra8bdaf7uie96gq-7fq80aclcb/s1600/dcn.png
 152. http://arxiv.org/abs/1511.07838
 153. http://arxiv.org/abs/1602.07261
 154. https://4.bp.blogspot.com/-t5ykn7kwxuk/vzuph1020ki/aaaaaaaaoou/agqstqjozfmpzwnc0t0le5zgehzbdqd4qclcb/s1600/rir.png
 155. https://arxiv.org/abs/1603.08029
 156. https://3.bp.blogspot.com/-gtahkirasim/vzuohfb5yii/aaaaaaaaoom/lqeaqntsn9sx1gk89ol4itzb0fxcplrhaclcb/s1600/magnet.png
 157. http://arxiv.org/abs/1511.05939
 158. https://1.bp.blogspot.com/-0lb4zlszsoo/vzui9fcyjoi/aaaaaaaaony/2fltptovynstjbdpbloiavwinu5b7g7raclcb/s1600/npi.png
 159. http://arxiv.org/abs/1511.06279
 160. https://2.bp.blogspot.com/-rphlusus8bq/vzujcpzpgii/aaaaaaaaonc/pqh4xkxy9_42k9ukiawh6ig13cuot58bqclcb/s1600/nram.png
 161. http://arxiv.org/abs/1511.06392
 162. https://2.bp.blogspot.com/-uemflkppdwg/vz4nr6mcqbi/aaaaaaaaopa/nudixovut6qrsyfnazo1dpfmhcvd30mjgclcb/s1600/segmentation.png
 163. http://arxiv.org/abs/1511.07386
 164. https://2.bp.blogspot.com/-yrire3-qqjk/vz4ohzsgxui/aaaaaaaaope/ahgwpsqdqe8vswq59kss2r1xb8bbhcvegclcb/s1600/decompnet.png
 165. http://arxiv.org/abs/1511.06449
 166. https://4.bp.blogspot.com/-2lqf23x7i7w/vz4opsvhzzi/aaaaaaaaopm/tnydep_1zlq7bmuw7_ndonucj0jjrqlogclcb/s1600/dilated_convolutions.png
 167. https://arxiv.org/abs/1511.07122
 168. http://yosinski.com/
 169. http://www.geometricintelligence.com/
 170. https://openai.com/
 171. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 172. https://3.bp.blogspot.com/-xuv5xuzwxss/vzuj8xc4n9i/aaaaaaaaonk/fvqx11nqdpc45ccfj-aijfghe2hhubaigclcb/s1600/jason.png
 173. http://arxiv.org/abs/1511.07543
 174. http://videolectures.net/iclr2016_yosinski_convergent_learning/
 175. https://2.bp.blogspot.com/-6uqar3bm4ew/vzukdehrhfi/aaaaaaaaons/z-meozwrtdwavbtw_t-ykquhnezchfobaclcb/s1600/karpathy.png
 176. https://arxiv.org/abs/1506.02078
 177. https://1.bp.blogspot.com/-zyh8fc4l0dg/vzullc5doji/aaaaaaaaon4/eycgir07zwyfdtbkgtexjksqmjuwnc4ywclcb/s1600/caruana.png
 178. https://arxiv.org/abs/1603.05691
 179. https://4.bp.blogspot.com/-bshx-uygcbw/vzulyyrek6i/aaaaaaaaooa/mgatubrhemw5yfvx89o1adhxcttpkhldgclcb/s1600/net2deepernet.png
 180. http://arxiv.org/abs/1511.05641
 181. http://videolectures.net/iclr2016_dyer_model_architecture/
 182. http://arxiv.org/abs/1508.00657
 183. https://1.bp.blogspot.com/-mztghepvtgc/vzurw9bbtmi/aaaaaaaaook/cuasxjkqaru8rcdn5sbx1xpz0xvba6xuwclcb/s1600/order.png
 184. http://arxiv.org/abs/1511.06361
 185. http://videolectures.net/iclr2016_vendrov_order_embeddings/
 186. https://4.bp.blogspot.com/-klhaxcyujtu/vzugls1xnri/aaaaaaaaom4/q4xi90vbgno8jgzdg87ijezbikkjrqumgclcb/s1600/mac.png
 187. http://arxiv.org/abs/1511.05879
 188. https://2.bp.blogspot.com/-_gd5f9tsbv8/vzugqxbphki/aaaaaaaaom8/w0umipyn8ya6mhke88khxhodzzhf4kuxgclcb/s1600/rss.png
 189. http://eprints.qut.edu.au/84931/1/rss15_placerec.pdf
 190. http://juxi.net/workshop/deep-learning-rss-2016/
 191. http://www.computervisionblog.com/2012/05/why-your-vision-lab-needs-reading-group.html
 192. http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html
 193. http://www.computervisionblog.com/2015/06/deep-down-rabbit-hole-cvpr-2015-and.html
 194. http://www.computervisionblog.com/2015/11/the-deep-learning-gold-rush-of-2015.html
 195. http://www.computervisionblog.com/2015/03/deep-learning-vs-machine-learning-vs.html
 196. http://www.computervisionblog.com/2015/04/deep-learning-vs-probabilistic.html
 197. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 198. http://www.recode.net/2015/7/15/11614684/ai-conspiracy-the-scientists-behind-deep-learning
 199. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 200. http://googleresearch.blogspot.com/2016/03/deep-learning-for-robots-learning-from.html
 201. https://plus.google.com/107912691630546731185
 202. http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html
 203. http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html#comment-form
 204. https://www.blogger.com/share-post.g?blogid=15418143&postid=5529343758137673151&target=email
 205. https://www.blogger.com/share-post.g?blogid=15418143&postid=5529343758137673151&target=blog
 206. https://www.blogger.com/share-post.g?blogid=15418143&postid=5529343758137673151&target=twitter
 207. https://www.blogger.com/share-post.g?blogid=15418143&postid=5529343758137673151&target=facebook
 208. https://www.blogger.com/share-post.g?blogid=15418143&postid=5529343758137673151&target=pinterest
 209. http://www.computervisionblog.com/search/label/deep calculators
 210. http://www.computervisionblog.com/search/label/deep compression
 211. http://www.computervisionblog.com/search/label/deep learning
 212. http://www.computervisionblog.com/search/label/deepmind
 213. http://www.computervisionblog.com/search/label/facebook
 214. http://www.computervisionblog.com/search/label/google
 215. http://www.computervisionblog.com/search/label/iclr
 216. http://www.computervisionblog.com/search/label/karpathy
 217. http://www.computervisionblog.com/search/label/lstm
 218. http://www.computervisionblog.com/search/label/metric learning
 219. http://www.computervisionblog.com/search/label/openai
 220. http://www.computervisionblog.com/search/label/resnet
 221. http://www.computervisionblog.com/search/label/id56s
 222. http://www.computervisionblog.com/search/label/robotics
 223. http://www.computervisionblog.com/search/label/tensorflow
 224. http://www.computervisionblog.com/search/label/urtasun
 225. http://www.computervisionblog.com/search/label/visualization
 226. http://www.computervisionblog.com/search/label/yann lecun
 227. http://www.computervisionblog.com/search/label/yoshua bengio
 228. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 229. http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html
 230. http://wp.doc.ic.ac.uk/thefutureofslam/programme/
 231. http://1.bp.blogspot.com/-3wndepdkhqw/vpoawv91xwi/aaaaaaaaocy/q6oxfwf14jw/s1600/slammies2.png
 232. http://openmvg.readthedocs.org/en/latest/_images/structurefrommotion.png
 233. http://openmvg.readthedocs.org/en/latest/
 234. http://graphics.cs.cmu.edu/courses/16-824-s15/index.html
 235. http://www.cs.cmu.edu/~hebert/geom.html
 236. http://www.cs.cornell.edu/~snavely/bundler/images/colosseum.jpg
 237. http://www.cs.cornell.edu/~snavely/bundler/
 238. http://www.cs.cornell.edu/~snavely/bundler/
 239. http://ceres-solver.org/
 240. http://www.robots.ox.ac.uk/~vgg/hzbook/code/
 241. http://spectrum.ieee.org/image/1948541
 242. http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/how-google-self-driving-car-works
 243. http://www.computervisionblog.com/2015/03/mobileyes-quest-to-put-deep-learning.html
 244. http://mappingignorance.org/2014/04/07/one-way-googles-cars-localize/
 245. http://www.doc.ic.ac.uk/~ajd/index.html
 246. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/slides_ajd.pdf
 247. http://www.computervisionblog.com/2015/05/dyson-360-eye-and-baidu-deep-learning.html
 248. http://www.cs.bris.ac.uk/research/vision/realtime/bmvctutorial/
 249. https://mitpress.mit.edu/books/robot-vision
 250. http://3.bp.blogspot.com/-oh94izlctla/vpwlswn_wei/aaaaaaaaodw/fkdbj8kqogm/s1600/robotvision-01.png
 251. http://www.robots.ox.ac.uk/~vgg/hzbook/
 252. http://www.probabilistic-robotics.org/
 253. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/slides_ajd.pdf
 254. http://vision.in.tum.de/members/kerl
 255. http://2.bp.blogspot.com/-vjaoqhdmtbg/vpodsbmvz5i/aaaaaaaaock/lbf_fqh5_em/s1600/kerl.png
 256. http://1.bp.blogspot.com/-gxml1jkplgs/vpoem6iosci/aaaaaaaaocs/yzlds-wemqm/s1600/shutter.png
 257. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/kerl_etal_iccv2015_futureofslam_talk.pdf
 258. http://vision.in.tum.de/_media/spezial/bib/kerl15iccv.pdf
 259. http://vision.in.tum.de/members/engelj
 260. http://4.bp.blogspot.com/-vh3gehisfky/vnl44gfst3i/aaaaaaaaobw/myun2v6_c4m/s1600/lsd-slam.png
 261. http://vision.in.tum.de/research/vslam/lsdslam
 262. https://github.com/tum-vision/lsd_slam
 263. http://vision.in.tum.de/research/vslam/lsdslam
 264. http://vision.in.tum.de/_media/spezial/bib/engel14eccv.pdf
 265. https://youtu.be/gnuqzp3gty4
 266. http://4.bp.blogspot.com/-wuowvgunruk/vpoe_93z_gi/aaaaaaaaoc4/4kuduertd80/s1600/omni.png
 267. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/iccv-slam-workshop_jakobengel.pdf
 268. http://vision.in.tum.de/_media/spezial/bib/caruso2015_omni_lsdslam.pdf
 269. https://youtu.be/v0nqmm7q6s8
 270. http://2.bp.blogspot.com/-076ah06nqno/vpwpi1ty0bi/aaaaaaaaod8/nvhykjmfoxu/s1600/stereo-lsd.png
 271. http://vision.in.tum.de/_media/spezial/bib/engel2015_stereo_lsdslam.pdf
 272. https://youtu.be/ojt3ln8h03s
 273. http://2.bp.blogspot.com/-e7fyrfwgeze/vpwru97d4oi/aaaaaaaaoei/wf4toryot88/s1600/quadrotor.png
 274. https://youtu.be/eznmokfqmpc
 275. http://1.bp.blogspot.com/-i2l2pgdiove/vpofbrplrhi/aaaaaaaaoda/mze1kpkil4m/s1600/feature-vs-direct.png
 276. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/iccv-slam-workshop_jakobengel.pdf
 277. https://www.graphics.rwth-aachen.de/person/21/
 278. http://4.bp.blogspot.com/-zxng4no_gme/vpofsvoweii/aaaaaaaaodi/l1pj_ah1ua4/s1600/mobileloc.png
 279. https://www.graphics.rwth-aachen.de/publication/213/eccv14_preprint.pdf
 280. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/sattler_challenges_large_scale_loc_and_mapping.pdf
 281. http://4.bp.blogspot.com/-vk0x1y6zfru/vpx4gxx7tpi/aaaaaaaaoe8/8olokl7idcw/s1600/types-of-slam.jpg
 282. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/iccv15_slamws_raulmur.pdf
 283. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/iccv15_slamws_raulmur.pdf
 284. http://webdiis.unizar.es/~raulmur/murmontieltardostro15.pdf
 285. http://github.com/raulmur/orb_slam
 286. http://webdiis.unizar.es/~raulmur/orbslam/
 287. https://www.google.com/atap/project-tango/
 288. https://3.bp.blogspot.com/-cyvkhspdkxg/v1q0jg-pvli/aaaaaaaaork/gukjttu-irsiqwwgaoq5zfghtk6wlxsiqclcb/s1600/google-project-tango-3d-mapping-video.jpeg
 289. http://2.bp.blogspot.com/-5q_eolijgwm/vpwumvdpcgi/aaaaaaaaoeu/oneayjx8f58/s1600/placeless.png
 290. https://3234f89137bccf2ede29cc86e315c75116020d70.googledrive.com/host/0b64gj60h3ai1mvvwwtzwekhtcfu/publications/bib/lynen_3dv14.pdf
 291. https://www.youtube.com/watch?v=hfwvwqrcwwa
 292. https://3234f89137bccf2ede29cc86e315c75116020d70.googledrive.com/host/0b64gj60h3ai1mvvwwtzwekhtcfu/publications/bib/lynen_3dv14.pdf
 293. https://www.youtube.com/watch?v=ip9m9a2ken4
 294. http://2.bp.blogspot.com/-w6wf-6un-vs/vpogvirvs9i/aaaaaaaaody/a6g-e0arosm/s1600/kintinuous.png
 295. http://wp.doc.ic.ac.uk/thefutureofslam/wp-content/uploads/sites/93/2015/12/elasticfusion.pdf
 296. http://www.doc.ic.ac.uk/~bglocker/pdfs/whelan2015rss.pdf
 297. http://homes.cs.washington.edu/~newcombe/papers/newcombe_etal_iccv2011.pdf
 298. http://homes.cs.washington.edu/~newcombe/papers/newcombe_etal_ismar2011.pdf
 299. http://grail.cs.washington.edu/projects/dynamicfusion/papers/dynamicfusion.pdf
 300. http://4.bp.blogspot.com/-s3mmc77onmm/vpww25sfsli/aaaaaaaaoeg/16-yjs3a-sc/s1600/dynamicfusion.png
 301. http://grail.cs.washington.edu/projects/dynamicfusion/papers/dynamicfusion.pdf
 302. https://www.youtube.com/watch?v=i1ezekcc_lm
 303. http://grail.cs.washington.edu/projects/dynamicfusion/papers/dynamicfusion.pdf
 304. http://grail.cs.washington.edu/projects/dynamicfusion/papers/dynamicfusion.pdf
 305. http://homes.cs.washington.edu/~newcombe/papers/salas-moreno_etal_cvpr2013.pdf
 306. http://homes.cs.washington.edu/~newcombe/papers/newcombe_etal_ismar2011.pdf
 307. http://3.bp.blogspot.com/-awqocykppqy/vpysrlbuf8i/aaaaaaaaofk/mbknnjh_yss/s1600/img_0500.jpg
 308. https://www.inf.ethz.ch/personal/marc.pollefeys/
 309. http://1.bp.blogspot.com/-gojolf_dwkq/vpohu7r_o4i/aaaaaaaaodg/vq85soenlbu/s1600/semantics.png
 310. https://www.doc.ic.ac.uk/~rfs09/docs/salas-moreno-r-2014-phd-thesis.pdf
 311. http://www.zeeshanzia.com/
 312. http://3.bp.blogspot.com/-sodinuq3bbg/vpygckrwjci/aaaaaaaaofm/2jbo1sny3a0/s1600/convnet_lecun_stereo.png
 313. http://arxiv.org/abs/1409.4326
 314. http://wordpress.viu.ca/ciel/files/2013/01/134992626.jpg
 315. http://wordpress.viu.ca/ciel/2013/01/23/gaming-and-student-disengagement/
 316. https://plus.google.com/107912691630546731185
 317. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 318. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html#comment-form
 319. https://www.blogger.com/share-post.g?blogid=15418143&postid=2190638093839413385&target=email
 320. https://www.blogger.com/share-post.g?blogid=15418143&postid=2190638093839413385&target=blog
 321. https://www.blogger.com/share-post.g?blogid=15418143&postid=2190638093839413385&target=twitter
 322. https://www.blogger.com/share-post.g?blogid=15418143&postid=2190638093839413385&target=facebook
 323. https://www.blogger.com/share-post.g?blogid=15418143&postid=2190638093839413385&target=pinterest
 324. http://www.computervisionblog.com/search/label/andrew davison
 325. http://www.computervisionblog.com/search/label/bundle adjustment
 326. http://www.computervisionblog.com/search/label/dtam
 327. http://www.computervisionblog.com/search/label/dynamicfusion
 328. http://www.computervisionblog.com/search/label/iccv 2015
 329. http://www.computervisionblog.com/search/label/jakob engel
 330. http://www.computervisionblog.com/search/label/kinectfusion
 331. http://www.computervisionblog.com/search/label/lsd-slam
 332. http://www.computervisionblog.com/search/label/marc pollefeys
 333. http://www.computervisionblog.com/search/label/pose
 334. http://www.computervisionblog.com/search/label/ptam
 335. http://www.computervisionblog.com/search/label/real-time
 336. http://www.computervisionblog.com/search/label/richard newcombe
 337. http://www.computervisionblog.com/search/label/robotics
 338. http://www.computervisionblog.com/search/label/segmentation
 339. http://www.computervisionblog.com/search/label/sfm
 340. http://www.computervisionblog.com/search/label/slam
 341. http://www.computervisionblog.com/search/label/workshop
 342. http://www.computervisionblog.com/search/label/zisserman
 343. http://www.computervisionblog.com/search?updated-max=2016-01-13t04:20:00-05:00&max-results=7
 344. http://www.computervisionblog.com/
 345. http://www.computervisionblog.com/feeds/posts/default
 346. http://www.computervisionblog.com/2015/03/deep-learning-vs-machine-learning-vs.html
 347. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 348. http://www.computervisionblog.com/2015/01/from-feature-descriptors-to-deep.html
 349. http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html
 350. http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html
 351. http://www.computervisionblog.com/2015/04/deep-learning-vs-probabilistic.html
 352. http://www.computervisionblog.com/2014/01/can-person-specific-face-recognition.html
 353. http://helplogger.blogspot.com/2014/11/5-cool-recent-post-widgets-for-blogger.html
 354. http://people.csail.mit.edu/tomasz/
 355. http://scholar.google.com/citations?user=rctetv0aaaaj&hl=en
 356. https://github.com/quantombone
 357. javascript:void(0)
 358. http://www.computervisionblog.com/2018/
 359. javascript:void(0)
 360. http://www.computervisionblog.com/2018/05/
 361. http://www.computervisionblog.com/2018/05/deepfakes-ai-powered-deception-machines.html
 362. javascript:void(0)
 363. http://www.computervisionblog.com/2016/
 364. javascript:void(0)
 365. http://www.computervisionblog.com/2016/12/
 366. javascript:void(0)
 367. http://www.computervisionblog.com/2016/06/
 368. javascript:void(0)
 369. http://www.computervisionblog.com/2016/01/
 370. javascript:void(0)
 371. http://www.computervisionblog.com/2015/
 372. javascript:void(0)
 373. http://www.computervisionblog.com/2015/12/
 374. javascript:void(0)
 375. http://www.computervisionblog.com/2015/11/
 376. javascript:void(0)
 377. http://www.computervisionblog.com/2015/06/
 378. javascript:void(0)
 379. http://www.computervisionblog.com/2015/05/
 380. javascript:void(0)
 381. http://www.computervisionblog.com/2015/04/
 382. javascript:void(0)
 383. http://www.computervisionblog.com/2015/03/
 384. javascript:void(0)
 385. http://www.computervisionblog.com/2015/01/
 386. javascript:void(0)
 387. http://www.computervisionblog.com/2014/
 388. javascript:void(0)
 389. http://www.computervisionblog.com/2014/11/
 390. javascript:void(0)
 391. http://www.computervisionblog.com/2014/10/
 392. javascript:void(0)
 393. http://www.computervisionblog.com/2014/01/
 394. javascript:void(0)
 395. http://www.computervisionblog.com/2013/
 396. javascript:void(0)
 397. http://www.computervisionblog.com/2013/12/
 398. javascript:void(0)
 399. http://www.computervisionblog.com/2013/10/
 400. javascript:void(0)
 401. http://www.computervisionblog.com/2013/09/
 402. javascript:void(0)
 403. http://www.computervisionblog.com/2013/07/
 404. javascript:void(0)
 405. http://www.computervisionblog.com/2013/06/
 406. javascript:void(0)
 407. http://www.computervisionblog.com/2013/04/
 408. javascript:void(0)
 409. http://www.computervisionblog.com/2012/
 410. javascript:void(0)
 411. http://www.computervisionblog.com/2012/07/
 412. javascript:void(0)
 413. http://www.computervisionblog.com/2012/06/
 414. javascript:void(0)
 415. http://www.computervisionblog.com/2012/05/
 416. javascript:void(0)
 417. http://www.computervisionblog.com/2012/04/
 418. javascript:void(0)
 419. http://www.computervisionblog.com/2012/03/
 420. javascript:void(0)
 421. http://www.computervisionblog.com/2012/01/
 422. javascript:void(0)
 423. http://www.computervisionblog.com/2011/
 424. javascript:void(0)
 425. http://www.computervisionblog.com/2011/12/
 426. javascript:void(0)
 427. http://www.computervisionblog.com/2011/11/
 428. javascript:void(0)
 429. http://www.computervisionblog.com/2011/10/
 430. javascript:void(0)
 431. http://www.computervisionblog.com/2011/09/
 432. javascript:void(0)
 433. http://www.computervisionblog.com/2011/08/
 434. javascript:void(0)
 435. http://www.computervisionblog.com/2011/07/
 436. javascript:void(0)
 437. http://www.computervisionblog.com/2011/06/
 438. javascript:void(0)
 439. http://www.computervisionblog.com/2011/04/
 440. javascript:void(0)
 441. http://www.computervisionblog.com/2011/03/
 442. javascript:void(0)
 443. http://www.computervisionblog.com/2011/01/
 444. javascript:void(0)
 445. http://www.computervisionblog.com/2010/
 446. javascript:void(0)
 447. http://www.computervisionblog.com/2010/12/
 448. javascript:void(0)
 449. http://www.computervisionblog.com/2010/11/
 450. javascript:void(0)
 451. http://www.computervisionblog.com/2010/08/
 452. javascript:void(0)
 453. http://www.computervisionblog.com/2010/06/
 454. javascript:void(0)
 455. http://www.computervisionblog.com/2010/05/
 456. javascript:void(0)
 457. http://www.computervisionblog.com/2010/04/
 458. javascript:void(0)
 459. http://www.computervisionblog.com/2010/03/
 460. javascript:void(0)
 461. http://www.computervisionblog.com/2010/02/
 462. javascript:void(0)
 463. http://www.computervisionblog.com/2010/01/
 464. javascript:void(0)
 465. http://www.computervisionblog.com/2009/
 466. javascript:void(0)
 467. http://www.computervisionblog.com/2009/12/
 468. javascript:void(0)
 469. http://www.computervisionblog.com/2009/11/
 470. javascript:void(0)
 471. http://www.computervisionblog.com/2009/10/
 472. javascript:void(0)
 473. http://www.computervisionblog.com/2009/09/
 474. javascript:void(0)
 475. http://www.computervisionblog.com/2009/08/
 476. javascript:void(0)
 477. http://www.computervisionblog.com/2009/07/
 478. javascript:void(0)
 479. http://www.computervisionblog.com/2009/06/
 480. javascript:void(0)
 481. http://www.computervisionblog.com/2009/03/
 482. javascript:void(0)
 483. http://www.computervisionblog.com/2009/02/
 484. javascript:void(0)
 485. http://www.computervisionblog.com/2009/01/
 486. javascript:void(0)
 487. http://www.computervisionblog.com/2008/
 488. javascript:void(0)
 489. http://www.computervisionblog.com/2008/12/
 490. javascript:void(0)
 491. http://www.computervisionblog.com/2008/11/
 492. javascript:void(0)
 493. http://www.computervisionblog.com/2008/10/
 494. javascript:void(0)
 495. http://www.computervisionblog.com/2008/09/
 496. javascript:void(0)
 497. http://www.computervisionblog.com/2008/08/
 498. javascript:void(0)
 499. http://www.computervisionblog.com/2008/07/
 500. javascript:void(0)
 501. http://www.computervisionblog.com/2008/06/
 502. javascript:void(0)
 503. http://www.computervisionblog.com/2008/05/
 504. javascript:void(0)
 505. http://www.computervisionblog.com/2008/04/
 506. javascript:void(0)
 507. http://www.computervisionblog.com/2008/03/
 508. javascript:void(0)
 509. http://www.computervisionblog.com/2008/02/
 510. javascript:void(0)
 511. http://www.computervisionblog.com/2007/
 512. javascript:void(0)
 513. http://www.computervisionblog.com/2007/09/
 514. javascript:void(0)
 515. http://www.computervisionblog.com/2007/08/
 516. javascript:void(0)
 517. http://www.computervisionblog.com/2007/07/
 518. javascript:void(0)
 519. http://www.computervisionblog.com/2007/06/
 520. javascript:void(0)
 521. http://www.computervisionblog.com/2007/04/
 522. javascript:void(0)
 523. http://www.computervisionblog.com/2007/03/
 524. javascript:void(0)
 525. http://www.computervisionblog.com/2007/02/
 526. javascript:void(0)
 527. http://www.computervisionblog.com/2007/01/
 528. javascript:void(0)
 529. http://www.computervisionblog.com/2006/
 530. javascript:void(0)
 531. http://www.computervisionblog.com/2006/12/
 532. javascript:void(0)
 533. http://www.computervisionblog.com/2006/11/
 534. javascript:void(0)
 535. http://www.computervisionblog.com/2006/10/
 536. javascript:void(0)
 537. http://www.computervisionblog.com/2006/09/
 538. javascript:void(0)
 539. http://www.computervisionblog.com/2006/08/
 540. javascript:void(0)
 541. http://www.computervisionblog.com/2006/07/
 542. javascript:void(0)
 543. http://www.computervisionblog.com/2006/06/
 544. javascript:void(0)
 545. http://www.computervisionblog.com/2006/05/
 546. javascript:void(0)
 547. http://www.computervisionblog.com/2006/04/
 548. javascript:void(0)
 549. http://www.computervisionblog.com/2006/03/
 550. javascript:void(0)
 551. http://www.computervisionblog.com/2006/02/
 552. javascript:void(0)
 553. http://www.computervisionblog.com/2006/01/
 554. javascript:void(0)
 555. http://www.computervisionblog.com/2005/
 556. javascript:void(0)
 557. http://www.computervisionblog.com/2005/12/
 558. javascript:void(0)
 559. http://www.computervisionblog.com/2005/11/
 560. javascript:void(0)
 561. http://www.computervisionblog.com/2005/10/
 562. javascript:void(0)
 563. http://www.computervisionblog.com/2005/09/
 564. javascript:void(0)
 565. http://www.computervisionblog.com/2005/08/
 566. http://www.computervisionblog.com/search/label/3d recognition
 567. http://www.computervisionblog.com/search/label/abhinav gupta
 568. http://www.computervisionblog.com/search/label/antonio torralba
 569. http://www.computervisionblog.com/search/label/artificial intelligence
 570. http://www.computervisionblog.com/search/label/cognitive science
 571. http://www.computervisionblog.com/search/label/id161
 572. http://www.computervisionblog.com/search/label/cvpr
 573. http://www.computervisionblog.com/search/label/deep learning
 574. http://www.computervisionblog.com/search/label/entrepreneurship
 575. http://www.computervisionblog.com/search/label/future directions
 576. http://www.computervisionblog.com/search/label/id114
 577. http://www.computervisionblog.com/search/label/iccv
 578. http://www.computervisionblog.com/search/label/image understanding
 579. http://www.computervisionblog.com/search/label/matlab
 580. http://www.computervisionblog.com/search/label/mit
 581. http://www.computervisionblog.com/search/label/nips
 582. http://www.computervisionblog.com/search/label/object recognition
 583. http://www.computervisionblog.com/search/label/philosophy
 584. http://www.computervisionblog.com/search/label/programming
 585. http://www.computervisionblog.com/search/label/psychology
 586. http://www.computervisionblog.com/search/label/scene understanding
 587. http://www.computervisionblog.com/search/label/segmentation
 588. http://www.computervisionblog.com/search/label/startups
 589. http://www.computervisionblog.com/search/label/id166
 590. http://www.computervisionblog.com/search/label/training
 591. http://www.computervisionblog.com/search/label/visual memex
 592. http://www.computervisionblog.com/search/label/vmx
 593. http://www.statcounter.com/
 594. https://www.netvibes.com/subscribe.php?url=http://www.computervisionblog.com/feeds/posts/default
 595. https://add.my.yahoo.com/content?url=http://www.computervisionblog.com/feeds/posts/default
 596. http://www.computervisionblog.com/feeds/posts/default
 597. https://www.netvibes.com/subscribe.php?url=http://www.computervisionblog.com/feeds/comments/default
 598. https://add.my.yahoo.com/content?url=http://www.computervisionblog.com/feeds/comments/default
 599. http://www.computervisionblog.com/feeds/comments/default
 600. https://www.blogger.com/

   hidden links:
 602. https://www.blogger.com/email-post.g?blogid=15418143&postid=7872786804655838317
 603. https://www.blogger.com/post-edit.g?blogid=15418143&postid=7872786804655838317&from=pencil
 604. https://www.blogger.com/email-post.g?blogid=15418143&postid=6547994887448818346
 605. https://www.blogger.com/post-edit.g?blogid=15418143&postid=6547994887448818346&from=pencil
 606. https://arxiv.org/abs/1506.02142
 607. https://www.blogger.com/email-post.g?blogid=15418143&postid=8839595873640006183
 608. https://www.blogger.com/post-edit.g?blogid=15418143&postid=8839595873640006183&from=pencil
 609. http://arxiv.org/pdf/1511.06811.pdf
 610. http://arxiv.org/abs/1511.07543
 611. http://arxiv.org/abs/1511.06361
 612. https://www.blogger.com/email-post.g?blogid=15418143&postid=5529343758137673151
 613. https://www.blogger.com/post-edit.g?blogid=15418143&postid=5529343758137673151&from=pencil
 614. https://www.blogger.com/email-post.g?blogid=15418143&postid=2190638093839413385
 615. https://www.blogger.com/post-edit.g?blogid=15418143&postid=2190638093839413385&from=pencil
 616. http://www.computervisionblog.com/2015/03/deep-learning-vs-machine-learning-vs.html
 617. http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html
 618. http://www.computervisionblog.com/2015/01/from-feature-descriptors-to-deep.html
 619. http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html
 620. http://www.computervisionblog.com/2015/12/iccv-2015-twenty-one-hottest-research.html
 621. http://www.computervisionblog.com/2015/04/deep-learning-vs-probabilistic.html
 622. http://www.computervisionblog.com/2014/01/can-person-specific-face-recognition.html
 623. http://www.blogger.com/rearrange?blogid=15418143&widgettype=popularposts&widgetid=popularposts2&action=editwidget&sectionid=sidebar-right-1
 624. http://www.blogger.com/rearrange?blogid=15418143&widgettype=html&widgetid=html2&action=editwidget&sectionid=sidebar-right-1
 625. http://www.blogger.com/rearrange?blogid=15418143&widgettype=linklist&widgetid=linklist1&action=editwidget&sectionid=sidebar-right-1
 626. http://www.blogger.com/rearrange?blogid=15418143&widgettype=blogarchive&widgetid=blogarchive1&action=editwidget&sectionid=sidebar-right-1
 627. http://www.blogger.com/rearrange?blogid=15418143&widgettype=label&widgetid=label2&action=editwidget&sectionid=sidebar-right-1
 628. http://www.blogger.com/rearrange?blogid=15418143&widgettype=html&widgetid=html1&action=editwidget&sectionid=footer-1
 629. http://www.blogger.com/rearrange?blogid=15418143&widgettype=followbyemail&widgetid=followbyemail1&action=editwidget&sectionid=footer-2-2
 630. http://www.blogger.com/rearrange?blogid=15418143&widgettype=subscribe&widgetid=subscribe1&action=editwidget&sectionid=footer-2-2
 631. http://www.blogger.com/rearrange?blogid=15418143&widgettype=attribution&widgetid=attribution1&action=editwidget&sectionid=footer-3
