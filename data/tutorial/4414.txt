   #[1]search [2]2.3. id91 [3]2.1. gaussian mixture models

   [4]logo
     * [5]home
     * [6]installation
     * [7]documentation
          + scikit-learn
          + [8]tutorials
          + [9]user guide
          + [10]api
          + [11]glossary
          + [12]faq
          + [13]development
          + [14]roadmap
          +
     * [15]all available versions
     * [16]pdf documentation

     [17]examples

   [18]fork me on github

   [19]previous
   2.1. gaussian... 2.1. gaussian mixture models

   [20]next
   2.3. id91 2.3. id91

   [21]up
   2. unsupervis... 2. unsupervised learning

   scikit-learn v0.20.3
   [22]other versions

   please [23]cite us if you use the software.
     * [24]2.2. manifold learning
          + [25]2.2.1. introduction
          + [26]2.2.2. isomap
               o [27]2.2.2.1. complexity
          + [28]2.2.3. locally linear embedding
               o [29]2.2.3.1. complexity
          + [30]2.2.4. modified locally linear embedding
               o [31]2.2.4.1. complexity
          + [32]2.2.5. hessian eigenmapping
               o [33]2.2.5.1. complexity
          + [34]2.2.6. spectral embedding
               o [35]2.2.6.1. complexity
          + [36]2.2.7. local tangent space alignment
               o [37]2.2.7.1. complexity
          + [38]2.2.8. multi-dimensional scaling (mds)
               o [39]2.2.8.1. metric mds
               o [40]2.2.8.2. nonmetric mds
          + [41]2.2.9. t-distributed stochastic neighbor embedding (id167)
               o [42]2.2.9.1. optimizing id167
               o [43]2.2.9.2. barnes-hut id167
          + [44]2.2.10. tips on practical use

   [x]

2.2. manifold learning[45]  

   look for the bare necessities
   the simple bare necessities
   forget about your worries and your strife
   i mean the bare necessities
   old mother nature   s recipes
   that bring the bare necessities of life
       baloo   s song [the jungle book]
   [46]../_images/sphx_glr_plot_compare_methods_0011.png

   manifold learning is an approach to non-linear dimensionality
   reduction. algorithms for this task are based on the idea that the
   dimensionality of many data sets is only artificially high.

2.2.1. introduction[47]  

   high-dimensional datasets can be very difficult to visualize. while
   data in two or three dimensions can be plotted to show the inherent
   structure of the data, equivalent high-dimensional plots are much less
   intuitive. to aid visualization of the structure of a dataset, the
   dimension must be reduced in some way.

   the simplest way to accomplish this id84 is by
   taking a random projection of the data. though this allows some degree
   of visualization of the data structure, the randomness of the choice
   leaves much to be desired. in a random projection, it is likely that
   the more interesting structure within the data will be lost.

   [48]digits_img [49]projected_img

   to address this concern, a number of supervised and unsupervised linear
   id84 frameworks have been designed, such as
   principal component analysis (pca), independent component analysis,
   id156, and others. these algorithms define
   specific rubrics to choose an    interesting    linear projection of the
   data. these methods can be powerful, but often miss important
   non-linear structure in the data.

   [50]pca_img [51]lda_img

   manifold learning can be thought of as an attempt to generalize linear
   frameworks like pca to be sensitive to non-linear structure in data.
   though supervised variants exist, the typical manifold learning problem
   is unsupervised: it learns the high-dimensional structure of the data
   from the data itself, without the use of predetermined classifications.

   examples:
     * see [52]manifold learning on handwritten digits: locally linear
       embedding, isomap    for an example of id84 on
       handwritten digits.
     * see [53]comparison of manifold learning methods for an example of
       id84 on a toy    s-curve    dataset.

   the manifold learning implementations available in scikit-learn are
   summarized below

2.2.2. isomap[54]  

   one of the earliest approaches to manifold learning is the isomap
   algorithm, short for isometric mapping. isomap can be viewed as an
   extension of multi-dimensional scaling (mds) or kernel pca. isomap
   seeks a lower-dimensional embedding which maintains geodesic distances
   between all points. isomap can be performed with the object [55]isomap.
   [56]../_images/sphx_glr_plot_lle_digits_0051.png

2.2.2.1. complexity[57]  

   the isomap algorithm comprises three stages:
    1. nearest neighbor search. isomap uses [58]sklearn.neighbors.balltree
       for efficient neighbor search. the cost is approximately \(o[d
       \log(k) n \log(n)]\), for \(k\) nearest neighbors of \(n\) points
       in \(d\) dimensions.
    2. shortest-path graph search. the most efficient known algorithms for
       this are dijkstra   s algorithm, which is approximately \(o[n^2(k +
       \log(n))]\), or the floyd-warshall algorithm, which is \(o[n^3]\).
       the algorithm can be selected by the user with the path_method
       keyword of isomap. if unspecified, the code attempts to choose the
       best algorithm for the input data.
    3. partial eigenvalue decomposition. the embedding is encoded in the
       eigenvectors corresponding to the \(d\) largest eigenvalues of the
       \(n \times n\) isomap kernel. for a dense solver, the cost is
       approximately \(o[d n^2]\). this cost can often be improved using
       the arpack solver. the eigensolver can be specified by the user
       with the path_method keyword of isomap. if unspecified, the code
       attempts to choose the best algorithm for the input data.

   the overall complexity of isomap is \(o[d \log(k) n \log(n)] + o[n^2(k
   + \log(n))] + o[d n^2]\).
     * \(n\) : number of training data points
     * \(d\) : input dimension
     * \(k\) : number of nearest neighbors
     * \(d\) : output dimension

   references:
     * [59]   a global geometric framework for nonlinear dimensionality
       reduction    tenenbaum, j.b.; de silva, v.; & langford, j.c. science
       290 (5500)

2.2.3. locally linear embedding[60]  

   locally linear embedding (lle) seeks a lower-dimensional projection of
   the data which preserves distances within local neighborhoods. it can
   be thought of as a series of local principal component analyses which
   are globally compared to find the best non-linear embedding.

   locally linear embedding can be performed with function
   [61]locally_linear_embedding or its object-oriented counterpart
   [62]locallylinearembedding.
   [63]../_images/sphx_glr_plot_lle_digits_0061.png

2.2.3.1. complexity[64]  

   the standard lle algorithm comprises three stages:
    1. nearest neighbors search. see discussion under isomap above.
    2. weight matrix construction. \(o[d n k^3]\). the construction of the
       lle weight matrix involves the solution of a \(k \times k\) linear
       equation for each of the \(n\) local neighborhoods
    3. partial eigenvalue decomposition. see discussion under isomap
       above.

   the overall complexity of standard lle is \(o[d \log(k) n \log(n)] +
   o[d n k^3] + o[d n^2]\).
     * \(n\) : number of training data points
     * \(d\) : input dimension
     * \(k\) : number of nearest neighbors
     * \(d\) : output dimension

   references:
     * [65]   nonlinear id84 by locally linear
       embedding    roweis, s. & saul, l. science 290:2323 (2000)

2.2.4. modified locally linear embedding[66]  

   one well-known issue with lle is the id173 problem. when the
   number of neighbors is greater than the number of input dimensions, the
   matrix defining each local neighborhood is rank-deficient. to address
   this, standard lle applies an arbitrary id173 parameter \(r\),
   which is chosen relative to the trace of the local weight matrix.
   though it can be shown formally that as \(r \to 0\), the solution
   converges to the desired embedding, there is no guarantee that the
   optimal solution will be found for \(r > 0\). this problem manifests
   itself in embeddings which distort the underlying geometry of the
   manifold.

   one method to address the id173 problem is to use multiple
   weight vectors in each neighborhood. this is the essence of modified
   locally linear embedding (mlle). mlle can be performed with function
   [67]locally_linear_embedding or its object-oriented counterpart
   [68]locallylinearembedding, with the keyword method = 'modified'. it
   requires n_neighbors > n_components.
   [69]../_images/sphx_glr_plot_lle_digits_0071.png

2.2.4.1. complexity[70]  

   the mlle algorithm comprises three stages:
    1. nearest neighbors search. same as standard lle
    2. weight matrix construction. approximately \(o[d n k^3] + o[n (k-d)
       k^2]\). the first term is exactly equivalent to that of standard
       lle. the second term has to do with constructing the weight matrix
       from multiple weights. in practice, the added cost of constructing
       the mlle weight matrix is relatively small compared to the cost of
       steps 1 and 3.
    3. partial eigenvalue decomposition. same as standard lle

   the overall complexity of mlle is \(o[d \log(k) n \log(n)] + o[d n k^3]
   + o[n (k-d) k^2] + o[d n^2]\).
     * \(n\) : number of training data points
     * \(d\) : input dimension
     * \(k\) : number of nearest neighbors
     * \(d\) : output dimension

   references:
     * [71]   mlle: modified locally linear embedding using multiple
       weights    zhang, z. & wang, j.

2.2.5. hessian eigenmapping[72]  

   hessian eigenmapping (also known as hessian-based lle: hlle) is another
   method of solving the id173 problem of lle. it revolves around
   a hessian-based quadratic form at each neighborhood which is used to
   recover the locally linear structure. though other implementations note
   its poor scaling with data size, sklearn implements some algorithmic
   improvements which make its cost comparable to that of other lle
   variants for small output dimension. hlle can be performed with
   function [73]locally_linear_embedding or its object-oriented
   counterpart [74]locallylinearembedding, with the keyword method =
   'hessian'. it requires n_neighbors > n_components * (n_components + 3)
   / 2.
   [75]../_images/sphx_glr_plot_lle_digits_0081.png

2.2.5.1. complexity[76]  

   the hlle algorithm comprises three stages:
    1. nearest neighbors search. same as standard lle
    2. weight matrix construction. approximately \(o[d n k^3] + o[n
       d^6]\). the first term reflects a similar cost to that of standard
       lle. the second term comes from a qr decomposition of the local
       hessian estimator.
    3. partial eigenvalue decomposition. same as standard lle

   the overall complexity of standard hlle is \(o[d \log(k) n \log(n)] +
   o[d n k^3] + o[n d^6] + o[d n^2]\).
     * \(n\) : number of training data points
     * \(d\) : input dimension
     * \(k\) : number of nearest neighbors
     * \(d\) : output dimension

   references:
     * [77]   hessian eigenmaps: locally linear embedding techniques for
       high-dimensional data    donoho, d. & grimes, c. proc natl acad sci
       usa. 100:5591 (2003)

2.2.6. spectral embedding[78]  

   spectral embedding is an approach to calculating a non-linear
   embedding. scikit-learn implements laplacian eigenmaps, which finds a
   low dimensional representation of the data using a spectral
   decomposition of the graph laplacian. the graph generated can be
   considered as a discrete approximation of the low dimensional manifold
   in the high dimensional space. minimization of a cost function based on
   the graph ensures that points close to each other on the manifold are
   mapped close to each other in the low dimensional space, preserving
   local distances. spectral embedding can be performed with the function
   [79]spectral_embedding or its object-oriented counterpart
   [80]spectralembedding.

2.2.6.1. complexity[81]  

   the spectral embedding (laplacian eigenmaps) algorithm comprises three
   stages:
    1. weighted graph construction. transform the raw input data into
       graph representation using affinity (adjacency) matrix
       representation.
    2. graph laplacian construction. unnormalized graph laplacian is
       constructed as \(l = d - a\) for and normalized one as \(l =
       d^{-\frac{1}{2}} (d - a) d^{-\frac{1}{2}}\).
    3. partial eigenvalue decomposition. eigenvalue decomposition is done
       on graph laplacian

   the overall complexity of spectral embedding is \(o[d \log(k) n
   \log(n)] + o[d n k^3] + o[d n^2]\).
     * \(n\) : number of training data points
     * \(d\) : input dimension
     * \(k\) : number of nearest neighbors
     * \(d\) : output dimension

   references:
     * [82]   laplacian eigenmaps for id84 and data
       representation    m. belkin, p. niyogi, neural computation, june
       2003; 15 (6):1373-1396

2.2.7. local tangent space alignment[83]  

   though not technically a variant of lle, local tangent space alignment
   (ltsa) is algorithmically similar enough to lle that it can be put in
   this category. rather than focusing on preserving neighborhood
   distances as in lle, ltsa seeks to characterize the local geometry at
   each neighborhood via its tangent space, and performs a global
   optimization to align these local tangent spaces to learn the
   embedding. ltsa can be performed with function
   [84]locally_linear_embedding or its object-oriented counterpart
   [85]locallylinearembedding, with the keyword method = 'ltsa'.
   [86]../_images/sphx_glr_plot_lle_digits_0091.png

2.2.7.1. complexity[87]  

   the ltsa algorithm comprises three stages:
    1. nearest neighbors search. same as standard lle
    2. weight matrix construction. approximately \(o[d n k^3] + o[k^2
       d]\). the first term reflects a similar cost to that of standard
       lle.
    3. partial eigenvalue decomposition. same as standard lle

   the overall complexity of standard ltsa is \(o[d \log(k) n \log(n)] +
   o[d n k^3] + o[k^2 d] + o[d n^2]\).
     * \(n\) : number of training data points
     * \(d\) : input dimension
     * \(k\) : number of nearest neighbors
     * \(d\) : output dimension

   references:
     * [88]   principal manifolds and nonlinear id84 via
       tangent space alignment    zhang, z. & zha, h. journal of shanghai
       univ. 8:406 (2004)

2.2.8. multi-dimensional scaling (mds)[89]  

   [90]multidimensional scaling ([91]mds) seeks a low-dimensional
   representation of the data in which the distances respect well the
   distances in the original high-dimensional space.

   in general, is a technique used for analyzing similarity or
   dissimilarity data. [92]mds attempts to model similarity or
   dissimilarity data as distances in a geometric spaces. the data can be
   ratings of similarity between objects, interaction frequencies of
   molecules, or trade indices between countries.

   there exists two types of mds algorithm: metric and non metric. in the
   scikit-learn, the class [93]mds implements both. in metric mds, the
   input similarity matrix arises from a metric (and thus respects the
   triangular inequality), the distances between output two points are
   then set to be as close as possible to the similarity or dissimilarity
   data. in the non-metric version, the algorithms will try to preserve
   the order of the distances, and hence seek for a monotonic relationship
   between the distances in the embedded space and the
   similarities/dissimilarities.
   [94]../_images/sphx_glr_plot_lle_digits_0101.png

   let \(s\) be the similarity matrix, and \(x\) the coordinates of the
   \(n\) input points. disparities \(\hat{d}_{ij}\) are transformation of
   the similarities chosen in some optimal ways. the objective, called the
   stress, is then defined by \(sum_{i < j} d_{ij}(x) - \hat{d}_{ij}(x)\)

2.2.8.1. metric mds[95]  

   the simplest metric [96]mds model, called absolute mds, disparities are
   defined by \(\hat{d}_{ij} = s_{ij}\). with absolute mds, the value
   \(s_{ij}\) should then correspond exactly to the distance between point
   \(i\) and \(j\) in the embedding point.

   most commonly, disparities are set to \(\hat{d}_{ij} = b s_{ij}\).

2.2.8.2. nonmetric mds[97]  

   non metric [98]mds focuses on the ordination of the data. if \(s_{ij} <
   s_{kl}\), then the embedding should enforce \(d_{ij} < d_{jk}\). a
   simple algorithm to enforce that is to use a monotonic regression of
   \(d_{ij}\) on \(s_{ij}\), yielding disparities \(\hat{d}_{ij}\) in the
   same order as \(s_{ij}\).

   a trivial solution to this problem is to set all the points on the
   origin. in order to avoid that, the disparities \(\hat{d}_{ij}\) are
   normalized.
   [99]../_images/sphx_glr_plot_mds_0011.png

   references:
     * [100]   modern multidimensional scaling - theory and applications   
       borg, i.; groenen p. springer series in statistics (1997)
     * [101]   nonmetric multidimensional scaling: a numerical method   
       kruskal, j. psychometrika, 29 (1964)
     * [102]   multidimensional scaling by optimizing goodness of fit to a
       nonmetric hypothesis    kruskal, j. psychometrika, 29, (1964)

2.2.9. t-distributed stochastic neighbor embedding (id167)[103]  

   id167 ([104]tsne) converts affinities of data points to probabilities.
   the affinities in the original space are represented by gaussian joint
   probabilities and the affinities in the embedded space are represented
   by student   s t-distributions. this allows id167 to be particularly
   sensitive to local structure and has a few other advantages over
   existing techniques:
     * revealing the structure at many scales on a single map
     * revealing data that lie in multiple, different, manifolds or
       clusters
     * reducing the tendency to crowd points together at the center

   while isomap, lle and variants are best suited to unfold a single
   continuous low dimensional manifold, id167 will focus on the local
   structure of the data and will tend to extract clustered local groups
   of samples as highlighted on the s-curve example. this ability to group
   samples based on the local structure might be beneficial to visually
   disentangle a dataset that comprises several manifolds at once as is
   the case in the digits dataset.

   the kullback-leibler (kl) divergence of the joint probabilities in the
   original space and the embedded space will be minimized by gradient
   descent. note that the kl divergence is not convex, i.e. multiple
   restarts with different initializations will end up in local minima of
   the kl divergence. hence, it is sometimes useful to try different seeds
   and select the embedding with the lowest kl divergence.

   the disadvantages to using id167 are roughly:
     * id167 is computationally expensive, and can take several hours on
       million-sample datasets where pca will finish in seconds or minutes
     * the barnes-hut id167 method is limited to two or three dimensional
       embeddings.
     * the algorithm is stochastic and multiple restarts with different
       seeds can yield different embeddings. however, it is perfectly
       legitimate to pick the embedding with the least error.
     * global structure is not explicitly preserved. this is problem is
       mitigated by initializing points with pca (using init=   pca   ).

   [105]../_images/sphx_glr_plot_lle_digits_0131.png

2.2.9.1. optimizing id167[106]  

   the main purpose of id167 is visualization of high-dimensional data.
   hence, it works best when the data will be embedded on two or three
   dimensions.

   optimizing the kl divergence can be a little bit tricky sometimes.
   there are five parameters that control the optimization of id167 and
   therefore possibly the quality of the resulting embedding:
     * perplexity
     * early exaggeration factor
     * learning rate
     * maximum number of iterations
     * angle (not used in the exact method)

   the perplexity is defined as \(k=2^{(s)}\) where \(s\) is the shannon
   id178 of the id155 distribution. the perplexity of
   a \(k\)-sided die is \(k\), so that \(k\) is effectively the number of
   nearest neighbors id167 considers when generating the conditional
   probabilities. larger perplexities lead to more nearest neighbors and
   less sensitive to small structure. conversely a lower perplexity
   considers a smaller number of neighbors, and thus ignores more global
   information in favour of the local neighborhood. as dataset sizes get
   larger more points will be required to get a reasonable sample of the
   local neighborhood, and hence larger perplexities may be required.
   similarly noisier datasets will require larger perplexity values to
   encompass enough local neighbors to see beyond the background noise.

   the maximum number of iterations is usually high enough and does not
   need any tuning. the optimization consists of two phases: the early
   exaggeration phase and the final optimization. during early
   exaggeration the joint probabilities in the original space will be
   artificially increased by multiplication with a given factor. larger
   factors result in larger gaps between natural clusters in the data. if
   the factor is too high, the kl divergence could increase during this
   phase. usually it does not have to be tuned. a critical parameter is
   the learning rate. if it is too low id119 will get stuck in
   a bad local minimum. if it is too high the kl divergence will increase
   during optimization. more tips can be found in laurens van der maaten   s
   faq (see references). the last parameter, angle, is a tradeoff between
   performance and accuracy. larger angles imply that we can approximate
   larger regions by a single point, leading to better speed but less
   accurate results.

   [107]   how to use id167 effectively    provides a good discussion of the
   effects of the various parameters, as well as interactive plots to
   explore the effects of different parameters.

2.2.9.2. barnes-hut id167[108]  

   the barnes-hut id167 that has been implemented here is usually much
   slower than other manifold learning algorithms. the optimization is
   quite difficult and the computation of the gradient is \(o[d n
   log(n)]\), where \(d\) is the number of output dimensions and \(n\) is
   the number of samples. the barnes-hut method improves on the exact
   method where id167 complexity is \(o[d n^2]\), but has several other
   notable differences:
     * the barnes-hut implementation only works when the target
       dimensionality is 3 or less. the 2d case is typical when building
       visualizations.
     * barnes-hut only works with dense input data. sparse data matrices
       can only be embedded with the exact method or can be approximated
       by a dense low rank projection for instance using
       [109]sklearn.decomposition.truncatedsvd
     * barnes-hut is an approximation of the exact method. the
       approximation is parameterized with the angle parameter, therefore
       the angle parameter is unused when method=   exact   
     * barnes-hut is significantly more scalable. barnes-hut can be used
       to embed hundred of thousands of data points while the exact method
       can handle thousands of samples before becoming computationally
       intractable

   for visualization purpose (which is the main use case of id167), using
   the barnes-hut method is strongly recommended. the exact id167 method
   is useful for checking the theoretically properties of the embedding
   possibly in higher dimensional space but limit to small datasets due to
   computational constraints.

   also note that the digits labels roughly match the natural grouping
   found by id167 while the linear 2d projection of the pca model yields a
   representation where label regions largely overlap. this is a strong
   clue that this data can be well separated by non linear methods that
   focus on the local structure (e.g. an id166 with a gaussian rbf kernel).
   however, failing to visualize well separated homogeneously labeled
   groups with id167 in 2d does not necessarily imply that the data cannot
   be correctly classified by a supervised model. it might be the case
   that 2 dimensions are not low enough to accurately represents the
   internal structure of the data.

   references:
     * [110]   visualizing high-dimensional data using id167    van der
       maaten, l.j.p.; hinton, g. journal of machine learning research
       (2008)
     * [111]   t-distributed stochastic neighbor embedding    van der maaten,
       l.j.p.
     * [112]   accelerating id167 using tree-based algorithms.    l.j.p. van
       der maaten. journal of machine learning research 15(oct):3221-3245,
       2014.

2.2.10. tips on practical use[113]  

     * make sure the same scale is used over all features. because
       manifold learning methods are based on a nearest-neighbor search,
       the algorithm may perform poorly otherwise. see [114]standardscaler
       for convenient ways of scaling heterogeneous data.
     * the reconstruction error computed by each routine can be used to
       choose the optimal output dimension. for a \(d\)-dimensional
       manifold embedded in a \(d\)-dimensional parameter space, the
       reconstruction error will decrease as n_components is increased
       until n_components == d.
     * note that noisy data can    short-circuit    the manifold, in essence
       acting as a bridge between parts of the manifold that would
       otherwise be well-separated. manifold learning on noisy and/or
       incomplete data is an active area of research.
     * certain input configurations can lead to singular weight matrices,
       for example when more than two points in the dataset are identical,
       or when the data is split into disjointed groups. in this case,
       solver='arpack' will fail to find the null space. the easiest way
       to address this is to use solver='dense' which will work on a
       singular matrix, though it may be very slow depending on the number
       of input points. alternatively, one can attempt to understand the
       source of the singularity: if it is due to disjoint sets,
       increasing n_neighbors may help. if it is due to identical points
       in the dataset, removing these points may help.

   see also

   [115]totally random trees embedding can also be useful to derive
   non-linear representations of feature space, also it does not perform
   id84.

      2007 - 2018, scikit-learn developers (bsd license). [116]show this
   page source

   [117]previous
   [118]next

references

   visible links
   1. https://scikit-learn.org/stable/search.html
   2. https://scikit-learn.org/stable/modules/id91.html
   3. https://scikit-learn.org/stable/modules/mixture.html
   4. https://scikit-learn.org/stable/index.html
   5. https://scikit-learn.org/stable/index.html
   6. https://scikit-learn.org/stable/install.html
   7. https://scikit-learn.org/stable/documentation.html
   8. https://scikit-learn.org/stable/tutorial/index.html
   9. https://scikit-learn.org/stable/user_guide.html
  10. https://scikit-learn.org/stable/modules/classes.html
  11. https://scikit-learn.org/stable/glossary.html
  12. https://scikit-learn.org/stable/faq.html
  13. https://scikit-learn.org/stable/developers/index.html
  14. https://scikit-learn.org/stable/roadmap.html
  15. http://scikit-learn.org/dev/versions.html
  16. https://scikit-learn.org/stable/_downloads/scikit-learn-docs.pdf
  17. https://scikit-learn.org/stable/auto_examples/index.html
  18. https://github.com/scikit-learn/scikit-learn
  19. https://scikit-learn.org/stable/modules/mixture.html
  20. https://scikit-learn.org/stable/modules/id91.html
  21. https://scikit-learn.org/stable/unsupervised_learning.html
  22. http://scikit-learn.org/dev/versions.html
  23. https://scikit-learn.org/stable/about.html#citing-scikit-learn
  24. https://scikit-learn.org/stable/modules/manifold.html
  25. https://scikit-learn.org/stable/modules/manifold.html#introduction
  26. https://scikit-learn.org/stable/modules/manifold.html#isomap
  27. https://scikit-learn.org/stable/modules/manifold.html#complexity
  28. https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding
  29. https://scikit-learn.org/stable/modules/manifold.html# 
  30. https://scikit-learn.org/stable/modules/manifold.html#modified-locally-linear-embedding
  31. https://scikit-learn.org/stable/modules/manifold.html# 
  32. https://scikit-learn.org/stable/modules/manifold.html#hessian-eigenmapping
  33. https://scikit-learn.org/stable/modules/manifold.html# 
  34. https://scikit-learn.org/stable/modules/manifold.html#spectral-embedding
  35. https://scikit-learn.org/stable/modules/manifold.html# 
  36. https://scikit-learn.org/stable/modules/manifold.html#local-tangent-space-alignment
  37. https://scikit-learn.org/stable/modules/manifold.html# 
  38. https://scikit-learn.org/stable/modules/manifold.html#multi-dimensional-scaling-mds
  39. https://scikit-learn.org/stable/modules/manifold.html#metric-mds
  40. https://scikit-learn.org/stable/modules/manifold.html#nonmetric-mds
  41. https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-id167
  42. https://scikit-learn.org/stable/modules/manifold.html#optimizing-id167
  43. https://scikit-learn.org/stable/modules/manifold.html#barnes-hut-id167
  44. https://scikit-learn.org/stable/modules/manifold.html#tips-on-practical-use
  45. https://scikit-learn.org/stable/modules/manifold.html#manifold-learning
  46. https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html
  47. https://scikit-learn.org/stable/modules/manifold.html#introduction
  48. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  49. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  50. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  51. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  52. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py
  53. https://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html#sphx-glr-auto-examples-manifold-plot-compare-methods-py
  54. https://scikit-learn.org/stable/modules/manifold.html#isomap
  55. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.isomap.html#sklearn.manifold.isomap
  56. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  57. https://scikit-learn.org/stable/modules/manifold.html#complexity
  58. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.balltree.html#sklearn.neighbors.balltree
  59. http://science.sciencemag.org/content/290/5500/2319.full
  60. https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding
  61. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding
  62. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locallylinearembedding.html#sklearn.manifold.locallylinearembedding
  63. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  64. https://scikit-learn.org/stable/modules/manifold.html# 
  65. http://www.sciencemag.org/content/290/5500/2323.full
  66. https://scikit-learn.org/stable/modules/manifold.html#modified-locally-linear-embedding
  67. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding
  68. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locallylinearembedding.html#sklearn.manifold.locallylinearembedding
  69. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  70. https://scikit-learn.org/stable/modules/manifold.html# 
  71. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382
  72. https://scikit-learn.org/stable/modules/manifold.html#hessian-eigenmapping
  73. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding
  74. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locallylinearembedding.html#sklearn.manifold.locallylinearembedding
  75. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  76. https://scikit-learn.org/stable/modules/manifold.html# 
  77. http://www.pnas.org/content/100/10/5591
  78. https://scikit-learn.org/stable/modules/manifold.html#spectral-embedding
  79. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html#sklearn.manifold.spectral_embedding
  80. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectralembedding.html#sklearn.manifold.spectralembedding
  81. https://scikit-learn.org/stable/modules/manifold.html# 
  82. http://web.cse.ohio-state.edu/~mbelkin/papers/lem_nc_03.pdf
  83. https://scikit-learn.org/stable/modules/manifold.html#local-tangent-space-alignment
  84. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html#sklearn.manifold.locally_linear_embedding
  85. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.locallylinearembedding.html#sklearn.manifold.locallylinearembedding
  86. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  87. https://scikit-learn.org/stable/modules/manifold.html# 
  88. http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.3693
  89. https://scikit-learn.org/stable/modules/manifold.html#multi-dimensional-scaling-mds
  90. https://en.wikipedia.org/wiki/multidimensional_scaling
  91. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.mds.html#sklearn.manifold.mds
  92. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.mds.html#sklearn.manifold.mds
  93. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.mds.html#sklearn.manifold.mds
  94. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
  95. https://scikit-learn.org/stable/modules/manifold.html#metric-mds
  96. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.mds.html#sklearn.manifold.mds
  97. https://scikit-learn.org/stable/modules/manifold.html#nonmetric-mds
  98. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.mds.html#sklearn.manifold.mds
  99. https://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html
 100. http://www.springer.com/fr/book/9780387251509
 101. http://link.springer.com/article/10.1007/bf02289694
 102. http://link.springer.com/article/10.1007/bf02289565
 103. https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-id167
 104. https://scikit-learn.org/stable/modules/generated/sklearn.manifold.tsne.html#sklearn.manifold.tsne
 105. https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
 106. https://scikit-learn.org/stable/modules/manifold.html#optimizing-id167
 107. http://distill.pub/2016/misread-tsne/
 108. https://scikit-learn.org/stable/modules/manifold.html#barnes-hut-id167
 109. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.truncatedsvd.html#sklearn.decomposition.truncatedsvd
 110. http://jmlr.org/papers/v9/vandermaaten08a.html
 111. https://lvdmaaten.github.io/tsne/
 112. https://lvdmaaten.github.io/publications/papers/jmlr_2014.pdf
 113. https://scikit-learn.org/stable/modules/manifold.html#tips-on-practical-use
 114. https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler
 115. https://scikit-learn.org/stable/modules/ensemble.html#random-trees-embedding
 116. https://scikit-learn.org/stable/_sources/modules/manifold.rst.txt
 117. https://scikit-learn.org/stable/modules/mixture.html
 118. https://scikit-learn.org/stable/modules/id91.html

   hidden links:
 120. javascript:void(0);
