community

   news
   beta
   tutorials
   cheat sheets
   open courses
   podcast - dataframed
   chat
   new

datacamp

   official blog
   tech thoughts
   (button)
   search
   [1](button)
   log in
   (button)
   create account
   (button)
   share an article
   (button)
   back to tutorials
   tutorials
   [2]0
   300
   300
   karlijn willems
   january 16th, 2019
   must read
   python
   +4

tensorflow tutorial for beginners

   learn how to build a neural network and how to train, evaluate and
   optimize it with tensorflow
   [tensorflow-tutorial_eueeae.png]

   deep learning is a subfield of machine learning that is a set of
   algorithms that is inspired by the structure and function of the brain.

   tensorflow is the second machine learning framework that google created
   and used to design, build, and train deep learning models. you can use
   the tensorflow library do to numerical computations, which in itself
   doesn   t seem all too special, but these computations are done with data
   flow graphs. in these graphs, nodes represent mathematical operations,
   while the edges represent the data, which usually are multidimensional
   data arrays or tensors, that are communicated between these edges.

   you see? the name    tensorflow    is derived from the operations which
   neural networks perform on multidimensional data arrays or tensors!
   it   s literally a flow of tensors. for now, this is all you need to know
   about tensors, but you   ll go deeper into this in the next sections!

   today   s tensorflow tutorial for beginners will introduce you to
   performing deep learning in an interactive way:
     * you   ll first learn more about [3]tensors;
     * then, the tutorial you   ll briefly go over some of the ways that you
       can [4]install tensorflow on your system so that you   re able to get
       started and load data in your workspace;
     * after this, you   ll go over some of the [5]tensorflow basics: you   ll
       see how you can easily get started with simple computations.
     * after this, you get started on the real work: you   ll load in data
       on belgian traffic signs and [6]exploring it with simple statistics
       and plotting.
     * in your exploration, you   ll see that there is a need to
       [7]manipulate your data in such a way that you can feed it to your
       model. that   s why you   ll take the time to rescale your images and
       convert them to grayscale.
     * next, you can finally get started on [8]your neural network model!
       you   ll build up your model layer per layer;
     * once the architecture is set up, you can use it to [9]train your
       model interactively and to eventually also [10]evaluate it by
       feeding some test data to it.
     * lastly, you   ll get some pointers for [11]further improvements that
       you can do to the model you just constructed and how you can
       continue your learning with tensorflow.

   download the notebook of this tutorial [12]here.

   also, you could be interested in a course on [13]deep learning in
   python, datacamp's [14]keras tutorial or the [15]keras with r tutorial.

introducing tensors

   to understand tensors well, it   s good to have some working knowledge of
   id202 and vector calculus. you already read in the
   introduction that tensors are implemented in tensorflow as
   multidimensional data arrays, but some more introduction is maybe
   needed in order to completely grasp tensors and their use in machine
   learning.

plane vectors

   before you go into plane vectors, it   s a good idea to shortly revise
   the concept of    vectors   ; vectors are special types of matrices, which
   are rectangular arrays of numbers. because vectors are ordered
   collections of numbers, they are often seen as column matrices: they
   have just one column and a certain number of rows. in other terms, you
   could also consider vectors as scalar magnitudes that have been given a
   direction.

   remember: an example of a scalar is    5 meters    or    60 m/sec   , while a
   vector is, for example,    5 meters north    or    60 m/sec east   . the
   difference between these two is obviously that the vector has a
   direction. nevertheless, these examples that you have seen up until now
   might seem far off from the vectors that you might encounter when
   you   re working with machine learning problems. this is normal; the
   length of a mathematical vector is a pure number: it is absolute. the
   direction, on the other hand, is relative: it is measured relative to
   some reference direction and has units of radians or degrees. you
   usually assume that the direction is positive and in counterclockwise
   rotation from the reference direction.
   [content_plane_vector.png]


   visually, of course, you represent vectors as arrows, as you can see in
   the picture above. this means that you can consider vectors also as
   arrows that have direction and length. the direction is indicated by
   the arrow   s head, while the length is indicated by the length of the
   arrow.

   so what about plane vectors then?

   plane vectors are the most straightforward setup of tensors. they are
   much like regular vectors as you have seen above, with the sole
   difference that they find themselves in a vector space. to understand
   this better, let   s start with an example: you have a vector that is 2 x
   1. this means that the vector belongs to the set of real numbers that
   come paired two at a time. or, stated differently, they are part of
   two-space. in such cases, you can represent vectors on the coordinate
   (x,y) plane with arrows or rays.

   working from this coordinate plane in a standard position where vectors
   have their endpoint at the origin (0,0), you can derive the x
   coordinate by looking at the first row of the vector, while you   ll find
   the y coordinate in the second row. of course, this standard position
   doesn   t always need to be maintained: vectors can move parallel to
   themselves in the plane without experiencing changes.

   note that similarly, for vectors that are of size 3 x 1, you talk about
   the three-space. you can represent the vector as a three-dimensional
   figure with arrows pointing to positions in the vectors pace: they are
   drawn on the standard x, y and z axes.

   it   s nice to have these vectors and to represent them on the coordinate
   plane, but in essence, you have these vectors so that you can perform
   operations on them and one thing that can help you in doing this is by
   expressing your vectors as bases or unit vectors.

   unit vectors are vectors with a magnitude of one. you   ll often
   recognize the unit vector by a lowercase letter with a circumflex, or
      hat   . unit vectors will come in convenient if you want to express a
   2-d or 3-d vector as a sum of two or three orthogonal components, such
   as the x    and y   axes, or the z   axis.

   and when you are talking about expressing one vector, for example, as
   sums of components, you   ll see that you   re talking about component
   vectors, which are two or more vectors whose sum is that given vector.

   tip: watch [16]this video, which explains what tensors are with the
   help of simple household objects!

tensors

   next to plane vectors, also covectors and linear operators are two
   other cases that all three together have one thing in common: they are
   specific cases of tensors. you still remember how a vector was
   characterized in the previous section as scalar magnitudes that have
   been given a direction. a tensor, then, is the mathematical
   representation of a physical entity that may be characterized by
   magnitude and multiple directions.

   and, just like you represent a scalar with a single number and a vector
   with a sequence of three numbers in a 3-dimensional space, for example,
   a tensor can be represented by an array of 3r numbers in a
   3-dimensional space.

   the    r    in this notation represents the rank of the tensor: this means
   that in a 3-dimensional space, a second-rank tensor can be represented
   by 3 to the power of 2 or 9 numbers. in an n-dimensional space, scalars
   will still require only one number, while vectors will require n
   numbers, and tensors will require n^r numbers. this explains why you
   often hear that scalars are tensors of rank 0: since they have no
   direction, you can represent them with one number.

   with this in mind, it   s relatively easy to recognize scalars, vectors,
   and tensors and to set them apart: scalars can be represented by a
   single number, vectors by an ordered set of numbers, and tensors by an
   array of numbers.

   what makes tensors so unique is the combination of components and basis
   vectors: basis vectors transform one way between reference frames and
   the components transform in just such a way as to keep the combination
   between components and basis vectors the same.

installing tensorflow

   now that you know more about tensorflow, it   s time to get started and
   install the library. here, it   s good to know that tensorflow provides
   apis for python, c++, haskell, java, go, rust, and there   s also a
   third-party package for r called tensorflow.

   tip: if you want to know more about deep learning packages in r,
   consider checking out datacamp   s [17]keras: deep learning in r
   tutorial.

   in this tutorial, you will download a version of tensorflow that will
   enable you to write the code for your deep learning project in python.
   on the [18]tensorflow installation webpage, you   ll see some of the most
   common ways and latest instructions to install tensorflow using
   virtualenv, pip, docker and lastly, there are also some of the other
   ways of installing tensorflow on your personal computer.

   note you can also install tensorflow with conda if you   re working on
   windows. however, since the installation of tensorflow is community
   supported, it   s best to check the [19]official installation
   instructions.

   now that you have gone through the installation process, it   s time to
   double check that you have installed tensorflow correctly by importing
   it into your workspace under the alias tf:
import tensorflow as tf

   note that the alias that you used in the line of code above is sort of
   a convention - it   s used to ensure that you remain consistent with
   other developers that are using tensorflow in data science projects on
   the one hand, and with open-source tensorflow projects on the other
   hand.

getting started with tensorflow: basics

   you   ll generally write tensorflow programs, which you run as a chunk;
   this is at first sight kind of contradictory when you   re working with
   python. however, if you would like, you can also use tensorflow   s
   interactive session, which you can use to work more interactively with
   the library. this is especially handy when you   re used to working with
   ipython.

   for this tutorial, you   ll focus on the second option: this will help
   you to get kickstarted with deep learning in tensorflow. but before you
   go any further into this, let   s first try out some minor stuff before
   you start with the heavy lifting.

   first, import the tensorflow library under the alias tf, as you have
   seen in the previous section. then initialize two variables that are
   actually constants. pass an array of four numbers to the constant()
   function.

   note that you could potentially also pass in an integer, but that more
   often than not, you   ll find yourself working with arrays. as you saw in
   the introduction, tensors are all about arrays! so make sure that you
   pass in an array :) next, you can use multiply() to multiply your two
   variables. store the result in the result variable. lastly, print out
   the result with the help of the print() function.
   eyjsyw5ndwfnzsi6inb5dghvbiisinnhbxbszsi6iimgsw1wb3j0igb0zw5zb3jmbg93yfx
   uaw1wb3j0ihrlbnnvcmzsb3cgyxmgdgzcblxuiybjbml0awfsaxplihr3bybjb25zdgfudh
   ncbngxid0gdgyuy29uc3rhbnqowzesmiwzldrdkvxuedigpsb0zi5jb25zdgfudchbnsw2l
   dcsof0pxg5cbimgtxvsdglwbhlcbnjlc3vsdca9ihrmlm11bhrpcgx5khgxlcb4milcblxu
   iybqcmludcb0agugcmvzdwx0xg5wcmludchyzxn1bhqpin0=

   note that you have defined constants in the datacamp light code chunk
   above. however, there are two other types of values that you can
   potentially use, namely [20]placeholders, which are values that are
   unassigned and that will be initialized by the session when you run it.
   like the name already gave away, it   s just a placeholder for a tensor
   that will always be fed when the session is run; there are also
   [21]variables, which are values that can change. the constants, as you
   might have already gathered, are values that don   t change.

   the result of the lines of code is an abstract tensor in the
   computation graph. however, contrary to what you might expect, the
   result doesn   t actually get calculated. it just defined the model, but
   no process ran to calculate the result. you can see this in the
   print-out: there   s not really a result that you want to see (namely,
   30). this means that tensorflow has a lazy evaluation!

   however, if you do want to see the result, you have to run this code in
   an interactive session. you can do this in a few ways, as is
   demonstrated in the datacamp light code chunks below:
   eyjsyw5ndwfnzsi6inb5dghvbiisinnhbxbszsi6iimgsw1wb3j0igb0zw5zb3jmbg93ycb
   cbmltcg9ydcb0zw5zb3jmbg93igfzihrmxg5cbimgsw5pdglhbgl6zsb0d28gy29uc3rhbn
   rzxg54msa9ihrmlmnvbnn0yw50kfsxldismyw0xslcbngyid0gdgyuy29uc3rhbnqowzusn
   iw3ldhdkvxuxg4jie11bhrpcgx5xg5yzxn1bhqgpsb0zi5tdwx0axbsesh4mswgedipxg5c
   bimgsw50awfsaxplihrozsbtzxnzaw9uxg5zzxnzid0gdgyuu2vzc2lvbigpxg5cbimguhj
   pbnqgdghlihjlc3vsdfxuchjpbnqoc2vzcy5ydw4ocmvzdwx0kslcblxuiybdbg9zzsb0ag
   ugc2vzc2lvblxuc2vzcy5jbg9zzsgpin0=

   note that you can also use the following lines of code to start up an
   interactive session, run the result and close the session automatically
   again after printing the output:
   eyjsyw5ndwfnzsi6inb5dghvbiisinnhbxbszsi6iimgsw1wb3j0igb0zw5zb3jmbg93yfx
   uaw1wb3j0ihrlbnnvcmzsb3cgyxmgdgzcblxuiybjbml0awfsaxplihr3bybjb25zdgfudh
   ncbngxid0gdgyuy29uc3rhbnqowzesmiwzldrdkvxuedigpsb0zi5jb25zdgfudchbnsw2l
   dcsof0pxg5cbimgtxvsdglwbhlcbnjlc3vsdca9ihrmlm11bhrpcgx5khgxlcb4milcblxu
   iybjbml0awfsaxplifnlc3npb24gyw5kihj1bibgcmvzdwx0yfxud2l0acb0zi5tzxnzaw9
   ukckgyxmgc2vzczpcbiagb3v0chv0id0gc2vzcy5ydw4ocmvzdwx0kvxuicbwcmludchvdx
   rwdxqpin0=

   in the code chunks above you have just defined a default session, but
   it   s also good to know that you can pass in options as well. you can,
   for example, specify the config argument and then use the configproto
   protocol buffer to add configuration options for your session.

   for example, if you add
config=tf.configproto(log_device_placement=true)

   to your session, you make sure that you log the gpu or cpu device that
   is assigned to an operation. you will then get information which
   devices are used in the session for each operation. you could use the
   following configuration session also, for example, when you use soft
   constraints for the device placement:
config=tf.configproto(allow_soft_placement=true)


   now that you   ve got tensorflow installed and imported into your
   workspace and you   ve gone through the basics of working with this
   package, it   s time to leave this aside for a moment and turn your
   attention to your data. just like always, you   ll first take your time
   to explore and understand your data better before you start modeling
   your neural network.

belgian traffic signs: background

   even though traffic is a topic that is generally known amongst you all,
   it doesn   t hurt going briefly over the observations that are included
   in this dataset to see if you understand everything before you start.
   in essence, in this section, you   ll get up to speed with the domain
   knowledge that you need to have to go further with this tutorial.

   of course, because i   m belgian, i   ll make sure you   ll also get some
   anecdotes :)
     * belgian traffic signs are usually in dutch and french. this is good
       to know, but for the dataset that you   ll be working with, it   s not
       too important!
     * there are six categories of traffic signs in belgium: warning
       signs, priority signs, prohibitory signs, mandatory signs, signs
       related to parking and standing still on the road and, lastly,
       designatory signs.
     * on january 1st, 2017, more than 30,000 traffic signs were removed
       from belgian roads. these were all prohibitory signs relating to
       speed.
     * talking about removal, the overwhelming presence of traffic signs
       has been an ongoing discussion in belgium (and by extension, the
       entire european union).

loading and exploring the data

   now that you have gathered some more background information, it   s time
   to download the dataset [22]here. you should get the two zip files
   listed next to "belgiumts for classification (cropped images), which
   are called "belgiumtsc_training" and "belgiumtsc_testing".

   tip: if you have downloaded the files or will do so after completing
   this tutorial, take a look at the folder structure of the data that
   you   ve downloaded! you   ll see that the testing, as well as the training
   data folders, contain 61 subfolders, which are the 62 types of traffic
   signs that you   ll use for classification in this tutorial.
   additionally, you   ll find that the files have the file extension .ppm
   or portable pixmap format. you have downloaded images of the traffic
   signs!

   let   s get started with importing the data into your workspace. let   s
   start with the lines of code that appear below the user-defined
   function (udf) load_data():
     * first, set your root_path. this path is the one where you have made
       the directory with your training and test data.
     * next, you can add the specific paths to your root_path with the
       help of the join() function. you store these two specific paths in
       train_data_directory and test_data_directory.
     * you see that after, you can call the load_data() function and pass
       in the train_data_directory to it.
     * now, the load_data() function itself starts off by gathering all
       the subdirectories that are present in the train_data_directory; it
       does so with the help of list comprehension, which is quite a
       natural way of constructing lists - it basically says that, if you
       find something in the train_data_directory, you   ll double check
       whether this is a directory, and if it is one, you   ll add it to
       your list. remember that each subdirectory represents a label.
     * next, you have to loop through the subdirectories. you first
       initialize two lists, labels and images. next, you gather the paths
       of the subdirectories and the file names of the images that are
       stored in these subdirectories. after, you can collect the data in
       the two lists with the help of the append() function.

def load_data(data_directory):
    directories = [d for d in os.listdir(data_directory)
                   if os.path.isdir(os.path.join(data_directory, d))]
    labels = []
    images = []
    for d in directories:
        label_directory = os.path.join(data_directory, d)
        file_names = [os.path.join(label_directory, f)
                      for f in os.listdir(label_directory)
                      if f.endswith(".ppm")]
        for f in file_names:
            images.append(skimage.data.imread(f))
            labels.append(int(d))
    return images, labels

root_path = "/your/root/path"
train_data_directory = os.path.join(root_path, "trafficsigns/training")
test_data_directory = os.path.join(root_path, "trafficsigns/testing")

images, labels = load_data(train_data_directory)

   note that in the above code chunk, the training and test data are
   located in folders named "training" and "testing", which are both
   subdirectories of another directory "trafficsigns". on a local machine,
   this could look something like "/users/name/downloads/trafficsigns",
   with then two subfolders called "training" and "testing".

   tip: review how to write functions in python with datacamp's [23]python
   functions tutorial.

traffic sign statistics

   with your data loaded in, it   s time for some data inspection! you can
   start with a pretty simple analysis with the help of the ndim and size
   attributes of the images array:

   note that the images and labels variables are lists, so you might need
   to use np.array() to convert the variables to an array in your own
   workspace. this has been done for you here!
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ig51bxb
   5igfzig5wxg5pbxbvcnqgaw9cbmltcg9ydcb1cmxsawjcbnvybf9pbwdzid0gxcjodhrwcz
   ovl3mzlmftyxpvbmf3cy5jb20vyxnzzxrzlmrhdgfjyw1wlmnvbs9ibg9nx2fzc2v0cy9pb
   wfnzxmubnb6xcjcbmltz3mgpsbucc5sb2fkkglvlkj5dgvzsu8odxjsbglilnjlcxvlc3qu
   dxjsb3blbih1cmxfaw1ncykucmvhzcgpkslcbmltywdlcya9igltz3nbxcjhcnjfmfwixsi
   sinnhbxbszsi6iimguhjpbnqgdghligbpbwfnzxngigrpbwvuc2lvbnncbnbyaw50kgltyw
   dlcy5uzgltkvxuxg4jifbyaw50ihrozsbudw1izxigb2ygygltywdlc2ancyblbgvtzw50c
   1xuchjpbnqoaw1hz2vzlnnpemupxg5cbimguhjpbnqgdghligzpid980igluc3rhbmnlig9m
   igbpbwfnzxngxg5pbwfnzxnbmf0ifq==

   note that the images[0] that you printed out is, in fact, one single
   image that is represented by arrays in arrays! this might seem
   counterintuitive at first, but it   s something that you   ll get used to
   as you go further into working with images in machine learning or deep
   learning applications.

   next, you can also take a small look at the labels, but you shouldn   t
   see too many surprises at this point:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ig51bxb
   5igfzig5wxg5pbxbvcnqgaw9cbmltcg9ydcb1cmxsawjcbnvybf9sywjlbhmgpsbcimh0dh
   bzoi8vczmuyw1hem9uyxdzlmnvbs9hc3nldhmuzgf0ywnhbxauy29tl2jsb2dfyxnzzxrzl
   2xhymvscy50ehrcilxucmf3x2xhymvscya9ihvybgxpyi5yzxf1zxn0lnvybg9wzw4odxjs
   x2xhymvscylcbmxhymvscya9ig5wlmxvywr0ehqocmf3x2xhymvscywgzgvsaw1pdgvypvw
   ilfwiksisinnhbxbszsi6iimguhjpbnqgdghligbsywjlbhngigrpbwvuc2lvbnncbnbyaw
   50kgxhymvscy5uzgltkvxuxg4jifbyaw50ihrozsbudw1izxigb2ygygxhymvsc2ancyblb
   gvtzw50c1xuchjpbnqobgfizwxzlnnpemupxg5cbimgq291bnqgdghlig51bwjlcibvzibs
   ywjlbhncbnbyaw50kgxlbihzzxqobgfizwxzkskpin0=

   these numbers already give you some insights into how successful your
   import was and the exact size of your data. at first sight, everything
   has been executed the way you expected it to, and you see that the size
   of the array is considerable if you take into account that you   re
   dealing with arrays within arrays.

   tip try adding the following attributes to your arrays to get more
   information about the memory layout, the length of one array element in
   bytes and the total consumed bytes by the array   s elements with the
   flags, itemsize, and nbytes attributes. you can test this out in the
   ipython console in the datacamp light chunk above!

   next, you can also take a look at the distribution of the traffic
   signs:
   eyjsyw5ndwfnzsi6inb5dghvbiisinbyzv9legvyy2lzzv9jb2rlijoiaw1wb3j0ig51bxb
   5igfzig5wxg5pbxbvcnqgdxjsbglixg51cmxfbgfizwxzid0gxcjodhrwczovl3mzlmftyx
   pvbmf3cy5jb20vyxnzzxrzlmrhdgfjyw1wlmnvbs9ibg9nx2fzc2v0cy9sywjlbhmudhh0x
   cjcbnjhd19sywjlbhmgpsb1cmxsawiucmvxdwvzdc51cmxvcgvukhvybf9sywjlbhmpxg5s
   ywjlbhmgpsbucc5sb2fkdhh0khjhd19sywjlbhmsigrlbgltaxrlcj1ciixciikilcjzyw1
   wbguioiijieltcg9ydcb0agugyhb5cgxvdgagbw9kdwxlxg5pbxbvcnqgbwf0cgxvdgxpyi
   5wexbsb3qgyxmgcgx0ifxuxg4jie1ha2ugysboaxn0b2dyyw0gd2l0aca2mibiaw5zig9mi
   hrozsbgbgfizwxzycbkyxrhxg5wbhquaglzdchsywjlbhmsidyykvxuxg4jifnob3cgdghl
   ihbsb3rcbnbsdc5zag93kckifq==

   awesome job! now let   s take a closer look at the histogram that you
   made!
   [content_traffic_signs2.png]

   you clearly see that not all types of traffic signs are equally
   represented in the dataset. this is something that you   ll deal with
   later when you   re manipulating the data before you start modeling your
   neural network.

   at first sight, you see that there are labels that are more heavily
   present in the dataset than others: the labels 22, 32, 38, and 61
   definitely jump out. at this point, it   s nice to keep this in mind, but
   you   ll definitely go further into this in the next section!

visualizing the traffic signs

   the previous, small analyses or checks have already given you some idea
   of the data that you   re working with, but when your data mostly
   consists of images, the step that you should take to explore your data
   is by visualizing it.

   let   s check out some random traffic signs:
     * first, make sure that you import the pyplot module of the
       matplotlib package under the common alias plt.
     * then, you   re going to make a list with 4 random numbers. these will
       be used to select traffic signs from the images array that you have
       just inspected in the previous section. in this case, you go for
       300, 2250, 3650 and 4000.
     * next, you   ll say that for every element in the length of that list,
       so from 0 to 4, you   re going to create subplots without axes (so
       that they don   t go running with all the attention and your focus is
       solely on the images!). in these subplots, you   re going to show a
       specific image from the images array that is in accordance with the
       number at the index i. in the first loop, you   ll pass 300 to
       images[], in the second round 2250, and so on. lastly, you   ll
       adjust the subplots so that there   s enough width in between them.
     * the last thing that remains is to show your plot with the help of
       the show() function!

   there you go:
# import the `pyplot` module of `matplotlib`
import matplotlib.pyplot as plt

# determine the (random) indexes of the images that you want to see
traffic_signs = [300, 2250, 3650, 4000]

# fill out the subplots with the random images that you defined
for i in range(len(traffic_signs)):
    plt.subplot(1, 4, i+1)
    plt.axis('off')
    plt.imshow(images[traffic_signs[i]])
    plt.subplots_adjust(wspace=0.5)

plt.show()

   as you guessed by the 62 labels that are included in this dataset, the
   signs are different from each other.

   but what else do you notice? take another close look at the images
   below:
   [content_traffic_signs1.png]

   these four images are not of the same size!

   you can obviously toy around with the numbers that are contained in the
   traffic_signs list and follow up more thoroughly on this observation,
   but be as it may, this is an important observation which you will need
   to take into account when you start working more towards manipulating
   your data so that you can feed it to the neural network.

   let   s confirm the hypothesis of the differing sizes by printing the
   shape, the minimum and maximum values of the specific images that you
   have included into the subplots.

   the code below heavily resembles the one that you used to create the
   above plot, but differs in the fact that here, you   ll alternate sizes
   and images instead of plotting just the images next to each other:
# import `matplotlib`
import matplotlib.pyplot as plt

# determine the (random) indexes of the images
traffic_signs = [300, 2250, 3650, 4000]

# fill out the subplots with the random images and add shape, min and max values
for i in range(len(traffic_signs)):
    plt.subplot(1, 4, i+1)
    plt.axis('off')
    plt.imshow(images[traffic_signs[i]])
    plt.subplots_adjust(wspace=0.5)
    plt.show()
    print("shape: {0}, min: {1}, max: {2}".format(images[traffic_signs[i]].shape
,
                                                  images[traffic_signs[i]].min()
,
                                                  images[traffic_signs[i]].max()
))

   note how you use the format() method on the string "shape: {0}, min:
   {1}, max: {2}" to fill out the arguments {0}, {1}, and {2} that you
   defined.
   [content_traffic_signs5.png]

   now that you have seen loose images, you might also want to revisit the
   histogram that you printed out in the first steps of your data
   exploration; you can easily do this by plotting an overview of all the
   62 classes and one image that belongs to each class:
# import the `pyplot` module as `plt`
import matplotlib.pyplot as plt

# get the unique labels
unique_labels = set(labels)

# initialize the figure
plt.figure(figsize=(15, 15))

# set a counter
i = 1

# for each unique label,
for label in unique_labels:
    # you pick the first image for each label
    image = images[labels.index(label)]
    # define 64 subplots
    plt.subplot(8, 8, i)
    # don't include axes
    plt.axis('off')
    # add a title to each subplot
    plt.title("label {0} ({1})".format(label, labels.count(label)))
    # add 1 to the counter
    i += 1
    # and you plot this first image
    plt.imshow(image)

# show the plot
plt.show()

   note that even though you define 64 subplots, not all of them will show
   images (as there are only 62 labels!). note also that again, you don   t
   include any axes to make sure that the readers    attention doesn   t dwell
   far from the main topic: the traffic signs!
   [content_traffic_signs3.png]

   as you mostly guessed in the histogram above, there are considerably
   more traffic signs with labels 22, 32, 38, and 61. this hypothesis is
   now confirmed in this plot: you see that there are 375 instances with
   label 22, 316 instances with label 32, 285 instances with label 38 and,
   lastly, 282 instances with label 61.

   one of the most interesting questions that you could ask yourself now
   is whether there   s a connection between all of these instances - maybe
   all of them are designatory signs?

   let   s take a closer look: you see that label 22 and 32 are prohibitory
   signs, but that labels 38 and 61 are designatory and a prioritory
   signs, respectively. this means that there   s not an immediate
   connection between these four, except for the fact that half of the
   signs that have a substantial presence in the dataset is of the
   prohibitory kind.

feature extraction

   now that you have thoroughly explored your data, it   s time to get your
   hands dirty! let   s recap briefly what you discovered to make sure that
   you don   t forget any steps in the manipulation:
     * the size of the images was unequal;
     * there are 62 labels or target values (as your labels start at 0 and
       end at 61);
     * the distribution of the traffic sign values is pretty unequal;
       there wasn   t really any connection between the signs that were
       heavily present in the dataset.

   now that you have a clear idea of what you need to improve, you can
   start with manipulating your data in such a way that it   s ready to be
   fed to the neural network or whichever model you want to feed it too.
   let   s start first with extracting some features - you   ll rescale the
   images, and you   ll convert the images that are held in the images array
   to grayscale. you   ll do this color conversion mainly because the color
   matters less in classification questions like the one you   re trying to
   answer now. for detection, however, the color does play a big part! so
   in those cases, it   s not needed to do that conversion!

rescaling images

   to tackle the differing image sizes, you   re going to rescale the
   images; you can easily do this with the help of the skimage or
   scikit-image library, which is a collection of algorithms for image
   processing.

   in this case, the transform module will come in handy, as it offers you
   a resize() function; you   ll see that you make use of list comprehension
   (again!) to resize each image to 28 by 28 pixels. once again, you see
   that the way you actually form the list: for every image that you find
   in the images array, you   ll perform the transformation operation that
   you borrow from the skimage library. finally, you store the result in
   the images28 variable:
# import the `transform` module from `skimage`
from skimage import transform

# rescale the images in the `images` array
images28 = [transform.resize(image, (28, 28)) for image in images]

   this was fairly easy wasn   t it?

   note that the images are now four-dimensional: if you convert images28
   to an array and if you concatenate the attribute shape to it, you   ll
   see that the printout tells you that images28   s dimensions are (4575,
   28, 28, 3). the images are 784-dimensional (because your images are 28
   by 28 pixels).

   you can check the result of the rescaling operation by re-using the
   code that you used above to plot the 4 random images with the help of
   the traffic_signs variable. just don   t forget to change all references
   to images to images28.

   check out the result here:
   [content_traffic_signs6.png]

   note that because you rescaled, your min and max values have also
   changed; they seem to be all in the same ranges now, which is really
   great because then you don   t necessarily need to normalize your data!

image conversion to grayscale

   as said in the introduction to this section of the tutorial, the color
   in the pictures matters less when you   re trying to answer a
   classification question. that   s why you   ll also go through the trouble
   of converting the images to grayscale.

   note, however, that you can also test out on your own what would happen
   to the final results of your model if you don   t follow through with
   this specific step.

   just like with the rescaling, you can again count on the scikit-image
   library to help you out; in this case, it   s the color module with its
   rgb2gray() function that you need to use to get where you need to be.

   that   s going to be nice and easy!

   however, don   t forget to convert the images28 variable back to an
   array, as the rgb2gray() function does expect an array as an argument.
# import `rgb2gray` from `skimage.color`
from skimage.color import rgb2gray

# convert `images28` to an array
images28 = np.array(images28)

# convert `images28` to grayscale
images28 = rgb2gray(images28)

   double check the result of your grayscale conversion by plotting some
   of the images; here, you can again re-use and slightly adapt some of
   the code to show the adjusted images:
import matplotlib.pyplot as plt

traffic_signs = [300, 2250, 3650, 4000]

for i in range(len(traffic_signs)):
    plt.subplot(1, 4, i+1)
    plt.axis('off')
    plt.imshow(images28[traffic_signs[i]], cmap="gray")
    plt.subplots_adjust(wspace=0.5)

# show the plot
plt.show()

   note that you indeed have to specify the color map or cmap and set it
   to "gray" to plot the images in grayscale. that is because imshow() by
   default uses, by default, a heatmap-like color map. read more [24]here.
   [content_traffic_signs4.png]

   tip: since you have been re-using this function quite a bit in this
   tutorial, you might look into how you can make it into a function :)

   these two steps are very basic ones; other operations that you could
   have tried out on your data include data augmentation (rotating,
   blurring, shifting, changing brightness,   ). if you want, you could also
   set up an entire pipeline of data manipulation operations through which
   you send your images.

deep learning with tensorflow

   now that you have explored and manipulated your data, it   s time to
   construct your neural network architecture with the help of the
   tensorflow package!

modeling the neural network

   just like you might have done with keras, it   s time to build up your
   neural network, layer by layer.

   if you haven   t done so already, import tensorflow into your workspace
   under the conventional alias tf. then, you can initialize the graph
   with the help of graph(). you use this function to define the
   computation. note that with the graph, you don   t compute anything,
   because it doesn   t hold any values. it just defines the operations that
   you want to be running later.

   in this case, you set up a default context with the help of
   as_default(), which returns a context manager that makes this specific
   graph the default graph. you use this method if you want to create
   multiple graphs in the same process: with this function, you have a
   global default graph to which all operations will be added if you don   t
   explicitly create a new graph.

   next, you   re ready to add operations to your graph. as you might
   remember from working with keras, you build up your model, and then in
   compiling it, you define a id168, an optimizer, and a metric.
   this now all happens in one step when you work with tensorflow:
     * first, you define placeholders for inputs and labels because you
       won   t put in the    real    data yet. remember that placeholders are
       values that are unassigned and that will be initialized by the
       session when you run it. so when you finally run the session, these
       placeholders will get the values of your dataset that you pass in
       the run() function!
     * then, you build up the network. you first start by flattening the
       input with the help of the flatten() function, which will give you
       an array of shape [none, 784] instead of the [none, 28, 28], which
       is the shape of your grayscale images.
     * after you have flattened the input, you construct a fully connected
       layer that generates logits of size [none, 62]. logits is the
       function operates on the unscaled output of previous layers, and
       that uses the relative scale to understand the units is linear.
     * with the multi-layer id88 built out, you can define the loss
       function. the choice for a id168 depends on the task that
       you have at hand: in this case, you make use of
sparse_softmax_cross_id178_with_logits()
     * this computes sparse softmax cross id178 between logits and
       labels. in other words, it measures the id203 error in
       discrete classification tasks in which the classes are mutually
       exclusive. this means that each entry is in exactly one class.
       here, a traffic sign can only have one single label. remember that,
       while regression is used to predict continuous values,
       classification is used to predict discrete values or classes of
       data points. you wrap this function with reduce_mean(), which
       computes the mean of elements across dimensions of a tensor.
     * you also want to define a training optimizer; some of the most
       popular optimization algorithms used are the stochastic gradient
       descent (sgd), adam and rmsprop. depending on whichever algorithm
       you choose, you   ll need to tune certain parameters, such as
       learning rate or momentum. in this case, you pick the adam
       optimizer, for which you define the learning rate at 0.001.
     * lastly, you initialize the operations to execute before going over
       to the training.

# import `tensorflow`
import tensorflow as tf

# initialize placeholders
x = tf.placeholder(dtype = tf.float32, shape = [none, 28, 28])
y = tf.placeholder(dtype = tf.int32, shape = [none])

# flatten the input data
images_flat = tf.contrib.layers.flatten(x)

# fully connected layer
logits = tf.contrib.layers.fully_connected(images_flat, 62, tf.nn.relu)

# define a id168
loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_id178_with_logits(labels = y,

                                                                    logits = log
its))
# define an optimizer
train_op = tf.train.adamoptimizer(learning_rate=0.001).minimize(loss)

# convert logits to label indexes
correct_pred = tf.argmax(logits, 1)

# define an accuracy metric
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

   you have now successfully created your first neural network with
   tensorflow!

   if you want, you can also print out the values of (most of) the
   variables to get a quick recap or checkup of what you have just coded
   up:
print("images_flat: ", images_flat)
print("logits: ", logits)
print("loss: ", loss)
print("predicted_labels: ", correct_pred)

   tip: if you see an error like    module 'pandas' has no attribute
   'computation'   , consider upgrading the packages dask by running pip
   install --upgrade dask in your command line. see [25]this stackoverflow
   post for more information.

running the neural network

   now that you have built up your model layer by layer, it   s time to
   actually run it! to do this, you first need to initialize a session
   with the help of session() to which you can pass your graph that you
   defined in the previous section. next, you can run the session with
   run(), to which you pass the initialized operations in the form of the
   init variable that you also defined in the previous section.

   next, you can use this initialized session to start epochs or training
   loops. in this case, you pick 201 because you want to be able to
   register the last loss_value; in the loop, you run the session with the
   training optimizer and the loss (or accuracy) metric that you defined
   in the previous section. you also pass a feed_dict argument, with which
   you feed data to the model. after every 10 epochs, you   ll get a log
   that gives you more insights into the loss or cost of the model.

   as you have seen in the section on the tensorflow basics, there is no
   need to close the session manually; this is done for you. however, if
   you want to try out a different setup, you probably will need to do so
   with sess.close() if you have defined your session as sess, like in the
   code chunk below:
tf.set_random_seed(1234)
sess = tf.session()

sess.run(tf.global_variables_initializer())

for i in range(201):
        print('epoch', i)
        _, accuracy_val = sess.run([train_op, accuracy], feed_dict={x: images28,
 y: labels})
        if i % 10 == 0:
            print("loss: ", loss)
        print('done with epoch')

   remember that you can also run the following piece of code, but that
   one will immediately close the session afterward, just like you saw in
   the introduction of this tutorial:
tf.set_random_seed(1234)

with tf.session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(201):
        _, loss_value = sess.run([train_op, loss], feed_dict={x: images28, y: la
bels})
        if i % 10 == 0:
            print("loss: ", loss)

   note that you make use of global_variables_initializer() because the
   initialize_all_variables() function is deprecated.

   you have now successfully trained your model! that wasn   t too hard, was
   it?

evaluating your neural network

   you   re not entirely there yet; you still need to evaluate your neural
   network. in this case, you can already try to get a glimpse of well
   your model performs by picking 10 random images and by comparing the
   predicted labels with the real labels.

   you can first print them out, but why not use matplotlib to plot the
   traffic signs themselves and make a visual comparison?
# import `matplotlib`
import matplotlib.pyplot as plt
import random

# pick 10 random images
sample_indexes = random.sample(range(len(images28)), 10)
sample_images = [images28[i] for i in sample_indexes]
sample_labels = [labels[i] for i in sample_indexes]

# run the "correct_pred" operation
predicted = sess.run([correct_pred], feed_dict={x: sample_images})[0]

# print the real and predicted labels
print(sample_labels)
print(predicted)

# display the predictions and the ground truth visually.
fig = plt.figure(figsize=(10, 10))
for i in range(len(sample_images)):
    truth = sample_labels[i]
    prediction = predicted[i]
    plt.subplot(5, 2,1+i)
    plt.axis('off')
    color='green' if truth == prediction else 'red'
    plt.text(40, 10, "truth:        {0}\nprediction: {1}".format(truth, predicti
on),
             fontsize=12, color=color)
    plt.imshow(sample_images[i],  cmap="gray")

plt.show()

   [content_traffic_signs8.png]

   however, only looking at random images don   t give you many insights
   into how well your model actually performs. that   s why you   ll load in
   the test data.

   note that you make use of the load_data() function, which you defined
   at the start of this tutorial.
# import `skimage`
from skimage import transform

# load the test data
test_images, test_labels = load_data(test_data_directory)

# transform the images to 28 by 28 pixels
test_images28 = [transform.resize(image, (28, 28)) for image in test_images]

# convert to grayscale
from skimage.color import rgb2gray
test_images28 = rgb2gray(np.array(test_images28))

# run predictions against the full test set.
predicted = sess.run([correct_pred], feed_dict={x: test_images28})[0]

# calculate correct matches
match_count = sum([int(y == y_) for y, y_ in zip(test_labels, predicted)])

# calculate the accuracy
accuracy = match_count / len(test_labels)

# print the accuracy
print("accuracy: {:.3f}".format(accuracy))

   remember to close off the session with sess.close() in case you didn't
   use the with tf.session() as sess: to start your tensorflow session.

where to go next?

   if you want to continue working with this dataset and the model that
   you have put together in this tutorial, try out the following things:
     * apply regularized lda on the data before you feed it to your model.
       this is a suggestion that comes from [26]one of the original
       papers, written by the researchers that gathered and analyzed this
       dataset.
     * you could also, as said in the tutorial itself, also look at some
       other data augmentation operations that you can perform on the
       traffic sign images. additionally, you could also try to tweak this
       network further; the one that you have created now was fairly
       simple.
     * early stopping: keep track of the training and testing error while
       you train the neural network. stop training when both errors go
       down and then suddenly go back up - this is a sign that the neural
       network has started to overfit the training data.
     * play around with the optimizers.

   make sure to check out the [27]machine learning with tensorflow book,
   written by nishant shukla.

   tip also check out the [28]tensorflow playground and the
   [29]tensorboard.

   if you want to keep on working with images, definitely check out
   datacamp   s [30]scikit-learn tutorial, which tackles the mnist dataset
   with the help of pca, id116 and support vector machines (id166s). or
   take a look at other tutorials such as [31]this one that uses the
   belgian traffic signs dataset.
   300
   300
   [32]0
   related posts
   must read
   python
   +1

[33]python machine learning: scikit-learn tutorial

   karlijn willems
   february 25th, 2019
   must read
   python
   +4

[34]keras tutorial: deep learning in python

   karlijn willems
   february 4th, 2019
   must read
   machine learning
   +4

[35]detecting fake news with scikit-learn

   katharine jarmul
   august 24th, 2017
   (button)
   post a comment

   [36]subscribe to rss
   [37]about[38]terms[39]privacy

   want to leave a comment?

references

   visible links
   1. https://www.datacamp.com/users/sign_in
   2. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#comments
   3. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#tensors
   4. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#install
   5. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#basics
   6. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#explore
   7. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#manipulate
   8. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#model
   9. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#train
  10. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#evaluate
  11. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#further
  12. https://github.com/datacamp/datacamp-community-tutorials
  13. https://www.datacamp.com/courses/deep-learning-in-python
  14. https://www.datacamp.com/community/tutorials/deep-learning-python
  15. https://www.datacamp.com/community/tutorials/keras-r-deep-learning
  16. https://www.youtube.com/watch?v=f5liquk0ztw
  17. https://www.datacamp.com/community/tutorials/keras-r-deep-learning
  18. https://www.tensorflow.org/install/
  19. https://www.tensorflow.org/install/install_windows
  20. https://www.tensorflow.org/api_docs/python/tf/placeholder
  21. https://www.tensorflow.org/api_docs/python/tf/variable
  22. http://btsd.ethz.ch/shareddata/
  23. https://www.datacamp.com/community/tutorials/functions-python-tutorial
  24. https://stackoverflow.com/questions/39805697/skimage-why-does-rgb2gray-from-skimage-color-result-in-a-colored-image
  25. https://stackoverflow.com/questions/43833081/attributeerror-module-object-has-no-attribute-computation
  26. http://btsd.ethz.ch/shareddata/publications/mathias-ijid98-2013.pdf
  27. https://www.manning.com/books/machine-learning-with-tensorflow
  28. http://playground.tensorflow.org/#activation=tanh&batchsize=10&dataset=circle&regdataset=reg-plane&learningrate=0.03&id173rate=0&noise=0&networkshape=4,2&seed=0.90110&showtestdata=false&discretize=false&perctraindata=50&x=true&y=true&xtimesy=false&xsquared=false&ysquared=false&cosx=false&sinx=false&cosy=false&siny=false&collectstats=false&problem=classification&initzero=false&hidetext=false
  29. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  30. https://www.datacamp.com/community/tutorials/machine-learning-python
  31. https://github.com/waleedka/traffic-signs-tensorflow/blob/master/notebook1.ipynb
  32. https://www.datacamp.com/community/tutorials/tensorflow-tutorial#comments
  33. https://www.datacamp.com/community/tutorials/machine-learning-python
  34. https://www.datacamp.com/community/tutorials/deep-learning-python
  35. https://www.datacamp.com/community/tutorials/scikit-learn-fake-news
  36. https://www.datacamp.com/community/rss.xml
  37. https://www.datacamp.com/about
  38. https://www.datacamp.com/terms-of-use
  39. https://www.datacamp.com/privacy-policy

   hidden links:
  41. https://www.datacamp.com/
  42. https://www.datacamp.com/community
  43. https://www.datacamp.com/community/tutorials
  44. https://www.datacamp.com/community/data-science-cheatsheets
  45. https://www.datacamp.com/community/open-courses
  46. https://www.datacamp.com/community/podcast
  47. https://www.datacamp.com/community/chat
  48. https://www.datacamp.com/community/blog
  49. https://www.datacamp.com/community/tech
  50. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  51. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  52. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  53. https://www.datacamp.com/profile/karlijn
  54. https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  55. https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  56. https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/tensorflow-tutorial
  57. https://www.datacamp.com/profile/karlijn
  58. https://www.datacamp.com/profile/karlijn
  59. https://www.datacamp.com/profile/katharinecc6b90c27e1b40129c2745c9215cc689
  60. https://www.facebook.com/pages/datacamp/726282547396228
  61. https://twitter.com/datacamp
  62. https://www.linkedin.com/company/datamind-org
  63. https://www.youtube.com/channel/uc79gv3myp6zkiswyemeik9a
