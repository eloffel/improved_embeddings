   #[1]adit deshpande - cs undergrad at ucla ('19)

   [2][pic.jpg]

[3]adit deshpande

   cs undergrad at ucla ('19)

   [4]blog [5]about [6]github [7]projects [8]resume

why machine learning is a metaphor for life

   [cover10th.png]

   seriously. hear me out on this.

   the more i learn about ml, the more i see the number of similarities
   there are between life and machine learning concepts.

   specifically, let   s think about neural networks.
   [metaphor1.png]

   let's think of a neural net that has a bunch of input nodes and one
   output node. the input features are encapsulated in some input vector
   x, and we   d like to come up with a single scalar prediction   . the way
   you compute    is by passing x through a series of linear operations
   with weight matrices mixed in with a couple id180.
   simple enough. but in order to get accurate    predictions, we need to
   first train our network.

   when training neural networks, the goal is to be able to minimize a
   id168.
   [metaphor2.png]

   that   s the name of the game. we want to be able to minimize the
   difference between the actual labels in our training set and our
   predictions, and adjust the weights of our network in hope of training
   it well enough so that it can generalize to test examples.

   i realized that this idea of a minimizing a id168 kind of
   relates to our daily lives. think about it. when you drove to
   work/school this morning, did you take the fastest route possible or
   did you take a couple of random highways and a few side streets,
   eventually ending up at your destination? when you have an important
   project due, do you waste time on facebook or do you buckle down and
   put away your phone (hopefully there's only one right answer here).
   when you're a coder who's working on a couple of difficult assignments,
   do you calmly split up the jobs and work on each one at a time or do
   you panic and work for 36 hours straight to get them done (yes, there
   is a masochistic answer but we're not going with that).

   the idea here is that we   re all, consciously or subconsciously, solving
   optimization problems every single day. we   re always trying to minimize
   the amount of stress, the number of distractions, and the time it takes
   to do something.
   [metaphor3.png]

   the first time you drove to work/school, it's very possible that you
   could've taken an inefficient and long route. however, over time, you
   start to recognize traffic and shortcuts, and eventually start to
   minimize the time spent driving.

   what about those times where you can't seem to break the habit of
   checking your news feed every hour, or the times where you feel like
   there's no way to decrease the amount of soul crushing stress of school
   or work? well, we can think of those as just local minima or saddle
   points. in ml, these are points in the vector space where the gradient
   is close to 0, thus making it difficult to minimize f(x) any further.
   in life, these are moments where we feel like there's nothing we can
   really do, no changes we can make to alleviate the current situation.
   fighting these local minima and saddle points requires second order
   optimization and some other complex techniques. from a high level, they
   look for ways to escape the unfavorable points by computing additional
   information in the form of hessians, curvature info, etc. so, in life,
   when you feel like you're in one of those unpleasant local minimas,
   just remember to always look around because there could always useful
   information to help you out of your current situation and further
   minimize your f(x).

   alright, let's look at a different part of the ml pipeline now.

   one of the other main components in your ml model is your training
   dataset. you have some set x where each x[i]     x is a particular
   collection of features (in vector form) describing the input example,
   and you have some set y where each y[i]     y is a label that describes
   the category for classification or a real valued number for regression.
   these examples are there to help us train our network to output
   accurate    values.

   if our lives have so many optimization problems, what are the x[i]   s
   and y[i]   s? in ml problems, x[i] can be a matrix of pixel values, a
   word vector, or the spectrogram of an audio clip, while y[i] is often a
   category or a real valued number.
   [metaphor4.png]

   let   s take the example of driving to work. in real life, we can think
   of that input vector as describing the set of circumstances surrounding
   that day. was there an accident on the major freeway? is there bad
   weather right now? am i leaving during rush hour? all of this
   information is encapsulated in some mental representation in our
   brains, and is processed to be able to determine a driving route r.
   based on that route, we record the actual time it took to reach the
   destination, which we'll denote f(x, r). we train our mental model so
   that f(x, r) gets as close to y[i], the optimal amount of time it takes
   to get to the destination.
   [metaphor5.png]

   you can think of every single day as one example pair (x[i], y[i]) in
   our training set (x,y). each x[i] represents the conditions and
   each y[i] represents the optimal time taken. in ml, we train our
   network by looking at our training examples and updating weights based
   on those example pairs. for us, we   re training our own network to get
   better and better at finding routes and minimizing the difference
   between f(x, r) and y[i] by looking at our own past experiences. these
   act as our training set. we think about that day we could have turned
   left at that side street. we think about the times we should   ve taken
   101 instead of 280. and we also think about the times we made it to
   work in only 10 minutes because we merged into the fastest lane in the
   freeway. we   re constantly using this training set as an indicator for
   how we should act in the future.

   alright let's switch gears a bit.

   one of the chief concerns with machine learning is the prospect of
   overfitting. this is the moment where your ml model fits too well to
   the dataset that it   s trained on. this results in poor generalization
   performance on data it hasn't seen before.
   [metaphor6.png]

   in life, this can happen when perhaps we spend too much time looking at
   the past moments in our lives as indicators of what   s to come. our
   minds are so glued to the experiences in our training set that we
   aren't able to respond well to unique events we haven't encountered
   yet.

   it   s when we think about our previous breakups as indicator to how
   future relationships will turn out. it   s when we   re unconfident about a
   final when we remember how poorly we did on the midterms. overfitting
   can have disastrous consequences as test accuracy for a model is almost
   never as good as the accuracy during training. when we have that
   mentality of replicating our behavior and thought processes based on
   past experiences, we become inflexible and can   t properly adapt to
   what   s right in front us. if we   re constantly thinking about that time
   we forgot our lines in the annual elementary school play, we   ll never
   have the confidence and belief to take the stage once more (true
   story).

   what   s the solution to this? well, in machine learning, we use
   regularizers. the first form (and the most common) of id173
   that i first learned about was l2 id173 or weight decay.
   [metaphor7.png]

   this type of id173 is basically imposing a soft constraint on
   the cost function. we   re telling the network    hey, we want you to
   minimize the loss from the training examples, but it would also be cool
   if you keep the weights of your network at a low value because your
   cost is gonna increase a lot if those values are high    (btw, a hard
   constraint would mean that we put specific limits on the weight values
   and don   t allow them to increase beyond some threshold).

   so how does this regularizer help in real life? let   s consider that
   scenario of finding a good relationship after past experiences with
   breakups. let   s say that we have some f(x) which represents the overall
   happiness you get from the relationship. ideally, we   d like to maximize
   this (or minimize its negative). given the low past values for f(x),
   there   s a chance we could overfit to the training data and simply fall
   down the same holes as the previous relationships. so instead of just
   focusing on maximizing f(x), we should also pay attention to how our
   weights w are structured. maybe we should impose soft constraints on
   the amount (or lack) of communication or specific shared activities.
   this forces us to stop fitting so tightly to the past experiences, but
   instead focusing on adjusting parameters in ways that can help us
   generalize better to new opportunities.

   other regularizers often come in the form of dropout, a simpler
   network, and data augmentation. maybe we can    forget    about some of the
   features describing a particular event in our past. maybe we can stop
   thinking about so many different features or complicated computations
   or representations in our attempts to get a favorable output value. or
   maybe we can simply obtain more data by just living through more
   experiences.

   alright, last one.

   another cool analogy is that of the epsilon greedy policy. this is a
   term used in id23 to fight the problem of exploration
   vs exploitation. the basic idea is that the rl agent will take a random
   action (instead of the optimal action according to its current policy)
   with id203   , in hope of searching a larger area of the state
   space, and eventually getting a better reward.

   you can relate this to life whenever you   re thinking of trying
   something new and going outside your comfort zone. while your initial
   thought process might be to just do the activity that you know will
   give you a solid reward (e.g watching netflix at home), you could
   explore a different state space and try an activity you haven   t done
   before, like oh i don   t know, go sky diving. the reward of such an
   activity might be lower and you may end up hating it and having a
   perpetual fear of heights, but there   s also a chance that it could be
   an absolutely incredible experience that was much better than any house
   of cards episode.

   so what do we do? do we go with the activity we know will give us the
   highest reward or do we try something new?
   [metaphor8.png]

   does this all make sense? yeah? no?

   or maybe i   m going crazy because of the amount of ml related arxiv
   papers i   ve read over the past month? not sure .
   [metaphor9.png]

   concepts i couldn't find analogies for:
     * batch norm
     * adam (i do have a friend named adam, don't think that counts
       though)
     * weight initialization
     * cross validation
     * id26

   dueces.
   [9]sources [10]tweet

   written on august 16, 2017

   please enable javascript to view the [11]comments powered by disqus.

references

   visible links
   1. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
   2. https://adeshpande3.github.io/adeshpande3.github.io/
   3. https://adeshpande3.github.io/adeshpande3.github.io/
   4. https://adeshpande3.github.io/adeshpande3.github.io/
   5. https://adeshpande3.github.io/adeshpande3.github.io/about
   6. https://github.com/adeshpande3
   7. https://adeshpande3.github.io/adeshpande3.github.io/projects
   8. https://adeshpande3.github.io/adeshpande3.github.io/resume.pdf
   9. https://adeshpande3.github.io/assets/sources10.txt
  10. https://twitter.com/share
  11. http://disqus.com/?ref_noscript

   hidden links:
  13. mailto:adeshpande3@g.ucla.edu
  14. https://www.facebook.com/adit.deshpande.5
  15. https://github.com/adeshpande3
  16. https://instagram.com/thejugglinguy
  17. https://www.linkedin.com/in/aditdeshpande
  18. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
  19. https://www.twitter.com/aditdeshpande3
