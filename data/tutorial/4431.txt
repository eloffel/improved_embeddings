conll 2017 shared task

     * [1]home
     * [2]proceedings | [3]program (schedule)
     * [4]paper submission guidelines
     * [5]data | [6]surprise
     * [7]evaluation | [8]baseline | [9]results
     * [10]tira | [11]source code proceedings
     * [12]organization
     * [13]registration | [14]teams
     * [15]timeline | [16]announcements

contact

     * [17]ud.conll.shared.task.2017@gmail.com

news

     * [18]next shared task here!
     * september 28: [19]system outputs published
     * july 31: [20]acl anthology
     * june 23: [21]proceedings
     * may 28: [22]systems in a nutshell
     * may 18: [23]test data made public
     * may 16: [24]main results
     * may 8: [25]test phase started
     * may 1: [26]surprise languages
     * march 16: [27]crawled raw data
     * march 15: [28]baseline models
     * march 11: [29]eval script 1.0
     * march 9: [30]additional resources
     * march 1: [31]training data

   there will be a second shared task at conll 2018 in brussels. see
   [32]here!

multilingual parsing from raw text to universal dependencies

   ten years ago, two [33]conll shared tasks were a major milestone for
   parsing research in general and id33 in particular. for
   the first time dependency treebanks in more than ten languages were
   available for learning parsers; many of them were used in follow-up
   work, evaluating parsers on multiple languages became a standard; and
   multiple state-of-the art, open-source parsers became available,
   facilitating production of dependency structures to be used in
   downstream applications. while the 2006 and 2007 tasks were extremely
   important in setting the scene for the following years, there were also
   limitations that complicated application of their results: 1.
   gold-standard id121 and tags in the test data moved the tasks
   away from real-world scenarios, and 2. incompatible annotation schemes
   made cross-linguistic comparison impossible. conll 2017 will pick up
   the threads of the pioneering tasks and address these two issues.

   the focus of the 2017 task is learning syntactic dependency parsers
   that can work in a real-world setting, starting from raw text, and that
   can work over many typologically different languages, even surprise
   languages for which there is little or no training data, by exploiting
   a common syntactic annotation standard. this task has been made
   possible by the [34]universal dependencies initiative (ud), which has
   developed treebanks for 40+ languages with cross-linguistically
   consistent annotation and recoverability of the original raw texts. for
   the shared task, the universal dependencies version 2 (ud v2)
   annotation scheme will be used.

   participants will get ud treebanks in many languages, with raw text,
   gold-standard sentence and id40, pos tags, dependency
   relations, and in many cases also lemmas and morphological features.
   the test data will contain none of the gold-standard annotations, but
   baseline predicted segmentation and pos tags will be available. labeled
   attachment score (las) will be computed for every test set, and the
   macro-average of the scores over all test sets will provide the main
   system ranking.

   the test sets will include a few surprise languages. we will not
   provide training data for these languages, only a small sample shortly
   before the test phase. to succeed in parsing these languages, systems
   will have to employ low-resource language techniques, utilizing data
   from other languages.

   there will be no separate open and closed tracks. instead, we will
   include every system in a single track, which will be formally closed,
   but where the list of permitted resources is rather broad and includes
   large raw corpora and parallel corpora (see the [35]data description).

   participating systems will have to find labeled syntactic dependencies
   between words, i.e. a syntactic head for each word, and a label
   classifying the type of the dependency relation. participants will
   parse raw text where no gold-standard pre-processing (id121,
   lemmas, morphology) is available. however, there are at least two
   open-source pipelines ([36]udpipe and [37]syntaxnet) that the
   participants can run instead of training their own models for any steps
   preceding the dependency analysis. we will even provide variants of the
   test data that have been preprocessed by udpipe. we believe that this
   makes the task reasonably accessible.

   the task is open to everyone. the organizers rely, as is usual in large
   shared tasks, on the honesty of all participants who might have some
   prior knowledge of part of the data that will eventually be used for
   evaluation, not to unfairly use such knowledge. the only exception is
   the chair of the organizing team, who cannot submit a system, and who
   will serve as an authority to resolve any disputes concerning ethical
   issues or completeness of system descriptions.

acknowledgments

   the organization of the shared task was partially supported by the
   following projects:
     * [ga_cr_logo.jpg] czech science foundation (ga  r) project no.
       15-10472s.
     * [cracker-logo-no-tag-large.png] cracker, an eu h2020 project.

references

   1. http://universaldependencies.org/conll17/
   2. http://universaldependencies.org/conll17/proceedings/
   3. http://universaldependencies.org/conll17/program.html
   4. http://universaldependencies.org/conll17/submissions.html
   5. http://universaldependencies.org/conll17/data.html
   6. http://universaldependencies.org/conll17/surprise.html
   7. http://universaldependencies.org/conll17/evaluation.html
   8. http://universaldependencies.org/conll17/baseline.html
   9. http://universaldependencies.org/conll17/results.html
  10. http://universaldependencies.org/conll17/tira.html
  11. http://github.com/conll-ud-2017
  12. http://universaldependencies.org/conll17/organization.html
  13. http://universaldependencies.org/conll17/registration.html
  14. http://universaldependencies.org/conll17/registered-teams.html
  15. http://universaldependencies.org/conll17/timeline.html
  16. http://universaldependencies.org/conll17/announcements.html
  17. mailto:ud.conll.shared.task.2017@gmail.com
  18. http://universaldependencies.org/conll18/
  19. http://hdl.handle.net/11234/1-2424
  20. http://www.aclweb.org/anthology/k/k17/#3000
  21. http://universaldependencies.org/conll17/proceedings/
  22. http://universaldependencies.org/conll17/systems-in-a-nutshell.html
  23. http://hdl.handle.net/11234/1-2184
  24. http://universaldependencies.org/conll17/results.html
  25. http://universaldependencies.org/conll17/tira.html#test-phase
  26. http://universaldependencies.org/conll17/surprise.html
  27. http://universaldependencies.org/conll17/data.html#raw-data
  28. http://universaldependencies.org/conll17/data.html#treebanks
  29. http://universaldependencies.org/conll17/eval.zip
  30. http://universaldependencies.org/conll17/data.html#additional-resources
  31. http://hdl.handle.net/11234/1-1976
  32. http://universaldependencies.org/conll18/
  33. http://www.conll.org/
  34. http://universaldependencies.org/
  35. http://universaldependencies.org/conll17/data.html
  36. https://ufal.mff.cuni.cz/udpipe/
  37. https://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/index.html
