   #[1]adventures in machine learning    a microsoft cntk tutorial in
   python     build a neural network comments feed [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

a microsoft cntk tutorial in python     build a neural network

   by [9]admin | [10]cntk

     * you are here:
     * [11]home
     * [12]deep learning
     * [13]a microsoft cntk tutorial in python     build a neural network

   aug 03
   [14]3
   pytorch tutorial - fully connected neural network example architecture

   in previous tutorials ([15]python tensorflow tutorial, [16]id98
   tutorial, and the [17]id97 tutorial) on deep learning, i have
   taught how to build networks in the tensorflow deep learning framework.
    there is no doubt that tensorflow is an immensely popular deep
   learning framework at present, with a large community supporting it.
   however, there is another contending framework which i think may
   actually be better     it is called the [18]microsoft cognitive toolkit,
   or more commonly known as cntk.  why do i believe it to be better?  two
   main reasons     it has a more intuitive and easy to use python api than
   tensorflow, and it is faster.  it also can be used as a back-end to
   [19]keras, but i would argue that there is little benefit to doing so
   as cntk is already very streamlined.  how much faster is it?  some
   [20]benchmarks show that is it generally faster than tensorflow and up
   to 5-10 times faster for recurrent / id137.  that   s a pretty
   impressive achievement.  this article is a comprehensive cntk tutorial
   to teach you more about this exciting framework.

   should you switch from using tensorflow to cntk?  tensorflow definitely
   has much more hype than microsoft   s cntk and therefore a bigger
   development community, more answers on stack overflow and so on.  also,
   many people are down on microsoft which is often perceived as a big
   greedy corporation.  however, microsoft has opened up a lot, and cntk
   is now open-source, so i would recommend giving it a try.  let me know
   what you think in the comments.  this post will be a comprehensive cntk
   tutorial which you can use to get familiar with the framework     i
   suspect that you might be surprised at how streamlined it is.

   before going on     if you   re not familiar with neural networks i   d
   recommend my [21]neural networks tutorial or the course below as an
   introduction.  also, cntk uses the computational graph methodology, as
   does tensorflow.  if you   re unfamiliar with this concept     check out
   the first section of my [22]tensorflow tutorial.  the code for this
   course, which is loosely based on the cntk tutorial [23]here, can be
   found on this site   s [24]github page.  also checkout liping yang   s good
   collection of cntk resources [25]here.
     __________________________________________________________________

   recommended online course for neural networks: if you like video
   courses, i   d recommend the following inexpensive udemy course on neural
   networks: [26]deep learning a-z: hands-on id158s
   [show?id=jbc0n5zkdzk&amp;bids=323058.1151632&amp;type=2&amp;subid=0]
     __________________________________________________________________

cntk inputs and variables

   the first thing to learn about any deep learning framework is how it
   deals with input data, variables and how it executes operations/nodes
   in the computational graph.  in this cntk tutorial, we   ll be creating a
   three layer densely connected neural network to recognize handwritten
   images in the mnist data-set, so in the below explanations, i   ll be
   using examples from this problem.  see the above-mentioned tutorials
   ([27]here and [28]here) for other implementations of the mnist
   classification problem.

variables

   for the mnist classification task, each training sample will have a
   flattened 28 x 28 = 784 pixels grey scale input and ten labels to
   classify (one for each hand-written digit).  in cntk we can declare the
   variables to hold this data like so:
import cntk as c
input_dim = 784
num_output_classes = 10
feature = c.input_variable(input_dim)
label = c.input_variable(num_output_classes)

   these input_variable functions are like the placeholder variables in
   tensorflow.  however, cntk removes the necessity to explicitly identify
   the number of samples/batch size and we simply supply the dimensions
   for each training/evaluation sample (in tensorflow, one had to
   explicitly use the    ?    symbol to designate unknown batch size).  in
   this case, we have the flattened 28 x 28 = 784 pixel input and 10
   output labels / classes.  if we wanted to maintain the un-flattened
   input shape for, say, a [29]convolutional neural network task, we would
   instead specify input_dim = (1, 28, 28).

   as will be shown later, these variables can be easily    loaded up    with
   our batch training or evaluation data.

data readers

   some of the most time consuming (and annoying tasks) in machine
   learning involve getting your training data into a good format, reading
   it into your model and setting up batch retrieval functions (if you   re
   not sure about what a    mini-batch    is in this context, check out my
   [30]stochastic id119 tutorial).  the cntk framework has some
   helper modules which can aid in at least some of this process.  for
   this tutorial, we will be using the mnist data-set obtained from the
   following website     [31]http://yann.lecun.com/exdb/mnist/.   to access
   this data, we need to download two python files from the following
   repository: [32]https://github.com/microsoft/cntk/tree/v2.0/examples/im
   age/datasets/mnist called install_mnist.py and mnist_utils.py. after
   running install_mnist.py, it will download the data-set into your
   project directory.  once this has been done, you can run two different
   functions     the first of which is called
   ctfdeserializer().  ctfdeserializer() reads text files in a special
   cntk format.  the cntk format requires a sample per line, with the data
   of the sample being given a name/label using the pipe    |    delimiter.
   for our mnist case, the data has been put in the following format for a
   single sample:

   |labels 0 0 0 0 0 1 0 0 0 0 |features 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
   0 0 0 0   .

   as can be observed, we have two fields per sample     the label of the
   mnist image, which corresponds to the one-hot designation of the digit
   in the image (i.e. a digit of    2    will be represented as 0 0 1 0 0 0 0
   0 0 0) and the features of the image which represent each of the 784
   (flattened) pixels of the image.  we read this cntk file format by
   using the ctfdeserializer() like follows:
from cntk.io import minibatchsource, ctfdeserializer, streamdef, streamdefs
ctfdeserializer(path, streamdefs(
        features=streamdef(field='features', shape=input_dim),
        labels=streamdef(field='labels', shape=num_output_classes)))

   the first argument to the ctfdeserializer() function is the path of the
   data it is to read from.  the second argument is a
   streamdefs() dictionary-like object.  this object assigns a set of keys
   to different streamdef() objects.  a streamdef() object specifies what
   pipe (   |   ) label it should be searching for in the ct file, and what
   the size of data per sample it should be retrieving.  so if we take the
      |labels    pipe designation in the file, we know that it should be
   followed by the number of labels: num_output_classes=10.  we then feed
   the streamdefs() object, which contains the description of each
   data-point per sample, to the ctfdeserializer() function.  when this
   function is called, it will read the data into the specified keys
   (features and labels).

   the next step is to setup the cntk object which can draw random
   mini-batch samples from our ctfdeserializer().  this object is
   called minibatchsource().  to use it, we simply supply it a serializer,
   in this case, our previously mentioned ctfdeserializer().  it will then
   automatically draw random mini-batch samples from our serialized data
   source of a size that we will specify later when we look at
   the training_session() object.  to create the minibatchsource() object,
   we simply do the following:
reader_train = minibatchsource(ctfdeserializer(path, streamdefs(
        features=streamdef(field='features', shape=input_dim),
        labels=streamdef(field='labels', shape=num_output_classes))))

   the above variables, functions, and operations now allow us to draw
   data into our training and evaluation procedures in the correct
   fashion.

cntk operations

   as with tensorflow, cntk has operations which are nodes in a
   computational graph.  these nodes and operations also flow into each
   other.  in other words, if the output of an operation b first requires
   the output of another operation a, then cntk understands this
   relationship.  if we wish to know what the output of operation b is,
   all we have to do is call b.eval() and this will automatically
   call a.eval().  in cntk there are all sorts of operations which we can
   run     from simple multiplication and division to softmax and
   convolutional operations.  again, if we need to explicitly evaluate any
   of these in our code all we have to do is call the operation by
   executing eval() on the operation name.  however, most operations we
   won   t have to explicitly evaluate, they will simply be evaluated
   implicitly during the execution of the final layer of our networks.

   in this mnist example, a simple operation we need to perform is to
   scale the input features.  the grey scale pixel images will have
   maximum values up to 256 in the raw data.  it is better to scale these
   feature values to between 0 and 1, therefore we need to multiply all
   the values by 1/256 ~ 0.00390 to achieve this scaling:
from cntk.ops import relu, element_times, constant
scaled_input = element_times(constant(0.00390625), feature)

   here we declare a constant of 0.00390 and use the element_times()
   operation to multiply it by the input variable feature which we
   declared earlier.  this scales our input data to between 0 and 1.

   now we are ready to start building our densely connected neural network
   using cntk layers.

cntk layers

creating a single layer

   as with any deep learning framework, cntk gives us the ability to
   create neural network layers.  these layers come in many flavors, such
   as dense, convolution, maxpooling, recurrent, and lstm, all of which
   can be reviewed [33]here.  in our case, we wish to create some densely
   connected layers for our standard neural network classifier of the
   mnist dataset.  the architecture we are going to create looks like
   this:
   cntk dense mnist architecture

   dense mnist architecture example

   so basically we have our 784 flattened pixel input layer, followed up
   by two hidden layers both of size 200, and finally, our output layer
   which will be softmax activated.  so how do we create our first hidden
   layer?  quite easily in cntk:
from cntk.ops import relu
from cntk.layers import dense
hidden_layers_dim = 200
h1 = dense(hidden_layers_dim, activation=relu)(scaled_input)

   to create a densely connected hidden layer, as shown in the example
   figure above, all we need to do is declare a dense() object.  the first
   argument to this object is the size of the hidden layer     in this case,
   it is equal to 200.  then we specify the node activation type     in this
   case, we will use [34]rectified linear units or relu.  following this
   declaration, we pass the scaled_input variable/operation to the layer
   in brackets     this is the data that will be fed into the first hidden
   layer.  cntk takes care of all the relevant connections and handles the
   dimensions of the input tensor automatically.  it also includes the
   weight and bias variables internally, so you don   t have to go
   explicitly declaring them as you do in tensorflow.  we can supply an
   initialization function for these weights/bias values, by supplying an
   optional argument init.  in the absence of this argument, the
   initialization defaults to the [35]glorot_uniform()
   function/distribution.

   we can then build a hierarchical graph of operations connecting the
   other hidden layer and the output layer:
h2 = dense(hidden_layers_dim, activation=relu)(h1)
z = dense(num_output_classes, activation=none)(h2)

   for h2, we declare an identical dense hidden layer and feed it the
   output of the first layer (h1).  finally, we create an output layer
   equal to the number of output classes, but we set the activation
   function here to none     this is to make the output layer nodes simple
   summing nodes with no activation.  we will apply the softmax function
   to this simple summation later.

   cntk includes some additional options to make the declaration of layers
   easier.

some layer creation helpers

   often one wants to supply the same initialization functions and
   id180 to multiple layers, and it is often the case that
   we have repeating structures in our networks.  in this vein, cntk has
   some helper functions/objects to make our network definitions more
   streamlined.  the first, and probably most important of these is the
   sequential() module     this is very similar to the sequential() paradigm
   used in keras (see [36]this keras tutorial).  it allows the user to
   simply stack layers on top of each other in a sequential fashion
   without having to explicitly pass the output of one layer to the input
   of the next.  in this example, the use of sequential() looks like this:
z = sequential([
        dense(hidden_layers_dim, activation=relu),
        dense(hidden_layers_dim, activation=relu),
        dense(num_output_classes)])(scaled_input)

   here we can see that we   ve simply added our two hidden layers, and the
   final output layer, into a list argument required by the sequential()
   module.  we can reduce this code even further, by using the for()
   function available in the layers cntk interface.  this is basically an
   embedded for loop that can be used in the sequential() module:
num_hidden_layers = 2
z = sequential([for(range(num_hidden_layers),
lambda i: dense(hidden_layers_dim, activation=relu)),
dense(num_output_classes)])(scaled_input)

   the for() loop uses a lambda function to construct the layers     in the
   above, this is performed over two hidden layers, with the final output
   layer appended to the end of this for() construction.

   one final construct in cntk that can assist in stream-lining is the
   layers.default_options() module     this is not particularly useful in
   this example, but it can be useful in more complicated networks, so i
   include it here for illustrative purposes.  basically, it allows the
   user to specify common id180, weight initialization
   procedures, and other options.  we use the python with functionality to
   call it:
with default_options(activation=relu, init=c.glorot_uniform()):
    z = sequential([for(range(num_hidden_layers),
        lambda i: dense(hidden_layers_dim)),
        dense(num_output_classes, activation=none)])(scaled_input)

   notice that we no longer have to specify the activation function for
   the hidden layers, yet we can override the default_option for the
   output layer to none which allows us to apply a softmax in the loss
   function (which will be demonstrated in the next section).  the same
   initialization of the glorot_uniform() is applied to each layer.

   we have now defined the structure of our neural network, and you can
   see how simple it is to do     especially using the sequential() module
   in cntk.  now we have to setup our id168 and training session.

cntk losses, errors, and training

defining the loss and error

   the cntk library has many id168s to choose from in order to
   train our model.  these range from the standard cross id178 and
   squared error to the cosine distance (good for measuring vector
   similarities, such as [37]id27s) and [38]lambda rank.  you
   can also define your own custom id168s.  for our purposes, as
   is often the case for classification tasks, we   ll use the
   cross_id178_with_softmax() option:
ce = cross_id178_with_softmax(z, label)

   here we simply supply the output layer z (which you   ll remember doesn   t
   yet have a nonlinear activation function applied) and the label output
   variable/placeholder and we get the cross id178 loss with softmax
   applied to z.

   next, we want to have some way of assessing the error (or conversely
   accuracy) on our test set, as well as when we are training our model.
   cntk has a few handy metrics we can use, along with the ability to
   define our own.  seeing this is a classification task, we can use the
   handy classification_error() function:
pe = classification_error(z, label)

   now we need to setup the required training objects.

training models in cntk

   in order to perform training in cntk, we first have to define our input
   map, which is simply a dictionary containing all the input and output
   training pairs:
input_map = {
        feature: reader_train.streams.features,
        label: reader_train.streams.labels
    }

   note that here we are using the minibatchsource() object (with the
   associated reader/deserializer) called reader_train that we created
   earlier.  one can access the streams/data by using the dot notation
   shown above.

   the next step is to create a progress writer object, called
   progressprinter.  this object allows us to output metrics, such as the
   loss and classification error, to the command window while training.
   it also allows the ability to print the data to text files, vary
   printing frequency and has a number of methods [39]which bear
   examining.  in this case, we will just use it fairly simply:
num_sweeps_to_train_with = 10
# instantiate progress writers.
progress_writers = [progressprinter(
    tag='training',
    num_epochs=num_sweeps_to_train_with)]

   in the above, the tag argument is what shows up in the log attached to
   each update.  the num_epochs is used to allow a counter to count up to
   the total number of epochs during the training.  this will become clear
   later as we print out the results of our training.

the learning rate schedule object

   it is often the case that, during training, we wish to vary
   optimization variables such as the learning rate.  this can aid in
   convergence to an optimal solution, with learning rates often reduced
   as the number of epochs increase     for example, reducing the step size
   in our id119 steps (for more information about gradient
   descent, see my [40]neural networks tutorial).  we can setup such a
   learning rate schedule using the learning_rate_schedule() object.  with
   this object, we can select a learning rate of, say, 0.1 for the first
   1,000 samples, then a learning rate of 0.01 for the next 1,000 samples,
   then 0.001 for all remaining samples.  to do this, we would create a
   learning_rate_schedule() using the following code:
lr = learning_rate_schedule([0.1, 0.01, 0.001], unittype.sample, 1000)
lr[0], lr[1000], lr[2000]
0.1, 0.01, 0.001

   the first argument in this declaration is the schedule, which is the
   set of learning rate values that will be used as the training
   progresses.  the second argument unittype.sample designates that the
   learning rate will change depending on a number of samples.  the other
   alternative is unittype.minibatch which means that the learning rate
   will change depending on a number of mini batches.  finally, we have
   the epoch size, which is the number of samples in this case, that need
   to be processed before the next change in learning rate.

   in the case of our simple network, we can use the same learning rate
   for all samples, so we just declare our lr variable by:
from cntk.learners import learning_rate_schedule, unittype
lr = learning_rate_schedule(1, unittype.sample)

   for each sample, the learning rate will be 1.

the trainer object

   next, we need to setup a trainer object.  this module is what performs
   the training of our model     therefore we need to feed it a number of
   pieces of information.  first, we need to give it the output layer
   operation (the variable z in our case)     the prior layers will be
   trained using the computational graph structure.  next, we need to give
   it our id168 that we are going to use for computing our
   gradients.  we also want to supply our metric that we will track during
   training.  then we need to specify what type of optimizer to use    
   stochastic id119, adagrad, adam etc.  (see the list
   available [41]here) and also our learning rate schedule.  finally, we
   need to specify any progress writers that we wish to use.  in our case,
   it looks like this:
trainer = trainer(z, (ce, pe), [adadelta(z.parameters, lr)], progress_writers)

   the first argument is our output layer operation z.  the next argument
   can be either the id168 alone or a tuple containing the loss
   function and a metric to track     this is the option we have taken here.
    third, we have a list of optimizers to use     in this case, we use a
   single optimizer called adadelta().  the optimizer takes the final
   layer   s parameters (which implicitly include the previous layer
   parameters via the directed computational graph     see [42]here for more
   information) and a learning rate schedule which we have already
   defined.  finally, we include our progress writer object.

   now we are ready to create a training session object which we can use
   to train the model.

the training session object

   the cntk library has a great way of training models using
   the training_session() object.  this handles all of our mini-batch data
   extraction from the source, our input data, the frequency of logging
   and how long we want to run our training for.  this is what it looks
   like:
minibatch_size = 64
num_samples_per_sweep = 60000
num_sweeps_to_train_with = 10
training_session(
        trainer=trainer,
        mb_source=reader_train,
        mb_size=minibatch_size,
        model_inputs_to_streams=input_map,
        max_samples=num_samples_per_sweep * num_sweeps_to_train_with,
        progress_frequency=num_samples_per_sweep
    ).train()

   first, we supply the training_session() a trainer object over which the
   optimization and parameter learning will occur.  then we provide it a
   source from which to extract mini-batch data from     in this case, it is
   our reader_train minibatchsource object that we created earlier.  we
   then let the session object know how many samples we wish to extract
   per mini-batch.

   next comes our input map.  this matches our input variables
   (label and feature) with the appropriate streams from the deserializer
   object embedded in reader_train.  we then supply the maximum number of
   samples we wish the training session to process     once this limit of
   samples passed through the model has been reached, the training will
   cease.  finally, we supply the frequency at which we wish to print the
   progress via the progress_writers object we created.

   there are other options that training_session() makes available, such
   as saving the model at checkpoints, cross validation, and testing.
   these will be topics for a future post.

   in this case, we are going to try to cover all the data set
   (num_samples_per_sweep=60,000) by sweeping over it 10 times
   (num_sweeps_to_train_with=10).  the selection of data will be random
   via the mini-batches, but it is statistically likely that most of the
   data will be sampled in a mini-batch at some point in the 10 sweeps.

   this is what the output of the training looks like:
   cntk training output

   output from the progress writer during training

   we can see that our model error on the training data set gets down to
   0.9%     not bad!

   however, as any machine learning practitioner will tell you, we should
   test our model on a separate test data set, as we may have overfitted
   our training set.

testing the model

   first, we need to setup another minibatchsource which reads in our test
   data, along with a new input map that refers to the test data:
# load test data
path = abs_path + "\test-28x28_cntk_text.txt"

reader_test = minibatchsource(ctfdeserializer(path, streamdefs(
    features=streamdef(field='features', shape=input_dim),
    labels=streamdef(field='labels', shape=num_output_classes))))

input_map = {
    feature: reader_test.streams.features,
    label: reader_test.streams.labels
}

   a handy thing about the minibatchsource object is that we can extract
   mini-batches from the reader object by using .next_minibatch().  this
   allows us to run through a whole bunch of mini-batches from our test
   set to estimate what the average classification error is on our test
   data.  we can also feed in this mini-batch data, one batch at a time,
   to our trainer object to get the classification error on that batch.
   to do so, the trainer object has a handy method
   called .test_minibatch().  the code below shows how all this works:
# test data for trained model
test_minibatch_size = 1024
num_samples = 10000
num_minibatches_to_test = num_samples / test_minibatch_size
test_result = 0.0
for i in range(0, int(num_minibatches_to_test)):
    mb = reader_test.next_minibatch(test_minibatch_size, input_map=input_map)
    eval_error = trainer.test_minibatch(mb)
    test_result = test_result + eval_error

   by dividing the test_result value in the above code by
   the num_minibatches_to_test variable, we can get our average
   classification error on the test set.  in this case, it is 1.98%    
   pretty good for a densely connected neural network.  in a future post,
   i   ll show you how to create convolutional neural networks in cntk which
   perform even better.

   so there you have it     in this post, i have shown you the basics of
   microsoft   s challenge to tensorflow     the cognitive toolkit or cntk.
   it is an impressive framework, with very streamlined ways of creating
   the model structure and training.  this coupled with proven high speeds
   makes it a serious competitor to tensorflow, and well worth checking
   out.  i hope this post has been a help for you in getting started with
   this deep learning framework.


about the author

     shaheer says:
   [43]april 12, 2018 at 9:36 am

   cntk is so streamlined and so intuitive. fantastic. thanks for the
   tutorial.

     [44]miguel says:
   [45]october 8, 2018 at 12:25 pm

   hi andy, this is a great tutorial. the explanations of all the concepts
   and steps is highly appreciated, this kind of knowledge is impossible
   to find in the documentation, and is what really helps to in-depth
   understanding of this great toolkit and the concepts of neural
   networks.

   i would like to ask you though, to create a tutorial (if possible)
   about how to build a id98 with pooling layers and convolution layers
   using cntk.

   thanks a lot for the great work!!
     * andy says:
       [46]october 8, 2018 at 7:46 pm
       thanks miguel, i appreciate it. i   ll see what i can do

   ____________________ (button)

   recent posts
     * [47]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [48]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [49]keras, eager and tensorflow 2.0     a new tf paradigm
     * [50]introduction to tensorboard and tensorflow visualization
     * [51]tensorflow eager tutorial

   recent comments
     * andry on [52]neural networks tutorial     a pathway to deep learning
     * sandipan on [53]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [54]neural networks tutorial     a pathway to deep learning
     * martin on [55]neural networks tutorial     a pathway to deep learning
     * uri on [56]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [57]march 2019
     * [58]january 2019
     * [59]october 2018
     * [60]september 2018
     * [61]august 2018
     * [62]july 2018
     * [63]june 2018
     * [64]may 2018
     * [65]april 2018
     * [66]march 2018
     * [67]february 2018
     * [68]november 2017
     * [69]october 2017
     * [70]september 2017
     * [71]august 2017
     * [72]july 2017
     * [73]may 2017
     * [74]april 2017
     * [75]march 2017

   categories
     * [76]amazon aws
     * [77]cntk
     * [78]convolutional neural networks
     * [79]cross id178
     * [80]deep learning
     * [81]gensim
     * [82]gpus
     * [83]keras
     * [84]id168s
     * [85]lstms
     * [86]neural networks
     * [87]nlp
     * [88]optimisation
     * [89]pytorch
     * [90]recurrent neural networks
     * [91]id23
     * [92]tensorboard
     * [93]tensorflow
     * [94]tensorflow 2.0
     * [95]weight initialization
     * [96]id97

   meta
     * [97]log in
     * [98]entries rss
     * [99]comments rss
     * [100]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [101]thrive themes | powered by [102]wordpress

   (button) close dialog

   session expired

   [103]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[104]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/microsoft-cntk-tutorial/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/microsoft-cntk-tutorial/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/microsoft-cntk-tutorial/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/
  13. https://adventuresinmachinelearning.com/microsoft-cntk-tutorial/
  14. http://adventuresinmachinelearning.com/microsoft-cntk-tutorial/#comments
  15. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  16. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  17. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  18. https://www.microsoft.com/en-us/cognitive-toolkit/
  19. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  20. http://dlbench.comp.hkbu.edu.hk/
  21. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  22. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  23. https://github.com/microsoft/cntk/blob/v2.1/examples/image/classification/mlp/python/simplemnist.py
  24. https://github.com/adventuresinml/adventures-in-ml-code
  25. http://deeplearning.lipingyang.org/microsoft-cognitive-toolkit-cntk-resources/
  26. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.1151632&type=2&murl=https://www.udemy.com/deeplearning/
  27. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  28. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  29. https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-tensorflow/
  30. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  31. http://yann.lecun.com/exdb/mnist/
  32. https://github.com/microsoft/cntk/tree/v2.0/examples/image/datasets/mnist
  33. https://www.cntk.ai/pythondocs/layerref.html
  34. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  35. http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization
  36. https://adventuresinmachinelearning.com/keras-tutorial-id98-11-lines/
  37. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  38. https://www.microsoft.com/en-us/research/publication/from-ranknet-to-lambdarank-to-lambdamart-an-overview/
  39. https://www.cntk.ai/pythondocs/cntk.logging.progress_print.html
  40. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  41. https://www.cntk.ai/pythondocs/cntk.learners.html
  42. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  43. https://adventuresinmachinelearning.com/microsoft-cntk-tutorial/#comments/5033
  44. https://github.com/miguel2488
  45. https://adventuresinmachinelearning.com/microsoft-cntk-tutorial/#comments/5034
  46. https://adventuresinmachinelearning.com/microsoft-cntk-tutorial/#comments/5035
  47. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  48. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  49. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  50. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  51. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  52. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  53. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  54. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  55. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  56. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  57. https://adventuresinmachinelearning.com/2019/03/
  58. https://adventuresinmachinelearning.com/2019/01/
  59. https://adventuresinmachinelearning.com/2018/10/
  60. https://adventuresinmachinelearning.com/2018/09/
  61. https://adventuresinmachinelearning.com/2018/08/
  62. https://adventuresinmachinelearning.com/2018/07/
  63. https://adventuresinmachinelearning.com/2018/06/
  64. https://adventuresinmachinelearning.com/2018/05/
  65. https://adventuresinmachinelearning.com/2018/04/
  66. https://adventuresinmachinelearning.com/2018/03/
  67. https://adventuresinmachinelearning.com/2018/02/
  68. https://adventuresinmachinelearning.com/2017/11/
  69. https://adventuresinmachinelearning.com/2017/10/
  70. https://adventuresinmachinelearning.com/2017/09/
  71. https://adventuresinmachinelearning.com/2017/08/
  72. https://adventuresinmachinelearning.com/2017/07/
  73. https://adventuresinmachinelearning.com/2017/05/
  74. https://adventuresinmachinelearning.com/2017/04/
  75. https://adventuresinmachinelearning.com/2017/03/
  76. https://adventuresinmachinelearning.com/category/amazon-aws/
  77. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  78. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  79. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  80. https://adventuresinmachinelearning.com/category/deep-learning/
  81. https://adventuresinmachinelearning.com/category/nlp/gensim/
  82. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  83. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  84. https://adventuresinmachinelearning.com/category/loss-functions/
  85. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  86. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  87. https://adventuresinmachinelearning.com/category/nlp/
  88. https://adventuresinmachinelearning.com/category/optimisation/
  89. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  90. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  91. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  92. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  93. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  94. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  95. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  96. https://adventuresinmachinelearning.com/category/nlp/id97/
  97. https://adventuresinmachinelearning.com/wp-login.php
  98. https://adventuresinmachinelearning.com/feed/
  99. https://adventuresinmachinelearning.com/comments/feed/
 100. https://wordpress.org/
 101. https://www.thrivethemes.com/
 102. http://www.wordpress.org/
 103. https://adventuresinmachinelearning.com/wp-login.php
 104. http://adventuresinmachinelearning.com/microsoft-cntk-tutorial/

   hidden links:
 106. https://adventuresinmachinelearning.com/author/admin/
