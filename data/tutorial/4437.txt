   #[1]adventures in machine learning    id97 id27 tutorial
   in python and tensorflow comments feed [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

id97 id27 tutorial in python and tensorflow

   by [9]admin | [10]nlp

     * you are here:
     * [11]home
     * [12]nlp
     * [13]id97 id27 tutorial in python and tensorflow

   jul 21
   [14]18
   gensim id27 softmax trainer

   in coming tutorials on this blog i will be dealing with how to create
   deep learning models that predict text sequences.  however, before we
   get to that point we have to understand some key natural language
   processing (nlp) ideas.  one of the key ideas in nlp is how we can
   efficiently convert words into numeric vectors which can then be    fed
   into    various machine learning models to perform predictions.  the
   current key technique to do this is called    id97    and this is what
   will be covered in this tutorial.  after discussing the relevant
   background material, we will be implementing id97 embedding using
   tensorflow (which makes our lives a lot easier).  to get up to speed in
   tensorflow, check out my [15]tensorflow tutorial. also, if you prefer
   keras     check out my [16]id97 keras tutorial.
     __________________________________________________________________

eager to learn more? get the book [17]here
     __________________________________________________________________

why do we need id97?

   if we want to feed words into machine learning models, unless we are
   using tree based methods, we need to convert the words into some set of
   numeric vectors.  a straight-forward way of doing this would be to use
   a    one-hot    method of converting the word into a sparse representation
   with only one element of the vector set to 1, the rest being zero.
   this is the same method we use for classification tasks     see [18]this
   tutorial.

   so, for the sentence    the cat sat on the mat    we would have the
   following vector representation:

   \begin{equation}
   \begin{pmatrix}
   the \\
   cat \\
   sat \\
   on \\
   the \\
   mat \\
   \end{pmatrix}
   =
   \begin{pmatrix}
   1 & 0 & 0 & 0 & 0 \\
   0 & 1 & 0 & 0 & 0 \\
   0 & 0 & 1 & 0 & 0 \\
   0 & 0 & 0 & 1 & 0 \\
   1 & 0 & 0 & 0 & 0 \\
   0 & 0 & 0 & 0 & 1
   \end{pmatrix}
   \end{equation}

   here we have transformed a six word sentence into a 6  5 matrix, with
   the 5 being the size of the vocabulary (   the    is repeated).  in
   practical applications, however, we will want machine and deep learning
   models to learn from gigantic vocabularies i.e. 10,000 words plus.  you
   can begin to see the efficiency issue of using    one hot   
   representations of the words     the input layer into any neural network
   attempting to model such a vocabulary would have to be at least 10,000
   nodes.  not only that, this method strips away any local context of the
   words     in other words, it strips away information about words which
   commonly appear close together in sentences (or between sentences).

   for instance, we might expect to see    united    and    states    to appear
   close together, or    soviet    and    union   .  or    food    and    eat   , and so
   on.  this method loses all such information, which, if we are trying to
   model natural language, is a large omission.  therefore, we need an
   efficient representation of the text data which also conserves
   information about local word context.  this is where the id97
   methodology comes in.

the id97 methodology

   as mentioned previously, there is two components to the id97
   methodology.  the first is the mapping of a high dimensional one-hot
   style representation of words to a lower dimensional vector. this might
   involve transforming a 10,000 columned matrix into a 300 columned
   matrix, for instance. this process is called id27.  the
   second goal is to do this while still maintaining word context and
   therefore, to some extent, meaning. one approach to achieving these two
   goals in the id97 methodology is by taking an input word and then
   attempting to estimate the id203 of other words appearing close
   to that word.  this is called the skip-gram approach.  the alternative
   method, called continuous bag of words (cbow), does the opposite     it
   takes some context words as input and tries to find the single word
   that has the highest id203 of fitting that context.  in this
   tutorial, we will concentrate on the skip-gram method.

   what   s a gram?  a gram is a group of n words, where n is the gram
   window size.  so for the sentence    the cat sat on the mat   , a 3-gram
   representation of this sentence would be    the cat sat   ,    cat sat on   ,
      sat on the   ,    on the mat   .  the    skip    part refers to the number of
   times an input word is repeated in the data-set with different context
   words (more on this later).  these grams are fed into the id97
   context prediction system. for instance, assume the input word is    cat   
       the id97 tries to predict the context (   the   ,    sat   ) from this
   supplied input word.  the id97 system will move through all the
   supplied grams and input words and attempt to learn appropriate mapping
   vectors (embeddings) which produce high probabilities for the right
   context given the input words.

   what is this id97 prediction system?  nothing other than a neural
   network.

the softmax id97 method

   consider the diagram below     in this case we   ll assume the sentence
      the cat sat on the mat    is part of a much larger text database, with a
   very large vocabulary     say 10,000 words in length.  we want to reduce
   this to a 300 length embedding.
   id97 softmax trainer

   a id97 softmax trainer

   with respect to the diagram above, if we take the word    cat    it will be
   one of the words in the 10,000 word vocabulary.  therefore we can
   represent it as a 10,000 length one-hot vector.  we then interface this
   input vector to a 300 node hidden layer (if you need to scrub up on
   neural networks, see [19]this tutorial).  the weights connecting this
   layer will be our new word vectors     more on this soon.  the
   activations of the nodes in this hidden layer are simply linear
   summations of the weighted inputs (i.e. no non-linear activation, like
   a sigmoid or tanh, is applied).  these nodes are then fed into a
   softmax output layer.  during training, we want to change the weights
   of this neural network so that words surrounding    cat    have a higher
   id203 in the softmax output layer.  so, for instance, if our text
   data set has a lot of dr seuss books, we would want our network to
   assign large probabilities to words like    the   ,    sat    and    on    (given
   lots of sentences like    the cat sat on the mat   ).

   by training this network, we would be creating a 10,000 x 300 weight
   matrix connecting the 10,000 length input with the 300 node hidden
   layer.  each row in this matrix corresponds to a word in our 10,000
   word vocabulary     so we have effectively reduced 10,000 length one-hot
   vector representations of our words to 300 length vectors.  the weight
   matrix essentially becomes a look-up or encoding table of our words.
   not only that, but these weight values contain context information due
   to the way we   ve trained our network.  once we   ve trained the network,
   we abandon the softmax layer and just use the 10,000 x 300 weight
   matrix as our id27 lookup table.

   what does this look like in code?

the softmax id97 method in tensorflow

   as with any machine learning problem, there are two components     the
   first is getting all the data into a usable format, and the next is
   actually performing the training, validation and testing.  first i   ll
   go through how the data can be gathered into a usable format, then
   we   ll talk about the tensorflow graph of the model.  note that the code
   that i will be going through can be found in its entirety at this
   site   s [20]github repository.  in this case, the code is mostly based
   on the tensorflow id97 tutorial [21]here with some personal
   changes.

preparing the text data

   the previously mentioned tensorflow tutorial has a few functions that
   take a text database and transform it so that we can extract input
   words and their associated grams in mini-batches for training the
   id97 system / embeddings (if you   re not sure what    mini-batch   
   means, check out [22]this tutorial).  i   ll briefly talk about each of
   these functions in turn:
def maybe_download(filename, url, expected_bytes):
    """download a file if not present, and make sure it's the right size."""
    if not os.path.exists(filename):
        filename, _ = urllib.request.urlretrieve(url + filename, filename)
    statinfo = os.stat(filename)
    if statinfo.st_size == expected_bytes:
        print('found and verified', filename)
    else:
        print(statinfo.st_size)
        raise exception(
            'failed to verify ' + filename + '. can you get to it with a browser
?')
    return filename

   this function checks to see if the filename already has been downloaded
   from the supplied url.  if not, it uses the urllib.request python
   module which retrieves a file from the given url argument, and
   downloads the file into the local code directory.  if the file already
   exists (i.e. os.path.exists(filename) returns true), then the function
   does not try to download the file again.  next, the function checks the
   size of the file and makes sure it lines up with the expected file
   size, expected_bytes.  if all is well, it returns the filename object
   which can be used to extract the data from.  to call the function with
   the data-set we are using in this example, we execute the following
   code:
url = 'http://mattmahoney.net/dc/'
filename = maybe_download('text8.zip', url, 31344016)

   the next thing we have to do is take the filename object, which points
   to the downloaded file, and extract the data using the python zipfile
   module.
# read the data into a list of strings.
def read_data(filename):
    """extract the first file enclosed in a zip file as a list of words."""
    with zipfile.zipfile(filename) as f:
        data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

   using zipfile.zipfile() to extract the zipped file, we can then use the
   reader functionality found in this zipfile module.  first,
   the namelist() function retrieves all the members of the archive     in
   this case there is only one member, so we access this using the zero
   index.  then we use the read() function which reads all the text in the
   file and pass this through the tensorflow function as_str which ensures
   that the text is created as a string data-type.  finally, we use
   split() function to create a list with all the words in the text file,
   separated by white-space characters.  we can see some of the output
   here:
vocabulary = read_data(filename)
print(vocabulary[:7])
['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse']

   as you can observe, the returned vocabulary data contains a list of
   plain english words, ordered as they are in the sentences of the
   original extracted text file.  now that we have all the words extracted
   in a list, we have to do some further processing to enable us to create
   our skip-gram batch data.  these further steps are:
    1. extract the top 10,000 most common words to include in our
       embedding vector
    2. gather together all the unique words and index them with a unique
       integer value     this is what is required to create an equivalent
       one-hot type input for the word.  we   ll use a dictionary to do this
    3. loop through every word in the dataset (vocabulary variable) and
       assign it to the unique integer word identified, created in step 2
       above.  this will allow easy lookup / processing of the word data
       stream

   the function which performs all this magic is shown below:
def build_dataset(words, n_words):
    """process raw inputs into a dataset."""
    count = [['unk', -1]]
    count.extend(collections.counter(words).most_common(n_words - 1))
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0  # dictionary['unk']
            unk_count += 1
        data.append(index)
    count[0][1] = unk_count
    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reversed_dictionary

   the first step is setting up a    counter    list, which will store the
   number of times a word is found within the data-set.  because we are
   restricting our vocabulary to only 10,000 words, any words not within
   the top 10,000 most common words will be marked with an    unk   
   designation, standing for    unknown   .  the initialized count list is
   then extended, using the python collections module and the counter()
   class and the associated most_common() function.  these count the
   number of words in the given argument (words) and then returns
   the n most common words in a list format.

   the next part of this function creates a dictionary, called dictionary
   which is populated by keys corresponding to each unique word.  the
   value assigned to each unique word key is simply an increasing integer
   count of the size of the dictionary.  so, for instance, the most common
   word will receive the value 1, the second most common the value 2, the
   third most common word the value 3, and so on (the integer 0 is
   assigned to the    unk    words).   this step creates a unique integer
   value for each word within the vocabulary     accomplishing the second
   step of the process which was defined above.

   next, the function loops through each word in our full words data set    
   the data set which was output from the read_data() function.  a list
   called data is created, which will be the same length as words but
   instead of being a list of individual words, it will instead be a list
   of integers     with each word now being represented by the unique
   integer that was assigned to this word in dictionary.  so, for the
   first sentence of our data-set [   anarchism   ,    originated   ,    as   ,    a   ,
      term   ,    of   ,    abuse   ], now looks like this in the data
   variable: [5242, 3083, 12, 6, 195, 2, 3136].  this part of the function
   addresses step 3 in the list above.

   finally, the function creates a dictionary called reverse_dictionary
   that allows us to look up a word based on its unique integer
   identifier, rather than looking up the identifier based on the word
   i.e. the original dictionary.

   the final aspect of setting up our data is now to create a data set
   comprising of our input words and associated grams, which can be used
   to train our id97 embedding system.  the code to do this is:
data_index = 0
# generate batch data
def generate_batch(data, batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips <= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # input word at the center of the buffer
        targets_to_avoid = [skip_window]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]  # this is the input
word
            context[i * num_skips + j, 0] = buffer[target]  # these are the cont
ext words
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    # backtrack a little bit to avoid skipping words in the end of a batch
    data_index = (data_index + len(data) - span) % len(data)
    return batch, context

   this function will generate mini-batches to use during our training
   (again, see [23]here for information on mini-batch training).  these
   batches will consist of input words (stored in batch) and random
   associated context words within the gram as the labels to predict
   (stored in context).  for instance, in the 5-gram    the cat sat on the   ,
   the input word will be center word i.e.    sat    and the context words
   that will be predicted will be drawn randomly from the remaining words
   of the gram: [   the   ,    cat   ,    on   ,    the   ].  in this function, the number
   of words drawn randomly from the surrounding context is defined by the
   argument num_skips.  the size of the window of context words to draw
   from around the input word is defined in the argument skip_window     in
   the example above (   the cat sat on the   ), we have a skip window width
   of 2 around the input word    sat   .

   in the function above, first the batch and label outputs are defined as
   variables of size batch_size.  then the span size is defined, which is
   basically the size of the word list that the input word and context
   samples will be drawn from.  in the example sub-sentence above    the cat
   sat on the   , the span is 5 = 2 x skip window + 1.  after this a buffer
   is created:
buffer = collections.deque(maxlen=span)
for _ in range(span):
    buffer.append(data[data_index])
    data_index = (data_index + 1) % len(data)

   this buffer will hold a maximum of span elements and will be a kind of
   moving window of words that samples are drawn from.  whenever a new
   word index is added to the buffer, the left most element will drop out
   of the buffer to allow room for the new word index being added.  the
   position of the buffer in the input text stream is stored in a global
   variable data_index which is incremented each time a new word is added
   to the buffer.  if it gets to the end of the text stream, the    %
   len(data)    component of the index update will basically reset the count
   back to zero.

   the code below fills out the batch and context variables:
for i in range(batch_size // num_skips):
    target = skip_window  # input word at the center of the buffer
    targets_to_avoid = [skip_window]
    for j in range(num_skips):
        while target in targets_to_avoid:
            target = random.randint(0, span - 1)
        targets_to_avoid.append(target)
        batch[i * num_skips + j] = buffer[skip_window]  # this is the input word
        context[i * num_skips + j, 0] = buffer[target]  # these are the context
words
    buffer.append(data[data_index])
    data_index = (data_index + 1) % len(data)

   the first    target    word selected is the word at the center of the span
   of words and is therefore the input word.  then other words are
   randomly selected from the span of words, making sure that the input
   word is not selected as part of the context, and each context word is
   unique.  the batch variable will feature repeated input words
   (buffer[skip_window]) which are matched with each context word
   in context.

   the batch and context variables are then returned     and now we have a
   means of drawing batches of data from the data set.  we are now in a
   position to create our id97 training code in tensorflow.  however,
   before we get to that, we   ll first create a validation data-set that we
   can use to test how our model is doing.  we do that by measuring the
   vectors closest together in vector-space, and make sure these words
   indeed are similar using our knowledge of english.  this will be
   discussed more in the next section.  however, for now, the code below
   shows how to grab some random validation words from the most common
   words in our vocabulary:
# we pick a random validation set to sample nearest neighbors. here we limit the
# validation samples to the words that have a low numeric id, which by
# construction are also the most frequent.
valid_size = 16     # random set of words to evaluate similarity on.
valid_window = 100  # only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=false)

   the code above randomly chooses 16 integers from 0-100     this
   corresponds to the integer indexes of the most common 100 words in our
   text data.  these will be the words we examine to assess how our
   learning is progressing in associating related words together in the
   vector-space.  now, onto creating the tensorflow model.

creating the tensorflow model

   for a refresher on tensorflow, check out [24]this tutorial.  below i
   will step through the process of creating our id97 id27s
   in tensorflow.  what does this involve?  simply, we need to setup the
   neural network which i previously presented, with a id27
   matrix acting as the hidden layer and an output softmax layer in
   tensorflow.  by training this model, we   ll be learning the best word
   embedding matrix and therefore we   ll be learning a reduced, context
   maintaining, mapping of words to vectors.

   the first thing to do is set-up some variables which we   ll use later on
   in the code     the purposes of these variables will become clear as we
   progress:
batch_size = 128
embedding_size = 128  # dimension of the embedding vector.
skip_window = 1       # how many words to consider left and right.
num_skips = 2         # how many times to reuse an input to generate a context.

   next we setup some tensorflow placeholders that will hold our input
   words (their integer indexes) and context words which we are trying to
   predict.  we also need to create a constant to hold our validation set
   indexes in tensorflow:
train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

   next, we need to setup the embedding matrix variable / tensor     this is
   straight-forward using the tensorflow embedding_lookup() function,
   which i   ll explain shortly:
# look up embeddings for inputs.
embeddings = tf.variable(
    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
embed = tf.nn.embedding_lookup(embeddings, train_inputs)

   the first step in the code above is to create the embeddings variable,
   which is effectively the weights of the connections to the linear
   hidden layer.  we initialize the variable with a random uniform
   distribution between -1.0 to 1.0.  the size of this variable is
   (vocabulary_size, embedding_size)     the vocabulary_size is the 10,000
   words that we have used to setup our data in the previous section.
   this is basically our one-hot vector input, where the only element with
   a value of    1    is the current input word, all the other values are set
   to    0   .  the second dimension, embedding_size, is our hidden layer
   size, and is the length of our new, smaller, representation of our
   words.  we can also think of this tensor as a big lookup table     the
   rows are each word in our vocabulary, and the columns are our new
   vector representation of each of these words.  here   s a simplified
   example (using dummy values),
   where vocabulary_size=7 and embedding_size=3:

   \begin{equation}
   \begin{array}{c|c c c}
   anarchism & 0.5 & 0.1 & -0.1\\
   originated & -0.5 & 0.3 & 0.9 \\
   as & 0.3 & -0.5 & -0.3 \\
   a & 0.7 & 0.2 & -0.3\\
   term & 0.8 & 0.1 & -0.1 \\
   of & 0.4 & -0.6 & -0.1 \\
   abuse & 0.7 & 0.1 & -0.4
   \end{array}
   \end{equation}

   as can be observed,    anarchism    (which would actually be represented by
   a unique integer or one-hot vector) is now expressed as [0.5, 0.1,
   -0.1].  we can    look up    anarchism by finding its integer index and
   searching the rows of embeddings to find the embedding vector: [0.5,
   0.1, -0.1].

   the next line in the code involves the tf.nn.embedding_lookup()
   function, which is a useful helper function in tensorflow for this type
   of task.  here   s how it works     it takes an input vector of integer
   indexes     in this case our train_input tensor of training input words,
   and    looks up    these indexes in the supplied embeddings tensor.
   therefore, this command will return the current embedding vector for
   each of the supplied input words in the training batch.  the full
   embedding tensor will be optimized during the training process.

   next we have to create some weights and bias values to connect the
   output softmax layer, and perform the appropriate multiplication and
   addition.  this looks like:
# construct the variables for the softmax
weights = tf.variable(tf.truncated_normal([vocabulary_size, embedding_size],
                          stddev=1.0 / math.sqrt(embedding_size)))
biases = tf.variable(tf.zeros([vocabulary_size]))
hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases

   the weight variable, as it is connecting the hidden layer and the
   output layer, is of size (out_layer_size, hidden_layer_size) =
   (vocabulary_size, embedding_size).  the biases, as usual, will only be
   single dimensional and the size of the output layer.  we then multiply
   the embedded variable (embed) by the weights and add the bias.  now we
   are ready to create a softmax operation and we will use cross id178
   loss to optimize the weights, biases and embeddings of the model.  to
   do this easily, we will use the tensorflow
   function softmax_cross_id178_with_logits().  however, to use this
   function we first have to convert the context words / integer indices
   into one-hot vectors.  the code below performs both of these steps, and
   also adds a id119 optimization operation:
# convert train_context to a one-hot format
train_one_hot = tf.one_hot(train_context, vocabulary_size)
cross_id178 = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(logits=hi
dden_out,
    labels=train_one_hot))
# construct the sgd optimizer using a learning rate of 1.0.
optimizer = tf.train.gradientdescentoptimizer(1.0).minimize(cross_id178)

   next, we need to perform our similarity assessments to check on how the
   model is performing as it trains.  to determine which words are similar
   to each other, we need to perform some sort of operation that measures
   the    distances    between the various id27 vectors for the
   different words.  in this case, we will use the [25]cosine similarity
   measure of distance between vectors.  it is defined as:

   $$similarity = cos(\theta) =
   \frac{\textbf{a}\cdot\textbf{b}}{\parallel\textbf{a}\parallel_2
   \parallel \textbf{b} \parallel_2}$$

   here the bolded a and b are the two vectors that we are measuring the
   similarity between.  the double parallel lines with the 2 subscript
   ($\parallel\textbf{a}\parallel_2$) refers to the l2 norm of the vector.
    to get the l2 norm of a vector, you square every dimension of the
   vector (in this case n=300, the width of our embedding vector), sum up
   the squared elements then take the square root of the product i.e.:

   $$\sqrt{\sum_{i=1}^n a_{i}^2}$$

   the best way to calculate the cosine similarity in tensorflow is to
   normalize each vector like so:

   $$\frac{\textbf{a}}{\parallel\textbf{a}\parallel_2}$$

   then we can simply multiply these normalized vectors together to get
   the cosine similarity.  we will multiply the validation vectors/words
   that were discussed earlier with all of the words in our embedding
   vector, then we can sort in descending order to get those words most
   similar to our validation words.

   first, we calculate the l2 norm of each vector using the tf.square(),
   tf.reduce_sum() and tf.sqrt() functions to calculate the square,
   summation and square root of the norm, respectively:
# compute the cosine similarity between minibatch examples and all embeddings.
norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=true))
normalized_embeddings = embeddings / norm

   now we can look up our validation words / vectors using
   the tf.nn.embedding_lookup() that we discussed earlier:
valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)

   as before, we are supplying a list of integers (that correspond to our
   validation vocabulary words) to the embedding_lookup() function, which
   looks up these rows in the normalized_embeddings tensor, and returns
   the subset of validation normalized embeddings.  now that we have the
   normalized validation tensor, valid_embeddings, we can multiply this by
   the full normalized vocabulary (normalized_embedding) to finalize our
   similarity calculation:
similarity = tf.matmul(
      valid_embeddings, normalized_embeddings, transpose_b=true)

   this operation will return a (validation_size, vocabulary_size) sized
   tensor, where each row refers to one of our validation words and the
   columns refer to the similarity between the validation word and all the
   other words in the vocabulary.

running the tensorflow model

   the code below initializes the variables and feeds in each data batch
   to the training loop, printing the average loss every 2000 iterations.
    if this code doesn   t make sense to you, check out my [26]tensorflow
   tutorial.
with tf.session(graph=graph) as session:
  # we must initialize all variables before we use them.
  init.run()
  print('initialized')

  average_loss = 0
  for step in range(num_steps):
    batch_inputs, batch_context = generate_batch(data,
        batch_size, num_skips, skip_window)
    feed_dict = {train_inputs: batch_inputs, train_context: batch_context}

    # we perform one update step by evaluating the optimizer op (including it
    # in the list of returned values for session.run()
    _, loss_val = session.run([optimizer, cross_id178], feed_dict=feed_dict)
    average_loss += loss_val

    if step % 2000 == 0:
      if step > 0:
        average_loss /= 2000
      # the average loss is an estimate of the loss over the last 2000 batches.
      print('average loss at step ', step, ': ', average_loss)
      average_loss = 0

   next, we want to print out the words which are most similar to our
   validation words     we do this by calling the similarity operation we
   defined above and sorting the results (note, this is only performed
   every 10,000 iterations as it is computationally expensive):
# note that this is expensive (~20% slowdown if computed every 500 steps)
if step % 10000 == 0:
    sim = similarity.eval()
    for i in range(valid_size):
        valid_word = reverse_dictionary[valid_examples[i]]
        top_k = 8  # number of nearest neighbors
        nearest = (-sim[i, :]).argsort()[1:top_k + 1]
        log_str = 'nearest to %s:' % valid_word
        for k in range(top_k):
            close_word = reverse_dictionary[nearest[k]]
            log_str = '%s %s,' % (log_str, close_word)
        print(log_str)

   this function first evaluates the similarity operation, which returns
   an array of cosine similarity values for each of the validation words.
    then we iterate through each of the validation words, taking the top 8
   closest words by using argsort() on the negative of the similarity to
   arrange the values in descending order.  the code then prints out these
   8 closest words so we can monitor how the embedding process is
   performing.

   finally, after all the training iterations are finished, we can assign
   the final embeddings to a separate tensor for use later (most likely in
   some sort of other deep learning or machine learning process):
final_embeddings = normalized_embeddings.eval()

   so now we   re done     or are we?  the code for this softmax method of
   id97 is on this site   s github repository     you could try running
   it, but i wouldn   t recommend it.  why?  because it is seriously slow.

speeding things up     the    true    id97 method

   the fact is, performing softmax evaluations and updating the weights
   over a 10,000 word output/vocabulary is really slow.  why   s that?
   consider the softmax definition:

   $$p(y = j \mid x) = \frac{e^{x^t w_j}}{\sum_{k=1}^k e^{x^t w_k}}$$

   in the context of what we are working on, the softmax function will
   predict what words have the highest id203 of being in the context
   of the input word.  to determine that id203 however, the
   denominator of the softmax function has to evaluate all the possible
   context words in the vocabulary.  therefore, we need 300 x 10,000 = 3m
   weights, all of which need to be trained for the softmax output.  this
   slows things down.

   there is an alternative, faster scheme called [27]noise contrastive
   estimation (nce).  instead of taking the id203 of the context
   word compared to all of the possible context words in the vocabulary,
   this method randomly samples 2-20 possible context words and evaluates
   the id203 only from these.  i won   t go into the nitty gritty
   details here, but suffice to say that this method has been shown to
   perform well and drastically speeds up the training process.

   tensorflow has helped us out here, and has supplied an nce loss
   function that we can use called tf.nn.nce_loss() which we can supply
   weight and bias variables to.  using this function, the time to perform
   100 training iterations reduced from 25 seconds with the softmax method
   to less than 1 second using the nce method.  an awesome improvement!
   we replace the softmax lines with the following in our code:
# construct the variables for the nce loss
nce_weights = tf.variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
nce_biases = tf.variable(tf.zeros([vocabulary_size]))

nce_loss = tf.reduce_mean(
        tf.nn.nce_loss(weights=nce_weights,
                       biases=nce_biases,
                       labels=train_context,
                       inputs=embed,
                       num_sampled=num_sampled,
                       num_classes=vocabulary_size))

optimizer = tf.train.gradientdescentoptimizer(1.0).minimize(nce_loss)

   now we are good to run the code.  you can get the full code [28]here.
   as discussed, every 10,000 iterations the code outputs the validation
   words and the words that the id97 system deems are similar.  below,
   you can see the improvement for some selected validation words between
   the random initialization and at the 50,000 iteration mark:

   at the beginning:

     nearest to nine: heterosexual, scholarly, scandal, serves, humor,
     realized, cave, himself

     nearest to this: contains, alter, numerous, harmonica, nickname,
     ghana, bogart, marxist

   after 10,000 iterations:

     nearest to nine: zero, one, and, coke, in, unk, the, jpg

     nearest to this: the, a, unk, killing, meter, afghanistan, ada,
     indiana

   finally after 50,000 iterations:

     nearest to nine: eight, one, zero, seven, six, two, five, three

     nearest to this: that, the, a, unk, one, it, he, an

   by examining the outputs above, we can first see that the word    nine   
   becomes increasingly associated with other number words (   eight   ,
      one   ,    seven    etc.).  this makes sense. the word    this   , which acts as
   a pronoun and definite article in sentences, becomes associated with
   other pronouns (   he   ,    it   ) and other definite articles (   the   ,    that   ,
   etc.) the more iterations we run.

   in summary then, we have learnt how to use the id97 methodology to
   reduce large one-hot word vectors to much reduced id27
   vectors which preserve the context and meaning of the original words.
   these id27 vectors can then be used as a more efficient and
   effective input to deep learning techniques which aim to model natural
   language.  these techniques, such as recurrent neural networks, will be
   the subject of future posts.
     __________________________________________________________________

eager to learn more? get the book [29]here
     __________________________________________________________________


about the author

     neck says:
   [30]august 30, 2017 at 1:20 pm

   good tutorials..thanks
   waiting for id56
   keep it up
     * andy says:
       [31]august 30, 2017 at 7:48 pm
       thanks! an id56 and lstm tutorial is currently in the works,
       hopefully in the next few weeks

     abhash sinha says:
   [32]september 3, 2017 at 11:01 pm

   though i haven   t executed the code myself, but i am pretty much
   convinced with the code provided in the article and the explanation
   given for each line. its so well articulated that anyone can understand
   it. so thanks to you.

     phoenix says:
   [33]september 12, 2017 at 11:02 pm

   thanks for the awesome tutorial. however, the issue i have with the
   tensorflow tutorial is that it seems the batch generator does not
   shuffle the batches. it just goes linearly through the document without
   ever changing the order of batches or the order of samples in a batch.
   the only small change in batches occurs if the length of your document
   is not divisible by your batch size.
   do you have any suggestion on how to fix this?
     * andy says:
       [34]september 15, 2017 at 8:14 pm
       hi phoenix     that   s correct, it does just go linearly through the
       document. however, this isn   t really an issue in this case     often
       with nlp problems one takes samples linearly from the corpus
       without performing random shuffling. thanks for the comment

     dipesh singhal says:
   [35]september 21, 2017 at 5:45 am

   great tutorial! if you could have a tutorial series on how to visualize
   it using tsne that would be of great help.
   thanks!

     staffan ekvall says:
   [36]october 30, 2017 at 7:06 am

   thanks for a great tutorial!
     * andy says:
       [37]october 30, 2017 at 7:54 am
       thanks for the feedback! your welcome

     cwl says:
   [38]november 19, 2017 at 4:56 pm

   this is a fantastic tutorial! love it!

   could you explain a little bit in the function generate_batch, what
   exactly are buffer and batch, what are their formats and what exactly
   do they mean? i can guess batch is the list of mini batches but i can   t
   figure out how will it be used as a training data set.

     yong q xie says:
   [39]november 26, 2017 at 4:44 pm

   excellent tutorial. i   m also a phd not in this, but i   m into this very
   much.i   m now leading projects associated with building deep nn for
   images and time series. i   m exploring and learning techniques like in
   your tutorials relating to nlp after work. i   m impressed by your
   elaborative written.

     kush shrivastava says:
   [40]february 21, 2018 at 5:15 am

   thanks for the tutorial    all doubts just vanished



   harikrishnan rajeev says:
   [41]march 8, 2018 at 6:48 pm

   very useful, thank you



   pier says:
   [42]march 28, 2018 at 9:21 am

   thank you. i have been looking through various
   articles/videos/tutorials. with respect to the tensorflow code, yours
   was the most complete.



   [43]elie says:
   [44]april 19, 2018 at 4:49 pm

   thanks. this is the best tutorial on embeddings with tensorflow. i have
   been looking to simulate embeddings for a foreign language other than
   english and see how it works. i am now convinced that this tutorial is
   one step towards that direction.



   abidan lee says:
   [45]june 15, 2018 at 7:27 am

   thank you for the great article.
   bless you!



   meenal jain says:
   [46]august 29, 2018 at 6:49 am

   very useful tutorial, explained the code thoroughly, also keeping it
   simple to use.

     * andy says:
       [47]august 29, 2018 at 6:55 am
       thanks meenal



   kbrom says:
   [48]september 2, 2018 at 1:32 pm

   thank you! really.

   ____________________ (button)

   recent posts
     * [49]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [50]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [51]keras, eager and tensorflow 2.0     a new tf paradigm
     * [52]introduction to tensorboard and tensorflow visualization
     * [53]tensorflow eager tutorial

   recent comments
     * andry on [54]neural networks tutorial     a pathway to deep learning
     * sandipan on [55]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [56]neural networks tutorial     a pathway to deep learning
     * martin on [57]neural networks tutorial     a pathway to deep learning
     * uri on [58]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [59]march 2019
     * [60]january 2019
     * [61]october 2018
     * [62]september 2018
     * [63]august 2018
     * [64]july 2018
     * [65]june 2018
     * [66]may 2018
     * [67]april 2018
     * [68]march 2018
     * [69]february 2018
     * [70]november 2017
     * [71]october 2017
     * [72]september 2017
     * [73]august 2017
     * [74]july 2017
     * [75]may 2017
     * [76]april 2017
     * [77]march 2017

   categories
     * [78]amazon aws
     * [79]cntk
     * [80]convolutional neural networks
     * [81]cross id178
     * [82]deep learning
     * [83]gensim
     * [84]gpus
     * [85]keras
     * [86]id168s
     * [87]lstms
     * [88]neural networks
     * [89]nlp
     * [90]optimisation
     * [91]pytorch
     * [92]recurrent neural networks
     * [93]id23
     * [94]tensorboard
     * [95]tensorflow
     * [96]tensorflow 2.0
     * [97]weight initialization
     * [98]id97

   meta
     * [99]log in
     * [100]entries rss
     * [101]comments rss
     * [102]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [103]thrive themes | powered by [104]wordpress

   (button) close dialog

   session expired

   [105]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[106]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/nlp/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/nlp/
  13. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  14. http://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments
  15. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  16. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  17. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  18. https://adventuresinmachinelearning.com/neural-networks-tutorial/#setting-up-output
  19. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  20. https://github.com/adventuresinml/adventures-in-ml-code
  21. https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/id97/id97_basic.py
  22. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  23. https://adventuresinmachinelearning.com/stochastic-gradient-descent/
  24. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  25. https://en.wikipedia.org/wiki/cosine_similarity
  26. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  27. http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
  28. https://github.com/adventuresinml/adventures-in-ml-code
  29. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
  30. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4827
  31. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4828
  32. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4830
  33. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4836
  34. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4837
  35. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4841
  36. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4887
  37. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4888
  38. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4890
  39. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4893
  40. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4902
  41. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4904
  42. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4907
  43. http://fallfordata.com/
  44. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4910
  45. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4916
  46. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4922
  47. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4923
  48. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/#comments/4924
  49. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  50. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  51. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  52. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  53. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  54. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  55. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  56. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  57. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  58. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  59. https://adventuresinmachinelearning.com/2019/03/
  60. https://adventuresinmachinelearning.com/2019/01/
  61. https://adventuresinmachinelearning.com/2018/10/
  62. https://adventuresinmachinelearning.com/2018/09/
  63. https://adventuresinmachinelearning.com/2018/08/
  64. https://adventuresinmachinelearning.com/2018/07/
  65. https://adventuresinmachinelearning.com/2018/06/
  66. https://adventuresinmachinelearning.com/2018/05/
  67. https://adventuresinmachinelearning.com/2018/04/
  68. https://adventuresinmachinelearning.com/2018/03/
  69. https://adventuresinmachinelearning.com/2018/02/
  70. https://adventuresinmachinelearning.com/2017/11/
  71. https://adventuresinmachinelearning.com/2017/10/
  72. https://adventuresinmachinelearning.com/2017/09/
  73. https://adventuresinmachinelearning.com/2017/08/
  74. https://adventuresinmachinelearning.com/2017/07/
  75. https://adventuresinmachinelearning.com/2017/05/
  76. https://adventuresinmachinelearning.com/2017/04/
  77. https://adventuresinmachinelearning.com/2017/03/
  78. https://adventuresinmachinelearning.com/category/amazon-aws/
  79. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  80. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  81. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  82. https://adventuresinmachinelearning.com/category/deep-learning/
  83. https://adventuresinmachinelearning.com/category/nlp/gensim/
  84. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  85. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  86. https://adventuresinmachinelearning.com/category/loss-functions/
  87. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  88. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  89. https://adventuresinmachinelearning.com/category/nlp/
  90. https://adventuresinmachinelearning.com/category/optimisation/
  91. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  92. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  93. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  94. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  95. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  96. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  97. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  98. https://adventuresinmachinelearning.com/category/nlp/id97/
  99. https://adventuresinmachinelearning.com/wp-login.php
 100. https://adventuresinmachinelearning.com/feed/
 101. https://adventuresinmachinelearning.com/comments/feed/
 102. https://wordpress.org/
 103. https://www.thrivethemes.com/
 104. http://www.wordpress.org/
 105. https://adventuresinmachinelearning.com/wp-login.php
 106. http://adventuresinmachinelearning.com/id97-tutorial-tensorflow/

   hidden links:
 108. https://adventuresinmachinelearning.com/author/admin/
