   #[1]analytics vidhya    feed [2]analytics vidhya    comments feed
   [3]analytics vidhya    bayesian statistics explained to beginners in
   simple english comments feed [4]alternate [5]alternate

   iframe: [6]//googletagmanager.com/ns.html?id=gtm-mpsm42v

   [7]new certified ai & ml blackbelt program (beginner to master) -
   enroll today @ launch offer (coupon: blackbelt10)

   (button) search______________
     * [8]learn
          + [9]blog archive
               o [10]machine learning
               o [11]deep learning
               o [12]career
               o [13]stories
          + [14]datahack radio
          + [15]infographics
          + [16]training
          + [17]learning paths
               o [18]sas business analyst
               o [19]learn data science on r
               o [20]data science in python
               o [21]data science in weka
               o [22]data visualization with tableau
               o [23]data visualization with qlikview
               o [24]interactive data stories with d3.js
          + [25]glossary
     * [26]engage
          + [27]discuss
          + [28]events
          + [29]datahack summit 2018
          + [30]datahack summit 2017
          + [31]student datafest
          + [32]write for us
     * [33]compete
          + [34]hackathons
     * [35]get hired
          + [36]jobs
     * [37]courses
          + [38]id161 using deep learning
          + [39]natural language processing using python
          + [40]introduction to data science
          + [41]microsoft excel
          + [42]more courses
     * [43]contact

     *
     *
     *
     *

     * [44]home
     * [45]blog archive
     * [46]trainings
     * [47]discuss
     * [48]datahack
     * [49]jobs
     * [50]corporate

     *

   [51]analytics vidhya - learn everything about analytics

learn everything about analytics

   [52][black-belt-2.gif]
   [53][black-belt-2.gif]
   [54][black-belt-2.gif]
   (button) search______________

   [55]analytics vidhya - learn everything about analytics
     * [56]learn
          + [57]blog archive
               o [58]machine learning
               o [59]deep learning
               o [60]career
               o [61]stories
          + [62]datahack radio
          + [63]infographics
          + [64]training
          + [65]learning paths
               o [66]sas business analyst
               o [67]learn data science on r
               o [68]data science in python
               o [69]data science in weka
               o [70]data visualization with tableau
               o [71]data visualization with qlikview
               o [72]interactive data stories with d3.js
          + [73]glossary
     * [74]engage
          + [75]discuss
          + [76]events
          + [77]datahack summit 2018
          + [78]datahack summit 2017
          + [79]student datafest
          + [80]write for us
     * [81]compete
          + [82]hackathons
     * [83]get hired
          + [84]jobs
     * [85]courses
          + [86]id161 using deep learning
          + [87]natural language processing using python
          + [88]introduction to data science
          + [89]microsoft excel
          + [90]more courses
     * [91]contact

   [92]home [93]business analytics [94]bayesian statistics explained to
   beginners in simple english

   [95]business analytics[96]r

bayesian statistics explained to beginners in simple english

   [97]nss, june 20, 2016

introduction

   bayesian statistics continues to remain incomprehensible in the ignited
   minds of many analysts. being amazed by the incredible power of
   [98]machine learning, a lot of us have become unfaithful to statistics.
   our focus has narrowed down to exploring machine learning. isn   t it
   true?

   we fail to understand that machine learning is not the only way to
   solve real world problems. in several situations, it does not help us
   solve business problems, even though there is data involved in these
   problems. to say the least, [99]knowledge of statistics will allow you
   to work on complex analytical problems, irrespective of the size of
   data.

   in 1770s, thomas bayes introduced    id47   . even after centuries
   later, the importance of    bayesian statistics    hasn   t faded away. in
   fact, today this topic is being taught in great depths in some of the
   world   s leading universities.

   with this idea, i   ve created this beginner   s guide on bayesian
   statistics. i   ve tried to explain the concepts in a simplistic manner
   with examples. prior knowledge of [100]basic id203 &
   statistics is desirable. you should check out [101]this course to get a
   comprehensive low down on statistics and id203.

   by the end of this article, you will have a concrete understanding of
   bayesian statistics and its associated concepts.

   explaining bayesian statistics in simple english


table of contents

    1. frequentist statistics
    2. the inherent flaws in frequentist statistics
    3. bayesian statistics
          + id155
          + id47
    4. bayesian id136
          + bernoulli likelihood function
          + prior belief distribution
          + posterior belief distribution
    5. test for significance     frequentist vs bayesian
          + p-value
          + confidence intervals
          + bayes factor
          + high density interval (hdi)


   before we actually delve in bayesian statistics, let us spend a few
   minutes understanding frequentist statistics, the more popular version
   of statistics most of us come across and the inherent problems in that.


1. frequentist statistics

   the debate between frequentist and bayesian have haunted beginners for
   centuries. therefore, it is important to understand the difference
   between the two and how does there exists a thin line of demarcation!

   it is the most widely used inferential technique in the statistical
   world. infact, generally it is the first school of thought that a
   person entering into the statistics world comes across.

   frequentist statistics tests whether an event (hypothesis) occurs or
   not. it calculates the id203 of an event in the long run of
   the experiment (i.e the experiment is repeated under the same
   conditions to obtain the outcome).

   here, the sampling distributions of fixed size are taken. then, the
   experiment is theoretically repeated infinite number of times but
   practically done with a stopping intention. for example, i perform an
   experiment with a stopping intention in mind that i will stop the
   experiment when it is repeated 1000 times or i see minimum 300 heads in
   a coin toss.

   let   s go deeper now.

   now, we   ll understand frequentist statistics using an example of coin
   toss. the objective is to estimate the fairness of the coin. below is a
   table representing the frequency of heads:

   [fig.10c.png]

   we know that id203 of getting a head on tossing a fair coin is
   0.5. no. of heads represents the actual number of heads obtained.
   difference is the difference between 0.5*(no. of tosses) - no. of
   heads.

   an important thing is to note that, though the difference between the
   actual number of heads and expected number of heads( 50% of number of
   tosses) increases as the number of tosses are increased, the proportion
   of number of heads to total number of tosses approaches 0.5 (for a fair
   coin).

   this experiment presents us with a very common flaw found in
   frequentist approach i.e. dependence of the result of an experiment on
   the number of times the experiment is repeated.

   to know more about frequentist statistical methods, you can head to
   this [102]excellent course on inferential statistics.


2. the inherent flaws in frequentist statistics

   till here, we   ve seen just one flaw in frequentist statistics. well,
   it   s just the beginning.

   20th century saw a massive upsurge in the frequentist statistics being
   applied to numerical models to check whether one sample is different
   from the other, a parameter is important enough to be kept in the model
   and variousother  manifestations of hypothesis testing. but frequentist
   statistics suffered some great flaws in its design and interpretation
   which posed a serious concern in all real life problems. for example:

   1. p-values measured against a sample (fixed size) statistic with some
   stopping intention changes with change in intention and sample size.
   i.e if two persons work on the same data and have different stopping
   intention, they may get two different  p- values for the same data,
   which is undesirable.

   for example: person a may choose to stop tossing a coin when the total
   count reaches 100 while b stops at 1000. for different sample sizes, we
   get different t-scores and different p-values. similarly, intention to
   stop may change from fixed number of flips to total duration of
   flipping. in this case too, we are bound to get different p-values.

   2- confidence interval (c.i) like p-value depends heavily on the sample
   size. this makes the stopping potential absolutely absurd since no
   matter how many persons perform the tests on the same data, the results
   should be consistent.

   3- confidence intervals (c.i) are not id203 distributions
   therefore they do not provide the most probable value for a parameter
   and the most probable values.

   these three reasons are enough to get you going into thinking about the
   drawbacks of the frequentist approach and why is there a need for
   bayesian approach. let   s find it out.

   from here, we   ll first understand the basics of bayesian statistics.


3. bayesian statistics

      bayesian statistics is a mathematical procedure that applies
   probabilities to statistical problems. it provides people the tools to
   update their beliefs in the evidence of new data.   

   you got that? let me explain it with an example:

   suppose, out of all the 4 championship races (f1) between [103]niki
   lauda and [104]james hunt, niki won 3 times while james managed only 1.

   so, if you were to bet on the winner of next race, who would he be ?

   i bet you would say niki lauda.

   here   s the twist. what if you are told that it rained once when james
   won and once when niki won and it is definite that it will rain on the
   next date. so, who would you bet your money on now ?

   by intuition, it is easy to see that chances of winning for james have
   increased drastically. but the question is: how much ?

   to understand the problem at hand, we need to become familiar with some
   concepts, first of which is id155 (explained below).

   in addition, there are certain pre-requisites:

   pre-requisites:
    1. id202 : to refresh your basics, you can check out
       [105]khan   s academy algebra.
    2. id203 and basic statistics : to refresh your basics, you can
       check out [106]another course by khan academy.


3.1 id155

   it is defined as the: id203 of an event a given b equals the
   id203 of b and a happening together divided by the id203 of
   b.   

   for example: assume two partially intersecting sets a and b as shown
   below.

   set a represents one set of events and set b represents another. we
   wish to calculate the id203 of a given b has already happened.
   lets represent the happening of event b by shading it with red.

   1

   now since b has happened, the part which now matters for a is the part
   shaded in blue which is interestingly codecogseqn . so, the id203
   of a given b turns out to be:

   [gif.latex?%5cfrac%7bblue%20area%7d%7bred%20area+blue%20area%7d]

   therefore, we can write the formula for event b given a has already
   occurred by:

   [gif.latex?p%28b%7ca%29%3d%5cfrac%7bp%28a%5ccap%20b%29%7d%7bp%28a%29%7d
   ]

   or
   [gif.latex?p%28a%7cb%29%3d%5cfrac%7bp%28a%5ccap%20b%29%7d%7bp%28b%29%7d
   ]

   now, the second equation can be rewritten as :

   [gif.latex?p%28a%7cb%29%3d%5cfrac%7bp%28b%7ca%29xp%28a%29%7d%7bp%28b%29
   %7d]

   this is known as id155.

   let   s try to answer a betting problem with this technique.

   suppose, b be the event of winning of james hunt. a be the event of
   raining. therefore,
    1. p(a) =1/2, since it rained twice out of four days.
    2. p(b) is 1/4, since james won only one race out of four.
    3. p(a|b)=1, since it rained every time when james won.

   substituting the values in the id155 formula, we get
   the id203 to be around 50%, which is almost the double of 25%
   when rain was not taken into account (solve it at your end).

   this further strengthened our belief  of  james winning in the light of
   new evidence i.e rain. you must be wondering that this formula bears
   close resemblance to something you might have heard a lot about. think!

   probably, you guessed it right. it looks like id47.

   bayes  theorem is built on top of id155 and lies in
   the heart of bayesian id136. let   s understand it in detail now.


3.2 id47

   id47 comes into effect when multiple events
   [gif.latex?%5ea%7bi%7d]  form an exhaustive set with another event b.
   this could be understood with the help of the below diagram.

   capture


   now, b can be written as

   [gif.latex?b%3d%5csum_%7bi%3d1%7d%5e%7bn%7d%20b%5ccap%20a_%7bi%7d]

   so, id203 of b can be written as,

   [gif.latex?p%28b%29%3d%5csum_%7bi%3d1%7d%5e%7bn%7d%20p%28b%5ccap%20a_%7
   bi%7d%29]

   but

   so, replacing p(b) in the equation of id155 we get


   this is the equation of id47.


4. bayesian id136

   there is no point in diving into the theoretical aspect of it. so,
   we   ll learn how it works! let   s take an example of coin tossing to
   understand the idea behind bayesian id136.

   an important part of bayesian id136 is the establishment of
   parameters and models.

   models are the mathematical formulation of the observed events.
   parameters are the factors in the models affecting the observed data.
   for example, in tossing a coin, fairness of coin may be defined as the
   parameter of coin denoted by   . the outcome of the events may be
   denoted by d.

   answer this now. what is the id203 of 4 heads out of 9 tosses(d)
   given the fairness of coin (  ). i.e p(d|  )

   wait, did i ask the right question? no.

   we should be more interested in knowing : given an outcome (d) what is
   the probbaility of coin being fair (  =0.5)

   lets represent it using id47:

   p(  |d)=(p(d|  ) x p(  ))/p(d)

   here, p(  ) is the prior i.e the strength of our belief in the fairness
   of coin before the toss. it is perfectly okay to believe that coin can
   have any degree of fairness between 0 and 1.

   p(d|  ) is the likelihood of observing our result given our distribution
   for   . if we knew that coin was fair, this gives the id203 of
   observing the number of heads in a particular number of flips.

   p(d) is the evidence. this is the id203 of data as determined by
   summing (or integrating) across all possible values of   , weighted by
   how strongly we believe in those particular values of   .

   if we had multiple views of what the fairness of the coin is (but
   didn   t know for sure), then this tells us the id203 of seeing a
   certain sequence of flips for all possibilities of our belief in the
   coin   s fairness.

   p(  |d) is the posterior belief of our parameters after observing the
   evidence i.e the number of heads .

   from here, we   ll dive deeper into mathematical implications of this
   concept. don   t worry. once you understand them, getting to its
   mathematics is pretty easy.

   to define our model correctly , we need two mathematical models before
   hand. one to represent the likelihood function p(d|  )  and the other
   for representing the distribution of prior beliefs . the product of
   these two gives the posterior belief p(  |d) distribution.

   since prior and posterior are both beliefs about the distribution of
   fairness of coin, intuition tells us that both should have the same
   mathematical form. keep this in mind. we will come back to it again.

   so, there are several functions which support the existence of bayes
   theorem. knowing them is important, hence i have explained them in
   detail.


4.1. bernoulli likelihood function

   lets recap what we learned about the likelihood function. so, we
   learned that:

   it is the id203 of observing a particular number of heads in a
   particular number of flips for a given fairness of coin. this means our
   id203 of observing heads/tails depends upon the fairness of coin
   (  ).

   p(y=1|  )=  [gif.latex?%5ctheta%20%5ey]     [if coin is fair   =0.5,
   id203 of observing heads (y=1) is 0.5]

   p(y=0|  )= [gif.latex?%281-%5ctheta%20%29%5e%7b1-y%7d]  [if coin is fair
     =0.5, id203 of observing tails(y=0) is 0.5]

   it is worth noticing that representing 1 as heads and 0 as tails is
   just a mathematical notation to formulate a model. we can combine the
   above mathematical definitions into a single definition to represent
   the id203 of both the outcomes.

   p(y|  )=  [gif.latex?%5ctheta%20%5ey.%281-%5ctheta%20%29%5e%7b1-y%7d]

   this is called the bernoulli likelihood function and the task of coin
   flipping is called bernoulli   s trials.

   y={0,1},  =(0,1)

   and, when we want to see a series of heads or flips, its id203 is
   given by:

   [gif.latex?p%28y_%7b1%7d%2cy_%7b2%7d%2c...y_%7bn%7d%7c%5ctheta%20%29%3d
   %5cprod_%7b1%7d%5e%7bn%7dp%28y_%7bi%7d%7c%5ctheta%20%29]

   [gif.latex?p%28y_%7b1%7d%2cy_%7b2%7d%2c...%2cy_%7bn%7d%7c%5ctheta%20%29
   %3d%5cprod_%7b1%7d%5e%7bn%7d%5ctheta%20%5e%7by_%7bi%7d%7d.%281-%5ctheta
   %20%29%5e%7b1-y_%7bi%7d%7d]

   furthermore, if we are interested in the id203 of number of heads
   z turning up in n number of flips then the id203 is given by:

   [gif.latex?p%28z%2cn%7c%5ctheta%20%29%3d%5ctheta%20%5e%7bz%7d.%281-%5ct
   heta%29%5e%7bn-z%7d]


4.2. prior belief  distribution

   this distribution is used to represent our strengths on beliefs about
   the parameters based on the previous experience.

   but, what if one has no previous experience?

   don   t worry. mathematicians have devised methods to mitigate this
   problem too. it is known as uninformative priors. i would like to
   inform you beforehand that it is just a misnomer. every uninformative
   prior always provides some information event the constant distribution
   prior.

   well, the mathematical function used to represent the prior beliefs is
   known as beta distribution. it has some very nice mathematical
   properties which enable us to model our beliefs about a binomial
   distribution.

   id203 density function of beta distribution is of the form :

   [b%28%5calpha%20%2c%5cbeta%20%29]

   where, our focus stays on numerator. the denominator is there just to
   ensure that the total id203 density function upon integration
   evaluates to 1.

      and    are called the shape deciding parameters of the density
   function. here    is analogous to number of heads in the trials and   
   corresponds to the number of tails. the diagrams below will help you
   visualize the beta distributions for different values of    and   

   bayesian update using beta-binomial model

   you too can draw the beta distribution for yourself using the following
   code in r:

   > library(stats)
   > par(mfrow=c(3,2))
   > x=seq(0,1,by=o.1)
   > alpha=c(0,2,10,20,50,500)
   > beta=c(0,2,8,11,27,232)
   > for(i in 1:length(alpha)){
          y<-dbeta(x,shape1=alpha[i],shape2=beta[i])
          plot(x,y,type="l")
   }

   note:    and    are intuitive to understand since they can be calculated
   by knowing the mean (  ) and standard deviation (  ) of the distribution.
   in fact, they are related as :

   [gif.latex?%5cmu%20%3d%20%5cfrac%7b%5calpha%7d%7b%5calpha%20&amp;plus;%
   20%5cbeta%7d]

   [gif.latex?%5csigma%20%3d%20%5csqrt%7b%5cfrac%7b%5calpha%20%5cbeta%7d%7
   b%28%5calpha%20&amp;plus;%20%5cbeta%29%5e2%20%28%5calpha%20&amp;plus;%2
   0%5cbeta%20&amp;plus;%201%29%7d%7d]

   if mean and standard deviation of a distribution are known , then there
   shape parameters can be easily calculated.

   id136 drawn from graphs above:
    1. when there was no toss we believed that every fairness of coin is
       possible as depicted by the flat line.
    2. when there were more number of heads than the tails, the graph
       showed a peak shifted towards the right side, indicating higher
       id203 of heads and that coin is not fair.
    3. as more tosses are done, and heads continue to come in larger
       proportion the peak narrows increasing our confidence in the
       fairness of the coin value.


4.3. posterior belief distribution

   the reason that we chose prior belief is to obtain a beta distribution.
   this is because when we multiply it with a likelihood function,
   posterior distribution yields a form similar to the prior distribution
   which is much easier to relate to and understand. if this much
   information whets your appetite, i   m sure you are ready to walk an
   extra mile.

   let   s calculate posterior belief using id47.

   calculating posterior belief using id47

   [p%28z%2cn%29]

   [%5bb%28%5calpha%2c%5cbeta%29p%28z%2cn%29%5d]

   [%5bb%28z&amp;plus;%5calpha%2cn-z&amp;plus;%5cbeta%29%5d]

   now, our posterior belief becomes,

   [gif.latex?p%28%5ctheta%7cz&amp;plus;%5calpha%2cn-z&amp;plus;%5cbeta%29
   ]

   this is interesting. just knowing the mean and standard distribution of
   our belief about the parameter    and by observing the number of heads
   in n flips, we can update our belief about the model parameter(  ).

   lets understand this with the help of a simple example:

   suppose, you think that a coin is biased. it has a mean (  ) bias of
   around 0.6 with standard deviation of 0.1.

   then ,

     = 13.8 ,   =9.2

   i.e our distribution will be biased on the right side. suppose, you
   observed 80 heads (z=80) in 100 flips(n=100). let   s see how our prior
   and posterior beliefs are going to look:

   prior = p(  |  ,  )=p(  |13.8,9.2)

   posterior = p(  |z+  ,n-z+  )=p(  |93.8,29.2)

   lets visualize both the beliefs on a graph:

   [new.jpg]

   the r code for the above graph is as:

   > library(stats)
   > x=seq(0,1,by=0.1)
   > alpha=c(13.8,93.8)
   > beta=c(9.2,29.2)
   > for(i in 1:length(alpha)){
         y<-dbeta(x,shape1=alpha[i],shape2=beta[i])
         plot(x,y,type="l",xlab = "theta",ylab = "density")

   }

   as more and more flips are made and new data is observed, our beliefs
   get updated. this is the real power of bayesian id136.


5. test for significance     frequentist vs bayesian

   without going into the rigorous mathematical structures, this section
   will provide you a quick overview of different approaches of
   frequentist and bayesian methods to test for significance and
   difference between groups and which method is most reliable.


5.1. p-value

   in this, the t-score for a particular sample from a sampling
   distribution of fixed size is calculated. then, p-values are predicted.
   we can interpret p values as (taking an example of p-value as 0.02 for
   a distribution of mean 100) : there is 2% id203 that the sample
   will have mean equal to 100.

   this interpretation suffers from the flaw that for sampling
   distributions of different sizes, one is bound to get different t-score
   and hence different p-value. it is completely absurd. a p-value less
   than 5% does not guarantee that null hypothesis is wrong nor a p-value
   greater than 5% ensures that null hypothesis is right.


5.2. confidence intervals

   confidence intervals also suffer from the same defect. moreover since
   c.i is not a id203 distribution , there is no way to know which
   values are most probable.


5.3. bayes factor

   bayes factor is the equivalent of p-value in the bayesian framework.
   lets understand it in an comprehensive manner.

   the null hypothesis in bayesian framework assumes     id203
   distribution only at a particular value of a parameter (say   =0.5) and
   a zero id203 else where. (m1)

   the alternative hypothesis is that all values of    are possible, hence
   a flat curve representing the distribution. (m2)

   now, posterior distribution of the new data looks like below.

   [screenshot%20%2842%29.png]

   bayesian statistics adjusted credibility (id203) of various
   values of   . it can be easily seen that the id203 distribution
   has shifted towards m2 with a value higher than m1 i.e m2 is more
   likely to happen.

   bayes factor does not depend upon the actual distribution values of   
   but the magnitude of shift in values of m1 and m2.

   in panel a (shown above): left bar (m1) is the prior id203 of the
   null hypothesis.

   in panel b (shown), the left bar is the posterior id203 of the
   null hypothesis.

   bayes factor is defined as the ratio of the posterior odds to the prior
   odds,

   [%5cfrac%7bp%28m%3dnull%29%7d%7bp%28m%3dalt%29%7d]

   to reject a null hypothesis, a bf <1/10 is preferred.

   we can see the immediate benefits of using bayes factor instead of
   p-values since they are independent of intentions and sample size.


5.4. high density interval (hdi)

   hdi is formed from the posterior distribution after observing the new
   data. since hdi is a id203, the 95% hdi gives the 95% most
   credible values. it is also guaranteed that 95 % values will lie in
   this interval unlike c.i.

   notice, how the 95% hdi in prior distribution is wider than the 95%
   posterior distribution. this is because our belief in hdi increases
   upon observation of new data.

   [screenshot%20%2843%29.png]


end notes

   the aim of this article was to get you thinking about the different
   type of statistical philosophies out there and how any single of them
   cannot be used in every situation.

   it   s a high time that both the philosophies are merged to mitigate the
   real world problems by addressing the flaws of the other. part ii of
   this series will focus on the id84 techniques using
   mcmc (id115) algorithms. part iii will be based on
   creating a bayesian regression model from scratch and interpreting its
   results in r. so, before i start with part ii, i would like to have
   your suggestions / feedback on this article.

   did you like reading this article ? as a beginner, were you able to
   understand the concepts? let me know in comments.

you can test your skills and knowledge. check out [107]live competitions and
compete with best data scientists from all over the world.

   you can also read this article on analytics vidhya's android app
   [108]get it on google play

share this:

     * [109]click to share on linkedin (opens in new window)
     * [110]click to share on facebook (opens in new window)
     * [111]click to share on twitter (opens in new window)
     * [112]click to share on pocket (opens in new window)
     * [113]click to share on reddit (opens in new window)
     *

related articles

   [ins: :ins]

   tags : [114]bayes id136, [115]id47, [116]bayesian
   analysis, [117]id155, [118]frequentist, [119]p value,
   [120]id203 distribution, [121]statistics
   next article

operations analytics case study (level : hard)

   previous article

web analytics     bangalore (5 -7 years of experience)

[122]nss

   i am a perpetual, quick learner and keen to explore the realm of data
   analytics and science. i am deeply excited about the times we live in
   and the rate at which data is being generated and being transformed as
   an asset. i am well versed with a few tools for dealing with data and
   also in the process of learning some other tools and knowledge required
   to exploit data.
     *

   this article is quite old and you might not get a prompt response from
   the author. we request you to post this comment on analytics vidhya's
   [123]discussion portal to get your queries resolved

32 comments

     * alex says:
       [124]june 20, 2016 at 6:47 am
       thx for this great explanation.
       i   m a beginner in statistics and data science and i really
       appreciate it.
       if you   re interested to see another approach, how toddler   s brain
       use bayesian statistics in a natural way there is a few
       easy-to-understand neuroscience courses :
       [125]http://www.college-de-france.fr/site/en-stanislas-dehaene/_cou
       rse.htm
       [126]reply
     * nikhil says:
       [127]june 20, 2016 at 8:55 am
       hey one question `difference` -> 0.5*(no. of tosses)     no. of heads
       is it correct?
       [128]reply
          + nss says:
            [129]june 23, 2016 at 8:04 am
            @nikhil    thanks for bringing it to the notice. it should be
            no.of heads     0.5(no.of tosses).
            [130]reply
     * cicek says:
       [131]june 20, 2016 at 12:44 pm
       did you miss the index i of a in the general formula of the bayes   
       theorem on the left hand side of the equation (section 3.2)?
       [132]reply
          + nss says:
            [133]june 23, 2016 at 8:07 am
            no, i didn   t.     
            [134]reply
               o sharan lobana says:
                 [135]september 21, 2016 at 1:05 am
                 are you sure you the    i    in the subscript of the final
                 equation of section 3.2 isn   t required. i think it should
                 be a instead of ai on the right hand side numerator.
                 [136]reply
                    # nss says:
                      [137]june 28, 2017 at 10:37 am
                      yes, it is required. i have made the necessary
                      changes.
                      [138]reply
               o alper says:
                 [139]june 16, 2017 at 6:00 pm
                 cicek: i also think the index i is missing in lhs of the
                 general formula in subsection 3.2 (the last equation in
                 that subsection).
                 [140]reply
                    # nss says:
                      [141]june 28, 2017 at 10:38 am
                      yes, it has been updated. thanks for pointing out.
                      [142]reply
     * bharath says:
       [143]june 20, 2016 at 2:54 pm
       nice visual to represent id47, thanks
       [144]reply
          + nss says:
            [145]june 23, 2016 at 8:09 am
            thanks bharath       it keeps us motivated.
            [146]reply
     * anja rebber says:
       [147]june 20, 2016 at 8:01 pm
       i will let you know tomorrow! because tomorrow i have to do
       teaching assistance in a class on bayesian statistics. i will try
       to explain it your way, then i tell you how it worked out.
       for me it looks perfect! thanks!
       [148]reply
     * melissa l moser says:
       [149]june 21, 2016 at 6:01 am
       thorough and easy to understand synopsis. good stuff. thanks.
       [150]reply
     * gaurav says:
       [151]june 21, 2016 at 9:25 am
       it was a really nice article, with nice flow to compare frequentist
       vs bayesian approach. i will look forward to next part of the
       tutorials.
       [152]reply
     * sachin hosamani says:
       [153]june 21, 2016 at 9:28 am
       excellent article. i didn   t knew much about bayesian statistics,
       however this article helped me improve my understanding of bayesian
       statistics.
       [154]reply
     * fang xianfu says:
       [155]june 21, 2016 at 1:31 pm
       without wanting to suggest that one approach or the other is
       better, i don   t think this article fulfilled its objective of
       communicating in    simple english   .
       the communication of the ideas was fine enough, but if the focus is
       to be on    simple english    then i think that the terminology needs
       to be introduced with more care, and mathematical explanations
       should be limited and vigorously explained.
       [156]reply
     * davide caldara says:
       [157]june 21, 2016 at 4:24 pm
       it   s a good article.
       as a beginner i have a few difficulties with the last part (chapter
       5) but the previous parts were really good
       [158]reply
     * scott says:
       [159]june 22, 2016 at 1:48 am
       very nice refresher. thank you and keep them coming.
       [160]reply
     * sailesh says:
       [161]june 22, 2016 at 2:38 am
       thank you, nss for this wonderful introduction to bayesian
       statistics. the visualizations were just perfect to establish the
       concepts discussed. although i lost my way a little towards the
       end(bayesian factor), appreciate your effort!
       [162]reply
     * roel says:
       [163]june 22, 2016 at 9:46 am
       this    stopping intention    is not a regular thing in frequentist
       statistics. in fact i only hear about it today. it sort of
       distracts me from the bayesian thing that is the real topic of this
       post. perhaps you never worked with frequentist statistics?
       [164]reply
          + nss says:
            [165]june 23, 2016 at 8:23 am
            @roel
            irregularities is what we care about ? isn   t it ? and well,
            stopping intentions do play a role. what if as a simple
            example: person a performs hypothesis testing for coin toss
            based on total flips and person b based on time duration . do
            we expect to see the same result in both the cases ?
            [166]reply
     * roel says:
       [167]june 22, 2016 at 9:59 am
       some small notes, but let me make this clear: i think bayesian
       statistics makes often much more sense, but i would love it if you
       at least make the description of the frequentist statistics
       correct. also let   s not make this a debate about which is better,
       it   s as useless as the python vs r debate, there is none.
          in this, the t-score for a particular sample from a sampling
       distribution of fixed size is calculated. then, p-values are
       predicted. we can interpret p values as (taking an example of
       p-value as 0.02 for a distribution of mean 100) : there is 2%
       id203 that the sample will have mean equal to 100.   
       this is incorrect. a p-value says something about the population.
       you id136 about the population based on a sample. if mean 100
       in the sample has p-value 0.02 this means the id203 to see
       this value in the population under the nul-hypothesis is .02. which
       makes it more likely that your alternative hypothesis is true.
          sampling distributions of different sizes, one is bound to get
       different t-score and hence different p-value. it is completely
       absurd.   
       correct it is an estimation, and you correct for the uncertainty in
       i know it makes no sense, we test for an effect by looking at the
       probabilty of a score when there is no effect. if that is a small
       change we say that the alternative is more likely.
       [168]reply
          + nss says:
            [169]june 23, 2016 at 8:17 am
            @roel
            i agree this post isn   t about the debate on which is better-
            bayesian or frequentist.
            and i quote again-    the aim of this article was to get you
            thinking about the different type of statistical philosophies
            out there and how any single of them cannot be used in every
            situation   .
            regarding p-value , what you said is correct- given your
            hypothesis, the id203         .
            but generally, what people infer is     the id203 of your
            hypothesis,given the p-value   ..
            but, still p-value is not the robust mean to validate
            hypothesis, i feel.
            i would like to hear more.
            thanks for commenting.     
            [170]reply
     * jos   avila says:
       [171]june 23, 2016 at 4:18 pm
       this is a really good post! thanks for share this information in a
       simple way!
       i have some questions that i would like to ask!
       1) i didn   t understand very well why the c.i.    do not provide the
       most probable value for a parameter and the most probable values   .
       before to read this post i was thinking in this way: the real mean
       of population is between the range given by the ci with a, for
       example, 95%)
       2) i read a recent paper which states that rejecting the null
       hypothesis by bayes factor at <1/10 could be equivalent as assuming
       a p value <0.001 for reject the null hypothesis (actually, i don't
       remember very well the exact values, but the idea of makeing this
       equivalence is correct? could be good to apply this equivalence in
       research?)
       3) for making bayesian statistics, is better to use r or phyton? or
       it depends on each person? now i m learning phyton because i want
       to apply it to my research (i m biologist!)
       thanks in advance and sorry for my not so good english!
       jose avila
       [172]reply
     * shane says:
       [173]june 24, 2016 at 12:38 pm
       hi nss,
       a quick question about section 4.2: if alpha = no. of heads and
       beta = no. of tail
       why the alpha value = the number of trails in the r code:
       > alpha=c(0,2,10,20,50,500) # it looks like the total number of
       trails, instead of number of heads   .
       > beta=c(0,2,8,11,27,232)
       i plotted the graphs and the second one looks different from yours   
       thanks,
       shane
       [174]reply
     * ayush mehta says:
       [175]june 26, 2016 at 9:52 am
       how can i know when the other posts in this series are released?
       [176]reply
     * boomy says:
       [177]june 26, 2016 at 4:36 pm
       thank you for this blog. i like it and i understand about concept
       bayesian. i can practice in r and i can see something.
       i think, you should write the next guide on bayesian in the next
       time.
       i will wait.
       [178]reply
     * asanka says:
       [179]june 29, 2016 at 8:12 am
       hi   
       good post and keep it up     very useful   
       [180]reply
     * nikhil says:
       [181]august 4, 2016 at 7:11 pm
       printer friendly version please!
       [182]reply
     * [183]nishtha says:
       [184]march 25, 2017 at 2:13 pm
       hi nss
       thanks for the much needed comprehensive article. please tell me a
       thing :-
          since hdi is a id203, the 95% hdi gives the 95% most
       credible values. it is also guaranteed that 95 % values will lie in
       this interval unlike c.i.   
       how is this unlike ci? as far as i know ci is the exact same thing.
       [185]reply
          + nss says:
            [186]march 25, 2017 at 3:45 pm
            @nishtha    . ci is the id203 of the intervals containing
            the population parameter i.e 95% ci would mean 95% of
            intervals would contain the population parameter whereas in
            hdi it is the presence of a population parameter in an
            interval with 95% id203. both are different things. hope
            this helps.
            [187]reply
     * stev says:
       [188]june 19, 2017 at 9:22 am
       hi, greetings from latam. i liked this. you   ve given us a good and
       simple explanation about bayesian statistics. help me, i   ve not
       found the next parts yet.
       [189]reply

   [ins: :ins]

top analytics vidhya users

   rank                  name                  points
   1    [1.jpg?date=2019-04-06] [190]srk       3924
   2    [2.jpg?date=2019-04-06] [191]mark12    3510
   3    [3.jpg?date=2019-04-06] [192]nilabha   3261
   4    [4.jpg?date=2019-04-06] [193]nitish007 3237
   5    [5.jpg?date=2019-04-06] [194]tezdhar   3082
   [195]more user rankings
   [ins: :ins]
   [ins: :ins]

popular posts

     * [196]24 ultimate data science projects to boost your knowledge and
       skills (& can be accessed freely)
     * [197]understanding support vector machine algorithm from examples
       (along with code)
     * [198]essentials of machine learning algorithms (with python and r
       codes)
     * [199]a complete tutorial to learn data science with python from
       scratch
     * [200]7 types of regression techniques you should know!
     * [201]6 easy steps to learn naive bayes algorithm (with codes in
       python and r)
     * [202]a simple introduction to anova (with applications in excel)
     * [203]stock prices prediction using machine learning and deep
       learning techniques (with python codes)

   [ins: :ins]

recent posts

   [204]top 5 machine learning github repositories and reddit discussions
   from march 2019

[205]top 5 machine learning github repositories and reddit discussions from
march 2019

   april 4, 2019

   [206]id161 tutorial: a step-by-step introduction to image
   segmentation techniques (part 1)

[207]id161 tutorial: a step-by-step introduction to image
segmentation techniques (part 1)

   april 1, 2019

   [208]nuts and bolts of id23: introduction to temporal
   difference (td) learning

[209]nuts and bolts of id23: introduction to temporal
difference (td) learning

   march 28, 2019

   [210]16 opencv functions to start your id161 journey (with
   python code)

[211]16 opencv functions to start your id161 journey (with python
code)

   march 25, 2019

   [212][ds-finhack.jpg]

   [213][hikeathon.png]

   [av-white.d14465ee4af2.png]

analytics vidhya

     * [214]about us
     * [215]our team
     * [216]career
     * [217]contact us
     * [218]write for us

   [219]about us
   [220]   
   [221]our team
   [222]   
   [223]careers
   [224]   
   [225]contact us

data scientists

     * [226]blog
     * [227]hackathon
     * [228]discussions
     * [229]apply jobs
     * [230]leaderboard

companies

     * [231]post jobs
     * [232]trainings
     * [233]hiring hackathons
     * [234]advertising
     * [235]reach us

   don't have an account? [236]sign up here.

join our community :

   [237]46336 [238]followers
   [239]20222 [240]followers
   [241]followers
   [242]7513 [243]followers
   ____________________ >

      copyright 2013-2019 analytics vidhya.
     * [244]privacy policy
     * [245]terms of use
     * [246]refund policy

   don't have an account? [247]sign up here

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [248](button) join now

   subscribe!

   [loading.gif]
   ____________________

   ____________________

   ____________________
   [button input] (not implemented)_________________

   download resource

join the nextgen data science ecosystem

     * learn: get access to some of the best courses on data science
       created by us
     * engage: interact with thousands of data science professionals
       across the globe!
     * compete: compete in our hackathons and win exciting prizes
     * get hired: get information of jobs in data science community and
       build your profile

   [249](button) join now

   subscribe!

references

   visible links
   1. https://www.analyticsvidhya.com/feed/
   2. https://www.analyticsvidhya.com/comments/feed/
   3. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/feed/
   4. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
   5. https://www.analyticsvidhya.com/wp-json/oembed/1.0/embed?url=https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/&format=xml
   6. https://googletagmanager.com/ns.html?id=gtm-mpsm42v
   7. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=blog&utm_medium=flashstrip
   8. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
   9. https://www.analyticsvidhya.com/blog-archive/
  10. https://www.analyticsvidhya.com/blog/category/machine-learning/
  11. https://www.analyticsvidhya.com/blog/category/deep-learning/
  12. https://www.analyticsvidhya.com/blog/category/career/
  13. https://www.analyticsvidhya.com/blog/category/stories/
  14. https://www.analyticsvidhya.com/blog/category/podcast/
  15. https://www.analyticsvidhya.com/blog/category/infographics/
  16. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  17. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  18. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  19. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  20. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  21. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  22. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  23. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  24. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  25. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  26. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  27. https://discuss.analyticsvidhya.com/
  28. https://www.analyticsvidhya.com/blog/category/events/
  29. https://www.analyticsvidhya.com/datahack-summit-2018/
  30. https://www.analyticsvidhya.com/datahacksummit/
  31. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  32. http://www.analyticsvidhya.com/about-me/write/
  33. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  34. https://datahack.analyticsvidhya.com/contest/all
  35. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  36. https://www.analyticsvidhya.com/jobs/
  37. https://courses.analyticsvidhya.com/
  38. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  39. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  40. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  41. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  42. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  43. https://www.analyticsvidhya.com/contact/
  44. https://www.analyticsvidhya.com/
  45. https://www.analyticsvidhya.com/blog-archive/
  46. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  47. https://discuss.analyticsvidhya.com/
  48. https://datahack.analyticsvidhya.com/
  49. https://www.analyticsvidhya.com/jobs/
  50. https://www.analyticsvidhya.com/corporate/
  51. https://www.analyticsvidhya.com/blog/
  52. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  53. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  54. https://courses.analyticsvidhya.com/bundles/ai-blackbelt-beginner-to-master?utm_source=avtopbanner&utm_medium=display
  55. https://www.analyticsvidhya.com/blog/
  56. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  57. https://www.analyticsvidhya.com/blog-archive/
  58. https://www.analyticsvidhya.com/blog/category/machine-learning/
  59. https://www.analyticsvidhya.com/blog/category/deep-learning/
  60. https://www.analyticsvidhya.com/blog/category/career/
  61. https://www.analyticsvidhya.com/blog/category/stories/
  62. https://www.analyticsvidhya.com/blog/category/podcast/
  63. https://www.analyticsvidhya.com/blog/category/infographics/
  64. https://courses.analyticsvidhya.com/?utm_source=home_blog_navbar
  65. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/
  66. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-business-analyst-sas/
  67. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-r-data-science/
  68. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/learning-path-data-science-python/
  69. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/weka-gui-learn-machine-learning/
  70. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/tableau-learning-path/
  71. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/qlikview-learning-path/
  72. https://www.analyticsvidhya.com/learning-paths-data-science-business-analytics-business-intelligence-big-data/newbie-d3-js-expert-complete-path-create-interactive-visualization-d3-js/
  73. https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/
  74. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  75. https://discuss.analyticsvidhya.com/
  76. https://www.analyticsvidhya.com/blog/category/events/
  77. https://www.analyticsvidhya.com/datahack-summit-2018/
  78. https://www.analyticsvidhya.com/datahacksummit/
  79. https://www.analyticsvidhya.com/student-datafest-2018/?utm_source=homepage_menu
  80. http://www.analyticsvidhya.com/about-me/write/
  81. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  82. https://datahack.analyticsvidhya.com/contest/all
  83. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  84. https://www.analyticsvidhya.com/jobs/
  85. https://courses.analyticsvidhya.com/
  86. https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning/?utm_source=blog-navbar&utm_medium=web
  87. https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog-navbar&utm_medium=web
  88. https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog-navbar&utm_medium=web
  89. https://courses.analyticsvidhya.com/courses/microsoft-excel-beginners-to-advanced/?utm_source=blog-navbar&utm_medium=web
  90. https://courses.analyticsvidhya.com/collections/?utm_source=blog-navbar&utm_medium=web
  91. https://www.analyticsvidhya.com/contact/
  92. https://www.analyticsvidhya.com/
  93. https://www.analyticsvidhya.com/blog/category/business-analytics/
  94. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/
  95. https://www.analyticsvidhya.com/blog/category/business-analytics/
  96. https://www.analyticsvidhya.com/blog/category/r/
  97. https://www.analyticsvidhya.com/blog/author/nss/
  98. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=bayesianstatisticsforbeginnersarticle
  99. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=bayesianstatisticsforbeginnersarticle
 100. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=bayesianstatisticsforbeginnersarticle
 101. http://courses.analyticsvidhya.com/courses/introduction-to-data-science-2?utm_source=blog&utm_medium=bayesianstatisticsforbeginnersarticle
 102. https://classroom.udacity.com/courses/ud201/lessons/1306898579/concepts/1611758530923
 103. https://en.wikipedia.org/wiki/niki_lauda
 104. https://en.wikipedia.org/wiki/james_hunt
 105. https://www.khanacademy.org/math/linear-algebra
 106. https://www.khanacademy.org/math/id203
 107. http://datahack.analyticsvidhya.com/contest/all
 108. https://play.google.com/store/apps/details?id=com.analyticsvidhya.android&utm_source=blog_article&utm_campaign=blog&pcampaignid=mkt-other-global-all-co-prtnr-py-partbadge-mar2515-1
 109. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/?share=linkedin
 110. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/?share=facebook
 111. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/?share=twitter
 112. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/?share=pocket
 113. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/?share=reddit
 114. https://www.analyticsvidhya.com/blog/tag/bayes-id136/
 115. https://www.analyticsvidhya.com/blog/tag/bayes-theorem/
 116. https://www.analyticsvidhya.com/blog/tag/bayesian-analysis/
 117. https://www.analyticsvidhya.com/blog/tag/conditional-id203/
 118. https://www.analyticsvidhya.com/blog/tag/frequentist/
 119. https://www.analyticsvidhya.com/blog/tag/p-value/
 120. https://www.analyticsvidhya.com/blog/tag/id203-distribution/
 121. https://www.analyticsvidhya.com/blog/tag/statistics/
 122. https://www.analyticsvidhya.com/blog/author/nss/
 123. https://discuss.analyticsvidhya.com/
 124. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112415
 125. http://www.college-de-france.fr/site/en-stanislas-dehaene/_course.htm
 126. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112415
 127. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112431
 128. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112431
 129. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112573
 130. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112573
 131. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112440
 132. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112440
 133. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112574
 134. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112574
 135. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-116263
 136. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-116263
 137. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-131232
 138. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-131232
 139. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-130604
 140. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-130604
 141. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-131233
 142. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-131233
 143. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112444
 144. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112444
 145. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112575
 146. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112575
 147. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112452
 148. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112452
 149. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112471
 150. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112471
 151. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112479
 152. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112479
 153. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112480
 154. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112480
 155. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112492
 156. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112492
 157. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112497
 158. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112497
 159. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112513
 160. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112513
 161. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112514
 162. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112514
 163. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112532
 164. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112532
 165. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112577
 166. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112577
 167. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112533
 168. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112533
 169. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112576
 170. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112576
 171. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112601
 172. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112601
 173. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112641
 174. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112641
 175. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112700
 176. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112700
 177. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112714
 178. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112714
 179. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112825
 180. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-112825
 181. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-114453
 182. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-114453
 183. http://www.hopeandhappiness.co/
 184. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-125531
 185. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-125531
 186. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-125535
 187. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-125535
 188. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-130723
 189. https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/#comment-130723
 190. https://datahack.analyticsvidhya.com/user/profile/srk
 191. https://datahack.analyticsvidhya.com/user/profile/mark12
 192. https://datahack.analyticsvidhya.com/user/profile/nilabha
 193. https://datahack.analyticsvidhya.com/user/profile/nitish007
 194. https://datahack.analyticsvidhya.com/user/profile/tezdhar
 195. https://datahack.analyticsvidhya.com/top-competitor/?utm_source=blog-navbar&utm_medium=web
 196. https://www.analyticsvidhya.com/blog/2018/05/24-ultimate-data-science-projects-to-boost-your-knowledge-and-skills/
 197. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/
 198. https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/
 199. https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-learn-data-science-python-scratch-2/
 200. https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/
 201. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/
 202. https://www.analyticsvidhya.com/blog/2018/01/anova-analysis-of-variance/
 203. https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/
 204. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 205. https://www.analyticsvidhya.com/blog/2019/04/top-5-machine-learning-github-reddit/
 206. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 207. https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/
 208. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 209. https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/
 210. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 211. https://www.analyticsvidhya.com/blog/2019/03/opencv-functions-computer-vision-python/
 212. https://datahack.analyticsvidhya.com/contest/ltfs-datascience-finhack-an-online-hackathon/?utm_source=sticky_banner1&utm_medium=display
 213. https://datahack.analyticsvidhya.com/contest/hikeathon/?utm_source=sticky_banner2&utm_medium=display
 214. http://www.analyticsvidhya.com/about-me/
 215. https://www.analyticsvidhya.com/about-me/team/
 216. https://www.analyticsvidhya.com/career-analytics-vidhya/
 217. https://www.analyticsvidhya.com/contact/
 218. https://www.analyticsvidhya.com/about-me/write/
 219. http://www.analyticsvidhya.com/about-me/
 220. https://www.analyticsvidhya.com/about-me/team/
 221. https://www.analyticsvidhya.com/about-me/team/
 222. https://www.analyticsvidhya.com/about-me/team/
 223. https://www.analyticsvidhya.com/career-analytics-vidhya/
 224. https://www.analyticsvidhya.com/about-me/team/
 225. https://www.analyticsvidhya.com/contact/
 226. https://www.analyticsvidhya.com/blog
 227. https://datahack.analyticsvidhya.com/
 228. https://discuss.analyticsvidhya.com/
 229. https://www.analyticsvidhya.com/jobs/
 230. https://datahack.analyticsvidhya.com/users/
 231. https://www.analyticsvidhya.com/corporate/
 232. https://trainings.analyticsvidhya.com/
 233. https://datahack.analyticsvidhya.com/
 234. https://www.analyticsvidhya.com/contact/
 235. https://www.analyticsvidhya.com/contact/
 236. https://datahack.analyticsvidhya.com/signup/
 237. https://www.facebook.com/analyticsvidhya/
 238. https://www.facebook.com/analyticsvidhya/
 239. https://twitter.com/analyticsvidhya
 240. https://twitter.com/analyticsvidhya
 241. https://plus.google.com/+analyticsvidhya
 242. https://in.linkedin.com/company/analytics-vidhya
 243. https://in.linkedin.com/company/analytics-vidhya
 244. https://www.analyticsvidhya.com/privacy-policy/
 245. https://www.analyticsvidhya.com/terms/
 246. https://www.analyticsvidhya.com/refund-policy/
 247. https://id.analyticsvidhya.com/accounts/signup/
 248. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web
 249. https://id.analyticsvidhya.com/accounts/login/?next=https://www.analyticsvidhya.com/blog/&utm_source=blog-subscribe&utm_medium=web

   hidden links:
 251. https://www.facebook.com/analyticsvidhya
 252. https://twitter.com/analyticsvidhya
 253. https://plus.google.com/+analyticsvidhya/posts
 254. https://in.linkedin.com/company/analytics-vidhya
 255. https://www.analyticsvidhya.com/blog/2016/06/operations-analytics-case-study-level-hard/
 256. https://www.analyticsvidhya.com/blog/2016/06/web-analytics-bangalore-5-7-years-experience/
 257. https://www.analyticsvidhya.com/blog/author/nss/
 258. https://in.linkedin.com/in/neeraj-singh-sarwan-a84b6965
 259. http://www.edvancer.in/certified-data-scientist-with-python-course?utm_source=av&utm_medium=avads&utm_campaign=avadsnonfc&utm_content=pythonavad
 260. https://www.facebook.com/analyticsvidhya/
 261. https://twitter.com/analyticsvidhya
 262. https://plus.google.com/+analyticsvidhya
 263. https://plus.google.com/+analyticsvidhya
 264. https://in.linkedin.com/company/analytics-vidhya
 265. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 266. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 267. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 268. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 269. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 270. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 271. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 272. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 273. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 274. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 275. javascript:void(0);
 276. javascript:void(0);
 277. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 278. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 279. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 280. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 281. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 282. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 283. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 284. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 285. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 286. https://www.addtoany.com/add_to/whatsapp?linkurl=https%3a%2f%2fwww.analyticsvidhya.com%2fblog%2f2016%2f06%2fbayesian-statistics-beginners-simple-english%2f&linkname=bayesian%20statistics%20explained%20in%20simple%20english%20for%20beginners
 287. javascript:void(0);
 288. javascript:void(0);
