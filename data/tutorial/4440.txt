   #[1]awni hannun - writing about machine learning

   [2][952ab78c5d7894a988d7cd403bbf9cf4?s=80]

[3]awni hannun

   writing about machine learning

   [4]about

pytorch or tensorflow?

   posted on august 17, 2017

   this is a guide to the main differences i   ve found between [5]pytorch
   and [6]tensorflow. this post is intended to be useful for anyone
   considering starting a new project or making the switch from one deep
   learning framework to another. the focus is on programmability and
   flexibility when setting up the components of the training and
   deployment deep learning stack. i won   t go into performance (speed /
   memory usage) trade-offs.

summary

   pytorch is better for rapid prototyping in research, for hobbyists and
   for small scale projects. tensorflow is better for large-scale
   deployments, especially when cross-platform and embedded deployment is
   a consideration.

ramp-up time

   winner: pytorch

   pytorch is essentially a gpu enabled drop-in replacement for numpy
   equipped with higher-level functionality for building and training deep
   neural networks. this makes pytorch especially easy to learn if you are
   familiar with numpy, python and the usual deep learning abstractions
   (convolutional layers, recurrent layers, sgd, etc.).

   on the other hand, a good mental model for tensorflow is a programming
   language embedded within python. when you write tensorflow code it gets
      compiled    into a graph by python and then run by the tensorflow
   execution engine. i   ve seen newcomers to tensorflow struggle to wrap
   their head around this added layer of indirection. also because of
   this, tensorflow has a few extra concepts to learn such as the session,
   the graph, variable scoping and placeholders. also more boilerplate
   code is needed to get a basic model running. the ramp-up time to get
   going with tensorflow is definitely longer than pytorch.

graph creation and debugging

   winner: pytorch

   creating and running the computation graph is perhaps where the two
   frameworks differ the most. in pytorch the graph construction is
   dynamic, meaning the graph is built at run-time. in tensorflow the
   graph construction is static, meaning the graph is    compiled    and then
   run. as a simple example, in pytorch you can write a for loop
   construction using standard python syntax
for _ in range(t):
    h = torch.matmul(w, h) + b

   and t can change between executions of this code. in tensorflow this
   requires the use of [7]control flow operations in constructing the
   graph such as the tf.while_loop. tensorflow does have the dynamic_id56
   for the more common constructs but creating custom dynamic computations
   is more difficult.

   the simple graph construction in pytorch is easier to reason about, but
   perhaps even more importantly, it   s easier to debug. debugging pytorch
   code is just like debugging python code. you can use pdb and set a
   break point anywhere. debugging tensorflow code is not so easy. the two
   options are to request the variables you want to inspect from the
   session or to learn and use the tensorflow debugger (tfdbg).

coverage

   winner: tensorflow

   as pytorch ages, i expect the gap here will converge to zero. however,
   there is still some functionality which tensorflow supports that
   pytorch doesn   t. a few features that pytorch doesn   t have (at the time
   of writing) are:
     * flipping a tensor along a dimension (np.flip, np.flipud, np.fliplr)
     * checking a tensor for nan and infinity (np.is_nan, np.is_inf)
     * fast fourier transforms (np.fft)

   these are all supported in tensorflow. also the tensorflow contrib
   package has many more higher level functions and models than pytorch.

serialization

   winner: tensorflow

   saving and loading models is simple in both frameworks. pytorch has an
   especially simple api which can either save all the weights of a model
   or pickle the entire class. the tensorflow saver object is also easy to
   use and exposes a few more options for check-pointing.

   the main advantage tensorflow has in serialization is that the entire
   graph can be saved as a protocol buffer. this includes parameters as
   well as operations. the graph can then be loaded in other supported
   languages (c++, java). this is critical for deployment stacks where
   python is not an option. also this can, in theory, be useful when you
   change the model source code but want to be able to run old models.

deployment

   winner: tensorflow

   for small scale server-side deployments both frameworks are easy to
   wrap in e.g. a flask web server.

   for mobile and embedded deployments tensorflow [8]works. this is more
   than can be said of most other deep learning frame-works including
   pytorch. deploying to android or ios does require a non-trivial amount
   of work in tensorflow but at least you don   t have to rewrite the entire
   id136 portion of your model in java or c++.

   for high-performance server-side deployments there is [9]tensorflow
   serving. i don   t have experience with tensorflow serving, so i can   t
   write confidently about the pros and cons. for heavily used machine
   learning services, i suspect tensorflow serving could be a sufficient
   reason to stay with tensorflow. other than performance, one of the
   noticeable features of tensorflow serving is that models can be
   hot-swapped easily without bringing the service down. checkout this
   [10]blog post from zendesk for an example deployment of a qa bot with
   tensorflow serving.

documentation

   winner: tie

   i   ve found everything i need in the docs for both frameworks. the
   python apis are well documented and there are enough examples and
   tutorials to learn either framework.

   one edge case gripe is that the pytorch c library is mostly
   undocumented. however, this really only matters when writing a custom c
   extension and perhaps if contributing to the software.

data loading

   winner: pytorch

   the apis for data loading are well designed in pytorch. the interfaces
   are specified in a dataset, a sampler, and a data loader. a data loader
   takes a dataset and a sampler and produces an iterator over the dataset
   according to the sampler   s schedule. parallelizing data loading is as
   simple as passing a num_workers argument to the data loader.

   i haven   t found the tools for data loading in tensorflow (readers,
   queues, queue runners, etc.) especially useful. in part this is because
   adding all the preprocessing code you want to run in parallel into the
   tensorflow graph is not always straight-forward (e.g. computing a
   spectrogram). also, the api itself is more verbose and harder to learn.

device management

   winner: tensorflow

   device management in tensorflow is about as seaid113ss as it gets.
   usually you don   t need to specify anything since the defaults are set
   well. for example, tensorflow assumes you want to run on the gpu if one
   is available. in pytorch you have to explicitly move everything onto
   the device even if cuda is enabled.

   the only downside with tensorflow device management is that by default
   it consumes all the memory on all available gpus even if only one is
   being used. the simple workaround is to specify cuda_visible_devices.
   sometimes people forget this, and gpus can appear to be busy when they
   are in fact idle.

   in pytorch, i   ve found my code needs more frequent checks for cuda
   availability and more explicit device management. this is especially
   the case when writing code that should be able to run on both the cpu
   and gpu. also converting say a pytorch variable on the gpu into a numpy
   array is somewhat verbose.
numpy_var = variable.cpu().data.numpy()

custom extensions

   winner: pytorch

   building or binding custom extensions written in c, c++ or cuda is
   doable with both frameworks. tensorflow again requires more boiler
   plate code though is arguably cleaner for supporting multiple types and
   devices. in pytorch you simply write an interface and corresponding
   implementation for each of the cpu and gpu versions. compiling the
   extension is also straight-forward with both frameworks and doesn   t
   require downloading any headers or source code outside of what   s
   included with the pip installation.

a note on tensorboard

   tensorboard is a tool for visualizing various aspects of training
   machine learning models. it   s one of the most useful features to come
   out of the tensorflow project. with a few code snippets in a training
   script you can view training curves and validation results of any
   model. tensorboard runs as a web service which is especially convenient
   for visualizing results stored on headless nodes.

   this was one feature that i made sure i could keep (or find an
   alternative to) before using pytorch. thankfully there are, at least,
   two open-source projects which allow for this. the first is
   [11]tensorboard_logger and the second is [12]crayon. the
   tensorboard_logger library is even easier to use than tensorboard
      summaries    in tensorflow, though you need tensorboard installed to use
   it. the crayon project is a complete replacement for tensorboard but
   requires more setup (docker is a prerequisite).

a note on keras

   [13]keras is a higher-level api with a configurable back-end. at the
   moment tensorflow, theano and cntk are supported, though perhaps in the
   not too distant future pytorch will be included as well. keras is also
   distributed with tensorflow as a part of tf.contrib.

   though i didn   t discuss keras above, the api is especially easy to use.
   it   s one of the fastest ways to get running with many of the more
   commonly used deep neural network architectures. that said, the api is
   not as flexible as pytorch or core tensorflow.

a note on tensorflow fold

   google announced [14]tensorflow fold in february of 2017. the library
   is built on top of tensorflow and allows for more dynamic graph
   construction. the main advantage of the library appears to be the
   dynamic batching. dynamic batching automatically batches computations
   on inputs of varying size (think recursive networks on parse trees). in
   terms of programmability, the syntax is not as straightforward as
   pytorch, though in a some cases the performance improvements from
   batching may be worth the cost.
   please enable javascript to view the [15]comments powered by disqus.

references

   visible links
   1. https://awni.github.io/feed.xml
   2. https://awni.github.io/
   3. https://awni.github.io/
   4. https://awni.github.io/about
   5. http://pytorch.org/
   6. https://www.tensorflow.org/
   7. https://www.tensorflow.org/api_guides/python/control_flow_ops#control_flow_operations
   8. https://www.tensorflow.org/mobile/
   9. https://www.tensorflow.org/serving/
  10. https://medium.com/zendesk-engineering/how-zendesk-serves-tensorflow-models-in-production-751ee22f0f4b
  11. https://github.com/teamhg-memex/tensorboard_logger
  12. https://github.com/torrvision/crayon
  13. https://keras.io/
  14. https://research.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html
  15. http://disqus.com/?ref_noscript

   hidden links:
  17. https://github.com/awni
  18. https://www.twitter.com/awnihannun
