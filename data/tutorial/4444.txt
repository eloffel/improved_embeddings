   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

id23 w/ keras + openai: actor-critic models

   [16]go to the profile of yash patel
   [17]yash patel (button) blockedunblock (button) followfollowing
   jul 30, 2017

   quick recap

   last time in our keras/openai tutorial, we discussed a very fundamental
   algorithm in id23: the id25. the deep q-network is
   actually a fairly new advent that arrived on the seen only a couple
   years back, so it is quite incredible if you were able to understand
   and implement this algorithm having just gotten a start in the field.
   as with the original post, let   s take a quick moment to appreciate how
   incredible results we achieved are: in a continuous output space
   scenario and starting with absolutely no knowledge on what    winning   
   entails, we were able to explore our environment and    complete    the
   trials.

   put yourself in the situation of this simulation. this would
   essentially be like asking you to play a game, without a rulebook or
   specific endgoal, and demanding you to continue to play until you win
   (almost seems a bit cruel). and not only that: the possible result
   states you could reach with a series of actions is infinite (i.e.
   continuous observation space)! yet, the id25 converges surprising
   quickly in tackling this seemingly impossible task by maintaining and
   slowly updating value internally to actions.

   even more complex environment

   the step up from the previous mountaincar environment to the pendulum
   is very similar to that from cartpole to mountaincar: we are expanding
   from a discrete environment to continuous. the pendulum environment has
   an infinite input space, meaning that the number of actions you can
   take at any given time is unbounded. why is id25 no longer applicable in
   this environment? wasn   t our implementation of it completely
   independent of the structure of the environment actions?
   [1*j_oex0kpbpwxovmrytn6qg.gif]
   unlike the mountaincar-v0, pendulum-v0 poses an even greater challenge
   by giving us an infinite input space to work with.

   while it was indepedent of what the actions were, the id25 was
   fundamentally premised on having a finite output space. after all,
   think about how we structured the code: the prediction looked to assign
   a score to each of the possible actions at each time step (given the
   current environment state) and simply taking the action that had the
   highest score. we had previously reduced the problem of reinforcement
   learning to effectively assigning scores to actions. but, how would
   this be possible if we have an infinite input space? we would need an
   infinitely large table to keep track of all the q values!
   [1*uiclhh6szihlnmo7q9ytaw.png]
   infinite spreadsheets sound pretty far from ideal!

   so, how do we go about tackling this seemingly impossible task? after
   all, we   re being asked to do something even more insane than before:
   not only are we given a game without instructions to play and win, but
   this game has a controller with infinite buttons on it! let   s see why
   it is that id25 is restricted to a finite number of actions.

   the reason stems from how the model is structured: we have to be able
   to iterate at each time step to update how our position on a particular
   action has changed. that   s exactly why we were having the model predict
   the q values rather than directly predicting what action to take. if we
   did the latter, we would have no idea how to update the model to take
   into account the prediction and what reward we received for future
   predictions. so, the fundamental issue stems from the fact that it
   seems like our model has to output a tabulated calculation of the
   rewards associated with all the possible actions. what if, instead, we
   broke this model apart? what if we had two separate models: one
   outputting the desired action (in the continuous space) and another
   taking in an action as input to produce the q values from id25s? that
   seems to solve our problems and is exactly the basis of the
   actor-critic model!

   actor-critic model theory
   [1*-gfrvlwhcusyhg25rn0iba.png]
   unlike id25s, the actor-critic model (as implied by its name) has two
   separate networks: one that   s used for doing predictions on what action
   to take given the current environment state and another to find the
   value of an action/state

   as we went over in previous section, the entire actor-critic (ac)
   method is premised on having two interacting models. this theme of
   having multiple neural networks that interact is growing more and more
   relevant in both rl and supervised learning, i.e. gans, ac, a3c, did25
   (dueling id25), and so on. getting familiar with these architectures may
   be somewhat intimidating the first time through but is certainly a
   worthwhile exercise: you   ll be able to understand and program some of
   the algorithms that are at the forefront of modern research in the
   field!

   getting back to the topic at hand, the ac model has two aptly named
   components: an actor and a critic. the former takes in the current
   environment state and determines the best action to take from there. it
   is essentially what would have seemed like the natural way to implement
   the id25. the critic plays the    evaluation    role from the id25 by taking
   in the environment state and an action and returning a score that
   represents how apt the action is for the state.

   imagine this as a playground with a kid (the    actor   ) and her parent
   (the    critic   ). the kid is looking around, exploring all the possible
   options in this environment, such as sliding up a slide, swinging on a
   swing, and pulling grass from the ground. the parent will look at the
   kid, and either criticize or complement here based on what she did,
   taking the environment into account. the fact that the parent   s
   decision is environmentally-dependent is both important and intuitive:
   after all, if the child tried to swing on the swing, it would deserve
   far less praise than if she tried to do so on a slide!

   brief aside: chain rule (optional)

   the main point of theory you need to understand is one that underpins a
   large part of modern-day machine learning: the chain rule. it would not
   be a tremendous overstatement to say that chain rule may be one of the
   most pivotal, even though somewhat simple, ideas to grasp to understand
   practical machine learning. in fact, you could probably get away with
   having little math background if you just intuitively understand what
   is conceptually convenyed by the chain rule. i   ll take a very quick
   aside to describe the chain rule, but if you feel quite comfortable
   with it, feel free to jump to the next section, where we actually see
   what the practical outline for developing the ac model looks like and
   how the chain rule fits into that plan.
   [1*udtlmxua_agarxlswcfawg.png]
   a seemingly simple concept probably from your first calculus class, but
   one that forms the modern basis of practical machine learning, due to
   the incredible speedup it enabled in backprop and similar algorithms

   pictorially, this equation seems to make very intuitive sense: after
   all, just    cancel out the numerator/denominator.    there   s one major
   problem with this    intuitive explanation    though: the reasoning in this
   explanation is completely backwards! it is important to remember that
   math is just as much about developing intuitive notation as it is about
   understanding the concepts. and so, people developing this    fractional   
   notation because the chain rule behaves very similarly to simplifying
   fractional products. so, people who try to explain the concept just
   through the notation are skipping a key step: why is it that this
   notation is even applicable? as in, why do derivatives behave this way?
   [1*x3xlltsnrag4fzjvipsf_g.png]
   the classic series of springs example is actually a pretty direct way
   you can visualize the chain rule in movement

   the underlying concept is actually not too much more difficult to grasp
   than this notation. imagine we had a series of ropes that are tied
   together at some fixed points, similar to how springs in series would
   be attached. let   s say you   re holding one end of this spring system and
   your goal is to shake the opposite end at some rate 10 ft/s. you could
   just shake your end at that speed and have it propagate to the other
   end. or you could hook up some intermediary system that shakes the
   middle connection at some lower rate, i.e. at 5 ft/s. in that case,
   you   d only need to move your end at 2 ft/s, since whatever movement
   you   re making will be carried on from where you making the movement to
   the endpoint. this is because the physical connections force the
   movement on one end to be carried through to the end. note: of course,
   as with any analogy, there are points of discrepancy here, but this was
   mostly for the purposes of visualization.

   in a very similar way, if we have two systems where the output of one
   feeds into the input of the other, jiggling the parameters of the
      feeding network,    will shake its output, which will propagate and be
   multiplied by any further changes through to the end of the pipeline.

   ac model overview

   therefore, we have to develop an actorcritic class that has some
   overlap with the id25 we previously implemented, but is more complex in
   its training. because we   ll need some more advanced features, we   ll
   have to make use of the underlying library keras rests upon:
   tensorflow. note: you can definitely implement this in theano as well,
   but i haven   t worked with it in the past and so have not included its
   code. feel free to submit expansions of this code to theano if you
   choose to do so to me!

   the model implementation will consist of four main parts, which
   directly parallel how we implemented the id25 agent:
     * model parameters/setup
     * training code
     * prediction code

   ac parameters

   first off, just the imports we   ll be needing:
import gym
import numpy as np
from keras.models import sequential, model
from keras.layers import dense, dropout, input
from keras.layers.merge import add, multiply
from keras.optimizers import adam
import keras.backend as k
import tensorflow as tf
import random
from collections import deque

   the parameters are very similar to those in the id25. after all, this
   actor-critic model has to do the same exact tasks as the id25 except in
   two separate modules. we also continue to use the    target network hack   
   that we discussed in the id25 post to ensure the network successfully
   converges. the only new parameter is referred to as    tau    and relates
   to a slight change in how the target network learning takes place in
   this case:
class actorcritic:
    def __init__(self, env, sess):
        self.env  = env
        self.sess = sess
        self.learning_rate = 0.001
        self.epsilon = 1.0
        self.epsilon_decay = .995
        self.gamma = .95
        self.tau   = .125
        self.memory = deque(maxlen=2000)

   the exact use of the tau parameter is explained more in the training
   section that follows but essentially plays the role of shifting from
   the prediction models to the target models gradually. now, we reach the
   main points of interest: defining the models. as described, we have two
   separate models, each associated with its own target network.

   we start with defining actor model. the purpose of the actor model is,
   given the current state of the environment, determine the best action
   to take. once again, this task has numeric data that we are given,
   meaning there is no room or need to involve any more complex layers in
   the network than simply the dense/fully-connected layers we   ve been
   using thus far. and so, the actor model is quite simply a series of
   fully connected layers that maps from the environment observation to a
   point in the environment space:
def create_actor_model(self):
        state_input = input(shape=self.env.observation_space.shape)
        h1 = dense(24, activation='relu')(state_input)
        h2 = dense(48, activation='relu')(h1)
        h3 = dense(24, activation='relu')(h2)
        output = dense(self.env.action_space.shape[0],
            activation='relu')(h3)

        model = model(input=state_input, output=output)
        adam  = adam(lr=0.001)
        model.compile(loss="mse", optimizer=adam)
        return state_input, model

   the main difference is that we return the a reference to the input
   layer. the reason for this will be more clear by the end of this
   section, but briefly, it is for how we handle the training differently
   for the actor model.

   the tricky part for the actor model comes in determining how to train
   it, and this is where the chain rule comes into play. but before we
   discuss that, let   s think about why it is any different than the
   standard critic/id25 network training. after all, aren   t we simply going
   to fit as in the id25 case, where we fit the model according to the
   current state and what the best action would be based on current and
   discounted future rewards? the problem lies in the question: if we   re
   able to do what we asked, then this would be a solved issue. the issue
   arises in how we determine what the    best action    to take would be,
   since the q scores are now calculated separately in the critic network.

   so, to overcome this, we choose an alternate approach. rather than
   finding the    best option    and fitting on that, we essentially do hill
   climbing (gradient ascent). for those not familiar with the concept,
   hill climbing is a simple concept: from your local pov, determine the
   steepest direction of incline and move incrementally in that direction.
   in other words, hill climbing is attempting to reach a global max by
   simply doing the naive thing and following the directions of the local
   maxima. there are scenarios you could imagine where this would be
   hopelessly wrong, but more often than not, it works well in practical
   situations.

   as a result, we want to use this approach to updating our actor model:
   we want to determine what change in parameters (in the actor model)
   would result in the largest increase in the q value (predicted by the
   critic model). since the output of the actor model is the action and
   the critic evaluates based on an environment state+action pair, we can
   see how the chain rule will play a role. we   ll want to see how changing
   the parameters of the actor will change the eventual q, using the
   output of the actor network as our    middle link    (code below is all in
   the    __init__(self)    method):
        self.actor_state_input, self.actor_model = \
            self.create_actor_model()
        _, self.target_actor_model = self.create_actor_model()
        self.actor_critic_grad = tf.placeholder(tf.float32,
            [none, self.env.action_space.shape[0]])

        actor_model_weights = self.actor_model.trainable_weights
        self.actor_grads = tf.gradients(self.actor_model.output,
            actor_model_weights, -self.actor_critic_grad)
        grads = zip(self.actor_grads, actor_model_weights)
        self.optimize =  tf.train.adamoptimizer(
            self.learning_rate).apply_gradients(grads)

   we see that here we hold onto the gradient between the model weights
   and the output (action). we   ve also scaled it by the negation of
   self.actor_critic_grad (since we want to do gradient ascent in this
   case), which is held by a placeholder. for those unfamiliar with
   tensorflow or learning for the first time, a placeholder plays the role
   of where you    input data    when you run the tensorflow session. i won   t
   go into details about how it works, but the tensorflow.org tutorial
   goes through the material quite beautifully.

   moving on to the critic network, we are essentially faced with the
   opposite issue. that is, the network definition is slightly more
   complicated, but its training is relatively straightforward. the critic
   network is intended to take both the environment state and action as
   inputs and calculate a corresponding valuation. we do this by a series
   of fully-connected layers, with a layer in the middle that merges the
   two before combining into the final q-value prediction:
def create_critic_model(self):
        state_input = input(shape=self.env.observation_space.shape)
        state_h1 = dense(24, activation='relu')(state_input)
        state_h2 = dense(48)(state_h1)

        action_input = input(shape=self.env.action_space.shape)
        action_h1    = dense(48)(action_input)

        merged    = add()([state_h2, action_h1])
        merged_h1 = dense(24, activation='relu')(merged)
        output = dense(1, activation='relu')(merged_h1)
        model  = model(input=[state_input,action_input],
            output=output)

        adam  = adam(lr=0.001)
        model.compile(loss="mse", optimizer=adam)
        return state_input, action_input, model

   the main points of note are the asymmetry in how we handle the inputs
   and what we   re returning. for the first point, we have one extra fc
   (fully-connected) layer on the environment state input as compared to
   the action. i did so because that is the recommended architecture for
   these ac networks, but it probably works equally (or marginally less)
   well with the fc layer slapped onto both inputs. as for the latter
   point (what we   re returning), we need to hold onto references of both
   the input state and action, since we need to use them in doing updates
   for the actor network:
        self.critic_state_input, self.critic_action_input, \
            self.critic_model = self.create_critic_model()
        _, _, self.target_critic_model = self.create_critic_model()
        self.critic_grads = tf.gradients(self.critic_model.output,
            self.critic_action_input)

        # initialize for later gradient calculations
        self.sess.run(tf.initialize_all_variables())

   here we set up the missing gradient to be calculated: the output q with
   respect to the action weights. this is directly called in the training
   code, as we will now look into.

   ac training

   the last main part of this code that is different from the id25 is the
   actual training. we do, however, make use of the same basic structure
   of pulling episodes from memory and learning from those. since we have
   two training methods, we have separated the code into different
   training functions, cleanly calling them as:
def train(self):
        batch_size = 32
        if len(self.memory) < batch_size:
            return
        rewards = []
        samples = random.sample(self.memory, batch_size)
        self._train_critic(samples)
        self._train_actor(samples)

   now we define the two train methods. the, however, is very similar to
   that from the id25: we are simply finding the discounted future reward
   and training on that. the only difference is that we   re training on the
   state/action pair and are using the target_critic_model to predict the
   future reward rather than the actor:
    def _train_critic(self, samples):
        for sample in samples:
            cur_state, action, reward, new_state, done = sample
            if not done:
                target_action =
                    self.target_actor_model.predict(new_state)
                future_reward = self.target_critic_model.predict(
                    [new_state, target_action])[0][0]
                reward += self.gamma * future_reward
            self.critic_model.fit([cur_state, action],
                reward, verbose=0)

   as for the actor, we luckily did all the hard work before! we already
   set up how the gradients will work in the network and now simply have
   to call it with the actions and states we encounter:
def _train_actor(self, samples):
        for sample in samples:
            cur_state, action, reward, new_state, _ = sample
            predicted_action = self.actor_model.predict(cur_state)
            grads = self.sess.run(self.critic_grads, feed_dict={
                self.critic_state_input:  cur_state,
                self.critic_action_input: predicted_action
            })[0]
            self.sess.run(self.optimize, feed_dict={
                self.actor_state_input: cur_state,
                self.actor_critic_grad: grads
            })

   as mentioned, we made use of the target model. and so, we have to
   update its weights at every time step. however, we only do so slowly.
   more concretely, we retain the value of the target model by a fraction
   self.tau and update it to be the corresponding model weight the
   remainder (1-self.tau) fraction. we do this for both the actor/critic,
   but only the actor is given below (you can see the critic in the full
   code at the bottom of the post):
def _update_actor_target(self):
        actor_model_weights  = self.actor_model.get_weights()
        actor_target_weights =self.target_critic_model.get_weights()

        for i in range(len(actor_target_weights)):
            actor_target_weights[i] = actor_model_weights[i]
        self.target_critic_model.set_weights(actor_target_weights

   ac prediction

   this is identical to how we did it in the id25, and so there   s not much
   to discuss on its implementation:
def act(self, cur_state):
  self.epsilon *= self.epsilon_decay
  if np.random.random() < self.epsilon:
   return self.env.action_space.sample()
  return self.actor_model.predict(cur_state)

   prediction code

   the prediction code is also very much the same as it was in previous
   id23 algorithms. that is, we just have to iterate
   through the trial and call predict, remember, and train on the agent:
def main():
    sess = tf.session()
    k.set_session(sess)
    env = gym.make("pendulum-v0")
    actor_critic = actorcritic(env, sess)

    num_trials = 10000
    trial_len  = 500

    cur_state = env.reset()
    action = env.action_space.sample()
    while true:
        env.render()
        cur_state = cur_state.reshape((1,
            env.observation_space.shape[0]))
        action = actor_critic.act(cur_state)
        action = action.reshape((1, env.action_space.shape[0]))

        new_state, reward, done, _ = env.step(action)
        new_state = new_state.reshape((1,
            env.observation_space.shape[0]))

        actor_critic.remember(cur_state, action, reward,
            new_state, done)
        actor_critic.train()

        cur_state = new_state

   complete code

   with that, here is the complete code used for training against the
      pendulum-v0    environment using ac (actor-critic)!

   iframe: [18]/media/5d2fdb7b2612c0f58492d62f0a3df5b4?postid=f084612cfd69

   boy, that was long: thanks for reading all the way through (or at least
   skimming)! keep an eye out for the next keras+openai tutorial!

comment and click that        below to show support!

     * [19]machine learning
     * [20]id23
     * [21]openai
     * [22]artificial intelligence
     * [23]python

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 25 (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of yash patel

[25]yash patel

   developer interested in id161, graphics, and vr working at
   oculus vr (facebook). graduate from princeton university
   ([26]http://www.ypatel.io)

     (button) follow
   [27]towards data science

[28]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [29]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [30]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f084612cfd69
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/reinforcement-learning-w-keras-openai-actor-critic-models-f084612cfd69&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_fnhu3ugakojy---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@yashpatel_86510?source=post_header_lockup
  17. https://towardsdatascience.com/@yashpatel_86510
  18. https://towardsdatascience.com/media/5d2fdb7b2612c0f58492d62f0a3df5b4?postid=f084612cfd69
  19. https://towardsdatascience.com/tagged/machine-learning?source=post
  20. https://towardsdatascience.com/tagged/reinforcement-learning?source=post
  21. https://towardsdatascience.com/tagged/openai?source=post
  22. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  23. https://towardsdatascience.com/tagged/python?source=post
  24. https://towardsdatascience.com/@yashpatel_86510?source=footer_card
  25. https://towardsdatascience.com/@yashpatel_86510
  26. http://www.ypatel.io/
  27. https://towardsdatascience.com/?source=footer_card
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/f084612cfd69/share/twitter
  33. https://medium.com/p/f084612cfd69/share/facebook
  34. https://medium.com/p/f084612cfd69/share/twitter
  35. https://medium.com/p/f084612cfd69/share/facebook
