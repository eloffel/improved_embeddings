   [1]

linkedin

     * [2]sign in
     * [3]join now

   how tensorflow calculates gradients

                      how tensorflow calculates gradients

   published on august 17, 2017august 17, 2017     29 likes     6 comments

   [4]shamane siriwardhana

[5]shamane siriwardhana[6]follow

masters student - university of auckland | deep id23

     * (button) like29
     * (button) comment6
     * [ ] share
          + (button) linkedin
          + (button) facebook
          + (button) twitter
       8

  let's reveal the magic behind tf backward pass ! this is very important to
  know if you are to build complex deep learning modules with tf .

   in deep learning models there are two phases
    1. forward pass
    2. backward pass

   in the forward pass you will do bunch of operations and obtain some
   predictions or a scores .

   for example in a convolutional neural net we should write functions for
   following in a stacking manner
     * input tensor
     * convolution operation (sliding a k*k filter)
     * applying non-linearity to neurons (relu) in the feature maps
     * max pooling
     * softmax for get logits
     * cross id178 error

   then what ? we need to come backward

   [:0]

   now if you are playing with a id98 made of python numpy as in [7]cs231
   assignments you need to write a backward api too ;) .

   basically you have to implement the chain rule

     making sure you know differentiation :d :d

   it's like for each above mentioned operations you have to implement
   their derivatives

   yes ! yes ! i know it's sucks !

   but in tf we only consider about writing forward pass api . how cool ..

   [:0]

   what is the magic ?

     there's no magic someone has implemented them for you ;)

   let's explore , how they do it !

   when thinking of execution of a tf programme we all are familiar with
   following ,
    1. [8]graph creation
    2. [9]session execution

   basically first one is for building the model and second one is for
   feeding the data in and getting results . you can read more on them .

   always remember tf does each and everything on c++ engine.

  even a little multiplication is not executed on python.

     python is just a wrapper

   the most important thing in tensorflow graph is the backward pass magic
   which we call

auto differentiation

   there are two types of auto differentiation methods
    1. reverse mode - derivation of single output w.r.t all inputs .
    2. forward mode     derivation of all outputs w.r.t one input .

   the basic unit of above two methods is the computational graphs

   this is a simple computation graph for each operation .

   [:0]

   i found a perfect introduction to computational graphs in colah's blog
   .

     [10]calculus on computational graphs: id26

   so basically for the each operation in the forward pass we mention
   tensorflow create it's graph connecting operations top to bottom .

   here's a simple tf computational graph visualized with [11]tensor-board

   [:0]

   the above graph is corresponding to simple equation ,

   output =dropout(sigmoid(wx+b))

   now this is simple! even though we don't care about the backward pass
   tf automatically create derivatives for all the operations top to
   bottom (id168 to weights )

     when we start a session , tf automatically calculates gradients for
     all the deferential operations in the graph and use them in chain
     rule.

   basically tf c++ engine consist of following two things .
    1. efficient implementations for operations like convolution , max
       pool ,sigmoid etc .
    2. derivatives of forward mode operation .

   finally here's a nice graph i found in [12]cs224 - deep learning for
   nlp .

   [:0]

   forward pass is consist of this .

   so the forward pass consists of ,
    1. variables , place holders (weights -w, input-x , bias -b)
    2. operations (nonlinear operation -relu , softmax , cross id178
       loss etc)

  in following graph you can clearly see how tf automatically creates backward
  pass

   [:0]

     * left - forward pass graph (this is what we create with tf )
     * right - backward pass graph (tf automatically create this )

   you can see the connection lines between two graphs . it is also
   automatically get generated (reverse mode auto differentiation) .

   they make the training process running by transferring data when doing
   chain rule .

that's that ! flow with tensor flow :)

   [13]shamane siriwardhana

[14]shamane siriwardhana

masters student - university of auckland | deep id23

   [15]follow

   6 comments
   article-comment__guest-image
   [16]sign in to leave your comment
   show more comments.
     __________________________________________________________________

more from shamane siriwardhana

   [17]32 articles
   human-like decision making - generative adversarial imitaion learning

[18]human-like decision making - generative   

   november 13, 2018

   ai to forcast serious things - beyond supervised learning

[19]ai to forcast serious things - beyond   

   may 5, 2018

   why inverse id23 is gold!

[20]why inverse id23 is gold!

   april 12, 2018

     *    2019
     * [21]about
     * [22]user agreement
     * [23]privacy policy
     * [24]cookie policy
     * [25]copyright policy
     * [26]brand policy
     * [27]manage subscription
     * [28]community guidelines
     * [ ]
          + (button) bahasa indonesia
          + (button) bahasa malaysia
          + (button)   e  tina
          + (button) dansk
          + (button) deutsch
          + (button) english
          + (button) espa  ol
          + (button)             
          + (button) fran  ais
          + (button)          
          + (button) italiano
          + (button)             
          + (button) nederlands
          + (button)          
          + (button) norsk
          + (button) polski
          + (button) portugu  s
          + (button) rom  n  
          + (button)               
          + (button) svenska
          + (button) tagalog
          + (button)                      
          + (button) t  rk  e
          + (button)               
       languagelanguage

references

   1. https://www.linkedin.com/?trk=header_logo
   2. https://www.linkedin.com/uas/login?trk=header_signin
   3. https://www.linkedin.com/start/join?trk=header_join
   4. https://nz.linkedin.com/in/shamane-siriwardhana?trk=author_mini-profile_image
   5. https://nz.linkedin.com/in/shamane-siriwardhana?trk=author_mini-profile_title
   6. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/how-tensorflow-calculate-gradients-shamane-siriwardhana&trk=author-info__follow-button
   7. http://cs231n.github.io/assignments2017/assignment1/
   8. https://www.tensorflow.org/api_docs/python/tf/graph
   9. https://www.tensorflow.org/api_docs/python/tf/session
  10. http://colah.github.io/posts/2015-08-backprop/
  11. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  12. http://web.stanford.edu/class/cs224n/
  13. https://nz.linkedin.com/in/shamane-siriwardhana?trk=author_mini-profile_image
  14. https://nz.linkedin.com/in/shamane-siriwardhana?trk=author_mini-profile_title
  15. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/how-tensorflow-calculate-gradients-shamane-siriwardhana&trk=author-info__follow-button-bottom
  16. https://www.linkedin.com/uas/login?session_redirect=https://www.linkedin.com/pulse/how-tensorflow-calculate-gradients-shamane-siriwardhana&trk=article-reader_leave-comment
  17. https://www.linkedin.com/today/author/shamane-siriwardhana
  18. https://www.linkedin.com/pulse/human-like-decision-making-shamane-siriwardhana?trk=related_artice_human-like decision making - generative adversarial imitaion learning   _article-card_title
  19. https://www.linkedin.com/pulse/ai-forcast-serious-things-beyond-supervised-shamane-siriwardhana?trk=related_artice_ai to forcast serious things - beyond supervised learning _article-card_title
  20. https://www.linkedin.com/pulse/why-inverse-reinforcement-learning-gold-shamane-siriwardhana?trk=related_artice_why inverse id23 is gold!_article-card_title
  21. https://press.linkedin.com/about-linkedin?trk=article_reader_footer_footer-about
  22. https://www.linkedin.com/legal/user-agreement?trk=article_reader_footer_footer-user-agreement
  23. https://www.linkedin.com/legal/privacy-policy?trk=article_reader_footer_footer-privacy-policy
  24. https://www.linkedin.com/legal/cookie-policy?trk=article_reader_footer_footer-cookie-policy
  25. https://www.linkedin.com/legal/copyright-policy?trk=article_reader_footer_footer-copyright-policy
  26. https://brand.linkedin.com/policies?trk=article_reader_footer_footer-brand-policy
  27. https://www.linkedin.com/psettings/guest-controls?trk=article_reader_footer_footer-manage-sub
  28. https://www.linkedin.com/help/linkedin/answer/34593?lang=en&trk=article_reader_footer_footer-community-guide
