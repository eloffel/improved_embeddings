interpretable machine learning:  

the fuss, the concrete and the questions 

credit:https://s-media-cache-ak0.pinimg.com 

been kim 

google brain 

       with finale doshi-velez, harvard university                          

tutorial, icml 2017 

1

contents of this tutorial is largely based on our paper  

towards a rigorous science of interpretable machine learning 

https://arxiv.org/abs/1702.08608

2

https://xkcd.com/

3

https://www.youtube.com/watch?v=icqdxnab3do

https://xkcd.com/

4

potentially serious 
consequences? yes. 

    cost-effective health care (cehc) built models to predict 

id203 of death for patients [cooper et al. 97]  

    hasasthma(x)     lowerrisk for pneumonia (x)

doctors think  
he/she is high risk

aggressive  
treatment
5
5

example borrowed from [caruana et al.    15]

https://xkcd.com/

potentially serious 
consequences? yes. 

    cost-effective health care (cehc) built models to predict 

id203 of death for patients [cooper et al. 97]  

what else did it learn?!

    hasasthma(x)     lowerrisk for pneumonia (x)

doctors think  
he/she is high risk

aggressive  
treatment
6
6

example borrowed from [caruana et al.    15]

ml community is responding

20000

15000

10000

5000

 

i

 

 
f
o
c
p
o
t
 
n
o
 
s
r
e
p
a
p
 
f
o
 
r
e
b
m
u
n

i

g
n
n
r
a
e

l
 

i

l

 

e
n
h
c
a
m
e
b
a
t
e
r
p
r
e
t
n

i

0
98-00

01-03

04-06

07-09

10-12

12-15

15-17

year

7

why now?

widespread data collect + vast computation resources 

       ml everywhere!

8

i heard you can just use 

id90    

can we go home now?

http://www.ogroup.com.au/raise-your-hand-when-you-should-and-why-you-should/

9

experiment.

    i will show you a decision tree. follow the 

right path given an input, and you do: 

input = [owl, icml]

animal 
= owl

yes

no

left 

right  

stomp

clap! 

conference 

= icml

conference 

= nips

    as soon as you know the answer, do the 

action!

left

right 

stomp

clap!

10

experiment.

    i will show you a decision tree. follow the 

right path given an input, and you do: 

input = [kangaroo, icml]

animal 
= owl

yes

no

left 

right  

stomp

clap! 

conference 

= icml

conference 

= nips

    as soon as you know the answer, do the 

action!

left

right 

stomp

clap!

11

sample decision tree #1

input: [ icml, 2017, australia, kangaroo, sunny ]  

year < 2015

yes

no

conference 

= icml

continent = 
australia

left

right 

stomp

clap!

12

sample decision tree #2

input: [ icml, 2017, australia, kangaroo, sunny ]  

continent = 
antarctica

no

year < 2014

yes

animal 
= bird

year > 2015

conference

= icml

animal 
= reptile

conference 

= icml

clap!

animal = 
penguin

year 
> 1990

conferece 
= icml

left

stomp

left

right 

stomp

clap!

left

stomp

right 

clap!

sample decision tree #3

input: [ icml, 2017, australia, kangaroo, sunny ]  

yes

weather = 

cloudy

animal = mammal

conference= 

nips

conference 

= uai

right 

weather 
= rainy

no

year > 2014

weather 
= sunny

year > 2015

left

contient = antarctica

animal = 
monkey

year > 1990

clap! stomp

animal 
= reptile

contient 
= asia

year < 2020

year < 2016

contient 
= australia

left

left

right 

clap!

stomp

year < 2016

weather 
= rainy

left

contient 
= europe

animal 
= reptile

year > 1990

stomp

clap!

animal = 
mammal

left

right 

clap!

stomp

right 

clap!

left

clap!

stomp

sample decision tree #3

input: [ icml, 2017, australia, kangaroo, sunny ]  

yes

weather = 

cloudy

animal = mammal

and can you explain 
conference= 

no

year > 2014

year > 2015

what the overall logic of the system was? 
weather 
= sunny

conference 

right 

nips

= uai

weather 
= rainy

animal 
= reptile

year > 1990

left

can you guess which feature (e.g., animal, 

contient = antarctica
year ,conference) was more    important    (i.e., used in 

animal = 
monkey

year > 1990

year < 2020

more number of examples) given a decision tree 

year < 2016

animal 
= reptile

contient 
= asia

clap! stomp

contient 
= australia

given lots of data?

left

stomp

clap!

contient 
= europe

animal = 
mammal

left

right 

clap!

stomp

year < 2016

weather 
= rainy

left

left

right 

clap!

stomp

right 

clap!

left

clap!

stomp

do we need a different model?   

how about rule lists?  

 

if ( sunny and hot )  
else if ( sunny and cold )  
else  
else if ( at icml )   
 
else if ( cloudy and hot ) 
else if ( snowing )  
 
else if ( new dr. who )  
else if ( paper deadline ) 
else if ( sick and bored ) 
 
else if ( tired )  
else if ( advisor might come ) 
else if ( code running )  
 
else  

 
 
 
  
 
 
 

 
 

 

 

 

 

go ski  
go work 
               
attend tutorial 

  

 
 
      

 

       go swim 
 
 
 

then  
then  
then  
then  
 
then               go swim 
  
then  
then  
then  
then  
then  
then  
then  
then               go work

go ski 
watch tv 
go work 
watch tv 
watch tv 
go work 
watch tv 

 
 
 
 
 
 
 

  
  
 

 

 

 

 

 
 
 
  
  
 

                         

16

do we need a different model?   

how about rule lists?  

if ( sunny and hot )  
 
else if ( sunny and cold )  
else if ( wet and weekday )   
else if ( at icml )   
 
 
 
else if ( cloudy and hot ) 
 
else if ( snowing )  
 
else if ( new dr. who )  
  
 
else if ( paper deadline ) 
else if ( sick and bored ) 
 
 
 
else if ( tired )  
else if ( advisor might come ) 
else if ( code running )  
 
else  

 
 

 

 

 

 

go ski  
go work 
               
attend tutorial 

  
  
 
 
      

 

       go swim 
 
 
 

then  
then  
then  
then  
 
then               go swim 
  
then  
then  
then  
then  
then  
then  
then  
then               go work

go ski 
watch tv 
go work 
watch tv 
watch tv 
go work 
watch tv 

 
 
 
 
 
 
 

  
  
 

 

 

 

 

 
 
 
  
  
 

                         

17

maybe rule sets are better?

 if ( sunny and hot ) or ( cloudy and hot ) 
 then go to beach 
 else work  

18

maybe rule sets are better?

 if ( sunny and hot ) or ( cloudy and hot ) or 
( sunny and thirsty and bored ) or ( bored and 
tired ) or (thirty and tired ) or ( code running ) or 
( friends away and bored ) or ( sunny and want to 
swim ) or ( sunny and friends visiting ) or ( need 
exercise ) or ( want to build castles ) or ( sunny 
and bored ) or ( done with deadline and hot ) or ( 
need vitamin d and sunny ) or ( just feel like it ) 
 then go to beach 
 else work  

19

wait    why am i here then?

https://ameblo.jp/kamar-saya-meg/entry-12247929580.html

20

model knows that we will never understand?

is interpretability possible at all?

interpretability is not about understanding all bits and bytes of the 

model for all data points (we cannot). 

key point: 

it   s about knowing enough for your downstream tasks.

21

https://www.wired.com/story/our-machines-now-have-knowledge-well-never-understand/

note
are you saying decision 
trees, rule lists and rule 

sets don   t work?! 

decision tree, rule lists or rule 
sets may work for your case!  

the point here is that there is 
no one-fits-all method.

http://blog.xfree.hu/myblog.tvn?sid=&from=20&pid=&pev=2016&pho=02&pnap=&kat=1083&searchkey=&hol=&n=sarkadykati

22

what is interpretability?
    not as simple as decision rules 

    not as simple as rule lists or rule sets. 
    not about understanding every bits and bytes of the 

model. 

our goal:  

bring us toward more precise notion of what interpretability 

entails, when it is needed, and how to evaluate it.    

just the start of a discussion!   

23

interpretability

dictionary definition: 

interpretation is the process of giving 

explanations 

24

interpretability

dictionary definition: 

interpretation is the process of giving 

explanations 

to humans

25

interpretability

why and when?

dictionary definition: 

how can we do 

this?

interpretation is the process of giving 

explanations 

how can we 

measure    good    
explanations?

to humans

26

agenda

1. why and when?

dictionary definition: 

2. how can we do 

this?

interpretation is the process of giving 

explanations 

3. how can we 
measure    good    
explanations?

to humans

27

agenda

1. why and when?

dictionary definition: 

2. how can we do 

this?

interpretation is the process of giving 

explanations 

3. how can we 
measure    good    
explanations?

to humans

28

why interpretability?

fundamental underspeci   cation in the problem

29

why interpretability?

fundamental underspeci   cation in the problem

more data or more clever 

algorithm won   t help.

30

underspecification example 1: 

safety

https://www.ll.mit.edu/publications/labnotes/automation.html

31

interpretability is not safety. 
but it can help to expose 

safety issues.

underspecification example 1: 

safety

    cost-effective health care (cehc) built models to predict 

id203 of death for patients [cooper et al. 97]  

    hasasthma(x)     lowerrisk for pneumonia (x)

doctors think  
he/she is high risk

aggressive  
treatment

    a patient with asthmatic -> high risk -> immediate admission -> 

example borrowed from [caruana et al.    15]

32

underspecification example 2: 

debugging

    we want to understand why the system doesn   t work, and 

fix it.

https://yorktown.cbe.wwu.edu/sandvig/mis314/lectures/images/rubber-duck-debugging.jpg

33

patient. which side-effects is a patient willing to risk?

underspecification example 3:  

mismatched objectives and  
multi-objective trade-offs

    what you optimize is not what you meant to optimize.

all of these may address depression. 
which side-effects are you willing to 

risk?

 http://img.medscapestatic.com/pi/features/drugdirectory/octupdate/myn62330.jpg, , http://img.medscapestatic.com/pi/features/drugdirectory/octupdate/plv04411.jpg , https://www.google.com/url?
sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=0ahukewjkgaazkzbvahxdpt4khsz3d-mqjrwibw&url=http%3a%2f%2fwww.webmd.com%2fdrugs%2f2%2fdrug-4870-5047%2fvenlafaxine-
oral%2fvenlafaxine-oral%2fdetails&psig=afqjcnhmqn9d8bhqzufyxfhd9aoy5yxq5g&ust=1500580783703785

34
34

underspecification example 4: 

science

get me something new.  

something    new.

http://cdn.playbuzz.com/cdn/a6006912-25e4-4cb5-867d-36c333b437c2/f2519ae0-e3d9-48e9-8f0d-4e68e2c99e26.jpeg

35

underspecification example 5:  

legal/ethics

    we're legally required to provide an explanation and/or 
we don't want to discriminate against particular groups.

http://leap.utah.edu/_images/program-options/pre-law.jpg

36

examples of underspecification

decisions.  

so we can fix it. 

    safety:  we  want  to  make  sure  the  system  is  making  sound 
    debugging: we want to understand why a system doesn't work, 
    science: we want to understand something new. 
    mismatched  objectives  and  multi-objectives  trade-offs:  the 
    legal/ethics:  we're  legally  required  to  provide  an  explanation 
and/or we don't want to discriminate against particular groups. 
+ your case?

system may not be optimizing the true objective. 

fundamental underspeci   cation in the problem

37

what is not 

underspecification?

https://www.pinterest.com/dowd3128/type-o-negative/

38

underspecification is not 

uncertainty

https://www.pinterest.com/pin/461126449319612657/

[k., kaess, fletcher, leonard, bachrach, roy and teller    10]

39

our cousins are not us

interpretability

privacy 

accountability 

trust 

causality etc.

    interpretability can help with them when we cannot 

formalize these ideas 

    but once formalized, you may not need interpretability.

40

when we may not want 

interpretability

    no significant consequences or 

when predictions are all you need. 

    sufficiently well-studied problem 

    prevent gaming the system - 

mismatched objectives.

https://cdn.theatlantic.com/assets/media/img/mt/2015/04/shutterstock_11926084/lead_large.jpg
https://www.jal.com/assets/img/flight/safety/equipment/pic_tcas_001_en.jpg

http://www.cinemablend.com/pop/netflix-using-amazon-cloud-explore-artificial-intelligence-movie-recommendations-62248.html 

41

agenda

1. why and when?

dictionary definition: 

2. how can we do 

this?

interpretation is the process of giving 

explanations 

3. how can we 
measure    good    
explanations?

to humans

42

types of interpretable methods

my ml

43

types of interpretable methods

before building 

any model

building  
a new model

after  

building a model

44

not mutually exclusive categories 

nor 

exhaustive list of literatures

45

types of interpretable methods

before building 

any model

    visualization 
    exploratory data analysis

46

before building any model: 

visualization for data exploration

need more 
participations 

from  hci 
communities!

[vi  gas and wattenberg    07] 
[maaten et al.    08] 
[amershi et al.    09] 
[patel et al.    10] 
[varshney et al.    12]

https://pair-code.github.io/facets/quickdraw.html47

types of interpretable methods

before building 

any model

    visualization 
    exploratory data analysis

[tukey 77]

48

before building any ml models. 

before building any model: 
exploratory data analysis

observed 

data

49

before building any ml models. 

before building any model: 
exploratory data analysis

kmeans, knn 

observed 

data

[simon et al.,    07] 
[lin and bilmes,    11] 

50

before building any ml models. 

before building any model: 
exploratory data analysis

criticism 1

criticism 2

observed 

data

prototype 1

prototype 2

mmd-critic [k. khanna, koyejo    16]

51

before building any ml models. 

before building any model: 
exploratory data analysis

criticism 1

criticism 2

observed 

data

fit distribution p 
(prototypes) that best 
fit the data points

prototype 1

prototype 2

mmd-critic [k. khanna, koyejo    16]

52

before building any ml models. 

before building any model: 
exploratory data analysis

fit distribution p 
(prototypes) that best 
fit the data points

prototype 1

criticism 1

prototype 2

criticism 2

fit distribution q 
(criticisms) the 
difference between 
data points and p

observed 

data

mmd-critic [k. khanna, koyejo    16]

53

before building any ml models. 

before building any model: 
exploratory data analysis

fit distribution p 
(prototypes) that best 
fit the data points

prototype 1

criticism 1

criticism 2

fit distribution q 
(criticisms) the 
difference between 
data points and p

observed 

data

prototype 2

use mmd to do this 
only using samples 
without ever having to 
write down what p and 

q look like.

mmd-critic [k. khanna, koyejo    16]

54

types of interpretable methods

before building 

any model

    visualization 
    exploratory data analysis

55

types of interpretable methods

building  
a new model

    rule-based, per-feature-

based 

    case-based 
    sparsity 

    monotonicity

56

types of interpretable methods

building  
a new model

    rule-based, per-feature-
based 

    case-based 
    sparsity 

    monotonicity

57

building a new model: rule-based

yeah > 
2015

yes

no

conference 

= icml

continent 
= antarctica

 if ( sunny and hot ) or ( cloudy and hot ) 
 then go to beach 
 else work  

left

right 

stomp

clap!

id90, rule lists, rule sets
[breiman, friedman, stone, olshen 84] 
[rivest 87] 
[muggleton and de raedt 94] 
[wang and rudin 15] 
[letham, rudin, mccormick, madigan    15] 
[hauser, toubia, evgeniou, befurt, dzyabura 10] 
[wang, rudin, doshi-velez, liu, klampfl, macneille 17]

building a new model:  

per-feature based

y

one feature

linear model 

generalized linear model 

generalized additive model 

generalized additive2 model

table from [gehrke et al.    12]

59

building a new model:  

per-feature based

y

one feature

linear model 

generalized linear model 

generalized additive model 

generalized additive2 model

table from [gehrke et al.    12]

[gehrke et al.    12]

60

which one is not the limitations of  

rule-based methods?

it may not be as interpretable as you may think 
it only works if the original features are interpretable 

a.
b.
c. the data might not cluster 
d. none of the above

61

which one is not the limitations of  

rule-based methods?

it may not be as interpretable as you may think 
it only works if the original features are interpretable 

a.
b.
c. the data might not cluster 
d. none of the above

too big 

    depth/length of the tree might be 
    complexity of rules might be high 
    may need lots of splits to    t 

complex function

62

types of interpretable methods

building  
a new model

    rule-based, per-feature-

based 
    case-based 
    sparsity 

    monotonicity

63

building a new model: case-based

   i recommend treatment x because it worked for 

other patients like you...   

[frey, dueck '10]   
[yen, malioutov , kumar '16]  
[arnold , el-saden , bui , taira '10]  
[floyd , aha '16]  
[homem, perico , santos , bianchi , de mantaras '16] 
[jalali , leake    15]  
[reid , tibshirani '16]

64

building a new model: case-based

    explain id91 results using examples (bayesian case model) 

cluster labels
    joint id136 on prototypes, subspaces and cluster labels

prototypes subspaces

cluster a

cluster b

cluster c

   

[k. rudin, shah    14]

65

building a new model: case-based

cluster a

cluster b

cluster c

prototypes

taco

basic crepe

chocolate berry tart

subspaces

salsa 
sour cream 
avocado 
salt, pepper, taco 
shell, lettuce, oil

flour  
egg 
water, salt, milk, 
butter

chocolate 
strawberry 
pie crust, whipping cream, 
kirsch, almonds

66
[k. rudin, shah    14]

building a new model: case-based
ibcm + overcode system

select/unselect 

subspaces 
(keywords)

promote/demote 

prototypes

[k. glassman, johnson, shah    15]

67

 d

68

[k. glassman, johnson, shah    15]

68

which one is not the limitations 

none of data points 
are representative!

of case-based models?

a. the complexity of explanation is higher than that of 

data points 

b. there may not be a good representative examples 

c. human may overgeneralize 

d. none of the above

69

break-y 
5 mins

http://weknowmemes.com/generator/meme/co   ee-kicking-in/335689/

after the break: 

    interpretability methods when you already 

have a model (e.g., deep learning) 

how to evaluate explanations

   

types of interpretable methods

building  
a new model

    rule-based, per-feature-

based 

    case-based 
    sparsity 
    monotonicity

71

building a new model: sparsity-based

(all other ai   s set to zero)

[ jain , rao , dhillon '16 ]  
[ yang , barber , jain , lafferty '16 ]  
[ greenlaw , szefer , graham , lesperance , nathoo '17 ] 
[ kim , xing '10 ] 
[ meier , van de geer , buhlmann '08] 
[ bach , jenatton , mairal , obozinski '12 ] 

72

building a new model: sparsity-based

all diseases

nervous 
system

epilepsy

...

...

...

...

...

...

intellectual 
disability

...
...

non-convulsive grand mal

focal

petit mal

convulsive

in-tractable

tractable

in-tractable

tractable

in-tractable

tractable

correlations across subtrees: may be a single cause 
manifesting in multiple aspects.  model that!

pr( data ) = mult(                                        ) 

patient- 
subtype 

  n

subtype-
concept 

  k

concept- 
diagnosis 

  c

graph-sparse lda [doshi-velez et al.   15]

which one is not the limitations of 

sparsity methods?

a. the model may not be able to represent what it 

learned in a sparse fashion. 

b. there might be the case that only the 

collections of factors make more sense 

c. none of the above

   sparsity is good, but not enough, but 

just because it is sparse, doesn   t 
mean it   s interpretable.    [freitas    10] 
74

types of interpretable methods

building  
a new model

    rule-based, per-feature-

based 

    case-based 
    sparsity 
    monotonicity

75

building a new model: monotonicity

y

f1

piecewise 
monotonic

one feature

y

f2

f1

two features

76

 [gupta et al.    16]

building a new model: monotonicity

    learn piecewise monotonic function within a user specified 

lattice (intervals) [gupta et al.    16] 

    monotonic neural networks by constraining weights 

[neumann et al.   13, riihimaki and vehtari    10]

77

types of interpretable methods

after  

building a model

    sensitivity analysis, 

gradient-based methods 

    mimic/surrogate models 
    investigation on hidden 

layers

78

types of interpretable methods

after  

building a model

    sensitivity analysis, 
gradient-based methods 

    mimic/surrogate models 
    investigation on hidden 

layers

79

after building a model:  

sensitivity analysis

what would happen to output      

if we perturb the input                            ? 

xkcd.com

        can be group of features, data points, specific inputs 

    for nonlinear functions                 , higher order 

derivatives will get involved   

80

after building a model:  

sensitivity analysis

sensitivity analysis on model 

[ribeiro et al.    16]

influential functions 

 [koh et al.   17]

want local explanation


 of the + data point

to classify this image:

model found these images most helpful

id166

inception

locally    tted 
linear function

[simonyan et al.,    13] 
[li et al.,    16] 
[datta et al.    16] 
[adler et al.,    16]

81

after building a model:  
saliency/attribution maps

    give me the features in the input space that 

mattered for the classification

82

after building a model:  
saliency/attribution maps

grad-cam [selvaraju et al. 16]

smoothgrad [smilkov et al. 17]

integrated gradients [sundararajan et al. 17]

[erhan 2009] [springenberg,    14] [shrikumar    17]

83

after building a model:  
saliency/attribution maps

grad-cam [selvaraju et al. 16]

smoothgrad [smilkov et al. 17]

integrated gradients [sundararajan et al. 17]

oh yeah, gradients makes sense. 
it   s about how much the label 
would changes as i change the 
data   

[erhan 2009] [springenberg,    14] [shrikumar    17]

pop quiz  

idea borrowed from [sundararajan et al. 17]

number of books 

bought

price of a 

book

currency

total 
pro   t

2016

2017

only this feature 

changed

4
1
3

12

5
2
4

40

(5-4)*1* 3 = 3
4*(2-1)*3 = 12
4*1*(4-3) = 4

19

a
p
c

e

increase in e: 28!

85

pop quiz  

idea borrowed from [sundararajan et al. 17]

number of books 

bought

price of a 

book

currency

total 
pro   t

2016

2017

only this feature 

changed

4
1
3

12

5
2
4

40

(5-4)*1* 3 = 3
4*(2-1)*3 = 12
4*1*(4-3) = 4

19

a
p
c

e

increase in e: 28!

86

pop quiz  

idea borrowed from [sundararajan et al. 17]

number of books 

bought

price of a 

book

currency

total 
pro   t

2016

2017

only this feature 

changed

what?!

a
p
c

e

4
1
3

12

5
2
4

40

(5-4)*1* 3 = 3
4*(2-1)*3 = 12
4*1*(4-3) = 4

19

increase in e: 28!

where is my 9?

87

which one is not the limitations of 

sensitivity analysis/gradient-based methods?

it may not be truthful to the model 

a.
b. the model may not allow sensitivity analysis 
c. two local explanations may conflict 
d. the perturbed x may not be from the data 

distribution 

e.

interactions of sensitivity (changing two 

variables) is expensive

88

types of interpretable methods

after  

building a model

    sensitivity analysis, 

gradient-based methods 
    mimic models 
    investigation on hidden 

layers

89

after building a model:  

mimic models

    train a black box on      and     :                     
    train an interpretable model on     and      :   

world

approx

blackbox

approx

mimic  
model

after building a model:  

mimic models

    model compression or distillation [bucila et al.    06, ba et al.    14, 

hinton et al.    15] 

    visual explanations [hendricks et al.    16]

91

which one is not the 

limitations of mimic models?

a. you may not be able to distill - there may not be simpler 

model at all 

b. there might be a gap between what the actual model is 

doing and your mimic model is doing 

c. the simpler model may not be interpretable  

d. none of the above

92

types of interpretable methods

after  

building a model

    sensitivity analysis, 

gradient-based methods 

    mimic/surrogate models 
    investigation on hidden 
layers

93

after building a model:  

investigation on hidden layers

 [dosovitskiy et al.    16]

 [zeiler et al.    13]  

[bau and zhou et al.    17] 

94

which one is not the limitations of 

investigation on hidden layers?

a. they may be lack of actionable insights 
b.

it is unclear if visualizing neuron vs. per layer vs. 
per subspaces is more meaningful than others 

c. a golden dataset with detailed labels with 
human concepts are often not available 

d. none of the above

95

what   s the best interpretability 

method for me?

?

?

?

96

what   s the best interpretability 

3. how can we 
measure    good    
explanations?

?

method for me?

?

?

97

agenda

1. why and when?

dictionary definition: 

2. how can we do 

this?

interpretation is the process of giving 

explanations 

3. how can we 
measure    good    
explanations?

to humans

98

how are we measuring  
explanation quality now?

   you know it when you see it   

99

how are we measuring  
explanation quality now?

   you know it when you see it   

https://www.pinterest.se/pin/365987907189893478/

100

how are we measuring  
explanation quality now?

   you know it when you see it   

these are great papers 

and  

i had de   nitely also 
made these claims in 

my work!

101

how are we measuring  
explanation quality now?

   you know it when you see it   

we want evidence-
based so that we 
can compare work a 

to work b, and to 

generalize.

102

how are we measuring  
explanation quality now?

   you know it when you see it   

give human a task, then 
measure how well they do 

q. which group  

does this  

new data point  

belong to?

a. group 1

b. group 2

103
[k. 16]

how are we measuring  
explanation quality now?

   you know it when you see it   

give human a task, then 
measure how well they do 

we want a 

measurement 

methods that can be 

generalized.

q. which group  

does this  

new data point  

belong to?

a. group 1

b. group 2

104
[k. 16]

spectrum of evaluation

function-based

a variety of synthetic 

and standard 
benchmarks 

e.g, uci datasets, 

id163

application-based

backing up claims 

e.g., performance on a 
cool medical dataset, 
winning go games

105

i

g
n
n
r
a
e
l
 
e
n
h
c
a
m

i

spectrum of evaluation

function-based

how sparse are 
the features? 

does it look  
reasonable?

application-based

how much did  

we improve patient 

outcomes? 

do scientists find 
the explanations 

useful?

q
u
a
n
t
i
t
a
t
i
v
e

q
u
a

l
i
t
a
t
i
v
e

106

i

i

g
n
n
r
a
e
l
 
e
n
h
c
a
m
 
e
b
a
t
e
r
p
r
e
t
n

l

i

evaluate: application-based

    does providing interpretability assist with a down-stream 

task, such as increasing fairness, safety, scientific 
discovery, or productivity?   

interpretable 

ml tool

commonly used in hci, visualization communities

https://www.bu.edu/today/2012/do-scholarly-articles-want-to-be-free/

107

evaluate: application-based

    does providing interpretability assist with a down-stream 

task, such as increasing fairness, safety, scientific 
discovery, or productivity?   

interpretable 

ml tool

it   s real evaluation, but it   s costly and hard to compare work a to b

https://www.bu.edu/today/2012/do-scholarly-articles-want-to-be-free/

108

evaluation: function-based

    can we use some proxy such as sparsity monotonicity or 

non-negativity?

109

evaluation: function-based

    can we use some proxy such as sparsity monotonicity or 

non-negativity?

it's easy to formalize, optimize, and evaluate    

but may not solve a real need 

e.g., 5 unit sparsity will save more patients than 10 unit sparsity?

110

spectrum of evaluation

function-based

cognition-based

application-based

i

i

g
n
n
r
a
e
l
 
e
n
h
c
a
m
 
e
b
a
t
e
r
p
r
e
t
n

l

how sparse are 
the features? 

does it look  
reasonable?

i

low cost

low validity

how much did  

we improve patient 

outcomes? 

do scientists find 
the explanations 

useful?

q
u
a
n
t
i
t
a
t
i
v
e

q
u
a

l
i
t
a
t
i
v
e

high cost

high validity

111

spectrum of evaluation

function-based

cognition-based

application-based

how sparse are 
the features? 

what factor should 
change to change 

the outcome?

how much did  

we improve patient 

outcomes? 

does it look  
reasonable?

what are the 
discriminative 

features? 

do scientists find 
the explanations 

useful?

q
u
a
n
t
i
t
a
t
i
v
e

q
u
a

l
i
t
a
t
i
v
e

evaluations:  cognition-based

    human subject experiments on general forms

animal = 

owl

yes

no

conference 

= icml

conference 

= nips

left

right 

stomp

clap!

113

evaluations:  cognition-based

    human subject experiments on general forms

what

{

input
weight
cost

}

would change

{

predictions for x

 cluster of x

} ?

e.g., 

forward simulation, 

counterfactual reasoning 
identify important features

114

spectrum of evaluation

problem-related factors 

1.
2.
3.

global vs. local 

time budget 
severity of 

underspecification 

method-related factors 

4.
5.

cognitive chunks 
audience training

cognition-based

what factor should 
change to change 

the outcome?

what are the 
discriminative 

features? 

q
u
a
n

t
i
t

a

t
i
v
e

q
u
a

l
i
t

a

t
i
v
e

spectrum of evaluation

problem-related factors 

1.
2.
3.

global vs. local 

time budget 
severity of 

underspecification 

method-related factors 

4.
5.

cognitive chunks 
audience training

cognition-based

what factor should 
change to change 

the outcome?

what are the 
discriminative 

features? 

e.g.,  

humans capacity as 
q
function of factors, 
u
a
n
set of factors that 
carries over well to 
q
u
a

application

t
i
v
e

t
i
t

a

l
i
t

a

t
i
v
e

problem-related factor: global vs. local

cluster a

cluster b

cluster c

taco

salsa 
sour cream 
avocado 
salt, pepper, 
taco shell, 

basic crepe

flour  
egg 
water, salt, 
milk, butter

chocolate berry tart

chocolate 
strawberry 
pie crust, 
whipping cream, 

prototypes

subspaces

117

problem-related factor: time budget

https://www.greek-names.info/names-of-ancient-greek-astronomers/

http://www.idonme.com/application-medical.php

118

problem-related factor: 

 severity of underspecification

 solve f(x) + a bounded term   
 (stop if obstacle within 2m)   

 vs.   

 make a safe autonomous car   

 solve ai   

119

7 plus 
minus 2 
or what?

method-related factor: 

 cognitive chunks

lmci,aiu,psni 

vs. 

icml,uai,nips

120

method-related factor: 

audience training

http://www.ufo-blogger.com

    the expert   s background will affect what cognitive chunks 

and relations they have available

121

spectrum of evaluation

recommendations 

list these factors in your 
work so that others can 
compare your work to 

function-based
theirs. 

find more factors.
how sparse are 
the features? 

does it look  
reasonable?

cognitive-based

what factor should 
change to change 

the outcome?

what are the 
discriminative 

features? 

problem-related factors 

1.
2.
3.

global vs. local 

time budget 
severity of 

underspecification 

application-based

method-related factors 

how much did  

4.
we improve patient 
5.

cognitive chunks 
audience training

outcomes? 

t
i
t

a

q
u
a
n

t
i
v
e

do scientists find 
the explanations 

useful?

q
u
a

l
i
t

a

t
i
v
e

wrap up

1. why and when?

dictionary definition: 

2. how can we do 

this?

interpretation is the process of giving 

explanations 

3. how can we 
measure    good    
explanations?

to humans

123

how shall we move the field 

forward?

https://img   ip.com

   and pair @ brain,  

we are hiring.
ai.google/pair

qna

recommendations 

list these factors in your 
work so that others can 
compare your work to 

function-based
theirs. 

find more factors.
how sparse are 
the features? 

does it look  
reasonable?

cognitive-based

what factor should 
change to change 

the outcome?

what are the 
discriminative 

features? 

problem-related factors 

1.
2.
3.

global vs. local 

time budget 
severity of 

underspecification 

application-based

method-related factors 

how much did  

4.
we improve patient 
5.

cognitive chunks 
audience training

outcomes? 

t
i
t

a

q
u
a
n

t
i
v
e

do scientists find 
the explanations 

useful?

q
u
a

l
i
t

a

t
i
v
e

