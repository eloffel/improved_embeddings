smaller, faster, deeper: university of edinburgh mt

submittion to wmt 2017

rico sennrich, alexandra birch, anna currey,

ulrich germann, barry haddow, kenneth hea   eld,

antonio valerio miceli barone, philip williams

university of edinburgh

july 19 2017

alexandra birch

uedin wmt 2017

1 / 36

main collaborators

rico sennrich

barry haddow

alexandra birch

uedin wmt 2017

1 / 36

uedin wmt 2017

1

introduction to neural mt

2 making models smaller

3 making training faster

4 making models bigger

5 using monolingual data

6 ensembling and reranking

7 results

alexandra birch

uedin wmt 2017

2 / 36

linear models in mt

phrase-based machine translation

log-linear model: p(t) = exp(cid:80)n

i=1   ihi(x)

weighted model

number of feature funtions n
random variables x = (e, f, start, end)
feature functions

h1 = p(e|f ) translation id203
h2 = d(starte     startf ) distortion
h3 = d(plm ) language model

weights   

alexandra birch

uedin wmt 2017

3 / 36

smt framework

alexandra birch

uedin wmt 2017

4 / 36

neural mt

alexandra birch

uedin wmt 2017

5 / 36

modellingtranslationasaneuralnetworknat  urlichhatjohnspa  amspielofcoursejohnhasfunwiththegamep(t|s)=f(s)f(s)isanon-linearfunctionrepresentedbyneuralnetworkbarryhaddow(uedin)id4:successandchallengesamazon7/45neural versus phrase-base mt

phrase-based smt
learn segment-segment correspondances from bitext

training is multistage pipeline of heuristics
fixed weights for features
limited ability to encode history
strong independence assumptions

neural mt
learn mathematical function on vectors from bitext

end-to-end trained model
output conditioned on full source text and target history
non-linear dependence on information sources

alexandra birch

uedin wmt 2017

6 / 36

recurrent neural network

hi = f (xi, hi   1)
yi = g(hi)

alexandra birch

uedin wmt 2017

7 / 36

recurrentnnshi 1hihi+1xi 1xixi+1yi 1yiyi+1hi=f(xi,hi 1)yi=g(hi)barryhaddow(uedin)id4:successandchallengesamazon8/45id56 for languagel modelling

predict wi conditioned on w1 . . . wi   1
allows unlimited history
outperforms traditional count-based id165 models

alexandra birch

uedin wmt 2017

8 / 36

applicationtolanguagemodellinghi 1hihi+1wi 1wiwi+1wiwi+1wi+2predictwiconditionedonw1...wi 1.allowsunlimitedhistoryoutperformstraditionalcount-basedid165modelsbarryhaddow(uedin)id4:successandchallengesamazon9/45encoder-decoder

alexandra birch

uedin wmt 2017

9 / 36

encoder-decoderfortranslations1s2s3s4s5e1e2e3e4e5ofcoursejohnhasfunc1c2c3c4f1f2f3f4nat  urlichhatjohnspa  decoderencoderbarryhaddow(uedin)id4:successandchallengesamazon10/45encoder-decoder with attention

alexandra birch

uedin wmt 2017

10 / 36

encoder-decoderwithattentionc1c2c3c4f1f2f3f4nat  urlichhatjohnspa  +s1s2s3s4s5e1e2e3e4e5ofcoursejohnhasfun0.10.10.70.1decoderencoderbarryhaddow(uedin)id4:successandchallengesamazon11/45limitations of neural mt

limitations

limited memory on gpus
slow training times
not really deep deep learning models
training is not very stable

alexandra birch

uedin wmt 2017

11 / 36

uedin wmt 2017

1

introduction to neural mt

2 making models smaller

3 making training faster

4 making models bigger

5 using monolingual data

6 ensembling and reranking

7 results

alexandra birch

uedin wmt 2017

12 / 36

improvements to subid40

byte-pair encoding [sennrich et al., 2016]

iterative, frequency-based merging of subword units into larger units
"joint bpe" on parallel corpus for more consistent segmentation

problems

subword unit can be part of (frequent) larger unit, but rare on its own

allergikerzimmer
allergiker:

330
10

subword unit can be frequent in one language, but rare in the other

nationalities

541 (en)

1 (de)

consequences

model is unlikely to learn good representation for rare subwords
bpe may even produce subword that is unknown at test time
having rare subwords in vocabulary is wasteful

alexandra birch

uedin wmt 2017

13 / 36

improvements to subid40

solution

require that subword has been observed in source training corpus
optionally require minimum frequency
even in joint bpe, condition is checked for each side individually
split up (reverse merge of) subwords that don   t meet this requirement

vocabulary size (en   de with joint bpe; 90k merge operations)

bpe
no    lter
threshold 50

en

de

83227
52652

91921
73297

only minimal change in sequence length and id7
advantage: smaller models; no unk
disadvantage: additional hyperparameter: frequency threshold

alexandra birch

uedin wmt 2017

14 / 36

parameter tying

embedding and output layer

in nematus, last hidden layer has same size as target-size embedding
    output matrix: vocabulary size    embedding layer size
    target embedding matrix: embedding layer size    vocabulary size
[press and wolf, 2017] propose tying weights of embedding matrix
and transpose of output matrix.
little effect on quality, but smaller models.

alexandra birch

uedin wmt 2017

15 / 36

uedin wmt 2017

1

introduction to neural mt

2 making models smaller

3 making training faster

4 making models bigger

5 using monolingual data

6 ensembling and reranking

7 results

alexandra birch

uedin wmt 2017

16 / 36

optimizers

optimizer

adaptive learning rates tend to speed up training
this year we used adam [kingma and ba, 2015] instead of adadelta
[zeiler, 2012]

learning rate annealing

our wmt systems use adam without annealing
sgd with annealing is popular
[sutskever et al., 2014, wu et al., 2016]
adam with annealing recommended by
[denkowski and neubig, 2017]
    "--anneal_restarts 2 --patience 3" in nematus

alexandra birch

uedin wmt 2017

17 / 36

layer id172

if input distribution to nn layer changes, parameters need to adapt to
this covariate shift.
id172 of layers reduces shift, and improves training stability.
for layer a with h units, re-center and re-scale layer.
id172 changes representation power:
two bias parameters, g and b, restore original representation power

ai

1
h

h(cid:88)
h(cid:88)

i=1

i=1

(ai       )2
(cid:105)

   =

(cid:118)(cid:117)(cid:117)(cid:116) 1
(cid:104) g

h

   =

h =

   (cid:12) (a       ) + b

alexandra birch

uedin wmt 2017

(1)

(2)

(3)

18 / 36

uedin wmt 2017

1

introduction to neural mt

2 making models smaller

3 making training faster

4 making models bigger

5 using monolingual data

6 ensembling and reranking

7 results

alexandra birch

uedin wmt 2017

19 / 36

stacked id56s

. . .

. . .

. . .

. . .

. . .

. . .

. . .

figure: alternating stacked encoder [zhou et al., 2016].

alexandra birch

uedin wmt 2017

20 / 36

deep transition networks

. . .

. . .

. . .

figure: deep transition network.

alexandra birch

uedin wmt 2017

21 / 36

deep architectures: discussion

we use depth of 4 for submission systems
all models were trained on single gpu
in post-submission experiments [miceli barone et al., 2017], bideep
architecture (combination of deep transition and stack) performed
best

alexandra birch

uedin wmt 2017

22 / 36

uedin wmt 2017

1

introduction to neural mt

2 making models smaller

3 making training faster

4 making models bigger

5 using monolingual data

6 ensembling and reranking

7 results

alexandra birch

uedin wmt 2017

23 / 36

monolingual data

back-translations

all systems in the news shared task use target-side news data,
automatically translated into source language

copying

the en   tr systems use monolingual data that is paired with a copy
on the source

biomedical task

pseudo in-domain monolingual data is extracted from commoncrawl
as follows:

automatically translate in-domain source corpus
perform moore-lewis data selection in target-language commoncrawl
corpus

alexandra birch

uedin wmt 2017

24 / 36

uedin wmt 2017

1

introduction to neural mt

2 making models smaller

3 making training faster

4 making models bigger

5 using monolingual data

6 ensembling and reranking

7 results

alexandra birch

uedin wmt 2017

25 / 36

ensembling and reranking: research questions

ensembling

last year, we used checkpoint ensemble (of last 4 checkpoints).
this year, we contrast this with ensemble of independent models.

reranking with right-to-left models

last year, reranking with right-to-left models gave signi   cant
improvements for three translation directions.
this year, we evaluate strategy on stronger baseline and more
systems.

alexandra birch

uedin wmt 2017

26 / 36

news task: into english

system
wmt-16 single system
baseline
+layer id172
+deep model
+checkpoint ensemble
+independent ensemble
+right-to-left reranking
wmt-17 submission

cs   en de   en lv   en ru   en tr   en zh   en
2017
2017
   
25.9
21.7
27.5
22.5
28.2
22.9
28.9
23.6
29.4
30.3
25.1
25.7
31.1
30.9
25.7

2017
   
19.7
18.8
20.6
21.0
21.6
22.3
20.1

2017
31.1
32.0
32.1
33.5
33.8
34.4
35.1
35.1

2017
   
16.4
17.0
16.6
17.7
18.5
19.0
19.0

2017
29.6
31.3
32.3
32.7
33.3
33.6
34.6
30.8

alexandra birch

uedin wmt 2017

27 / 36

news task: into english

system
wmt-16 single system
baseline

cs   en de   en lv   en ru   en tr   en zh   en
2017
2017
   
25.9
27.5
21.7

2017
   
19.7

2017
31.1
32.0

2017
   
16.4

2017
29.6
31.3

we start from stronger baselines (more data, adam, new bpe)

alexandra birch

uedin wmt 2017

27 / 36

news task: into english

system

cs   en de   en lv   en ru   en tr   en zh   en
2017
2017

2017

2017

2017

2017

baseline
+layer id172
+deep model

27.5
28.2
28.9

32.0
32.1
33.5

16.4
17.0
16.6

31.3
32.3
32.7

19.7
18.8
20.6

21.7
22.5
22.9

we start from stronger baselines (more data, adam, new bpe)
layer id172 and deep models generally help

alexandra birch

uedin wmt 2017

27 / 36

news task: into english

system

cs   en de   en lv   en ru   en tr   en zh   en
2017
2017

2017

2017

2017

2017

+deep model
+checkpoint ensemble
+independent ensemble

28.9
29.4
30.3

33.5
33.8
34.4

16.6
17.7
18.5

32.7
33.3
33.6

20.6
21.0
21.6

22.9
23.6
25.1

we start from stronger baselines (more data, adam, new bpe)
layer id172 and deep models generally help
checkpoint ensembles help, but independent ensembles are better

alexandra birch

uedin wmt 2017

27 / 36

news task: into english

system

cs   en de   en lv   en ru   en tr   en zh   en
2017
2017

2017

2017

2017

2017

+independent ensemble
+right-to-left reranking

30.3
31.1

34.4
35.1

18.5
19.0

33.6
34.6

21.6
22.3

25.1
25.7

we start from stronger baselines (more data, adam, new bpe)
layer id172 and deep models generally help
checkpoint ensembles help, but independent ensembles are better
reranking helps

alexandra birch

uedin wmt 2017

27 / 36

news task: into english

system
wmt-16 single system
baseline
+layer id172
+deep model
+checkpoint ensemble
+independent ensemble
+right-to-left reranking
wmt-17 submission

cs   en de   en lv   en ru   en tr   en zh   en
2017
2017
   
25.9
21.7
27.5
22.5
28.2
22.9
28.9
23.6
29.4
30.3
25.1
25.7
31.1
30.9
25.7

2017
   
19.7
18.8
20.6
21.0
21.6
22.3
20.1

2017
31.1
32.0
32.1
33.5
33.8
34.4
35.1
35.1

2017
   
16.4
17.0
16.6
17.7
18.5
19.0
19.0

2017
29.6
31.3
32.3
32.7
33.3
33.6
34.6
30.8

we start from stronger baselines (more data, adam, new bpe)
layer id172 and deep models generally help
checkpoint ensembles help, but independent ensembles are better
reranking helps
large improvements over baseline

alexandra birch

uedin wmt 2017

27 / 36

news task: out of english

system
wmt16 single system
baseline
+layer id172
+deep model
+checkpoint ensemble
+independent ensemble
+right-to-left reranking
wmt-17 submission

en   cs en   de en   lv en   ru en   tr en   zh
2017
2017
19.7
   
31.3
20.5
32.3
20.5
33.4
21.1
33.5
22.0
22.8
35.8
36.3
22.8
22.8
36.3

2017
24.9
26.1
26.1
26.6
27.5
28.3
28.3
28.3

2017
   
14.6
14.9
15.1
16.1
16.7
16.9
16.9

2017
   
15.6
15.7
16.2
16.7
17.6
18.1
16.5

2017
26.7
28.0
28.7
29.9
31.0
31.6

29.8

   

alexandra birch

uedin wmt 2017

28 / 36

biomedical task

system
baseline
+layer id172
+deep model
+checkpoint ensemble
+independent ensemble
+right-to-left reranking
wmt17 submission

coch nhs24 coch nhs24
26.2
25.5
25.9
28.4
28.1
28.6
29.0

en   ro
23.0
24.7
27.3
27.0
28.3
29.0
29.3

en   pl
18.2
20.2
20.2
21.3
21.6
22.5
23.2

36.8
35.6
37.8
39.1
40.5
40.8
41.2

alexandra birch

uedin wmt 2017

29 / 36

copied monolingual data

table: id7 scores for en   tr when adding copied monolingual data.

tr   en
2017
system 2016
19.7
20.0
baseline
+copied
20.2
19.7

en   tr
2017
2016
14.7
13.2
13.8
15.6

alexandra birch

uedin wmt 2017

30 / 36

biomedical task: id20

system
generic (single)
generic (ensemble 4)
   ne-tuned (single)
   ne-tuned (ensemble 4)

coch nhs24 coch nhs24
22.8
23.6
27.2
27.4

en   ro
26.5
27.9
27.0
26.0

en   pl
16.6
19.9
19.5
20.9

37.6
39.2
38.6
39.9

alexandra birch

uedin wmt 2017

31 / 36

thank you!

alexandra birch

uedin wmt 2017

32 / 36

bibliography i

denkowski, m. and neubig, g. (2017).
stronger baselines for trustable results in id4.
arxiv e-prints.

kingma, d. p. and ba, j. (2015).
adam: a method for stochastic optimization.
in the international conference on learning representations, san diego, california, usa.

miceli barone, a. v., helcl, j., sennrich, r., haddow, b., and birch, a. (2017).
deep architectures for id4.
in proceedings of the second conference on machine translation, volume 1: research papers, copenhagen, denmark.
association for computational linguistics.

press, o. and wolf, l. (2017).
using the output embedding to improve language models.
in proceedings of the 15th conference of the european chapter of the association for computational linguistics (eacl),
valencia, spain.

sennrich, r., haddow, b., and birch, a. (2016).
id4 of rare words with subword units.
in proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages
1715   1725, berlin, germany. association for computational linguistics.

sutskever, i., vinyals, o., and le, q. v. (2014).
sequence to sequence learning with neural networks.
in
advances in neural information processing systems 27: annual conference on neural information processing systems 2014,
pages 3104   3112, montreal, quebec, canada.

alexandra birch

uedin wmt 2017

33 / 36

bibliography ii

wu, y., schuster, m., chen, z., le, q. v., norouzi, m., macherey, w., krikun, m., cao, y., gao, q., macherey, k., klingner, j.,
shah, a., johnson, m., liu, x., kaiser,   ., gouws, s., kato, y., kudo, t., kazawa, h., stevens, k., kurian, g., patil, n., wang, w.,
young, c., smith, j., riesa, j., rudnick, a., vinyals, o., corrado, g., hughes, m., and dean, j. (2016).
google   s id4 system: bridging the gap between human and machine translation.
arxiv e-prints.

zeiler, m. d. (2012).
adadelta: an adaptive learning rate method.
corr, abs/1212.5701.

zhou, j., cao, y., wang, x., li, p., and xu, w. (2016).
deep recurrent models with fast-forward connections for id4.
transactions of the association of computational linguistics     volume 4, issue 1, pages 371   383.

alexandra birch

uedin wmt 2017

34 / 36

