global optimality in matrix and tensor 
factorization, deep learning & beyond

ben haeffele and ren   vidal 

center for imaging science 
johns hopkins university 

impact of deep learning in id161
id98 
    2012-2014 classification results in id163 

non-id98

    2015 results: msr under 3.5% error using 150 layers!

slide from yann lecun   s cvpr   15 plenary and iccv   15 tutorial intro by joan bruna

why these improvements in performance?
    features are learned rather than hand-crafted 
1

mean ap

    more layers capture more invariances [1]  

0.8

    more data to train deeper networks 

    more computing (gpus) 

    better id173: dropout 

    new nonlinearities 

    max pooling, rectified linear units (relu) 

p
a

0.6

0.4

0.2

 3

 7

11

15
level

19

23

(a)

    theoretical understanding of deep networks remains shallow

[1] razavian, azizpour, sullivan, carlsson, id98 features off-the-shelf: an astounding baseline for recognition. cvprw   14.

figure 2: a) evolution of the mean image classi   cation ap over pas-
cal voc 2007 classes as we use a deeper representation from the
overfeat id98 trained on the ilsvrc dataset. overfeat considers
convolution, max pooling, nonlinear activations, etc. as separate layers.
the re-occurring decreases in the plot is of the activation function layer
which loses information by half rectifying the signal. b) confusion matrix
for the mit-67 indoor dataset. some of the off-diagonal confused classes
have been annotated, these particular cases could be hard even for a human
to distinguish.

last 2 layers the performance increases. we observed the

deep learning problem is non convex

 (x 1, . . . , x k) =  k(      

 2(

 1(

v x 1

)x 2
)

       x k)

nonlinearity

features

weights

min

x 1,...,xk

`(y,  (x 1, . . . , x k)) +     (x 1, . . . , x k)

loss

labels

regularizer

how is non convexity handled?
    the learning problem is non-convex 

min

`(y,  (x 1, . . . , x k)) +     (x 1, . . . , x k)

x 1,...,xk
    back-propagation, alternating minimization, descent method 

    to get a good local minima 

    random initialization 
    if training error does not decrease fast enough, start again 
    repeat multiple times 

    mysteries 

    one can find many solutions with similar objective values 
    rectified linear units work better than sigmoid/hyperbolic tangent  
    dead units (zero weights)

prior work on optimization for neural nets
    earlier work 

    no spurious local optima for linear networks (baldi & hornik    89) 
    stuck in local minima (brady    89, gori & tesi    92), but guaranteed to 

converge for linearly separable data (gori & tesi    92) 

    manifold of spurious local optima (frasconi    97) 

    recent work 

    convex neural networks in infinite number of variables: bengio    05 
    networks with many hidden units can learn polynomials: andoni   14 
    the loss surface of multilayer networks: choromanska    15 
    attacking the saddle point problem: dauphin    14 
    effect of gradient noise on the energy landscape: chaudhuri    15 
    guaranteed training of nns using tensor methods: janzamin    15 

    today 

    guarantees of global optimality in neural network training: haeffele    15   

main results

min

x 1,...,xk

`(y,  (x 1, . . . , x k)) +     (x 1, . . . , x k)

    assumptions: 

                    : convex and once differentiable in 
         and     : sums of positively homogeneous functions of same degree 

`(y, x)
 
   

x

f (   x 1, . . . ,    x k) =    pf (x 1, . . . , x k) 8      0
    theorem 1: a local minimizer such that for some i and all k   

x k
                is a global minimizer 

i = 0

    theorem 2: if the size of the network is large enough, local 
descent can reach a global minimizer from any initialization 

benjamin d. haeffele, rene vidal. global optimality in tensor factorization, deep learning, and beyond. arxiv:1506.07540, 2015

main results

min

x 1,...,xk

`(y,  (x 1, . . . , x k)) +     (x 1, . . . , x k)

    assumptions: 

                    : convex and once differentiable in 
         and     : sums of positively homogeneous functions of same degree 

`(y, x)
 
   

x

f (   x 1, . . . ,    x k) =    pf (x 1, . . . , x k) 8      0

    theorem 2: spurious local minima guaranteed not to exist
chapter 4. generalized factorizations

critical points of non-convex function

guarantees of our framework

(a)

(c)

(b)

(d)

(e)

(f)

(g)

(h)

(i)

benjamin d. haeffele, rene vidal. global optimality in tensor factorization, deep learning, and beyond. arxiv:1506.07540, 2015

figure 4.1: left: example critical points of a non-convex function (shown in red).
(a) saddle plateau (b,d) global minima (c,e,g) local maxima (f,h) local minima (i
- right panel) saddle point. right: guaranteed properties of our framework. from

outline

min

x 1,...,xk

`(y,  (x 1, . . . , x k)) +     (x 1, . . . , x k)

    global optimality in structured id105 [1,2] 

    pca, robust pca, matrix completion 
    nonnegative id105 
    dictionary learning 
    structured id105 

x

u

   

v >

    global optimality in positively homogeneous factorization [2] 

    tensor factorization 
    deep learning 
    more

[1] haeffele, young, vidal. structured low-rank id105: optimality, algorithm, and applications to image processing, icml    14 
[2] haeffele, vidal. global optimality in tensor factorization, deep learning and beyond, arxiv,    15

global optimality in structured matrix 

factorization

ben haeffele and ren   vidal 

center for imaging science 
johns hopkins university

low-rank modeling
    models involving factorization are ubiquitous 

    principal component analysis 
    nonnegative id105  
    sparse dictionary learning 
    low-rank matrix completion 
    robust pca

face id91 and classification

hyperspectral imaging id126s

affine structure from motion

typical low-rank formulations
    convex formulations: 

min
x

`(y, x) +     (x)

x
x ky   xk2
min

f +  kxk   
    low-rank matrix approximation 
    low-rank matrix completion 
    robust pca 
    convex 
x ky   xk1 +  kxk   
min
    large problem size 
    unstructured factors

    factorized formulations: 
min
u,v

`(y, u v >) +     (u, v )

(1)

u

v >

(2)

    principal component analysis 
    nonnegative id105 
    sparse dictionary learning 

(3)
    non-convex 
    small problem size 
    structured factors

convex formulations of id105
    convex formulations: 

`(y, x) +      (x)

              : convex in 

`,    

x

min
x

    low-rank matrix approximation:  

low-rank matrix recovery and completion via id76

min
x

1
2ky   xk2

f +   kxk   

11/9/15, 11:27 pm

low-rank matrix recovery and completion via id76

    robust pca:

welcome!

this website introduces new tools for recovering low-rank matrices from incomplete or corrupted observations.

x ky   xk1 +   kxk   
min

credits people

+

introduction

references

sample code

applications

kxk    =x  i(x)

    convex 
    large problem size 
    unstructured factors

   2015  

university of illinois

matrix of corrupted observations

ej cand  s, b recht. exact matrix completion via id76. foundations of computational mathematics, 2009. 
rh keshavan, a montanari, s oh. matrix completion from a few entries. ieee trans. on id205, 2010.   
ej cand  s, t tao. the power of convex relaxation: near-optimal matrix completion. ieee trans. on id205, 2010 
candes, li, ma, wright. robust principal component analysis? journal of the acm, 2011. 
h xu, c caramanis, s sanghavi. robust pca via outlier pursuit. nips 2010

a common modeling assumption in many engineering applications is that the underlying data lies (approximately) on a
low-dimensional linear subspace. this property has been widely exploited by classical principal component analysis

underlying low-rank matrix 

sparse error matrix

factorized formulations id105
    factorized formulations: 
`(y, u v >) +     (u, v )

min
u,v

                    : convex in 

`(y, x)

x

    pca [1]: 

    nmf [2]: 

    sdl [3-5]:

f

u,v ky   u v >k2
min
u,v ky   u v >k2
min
u,v ky   u v >k2
min

f

f

s.t. u>u = i

s.t. u   0, v   0
s.t.

kuik2     1,kvik0     r

    small problem size 
    structured factors

    need to specify size a priori 
    non-id76 problem

[1] jolliffe. principal component analysis. springer, 1986 
[2] lee and seung. "learning the parts of objects by non-negative id105." nature, 1999 
[3] olshausen and field,    sparse coding with an overcomplete basis set: a strategy employed by v1?,    vision research, 1997 
[4] engan, aase, and hakon-husoy,    method of optimal directions for frame design,    icassp 1999   
[5] aharon, elad, bruckstein, "k-svd: an algorithm for designing overcomplete dictionaries for sparse representation", tsp 2006

main results

min
u,v
    assumptions: 

`(y, u v >) +     (u, v )

                    : convex and once differentiable in 
          : sum of positively homogeneous functions of degree 2 

`(y, x)
   

x

rxi=1

   (u, v ) =

   (ui, vi),    (   u,    v) =    2   (u, v),8      0

    theorem 1: a local minimizer (u,v) such that for some i   

                          is a global minimizer 
ui = vi = 0

    theorem 2: if the size of the factors is large enough, local 
descent can reach a global minimizer from any initialization 

b. haeffele, e. young, r. vidal. structured low-rank id105: optimality, algorithm, and applications to image processing. icml 2014 
benjamin d. haeffele, rene vidal. global optimality in tensor factorization, deep learning, and beyond. arxiv:1506.07540, 2015

main results: nuclear norm case
    convex problem                   factorized problem 

min
x

`(y, x) +  kxk    min
    variational form of the nuclear norm 

u,v

`(y, u v >) +     (u, v )

rxi=1

kxk    = min

u,v

|ui|2|vi|2

s.t. u v > = x

    theorem 1: assume loss    is convex and once differentiable 
in x. a local minimizer of the factorized problem such that for 
some i                         is a global minimizer of both problems 

ui = vi = 0

`

   

intuition: regularizer         comes from a convex function   

   

the following papers study the case of a square id168 using techniques from semi-definite programming: 
[1] s. burer and r. monteiro. local minima and convergence in low- rank semidefinite programming. math. prog., 103(3):427   444, 2005. 
[2] r. cabral, f. de la torre, j. p. costeira, and a. bernardino,    unifying nuclear norm and bilinear factorization approaches for low-rank matrix 
decomposition,    in ieee international conference on id161, 2013, pp. 2488   2495.

main results: nuclear norm case
    convex problem                   factorized problem 

min
x

`(y, x) +  kxk    min

u,v

`(y, u v >) +     (u, v )

x

u

v >

    theorem 1: assume loss    is convex and once differentiable 
in x. a local minimizer of the factorized problem such that for 
some i                         is a global minimizer of both problems

ui = vi = 0

`

the following papers study the case of a square id168 using techniques from semi-definite programming: 
[1] s. burer and r. monteiro. local minima and convergence in low- rank semidefinite programming. math. prog., 103(3):427   444, 2005. 
[2] r. cabral, f. de la torre, j. p. costeira, and a. bernardino,    unifying nuclear norm and bilinear factorization approaches for low-rank matrix 
decomposition,    in ieee international conference on id161, 2013, pp. 2488   2495.

main results: projective tensor norm case
    a natural generalization is the projective tensor norm [1,2] 

kxku,v = min

u,v

kuikukvikv

s.t. u v > = x

    theorem 1 [3,4]: a local minimizer of the factorized problem   

rxi=1

`(y, u v >) +  

min
u,v

kuikukvikv

rxi=1

such that for some i                        , is a global minimizer of 
both the factorized problem and of the convex problem

ui = vi = 0

min
x

`(y, x) +  kxku,v

[1] bach, mairal, ponce, convex sparse id105s, arxiv 2008. 
[2] bach. convex relaxations of structured id105s, arxiv 2013. 
[3] haeffele, young, vidal. structured low-rank id105: optimality, algorithm, and applications to image processing, icml    14 
[4] haeffele, vidal. global optimality in tensor factorization, deep learning and beyond, arxiv    15

   
   
   
   
main results: projective tensor norm case
    theorem 2: if the number of columns is large enough, local 
descent can reach a global minimizer from any initialization 
chapter 4. generalized factorizations

critical points of non-convex function

guarantees of our framework

(a)

(c)

(b)

(d)

(e)

(f)

(g)

(h)

(i)

figure 4.1: left: example critical points of a non-convex function (shown in red).
(a) saddle plateau (b,d) global minima (c,e,g) local maxima (f,h) local minima (i
    meta-algorithm:  
- right panel) saddle point. right: guaranteed properties of our framework. from
    if not at a local minima, perform local descent 
any initialization a non-increasing path exists to a global minimum. from points on
a    at plateau a simple method exists to    nd the edge of the plateau (green points).
    at local minima, test if theorem 1 is satisfied. if yes => global minima 
    if not, increase size of factorization and find descent direction (u,v)

plateaus (a,c) for which there is no local descent direction1, there is a simple method

r   r + 1 u     u u   

v     v

[1] haeffele, vidal. global optimality in tensor factorization, deep learning and beyond, arxiv    15

points). taken together, these results will imply a theoretical meta-algorithm that is

to    nd the edge of the plateau from which there will be a descent direction (green

v   

algorithm: projective tensor norm case

`(y, u v >) +  

min
u,v

kuikukvikv

    convex in u given v and vice versa 

rxi=1

    alternating proximal id119 

    calculate gradient of smooth term 
    compute proximal operator 
    acceleration via extrapolation 

    advantages 

    easy to implement 
    highly parallelizable 
    guaranteed to converge to nash equilibrium (may not be local min) [1]

y. xu and w. yin,    a block coordinate descent method for regularized multiid76 with applications to nonnegative tensor factorization and 
completion,    siam journal of imaging sciences, vol. 6, no. 3, pp. 1758   1789, 2013.

example: nonnegative id105
    original formulation 

u,v ky   u v >k2
min

f

s.t. u   0, v   0

    new factorized formulation 

u,v ky   u v >k2
min

f +  xi

|ui|2|vi|2 s.t. u, v   0

    note: id173 limits the number of columns in (u,v)

example: sparse dictionary learning
    original formulation 

u,v ky   u v >k2
min

f

s.t.

kuik2     1,kvik0     r

    new factorized formulation

u,v ky   u v >k2
min

f +  xi

|ui|2(|vi|2 +  |vi|1)

non example: robust pca
    original formulation [1] 

x,e kek1 +  kxk   
min

s.t. y = x + e

    equivalent formulation 

x ky   xk1 +  kxk   
min

    new factorized formulation 

min

u,v ky   u v >k1 +  xi

|ui|2|vi|2

    not an example because loss is not differentiable

[1] candes, li, ma, wright. robust principal component analysis? journal of the acm, 2011.

application: calcium imaging segmentation
    fluorescent microscopy technique 

    optical recording of brain activity 
    neurons    flash    when active electrically

60 microns

470 microns

application: calcium imaging segmentation

why do we need structure?

application: calcium imaging segmentation
    find neuronal shapes and spike trains in calcium imaging

u,v ky    (u v >)k2
min

(5)

f +  

rxi=1

kuikukvikv

why do we need structure?
y 2 rp   t
why do we need structure?
p = number of pixels
y 2 rp   t
t = number of video frames
p = number of pixels
u1
t = number of video frames
u2
u1
z
z2
1
v1
u2
k    ku = k    k2 + k    k1 + k    kt v
y 2 rp   t
v1
v2
 (u v >)
v2
k    kv = k    k2 + k    k1
p = number of pixels
 (u v >)
t = number of video frames
y
u1
u2
v1
a
1
v2
time
 (u v >)

y 2 rp   t
p = number of pixels
(6)
t = number of video frames
u1
u2
v1
v2
a
2
 (u v >)

y 2 rp   t
p = number of pixels
t = number of video frames
u1
u2
v1
v2
 (u v >)
  (az )t

neuron shape

spike times

true signal

time

data

in vivo results (small area)

u,v ky    (u v >)k2
min
f +  

u,v ky    (u v >)k2
min

f +  

rxi=1

rxi=1

kuikukvikv

kuikukvikv

60 microns

k    ku = k    k2 + k    k1 + k    kt v
k    kv = k    k2 + k    k1

k    ku = k    k2 + k    k1 + k    kt v
k    kv = k    k2 + k    k1

raw data

sparse

+ low rank

+ total variation

conclussions
    structured low rank id105 

    structure on the factors captured by the projective tensor norm 
    efficient optimization for large scale problems 

    local minima of the non-convex factorized form are global 

minima of both the convex and non-convex forms 

    advantages in applications 

    neural calcium image segmentation  
    compressed recovery of hyperspectral images

global optimality in positively 
homogeneous factorization

ben haeffele and ren   vidal 

center for imaging science 
johns hopkins university

from id105s to deep learning

from id105s to deep learning
(10)
    two-layer nn 

x 1 2 rd1   r

v 2 rn   d1

    input: 
    weights:  
    nonlinearity: relu 

v 2 rn   d1
x k 2 rdk   r
x1 2 rd1   r

x 1 2 rd1   r

(10)

x 2 2 rd2   r

(11)

x 2 2 rd2   r

(11)

v 2 rn   d1
 (x 1, x 2) =  1(v x 1)(x 2)>

(12)

x2 2 rd2   r

(12)
 (x 1, x 2) =  1(v x 1)(x 2)>

x 1 2 rd1   r

(13)

from id105s to deep learning

 1(x) = max(x, 0)

   

v 2 rn   d1

   almost    like id105 
    r = rank 
    r = #neurons in hidden layer

x 1 2 rd1   r

(10)

x 2 2 rd2   r

(11)

 (x 1, x 2) =  1(v x 1)(x 2)>

(12)

x 2 2 rd2   r

(13)

from id105s to deep learning
    recall the generalized factorization problem 

min

x 1,...,xk

`(y,  (x 1, . . . , x k)) +     (x 1, . . . , x k)

    id105 is a particular case where k=2 

 (u, v ) =

uiv >i

,    (u, v ) =

rxi=1

rxi=1

kuikukvikv

 

   

    both     and     are sums of positively homogeneous functions  
f (   x 1, . . . ,    x k) =    pf (x 1, . . . , x k) 8      0

    other examples 

    relu + max pooling is positively homogeneous of degree 1

   id127    for k > 2
   

in id105 we have 

    by analogy we define   

 (x 1, . . . , x k) =

 (u, v ) = u v > =

uiv >i

rxi=1

 (x 1

i , . . . , x k
i )

rxi=1

x k

where        is a tensor,        is its i-th slice along its last 
dimension, and      is a positively homogeneous function 

x k
i

 

    examples 

    id127: 
    tensor product: 
    relu neural network:

 (x 1, x 2) = x 1x 2>

 (x 1, . . . , x k) = x 1                x k
 (x 1, . . . , x k) =  k(        2( 1(v x 1)x 2)       x k)

   
   
   
example: cp tensor factorization

 (x 1, . . . , x k) =

 (x 1

i , . . . , x k
i )

chapter 4. generalized factorizations

rxi=1

d1

x 1

r

d2

x 2

r

d3

x 3

r

 (x 1

r

2

3
, x   , x )

d1

d2

d3

x 1

1

x 3

1

x 2

1

x 3

2

x 2

2

x 1

2

x 3

r

x 2

r

x 1

r

r

figure 4.2: rank-r cp decomposition of a 3rd order tensor.

(where     denotes the tensor outer product) results in  r(x 1, . . . , x k) being the

example: deep learning

 (x 1
chapter 4. generalized factorizations

 (x 1, . . . , x k) =

i , . . . , x k
i )

rxi=1

relu network with one hidden layer

x1
1

x2
1

v

 4(x 1, x 2)

x1
4

x2
4

rectified linear unit (relu)

  

0

x1
1

x2
1

x3

1 x4

1

multilayer relu 
parallel network

 (

x1
1

x3
x2
,     ,     ,
1

1 x4

1

)

v

 4

x1
(

x2
,     ,     ,

x3 x4

)

 (

x1
4

x3
x2
,     ,     ,
4

4 x4

4

)

figure 4.3: example relu networks. (left panel) relu network with a single hidden
layer with the mapping described by the equation in (4.10) with (r = 4, d1 = 3, d2 =

x1
4

x2
4

x3

4 x4

4

factorization id173 for    k > 2      

   

in id105 we had    generalized nuclear norm    
kxku,v = min

kuikukvikv

s.t. u v > = x

u,v

    by analogy we define    nuclear deep net regularizer      

    ,   (x) = min
{x k}

   (x 1

i , . . . , x k

i ) s.t.  (x 1, . . . , x k) = x

where     is positively homogeneous of the same degree as 

   

 

    proposition:                is convex 

    ,   

   

intuition: regularizer         comes from a convex function   

   

rxi=1
rxi=1

   
   
   
   
examples of deep network regularizers
    different norms for different properties on each factor  

    different norms plus conic set constraints on the factors 

    conic set examples 

    kernel of linear operator 
    inequalities w.r.t. linear operator 
    constraints on non-zero support 
    semidefinite matrices

main results
    theorem 1: a local minimizer of the factorized formulation   

` y,

rxi=1

 (x 1

i , . . . , x k

min
{x k}
such that for some i and all k                 is a global minimizer 
for both the factorized problem and of the convex formulation 

i = 0

i , . . . , x k
i )

   (x 1

x k

i )  +  

rxi=1

min
x

`(y, x) +      ,   (x)

    examples 

    id105 
    tensor factorization 
    deep learning

[1] haeffele, vidal. global optimality in tensor factorization, deep learning and beyond, arxiv    15

   
   
   
   
main results
    theorem 2: if the size of the network is large enough, local 
descent can reach a global minimizer from any initialization 
chapter 4. generalized factorizations

critical points of non-convex function

guarantees of our framework

(a)

(c)

(b)

(d)

(e)

(f)

(g)

(h)

(i)

figure 4.1: left: example critical points of a non-convex function (shown in red).
(a) saddle plateau (b,d) global minima (c,e,g) local maxima (f,h) local minima (i
    meta-algorithm:  
- right panel) saddle point. right: guaranteed properties of our framework. from
any initialization a non-increasing path exists to a global minimum. from points on
a    at plateau a simple method exists to    nd the edge of the plateau (green points).

    if not at a local minima, perform local descent 
    at a local minima, test if theorem 1 is satisfied. if yes => global minima 
    if not, increase size by 1 (add network in parallell) and continue 
    maximum r guaranteed to be bounded by the dimensions of the 

plateaus (a,c) for which there is no local descent direction1, there is a simple method

network output

to    nd the edge of the plateau from which there will be a descent direction (green

[1] haeffele, vidal. global optimality in tensor factorization, deep learning and beyond, arxiv    15

points). taken together, these results will imply a theoretical meta-algorithm that is

current limitations
    requires networks with parallel architecture 

    future work to explore more general id173 strategies to 

control other aspects of the network architecture 

    results only apply to local minima, not saddle points  
    finding descent direction from saddle point can be np-hard 

    upper bound on size of network is impractically large 

    o(# of training examples in dataset) 
    but, this is a worst case upper bound for any possible initialization

relation to dropout
    our theory suggests that a highly parallel architecture is 

advantageous for optimization 

    similar to dropout id173 (not an exact analogy) 

    sum of exponential number of subnetworks

[1] srivastava, et al, "dropout: a simple way to prevent neural networks from overfitting." journal of machine learning research, 2014.

balanced degrees of homogeneity
    weight decay is often cited as not performing as well as 

dropout in relu networks [1-3]. 
    ex: l2 decay 

min

x 1,...,xk

`(y,  (x 1, . . . , x k)) +  

kx kk2

f

kxk=1

    degrees of homogeneity are not typically balanced 

 (   x 1, . . . ,    x k) =    k (x 1, . . . , x k)

kxk=1

k   x kk2

f =    2

kx kk2

f

kxk=1

    proposition: if k > 2 there exist spurious local minima

[1] srivastava, et al, "dropout: a simple way to prevent neural networks from overfitting." jmlr, 2014. 
[2] krizhevsky, et al,    id163 classification with deep convolutional neural networks.    nips, 2012. 
[3] wan et al,    id173 of neural networks using dropconnect.    icml, 2013.

conclusions and future directions
    size matters 

    optimize not only the network weights, but also the network size 
    today: size = number of neurons or number of parallel networks 
    tomorrow: size = number of layers + number of neurons per layer 

    id173 matters 

    use    positively homogeneous regularizer    of same degree as network 
    how to build a regularizer that controls number of layers + number of 

neurons per layer 

    not done yet 

    checking if we are at a local minimum or finding a descent direction 

can be np hard 

    need    computationally tractable    regularizers

more information,

vision lab @ johns hopkins university 

http://www.vision.jhu.edu 

center for imaging science @ johns hopkins university 

http://www.cis.jhu.edu 

thank you!

