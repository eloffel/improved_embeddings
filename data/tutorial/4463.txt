deep learning for semantic composition

xiaodan zhu    & edward grefenstette   

   national research council canada

queen   s university
zhu2048@gmail.com

   deepmind

etg@google.com

july 30th, 2017

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

1 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

2 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

3 / 119

principle of compositionality

principle of compositionality: the meaning of a whole is a function of
the meaning of the parts.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

4 / 119

principle of compositionality

principle of compositionality: the meaning of a whole is a function of
the meaning of the parts.

while we focus on natural language, compositionality exists not just
in language.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

4 / 119

principle of compositionality

principle of compositionality: the meaning of a whole is a function of
the meaning of the parts.

while we focus on natural language, compositionality exists not just
in language.

sound/music

music notes are composed with some regularity but not randomly
arranged to form a song.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

4 / 119

principle of compositionality

principle of compositionality: the meaning of a whole is a function of
the meaning of the parts.

while we focus on natural language, compositionality exists not just
in language.

sound/music

music notes are composed with some regularity but not randomly
arranged to form a song.

vision

natural scenes are composed of meaningful components.
arti   cial visual art pieces often convey certain meaning with regularity
from their parts.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

4 / 119

principle of compositionality

compositionality is regarded by many as a fundamental component of
intelligence in addition to language understanding (miller et al., 1976;
fodor et al., 1988; bienenstock et al., 1996; lake et al., 2016).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

5 / 119

principle of compositionality

compositionality is regarded by many as a fundamental component of
intelligence in addition to language understanding (miller et al., 1976;
fodor et al., 1988; bienenstock et al., 1996; lake et al., 2016).

for example, lake et al. (2016) emphasize several essential
ingredients for building machines that    learn and think like people   :

compositionality
intuitive physics/psychology
learning-to-learn
causality models

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

5 / 119

principle of compositionality

compositionality is regarded by many as a fundamental component of
intelligence in addition to language understanding (miller et al., 1976;
fodor et al., 1988; bienenstock et al., 1996; lake et al., 2016).

for example, lake et al. (2016) emphasize several essential
ingredients for building machines that    learn and think like people   :

compositionality
intuitive physics/psychology
learning-to-learn
causality models

note that many of these challenges present in natural language
understanding.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

5 / 119

principle of compositionality

compositionality is regarded by many as a fundamental component of
intelligence in addition to language understanding (miller et al., 1976;
fodor et al., 1988; bienenstock et al., 1996; lake et al., 2016).

for example, lake et al. (2016) emphasize several essential
ingredients for building machines that    learn and think like people   :

compositionality
intuitive physics/psychology
learning-to-learn
causality models

note that many of these challenges present in natural language
understanding.

they are re   ected in the sparseness in training a nlp model.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

5 / 119

principle of compositionality

compositionality is regarded by many as a fundamental component of
intelligence in addition to language understanding (miller et al., 1976;
fodor et al., 1988; bienenstock et al., 1996; lake et al., 2016).

for example, lake et al. (2016) emphasize several essential
ingredients for building machines that    learn and think like people   :

compositionality
intuitive physics/psychology
learning-to-learn
causality models

note that many of these challenges present in natural language
understanding.

they are re   ected in the sparseness in training a nlp model.

note also that compositionality may be entangled with the other
   ingredients    listed above.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

5 / 119

semantic composition in natural language

good     very good     not very good     ...

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

6 / 119

semantic composition in natural language

figure: results from (zhu et al., 2014).
a dot in the    gure corresponds to a negated
phrase (e.g., not very good) in stanford
sentiment treebank (socher et al., 2013).
the y-axis is its sentiment value and x-axis
the sentiment of its argument.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

7 / 119

semantic composition in natural language

even a one-layer composition, over
one dimension of meaning (e.g.,
semantic orientation (osgood
et al., 1957)), could be a
complicated mapping.

figure: results from (zhu et al., 2014).
a dot in the    gure corresponds to a negated
phrase (e.g., not very good) in stanford
sentiment treebank (socher et al., 2013).
the y-axis is its sentiment value and x-axis
the sentiment of its argument.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

7 / 119

semantic composition in natural language

good     very good     not very good     ...
senator     former senator     ...
basketball player     short basketball player     ...
giant     small giant     ...
empty/full     half empty/full     almost half empty/full     ...1

1see more examples in (partee, 1995).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

8 / 119

semantic composition in natural language

semantic composition in natural language: the task of modelling the
meaning of a larger piece of text by composing the meaning of its
constituents.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

9 / 119

semantic composition in natural language

semantic composition in natural language: the task of modelling the
meaning of a larger piece of text by composing the meaning of its
constituents.

modelling : learning a representation

the compositionality in language is very challenging as discussed
above.
compositionality can entangle with other challenges such as those
emphasized in (lake et al., 2016).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

9 / 119

semantic composition in natural language

semantic composition in natural language: the task of modelling the
meaning of a larger piece of text by composing the meaning of its
constituents.

modelling : learning a representation

the compositionality in language is very challenging as discussed
above.
compositionality can entangle with other challenges such as those
emphasized in (lake et al., 2016).

a larger piece of text: a phrase, sentence, or document.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

9 / 119

semantic composition in natural language

semantic composition in natural language: the task of modelling the
meaning of a larger piece of text by composing the meaning of its
constituents.

modelling : learning a representation

the compositionality in language is very challenging as discussed
above.
compositionality can entangle with other challenges such as those
emphasized in (lake et al., 2016).

a larger piece of text: a phrase, sentence, or document.

constituents: subword components, words, phrases.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

9 / 119

semantic composition in natural language

semantic composition in natural language: the task of modelling the
meaning of a larger piece of text by composing the meaning of its
constituents.

modelling : learning a representation

the compositionality in language is very challenging as discussed
above.
compositionality can entangle with other challenges such as those
emphasized in (lake et al., 2016).

a larger piece of text: a phrase, sentence, or document.

constituents: subword components, words, phrases.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

9 / 119

introduction

two key problems:

how to represent meaning?

how to learn such a representation?

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

10 / 119

representation

let   s    rst very brie   y revisit the representation we assume in this tutorial
... and leave the learning problem to the entire tutorial that follows.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

11 / 119

representation

let   s    rst very brie   y revisit the representation we assume in this tutorial
... and leave the learning problem to the entire tutorial that follows.

love

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

11 / 119

representation

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

12 / 119

representation

love, admiration, satisfaction ...
anger, fear, hunger ...

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

12 / 119

representation

a viewpoint from the emotion machine (minsky, 2006)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

13 / 119

representation

a viewpoint from the emotion machine (minsky, 2006)

each variable responds to di   erent concepts and each concept is
represented by di   erent variables.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

13 / 119

representation

a viewpoint from the emotion machine (minsky, 2006)

each variable responds to di   erent concepts and each concept is
represented by di   erent variables.

this is exactly a distributed representation.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

13 / 119

representation

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

14 / 119

modelling composition functions

how do we model the composition functions?

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

15 / 119

representation

deep learning for semantic composition

deep learning: we focus on deep learning models in this tutorial.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

16 / 119

representation

deep learning for semantic composition

deep learning: we focus on deep learning models in this tutorial.

   wait a minute, deep learning again?   
   dl people, leave language along ...   

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

16 / 119

representation

deep learning for semantic composition

deep learning: we focus on deep learning models in this tutorial.

   wait a minute, deep learning again?   
   dl people, leave language along ...   

asking some questions may be helpful:

are deep learning models providing nice function or density
approximation, the problems that many speci   c nlp tasks essentially
seek to solve? x   y
are continuous vector representations of meaning e   ective for (as
least some) nlp tasks? are dl models convenient for computing
such continuous representations?
do dl models naturally bridge language with other modalities in
terms of both representation and learning? (this could be important.)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

16 / 119

introduction

more questions:

what nlp problems (e.g., semantic problems here) can be better
handled with dl and what cannot?

can nlp bene   t from combining dl and other approaches (e.g.,
symbolic approaches)?

in general, has the e   ectiveness of dl models for semantics already
been well understood?

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

17 / 119

introduction

deep learning for semantic composition

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

18 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

19 / 119

formal semantics

montague semantics (1970   1973):

treat natural language like a formal language via

an interpretation function [[. . .]], and
a mapping from id18 rules to function application order.

interpretation of a sentence reduces to logical form via   -reduction.

high level idea
syntax guides composition, types determine their semantics, predicate
logic does the rest.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

20 / 119

formal semantics

syntactic analysis
s     np vp
np     cats, milk, etc.
vp     vt np
vt     like, hug, etc.

semantic interpretation
[[vp]]([[np]])
[[cats]], [[milk]], . . .
[[vt]]([[np]])
  yx.[[like]](x, y ), . . .

[[like]]([[cats]], [[milk]])

[[cats]]

  x.[[like]](x, [[milk]])

  yx.[[like]](x, y )

[[milk]]

cats like milk.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

21 / 119

formal semantics

pros:

intuitive and interpretable(?) representations.

leverage the power of predicate logic to model semantics.

evaluate the truth of statements, derive conclusions, etc.

cons:

brittle, requires robust parsers.

extensive logical model required for evaluation of clauses.

extensive set of rules required to do anything useful.

overall, an intractable (or unappealing) learning problem.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

22 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

23 / 119

simple parametric models

basic models with pre-de   ned function form (mitchell et al., 2008):

general form :

add :

weightadd :

multiplicative :

combined :

p = f (u, v , r, k )
p = u + v
p =   t u +   t v
p = u     v
p =   t u +   t v +   t (u     v )

we will see later in this tutorial that the above models could be seen as
special cases of more complicated composition models.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

24 / 119

results

reference (r):
high-similarity landmark (h): the color dissolved.
the color galloped.
low-similarity landmark (l):

the color ran.

a good composition model should give the above r-h pair a similarity score
higher than that given to the r-l pair. also, a good model should assign such
similarity scores with a high correlation (  ) to what human assigned.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

25 / 119

results

reference (r):
high-similarity landmark (h): the color dissolved.
the color galloped.
low-similarity landmark (l):

the color ran.

a good composition model should give the above r-h pair a similarity score
higher than that given to the r-l pair. also, a good model should assign such
similarity scores with a high correlation (  ) to what human assigned.

models
noncomp
add
weightadd
kintsch
multiply
combined
upperbound

r-h similarity r-l similarity
0.27
0.59
0.35
0.47
0.42
0.38
4.94

0.26
0.59
0.34
0.45
0.28
0.28
3.25

  
0.08**
0.04*
0.09**
0.09**
0.17**
0.19**
0.40**

table: mean cosine similarities for the r-h pairs and r-l pairs as well as the
correlation coe   cients (  ) with human judgments (*: p < 0.05, **: p < 0.01).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

25 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

26 / 119

parameterizing composition functions

to move beyond simple algebraic or parametric models we need function
approximators which, ideally:

can approximate any arbitrary function (e.g. anns).
can cope with variable size sequences.
can capture long range or unbounded dependencies.
can implicitly or explicitly model structure.
can be trained against a supervised or unsupervised objective (or
both     semi-supervised training).
can be trained chie   y or primarily through id26.

a neural network model zoo
this section presents a selection of models satisfying some (if not all) of
these criteria.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

27 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

28 / 119

recurrent neural networks

bounded methods
many methods impose explicit or implicit length limits on conditioning
information. for example:

order-n markov assumption in nlm/lbl

fully-connected layers and dynamic pooling in conv-nets

recurrent neural networks introduce a repeatedly
composable unit, the recurrent cell, which both
models an unbounded sequence pre   x and express a
function over it.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

29 / 119

wjf(w1:j)hj-1hjthe mathematics of recurrence

building blocks

an input vector wj     r|w|
a previous state hj   1     r|h|
a next state hj     r|h|
an output yj     r|y|
fy : r|w|    r|h|     r|y|
fh : r|w|    r|h|     r|h|

putting it together

hj = fh(wj , hj   1)
yj = fy (wj , hj )

so yj = fy (wj , fh(wj   1, hj   1)) = fy (wj , fh(wj   1, fh(wj   2, hj   2))) = . . .

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

30 / 119

wjf(w1:j)hj-1hjpreviousstatenextstateinputsoutputsid56s for language modelling

n(cid:89)

language modelling
we want to model the joint id203 of tokens t1, . . . tn in a sequence:

p(t1, . . . tn) = p(t1)

p(ti|t1, . . . ti   1)

i=2

adapting a recurrence for basic lm
for vocab v, de   ne an embedding matrix e     r|v|  |w| and a logit
projection matrix wv     r|y|  |v|. then:

wj = embed(tj , e )
yj = fy (wj , hj ) hj = fh(wj , hj   1)
pj = softmax(yj wv )
p(tj+1|t1, . . . , tj ) = categorical(tj+1; pj )

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

31 / 119

aside: the vanishing gradient problem and lstm id56s

id56 is deep    by time   , so it could
seriously su   er from the vanishing
gradient issue.

lstm con   gures memory cells and
multiple    gates    to control
information    ow. if properly learned,
lstm can keep pretty long-distance
(hundreds of time steps) information
in memory.
memory-cell details:
it =   (wxi xt + whi ht   1 + wci ct   1)
ft =   (wxf xt + whf ht   1 + wcf ct   1)
ct =   (ftct   1 + ittanh(wxc xt + whc ht   1))
ot =   (wxoxt + whoht   1 + wcoct)
ht =   (ottanh(ct))

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

32 / 119

conditional language models

conditional language modelling
a strength of id56s is that hj can model not only the history of the
generated/observed sequence t1, . . . , tj , but any conditioning information
  , e.g. by setting h0 =   .

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

33 / 119

w1w2w3 w1w2w3 encoder-decoder models with id56s

model p(t1, . . . , tn|s1, . . . , sm)
he
i = id56encoder (si , he
hd
i = id56decoder (ti , hd
hd
0 = he
m
ti+1     categorical(t; fv (hi ))

i   1)
i   1)

cf. kalchbrenner et al., 2013; sutskever et al., 2014

the encoder id56 as a composition module
all information needed to transduce the source into the target sequence
using id56decoder needs to be present in the start state hd
0 .
this start state is produced by id56encoder , which will learn to compose.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

34 / 119

leschiensaimentlesos|||dogslovebonesdogslovebones</s>source sequencetarget sequenceid56s as sentence encoders

this idea of id56s as sentence encoder works for classi   cation as well:

data is labelled sequences (s1, . . . , s|s|;   y ).
id56 is run over s to produce    nal state h|s| = id56(s).
a di   erentiable function of h|s| classi   es: y = f  (h|s|)
h|s| can be taken to be the composed meaning of s, with regard to
the task at hand.

an aside: bi-directional id56 encoders
for both sequence classi   cation and generation, sometimes a
bi-directional id56 is used to encode:

i = id56   (si , h   
h   
i+1)

i = id56   (si , h   
h   
i   1)
1 , h   
h|s| = concat(h   
|s|)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

35 / 119

a transduction bottleneck

single vector representation of
sentences causes problems:

training focusses on learning
marginal language model of
target language    rst.

longer input sequences cause
compressive loss.

encoder gets signi   cantly
diminished gradient.

in the words of ray mooney. . .
   you can   t cram the meaning of a whole %&!$ing sentence into a single
$&!*ing vector!   

yes, the censored-out swearing is copied verbatim.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

36 / 119

leschiensaimentlesos|||dogslovebonesdogslovebones</s>source sequencetarget sequenceattention

we want to use he
predicting ti by conditioning on
words that might relate to ti :

1, . . . , he

m when

1 compute hd
i (id56 update)
i , he
2 eij = fatt(hd
j )
3 aij = softmax(ei )j
4 hatt
j=1 aij he
j
5   hi = concat(hd

i =(cid:80)m

i , hatt

)

i

ti+1     categorical(t; fv (  hi ))

cf. bahdanau et al., 2014

6

the many faces of attention
many variants on the above process: early attention (based on hd
used to update hd
projected inner products, or mlps), and so on.

i ), di   erent attentive functions fatt (e.g. based on

i   1 and ti ,

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

37 / 119

leschiensaimentlesos|||dogslovebonesdogslovebones</s>source sequencetarget sequenceattention and composition

we refer to the set of source activation vectors he
slides as an attention matrix. is it a suitable sentence representation?

1, . . . , he

m in the previous

pros:

locally compositional: vectors contain information about other words
(especially with bi-directional id56 as encoder).
variable size sentence representation: longer sentences yield larger
representation with more capacity.

cons:

single vector representation of sentences is convenient (many
decoders, classi   ers, etc. expect    xed-width feature vectors as input)
locally compositional, but are long range dependencies resolved in
the attention matrix? does it truly express the sentence   s meaning as
a semantic unit (or is it just good for sequence transduction)?

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

38 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

39 / 119

id56s

recursive networks: a generalization of (chain) recurrent networks with
a computational graph, often a tree (pollack, 1990; francesconi et al.,
1997; socher et al., 2011a,b,c, 2013; zhu et al., 2015b)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

40 / 119

id56s

successfully applied to consider input data structures.

natural language processing (socher et al., 2011a,c; le et al., 2015;
tai et al., 2015; zhu et al., 2015b)
id161 (socher et al., 2011b)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

41 / 119

id56s

successfully applied to consider input data structures.

natural language processing (socher et al., 2011a,c; le et al., 2015;
tai et al., 2015; zhu et al., 2015b)
id161 (socher et al., 2011b)

how to determine the structures.

encode given    external    knowledge about the structure of the input
data,

e.g., syntactic structures; modelling sentential semantics and syntax is
one of the most interesting problems in language.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

41 / 119

id56s

successfully applied to consider input data structures.

natural language processing (socher et al., 2011a,c; le et al., 2015;
tai et al., 2015; zhu et al., 2015b)
id161 (socher et al., 2011b)

how to determine the structures.

encode given    external    knowledge about the structure of the input
data,

e.g., syntactic structures; modelling sentential semantics and syntax is
one of the most interesting problems in language.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

41 / 119

id56s

successfully applied to consider input data structures.

natural language processing (socher et al., 2011a,c; le et al., 2015;
tai et al., 2015; zhu et al., 2015b)
id161 (socher et al., 2011b)

how to determine the structures.

encode given    external    knowledge about the structure of the input
data,

e.g., syntactic structures; modelling sentential semantics and syntax is
one of the most interesting problems in language.

encode simply a complete tree.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

41 / 119

integrating syntactic parses in composition

recursive neural tensor network (socher et al., 2012):

the structure is given (here by a constituency parser.)
each node here is implemented as a regular feed-forward layer plus a
3rd -order tensor.

the tensor captures 2nd -degree (quadratic) polynomial interaction of
children, e.g., b2

i , bi cj , and c 2
j .

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

42 / 119

results

the models have been successfully applied to a number of tasks such as
id31 (socher et al., 2013).

table: accuracy for    ne grained (5-class) and binary predictions at the
sentence level (root) and for all nodes.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

43 / 119

tree-lstm

tree-structured lstm (le, *sem-15; tai, acl-15; zhu, icml-15): it is
an extension of chain lstm to tree structures.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

44 / 119

tree-lstm

tree-structured lstm (le, *sem-15; tai, acl-15; zhu, icml-15): it is
an extension of chain lstm to tree structures.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

44 / 119

if your have a non-binary tree, a
simple solution is to binarize it.

tree-lstm application: id31

sentiment composed over a constituency parse tree:

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

45 / 119

tree-lstm application: id31

results on stanford sentiment treebank (zhu et al., 2015b):

models
nb
id166
rvnn
rntn
tree-lstm

roots
41.0
40.7
43.2
45.7
48.9

phrases
67.2
64.3
79.0
80.7
81.9

table: performances (accuracy) of models on stanford sentiment treebank, at
the sentence level (roots) and the phrase level.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

46 / 119

tree-lstm application: natural language id136

applied to natural language id136 (nli): determine if a sentence
entails another, if they contradict, or have no relation (chen et al., 2017).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

47 / 119

tree-lstm application: natural language id136

accuracy on stanford natural language id136 (snli) dataset:
(chen et al., 2017)

* welcome to the poster at 6:00-9:30pm on july 31.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

48 / 119

learning representation for natural language id136

repeval-2017 shared task (williams et al., 2017): learn sentence
representation as a    xed-length vector.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

49 / 119

tree-lstm without syntactic parses

how if we simply apply recursive networks over trees that are not
generated from syntactic parses, e.g., a complete binary trees?

multiple e   orts on snli (munkhdalai
et al., 2016; chen et al., 2017) have
observed that the models outperform
sequential (chain) lstm.

this could be related to the discussion
that recursive nets may capture
long-distance dependency (goodfellow
et al., 2016).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

50 / 119

spinn: doing away with test-time trees

image credit: sam bowman and co-authors.

cf. bowman et al., 2016

shift-reduce parsers:

exploit isomorphism between binary branching trees with t leaves
and sequences of 2t     1 binary shift/reduce actions.
shift unattached leaves from a bu   er onto a processing stack.
reduce the top two child nodes on the stack to a single parent node.

spinn: jointly train a treeid56 and a vector-based shift-reduce parser.

training time trees o   er supervision for shift-reduce parser.

no need for test time trees!

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

51 / 119

bu   erstackt=0downsatcattheshiftt=1downsatcattheshiftt=2downsatcatthereducet=3downsatthecatshiftt=4downsatthecatshiftt=5downsatthecatreducet=6satdownthecatreducet=7=t(thecat)(satdown)outputtomodelforsemantictaskspinn:doing away with test-time trees

image credit: sam bowman and co-authors.

word vectors start on bu   er b (top:    rst word in sentence).
shift moves word vectors from bu   er to stack s.
reduce pops top two vectors o    the stack, applies
f r : rd    rd     rd , and pushes the result back to the stack
(i.e. treeid56 composition).
tracker lstm tracks parser/composer state across operations,
decides shift-reduce operations a, is supervised by both observed
shift-reduce operations and end-task:

ht = lstm(f c (bt   1[0], st   1[0], st   1[1]), ht   1)

at     f a(ht)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

52 / 119

bu   erdownsatstackcatthecompositiontrackingtransitionreducedownsatthecatcompositiontrackingtransitionshiftdownsatthecattrackinga quick introduction to reinforce

what if some part of our process is not di   erentiable (e.g. samples from
the shift-reduce module in spinn) but we want to learn with no labels. . .

(cid:88)
(cid:88)

z

p(y|x) = ep  (z|x) [f  (z, x)]

s.t. y     f  (z, x) or y = f  (z, x)

     p(y|x) =
     p(y|x) =

p  (z|x)     f  (z, x) = ep  (z|x) [     f  (z, x)]
f  (z, x)     p  (z|x) = ???

z

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

53 / 119

xyxyza quick introduction to reinforce

the reinforce trick (r. j. williams, 1992)
      log p  (z|x) =

     p  (z|x)
p  (z|x)

         p  (z|x) = p  (z|x)      log p  (z|x)

     p(y|x) =

(cid:88)
f  (z, x)     p  (z|x)
(cid:88)
f  (z, x)p  (z|x)      log p  (z|x)
=
= ep  (z|x) [f  (z, x)      log p  (z|x)]

z

z

this naturally extends to cases where p(z|x) = p(z1, . . . , zn|x).

rl vocab: samples of such sequences of of discrete actions are referred to
as    traces   . we often refer to p  (z|x) as a policy     (z; x).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

54 / 119

spinn+rl: doing away with training-time trees

   drop in    extension to spinn (yogatama et al., 2016):

treat at     f a(ht) as policy   a
   (at; ht), trained via reinforce.
reward is negated loss of the end task, e.g. log-likelihood of the
correct label.
everything else is trained by id26 against the end task:
tracker lstm, representations, etc. receive gradient both from the
supervised objective, and from reinforce via the shift-reduce policy.

model recovers linguistic-like structures (e.g. noun phrases, auxiliary verb-verb pairing, etc.).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

55 / 119

awomanwearingsunglassesisfrowning.aboydragshissledsthroughthesnow.spinn+rl: doing away with training-time trees

does rl-spinn work? according to yogatama et al. (2016):

better than lstm baselines: model captures and exploits structure.
better than spinn benchmarks: model is not biased by what
linguists think trees should be like, only has a loose inductive biase
towards tree structures.
but some parses do not re   ect order of composition (see below).
semi-supervised setup may be sensible.

some    bad    parses, but not necessarily worse results.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

56 / 119

twomenareplayingfrisbeeinthepark.familymembersstandingoutsideahome.outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

57 / 119

convolution neural networks

visual inspiration: how do we learn to recognise pictures?
will a fully connected neural network do the trick?

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

58 / 119

8convnets for pictures

problem: lots of variance that shouldn   t matter (position, rotation, skew,
di   erence in font/handwriting).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

59 / 119

888888convnets for pictures

solution: accept that features are local. search for local features with a
window.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

60 / 119

8convnets for pictures

convolutional window acts as a classifer for local features.

   

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

61 / 119

8convnets for pictures

di   erent convolutional maps can be trained to recognise di   erent features
(e.g. edges, curves, serifs).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

62 / 119

...convnets for pictures

stacked convolutional layers learn higher-level features.

one or more fully-connected layers learn classi   cation function over
highest level of representation.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

63 / 119

fully connected layerconvolutional layer88raw imagefirst order local featureshigher order featurespredictionconvnets for language

convolutional neural networks    t natural language well.

deep convnets capture:

language has:

positional invariances

local features

hierarchical structure

some positional invariance

local features (e.g. pos)

hierarchical structure (phrases,
dependencies)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

64 / 119

convnets for language

how do we go from images to sentences? sentence matrices!

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

65 / 119

w1w2w3w4w5convnets for language

does a convolutional window make sense for language?

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

66 / 119

w1w2w3w4w5convnets for language

a better solution: feature-speci   c windows.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

67 / 119

w1w2w3w4w5word level sentence vectors with convnets

cf. kalchbrenner et al., 2014

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

68 / 119

k-max pooling(k=3) fully connectedlayerfoldingwideconvolution(m=2)dynamick-max pooling (k= f(s) =5) projectedsentence matrix(s=7)wideconvolution(m=3)game's the same, just got more    ercecharacter level sentence vectors with convnets

naively, we could just represent
everything at character level.

convolutions seem to work well
for low-level patterns
(e.g. morphology)

one interpretation: multiple
   lters can capture the low-level
idiosyncrasies of natural
language (e.g. arbitrary spelling)
whereas language is more
compositional at a higher level.

image credit: yoon kim and co-authors.

cf. kim et al., 2016

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

69 / 119

convnet-like architectures for composition

many other id98-like
architectures (e.g. bytenet from
kalchbrenner et al. (2016))

common recipe components:
dilated convolutions and resnet
blocks.

these model sequences well in
domains like speech, and are
beginning to    nd applications in
nlp, so worth reading up on.

image credit: nal kalchbrenner and co-authors.

cf. kalchbrenner et al., 2016

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

70 / 119

t0t1t2t3t4t5t6t7t8t9t11t12t13t14t15t16t10s0s1s2s3s4s5s6s7s8s9s10s11s12s13s14s15s16t11t12t13t14t15t16t17t10t9t8t7t6t5t4t3t2t1outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

71 / 119

unsupervised composition models

why care about unsupervised learning?

much more unlabelled linguistic data than labelled data.

learn general purpose representations and composition functions.

suitable pre-training for supervised models, semi-supervised, or
multi-task objectives.

in the (paraphrased) words of yann lecun: unsupervised learning is a
cake, supervised learning is frosting, and rl is the cherry on top!

plot twist: it   s possibly a cherry cake.

yes, that   s nice. . . but what are we doing, concretely?
good question! usually, just modelling   directly or indirectly   some
aspect of the id203 of the observed data.

further suggestions on a postcard, please!

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

72 / 119

autoencoders

autoencoders provide an unsupervised method for representation learning:

we minimise an objective function over inputs xi , i     n and their
reconstructions x(cid:48)
i :

n(cid:88)

(cid:13)(cid:13)x(cid:48)

i

(cid:13)(cid:13)2

i     xi

j =

1
2

warning: degenerate solution if xi can be updated (   i.xi = 0).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

73 / 119

recursive autoencoders

to auto-encode variable length
sequences, we can chain
autoencoders to create a recursive
structure.

objective function
minimizing the reconstruction error
will learn a compression function over
the inputs:

(cid:13)(cid:13)(cid:13)xi     x(cid:48)

i

(cid:13)(cid:13)(cid:13)2

cf. socher et al., 2011a

erec (i,   ) =

1
2

a    modern    alternative: use sequence to sequence model, and
log-likelihood objective.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

74 / 119

what   s wrong with auto-encoders?

empirically, narrow auto-encoders produce sharp latent codes, and
unregularised wide auto-encoders learn identity functions.

reconstruction objective includes nothing about distance preservation
in latent space: no guarantee that

dist(a, b)     dist(a, c)

    dist(encode(a), encode(b))     dist(encode(a), encode(c))

conversely, little incentive for similar latent codes to generate
radically di   erent (but semantically equivalent) observations.

ultimately, compression (cid:54)= meaning.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

75 / 119

skip-thought

image credit: jamie kiros and co-authors.

cf. kiros et al., 2015

similar to auto-encoding objective: encode sentence, but decode
neighbouring sentences.

pair of lstm-based id195 models with share encoder, but
alternative formulations are possible.

conceptually similar to id65: a unit   s
representation is a function of its neighbouring units, except units are
sentence instead of words.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

76 / 119

variational auto-encoders

semantically weak codes
generally, auto-encoders sparsely encode or densely compress information.
no pressure to ensure similarity continuum amongst codes.

factorized generative picture

p(x) =

(cid:90)
(cid:90)
=
= ep(z) [p(x|z)]

p(x, z)dz
p(x|z)p(z)dz

prior on z enforces semantic continuum (e.g. no arbitrarily unrelated codes
for similar data), but expectation is typically intractable to compute
exactly, and monte carlo estimate of gradients will be high variance.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

77 / 119

zxn(0, i)variational auto-encoders

goal
estimate, by maximising p(x):

the parameters    of a function modelling part of the generative
process p  (x|z) given samples from a    xed prior z     p(z).
the parameters    of a distribution q  (z|x) approximating the true
posterior p(z|x).

how do we do it? we maximise p(x) via a variational lower bound (vlb):

log p(x)     eq  (z|x) [log p  (x|z)]     dkl (q  (z|x)(cid:107)p(z))

equivalently we can minimise nll(x):

nll(x)     eq  (z|x)[nll  (x|z)] + dkl (q  (z|x)(cid:107)p(z))

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

78 / 119

variational auto-encoders

let   s derive the vlb:

log p(x) = log

= log

(cid:90)
(cid:90) q  (z|x)
(cid:20)

q  (z|x)

1    p  (x|z)p(z)dz

p  (x|z)p(z)dz

(cid:20) p(z)

(cid:21)

p  (x|z)

= log eq  (z|x)
    eq  (z|x)
= eq  (z|x) [log p  (x|z)]     dkl (q  (z|x)(cid:107)p(z))

q  (z|x)
p(z)
q  (z|x)

+ log p  (x|z)

log

(cid:21)

for right q  (z|x) and p(z) (e.g. gaussians) there is a closed-form
expression of dkl (q  (z|x)(cid:107)p(z)).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

79 / 119

variational auto-encoders

the problem of stochastic gradients
estimating    
samples z     q  (z|x). for some choices of q, such as gaussians there are
     
reparameterization tricks (cf. kingma et al., 2013)

eq  (z|x) [log p  (x|z)] requires backpropagating through

reparameterizing gaussians (kingma et al., 2013)

z     n(z;   ,   2)

equivalent to z =    +    
where       n( ; 0, i)

trivially:

   z
     

= 1

   z
     

=  

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

80 / 119

variational auto-encoders for sentences

1 observe a sentence w1, . . . , wn. encode it, e.g. with an lstm:

he = lstm e(w1, . . . , wn)

2 predict    = f   (he) and   2 = f   (he) (in practice we operate in log

space for   2 by determining log   ).
3 sample z     q(z|x) = n(z;   ,   2)
4 use conditional id56 to decode and measure log p(x|z). use

closed-form formula of kl divergence of two gaussians to calculate
   dkl (q  (z|x)(cid:107)p(z)). add both to obtain maximisation objective.

5 backpropagate gradient through decoder normally based on log
component of the objective, and use reparameterisation trick to
backpropagate through sampling operation back to encoder.

6 gradient of the kl divergence component of the loss with regard to

the encoder parameters is straightforward id26.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

81 / 119

variational auto-encoders and autoregressivity

the problem of powerful auto-regressive decoders
we want to minimise nll(x)     eq(z|x)[nll(x|z)] + dkl (q(z|x)(cid:107)p(z)).
what if the decoder is powerful enough to model x without using z?

a degenerate solution:

if z can be ignored when minimising the reconstruction loss of x given
z, the model can safely let q(z|x) collapse to the prior p(z) to
minimise dkl (q(z|x)(cid:107)p(z)).
since q need not depend on x (e.g. the encoder can just ignore x and
predict the mean and variance of the prior), z bears no relation to x.
result: useless encoder, useless latent variable.

is this really a problem?
if your decoder is not auto-regressive (e.g. mlps expressing the id203
of pixels which are conditionally independent given z), then no.
if your decoder is an id56 and domain has systematic patterns, then yes.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

82 / 119

variational auto-encoders and autoregressivity

what are some solutions to this problem?

pick a non-autoregressive decoder. if you care more about the latent
code than having a good generative model (e.g. document modelling),
this isn   t a bad idea, but frustrating if this is the only solution.
kl annealing: set eq(z|x)[nll(x|z)] +   dkl (q(z|x)(cid:107)p(z)) as
objective. start with    = 0 (basic id195 model). increase    to 1
over time during training. works somewhat, but unprincipled
changing of the objective function.
set as objective eq(z|x)[nll(x|z)] + max(  , dkl (q(z|x)(cid:107)p(z))) where
       0 is a scalar or vector hyperparameter. once the kl dips below   ,
there is no bene   t, so the model must rely on z to some extent. this
objective is still a valid upper bound on nll(x) (albeit a looser one).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

83 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

84 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

85 / 119

compositional or non-compositional representation

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

86 / 119

compositional or non-compositional representation

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

87 / 119

compositional or non-compositional representation

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

88 / 119

compositional or non-compositional representation

such    hard    or    soft    non-compositionalilty exists at di   erent
granularities of texts.

we will discuss some models on how to handle this at the
word-phrase level.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

88 / 119

compositional and non-id152

compositionality/non-compositionality is a common phenomenon in
language.

a framework that is able to consider both
compositionality/non-compositionality is of interest.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

89 / 119

compositional and non-id152

compositionality/non-compositionality is a common phenomenon in
language.

a framework that is able to consider both
compositionality/non-compositionality is of interest.

a pragmatic viewpoint: if one is able to obtain holistically the
representation of an id165 or a phrase in text, it would be desirable
that a composition model has the ability to decide the sources of
knowledge it will use.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

89 / 119

compositional and non-id152

compositionality/non-compositionality is a common phenomenon in
language.

a framework that is able to consider both
compositionality/non-compositionality is of interest.

a pragmatic viewpoint: if one is able to obtain holistically the
representation of an id165 or a phrase in text, it would be desirable
that a composition model has the ability to decide the sources of
knowledge it will use.
in addition to composition, considering non-compositionality may
avoid back-propagating errors unnecessarily to confuse word
embedding.

think about the    kick the bucket    example.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

89 / 119

integrating compositional and non-compositional
semantics

integrating non-compositionality in recursive networks (zhu et al., 2015a):
basic idea: enabling individual composition operations to be able to
choose information from di   erent resources, compositional or
non-compositional (e.g., holistically learned).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

90 / 119

integrating compositional and non-compositional
semantics

model 1: regular bilinear merge (zhu et al., 2015a):

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

91 / 119

integrating compositional and non-compositional
semantics

model 2: tensor-based merging (zhu et al., 2015a)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

92 / 119

integrating compositional and non-compositional
semantics

model 3: explicitly gated merging (zhu et al., 2015a):

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

93 / 119

experiment set-up

task: id31
data: stanford sentiment treebank
non-compositional sentiment

sentiment of ngrams automatically learned from tweets (mohammad
et al., 2013).

polled the twitter api every four hours from april to december 2012
in search of tweets with either a positive word hashtag or a negative
word hashtag.
using 78 seed hashtags (32 positive and 36 negative) such as #good,
#excellent, and #terrible to annotate sentiment.
775,000 tweets that contain at least a positive hashtag or a negative
hashtag were used as the learning corpus.
point-wise mutual information (pmi) is calculated for each bigrams
and trigrams.
each sentiment score is converted to a one-hot vector; e.g. a bigram
with a score of -1.5 will be assigned a 5-dimensional vector [0, 1, 0, 0,
0] (i.e., the e vector).

using the human annotation coming with stanford sentiment
treebank for bigrams and trigrams.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

94 / 119

results

models
(1) rntn
(2) regular-bilinear (auto)
(3) regular-bilinear (manu)
(4) explicitly-gated (auto)
(5) explicitly-gated (manu)
(6) con   ned-tensor (auto)
(7) con   ned-tensor (manu)

sentence-level (roots)

all phrases (all nodes)

42.44
42.37
42.98
42.58
43.21
42.99
43.75   

79.95
79.97
80.14
80.06
80.21
80.49
80.66   

table: model performances (accuracy) on predicting 5-category sentiment at the
sentence (root) level and phrase level.

1the results is based on the version 3.3.0 of the stanford corenlp.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

95 / 119

integrating compositional and non-compositional
semantics

we have discussed integrating non-compositionality in recursive
networks.
how if there are no prior input structures available?

remember we have discussed the models that capture hidden
structures.

how if a syntactic parsing tree is not very reliable?

e.g., for data like social media text or speech transcripts.

in these situations, how can we still consider non-compositionality in
the composition process.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

96 / 119

integrating compositional and non-compositional
semantics

integrating non-compositionality in chain recurrent networks (zhu et al.,
2016)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

97 / 119

integrating compositional and non-compositional
semantics

non-compositional nodes:

form the non-compositional paths (e.g., 3-8-9 or 4-5-9).

allow the embedding spaces of a non-compositional node to be
di   erent from those of a compositional node.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

98 / 119

integrating compositional and non-compositional
semantics

fork nodes:

summarizing history so far to
support both compositional and
non-compositional paths.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

99 / 119

integrating compositional and non-compositional
semantics

merging nodes:

combining information from compositional and non-compositional paths.

binarization

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

100 / 119

integrating compositional and non-compositional
semantics

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

101 / 119

integrating compositional and non-compositional
semantics

binarization:

binarizing the composition of in-bound
paths (we do not worry too much about
the order of merging.)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

101 / 119

now we do not need to design di   erent
nodes for di   erent fan-in, but let
parameter-sharing be all over the nets.

use the tree-lstm above.

results

method
majority baseline
unigram (id166)
3rd best model
2nd best model
the best model
dag-lstm

semeval-13 semeval-14

29.19
56.95
64.86
65.27
69.02
70.88

34.46
58.58
69.95
70.14
70.96
71.97

table: performances of di   erent models in o   cial evaluation metric (macro
f-scores) on the test sets of semeval-2013 and semeval-2014 id31
in twitter in predicting the sentiment of the tweet messages.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

102 / 119

results

method
dag-lstm
full paths
full     {autopaths}
full     {tripaths}
full     {tripaths, bipaths}
full     {manupaths}
lstm without dag
full     {autopaths,manupaths}

semeval-13

semeval-14

70.88
69.36
70.16
69.55
69.88

64.00

71.97
69.27
70.77
69.93
70.58

66.40

table: ablation performances (macro-averaged f-scores) of dag-lstm with
di   erent types of paths being removed.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

103 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

104 / 119

subword composition

composition can also be performed to learn representations for words
from subword components (botha et al., 2014; ling et al., 2015;
luong et al., 2015; kim et al., 2016; sennrich et al., 2016).

rich morphology: some languages have larger vocabularies than others.
informal text: very coooooool!

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

105 / 119

subword composition

composition can also be performed to learn representations for words
from subword components (botha et al., 2014; ling et al., 2015;
luong et al., 2015; kim et al., 2016; sennrich et al., 2016).

rich morphology: some languages have larger vocabularies than others.
informal text: very coooooool!
basically alleviate sparseness!

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

105 / 119

subword composition

composition can also be performed to learn representations for words
from subword components (botha et al., 2014; ling et al., 2015;
luong et al., 2015; kim et al., 2016; sennrich et al., 2016).

rich morphology: some languages have larger vocabularies than others.
informal text: very coooooool!
basically alleviate sparseness!

one perspective of viewing subword models:

morpheme based composition: deriving word representation from
morphemes.
character based composition: deriving word representation from
characters (pretty e   ective as well, even used by itself!)

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

105 / 119

subword composition

composition can also be performed to learn representations for words
from subword components (botha et al., 2014; ling et al., 2015;
luong et al., 2015; kim et al., 2016; sennrich et al., 2016).

rich morphology: some languages have larger vocabularies than others.
informal text: very coooooool!
basically alleviate sparseness!

one perspective of viewing subword models:

morpheme based composition: deriving word representation from
morphemes.
character based composition: deriving word representation from
characters (pretty e   ective as well, even used by itself!)

another perspective (by model architectures):

recursive models
convolutional models
recurrent models

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

105 / 119

subword composition

composition can also be performed to learn representations for words
from subword components (botha et al., 2014; ling et al., 2015;
luong et al., 2015; kim et al., 2016; sennrich et al., 2016).

rich morphology: some languages have larger vocabularies than others.
informal text: very coooooool!
basically alleviate sparseness!

one perspective of viewing subword models:

morpheme based composition: deriving word representation from
morphemes.
character based composition: deriving word representation from
characters (pretty e   ective as well, even used by itself!)

another perspective (by model architectures):

recursive models
convolutional models
recurrent models

we will discuss several typical methods here only brie   y.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

105 / 119

subword composition: recursive networks

morphological id56s (luong et al., 2013):
extending id56s (socher et al., 2011b) to learn word
representation through composition over morphemes.

assume the availability of morphemic analyses.
each tree node combines a stem vector and an a   x vector.

figure. context insensitive (left) and sensitive (right) morphological

id56s.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

106 / 119

subword composition: recurrent networks

bi-directional lstm for subword composition (ling et al., 2015).

figure. character id56 for sub-word composition.

some more details ...
xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

107 / 119

subword composition: convolutional networks

convolutional neural networks for subword composition (zhang et al.,
2015)

figure. character id98 for sub-word composition.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

108 / 119

subword composition: convolutional networks

convolutional neural networks for subword composition (zhang et al.,
2015)

figure. character id98 for sub-word composition.

in general, subword models have been successfully used in a wide
variety of problems such as translation, id31, question
answering, etc.
you should seriously consider it in the situations such as oov is high
or the word distribution has a long tail.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

108 / 119

outline

1

introduction

semantic composition
formal methods
simple parametric models

2 parameterizing composition functions

recurrent composition models
recursive composition models
convolutional composition models
unsupervised models

3 selected topics

compositionality and non-compositionality
subword composition methods

4 summary

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

109 / 119

summary

the tutorial discusses semantic composition with distributed
representation learned with neural networks.
neural networks are able to learn powerful representation and
complicated composition functions.

the models can achieve state-of-the-art performances on a wide
range of nlp tasks.

we expect further studies would continue to deepen our
understanding on such approaches:

unsupervised models
compositionality with other    ingredients    of intelligence
compositionality in multi-modalities
interpretability of models
distributed vs./and symbolic composition models
... ...

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

110 / 119

references i

c. e. osgood, g. j. suci, and p. h. tannenbaum. the
measurement of meaning. university of illinois press, 1957.

richard montague.    english as a formal language   .
nella societa e nella tecnica. ed. by bruno visentini. edizioni di
communita, 1970, pp. 188   221.

in: linguaggi

g. a. miller and p. n. johnson-laird. language and perception.
cambridge, ma: belknap press, 1976.

j. a. fodor and z. w. pylyshyn.    connectionism and cognitive
architecture: a critical analysis   .

in: cognition 28 (1988), pp. 3   71.

jordan b. pollack.    recursive distributed representations   .
artif. intell. 46.1-2 (1990), pp. 77   105.

in:

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

111 / 119

references ii

ronald j. williams.    simple statistical gradient-following
algorithms for connectionist id23   .
learning 8 (1992), pp. 229   256.

in: machine

barbara partee.    lexical semantics and compositionality   .
invitation to cognitive science 1 (1995), pp. 311   360.

in:

elie bienenstock, stuart geman, and daniel potter.
   compositionality, mdl priors, and object recognition   .
1996.

in: nips.

enrico francesconi et al.    logo recognition by recursive neural
networks   .

in: grec. 1997.

je    mitchell and mirella lapata.    vector-based models of semantic
composition   .

in: acl. 2008, pp. 236   244.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

112 / 119

references iii

richard socher et al.    dynamic pooling and unfolding recursive
autoencoders for paraphrase detection   .
pp. 801   809.

in: nips. 2011,

richard socher et al.    parsing natural scenes and natural language
with id56s   .

in: icml. 2011, pp. 129   136.

richard socher et al.    semi-supervised recursive autoencoders for
predicting sentiment distributions   .

in: emnlp. 2011.

richard socher et al.    semantic compositionality through recursive
matrix-vector spaces   .

in: emnlp-conll. 2012, pp. 1201   1211.

nal kalchbrenner and phil blunsom.    recurrent continuous
translation models.   .
in: emnlp. vol. 3. 39. 2013, p. 413.

diederik p. kingma and max welling.    auto-encoding variational
bayes   .

in: corr abs/1312.6114 (2013).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

113 / 119

references iv

thang luong, richard socher, and christopher d. manning.
   better word representations with id56s for
morphology   .

in: conll. 2013.

saif mohammad, svetlana kiritchenko, and xiao-dan zhu.
   nrc-canada: building the state-of-the-art in id31
of tweets   .

in: semeval@naacl-hlt. 2013.

richard socher et al.    recursive deep models for semantic
compositionality over a sentiment treebank   .
pp. 1631   1642.

in: emnlp. 2013,

dzmitry bahdanau, kyunghyun cho, and yoshua bengio.    neural
machine translation by jointly learning to align and translate   .
in:
arxiv preprint arxiv:1409.0473 (2014).

jan a. botha and phil blunsom.    compositional morphology for
word representations and language modelling   .

in: icml. 2014.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

114 / 119

references v

nal kalchbrenner, edward grefenstette, and phil blunsom.    a
convolutional neural network for modelling sentences   .
preprint arxiv:1404.2188 (2014).

in: arxiv

ilya sutskever, oriol vinyals, and quoc v le.    sequence to
sequence learning with neural networks   .
information processing systems. 2014, pp. 3104   3112.

in: advances in neural

xiaodan zhu et al.    an empirical study on the e   ect of negation
words on sentiment   .

in: acl. 2014.

ryan kiros et al.    skip-thought vectors   .
information processing systems. 2015, pp. 3294   3302.

in: advances in neural

phong le and willem zuidema.    compositional distributional
semantics with long short term memory   .
*sem@naacl-hlt. 2015.

in:

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

115 / 119

references vi

wang ling et al.    finding function in form: compositional
character models for open vocabulary word representation   .
2015.

in:

thang luong et al.    addressing the rare word problem in neural
machine translation   .

in: acl. 2015.

sheng kai tai, richard socher, and d. christopher manning.
   improved semantic representations from tree-structured long
short-term memory networks   .
in: acl. 2015, pp. 1556   1566.

xiang zhang and yann lecun.    text understanding from scratch   .
in: corr abs/1502.01710 (2015).

xiaodan zhu, hongyu guo, and parinaz sobhani.    neural networks
for integrating compositional and non-compositional sentiment in
sentiment composition   .

in: *sem@naacl-hlt. 2015.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

116 / 119

references vii

xiaodan zhu, parinaz sobhani, and hongyu guo.    long short-term
memory over recursive structures   .
pp. 1604   1612.

in: icml. 2015,

samuel r bowman et al.    a fast uni   ed model for parsing and
sentence understanding   .
in: arxiv preprint arxiv:1603.06021
(2016).

ian goodfellow, yoshua bengio, and aaron courville. deep learning.
mit press, 2016.

nal kalchbrenner et al.    id4 in linear
time   .

in: corr abs/1610.10099 (2016).

yoon kim et al.    character-aware neural language models   .
aaai. 2016.

in:

b. m. lake et al.    building machines that learn and think like
people   .

in: behavioral and brain sciences. (in press). (2016).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

117 / 119

references viii

tsendsuren munkhdalai and hong yu.    neural tree indexers for
text understanding   .

in: corr abs/1607.04492 (2016).

rico sennrich, barry haddow, and alexandra birch.    neural
machine translation of rare words with subword units   .
2016.

in: acl.

dani yogatama et al.    learning to compose words into sentences
with id23   .
in: corr abs/1611.09100 (2016).

xiaodan zhu, parinaz sobhani, and hongyu guo.    dag-structured
long short-term memory for semantic compositionality   .
naacl. 2016.

in:

qian chen et al.    enhanced lstm for natural language id136   .
in: acl. 2017.

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

118 / 119

references ix

williams, nikita nangia, and samuel r. bowman.    a
broad-coverage challenge corpus for sentence understanding
through id136   .

in: corr abs/1704.05426 (2017).

xiaodan zhu & edward grefenstette

dl for composition

july 30th, 2017

119 / 119

