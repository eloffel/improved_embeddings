tutorial: deep id23

david silver, google deepmind

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

id23 in a nutshell

rl is a general-purpose framework for decision-making

(cid:73) rl is for an agent with the capacity to act
(cid:73) each action in   uences the agent   s future state
(cid:73) success is measured by a scalar reward signal
(cid:73) goal: select actions to maximise future reward

deep learning in a nutshell

dl is a general-purpose framework for representation learning

(cid:73) given an objective
(cid:73) learn representation that is required to achieve objective
(cid:73) directly from raw inputs
(cid:73) using minimal domain knowledge

deep id23: ai = rl + dl

we seek a single agent which can solve any human-level task

(cid:73) rl de   nes the objective
(cid:73) dl gives the mechanism
(cid:73) rl + dl = general intelligence

examples of deep rl @deepmind

(cid:73) play games: atari, poker, go, ...
(cid:73) navigate worlds: 3d worlds, labyrinth, ...
(cid:73) control physical systems: manipulate, walk, swim, ...
(cid:73) interact with users: recommend, optimise, personalise, ...

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

deep representations

(cid:73) a deep representation is a composition of many functions

x

/ h1

w1

/ ...

...

/ hn

wn

/ y

/ l

(cid:73) its gradient can be backpropagated by the chain rule

   h1
   x

   l
   x

   h2
   h1

   l
   h1

   h1
   w1  
   l
   w1

...

...

   hn
   hn   1

   y
   hn

   l
   y

   l
   hn

   hn
   wn  
   l
   wn

/
/
/
/
/
o
o
o
o
o
o
 
o
o
o
o
 
o
o
deep neural network

a deep neural network is typically composed of:

(cid:73) linear transformations

hk+1 = whk

(cid:73) non-linear id180

hk+2 = f (hk+1)

(cid:73) a id168 on the output, e.g.
    y||2

(cid:73) mean-squared error l = ||y   
(cid:73) log likelihood l = log p [y   ]

training neural networks by stochastic id119

(cid:73) sample gradient of expected loss l(w) = e [l]

(cid:20)    l

(cid:21)

   w

   l

   w     e

=

   l(w)

   w

(cid:73) adjust w down the sampled gradient

   w    

   l
   w

!"#$%"&'(%")*+,#'-'!"#$%%"(%")*+,#'.+/0+,#(cid:121)!"#$%&'('%$&#()&*+$,*$#&&-&$$$$."'%"$'*$%-/0'*,('-*$.'("$("#$1)*%('-*$,22&-3'/,(-&%,*$0#$)+#4$(-$%&#,(#$,*$#&&-&$1)*%('-*$$$$$$$$$$$$$(cid:121)!"#$2,&(',5$4'11#&#*(',5$-1$("'+$#&&-&$1)*%('-*$$$$$$$$$$$$$$$$6$("#$7&,4'#*($%,*$*-.$0#$)+#4$(-$)24,(#$("#$'*(#&*,5$8,&',05#+$'*$("#$1)*%('-*$,22&-3'/,(-&9,*4$%&'('%:;$$$$$$<&,4'#*($4#+%#*($=>?weight sharing

recurrent neural network shares weights between time-steps

yt

/ ht

xt

...

w

yt+1

ht+1

...

w

xt+1

convolutional neural network shares weights between local regions

w1w1w2w2xh1h2/
/
/
o
o
/
/
o
o
?
?
o
o
=
=
o
o
outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

many faces of id23

computer scienceeconomicsmathematicsengineeringneurosciencepsychologymachine learningclassical/operantconditioningoptimal controlrewardsystemoperations researchrationality/game theoryid23agent and environment

(cid:73) at each step t the agent:

(cid:73) executes action at
(cid:73) receives observation ot
(cid:73) receives scalar reward rt

(cid:73) the environment:

(cid:73) receives action at
(cid:73) emits observation ot+1
(cid:73) emits scalar reward rt+1

observationrewardactionatrtotstate

(cid:73) experience is a sequence of observations, actions, rewards

o1, r1, a1, ..., at   1, ot, rt

(cid:73) the state is a summary of experience

st = f (o1, r1, a1, ..., at   1, ot, rt)

(cid:73) in a fully observed environment

st = f (ot)

major components of an rl agent

(cid:73) an rl agent may include one or more of these components:

(cid:73) policy: agent   s behaviour function
(cid:73) value function: how good is each state and/or action
(cid:73) model: agent   s representation of the environment

policy

(cid:73) a policy is the agent   s behaviour
(cid:73) it is a map from state to action:
(cid:73) deterministic policy: a =   (s)
(cid:73) stochastic policy:   (a|s) = p [a|s]

value function

(cid:73) a value function is a prediction of future reward

(cid:73)    how much reward will i get from action a in state s?   

(cid:73) q-value function gives expected total reward

(cid:73) from state s and action a
(cid:73) under policy   
(cid:73) with discount factor   

q   (a|s) = e(cid:2)rt+1 +   rt+2 +   2rt+3 + ... | s, a(cid:3)

value function

(cid:73) a value function is a prediction of future reward

(cid:73)    how much reward will i get from action a in state s?   

(cid:73) q-value function gives expected total reward

(cid:73) from state s and action a
(cid:73) under policy   
(cid:73) with discount factor   

q   (a|s) = e(cid:2)rt+1 +   rt+2 +   2rt+3 + ... | s, a(cid:3)
q   (a|s) = es(cid:48),a(cid:48)(cid:2)r +   q   (a(cid:48)
|s(cid:48)) | s, a(cid:3)

(cid:73) value functions decompose into a bellman equation

optimal value functions

(cid:73) an optimal value function is the maximum achievable value

q   (s, a) = max

  

q   (s, a) = q      

(s, a)

optimal value functions

(cid:73) an optimal value function is the maximum achievable value

q   (s, a) = max

  

q   (s, a) = q      

(s, a)

(cid:73) once we have q    we can act optimally,

  

   (s) = argmax

a

q   (s, a)

optimal value functions

(cid:73) an optimal value function is the maximum achievable value

q   (s, a) = max

  

q   (s, a) = q      

(s, a)

(cid:73) once we have q    we can act optimally,

  

   (s) = argmax

a

q   (s, a)

(cid:73) optimal value maximises over all decisions. informally:

q   (s, a) = rt+1 +    max

at+1
= rt+1 +    max
at+1

rt+2 +   2 max
at+2
q   (st+1, at+1)

rt+3 + ...

optimal value functions

(cid:73) an optimal value function is the maximum achievable value

q   (s, a) = max

  

q   (s, a) = q      

(s, a)

(cid:73) once we have q    we can act optimally,

  

   (s) = argmax

a

q   (s, a)

(cid:73) optimal value maximises over all decisions. informally:

q   (s, a) = rt+1 +    max

at+1
= rt+1 +    max
at+1

rt+2 +   2 max
at+2
q   (st+1, at+1)

rt+3 + ...

(cid:73) formally, optimal values decompose into a bellman equation

(cid:20)

(cid:21)

q   (s, a) = es(cid:48)

r +    max

a(cid:48) q   (s(cid:48)

, a(cid:48)) | s, a

value function demo

model

observationrewardactionatrtotmodel

(cid:73) model is learnt from experience
(cid:73) acts as proxy for environment
(cid:73) planner interacts with model
(cid:73) e.g. using lookahead search

observationrewardactionatrtotapproaches to id23

value-based rl

(cid:73) estimate the optimal value function q   (s, a)
(cid:73) this is the maximum value achievable under any policy

policy-based rl

(cid:73) search directly for the optimal policy      
(cid:73) this is the policy achieving maximum future reward

model-based rl

(cid:73) build a model of the environment
(cid:73) plan (e.g. by lookahead) using model

deep id23

(cid:73) use deep neural networks to represent

(cid:73) value function
(cid:73) policy
(cid:73) model

(cid:73) optimise id168 by stochastic id119

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

q-networks

represent value function by q-network with weights w

q(s, a, w)     q   (s, a)

ssaq(s,a,w)q(s,a1,w)q(s,am,w)   wwid24

(cid:21)

(cid:20)

(cid:73) optimal q-values should obey bellman equation

q   (s, a) = es(cid:48)

r +    max

a(cid:48) q(s(cid:48)

, a(cid:48))   

| s, a

(cid:73) treat right-hand side r +    max
(cid:73) minimise mse loss by stochastic id119

a(cid:48) q(s(cid:48), a(cid:48), w) as a target
(cid:17)2

(cid:16)

l =

r +    max

a

q(s(cid:48)

, a(cid:48)

, w)     q(s, a, w)

id24

(cid:21)

(cid:20)

(cid:73) optimal q-values should obey bellman equation

q   (s, a) = es(cid:48)

r +    max

a(cid:48) q(s(cid:48)

, a(cid:48))   

| s, a

(cid:73) treat right-hand side r +    max
(cid:73) minimise mse loss by stochastic id119

a(cid:48) q(s(cid:48), a(cid:48), w) as a target
(cid:17)2

(cid:16)

l =

r +    max

a

q(s(cid:48)

, a(cid:48)

, w)     q(s, a, w)

(cid:73) converges to q    using table lookup representation

id24

(cid:21)

(cid:20)

(cid:73) optimal q-values should obey bellman equation

q   (s, a) = es(cid:48)

r +    max

a(cid:48) q(s(cid:48)

, a(cid:48))   

| s, a

(cid:73) treat right-hand side r +    max
(cid:73) minimise mse loss by stochastic id119

a(cid:48) q(s(cid:48), a(cid:48), w) as a target
(cid:17)2

(cid:16)

l =

r +    max

a

q(s(cid:48)

, a(cid:48)

, w)     q(s, a, w)

(cid:73) converges to q    using table lookup representation
(cid:73) but diverges using neural networks due to:

(cid:73) correlations between samples
(cid:73) non-stationary targets

deep q-networks (id25): experience replay

to remove correlations, build data-set from agent   s own experience

s1, a1, r2, s2
s2, a2, r3, s3
s3, a3, r4, s4

...

    s, a, r , s(cid:48)

st, at, rt+1, st+1     st, at, rt+1, st+1
sample experiences from data-set and apply update

(cid:18)

(cid:19)2

l =

r +    max

a(cid:48) q(s(cid:48)

, a(cid:48)

, w   )     q(s, a, w)

to deal with non-stationarity, target parameters w    are held    xed

deep id23 in atari

staterewardactionatrtstid25 in atari

(cid:73) end-to-end learning of values q(s, a) from pixels s
(cid:73) input state s is stack of raw pixels from last 4 frames
(cid:73) output is q(s, a) for 18 joystick/button positions
(cid:73) reward is change in score for that step

network architecture and hyperparameters    xed across all games

id25 results in atari

id25 atari demo

id25 paper
www.nature.com/articles/nature14236

id25 source code:
sites.google.com/a/deepmind.com/id25/

nature.com/nature26 february 2015   10vol. 518, no. 7540epidemiologyshare data in outbreaksforge open access to sequences and more page 477cosmologya giant in the early universea supermassive black hole at a redshift of 6.3pages 490 & 512quantum physicsteleportation for twotransferring two properties of a single photon pages 491 & 516innovations inthe microbiomeself-taught ai software attains human-level performance in video games  pages 486 & 529the international weekly journal of scienceimprovements since nature id25

(cid:73) double id25: remove upward bias caused by max
a

(cid:73) current q-network w is used to select actions
(cid:73) older q-network w    is used to evaluate actions

q(s, a, w)

(cid:19)2

(cid:18)

l =

r +   q(s(cid:48)

, argmax

a(cid:48)

q(s(cid:48)

, a(cid:48)

, w), w   )     q(s, a, w)

improvements since nature id25

(cid:73) double id25: remove upward bias caused by max
a

(cid:73) current q-network w is used to select actions
(cid:73) older q-network w    is used to evaluate actions

q(s, a, w)

(cid:19)2

(cid:18)

l =

r +   q(s(cid:48)

, w), w   )     q(s, a, w)
(cid:73) prioritised replay: weight experience according to surprise
(cid:73) store experience in priority queue according to id25 error

, argmax

q(s(cid:48)

, a(cid:48)

a(cid:48)

(cid:12)(cid:12)(cid:12)r +    max

a(cid:48) q(s(cid:48)

, a(cid:48)

, w   )     q(s, a, w )

(cid:12)(cid:12)(cid:12)

improvements since nature id25

(cid:18)

(cid:73) double id25: remove upward bias caused by max
a

(cid:73) current q-network w is used to select actions
(cid:73) older q-network w    is used to evaluate actions

q(s, a, w)

l =

r +   q(s(cid:48)

, w), w   )     q(s, a, w)
(cid:73) prioritised replay: weight experience according to surprise
(cid:73) store experience in priority queue according to id25 error

, argmax

q(s(cid:48)

, a(cid:48)

a(cid:48)

(cid:19)2

(cid:12)(cid:12)(cid:12)r +    max

, w   )     q(s, a, w )
(cid:73) duelling network: split q-network into two channels

a(cid:48) q(s(cid:48)

, a(cid:48)

(cid:73) action-independent value function v (s, v )
(cid:73) action-dependent advantage function a(s, a, w)

q(s, a) = v (s, v ) + a(s, a, w)

(cid:12)(cid:12)(cid:12)

improvements since nature id25

(cid:18)

(cid:73) double id25: remove upward bias caused by max
a

(cid:73) current q-network w is used to select actions
(cid:73) older q-network w    is used to evaluate actions

q(s, a, w)

l =

r +   q(s(cid:48)

, w), w   )     q(s, a, w)
(cid:73) prioritised replay: weight experience according to surprise
(cid:73) store experience in priority queue according to id25 error

, argmax

q(s(cid:48)

, a(cid:48)

a(cid:48)

(cid:19)2

(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)r +    max

, w   )     q(s, a, w )
(cid:73) duelling network: split q-network into two channels

a(cid:48) q(s(cid:48)

, a(cid:48)

(cid:73) action-independent value function v (s, v )
(cid:73) action-dependent advantage function a(s, a, w)

q(s, a) = v (s, v ) + a(s, a, w)

combined algorithm: 3x mean atari score vs nature id25

gorila (general id23 architecture)

(cid:73) 10x faster than nature id25 on 38 out of 49 atari games
(cid:73) applied to recommender systems within google

asynchronous id23

(cid:73) exploits multithreading of standard cpu
(cid:73) execute many instances of agent in parallel
(cid:73) network parameters shared between threads
(cid:73) parallelism decorrelates data

(cid:73) viable alternative to experience replay
(cid:73) parallelism decorrelates data

(cid:73) similar speedup to gorila - on a single machine!

outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

deep policy networks

(cid:73) represent policy by deep network with weights u

a =   (a|s, u) or a =   (s, u)

(cid:73) de   ne objective function as total discounted reward

l(u) = e(cid:2)r1 +   r2 +   2r3 + ... |   (  , u)(cid:3)

(cid:73) optimise objective end-to-end by sgd
(cid:73) i.e. adjust policy parameters u to achieve more reward

policy gradients

how to make high-value actions more likely:

(cid:73) the gradient of a stochastic policy   (a|s, u) is given by

(cid:20)    log   (a|s, u)

   u

(cid:21)

q   (s, a)

   l(u)

   u

= e

policy gradients

how to make high-value actions more likely:

(cid:73) the gradient of a stochastic policy   (a|s, u) is given by

(cid:21)

q   (s, a)

(cid:20)    log   (a|s, u)
(cid:20)    q   (s, a)

   u

= e

   a

(cid:21)

   a
   u

   l(u)

   u

= e

   l(u)

   u

(cid:73) the gradient of a deterministic policy a =   (s) is given by

(cid:73) if a is continuous and q is di   erentiable

actor-critic algorithm

(cid:73) estimate value function q(s, a, w)     q   (s, a)
(cid:73) update policy parameters u by stochastic gradient ascent

or

   l
   u

=

   log   (a|s, u)

   u

q(s, a, w)

   l
   u

=

   q(s, a, w)

   a

   a
   u

asynchronous advantage actor-critic (a3c)

(cid:73) estimate state-value function

v (s, v)     e [rt+1 +   rt+2 + ...|s]

(cid:73) q-value estimated by an n-step sample

q(st, at) = rt+1 +   rt+2... +   n   1rt+n +   nv (st+n, v)

asynchronous advantage actor-critic (a3c)

(cid:73) estimate state-value function

v (s, v)     e [rt+1 +   rt+2 + ...|s]

(cid:73) q-value estimated by an n-step sample

q(st, at) = rt+1 +   rt+2... +   n   1rt+n +   nv (st+n, v)

(cid:73) actor is updated towards target
   log   (at|st, u)

=

   lu
   u

   u

(q(st, at)     v (st, v))

(cid:73) critic is updated to minimise mse w.r.t. target

lv = (q(st, at)     v (st, v))2

asynchronous advantage actor-critic (a3c)

(cid:73) estimate state-value function

v (s, v)     e [rt+1 +   rt+2 + ...|s]

(cid:73) q-value estimated by an n-step sample

q(st, at) = rt+1 +   rt+2... +   n   1rt+n +   nv (st+n, v)

(cid:73) actor is updated towards target
   log   (at|st, u)

=

   lu
   u

   u

(q(st, at)     v (st, v))

(cid:73) critic is updated to minimise mse w.r.t. target

lv = (q(st, at)     v (st, v))2

(cid:73) 4x mean atari score vs nature id25

deep id23 in labyrinth

a3c in labyrinth

(cid:73) end-to-end learning of softmax policy   (a|st) from pixels
(cid:73) observations ot are raw pixels from current frame
(cid:73) state st = f (o1, ..., ot) is a recurrent neural network (lstm)
(cid:73) outputs both value v (s) and softmax over actions   (a|s)
(cid:73) task is to collect apples (+1 reward)

deepreinforcementlearninginlabyrinthdeepreinforcementlearninginlabyrinthdeepreinforcementlearninginlabyrinthstst+1st-1ot-1otot+1  (a|st-1)  (a|st)  (a|st+1)v(st-1)v(st)v(st-1)a3c labyrinth demo

demo:
www.youtube.com/watch?v=nmr5mjcfzcw&feature=youtu.be

labyrinth source code (coming soon):
sites.google.com/a/deepmind.com/labyrinth/

deep id23 with continuous actions

how can we deal with high-dimensional continuous action spaces?

(cid:73) can   t easily compute max
a

q(s, a)

(cid:73) actor-critic algorithms learn without max

(cid:73) q-values are di   erentiable w.r.t a

(cid:73) deterministic policy gradients exploit knowledge of    q
   a

deep dpg

dpg is the continuous analogue of id25

(cid:18)

(cid:19)2

(cid:73) experience replay: build data-set from agent   s experience
(cid:73) critic estimates value of current policy by id25

lw =

r +   q(s(cid:48)

,   (s(cid:48)

, u   ), w   )     q(s, a, w)

to deal with non-stationarity, targets u   , w    are held    xed

(cid:73) actor updates policy in direction that improves q

   lu
   u

=

   q(s, a, w)

   a

   a
   u

(cid:73) in other words critic provides id168 for actor

dpg in simulated physics

(cid:73) physics domains are simulated in mujoco
(cid:73) end-to-end learning of control policy from raw pixels s
(cid:73) input state s is stack of raw pixels from last 4 frames
(cid:73) two separate convnets are used for q and   
(cid:73) policy    is adjusted in direction that most improves q

q(s,a)  (s)adpg in simulated physics demo

(cid:73) demo: dpg from pixels

a3c in simulated physics demo

(cid:73) asynchronous rl is viable alternative to experience replay
(cid:73) train a hierarchical, recurrent locomotion controller
(cid:73) retrain controller on more challenging tasks

fictitious self-play (fsp)

can deep rl    nd nash equilibria in multi-agent games?

(cid:73) q-network learns    best response    to opponent policies

(cid:73) by applying id25 with experience replay
(cid:73) c.f.    ctitious play

(cid:73) policy network   (a|s, u) learns an average of best responses

   l
   u

=

   log   (a|s, u)

   u

(cid:73) actions a sample mix of policy network and best response

neural fsp in texas hold   em poker

(cid:73) heads-up limit texas hold   em
(cid:73) nfsp with raw inputs only (no prior knowledge of poker)
(cid:73) vs smooct (3x medal winner 2015, handcrafted knowlege)

-800-700-600-500-400-300-200-100 0 100 0 5e+06 1e+07 1.5e+07 2e+07 2.5e+07 3e+07 3.5e+07mbb/hiterationssmooctnfsp, best response strategynfsp, greedy-average strategynfsp, average strategy(a)winratesagainstsmooct.theestimatedstan-darderrorofeachevaluationislessthan10mbb/h.match-upnfspescabeche-52.1  8.5smooct-17.4  9.0hyperborean-13.6  9.2(b)winratesofnfsp   sgreedy-averagestrategyagainstthetop3agentsoftheacpc2014.figure2:performanceofnfspinlimittexashold   em.1024,512,1024and512neuronswithrecti   edlinearactivations.thememorysizesweresetto255600kand30mformrlandmslrespectively.mrlfunctionedasacircularbuffercontaininga256recentwindowofexperience.mslwasupdatedwithexponentially-averagedreservoirsampling257(osborneetal.,2014),replacingentriesinmslwithminimumid2030.25.weusedvanilla258sgdwithoutmomentumforbothreinforcementandsupervisedlearning,withlearningratessetto2590.1and0.01respectively.eachagentperformed2stochasticgradientupdatesofmini-batchsize256260pernetworkforevery256stepsinthegame.thetargetnetworkwasre   ttedevery1000updates.261nfsp   santicipatoryparameterwassetto   =0.1.the   -greedypolicies   explorationstartedat0.08262anddecayedto0,moreslowlythaninleduchold   em.inadditiontonfsp   smain,averagestrategy263pro   lewealsoevaluatedthebestresponseandgreedy-averagestrategies,whichdeterministically264chooseactionsthatmaximizethepredictedactionvaluesorprobabilitiesrespectively.265toprovidesomeintuitionforwinratesinheads-uplhe,aplayerthatalwaysfoldswilllose750266mbb/h,andexperthumanplayerstypicallyachieveexpectedwinratesof40-60mbb/hatonline267high-stakesgames.similarly,thetophalfofcomputeragentsintheacpc2014achievedupto50268mbb/hbetweenthemselves.whiletraining,weperiodicallyevaluatednfsp   sperformanceagainst269smooctfromsymmetricplayfor25000handseach.figure2apresentsthelearningperformanceof270nfsp.nfsp   saverageandgreedy-averagestrategypro   lesexhibitastableandrelativelymonotonic271performanceimprovement,andachievewinratesofaround-50and-20mbb/hrespectively.the272bestresponsestrategypro   leexhibitedmorenoisyperformance,mostlyrangingbetween-50and0273mbb/h.wealsoevaluatedthe   nalgreedy-averagestrategyagainsttheothertop3competitorsof274theacpc2014.table2bpresentstheresults.nfspachieveswinratessimilartothoseofthetop275halfofcomputeragentsintheacpc2014andthusiscompetitivewithsuperhumancomputepoker276programs.2775relatedwork278relianceonhumanexpertknowledgecanbeexpensive,pronetohumanbiasesandlimitingifsuch279knowledgeissuboptimal.yetmanymethodsthathavebeenappliedtogameshavereliedonhuman280expertknowledge.deepblueusedahuman-engineeredevaluationfunctionforchess(campbelletal.,2812002).incomputergo,maddisonetal.(2015)andclarkandstorkey(2015)traineddeepneural282networksfromdataofexperthumanplay.incomputerpoker,currentgame-theoreticapproaches283useheuristicsofcardstrengthtoabstractthegametoatractablesize(zinkevichetal.,2007;gilpin284etal.,2007;johansonetal.,2013).waughetal.(2015)recentlycombinedoneofthesemethods285withfunctionapproximation.however,theirfull-widthalgorithmhastoimplicitlyreasonaboutall286informationstatesateachiteration,whichisprohibitivelyexpensiveinlargedomains.incontrast,287nfspfocusesonthesample-basedreinforcementlearningsettingwherethegame   sstatesneednot288beexhaustivelyenumeratedandthelearnermaynotevenhaveamodelofthegame   sdynamics.289nashequilibriaaretheonlystrategypro   lesthatrationalagentscanhopetoconvergeoninself-290play(bowlingandveloso,2001).td-gammon(tesauro,1995)isaworld-classbackgammon2917outline

introduction to deep learning

introduction to id23

value-based deep rl

policy-based deep rl

model-based deep rl

learning models of the environment

(cid:73) demo: generative model of atari
(cid:73) challenging to plan due to compounding errors

(cid:73) errors in the transition model compound over the trajectory
(cid:73) planning trajectories di   er from executed trajectories
(cid:73) at end of long, unusual trajectory, rewards are totally wrong

deep id23 in go

what if we have a perfect model? e.g. game rules are known

alphago paper:
www.nature.com/articles/nature16961

alphago resources:
deepmind.com/alphago/

nature.com/nature28 january 2016   10vol. 529, no. 7587conservationsongbirds   la carteillegal harvest of millionsof mediterranean birdspage 452research ethicssafeguard transparency don   t let openness backfire on individualspage 459popular sciencewhen genes got    selfish   dawkins   s  callingcard forty years onpage 462all systems goat last     a  computer program that can beat a champion go player  page 484the international weekly journal of sciencecover 28 january 2016.indd   120/01/2016   15:40conclusion

(cid:73) general, stable and scalable rl is now possible
(cid:73) using deep networks to represent value, policy, model
(cid:73) successful in atari, labyrinth, physics, poker, go
(cid:73) using a variety of deep rl paradigms

