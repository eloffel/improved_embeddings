yun-nung (vivian) chen

asli celikyilmaz
               

dilek hakkani-t  r

deepdialogue.miulab.tw

deep learning for dialogue systems

2

part i
introduction & background

brief history of dialogue systems

3

multi-modal systems
e.g., microsoft mipad, pocket pc

tv voice search
e.g., bing on xbox

virtual personal assistants

apple siri 
(2011)

google now (2012)
google assistant 

(2016)

microsoft cortana

(2014)

2017

early 2000s

amazon alexa/echo

(2014)

facebook  m & bot 

(2015)

google home 

(2016)

intent determination
(nuance   s emily   , at&t hmihy)
user:    uh   we want to move   we 
want to change our phone line 
from this house to another house   

darpa

calo project

task-specific argument extraction 
(e.g., nuance, speechworks)
user:    i want to fly from boston 
to new york next week.   

early 1990s

keyword spotting
(e.g., at&t)
system:    please say collect,  
calling card, person, third 
number, or operator   

language empowering intelligent assistant

4

apple siri (2011)

google now (2012)

google assistant (2016)

microsoft cortana (2014)

amazon alexa/echo (2014)

facebook m & bot (2015)

google home (2016)

challenges

5

(cid:133) variability in natural language
(cid:133) robustness
(cid:133) recall/precision trade-off
(cid:133) meaning representation
(cid:133) common sense, world knowledge
(cid:133) ability to learn
(cid:133) transparency

5

dialogue systems

6

task-oriented
    personal assistant, helps 
users achieve a certain task
    combination of rules and statistical
components
    examples:
    pomdp for spoken dialog 
systems (williams and young, 
2007)
    end-to-end trainable task-
oriented dialogue system (wen et 
al., 2016)
    end-to-end reinforcement 
learning dialogue system (zhao 
and eskenazi, 2016)

chit-chat
    no specific goal, focus on natural 
responses
    using variants of id195 model
    examples:
    a neural conversation model 
(vinyals and le, 2015)
    id23 for 
dialogue generation (li et al., 
2016)
    conversational contextual cues 
for response ranking (ai-rfou et 
al., 2016)

task-oriented dialogue system (young, 2000)

http://rsta.royalsocietypublishing.org/content/358/1769/1389.short

7

speech signal

hypothesis
are there any action movies to 
see this weekend

speech 

recognition

text input
are there any action movies to see this weekend?

language understanding (lu)
    domain identification
    user intent detection
    slot filling
semantic frame
request_movie
genre=action, date=this weekend

text response
where are you located?

natural language 
generation (id86)

system action/policy
request_location

dialogue management (dm)
    dialogue state tracking (dst)
    dialogue policy

backend action / 
knowledge providers

7

outline

8

(cid:133) introduction & background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

8

a single neuron

9

1w
1x
2w
2x
nw   
nx

b
bias

1

activation function

z

(cid:11) (cid:12)z(cid:86)

(cid:14)

y

(cid:86)

(cid:32)

1
(cid:11) (cid:12)
z
ze
(cid:16)(cid:14)
sigmoid function

1

w, b are the parameters of this neuron

(cid:11) (cid:12)z(cid:86)

z

9

a single neuron

10

1w
1x
2w
2x
nw   
nx

b
bias

1

rf
(cid:111):

n

r

m

z

(cid:14)

y

is
"2"    
(cid:173)
(cid:174)
not
"2" 
(cid:175)

y
y

(cid:116)
(cid:31)

5.0
5.0

a single neuron can only handle binary classification

10

a layer of neurons

11

(cid:133) handwriting digit classification

1x
2x

   

nx

1

(cid:14)

(cid:14)

(cid:14)

m

r

n

rf
(cid:111):
1y

   1    or not

2y

   2    or not

3y

   3    or not

which 
one is 
max?

10 neurons/10 classes

      

a layer of neurons can handle multiple possible output,

and the result depends on the max one

deep neural networks (dnn)

12

(cid:133) fully connected feedforward network

layer 1

layer 2

layer l

input
1x
2x

vector 

x

   
   

   
   

   
   
nx

deep nn: multiple hidden layers

m

r

n

rf
(cid:111):
output

      

      

      

   
   

1y

2y

my

vector 

y

recurrent neural network (id56)

13

: tanh, relu

id56 can learn accumulated sequential information (time-series)

http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/

time

vanishing gradient: gating mechanism

14

(cid:133) id56: keeps temporal sequence information

   i grew up in france   
i speak fluent french.   

issue: in theory, id56s can handle    long-term    info , but cannot in practice

(cid:198) use gates to directly encode the long-distance information

http://colah.github.io/posts/2015-08-understanding-lstms/

long short-term memory (lstm)

15

(cid:133) lstms are explicitly designed to avoid the long-term 

dependency problem

vanilla id56

15

long short-term memory (lstm)

16

lstm

16

long short-term memory (lstm)

17

lstm

17

long short-term memory (lstm)

18

lstm

runs straight down the chain 
with minor linear interactions
(cid:198) easy for information to flow 
along it unchanged

gates are a way to optionally let 
information through
(cid:198) composed of a sigmoid and a 
pointwise multiplication 
operation

18

long short-term memory (lstm)

19

lstm

forget gate (a sigmoid layer): 
decides what information we   re 
going to throw away from the cell 
state

    1:    completely keep this   
    0:    completely get rid of this   

19

long short-term memory (lstm)

20

lstm

input gate (a sigmoid layer): decides 
what new information we   re going 
to store in the cell state

vanilla id56

example: we want to add the new subject   s gender to the cell state for replacing the old one.

20

long short-term memory (lstm)

21

lstm

cell state update: forgets the things 
we decided to forget earlier and 
add the new candidate values, 
scaled by how much we decided to 
update each state value

where we actually drop the information about the 
old subject   s gender and add the new information

   
   

ft: decides which to forget
it: decide which to update

21

long short-term memory (lstm)

22

lstm

output gate (a sigmoid layer): 
decides what new information 
we   re going to output

example: it might output whether the subject is singular or plural, so that we know what form 
a verb should be conjugated into if that   s what follows next.

addressing gradient vanishing issues in id56

22

id195 model (sutskever et al., 2014)

23

http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf

(cid:133) encode source into a fixed length vector, use it as 

initial recurrent state for target decoder model
(cid:133) cascade two id56s,    encoder-decoder model   

(cid:134) input: word sequences in the question
(cid:134) output: word sequences in the response

the input and output should be model in a sequential way

memory networks (weston et al., 2014)

supervision (direct or reward-based)

24

memory 
module

        
    1,    2,   ,        

output
r

    2
        1

controller 
module

https://arxiv.org/abs/1410.3916

(cid:133) memory networks have 4 

components:
(cid:134) i: (input feature map) convert 
incoming data to the internal 
feature representation

(cid:134) g: (generalization) update 
memories given new input
(cid:134) o: produce new output (in 

feature representation space) 
given the memories

s to ry (2: 2 s u p p o rtin g  fa c ts )
j ohn droppe d the  milk.
j ohn took the  m ilk the re .
s a ndra  we nt ba ck to the  ba throom .
j ohn move d to the  ha llwa y.
ma ry we nt ba ck to the  be droom .
wh e re  is  th e  m ilk?    an s we r: h a llwa y    p re d ic tio n : h a llw a y

(cid:134) r: (response) convert output 
o into a response seen by the 
outside world

s u p p o rt ho p  1
0.06
0.88
0.00
0.00
0.00

ye s

ye s

input

s to ry (1: 1 s u p p o rtin g  fa c t)
0.00
da nie l we nt to the  ba throom.
0.00
ma ry tra ve lle d to the  ha llwa y.
0.02
j ohn we nt to the  be droom.
0.98
j ohn tra ve lle d to the  ba throom.
0.00
ma ry we nt to the  office .
wh e re  is  j o h n ?    an s we r: b a th ro o m     p re d ic tio n : b a th ro o m

s u p p o rt ho p  1 ho p  2 ho p  3
0.00
0.03
internal state vectors 
0.00
0.00
(initially: query)
0.37
0.00
0.60
0.96
0.01
0.00

memory vectors

i

ye s

s to ry (16: b a s ic  in d u c tio n )
bria n is  a  frog.
lily is  gra y.
bria n is  ye llow.
j ulius  is  gre e n.
g re g is  a  frog.
wh a t c o lo r is  gre g ?   an s w e r: ye llo w    p re d ic tio n : ye llo w

s u p p o rt ho p  1 ho p  2 ho p  3
0.00
0.00
1.00
0.00
0.00

0.98
0.00
0.00
0.00
0.02

0.00
0.07
0.07
0.06
0.76

ye s

ye s

ye s

memory module stores the 

s to ry (18: s ize  re a s o n in g )
the  s uitca s e  is  bigge r tha n the  che s t.
the  box is  bigge r tha n the  chocola te .
the  che s t is  bigge r tha n the  chocola te .
the  che s t fits  ins ide  the  conta ine r.
the  che s t fits  ins ide  the  box.
do e s  th e  s u itc a s e  fit in  th e  c h o c o la te ?    an s we r: n o     p re d ic tio n : n o

history to make the model find 

s u p p o rt ho p  1
0.00
0.04
0.17
0.00
0.00

the supporting facts

ye s

ye s

24

outline

25

(cid:133) introduction & background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

25

id23

26

(cid:133) rl is a general purpose framework for decision making

(cid:134) rl is for an agent with the capacity to act
(cid:134) each action influences the agent   s future state
(cid:134) success is measured by a scalar reward signal
(cid:134) goal: select actions to maximize future reward

action

reward

observation

reinforcing learning

27

(cid:133) markov decision process (mdp)

(cid:134) s: state set
(cid:134) a: action set
(cid:134) r: s       (reward)

(cid:134) psa: transition probabilities (p(s,a,s   )   r)

(cid:134)   : discount factor

(cid:133) mdp = (s, a, r, psa,   )

(cid:134) alphago improves by self-playing
(cid:134) car autonomously learns driving up!

reinforcing learning

28

(cid:133) start from state s0
(cid:133) choose action a0
(cid:133) transit to s1 ~ p(s0, a0)
(cid:133) continue   

(cid:133) total reward:

goal: select actions that maximize the expected total reward

id23 approach

29

(cid:133) policy-based rl

(cid:134) search directly for optimal policy

is the policy achieving maximum future reward 

(cid:133) value-based rl

(cid:134) estimate the optimal value function

is maximum value achievable under any policy

(cid:133) model-based rl

(cid:134) build a model of the environment
(cid:134) plan (e.g. by lookahead) using model

q-networks (sutton et al., 1998)

30

http://ieeexplore.ieee.org/abstract/document/126844/

(cid:133) q-networks represent value functions with weights 

(cid:134) generalize from seen states to unseen states (#states is large)
(cid:134) update parameter 

for function approximation

30

id24

31

(cid:133) goal: estimate optimal q-values

(cid:134) optimal q-values obey a bellman equation

(cid:134) value iteration algorithms solve the bellman equation

learning target

31

deep q-networks (id25) (minh et al., 2013)

32

https://arxiv.org/abs/1312.5602

(cid:133) represent value function by deep q-network with weights  

(cid:133) objective is to minimize mse loss by sgd

(cid:133) leading to the following id24 gradient

issue: na  ve id24 oscillates or diverges using nn due to:

1) correlations between samples 2) non-stationary targets

32

stability by id25

33

(cid:133) naive id24 oscillates or diverges with neural nets

1)

sequential data: correlated, non-independent and 
identically distributed (cid:198) use experience replay

2) policy oscillation: changes rapidly with slight changes to 

q-values (cid:198) freeze target q-network

3) unknown scale of rewards and q-values (cid:198) clip rewards 

or normalize network adaptively to sensible range, 
double id24

33

34

part ii
deep learning based dialogue system

35

outline
(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

35

task-oriented dialogue system (young, 2000)

36

speech signal

hypothesis
are there any action movies to 
see this weekend

speech 

recognition

text input
are there any action movies to see this weekend?

language understanding (lu)
    domain identification
    user intent detection
    slot filling
semantic frame
request_movie
genre=action, date=this weekend

text response
where are you located?

natural language 
generation (id86)

system action/policy
request_location

dialogue management (dm)
    dialogue state tracking (dst)
    dialogue policy

backend action / 
knowledge providers

36

semantic frame representation

37

(cid:133) requires a domain ontology: early connection to backend
(cid:133) contains core content (intent, a set of slots with fillers)
restaurant 
domain

find me a cheap taiwanese restaurant in oakland

price

location

restaurant

type

find_restaurant (price=   cheap   , 
type=   taiwanese   , location=   oakland   )

movie 
domain

show me action movies directed by james cameron

genre

year

movie

director

find_movie (genre=   action   , 
director=   james cameron   )

37

language understanding (lu)

38

(cid:133) pipelined

1. domain 
classification

2. intent 

classification

3. slot filling

38

lu     domain/intent classification

d= {(u1,c1),   ,(un,cn)} where ci    c, train a 

    given a collection of utterances ui with labels ci, 
model to estimate labels for new utterances uk.

39

as an utterance 
classification 

task

find me a cheap taiwanese restaurant in oakland

movies
restaurants
music
sports
   
domain

find_movie, buy_tickets
find_restaurant, find_price, book_table
find_lyrics, find_singer
   

intent

deep neural networks for domain/intent 
classification     i (sarikaya et al, 2011)

40

http://ieeexplore.ieee.org/abstract/document/5947649/

(cid:133) deep belief nets (dbn)

(cid:134) unsupervised training of weights
(cid:134) fine-tuning by back-propagation
(cid:134) compared to maxent, id166, and boosting

40

deep neural networks for domain/intent 
classification     ii (tur et al., 2012; deng et al., 2012)

41

http://ieeexplore.ieee.org/abstract/document/6289054/; http://ieeexplore.ieee.org/abstract/document/6424224/

(cid:133) deep convex networks (dcn)

(cid:134) simple classifiers are stacked to learn complex functions
(cid:134) feature selection of salient id165s

(cid:133) extension to kernel-dcn

41

deep neural networks for domain/intent 
classification     iii (ravuri and stolcke, 2015)

42

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/id56lm_addressee.pdf

(cid:133) id56 and lstms for 

utterance classification

(cid:133) word hashing to deal with 
large number of singletons
(cid:134) kat: #ka, kat, at#
(cid:134) each character id165 is 

associated with a bit in the 
input encoding

42

lu     slot filling

43

as a sequence 
tagging task

    given a collection tagged word sequences,    

where ti   m, the goal is to estimate tags for a new word 

s={((w1,1,w1,2,   , w1,n1), (t1,1,t1,2,   ,t1,n1)),
((w2,1,w2,2,   ,w2,n2), (t2,1,t2,2,   ,t2,n2))    }
sequence.

flights from boston to new york today

flights

o
o

from boston
o
b-city
b-dept
o

to
o
o

new
b-city
b-arrival

york
i-city
i-arrival

today

o

b-date

entity tag
slot tag

recurrent neural nets for slot tagging     i 
(yao et al, 2013; mesnil et al, 2015)

44

http://131.107.65.14/en-us/um/people/gzweig/pubs/interspeech2013id56lu.pdf; http://dl.acm.org/citation.cfm?id=2876380

(cid:133) baseline: id49 on atis corpus
(cid:133) variations: 

a. id56s with lstm cells
b.
c. bi-directional lstms

input, sliding window of id165s

    1
    2
        
    0
   0
   1
   2
       
    0     1     2         

    0
    1
    2
        
   0
   1
   2
       
    0     1     2         

(a) lstm

(b) lstm-la

    0
    1
    2
        
           
   0    
   1    
   2    
   1    
   2    
   0    
           
    0     1     2         

(c) blstm

44

recurrent neural nets for slot tagging     ii 
(kurata et al., 2016; simonnet et al., 2015)

http://www.aclweb.org/anthology/d16-1223

45

(cid:133) encoder-decoder networks

(cid:134) leverages sentence level 

information

(cid:133) attention-based encoder-

decoder
(cid:134) use of attention (as in mt) 

in the encoder-decoder 
network

(cid:134) attention is estimated using 
a feed-forward network with 
input: ht and st at time t

   1

       
   2
   0
             2     1     0
   0
   1
   2
       
    0     1     2         

    1
    2
        
    0
    0     1     2         
    1
    2
        
    0
    0
    1
        
    2
   0           

ci

recurrent neural nets for slot tagging     iii 
(jaech et al., 2016; tafforeau et al., 2016)

46

https://arxiv.org/abs/1604.00117; http://www.sensei-conversation.eu/wp-content/uploads/2016/11/favre_is2016b.pdf

(cid:133) id72

(cid:134) goal: exploit data from domains/tasks with a lot of 

data to improve ones with less data

(cid:134) lower layers are shared across domains/tasks
(cid:134) output layer is specific to task

46

joint semantic frame parsing

47

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/is16_multijoint.pdf; https://arxiv.org/abs/1609.01454

    slot filling and 

intent prediction 
in the same 
output sequence

    intent prediction 

and slot filling 
are performed 
in two branches

parallel       
(liu and 
lane, 2016)

sequence-

based 

(hakkani-tur 
et al., 2016)

taiwanese

u

ht-
1

w

v
b-type

food
u

ht

o

v

please
u

ht+
1

o

v

w

w

w

slot filling

eos
u

ht+1
v

t

find_res
intent 
prediction

contextual lu

48

i

o

o

o

send_email

o
b-contact_name

domain identification (cid:198) intent prediction (cid:198) slot filling
d communication
u

just   sent   email   to   bob   about   fishing   this   weekend
o

s
b-subject i-subject i-subject
(cid:198) send_email(contact_name=   bob   , subject=   fishing this weekend   )
u1
s1
(cid:198) send_email(contact_name=   bob   )
u2
s2
(cid:198) send_email(message=   are we going to fish this weekend   )

are     we     going     to     fish     this     weekend

send email to bob

b-contact_name

b-message

i-message

i-message

i-message

i-message

i-message

i-message

48

contextual lu (bhargava et al., 2013; hori et al, 2015)

49

https://www.merl.com/publications/docs/tr2015-134.pdf

(cid:133) leveraging contexts

(cid:134) used for individual tasks

(cid:133) id195 model

(cid:134) words are input one at a time, tags are output at the 

end of each utterance

(cid:133) extension: lstm with speaker role dependent layers

49

e2e memnn for contextual lu (chen et al., 2016)

50

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/is16_contextualslu.pdf

1. sentence encoding

2. knowledge attention

3. knowledge encoding

knowledge attention distribution

id56 tagger

slot tagging sequence y

pi

mi

contextual 

sentence encoder

id56mem

x1

x2

xi   

memory representation
inner 
product

sentence 
encoder
id56in

history utterances {xi}

x1

x2

xi   

c

u

current utterance

weighted 

sum

h

w
m

yt-1
v
ht-1
u

wt-1

w
m

yt
v
ht

u

wt

w

wkg

   
knowledge encoding 

representation

o

idea: additionally incorporating contextual knowledge during slot tagging
50

(cid:198) track dialogue states in a latent way

51

structural lu (chen et al., 2016)

(cid:133) prior knowledge as a teacher

knowledge encoding module
root

sentence 
encoding

showme theflights fromseattleto sanfrancisco

http://arxiv.org/abs/1609.03286

input sentence

knowledge-

guided 

representation
   

knowledge-guided structure {xi}

knowledge 
encoding

pi

knowledge attention distribution

mi

encoded id99

weighted 

sum

id56 tagger

inner 
product

m

wt-1
u

m

wt
u

m

wt+1
u

w
v

w

w

w

yt-1

v
yt

v
yt+1
slot tagging sequence
51

structural lu (chen et al., 2016)

52

http://arxiv.org/abs/1609.03286

(cid:133) sentence structural knowledge stored as memory

show me the flights from seattle to san francisco

sentence s
syntax (dependency tree)

1.

2.

root

show

me

flights

the

from

to

3.

seattle

francisco
4.

san

semantics (amr graph)

show

1.
you

4.
i

flight

city
2.

city
3.

seattle

san francisco

52

lu evaluation

53

(cid:133) metrics

(cid:134) sub-sentence-level: intent accuracy, slot f1
(cid:134) sentence-level: whole frame accuracy

53

outline

54

(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

54

task-oriented dialogue system (young, 2000)

55

speech signal

hypothesis
are there any action movies to 
see this weekend

speech 

recognition

text input
are there any action movies to see this weekend?

language understanding (lu)
    domain identification
    user intent detection
    slot filling
semantic frame
request_movie
genre=action, date=this weekend

text response
where are you located?

natural language 
generation (id86)

system action/policy
request_location

dialogue management (dm)
    dialogue state tracking (dst)
    dialogue policy

backend action / 
knowledge providers

55

elements of dialogue management

56

(figure from ga  i  )
56

dialogue state tracking (dst)

57

(cid:133) dialogue state: a representation of the system's belief of the 

user's goal(s) at any time during the dialogue

(cid:133) inputs

(cid:134) current user utterance
(cid:134) preceding system response
(cid:134) results from previous turns

(cid:133) for

(cid:134) looking up knowledge or making api call(s)
(cid:134) generating the next system action/response

57

dialogue state tracking (dst)

58

58

dialogue state tracking (dst)

59

(cid:133) maintain a probabilistic distribution instead of a 1-best 

prediction for better robustness to recognition errors

incorrect 
for both!

59

dialogue state tracking (dst)

60

(cid:133) maintain a probabilistic distribution instead of a 1-best 

prediction for better robustness to slu errors or 
ambiguous input

slot
# people
time

slot
# people
time

value
5 (0.5)
5 (0.5)

value
3 (0.8)
5 (0.8)

how can i help you?

book a table at sumiko for 5

how many people?

3

60

dialog state tracking challenge (dstc)

(williams et al. 2013, henderson et al. 2014, henderson et al. 2014, kim et al. 2016, kim et al. 2016) 

61

challenge

dstc1

dstc2

dstc3

dstc4

dstc5

type
human-
machine
human-
machine
human-
machine
human-
human
human-
human

domain

bus route

data provider main theme

cmu

id74

restaurant

u. cambridge

user goal changes

tourist information u. cambridge

id20

tourist information

i2r

human conversation

tourist information

i2r

language adaptation

neural belief tracker (henderson et al., 2013; 
henderson et al., 2014; mrk  i   et al., 2015)

62

http://www.anthology.aclweb.org/w/w13/w13-4073.pdf; https://arxiv.org/abs/1506.07190

(figure from wen et al, 2016)
62

neural belief tracker (mrk  i   et al., 2016)

63

https://arxiv.org/abs/1606.03777

63

multichannel tracker (shi et al., 2016)

64

https://arxiv.org/abs/1701.06247

(cid:133) training a multichannel id98 

for each slot
(cid:134) chinese character id98
(cid:134) chinese word id98
(cid:134) english word id98

64

dst evaluation

65

(cid:133) dialogue state tracking challenges

(cid:134) dstc2-3, human-machine
(cid:134) dstc4-5, human-human

(cid:133) metric

(cid:134) tracked state accuracy with respect to user goal
(cid:134) recall/precision/f-measure individual slots

65

outline

66

(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

66

elements of dialogue management

67

(figure from ga  i  )
67

dialogue policy optimization

68

(cid:133) dialogue management in a rl framework

environment

u s e r

id86

language understanding

action a

reward r

observation o

dialogue manager

agent

the optimized dialogue policy selects the best action that maximizes the future reward.
correct rewards are a crucial factor in dialogue policy training 

68

reward for rl     evaluation for system

69

(cid:132) dialogue is a special rl task

(cid:132) human involves in interaction and rating (evaluation) of a 

dialogue

(cid:132) fully human-in-the-loop framework

(cid:132) rating: correctness, appropriateness, and adequacy
- expert rating
- user rating
- objective rating

high quality, high cost
unreliable quality, medium cost
check desired aspects, low cost

69

id23 for dialogue 
policy optimization

70

user input (o)

language 

understanding

response

language 
(response) 
generation

    
    

    =    (    )

dialogue 
policy

collect rewards

(    ,    ,    ,       )
    (    ,    )

optimize

type of bots

social chatbots

state

action 

reward

chat history

system response

# of turns maximized;
intrinsically motivated reward

infobots (interactive q/a) 

task-completion bots 

user current 
question + context
user current input + 
context

answers to current 
question
system dialogue act w/ 
slot value (or api calls)

relevance of answer;
# of turns minimized
task success rate;
# of turns minimized

goal: develop a generic deep rl algorithm to learn dialogue policy for all bot categories

70

dialogue id23 signal

71

typical reward function
(cid:132) -1 for per turn penalty
(cid:132) large reward at completion if successful
typically requires domain knowledge
    simulated user
    paid users (amazon mechanical turk)
   
    real users

the user simulator is usually required for 
dialogue system training before deployment

   

|||

71

id25 for dialogue management (li et al., 2017)

72

(cid:133) deep rl for training dm

https://arxiv.org/abs/1703.01008

(cid:134) input: current semantic frame observation, database 

returned results

(cid:134) output: system action

semantic frame
request_movie
genre=action, date=this weekend

72

id25-based 
dialogue 

management 

(dm)

simulated/paid/real 

user

backend db

system action/policy
request_location

online training (su et al., 2015; su et al., 2016)

73

http://www.anthology.aclweb.org/w/w15/w15-46.pdf#page=437; https://www.aclweb.org/anthology/p/p16/p16-1230.pdf

(cid:133) policy learning from real users

(cid:134) infer reward directly from dialogues (su et al., 2015)
(cid:134) user rating (su et al., 2016)

(cid:133) reward modeling on user binary success rating

embedding 
function

dialogue 

representation

reward 
model

query rating

success/fail

reinforcement 

signal

dialogue management evaluation

74

(cid:133) metrics

(cid:134) turn-level evaluation: system action accuracy
(cid:134) dialogue-level evaluation: task success rate, reward

74

75

outline
(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

75

task-oriented dialogue system (young, 2000)

76

speech signal

hypothesis
are there any action movies to 
see this weekend

speech 

recognition

text input
are there any action movies to see this weekend?

language understanding (lu)
    domain identification
    user intent detection
    slot filling
semantic frame
request_movie
genre=action, date=this weekend

text response
where are you located?

natural language 
generation (id86)

system action/policy
request_location

dialogue management (dm)
    dialogue state tracking (dst)
    dialogue policy

backend action / 
knowledge providers

76

id86 (id86)

77

(cid:133) mapping dialogue acts into natural language

inform(name=seven_days, foodtype=chinese)

seven days is a nice chinese restaurant

77

template-based id86

78

(cid:133) define a set of rules to map frames to nl
semantic frame
confirm()

natural language
   please tell me more about the product your are 
looking for.   
   do you want somewhere in the $v?   
   do you want a $v restaurant?   
   do you want a $v restaurant in the $w.   

confirm(area=$v)
confirm(food=$v)
confirm(food=$v,area=$w)

pros: simple, error-free, easy to control
cons: time-consuming, rigid, poor scalability

78

plan-based id86 (walker et al., 2002)

79

(cid:133) divide the problem into pipeline

sentence 

plan 

generator

sentence 

plan 

reranker

surface 
realizer

z house is a 
cheap restaurant.

inform(

name=z_house,
price=cheap

)

syntactic tree

(cid:134) statistical sentence plan generator (stent et al., 2009)
(cid:134) statistical surface realizer (dethlefs et al., 2013; cuay  huitl et al., 2014;    )

pros: can model complex linguistic structures
cons: heavily engineered, require domain knowledge

class-based lm id86 (oh and rudnicky, 2000)

80

http://dl.acm.org/citation.cfm?id=1117568

(cid:133) class-based id38

(cid:133) id86 by decoding

classes:
inform_area
inform_address
   
request_area
request_postcode

pros: easy to implement/ 
understand, simple rules
cons: computationally inefficient

80

phrase-based id86 (mairesse et al, 2010)

81

http://dl.acm.org/citation.cfm?id=1858838

charlie chan

is a

chinese restaurant

near      cineworld

in the        centre

phrase
dbn

semantic

dbn

inform(name=charlie chan, food=chinese, type= restaurant, near=cineworld, area=centre)
realization phrase semantic stack

d

d

pros: efficient, good performance
cons: require semantic alignments

81

id56-based lm id86 (wen et al., 2015)

82

input

http://www.anthology.aclweb.org/w/w15/w15-46.pdf#page=295

inform(name=din tai fung, food=taiwanese)

0, 0, 1, 0, 0,    , 1, 0, 0,    , 1, 0, 0, 0, 0, 0   

dialogue act 1-hot
representation

slot_name         serves            slot_food               .                 <eos>

conditioned on 
the dialogue act

output

<bos>
<bos>

slot_name        serves          slot_food               .
din tai fung       serves            taiwanese                .

delexicalisation

slot weight tying

handling semantic repetition

83

(cid:133) issue: semantic repetition

(cid:134) din tai fung is a great taiwanese restaurant that serves taiwanese.
(cid:134) din tai fung is a child friendly restaurant, and also allows kids.

(cid:133) deficiency in either model or decoding (or both)
(cid:133) mitigation

(cid:134) post-processing rules (oh & rudnicky, 2000)
(cid:134) gating mechanism (wen et al., 2015)
(cid:134) attention (mei et al., 2016; wen et al., 2015)

83

semantic conditioned lstm (wen et al., 2015)

84

(cid:133) original lstm cell

(cid:133) dialogue act (da) cell

(cid:133) modify ct

http://www.aclweb.org/anthology/d/d15/d15-1199.pdf
xt

ht-1

ht-1

xt

xt

ht-
1

it

lstm cell
da cell

ht-1

xt

dt-1

d0

ft

ct

rt

xt

ht-1

ot

ht

dt

dialog act 1-hot
representation

0, 0, 1, 0, 0,    , 1, 0, 0,    , 1, 0, 0,    
inform(name=seven_days, food=chinese)
idea: using gate mechanism to control the 
generated semantics (dialogue act/slots)

84

structural id86 (du  ek and jur      ek, 2016)

85

https://www.aclweb.org/anthology/p/p16/p16-2.pdf#page=79

(cid:133) goal: id86 based on the syntax tree

(cid:134) encode trees as sequences
(cid:134) id195 model for generation

85

contextual id86 (du  ek and jur      ek, 2016)

86

https://www.aclweb.org/anthology/w/w16/w16-36.pdf#page=203

(cid:133) goal: adapting users    

way of speaking, 
providing context-
aware responses
(cid:134) context encoder
(cid:134) id195 model

86

id86 evaluation

87

(cid:133) metrics

(cid:134) subjective: human judgement (stent et al., 2005)

(cid:132) adequacy: correct meaning
(cid:132) fluency: linguistic fluency
(cid:132) readability: fluency in the dialogue context
(cid:132) variation: multiple realizations for the same concept

(cid:134) objective: automatic metrics

(cid:132) word overlap: id7 (papineni et al, 2002), meteor, id8
(cid:132) id27 based: vector extrema, greedy matching, 

embedding average

there is a gap between human perception and automatic metrics

87

outline

88

(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

88

chitchat hierarchical id195             
(serban et.al., 2016) 

89

http://www.aaai.org/ocs/index.php/aaai/aaai16/paper/view/11957

(cid:133) a hierarchical id195 model for generating dialogues

89

chitchat hierarchical id195            
(serban et.al., 2017) 

90

https://arxiv.org/abs/1605.06069

(cid:133) a hierarchical id195 model with gaussian latent 

variable for generating dialogues

90

e2e joint nlu and dm (yang et al., 2017)

91

https://arxiv.org/abs/1612.00913

(cid:133) idea: errors from dm can be propagated to nlu 

for better robustness

dm

model

baseline (crf+id166s)
pipeline-blstm
jointmodel

dm
7.7
12.0
22.8

nlu
33.1
36.4
37.4

both dm and nlu 
performance is improved

91

e2e supervised dialogue system (wen et al., 2016)

92

https://arxiv.org/abs/1604.04562

intent network

can            i          have   <v.food>    

generation network

<v.name> serves  great   <v.food>      . 

zt

policy network

pt

copy 
field

db pointer

xt

0  0  0         0  1

mysql query:
   select * where 
food=korean   

qt

   

n
i
r
a
a

l

s
e
v
e
n
d
a
y
s

c
u
r
r
y
p
r
i
n
c
e

 

 

 

l
i
t
t
l
e
s
e
u
o

l

r
o
y
a

l
 

s
t
a
n
d
a
r
d

korean     
0.7
british      
0.2
french     
0.1

   

can     i    have  korean

belief tracker

database
database operator

92

e2e memnn for dialogues (bordes et al., 2016)

93

https://arxiv.org/abs/1605.07683

(cid:133) split dialogue system 
actions into subtasks
(cid:134) api issuing
(cid:134) api updating
(cid:134) option displaying
(cid:134) information informing

93

e2e rl-based info-bot (dhingra et al., 2016)

94

https://arxiv.org/abs/1609.00777

movie=?; actor=bill murray; release year=1993

find me the bill murray   s movie.

when was it released?

i think it came out in 1993.

user

groundhog day is a bill murray 
movie which came out in 1993. 

kb-infobot

knowledge base (head, relation, tail)

(groundhog day, actor, bill murray)
(groundhog day, release year, 1993)
(australia, actor, nicole kidman)
(mad max: fury road, release year, 2015)

idea: differentiable database for propagating the gradients

94

e2e rl-based system (zhao and eskenazi, 2016)

95

http://www.aclweb.org/anthology/w/w16/w16-36.pdf#page=19

(cid:133) joint learning

(cid:134) nlu, dst, dialogue policy

(cid:133) deep rl for training

(cid:134) deep q-network
(cid:134) deep recurrent network

hybrid-rl
rl
baseline

95

e2e lstm-based dialogue control        
(williams and zweig, 2016)

96

https://arxiv.org/abs/1606.01269

(cid:133) idea: an lstm maps from raw dialogue history directly to a 

distribution over system actions
(cid:134) developers can provide software including business rules & 

programmatic apis 
(cid:198) lstm can take actions in the real world on behalf of the user

(cid:134) the lstm can be optimized using sl or rl

96

e2e task-completion bot (tc-bot) (li et al., 2017)

https://arxiv.org/abs/1703.01008

97

user 
goal

text input
are there any action movies 
to see this weekend?

id86 (id86)

w0

w1

w2

eos

time t-2

time t-1

wi

wi
+1

wi+2

wi

time t
language understanding (lu)
o
eos
<intent

b-
wi
type
+1

o
wi+2

wi+2

wi
+1

wi

o

o

b-
type

>

<slot>

o

o

<intent>

user dialogue action
inform(location=san francisco)

user agenda modeling

system action / 
policy
request_location

user simulator

dialogue 

management 

(dm)

eos

eos

<intent

>

semantic frame
request_movie
genre=action, 
date=this weekend

end-to-end neural dialogue system

idea: supervised learning for each component and reinforcement 

learning for end-to-end training the neural dialogue system

97

e2e task-completion bot (tc-bot) (li et al., 2017)

98

https://arxiv.org/abs/1703.01008

(cid:133) user goal: two tickets for    the witch    tomorrow 9:30 pm at regal meridian 16, seattle.

rule based system
user: can i get 2 tickets for the witch?
agent: which theater do you want?
user: regal meridian 16
agent: what time would you like to see it?
user: 9:30 pm
agent: what date would you like to see it?
user: tomorrow!
agent: how many people are you?
user: just two adults.
agent: what is your location?
user: seattle!
agent: what kind of video format do you like?
user: oh, any kind is okay. 
agent: do you have any price constraint?
user: no, i don   t care.
agent: great - i was able to purchase 2 tickets for 
you to see the witch tomorrow at regal meridian 16 
theater in seattle at 9:30 pm.
user: thank you.
agent: thank you!

id23 system
user: can i get 2 tickets for the witch?
agent: what date would you like to see it?
user: tomorrow
agent: which theater do you prefer?
user: regal meridian 16
agent: what is your location?
user: seattle
agent: what time would you like to see it?
user: 9:30 pm.
agent: great - i was able to purchase 2 tickets for 
you to see the witch tomorrow at regal meridian 
16 theater in seattle at 9:30 pm.
user: thanks.
agent: thanks!

the system can learn how to efficiently 
interact with users for task completion
98

99

part iii
evaluation

outline

100

(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

100

dialogue system evaluation

101

(cid:133) dialogue model evaluation

(cid:134) crowd sourcing
(cid:134) user simulator

(cid:133) response generator evaluation

(cid:134) word overlap metrics 
(cid:134) embedding based metrics

101

crowd sourcing for dialog system 
evaluation (yang, et.al. 2012)

102

http://www-scf.usc.edu/~zhaojuny/docs/sdschapter_final.pdf

the normalized mean scores of q2 
and q5 for approved ratings in each 
category. a higher score maps to a 
higher level of task success

102

user simulation

103

keeps a list of its goals 

and actions

randomly generates an 

agenda

updates its list of goals 

and adds new ones

(cid:133) goal: generate natural and reasonable conversations to 

enable id23 for exploring the policy space

dialogue 
corpus

real user

simulated user

interaction

(cid:133) approach

dialogue management (dm)
    dialogue state tracking (dst)
    dialogue policy

(cid:134) rule-based crafted by experts (li et al., 2016)
(cid:134) learning-based (schatzmann et al., 2006; el asri et al., 2016)

elements of user simulation

104

user simulation

error model
   
   

recognition error
lu error

user model

reward model

distribution over 
user dialogue acts 
(semantic frames)

system dialogue acts

dialogue management (dm)

dialogue state 
tracking (dst)

dialogue policy 
optimization

reward

the error model enables the system to maintain 
the robustness during training

backend action / 
knowledge providers

rule-based simulator for rl based system 
(li et.al., 2016)

105

http://arxiv.org/abs/1612.05688

(cid:133) rule-based simulator + collected data
(cid:133) starts with sets of goals, actions, kb, slot types
(cid:133) publicly available simulation framework
(cid:133) movie-booking domain: ticket booking and movie seeking
(cid:133) provide procedures to add and test own agent

105

elements of user simulation

106

user simulation

error model
   
   

recognition error
lu error

user model

reward model

distribution over 
user dialogue acts 
(semantic frames)

system dialogue acts

dialogue management (dm)

dialogue state 
tracking (dst)

dialogue policy 
optimization

reward

the error model enables the system to maintain 
the robustness during training

backend action / 
knowledge providers

rule-based simulator for rl based system 
(li et.al., 2016)

107

http://arxiv.org/abs/1612.05688

(cid:133) rule-based simulator + collected data
(cid:133) starts with sets of goals, actions, kb, slot types.
(cid:133) presents publicly available simulation framework, 

for the movie-booking domain: movie ticket 
booking and movie seeking. 

(cid:133) provide procedures to add and test own agent in 

their proposed framework  

107

data-driven simulator for automated 
evaluation (jung et.al., 2009)

108

(cid:133) three step process
1) user intention simulator

request+search_loc

user   s current 

semantic 

user   s current 

semantic 
frame (t-1)

user   s current 

semantic 
frame (t)

discourse 

current 
discourse 
status (t-1)

current 
discourse 
status (t)

(*) compute all possible semantic frame 
given previous turn info
(*) randomly select one possible semantic frame

features (dd+di)

108

data-driven simulator for automated 
evaluation (jung et.al., 2009)

109

(cid:133) three step process
1) user intention simulator
2) user utterance simulator

request+search_loc

i want to go to the city hall
prp vb to vb to [loc_name]

given a list of pos tags associated with 
the semantic frame, using lm+rules
they generate the user utterance.

109

data-driven simulator for automated 
evaluation (jung et.al., 2009)

110

(cid:133) three step process:
1) user intention simulator
2) user utterance simulator
3) asr channel simulator
(cid:133) evaluate the generated 
sentences using blue-
like measures against the 
reference utterances 
collected from humans 
(with the same goal)

110

id195 user simulation (el asri et al., 2016)

111

https://arxiv.org/abs/1607.00070

(cid:133) id195 trained from dialogue data

(cid:134) input: ci encodes contextual features, such as the 
previous system action, consistency between user 
goal and machine provided values

(cid:134) output: a dialogue act sequence form the user

(cid:133) extrinsic evaluation for policy

user simulator for dialogue evaluation 
measures

112

understanding ability
    whether constrained values specified by users can be understood by 
the system
    agreement percentage of system/user understandings over the entire 
dialog (averaging all turns)

efficiency
    number of dialogue turns
    ratio between the dialogue turns (larger is better)

action appropriateness
    an explicit confirmation for an uncertain user utterance is an 
appropriate system action
    providing information based on misunderstood user requirements

112

how not to evaluate your dialog system    
(liu et.al., 2017)

113

https://arxiv.org/pdf/1603.08023.pdf

(cid:133) how to evaluate the quality of the 

generated response ? 
(cid:134) specifically investigated for chat-bots 
(cid:134) crucial for task-oriented tasks as well

(cid:133) metrics:

(cid:134) word overlap metrics, e.g., id7, 

meteor, id8, etc.

(cid:134) embeddings based metrics, e.g., 

contextual/meaning representation 
between target and candidate

113

dialog response evaluation (lowe et al., 2017)

114

(cid:133) problems of existing automatic 

evaluation
(cid:134) can be biased
(cid:134) correlate poorly with human 

judgements of response quality 

(cid:134) using word overlap may be misleading

(cid:133) solution

(cid:134) collect a dataset of accurate human 

scores for variety of dialogue 
responses (e.g., coherent/un-
coherent, relevant/irrelevant, etc.)

(cid:134) use this dataset to train an automatic 
dialogue evaluation model     learn to 
compare the reference to candidate 
responses!

(cid:134) use id56 to predict scores by 

comparing against human scores!

towards an automatic turing test

context of conversation 
speaker a: hey, what do you want 
to do tonight? 
speaker b: why don   t we go see a 
movie? 
model response 
nah, let   s do something active.
reference response 
yeah, the film about turing looks 
great! 

114

115

part iv
recent trends on learning dialogues

outline

116

(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

116

dialog state tracking challenge (dstc)

(williams et al. 2013, henderson et al. 2014, henderson et al. 2014, kim et al. 2016, kim et al. 2016) 

117

challenge

dstc1

dstc2

dstc3

dstc4

dstc5

dstc6

type
human-
machine
human-
machine
human-
machine
human-
human
human-
human

domain

bus route

data provider main theme

cmu

id74

restaurant

u. cambridge

user goal changes

tourist information u. cambridge

id20

tourist information

i2r

human conversation

tourist information

i2r

language adaptation

dstc renames as dialog system technology challenges

interactive rl for dm (shah et al., 2016)

118

https://research.google.com/pubs/pub45734.html

immediate 
feedback

118

interactive rl for dm (shah et al., 2016)

119

https://research.google.com/pubs/pub45734.html

user simulation
user simulation

error model
error model
   
   
   
   

recognition error
recognition error
lu error
lu error

user model
user model

reward model
reward model

state observation
state observation

action
action

reward
reward

dialogue 
dialogue 
manager 
manager 

(dm)
(dm)

backend 
backend 

db
db

use a third agent for providing interactive feedback to the dm

interpreting interactive feedback         
(shah et al., 2016)

120

https://research.google.com/pubs/pub45734.html

120

policy shaping for rl (shah et al., 2016)

121

https://research.google.com/pubs/pub45734.html

121

evolution roadmap

122

l

)
y
t
i
x
e
p
m
o
c
(
 
h
t
p
e
d
e
u
g
o
a
d

i

l

 

i feel sad   

i   ve got a cold what do i do?

tell me a joke.

single 
domain 
systems

what is influenza?

extended 
systems

multi-
domain 
systems

dialogue breadth (coverage)

open 
domain 
systems

122

intent expansion (chen et al., 2016)

123

http://ieeexplore.ieee.org/abstract/document/7472838/

(cid:133) transfer dialogue acts across domains

(cid:134) dialogue acts are similar for multiple domains
(cid:134) learning new intents by information from other domains

postpone my meeting to five pm

intent representation

training data

<change_note>

   adjust my note   

:

<change_setting>

   volume turn down   

new intent

<change_calender>

cdssm

embedding 
generation

1
2
:
k

k+1
k+2

the dialogue act representations can be 
automatically learned for other domains

u
a1
utterance

   

a2
action

p(a1 | u) p(a2 | u) p(an | u)

cossi

m
300

300

300

300

an

zero-shot learning (daupin et al., 2016)

124

https://arxiv.org/abs/1401.0509

(cid:133) semantic utterance classification

(cid:134) use query click logs to define a task that makes the 

networks learn the meaning or intent behind the queries

(cid:134) the semantic features are the last hidden layer of the dnn
(cid:134) use zero-shot discriminative embedding model combines 
h with the minimization of id178 of a zero-shot classifier

id20 for slu (kim et al., 2016)

125

http://www.aclweb.org/anthology/c/c16/c16-1038.pdf

(cid:133) frustratingly easy id20 
(cid:133) novel neural approaches to id20
(cid:133) improve slot tagging on several domains

policy for id20 (ga  i   et al., 2015)

126

http://ieeexplore.ieee.org/abstract/document/7404871/

(cid:133) bayesian committee machine (bcm) enables estimated 

q-function to share knowledge across domains

dh

qh

dr

qr

ql

dl

committee model

the policy from a new domain can be boosted by the committee policy

efficient exploration for domain expansion 
(lipton et al., 2016)

127

http://arxiv.org/abs/1608.05081

(cid:133) goal : dialogue domain extension
(cid:133) most goal-oriented dialogues require a closed and well-

defined domain

(cid:133) hard to include all domain-specific information up-front

efficient exploration for domain expansion 
(lipton et al., 2016)

128

http://arxiv.org/abs/1608.05081

(cid:133) bayesian by back-propogation

maintain point-
estimates of weights

maintain posterior 
distribution of weights

efficient exploration for domain expansion 
(lipton et al., 2016)

129

http://arxiv.org/abs/1608.05081

(cid:133) bayes by backprop q-network (bbq)

    weight posteriors are maintained

(cid:190) combine rl and bayes-by-bp
(cid:190) use variational id136 to scale 

up

   

thompson sampling for exploration

[a.k.a.    posterior sampling   ]

efficient exploration accelerates policy optimization

evolution roadmap

130

l

)
y
t
i
x
e
p
m
o
c
(
 
h
t
p
e
d
e
u
g
o
a
d

i

l

 

empathetic systems

i feel sad   

i   ve got a cold what do i do?

common sense system

tell me a joke.

what is influenza?

knowledge based system

dialogue breadth (coverage)

130

high-level intention for dialogue planning 
(sun et al., 2016; sun et al., 2016)

131

http://dl.acm.org/citation.cfm?id=2856818; http://www.lrec-conf.org/proceedings/lrec2016/pdf/75_paper.pdf

(cid:133) high-level intention may span several domains
schedule a lunch with asli.

find restaurant check location contact play music

what kind of restaurants do you prefer?
the distance is    
should i send the restaurant information to asli?

users can interact via high-level descriptions and the 

system learns how to plan the dialogues

empathy in dialogue system (fung et al., 2016)

132

https://arxiv.org/abs/1605.04072

(cid:133) embed an empathy module

(cid:134) recognize emotion using multimodality
(cid:134) generate emotion-aware responses

text

speech

vision

emotion recognizer

132

visual object discovery through dialogues 
(vries et al., 2017)

133

https://arxiv.org/pdf/1611.08481.pdf

(cid:133) recognize objects using    guess what?    game
(cid:133) includes    spatial   ,    visual   ,    object taxonomy    and 

   interaction    

133

134

part v
challenges

outline

135

(cid:133) introduction and background

(cid:134) neural networks
(cid:134) id23

(cid:133) deep learning based dialogue system

(cid:134) spoken/natural language understanding (slu/nlu)
(cid:134) dialogue state tracking (dst)
(cid:134) dialogue policy
(cid:134) id86 (id86)
(cid:134) end-to-end learning for dialogue systems

(cid:133) evaluation
(cid:133) recent trends on learning dialogues
(cid:133) challenges
(cid:133) conclusion

135

challenges in dialogue modeling - i

136

(cid:133) semantic schema induction (chen et al., 2013; athanasopoulou, et al., 2014)

(cid:134) no predefined semantic schema
(cid:134) how to learn from data?

(cid:133) tractability, and id84 methods

(cid:134) learning with large state action spaces

(cid:133) end-to-end learning methods

(cid:134) learning when the user input is complex nl utterance
(cid:134) learning with humans or kbs ?
(cid:134) learning under domain shifts

136

challenges in dialogue modeling - ii

137

(cid:133) multiple-state hypothesis

(cid:134) tracking a distribution over multiple dialog states can improve dialog 

accuracy

(cid:134) how does current id71 deal with this?

(cid:133) proactive v.s. reactive approaches to dialog modeling

(cid:134) how to build dm models when the agent is proactive (i.e., does not 

wait for the user but sends messages and drives the conversation)

(cid:133) localization, personalization, etc.

(cid:134) how to deal with issue pertaining to place, temporal and personal 
context. mostly dealt on speech side. how about dm side for when 
learning the policy?

(cid:133) hierarchical rl approach to policy learning actually works?

(cid:134) when are they useful? 
(cid:134) how about for open domain systems (like chit-chat) - are they 

powerful?

137

challenges in dialogue modeling - iii

138

(cid:133) chat-bot challenges

(cid:134) consistency: keep similar answers in spite of different wordings

(cid:132) human: what is your job?
(cid:132) machine: i am lawyer
(cid:132) human: what do you do ? 
(cid:132) machine: i am a doctor

(cid:134) quick domain-dependent adaptation: specially from un-

structured data (yan et.al, 2016)

(cid:134) personalization: handling profiles, interaction levels, and keep 

relevant context history (li et al., 2016)

(cid:134) long sentence generation: most sentence are short or common 

phrases 

138

challenge summary

139

human-robot interfaces is a hot topic but several components must be integrated!

most state-of-the-art technologies are based on dnn 
    requires huge amounts of labeled data 
    several frameworks/models are available 

fast id20 with scarse data + re-use of rules/knowledge 

handling reasoning 

data collection and analysis from un-structured data 

complex-cascade systems requires high accuracy for working good as a whole 

139

140

part vi
conclusion

briefly   

141

(cid:133) we introduced recent deep learning approaches 

that are used in building dialogue models

(cid:133) we highlighted the main components of dialogue 
systems and new deep learning architectures used 
for these components

(cid:133) we talked about the challenges and new avenues 

for future research

(cid:133) we provide all the material online!
http://deepdialogue.miulab.tw

141

references 

142

(cid:133) the full list of references can be found in: 

http://deepdialogue.miulab.tw

142

acknowledgement

143

(cid:133) we thanks tsung-hsien wen, pei-hao su, li deng, 

sungjin lee, milica ga  i  , lihong li for sharing 
their slides

143

144

thanks for your attendance!

(cid:45)

