front-end

back-end

back-end system

import mxnet as mx 
a = mx.nd.zeros((100, 50)) 
b = mx.nd.ones((100, 50)) 
c = a * b 
c += 1

import mxnet as mx 
net = mx.symbol.variable('data') 
net = mx.symbol.fullyconnected( 
         data=net, num_hidden=128) 
net = mx.symbol.softmaxoutput(data=net) 
texec = mx.module.module(net) 
texec.forward(data=c) 
texec.backward()

a

   

b

+

1

c

fullc

softmax

weight

bias

    optimization 
    memory optimization 
    operator fusion 
    scheduling 
    auto-parallelization

11

memory optimization

traverse the computation graph to reduce the memory footprint    
with time complexity linear in graph size

aliveness analysis

a

d

c

b
now a is  
deletable

shared space between 

variables

a

b

c

share a and b

12

results for deep id98s
winner neural networks

training
baseline

mxnet

1.8x

2.6x

1.8x

alexnet inception

vgg

)
b
g

(
 
y
r
o
m
e
m

9
6.75
4.5
2.25
0

)

b
g

(
 
y
r
o
m
e
m

9
6.75
4.5
2.25
0

prediction

baseline

mxnet

3.2x

4.4x

4x

alexnet inception

vgg

13

trade computation for memory

forward

backword

forward

backward

backward

segment 1

segment 2

only the 
segment 
head node 
results are 
stored  

re-

compute 
results 

re-

compute 
results 

    needs an extra forward pass 
    reduces the memory complexity from           to               ,    
o(n) o(pn)
where n is the number of layers

14

results on resnet

no optimization

with optimization

    batch size = 32 
    increase 30% 

computation cost 
when optimization is 
applied

)
b
g

(
 
y
r
o
m
e
m

100

10

1
100

157.4 gb

4.1 gb

1000

# of layers

15

operator fusion and runtime compilation

add

mul

x2

x0

x1

fusion

codegen

fused 
op

x0

x1

x2

fuse adam into a single operator

20% performance 

improvement on resnet

16

