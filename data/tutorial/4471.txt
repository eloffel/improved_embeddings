writing parallel programs is painful 

2-layer neural networks with 2 gpus

data[gpu0].copyfrom(data[0:50])

data = next_batch()

data[gpu0].copyfrom(data[51:100])

fc1[gpu0] = fullcforward(data[gpu0], 

fc2_wgrad[cpu]  =  

  fc2_wgrad[gpu0] + fc2_wgrad[gpu1] 

fc2[gpu0] = fullcforward(fc1[gpu0], 

fc2_weight[gpu0]) 

fc2_ograd[gpu0] = lossgrad(fc2[gpu0], 

label[0:50]) 

fc1_ograd[gpu0], fc2_wgrad[gpu0] = 
fullcbackward(fc2_ograd[gpu0] , 

fc2_weight[gpu0]) 

_, fc1_wgrad[gpu0] =  

fullcbackward(fc1_ograd[gpu0] , 

fc1_weight[gpu0]) 

fc2_weight[cpu] -= 
lr*fc12_wgrad[gpu0]  

fc2_weight[cpu].copyto( 

   fc2_weight[gpu0] , fc2_weight[gpu1]) 

fc1_wgrad[cpu]  =  

  fc1_wgrad[gpu0] + fc1_wgrad[gpu1] 

fc1_weight[cpu] -= lr *  

fc1_wgrad[gpu0]  

fc1_weight[cpu].copyto( 

    fc1_weight[gpu0] , fc1_weight[gpu1]) 

fc1[gpu1] = fullcforward(data[gpu1], 

fc1_weight[gpu1]) 

fc2[gpu1] = fullcforward(fc1[gpu1], 

fc2_weight[gpu1]) 

fc2_ograd[gpu1] = lossgrad(fc2[gpu1], 

label[51:100]) 

fc1_ograd[gpu1], fc2_wgrad[gpu1] = 
fullcbackward(fc2_ograd[gpu1] , 

fc2_weight[gpu1]) 

_, fc1_wgrad[gpu1] =  

fullcbackward(fc1_ograd[gpu1] , 

a network may have 
hundreds of layers

17

auto parallelization

write serial programs

>>> import mxnet as mx 
>>> a = mx.nd.ones((2,2)) *2 
>>> c = a + 2 
>>> b = a + 1  
>>> d = b * c

run in parallel

a = 2

c = a + 2

b = a + 1

d = b     c

18

data parallelism

key-value store

1. read a data partition 
2. pull the parameters 
3. compute the gradient 
4. push the gradient 
5. update the parameters

examples

19

distributed computing

a user does not need 
to change the codes 
when using multiple 

machines

key-value store

multiple  

server machines

push and pull    
over network

multiple  

worker machines

read over network

examples
examples

store data in  

a distributed    lesystem

20

scale to multiple gpu machines

hierarchical parameter server

1.25 gb/s  
10 gbit ethernet

15.75 gb/s  
pcie 3.0 16x

63 gb/s  
4 pcie 3.0 16x

network switch

cpu

pcie switch

g
p
u

g
p
u

g
p
u

g
p
u

cpus

gpus

level-2 servers

level-1 servers

workers

21

experiment setup

      
    1.2 million images with 1000 classes 
    resnet 152-layer model 
    ec2 p2.16xlarge

    minibatch sgd 
    synchronized updating 

cpu

pcie switches

gpu 0-15

22

scalability over multiple machines

1

0.75

0.5

0.25

h
t
a
b
 
/
 
)
c
e
s
(
 
e
m

i
t

0

0

115x

comm cost
batch size/gpu=2
batch size/gpu=4
batch size/gpu=8
batch size/gpu=16

32

64

96

128

# of gpus

23

    increase learning 

rate by 5x 

    increase learning 

rate by 10x, decrease 
it at epoch 50, 80

convergence

0.8

0.625

0.45

0.275

 

y
c
a
r
u
c
c
a
n
o
i
t
a
d
i
l
a
v
 
1
-
p
o
t

0.1

0

24

batch size=256
batch size=2,560
batch size=5,120

30

60

90

120

# of epochs

time to achieve 22.5% top-1 accuracy

8 gpus

80 gpus

160 gpus

9.6x

18.8x

~ 1 week

~ 1 day

~ half day

0

50 100 150 200 250

hour

25

