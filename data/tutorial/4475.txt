deep	residual	networks
deep	learning	gets	way	deeper

8:30-10:30am,	june	19

icml	2016	tutorial

kaiming	he

facebook	ai	research*

*as	of	july	2016.	formerly	affiliated	with	microsoft	research	asia

2
/
ol
o
p

	
,

2
/
	
,

4
6

	
,
v
n
o
c
	

7
x
7

4
6

	
,
v
n
o
c
	
1
x
1

4
6

	
,
v
n
o
c
	
3
x
3

6
5
2

	
,
v
n
o
c
	
1
x
1

4
6

	
,
v
n
o
c
	
1
x
1

4
6

	
,
v
n
o
c
	
3
x
3

6
5
2

	
,
v
n
o
c
	
1
x
1

4
6

	
,
v
n
o
c
	
1
x
1

4
6

	
,
v
n
o
c
	
3
x
3

6
5
2

	
,
v
n
o
c
	
1
x
1

2
/
	
,

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
1
x
1

8
2
1

	
,
v
n
o
c
	
3
x
3

2
1
5

	
,
v
n
o
c
	
1
x
1

2
/
	
,

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
1
x
1

6
5
2

	
,
v
n
o
c
	
3
x
3

4
2
0
1

	
,
v
n
o
c
	
1
x
1

2
/
	
,

2
1
5

	
,
v
n
o
c
	
1
x
1

2
1
5

	
,
v
n
o
c
	
3
x
3

8
4
0
2

	
,
v
n
o
c
	
1
x
1

2
1
5

	
,
v
n
o
c
	
1
x
1

2
1
5

	
,
v
n
o
c
	
3
x
3

8
4
0
2

	
,
v
n
o
c
	
1
x
1

2
1
5

	
,
v
n
o
c
	
1
x
1

2
1
5

	
,
v
n
o
c
	
3
x
3

8
4
0
2

	
,
v
n
o
c
	
1
x
1

0
0
0
1
	
c
f
	
,
l

o
o
p
e
v
a

	

overview

    introduction
    background

    from	shallow	to	deep

    deep	residual	networks

    from	10	layers	to	100	layers
    from	100	layers	to	1000	layers

    applications
    q	&	a

introduction

introduction

deep	residual	networks	(resnets)

       deep	residual	learning	for	image	recognition   .	cvpr	2016	(next	week)

    a	simple	and	clean	framework	of	training	   very   	deep	nets

    state-of-the-art	performance	for

    image	classification
    object	detection
    semantic	segmentation
    and	more   

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

resnets	@	ilsvrc	&	coco	2015	competitions

    1st	places	in	all	five	main	tracks

    id163	classification:	   ultra-deep   	152-layer nets	
    id163	detection: 16% better	than	2nd
    id163	localization: 27% better	than	2nd
    coco	detection: 11% better	than	2nd
    coco	segmentation: 12% better	than	2nd

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

*improvements	are	relative	numbers

revolution	of	depth

152	layers

28.2

25.8

16.4

11.7

22	layers

19	layers

6.7

7.3

3.57

8	layers

8	layers

shallow

ilsvrc'15	
resnet

ilsvrc'14	
googlenet

ilsvrc'14

vgg

ilsvrc'13

ilsvrc'12	
alexnet

ilsvrc'11

ilsvrc'10

id163	classification	top-5	error	(%)
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

revolution	of	depth

alexnet,	8	layers
(ilsvrc	2012)

11x11	conv,	96,	/4,	pool/2

5x5	conv,	256,	pool/2

3x3	conv,	384

3x3	conv,	384

3x3	conv,	256,	pool/2

fc,	4096

fc,	4096

fc,	1000

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

revolution	of	depth

vgg,	19	layers
(ilsvrc	2014)

alexnet,	8	layers
(ilsvrc	2012)

11x11	conv,	96,	/4,	pool/2

5x5	conv,	256,	pool/2

3x3	conv,	384

3x3	conv,	384

3x3	conv,	256,	pool/2

fc,	4096

fc,	4096

fc,	1000

googlenet,	22	layers

(ilsvrc	2014)

3x3	conv,	64

3x3	conv,	64,	pool/2

3x3	conv,	128

3x3	conv,	128,	pool/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

fc,	4096

fc,	4096

soft max2

soft maxact ivat ion

fc

averagepool 
7x7+ 1(v)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

soft max1

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

soft maxact ivat ion

maxpool 
3x3+ 2(s)

dept hconcat

fc

fc

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

averagepool 
5x5+ 3(v)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

soft max0

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

soft maxact ivat ion

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

fc

fc

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

averagepool 
5x5+ 3(v)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

maxpool 
3x3+ 2(s)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

dept hconcat

conv

1x1+ 1(s)

conv

3x3+ 1(s)

conv

5x5+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

conv

1x1+ 1(s)

maxpool 
3x3+ 1(s)

maxpool 
3x3+ 2(s)

localrespnorm

conv

3x3+ 1(s)

conv

1x1+ 1(v)

localrespnorm

maxpool 
3x3+ 2(s)

conv

7x7+ 2(s)

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

input

fc,	1000

revolution	of	depth

alexnet,	8	layers
(ilsvrc	2012)

11x11	conv,	96,	/4,	pool/2

5x5	conv,	256,	pool/2

3x3	conv,	384

3x3	conv,	384

3x3	conv,	256,	pool/2

fc,	4096

fc,	4096

fc,	1000

vgg,	19	layers
(ilsvrc	2014)

3x3	conv,	64

3x3	conv,	64,	pool/2

3x3	conv,	128

3x3	conv,	128,	pool/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512,	pool/2

fc,	4096

fc,	4096

fc,	1000

resnet,	152	layers

(ilsvrc	2015)

7x7	conv,	64,	/2,	pool/2

1x1	conv,	64

3x3	conv,	64

1x1	conv,	256

1x1	conv,	64

3x3	conv,	64

1x1	conv,	256

1x1	conv,	64

3x3	conv,	64

1x1	conv,	256

1x1	conv,	128,	/2

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	128

3x3	conv,	128

1x1	conv,	512

1x1	conv,	256,	/2

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	256

3x3	conv,	256

1x1	conv,	1024

1x1	conv,	512,	/2

3x3	conv,	512

1x1	conv,	2048

1x1	conv,	512

3x3	conv,	512

1x1	conv,	2048

1x1	conv,	512

3x3	conv,	512

1x1	conv,	2048

ave	pool,	fc	1000

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

revolution	of	depth

engines	of

visual	recognition

34

shallow

hog,	dpm

58

8	layers

alexnet
(rid98)

101	layers

86

66

16	layers

vgg
(rid98)

resnet

(faster	rid98)*

pascal	voc	2007	object	detection	map (%)

*w/	other	improvements	&	more	data
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

resnet   s object	detection	result	on	coco

*the	original	image	is	from	the	coco	dataset
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

very	simple,	easy	to	follow

    many	third-party	implementations	(list	in	https://github.com/kaiminghe/deep-residual-networks)

facebook	ai	research   s	torch	resnet:
torch,	cifar-10,	with	resnet-20	to	resnet-110,	training	code,	and	curves:	code
lasagne,	cifar-10,	with	resnet-32	and	resnet-56	and	training	code:	code

   
   
   
    neon,	cifar-10,	with	pre-trained	resnet-32	to	resnet-110	models,	training	code,	and	curves:	code
   
    a	winning	entry	in	kaggle's right	whale	recognition	challenge:	blog,	code
    neon,	place2	(mini),	40	layers:	blog,	code
       

torch,	mnist,	100	layers:	blog,	code

    easily	reproduced	results	(e.g.	torch	resnet:	https://github.com/facebook/fb.resnet.torch)
    a	series	of	extensions	and	follow-ups

    >	200	citations	in	6	months	after	posted	on	arxiv (dec.	2015)

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

background

from	shallow	to	deep

traditional	recognition

but	what   s	next?

classifier

   bus   ?

pixels

edges

classifier

   bus   ?

sift/hog

edges

histogram

classifier

   bus   ?

shallower

deeper

edges

histogram

id116/
sparse	code

classifier

   bus   ?

deep	learning

specialized	components,	 domain	knowledge	required

edges

histogram

id116/
sparse	code

classifier

   bus   ?

generic	components	(   layers   ),	less	domain	knowledge

repeat	elementary	layers	=>	going	deeper	

    end-to-end	learning
    richer	solution	space

   bus   ?

   bus   ?

spectrum	of	depth

5	layers:	easy

>10	layers:	initialization,	batch	id172

>30	layers:	skip	connections

>100	layers:	identity	skip	connections

>1000	layers:	?

deeper

shallower

initialization

weight    

input    
    +,

output

    =        
    567

       ,    ,    :	independent

if:
    linear	activation

then:

1-layer:                 =(    +,                )            [    ]
)            [    ]
                 =(2    3+,                3

multi-layer:

3

lecun et	al	1998	   efficient	backprop   
glorot &	bengio 2010	   understanding	the	difficulty	of	training	deep	feedforward	neural	networks   

both	forward	(response)	and	backward	(gradient)	
signal	can	vanish/explode

forward:

initialization

                 =(2    3+,                3
                          =(2    3567                3

backward:

3
3

)            [    ]
)            [            ]

exploding

ideal

vanishing

11

13

15

1

3

5

7

9

depth

lecun et	al	1998	   efficient	backprop   
glorot &	bengio 2010	   understanding	the	difficulty	of	training	deep	feedforward	neural	networks   

initialization

    initialization	under	linear assumption

=                    >? (healthy	forward)
=                    @?(healthy	backward)
*:	    3567=    3bc+, ,	so	

and

        3+,                3
3
        3567                3
3
    3+,                3 =1
    3567                3 =1

or*

d5,e7hg=,ijklmno,hpqkl
rs <   .
d5,e7fg

it	is	sufficient	to	use	either	form.

lecun et	al	1998	   efficient	backprop   
glorot &	bengio 2010	   understanding	the	difficulty	of	training	deep	feedforward	neural	networks   

   xavier   	init in	caffe

initialization

    initialization	under	relu activation

    cv    3+,                3
3
    cv    3567                3
3
12    3+,                3 =1
12    3567                3 =1

or

and

=                    >? (healthy	forward)
=                    @?(healthy	backward)
with	     layers,	a	factor	of	2 per	layer	has	
exponential	impact	of	2y

   msra   	init in	caffe

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   delving	deep	into	rectifiers:	surpassing	human-level	performance	on	id163	classification   .	iccv	2015.

initialization

22-layer	relu net:

good	init converges	faster

30-layer	relu net:

good	init is	able	to	converge

12                     =1
                     =1

xavier

ours

12                     =1
                     =1

ours
xavier

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   delving	deep	into	rectifiers:	surpassing	human-level	performance	on	id163	classification   .	iccv	2015.

*figures	show	the	beginning	of	training

batch	id172	(bn)

    normalizing	input	(lecun et	al	1998	   efficient	backprop   )

    bn:	normalizing	each	layer,	for	each	mini-batch

    greatly	accelerate	training

    less	sensitive	to	initialization

    improve	id173

s.	ioffe &	c.	szegedy.	batch	id172:	accelerating	deep	network	training	by	reducing	internal	covariate	shift.	icml	2015

batch	id172	(bn)

layer

    
       :	mean	of	     in	mini-batch
       :	std of	     in	mini-batch
       :	scale
       :	shift

    z=               
    =        z+    
       ,	    :	functions	of	    ,
       ,    :	parameters	to	be	learned,	

analogous	to	responses

analogous	to	weights

s.	ioffe &	c.	szegedy.	batch	id172:	accelerating	deep	network	training	by	reducing	internal	covariate	shift.	icml	2015

layer

    

batch	id172	(bn)

    z=               
       ,	     are	functions	of	    ;	backprop gradients
       ,	     are	pre-computed*	on	training	set

2	modes	of	bn:
    train	mode:

    test	mode:

    =        z+    

caution:	make	sure	your	bn	
is	in	a	correct	mode

s.	ioffe &	c.	szegedy.	batch	id172:	accelerating	deep	network	training	by	reducing	internal	covariate	shift.	icml	2015

*:	by	running	 average,	or	post-processing	 after	training

batch	id172	(bn)

best	of	w/	bn

w/o	bn

y
c
a
r
u
c
c
a

s.	ioffe &	c.	szegedy.	batch	id172:	accelerating	deep	network	training	by	reducing	internal	covariate	shift.	icml	2015

iter.

figure	taken	from	[s.	ioffe &	c.	szegedy]

deep	residual	networks

from	10	layers	to	100	layers

going	deeper

    initialization	algorithms	   
    batch	id172    

    is	learning	better	networks	as	simple	as	stacking	more	layers?

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

simply	stacking	layers?

train	error	(%)

cifar-10

test	error	(%)

20

10

20

10

56-layer

20-layer

56-layer

20-layer

0 
0

1

2

3

iter. (1e4)

4

5

6

0
0

1

2

3

iter. (1e4)

4

5

6

    plain nets:	stacking	3x3	conv	layers   
    56-layer	net	has	higher	training	error and	test	error	than	20-layer	net

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

simply	stacking	layers?

cifar-10

id163-1000

)

%

(
 
r
o
r
r
e

20

10

5

0
0

plain-20
plain-32
plain-44
plain-56

1

56-layer
44-layer
32-layer
20-layer

2

3

iter. (1e4)

4

5

6

solid:	test/val
dashed:	train

)

%

(
 
r
o
r
r
e

60

50

40

30

20
0

34-layer

18-layer

plain-18
plain-34
10

20

30
iter. (1e4)

40

50

       overly	deep   	plain	nets	have	higher	training	error
    a	general	phenomenon,	observed	in	many	datasets

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

a	shallower

model

(18	layers)

7x7	conv,	64,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256,	/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

fc	1000

   extra   	
layers

7x7	conv,	64,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256,	/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

fc	1000

a	deeper
counterpart
(34	layers)
    richer	solution	space

    a	deeper	model	should	not	have	higher	

training	error

    a	solution	by	construction:

    original	layers:	copied	from	a	

learned	shallower	model

    extra	layers:	set	as	identity
    at	least	the	same	training	error

    optimization	difficulties:	solvers	cannot	

find	the	solution	when	going	deeper   

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

         is	any	desired	mapping,
hope	the	2	weight	layers	fit	    (    )

deep	residual	learning

    plaint	net

any	two

stacked	layers

    
    (    )

weight	layer
relu
weight	layer
relu

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

deep	residual	learning

    residual net

    
    (    )
         =         +    

weight	layer
relu
weight	layer

         is	any	desired	mapping,
hope	the	2	weight	layers	fit	    (    )
hope the	2	weight	layers	fit	    (    )
let	         =         +    

identity    

relu

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

deep	residual	learning

            is	a	residual mapping	w.r.t.	identity
    
    (    )
identity    
         =         +    

weight	layer
relu
weight	layer

relu

    if	identity	were	optimal,
easy	to	set	weights	as	0

    if	optimal	mapping	is	closer	to	identity,

easier	to	find	small	fluctuations

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

related	works	    residual	representations

    vlad	&	fisher	vector	[jegou et	al	2010],	[perronnin et	al	2007]

    encoding	residual vectors;	powerful	shallower	representations.

    product	quantization	(ivf-adc)	[jegou et	al	2011]

    quantizing	residual vectors;	efficient	nearest-neighbor	search.

    multigrid	&	hierarchical	precondition	[briggs,	et	al	2000],	[szeliski	1990,	2006]

    solving	residual sub-problems;	efficient	pde	solvers.

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

network	   design   

plain	net

    keep	it	simple

    our	basic	design (vgg-style)

    all	3x3	conv	(almost)
    spatial	size	/2		=>	#	filters	x2	(~same	complexity	per	layer)
    simple	design;	just	deep!

    other	remarks:
    no	hidden	fc
    no	dropout

resnet

7x7	conv,	64,	/2

7x7	conv,	64,	/2

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	128,	/2

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256,	/2

3x3	conv,	256,	/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	512,	/2

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

fc	1000

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

fc	1000

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

training

    all	plain/residual	nets	are	trained	from	scratch

    all	plain/residual	nets	use	batch	id172

    standard	hyper-parameters	&	augmentation

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

cifar-10	experiments

cifar-10	plain	nets

cifar-10	resnets

)

%

(
 
r
o
r
r
e

20

10

5

0
0

plain-20
plain-32
plain-44
plain-56

1

2

3

iter. (1e4)

4

5

6

56-layer
44-layer
32-layer
20-layer

solid:	test
dashed:	train

20

)

%

(
 
r
o
r
r
e

10

5

0
0

resnet-20
resnet-32
resnet-44
resnet-56
resnet-110

20-layer
32-layer
44-layer
56-layer
110-layer

1

2

3

iter. (1e4)

4

5

6

    deep	resnets	can	be	trained	without	difficulties
    deeper	resnets	have	lower	training	error,	and	also	lower	test	error

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

id163	experiments

id163	plain	nets

id163	resnets

)

%

(
 
r
o
r
r
e

60

50

40

30

20
0

plain-18
plain-34
10

solid:	test
dashed:	train

20

30
iter. (1e4)

40

50

34-layer

18-layer

)

%

(
 
r
o
r
r
e

60

50

40

30

20
0

resnet-18
resnet-34

10

20

30
iter. (1e4)

40

50

18-layer

34-layer

    deep	resnets	can	be	trained	without	difficulties
    deeper	resnets	have	lower	training	error,	and	also	lower	test	error

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

id163	experiments

    a	practical	design	of	going	deeper

64-d

3x3,	64
relu

3x3,	64

relu

all-3x3

256-d

1x1,	64
relu
3x3,	64
relu
1x1,	256

relu

similar	
complexity

bottleneck

(for	resnet-50/101/152)

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

id163	experiments

this	model	has

lower	time	complexity

than	vgg-16/19

    deeper resnets	have	lower error

7.4

6.7

5.7

6.1

8

7

6

5

4

resnet-152

resnet-101
resnet-50
10-crop testing,	top-5	val error	(%)

resnet-34

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

id163	experiments

152	layers

28.2

25.8

16.4

11.7

22	layers

19	layers

6.7

7.3

3.57

8	layers

8	layers

shallow

ilsvrc'15	
resnet

ilsvrc'14	
googlenet

ilsvrc'14

vgg

ilsvrc'13

ilsvrc'12	
alexnet

ilsvrc'11

ilsvrc'10

id163	classification	top-5	error	(%)
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

discussions

representation,	optimization,	generalization

issues	on	learning	deep	models

    representation ability

    ability	of	model	to	fit	training	data,	if	

   

optimum	could	be	found
if	model	a   s	solution	space	is	a	superset	of	
b   s,		a	should	be	better.

    optimization ability

    feasibility	of	finding	an	optimum
    not	all	models	are	equally	easy	to	optimize

    generalization ability

    once	training	data	is	fit,	how	good	is	the	test	

performance

how	do	resnets	address	these	issues?

    representation ability

    no	explicit	advantage	on	representation	

(only	re-parameterization),	but

    allow	models	to	go	deeper

    optimization ability

    enable	very	smooth	forward/backward	prop
    greatly	ease	optimizing	deeper models	

    generalization ability

    not	explicitly	address	generalization,	but
    deeper+thinner is	good	generalization

on	the	importance	of

identity	mapping

from	100	layers	to	1000	layers

    (        )

on	identity	mappings	for	optimization

shortcut	mapping:	    =	identity
    after-add	mapping:	     =	relu

   

        
    cbc=    (       c +        c)

   (    c)

layer

layer

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

    (        )

on	identity	mappings	for	optimization

shortcut	mapping:	    =	identity
    after-add	mapping:	     =	relu
    what	if	     =	identity?

   

        
    cbc=    (       c +        c)

   (    c)

layer

layer

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

    (        )

on	identity	mappings	for	optimization

shortcut	mapping:	    =	identity
    after-add	mapping:	     =	relu
    what	if	     =	identity?

   

        
    cbc=    (       c +        c)

   (    c)

layer

layer

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

very	smooth	forward	propagation	

    cbc=    c+        c
    cbv=    cbc+        cbc

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

very	smooth	forward	propagation	

    cbc=    c+        c
    cbv=    cbc+        cbc
    cbv=    c+        c +        cbc

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

very	smooth	forward	propagation	

    cbc=    c+        c
    cbv=    cbc+        cbc
    cbv=    c+        c +        cbc
gic
    g=    c+h        +
+jc

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

very	smooth	forward	propagation	

gic
    g=    c+h        +
+jc
    any	    c is	directly forward-prop	to	any	    g,	
    any	    g is	an	additive	outcome.
in	contrast	to multiplicative:    g=        +    c
gic+jc

plus residual.

   

7x7	conv,	64,	/2

7x7	conv,	64,	/2

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	256,	/2

3x3	conv,	128,	/2

    c
    g

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256,	/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

fc	1000

fc	1000

very	smooth	backward	propagation	

gic
    g=    c+h        +
+jc
                c=                g        g        c=                g(1+             ch        +
gic
+jc

)

7x7	conv,	64,	/2

7x7	conv,	64,	/2

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	256,	/2

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

                c
                g

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256,	/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

fc	1000

fc	1000

very	smooth	backward	propagation	

                c=                g(1+             ch        +
)
gic
+jc
    any	lmlno is	directly back-prop	to	any	lmlnp,	
    any	lmlnp is additive;	unlikely	to	vanish
    in	contrast	to multiplicative:	lmlnp=        +lmlno
gic+jc

plus residual.

7x7	conv,	64,	/2

7x7	conv,	64,	/2

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	256,	/2

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

                c
                g

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

pool,	/2

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	64

3x3	conv,	128,	/2

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	128

3x3	conv,	256,	/2

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	256

3x3	conv,	512,	/2

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

3x3	conv,	512

avg	pool

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

fc	1000

fc	1000

residual	for	every	layer

gic
    g=    c+h        +
+jc
                c=                g(1+             ch        +
gic
+jc

forward:

backward:

enabled	by:
   

shortcut	mapping:	    =	identity
    after-add	mapping:	     =	identity
)

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

experiments	

    set	1:	what	if	shortcut	mapping	       	identity
    set	2:	what	if	after-add	mapping	     is	identity

    experiments	on	resnets	with	more	than	100	layers
    deeper	models	suffer	more	from	optimization	difficulty

what	if	shortcut	mapping	       	identity?	

experiment	set	1:

error:	6.6%

        =    
        = gate   	    
        = conv(    )

error:	8.7%
*similar	to	   highway	
network   

error:	12.2%

3x3	conv
relu
3x3	conv

3x3	conv
relu
3x3	conv

addition
relu

(a) original

0.5

0.5

addition
relu

(b) constant scaling

3x3	conv
relu
3x3	conv

1x1	conv
sigmoid
1-	

3x3	conv
relu
3x3	conv

1x1	conv
sigmoid
1-	

addition
relu

(c) exclusive gating

addition
relu

(d) shortcut-only gating

*	resnet-110	on	cifar-10	

error:	12.4%

        =0.5    
        = gate   	    
        = dropout(    )

error:	12.9%

error:	>	20%

3x3	conv
relu
3x3	conv

3x3	conv
relu
3x3	conv

dropout

1x1	conv

addition
relu

(e) conv shortcut
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

(f) dropout shortcut

addition
relu

error:	6.6%

        =    
        = gate   	    
        = conv(    )

error:	8.7%

error:	12.2%

*	resnet-110	on	cifar-10	

3x3	conv
relu
3x3	conv

3x3	conv
relu
3x3	conv

addition
relu

(a) original

0.5

0.5

addition
relu

(b) constant scaling

3x3	conv
relu
3x3	conv

1x1	conv
sigmoid
1-	

3x3	conv
relu
3x3	conv

1x1	conv
sigmoid
1-	

shortcuts	
(c) exclusive gating
blocked	by	

multiplications

3x3	conv
relu
3x3	conv

addition
relu

(d) shortcut-only gating

3x3	conv
relu
3x3	conv

dropout

addition
relu

1x1	conv

addition
relu

error:	12.4%

        =0.5    
        = gate   	    
        = dropout(    )

error:	12.9%

error:	>	20%

(e) conv shortcut
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

(f) dropout shortcut

addition
relu

if	    is	multiplicative,	e.g.	        =      
    g=  gic    c+h    u    +
gic
+jc
                c=                g(  gic+             ch    u    +
gic
+jc

   

forward:

backward:

*assuming	     =	identity

if	    is	multiplicative,	
)

shortcuts	are	blocked

    direct	propagation	is	decayed

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

solid:	test
dashed:	train

3x3	conv
relu
3x3	conv

    is	gating
    is	identity

1x1	conv
sigmoid
1-	

addition
relu

3x3	conv
relu
3x3	conv

addition
relu

    gating	should	have	better	representation	

ability	(identity	is	a	special	case),	but

    optimization	difficulty	dominates	results		

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

what	if	after-add	mapping	     is	identity

experiment	set	2:

xl

xl

xl

weight

bn

relu

weight

bn

weight

bn

relu

weight

addition

bn

relu

weight

bn

relu

addition

relu
xl+1

     is	relu

(original	resnet)

bn

relu
xl+1

     is	bn+relu

weight

addition
xl+1

     is	identity

(pre-activation resnet)

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

solid:	test
dashed:	train

    = bn+relu
    = relu

xl

xl

weight

bn

relu

weight

bn

relu

weight

bn

addition

    = relu

relu
xl+1

weight

bn

addition

    = bn+relu

relu
xl+1

    bn	could	block	prop
    keep	the	shortest	pass	as	

smooth	as	possible

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

1001-layer resnets	on	cifar-10

xl

xl

weight

bn

relu

bn

relu

weight

solid:	test
dashed:	train

    = identity

    = relu

weight

bn

addition

    = relu

relu
xl+1

bn

relu

weight

    = identity

addition
xl+1

    relu	could	block	prop	when	there	

are	1000	layers

    pre-activation	design	eases	

optimization	(and	improves	generalization;	see	paper)

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

comparisons	on	cifar-10/100

cifar-10

cifar-100

method
nin
dsn
fitnet
highway
resnet-110 (1.7m)
resnet-1202 (19.4m)
resnet-164, pre-activation	 (1.7m)
resnet-1001, pre-activation	 (10.2m) 4.92 (4.89  0.14)

error (%)
8.81
8.22
8.39
7.72
6.61
7.93
5.46

method
nin
dsn
fitnet
highway
resnet-164	(1.7m)
resnet-1001	(10.2m)
resnet-164, pre-activation (1.7m)
resnet-1001, pre-activation (10.2m) 22.71 (22.68  0.22)

error (%)
35.68
34.57
35.04
32.39
25.16
27.82
24.33

*all	based	on	moderate	augmentation
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

id163	experiments

id163	single-crop	(320x320)	val error

method
resnet-152, original
resnet-152, pre-activation
resnet-200, original
resnet-200, pre-activation
resnet-200, pre-activation

scale
scale
scale
scale

scale	+	aspect	ratio

21.3
21.1
21.8
20.7
20.1*

5.5
5.5
6.0
5.3
4.8*

data augmentation top-1	error	(%) top-5	error	(%)

*independently	reproduced	by:	

https://github.com/facebook/fb.resnet.torch/tree/master/pretrained#notes

training	code	and	models	available.		

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

summary	of	observations

    by	making	    and	     identity

    keep	the	shortest	path	as	smooth	as	possible

    forward/backward	signals	directly	flow	through	this	path

    features	of	any	layers	are	additive	outcomes

    1000-layer resnets	can	be	easily	trained	and	have	
better	accuracy

xl

addition
xl+1

bn

relu

weight

bn

relu

weight

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

future	works

    representation

    skipping	1	layer	vs.	multiple	layers?
    flat	vs.	bottleneck?
    inception-resnet [szegedy et	al	2016]
    resnet in	resnet [targ et	al	2016]
    width	vs.	depth	[zagoruyko &	komodakis 2016]

    generalization

    dropout,	maxout,	dropconnect,	   
    drop	layer	(stochastic	depth)	[huang	et	al	2016]

    optimization

    without	residual/shortcut?

xl

addition
xl+1

bn

relu

weight

bn

relu

weight

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

applications

   features	matter   

   features	matter.   	(quote	[girshick	et	al.	2014],	the	r-id98	paper)

task

id163	localization	(top-5	error)
id163	detection	(map@.5)
coco detection	(map@.5:.95)
coco	segmentation	(map@.5:.95)

2nd-place
winner	
12.0

resnets

9.0

53.6

absolute

8.5%	better!

62.1

33.5

25.1

37.3

28.2

margin
(relative)
27%
16%
11%
12%

    our	results	are	all	based	on	resnet-101
    deeper	features	are	well	transferrable

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

revolution	of	depth

engines	of

visual	recognition

34

shallow

hog,	dpm

58

8	layers

alexnet
(rid98)

101	layers

86

66

16	layers

vgg
(rid98)

resnet

(faster	rid98)*

pascal	voc	2007	object	detection	map (%)

*w/	other	improvements	&	more	data
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.

deep	learning	for	computer	vision

id163

data

backbone	
structure

classification	

network

features

pre-train

detection
network

(e.g.	r-id98)

segmentation

network
(e.g.	fcn)

   

.
.
.

human	pose
estimation
network

depth

estimation
network

target
data

fine-tune

example:	object	detection

   boat
   person

image	classification

(what?)

object	detection
(what	+	where?)

object	detection:	r-id98

figure	credit:	r.	girshick	et	al.

warped region

id98

aeroplane? no.

person? yes.

..
..

tvmonitor? no.

input	image

region	proposals

~2,000

1	id98	for	each	region

classify	regions

region-based id98	pipeline

girshick,	donahue,	 darrell,	malik.	rich	feature	hierarchies	for	accurate	object	detection	and	semantic	segmentation.	cvpr	2014

object	detection:	r-id98

    r-id98

feature

feature

feature

feature

id98

id98

id98

id98

image

end-to-end

training

pre-computed

regions-of-interest

(rois)

girshick,	donahue,	 darrell,	malik.	rich	feature	hierarchies	for	accurate	object	detection	and	semantic	segmentation.	cvpr	2014

object	detection:	fast	r-id98

    fast	r-id98

pre-computed

regions-of-interest

(rois)

feature

feature

feature

roi pooling

end-to-end

training

shared	conv	

layers

id98

image

girshick.	fast	r-id98.	iccv	2015

object	detection:	faster	r-id98

    faster	r-id98

    solely	based	on	id98
    no	external	modules
    each	step	is	end-to-end

proposals

region	proposal	net

features

roi pooling

end-to-end

training

feature	map

shaoqing ren,	kaiming	he,	ross	girshick,	&	jian	sun.	   faster	r-id98:	towards	real-time	object	detection	with	region	proposal	networks   .	nips	2015.

id98

image

object	detection

id163

data

backbone	
structure

classification	

network

features

pre-train

detection
network

detection

data

fine-tune

    alexnet
    vgg-16
    googlenet
    resnet-101
       
   plug-in   	
features

independently	

developed

fast	r-id98
faster	r-id98

    r-id98
   
   
    multibox
   
       

ssd

detectors

object	detection

    simply	   faster	r-id98	+	resnet   

proposals

classifier

roi pooling

faster	r-id98	

baseline
vgg-16

resnet-101

map@.5
41.5
48.4

map@.5:.95

region	proposal	net

21.5
27.2

feature	map

coco	detection results

resnet-101	has	28%	relative	gain	

vs	vgg-16

id98

image

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.
shaoqing	ren,	kaiming	he,	ross	girshick,	&	jian	sun.	   faster	r-id98:	towards	real-time	object	detection	with	region	proposal	networks   .	nips	2015.

object	detection

    rpn	learns proposals	by	extremely	deep	nets

    we	use	only	300	proposals	(no	hand-designed	proposals)

    add	components:
    iterative	localization
    context	modeling
    multi-scale	testing

    all	are	based	on	id98	features;	all	are	end-to-end

    all	benefit	more from	deeper features	    cumulative	gains!

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.
shaoqing	ren,	kaiming	he,	ross	girshick,	&	jian	sun.	   faster	r-id98:	towards	real-time	object	detection	with	region	proposal	networks   .	nips	2015.

resnet   s object	detection	result	on	coco

*the	original	image	is	from	the	coco	dataset
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	arxiv	2015.
shaoqing	ren,	kaiming	he,	ross	girshick,	&	jian	sun.	   faster	r-id98:	towards	real-time	object	detection	with	region	proposal	networks   .	nips	2015.

*the	original	image	is	from	the	coco	dataset
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	arxiv	2015.
shaoqing	ren,	kaiming	he,	ross	girshick,	&	jian	sun.	   faster	r-id98:	towards	real-time	object	detection	with	region	proposal	networks   .	nips	2015.

*the	original	image	is	from	the	coco	dataset
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	arxiv	2015.
shaoqing	ren,	kaiming	he,	ross	girshick,	&	jian	sun.	   faster	r-id98:	towards	real-time	object	detection	with	region	proposal	networks   .	nips	2015.

this	video	is	available	online:	https://youtu.be/wzmsmkk9vua

results	on	real	video.	models	trained	on	ms	coco	(80	categories).

(frame-by-frame;	no	temporal	processing)

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	arxiv	2015.
shaoqing	ren,	kaiming	he,	ross	girshick,	&	jian	sun.	   faster	r-id98:	towards	real-time	object	detection	with	region	proposal	networks   .	nips	2015.

more	visual	recognition	tasks

resnet-based	methods	lead	on	these	benchmarks	(incomplete	list):
    id163	classification,	detection,	localization
    ms	coco	detection,	segmentation
    pascal	voc	detection,	segmentation

resnet-101

    human	pose	estimation	[newell	et	al	2016]
    depth	estimation	[laina et	al	2016]
    segment	proposal	[pinheiro et	al	2016]
       

pascal	segmentation	leaderboard

resnet-101

pascal	detection	leaderboard

potential	applications

resnets	have

shown	outstanding	or	
promising	results	on:

visual	recognition

image	generation

(pixel	id56,	neural	art,	etc.)

natural	language	processing

(very	deep	id98)

speech	recognition
(preliminary	results)

advertising,	user	prediction

(preliminary	results)

conclusions	of	the	tutorial

    deep	residual	learning:

    ultra	deep	networks	can	be	easy	to	train
    ultra	deep	networks	can	gain	accuracy	from	depth
    ultra	deep	representations	are	well	transferrable
    now	200 layers	on	id163	and	1000	layers	on	cifar!

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

resources

    models	and	code

    our	id163	models	in	caffe:	https://github.com/kaiminghe/deep-residual-networks

    many	available	implementation
(list	in	https://github.com/kaiminghe/deep-residual-networks)

    facebook	ai	research   s	torch	resnet:	

https://github.com/facebook/fb.resnet.torch

    torch,	cifar-10,	with	resnet-20	to	resnet-110,	training	code,	and	curves:	code
    lasagne,	cifar-10,	with	resnet-32	and	resnet-56	and	training	code:	code
    neon,	cifar-10,	with	pre-trained	resnet-32	to	resnet-110	models,	training	code,	and	curves:	code
    torch,	mnist,	100	layers:	blog,	code
    a	winning	entry	in	kaggle's right	whale	recognition	challenge:	blog,	code
    neon,	place2	(mini),	40	layers:	blog,	code
       ....

kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   deep	residual	learning	for	image	recognition   .	cvpr	2016.
kaiming	he,	xiangyu	zhang,	shaoqing	ren,	&	jian	sun.	   identity	mappings	in	deep	residual	networks   .	arxiv	2016.

