y lecun

what's wrong

with

deep learning?

yann lecun
facebook ai research &
center for data science,  nyu
yann@cs.nyu.edu
http://yann.lecun.com

plan

y lecun

low-level
features

more

features

classifier

post-

processor

the motivation for convnets and deep learning: end-to-end learning

integrating feature extractor, classifier, contextual post-processor

a bit of archeology: ideas that have been around for a while

kernels with stride, non-shared local connections, metric learning...
   fully convolutional    training

what's missing from deep learning? 

1. theory
2. reasoning, id170
3. memory, short-term/working/episodic memory
4. unsupervised learning that actually works

deep learning = learning hierarchical representations

y lecun

traditional pattern recognition: fixed/handcrafted feature extractor

feature 
extractor

trainable 
classifier

mainstream modern pattern recognition: unsupervised mid-level features

feature 
extractor

mid-level
features

trainable 
classifier

deep learning: representations are hierarchical and trained

low-level
features

mid-level
features

high-level
features

trainable 
classifier

early hierarchical feature models for vision

y lecun

[hubel & wiesel 1962]: 

simple cells detect local features
complex cells    pool    the outputs of simple 
cells within a retinotopic neighborhood. 

   simple cells   

   complex 
cells   

multiple 
convolutions

pooling 
subsampling

cognitron & neocognitron [fukushima 1974-1982]

the mammalian visual cortex is hierarchical

y lecun

the ventral (recognition) pathway in the visual cortex has multiple stages
retina - lgn - v1 - v2 - v4 - pit - ait ....
lots of intermediate representations

[picture from simon thorpe]

[gallant & van essen] 

deep learning = learning hierarchical representations

y lecun

it's deep if it has more than one stage of non-linear feature transformation

low-level
feature

mid-level
feature

high-level

feature

trainable 
classifier

feature visualization of convolutional net trained on id163 from [zeiler & fergus 2013]

early networks [lecun 85, 86]

y lecun

binary threshold units
trained supervised
with    target prop   

hidden units compute a
virtual target

first convnets (u toronto)[lecun 88, 89]

y lecun

trained with backprop. 320 examples.

single layer

two layers fc

locally connected

shared weights

shared weights

- convolutions with stride (subsampling)
- no separate pooling layers

first    real    convnets at bell labs [lecun et al 89]

y lecun

trained with backprop. 
usps zipcode digits: 7300 training, 2000 test.
convolution with stride. no separate pooling.

convnet with separate pooling layer [lecun et al 90]

y lecun

filter bank +non-linearity

pooling

filter bank +non-linearity

pooling

filter bank +non-linearity

lenet1  [nips 1989]

convolutional network (vintage 1992) 

y lecun

filters-tanh     pooling     filters-tanh     pooling     filters-tanh

lenet1 demo from 1993

y lecun

running on a 486 pc with an at&t dsp32c add-on board (20 mflops!)

y lecun

integrating segmentation

multiple character recognition

multiple character recognition [matan et al 1992]

y lecun

sdnn: space displacement neural net
also known as    replicated convolutional net   , or just convnet 
    (are we going to call this    fully convolutional net    now?)
there is no such thing as a    fully connected layer   
they are actually convolutional layers with 1x1 convolution kernels.

multiple character recognition. integrated segmentation

y lecun

trained with    semi synthetic    data
    the individual character positions are known
training sample: a character painted with flanking characters or a inter-
character space

multiple character recognition. integrated segmentation

y lecun

word-level training with weak supervision [matan et al 1992]

y lecun

word-level training
no labeling of individual characters
how do we do the training?
we need a    deformable part model   

convnet

5

4

3

2

window width of 
each classifier

multiple classifiers

   deformable part model    on top of a convnet 
[driancourt, bottou 1991]

y lecun

spoken word recognition with trainable elastic word templates.
first example of id170 on top of deep learning
[driancourt&bottou 1991, bottou 1991, driancourt 1994]

object models
(elastic template)

sequence of
feature vectors

trainable feature
extractors

input sequence
(acoustic vectors)

energies

switch

lvq2 loss

warping
(latent var)

category
(output)

word-level training with elastic word models

y lecun

- isolated spoken word recognition
- trainable elastic templates and trainable feature extraction 
- globally trained at the word level
- elastic matching using dynamic time warping

- viterbi algorithm on a trellis.

trellis

energy

)
e
t
a
l
p
m
e
t
 
c
i
t
s
a
l
e
(

s
l
e
d
o
m

 
t
c
e
j
b
o

warping/path
(latent var)

sequence of
feature vectors

[driancourt&bottou 1991, bottou 1991, driancourt 1994]

the oldest example of id170 & deep learning 

y lecun

trainable automatic id103 system with a 
convolutional net (tdnn) and dynamic time warping (dtw)

the feature extractor and 
the structured classifier 
are trained 
simultanesously in an 
integrated fashion.

with the lvq2 loss :
    driancourt and 
bottou's speech 
recognizer (1991)
with neg log likelihood: 

    bengio's speech 

recognizer (1992)
    haffner's speech 
recognizer (1993)

end-to-end learning     word-level discriminative training

y lecun

energy

making every single module 
in the system trainable.

(factor graph)

language model

word hypotheses

(factor graph)

word geometry

character hypotheses

convnet or other 
deep architecture

every module is trained 
simultaneously so as to 
optimize a global loss 
function. 

includes the feature extractor, 
the recognizer, and the 
contextual post-processor 
(graphical model)

segmentation

(latent)

word image

(input)

word
(output)

problem: back-propagating 
gradients through the 
graphical model.

   shallow    id170

energy function is linear in the parameters

e ( x ,y ,z )=   i w i

t hi( x ,y ,z )

y lecun

e(x,y,z)

+

with the nll loss :

    conditional 

random field 
[lafferty, mccallum, 
pereira 2001]

with hinge loss: 

    max margin 

markov nets  and 
latent id166 [taskar, 
 altun, hofmann...]

with id88 loss
    structured 
id88 
[collins...]

params

features

w1

w2

w3

h(x,y,z)

h(x,y,z)

h(x,y,z)

outputs:

y1

y2

y3

y4

latent vars:

z1

z2

z3

input:

 x

deep id170

energy function is linear in the parameters

e ( x ,y ,z )=   i g i( x ,y , z ,w i)

graph transformer networks 

    [lecun, bottou, 

bengio,haffner 97,98]

y lecun

e(x,y,z)

+

    nll loss
    id88 loss

convnet

g(x,y,z,w)

g(x,y,z,w)

g(x,y,z,w)

outputs:

y1

y2

y3

y4

latent vars:

z1

z2

z3

input:

 x

y lecun

graph transformer 
networks

id170 
on top of deep learning

this example shows the structured 
id88 loss.

in practice, we used negative log-
likelihood loss.

deployed in 1996 in check reading 
machines.

y lecun

check reader

graph transformer network 
trained to read check amounts.
trained globally with negative-
log-likelihood loss.
50% percent correct, 49% 
reject, 1% error (detectable 
later in the process.
fielded in 1996, used in many 
banks in the us and europe.
processes an estimated 10% to 
20% of all the checks written in 
the us.

y lecun

id164

face detection [vaillant et al. 93, 94]

convnet applied to large images
heatmaps at multiple scales
non-maximum suppression for candidates
6 second on a sparcstation for 256x256 image

y lecun

mid 2000s: state of the art results on face detection

y lecun

data set->
false positives per image->

our detector

tilted

profile

mit+cmu

4.42

26.9

0.47

90%

97%

67%

3.36

83%

0.5

83%

1.28

88%

jones & viola (tilted)

90%

95%

x

jones & viola (profile)

x

70%

83%

rowley et al

89%

96%

schneiderman & kanade

86%

93%

x

x

x

x

[garcia & delakis 2003][osadchy et al. 2004] [osadchy et al, jmlr 2007]

simultaneous face detection and pose estimation

y lecun

y lecun

videos

y lecun

semantic segmentation

convnets for biological image segmentation

y lecun

biological image segmentation 
[ning et al. ieee-tip 2005]

pixel labeling with large context 
using a convnet

convnet takes a window of pixels and 
produces a label for the central pixel

cleanup using a kind of conditional 
random field (crf)

similar to a field of expert, but 
conditional.

3d version for connectomics

[jain et al. 2007]

convnet for long range adaptive robot vision 
(darpa lagr program 2005-2008)

y lecun

input  image
input  image

stereo  labels
stereo  labels

classifier  output
classifier  output

input  image
input  image

stereo  labels
stereo  labels

classifier  output
classifier  output

[hadsell et al., j. field robotics 2009]

long range vision with a convolutional net 

y lecun

pre-processing (125 ms)

     ground plane estimation
     horizon leveling
     conversion to yuv + local 

contrast id172

     scale invariant pyramid of 
distance-normalized image 
   bands   

convolutional net architecture

100 features per
3x12x25 input window

100@25x121

.
.
.

``

yuv image band
20-36 pixels tall,
36-500 pixels wide

convolutions  (6x5)

20@30x125

.
.
.

max subsampling  (1x4)

y lecun

.
.
.

20@30x484

3@36x484

yuv input

convolutions  (7x6)

scene parsing/labeling: multiscale convnet architecture

y lecun

each output sees a large input context:

46x46 window at full rez; 92x92 at    rez; 184x184 at    rez
[7x7conv]->[2x2pool]->[7x7conv]->[2x2pool]->[7x7conv]->
trained supervised on fully-labeled images

method 1: majority over super-pixel regions

y lecun

s
u
p
e
r
-
p
i
x
e
l
 

b
o
u
n
d
a
r
y

 

h
y
p
e
t
h
e
s
e
s

m
u
l
t
i
-
s
c
a
l
e
 
c
o
n
v
n
e
t

majority

vote
over

superpixels

superpixel boundaries

categories aligned
with region
boundaries

   soft    categories scores

c
o
n
v
o
l
u
t
i
o
n
a
l
 
c
l
a
s
s
i
f
i
e
r

[farabet et al. ieee t. pami 2013]

features from
convolutional net
(d=768 per pixel)

input image

scene parsing/labeling

y lecun

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling on rgb+depth images

y lecun

with temporal consistency

[couprie, farabet, najman, lecun iclr 2013, icip 2013]

scene parsing/labeling: performance

y lecun

stanford background dataset [gould 1009]: 8 categories

[rejected from cvpr 2012] 
[farabet et al. icml 2012][farabet et al. ieee t. pami 2013]

scene parsing/labeling: performance

y lecun

sift flow dataset
[liu 2009]: 
33 categories

barcelona dataset
[tighe 2010]: 
170 categories.

[farabet et al. ieee t. pami 2012]

scene parsing/labeling

y lecun

[farabet et al. icml 2012, pami 2013]

scene parsing/labeling

y lecun

no post-processing
frame-by-frame
convnet runs at 50ms/frame on virtex-6 fpga hardware

but communicating the features over ethernet limits system 
performance

then., two things happened...

the id163 dataset [fei-fei et al. 2012]

1.2 million training samples
1000 categories

matchstick

sea lion

y lecun

fast graphical processing units (gpu)

capable of 1 trillion operations/second

flute

strawberry

bathing cap

backpack

racket

very deep convnet for object recognition

y lecun

kernels: layer 1 (11x11) 

y lecun

layer 1: 3x96 kernels, rgb->96 feature maps, 11x11 kernels, stride 4

kernels: layer 1 (11x11) 

layer 1: 3x512 kernels, 7x7, 2x2 stride.

y lecun

learning in action

    how the filters in the first layer learn

y lecun

deep face

[taigman et al. cvpr 2014]
alignment
convnet
metric learning

y lecun

siamese architecture and id168

y lecun

id168:
    outputs 

corresponding to 
input samples 
that are neighbors 
in the 
neigborhood 
graph should be 
nearby

    outputs for input 
samples that are 
not neighbors 
should be far away 
from each other

make this small

make this large

dw

dw

   g w     x1      g w    x2      

   g w     x1      g w    x2      

gw     x1   

gw     x2   

gw     x1   

gw     x2   

x1

x 2

x1

x 2

similar images (neighbors 
in the neighborhood graph)

dissimilar images 
(non-neighbors in the 
neighborhood graph)

learning video features with c3d

y lecun

    c3d architecture

    8 convolution, 5 pool, 2 fully-connected layers
    3x3x3 convolution kernels
    2x2x2 pooling kernels

    dataset: sports-1m [karpathy et al. cvpr   14]

    1.1m videos of 487 different sport categories
    train/test splits are provided

du tran 

(1,2)

lubomir bourdev

(2)

rob fergus

(2,3)

lorenzo torresani
(1)

manohar paluri
(2)

(1) dartmouth college, (2) facebook ai research, (3) new york university

sport classification results

y lecun

video classification

    using a spatio-temporal convnet

y lecun

video classification

    using a spatio-temporal convnet

y lecun

video classification

    spatio-temporal convnet

y lecun

y lecun

now,

what's wrong 

with deep learning?

y lecun

missing some theory

theory

why are convnets a good architecture?

    scattering transform
    mark tygert's    complex convnet   

how many layers do we really need?

    really?

y lecun

how many effective free parameters are there in a large convnet

    the weights seem to be awfully redundant

what about local minima?

    turns out almost all the local minima are equivalent
    local minima are degenerate (very flat in most directions)
    random matrix / spin glass theory comes to the rescue
    [choromanska, henaff, mathieu, ben arous, lecun ai-stats 2015]

deep nets with relus: 
objective function is piecewise polynomial

p

if we use a hinge loss, delta now depends on label yk:
l (w )=   
c p( x ,y ,w )(    
piecewise polynomial in w with random 
coefficients
a lot is known about the distribution of critical 
points of polynomials on the sphere with random 
(gaussian) coefficients [ben arous et al.]

w ij)

(ij)   p

high-order spherical spin glasses
random matrix theory

y lecun

31

22

w31,22

w22,14

14

w14,3

3
z3

y lecun

missing: reasoning

reasoning as energy minimization (id170++)

y lecun

deep learning systems can be assembled into 
energy models aka factor graphs

energy function is a sum of factors
factors can embed whole deep learning 
systems
x: observed variables (inputs)
z: never observed (latent variables)
y: observed on training set (output 
variables)

id136 is energy minimization (map) or free 
energy minimization (marginalization) over z 
and y given an x

f(x,y) = min_z e(x,y,z)
f(x,y) = -log sum_z exp[-e(x,y,z) ]

f(x,y) = marg_z e(x,y,z)

e(x,y,z)

energy model
energy model
(factor graph)
(factor graph)

z 

(unobserved)

x 

(observed)

y

(observed on
training set)

energy-based learning [lecun et al. 2006]

push down on the energy of desired outputs
push up on everything else
[lecun et al 2006]    a tutorial on energy-based learning   

y lecun

y lecun

stick a crf on top of a convnet

pose estimation and attribute recovery with convnets

y lecun

pose-aligned network for deep attribute modeling
 [zhang et al. cvpr 2014] (facebook ai research)

real-time hand pose recovery
[tompson et al. trans. on graphics 14]

body pose estimation [tompson et al. iclr, 2014]

person detection and pose estimation 

y lecun

[tompson, goroshin, jain, lecun, bregler cvpr 2015]

person detection and pose estimation 

tompson, goroshin, jain, lecun, bregler arxiv:1411.4280 (2014)

y lecun

spatial model

start with a tree graphical model

mrf over spatial locations

y lecun

f~

   f

f ,~

f

   sf ,

s

local evidence function
   ss ,~

observed

s~

   es,

compatibility function

w~

69

   ww,~

w

   we,

e

   ee,~

e~

latent / hidden

joint distribution:

1
z

xx
,
i

j

 

i

i

,

j

~,
xx
i
i

wesfp

,

,

,

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
spatial model

y lecun

start with a tree graphical model

    and approximate it
 

fb

f

i

   f

f |

   s

f |

   f

   f

70

x
i

 

xf
i

|

xfc
i

|

fc

|

   f

fc

|

   s

   fb

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
spatial model: results

y lecun

flic(1) 
elbow

lsp(2) 
arms

flic(1) 
wrist

lsp(1) 
legs

(1)b. sapp and b. taskar. modec: multimodel decomposition models for human pose estimation. cvpr   13
(2)s. johnson and m. everingham. learning effective human pose estimation for inaccurate annotation. cvpr   11
71

y lecun

missing: memory

in natural language processing: id27

y lecun

id27 in continuous vector spaces
[bengio 2003][collobert & weston 2010]
id97 [mikolov 2011]
predict a word from previous words and/or following words

neural  net  of  some  kind

what  are  the  major  languages  spoken  in  greece  ?

compositional semantic property

y lecun

beijing     china + france = paris

embedding text (with convolutional or recurrent nets)

y lecun

embedding sentences into vector spaces

using a convolutional net or a recurrent net.

[sentence  vector]

convnet  or  recurrent  net

what  are  the  major  languages  spoken  in  greece  ?

question-answering system

y lecun

id27s 

lookup table

freebase embeddings 

lookup table

embedding 

model

embedding 

of the 
question

1-hot 

encoding 

of the 
question

question

   who  did  clooney  
   
marry  in  1987?

score

dot 
product

freebase 
subgraph
clooney

how the candidate 

answer fits the 

question

embedding 

of the 
subgraph

1-hot 

encoding 

of the 
subgraph
1987

k.preston

honolulu

subgraph of a 

candidate answer 
(here k. preston)

model

j. travolta

detection of 

freebase entity 
in the question

actor

male

lexington

ocean   s 

11
er

question-answering system

y lecun

what are bigos? 

["stew"]        ["stew"]

what are dallas cowboys colors? 

[   navy_blue", "royal_blue", "blue", "white", "silver"]  ["blue", "navy_blue", 

"white", "royal_blue", "silver"]

how is egyptian money called? 

["egyptian_pound"]      ["egyptian_pound"]
what are fun things to do in sacramento ca?     

["sacramento_zoo"]      ["raging_waters_sacramento", "sutter_s_fort", 

"b_street_theatre", "sacramento_zoo", "california_state_capitol_museum",    .]

how are john terry's children called? 

["georgie_john_terry", "summer_rose_terry"]   ["georgie_john_terry", 

"summer_rose_terry"]

what are the major languages spoken in greece?  

["greek_language", "albanian_language"] ["greek_language", "albanian_language"]

what was laura ingalls wilder famous for?       

["writer", "author"]    ["writer", "journalist", "teacher", "author"]

nlp: question-answering system

y lecun

who plays sheldon cooper mother on the big bang theory? 

["jim_parsons"] ["jim_parsons"]

who does peyton manning play football for?      

["denver_broncos"]      ["indianapolis_colts", "denver_broncos"]

who did vladimir lenin marry?   

["nadezhda_krupskaya"]  ["nadezhda_krupskaya"]

where was teddy roosevelt's house?      
["new_york_city"]       ["manhattan"]

who developed the tcp ip reference model?       

["vint_cerf", "robert_e._kahn"] ["computer_scientist", "engineer   ]

representing the world with    thought vectors   

y lecun

every object, concept or    thought    can be represented by a vector
[-0.2, 0.3, -4.2, 5.1,    ..] represent the concept    cat   
[-0.2, 0.4, -4.0, 5.1,    ..] represent the concept    dog   
the vectors are similar because cats and dogs have many properties in common
reasoning consists in manipulating thought vectors
comparing vectors for id53, information retrieval, content filtering
combining and transforming vectors for reasoning, planning, translating 
languages
memory stores thought vectors
memnn (memory neural network) is an example
at fair we want to    embed the world    in thought vectors

                            we call this world2vec

but how can neural nets remember things?

y lecun

recurrent networks cannot remember things for very long

the cortex only remember things for 20 seconds
we need a    hippocampus    (a separate memory module)

lstm [hochreiter 1997], registers
memory networks [weston et 2014] (fair), associative memory
ntm [deepmind 2014],    tape   .

recurrent  net

memory

memory network [weston, chopra, bordes 2014]

y lecun

add a short-term memory to a network

http://arxiv.org/abs/1410.3916

results on 
id53
task

y lecun

missing: 

unsupervised learning

energy-based unsupervised learning 

push down on the energy of desired outputs
push up on everything else

y lecun

seven strategies to shape the energy function

y lecun

 1. build the machine so that the volume of low energy stuff is constant

pca, id116, gmm, square ica

 2. push down of the energy of data points, push up everywhere else

max likelihood (needs tractable partition function)

 3. push down of the energy of data points, push up on chosen locations

 contrastive divergence, ratio matching, noise contrastive estimation, 
minimum id203 flow

 4. minimize the gradient and maximize the curvature around data points 

score matching

 5. train a dynamical system so that the dynamics goes to the manifold

denoising auto-encoder

 6. use a regularizer that limits the volume of space that has low energy

sparse coding, sparse auto-encoder, psd

 7. if e(y) = ||y - g(y)||^2, make g(y) as "constant" as possible.

contracting auto-encoder, saturating auto-encoder

#1: constant volume of low energy
energy surface for pca and id116

y lecun

 1. build the machine so that the volume of low energy stuff is constant

pca, id116, gmm, square ica...

pca
e (y )=   w t wy    y   2

id116,  
z constrained to 1-of-k code
e (y )=minz   i   y    w i z i   2

#6. use a regularizer that limits 
the volume of space that has low energy

y lecun

 sparse coding, sparse auto-encoder, predictive sparse decomposition

energy functions of various methods

y lecun

 2 dimensional toy dataset: spiral
 visualizing energy surface

(black = low, white = high)  

pca 

(1 code unit)

autoencoder
(1 code unit)

encoder
decoder
energy
loss
pull-up

w ' y
wz
   y    wz   2
f    y    
dimens.

      w ey    
w d z
   y    wz   2
f    y    
dimens.

sparse coding
(20 code units)
       w e z    
w d z
   y    wz   2
f    y    
sparsity

id116

(20  code units)
   
wz
   y    wz   2
f    y    
1-of-n code

y lecun

dictionary learning with 

fast approximate id136:

sparse auto-encoders

how to speed up id136 in a generative model?

y lecun

factor graph with an asymmetric factor
id136 z     y is easy

run z through deterministic decoder, and sample y

id136 y     z is hard, particularly if decoder function is many-to-one

map: minimize sum of two factors with respect to z
z* =  argmin_z  distance[decoder(z), y] + factorb(z)

examples: id116 (1of k), sparse coding (sparse), factor analysis

generative model
factor a

distance

decoder

factor b

input

y

z

latent
variable

sparse modeling: sparse coding + dictionary learning

y lecun

sparse linear reconstruction
energy  = reconstruction_error + code_prediction_error + code_sparsity

[olshausen & field 1997]

e (y i , z )=   y i   w d z   2+       j

   z j   

   y i       y   2

factor

w d z
deterministic
function

input y

       j .

features 

z

   z j   

variable

id136 is expensive: ista/fista, cgiht, coordinate descent....

y       z =argmin z e (y , z )

#6. use a regularizer that limits 
the volume of space that has low energy

y lecun

 sparse coding, sparse auto-encoder, predictive saprse decomposition

encoder architecture

y lecun

examples: most ica models, product of experts

factor b

z

latent
variable

input y

fast feed-forward model

factor a'

encoder

distance

encoder-decoder architecture

[kavukcuoglu, ranzato, lecun, rejected by every conference, 2008-2009]

train a    simple    feed-forward function to predict the result of a complex 
optimization on the data points of interest

y lecun

generative model
factor a

distance

decoder

factor b

input

y

fast feed-forward model

factor a'

encoder

distance

z

latent
variable

1. find optimal zi for all yi; 2. train encoder to predict zi from yi

y lecun

learning to perform

approximate id136:

predictive sparse decomposition

sparse auto-encoders

 sparse auto-encoder: predictive sparse decomposition (psd)

y lecun

[kavukcuoglu, ranzato, lecun, 2008     arxiv:1010.3467],
prediction the optimal code with a trained encoder
energy  = reconstruction_error + code_prediction_error + code_sparsity

e    y i , z    =   y i   w d z   2      z    ge   w e ,y i      2          j
ge(w e , y i)=shrinkage(w ey i)

   z j   

   y i       y   2

w d z

input y

       j .

z

   z j   

features 

ge   w e ,y i   

   z        z   2

regularized encoder-decoder model (auto-encoder) 
for unsupervised id171

y lecun

encoder: computes feature vector z from input x
decoder: reconstructs input x from feature vector z
feature vector: high dimensional and regularized (e.g. sparse)
factor graph with energy function e(x,z) with 3 terms:
linear decoding function and reconstruction error
non-linear encoding function and prediction error term
pooling function and id173 term (e.g. sparsity)

e (y,z )=   y    w d z   2+   z    g e (w e ,y )   2+   

   y i      y    2

w d z

j        

k     p j

2
z k
      .

input y

z

   (    z k
2)

g e(w e ,y i)

   z       z   2

features 

l2 norm within 
each pool

psd: basis functions on mnist

basis functions (and encoder matrix) are digit parts

y lecun

predictive sparse decomposition (psd): training

y lecun

training on natural images 
patches. 
12x12
256 basis functions

learned features on natural patches: 
v1-like receptive fields

y lecun

y lecun

learning to perform

approximate id136

lista

better idea: give the    right    structure to the encoder

y lecun

ista/fista: iterative algorithm that converges to optimal sparse code

input y

w e

+

sh()

z

lateral inhibition

s

ista/fista reparameterized:

lista (learned ista): learn the we and s matrices to get fast solutions

[gregor & lecun, icml 2010], [bronstein et al. icml 2012], [rolfe & lecun iclr 2013]

lista: train we and s matrices 
to give a good approximation quickly

y lecun

think of the fista flow graph as a recurrent neural net where we and s are 
trainable parameters

input y

w e

+

z

sh()

s

time-unfold the flow graph for k iterations
learn the we and s matrices with    backprop-through-time   
get the best approximate solution within k iterations

y

w e

+

sh()

s

+

sh()

s

z

learning ista (lista) vs ista/fista

y lecun

r
o
r
r
e
 
n
o
i
t
c
u
r
t
s
n
o
c
e
r

number of lista or fista iterations

lista with partial mutual inhibition matrix

y lecun

r
o
r
r
e
 
n
o
i
t
c
u
r
t
s
n
o
c
e
r

smallest elements
removed

proportion of s matrix elements that are non zero

learning coordinate descent (lcod): faster than lista

y lecun

r
o
r
r
e
 
n
o
i
t
c
u
r
t
s
n
o
c
e
r

number of lista or fista iterations

discriminative recurrent sparse auto-encoder (drsae)

lateral
inhibition

decoding
filters

x

architecture

w e

encoding
filters

()+

s

+

()+

can be repeated

l1

w d
w c

  z

  x

  y

y lecun

0

x

y

[rolfe & lecun iclr 2013]

 rectified linear units
classification loss: cross-id178
reconstruction loss: squared error
sparsity penalty: l1 norm of last hidden layer
rows of wd and columns of we constrained in unit sphere

drsae discovers manifold structure of handwritten digits

y lecun

image = prototype + sparse sum of    parts    (to move around the manifold)

convolutional sparse coding

y lecun

replace the dot products with dictionary element by convolutions.

input y is a full image
each code component zk is a feature map (an image)
each dictionary element is a convolution kernel

regular sparse coding

convolutional s.c.

y

=

.

   

k

wk

*

zk

   deconvolutional networks    [zeiler, taylor, fergus cvpr 2010]

convolutional psd: encoder with a soft sh() function 

y lecun

convolutional formulation

extend sparse coding from patch to image

patch based learning

convolutional learning

convolutional sparse auto-encoder on natural images

y lecun

filters and basis functions obtained with 1, 2, 4, 8, 16, 32, and 64 filters.

using psd to train a hierarchy of features

y lecun

phase 1: train first layer using psd

   y i      y   2

w d z

y

      .

z

   z j   

g e(w e ,y i)

   z       z   2

features 

using psd to train a hierarchy of features

y lecun

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor

y

g e(w e ,y i)

   z j   

features 

using psd to train a hierarchy of features

y lecun

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd

y

g e(w e ,y i)

   y i      y    2

w d z

   z j   

y

      .

z

   z j   

g e(w e ,y i)

   z       z   2

features 

using psd to train a hierarchy of features

y lecun

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd
phase 4: use encoder + absolute value as 2nd feature extractor

y

g e(w e ,y i)

   z j   

   z j   

g e(w e ,y i)

features 

using psd to train a hierarchy of features

y lecun

phase 1: train first layer using psd
phase 2: use encoder + absolute value as feature extractor
phase 3: train the second layer using psd
phase 4: use encoder + absolute value as 2nd feature extractor
phase 5: train a supervised classifier on top
phase 6 (optional): train the entire system with supervised back-propagation

y

g e(w e ,y i)

   z j   

   z j   

classifier

g e(w e ,y i)

features 

pedestrian detection: inria dataset. miss rate vs false 
positives

y lecun

convnet
color+skip
supervised

convnet
color+skip
unsup+sup

convnet
b&w
supervised

convnet
b&w
unsup+sup

[kavukcuoglu et al. nips 2010] [sermanet et al. arxiv 2012]

y lecun

unsupervised learning:

invariant features

learning invariant features with l2 group sparsity

y lecun

unsupervised psd ignores the spatial pooling step.
could we devise a similar method that learns the pooling layer as well?
idea [hyvarinen & hoyer 2001]: group sparsity on pools of features

minimum number of pools must be non-zero
number of features that are on within a pool doesn't matter
pools tend to regroup similar features

e (y,z )=   y    w d z   2+   z    g e (w e ,y )   2+   

j        

k     p j

2
z k

   y i      y   2

w d z

input y

g e(w e ,y i)

   z      z   2

      .

features 

z

   (    z k
2)

l2 norm within 
each pool

learning invariant features with l2 group sparsity

y lecun

idea: features are pooled in group. 

sparsity: sum over groups of l2 norm of activity in group.

[hyv  rinen hoyer 2001]:    subspace ica    

decoder only, square

[welling, hinton, osindero nips 2002]: pooled product of experts 

encoder only, overcomplete, log student-t penalty on l2 pooling

[kavukcuoglu, ranzato, fergus lecun, cvpr 2010]: invariant psd

encoder-decoder (like psd), overcomplete, l2 pooling

[le et al. nips 2011]: reconstruction ica

same as [kavukcuoglu 2010] with linear encoder and tied decoder 

[gregor & lecun arxiv:1006:0448,  2010] [le et al. icml 2012]

locally-connect non shared (tiled) encoder-decoder

input

y

encoder only (poe, ica),

decoder only or

encoder-decoder (ipsd, rica)

simple 
features 

l2 norm within 
each pool

z

   (    z k
2)

      .

invariant
features 

groups are local in a 2d topographic map

y lecun

the filters arrange 
themselves spontaneously 
so that similar filters enter 
the same pool.
the pooling units can be 
seen as complex cells
outputs of pooling units are 
invariant to local 
transformations of the input

for some it's 
translations, for others 
rotations, or other 
transformations.

image-level training, local filters but no weight sharing

y lecun

training on 115x115 images. kernels are 15x15 (not shared across 
space!)

decoder

[gregor & lecun 2010]
local receptive fields
no shared weights
4x overcomplete
l2 pooling
group sparsity over pools

reconstructed input

(inferred) code

predicted code

input

encoder

image-level training, local filters but no weight sharing

y lecun

training on 115x115 images. kernels are 15x15 (not shared across space!)

topographic maps

k obermayer and gg blasdel, journal of 
neuroscience, vol 13, 4114-4129 (monkey)

y lecun

119x119 image input

100x100 code

20x20 receptive field size

sigma=5

michael c. crair, et. al. the journal of neurophysiology 
vol. 77 no. 6 june 1997, pp. 3381-3385 (cat)

image-level training, local filters but no weight sharing

y lecun

color indicates orientation (by fitting gabors)

invariant features lateral inhibition

y lecun

replace the l1 sparsity term by a lateral inhibition matrix
easy way to impose some structure on the sparsity 

[gregor, szlam, lecun nips 2011]

invariant features via lateral inhibition: structured sparsity

y lecun

 each edge in the tree indicates a zero in the s matrix (no mutual inhibition)
sij is larger if two neurons are far away in the tree

invariant features via lateral inhibition: topographic maps

y lecun

non-zero values in s form a ring in a 2d topology

input patches are high-pass filtered

sparse auto-encoder with    slow feature    penalty

y lecun

supervised filters cifar10              sparse conv. auto-encoder               slow & sparse conv. auto-encoder
                                                                                                                 trained on youtube videos
[goroshin et al. arxiv:1412.6056]

invariant features through temporal constancy 

y lecun

object is cross-product of object type and instantiation parameters

mapping units [hinton 1981], capsules [hinton 2011]

object type

[karol gregor et al.]

object size

small

medium

large

what-where auto-encoder architecture

y lecun

decoder

st

st-1

st-2

predicted

input

w2

t

c2

t

c2

f

inferred 

code

predicted

code

  w 2
  w 2
st-2

  w 2

input

w1

w1

t

c1

t

c1

t-1

c1

t-1

c1

f       w 1

f       w 1

w1

t-2

c1

t-2

c1
f       w 1

encoder

st

st-1

low-level filters connected to each complex cell

y lecun

c1
(where)

c2
(what)

integrated supervised &
unsupervised learning

[zhao, mathieu, lecun arxiv:1506.02351]
stacked what-where auto-encoder

y lecun

y lecun

the end

y lecun

the babi tasks

questions that an ai system 
ought to be able to answer

(1) basic factoid qa with single supporting fact 

our first task consists of questions where a single supporting fact, 
previously given, provides the answer.
we test simplest cases of this, by asking for the location of a person.
a small sample of the task is thus:

john is in the playground.
bob is in the office.
where is john? a:playground

this kind of synthetic data was already used with memnns.
it can be considered the simplest case of some real world qa datasets 
such as in fader et al.,    13.

(2) factoid qa with two supporting facts

a harder task is to answer questions where two supporting statements 
have to be chained to answer the question:

john is in the playground.
bob is in the office.
john picked up the football.
bob went to the kitchen.
where is the football?  a:playground
where was bob before the kitchen? 
a:office

e.g. to answer the first question where is the football? both john 
picked up the football and john is in the playground are 
supporting facts.

again, this kind of task was already used with memnns.

(2) shuffled factoid qa with two supporting facts

    note that, to show the difficulty of these tasks for a learning 
machine with no other knowledge we can shuffle the letters 
of the alphabet and produce equivalent datasets:

sbdm ip im vdu yonrckblms.
abf ip im vdu bhhigu.
sbdm yigaus ly vdu hbbvfnoo.
abf zumv vb vdu aivgdum.
mduku ip vdu hbbvfnoo? 
a:yonrckblms
mduku znp abf fuhbku vdu aivgdum? 
a:bhhigu

(3) factoid qa with three supporting facts

similarly, one can make a task with three supporting facts:

john picked up the apple.
john went to the office.
john went to the kitchen.
john dropped the apple.
where was the apple before the kitchen? 
a:office

the first three statements are all required to answer 
this.

(4) two argument relations: subject vs. object

to answer questions the ability to differentiate and recognize 
subjects and objects is crucial.
we consider the extreme case: sentences feature re-ordered 
words:

the office is north of the bedroom.
the bedroom is north of the bathroom.
what is north of the bedroom? a:office
what is the bedroom north of? 
a:bathroom

note that the two questions above have exactly the 
same words, but in a different order, and different 
answers.

so a bag-of-words will not work.

(5) three argument relations

    similarly, sometimes one needs to differentiate three 
separate arguments, such as in the following task:

mary gave the cake to fred.
fred gave the cake to bill.
jeff was given the milk by bill.
who gave the cake to fred? a:mary
who did fred give the cake to? a:bill
what did jeff receive? a:milk
who gave the milk? a:bill

the last question is potentially the hardest for a learner 
as the first two can be answered by providing the actor 
that is not mentioned in the question.

(6) yes/no questions

    this task tests, in the simplest case possible (with a single 
supporting fact) the ability of a model to answer true/false 
type questions:

john is in the playground.
daniel picks up the milk.
is john in the classroom? a:no
does daniel have the milk? a:yes

(7) counting

    this task tests the ability of the qa system to perform 
simple counting operations, by asking about the number 
of objects with a certain property:

daniel picked up the football.
daniel dropped the football.
daniel got the milk.
daniel took the apple.
how many objects is daniel holding? 
a:two

(8) lists/sets

    while many of our tasks are designed to have single word 
answers for simplicity, this tasks tests the ability to 
produce a set of single word answers in the form of a list:

daniel picks up the football.
daniel drops the newspaper.
daniel picks up the milk.
what is daniel holding? a:milk,football

the task above can be seen as a qa task related to database 
search. 
note that we could also consider the following question types:
intersection: who is in the park carrying food?
union: who has milk or cookies?
set difference: who is in the park apart from bill?
however, we leave those for future work.

(9) simple negation

    we test one of the simplest types of negation, that of 
supporting facts that imply a statement is false:

sandra travelled to the office.
fred is no longer in the office.
is fred in the office? a:no
is sandra in the office? a:yes

the yes/no task (6) is a prerequisite.

slightly harder: we could add things like    is fred with 
sandra?   

(10) indefinite knowledge

    this task tests if we can model statements that describe 
possibilities rather than certainties:

john is either in the classroom or the 
playground.
sandra is in the garden.
is john in the classroom? a:maybe
is john in the office? a:no

the yes/no task (6) is a prerequisite.

slightly harder: we could add things like    is john with 
sandra?   

(11) basic coreference

    this task tests the simplest type of coreference, that of 
detecting the nearest referent, for example:

daniel was in the kitchen.
then he went to the studio.
sandra was in the office.
where is daniel? a:studio

next level of difficulty: flip order of last two statements, 
and it has to learn the difference between    he    and 
   she   .

much harder difficulty: adapt a real coref dataset 
into a question answer format. 

(12) conjunction

    this task tests referring to multiple subjects in a single 
statement, for example:

mary and jeff went to the kitchen.
then jeff went to the park.
where is mary? a:kitchen

(13) compound coreference

    this task tests coreference in the case where the pronoun 
can refer to multiple actors:

daniel and sandra journeyed to the 
office.
then they went to the garden.
sandra and john travelled to the kitchen.
after that they moved to the hallway.
where is daniel? a:garden

(14) time manipulation

    while our tasks so far have included time implicitly in the 
order of the statements, this task tests understanding the 
use of time expressions within the statements:
in the afternoon julie went to the park. 
yesterday julie was at school.
julie went to the cinema this evening.
where did julie go after the park? 
a:cinema

much harder difficulty: adapt a real time expression 
labeling dataset into a question answer format, e.g. 
uzzaman et al.,    12. 

(15) basic deduction

    this task tests basic deduction via inheritance of 
properties:

sheep are afraid of wolves.
cats are afraid of dogs.
mice are afraid of cats.
gertrude is a sheep.
what is gertrude afraid of? a:wolves

deduction should prove difficult for memnns because it 
effectively involves search, although our setup might 
be simple enough for it.

(16) basic induction

    this task tests basic induction via inheritance of 
properties:

lily is a swan.
lily is white. 
greg is a swan.
what color is greg? a:white

induction should prove difficult for memnns because it 
effectively involves search, although our setup might 
be simple enough for it.

(17) positional reasoning

    this task tests spatial reasoning, one of many 
components of the classical shrdlu system:

the triangle is to the right of the blue square.
the red square is on top of the blue square.
the red sphere is to the right of the blue 
square.
is the red sphere to the right of the blue 
square? a:yes
is the red square to the left of the triangle? 
a:yes

the yes/no task (6) is a prerequisite.

(18) reasoning about size

    this tasks requires reasoning about relative size of objects 
and is inspired by the commonsense reasoning examples 
in the winograd schema challenge:

the football fits in the suitcase.
the suitcase fits in the cupboard.
the box of chocolates is smaller than the 
football.
will the box of chocolates fit in the suitcase? 
a:yes
tasks 3 (three supporting facts) and 6 (yes/no) are 
prerequisites.

(19) path finding

    in this task the goal is to find the path between locations:

the kitchen is north of the hallway.
the den is east of the hallway.
how do you go from den to kitchen?  
a:west,north

this is going to prove difficult for memnns because it 
effectively involves search.
(the original memnn can also output only one word    )

(20) reasoning about agent   s motivations

    this task tries to ask why an agent performs a certain action.
    it addresses the case of actors being in a given state (hungry, 
thirsty, tired,    ) and the actions they then take:

john is hungry.
john goes to the kitchen.
john eats the apple.
daniel is hungry.
where does daniel go? a:kitchen
why did john go to the kitchen? a:hungry

one way of solving these tasks: memory 
networks!!

memnns have four component networks (which 
may or may not have shared parameters):
    i: (input feature map) this converts incoming 
data to the internal feature representation.
    g: (generalization) this updates memories 
given new input.
    o: this produces new output (in 
featurerepresentation space) given the 
memories.
    r: (response) converts output o into a 
response seen by the outside world.

experiments
    protocol: 1000 training qa pairs, 1000 for test.
   weakly supervised    methods:  
    ngram baseline, uses bag of ngram features 
from sentences that share a word with the 
question.
    lstm
fully supervised methods (for train data, have 
supporting facts labeled): 
    original memnns, and all our variants. 

action recognition results

baselines

use raw pixel inputs

use optical flows

