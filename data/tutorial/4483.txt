modeling sequential data 
with recurrent networks

chris dyer
deepmind 

carnegie mellon university

lxmls 2017

july 27, 2017

outline: part i

    neural networks as feature inducers 
    recurrent neural networks 

    application: language models 

    learning challenges and solutions 

    vanishing gradients 
    long short-term memories 

    id149 

    break

outline: part ii

    conditional sequence models 
    applications 

    translation 

    image id134 
    better learning with attention 

    applications 

    translation 

    image id134 

    implementation tricks (time permitting)

feature induction

  y = wx + b

1
m

mxi=1

2

||  yi   yi||2

f =
in id75, the goal is to learn w and b such that   
f is minimized for a dataset d consisting of m training 
instances. an engineer must select/design x carefully.

feature induction

  y = wx + b

1
m

mxi=1

2

||  yi   yi||2

f =
in id75, the goal is to learn w and b such that   
f is minimized for a dataset d consisting of m training 
instances. an engineer must select/design x carefully.

   nonid75   

h = g(vx + c)
  y = wh + b
use    naive features    x and learn their transformations 
(conjunctions, nonlinear transformation, etc.) into h.

feature induction

h = g(vx + c)
  y = wh + b
    what functions can this parametric form compute? 

    if h is big enough (i.e., enough dimensions), it can 

represent any vector-valued function to any degree of 
precision 

    this is a much more powerful regression model! 
    you can think of h as    induced features    in a linear classi   er 

    the network did the job of a feature engineer

feature induction

h = g(vx + c)
  y = wh + b
    what functions can this parametric form compute? 

    if h is big enough (i.e., enough dimensions), it can 

represent any vector-valued function to any degree of 
precision 

    this is a much more powerful regression model! 
    you can think of h as    induced features    in a linear classi   er 

    the network did the job of a feature engineer

recurrent neural networks

    lots of interesting data is sequential in nature 

    words in sentences 
    dna 
    notes in a melody 

    stock market returns 
        

    how do we represent an arbitrarily long history?

    we will train neural networks to build a representation of these arbitrarily 

big sequences

recurrent neural networks

    lots of interesting data is sequential in nature 

    words in sentences 
    dna 
    notes in a melody 

    stock market returns 
        

    how do we represent an arbitrarily long history?

    we will train neural networks to build a representation of these arbitrarily 

big sequences

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

  y

h

x

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

ht = g(vxt + uht 1 + c)
  yt = wht + b

recurrent nn

  y

h

x

  yt

ht

xt

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

recurrent nn

ht = g(vxt + uht 1 + c)
ht = g(v[xt; ht 1] + c)
  yt = wht + b

  y

h

x

  yt

ht

xt

recurrent neural networks
feed-forward nn
h = g(vx + c)
  y = wh + b

recurrent nn

ht = g(vxt + uht 1 + c)
ht = g(v[xt; ht 1] + c)
  yt = wht + b

ht 1

  y

h

x

  yt

ht

xt

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

how do we train the id56   s parameters?

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

y1

cost1

  y1

h1

x1

h0

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

recurrent neural networks
ht = g(vxt + uht 1 + c)
  yt = wht + b

f

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

recurrent neural networks

f

    the unrolled graph is a well-formed (dag) 
computation graph   we can run backprop 

    parameters are tied across time, derivatives are 

aggregated across all time steps  

    this is historically called    id26 

through time    (bptt)

parameter tying

ht = g(vxt + uht 1 + c)
  yt = wht + b

f

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

parameter tying

ht = g(vxt + uht 1 + c)
  yt = wht + b

f

y1

cost1

  y1

h1

x1

h0

u

y2

cost2

  y2

h2

x2

y3

cost3

  y3

h3

x3

y4

cost4

  y4

h4

x4

parameter tying

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

  y1

h1

x1

h0

u

parameter tying

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

  y1

h1

x1

h0

u

@f
@u

=

4xt=1

@ht
@u

@f
@ht

parameter tying

  y2

h2

x2

  y3

h3

x3

  y4

h4

x4

  y1

h1

x1

h0

u

4xt=1

=

@ht
@u

@f
@ht

@f
@u
parameter tying also came up when learning the    lters   
in convolutional networks (and in the transition matrices   
for id48s!).

parameter tying

    why do we want to tie parameters? 

    reduce the number of parameters to be learned  

    deal with arbitrarily long sequences 

    what if we always have short sequences? 

    maybe you might untie parameters, then. but you 

wouldn   t have an id56 anymore!

what else can we do?

ht = g(vxt + uht 1 + c)
  yt = wht + b

y1

cost1

  y1

h1

x1

h0

y2

cost2

  y2

h2

x2

f

y4

cost4

  y4

h4

x4

y3

cost3

  y3

h3

x3

   read and summarize   

ht = g(vxt + uht 1 + c)
  y = wh|x| + b
summarize a sequence into a single vector.   
(this will be useful later   )

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

view 2: recursive de   nition

    recall how to construct a list recursively:   
base case   
     [] is a list (the empty list)   
       
induction   
     [t | h] where t is a list and h is an atom is a list 

    id56s de   ne functions that compute representations recursively according 

to this de   nition of a list. 
    de   ne (learn) a representation of the base cases 

    learn a representation of the inductive step 

    anything you can construct recursively, you can obtain an    embedding    of 

with neural networks using this general strategy

view 2: recursive de   nition

    recall how to construct a list recursively:   
base case   
     [] is a list (the empty list)   
       
induction   
     [t | h] where t is a list and h is an atom is a list 

    id56s de   ne functions that compute representations recursively according 

to this de   nition of a list. 
    de   ne (learn) a representation of the base cases 

    learn a representation of the inductive step 

    anything you can construct recursively, you can obtain an    embedding    of 

with neural networks using this general strategy

view 2: recursive de   nition

    recall how to construct a list recursively:   
base case    
     [] is a list (the empty list)   
       
induction   
     [t | h] where t is a list and h is an atom is a list 

    id56s de   ne functions that compute representations recursively according 

to this de   nition of a list. 
    learn (or    x) a representation of the base case 
    learn a representation of the inductive step 

    anything you can construct recursively, you can obtain an 

   embedding    of with neural networks using this general strategy

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?w

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?b

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?b

p(e) =p(e1)   

p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
      

example: language model

the 
a 
and 
cat 
dog 
horse 
runs 
says 
walked 
walks 
walking 
pig 
lisbon 
sardines 
   

h

softmax

u = wh + b

pi =

exp uipj exp uj

h 2 rd
|v | = 100, 000
what are the   
dimensions of     ?b

p(e) =p(e1)   

p(e2 | e1)   
p(e3 | e1, e2)   
p(e4 | e1, e2, e3)   
      

 istories are sequences of words   
h

example: language model

example: language model

h0

h1

x1

<s>

example: language model

  p1

softmax

h0

h1

x1

<s>

example: language model
p(tom | hsi)

tom

   

  p1

softmax

h0

h1

x1

<s>

example: language model
p(tom | hsi)

tom

   

  p1

softmax

h0

h1

x1

<s>

x2

example: language model
p(tom | hsi)

tom

   

  p1

softmax

softmax

h2

x2

h0

h1

x1

<s>

example: language model
p(tom | hsi)

   p(likes | hsi, tom)

tom

   

  p1

likes

   

softmax

softmax

h0

h1

x1

<s>

h2

x2

example: language model
p(tom | hsi)

   p(likes | hsi, tom)

   p(beer | hsi, tom, likes)

tom

   

  p1

likes

   

beer

   

softmax

softmax

softmax

h0

h1

x1

<s>

h2

x2

h3

x3

example: language model
p(tom | hsi)

   p(likes | hsi, tom)

   p(beer | hsi, tom, likes)

   p(h/si | hsi, tom, likes, beer)

beer

</s>

   

   

tom

   

  p1

likes

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

h0

h1

x1

<s>

language model training

tom

   

  p1

likes

   

beer

   

</s>

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

h0

h1

x1

<s>

language model training

likes

cost2

beer

cost3

</s>

cost4

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

h1

x1

<s>

  p1

h0

language model training

likes

cost2

beer

cost3

</s>

cost4

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

{log loss/   
cross id178

  p1

h0

h1

x1

<s>

language model training

likes

cost2

f

</s>

cost4

beer

cost3

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

{log loss/   
cross id178

  p1

h0

h1

x1

<s>

language model training

likes

cost2

f

</s>

cost4

beer

cost3

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

tom

cost1

{log loss/   
cross id178

  p1

h0

h1

x1

<s>

id56 language models

    unlike markov (id165) models, id56s never forget 

    however we will see they might have trouble learning to 

use their memories (more soon   ) 

    algorithms 

    sample a sequence from the id203 distribution 

de   ned by the id56 

    train the id56 to minimize cross id178 (aka id113) 
    what about: what is the most probable sequence?

questions?

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z
qi=t

i=2

@hi
@hi 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@f
@h1

@   y
@h4

@h2
@h1

@h3
@h2

@h4
@h3

@f
@   y

=

|

{z

t=2

q4

@ht
@ht 1

}

training challenges

ht = g(vxt + uht 1 + c)
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@f

@ht 11a @   y

=0@
|x|yt=2

@f
@h1

@h|x|

@f
@   y

@ht

training challenges
z

vxt + uht 1 + c)

}|

{

zt

ht = g(
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

what happens to gradients as you go back   
in time?
@f
@f

@ht 11a @   y

=0@
|x|yt=2

@f
@h1

@h|x|

@f
@   y

@ht

y

f

  y

h4

x4

training challenges
z

vxt + uht 1 + c)

}|

{

zt

ht = g(
  y = wh|x| + b

h1

x1

h0

h2

x2

h3

x3

y

f

  y

h4

x4

what happens to gradients as you go back   
in time?
@f
@   y

@f
@h1

@ht
@zt

@zt

@ht 11a @   y

@h|x|

=0@
|x|yt=2

@f
@f

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

@f
@h1

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

@f
@h1

@ht
@zt

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

@f
@h1

@ht
@zt
@zt
@ht 1

?

= u

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

@f
@h1

@ht
@zt
@zt
@ht 1

= u

zt

training challenges
z
=0@
|x|yt=2

}|
{
@ht 11a @   y

ht = g(
  y = wh|x| + b
@ht
@zt

vxt + uht 1 + c)

@f
@f

@h|x|

@f
@   y

@zt

= diag(g0(zt))

= u

=

@ht
@zt

@zt
@ht 1

= diag(g0(zt))u

@f
@h1

@ht
@zt
@zt
@ht 1
@ht
@ht 1

zt

vxt + uht 1 + c)

ht = g(
  y = wh|x| + b
@ht
@zt

training challenges
z
=0@
|x|yt=2
=0@
|x|yt=2

}|
{
@ht 11a @   y
diag(g0(zt))u1a @   y

@f
@f
@f
@   y

@f
@f

@h|x|

@h|x|

@f
@   y

@zt

@f
@h1

@f
@h1

zt

vxt + uht 1 + c)

ht = g(
  y = wh|x| + b
@ht
@zt

training challenges
z
=0@
|x|yt=2
=0@
|x|yt=2

@f
@f
@f
@   y
three cases: largest eigenvalue is   
exactly 1; gradient propagation is stable   
<1; gradient vanishes (exponential decay) 
>1; gradient explodes (exponential growth)

}|
{
@ht 11a @   y
diag(g0(zt))u1a @   y

@f
@f

@h|x|

@h|x|

@f
@   y

@zt

@f
@h1

@f
@h1

vanishing gradients

    in practice, the spectral radius of u is small, and gradients vanish 
    in practice, this means that long-range dependencies are dif   cult to learn 

(although in theory they are learnable) 

    solutions 

    better optimizers (second order methods, approximate second order 

methods) 

    id172 to keep the gradient norms stable across time 
    clever initialization so that you at least start with good spectra (e.g., 

start with random orthonormal matrices) 

    alternative parameterizations: lstms and grus

alternative id56s

    long short-term memories (lstms; hochreiter and 

schmidthuber, 1997) 

    id149 (grus; cho et al., 2014) 
    intuition instead of multiplying across time (which 
leads to exponential growth), we want the error to 
be constant. 
    what is a function whose jacobian has a 

spectral radius of exactly i: the identity function

memory cells

ct = ct 1 + f (xt)

c1

x1

i

c2

x2

i

c3

x3

i

c4

x4

h0

memory cells

ct = ct 1 + f (xt)

f (v) = tanh(wv + b)

c1

x1

i

c2

x2

i

c3

x3

i

c4

x4

h0

memory cells

ct = ct 1 + f (xt)
ht = g(ct)

f (v) = tanh(wv + b)

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h4

c4

x4

h0

memory cells

ct = ct 1 + f (xt)
ht = g(ct)

f (v) = tanh(wv + b)

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ct 1 + f (xt)
ht = g(ct)

note:

f (v) = tanh(wv + b)

@ct
@ct 1

= i

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ct 1 + f ([xt; ht 1])
ht = g(ct)

@ct
@ct 1

= i + "

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ct 1 + f ([xt; ht 1])
ht = g(ct)

   almost constant   

@ct
@ct 1

= i + "

h1

c1

x1

i

h2

c2

x2

i

h3

c3

x3

i

h0

y

f

  y

h4

c4

x4

memory cells

ct = ft   ct 1 + it   f ([xt; ht 1])
ht = g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))

   forget gate   
   input gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

memory cells

ct = ft   ct 1 + it   f ([xt; ht 1])
ht = g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))

   forget gate   
   input gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

memory cells

ct = ft   ct 1 + it   f ([xt; ht 1])
ht = g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))

   forget gate   
   input gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

   forget gate   
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

   forget gate   
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm variant
ct = (1   it)   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

ft = 1   it
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

lstm variant
ct = (1   it)   ct 1 + it   f ([xt; ht 1])
ht = ot   g(ct)
ft =  (ff ([xt; ht 1]))
it =  (fi([xt; ht 1]))
ot =  (fo([xt; ht 1]))

ft = 1   it
   input gate   
   output gate   

h1

c1

x1

h0

h2

c2

x2

h3

c3

x3

y

f

  y

h4

c4

x4

another visualization

figure credit: christopher olah

another visualization

figure credit: christopher olah

another visualization

forget some of the past

figure credit: christopher olah

another visualization

forget some of the past

add new memories

figure credit: christopher olah

id149 

(grus)

ht = (1   zt)   ht 1 + zt     ht
zt =  (fz([ht 1; xt]))
rt =  (fr([ht 1; xt]))
  ht = f ([rt   ht 1; xt]))

h1

x1

h0

   

h2

x2

   

h3

x3

   

f

  y

h4

x4

summary

    better gradient propagation is possible when you use 

additive rather than multiplicative/highly non-linear 
recurrent dynamics 

id56
lstm
gru

ht = f ([xt; ht 1])
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = (1   zt)   ht 1 + zt   f ([xt; rt   ht 1])

    recurrent architectures are an active area of research, 

requires a mix of mathematical analysis, creativity, 
problem-speci   c knowledge 
    (lstms are hard to beat though!)

summary

    better gradient propagation is possible when you use 

additive rather than multiplicative/highly non-linear 
recurrent dynamics 

id56
lstm
gru

ht = f ([xt; ht 1])
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = (1   zt)   ht 1 + zt   f ([xt; rt   ht 1])

    recurrent architectures are an active area of research, 

requires a mix of mathematical analysis, creativity, 
problem-speci   c knowledge 
    (lstms are hard to beat though!)

summary

    better gradient propagation is possible when you use 

additive rather than multiplicative/highly non-linear 
recurrent dynamics 

id56
lstm
gru

ht = f ([xt; ht 1])
ct = ft   ct 1 + it   f ([xt; ht 1])
ht = (1   zt)   ht 1 + zt   f ([xt; rt   ht 1])

    recurrent architectures are an active area of research, 

requires a mix of mathematical analysis, creativity, 
problem-speci   c knowledge 
    (lstms are hard to beat though!)

questions?

break?

review: unconditional lms
a language model assigns probabilities to sequences of   
words,                                  .
w = (w1, w2, . . . , w`)
we saw that it is helpful to decompose this id203   
using the chain rule, as follows:

p(w) = p(w1)     p(w2 | w1)     p(w3 | w1, w2)              

p(w` | w1, . . . , w` 1)

p(wt | w1, . . . , wt 1)

=

|w|yt=1

this reduces the id38 problem to modeling   
the id203 of the next word, given the history of   
preceding words.

unconditional lms with id56s

random variable

p(w5|w1,w2,w3,w4)

z }| {

h4

softmax

id56 hidden state

vector, length=|vocab|

h1

w1

h0

h2

w2

w1

observed   
context word

w2

vector   
(id27)

h3

w3

w3

w4

w4

conditional lms
a conditional language model assigns probabilities to 
sequences of words,                                  , given some 
w = (w1, w2, . . . , w`)
conditioning context,    .
x
as with unconditional models, it is again helpful to use   
the chain rule to decompose this id203:

p(w | x) =

p(wt | x, w1, w2, . . . , wt 1)

what is the id203 of the next word, given the history of   
previously generated words and conditioning context    ?
x

`yt=1

conditional lms

         input   
x
an author
a topic label
{spam, not_spam}
a sentence in french
a sentence in english
a sentence in english
an image
a document
a document
meterological measurements
acoustic signal
conversational history + database dialogue system response
a question + a document
a question + an image

         text output   
w
a document written by that author
an article about that topic
an email
its english translation
its french translation
its chinese translation
a text description of the image
its summary
its translation
a weather report
transcription of speech

its answer
its answer

conditional lms

         input   
x
an author
a topic label
{spam, not_spam}
a sentence in french
a sentence in english
a sentence in english
an image
a document
a document
meterological measurements
acoustic signal
conversational history + database dialogue system response
a question + a document
a question + an image

         text output   
w
a document written by that author
an article about that topic
an email
its english translation
its french translation
its chinese translation
a text description of the image
its summary
its translation
a weather report
transcription of speech

its answer
its answer

conditional lms

         input   
x
an author
a topic label
{spam, not_spam}
a sentence in french
a sentence in english
a sentence in english
an image
a document
a document
meterological measurements
acoustic signal
conversational history + database dialogue system response
a question + a document
a question + an image

         text output   
w
a document written by that author
an article about that topic
an email
its english translation
its french translation
its chinese translation
a text description of the image
its summary
its translation
a weather report
transcription of speech

its answer
its answer

k
e
e

his w

t

n ext w eek
tw o w eeks

data for training conditional lms
to train conditional language models, we need paired   
samples,                     .
{(xi, wi)}n
data availability varies. it   s easy to think of tasks that   
could be solved by conditional language models, but the   
data just doesn   t exist.

i=1

relatively large amounts of data for:

translation, summarisation, id134,   
id103

algorithmic challenges
we often want to    nd the most likely      given some    . this   
is unfortunately generally an intractable problem.

w

x

w    = arg max
w

p(w | x)

we therefore approximate it using a id125 or with   
monte carlo methods since                           is often   
computationally easy. 
improving search/id136 is an open research question.

w(i)     p(w | x)

how can we search more effectively?
can we get guarantees that we have found the max?
can we limit the model a bit to make search easier?

section overview
the rest of this section will look at    encoder-decoder      
models that learn a function that maps     into a    xed-size   
vector and then uses a language model to    decode      
that vector into a sequence of words,    .w

x

x

kunst kann nicht gelehrt werden   

w

artistry can   t be taught   

section overview
the rest of this section will look at    encoder-decoder      
models that learn a function that maps     into a    xed-size   
vector and then uses a language model to    decode      
that vector into a sequence of words,    .w

x

x

w

a dog is playing on the beach.

section overview
    two questions

    how do we encode    as a    xed-size vector,   ? 

x

c

- problem (or at least modality) speci   c
- think about assumptions
c

    how do we condition on    in the decoding 

model?
- less problem speci   c
- we will review solution/architectures

kalchbrenner and blunsom 2013
encoder

c = embed(x)
s = vc

recurrent decoder

recurrent connection
embedding of  wt 1

source sentence

ht = g(w[ht 1; wt 1] + s + b])
ut = pht + b0

learnt bias

p(wt | x, w<t) = softmax(ut)
recall unconditional id56

ht = g(w[ht 1; wt 1] + b])

k&b 2013: encoder
how should we de   ne                        ?
c = embed(x)
the simplest model possible:

c =xi

xi

x1

x2

x3

x4

x5

x6

x1

x2

x3

x4

x5

x6

what do you think of this model?

k&b 2013: csm encoder
how should we de   ne                        ?
c = embed(x)
convolutional sentence model (csm)

k&b 2013: csm encoder

    good

    convolutions learn interactions among features in a local 

context 

    by stacking them, longer range dependencies can be learnt 

    deep convnets have a branching structure similar to trees, 

but no parser is required 

    bad 

    sentences have different lengths, need different depth trees; 

convnets are not usually so dynamic, but see*

* kalchbrenner et al. (2014). a convolutional neural network for modelling sentences. in proc. acl.

k&b 2013: id56 decoder
encoder

c = embed(x)
s = vc

recurrent decoder

recurrent connection
embedding of  wt 1

source sentence

ht = g(w[ht 1; wt 1] + s + b])
ut = pht + b0

learnt bias

p(wt | x, w<t) = softmax(ut)
recall unconditional id56

ht = g(w[ht 1; wt 1] + b])

k&b 2013: id56 decoder

s

h0

h1

x1

<s>

k&b 2013: id56 decoder

s

  p1

h0

softmax

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

s

  p1

h0

tom

   

softmax

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

   p(likes | s,hsi, tom)

s

  p1

tom

   

likes

   

softmax

softmax

h2

x2

h0

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

   p(likes | s,hsi, tom)

   p(beer | s,hsi, tom, likes)

s

  p1

tom

   

likes

   

beer

   

softmax

softmax

softmax

h2

x2

h3

x3

h0

h1

x1

<s>

k&b 2013: id56 decoder
p(tom | s,hsi)

   p(likes | s,hsi, tom)

   p(beer | s,hsi, tom, likes)

   p(h\si | s,hsi, tom, likes, beer)

likes

   

beer

   

</s>

   

s

  p1

tom

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

h0

h1

x1

<s>

sutskever et al. (2014)
lstm encoder

(c0, h0) are parameters
(ci, hi) = lstm(xi, ci 1, hi 1)

the encoding is               where             .
` = |x|

(c`, h`)

lstm decoder

w0 = hsi

(ct+`, ht+`) = lstm(wt 1, ct+` 1, ht+` 1)

ut = pht+` + b
p(wt | x, w<t) = softmax(ut)

sutskever et al. (2014)

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

are

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

are

dif   cult

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

beginnings

are

dif   cult

stop

         

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014)

    good

    id56s deal naturally with sequences of various lengths 

    lstms in principle can propagate gradients a long 

distance 

    very simple architecture! 

    bad 

    the hidden state has to remember a lot of information!

sutskever et al. (2014): tricks

beginnings

are

dif   cult

stop

         

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014): tricks
read the input sequence    backwards   : +4 id7

beginnings

are

dif   cult

stop

         

start

         

c

aller anfang

ist

schwer

stop

        

sutskever et al. (2014): tricks
use an ensemble of j independently trained models.

ensemble of 2 models: +3 id7
ensemble of 5 models: +4.5 id7

decoder:

(c(j)

t+`) = lstm(j)(wt 1, c(j)
t+`, h(j)
u(j)
t = ph(j)

t + b(j)

t+` 1, h(j)

t+` 1)

ut =

1
j

u(j0)

jxj0=1

p(wt | x, w<t) = softmax(ut)

a word about decoding
in general, we want to    nd the most probable (map) output   
given the input, i.e.

w    = arg max
w

p(w | x)

= arg max

w

|w|xt=1

log p(wt | x, w<t)

a word about decoding
in general, we want to    nd the most probable (map) output   
given the input, i.e.

w    = arg max
w

p(w | x)

= arg max

w

log p(wt | x, w<t)

|w|xt=1

this is, for general id56s, a hard problem. we therefore 
approximate it with a greedy search:
p(w1 | x)
p(w2 | x, w   1)

w   1 = arg max
w1
w   2 = arg max
w2

...

w   t = arg max
w2

p(wt | x, w   <t)

a word about decoding
in general, we want to    nd the most probable (map) output   
given the input, i.e.

w    = arg max
w

p(w | x)

|w|xt=1

= arg max

w

log p(wt | x, w<t)

undecidable :(
this is, for general id56s, a hard problem. we therefore 
approximate it with a greedy search:
p(w1 | x)
p(w2 | x, w   1)

w   1 = arg max
w1
w   2 = arg max
w2

...

w   t = arg max
w2

p(wt | x, w   <t)

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink

beer

hsilogprob=0

w0

w1

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

i
logprob=-5.80

w0

w1

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink
logprob=-6.28

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink
logprob=-6.28

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

a word about decoding
a slightly better approximation is to use a id125 with   
beam size b. key idea: keep track of top b hypothesis.
e.g., for b=2:
x = bier trinke ich
i

drink
logprob=-6.93

drink
logprob=-6.28

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

sutskever et al. (2014): tricks

use id125: +1 id7

x = bier trinke ich
i

drink

beer

hsilogprob=0

beer
logprob=-1.82

i
logprob=-2.11

w0

w1

drink
logprob=-6.93

drink
logprob=-6.28

i
logprob=-5.80

beer
logprob=-8.66

drink
logprob=-2.87

w2

like
logprob=-7.31

beer
logprob=-3.04

wine
logprob=-5.12

w3

image id134

    neural networks are great for working with multiple 

modalities   everything is a vector! 

    image id134 can therefore use the same 

techniques as translation modeling 

    a word about data 

    relatively few captioned images are available 

    pre-train image embedding model using another 

task, like image identi   cation (e.g., id163)

kiros et al. (2013)

    looks a lot like kalchbrenner and blunsom (2013) 

    convolutional network on the input 

    id165 language model on the output 

    innovation: multiplicative interactions in the 

decoder id165 model

kiros et al. (2013)

encoder

x = embed(x)

kiros et al. (2013)

encoder
unconditional id165 lm:

x = embed(x)

embedding of wt 1

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

p(wt | x, wt 1

t n+1) = softmax(ut)

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

p(wt | x, wt 1

t n+1) = softmax(ut)

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

how big is this tensor?

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj
what   s the intuition here?

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

how big is this tensor?

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

t n+1) = softmax(ut)

p(wt | x, wt 1
multiplicative id165 lm:
wi = ri,w
wi = ri,j,wxj
(u 2 r|v |   d, v 2 rd   k)
wi = uw,ivi,j
rt = w[wt n+1; wt n+2; . . . ; wt 1] + cx

kiros et al. (2013)

encoder
simple conditional id165 lm:

x = embed(x)

ht = w[wt n+1; wt n+2; . . . ; wt 1]
ut = pht + b

+cx

p(wt | x, wt 1
multiplicative id165 lm:

t n+1) = softmax(ut)

(u 2 r|v |   d, v 2 rd   k)
wi = uw,ivi,j
rt = w[wt n+1; wt n+2; . . . ; wt 1] + cx
ht = (wf rrt)   (wf xx)
ut = pht + b

p(wt | x, w<t) = softmax(ut)

kiros et al. (2013)

    two take-home messages: 

    feed-forward id165 models can be used in 

place of id56s in conditional models 

    modeling interactions between input modalities 

holds a lot of promise 

    although mlp-type models can approximate 

higher order tensors, multiplicative models 
appear to make learning interactions easier

conditioning with vectors
we are compressing a lot of information in a    nite-sized   
vector.

conditioning with vectors
we are compressing a lot of information in a    nite-sized   
vector.

   you can't cram the meaning of a whole %&!$#   

sentence into a single $&!#* vector!   

prof. ray mooney

conditioning with vectors
we are compressing a lot of information in a    nite-sized   
vector.

gradients have a long way to travel. even lstms forget!

conditioning with vectors
we are compressing a lot of information in a    nite-sized   
vector.

gradients have a long way to travel. even lstms forget!

what is to be done?

outline of final section

    machine translation with attention 

    image id134 with attention

outline of final section

    machine translation with attention 

    image id134 with attention

solving the vector problem 

in translation

    represent a source sentence as a matrix 

    generate a target sentence from a matrix 

    this will 

    solve the capacity problem 

    solve the gradient    ow problem

sentences as matrices

    problem with the    xed-size vector model 

    sentences are of different sizes but vectors are of 

the same size 

    solution: use matrices instead 

    fixed number of rows, but number of columns 

depends on the number of words 

    usually |f| = #cols

sentences as matrices

ich m  ochte ein bier

sentences as matrices

ich m  ochte ein bier

mach   s gut

sentences as matrices

ich m  ochte ein bier

mach   s gut

die wahrheiten der menschen sind die unwiderlegbaren irrt  umer

sentences as matrices

ich m  ochte ein bier

mach   s gut

die wahrheiten der menschen sind die unwiderlegbaren irrt  umer

question: how do we build these matrices?

with concatenation

    each word type is represented by an n-dimensional 

vector 

    take all of the vectors for the sentence and 

concatenate them into a matrix 

    simplest possible model 

    so simple, no one has bothered to publish how 

well/badly it works!

x1

x2

x3

x4

ich m  chte ein

bier

fi = xi

x1

x2

x3

x4

ich m  chte ein

bier

fi = xi

x1

x2

x3

x4

ich m  chte ein

bier

f 2 rn   |f|

ich m  ochte ein bier

with convolutional nets

    apply convolutional networks to transform the naive 

concatenated matrix to obtain a context-dependent matrix 

    explored in a recent iclr submission by gehring et al., 

2016 (from fair) 
    closely related to the neural translation model 
proposed by kalchbrenner and blunsom, 2013 

    note: convnets usually have a    pooling    operation at the 
top level that results in a    xed-sized representation. for 
sentences, leave this out.

x1

x2

x3

x4

ich m  chte ein

bier

x1

x2

x3

x4

ich m  chte ein

bier

   

filter 1

x1

x2

x3

x4

ich m  chte ein

bier

   

   

filter 1

filter 2

x1

x2

x3

x4

ich m  chte ein

bier

   

   

filter 1

filter 2

x1

x2

x3

x4

f 2 rf (n)   g(|f|)

ich m  ochte ein bier

ich m  chte ein

bier

with bidirectional id56s

    by far the most widely used matrix representation, due to 

bahdanau et al (2015)  

    one column per word 

    each column (word) has two halves concatenated together: 

    a    forward representation   , i.e., a word and its left context 

    a    reverse representation   , i.e., a word and its right context 
    implementation: bidirectional id56s (grus or lstms) to read 
f from left to right and right to left, concatenate representations

x1

x2

x3

x4

ich m  chte ein

bier

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

  h 1

  h 2

  h 3

  h 4

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

x1

x2

x3

x4

ich m  chte ein

bier

fi = [  h i; !h i]
  h 2
  h 3

  h 4

  h 1

 !h 1

 !h 2

 !h 3

 !h 4

f 2 r2n   |f|

x1

x2

x3

x4

ich m  ochte ein bier

ich m  chte ein

bier

where are we in 2017?

    there are lots of ways to construct f 

    very little systematic work comparing them 

    there are many more undiscovered things out there 

    convolutions are particularly interesting and under-explored 

    syntactic information can help (sennrich & haddow, 2016; nadejde 

et al., 2017), but many more integration strategies are possible 

    try something with phrase types instead of word types?
multi-word expressions are a pain in the neck .

generation from matrices

    we have a matrix f representing the input, now we need to generate from it 
    bahdanau et al. (2015) were the    rst to propose using attention for translating from matrix-

encoded sentences 

    high-level idea 

    generate the output sentence word by word using an id56 
    at each output position t, the id56 receives two inputs (in addition to any recurrent inputs) 

    a    xed-size vector embedding of the previously generated output symbol et-1 
    a    xed-size vector encoding a    view    of the input matrix 

    how do we get a    xed-size vector from a matrix that changes over time? 

    bahdanau et al: do a weighted sum of the columns of f (i.e., words) based on how 

important they are at the current time step. (i.e., just a matrix-vector product fat) 

    the weighting of the input columns at each time-step (at) is called attention

recall id56s   

    

recall id56s   

    

recall id56s   

i'd

    

recall id56s   

i'd

    

i'd

recall id56s   

i'd

like

    

i'd

recall id56s   

    

    

ich m  ochte ein bier

    

ich m  ochte ein bier

    

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

    

c1 = fa1

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

    

c1 = fa1

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

    

c1 = fa1

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

    

i'd

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

    

i'd

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

    

i'd

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

z

}|

{

    

i'd

c2 = fa2

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

z

}|

{

    

i'd

c2 = fa2

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

    

i'd

like

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

stopstop

    

i'd

like

a

beer

z

}|

{

ich m  ochte ein bier

attention history:

a>1
a>2
a>3
a>4
a>5

attention

    how do we know what to attend to at each time-

step? 

    that is, how do we compute     ?at

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the expected input embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the query key embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the query key embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the query key embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)

(called     in the paper)

ct = fat

computing attention

    at each time step (one time step = one output word), we want to be able to 

   attend    to different words in the source sentence 
    we need a weight for every column: this is an |f|-length vector at  
    here is a simpli   ed version of bahdanau et al.   s solution 

    use an id56 to predict model output, call the hidden states 
    at time t compute the query key embedding 
    take the dot product with every column in the source matrix to compute 

st
(st has a    xed dimensionality, call it m)
rt = vst 1

the attention energy. 

et
(since f has |f| columns,     has |f| rows)

    exponentiate and normalize to 1: 
   t
    finally, the input source vector for time t is

(called     in the paper)

(     is a learned parameter)
v
ut = f>rt
ut
at = softmax(ut)
(called      in the paper)
ct = fat

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = tanh (wf + rt) v

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = v> tanh(wf + rt)

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

nonlinear attention-energy 

model

    in the actual model, bahdanau et al. replace the dot 

product between the columns of f and rt with an mlp: 

ut = f>rt
ut = v> tanh(wf + rt)

(simple model)
(bahdanau et al)

    here, w and v are learned parameters of appropriate 

dimension and +    broadcasts    over the |f| columns in wf 

    this can learn more complex interactions 

    it is unclear if the added complexity is necessary for 

good performance

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
while et 6= h/si :

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et 1
et
(    and    are learned parameters)
p

b

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
while et 6= h/si :

doesn   t depend on output decisions

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et 1
et
(    and    are learned parameters)
p

b

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
x = wf
while et 6= h/si :

x

t = t + 1
rt = vst 1
ut = v> tanh(wf + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et
et 1
(    and    are learned parameters)
p

b

putting it all together

(part 1 of lecture)

f = encodeasmatrix(f )
e0 = hsi
s0 = w (learned initial state; bahdanau uses           )u  h 1
t = 0
x = wf
while et 6= h/si :

t = t + 1
rt = vst 1
ut = v> tanh(x + rt)
at = softmax(ut)
ct = fat
st = id56(st 1, [et 1; ct])
yt = softmax(pst + b)
et | e<t     categorical(yt)

}(compute attention; part 2 of lecture)

(        is a learned embedding of    )
et
et 1
(    and    are learned parameters)
p

b

attention in mt

add attention to id195 translation: +11 id7

a word about gradients

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

i'd

like

a

beer

z

}|

{

    

i'd

like

a

attention history:

a>1
a>2
a>3
a>4
a>5

ich m  ochte ein bier

attention and translation

    cho   s question: does a translator read and memorize 
the input sentence/document and then generate the 
output? 

    compressing the entire input sentence into a vector 

basically says    memorize the sentence    

    common sense experience says translators refer 

back and forth to the input. (also backed up by eye-
tracking studies) 

    should humans be a model for machines?

summary

    attention  

    provides the ability to establish information    ow directly from distant 

    closely related to    pooling    operations in convnets (and other architectures) 

    traditional attention model seems to only cares about    content    

    no obvious bias in favor of diagonals, short jumps, fertility, etc. 

    some work has begun to add other    structural    biases (luong et al., 2015; 

cohn et al., 2016), but there are lots more opportunities 

    factorization into keys and values (miller et al., 2016; ba et al., 2016, 

gulcehre et al., 2016) 

    attention weights provide interpretation you can look at

outline of lecture

    machine translation with attention 

    image id134 with attention

            

            

            

akiko

   

  p1

likes

   

pimm   s

   

</s>

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

sutskever et al. (2014)

h1

x1

<s>

a

   

  p1

man

   

is

   

rowing

   

softmax

softmax

softmax

softmax

h2

x2

h3

x3

h4

x4

vinyals et al. (2014) show and tell: a neural image caption generator

h1

x1

<s>

image id134

    can attention help caption modeling?

xu et al. (2015, icml)

regions in convnets

each point in a    higher    level of a convnet    
de   nes spatially localised feature vectors(/matrices).
xu et al. calls these    annotation vectors   , ai, i 2 {1, . . . , l}

a1

f =

a1h

i

a2

f =

a1h

a2

i

a3

f =

a1h

a2 a3       

i

attention

    attention    weights    (    ) are computed using 

exactly the same technique as discussed above 
    deterministic soft attention (bahdanau et al., 2014)   

at

(weighted average)

ct = fat

    stochastic hard attention (xu et al., 2015)

st     categorical(at)
ct = f:,st

(sample a column)

    what are the bene   ts of this model? 

    what are the challenges of this model?

attention

    attention    weights    (    ) are computed using 

exactly the same technique as discussed above 
    deterministic soft attention (bahdanau et al., 2014)   

at

ct = fat

(weighted average)

    stochastic hard attention (xu et al., 2015)

st     categorical(at)
ct = f:,st

(sample a column)

    what are the bene   ts of this model? 

    what are the challenges of this model?

attention

    attention    weights    (    ) are computed using 

exactly the same technique as discussed above 
    deterministic soft attention (bahdanau et al., 2014)   

at

ct = fat

(weighted average)

    stochastic hard attention (xu et al., 2015)

st     categorical(at)
ct = f:,st

(sample a column)

    what are the bene   ts of this model? 

    what are the challenges of learning the 

parameters of this model?

learning hard attention

p(w, s | x)

p(s | x)p(w | x, s)

l =   log p(w | x)

=   logxs
=   logxs
     xs

mc

     

1
n

nxi=1

p(s | x) log p(w | x, s)

(jensen   s inequality)

p(s(i) | x) log p(w | x, s)

learning hard attention

p(w, s | x)

p(s | x)p(w | x, s)

l =   log p(w | x)

=   logxs
=   logxs
     xs

mc

     

1
n

nxi=1

p(s | x) log p(w | x, s)

(jensen   s inequality)

p(s(i) | x) log p(w | x, s)

learning hard attention

p(w, s | x)

p(s | x)p(w | x, s)

l =   log p(w | x)

=   logxs
=   logxs
     xs

mc

     

1
n

nxi=1

p(s | x) log p(w | x, s)

(jensen   s inequality)

p(s(i) | x) log p(w | x, s)

learning hard attention

    sample n sequences of attention decisions from the 

model 

    the gradient is the id203 of the gradient of the 

id203 of this sequence scaled by the log id203 
of generating the target words using that sequence of 
attention decisions 

    this is equivalent to using the reinforce algorithm 

(williams, 1992) using the log id203 of the observed 
words as a    reward function   . reinforce a policy 
gradient algorithm used for id23.

attention in captioning

add soft attention to image captioning: +2 id7
add hard attention to image captioning: +4 id7

summary

    signi   cant performance improvements 

    better performance over vector-based encodings 

    better performance with smaller training data sets 

    model interpretability 
    better gradient    ow 

    better capacity (especially obvious for translation)

questions?

a few tricks of the trade

    depth 

    dropout/recurrent dropout 

    minibatching

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

  y

x1

x2

x3

x4

   deep    lstms

    this term has been de   ned several times, but the 

following is the most standard convention

  y

x1

x2

x3

x4

does depth matter?

    yes, it helps 

    it seems to play a less signi   cant role in text than in audio/visual processing 

    h1: more transformation of the input is required for asr, image recognition, etc., 
than for common text applications (word vectors become customized to be    good 
inputs    to id56s whereas you   re stuck with what nature gives you for speech/vision) 

    h2: less effort has been made to    nd good architectures (id56s are expensive to 

train; have been widely used for less long) 

    h3: back prop through time + depth is hard and we need better optimizers 

    many other possibilities    

    2-8 layers seems to be standard 

    input    skip    connections are used often but by no means universally

dropout and deep lstms

    applying dropout layers requires some care

  y

x1

x2

x3

x4

dropout and deep lstms

    apply dropout between layers, but not on the 

recurrent connections

dropout

dropout

dropout

x1

dropout

dropout

dropout

x2

dropout

dropout

dropout

x3

  y

dropout

dropout

dropout

dropout

x4

dropout and deep lstms

    apply dropout between layers, but not on the 

recurrent connections

    use the same dropout mask across time.
   recurrent dropout   . why?

dropout

dropout

dropout

x1

dropout

dropout

dropout

x2

dropout

dropout

dropout

x3

  y

dropout

dropout

dropout

dropout

x4

implementation details

    for speed

    use diagonal matrices instead of full matrices (esp. for gates) 

    concatenate parameter matrices for all gates and do a single matrix-

vector(/matrix) multiplication 

    use optimized implementations (from nvidia) 
    use grus or reduced-gate variant of lstms 

    for learning speed and performance

    initialize so that the bias on the forget gate is large (intuitively: at the 

beginning of training, the signal from the past is unreliable) 

    use random orthogonal matrices to initialize the square matrices

implementation details: 

minibatching

    gpu hardware is 

    pretty fast for elementwise operations (io bound- can   t get enough data 

through the gpu) 

    very fast for matrix-id127 (usually compute bound - the 

gpu will work at 100% capacity, and gpu cores are fast) 

    id56s, lstms, grus all consist of 

    lots of elementwise operations (addition, multiplication, nonlinearities,    ) 

    lots of matrix-vector products 

    minibatching: convert many matrix-vector products into a single matrix-

id127

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

minibatching

single-instance id56

ht = g(vxt + uht 1 + c)
  yt = wht + b

minibatch id56

z

x1

}|

x1

{

x1

x1

ht = g(vxt + uht 1 + c)
  yt = wht + b
we batch across instances,   
not across time.

anything wrong here?

questions?

h   perguntas?

obrigado!

