introduction to machine learning:

linear learners

lisbon machine learning school, 2017

stefan riezler

computational linguistics & iwr

heidelberg university, germany

riezler@cl.uni-heidelberg.de

intro: linear learners

1(119)

modeling the frog   s perceptual system

introduction

intro: linear learners

2(119)

modeling the frog   s perceptual system

introduction

(cid:73) [lettvin et al. 1959] show that the frog   s perceptual system

constructs reality by four separate operations:

(cid:73) contrast detection: presence of sharp boundary?
(cid:73) convexity detection: how curved and how big is object?
(cid:73) movement detection: is object moving?
(cid:73) dimming speed: how fast does object obstruct light?

(cid:73) the frog   s goal: capture any object of the size of an insect or

worm providing it moves like one.

intro: linear learners

3(119)

modeling the frog   s perceptual system

introduction

(cid:73) [lettvin et al. 1959] show that the frog   s perceptual system

constructs reality by four separate operations:

(cid:73) contrast detection: presence of sharp boundary?
(cid:73) convexity detection: how curved and how big is object?
(cid:73) movement detection: is object moving?
(cid:73) dimming speed: how fast does object obstruct light?

(cid:73) the frog   s goal: capture any object of the size of an insect or

worm providing it moves like one.

(cid:73) can we build a model of this perceptual system and learn to

capture the right objects?

intro: linear learners

3(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

intro: linear learners

4(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) =
, p(-) =
(cid:73) p(convex = small|-) =
p(speed = small|-) =
p(convex = small|+) =
p(speed = small|+) =

, p(convex = med|-) =
, p(speed = med|-) =
, p(convex = med|+) =
, p(speed = med|+) =

, p(convex = large|-) =

, p(speed = large|- ) =

, p(convex = large|+) =

, p(speed = large|+ ) =

intro: linear learners

4(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 6/14, p(-) = 8/14
(cid:73) p(convex = small|-) =
p(speed = small|-) =
p(convex = small|+) =
p(speed = small|+) =

, p(convex = med|-) =
, p(speed = med|-) =
, p(convex = med|+) =
, p(speed = med|+) =

, p(convex = large|-) =

, p(speed = large|- ) =

, p(convex = large|+) =

, p(speed = large|+ ) =

intro: linear learners

4(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 6/14, p(-) = 8/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

intro: linear learners

4(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 6/14, p(-) = 8/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) =
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) =

intro: linear learners

4(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 6/14, p(-) = 8/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) = 8/14    1/8    3/8 = 0.027
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) =

intro: linear learners

4(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 6/14, p(-) = 8/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) = 8/14    1/8    3/8 = 0.027
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) = 6/14    2/6    1/6 = 0.024

intro: linear learners

4(119)

learning from data

(cid:73) assume training data of edible (+) and inedible (-) objects

introduction

convex
small
small
small

medium

large
small
small
small

speed
small

medium
medium

small
small
small
large

medium

label

-
-
-
-
-
-
-
-

convex
small

medium
medium

large
large
large

speed
large
large
large
small
large

medium

label

+
+
+
+
+
+

(cid:73) learning model parameters from data:

(cid:73) p(+) = 6/14, p(-) = 8/14
(cid:73) p(convex = small|-) = 6/8, p(convex = med|-) = 1/8, p(convex = large|-) = 1/8

p(speed = small|-) = 4/8, p(speed = med|-) = 3/8, p(speed = large|- ) = 1/8
p(convex = small|+) = 1/6, p(convex = med|+) = 2/6, p(convex = large|+) = 3/6
p(speed = small|+) = 1/6, p(speed = med|+) = 1/6, p(speed = large|+ ) = 4/6

(cid:73) predict unseen p(label = ?, convex = med, speed = med)

(cid:73) p(-)    p(convex = med|-)    p(speed = med|-) = 8/14    1/8    3/8 = 0.027
(cid:73) p(+)    p(convex = med|+)    p(speed = med|+) = 6/14    2/6    1/6 = 0.024
(cid:73) inedible: p(convex = med, speed = med, label = -) > p(convex = med, speed = med, label = +)!

intro: linear learners

4(119)

machine learning is a frog   s world

introduction

(cid:73) machine learning problems can be seen as problems of

function estimation where

(cid:73) our models are based on a combined feature representation of

inputs and outputs

(cid:73) similar to the frog whose world is constructed by

four-dimensional feature vector based on detection operations

intro: linear learners

5(119)

machine learning is a frog   s world

introduction

(cid:73) machine learning problems can be seen as problems of

function estimation where

(cid:73) our models are based on a combined feature representation of

inputs and outputs

(cid:73) similar to the frog whose world is constructed by

four-dimensional feature vector based on detection operations

(cid:73) learning of parameter weights is done by optimizing    t of

model to training data

(cid:73) frog uses binary classi   cation into edible/inedible objects as

supervision signals for learning

intro: linear learners

5(119)

machine learning is a frog   s world

introduction

(cid:73) machine learning problems can be seen as problems of

function estimation where

(cid:73) our models are based on a combined feature representation of

inputs and outputs

(cid:73) similar to the frog whose world is constructed by

four-dimensional feature vector based on detection operations

(cid:73) learning of parameter weights is done by optimizing    t of

model to training data

(cid:73) frog uses binary classi   cation into edible/inedible objects as

supervision signals for learning

(cid:73) the model used in the frog   s perception example is called

naive bayes: it measures compatibility of inputs to outputs by
a linear model and optimizes parameters by convex
optimization

intro: linear learners

5(119)

introduction

lecture outline

(cid:73) preliminaries

(cid:73) data: input/output
(cid:73) feature representations
(cid:73) linear models

(cid:73) id76 for linear models

(cid:73) naive bayes
(cid:73) generative versus discriminative
(cid:73) id28
(cid:73) id88
(cid:73) large-margin learners (id166s)

(cid:73) id173
(cid:73) online learning
(cid:73) non-linear models

intro: linear learners

6(119)

inputs and outputs

preliminaries

(cid:73) input: x     x

(cid:73) e.g., document or sentence with some words x = w1 . . . wn

(cid:73) output: y     y

(cid:73) e.g., document class, translation, parse tree

(cid:73) input/output pair: (x, y)     x    y

(cid:73) e.g., a document x and its class label y,
(cid:73) a source sentence x and its translation y,
(cid:73) a sentence x and its parse tree y

intro: linear learners

7(119)

feature representations

preliminaries

(cid:73) most nlp problems can be cast as multiclass classi   cation
where we assume a high-dimensional joint feature map on
input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

intro: linear learners

8(119)

feature representations

preliminaries

(cid:73) most nlp problems can be cast as multiclass classi   cation
where we assume a high-dimensional joint feature map on
input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) common ranges:

(cid:73) categorical (e.g., counts):   i     {1, . . . , fi}, fi     n+
(cid:73) binary (e.g., binning):        {0, 1}m
(cid:73) continuous (e.g., id27s):        rm

intro: linear learners

8(119)

feature representations

preliminaries

(cid:73) most nlp problems can be cast as multiclass classi   cation
where we assume a high-dimensional joint feature map on
input-output pairs (x, y)
(cid:73)   (x, y) : x    y     rm

(cid:73) common ranges:

(cid:73) categorical (e.g., counts):   i     {1, . . . , fi}, fi     n+
(cid:73) binary (e.g., binning):        {0, 1}m
(cid:73) continuous (e.g., id27s):        rm
(cid:73) for any vector v     rm, let vj be the j th value

intro: linear learners

8(119)

examples

preliminaries

(cid:73) x is a document and y is a label

          1 if x contains the word    interest   

and y =      nancial   

0 otherwise

  j (x, y) =

we expect this feature to have a positive weight,    interest    is
a positive indicator for the label       nancial   

intro: linear learners

9(119)

examples

preliminaries

  j (x, y) = % of words in x containing punctuation and y =   scienti   c   

punctuation symbols - positive indicator or negative indicator for
scienti   c articles?

intro: linear learners

10(119)

examples

preliminaries

(cid:73) x is a word and y is a part-of-speech tag

(cid:26) 1 if x =    bank    and y = verb

  j (x, y) =

0 otherwise

what weight would it get?

intro: linear learners

11(119)

examples

preliminaries

(cid:73) x is a source sentence and y is translation

  j (x, y) =

and    are there    present in y

          1 if    y a-t-il    present in x
          1 if    y a-t-il    present in x

0 otherwise

0 otherwise

  k (x, y) =

and    are there any    present in y

which phrase indicator should be preferred?

intro: linear learners

12(119)

examples

preliminaries

note: label y includes sentence x

intro: linear learners

13(119)

linear models

(cid:73) linear model: de   nes a discriminant function that is based

linear models

on linear combination of features and weights
        (x, y)

f (x;   ) = arg max

y   y

= arg max

y   y

m(cid:88)

j=0

  j      j (x, y)

intro: linear learners

14(119)

linear models

(cid:73) linear model: de   nes a discriminant function that is based

linear models

on linear combination of features and weights
        (x, y)

f (x;   ) = arg max

y   y

= arg max

y   y

m(cid:88)

j=0

  j      j (x, y)

(cid:73) let        rm be a high dimensional weight vector
(cid:73) assume that    is known

(cid:73) multiclass classi   cation: y = {0, 1, . . . , n}
        (x, y(cid:48))

y = arg max

y(cid:48)   y

(cid:73) binary classi   cation just a special case of multiclass

intro: linear learners

14(119)

linear models

linear models for binary classi   cation

(cid:73)    de   nes a linear decision boundary that divides space of

instances in two classes

(cid:73) 2 dimensions: line
(cid:73) 3 dimensions: plane
(cid:73) n dimensions: hyperplane of n     1 dimensions

intro: linear learners

15(119)

12-2-112-2-1points along linehave scores of 0multiclass linear model

de   nes regions of space. visualization di   cult.

linear models

(cid:73) + are all points (x, y) where + = arg maxy         (x, y)

intro: linear learners

16(119)

id76 for supervised learning

id76

how to learn weight vector    in order to make decisions?

(cid:73) input:

(cid:73) i.i.d. (independent and identically distributed) training

examples t = {(xt, yt)}|t |

t=1

(cid:73) feature representation   

intro: linear learners

17(119)

id76 for supervised learning

id76

how to learn weight vector    in order to make decisions?

(cid:73) input:

(cid:73) i.i.d. (independent and identically distributed) training

examples t = {(xt, yt)}|t |

t=1

(cid:73) feature representation   

(cid:73) output:    that maximizes an objective function on the

training set

(cid:73)    = arg maxl(t ;   )
(cid:73) equivalently minimize:    = arg min   l(t ;   )

intro: linear learners

17(119)

objective functions

id76

(cid:73) l(t ;   )    (cid:80)

(cid:73) ideally we can decompose l by training pairs (x, y)

(cid:73) loss is a function that measures some value correlated with

(x,y)   t loss((x, y);   )

errors of parameters    on instance (x, y)

intro: linear learners

18(119)

objective functions

id76

(cid:73) l(t ;   )    (cid:80)

(cid:73) ideally we can decompose l by training pairs (x, y)

(cid:73) loss is a function that measures some value correlated with

(x,y)   t loss((x, y);   )

errors of parameters    on instance (x, y)

(cid:73) example:

(cid:73) y     {1,   1}, f (x;   ) is the prediction we make for x using   
(cid:73) 0-1 id168: loss((x, y);   ) =

if f (x;   ) = y,
else

(cid:26) 0

1

intro: linear learners

18(119)

convexity

id76

(cid:73) a function is convex if its graph lies on or below the line

segment connecting any two points on the graph
f (  x+  y)       f (x)+  f (y) for all   ,        0,   +   = 1 (1)

intro: linear learners

19(119)

gradient

id76

(cid:16)    

(cid:73) gradient of function f is vector of partial derivatives.

   f (x) =

f (x),    
   x2

f (x), ...,    
   xn

   x1

f (x)

(cid:73) rate of increase of f at point x in each of the axis-parallel

(cid:17)

directions.

intro: linear learners

20(119)

id76

id76

(cid:73) optimization problem is de   ned as problem of    nding a point

that minimizes our objective function (maximization is
minimization of    f (x))

intro: linear learners

21(119)

id76

id76

(cid:73) optimization problem is de   ned as problem of    nding a point

that minimizes our objective function (maximization is
minimization of    f (x))

(cid:73) in order to    nd minimum, follow opposite direction of gradient
(cid:73) for convex (or linear) functions, global minimum at point

where    f (x) = 0

intro: linear learners

21(119)

naive bayes

naive bayes

intro: linear learners

22(119)

naive bayes

naive bayes

(cid:73) probabilistic decision model:

arg max

y

p(y|x)     arg max

p(y)p(x|y)

y

(cid:73) uses bayes rule:

p(y|x) =

p(y)p(x|y)

p(x)

for    xed x

(cid:73) generative model since p(y)p(x|y) = p(x, y) is a joint

id203

(cid:73) because we model a distribution that can randomly generate

outputs and inputs, not just outputs

intro: linear learners

23(119)

naivety of naive bayes

naive bayes

(cid:73) we need to decide on the structure of p(x, y)
(cid:73) p(x|y) = p(  (x)|y) = p(  1(x), . . . ,   m(x)|y)

p(  1(x), . . . ,   m(x)|y) =(cid:81)
(cid:73) p(x, y) = p(y)(cid:81)m

naive bayes assumption
(conditional independence)

i=1 p(  i (x)|y)

i p(  i (x)|y)

intro: linear learners

24(119)

naive bayes     learning

naive bayes

(cid:73) input: t = {(xt, yt)}|t |
(cid:73) let   i (x)     {1, . . . , fi}
(cid:73) parameters p = {p(y), p(  i (x)|y)}

t=1

intro: linear learners

25(119)

naive bayes

id113

(cid:73) what   s left? de   ning an objective l(t )

(cid:73) p plays the role of   

(cid:73) what objective to use?

(cid:73) objective: id113 (id113)

|t |(cid:89)

(cid:32)

|t |(cid:89)

m(cid:89)

l(t ) =

p(xt, yt) =

p(yt)

p(  i (xt)|yt)

(cid:33)

t=1

t=1

i=1

intro: linear learners

26(119)

naive bayes     learning
id113 has closed form solution

p = arg max

p

naive bayes

(cid:33)

p(  i (xt)|yt)

|t |(cid:89)

t=1

m(cid:89)

i=1

p(yt)

(cid:32)
(cid:80)|t |
(cid:80)|t |
(cid:26) 1

|t |

p(y) =

(cid:80)|t |

t=1[[yt = y]]

p(  i (x)|y) =

t=1[[  i (xt) =   i (x) and yt = y]]

t=1[[yt = y]]

where [[p]] =

if p is true,
0 otherwise.

thus, these are just normalized counts over events in t

intro: linear learners

27(119)

naive bayes

deriving id113

(cid:32)

|t |(cid:89)

t=1

m(cid:89)

i=1

p(yt)

(cid:33)

p(  i (xt)|yt)

p = arg max

p

intro: linear learners

28(119)

deriving id113

naive bayes

p = arg max

p(  i (xt)|yt)

= arg max

log p(yt) +

log p(  i (xt)|yt)

m(cid:88)

i=1

(cid:33)

|t |(cid:88)

m(cid:88)

t=1

i=1

(cid:33)

m(cid:89)

i=1

p

p

t=1

(cid:32)
(cid:32)

p(yt)

|t |(cid:89)
|t |(cid:88)
|t |(cid:88)
y p(y) = 1,(cid:80)fi

t=1

t=1

such that(cid:80)

= arg max

p(y)

log p(yt) + arg max
p(  i (x)|y)

log p(  i (xt)|yt)

j=1 p(  i (x) = j|y) = 1, p(  )     0

intro: linear learners

28(119)

deriving id113

naive bayes

|t |(cid:88)

t=1

p = arg max

p(y)

log p(yt) + arg max
p(  i (x)|y)

|t |(cid:88)

m(cid:88)

t=1

i=1

log p(  i (xt)|yt)

intro: linear learners

29(119)

deriving id113

naive bayes

log p(yt) + arg max
p(  i (x)|y)

|t |(cid:88)
|t |(cid:88)
v count(v ) log p(v ), s.t. (cid:80)
(cid:80)

t=1

t=1

p = arg max

p(y)

arg maxp

both optimizations are of the form

m(cid:88)

i=1

log p(  i (xt)|yt)

v p(v ) = 1, p(v )     0

where v is event in t , either (yt = y) or (  i (xt) =   i (x), yt = y)

intro: linear learners

29(119)

naive bayes

deriving id113

arg maxp

(cid:80)
v count(v ) log p(v )
v p(v ) = 1, p(v )     0

s.t.,(cid:80)
(cid:80)
v count(v ) log p(v )        ((cid:80)

introduce lagrangian multiplier   , optimization becomes
v p(v )     1)

arg maxp,  

intro: linear learners

30(119)

naive bayes

deriving id113

arg maxp

(cid:80)
v count(v ) log p(v )
v p(v ) = 1, p(v )     0

s.t.,(cid:80)
(cid:80)
v count(v ) log p(v )        ((cid:80)

introduce lagrangian multiplier   , optimization becomes
v p(v )     1)

arg maxp,  

(cid:73) derivative w.r.t p(v ) is

count(v)

p(v )       

(cid:73) setting this to zero p(v ) =

count(v)

  

(cid:73) use(cid:80)

v p(v ) = 1, p(v )     0, then p(v ) =

(cid:80)

count(v)
v(cid:48) count(v(cid:48))

intro: linear learners

30(119)

naive bayes

deriving id113

reinstantiate events v in t :

p(y) =

(cid:80)|t |

t=1[[yt = y]]

(cid:80)|t |
(cid:80)|t |

|t |

t=1[[yt = y]]

p(  i (x)|y) =

t=1[[  i (xt) =   i (x) and yt = y]]

intro: linear learners

31(119)

naive bayes is a linear model

(cid:73) let   y = log p(y),    y     y
(cid:73) let     i (x),y = log p(  i (x)|y),    y     y,   i (x)     {1, . . . , fi}

naive bayes

intro: linear learners

32(119)

naive bayes is a linear model

naive bayes

(cid:73) let   y = log p(y),    y     y
(cid:73) let     i (x),y = log p(  i (x)|y),    y     y,   i (x)     {1, . . . , fi}

arg max

y

p(y|  (x))     arg max

y

= arg max

y

= arg max

y

= arg max

y

p(  (x), y) = arg max

p(y)

p(  i (x)|y)

m(cid:89)

m(cid:88)

i=1

y

i=1

log p(  i (x)|y)

log p(y) +

m(cid:88)

  y +

    i (x),y

i=1

  y  y(cid:48) (y) +

(cid:88)

y(cid:48)

m(cid:88)

fi(cid:88)

    i (x),y  i,j (x)

i=1

j=1

where   i,j (x) = [[  i (x) = j]],   y(cid:48) (y) = [[y = y(cid:48)]]

intro: linear learners

32(119)

naive bayes

discriminative versus generative models

(cid:73) generative models attempt to model inputs and outputs

(cid:73) e.g., naive bayes = id113 of joint distribution p(x, y)
(cid:73) statistical model must explain generation of input

(cid:73) occam   s razor:    among competing hypotheses, the one with

the fewest assumptions should be selected   

(cid:73) discriminative models

(cid:73) use l that directly optimizes p(y|x) (or something related)
(cid:73) id28     id113 of p(y|x)
(cid:73) id88 and id166s     minimize classi   cation error
(cid:73) generative and discriminative models use p(y|x) for

prediction

(cid:73) di   er only on what distribution they use to set   

intro: linear learners

33(119)

id28

id28

intro: linear learners

34(119)

id28

id28

de   ne a id155:

p(y|x) =

e      (x,y)

zx

,

where zx =

(cid:88)

y(cid:48)   y

e      (x,y(cid:48))

note: still a linear model

arg max

y

p(y|x) = arg max

y

= arg max

y

= arg max

y

e      (x,y)

zx

e      (x,y)
        (x, y)

intro: linear learners

35(119)

id28

id28

p(y|x) =

e      (x,y)

zx

(cid:73) q: how do we learn weights   
(cid:73) a: set weights to maximize log-likelihood of training data:

l(t ;   )

   = arg max

  

= arg max

|t |(cid:89)

  

t=1

p(yt|xt) = arg max

  

|t |(cid:88)

t=1

log p(yt|xt)

(cid:73) in a nutshell we set the weights    so that we assign as much
id203 to the correct label y for each x in the training set

intro: linear learners

36(119)

id28

p(y|x) =

e      (x,y)

zx

,

   = arg max

|t |(cid:88)

  

t=1

where zx =

log p(yt|xt) (*)

id28

(cid:88)

y(cid:48)   y

e      (x,y(cid:48))

(cid:73) the objective function (*) is concave
(cid:73) therefore there is a global maximum
(cid:73) no closed form solution, but lots of numerical techniques

(cid:73) gradient methods (gradient ascent, conjugate gradient,

iterative scaling)

(cid:73) id77s (limited-memory quasi-newton)

intro: linear learners

37(119)

gradient ascent

id28

intro: linear learners

38(119)

gradient ascent

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 log(cid:0)e      (xt ,yt )/zx

(cid:73) want to    nd arg max   l(t ;   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

id28

(cid:1)

  i =   i   1 +   (cid:79)l(t ;   i   1)

(cid:73)    > 0 is a step size / learning rate
(cid:73) (cid:79)l(t ;   ) is gradient of l w.r.t.   
l(t ;   ),    

(cid:73) a gradient is all partial derivatives over variables wi
(cid:73) i.e., (cid:79)l(t ;   ) = (    
l(t ;   ), . . . ,    
     m

     0

     1

l(t ;   ))

(cid:73) gradient ascent will always    nd    to maximize l

intro: linear learners

39(119)

id119

(cid:73) let l(t ;   ) =    (cid:80)|t |

t=1 log(cid:0)e      (xt ,yt )/zx

(cid:73) want to    nd arg min   l(t ;   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

id28

(cid:1)

  i =   i   1       (cid:79)l(t ;   i   1)

(cid:73)    > 0 is step size / learning rate
(cid:73) (cid:79)l(t ;   ) is gradient of l w.r.t.   
l(t ;   ),    

(cid:73) a gradient is all partial derivatives over variables wi
(cid:73) i.e., (cid:79)l(t ;   ) = (    
l(t ;   ), . . . ,    
     m
(cid:73) id119 will always    nd    to minimize l

     0

     1

l(t ;   ))

intro: linear learners

40(119)

the partial derivatives

id28

(cid:73) need to    nd all partial derivatives

l(t ;   )

   
     i

l(t ;   ) =

=

=

t

(cid:88)
(cid:88)
(cid:88)

t

t

log p(yt|xt)

(cid:80)
e      (xt ,yt )
y(cid:48)   y e      (xt ,y(cid:48))
(cid:80)
j   j    j (xt ,yt )

e

zxt

log

log

intro: linear learners

41(119)

partial derivatives - some reminders

id28

(cid:73) we always assume log is the natural logarithm loge

1.

2.

3.

4.

   

   x log f = 1

f

   
   x f

   

   x ef = ef    

(cid:80)
t ft =(cid:80)

   x f

t

f
g =

   x f   f    
g    

g 2

   
   x

   
   x

   
   x ft
   x g

intro: linear learners

42(119)

the partial derivatives

id28

l(t ;   ) =

   
     i

intro: linear learners

43(119)

the partial derivatives (1)

id28

l(t ;   ) =

   
     i

=

=

   
     i

(cid:88)
(cid:88)

t

(cid:80)
(cid:80)

e

e

(cid:88)

t

   
     i

log

log

j   j    j (xt ,yt )

zxt

j   j    j (xt ,yt )

zxt

(cid:80)
j   j    j (xt ,yt )

zxt

e

)(

   
     i

(

e

t

(cid:80)
j   j    j (xt ,yt )

zxt

)

intro: linear learners

44(119)

the partial derivatives

(cid:80)
j   j     j (xt ,yt )

zxt

=

now,

e

   
     i

id28

intro: linear learners

45(119)

the partial derivatives (2)
now,

(cid:80)
j   j    j (xt ,yt )

e

   
     i

zxt

(cid:80)

e

zxt

   
     i

(cid:80)

j   j    j (xt ,yt )     e
z 2
xt

(cid:80)
j   j    j (xt ,yt )    
     i

(cid:80)

zxt e

j   j    j (xt ,yt )  i (xt , yt )     e

j   j    j (xt ,yt )    
     i

zxt

id28

zxt

=

=

=

=

(cid:80)
j   j    j (xt ,yt )
(cid:80)
j   j    j (xt ,yt )

z 2
xt

e

e

z 2
xt

z 2
xt

(zxt   i (xt , yt )        
     i

zxt )

(zxt   i (xt , yt )

(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

e

(cid:48)

))

because

   
     i

zxt =

   
     i

(cid:88)

y(cid:48)   y

(cid:80)

e

j   j    j (xt ,y(cid:48)) =

j   j    j (xt ,y(cid:48))  i (xt , y

(cid:48)

)

    (cid:88)
(cid:88)
(cid:80)

y(cid:48)   y

e

y(cid:48)   y

intro: linear learners

46(119)

the partial derivatives

id28

intro: linear learners

47(119)

id28

the partial derivatives (3)
from (2),

(cid:80)
j   j    j (xt ,yt )

(cid:80)
j   j    j (xt ,yt )

e

   
     i

zxt

(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

(cid:48)

))

=

(

e

e

zxt

z 2
xt

(cid:80)

y(cid:48)   y

(zxt   i (xt , yt )

    (cid:88)
(cid:80)
(zxt   i (xt , yt )     (cid:88)
(cid:88)
  i (xt , yt )    (cid:88)
(cid:80)
j   j    j (xt ,y(cid:48))
(cid:88)
  i (xt , yt )    (cid:88)

j   j    j (xt ,yt ) )(

   
     i

1
zxt

y(cid:48)   y

y(cid:48)   y

p(y

zxt

e

e

e

t

t

(cid:88)
(cid:88)
(cid:88)
(cid:88)

t

t

t

t

y(cid:48)   y

(cid:48)|xt )  i (xt , y

(cid:48)

)

j   j    j (xt ,yt )
zxt
(cid:80)
j   j    j (xt ,y(cid:48))  i (xt , y

e

)

(cid:48)

)))

  i (xt , y

(cid:48)

)

sub this in (1),

l(t ;   ) =

   
     i

=

=

=

intro: linear learners

48(119)

finally!!!

id28

(cid:73) after all that,

l(t ;   ) =

   
     i

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

(cid:73) and the gradient is:
(cid:79)l(t ;   ) = (

   

l(t ;   ),

     0

l(t ;   ), . . . ,

   

     1

l(t ;   ))

   

     m

(cid:73) so we can now use gradient ascent to    nd   !!

intro: linear learners

49(119)

id28 summary

(cid:73) de   ne id155

id28

(cid:73) set weights to maximize log-likelihood of training data:

p(y|x) =

e      (x,y)

zx

(cid:88)

   = arg max

  

t

log p(yt|xt)

(cid:73) can    nd the gradient and run gradient ascent (or any

gradient-based optimization algorithm)

l(t ;   ) =

   
     i

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

intro: linear learners

50(119)

id28

id28 = maximum id178

(cid:73) well-known equivalence
(cid:73) max ent: maximize id178 subject to constraints on

features: p = arg maxp h(p) under constraints

(cid:73) empirical feature counts must equal expected counts

(cid:73) quick intuition

(cid:73) partial derivative in id28

(cid:88)

  i (xt, yt)    (cid:88)

t

t

(cid:88)

y(cid:48)   y

p(y(cid:48)|xt)  i (xt, y(cid:48))

l(t ;   ) =

   
     i

(cid:73) first term is empirical feature counts and second term is

expected counts

(cid:73) derivative set to zero maximizes function
(cid:73) therefore when both counts are equivalent, we optimize the

id28 objective!

intro: linear learners

51(119)

id88

id88

intro: linear learners

52(119)

id88 learning algorithm

id88

t=1

for n : 1..n

training data: t = {(xt, yt)}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.

return   i

for t : 1..t
let y(cid:48) = arg maxy(cid:48)   (i)      (xt, y(cid:48))
if y(cid:48) (cid:54)= yt

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))
i = i + 1

intro: linear learners

53(119)

id88: separability and margin

id88

(cid:73) given an training instance (xt, yt), de   ne:
(cid:73)   yt = y     {yt}
(cid:73) i.e.,   yt is the set of incorrect labels for xt

(cid:73) a training set t is separable with margin    > 0 if there exists

a vector u with (cid:107)u(cid:107) = 1 such that:

u      (xt, yt)     u      (xt, y(cid:48))       

(2)

for all y(cid:48)       yt and ||u|| =

(cid:113)(cid:80)

j u2
j

(cid:73) assumption: the training set is separable with margin   

intro: linear learners

54(119)

id88: main theorem

id88

(cid:73) theorem: for any training set separable with a margin of   ,

the following holds for the id88 algorithm:

mistakes made during training     r 2
  2

where r     ||  (xt, yt)       (xt, y(cid:48))|| for all (xt, yt)     t and
y(cid:48)       yt

(cid:73) thus, after a    nite number of training iterations, the error on

the training set will converge to zero

(cid:73) let   s prove it!

intro: linear learners

55(119)

id88

id88 learning algorithm
training data: t = {(xt , yt )}|t |

(cid:73) lower bound:

t=1

1.
2.
3.
4.
5.
6.
7.
8.

  (0) = 0; i = 0
for n : 1..n

for t : 1..t

let y(cid:48) = arg maxy(cid:48)   (i)      (xt , y(cid:48))
if y(cid:48) (cid:54)= yt
  (i+1) =   (i) +   (xt , yt )       (xt , y(cid:48))

i = i + 1

return   i

  (k   1) are weights before k th error
suppose k th error made at (xt , yt )
y(cid:48) = arg maxy(cid:48)   (k   1)      (xt , y(cid:48))
y(cid:48) (cid:54)= yt
  (k) =
  (k   1) +   (xt , yt )       (xt , y(cid:48))

intro: linear learners

56(119)

id88

id88 learning algorithm
training data: t = {(xt , yt )}|t |

(cid:73) lower bound:

1.
2.
3.
4.
5.
6.
7.
8.

t=1

for t : 1..t

  (0) = 0; i = 0
for n : 1..n

let y(cid:48) = arg maxy(cid:48)   (i)      (xt , y(cid:48))
if y(cid:48) (cid:54)= yt
  (i+1) =   (i) +   (xt , yt )       (xt , y(cid:48))

  (k   1) are weights before k th error
suppose k th error made at (xt , yt )
y(cid:48) = arg maxy(cid:48)   (k   1)      (xt , y(cid:48))
y(cid:48) (cid:54)= yt
  (k) =
  (k   1) +   (xt , yt )       (xt , y(cid:48))
return   i
u      (k) = u      (k   1) + u    (  (xt , yt )       (xt , y(cid:48)))     u      (k   1) +   , by (2)
since   (0) = 0 and u      (0) = 0, for all k: u      (k)     k  , by induction on k
since u      (k)     ||u||    ||  (k)||, by the law of cosines, and ||u|| = 1, then
||  (k)||     k  

i = i + 1

intro: linear learners

56(119)

id88

id88 learning algorithm
training data: t = {(xt , yt )}|t |

(cid:73) lower bound:

1.
2.
3.
4.
5.
6.
7.
8.

t=1

i = i + 1

for t : 1..t

  (0) = 0; i = 0
for n : 1..n

let y(cid:48) = arg maxy(cid:48)   (i)      (xt , y(cid:48))
if y(cid:48) (cid:54)= yt
  (i+1) =   (i) +   (xt , yt )       (xt , y(cid:48))

  (k   1) are weights before k th error
suppose k th error made at (xt , yt )
y(cid:48) = arg maxy(cid:48)   (k   1)      (xt , y(cid:48))
y(cid:48) (cid:54)= yt
  (k) =
  (k   1) +   (xt , yt )       (xt , y(cid:48))
return   i
u      (k) = u      (k   1) + u    (  (xt , yt )       (xt , y(cid:48)))     u      (k   1) +   , by (2)
since   (0) = 0 and u      (0) = 0, for all k: u      (k)     k  , by induction on k
since u      (k)     ||u||    ||  (k)||, by the law of cosines, and ||u|| = 1, then
||  (k)||     k  
(cid:73) upper bound:

||  (k)||2 = ||  (k   1)||2 + ||  (xt , yt )       (xt , y(cid:48))||2 + 2  (k   1)    (  (xt , yt )       (xt , y(cid:48)))
||  (k)||2     ||  (k   1)||2 + r 2, since r     ||  (xt , yt )       (xt , y(cid:48))||

and   (k   1)      (xt , yt )       (k   1)      (xt , y(cid:48))     0

    kr 2 for all k, by induction on k

intro: linear learners

56(119)

id88 learning algorithm

id88

(cid:73) we have just shown that ||  (k)||     k   and ||  (k)||2     kr 2

(cid:73) therefore,

k 2  2     ||  (k)||2     kr 2

(cid:73) and solving for k

k     r 2
  2

(cid:73) therefore the number of errors is bounded!

intro: linear learners

57(119)

id88 summary

id88

(cid:73) learns parameters of a linear model by minimizing error
(cid:73) guaranteed to    nd a    in a    nite amount of time
(cid:73) id88 is an example of an online learning algorithm
(cid:73)    is updated based on a single training instance in isolation

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))

intro: linear learners

58(119)

averaged id88

id88

t=1

for n : 1..n

training data: t = {(xt, yt)}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
6.
7.
8.

i   (i)(cid:1) / (n    t )

return(cid:0)(cid:80)

  (i+1) =   (i)

i = i + 1

else

for t : 1..t
let y(cid:48) = arg maxy(cid:48)   (i)      (xt, y(cid:48))
if y(cid:48) (cid:54)= yt

  (i+1) =   (i) +   (xt, yt)       (xt, y(cid:48))

intro: linear learners

59(119)

margin

training

testing

id88

denote the
value of the
margin by   

intro: linear learners

60(119)

maximizing margin

id88

(cid:73) for a training set t
(cid:73) margin of a weight vector    is smallest    such that

        (xt, yt)             (xt, y(cid:48))       
(cid:73) for every training instance (xt, yt)     t , y(cid:48)       yt

intro: linear learners

61(119)

maximizing margin

id88

(cid:73) intuitively maximizing margin makes sense
(cid:73) by cross-validation, the generalization error on unseen test
data can be shown to be proportional to the inverse of the
margin

     

r 2

  2    |t |

(cid:73) id88: we have shown that:

(cid:73) if a training set is separable by some margin, the id88

will    nd a    that separates the data

(cid:73) however, the id88 does not pick    to maximize the

margin!

intro: linear learners

62(119)

support vector machines

support vector machines (id166s)

intro: linear learners

63(119)

support vector machines

maximizing margin

let    > 0

such that:

max
||  ||=1

  

        (xt, yt)             (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

(cid:73) note: algorithm still minimizes error if data is separable
(cid:73) ||  || is bound since scaling trivially produces larger margin
  (        (xt, yt)             (xt, y(cid:48)))         , for some        1

intro: linear learners

64(119)

max margin = min norm

support vector machines

let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

intro: linear learners

65(119)

support vector machines

max margin = min norm
let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

intro: linear learners

66(119)

support vector machines

max margin = min norm
let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

min norm (step 1):

max

u

1
||u||

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

intro: linear learners

67(119)

max margin = min norm
let    > 0

support vector machines

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

min norm (step 1):

||u||

min
u

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

intro: linear learners

68(119)

max margin = min norm
let    > 0

support vector machines

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

min norm (step 2):

||u||

min
u

such that:
  u    (xt, yt)     u    (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

intro: linear learners

69(119)

max margin = min norm
let    > 0

support vector machines

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

min norm (step 2):

||u||

min
u

such that:
u    (xt, yt)   u    (xt, y(cid:48))     1

   (xt, yt)     t
and y(cid:48)       yt

intro: linear learners

70(119)

support vector machines

max margin = min norm
let    > 0

max margin:

max
||  ||=1

  

such that:
      (xt, yt)         (xt, y(cid:48))       

   (xt, yt)     t
and y(cid:48)       yt

  
change variables: u =
  
||  || = 1 i    ||u|| = 1/  ,
then    = 1/||u||

min norm (step 3):

min
u

||u||2

1
2

such that:
u    (xt, yt)   u    (xt, y(cid:48))     1

   (xt, yt)     t
and y(cid:48)       yt

intro: linear learners

71(119)

support vector machines

max margin = min norm

let    > 0

max margin:

min norm:

max
||  ||=1

  

min
u

||u||2

1
2

such that:
      (xt, yt)         (xt, y(cid:48))       

such that:
u    (xt, yt)   u    (xt, y(cid:48))     1

   (xt, yt)     t
and y(cid:48)       yt

   (xt, yt)     t
and y(cid:48)       yt

(cid:73) intuition: instead of    xing ||  || we    x the margin    = 1

intro: linear learners

72(119)

support vector machines

support vector machines

(cid:73) constrained optimization problem

such that:

   = arg min

  

||  ||2

1
2

        (xt, yt)             (xt, y(cid:48))     1

   (xt, yt)     t and y(cid:48)       yt

(cid:73) support vectors: examples where

        (xt, yt)             (xt, y(cid:48)) = 1
for training instance (xt, yt)     t and all y(cid:48)       yt

intro: linear learners

73(119)

support vector machines

support vector machines

(cid:73) what if data is not separable?

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)             (xt, y(cid:48))     1       t and   t     0

   (xt, yt)     t and y(cid:48)       yt

(cid:73)   t: slack variable representing amount of constraint violation
(cid:73) if data is separable, optimal solution has   i = 0,    i

c balances focus on margin and on error

intro: linear learners

74(119)

support vector machines

support vector machines

(cid:73) what if data is not separable?

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)             (xt, y(cid:48))     1       t and   t     0

   (xt, yt)     t and y(cid:48)       yt

(cid:73)   t: slack variable representing amount of constraint violation
(cid:73) if data is separable, optimal solution has   i = 0,    i

c balances focus on margin (c < 1

2 ) and on error (c > 1
2 )

intro: linear learners

75(119)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)             (xt, y(cid:48))     1       t
where   t     0 and    (xt, yt)     t and y(cid:48)       yt

(cid:73) computing the dual form results in a quadratic programming

problem     a well-known id76 problem

(cid:73) can we have representation of this objective that allows more

direct optimization?

intro: linear learners

76(119)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

        (xt, yt)     max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))     1       t

intro: linear learners

76(119)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 + c

1
2

|t |(cid:88)

t=1

  t

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

(cid:124)

        (xt, y(cid:48))             (xt, yt)

negated margin for example

(cid:123)(cid:122)

(cid:125)

intro: linear learners

76(119)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2 +

  
2

|t |(cid:88)

t=1

  t

   =

1
c

such that:

  t     1 + max
y(cid:48)(cid:54)=yt

(cid:124)

        (xt, y(cid:48))             (xt, yt)

negated margin for example

(cid:123)(cid:122)

(cid:125)

intro: linear learners

76(119)

support vector machines

support vector machines

  t     1 + max
y(cid:48)(cid:54)=yt

(cid:124)

(cid:123)(cid:122)

        (xt, y(cid:48))             (xt, yt)

(cid:125)

negated margin for example

(cid:73) if (cid:107)  (cid:107) classi   es (xt, yt) with margin 1, penalty   t = 0
(cid:73) otherwise:   t = 1 + maxy(cid:48)(cid:54)=yt         (xt, y(cid:48))             (xt, yt)
(cid:73) that means that in the end   t will be:

  t = max{0, 1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt)}

intro: linear learners

77(119)

support vector machines

support vector machines

   = arg min

  ,  

||  ||2+

  
2

|t |(cid:88)

t=1

  t s.t.   t     1+ max
y(cid:48)(cid:54)=yt

      (xt, y(cid:48))         (xt, yt)

   = arg min

l(t ;   ) = arg min

loss((xt, yt);   ) +

  
2

hinge loss

|t |(cid:88)

  

t=1

= arg min

  

max (0, 1 + max
y(cid:48)(cid:54)=yt

        (xt, y(cid:48))             (xt, yt))

||  ||2

  
2

(cid:73) hinge loss allows unconstrained optimization (later!)

intro: linear learners

78(119)

  

       |t |(cid:88)

t=1

||  ||2

       +

support vector machines

summary

what we have covered

(cid:73) linear models

(cid:73) naive bayes
(cid:73) id28
(cid:73) id88
(cid:73) support vector machines

what is next

(cid:73) id173
(cid:73) online learning
(cid:73) non-linear models

intro: linear learners

79(119)

id173

id173

intro: linear learners

80(119)

fit of a model

id173

(cid:73) two sources of error:

(cid:73) bias error, measures how well the hypothesis class    ts the

space we are trying to model

(cid:73) variance error, measures sensitivity to training set selection
(cid:73) want to balance these two things

intro: linear learners

81(119)

fit of a model

id173

intro: linear learners

82(119)

over   tting

id173

(cid:73) early in lecture we made assumption data was i.i.d.

(cid:73) rarely is this true, e.g., syntactic analyzers typically trained on

40,000 sentences from early 1990s wsj news text

(cid:73) even more common: t is very small

(cid:73) this leads to over   tting

(cid:73) e.g.:    fake    is never a verb in wsj treebank (only adjective)

(cid:73) high weight on      (x, y) = 1 if x=fake and y=adjective   
(cid:73) of course: leads to high log-likelihood / low error
(cid:73) other features might be more indicative, e.g., adjacent word
identities:    he wants to x his death        x=verb

intro: linear learners

83(119)

id173

id173

(cid:73) in practice, we regularize models to prevent over   tting

l(t ;   )       r(  )

arg max

  

(cid:73) where r(  ) is the id173 function
(cid:73)    controls how much to regularize
(cid:73) most common regularizer

(cid:73) l2: r(  )     (cid:107)  (cid:107)2 = (cid:107)  (cid:107) =(cid:112)(cid:80)

i   2

i     smaller weights desired

intro: linear learners

84(119)

id28 with l2 id173

id173

|t |(cid:88)

(cid:16)

(cid:73) perhaps most common learner in nlp

l(t ;   )       r(  ) =

log

e      (xt ,yt )/zx

t=1

(cid:73) what are the new partial derivatives?

   
   wi

l(t ;   )        
   wi

(cid:73) we know    
   wi
(cid:73) just need    
   wi

l(t ;   )
2(cid:107)  (cid:107)2 =    

  

   wi

  
2

(cid:16)(cid:112)(cid:80)

  r(  )
(cid:17)2

i   2
i

=    
   wi

(cid:17)       

2

(cid:107)  (cid:107)2

(cid:80)

i   2

i =     i

  
2

intro: linear learners

85(119)

support vector machines

id173

(cid:73) id166 in hinge-loss formulation: l2 id173 corresponds

to margin maximization!

   = arg min

  

l(t ;   ) +   r(  )

intro: linear learners

86(119)

support vector machines

id173

(cid:73) id166 in hinge-loss formulation: l2 id173 corresponds

to margin maximization!

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

intro: linear learners

86(119)

support vector machines

id173

(cid:73) id166 in hinge-loss formulation: l2 id173 corresponds

to margin maximization!

l(t ;   ) +   r(  )

   = arg min

  

= arg min

  

= arg min

|t |(cid:88)
|t |(cid:88)

t=1

  

t=1

loss((xt , yt );   ) +   r(  )

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +   r(  )

intro: linear learners

86(119)

support vector machines

id173

(cid:73) id166 in hinge-loss formulation: l2 id173 corresponds

to margin maximization!

l(t ;   ) +   r(  )

   = arg min

  

= arg min

  

= arg min

  

= arg min

t=1

|t |(cid:88)
|t |(cid:88)
|t |(cid:88)

t=1

  

t=1

loss((xt , yt );   ) +   r(  )

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +   r(  )

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

intro: linear learners

86(119)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

intro: linear learners

87(119)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

id166s/hinge-loss: max (0, 1 + maxy(cid:54)=yt (        (xt , y)             (xt , yt )))

|t |(cid:88)

   = arg min

  

t=1

max (0, 1 + max
y(cid:54)=yt

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

intro: linear learners

87(119)

id166s vs. id28

id173

l(t ;   ) +   r(  )

   = arg min

  

= arg min

|t |(cid:88)

  

t=1

loss((xt , yt );   ) +   r(  )

id166s/hinge-loss: max (0, 1 + maxy(cid:54)=yt (        (xt , y)             (xt , yt )))

|t |(cid:88)

   = arg min

max (0, 1 + max
y(cid:54)=yt

  

id28/log-loss:     log (cid:0)e      (xt ,yt )/zx

t=1

(cid:1)

        (xt , y)             (xt , yt )) +

(cid:107)  (cid:107)2

  
2

|t |(cid:88)

(cid:16)

    log

e      (xt ,yt )/zx

(cid:17)

+

(cid:107)  (cid:107)2

  
2

   = arg min

  

t=1

intro: linear learners

87(119)

summary: id168s

   = arg min

  

l(t ;   ) +   r(  ) = arg min

  

|t |(cid:88)

t=1

loss((xt , yt );   ) +   r(  )

id173

intro: linear learners

88(119)

online learning

online learning

intro: linear learners

89(119)

online vs. batch learning

online learning

batch(t );

(cid:73) for 1 . . . n

(cid:73)        update(t ;   )

(cid:73) return   

online(t );

(cid:73) for 1 . . . n

(cid:73) for (xt, yt)     t

(cid:73)        update((xt , yt );   )

(cid:73) end for

(cid:73) end for
(cid:73) return   

e.g., id166s, logistic regres-
sion, naive bayes

e.g., id88

   =    +   (xt, yt)       (xt, y)

intro: linear learners

90(119)

batch id119

online learning

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence

  i =   i   1       (cid:79)l(t ;   i   1)

|t |(cid:88)

=   i   1    

  (cid:79)loss((xt, yt);   i   1)

(cid:73)    > 0 and set so that l(t ;   i ) < l(t ;   i   1)

t=1

intro: linear learners

91(119)

stochastic id119

online learning

(cid:73) stochastic id119 (sgd)

(cid:73) approximate batch gradient (cid:79)l(t ;   ) with stochastic

gradient (cid:79)loss((xt, yt);   )

(cid:73) let l(t ;   ) =(cid:80)|t |

t=1 loss((xt, yt);   )

(cid:73) set   0 = o m
(cid:73) iterate until convergence
(cid:73) sample (xt , yt )     t
(cid:73)   i =   i   1       (cid:79)loss((xt , yt );   i   1)

//    stochastic   

(cid:73) return   

intro: linear learners

92(119)

online id28

online learning

(cid:73) stochastic id119 (sgd)
(cid:73) loss((xt, yt);   ) = log-loss

(cid:73) (cid:79)loss((xt, yt);   ) = (cid:79)(cid:0)    log (cid:0)e      (xt ,yt )/zxt
(cid:1)(cid:1)
(cid:32)
(cid:79)(cid:16)    log
  (xt, yt)    (cid:88)

(cid:73) from id28 section:

e      (xt ,yt )/zxt

(cid:17)(cid:17)

=    

(cid:16)

(cid:33)

p(y|x)  (xt, y)

(cid:73) plus id173 term (if part of model)

y

intro: linear learners

93(119)

online id166s

(cid:73) stochastic id119 (sgd)
(cid:73) loss((xt, yt);   ) = hinge-loss

(cid:79)loss((xt, yt);   ) = (cid:79)

max (0, 1 + max
y(cid:54)=yt

(cid:18)

(cid:73) subgradient is:

online learning

(cid:19)

        (xt, y)             (xt, yt))

(cid:19)

max (0, 1 + max
y(cid:54)=yt

        (xt, y)             (xt, yt))

if         (xt, yt)     maxy         (xt, y)     1
0,
  (xt, y)       (xt, yt), otherwise, where y = maxy         (xt, y)

(cid:73) plus id173 term (l2 norm for id166s):

||  ||2 =     

(cid:79)   
2

intro: linear learners

94(119)

(cid:18)
(cid:40)

(cid:79)

=

online learning

id88 and hinge-loss
id166 subgradient update looks like id88 update

(cid:40)

  i =   i   1     

if         (xt , yt )     maxy         (xt , y)     1
    ,
  (xt , y)       (xt , yt ) +     , otherwise, where y = maxy         (xt , y)

id88

  i =   i   1       

(cid:40)

if         (xt , yt )     maxy         (xt , y)     0
0,
  (xt , y)       (xt , yt ), otherwise, where y = maxy         (xt , y)

id88 = sgd optimization of no-margin hinge-loss (without
id173):

max (0, 1+ max
y(cid:54)=yt

        (xt, y)             (xt, yt))

intro: linear learners

95(119)

online vs. batch learning

online learning

(cid:73) online algorithms

(cid:73) each update step relies only on the derivative for a single

randomly chosen example

(cid:73) computational cost of one step is 1/t compared to batch
(cid:73) easier to implement

(cid:73) larger variance since each gradient is di   erent

(cid:73) variance slows down convergence
(cid:73) requires    ne-tuning of decaying learning rate

(cid:73) batch algorithms

(cid:73) higher cost of averaging gradients over t for each update

(cid:73) implementation more complex
(cid:73) less    ne-tuning, e.g., allows constant learning rates
(cid:73) faster convergence

intro: linear learners

96(119)

variance-reduced online learning

online learning

(cid:73) sgd update extended by velocity vector v weighted by

momentum coe   cient 0        < 1 [polyak 1964]:

(cid:73)

where

  i+1 =   i       (cid:79)loss((xt, yt);   i ) +   vi

vi =   i       i   1

(cid:73) momentum accelerates learning if gradients are aligned along
same direction, and restricts changes when successive gradient
are opposite of each other

(cid:73) general direction of gradient reinforced, perpendicular

directions    ltered out

(cid:73) best of both worlds: e   cient and e   ective!

intro: linear learners

97(119)

online-to-batch conversion

online learning

(cid:73) classical online learning:

(cid:73) data are given as an in   nite sequence of input examples
(cid:73) model makes prediction on next example in sequence

(cid:73) standard nlp applications:

(cid:73)    nite set of training data, prediction on new batch of test data
(cid:73) online learning applied by cycling over    nite data
(cid:73) online-to-batch conversion: which model to use at test time?

(cid:73) last model? random model? best model on heldout set?

intro: linear learners

98(119)

online-to-batch conversion by averaging

online learning

(cid:73) averaged id88

(cid:73)      =(cid:0)(cid:80)

i   (i)(cid:1) / (n    t )

(cid:73) use weight vector averaged over online updates for prediction
(cid:73) how does the id88 mistake bound carry over to batch?
(cid:73) let mk be number of mistakes made during online learning,

then with id203 of at least 1       :

e[loss((x, y);     )]     mk +

(cid:114) 2

ln

1
  

k

(cid:73) = generalization bound based on online performance

[cesa-bianchi et al. 2004]

(cid:73) can be applied to all online learners with convex losses

intro: linear learners

99(119)

summary

quick summary

intro: linear learners

100(119)

linear learners

summary

(cid:73) naive bayes, id88, id28 and id166s
(cid:73) objective functions and id168s
(cid:73) id76
(cid:73) id173
(cid:73) online vs. batch learning

intro: linear learners

101(119)

non-linear models

non-linear models

intro: linear learners

102(119)

non-linear models

non-linear models

(cid:73) some data sets require more than a linear decision boundary

to be correctly modeled

(cid:73) decision boundary is no longer a hyperplane in the feature

space

intro: linear learners

103(119)

kernel machines = id76 for
non-linear models

kernel machines

(cid:73) projecting a linear model into a higher dimensional feature
space can correspond to a non-linear model in the original
space and make non-separable problems separable

(cid:73) for classi   ers based on similarity functions (= kernels),

computing a non-linear kernel is often more e   cient than
calculating the corresponding dot product in the high
dimensional feature space

(cid:73) thus, kernels allow us to e   ciently learn non-linear models by

id76

intro: linear learners

104(119)

monomial features and polynomial kernels

kernel machines

(cid:73) monomial features = d th order products of entries xj of x s.t.
(cid:73) ordered monomial feature map:    : r2     r4 s.t.

for j1, . . . , jd     {1 . . . n}

xj1     xj2                xjd
(x1, x2) (cid:55)    (x 2

1 , x 2

2 , x1x2, x2x1)

(cid:73) computation of kernel from feature map:

  (x)      (x(cid:48)) =

4(cid:88)
1 x(cid:48)2
1 x(cid:48)2

i=1
= x 2
= x 2

=(cid:0)x1x(cid:48)

  i (x)  i (x(cid:48)) (def. dot product)
2 + x1x2x(cid:48)
1x(cid:48)
2 + x2x1x(cid:48)
(cid:1)2
2 + 2x1x2x(cid:48)
1x(cid:48)

2 x(cid:48)2
1 + x 2
2 x(cid:48)2
1 + x 2
1 + x2x(cid:48)

2

2

2x(cid:48)

1 (def.   )

(cid:73) direct application of kernel:   (x)      (x(cid:48)) = (x    x(cid:48))2

intro: linear learners

105(119)

direct application of kernel

kernel machines

(cid:73) let cd be a map from x     rm to vectors cd (x) of all

d th-degree ordered products of entries of x.
then the corresponding kernel computing the dot product of
vectors mapped by cd is:
k (x, x(cid:48)) = cd (x)    cd (x(cid:48)) = (x    x(cid:48))d
monomial:   2 : r2     r3 s.t. (x1, x2) (cid:55)    (x 2

(cid:73) alternative feature map satisfying this de   nition = unordered

2x1x2)

   

1 , x 2
2 ,

intro: linear learners

106(119)

non-linear feature map

kernel machines

  2 : r2     r3 s.t. (x1, x2) (cid:55)    (z1, z2, z3) = (x 2

1 , x 2
2 ,

   

2x1x2)

intro: linear learners

107(119)

kernel de   nition

(cid:73) a kernel is a similarity function between two points that is

symmetric and positive de   nite, which we denote by:

kernel machines

k (xt, xr )     r
(cid:73) let m be a n    n matrix such that ...

mt,r = k (xt, xr )

(cid:73) ... for any n points. called the gram matrix.
(cid:73) symmetric:

k (xt, xr ) = k (xr , xt)

(cid:73) positive de   nite: positivity on diagonal

k (x, x)     0 forall x with equality only for x = 0

intro: linear learners

108(119)

mercer   s theorem

kernel machines

(cid:73) mercer   s theorem: for any kernel k , there exists a    in

some rd , such that:

k (xt, xr ) =   (xt)      (xr )

(cid:73) this means that instead of mapping input data via non-lineear
feature map    and then computing dot product, we can apply
kernels directly without even knowing about   !

intro: linear learners

109(119)

kernel trick

kernel machines

(cid:73) de   ne a kernel, and do not explicitly use dot product between

vectors, only kernel calculations

(cid:73) in some high-dimensional space, this corresponds to dot

product

(cid:73) in that space, the decision boundary is linear, but in the

original space, we now have a non-linear decision boundary

intro: linear learners

110(119)

kernel trick

kernel machines

(cid:73) de   ne a kernel, and do not explicitly use dot product between

vectors, only kernel calculations

(cid:73) in some high-dimensional space, this corresponds to dot

product

(cid:73) in that space, the decision boundary is linear, but in the

original space, we now have a non-linear decision boundary
(cid:73) note: since our features are over pairs (x, y), we will write

kernels over pairs

k ((xt, yt), (xr , yr )) =   (xt, yt)      (xr , yr )

(cid:73) let   s do it for the id88!

intro: linear learners

110(119)

kernel machines

t=1

for n : 1..n

kernel trick     id88 algorithm
training data: t = {(xt , yt )}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.

for t : 1..t
let y = arg maxy   (i)      (xt , y)
if y (cid:54)= yt

  (i+1) =   (i) +   (xt , yt )       (xt , y)
i = i + 1

return   i

intro: linear learners

111(119)

kernel machines

t=1

for n : 1..n

kernel trick     id88 algorithm
training data: t = {(xt , yt )}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.
(cid:73) each feature function   (xt, yt) is added and   (xt, y) is

for t : 1..t
let y = arg maxy   (i)      (xt , y)
if y (cid:54)= yt

  (i+1) =   (i) +   (xt , yt )       (xt , y)
i = i + 1

return   i

subtracted to    say   y,t times

(cid:73)   y,t is proportional to the # of times during learning label y is

predicted for example t and caused an update because of
misclassi   cation

intro: linear learners

111(119)

kernel machines

t=1

for n : 1..n

kernel trick     id88 algorithm
training data: t = {(xt , yt )}|t |
1.   (0) = 0; i = 0
2.
3.
4.
5.
6.
7.
8.
(cid:73) each feature function   (xt, yt) is added and   (xt, y) is

for t : 1..t
let y = arg maxy   (i)      (xt , y)
if y (cid:54)= yt

  (i+1) =   (i) +   (xt , yt )       (xt , y)
i = i + 1

return   i

subtracted to    say   y,t times

(cid:73)   y,t is proportional to the # of times during learning label y is

predicted for example t and caused an update because of
misclassi   cation

(cid:73) thus,

   =

  y,t[  (xt, yt)       (xt, y)]

(cid:88)

t,y

intro: linear learners

111(119)

kernel trick     id88 algorithm

(cid:73) we can re-write the argmax function as:

kernel machines

  (i)      (x, y   )

y    = arg max

y   

= arg max

y   

= arg max

y   

= arg max

y   

(cid:88)
(cid:88)
(cid:88)

t,y

t,y

t,y

  y,t[  (xt, yt)       (xt, y)]      (x, y   )

  y,t[  (xt, yt)      (x, y   )       (xt, y)      (x, y   )]

  y,t[k ((xt, yt), (x, y   ))     k ((xt, y), (x, y   ))]

(cid:73) we can then re-write the id88 algorithm strictly with

kernels

intro: linear learners

112(119)

kernel trick     id88 algorithm

kernel machines

(cid:73) training: t = {(xt , yt )}|t |

1.
2.
3.
4.
5.
6.

t=1

   y, t set   y,t = 0
for n : 1..n

let y    = arg maxy   (cid:80)

for t : 1..t
if y    (cid:54)= yt

  y   ,t =   y   ,t + 1

t,y   y,t [k ((xt , yt ), (xt , y   ))     k ((xt , y), (xt , y   ))]

(cid:73) testing on unseen instance x:

y    = arg max

y   

  y,t[k ((xt, yt), (x, y   ))   k ((xt, y), (x, y   ))]

(cid:88)

t,y

intro: linear learners

113(119)

kernels summary

kernel machines

(cid:73) can turn a linear model into a non-linear model
(cid:73) kernels project feature space to higher dimensions

(cid:73) sometimes exponentially larger
(cid:73) sometimes an in   nite space!

(cid:73) can    kernelize    algorithms to make them non-linear
(cid:73) id76 methods still applicable to learn

parameters

(cid:73) disadvantage: exact kernel methods depend polynomially on
the number of training examples - infeasible for large datasets

intro: linear learners

114(119)

kernel machines

kernels for large training sets

(cid:73) alternative to exact kernels: explicit randomized feature map

[rahimi and recht 2007]

(cid:73) shallow neural network by random fourier/binning

transformation:

(cid:73) random weights from input to hidden units
(cid:73) cosine as transfer function
(cid:73) id76 of weights from hidden to output units

intro: linear learners

115(119)

kernel machines

summary

basic principles of machine learning:

(cid:73) to do learning, we set up an objective function that tells the

   t of the model to the data

(cid:73) we optimize with respect to the model (weights, id203

model, etc.)

(cid:73) can do it in a batch or online (preferred!) fashion

what model to use?

(cid:73) one example of a model: linear model
(cid:73) can kernelize/randomize these models to get non-linear

models

(cid:73) id76 applicable for both types of model

intro: linear learners

116(119)

kernel machines

outlook

(cid:73) multiclass linear models are basic building blocks for further

lectures: structured output prediction, id114,
multilayer id88 neural networks

intro: linear learners

117(119)

outlook

kernel machines

(cid:73) multiclass linear models are basic building blocks for further

lectures: structured output prediction, id114,
multilayer id88 neural networks

(cid:73) kernel machines

(cid:73) kernel machines introduce nonlinearity by using speci   c

feature maps or kernels

(cid:73) feature map or kernel is not part of optimization problem, thus

id76 of id168 for linear model possible

(cid:73) neural networks

(cid:73) similarities and nonlinear combinations of features are learned:

representation learning

(cid:73) we lose the advantages of id76 since objective

functions will be nonconvex

intro: linear learners

117(119)

wrap up and questions

wrap up and time for questions

intro: linear learners

118(119)

references

further reading
(cid:73) introductory example:
(cid:73) j. y. lettvin, h. r. maturana, w. s. mcculloch, and w. h. pitts. 1959.

what the frog   s eye tells the frog   s brain. proc. inst. radio engr., 47:1940   1951.

(cid:73) naive bayes:
(cid:73) pedro domingos and michael pazzani. 1997.

on the optimality of the simple bayesian classi   er under zero-one loss. machine
learning, (29):103   130.

(cid:73) id28:
(cid:73) bradley efron. 1975.

the e   ciency of id28 compared to normal discriminant analysis.
journal of the american statistical association, 70(352):892   898.

(cid:73) adam l. berger, vincent j. della pietra, and stephen a. della pietra. 1996.
a maximum id178 approach to natural language processing. computational
linguistics, 22(1):39   71.

(cid:73) stefan riezler, detlef prescher, jonas kuhn, and mark johnson. 2000.

intro: linear learners

118(119)

lexicalized stochastic modeling of constraint-based grammars using log-linear
measures and em training. in proceedings of the 38th annual meeting of the
association for computational linguistics (acl   00), hong kong.

references

(cid:73) id88:
(cid:73) albert b.j. noviko   . 1962.

on convergence proofs on id88s. symposium on the mathematical theory of
automata, 12:615   622.

(cid:73) yoav freund and robert e. schapire. 1999.

large margin classi   cation using the id88 algorithm. journal of machine
learning research, 37:277   296.

(cid:73) michael collins. 2002.

discriminative training methods for id48: theory and experiments
with id88 algorithms. in proceedings of the conference on empirical
methods in natural language processing (emnlp   02), philadelphia, pa.

(cid:73) id166:
(cid:73) vladimir n. vapnik. 1998.

statistical learning theory. wiley.

(cid:73) olivier chapelle. 2007.

intro: linear learners

118(119)

references

training a support vector machine in the primal. neural computation,
19(5):1155   1178.

(cid:73) ben taskar, carlos guestrin, and daphne koller. 2003.

max-margin markov networks. in advances in neural information processing
systems 17 (nips   03), vancouver, canada.

(cid:73) ioannis tsochantaridis, thomas hofmann, thorsten joachims, and yasemin altun.

2004.
support vector machine learning for interdependent and structured output spaces.
in proceedings of the 21st international conference on machine learning
(icml   04), ban   , canada.
(cid:73) kernels and id173:
(cid:73) bernhard sch  olkopf and alexander j. smola. 2002.

learning with kernels. support vector machines, id173, optimization, and
beyond. the mit press.

(cid:73) ali rahimi and ben recht. 2007.

random features for large-scale kernel machines. in advances in neural
information processing systems (nips), vancouver, b.c., canada.

intro: linear learners

118(119)

references

(cid:73) zhiyun lu, dong guo, alireza bagheri garakani, kuan liu, avner may, aurelien
bellet, linxi fan, michael collins, brian kingsbury, michael picheny, and fei sha.
2016.
a comparison between deep neural nets and kernel acoustic models for speech
recognition. in ieee international conference on acoustics, speech, and signal
processing (icassp).

(cid:73) convex and non-id76:
(cid:73) yurii nesterov. 2004.

introductory lectures on id76: a basic course. springer.

(cid:73) stephen boyd and lieven vandenberghe. 2004.

id76. cambridge university press.
(cid:73) dimitri p. bertsekas and john n. tsitsiklis. 1996.
neuro-id145. athena scienti   c.

(cid:73) online/stochastic optimization:
(cid:73) herbert robbins and sutton monro. 1951.

a stochastic approximation method. annals of mathematical statistics,
22(3):400   407.

(cid:73) boris t. polyak. 1964.

intro: linear learners

118(119)

thanks

some methods of speeding up the convergence of iteration methods. ussr
computational mathematics and mathematical physics, 4(5):1     17.

(cid:73) nicol`o cesa-bianchi, alex conconi, and claudio gentile. 2004.

on the generalization ablility of on-line learning algorithms. ieee transactons on
id205, 50(9):2050   2057.

(cid:73) ilya sutskever, james martens, george e. dahl, and geo   rey e. hinton. 2013.

on the importance of initialization and momentum in deep learning. in proceedings
of international conference on machine learning (icml), pages 1139   1147.

(cid:73) leon bottou, frank e. curtis, and jorge nocedal. 2016.

optimization methods for large-scale machine learning. corr,
abs/arxiv:1606.04838v1.

intro: linear learners

119(119)

thanks

thanks

(cid:73) thanks to steven abney, shay cohen, chris

dyer, philipp koehn, ryan mcdonald for
allowing the use of some their slide materials in
this lecture.

intro: linear learners

119(119)

