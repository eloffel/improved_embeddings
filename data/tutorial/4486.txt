real world interactive learning
alekh agarwal
john langford
icml tutorial, august 6
slides and full references at http://hunch.net/~rwil 

training labels
accurate digit classifier
2

supervised learner

the supervised learning paradigm
supervised learning is cool
how about news?
repeatedly: 
 observe features of user+articles
 choose a news article.
 observe click-or-not
goal: maximize fraction of clicks
a standard pipeline
q: what goes wrong?
a: need right signal for right answer
is ukraine                  interesting to john       ?
q: what goes wrong?
a: the world changes!
model value over time

features
action
consequence
(user history, news stories)
(click-or-not)
(selected news story)

learning
interactive learning
q: why interactive learning?
a: $$$
use free interaction data 
rather than expensive labels
q: why interactive learning?
ai: an economically viable digital agent that explores, learns, and acts
ai: a function programmed with data
ai: a function programmed with data
flavors of interactive learning
ex: which advice?
repeatedly: 
 observe features of user+advice
 choose an advice.
 observe steps walked
goal: healthy behaviors
other real-world applications
news rec: [lcls    10]
ad choice: [bpqccprss    12]
ad format: [trsa    13]
education: [mllbp    14]
music rec: [wwhw    14]
robotics: [pg    16]
wellness/health: [zkz    09, sllspm    11, nstwcsm    14, pgcrrh    14, nhs    15, khsbatm    15, hfkmty    16]
good fit for many real problems
take-aways
algs & theory overview
evaluate?
learn?
explore?
things that go wrong in practice
systems for going right
really doing it in practice
outline
contextual bandits
policies
policy maps features to actions.  
policy = classifier that acts.
exploration


policy

randomization
exploration


policy

randomization
exploration


policy
inverse propensity score(ips) [ht    52]

propensity score
what do we know about ips?
reward over time
offline estimate of system   s performance
system   s actual online performance
offline estimate of baseline   s performance
better evaluation techniques
double robust: [dll    11]

weighted ips: [k    92, sj    15]

clipping: [bl    08]
learning from exploration [   z 03]
vowpal wabbit: online/fast learning
bsd license, 10 year project
mailing list>500, github>1k forks, >4k stars, >1k issues, >100 contributors
command line/c++/c#/python/java/azureml/daemon


vw for contextual bandit learning
echo    1:2:0.5 | here are some features    | vw --cb 2

format: <action>:<loss>:<id203> | features   

training on a large dataset:
vw --cb 2 rcv1.cb.gz --ngram 2 --skips 4 -b 24
	        result: 0.048616
better learning from exploration data
policy gradient: [w    92]

offset tree: [bl    09]

double robust for learning: [dll    11]

multitask regression: unpublished, but in vowpal wabbit

weighted ips for learning: [sj    15]
evaluating online learning
problem: how do you evaluate an online learning algorithm offline?
answer: use progressive validation [bkl    99, id35    04]































theorem: 
  1) expected pv value = uniform expected policy value.
  2) trust like a test set error.
how do you do exploration?
better exploration algorithms
better algorithms maintain ensemble and explore amongst actions of this ensemble.
thompson sampling: [t    33]
exp4: [acfs    02]
epoch greedy: [lz    07]
polytime: [dhkklrz    11]
cover&bag: [ahklls    14]
bootstrap: [ek    14]
evaluating exploration algorithms
problem: how do you take the choice of examples acquired by an exploration algorithm into account?

answer: rejection sample from history. [dell    12] 






















theorem: realized history is unbiased up to length observed.

better versions: [dell    14] & vw code
more details!
nips tutorial: http://hunch.net/~jl/interact.pdf 

john   s spring 2017 cornell tech class (http://hunch.net/~mltf) with slides and recordings

[forthcoming] alekh   s fall 2017 columbia class with extensive class notes.
good fit for many problems
fundamental questions have useful answers
take-aways
