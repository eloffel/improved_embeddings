simple and ef   cient 
learning with dynamic 

neural networks

graham neubig

code examples:

https://github.com/neubig/lxmls-2017

neural networks for 

language

    neural networks give us new tools to process data:   

images, speech, text 

    particularly for text, we would like to use networks 

with complicated structure 

    and we want to go from idea to code quickly

example task: sentiment

i   hate   this  movie

i   love   this   movie

i do n   t hate this movie

very good 

good 
neutral 
bad 

very bad

very good 

good 
neutral 
bad 

very bad

very good 

good 
neutral 
bad 

very bad

what is necessary for 
neural network training
    de   ne computation 
    add data 
    calculate result (forward) 
    calculate gradients (backward) 
    update parameters

paradigm 1: static graphs   

(tensor   ow, theano)

    de   ne 
    for each data point: 

    add data 
forward

   

    backward 
    update

advantages/disadvantages 

of static graphs

    advantages:

    can be optimized at de   nition time 
    easy to feed data to gpus, etc., via data iterators 

    disadvantages:

    dif   cult to implement nets with varying structure (trees, 

graphs,    ow control) 

    need to learn big api that implements    ow control in the 

   graph    language

paradigm 2: dynamic graphs 

(chainer, dynet, pytorch)

    for each data point: 

    de   ne
    add data/forward 
    backward 
    update

advantages/disadvantages 

of dynamic graphs

    advantages:

    api is closer to standard python/c++ 

    easy to implement nets with varying structure 

    disadvantages:

    harder to optimize graphs (but still possible, see 

end of presentation! 

    harder to schedule of data transfer, etc.

dynet

https://github.com/clab/dynet

    dynamic graph toolkit implemented in c++ usable 

from c++, python, scala/java (soon haskell?) 

    very fast on cpu (good for prototyping nlp 

apps!), similar support to other toolkits for gpu 

    support for easy implementation of mini-

batching, even in dif   cult situations

programming examples

bag of words (bow)

i

hate

this movie

lookup

lookup

lookup

lookup

bias

scores

+

+

+

+

=

probs

softmax

at beginning of training

# start dynet and define trainer 
model = dy.model() 
trainer = dy.adamtrainer(model) 
# define the model 
w_sm = model.add_lookup_parameters((nwords, ntags)) 
b_sm = model.add_parameters((ntags)) 
trainer
our strategy for training the model (here adam)
regular parameters
a parameter vector/matrix/tensor (here b_sm is size ntags)
lookup parameters
one vector for each word (here w_sm has nwords words,   
vector of size ntags)

calculating the network

# a function to calculate scores for one sentence 
def calc_scores(words): 
  # create a computation graph, and add parameters 
  dy.renew_cg() 
  b_sm_exp = dy.parameter(b_sm) 
  # take the sum of all the embedding vectors for each word 
  score = dy.esum([dy.lookup(w_sm, x) for x in words]) 
  # add the bias vector and return 
  return score + b_sm_exp 

training time

# perform training over the entire corpus 
train_loss = 0.0 
for words, tag in train: 
  # calculate the scores for each candidate 
  my_scores = calc_scores(words) 
  # cross-id178 id168 for the correct tag. my_loss is a   
  # dynet expression (we have not performed calculation yet) 
  my_loss = dy.pickneglogsoftmax(my_words, tag) 
  # call the ".value()" function to perform actual calculation 
  train_loss += my_loss.value() 
  # perform backward calculation and update 
  my_loss.backward() 
  trainer.update() 
# print the values 
print("iter %r: train loss/sent=%.4f" % (iter, train_loss/len(train))) 

test time

test_correct = 0.0 
for words, tag in dev: 
  # define the computation graph 
  scores = calc_scores(words) 
  # calculate the actual values 
  score_values = scores.npvalue() 
  # find the tag with the highest score, and grade it 
  predict = np.argmax(score_values) 
  if predict == tag: 
    test_correct += 1 
print("iter %r: test acc=%.4f" % (iter, test_correct/len(dev))) 

code walk! 
(bow.ipynb)

continuous bag of words 

(cbow)
this movie

i

hate

lookup

lookup

lookup

lookup

+
=

+

w

+

+

=
scores

bias

code walk! 
(cbow.ipynb)

deep cbow

i

hate

this movie

+

+
=

+

tanh(   
  w1*h + b1)

tanh(   
  w2*h + b2)

w

+

=
scores

bias

code walk! 

(deep-cbow.ipynb)

bi-directional lstm

i

hate

this

movie

lstm

lstm

lstm

lstm

lstm

lstm

lstm

lstm

concat

w

+

=
scores

bias

builders:   

convenience classes for id56, etc.
    model de   nition time   
fwdlstm = dy.lstmbuilder(num_layers, 
                         embedding_size, 
                         hidden_size, 
                         model)
    training/testing time
# get the initial state 
fwd_state = fwdlstm.initial_state() 
# add the words one at a time 
for word_emb in word_embs: 
  fwd_state = fwd_state.add_input(word_emb) 
# create the output as an expression 
fwd_output = fwd_state.output() 

   
   
code walk! 
(lstm.ipynb)

tree-structured id56/lstm

i

hate

this

movie

id56

id56

id56

w

+

=
scores

bias

code walk! 

(tree-class.ipynb)

ef   ciency tricks:   
operation batching

ef   ciency tricks:   
mini-batching

    on modern hardware 10 operations of size 1 is 

much slower than 1 operation of size 10 

    minibatching combines together smaller operations 

into one big one

minibatching

manual mini-batching

    dynet has special minibatch operations for lookup 

and id168s, everything else automatic 

    you need to: 

    group sentences into a mini batch (optionally, for 

ef   ciency group sentences by length) 

    select the    t   th word in each sentence, and send 

them to the lookup and id168s

mini-batched code example

but what about these?

words

sentences

s

vp

vp

np

pp

alice

gave a message

to bob

phrases

documents

this film was completely unbelievable.
the characters were wooden and the plot was absurd.
that being said, i liked it.

automatic mini-batching!

    tensorflow fold (complicated combinators) 

    dynet autobatch (basically effortless implementation)

autobatching algorithm

    for each minibatch: 

    for each data point in mini-batch: 

    de   ne/add data

    sum losses
    forward (autobatch engine does magic!) 
    backward 
    update

speed improvements

questions?

https://github.com/neubig/lxmls-2017

https://github.com/clab/dynet

supplementary material

dynamic+immediate evaluation   

(pytorch, chainer)

    for each data point: 

    de   ne/add data/forward 
    backward 
    update

dynamic+lazy evaluation 

(dynet)

    for each data point: 
    de   ne/add data
    forward 
    backward 
    update

advantages/disadvantages 

of dynamic+immediate evaluation
    advantages:

    easy to implement nets with varying structure, 

api is closer to standard python/c++ 

    disadvantages:

    cannot be optimized at de   nition time 

    harder to schedule of data transfer, etc.

advantages/disadvantages 
of dynamic+lazy evaluation
    advantages:

    easy to implement nets with varying structure 

    api is closer to standard python/c++ 

    can be optimized at de   nition time (see end of 

presentation!) 
    disadvantages:

    harder to schedule of data transfer, etc.

