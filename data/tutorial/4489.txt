deep learning  

for  

id161 
nips 2013 tutorial 
rob fergus  
dept. of computer science 

 

new york university 

overview 

       primarily about object recognition, using 
supervised convnet models 

       focus on natural images 

      rather than digits 
      classi   cation & detection 

       brief discussion of  
other vision problems 

instead of 

motivation 

existing recognition approach 

image/video 

pixels 

hand-designed 

feature 
extraction 

       features are not learned 

trainable 
classi   er 

object 
class 

       trainable classi   er is often generic (e.g. id166) 

motivation 

       features are key to recent progress in recognition 

       multitude of hand-designed features currently in use 

       sift, hog, lbp, mser, color-sift            . 

       where next? better classi   ers? or keep building more features? 

felzenszwalb,  girshick,  

mcallester and ramanan, pami 2007 

yan & huang  

(winner of pascal 2010 classi   cation competition) 

what limits current performance? 
       ablation studies on deformable parts model  
       felzenszwalb, girshick, mcallester, ramanan, pami   10 
       replace each part with humans (amazon turk): 

parikh & zitnick, cvpr   10 

       also removal of part deformations has small 
(<2%) e   ect. are    deformable parts    necessary 

hand-crafted features 

       lp-    multiple kernel learning 

      gehler and nowozin, on feature combination 
for multiclass object classi   cation, iccv   09 

       39 di   erent kernels 
      phog, sift, v1s+, 
region cov.  etc.   

       mkl only gets  
 few % gain over  
 averaging features 
      features are  
doing the work 

what about learning the features? 

       perhaps get better performance? 
       deep models:  hierarchy of feature extractors 
       all the way from pixels       classi   er 
       one layer extracts features from output of previous layer 

image/video 

pixels 

layer 1 

layer 2 

layer 3 

simple  
classi   er 

       train all layers jointly 

deep  

learning 

supervised 

recurrent neural net 

convolutional neural net 

boosting 

neural net 

deep (sparse/denoising) autoencoder 

id88 

id166 
shallow 
autoencoderneural net 

sparse coding 

deep belief net 

restricted bm 

gmm 

deep 

sp 

bayesnp 

unsupervised 

slide: m. ranzato 

multistage  hubel  wiesel  architecture  

slide: y.lecun 

       [hubel & wiesel 1962] 

       simple cells detect local features 
       complex cells    pool    the outputs of 
simple cells within a retinotopic 
neighborhood.  

cognitron / neocognitron 
 [fukushima 1971-1982] 
       also hmax [poggio 2002-2006] 

convolutional networks 
 [lecun 1988-present]  

convolutional neural networks 
       lecun et al. 1989 
       neural network with specialized 
connectivity structure 

recap of convnets 

       feed-forward:  
       convolve input 
       non-linearity (recti   ed linear) 
       pooling (local max) 

       supervised 
       train convolutional    lters by  

back-propagating classi   cation error 

feature maps 

pooling 

non-linearity 

convolution (learned) 

input image 

[lecun et al. 1989] 

convnet successes 

       handwritten text/digits 

       mnist      (0.17% error [ciresan et al. 2011]) 
       arabic & chinese   [ciresan et al. 2012] 

       simpler  recognition benchmarks 

       cifar-10 
       tra   c sign recognition 

 (9.3% error [wan et al. 2013]) 

       0.56% error vs 1.16% for humans [ciresan et al. 2011] 

       but (until recently) less good at  

more complex datasets 
       e.g. caltech-101/256 (few training examples)  

application to id163 

the image cannot be displayed. your computer may not have enough memory to open the image, or the image may have been corrupted. 
restart your computer, and then open the    le again. if the red x still appears, you may have to delete the image and then insert it again.

the image cannot be displayed. your computer may not have enough memory to open the image, or the image may have been corrupted. 
restart your computer, and then open the    le again. if the red x still appears, you may have to delete the image and then insert it again.

the image cannot be displayed. your computer may not have enough memory to open the image, or the image may have been corrupted. 
restart your computer, and then open the    le again. if the red x still appears, you may have to delete the image and then insert it again.

       ~14 million labeled images, 20k classes 
       images gathered from internet 
       human labels via amazon turk  

[deng et al. cvpr 2009]  

[nips 2012] 

krizhevsky et al. [nips2012] 

       same model as lecun   98 but: 
  -   bigger model  (8 layers) 
-    more data    (106 vs 103 images) 
-    gpu implementation (50x speedup over cpu) 
-    better id173 (dropout) 

       7 hidden layers, 650,000 neurons, 60,000,000 parameters 
       trained on 2 gpus for a week 

id163 classification 2012 

       krizhevsky et al. -- 16.4% error (top-5) 
       next best (non-convnet)     26.2% error 

	
   

	
   

%
e
t
a
r
	
   
r
o
r
r
e
5
-     
p
o
t

	
   

35	
   

30	
   

25	
   

20	
   

15	
   

10	
   

5	
   

0	
   

supervision	
   

isi	
   

oxford	
   

inria	
   

amsterdam	
   

commercial deployment 
       google & baidu, spring 2013 for personal 
image search 

large convnets 

 for  

image classification 

large convnets for image 

classification 

       operations in each layer 

       architecture 

       training 

       results 

components of each layer 

  + non-linearity  

pixels / 
features 

filter with  
dictionary 
(convolutional 
or tiled) 

spatial/feature  
(sum or max)  

[optional] 

id172 
feature responses 

between  

output features 

compare: sift descriptor 

image  
pixels 

apply 
gabor    lters 

spatial pool  
(sum)  

normalize to 
unit length 

feature  
vector 

compare: spatial pyramid matching 

sift 
features 

filter with  
visual words 

max 

multi-scale 
spatial pool  
(sum)  

lazebnik,  
schmid,  
ponce  
[cvpr 2006] 

classi   er 

filtering 

       convolutional 

       dependencies are local  
       translation equivariance 
       tied    lter weights (few params) 
       stride 1,2,    (faster, less mem.)  

.
.
.

input 

feature map 

filtering 

       tiled 

       filters repeat every n 
       more    lters than 
convolution for given 
# features 

input 

filters 

feature maps 

non-linearity 

       non-linearity 

      per-feature independent 
      tanh 
      sigmoid: 1/(1+exp(-x)) 
      recti   ed linear 

       simpli   es backprop 
       makes learning faster 
       avoids saturation issues 
 
      preferred option 

pooling 

       spatial pooling 

      non-overlapping / overlapping regions 
      sum or max 
      boureau et al. icml   10 for theoretical analysis 

max 

sum 

pooling  

       pooling across feature groups 

       additional form of inter-feature competition 
       maxout networks [goodfellow et al. icml 2013] 

the image cannot be displayed. your computer may not have enough memory to open the image, or the image may have been corrupted. restart your 
computer, and then open the    le again. if the red x still appears, you may have to delete the image and then insert it again.

pooled
map 1

pooled
map 2

feature
map 1 

feature
map 4

role of pooling  

       spatial pooling 

      invariance to small transformations 
      larger receptive    elds  

(see more of input) 

visualization technique from 
[le et al. nips   10]: 

 
videos from: http://ai.stanford.edu/~quocle/tid98web 

zeiler, fergus [arxiv 2013] 

id172 

       contrast id172 

       see divisive id172 in neuroscience  

input 

filters 

id172 

       contrast id172 (between/across feature maps) 
      local mean = 0, local std. = 1,    local          7x7 gaussian  
      equalizes the features maps 

feature maps 
 

feature maps 

after contrast id172 

role of id172  

       introduces local competition between features 

        poor man   s version of    explaining away    in id114 
         just like top-down models 
         but more local mechanism 

       also helps to scale activations at each layer better for learning 

       makes energy surface more isotropic 
       so each gradient step makes more progress 

       empirically, seems to help a bit (1-2%) on id163 

       more on other datasets (see [jarrett et al. iccv   09] for interesting analysis) 

 

 

architecture 

importance of depth 

architecture of krizhevsky et al.  

       8 layers total 
       trained on id163 
dataset [deng et al. cvpr   09] 
       18.2% top-5 error  

       our reimplementation: 

 18.1% top-5 error 

softmax output 

layer 7: full 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

       remove top fully 
connected layer  
      layer 7 

       drop 16 million 
parameters 

       only 1.1% drop in 
performance! 

softmax output 

layer 6: full 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

       remove both fully connected 
layers  
      layer 6 & 7 

       drop ~50 million parameters 

       5.7% drop in performance 

softmax output 

layer 5: conv + pool 

layer 4: conv 

layer 3: conv 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

       now try removing upper feature 
extractor layers: 
      layers 3 & 4 

       drop ~1 million parameters 

       3.0% drop in performance 

softmax output 

layer 7: full 

layer 6: full 

layer 5: conv + pool 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

architecture of krizhevsky et al.  

       now try removing upper feature 
extractor layers & fully connected: 
      layers 3, 4, 6 ,7 
       now only 4 layers 
       33.5% drop in performance 
 
        depth of network is key 

 

softmax output 

layer 5: conv + pool 

layer 2: conv + pool 

layer 1: conv + pool 

input image 

tapping off features at each layer 
plug features from each layer into linear id166 or soft-max	
   

translation (vertical) 

 
t
u
p
t
u
o

)
s
s
a
c
 

l

e
u
r
t
(

p

 

 
7
 
r
e
y
a
l

e
c
n
a

t
s
d

i

 
l

i

a
c
n
o
n
a
c

lawn mower
(cid:54)(cid:75)(cid:76)(cid:75)(cid:239)(cid:55)(cid:93)(cid:88)
african crocodile
african grey
entertrainment center

(cid:239)(cid:21)(cid:19)
vertical translation (pixels)

(cid:21)(cid:19)

(cid:19)

(cid:23)(cid:19)

(cid:25)(cid:19)

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
   60

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

   40

0
 
   60

   40

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

   20
vertical translation (pixels)

20

0

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

   20
vertical translation (pixels)

20

0

 

40

60

 

40

60

 
1
 
r
e
y
a
l

e
c
n
a
t
s
d

i

i

 
l
a
c
n
o
n
a
c

(cid:20)(cid:19)
9
8
7
(cid:25)
5
(cid:23)
3
(cid:21)
1
(cid:19)
 
(cid:239)(cid:25)(cid:19)

(cid:239)(cid:23)(cid:19)

scale invariance 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
1

0.7

0.6

0.5

0.4

0.3

0.2

0.1

 
t
u
p
t
u
o

l

)
s
s
a
c
 
e
u
r
t
(

p

 

 
7
 
r
e
y
a
l

e
c
n
a

t
s
d

i

 
l

i

a
c
n
o
n
a
c

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

1.2

1.4
scale (ratio)

1.6

1.8

 

 

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

0
 
1

1.2

1.4
scale (ratio)

1.6

1.8

 
1
 
r
e
y
a
l

e
c
n
a

t
s
d

i

 
l

i

a
c
n
o
n
a
c

12

10

8

6

4

2

0
 
1

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

1.2

1.4
scale (ratio)

1.6

1.8

rotation invariance 

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

50

100

150

200

rotation degrees

 

250

300

350

 

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
 
0

1.4

1.2

1

0.8

0.6

0.4

0.2

 
t
u
p
t
u
o

)
s
s
a
c
 

l

e
u
r
t
(

p

 

 
7
 
r
e
y
a
l

e
c
n
a

t
s
d

i

 
l

i

a
c
n
o
n
a
c

150

200

rotation degrees

250

300

350

0
 
0

50

100

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

150

200

rotation degrees

250

300

350

 
1
 
r
e
y
a
l

e
c
n
a

t
s
d

i

 
l

i

a
c
n
o
n
a
c

15

10

5

0
 
0

50

100

lawn mower
shih   tzu
african crocodile
african grey
entertrainment center

visualizing  
convnets 

visualizing convnets 

       raw coe   cients of learned    lters in higher 
layers di   cult to interpret 

       several approaches look to optimize input 
to maximize activity in a high-level feature 
       erhan et al.  [tech report 2009] 
       le et al. [nips 2010] 
       depend on initialization 
       model invariance with hessian about 
(locally) optimal stimulus 

visualization using deconvolutional networks 

[zeiler et al. cvpr   10, iccv   11, arxiv   13]	
   

       provides way to map activations at 

high layers back to the input 

       same operations as convnet, but in 

reverse: 
       unpool feature maps 
       convolve unpooled maps 
       filters copied from convnet 
       used here purely as a probe 

learning method 

       originally proposed as unsupervised 
       no id136, no learning 

feature maps 

unpooling 

non-linearity 

convolution (learned) 

input image 

deconvnet projection from higher layers 

[zeiler and fergus. arxiv   13]	
   

0	
   

....	
   

0	
   

feature	
   

map	
   

filters	
   

....	
   

filters	
   

	
   
t
e
n
v
n
o
c
e
d

layer 2 reconstruction 

layer 2: feature maps 

layer 1 reconstruction 

layer 1: feature maps 

c
o
n
v
n
e
t
	
   

visualization 

input image 

details of operation 

deconvnet layer 

convnet layer 

unpooling operation 

layer 1 filters 

visualizations of higher layers 

       use id163 2012 validation set 
       push each image through network 

 

[zeiler and fergus. arxiv   13]	
   

feature	
   

map	
   

....	
   

filters	
   

lower	
   layers	
   

input	
   	
   
image	
   

validation images 

       take max activation from 

feature map associated 
with each    lter 

       use deconvnet to project 

back to pixel space 

       use pooling    switches    

peculiar to that activation 

layer 1: top-9 patches 

layer 2: top-1 

layer 2: top-9 

       not samples from model 
      
       non-parametric view on invariances learned by model 

 just parts of input image that give strong activation of this feature map 

layer 2: top-9 patches 

       patches from validation images that give maximal activation of a given feature map  

layer 3: top-1 

layer 3: top-9 

layer 3: top-9 patches 

layer 4: top-1 

layer 4: top-9 

layer 4: top-9 patches 

layer 5: top-1 

layer 5: top-9 

layer 5: top-9 patches 

diagnosing problems 

       visualization of krizhevsky et al.   s architecture 
showed some problems with layers 1 and 2 
      large stride of 4 used 
  

       alter architecture: smaller stride &    lter size 

      visualizations look better 
      performance improves 

comparison with krizhevsky et al. 
       layer 2 visualizations 

krizhevsky et al. 

ours 

comparison with krizhevsky et al. 
       layer 1    lters 

krizhevsky et al. 

ours 

11x11    lters, stride 4 

7x7    lters, stride 2 

id163 classification 2012 

[zeiler and fergus. arxiv   13]	
   

*   trained using imagnet 2011 and 2012 training sets. 

id163 classification 2013 results 

       http://www.image-net.org/challenges/lsvrc/2013/results.php 

	
   
)
5
-     
p
o
t
(
	
   
r
o
r
r
e
	
   
t
s
e
t

0.17	
   

0.16	
   

0.15	
   

0.14	
   

0.13	
   

0.12	
   

0.11	
   

0.1	
   

       pre-2012: 26.2% error        2012: 16.5% error       2013: 11.2% error 

how to choose architecture 

       task-dependent but many hyper-parameters: 

       # layers, # feature maps, strides in conv/pool 
       limited by amount of labeled data & gpus 

       cross-validation  
       grid search (need lots of gpus) 
       smarter strategies: 

       random [bergstra & bengio jmlr 2012] 
       bayesian optimization [snoek et al. nips 2012] 
       using visualizations [zeiler & fergus, arxiv 1311.2901] 

training big convnets 

       back-propagation of error 

       [rumelhart, hinton & williams 1986] + many others 
       chain rule   

       stochastic id119  
       2nd order methods expensive 
       l. bottou    stochastic gradient tricks    neural networks 2012 

       momentum 

       nesterov variant [sutskever et al. icml 2012] 

       classi   cation loss: cross-id178 
       gpu implementation 
 

pre-processing 

input representation

    centered (0-mean) rgb values.

       mean removal 

 

       whitening (zca) 

 

an input image (256x256)

      form of pca 
      removes correlations 
      but too expensive for entire image 
 

       contrast  
id172  

 

minus sign

the mean input image

[krizhevsky et al. nips   12] 

annealing of learning rate  

       start large & slowly reduce, to 100-1000x smaller by end  
       explore di   erent scales of energy surface 

evolution of features during training 

evolution of features during training 

improving generalization 

       data augmentation (crops/   ips etc. of images) 

       weight decay (l1 or l2 penalty on weights) 
 
       inject noise into network 

      dropout [hinton et al. 2012] 
      dropconnect [wan et al. icml 2012] 
      stochastic pooling [zeiler & fergus iclr   13] 

big model + regularize vs small model 

small model 

big model 

big model 
+ regularize 

dropout 

       g. e. hinton, n. srivastava, a. krizhevsky, i. sutskever and r. r. 

that had only 4 training examples. we also removed one category that covered a huge chunk
(25%) of the examples. this left us with 50 classes and 402,738 documents. we divided the
documents into equal-sized training and test sets randomly. each document was represented
using the 2000 most frequent non-stopwords in the dataset.

salakhutdinov, improving neural networks by preventing co-adaptation of 
feature detectors, arxiv:1207.0580 2012 
       fully connected layers only 
       randomly set activations in 
       gives ensemble of models 
       similar to id112 

layer to zero 

[breiman   94], but di   ers in 
that parameters are shared. 

(a)

(b)

fig. 7: classi   cation error rate on the (a) training and (b) validation sets of the reuters dataset
as learning progresses. the training error is computed using the stochastic nets.

thesis proposal: id164 with deep learning

dropconnect 

image classi   cation with deep network

empirical study of dropconnect network

       wan et al. icml 2013 
       fully-connected layers only 
understand dropconnect network with mnist data set
       random binary mask on weights 

 input 

 input 

 x 

 x 
 features  
v (n x 1) 

 features  
v (n x 1) 

feature  
extractor 
g(x;wg) 

feature  
extractor 
g(x;wg) 

800
hidden units

previous layer mask 

previous layer mask 

 

2.4

2.2

2

1.8

1.6

1.4

r
o
r
r

e

 
t
s
e
t
%

 

 
k
s
a
m

 
t
u
p
t
u
o
 
r
e
y
a
l
 
t
n
e
r
r
u
c

 
k
s
a
m

 
t
u
p
t
u
o

 
r
e
y
a
l
 
t
n
e
r
r
u
c

 

10   2

y
p
o
r
t

n
e
 
s
s
o
r
c

dropout
dropconnect

1.2
 
0

0.1

1600
b) dropconnect 
mask m 

b) dropconnect 
mask m 

0.2

0.6

0.5

0.4

0.7
0.3
c) effective dropout 

% of elements kept
c) effective dropout 
mask m    

mask m    

0.8

0.9

10   3

 

100

mnist 

 

no   drop train
no   drop test
dropout train
dropout test
dropconnect train
dropconnect test
400

300

200

500
epoch

600

700

800

900

1. testing error by varying the size of n-n network

stochastic pooling 

[zeiler and fergus, iclr 2013] 

       for conv layers 
       compute activations     :          
       normalize to sum to 1      ->  
       sample location,  , from multinomial 
       use activation from the location:  

   !

b)!filter!

1.6!

0!

0!

0!

0!

0!

0!

0!

2.4!

0.4!

0!

0!

0!

0!

0!

0!

0!

0.6!

a)!image!

c)!rec0   ed!linear!

d)!ac0va0ons,!ai 

e)!probabili0es,!pi 

sample!a!loca0on!
from!p():!e.g.!!l = 1 

1.6!

f)!sampled!!
!!!!ac0va0on,!s!

stochastic pooling: cifar-10 

r
o
r
r

 

e
%

35

30

25

20

15

10

5

0

 

 

avg (train)
avg (test)
max (train)
max (test)
stochastic (train)
stochastic (test)

50

100

150
epochs

200

250

other good things to know 

      
      

 check gradients numerically by    nite di   erences 
 plot feature maps: should be uncorrelated &  high variance 

samples 

good training: hidden units are sparse  

 

 

 

     across samples and across features.  

hidden unit 

slide credit: 
m. ranzato 

other good things to know 

      
      

 check gradients numerically by    nite di   erences 
 plot feature maps: should be uncorrelated &  high variance 

samples 

bad training: many hidden units ignore the input 

 

  and/or exhibit strong correlations. 

hidden unit 

slide credit: 
m. ranzato 

it   s not working     what do i do? 

        training diverges: 

       learning rate may be too large  
       backprop is buggy 

        decrease learning rate 
        numerical gradient checking 

        parameters collapse / loss is minimized but accuracy is low 

        check id168: 

       is it appropriate for the task you want to solve? 
       does it have degenerate solutions? 

 

slide credit: 
m. ranzato 
ranzato 

it   s not working     what do i do? 

      

 network is underperforming 
       compute    ops and # parameters  
                  if too small, make net larger 
       visualize hidden units/parameters  
                    x optimization   

        network is too slow 

       compute    ops and # parameters 
                 gpu, distributed framework, make net smaller  

slide credit: 
m. ranzato 
ranzato 

sample classification results 

validation classification

[krizhevsky et al. nips   12] 

 

 

sample classification results 

validation classification

[krizhevsky et al. nips   12] 

 

 

id164 

detection with convnets 

       so far, all about 
classi   cation 

       what about 
localizing objects 
within the scene? 

occlusion experiment 

       mask parts of input with occluding square 

       monitor output 
of classi   cation 
network 

       perhaps network using scene context? 

 

 

input image 

p(true class)  

most probable class 

input image 

p(true class)  

most probable class 

input image 

p(true class)  

most probable class 

sliding window with convnet 

conv 

conv 

conv 

conv 

conv 

full 

full 

sliding window with convnet 

conv 

conv 

conv 

conv 

conv 

full 

full 

224 

input window 

224 

feature extractor 

6 

6 

classifier 

256 

c 
classes 

sliding window with convnet 

conv 

conv 

conv 

conv 

conv 

full 

full 

240 

16 

224 

feature extractor 

7 

6 

1 

256 

c 
classes 

input window 

no need to compute two separate windows 
just one big input window, computed in a single pass 

convnets for detection 

feature 
maps 

class  
maps 

256 

c=1000 

feature  
extractor 

256 

classifier 

c=1000 

256 

256 

c=1000 

c=1000 

convnets for detection 

feature 
maps 

class  
maps 

256 

boat 

feature  
extractor 

256 

classifier 

boat 

256 

256 

boat 

boat 

convnets for detection 

feature 
maps 

class  
maps 

256 

boat 

feature  
extractor 

256 

classifier 

boat 

256 

256 

boat 

boat 

convnets for detection 

feature 
maps 

256 

256 

regression 
network 

output:  
[x,y,w,h] 

feature  
extractor 

256 

predicted 
bounding 

box 

256 

256 

ground 
truth 

bounding box prediction example 

[sermanet et al. cvpr   14, under review] 

detection results 

[sermanet et al. cvpr   14, under review] 

detection results   

[sermanet et al. cvpr   14, under review] 

pedestrian detection 

sermanet et al.    pedestrian detection with unsupervised multi-stage..    cvpr 2013 
       model helped by 

unsupervised pre-training 

 

27.18% latid166   v1

23.39% convnet   f

17.29% convnet   f   ms
17.10% convnet   u

14.79% multiftr+css

10.55% convnet   u   ms

2
10

e

t

a
r
 
s
s
m

i

1

.80

.64

.50

.40

.30

.20

.10

.05

 

 

23.39% convnet   f
17.29% convnet   f   ms
17.10% convnet   u
10.55% convnet   u   ms

   2

10

   1

10

0
10

1
10

2
10

false positives per image

figure 4: det curves on the    xed-inria dataset for large pedestrians measure report false positives per image (fppi) against miss
rate. algorithms are sorted from top to bottom using the proposed continuous area under curve measure between 0 and 1 fppi. on the
right, only the convnet variants are displayed to highlight the individual contributions of unsupervised learning (convnet-u) and

feature  

generalization 

using features on other datasets 
       train model on id163 2012 training set 

       re-train classi   er on new dataset 

      just the softmax layer 

       classify test set of new dataset 

donahue et al., decaf: a deep convolutional activation feature for generic visual 
recognition, arxiv 1310.1531, 2013 
decaf: a deep convolutional activation feature for generic visual recognition

caltech-101 

1

0.8

0.6

0.4

0.2

y
r
o
g
e

t

a
c

 
r
e
p
 
y
c
a
r
u
c
c
a
n
a
e
m

 

0

 
0

5

 

logreg decaf6 w/ dropout
id166 decaf6 w/ dropout
yang et al. (2009)

15

25
10
num train per category

20

30

35

figure 4. left: average accuracy per class on caltech-101 with 30 training samples per class across three hidden layers of the network
and two classi   ers. our result from the training protocol/classi   er combination with the best validation accuracy     id166 with layer 6
(+ dropout) features     is shown in bold. right: average accuracy per class on caltech-101 at varying training set sizes.

caltech 256 

zeiler & fergus, visualizing and understanding convolutional networks, arxiv 1311.2901, 2013 

(cid:8)
(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)

75
70
65
60
55
50
45
40
35
30
25
 
0

10

 

60

(cid:37)(cid:82)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
(cid:54)(cid:82)(cid:75)(cid:81)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
50

20
(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3)(cid:83)(cid:72)(cid:85)(cid:239)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)

30

40

caltech 256 

zeiler & fergus, visualizing and understanding convolutional networks, arxiv 1311.2901, 2013 

(cid:8)
(cid:3)
(cid:92)
(cid:70)
(cid:68)
(cid:85)
(cid:88)
(cid:70)
(cid:70)
(cid:36)

75
70
65
60
55
50
45
40
35
30
25
 
0

10

6 training examples 

(cid:50)(cid:88)(cid:85)(cid:3)(cid:48)(cid:82)(cid:71)(cid:72)(cid:79)
(cid:37)(cid:82)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
(cid:54)(cid:82)(cid:75)(cid:81)(cid:3)(cid:72)(cid:87)(cid:68)(cid:79)
50

20
(cid:55)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:86)(cid:3)(cid:83)(cid:72)(cid:85)(cid:239)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)

30

40

 

60

caltech 256 

zeiler & fergus, visualizing and understanding convolutional networks, arxiv 1311.2901, 2013 

[3] l. bo, x. ren, and d. fox. multipath sparse coding using hierarchical matching pursuit. 
in cvpr, 2013. 
[16] k. sohn, d. jung, h. lee, and a. hero iii. e   cient learning of sparse, distributed, 
convolutional feature representations for object recognition. in iccv, 2011. 

jeff donahue1,2 trevor darrell1,2

1uc berkeley and 2icsi

jitendra malik1

pascal voc detection 

{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu

girshick et al., rich feature hierarchies for accurate id164 and semantic 
segmentation, arxiv 1311.2524, 2013 

can a large convolutional neural network trained for
whole-image classi   cation on id163 be coaxed into de-
tecting objects in pascal? we show that the answer is
yes, and that the resulting system is simple, scalable, and
boosts mean average precision, relative to the venerable
deformable part model, by more than 40% (achieving a    -
nal map of 48% on voc 2007). our framework combines
powerful id161 techniques for generating bottom-
up region proposals with recent advances in learning high-
capacity convolutional neural networks. we call the result-
ing system r-id98: regions with id98 features. the same

r-id98: regions with id98 features

warped region

id98

aeroplane? no.

person? yes.

...
...

tvmonitor? no.
4. classify 

1. input 
image

2. extract region 
proposals (~2k)

3. compute 
id98 features
figure 1: id164 system overview. our system (1)
       43.1% mean ap vs previous 35.1% 
takes an input image, (2) extracts around 2000 bottom-up region
proposals, (3) computes features for each proposal using a large
convolutional neural network (id98), and then (4) classi   es each
region using class-speci   c linear id166s. this system achieves a
mean average precision (map) of 43.5% on pascal voc 2010.

regions

deep nets vs monkey vs human 
c.f. cadieu, h. hong, d. yamins, n. pinto, e.a. solomon, n.j. 
majaj, and j.j. dicarlo. deep neural networks rival the object 
recognition performance of the primate visual system. (plos one 
biology, in submission, 2013). 

a)

b)

cars

...

fruits

...

animals

...

...

planes
chairs
tables
faces

retinae representation

it cortex representation

evaluation

ventral stream

 

 

it

pixel representation

dnn representation

deep neural 
network (dnn)

  (x)

cars

fruits

m
v
s
-
r
a
e
n
i
l

s
i
s
y
l
a
n
a

 
l
e
n
r
e
k

it cortex dnn

y
c
a
r
u
c
c
a

n
o
i
s
i
c
e
r
p

complexity

it cortex

dnn

deep nets vs monkey vs humans 

[cadieu et al.] 

       rapid presentation experiments (100ms) 
       feed-forward processing only in monkey/humans 

	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   
	
   

other vision  
applications 

scene parsing 

       farabet et al.    learning hierarchical features for scene labeling    pami 2013 

action recognition from video 

taylor et al.    convolutional learning of spatio-temporal features    eccv 2010 

segmentation 

       ciresan et al.    dnn segment neuronal membranes...    nips 2012 
       turaga et al.    maximin learning of image segmentation    nips 2009 

biological detection 

       d. ciresan, a. giusti, l.m. gambardella, j. schmidhuber - mitosis 

detection in breast cancer histology images using deep neural networks 
(miccai 2013) 

3

denoising 

       burger et al.    can plain nns compete with bm3d?    cvpr 2012 

original 

noised 

denoised 

removing artifacts 

[eigen et al.    restoring an image taken through a window covered  
with dirt or rain     iccv 2013] 

removing artifacts 

[eigen et al.    restoring an image taken through a window covered  
with dirt or rain     iccv 2013] 

future 

directions 

video 
       relatively under-explored 
       learn about 3d structure from motion 
       need big labeled video dataset 

deep learning + id170 

       convnet feature extractor 
       combine with top-down reasoning 

stochastic grammars 

[fischler and r. elschlager 1973 ] 

[r. girshick, p. felzenszwalb, d. mcallester,  object 
detection with grammar models, nips 2011] 

summary 

       yann lecun was right! 

       deep convnets work well for recognition 

      quite a bit better than existing vision approaches 

 
       can be used for many other vision tasks 

       but unsupervised learning still an open problem 

 

demo   

url:

 http://horatio.cs.nyu.edu 

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
   

   
   

   
   
   

   
   
   

   
   
   

   
   
   

   
   
   

matthew zeiler: matthew is currently a 4th year phd student in the
dept. of computer science, courant institute, new york university.
   
he has a bsc in engineering science from the university of toronto,
   
   
completing his final year thesis with prof. geoff hinton and graduating
4th overall
   
in his class. he has won numerous awards and
   
scholarships including class of 5t3 engineering award in 2008 and
nserc msc and phd fellowships in 2009-  2012. he was recently a
   
summer intern with the google brain group where he rewrote the api
for the team's machine learning framework and developed large scale
deep learning algorithms to speed up training of deep networks for
   
id103 in android phones. he is the founder of a successful education company
where he designed and built the content management system, website and all of its products.
he   also   has   experience   developing   multiple   applications   for   mobile   platforms.
   
   

   
       
   

   
   
   
   

   
   
   

   
   
   

   
   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   

   
   

   
   

   
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

david eigen: david is currently a 3rd year phd student in the dept.
of computer science, courant institute, new york university. he has
scm and scb degrees from brown university, in computer science
and mathematics-  computer science. prior to joining nyu, he worked
for 5 years as a software engineer: first at netapp, where he worked
   

   
   
   
   

   
   
   
   

   
   
   

   
   

   
   

   
   

   

   
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
   

   
   

   
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   
   

   
   
   

   
   
   

   
   

   
   

   
   
   

   
   
   
   
   

   
   
   
   
   

   
   
   
   
   

acknowledgements 
       colleagues in cilvr lab @ nyu: 

   
focused on applying deep learning methods to object recognition, working with matt zeiler and
david eigen. he has received various prizes, including the best cs phd in the uk (2006), a
   
   
sloan fellowship (2011) and an nsf career award (2012). he has been an area chair nips,
cvpr, iccv, eccv and siggraph and is a program chair for iclr 2013. he is also the holder
   
of   two   patents   and   has   consulted   for   microsoft   research.
prof. yann lecun: yann lecun is silver professor of computer
   
   
pierre sermanet: pierre is currently a 5th year phd student in the
   
science at the courant institute of mathematical sciences of new
   
   
   
   
dept. of computer science, courant institute, new york university.
   
   
he holds msc   s in computer science from epita (paris) and new
   
   
york university. he also holds appointments at the center for neural
   
   
   
to starting his phd, he was a research
york university. prior
   
   
   
science and the department of electrical and computer engineering.
   
   
   
   
   
   
engineer at net-  scale technologies. during his 7-  year experience in
   
   
   
   
   
   
he received the electrical engineer diploma from ecole sup  rieure
   
   
   
   
   
machine learning, robotics and id161, he has designed
   
   
and trained several
large-  scale neural networks which hold
   
   
   
   
d'ing  nieurs en electrotechnique et electronique (esiee), paris in
   
international
records against a large number of
state-  of-  the-  art
   
   
   
   
   
1983, and a phd in computer science from universit   pierre et marie
   
   
   
research teams for different vision tasks. he is the main author of the
   
eblearn library, an open-  source c++ machine learning library used
   
curie (paris) in 1987. after a postdoc at the university of toronto, he
by numerous companies (e.g. sarnoff, siemens, mitre). he is also
   
   
joined at&t bell laboratories in holmdel, nj in 1988, and later
   
   
   
   
the author of an android version of the library which is now used by
   
museami for their music ocr application. has was a key member of
   
became head of
the image processing research department at
   
the nyu team in the darpa lagr robotics competition. he has
   
   
at&t labs-  research in 1996. he joined nyu as a professor in 2003,
interned   at   willow   garage   and   google   s   brain   group.
after a brief period as fellow at the nec research institute in princeton. his current interests
   
   
include machine learning, computer perception and vision, mobile robotics, and computational
   
matthew zeiler: matthew is currently a 4th year phd student in the
neuroscience. he has published over 170 technical papers and book chapters on these topics
   
dept. of computer science, courant institute, new york university.
   
as well as on neural networks, handwriting recognition, image processing and compression, and
   
   
he has a bsc in engineering science from the university of toronto,
the back-  propagation learning algorithm for neural
vlsi design. yann is a co-  inventor of
   
completing his final year thesis with prof. geoff hinton and graduating
   
   
   
in his class. he has won numerous awards and
4th overall
   
   
   
   
networks, and the inventor of the convolutional network model. he has been a major force in the
   
scholarships including class of 5t3 engineering award in 2008 and
   
renewal of interest in    deep learning    methods since the mid 2000   s. his handwriting recognition
   
   
nserc msc and phd fellowships in 2009-  2012. he was recently a
   
   
summer intern with the google brain group where he rewrote the api
technology based on convolutional nets is used by several banks around the world to read
for the team's machine learning framework and developed large scale
checks (at some point reading over 10% of all the checks in the us). his image compression
   
   
deep learning algorithms to speed up training of deep networks for
   
id103 in android phones. he is the founder of a successful education company
   
technology, called djvu, is used by hundreds of web sites and publishers and millions of users
   
   
where he designed and built the content management system, website and all of its products.
to access scanned documents on the web. and his learning-  based image recognition methods
   
   
he   also   has   experience   developing   multiple   applications   for   mobile   platforms.
are used in systems deployed by companies such as at&t, google, microsoft, nec, and others
   
david eigen: david is currently a 3rd year phd student in the dept.
for document recognition, human-  computer interaction, image indexing, and video analytics. he

matt zeiler 
       slides:   marc   aurelio ranzato 
       funding:     nsf, darpa, onr 
                   microsoft, google, facebook 

pierre sermanet  yann lecun 

david eigen 

   
   
   
   
   
   
   
   
   

   
   
   
   
   

   
   
   
   
   
   
   

   
   
   
   
   
   
   

   
   
   
   

   
   
   
   

   
   
   

   
   
   

   
   
   
   

   
   
   
   

   
   
       

   
   
   

   
   
   
   

   
   
   
   
   

   
   
   
   

   
   
   
   

       

       

   
   

   
   

   
   

   
   

   
   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   
   

   
   

   
   

   
   

   
   

   
   
   

   
   
   

   
   

   
   

   
   

   
   

   
   

   
   
   

   
   

   
   
   

   
   

   
   

   
   

   
   

   
   

   
   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   
   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

iclr 2014 conference   

       deadline: 20th december 2013 
       ban   , canada, april 14-16th 2014 
       welcome anything to do with representation learning! 

 

further resources 

       http://deeplearning.net/ 
       http://www.cs.toronto.edu/~hinton/csc2515/
deeprefs.html 

       nips 2013 workshop on deep learning and 
unsupervised id171 
       https://sites.google.com/site/deeplearningworkshopnips2013/ 
       torch7 (https://github.com/torch) 

references 

      
       p. felzenszwalb, r. girshick, d. mcallester, d. ramanan, id164 with 

[slide 5] 
discriminatively trained part based models,ieee transactions on pattern analysis and 
machine intelligence, vol. 32, no. 9, september 2010 
       zheng song*, qiang chen*, zhongyang huang, yang hua, and shuicheng yan. con  tex  tu  
al  iz  ing ob  ject de  tec  tion and clas  si       ca  tion. in cvpr'11. (* in  di  cates equal con  
tri  bu  tion) [no. 1 per  for  mance in voc'10 clas  si       ca  tion task] 
[slide 6] 
finding the weakest link in person detectors, d. parikh, and c. l. zitnick, cvpr, 2011. 
[slide 7] 

      
      
      
       gehler and nowozin, on feature combination for multiclass object classi   cation, iccv   09 

references 

  
      
       riesenhuber, m. & poggio, t. (1999). id187 of object recognition in cortex. 

[slide 11]  
nature neuroscience 2: 1019-1025. 
http://www.scholarpedia.org/article/neocognitron 
pattern recognition una   ected by shift in position", biological cybernetics, 36[4], pp. 193-202 
(april 1980). 
       y. lecun, l. bottou, y. bengio and p. ha   ner: gradient-based learning applied to 
document recognition, proceedings of the ieee, 86(11):2278-2324, november 1998 

      
       k. fukushima: "neocognitron: a self-organizing neural network model for a mechanism of 

      
      

[slide 12] 
y. lecun, b. boser, j. s. denker, d. henderson, r. e. howard, w. hubbard and l. d. jackel: handwritten 
digit recognition with a back-propagation network, in touretzky, david (eds), advances in neural 
information processing systems (nips 1989), 2, morgan kaufman, denver, co, 1990 

references 

[slide 14] 
      
       d. ciresan, u. meier, j. masci, j. schmidhuber - multi column deep 
neural network for tra   c sign classi   cation, neural networks 2012, 
http://dx.doi.org/10.1016/j.neunet.2012.02.023 
       d. ciresan, u. meier, j. schmidhuber - id21 for latin and 

chinese characters with deep neural networks ijid98 2012 

       li wan, matthew zeiler, sixin zhang, yann lecun, rob fergus, 

id173 of neural networks using dropconnect, icml 2013. 
[slide 15] 
j. deng, w. dong, r. socher, l.-j. li, k. li and l. fei-fei, id163: a 
large-scale hierarchical image database. ieee id161 and 
pattern recognition (cvpr), 2009. 
       krizhevsky, a., sutskever, i. and hinton, g. e., id163 classi   cation 
with deep convolutional neural networks, nips 2012 

      
      

references 

      
      

      
       y-lan boureau, jean ponce, and yann lecun, a theoretical analysis of feature 

[slide 27] 
pooling in vision algorithms, proc. international conference on machine learning 
(icml'10), 2010  
[slide 28] 
ian j. goodfellow, david warde-farley, mehdi mirza, aaron courville, and yoshua 
bengio, maxout networks, icml 2013  
[slide 29] 

      
       q.v. le, j. ngiam, z. chen, d. chia, p. koh, a.y. ng , tiled convolutional neural 
       http://ai.stanford.edu/~quocle/tid98web 
       matthew d. zeiler, graham w. taylor, and rob fergus, adaptive deconvolutional 
networks for mid and high level id171, international conference on 
id161 2011. 
       zeiler and fergus, visualizing and understanding convolutional networks, arxiv 
1311.2901 nov 2013. 

networks. nips, 2010 

references 

       [slide 32] 
       jarrett, k., kavukcuoglu, k., ranzato, m., and le- cun, y. 

what is the best multi-stage architecture for object 
recognition? in iccv, 2009. 

       [slide 45] 
       q.v. le, j. ngiam, z. chen, d. chia, p. koh, a.y. ng , tiled 
       visualizing higher-layer features of a deep network, 

convolutional neural networks. nips, 2010 
dumitru erhan, yoshua bengio, aaron courville, and pascal 
vincent,technical report 1341, june 2009 

references 

[slide 46] 
      
       matthew d. zeiler, graham w. taylor, and rob fergus, adaptive deconvolutional 
networks for mid and high level id171, international conference on 
id161 2011. 
       zeiler and fergus, visualizing and understanding convolutional networks, arxiv 
1311.2901 nov 2013. 
[slide 65] 

      
       krizhevsky, a., sutskever, i. and hinton, g. e., id163 classi   cation with deep 

convolutional neural networks, nips 2012 
[slide 68] 

      
       gunji, n., higuchi, t., yasumoto, k., muraoka, h., ushiku, y., harada, t., and 

kuniyoshi, y. classi   cation entry. in id163 competition, 2012. 

       krizhevsky, a., sutskever, i. and hinton, g. e., id163 classi   cation with deep 
       zeiler and fergus, visualizing and understanding convolutional networks, arxiv 

convolutional neural networks, nips 2012 
1311.2901 nov 2013. 

references 

[slide 69] 

       practical bayesian optimization of machine learning algorithms, jasper snoek, 

      
       http://www.image-net.org/challenges/lsvrc/2013/results.php 
      
      

[slide 70] 
j. bergstra and y. bengio (2012), random search for hyper-parameter 
optimization, journal of machine learning research 13:281   305.  
hugo larochelle and ryan prescott adams, neural information processing 
systems, 2012 
[slide 71] 
sutskever, james martens, george dahl, and geo   ery hinton, icml 2013 

      
       on the importance of momentum and initialization in deep learning, ilya 
       training deep and recurrent neural networks with hessian-free optimization, 

james martens and ilya sutskever, neural networks: tricks of the trade, 2012  
       l  on bottou: stochastic gradient tricks, neural networks, tricks of the trade, 
reloaded, 430   445, edited by gr  goire montavon, genevieve b. orr and klaus-
robert m  ller, lecture notes in computer science (lncs 7700), springer, 2012. 

references 

      
       g. e. hinton, n. srivastava, a. krizhevsky, i. sutskever and r. r. 

[slide 75] 
salakhutdinov, improving neural networks by preventing co-adaptation of 
feature detectors, arxiv:1207.0580 2012 

       id173 of neural networks using dropconnect, li wan, matthew 

zeiler, sixin zhang, yann lecun, rob fergus, icml 2013 

       zeiler and fergus, stochastic pooling, iclr 2013 
      
      

[slide 90] 
integrated recognition, detection and localization using convolutional 
networks, p. sermanet, d. eigen, m. mattheiu, x. zhang, r. fergus and y. 
lecun. under review, cvpr 2014. 
[slide 95] 
sermanet, k. kavukcuoglu, s. chintala and y. lecun,cvpr 2013. 

      
       pedestrian detection with unsupervised multi-stage id171 , p. 

references 

       [slide 98] 
       donahue et al., decaf: a deep convolutional activation 
feature for generic visual recognition, arxiv 1310.1531, 
2013 

       [slide 99] 
       zeiler & fergus, visualizing and understanding 
convolutional networks, arxiv 1311.2901, 2013 
       [slide 102] 
       girshick et al., rich feature hierarchies for accurate object 

detection and semantic segmentation, arxiv 1311.2524, 2013 

other useful references 

       yoshua bengio and yann lecun: scaling learning algorithms towards ai, in bottou, l. and 

chapelle, o. and decoste, d. and weston, j. (eds), large-scale kernel machines, mit 
press, 2007 

