      

 
  
 
 

 

stochastic gradient methods for large-scale 

machine learning

leon bottou 

facebook ai research 

 

 frank e. curtis 
lehigh university  

 

  jorge nocedal   

  northwestern university 

 
  
 
 

1 

      

our goal 

 
1.    this is a tutorial about the stochastic gradient (sg) method 

2.    why has it risen to such prominence? 

3.    what is the main mechanism that drives it? 
4.    what can we say about its behavior in convex and non-

convex cases? 

5.    what ideas have been proposed to improve upon sg? 

 

 

2 

      

organization 

 
i.    motivation for the stochastic gradient (sg) method:   
      jorge nocedal 

ii.    analysis of sg:  leon bottou 

iii. beyond sg: noise reduction and 2nd -order methods: 
      frank e. curtis 
 
 
 

 

 

3 

      

reference 

this tutorial is a summary of the paper 
 
 

    optimization methods for large-scale machine learning       

  

 l. bottou, f.e. curtis, j. nocedal 

 
 

http://arxiv.org/abs/1606.04838 

prepared for siam review 

4 

      

problem statement 
given training set {(x1,y1),   (xn,yn)} 
 
given a id168    (h,y)
 
find a prediction function h(x;w)
 
 
 
 
    (h(xi;w),yi)
                   
 
notation: random variable   = (xi,yi)

minw

1
n

i=1

   

n

(hinge loss, logistic,...)
(linear, dnn,...)

1
                    rn(w) =
n
the real objective

n

   
i=1

fi(w)

           

empirical risk

                    r(w) = e[ f (w;  )]          
 

  expected risk

5 

      

stochastic gradient method 

first present algorithms for empirical risk minimization
                   rn(w) =

           

fi(w)

n

1
n

   
i=1

wk+1 = wk      k   fi(wk)          i    {1,...,n} choose at random

       very cheap iteration; gradient w.r.t. just 1 data point 
       stochastic process dependent on the choice of  i 
       not a id119 method    
       robbins-monro 1951 
       descent in expectation 

6 

      

 batch optimization methods 

wk+1 = wk      k   rn(wk) 

batch gradient method 

wk+1= wk       k
n

n

       
    fi(wk)
   
i=1

       more expensive step 
       can choose among a wide range of optimization algorithms 
       opportunities for parallelism 

why has sg emerged at the preeminent method? 

understanding: study computational trade-offs between 
stochastic and batch methods, and their ability to minimize  r 

7 

      

intuition 
 
sg employs information more efficiently than batch method 
 
argument 1:  

suppose data is 10 copies of a set s 
iteration of batch method 10 times more expensive 
sg performs same computations 

 
argument 2: 

 training set (40%), test set (30%), validation set (30%). 
 why not 20%, 10%, 1%..? 

 

 

8 

      

practical experience 

rn

k
s
i
r

 
l

a
c

i
r
i

p
m
e

0.6

0.5

0.4

0.3

0.2

0.1

0

0

lbfgs

sgd

0.5

1

1.5

2

2.5

3

3.5

accessed data points

4
5
x 10

10 epochs 

fast initial progress 
of sg followed by 
drastic slowdown 
 
can we explain this? 

9 

      

example by bertsekas 

                   rn(w) =

1
n

n

   
i=1

fi(w)

           

w1    1 w1,*

region of confusion 

1

note that this is a geographical argument  
analysis: given wk  what is the expected decrease in the 
objective function rn as we choose one of the quadratics
randomly?

10 

      

a fundamental observation 

e[rn(wk+1)     rn(wk)]          k      rn(wk)   2

2 +   k

2 e      f (wk,  k)    2  

                                                                                                      

initially, gradient decrease dominates; then variance in gradient hinders 
progress (area of confusion) 

to ensure convergence   k     0 in sg method to control variance. 
what can we say when   k =    is constant?  
                                                                                                      
noise reduction methods in part 3 directly control the noise given in the last 
term 

11 

      

theoretical motivation              - strongly convex case 

    batch gradient: linear convergence
                 rn(wk)     rn(w*)     o(  k)
 
per iteration cost proportional to n 

  <1

    sg has sublinear rate of convergence
           e[rn(wk)     rn(w*)] = o(1/ k)
  
per iteration cost and convergence constant independent of n
same convergence rate for generalization error
           e[r(wk)     r(w*)] = o(1/ k)
 

12 

      

computational complexity 

 total work to obtain rn(wk)     rn(w*) +   
 batch gradient method:     nlog(1/   )  
 stochastic gradient method:   1/   
which one is better? 
           a discussion of these tradeoffs is next! 

 think of    = 10   3

disclaimer: although much is understood about the sg method 
there are still some great mysteries, e.g.: why is it 
so much better than batch methods on dnns? 

13 

      

end of part i 

 
       
                           
 
 
 

 

 

14 

