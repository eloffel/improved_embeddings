optimization methods for machine learning

part ii     the theory of sg

leon bottou

facebook ai research

frank e. curtis
lehigh university 

jorge nocedal

northwestern university

summary

1. setup
2. fundamental lemmas
3. sg for strongly convex objectives
4. sg for general objectives
5. work complexity for large-scale learning
6. comments

2

1- setup

3

the generic sg algorithm

we assume that we have access to three mechanisms

the sg algorithm produces successive iterates     "   	
      &
with the goal to minimize a certain function           &      .
1. given an iteration number     	
   ,
a mechanism to generate a realization of a random variable     ".
the     " form a sequence of jointly independent random variables
2. given an iterate     " and a realization     ", 
a mechanism to compute a stochastic vector         ",    "       &
a mechanism to compute a scalar stepsize    ">0

3. given an iteration number,

4

the generic sg algorithm

algorithm 4.1 (stochastic gradient (sg) method)

5

the generic sg algorithm

the function           &       could be

the stochastic vector could be

the expected risk,
the empirical risk.

the gradient for one example,

the gradient for a minibatch,

possibly rescaled

6

the generic sg algorithm

previous ones.

stochastic processes

    we assume that the     " are jointly independent to avoid the full 
machinery of stochastic processes.  but everything still holds if the     "
form an adapted stochastic process, where each     " can depend on the 
    we can handle more complex setups by view     " as a    random seed   .
for instance, in active learning,     (    ",    ") firsts construct a multinomial 
distribution on the training examples in a manner that depends on     ", 
then uses the random seed     " to pick one according to that distribution.

active learning

the same mathematics cover all these cases.

7

2- fundamental lemmas

8

smoothness

smoothness
    our analysis relies on a smoothness assumption.

we chose this path because it also gives results for the nonconvex case.
we   ll discuss other paths in the commentary section.

well known consequence

9

smoothness

       45
is the expectation with respect to the distribution of     " only.
       45    (    "67) is meaningful because     "67 depends on     " (step 6 of sg)

expected   decrease

noise

10

smoothness

11

moments

12

moments

in expectation     (    ",    ") is a sufficient descent direction.
    true if     45    (    ",    ") =            " with     =    ;=1.
    true if     45    (    ",    ") =    "        (    ") with bounded spectrum. 

   

13

moments

        45

denotes the variance w.r.t.     "

    variance of the noise must be bounded in a mild manner.

14

moments

    combining (4.7b) and (4.8) gives

with 

15

moments

expected   decrease

noise

    the convergence of sg depends on the balance between these two terms. 

16

moments

17

3- sg for strongly convex objectives

18

strong convexity

known consequence

why does strong convexity matter?
    it gives the strongest results.
    it often happens in practice  (one regularizes to facilitate  optimization!)
    it describes any smooth function near a strong local minimum.

19

total expectation

different expectations

is the expectation with respect to the distribution of     " only.
is the total expectation w.r.t. the joint distribution of all     ".

       45
       
for instance, since     " depends only on     7,    ?,   ,    "a7,
            " =	
       4b    4c       45db[        "]

results in expectation 
    we focus on results that characterize the properties of sg in expectation.
    the stochastic approximation literature usually relies on rather complex 
martingale techniques to establish almost sure convergence results. we 
avoid them because they do not give much additional insight.

20

sg with fixed stepsize

    only converges to a neighborhood of the optimal value.
    both (4.13) and (4.14) describe well the actual behavior.

21

sg with fixed stepsize (proof)

22

sg with fixed stepsize

note the interplay between the stepsize    g and the variance bound     .
    if     =0, one recovers the linear convergence of batch id119.
    if     >0, one reaches a point where the noise prevents further progress.

23

diminishing the stepsizes

"
    

    

stepsize   

stepsize   /2

=      

    n/?
       
    if we wait long enough, halving the stepsize   eventually halves         "           .
    we can even estimate           2    n/?       n

k

24

  

"
    

    
    

diminishing the stepsizes faster

divide      by 2 whenever             "            reaches 2s[\?]^.
  /2

  /4
reaches             /        .

       
    divide      by 2 whenever             "
    time     s between changes : 1                tu=1/3 means     s   1/    .
    whenever we halve      we must wait twice as long to halve     (    )          .
    overall convergence rate in     1        .

k

25

sg with diminishing stepsizes

26

sg with diminishing stepsizes

same maximal stepsize

stepsize decreases in 1/k

not too slow   

   otherwise

gap     stepsize

27

sg with diminishing stepsizes (proof)

28

mini batching

using minibatches with stepsize    g :
using single example with stepsize    g	
   /	
          a :
       a times more iterations that are        a times cheaper.

computation

       a

1

noise

    
    /       a

same

29

minibatching

ignoring implementation issues

    we can match minibatch sg with stepsize    g
using single example sg with stepsize    g	
   /	
          a .
    we can match single example sg with stepsize     g
using minibatch sg with stepsize     g	
     	
          a
provided     g	
     	
          a is smaller than the max stepsize.

with implementation issues
    minibatch implementations use the hardware better.
    especially on gpu.

30

4- sg for general objectives

31

nonconvex objectives

nonconvex training objectives are pervasive in deep learning.

nonconvex landscape in high dimension can be very complex.
    critical points can be local minima or saddle points.
    critical points can be first order of high order.
    critical points can be part of critical manifolds.
    a critical manifold can contain both local minima and saddle points.

we describe meaningful (but weak) guarantees
    essentially, sg goes to critical points.

the sg noise plays an important role in practice
    it seems to help navigating local minima and saddle points.
    more noise has been found to sometimes help optimization.
    but the theoretical understanding of these facts is weak.

32

nonconvex sg with fixed stepsize

33

nonconvex sg with fixed stepsize

same max stepsize

if the average norm of the 
gradient is small, then the 
norm of the gradient cannot 

be often large   

this does not

this goes to zero like 1/k

34

nonconvex sg with fixed stepsize (proof)

35

nonconvex sg with diminishing step sizes

36

5- work complexity for large-scale learning

37

large-scale learning

assume that we are in the large data regime
    training data is essentially unlimited.
    computation time is limited.

the good
    more training data     less overfitting
    less overfitting     richer models.

the bad
    using more training data or rich models quickly exhausts the time budget.

    how thoroughly do we need to optimize     d(    )
when we actually want another function     (    ) to be small ?

the hope

38

expected risk versus training time

i

   

k
s
r
d
e
t
c
e
p
x
e

    when we vary the number of examples

39

expected risk versus training time

i

   

k
s
r
d
e
t
c
e
p
x
e

    when we vary the number of examples, the model, and the optimizer   

40

expected risk versus training time

i

   

k
s
r
d
e
t
c
e
p
x
e

good   

combinations

    the optimal combination depends on the computing time budget

41

formalization

the components of the expected risk

question

    given a fixed model     and a time budget        gh, choose     ,       
    statistics tell us    klm(    ) decreases with a rate in range 1/           1/    .

approach

    for now, let   s work with the fastest rate compatible with statistics

42

batch versus stochastic

typical convergence rates
    batch algorithm:
    stochastic algorithm:

rate analysis

processing more 

training examples beats 

optimizing more 

thoroughly.

if    klm(    ) decreases 
slower than 1/    .  

this effect only grows 

43

6- comments

44

asymptotic performance of sg is fragile

diminishing stepsizes are tricky
    theorem 4.7 (strongly convex function) suggests

sg converges very 

slowly if      < 7]^
when      is above ?^[\q

sg usually diverges 

constant stepsizes are often used in practice
    sometimes with a simple halving protocol.

45

condition numbers

         and 

         appear in critical places

the ratios 

    theorem 4.6.  with	
       =1,    x=0, the optimal stepsize is     g=7[

46

distributed computing

    because it updates the parameters     	
   with high frequency

sg is notoriously hard to parallelize

    because it slows down with delayed updates.

sg still works with relaxed synchronization
    because this is just a little bit more noise.

communication overhead give room for new opportunities
    there is ample time to compute things while communication takes place.
    opportunity for optimization algorithms with higher per-iteration costs 
    sg may not be the best answer for distributed training.

47

smoothness versus convexity

analyses of sg that only rely on convexity

    bounding     "           ? instead of          "           
and assuming     45    (    ",    ") =    y    "            (    ")

gives a result similar to lemma 4.4.

expected decrease

noise

    ways to bound the expected decrease

    proof does not easily support second order methods.

48

