sg

noise reduction methods

second-order methods

other methods

beyond sg: noise reduction and second-order methods

https://arxiv.org/abs/1606.04838

frank e. curtis, lehigh university

joint work with

l  eon bottou, facebook ai research

jorge nocedal, northwestern university

international conference on machine learning (icml)

new york, ny, usa

19 june 2016

beyond sg: noise reduction and second-order methods

1 of 38

sg

noise reduction methods

second-order methods

other methods

outline

sg

noise reduction methods

second-order methods

other methods

beyond sg: noise reduction and second-order methods

2 of 38

sg

noise reduction methods

second-order methods

other methods

what have we learned about sg?

assumption (cid:104)l/c(cid:105)
the objective function f : rd     r is

(cid:73) c-strongly convex (    unique minimizer) and
(cid:73) l-smooth (i.e.,    f is lipschitz continuous with constant l).

theorem sg (sublinear convergence)
under assumption (cid:104)l/c(cid:105) and e  k [(cid:107)g(wk,   k)(cid:107)2

2]     m + o((cid:107)   f (wk)(cid:107)2
2),

wk+1     wk       kg(wk,   k)

yields

1
  k =
l
  k = o

(cid:18) 1

(cid:19)

k

=    e[f (wk)     f   ]     m
2c
=    e[f (wk)     f   ] = o

;

(cid:18) (l/c)(m/c)

(cid:19)

.

k

(*let   s assume unbiased gradient estimates; see paper for more generality.)

beyond sg: noise reduction and second-order methods

3 of 38

sg

noise reduction methods

second-order methods

other methods

illustration

figure: sg run with a    xed stepsize (left) vs. diminishing stepsizes (right)

beyond sg: noise reduction and second-order methods

4 of 38

sg

noise reduction methods

second-order methods

other methods

what can be improved?

stochastic
gradient

better
rate

better

constant

beyond sg: noise reduction and second-order methods

5 of 38

sg

noise reduction methods

second-order methods

other methods

what can be improved?

stochastic
gradient

better
rate

better

constant

better rate and
better constant

beyond sg: noise reduction and second-order methods

5 of 38

sg

noise reduction methods

second-order methods

other methods

two-dimensional schematic of methods

stochastic
gradient

batch

gradient

noise reduction

second-order

stochastic
newton

batch
newton

beyond sg: noise reduction and second-order methods

6 of 38

sg

noise reduction methods

second-order methods

other methods

nonconvex objectives

despite loss of convergence rate, motivation for nonconvex problems as well:

(cid:73) convex results describe behavior near strong local minimizer
(cid:73) batch gradient methods are unlikely to get trapped near saddle points
(cid:73) second-order information can

(cid:73) avoid negative e   ects of nonlinearity and ill-conditioning
(cid:73) require mini-batching (noise reduction) to be e   cient

conclusion: explore entire plane, not just one axis

beyond sg: noise reduction and second-order methods

7 of 38

sg

noise reduction methods

second-order methods

other methods

outline

sg

noise reduction methods

second-order methods

other methods

beyond sg: noise reduction and second-order methods

8 of 38

sg

noise reduction methods

second-order methods

other methods

two-dimensional schematic of methods

stochastic
gradient

batch

gradient

noise reduction

second-order

stochastic
newton

batch
newton

beyond sg: noise reduction and second-order methods

9 of 38

sg

noise reduction methods

second-order methods

other methods

2d schematic: noise reduction methods

stochastic
gradient

batch

gradient

noise reduction

dynamic sampling

gradient aggregation

iterate averaging

stochastic
newton

beyond sg: noise reduction and second-order methods

10 of 38

sg

noise reduction methods

second-order methods

other methods

ideal: linear convergence of a batch gradient method

f (wk)

wk

w

beyond sg: noise reduction and second-order methods

11 of 38

sg

noise reduction methods

second-order methods

other methods

ideal: linear convergence of a batch gradient method

f (wk)

f (w)?

f (w)?

wk

w

beyond sg: noise reduction and second-order methods

11 of 38

sg

noise reduction methods

second-order methods

other methods

ideal: linear convergence of a batch gradient method

f (wk ) +    f (wk )t (w     wk ) + 1

2

l(cid:107)w     wk(cid:107)2

2

f (wk)

wk

w

beyond sg: noise reduction and second-order methods

11 of 38

sg

noise reduction methods

second-order methods

other methods

ideal: linear convergence of a batch gradient method

f (wk ) +    f (wk )t (w     wk ) + 1

2

l(cid:107)w     wk(cid:107)2

2

f (wk)

f (wk ) +    f (wk )t (w     wk ) + 1

2

c(cid:107)w     wk(cid:107)2

2

wk

w

beyond sg: noise reduction and second-order methods

11 of 38

sg

noise reduction methods

second-order methods

other methods

ideal: linear convergence of a batch gradient method

f (wk ) +    f (wk )t (w     wk ) + 1

2

l(cid:107)w     wk(cid:107)2

2

f (wk)

f (wk ) +    f (wk )t (w     wk ) + 1

2

c(cid:107)w     wk(cid:107)2

2

choosing    = 1/l to minimize upper bound yields
(f (wk+1)     f   )     (f (wk)     f   )     1
while lower bound yields

2l(cid:107)   f (wk)(cid:107)2

2

1

2(cid:107)   f (wk)(cid:107)2

2     c(f (wk)     f   ),

wk

which together imply that

(f (wk+1)     f   )     (1     c

l )(f (wk)     f   ).

w

beyond sg: noise reduction and second-order methods

11 of 38

sg

noise reduction methods

second-order methods

other methods

illustration

figure: sg run with a    xed stepsize (left) vs. batch gradient with    xed stepsize (right)

beyond sg: noise reduction and second-order methods

12 of 38

sg

noise reduction methods

second-order methods

other methods

idea #1: dynamic sampling

we have seen

(cid:73) fast initial improvement by sg
(cid:73) long-term linear rate achieved by batch gradient
=    accumulate increasingly accurate gradient information during optimization.

beyond sg: noise reduction and second-order methods

13 of 38

sg

noise reduction methods

second-order methods

other methods

idea #1: dynamic sampling

we have seen

(cid:73) fast initial improvement by sg
(cid:73) long-term linear rate achieved by batch gradient
=    accumulate increasingly accurate gradient information during optimization.

but at what rate?

(cid:73) too slow: won   t achieve linear convergence
(cid:73) too fast: loss of optimal work complexity

beyond sg: noise reduction and second-order methods

13 of 38

sg

noise reduction methods

second-order methods

other methods

geometric decrease

correct balance achieved by decreasing noise at a geometric rate.

theorem 3
suppose assumption (cid:104)l/c(cid:105) holds and that

v  k [g(wk,   k)]     m   k   1 for some m     0 and        (0, 1).

then, the sg method with a    xed stepsize    = 1/l yields

where

e[f (wk)     f   ]         k   1,
(cid:111)

(cid:26) m
(cid:110)

   := max

, f (w1)     f   

(cid:27)

c
1     c
2l

and    := max

,   

< 1.

e   ectively ties rate of noise reduction with convergence rate of optimization.

beyond sg: noise reduction and second-order methods

14 of 38

sg

noise reduction methods

second-order methods

other methods

geometric decrease

proof.

the now familiar inequality

e  k [f (wk+1)]     f (wk)          (cid:107)   f (wk)(cid:107)2

2 + 1

2   2le  k [(cid:107)g(wk,   k)(cid:107)2
2],

strong convexity, and the stepsize choice lead to

e[f (wk+1)     f   ]    (cid:16)

(cid:17) e[f (wk)     f   ] +

1     c
l

  k   1.

m
2l

(cid:73) exactly as for batch gradient (in expectation) except for the last term.
(cid:73) an inductive argument completes the proof.

beyond sg: noise reduction and second-order methods

15 of 38

sg

noise reduction methods

second-order methods

other methods

practical geometric decrease (unlimited samples)

how can geometric decrease of the variance be achieved in practice?

   f (wk;   k,i) with |sk| = (cid:100)   k   1(cid:101) for    > 1,

(cid:88)

i   sk

gk :=

1
|sk|
since, for all i     sk,

v  k [gk]     v  k [   f (wk;   k,i)]

|sk|

    m ((cid:100)  (cid:101))k   1.

beyond sg: noise reduction and second-order methods

16 of 38

sg

noise reduction methods

second-order methods

other methods

practical geometric decrease (unlimited samples)

how can geometric decrease of the variance be achieved in practice?

   f (wk;   k,i) with |sk| = (cid:100)   k   1(cid:101) for    > 1,

(cid:88)

i   sk

gk :=

1
|sk|
since, for all i     sk,

but is it too fast? what about work complexity?

|sk|

v  k [gk]     v  k [   f (wk;   k,i)]
same as sg as long as       (cid:16)

    m ((cid:100)  (cid:101))k   1.

)   1(cid:105)

.

1, (1     c
2l

beyond sg: noise reduction and second-order methods

16 of 38

sg

noise reduction methods

second-order methods

other methods

illustration

figure: sg run with a    xed stepsize (left) vs. dynamic sg with    xed stepsize (right)

beyond sg: noise reduction and second-order methods

17 of 38

sg

noise reduction methods

second-order methods

other methods

additional considerations

in practice, choosing    is a challenge.

(cid:73) what about an adaptive technique?
(cid:73) guarantee descent in expectation
(cid:73) methods exist, but need geometric sample size increase as backup

beyond sg: noise reduction and second-order methods

18 of 38

sg

noise reduction methods

second-order methods

other methods

idea #2: gradient aggregation

   i   m minimizing a    nite sum and am willing to store previous gradient(s).   

f (w) = rn(w) =

1
n

fi(w).

n(cid:88)

i=1

idea: reuse and/or revise previous gradient information in storage.

(cid:73) svrg: store full gradient, correct sequence of steps based on perceived bias
(cid:73) saga: store elements of full gradient, revise as optimization proceeds

beyond sg: noise reduction and second-order methods

19 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic variance reduced gradient (svrg) method

at wk =: wk,1, compute a batch gradient:

   f1(wk)

   f2(wk)

(cid:124)

   f3(wk)

(cid:123)(cid:122)

gk,1        f (wk)

   f4(wk)

   f5(wk)

(cid:125)

then step

wk,2     wk,1       gk,1

beyond sg: noise reduction and second-order methods

20 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic variance reduced gradient (svrg) method

now, iteratively, choose an index randomly and correct bias:

(cid:124)

   f1(wk)

   f2(wk)

   f4(wk,2)

   f5(wk)

   f3(wk)

(cid:123)(cid:122)

gk,2        f (wk)        f4(wk) +    f4(wk,2)

(cid:125)

then step

wk,3     wk,2       gk,2

beyond sg: noise reduction and second-order methods

20 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic variance reduced gradient (svrg) method

now, iteratively, choose an index randomly and correct bias:

(cid:124)

   f1(wk)

   f2(wk,3)

   f4(wk)

   f5(wk)

   f3(wk)

(cid:123)(cid:122)

gk,3        f (wk)        f2(wk) +    f2(wk,3)

(cid:125)

then step

wk,4     wk,3       gk,3

beyond sg: noise reduction and second-order methods

20 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic variance reduced gradient (svrg) method

each gk,j is an unbiased estimate of    f (wk,j )!

algorithm svrg
1: choose an initial iterate w1     rd, stepsize    > 0, and positive integer m.
2: for k = 1, 2, . . . do
3:

compute the batch gradient    f (wk).
initialize wk,1     wk.
for j = 1, . . . , m do

chose i uniformly from {1, . . . , n}.
set gk,j        fi(wk,j )     (   fi(wk)        f (wk)).
set wk,j+1     wk,j       gk,j .

end for
option (a): set wk+1 =   wm+1
option (b): set wk+1 = 1
m
option (c): choose j uniformly from {1, . . . , m} and set wk+1 =   wj+1.

j=1   wj+1

(cid:80)m

4:

5:

6:

7:

8:

9:

10:

11:

12:
13: end for

under assumption (cid:104)l/c(cid:105), options (b) and (c) linearly convergent for certain (  , m)

beyond sg: noise reduction and second-order methods

21 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic average gradient (saga) method

at w1, compute a batch gradient:

   f1(w1)

   f2(w1)

(cid:124)

   f3(w1)

(cid:123)(cid:122)

g1        f (w1)

   f4(w1)

   f5(w1)

(cid:125)

then step

w2     w1       g1

beyond sg: noise reduction and second-order methods

22 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic average gradient (saga) method

now, iteratively, choose an index randomly and revise table entry:

(cid:124)

   f1(w1)

   f2(w1)

   f4(w2)

   f5(w1)

   f3(w1)

(cid:123)(cid:122)

(cid:125)

g2     new entry     old entry + average of entries (before replacement)

then step

w3     w2       g2

beyond sg: noise reduction and second-order methods

22 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic average gradient (saga) method

now, iteratively, choose an index randomly and revise table entry:

(cid:124)

   f1(w1)

   f2(w3)

   f4(w2)

   f5(w1)

   f3(w1)

(cid:123)(cid:122)

(cid:125)

g3     new entry     old entry + average of entries (before replacement)

then step

w4     w3       g3

beyond sg: noise reduction and second-order methods

22 of 38

sg

noise reduction methods

second-order methods

other methods

stochastic average gradient (saga) method

each gk is an unbiased estimate of    f (wk)!

algorithm saga
1: choose an initial iterate w1     rd and stepsize    > 0.
2: for i = 1, . . . , n do
3:

compute    fi(w1).
store    fi(w[i])        fi(w1).

4:
5: end for
6: for k = 1, 2, . . . do
7:

choose j uniformly in {1, . . . , n}.
compute    fj (wk).
set gk        fj (wk)        fj (w[j]) + 1
store    fj (w[j])        fj (wk).
set wk+1     wk       gk.

n

8:

9:

10:

11:
12: end for

(cid:80)n
i=1    fi(w[i]).

under assumption (cid:104)l/c(cid:105), linearly convergent for certain   

(cid:73) storage of gradient vectors reasonable in some applications
(cid:73) with access to feature vectors, need only store n scalars

beyond sg: noise reduction and second-order methods

23 of 38

sg

noise reduction methods

second-order methods

other methods

idea #3: iterative averaging

averages of sg iterates are less noisy:

wk+1     wk       kg(wk,   k)
  wk+1     1

k+1(cid:88)

k + 1

j=1

wj (in practice: running average)

unfortunately, no better theoretically when   k = o(1/k), but
   
(cid:73) long steps (say,   k = o(1/
(cid:73) lead to a better sublinear rate (like a second-order method?)

k)) and averaging

see also

(cid:73) mirror descent
(cid:73) primal-dual averaging

beyond sg: noise reduction and second-order methods

24 of 38

sg

noise reduction methods

second-order methods

other methods

idea #3: iterative averaging

averages of sg iterates are less noisy:

wk+1     wk       kg(wk,   k)
  wk+1     1

k+1(cid:88)

k + 1

j=1

wj (in practice: running average)

figure: sg run with o(1/

   

k) stepsizes (left) vs. sequence of averages (right)

beyond sg: noise reduction and second-order methods

24 of 38

sg

noise reduction methods

second-order methods

other methods

outline

sg

noise reduction methods

second-order methods

other methods

beyond sg: noise reduction and second-order methods

25 of 38

sg

noise reduction methods

second-order methods

other methods

two-dimensional schematic of methods

stochastic
gradient

batch

gradient

noise reduction

second-order

stochastic
newton

batch
newton

beyond sg: noise reduction and second-order methods

26 of 38

sg

noise reduction methods

second-order methods

other methods

2d schematic: second-order methods

stochastic
gradient

batch

gradient

second-order

diagonal scaling

natural gradient

gauss-newton

quasi-newton

hessian-free newton

stochastic
newton

beyond sg: noise reduction and second-order methods

27 of 38

sg

noise reduction methods

second-order methods

other methods

ideal: scale invariance

neither sg nor batch gradient are invariant to linear transformations!

min
w   rd
min
  w   rd

f (w)

f (b   w)

=    wk+1     wk       k   f (wk)
=      wk+1       wk       kb   f (b   wk)

(for given b (cid:31) 0)

beyond sg: noise reduction and second-order methods

28 of 38

sg

noise reduction methods

second-order methods

other methods

ideal: scale invariance

neither sg nor batch gradient are invariant to linear transformations!

min
w   rd
min
  w   rd

f (w)

f (b   w)

=    wk+1     wk       k   f (wk)
=      wk+1       wk       kb   f (b   wk)

(for given b (cid:31) 0)

scaling latter by b and de   ning {wk} = {b   wk} yields
wk+1     wk       kb2   f (wk)

(cid:73) algorithm is clearly a   ected by choice of b
(cid:73) surely, some choices may be better than others (in general?)

beyond sg: noise reduction and second-order methods

28 of 38

sg

noise reduction methods

second-order methods

other methods

newton scaling

consider the function below and suppose that wk = (0, 3):

beyond sg: noise reduction and second-order methods

29 of 38

sg

noise reduction methods

second-order methods

other methods

newton scaling

batch gradient step      k   f (wk) ignores curvature of the function:

beyond sg: noise reduction and second-order methods

29 of 38

sg

noise reduction methods

second-order methods

other methods

newton scaling

newton scaling (b = (   f (wk))   1/2): gradient step moves to the minimizer:

beyond sg: noise reduction and second-order methods

29 of 38

sg

noise reduction methods

second-order methods

other methods

newton scaling

. . . corresponds to minimizing a quadratic model of f in the original space:

wk+1     wk +   ksk where    2f (wk)sk =       f (wk)

beyond sg: noise reduction and second-order methods

29 of 38

sg

noise reduction methods

second-order methods

other methods

deterministic case

what is known about newton   s method for deterministic optimization?

(cid:73) local rescaling based on inverse hessian information
(cid:73) locally quadratically convergent near a strong minimizer
(cid:73) global convergence rate better than gradient method (when regularized)

beyond sg: noise reduction and second-order methods

30 of 38

sg

noise reduction methods

second-order methods

other methods

deterministic case to stochastic case

what is known about newton   s method for deterministic optimization?

(cid:73) local rescaling based on inverse hessian information
(cid:73) locally quadratically convergent near a strong minimizer
(cid:73) global convergence rate better than gradient method (when regularized)

however, it is way too expensive in our case.

(cid:73) but all is not lost: scaling is viable.
(cid:73) wide variety of scaling techniques improve performance.
(cid:73) our convergence theory for sg still holds with b-scaling.
(cid:73) . . . could hope to remove condition number (l/c) from convergence rate!
(cid:73) added costs can be minimial when coupled with noise reduction.

beyond sg: noise reduction and second-order methods

30 of 38

sg

noise reduction methods

second-order methods

other methods

idea #1: inexact hessian-free newton

compute newton-like step

   2fsh

k

(wk)sk =       fsg

k

(wk)

k | < |sg

(cid:73) mini-batch size for hessian =: |sh
(cid:73) cost for mini-batch gradient: gcost
(cid:73) use cg and terminate early: maxcg iterations
(cid:73) in cg, cost for each hessian-vector product: f actor    gcost
(cid:73) choose maxcg    f actor     small constant so total per-iteration cost:

k| := mini-batch size for gradient

maxcg    f actor    gcost = o(gcost)

(cid:73) convergence guarantees for |sh

k | = |sg

k| = n are well-known

beyond sg: noise reduction and second-order methods

31 of 38

sg

noise reduction methods

second-order methods

other methods

idea #2: (generalized) gauss-newton

classical approach for nonlinear least squares, linearize inside of loss/cost:

f (w;   ) = 1
    1

2(cid:107)h(x  ; w)     y  (cid:107)2
2(cid:107)h(x  ; wk) + jh(wk;   )(w     wk)     y  (cid:107)2

2

2

leads to gauss-newton approximation for second-order terms:

gsh

k

(wk;   h

k ) =

1
k | jh(wk;   k,i)t jh(wk;   k,i)
|sh

beyond sg: noise reduction and second-order methods

32 of 38

sg

noise reduction methods

second-order methods

other methods

idea #2: (generalized) gauss-newton

classical approach for nonlinear least squares, linearize inside of loss/cost:

f (w;   ) = 1
    1

2(cid:107)h(x  ; w)     y  (cid:107)2
2(cid:107)h(x  ; wk) + jh(wk;   )(w     wk)     y  (cid:107)2

2

2

leads to gauss-newton approximation for second-order terms:

gsh

k

(wk;   h

k ) =

1
k | jh(wk;   k,i)t jh(wk;   k,i)
|sh

can be generalized for other (convex) losses:

(cid:101)gsh

k

(wk;   h

k ) =

(cid:125)
1
k | jh(wk;   k,i)t h(cid:96)(wk;   k,i)
|sh

(cid:123)(cid:122)

(cid:124)

=

   2(cid:96)
   h2

jh(wk;   k,i)

(cid:73) costs similar as for inexact newton
(cid:73) . . . but scaling matrices are always positive (semi)de   nite
(cid:73) see also natural gradient, invariant to more than just linear transformations

beyond sg: noise reduction and second-order methods

32 of 38

sg

noise reduction methods

second-order methods

other methods

idea #3: (limited memory) quasi-newton

only approximate second-order information with gradient displacements:

wk+1

wk

w

secant equation hkvk = sk to match gradient of f at wk, where

sk := wk+1     wk and vk :=    f (wk+1)        f (wk)

beyond sg: noise reduction and second-order methods

33 of 38

sg

noise reduction methods

second-order methods

other methods

deterministic case

standard update for inverse hessian (wk+1     wk       khkgk) is bfgs:

(cid:33)t

(cid:32)
i     vkst
k
st
k vk

(cid:33)

(cid:32)
i     vkst
k
st
k vk

hk

+

skst
k
st
k vk

hk+1    

what is known about quasi-id77s for deterministic optimization?

(cid:73) local rescaling based on iterate/gradient displacements
(cid:73) strongly convex function =    positive de   nite (p.d.) matrices
(cid:73) only    rst-order derivatives, no linear system solves
(cid:73) locally superlinearly convergent near a strong minimizer

beyond sg: noise reduction and second-order methods

34 of 38

sg

noise reduction methods

second-order methods

other methods

deterministic case to stochastic case

standard update for inverse hessian (wk+1     wk       khkgk) is bfgs:

(cid:33)t

(cid:32)
i     vkst
k
st
k vk

(cid:33)

(cid:32)
i     vkst
k
st
k vk

hk

+

skst
k
st
k vk

hk+1    

what is known about quasi-id77s for deterministic optimization?

(cid:73) local rescaling based on iterate/gradient displacements
(cid:73) strongly convex function =    positive de   nite (p.d.) matrices
(cid:73) only    rst-order derivatives, no linear system solves
(cid:73) locally superlinearly convergent near a strong minimizer

extended to stochastic case? how?

(cid:73) noisy gradient estimates =    challenge to maintain p.d.
(cid:73) correlation between gradient and hessian estimates
(cid:73) overwriting updates =    poor scaling that plagues!

beyond sg: noise reduction and second-order methods

34 of 38

sg

noise reduction methods

second-order methods

other methods

proposed methods

(cid:73) gradient displacements using same sample:

vk :=    fsk (wk+1)        fsk (wk)

(requires two stochastic gradients per iteration)

(cid:73) gradient displacement replaced by action on subsampled hessian:

vk :=    2fsh

k

(wk)(wk+1     wk)

(cid:73) decouple iteration and hessian update to amortize added cost
(cid:73) limited memory approximations (e.g., l-bfgs) with per-iteration cost 4md

beyond sg: noise reduction and second-order methods

35 of 38

sg

noise reduction methods

second-order methods

other methods

idea #4: diagonal scaling

restrict added costs through only diagonal scaling:
wk+1     wk       kdkgk

ideas:

(cid:73) d   1
(cid:73) d   1
(cid:73) d   1

k     diag(hessian (approximation))
k     diag(gauss-newton approximation)
k     running average/sum of gradient components

last approach can be motivated by minimizing regret.

beyond sg: noise reduction and second-order methods

36 of 38

sg

noise reduction methods

second-order methods

other methods

outline

sg

noise reduction methods

second-order methods

other methods

beyond sg: noise reduction and second-order methods

37 of 38

sg

noise reduction methods

second-order methods

other methods

plenty of ideas not covered here!

(cid:73) gradient methods with momentum
(cid:73) gradient methods with acceleration
(cid:73) coordinate descent/ascent in the primal/dual
(cid:73) proximal gradient/newton for regularized problems
(cid:73) alternating direction methods
(cid:73) expectation-maximization
(cid:73) . . .

beyond sg: noise reduction and second-order methods

38 of 38

