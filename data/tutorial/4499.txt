id4 

and beyond 

kyunghyun	
   cho	
   

courant	
   ins/tute	
   &	
   center	
   for	
   data	
   science,	
   new	
   york	
   university	
   

facebook	
   ai	
   research	
   

a brief history 

for	
   a	
   comprehensive	
   history	
   of	
   machine	
   transla/on,	
   go	
      nd	
   andy	
   way	
   

       [allen	
   1987	
   ieee	
   1st	
   iid98]	
   
       3310	
   en-     es	
   pairs	
   constructed	
   on	
   31	
   
en,	
   40	
   es	
   words,	
   max	
   10/11	
   word	
   
sentence;	
   33	
   used	
   as	
   test	
   set	
   
       binary	
   encoding	
   of	
   words	
      	
   50	
   
inputs,	
   66	
   outputs;	
   1	
   or	
   3	
   hidden	
   
150-     unit	
   layers.	
   ave	
   wer:	
   1.3	
   
words	
   

       [chrisman	
   1992	
   connec&on	
   science]	
   
       dual-     ported	
   raam	
   architecture	
   
[pollack	
   1990	
   ar&   cial	
   intelligence]	
   
applied	
   to	
   corpus	
   of	
   216	
   parallel	
   pairs	
   
of	
   simple	
   en-     es	
   sentences:	
   
       split	
   50/50	
   as	
   train/test,	
   75%	
   of	
   
sentences	
   correctly	
   translated!	
   

brief	
   resurrec/on	
   in	
   1997:	
   spain	
   

modern id4 

source
sentence 

source
sentence 

source
sentence 

source
sentence 

source
sentence 

source
sentence 

source

sentence 

source

sentence 

neural 
neural 
network
network

smt

smt

smt

smt

neural net

neural net

neural net

neural net

neural 
network

target
sentence 

target
sentence 

target 
sentence

target 
sentence

target 
sentence

target 
sentence

target

sentence 

smt

neural net

target 

sentence

neural mt

neural mt

(schwenk et al. 2006)

(schwenk et al. 2006)

(devlin et al. 2014)

(devlin et al. 2014)

neural mt

(schwenk et al. 2006)

1	
   year	
   

wmt	
   2017:	
   news	
   transla/on	
   task	
   

a	
   be3er	
   single-     pair	
   transla&on	
   system	
   has	
   	
   

never	
   been	
      the   	
   goal	
   of	
   neural	
   mt	
   

multilingual translation via 
continuous representation 

what	
   if	
   we	
   can	
   project	
   sentences	
   in	
   mul/ple	
   languages	
   into	
   a	
   single	
   vector	
   space?	
   	
   

what does id4 do? 
encoder	
   
       project	
   a	
   source	
   sentence	
   into	
   a	
   
set	
   of	
   con/nuous	
   vectors	
   

decoder+a1en2on	
   
       decode	
   a	
   target	
   sentence	
   from	
   a	
   
set	
   of	
      source   	
   con/nuous	
   
vectors	
   

what is this    continuous vector space   ? 

       similar	
   sentences	
   are	
   near	
   each	
   other	
   
in	
   this	
   vector	
   space	
   
       mul/ple	
   dimensions	
   of	
   similarity	
   are	
   
encoded	
   simultaneously	
   

(sutskever	
   et	
   al.,	
   2014)	
   

what is this    continuous vector space   ? 

       similar	
   sentences	
   are	
   near	
   each	
   other	
   
in	
   this	
   vector	
   space	
   
       mul/ple	
   dimensions	
   of	
   similarity	
   are	
   
encoded	
   simultaneously	
   

       (trainable)	
   near-     bijec/ve	
   mapping	
   
between	
   the	
   con/nuous	
   vector	
   space	
   
and	
   the	
   sentence	
   space	
   

       stripped	
   of	
   hard	
   linguis2c	
   symbols	
   

what is this    continuous vector space   ? 

       can	
   this	
   con/nuous	
   vector	
   space	
   be	
   shared	
   across	
   mul/ple	
   languages?	
   

(firat	
   et	
   al.,	
   2016;	
   luong	
   et	
   al.,	
   2015;	
   dong	
   et	
   al.,	
   2015)	
   

multi-way, multilingual machine translation 
(1) 
       one	
   encoder	
   per	
   source	
   language	
   
       one	
   decoder	
   per	
   target	
   language	
   
       aken/on/alignment	
   shared	
   across	
   
all	
   the	
   language	
   pairs	
   
       only	
   bilingual	
   parallel	
   	
   
corpora	
   necessary	
   	
   
       no	
   mul/-     way	
   parallel	
   corpus	
   needed	
   

language-     agnos/c	
   
con/nuous	
   vector	
   

space	
   

(firat	
   et	
   al.,	
   2016)	
   

multi-way, multilingual machine translation 
(2) 
       neural	
   nets	
   are	
   like	
   lego	
   
       build	
   one	
   encoder	
   per	
   source	
   
       build	
   one	
   decoder	
   per	
   target	
   
       build	
   one	
   aken/on	
   mechanism	
   
       given	
   a	
   sentence	
   pair	
   	
   

(x source, y target)

       	
   	
   
       	
   	
   

h = encodersource(x source)
y = decodertarget(h, attention)

(firat	
   et	
   al.,	
   2016)	
   

multi-way, multilingual machine translation 
(3) 

       sentence-     level	
   posi/ve	
   language	
   transfer	
   	
   
       helps	
   low-     resource	
   language	
   pairs	
   
       why?	
   

1.    beker	
   structural	
   constraint	
   on	
   the	
   
2.    regulariza/on	
   

con/nuous	
   vector	
   space	
   

       real-     valued	
   vector-     based	
   interlingua?	
   

(firat	
   et	
   al.,	
   2016)	
   

language-     
agnos/c	
   
con/nuous	
   
vector	
   space	
   

how does the continuous space look 
like? 

(johnson	
   et	
   al.,	
   2016)	
   

beyond languages: multimodal 
translation 

f = (a,   man,   is,   jumping,   into,   a,   lake,   .)

apparently	
   not	
   so	
   cool	
   anymore!	
   

e ui

l
p
m
a
s
s

 

d
r
o
w

t
n
e
r
r
u
c
e
r

e zi

t
a
t
s

m

s
i
n
a
h
c
e

m

n
o
i
t
n
e
t
t

a

a

j

attention 
       weight

aj(cid:1) =1

+

k
r
o
w
t
e
n

 
l
a
r
u
e
n

 
l
a
n
o
i
t
u
l
o
v
n
o
c

annotation
vectors

hj

(cid:85)(cid:115)(cid:109) (cid:50)(cid:105) (cid:28)(cid:72)(cid:88)(cid:45) (cid:107)(cid:121)(cid:82)(cid:56)(cid:86)(cid:45) (cid:85)(cid:117)(cid:28)(cid:81) (cid:50)(cid:105) (cid:28)(cid:72)(cid:88)(cid:45) (cid:107)(cid:121)(cid:82)(cid:56)(cid:86)

(cid:88)

(xu	
   et	
   al.,	
   2015)	
   

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

(cid:88)

(cid:88)

beyond languages: multimodal 
translation 

(caglayan	
   et	
   al.,	
   2016;	
   elliok	
   &	
   kadar,	
   2017)	
   

what is a sentence?  

is	
   a	
   sentence	
   a	
   sequence	
   of	
   phrases,	
   words,	
   morphemes	
   or	
   characters?	
   	
   

what is a sentence to a neural net? 
       each	
   word/symbol:	
   one-     hot	
   vector	
   
       prior-     less	
   encoding	
   
       permuta/on	
   invariant	
   

       sentence	
   

       to	
   us:	
   a	
   sequence	
   of	
   words	
   
       to	
   nn:	
   a	
   sequence	
   of	
   one-     hot	
   vectors	
   

       what	
   does	
   it	
   mean?	
   

why not words? 
       ine   cient	
   handling	
   of	
   various	
   morphological	
   variants	
   

       sub-     op/mal	
   segmenta/on/tokeniza/on	
   
          etxaberria   ,	
      etxazarra   ,	
      etxaguren   ,	
      etxarren   :	
   four	
   independent	
   vectors	
   

       lack	
   of	
   generaliza/on	
   to	
   novel/rare	
   morphological	
   variants	
   

       for	
   instance,	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   in	
   arabic	
   =>	
      and	
   to	
   his	
   vehicle   	
   

       one	
   vector	
   for	
   compound	
   words?	
   

          kolmi/vaihe/kilo/wau/tun//mikari   	
   =>	
   one	
   vector?	
   
          kolme   	
   =>	
   one	
   vector?	
   

       spelling	
   issues	
   	
   

       social	
   media:	
   full	
   of	
   misspelt	
   and	
   non-     trivial	
   words	
   
       historical	
   documents:	
   spelling	
   normaliza/on	
   
       good	
   segmenta/on/tokeniza/on	
   needed	
   for	
   each	
   language	
   

       so,	
   no,	
   words	
   don   t	
   look	
   like	
   the	
   units	
   we	
   want	
   to	
   work	
   with   	
   

then, what should we do   ? 
       original:	
                                                	
   
       word-     level	
   modelling:	
   
(            ,	
         ,	
         ,	
                     )	
   
       subword-     level	
   modelling	
   (sennrich	
   et	
   al.,	
   2015;	
   wu	
   et	
   al.,	
   2016)	
   
(         ,	
      ,	
         ,	
      ,	
      ,	
         ,	
            ,	
      )	
   
       character-     level	
   modelling	
   with	
   segmenta/on	
   	
   
(wang	
   et	
   al.,	
   2015;	
   luong	
   &	
   manning,	
   2016;	
   costa-     jussa	
   &	
   fonollosa,	
   2016)	
   
((   ,   ,   ,   ,   ,   ,   ,   ),	
   (   ,   ,   ,   ,   ),	
   (   ,   ,   ,   ),	
   (   ,   ,   ,   ,   ,   ,   ,   ,   ,
   ,   ,   ,   ,   ))	
   
       fully	
   character-     level	
   modelling	
   (chung	
   et	
   al.,	
   2016;	
   lee	
   et	
   al.,	
   2017)	
   
(   ,   ,   ,   ,   ,   ,   ,   ,_,   ,   ,   ,   ,   ,_,   ,   ,   ,   ,_,   ,   ,   ,   ,   ,   ,   ,   ,   ,   ,
   ,   ,   ,   ))	
   
       visual	
   modelling	
   of	
   a	
   character	
   (liu	
   et	
   al.,	
   2017)	
   

character-level translation 
       source:	
   subword-     level	
   representa/on	
   
       target:	
   character-     level	
   representa/on	
   
       the	
   decoder	
   implicitly	
   learned	
   word-     like	
   units	
   automa&cally!	
   

(chung	
   et	
   al.,	
   2017)	
   

(lee	
   et	
   al.,	
   2017)	
   

depth	
   ma3ers	
   more	
   when	
   the	
   
representa&on	
   is	
   more	
   raw	
   

fully character-level translation 
       source:	
   character-     level	
   representa/on	
   
       target:	
   character-     level	
   representa/on	
   
       e   cient	
   modelling	
   with	
   	
   
a	
   convolu/onal-     recurrent	
   encoder	
   
       works	
   as	
   well	
   as,	
   or	
   be3er	
   than,	
   
subword-     level	
   transla/on	
   

       more	
   robust	
   to	
   errors	
   
       beker	
   handles	
   rare	
   tokens	
   

       rare	
   tokens	
   are	
   not	
   necessary	
   rare!	
   

(lee	
   et	
   al.,	
   2017)	
   

character-level multilingual translation  
       when	
   symbols	
   are	
   shared	
   across	
   mul/ple	
   languages,	
   why	
   not	
   share	
   a	
   
single	
   encoder/decoder	
   for	
   them?	
   
1.    language	
   transfer	
   at	
   all	
   levels:	
   lekers,	
   words,	
   phrases,	
   sentences,	
      	
   
2.   

intra-     sentence	
   code-     switching	
   without	
   any	
   speci   c	
   data	
   

(lee	
   et	
   al.,	
   2017;	
   johnson	
   et	
   al.,	
   2016;	
   ha	
   et	
   al.,	
   2016)	
   

character-level multilingual translation  
       prevents	
   over   ung	
   with	
   low-     resource	
   language	
   pairs	
   
       perhaps,	
   a	
   way	
   to	
   build	
   a	
   mt	
   
system	
   for	
   basque	
   languages?	
   
       many	
   dialects:	
   biscayan,	
   	
   
gipuzkoan,	
   navarrese,	
   	
   
navarro-     lapurdian,	
   soule/n	
   
	
   

(lee	
   et	
   al.,	
   2017)	
   

trainable decoding of  
id4 

jiatao	
   gu,	
   graham	
   neubig,	
   k	
   cho	
   and	
   victor	
   li.	
   learning	
   to	
   translate	
   in	
   real-     /me	
   
with	
   neural	
   machine	
   transla/on.	
   eacl	
   2017.	
   
jiatao	
   gu,	
   k	
   cho	
   and	
   victor	
   li.	
   trainable	
   greedy	
   decoding	
   for	
   neural	
   machine	
   
transla/on.	
   emnlp	
   2017.	
   

trainable decoding 
mo2va2on	
   
       many	
   decoding	
   objec/ves	
   unknown	
   while	
   training	
   
       lack	
   of	
   target	
   training	
   examples	
   
       arbitrary	
   (non-     di   eren/able)	
   decoding	
   objec/ves	
   

trainable 

nn 
module

our	
   approach	
   
       train	
   id4	
   with	
   supervised	
   learning	
   
       train	
   a	
   decoding	
   module	
   on	
   top	
   

output

output

output

neural 
network

neural 
network

neural 
network

input
t-1

input

t

input
t+1

(1) real-time translation 

decoding	
   
1.    start	
   with	
   a	
   pretrained	
   id4	
   
2.    a	
   simultaenous	
   decoder	
   intercepts	
   and	
   

interprets	
   the	
   incoming	
   signal	
   

3.    the	
   simultaneous	
   decoder	
   forces	
   the	
   

pretrained	
   model	
   to	
   either	
   
1.    output	
   a	
   target	
   symbol,	
   or	
   
2.    wait	
   for	
   a	
   next	
   source	
   symbol	
   

learning	
   
1.    trade-     o   	
   between	
   delay	
   and	
   quality	
   
2.    stochas/c	
   policy	
   gradient	
   (reinforce)	
   

(gu,	
   neubig,	
   cho	
   &	
   li,	
   eacl	
   2017)	
   
	
   

(1) real-time translation 

(2) trainable greedy decoding 

decoding	
   
1.    start	
   with	
   a	
   pretrained	
   id4	
   
2.    a	
   trainable	
   decoder	
   intercepts	
   and	
   

interprets	
   the	
   incoming	
   signal	
   

3.    the	
   trainable	
   decoder	
   sends	
   out	
   	
   

the	
   altering	
   signal	
   back	
   to	
   the	
   
pretrained	
   model	
   

ht 1,   yt 1

learning	
   
1.    determinis/c	
   policy	
   gradient	
   
2.    maximize	
   any	
   arbitrary	
   objec/ve	
   

y
r
o
m
e
m

trainable
decoder
yt|  y<t, x
decoder
gru/lstm

ct

attention

inspect

s

e

l

e

c

t

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

(gu,	
   cho	
   &	
   li,	
   2017)	
   
	
   

agent (encoder)

                                    .

(2) trainable greedy decoding 
models	
   
1.    actor	
   

    : r3d ! rd
ht 1

       input:	
   prev.	
   hid.	
   state	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   prev.	
   symbol	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   ,	
   and	
   
       output:	
   addi/ve	
   bias	
   for	
   hid.	
   state	
   	
   
       example:	
   	
   

context	
   	
   	
   	
   	
   	
   	
   	
   from	
   the	
   aken/on	
   model	
   
zt

  yt 1

ct

ht 1,   yt 1

2.    cri/c	
   

zt = u   (w [ht 1; e(  y); ct] + b) + c
rc : rd                rd ! r

       input:	
   a	
   sequence	
   of	
   the	
   hidden	
   states	
   from	
   the	
   decoder	
   
       output:	
   a	
   predicted	
   return	
   
       in	
   our	
   case,	
   the	
   cri/c	
   es/mates	
   the	
   full	
   return	
   rather	
   than	
   

q	
   at	
   each	
   /me	
   step	
   	
   

trainable
decoder
yt|  y<t, x
decoder
gru/lstm

ct

attention

inspect

s

e

l

e

c

t

y
r
o
m
e
m

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

(gu,	
   cho	
   &	
   li,	
   2017)	
   
	
   

agent (encoder)

                                    .

(2) trainable greedy decoding 
learning	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   and	
   	
   
((h1, z1), . . . , (ht , zt ))

1)    generate	
   transla/on	
   given	
   a	
   source	
   sentence	
   with	
   noise	
   
2)    train	
   the	
   cri/c	
   to	
   minimize	
   
3)    generate	
   mul/ple	
   transla/ons	
   with	
   noise	
   

r
(rc(h1, . . . , ht )   r)2
t )  
t , zm

4)    cri/c-     aware	
   actor	
   learning:	
   newly	
   proposed	
   

t )  , . . . , (hm

  (h1

1 ), . . . , (hm

1), . . . , (h1

1 , zm

t , z1

1, z1

ht 1,   yt 1

trainable
decoder
yt|  y<t, x
decoder
gru/lstm

ct

attention

inspect

s

e

l

e

c

t

	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   where	
   	
   

	
   
id136:	
   simply	
   throw	
   away	
   the	
   cri/c	
   and	
   use	
   the	
   actor	
   

(gu,	
   cho	
   &	
   li,	
   emnlp	
   2017)	
   
	
   

y
r
o
m
e
m

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

c
o
d
e
 
v
e
c
t
o
r

agent (encoder)

                                    .

(2) trainable greedy decoding 

       the	
   trainable	
   decoder	
   does	
   improve	
   the	
   target	
   decoding	
   objec/ve	
   
       training	
   is	
   quite	
   unstable	
   without	
   the	
   cri/c-     aware	
   actor	
   learning	
   algorithm	
   
       more	
   work	
   is	
   de   nitely	
   needed	
   for	
   further	
   improvement	
   

non-parametric  
id4 

gu,	
   wang,	
   cho,	
   li.	
   search	
   engine	
   guided	
   non-     parametric	
   neural	
   machine	
   
transla/on.	
   about	
   to	
   be	
   rejected	
   from	
   nips   17.	
   

parametric ml: learning as compression 
       what	
   does	
   learning	
   do?	
   

training	
   

data	
   

learning	
   

neural	
   
network	
   

neural	
   
network	
   

id136	
   

       parametric	
   machine	
   learning:	
   data	
   compression	
   +	
   pakern	
   matching	
   

non-parametric id4 
       bring	
   the	
   whole	
   training	
   corpus	
   together	
   with	
   a	
   model	
   
       retrieved	
   a	
   small	
   subset	
   of	
   examples	
   using	
   a	
   fast	
   search	
   engine	
   
       let	
   id4	
      gure	
   out	
   how	
   to	
   fuse	
   	
   
1.    the	
   current	
   sentence,	
   and	
   	
   
2.    the	
   retrieved	
   transla/on	
   pairs	
   

retrieval stage 
x?

current source sentence

query

search

scoring

w
e
f

(  x,   y)

s
d
e
r
d
n
u
h

(x10, y10)
(x20, y20)
...
(xn0, yn0)

s
n
o

i
l
l
i

m

(x1, y1)
(x2, y2)
(x3, y3)
(x4, y4)
...
(xn , yn )

       o   -     the-     shelf	
   search	
   engine	
   
indexes	
   the	
   whole	
   training	
   set	
   
       the	
   engine	
   is	
   queried	
   with	
   a	
   
current	
   source	
   sentence	
   
       only	
   top-     few	
   transla/on	
   pairs	
   
are	
   selected	
   at	
   the	
   end	
   

       extremely	
   e   cient!	
   

(gu	
   et	
   al.,	
   2017)	
   

translation stage (1) 
       store	
   the	
   retrieved	
   pair	
   in	
   a	
   key-     
value	
   memory	
   (gulcehre	
   et	
   al.,	
   
2016;	
   hena   	
   et	
   al.,	
   2017)	
   
       use	
   the	
   aken/on-     based	
   neural	
   
machine	
   transla/on	
   
       key:	
   the	
   context	
   vector	
   
       value:	
   the	
   target	
   symbol	
   

value

key

y1 y2 y3
c1 c2 c3

yt
ct

x1 x2 x3

xt 0

(gu	
   et	
   al.,	
   2017)	
   

translation stage (2) 

p(yt| . . .)

       retrieves	
   relevant	
   target	
   symbols	
   from	
   the	
   
memory	
   
       incorporate	
   the	
   retrieved	
   value	
   into	
   
compu/ng	
   the	
   target	
   distribu/on	
   
       similar	
   to	
   larger-     context	
   id4	
   

       [wang	
   et	
   al.,	
   2017;	
   jean	
   et	
   al.,	
   2017]	
   

       similar	
   to	
   id4	
   with	
   external	
   knowledge	
   

       [ahn	
   et	
   al.,	
   2016;	
   bahdanau	
   et	
   al.,	
   2017]	
   

x?
1 x?
2

  yt 1
zt 1

fusion

zt

ct

match

 1  2  3  t score
y1 y2 y3
c1 c2 c3

yt
value
ct key

x?
t ?

x1 x2 x3

xt 0

(gu	
   et	
   al.,	
   2017)	
   

better retrieval, better translation 

more consistent translation 

       transla/on	
   with	
   consistent	
   style	
   and	
   word	
   choice	
   
       personalized	
   transla/on	
   

language modelling  
with external knowledge 

hill,	
   cho,	
   korhonen,	
   bengio.	
   learning	
   to	
   understand	
   phrases	
   by	
   embedding	
   the	
   dic/onary.	
   
tacl	
   2015.	
   
ahn,	
   choi,	
   p  mamaa,	
   bengio.	
   a	
   neural	
   knowledge	
   language	
   model.	
   arxiv	
   2016.	
   
bahdanau	
   et	
   al.	
   learning	
   to	
   compute	
   word	
   embeddings	
   on	
   the	
   fly.	
   arxiv	
   2017.	
   

what does it mean to have language? 

direct	
   
percep/on	
   

collec2ve	
   
percep2on	
   

sources of 
collective, historical 
knowledge 
       some	
   structured,	
   some	
   unstructured	
   
external	
   knowledge	
   sources	
   

neural knowledge language modelling 
       (symbol,	
   fact)	
   pairs	
   from	
   
knowledge	
   base	
   
       neural	
   language	
   model	
   searches	
   
for	
   a	
   relevant	
   fact	
   
       if	
   deemed	
   appropriate,	
   use	
   the	
   
symbol	
   associated	
   with	
   the	
   
retrieved	
   fact	
   

(ahn	
   et	
   al.,	
   2016)	
   

dictionary-based id27s 

       di   cult	
   to	
   learn	
   about	
   rare	
   words	
   +	
   the	
   vocabulary	
   explodes	
   
       the	
   def   s	
   of	
   rare	
   words	
   are	
   o|en	
   available	
   in	
   a	
   dic/onary	
   
       the	
   dic/onary	
   de   ni/on	
   =>	
   rare	
   word	
   embedding	
   vector	
   

(bahdanau	
   et	
   al.,	
   2017)	
   

other advances in neural machine 
translation 

       discourse-     level	
   machine	
   transla/on	
   
       [jean	
   et	
   al.,	
   2017;	
   wang	
   et	
   al.,	
   2017]	
   

       beker	
   decoding	
   strategies	
   

a	
   monitor	
   at	
   the	
   summer	
   school	
   

       learning-     to-     search	
   [wiseman	
   &	
   rush,	
   2016]	
   
       reinforcement	
   learning	
   [shen	
   et	
   al.,	
   2016;	
   ranzato	
   et	
   al.,	
   2015;	
   bahdanau	
   et	
   al.,	
   2015]	
   
       trainable	
   decoding	
   [gu	
   et	
   al.,	
   2017ab]	
   
       alterna/ve	
   decoding	
   cost	
   [li	
   et	
   al.,	
   2016;	
   li	
   et	
   al.,	
   2017]	
   
       con/nuous	
   relaxa/on	
   [cohn	
   et	
   al.,	
   2016]	
   

       linguis/cs-     guided	
   neural	
   machine	
   transla/on	
   

       learning	
   to	
   parse	
   and	
   translate	
   	
   
[eriguchi	
   et	
   al.,	
   2017;	
   rohee	
   &	
   goldberg,	
   2017;	
   luong	
   et	
   al.,	
   2016]	
   
       syntax-     aware	
   neural	
   machine	
   transla/on	
   [nadejde	
   et	
   al.,	
   2017;	
   bas2ngs	
   et	
   al.,	
   2017]	
   

       see	
   chris	
   dyer   s	
   lecture	
   slides	
   from	
   this	
   morning	
   

task-driven linguistics 

understanding	
   what	
   neural	
   networks	
   have	
   learned	
   about	
   languages	
   

task-driven linguistics (1) 

?	
   

f = (la, croissance,   conomique, s'est, ralentie, ces, derni  res, ann  es, .)

corpora

       what	
   has	
   the	
   neural	
   net	
   learned	
   about	
   natural	
   languages?	
   
       can	
   it	
   tell	
   us	
   one	
   or	
   two	
   things	
   about	
   language	
   that	
   we	
   didn   t	
   know?	
   

e = (economic, growth, has, slowed, down, in, recent, years, .)

task-driven linguistics (2) 
       dependency	
   parser	
   (stanford	
   parser)	
   

       unsupervised	
   recurrent	
   language	
   model	
   (lee,	
   levy	
   &	
   zeklemoyer,	
   2017)	
   

task-driven linguistics (3) 

the	
   germanic	
   languages	
   (harbert,	
   2006)	
   

mul2lingual	
   recurrent	
   language	
   model	
   
(  stling	
   &	
   tiedemann,	
   2016)	
   
	
   

task-driven linguistics (4) 

pragma2cs	
   and	
   grounding	
   

communica2ng	
   neural	
   networks	
   
(lazaridou	
   et	
   al.,	
   2017;	
   jorge	
   et	
   al.,	
   2017;	
   ev&mova	
   et	
   al.;	
   
and	
   others)	
   
	
   

task-driven linguistics     proposal 
1.    build/update	
   a	
   end-     to-     end	
   trainable	
   system	
   for	
   a	
   task	
   
2.    train	
   the	
   system	
   for	
   the	
   task	
   using	
   (large)	
   data	
   
3.    analyze	
   the	
   trained	
   system	
   with	
   respect	
   to	
   known	
   facts/proper/es	
   
4.    extract	
   new	
   knowledge	
   out	
   of	
   the	
   trained	
   system.	
   
5.    go	
   back	
   to	
   1	
   

thank you! 

