learning structured predictors

xavier carreras

1/70

supervised (structured) prediction

(cid:73) learning to predict: given training data

(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y that works well on unseen inputs x

(cid:73) non-id170: outputs y are atomic

(cid:73) binary prediction: y     {   1, +1}
(cid:73) multiclass prediction: y     {1, 2, . . . , l}

(cid:73) id170: outputs y are structured

(cid:73) sequence prediction: y are sequences
(cid:73) parsing: y are trees
(cid:73) . . .

2/70

id39

y per
x

jim bought

-

qnt
300

-

shares

org

org
-
of acme corp.

-
in

time
2006

3/70

id39

y per
x

jim bought

-

qnt
300

-

shares

org

org
-
of acme corp.

-
in

time
2006

y per
x jack

per

-

london went

loc
-
to paris

per

y
x paris hilton went

-

per

-
to

loc

london

per

y
x jackie went

-

-
to

loc
lisdon

3/70

part-of-speech tagging

y nnp nnp vbz nnp .
x ms.
.

elianti

haag

plays

4/70

syntactic parsing

root

vc

p

loc

obj

nmod

sbj

tmp

nmod

pmod

name

(cid:63) unesco

is

now holding

its

biennial meetings

in new york

.

x are sentences

y are syntactic dependency trees

5/70

machine translation

(?)

x are sentences in chinese

6/70

rulesmaydescribethetransformationofdoesnotintone...pasinfrench.aparticularinstancemaylooklikethis:vp(aux(does),rb(not),x0:vb)   ne,x0,paslhs(ri)canbeanyarbitrarysyntaxtreefragment.itsleavesareeitherlexicalized(e.g.does)orvari-ables(x0,x1,etc).rhs(ri)isrepresentedasase-quenceoftarget-languagewordsandvariables.nowwegiveabriefoverviewofhowsuchtransformationalrulesareacquiredautomaticallyinghkm.1infigure1,the(  ,f,a)tripleisrep-resentedasadirectedgraphg(edgesgoingdown-ward),withnodistinctionbetweenedgesof  andalignments.eachnodeofthegraphislabeledwithitsspanandcomplementspan(thelatterinitalicinthe   gure).thespanofanodenisde   nedbytheindicesofthe   rstandlastwordinfthatarereachablefromn.thecomplementspanofnistheunionofthespansofallnodesn   ingthatareneitherdescendantsnorancestorsofn.nodesofgwhosespansandcomplementspansarenon-overlappingformthefrontiersetf   g.whatisparticularlyinterestingaboutthefron-tierset?foranyfrontierofgraphgcontainingagivennoden   f,spansonthatfrontierde-   neanorderingbetweennandeachotherfrontiernoden   .forexample,thespanofvp[4-5]eitherprecedesorfollows,butneveroverlapsthespanofanynoden   onanygraphfrontier.thispropertydoesnotholdfornodesoutsideoff.forinstance,pp[4-5]andvbg[4]aretwonodesofthesamegraphfrontier,buttheycannotbeorderedbecauseoftheiroverlappingspans.thepurposeofxrsrulesinthisframeworkistoorderconstituentsalongsensiblefrontiersing,andallfrontierscontainingunde   nedorderings,asbetweenpp[4-5]andvbg[4],mustbedisre-gardedduringruleextraction.toensurethatxrsrulesarepreventedfromattemptingtore-orderanysuchpairofconstituents,theserulesarede-signedinsuchawaythatvariablesintheirlhscanonlymatchnodesofthefrontierset.rulesthatsatisfythispropertyaresaidtobeinducedbyg.2forexample,rule(d)intable1isvalidaccord-ingtoghkm,sincethespanscorrespondingto1notethatweuseaslightlydifferentterminology.2speci   cally,anxrsruleriisextractedfromgbytakingasubtree       aslhs(ri),appendingavariabletoeachleafnodeof  thatisinternalto  ,addingthosevariablestorhs(ri),orderingtheminaccordancetoa,andifnecessaryinsertinganywordofftoensurethatrhs(ri)isasequenceofcontiguousspans(e.g.,[4-5][6][7-8]forrule(f)intable1).!"#!$%&''()'''&'&''($%*!""#$%&''()#"!'(*$&)!!"#$%"&"!"&"!"&##"&$%&!"'$&'!"&'!"&(!"%$("&)!")#%"*"&'&$%&!"'$&'&(!"%$+("&&&'%(!"%$("&$&'%(!"*$("&'&'%&!"*$&$&!%&!"#$&(#%)!!!"#$%&'()*+,""#$%$&$'&($)*+(,-$.%/0'*.,/%+'1)*230'140.*+$++5-figure1:spansandcomplement-spansdeterminewhatrulesareextracted.constituentsingrayaremembersofthefrontierset;aminimalruleisextractedfromeachofthem.(a)s(x0:np,x1:vp,x2:.)   x0,x1,x2(b)np(x0:dt,cd(7),nns(people))   x0,7(c)dt(these)   (d)vp(x0:vbp,x1:np)   x0,x1(e)vbp(include)   (f)np(x0:np,x1:vp)   x1,,x0(g)np(x0:nns)   x0(h)nns(astronauts)   ,(i)vp(vbg(coming),pp(in(from),x0:np))   ,x0(j)np(x0:nnp)   x0(k)nnp(france)   (l).(.)   .table1:aminimalderivationcorrespondingtofigure1.itsrhsconstituents(vbp[3]andnp[4-8])donotoverlap.conversely,np(x0:dt,x1:cd:,x2:nns)isnotthelhsofanyruleextractiblefromg,sinceitsfrontierconstituentscd[2]andnns[2]haveoverlappingspans.3finally,theghkmproce-dureproducesasinglederivationfromg,whichisshownintable1.theconcerninghkmwastoextractminimalrules,whereasoursistoextractrulesofanyarbi-trarysize.minimalrulesde   nedovergarethosethatcannotbedecomposedintosimplerrulesin-ducedbythesamegraphg,e.g.,allrulesinta-ble1.wecallminimaladerivationthatonlycon-tainsminimalrules.conversely,acomposedruleresultsfromthecompositionoftwoormoremin-imalrules,e.g.,rule(b)and(c)composeinto:np(dt(these),cd(7),nns(people))   ,73itisgenerallyreasonabletoalsorequirethattherootnoflhs(ri)bepartoff,becausenoruleinducedbygcancomposewithriatn,duetotherestrictionsimposedontheextractionprocedure,andriwouldn   tbepartofanyvalidderivation.id164

(?)

x are images

y are grids labeled with object types

7/70

id164

(?)

x are images

y are grids labeled with object types

7/70

today   s goals

(cid:73) introduce basic concepts for id170

(cid:73) we will restrict to sequence prediction

(cid:73) what can we can borrow from standard classi   cation?

(cid:73) learning paradigms and algorithms, in essence, work here too
(cid:73) however, computations behind algorithms are prohibitive

(cid:73) what can we borrow from id48 and other structured formalisms?

(cid:73) representations of structured data into feature spaces
(cid:73) id136/search algorithms for tractable computations
(cid:73) e.g., algorithms for id48s (viterbi, forward-backward) will play a major role in today   s

methods

8/70

today   s goals

(cid:73) introduce basic concepts for id170

(cid:73) we will restrict to sequence prediction

(cid:73) what can we can borrow from standard classi   cation?

(cid:73) learning paradigms and algorithms, in essence, work here too
(cid:73) however, computations behind algorithms are prohibitive

(cid:73) what can we borrow from id48 and other structured formalisms?

(cid:73) representations of structured data into feature spaces
(cid:73) id136/search algorithms for tractable computations
(cid:73) e.g., algorithms for id48s (viterbi, forward-backward) will play a major role in today   s

methods

8/70

sequence prediction

y
x

per
jack

per

-

london went

loc
-
to paris

9/70

sequence prediction

(cid:73) x = x1x2 . . . xn are input sequences, xi     x
(cid:73) y = y1y2 . . . yn are output sequences, yi     {1, . . . , l}

(cid:73) goal: given training data

(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y that works well on unseen inputs x

(cid:73) what is the form of our prediction model?

10/70

exponentially-many solutions

(cid:73) let y = {-, per, loc}

(cid:73) the solution space (all output sequences):

jack

london

went

-

per

loc

-

per

loc

-

per

loc

to

-

per

loc

paris

-

per

loc

(cid:73) each path is a possible solution

(cid:73) for an input sequence of size n, there are |y|n possible outputs

11/70

exponentially-many solutions

(cid:73) let y = {-, per, loc}

(cid:73) the solution space (all output sequences):

jack

london

went

-

per

loc

-

per

loc

-

per

loc

to

-

per

loc

paris

-

per

loc

(cid:73) each path is a possible solution

(cid:73) for an input sequence of size n, there are |y|n possible outputs

11/70

approach 1: local classi   ers

?

jack

london went

to paris

decompose the sequence into n classi   cation problems:

(cid:73) a classi   er predicts individual labels at each position

  yi =

argmax

l     {loc, per, -}

w    f (x, i, l)

(cid:73) f (x, i, l) represents an assignment of label l for xi
(cid:73) w is a vector of parameters, has a weight for each feature of f

(cid:73) use standard classi   cation methods to learn w

(cid:73) at test time, predict the best sequence by

a simple concatenation of the best label for each position

12/70

approach 1: local classi   ers

?

jack

london went

to paris

decompose the sequence into n classi   cation problems:

(cid:73) a classi   er predicts individual labels at each position

  yi =

argmax

l     {loc, per, -}

w    f (x, i, l)

(cid:73) f (x, i, l) represents an assignment of label l for xi
(cid:73) w is a vector of parameters, has a weight for each feature of f

(cid:73) use standard classi   cation methods to learn w

(cid:73) at test time, predict the best sequence by

a simple concatenation of the best label for each position

12/70

indicator features

(cid:73) f (x, i, l) is a vector of d features representing label l for xi

[ f1(x, i, l), . . . , fj(x, i, l), . . . , fd(x, i, l) ]

(cid:73) what   s in a feature fj(x, i, l)?

(cid:73) anything we can compute using x and i and l
(cid:73) anything that indicates whether l is (not) a good label for xi
(cid:73) indicator features: binary-valued features looking at:

(cid:73) a simple pattern of x and target position i
(cid:73) and the candidate label l for position i

fj(x, i, l) =(cid:26) 1
fk(x, i, l) =(cid:26) 1

0

0

if xi =london and l =loc
otherwise
if xi+1 =went and l =loc
otherwise

13/70

feature templates

(cid:73) feature templates generate many indicator features mechanically
(cid:73) a feature template is identi   ed by a type, and a number of values

(cid:73) example: template word extracts the current word

f(cid:104)word,a,w(cid:105)(x, i, l) =(cid:26) 1

0

if xi = w and l = a
otherwise

(cid:73) a feature of this type is identi   ed by the tuple (cid:104)word, a, w(cid:105)
(cid:73) generates a feature for every label a     y and every word w

e.g.: a = loc w = london,

a = loc w = paris
a = per w = john

w = london

a = -
a = per w = paris
a = -
w = the

14/70

feature templates

(cid:73) feature templates generate many indicator features mechanically
(cid:73) a feature template is identi   ed by a type, and a number of values

(cid:73) example: template word extracts the current word

f(cid:104)word,a,w(cid:105)(x, i, l) =(cid:26) 1

0

if xi = w and l = a
otherwise

(cid:73) a feature of this type is identi   ed by the tuple (cid:104)word, a, w(cid:105)
(cid:73) generates a feature for every label a     y and every word w

e.g.: a = loc w = london,

a = loc w = paris
a = per w = john

w = london

a = -
a = per w = paris
a = -
w = the

(cid:73) in feature-based models:

(cid:73) de   ne feature templates manually
(cid:73) instantiate the templates on every set of values in the training data

    generates a very high-dimensional feature space

(cid:73) de   ne parameter vector w indexed by such feature tuples
(cid:73) let the learning algorithm choose the relevant features

14/70

more features for ne recognition

per

jack

london went

to paris

in practice, construct f (x, i, l) by . . .

(cid:73) de   ne a number of simple patterns of x and i

(cid:73) current word xi
(cid:73) is xi capitalized?
(cid:73) xi has digits?
(cid:73) pre   xes/su   xes of size 1, 2, 3, . . .
(cid:73) is xi a known location?
(cid:73) is xi a known person?

(cid:73) next word
(cid:73) previous word
(cid:73) current and next words together
(cid:73) other combinations

(cid:73) de   ne feature templates by combining patterns with labels l
(cid:73) generate actual features by instantiating templates on training data

15/70

more features for ne recognition

per
jack

per

-

london went

to paris

in practice, construct f (x, i, l) by . . .

(cid:73) de   ne a number of simple patterns of x and i

(cid:73) current word xi
(cid:73) is xi capitalized?
(cid:73) xi has digits?
(cid:73) pre   xes/su   xes of size 1, 2, 3, . . .
(cid:73) is xi a known location?
(cid:73) is xi a known person?

(cid:73) next word
(cid:73) previous word
(cid:73) current and next words together
(cid:73) other combinations

(cid:73) de   ne feature templates by combining patterns with labels l
(cid:73) generate actual features by instantiating templates on training data

main limitation: features can   t capture interactions between labels!

15/70

approach 2: id48 for sequence prediction

  per

per

tper,per

per

-

oper, london

jack

london

went

-

to

loc

paris

(cid:73) de   ne an id48 were each label is a state
(cid:73) model parameters:

(cid:73)   l : id203 of starting with label l
(cid:73) tl,l(cid:48): id203 of transitioning from l to l(cid:48)
(cid:73) ol,x: id203 of generating symbol x given label l

(cid:73) predictions:

p(x, y) =   y1oy1,x1(cid:89)i>1

tyi   1,yioyi,xi

(cid:73) learning: relative counts + smoothing
(cid:73) prediction: viterbi algorithm

16/70

approach 2: representation in id48

  per

per

tper,per

per

-

oper, london

jack

london

went

-

to

loc

paris

(cid:73) label interactions are captured in the transition parameters
(cid:73) but interactions between labels and input symbols are quite limited!

(cid:73) only oyi,xi = p(xi | yi)
(cid:73) not clear how to exploit patterns such as:
(cid:73) capitalization, digits
(cid:73) pre   xes and su   xes
(cid:73) next word, previous word
(cid:73) combinations of these with label transitions

(cid:73) why? id48 independence assumptions:

given label yi, token xi is independent of anything else

17/70

approach 2: representation in id48

  per

per

tper,per

per

-

oper, london

jack

london

went

-

to

loc

paris

(cid:73) label interactions are captured in the transition parameters
(cid:73) but interactions between labels and input symbols are quite limited!

(cid:73) only oyi,xi = p(xi | yi)
(cid:73) not clear how to exploit patterns such as:
(cid:73) capitalization, digits
(cid:73) pre   xes and su   xes
(cid:73) next word, previous word
(cid:73) combinations of these with label transitions

(cid:73) why? id48 independence assumptions:

given label yi, token xi is independent of anything else

17/70

local classi   ers vs. id48

local classifiers

id48

(cid:73) form:

(cid:73) form:

w    f (x, i, l)

(cid:73) learning: standard classi   ers
(cid:73) prediction: independent for each xi
(cid:73) advantage: feature-rich
(cid:73) drawback: no label interactions

  y1oy1,x1(cid:89)i>1

tyi   1,yioyi,xi

(cid:73) learning: relative counts
(cid:73) prediction: viterbi
(cid:73) advantage: label interactions
(cid:73) drawback: no    ne-grained features

18/70

approach 3: global sequence predictors

y:
x:

per
jack

per

-

london went

loc
-
to paris

learn a single classi   er from x     y

predict(x1:n) = argmax
y   y n

w    f (x, y)

next questions: . . .

(cid:73) how do we represent entire sequences in f (x, y)?
(cid:73) there are exponentially-many sequences y for a given x,

how do we solve the argmax problem?

19/70

approach 3: global sequence predictors

y:
x:

per
jack

per

-

london went

loc
-
to paris

learn a single classi   er from x     y

predict(x1:n) = argmax
y   y n

w    f (x, y)

next questions: . . .

(cid:73) how do we represent entire sequences in f (x, y)?
(cid:73) there are exponentially-many sequences y for a given x,

how do we solve the argmax problem?

19/70

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

20/70

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

20/70

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

20/70

factored representations

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) how do we represent entire sequences in f (x, y)?

(cid:73) look at individual assignments yi (standard classi   cation)
(cid:73) look at bigrams of outputs labels (cid:104)yi   1, yi(cid:105)
(cid:73) look at trigrams of outputs labels (cid:104)yi   2, yi   1, yi(cid:105)
(cid:73) look at id165s of outputs labels (cid:104)yi   n+1, . . . , yi   1, yi(cid:105)
(cid:73) look at the full label sequence y (intractable)

(cid:73) a factored representation will lead to a tractable model

20/70

bigram feature templates

1
y per
x jack

2

per

3
-

london went

5

4
loc
-
to paris

(cid:73) a template for word + bigram:

f(cid:104)wb,a,b,w(cid:105)(x, i, yi   1, yi) =         

1 if xi = w and

yi   1 = a and yi = b

0 otherwise

e.g., f(cid:104)wb,per,per,london(cid:105)(x, 2, per, per) = 1

f(cid:104)wb,per,per,london(cid:105)(x, 3, per, -) = 0
f(cid:104)wb,per,-,went(cid:105)(x, 3, per, -) = 1

21/70

more templates for ner

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f(cid:104)w,per,per,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f(cid:104)w,per,loc,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f(cid:104)prep,loc,to(cid:105)(. . .) = 1 i    xi   1 =   to    and xi    /[a-z]/ and yi = loc
f(cid:104)city,loc(cid:105)(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f(cid:104)fname,per(cid:105)(. . .) = 1 i    yi = per and first-names(xi) = 1

22/70

more templates for ner

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f(cid:104)w,per,per,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f(cid:104)w,per,loc,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f(cid:104)prep,loc,to(cid:105)(. . .) = 1 i    xi   1 =   to    and xi    /[a-z]/ and yi = loc
f(cid:104)city,loc(cid:105)(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f(cid:104)fname,per(cid:105)(. . .) = 1 i    yi = per and first-names(xi) = 1

22/70

more templates for ner

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f(cid:104)w,per,per,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f(cid:104)w,per,loc,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f(cid:104)prep,loc,to(cid:105)(. . .) = 1 i    xi   1 =   to    and xi    /[a-z]/ and yi = loc
f(cid:104)city,loc(cid:105)(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f(cid:104)fname,per(cid:105)(. . .) = 1 i    yi = per and first-names(xi) = 1

22/70

more templates for ner

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f(cid:104)w,per,per,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f(cid:104)w,per,loc,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f(cid:104)prep,loc,to(cid:105)(. . .) = 1 i    xi   1 =   to    and xi    /[a-z]/ and yi = loc
f(cid:104)city,loc(cid:105)(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f(cid:104)fname,per(cid:105)(. . .) = 1 i    yi = per and first-names(xi) = 1

22/70

more templates for ner

1

jack
per
per

-
my

x
y
y(cid:48)
y(cid:48)(cid:48)
x(cid:48)

2

3

london went

per
loc

-

trip

-
-
-
to

4
to
-
-

loc

london

5

paris
loc
loc

-
. . .

f(cid:104)w,per,per,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = per

f(cid:104)w,per,loc,london(cid:105)(. . .) = 1 i    xi =   london    and yi   1 = per and yi = loc

f(cid:104)prep,loc,to(cid:105)(. . .) = 1 i    xi   1 =   to    and xi    /[a-z]/ and yi = loc
f(cid:104)city,loc(cid:105)(. . .) = 1 i    yi = loc and world-cities(xi) = 1

f(cid:104)fname,per(cid:105)(. . .) = 1 i    yi = per and first-names(xi) = 1

22/70

representations factored at bigrams

y:
x:

per
jack

per

-

london went

loc
-
to paris

(cid:73) f (x, i, yi   1, yi)

(cid:73) a d-dimensional feature vector of a label bigram at i
(cid:73) each dimension is typically a boolean indicator (0 or 1)

(cid:73) f (x, y) =(cid:80)n

i=1 f (x, i, yi   1, yi)

(cid:73) a d-dimensional feature vector of the entire y
(cid:73) aggregated representation by summing bigram feature vectors
(cid:73) each dimension is now a count of a feature pattern

23/70

linear sequence prediction

where

predict(x1:n) = argmax
y   y n
f (x, i, yi   1, yi)

f (x, y) =

w    f (x, y)

n(cid:88)i=1

(cid:73) note the linearity of the expression:

w    f (x, y) = w   

=

n(cid:88)i=1

(cid:73) next questions:

(cid:73) how do we solve the argmax problem?
(cid:73) how do we learn w?

n(cid:88)i=1

f (x, i, yi   1, yi)

w    f (x, i, yi   1, yi)

24/70

linear sequence prediction

where

predict(x1:n) = argmax
y   y n
f (x, i, yi   1, yi)

f (x, y) =

w    f (x, y)

n(cid:88)i=1

(cid:73) note the linearity of the expression:

w    f (x, y) = w   

=

n(cid:88)i=1

(cid:73) next questions:

(cid:73) how do we solve the argmax problem?
(cid:73) how do we learn w?

n(cid:88)i=1

f (x, i, yi   1, yi)

w    f (x, i, yi   1, yi)

24/70

linear sequence prediction

where

predict(x1:n) = argmax
y   y n
f (x, i, yi   1, yi)

f (x, y) =

w    f (x, y)

n(cid:88)i=1

(cid:73) note the linearity of the expression:

w    f (x, y) = w   

=

n(cid:88)i=1

(cid:73) next questions:

(cid:73) how do we solve the argmax problem?
(cid:73) how do we learn w?

n(cid:88)i=1

f (x, i, yi   1, yi)

w    f (x, i, yi   1, yi)

24/70

predicting with factored sequence models

(cid:73) consider a    xed w. given x1:n    nd:

argmax
y   y n

n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) use the viterbi algorithm, takes o(n|y|2)

(cid:73) notational change: since w and x1:n are    xed we will use

s(i, a, b) = w    f (x, i, a, b)

25/70

viterbi for factored sequence models

(cid:73) given scores s(i, a, b) for each position i and output bigram a, b,    nd:

argmax
y   y n

n(cid:88)i=1

s(i, yi   1, yi)

(cid:73) use the viterbi algorithm, takes o(n|y|2)

(cid:73) intuition: output sequences that share bigrams will share scores

1
best subsequence with yi   1 = per

i     2

. . .

i     1

best subsequence with yi   1 = loc

c ,

o

l

,

i

(

s

i

i + 1 . . .

n
best subsequence with yi = per

r

)

e

p

best subsequence with yi = loc

best subsequence with yi   1 =    

best subsequence with yi =    

26/70

intuition for viterbi

(cid:73) assume we have the best sub-sequence up to position i     1 ending with each label:

i     1
1
best subsequence with yi   1 = per

. . .

best subsequence with yi   1 = loc

best subsequence with yi   1 =    

(cid:73) what is the best sequence up to position i with yi =loc?

i

27/70

intuition for viterbi

(cid:73) assume we have the best sub-sequence up to position i     1 ending with each label:

i     1
1
best subsequence with yi   1 = per

. . .

best subsequence with yi   1 = loc

best subsequence with yi   1 =    

(cid:73) what is the best sequence up to position i with yi =loc?

i

27/70

intuition for viterbi

(cid:73) assume we have the best sub-sequence up to position i     1 ending with each label:

i     1
1
best subsequence with yi   1 = per

. . .

best subsequence with yi   1 = loc

best subsequence with yi   1 =    

i

s(i,per,loc

)

s(i,loc, loc)

l o c

)

,

   

,

i

(

s

(cid:73) what is the best sequence up to position i with yi =loc?

27/70

viterbi for linear factored predictors

  y = argmax

y   y n

n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) de   nition: score of optimal sequence for x1:i ending with a     y

  (i, a) = max

y   y i:yi=a

s(j, yj   1, yj)

i(cid:88)j=1

(cid:73) use the following recursions, for all a     y:

  (1, a) = s(1, y0 = null, a)
  (i, a) = max
b   y

  (i     1, b) + s(i, b, a)

(cid:73) the optimal score for x is maxa   y   (n, a)
(cid:73) the optimal sequence   y can be recovered through back-pointers

28/70

linear factored sequence prediction

predict(x1:n) = argmax
y   y n

w    f (x, y)

(cid:73) factored representation, e.g. based on bigrams
(cid:73) flexible, arbitrary features of full x and the factors
(cid:73) e   cient prediction using viterbi
(cid:73) next, learning w:

(cid:73) probabilistic id148:

(cid:73) local learning, a.k.a. maximum-id178 markov models
(cid:73) global learning, a.k.a. id49

(cid:73) margin-based methods:

(cid:73) structured id88
(cid:73) structured id166

29/70

the learner   s game

training data

weight vector w

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

30/70

the learner   s game

training data

weight vector w

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

w(cid:104)lower,-(cid:105) = +1

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

30/70

the learner   s game

training data

weight vector w

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

w(cid:104)lower,-(cid:105) = +1
w(cid:104)upper,per(cid:105) = +1

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

30/70

the learner   s game

training data

weight vector w

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

w(cid:104)lower,-(cid:105) = +1
((((((((
w(cid:104)upper,per(cid:105) = +1
w(cid:104)upper,loc(cid:105) = +1

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

30/70

the learner   s game

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

training data

weight vector w

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

w(cid:104)lower,-(cid:105) = +1
((((((((
w(cid:104)upper,per(cid:105) = +1
w(cid:104)upper,loc(cid:105) = +1
w(cid:104)word,per,maria(cid:105) = +2

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

30/70

the learner   s game

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

training data

weight vector w

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

w(cid:104)lower,-(cid:105) = +1
((((((((
w(cid:104)upper,per(cid:105) = +1
w(cid:104)upper,loc(cid:105) = +1
w(cid:104)word,per,maria(cid:105) = +2
w(cid:104)word,per,jack(cid:105) = +2

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

30/70

the learner   s game

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

training data

weight vector w

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

w(cid:104)lower,-(cid:105) = +1
((((((((
w(cid:104)upper,per(cid:105) = +1
w(cid:104)upper,loc(cid:105) = +1
w(cid:104)word,per,maria(cid:105) = +2
w(cid:104)word,per,jack(cid:105) = +2
w(cid:104)nextw,per,went(cid:105) = +2

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

30/70

the learner   s game

training data

weight vector w

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

w(cid:104)lower,-(cid:105) = +1
((((((((
w(cid:104)upper,per(cid:105) = +1
w(cid:104)upper,loc(cid:105) = +1
w(cid:104)word,per,maria(cid:105) = +2
w(cid:104)word,per,jack(cid:105) = +2
w(cid:104)nextw,per,went(cid:105) = +2
w(cid:104)nextw,org,played(cid:105) = +2

argentina

played

against germany

30/70

the learner   s game

training data

weight vector w

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

w(cid:104)lower,-(cid:105) = +1
((((((((
w(cid:104)upper,per(cid:105) = +1
w(cid:104)upper,loc(cid:105) = +1
w(cid:104)word,per,maria(cid:105) = +2
w(cid:104)word,per,jack(cid:105) = +2
w(cid:104)nextw,per,went(cid:105) = +2
w(cid:104)nextw,org,played(cid:105) = +2
w(cid:104)prevw,org,against(cid:105) = +2

argentina

played

against germany

30/70

the learner   s game

training data

weight vector w

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

(cid:73)

per
maria

loc

lisbon

-
is

-
is

-

beautiful

-

beautiful

per
jack went

-

-
to

loc

lisbon

loc

argentina

-
is

-

nice

-

per

per
jack

london went

org

-

-
to

-

loc
south

loc
paris

org

argentina

played

against germany

w(cid:104)lower,-(cid:105) = +1
((((((((
w(cid:104)upper,per(cid:105) = +1
w(cid:104)upper,loc(cid:105) = +1
w(cid:104)word,per,maria(cid:105) = +2
w(cid:104)word,per,jack(cid:105) = +2
w(cid:104)nextw,per,went(cid:105) = +2
w(cid:104)nextw,org,played(cid:105) = +2
w(cid:104)prevw,org,against(cid:105) = +2
. . .
w(cid:104)upperbigram,per,per(cid:105) = +2
w(cid:104)upperbigram,loc,loc(cid:105) = +2
w(cid:104)nextw,loc,played(cid:105) =    1000

30/70

id148

for sequence prediction

y
x

per
jack

per

-

london went

loc
-
to paris

31/70

id148 for sequence prediction

(cid:73) model the conditional distribution:

pr(y | x; w) =

exp{w    f (x, y)}

z(x; w)

where

   
(cid:73) x = x1x2 . . . xn     x
    and y = {1, . . . , l}
(cid:73) y = y1y2 . . . yn     y
(cid:73) f (x, y) represents x and y with d features
(cid:73) w     rd are the parameters of the model
(cid:73) z(x; w) is a normalizer called the partition function
z(x; w) = (cid:88)z   y   

exp{w    f (x, z)}

(cid:73) to predict the best sequence

predict(x1:n) = argmax
y   y n

pr(y|x)

32/70

id148: name

(cid:73) let   s take the log of the id155:

log pr(y | x; w) = log

exp{w    f (x, y)}

z(x; w)

= w    f (x, y)     log(cid:88)y

= w    f (x, y)     log z(x; w)

exp{w    f (x, y)}

(cid:73) partition function: z(x; w) =(cid:80)y exp{w    f (x, y)}

(cid:73) log z(x; w) is a constant for a    xed x
(cid:73) in the log space, computations are linear,

i.e., we model log-probabilities using a linear predictor

33/70

making predictions with id148
(cid:73) for tractability, assume f (x, y) decomposes into bigrams:

f (x1:n, y1:n) =

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) given w, given x1:n,    nd:

argmax

y1:n

pr(y1:n|x1:n; w) = amax

y

= amax

y

= amax

y

exp{(cid:80)n
exp(cid:40) n(cid:88)i=1
n(cid:88)i=1

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

w    f (x, i, yi   1, yi)(cid:41)

w    f (x, i, yi   1, yi)

(cid:73) we can use the viterbi algorithm

34/70

making predictions with id148
(cid:73) for tractability, assume f (x, y) decomposes into bigrams:

f (x1:n, y1:n) =

n(cid:88)i=1

f (x, i, yi   1, yi)

(cid:73) given w, given x1:n,    nd:

argmax

y1:n

pr(y1:n|x1:n; w) = amax

y

= amax

y

= amax

y

exp{(cid:80)n
exp(cid:40) n(cid:88)i=1
n(cid:88)i=1

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

w    f (x, i, yi   1, yi)(cid:41)

w    f (x, i, yi   1, yi)

(cid:73) we can use the viterbi algorithm

34/70

parameter estimation in id148

pr(y | x; w) =

exp{w    f (x, y)}

z(x; w)

(cid:73) given training data

(cid:110)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:111) ,

(cid:73) how to estimate w?
(cid:73) de   ne the conditional log-likelihood of the data:

l(w) =(cid:88)a

k = 1m log pr(y(k)|x(k); w)

(cid:73) l(w) measures how well w explains the data. a good value for w will give a high

value for pr(y(k)|x(k); w) for all k = 1 . . . m.

(cid:73) we want w that maximizes l(w)

35/70

parameter estimation in id148

pr(y | x; w) =

exp{w    f (x, y)}

z(x; w)

(cid:73) given training data

(cid:110)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:111) ,

(cid:73) how to estimate w?
(cid:73) de   ne the conditional log-likelihood of the data:

l(w) =(cid:88)a

k = 1m log pr(y(k)|x(k); w)

(cid:73) l(w) measures how well w explains the data. a good value for w will give a high

value for pr(y(k)|x(k); w) for all k = 1 . . . m.

(cid:73) we want w that maximizes l(w)

35/70

learning id148: loss + id173

(cid:73) solve:

w    = argmin
w   rd

loss

(cid:122) (cid:125)(cid:124) (cid:123)

   l(w) +

id173

(cid:122) (cid:125)(cid:124) (cid:123)
  
2||w||2

where

(cid:73) the    rst term is the negative conditional log-likelihood
(cid:73) the second term is a id173 term, it penalizes solutions with large norm
(cid:73)        r controls the trade-o    between loss and id173

(cid:73) id76 problem     id119
(cid:73) two common losses based on log-likelihood that make learning tractable:

(cid:73) local loss (memm): assume that pr(y | x; w) decomposes
(cid:73) global loss (crf): assume that f (x, y) decomposes

36/70

learning id148: loss + id173

(cid:73) solve:

w    = argmin
w   rd

loss

(cid:122) (cid:125)(cid:124) (cid:123)

   l(w) +

id173

(cid:122) (cid:125)(cid:124) (cid:123)
  
2||w||2

where

(cid:73) the    rst term is the negative conditional log-likelihood
(cid:73) the second term is a id173 term, it penalizes solutions with large norm
(cid:73)        r controls the trade-o    between loss and id173

(cid:73) id76 problem     id119
(cid:73) two common losses based on log-likelihood that make learning tractable:

(cid:73) local loss (memm): assume that pr(y | x; w) decomposes
(cid:73) global loss (crf): assume that f (x, y) decomposes

36/70

maximum id178 markov models (memm)
?

(cid:73) similarly to id48s:

pr(y1:n | x1:n) = pr(y1 | x1:n)    pr(y2:n | x1:n, y1)

= pr(y1 | x1:n)   

pr(yi|x1:n, y1:i   1)

= pr(y1|x1:n)   

pr(yi|x1:n, yi   1)

n(cid:89)i=2
n(cid:89)i=2

(cid:73) assumption under memms:

pr(yi|x1:n, y1:i   1) = pr(yi|x1:n, yi   1)

37/70

parameter estimation in memm

pr(y1:n | x1:n) = pr(y1 | x1:n)   

n(cid:89)i=2

pr(yi|x1:n, i, yi   1)

(cid:73) the log-linear model is normalized locally (i.e. at each position):

pr(y | x, i, y

(cid:48)

) =

(cid:73) the log-likelihood is also local :

exp{w    f (x, i, y(cid:48), y)}

z(x, i, y(cid:48))

l(w) =

m(cid:88)k=1
(cid:125)(cid:124)

observed

fj(x(k), i, y(k)

i   1, y(k)

i

            
(cid:122)

n(k)(cid:88)i=1

log pr(y(k)

i

|x(k), i, y(k)
i   1)

expected

(cid:123)

)   

(cid:122)
(cid:88)y   y

(cid:125)(cid:124)
pr(y|x(k), i, y(k)

i   1, y) fj(x(k), i, y(k)

i   1, y)            38/70
(cid:123)

   l(w)

   wj

=

1
m

m(cid:88)k=1

n(k)(cid:88)i=1

id49
(?)

(cid:73) log-linear model of the conditional distribution:

pr(y|x; w) =

exp{w    f (x, y)}

z(x)

where

(cid:73) x and y are input and output sequences
(cid:73) f (x, y) is a feature vector of x and y that decomposes into factors
(cid:73) w are model parameters

(cid:73) to predict the best sequence

  y = argmax

   

y   y

pr(y|x)

(cid:73) log-likelihood at the global (sequence) level:

l(w) =

m(cid:88)k=1

log pr(y(k)|x(k); w)

39/70

computing the gradient in crfs

consider a parameter wj and its associated feature fj:

   l(w)
   wj

=

1
m

m(cid:88)k=1

where

observed

(cid:125)(cid:124)

            
(cid:122)
(cid:123)
fj(x(k), y(k))   
n(cid:88)i=1

fj(x, y) =

(cid:122)
(cid:88)y   y

   

expected

pr(y|x(k); w) fj(x(k), y)            
(cid:123)

(cid:125)(cid:124)

fj(x, i, yi   1, yi)

(cid:73) first term: observed value of fj in training examples
(cid:73) second term: expected value of fj under current w
(cid:73) in the optimal, observed = expected

40/70

computing the gradient in crfs

(cid:73) the    rst term is easy to compute, by counting explicitly

(cid:88)i

(cid:73) the second term is more involved,

fj(x, i, y(k)

i   1, y(k)

i

)

pr(y|x(k); w)(cid:88)i
because it sums over all sequences y     y n

(cid:88)y   y

   

fj(x(k), i, yi   1, yi)

(cid:73) but there is an e   cient solution . . .

41/70

computing the gradient in crfs

(cid:73) for an example (x(k), y(k)):

(cid:88)y   y n

pr(y|x(k); w)

n(cid:88)i=1

fj(x(k), i, yi   1, yi) =

n(cid:88)i=1 (cid:88)a,b   y

  k
i (a, b)fj(x(k), i, a, b)

(cid:73)   k

i (a, b) is the marginal id203 of having labels (a, b) at position i:

i (a, b) = pr((cid:104)i, a, b(cid:105) | x(k); w) =
  k

(cid:88)y   y n : yi   1=a, yi=b

pr(y|x(k); w)

(cid:73) the quantities   k

i can be computed e   ciently in o(nl2) using the forward-backward

algorithm

42/70

forward-backward for crfs

(cid:73) assume    xed x and w.
(cid:73) for notational convenience, de   ne the score of a label bigram as:

s(i, a, b) = exp{w    f (x, i, a, b)}

such that we can write

pr(y | x) =

exp{w    f (x, y)}

z(x)

=

exp{(cid:80)n

i=1 w    f (x, i, yi   1, yi)}

z(x)

(cid:73) normalizer: z =(cid:80)y(cid:81)n

(cid:73) marginals:   (i, a, b) = 1

i=1 s(i, yi   1, yi)

z(cid:80)y,s.t.yi   1=a,yi=b(cid:81)n

i=1 s(i, yi   1, yi)

= (cid:81)n

i=1 s(i, yi   1, yi)

z

43/70

forward-backward for crfs

(cid:73) de   nition: forward and backward quantities

  i(a) = (cid:88)y1:i   y i:yi=a(cid:81)i

j=1 s(j, yj   1, yj)

  i(b) =

(cid:88)yi:n   y (n   i+1):yi=b(cid:81)n
(cid:73) z =(cid:80)a   n(a)
(cid:73)   i(a, b) = {  i   1(a)     s(i, a, b)}       i(b)     z   1}
(cid:73) similarly to viterbi,   i(a) and   i(b) can be computed recursively in o(n|y|2)

j=i+1 s(j, yj   1, yj)

44/70

forward-backward for crfs

(cid:73) de   nition: forward and backward quantities

  i(a) = (cid:88)y1:i   y i:yi=a(cid:81)i

j=1 s(j, yj   1, yj)

  i(b) =

(cid:88)yi:n   y (n   i+1):yi=b(cid:81)n
(cid:73) z =(cid:80)a   n(a)
(cid:73)   i(a, b) = {  i   1(a)     s(i, a, b)}       i(b)     z   1}
(cid:73) similarly to viterbi,   i(a) and   i(b) can be computed recursively in o(n|y|2)

j=i+1 s(j, yj   1, yj)

44/70

crfs: summary so far

(cid:73) id148 for sequence prediction, pr(y|x; w)
(cid:73) computations factorize on label bigrams
(cid:73) model form:

argmax

y   y

    (cid:88)i

w    f (x, i, yi   1, yi)

(cid:73) prediction: uses viterbi (from id48s)
(cid:73) parameter estimation:

(cid:73) gradient-based methods, in practice l-bfgs
(cid:73) computation of gradient uses forward-backward (from id48s)

45/70

crfs: summary so far

(cid:73) id148 for sequence prediction, pr(y|x; w)
(cid:73) computations factorize on label bigrams
(cid:73) model form:

argmax

y   y

    (cid:88)i

w    f (x, i, yi   1, yi)

(cid:73) prediction: uses viterbi (from id48s)
(cid:73) parameter estimation:

(cid:73) gradient-based methods, in practice l-bfgs
(cid:73) computation of gradient uses forward-backward (from id48s)

(cid:73) next question: memms or crfs? id48s or crfs?

45/70

memms and crfs

memms: pr(y | x) =

exp{w    f (x, i, yi   1, yi)}

z(x, i, yi   1; w)

crfs: pr(y | x) =

i=1 w    f (x, i, yi   1, yi)}

z(x)

n(cid:89)i=1
exp{(cid:80)n

(cid:73) both exploit the same factorization, i.e. same features
(cid:73) same computations to compute argmaxy pr(y | x)
(cid:73) memms locally normalized; crfs globally normalized

(cid:73) memm assume that pr(yi | x1:n, y1:i   1) = pr(yi | x1:n, yi   1)
(cid:73) leads to    label bias problem   
(cid:73) memms are cheaper to train (reduces to multiclass learning)
(cid:73) crfs are easier to extend to other structures (next lecture)

46/70

id48s for sequence prediction

(cid:73) x are the observations, y are the hidden states
(cid:73) id48s model the joint distributon pr(x, y)
(cid:73) parameters: (assume x = {1, . . . , k} and y = {1, . . . , l})

(cid:73)        rl,   a = pr(y1 = a)
(cid:73) t     rl  l, ta,b = pr(yi = b|yi   1 = a)
(cid:73) o     rl  k, oa,c = pr(xi = c|yi = a)

(cid:73) model form

pr(x, y) =   y1oy1,x1

tyi   1,yioyi,xi

n(cid:89)i=2

(cid:73) parameter estimation: maximum likelihood by counting events and normalizing

47/70

id48s and crfs

(cid:73) in id48s:

(cid:73) in crfs:   y = amaxy(cid:80)i w    f (x, i, yi   1, yi)
  y = amaxy   y1oy1,x1(cid:81)n
= amaxy log(  y1oy1,x1) +(cid:80)n

i=2 tyi   1,yioyi,xi

i=2 log(tyi   1,yioyi,xi)

(cid:73) an id48 can be expressed as factored linear models:

fj(x, i, y, y(cid:48))
i = 1 & y(cid:48) = a

i > 1 & y = a & y(cid:48) = b

y(cid:48) = a & xi = c

wj

log(  a)
log(ta,b)
log(oa,b)

(cid:73) hence, id48 are factored linear models

48/70

id48s and crfs: main di   erences

(cid:73) representation:

(cid:73) id48    features    are tied to the generative process.
(cid:73) crf features are very    exible. they can look at the whole input x paired with a label

bigram (yi, yi+1).

(cid:73) in practice, for prediction tasks,    good    discriminative features can improve accuracy a

lot.

(cid:73) parameter estimation:

(cid:73) id48s focus on explaining the data, both x and y.
(cid:73) crfs focus on the mapping from x to y.
(cid:73) a priori, it is hard to say which paradigm is better.
(cid:73) same dilemma as naive bayes vs. maximum id178.

49/70

id170

id88, id166s, crfs

50/70

learning structured predictors

(cid:73) goal: given training data(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y with small error on unseen inputs

(cid:73) in a crf:

argmax

   

y   y

p (y|x; w) =

=

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

exp{(cid:80)n
n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) to predict new values, z(x; w) is not relevant
(cid:73) parameter estimation: w is set to maximize likelihood

(cid:73) can we learn w more directly, focusing on errors?

51/70

learning structured predictors

(cid:73) goal: given training data(cid:8)(x(1), y(1)), (x(2), y(2)), . . . , (x(m), y(m))(cid:9)

learn a predictor x     y with small error on unseen inputs

(cid:73) in a crf:

argmax

   

y   y

p (y|x; w) =

=

i=1 w    f (x, i, yi   1, yi)}

z(x; w)

exp{(cid:80)n
n(cid:88)i=1

w    f (x, i, yi   1, yi)

(cid:73) to predict new values, z(x; w) is not relevant
(cid:73) parameter estimation: w is set to maximize likelihood

(cid:73) can we learn w more directly, focusing on errors?

51/70

the structured id88
?

(cid:73) set w = 0
(cid:73) for t = 1 . . . t

(cid:73) for each training example (x, y)

1. compute z = argmaxz w    f (x, z)
2.

if z (cid:54)= y

(cid:73) return w

w     w + f (x, y)     f (x, z)

52/70

the structured id88 + averaging
?; ?

(cid:73) set w = 0, wa = 0
(cid:73) for t = 1 . . . t

(cid:73) for each training example (x, y)

1. compute z = argmaxz w    f (x, z)
2.

if z (cid:54)= y

w     w + f (x, y)     f (x, z)

3. wa = wa + w

(cid:73) return wa/mt , where m is the number of training examples

53/70

id88 updates: example

y per
per
z
x jack

per
loc

-
-

london went

loc
-
loc
-
to paris

(cid:73) let y be the correct output for x.
(cid:73) say we predict z instead, under our current w
(cid:73) the update is:

g = f (x, y)     f (x, z)

f (x, i, yi   1, yi)    (cid:88)i

= (cid:88)i
= f (x, 2, per, per)     f (x, 2, per, loc)
+ f (x, 3, per, -)     f (x, 3, loc, -)

f (x, i, zi   1, zi)

(cid:73) id88 updates are typically very sparse

54/70

properties of the id88

(cid:73) online algorithm. often much more e   cient than    batch    algorithms
(cid:73) if the data is separable, it will converge to parameter values with 0 errors
(cid:73) number of errors before convergence is related to a de   nition of margin. can also

relate margin to generalization properties

(cid:73) in practice:

1. averaging improves performance a lot
2. typically reaches a good solution after only a few (say 5) iterations over the training set
3. often performs nearly as well as crfs, or id166s

55/70

averaged id88 convergence

iteration accuracy

1
2
3
4
5
6
7
8
9
10
11
12
. . .

90.79
91.20
91.32
91.47
91.58
91.78
91.76
91.82
91.88
91.91
91.92
91.96

(results on validation set for a parsing task)

56/70

margin-based id170

(cid:73) let f (x, y) =(cid:80)n

(cid:73) model: argmaxy   y

i=1 f (x, i, yi   1, yi)

    w    f (x, y)

(cid:73) consider an example (x(k), y(k)):

   y (cid:54)= y(k)

: w    f (x(k), y(k)) < w    f (x(k), y) =    error

(cid:73) let y(cid:48) = argmaxy   y

   :y(cid:54)=y(k) w    f (x(k), y)
de   ne   k = w    (f (x(k), y(k))     f (x(k), y(cid:48)))

(cid:73) the quantity   k is a notion of margin on example k:

  k > 0        no mistakes in the example
high   k        high con   dence

57/70

margin-based id170

(cid:73) let f (x, y) =(cid:80)n

(cid:73) model: argmaxy   y

i=1 f (x, i, yi   1, yi)

    w    f (x, y)

(cid:73) consider an example (x(k), y(k)):

   y (cid:54)= y(k)

: w    f (x(k), y(k)) < w    f (x(k), y) =    error

(cid:73) let y(cid:48) = argmaxy   y

   :y(cid:54)=y(k) w    f (x(k), y)
de   ne   k = w    (f (x(k), y(k))     f (x(k), y(cid:48)))

(cid:73) the quantity   k is a notion of margin on example k:

  k > 0        no mistakes in the example
high   k        high con   dence

57/70

margin-based id170

(cid:73) let f (x, y) =(cid:80)n

(cid:73) model: argmaxy   y

i=1 f (x, i, yi   1, yi)

    w    f (x, y)

(cid:73) consider an example (x(k), y(k)):

   y (cid:54)= y(k)

: w    f (x(k), y(k)) < w    f (x(k), y) =    error

(cid:73) let y(cid:48) = argmaxy   y

   :y(cid:54)=y(k) w    f (x(k), y)
de   ne   k = w    (f (x(k), y(k))     f (x(k), y(cid:48)))

(cid:73) the quantity   k is a notion of margin on example k:

  k > 0        no mistakes in the example
high   k        high con   dence

57/70

mistake-augmented margins
?

x(k)
y(k)
y(cid:48)
y(cid:48)(cid:48)
y(cid:48)(cid:48)(cid:48)

jack
per
per
per

-

london went

per
loc

-
-

-
-
-

per

to
-
-
-

per

paris
loc
loc

-
-

e(y(k),  )

0
1
2
5

(cid:73) def: e(y, y(cid:48)) =(cid:80)n

i=1[yi (cid:54)= y(cid:48)i]

e.g., e(y(k), y(k)) = 0, e(y(k), y(cid:48)) = 1, e(y(k), y(cid:48)(cid:48)(cid:48)) = 5

(cid:73) we want a w such that
   y (cid:54)= y(k)

: w    f (x(k), y(k)) > w    f (x(k), y) + e(y(k), y)

(the higher the error of y, the larger the separation should be)

58/70

mistake-augmented margins
?

x(k)
y(k)
y(cid:48)
y(cid:48)(cid:48)
y(cid:48)(cid:48)(cid:48)

jack
per
per
per

-

london went

per
loc

-
-

-
-
-

per

to
-
-
-

per

paris
loc
loc

-
-

e(y(k),  )

0
1
2
5

(cid:73) def: e(y, y(cid:48)) =(cid:80)n

i=1[yi (cid:54)= y(cid:48)i]

e.g., e(y(k), y(k)) = 0, e(y(k), y(cid:48)) = 1, e(y(k), y(cid:48)(cid:48)(cid:48)) = 5

(cid:73) we want a w such that
   y (cid:54)= y(k)

: w    f (x(k), y(k)) > w    f (x(k), y) + e(y(k), y)

(the higher the error of y, the larger the separation should be)

58/70

mistake-augmented margins
?

x(k)
y(k)
y(cid:48)
y(cid:48)(cid:48)
y(cid:48)(cid:48)(cid:48)

jack
per
per
per

-

london went

per
loc

-
-

-
-
-

per

to
-
-
-

per

paris
loc
loc

-
-

e(y(k),  )

0
1
2
5

(cid:73) def: e(y, y(cid:48)) =(cid:80)n

i=1[yi (cid:54)= y(cid:48)i]

e.g., e(y(k), y(k)) = 0, e(y(k), y(cid:48)) = 1, e(y(k), y(cid:48)(cid:48)(cid:48)) = 5

(cid:73) we want a w such that
   y (cid:54)= y(k)

: w    f (x(k), y(k)) > w    f (x(k), y) + e(y(k), y)

(the higher the error of y, the larger the separation should be)

58/70

structured hinge loss

(cid:73) de   ne a mistake-augmented margin

  k,y =w    f (x(k), y(k))     w    f (x(k), y)     e(y(k), y)
  k = min
y(cid:54)=y(k)

  k,y

(cid:73) de   ne id168 on example k as:

l(w, x(k), y(k)) = max

y   y   (cid:110)w    f (x(k), y) + e(y(k), y)     w    f (x(k), y(k))(cid:111)

(cid:73) leads to an id166 for id170
(cid:73) given a training set,    nd:

argmin
w   rd

m(cid:88)k=1

l(w, x(k), y(k)) +

  
2(cid:107)w(cid:107)2

59/70

regularized loss minimization

(cid:73) given a training set(cid:8)(x(1), y(1)), . . . , (x(m), y(m))(cid:9) .

find:

argmin
w   rd

m(cid:88)k=1

l(w, x(k), y(k)) +

  
2(cid:107)w(cid:107)2

(cid:73) two common id168s l(w, x(k), y(k)) :

(cid:73) log-likelihood loss (crfs)

    log p (y(k) | x(k); w)

(cid:73) hinge loss (id166s)

max

y   y   (cid:16)w    f (x(k), y) + e(y(k), y)     w    f (x(k), y(k))(cid:17)

60/70

learning structure predictors: summary so far

(cid:73) linear models for sequence prediction

argmax

y   y

    (cid:88)i

w    f (x, i, yi   1, yi)

(cid:73) computations factorize on label bigrams

(cid:73) decoding: using viterbi
(cid:73) marginals: using forward-backward

(cid:73) parameter estimation:

(cid:73) id88, log-likelihood, id166s
(cid:73) extensions from classi   cation to the structured case
(cid:73) optimization methods:

(cid:73) stochastic (sub)gradient methods (??)
(cid:73) exponentiated gradient (?)
(cid:73) id166 struct (?)
(cid:73) structured mira (?)

61/70

beyond linear sequence prediction

62/70

factored sequence prediction, beyond bigrams

(cid:73) it is easy to extend the scope of features to k-grams

f (x, i, yi   k+1:i   1, yi)

(cid:73) in general, think of state   i remembering relevant history

(cid:73)   i = yi   1 for bigrams
(cid:73)   i = yi   k+1:i   1 for k-grams
(cid:73)   i can be the state at time i of a deterministic automaton generating y

(cid:73) the structured predictor is

argmax

y   y

    (cid:88)i

w    f (x, i,   i, yi)

(cid:73) viterbi and forward-backward extend naturally, in o(nlk)

63/70

dependency structures

(cid:73) directed arcs represent dependencies between a head word and a modi   er word.

(cid:73) e.g.:

movie modi   es saw,
john modi   es saw,
today modi   es saw

64/70

dependencystructureslikedtoday*johnsawamoviethathe!directedarcsrepresentdependenciesbetweenaheadwordandamodi   erword.!e.g.:moviemodi   essaw,johnmodi   essaw,todaymodi   essawid33: arc-factored models
?

(cid:73) parse trees decompose into single dependencies (cid:104)h, m(cid:105)

argmax

y   y(x) (cid:88)(cid:104)h,m(cid:105)   y

w    f (x, h, m)

(cid:73) some features:

f1(x, h, m) = [    saw           movie    ]
f2(x, h, m) = [ distance = +2 ]

(cid:73) tractable id136 algorithms exist (tomorrow   s lecture)

65/70

dependencyparsing:arc-factoredmodels(mcdonaldetal.2005)likedtoday*johnsawamoviethathe!parsetreesdecomposeintosingledependencies!h,m"argmaxy   y(x)!"h,m#   yw  f(x,h,m)!somefeatures:f1(x,h,m)=[   saw         movie   ]f2(x,h,m)=[distance=+2]!tractableid136algorithmsexist(tomorrow   slecture)linear id170

(cid:73) sequence prediction (bigram factorization)

argmax

y   y(x) (cid:88)i

w    f (x, i, yi   1, yi)

(cid:73) id33 (arc-factored)

argmax

y   y(x) (cid:88)(cid:104)h,m(cid:105)   y

w    f (x, h, m)

(cid:73) in general, we can enumerate parts r     y
argmax

y   y(x) (cid:88)r   y

w    f (x, r)

66/70

factored sequence prediction: from linear to non-linear

(cid:73) linear:

score(x, y) =(cid:88)i

s(x, i, yi   1, yi)

s(x, i, yi   1, yi) = w    f (x, i, yi   1, yi)

(cid:73) non-linear, using a feed-forward neural network:

where:

s(x, i, yi   1, yi) = wyi   1,yi    h(f (x, i))

h(f (x, i)) =   (w 2  (w 1  (w 0f (x, i))))

(cid:73) remarks:

(cid:73) the non-linear model computes a hidden representation of the input
(cid:73) still factored: viterbi and forward-backward work
(cid:73) parameter estimation becomes non-convex, use id26

67/70

recurrent sequence prediction

y1

h1

x1

y2

h2

x2

y3

h3

x3

yn

hn

xn

(cid:73) maintains a state: a hidden variable that keeps track of previous observations and

predictions

(cid:73) making predictions is not tractable

(cid:73) in practice: greedy predictions or id125

(cid:73) learning is non-convex
(cid:73) popular methods: id56, lstm, spectral models, . . .

68/70

. . .thanks!

69/70

references i

michael collins. discriminative training methods for id48: theory and experiments with id88 algorithms. in proceedings of the

acl-02 conference on empirical methods in natural language processing-volume 10, pages 1   8. association for computational linguistics, 2002.

michael collins, amir globerson, terry koo, xavier carreras, and peter l bartlett. exponentiated gradient algorithms for conditional random    elds and

max-margin markov networks. the journal of machine learning research, 9:1775   1822, 2008.

koby crammer, ryan mcdonald, and fernando pereira. scalable large-margin online learning for structured classi   cation. in nips workshop on learning

with structured outputs, 2005.

yoav freund and robert e. schapire. large margin classi   cation using the id88 algorithm. mach. learn., 37(3):277   296, december 1999. issn

0885-6125. doi: 10.1023/a:1007662407062. url http://dx.doi.org/10.1023/a:1007662407062.

michel galley, jonathan graehl, kevin knight, daniel marcu, steve deneefe, wei wang, and ignacio thayer. scalable id136 and training of context-rich

syntactic translation models. in proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the
association for computational linguistics, pages 961   968. association for computational linguistics, 2006.

sanjiv kumar and martial hebert. man-made structure detection in natural images using a causal multiscale random    eld. in 2003 ieee computer society

conference on id161 and pattern recognition (cvpr 2003), 16-22 june 2003, madison, wi, usa, pages 119   126, 2003. doi:
10.1109/cvpr.2003.1211345. url https://doi.org/10.1109/cvpr.2003.1211345.

john d. la   erty, andrew mccallum, and fernando c. n. pereira. conditional random    elds: probabilistic models for segmenting and labeling sequence data.

in proceedings of the eighteenth international conference on machine learning, icml    01, pages 282   289, san francisco, ca, usa, 2001. morgan
kaufmann publishers inc. isbn 1-55860-778-1. url http://dl.acm.org/citation.cfm?id=645530.655813.

yann lecun, l  eon bottou, yoshua bengio, and patrick ha   ner. gradient-based learning applied to document recognition. proceedings of the ieee, 86(11):

2278   2324, 1998.

andrew mccallum, dayne freitag, and fernando cn pereira. maximum id178 markov models for information extraction and segmentation. in icml,

volume 17, pages 591   598, 2000.

ryan mcdonald, fernando pereira, kiril ribarov, and jan haji  c. non-projective id33 using spanning tree algorithms. in proceedings of the

conference on human language technology and empirical methods in natural language processing, pages 523   530. association for computational
linguistics, 2005.

shai shalev-shwartz, yoram singer, nathan srebro, and andrew cotter. pegasos: primal estimated sub-gradient solver for id166. mathematical programming,

127(1):3   30, 2011.

ben taskar, carlos guestrin, and daphne koller. max-margin markov networks. in advances in neural information processing systems, volume 16, 2003.
ioannis tsochantaridis, thorsten joachims, thomas hofmann, and yasemin altun. large margin methods for structured and interdependent output variables.

journal of machine learning research, 6(sep):1453   1484, 2005.

70/70

