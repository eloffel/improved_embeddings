deep learning for id103

mark gales

july 2017

apple siri (2011)

2/57

speech application areas

3/57

speech processing:
proof of concept

4/57

speech production (synthesis)

5/57

speech perception (recognition)

6/57

speech understanding

7/57

should speech recognisers have ears?

8/57

should speech recognisers have ears?

no - i   m and engineer!

9/57

should speech recognisers have ears?

no - i   m an engineer!

10/57

id103

11/57

id103

12/57

/w/   /o/+/n/context   dependentwordsusuwuthengileuphethiloliyawonawaveformphonesphonesfeatures/w/ /o/ /n/ /a/id103 (traditional)

13/57

usuwuthengileuphethiloliyawonawaveformphonesphonesfeatures/w/ /o/ /n/ /a/context   dependentwords/w/   /o/+/n/id103 (traditional)

14/57

usuwuthengileuphethiloliyawaveformphonesphonesfeatures/w/ /o/ /n/ /a/context   dependentwords/w/   /o/+/n/wonaid103 (traditional)

15/57

usuwuthengileuphethiloliyawaveformphonesphonesfeatureswordswona/n/ /a//o//w//w/   /o/+/n/context   dependentid103 (traditional)

16/57

usuwuthengileuphethiloliyawaveformphonesphonesfeatureswordswona/n/ /a//o//w//w/   /o/+/n/timecontext   dependentsequence-to-sequence modelling

    sequence-to-sequence modelling central to speech/language:

    id133:

    machine translation:

word sequence (discrete)    word sequence (discrete)
word sequence (discrete)    waveform (continuous)
waveform (continuous)    word sequence (discrete)
    waveform sampled at 10ms/5ms frame-rate - t-length x1   t
    word/token sequences - l-length   1   l

    id103:

    the sequence lengths on either side can di   er

17/57

id103 framework (traditional)

    acoustic model: likelihood model generating observed features
    language model: id203 of any word sequence
    lexicon: maps words to sub-word units (phones)

18/57

languagemodelwaveformya uphe...featuresdecoderacousticmodellexicongenerative models [2, 3]

    consider two sequences (note l    t):
    features: x1   t={x1, x2, . . . , xt}
    words:   1   l={  1,   2, . . . ,   l}

    consider generative model

p(  1   l, x1   t) = p(  1   l)p(x1   t   1   l)

    p(  1   l): language model
    p(x1   t   1   l): acoustic model

19/57

language model: id165s [15]

<s>

the

cat

sat

on

the

mat

</s>

<s>
<s>
<s>
<s>
<s>
<s>
<s>

the
the
the
the
the
the
the

cat
cat
cat
cat
cat
cat
cat

sat
sat
sat
sat
sat
sat
sat

on
on
on
on
on
on
on

the
the
the
the
the
the
the

mat
mat
mat
mat
mat
mat
mat

</s>
</s>
</s>
</s>
</s>
</s>
</s>

p(  1   l)= lm
i=1

p(  i   1   i   1)    lm
i=1

p(  i   i   n+1   i   1)

20/57

acoustic model: id48 [1, 8, 23]

    id48s standard model for many year (1970s-2010s)

    each (context-dependent) phone modelled by an id48
    typically 3-emitting state topology, left-right
    non-emitting (end) states used for    gluing    models together

      1   t is the t-length state-sequence

      t indicates the id48-state at time instance t

21/57

33a224423451a12a23a34aa45atxt+1tt+1    xacoustic model: id48s [1, 8]

    important sequence model: hidden markov model (id48)

    an example of a dynamic id110 (dbn)

    discrete latent variables

      t describes discrete state-space
    conditional independence assumptions

p(  t   1   t   1)= p(  t   t   1)
p(xt x1   t   1,   1   t)= p(xt   t)
p(xt   t)p(  t   t   1) 

    the likelihood of the data is

p(x1   t   1   l)= q
  1   t       1   l

  tm
t=1

22/57

txt+1tt+1    xdecoding (traditional) [23]

    use bayes    decision rule

     = arg max
= arg max
= arg max

  

  

{p(   x1   t)}
{p(  , x1   t)}
{p(  )p(x1   t   )}

  

    need to e   ciently search over all possible word sequences

    viterbi decoding used for e   ciency with id48s & id165s

    leverages model conditional independence assumptions

23/57

deep learning and

recurrent neural networks

24/57

what is deep learning?

from wikipedia:

deep learning is a branch of machine learning based on a
set of algorithms that attempt to model high-level
abstractions in data by using multiple processing layers,
with complex structures or otherwise, composed of
multiple non-linear transformations.

25/57

what is deep learning?

from wikipedia:

deep learning is a branch of machine learning based on a
set of algorithms that attempt to model high-level
abstractions in data by using multiple processing layers,
with complex structures or otherwise, composed of
multiple non-linear transformations.

26/57

deep neural networks [13]

    general mapping process from input x to output y(x)

y(x)=f(x)
    x(k) is the input to layer k
    x(k+1)= y(k) the output from layer k

    deep refers to number of hidden layers

    output from the previous layer connected to following layer:

27/57

y(x)xneural network layer/node

    general form for layer k:

y(k)

i

=   (w   

ix(k)+ bi)=   (z(k)

i

)

28/57

  ()wizirecurrent neural networks [19, 18]

29/57

txt   1httimehdelay1:ty(x   )recurrent neural networks

    consider a causal sequence of observations x1   t={x1, . . . , xt}
hht   1+ bh 

ht = fh wf
hxt+ wr
y(x1   t)= ff wyht+ by 

    introduce recurrent units

    ht history vector at time t

    uses approximation to model history of observations

f(x1   t)=f(xt, x1   t   1)   f(xt, ht   1)   f(ht)= y(x1   t)

    network has (causal) memory encoded in history vector (ht)

30/57

txt   1httimehdelay1:ty(x   )id56: dynamic id110

    maps between two sequences x1   t    y1   t

    figure on right is unwrapped in time

    shows dependencies - shaded blue are deterministic mappings

    seen similar models - id48s, crfs, sid166s ..

    doesn   t handle sequence length mappings in asr

31/57

txt   1httimehdelay1:ty(x   )xt+1xthtt+1hytyt+1id56: variants and extensions [21, 6, 14, 11, 22, 7]

    extensions of standard id56 structure:

    bi-directional id56 (depends on future and past)
    latent-variable id56s (continuous latent variables)

    modi   cation to the recurrent units (gating)

    long-short term memory units (lstms)
    id149 (grus)
    highway connections (gating in time)

32/57

acoustic modelling

33/57

id48 [1, 8]

    discrete latent variables

      t describes discrete state-space
    conditional independence assumptions

p(  t   1   t   1)= p(  t   t   1)
p(xt x1   t   1,   1   t)= p(xt   t)
p(x1   t   1   t)p(  1   t)
  tm
p(xt   t)p(  t   t   1) 
t=1

    the likelihood of the data is

p(x1   t   1   l) =
=

q
  1   t       1   l
q
  1   t       1   l

34/57

txt+1tt+1    xhistory approximations and id136

id48

t=1 p(xt   t)
   t

finite state

t=1 p(xt   t, ht   1)
   t

finite feature

t=1 p(xt   ht)
   t

    id136 costs signi   cantly di   erent:

       nite state: all past history observed - deterministic
       nite feature: past history unobserved - depends on path

35/57

txt+1tt+1    x    xtxt+1ht   1thtt+1    xtxt+1th~ht+1~tt+1acoustic model approximations

    id48: simplest form of approximation

p(x1   t   1   t)    tm
t=1

p(xt   t)
p(xt   t, x1   t   1)    tm
t=1
p(xt   1   t)    tm
t=1

p(x1   t   1   t)    tm
t=1

    finite state:

p(x1   t   1   t)    tm
t=1

    finite feature:

p(xt   t, ht   1)
p(xt   ht)

36/57

   likelihoods    [3]

    deep learning can be used to estimate distributions

    mixture density neural network (mdnn)
    more often trained as a discriminative model
    need to convert to a    likelihood   

37/57

   likelihoods    [3]

    deep learning can be used to estimate distributions

    most common form (for id56 acoustic model):

    mixture density neural network (mdnn)
    more often trained as a discriminative model
    need to convert to a    likelihood   

p(xt   t, ht   1) = p(  t xt, ht   1)p(xt ht   1)

p(  t ht   1)
    p(  t xt, ht   1)
p(  t ht   1)
    p(  t xt, ht   1)
p(  t)
    p(  t xt , ht   1): modelled by a standard id56
    p(  t): state/phone prior id203

38/57

   baseline    acoustic training criteria

    originally generative models (gmm-id48 systems) used ml

= log

fml = log(p(x1   t   ref))
p(x1   t   1   t)p(  1   t)     
       q
  1   t       ref
log p(    t xt, ht   1) 
fce =     tq
t=1
    1   t = arg max
  1   t       ref

{p(  1   t x1   t)}

    neural networks: cross-id178 with    xed alignment,

39/57

example    generative    acoustic model

[20]

    example architecture from google (2015)

    c: id98 layer (with pooling)
    l: lstm layer
    d: fully connected layer
    two multiple layer    skips   

    (1) connects input to lstm input
    (2) connects id98 output to dnn input

    additional linear projection layer

    reduces dimensionality
    and number of network parameters!

40/57

understandthecldnnarchitecturearepresentedinsection4.re-sultsonthelargerdatasetsarethendiscussedinsection5.finally,section6concludesthepaperanddiscussesfuturework.2.modelarchitecturethissectiondescribesthecldnnarchitectureshowninfigure1.2.1.cldnnframext,surroundedbylcontextualvectorstotheleftandrcon-textualvectorstotheright,ispassedasinputtothenetwork.thisinputisdenotedas[xt l,...,xt+r].inourwork,eachframextisa40-dimensionallog-melfeature.first,wereducefrequencyvarianceintheinputsignalbypass-ingtheinputthroughafewconvolutionallayers.thearchitectureusedforeachid98layerissimilartothatproposedin[2].specif-ically,weuse2convolutionallayers,eachwith256featuremaps.weusea9x9frequency-time   lterforthe   rstconvolutionallayer,followedbya4x3   lterforthesecondconvolutionallayer,andthese   ltersaresharedacrosstheentiretime-frequencyspace.ourpool-ingstrategyistousenon-overlappingmaxpooling,andpoolinginfrequencyonlyisperformed[11].apoolingsizeof3wasusedforthe   rstlayer,andnopoolingwasdoneinthesecondlayer.thedimensionofthelastlayeroftheid98islarge,duetothenumberoffeature-maps   time   frequencycontext.thus,weaddalinearlayertoreducefeaturedimension,beforepassingthistothelstmlayer,asindicatedinfigure1.in[12]wefoundthataddingthislinearlayeraftertheid98layersallowsforareductioninpa-rameterswithnolossinaccuracy.inourexperiments,wefoundthatreducingthedimensionality,suchthatwehave256outputsfromthelinearlayer,wasappropriate.afterfrequencymodelingisperformed,wenextpasstheid98outputtolstmlayers,whichareappropriateformodelingthesig-nalintime.followingthestrategyproposedin[3],weuse2lstmlayers,whereeachlstmlayerhas832cells,anda512unitprojec-tionlayerfordimensionalityreduction.unlessotherwiseindicated,thelstmisunrolledfor20timestepsfortrainingwithtruncatedid26throughtime(bptt).inaddition,theoutputstatelabelisdelayedby5frames,aswehaveobservedwithdnnsthatinformationaboutfutureframeshelpstobetterpredictthecurrentframe.theinputfeatureintotheid98haslcontextualframestotheleftandrtotheright,andtheid98outputisthenpassedtothelstm.inordertoensurethatthelstmdoesnotseemorethan5framesoffuturecontext,whichwouldincreasethedecodinglatency,wesetr=0forcldnns.finally,afterperformingfrequencyandtemporalmodeling,wepasstheoutputofthelstmtoafewfullyconnecteddnnlayers.asshownin[5],thesehigherlayersareappropriateforproducingahigher-orderfeaturerepresentationthatismoreeasilyseparableintothedifferentclasseswewanttodiscriminate.eachfullyconnectedlayerhas1,024hiddenunits.2.2.multi-scaleadditionstheid98takesalong-termfeature,seeingacontextoft ltot(i.e.,r=0inthecldnn),andproducesahigherorderrepresentationofthistopassintothelstm.thelstmisthenunrolledfor20timesteps,andthusconsumesalargercontextof20+l.however,wefeelthereiscomplementaryinformationinalsopassingtheshort-termxtfeaturetothelstm.infact,theoriginallstmworkin[3]lookedatmodelingasequenceof20consecutiveshort-termxtc...ddllcconvolutionallayerslstmlayersfullyconnectedlayersoutput targets[xt-l,..., xt, ...., xt+r]linearlayerdimred(1)xt(2)fig.1.cldnnarchitecturefeatures,withnocontext.inordertomodelshortandlong-termfeatures,wetaketheoriginalxtandpassthisasinput,alongwiththelong-termfeaturefromtheid98,intothelstm.thisisshownbydashedstream(1)infigure1.theuseofshortandlong-termfeaturesinaneuralnetworkhasbeenexploredpreviously(i.e.,[13,14]).themaindifferencebe-tweenpreviousworkandoursisthatweareabletodothisjointlyinonenetwork,namelybecauseofthepowerofthelstmsequen-tialmodeling.inaddition,ourcombinationofshortandlong-termfeaturesresultsinanegligibleincreaseinthenumberofnetworkparameters.inaddition,weexploreifthereiscomplementaritybetweenmodelingtheoutputoftheid98temporallywithanlstm,aswellasdiscriminativelywithadnn.speci   cally,motivatedbyworkincomputervision[10],weexplorepassingtheoutputoftheid98intoboththelstmanddnn.thisisindicatedbythedashedstream(2)infigure1.thisideaofcombininginformationfromid98anddnnlayershasbeenexploredbeforeinspeech[11,15],thoughpreviousworkaddedextradnnlayerstodothecombination.ourworkdiffersinthatwepasstheoutputoftheid98directlyintothednn,withoutextralayersandthusminimalparameterincrease.discriminative models
(   end-to-end    models)

41/57

id103 framework

    apply bayes    decision rule

    = arg max

{p(   x1   t)}

  

    directly train model to solve task (   speech-to-text   )

    single model trained
    no separate acoustic and language models

    more complicated to incorporate additional lm data

42/57

waveformya uphe...modelend   to   endfeaturesdecoderdiscriminative models [2]

    compute posterior of word sequence

p(  1   l x1   t)= q
  1   t       1   l

p(  1   l   1   t)p(  1   t x1   t)

43/57

discriminative models

    compute posterior of word sequence

p(  1   l x1   t)= q
  1   t       1   l

p(  1   l   1   t)p(  1   t x1   t)
       :1

44/57

discriminative models

    compute posterior of word sequence

p(  1   l   1   t)p(  1   t x1   t)
       :1

p(  1   l x1   t)= q
  1   t       1   l
tm
t=1
tm
t=1

p(  1   t x1   t)    
   

p(  t x1   t)
p(  t xt, ht   1)    tm
t=1

       nite state id56s used to model history/alignment

    expression does not have a language model

p(  t ht)

45/57

connectionist temporal classi   cation [10]

    ctc: discriminative model, no explicit alignment model

    introduces a blank output symbol ( )

    consider word: cat

    pronunciation: /c/ /a/ /t/

    observe 7 frames

    possible state transitions
    example path:

~c~   ~a~ ~a~   ~t~  

46/57

/c/time      /t//a/including state history?

memm [16]

ctc

full history

    interesting to consider state dependencies (right)

p(  1   t x1   t)    tm
t=1

p(  t x1   t,   1   t   1)    tm
t=1

p(  t   ht)

47/57

txt+1tt+1    xt+1t  ht+1t+1  thxtx    xtxt+1th~ht+1~tt+1nature of targets

    one trend for discriminative models:

graphemes (letters) rather than context-dependent phones

    take the example of the lexicon entry cat: /k/ /a/ /t/

sil
sil

sil
sil

k

sil-/k/+/a/

a

/k/-/a/+/t/

t

/a/-/t/+sil

sil-/c/+/a/

c

/c/-/a/+/t/

a

/a/-/t/+sil

t

sil
sil

sil
sil

    can be run at the character level

    no need to have a lexicon (hence no oovs)
    language model implicit by history vector (of features)

48/57

discriminative models and    priors    [12]

    no language models in (this form of) discriminative model

    treat as a product of experts (log-linear model): for ctc

p(  1   l x1   t)=

    in ctc the word history    captured    in frame history
    no explicit dependence on state (word) history

        t

z(x1   t) exp

       log      1   t       1   l
log   p(  1   l) 
      p(  1   l) standard    prior    (language) model

       trainable parameter (related to lm scale)

1

p(  1   t x1   t) 

    normalisation term not required in decoding

       often empirically tuned

      
     

49/57

encoder-decoder style models

    directly model relationship

p(  i   1   i   1, x1   t)
p(  i   i   1,   hi   2, c)

p(  1   l x1   t) =
lm
i=1
   
lm
i=1
c=   (x1   t)

    looks like an id56 lm with additional dependence on c

    c is a    xed length vector - like a sequence kernel

50/57

id56 encoder-decoder model

[9, 17]

    simplest form is to use hidden unit from acoustic id56/lstm

c=   (x1   t)= ht

    dependence on context is global via c - possibly limiting

51/57

xt+1thxthtt   1hxtht   1i+1yiyhi   1ih~~decoderencoderattention-based models [5, 4, 17]

52/57

decoderattentionci+1icxt+1thxthtt   1hxtht   1encoderi+1yiyhi   1ih~~attention-based models

    introduce attention layer to system
    introduce dependence on locality i

p(  1   l x1   t)    lm
i=1
  i  h  ;   i  =
ci= tq
  =1
    ei   how well position i   1 in input matches position    in output

p(  i   hi   1)
p(  i   i   1,   hi   2, ci)    lm
i=1
exp(ei  )
ei  = f e   hi   2, h   
k=1 exp(eik) ,
   t

    h   is representation (id56) for the input at position   

    attention can    wander    with large input size (t)

    use a pyramidal network to reduce frame-rate for attention

53/57

conclusions

54/57

deep learning and id103

it   s an interesting time!

    deep learning integrated into standard speech toolkits

    kaldi, htk etc

    rich variety of models and topologies supported by:

    large quantities of training data
    gpu-based training (and parallel implementations)
    array of software tools: tensorflow, cntk, theano ...

    most state-of-the-art still    generative   

    but next conference in august ...

55/57

network interpretation [24]

standard /ay/

stimulated /ay/

    deep learning usually highly distributed - hard to interpret

    awkward to adapt/understand/regularise
    modify training - add stimulation regularisation
    improves asr performance ...

56/57

thank-you!

57/57

[1]

l. baum and j. eagon,    an inequality with applications to statistical estimation for probabilistic functions
of markov processes and to a model for ecology,    bull amer math soc, vol. 73, pp. 360   363, 1967.
c. m. bishop, pattern recognition and machine learning. springer verlag, 2006.

[2]
[3] h. bourlard and n. morgan,    connectionist id103: a hybrid approach,    1994.
[4] w. chan, n. jaitly, q. v. le, and o. vinyals,    listen, attend and spell,    corr, vol. abs/1508.01211, 2015.

[online]. available: http://arxiv.org/abs/1508.01211
j. chorowski, d. bahdanau, d. serdyuk, k. cho, and y. bengio,    attention-based models for speech
recognition,    corr, vol. abs/1506.07503, 2015. [online]. available: http://arxiv.org/abs/1506.07503
j. chung, c. gulcehre, k. cho, and y. bengio,    empirical evaluation of gated recurrent neural networks on
sequence modeling,    arxiv preprint arxiv:1412.3555, 2014.
j. chung, k. kastner, l. dinh, k. goel, a. c. courville, and y. bengio,    a recurrent latent variable model
for sequential data,    corr, vol. abs/1506.02216, 2015. [online]. available:
http://arxiv.org/abs/1506.02216

[8] m. gales and s. young,    the application of id48 in id103,    foundations and

trends in signal processing, vol. 1, no. 3, 2007.
a. graves,    sequence transduction with recurrent neural networks,    corr, vol. abs/1211.3711, 2012.
[online]. available: http://arxiv.org/abs/1211.3711

[5]

[6]

[7]

[9]

[10] a. graves, s. fern  ndez, f. gomez, and j. schmidhuber,    connectionist temporal classi   cation: labelling

unsegmented sequence data with recurrent neural networks,    in proceedings of the 23rd international
conference on machine learning. acm, 2006, pp. 369   376.

[11] a. graves, a.-r. mohamed, and g. hinton,    id103 with deep recurrent neural networks,    in

2013 ieee international conference on acoustics, speech and signal processing.

ieee, 2013, pp. 6645   6649.

[12] g. e. hinton,    products of experts,    in proceedings of the ninth international conference on arti   cial

neural networks (icann 99), 1999, pp. 1   6.

57/57

[13] g. hinton, l. deng, d. yu, g. e. dahl, a.-r. mohamed, n. jaitly, a. senior, v. vanhoucke, p. nguyen,

t. n. sainath, et al.,    deep neural networks for acoustic modeling in id103: the shared views of
four research groups,    ieee signal processing magazine, vol. 29, no. 6, pp. 82   97, 2012.

[14] s. hochreiter and j. schmidhuber,    long short-term memory,    neural comput., vol. 9, no. 8, pp.

1735   1780, nov. 1997.

[15] f. jelinek, statistical methods for id103, ser. language, speech, and communication.

cambridge (mass.), london: mit press, 1997.

[16] h.-k. kuo and y. gao,    maximum id178 direct models for id103,    ieee transactions audio

speech and language processing, 2006.

[17] l. lu, x. zhang, k. cho, and s. renals,    a study of the recurrent neural network encoder-decoder for large

vocabulary id103,    in proc. interspeech, 2015.

[18] t. robinson and f. fallside,    a recurrent error propagation network id103 system,    computer

speech & language, vol. 5, no. 3, pp. 259   274, 1991.

[19] d. e. rumelhart, g. e. hinton, and r. j. williams,    parallel distributed processing: explorations in the

microstructure of cognition, vol. 1,    d. e. rumelhart, j. l. mcclelland, and c. pdp research group, eds.
cambridge, ma, usa: mit press, 1986, ch. learning internal representations by error propagation, pp.
318   362.

[20] t. n. sainath, o. vinyals, a. senior, and h. sak,    convolutional, long short-term memory, fully connected
deep neural networks,    in 2015 ieee international conference on acoustics, speech and signal processing
(icassp).

ieee, 2015, pp. 4580   4584.

[21] m. schuster and k. k. paliwal,    id182,    ieee transactions on signal

processing, vol. 45, no. 11, pp. 2673   2681, 1997.

[22] r. k. srivastava, k. gre   , and j. schmidhuber,    id199,    corr, vol. abs/1505.00387, 2015.

[online]. available: http://arxiv.org/abs/1505.00387

[23] a. viterbi,    error bounds for convolutional codes and an asymptotically optimum decoding algorithm,   

vol. 13, no. 2, pp. 260   269, 1967.

57/57

[24] c. wu, p. karanasou, m. gales, and k. c. sim,    stimulated deep neural network for id103,    in

proceedings interspeech, 2016.

57/57

