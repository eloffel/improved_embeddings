long short-term memory over recursive structures

xiaodan zhu
national research council canada, 1200 montreal road m-50, ottawa, on k1a 0r6 canada
parinaz sobhani
psobh090@uottawa.ca
school of electrical engineering and computer science, university of ottawa, 800 king edward avenue, ottawa, on
k1n 6n5 canada
hongyu guo
national research council canada, 1200 montreal road m-50, ottawa, on k1a 0r6 canada

xiaodan.zhu@nrc-cnrc.gc.ca

hongyu.guo@nrc-cnrc.gc.ca

abstract

the chain-structured long short-term memory
(lstm) has showed to be effective in a wide
range of problems such as id103
and machine translation. in this paper, we pro-
pose to extend it to tree structures, in which a
memory cell can re   ect the history memories
of multiple child cells or multiple descendant
cells in a recursive process. we call the model
s-lstm, which provides a principled way of
considering long-distance interaction over hier-
archies, e.g., language or image parse structures.
we leverage the models for semantic composi-
tion to understand the meaning of text, a funda-
mental problem in natural language understand-
ing, and show that it outperforms a state-of-the-
art recursive model by replacing its composition
layers with the s-lstm memory blocks. we also
show that utilizing the given structures is helpful
in achieving a performance better than that with-
out considering the structures.

1. introduction
recent years have seen a revival of the long short-term
memory (lstm) (hochreiter & schmidhuber, 1997), with
its effectiveness being demonstrated on a wide range of
problems such as id103 (graves et al., 2013),
machine translation (sutskever et al., 2014; cho et al.,
2014), and image-to-text conversion (vinyals et al., 2014),

proceedings of the 32 nd international conference on machine
learning, lille, france, 2015. jmlr: w&cp volume 37. copy-
right 2015 by the author(s).

among many others, in which history is summarized and
coded in the memory blocks in a full-order fashion.
recursion is a fundamental process associated with many
problems   a recursive process and the structure it forms
are common in different modalities. for example, seman-
tics of sentences in human languages is arguably to be car-
ried not merely by a linear concatenation of words; in-
stead, sentences often have structures (manning & sch  utze,
1999). image understanding, as another example, may ben-
e   t from recursive modeling over structures, which yielded
the state-of-the-art performance on tasks like scene seg-
mentation (socher et al., 2011).
in this paper, we extend lstm to tree structures, in which
we attempt to learn memory blocks that can re   ect the his-
tory memories of multiple child cells and hence multiple
descendant cells. we call the model s-lstm. compared
with previous id56s (socher et al.,
2013; 2012), s-lstm has the potentials of avoiding gra-
dient vanishing and hence may model long-distance inter-
actions over trees. this is a desirable characteristic as many
of such structures are deep. s-lstm can be viewed as con-
sidering together a id56 and a recurrent
neural network1. in brief, s-lstm wires memory blocks
in a partial-order tree structures instead of in a full-order
sequence as in a chain-structured lstm.
we leverage the s-lstm model to solve a semantic com-
position problem that learns the meaning for a piece of
texts   learning a good representation for the meaning of a
span of text is core to automatically understanding human
languages. more speci   cally, we experiment with the mod-

1as both of them can be shortened to be id56, in the rest of
this paper we refer to a recurrent neural network as id56 and a
id56 as rvnn.

els on the stanford sentiment treebank dataset (socher
et al., 2013) to determine the sentiment for different granu-
larities of phrases in a tree. the dataset has favorable prop-
erties: in addition to being a benchmark for much previous
work, it provides with human annotations at all nodes of
the trees, enabling us to comprehensively explore the prop-
erties of s-lstm. we experimentally show that s-lstm
outperforms a state-of-the-art recursive model by simply
replacing the original tensor-enhanced composition with
the s-lstm memory block that we propose here. we show
that utilizing the given structures is helpful in achieving a
better performance than that without considering the struc-
tures.

2. related work
id56s recursion is a fundamental
process in different modalities. in recent years, recursive
neural networks (rvnn) have been introduced and demon-
strated to achieve the state-of-the-art performances on dif-
ferent problems such as semantic analysis in natural lan-
guage processing and image segmentation (socher et al.,
2013; 2011). these networks are de   ned over recursive
tree structures   a tree node is a vector computed from its
children. in a recursive fashion, the information from the
leaf nodes of a tree and its internal nodes are combined in
a bottom-up manner through the tree. derivatives of errors
are computed with id26 over structures (goller
& kuchler, 1996).
in addition, the literature has also included many other ef-
forts of applying feedforward-based neural network over
structures,
including (goller & kuchler, 1996; chater,
1992; starzyk & he, 2007; hammer et al., 2004; guo
et al., 2014), amongst others. for instance, legrand and
collobert leveraged neural networks over greedy syntac-
tic parsing (pinheiro & collobert, 2014).
in (irsoy &
cardie, 2014), a deep id56 was pro-
posed, and in (hermann & blunsom, 2013) composition
was performed with the consideration of linguistic motiva-
tions speci   cally from the id35
(id35) formalism. nevertheless, over the often deep struc-
tures, the networks are potentially subject to the vanish-
ing gradient problem, resulting in dif   culties in leveraging
long-distance dependencies in the structures. in this paper,
we propose the s-lstm model that wires memory blocks
in recursive structures. we compare our model with the
rvnn models presented in (socher et al., 2013), as we
directly replaced the tensor-enhanced composition layer at
each tree node with a s-lstm memory block. we show
the advantages of our proposed model in achieving signi   -
cantly better results.
recurrent neural networks and lstm unlike a feed-
forward network, a recurrent neural network (id56) shares

their hidden states across time. the sequential history is
summarized in a hidden vector. id56 also suffers from
the decaying of gradient, or less frequently, blowing-up
of gradient problem. lstm replaces the hidden vector
of a recurrent neural network with memory blocks which
are equipped with gates;
it can in principle keep long-
term memory by training proper gating weights (refer to
(graves, 2008) for intuitive illustrations and good discus-
sions), and it has practically showed to be very useful,
achieving the state of the art on a range of problems includ-
ing id103 (graves et al., 2013), digit hand-
writing recognition (liwicki et al., 2007; graves, 2008),
and achieve interesting results on statistical machine trans-
lation (sutskever et al., 2014; cho et al., 2014) and music
composition (eck & schmidhuber, 2002b;a).
in (graves
et al., 2013), a deep lstm network achieved the state-of-
the-art results on the timit phoneme recognition bench-
mark. in (sutskever et al., 2014; cho et al., 2014), a pair of
id137 are trained to encode and decode human
language for automatic machine translation, which is in
particular effective for the more challenging long sentence
translation. in (liwicki et al., 2007; graves, 2008), lstm
networks are found to be very useful for digit writing recog-
nition because of the network   s capability of memorizing
context information in a long sequence. in (eck & schmid-
huber, 2002b;a), id137 are trained to effectively
capture global structures of the temporal data. with the
memory cells, lstm is able to keep track of temporally
distant events that indicate global music structures. as a
result, lstm can be successfully trained to compose mu-
sic, where other id56s have failed to do so.
although promising results have been observed by apply-
ing chain-structured lstm, many other interesting prob-
lems are inherently associated with input structures that
are more complicated than a sequence. for example, sen-
tences in human languages are believed to be carried by
not merely a linear sequence of words; instead, meaning
is thought to interweave with structures. while a sequen-
tial application of lstm may capture structural informa-
tion implicitly, in practice it sometimes lacks the claimed
power. for example, even simply reversing the input se-
quences may result in signi   cant differences in modeling
performances, in tasks such as machine translation and
id103. unlike in previous work, we propose
here to directly wire memory blocks in recursive struc-
tures. we show the proposed s-lstm model does utilize
the structures and achieve results better than those ignoring
such priori structures.
in addition,
independent re-
search (tai et al., 2015; le & zuidema, 2015) has been
conducted to explore ideas close to ours. we refer the read-
ers to these coming interesting works as well.

in parallel to our efforts,

language parsing) to render structures.

3. the model
model brief in this paper, we extend lstm to structures,
in which a memory cell can re   ect the history memories of
multiple child cells and hence multiple descendant cells in
a hierarchical structure. as intuitively showed in figure 1,
the root of the tree can in principle consider information
from long-distance interactions over the tree   in this    g-
ure, the gray and light-blue leaf. in the    gure, the small
circle (         ) or short line (         ) at each arrowhead indicates
pass and block of information, respectively. note that the
   gure shows a binary case, while in real models a soft ver-
sion of gating is applied, where a gating signal is in the
range of [0, 1], often enforced with a logistic sigmoid func-
tion. through learning the gating signals, as detailed later
in this section, s-lstm provides a principled way of con-
sidering long-distance interplays over the input structures.

figure 2. a s-lstm memory block, consisting of an input gate,
two forget gates, and an output gate. hidden vectors h   
t   1 and
cell vectors c   
t   1 from the left (red arrows) and right (blue ar-
rows) children are deployed to compute ct and ht.     denotes a
hadamard product, and the    s    shaped sign is a squashing func-
tion (in this paper the tanh function).

figure 1. an example of s-lstm, a long-short term memory net-
work on tree structures. a tree node can consider information
from multiple descendants.
information of the other nodes in
white are blocked. the small circle (         ) or short line (         ) at
each arrowhead indicates a pass or block of information, respec-
tively, while in the real model the gating is a soft version of gating.

the memory block each node in figure 1 is composed of
a s-lstm memory block. we present a speci   c wiring of
such a block in figure 2. each memory block contains one
input gate and one output gate. the number of forget gates
depends on the structure, i.e., the number of children of a
node. in this paper, we assume there are two children at
each nodes, same as in (socher et al., 2013) and therefore
we use their data in our experiments. that is, we have two
forget gates. extending the model to handle a n-ary tree
is rather straightforward. in addition, it may also deserve
to note that on the other hand, binarizing a tree to a binary
tree is also a common practice in some domains (e.g., in

t   1 and cr

t   1 and hr

t   1) and cell vectors (cl

t   1 for the left child and hr

as shown in the    gure, the hidden vectors of the two chil-
dren, denoted as hl
t   1 for the
right, are taken in as input of the current block. the input
gate it consider four resources of information: the hidden
vectors (hl
t   1) of
its two children. these four sources of information are also
used to form the gating signals for the left forget gate f l
t   1
and right forget gate f r
t   1, where the weights used to com-
bining them are speci   c to each of these gates, denoted as
different w in the formulas below. different from the pro-
cess in a regular lstm, the cell here considers the copies
t   1), gated with
from both children   s cell vectors (cl
separated forget gates. the left and right forget gates can
be controlled independently, allowing the pass-through of
information from children   s cell vectors. the output gate
ot considers the hidden vectors from the children and the
current cell vector. in turn, the hidden vector ht and the
cell vector ct of the current block are passed to the par-
ent and are used depending on if the current block is a left
or right child of its parent. in this way, the memory cell,
through merging the gated cell vectors of the children, can
re   ect multiple direct or indirect descendant cells. as a re-
sult, the long-distance interplays over the structures can be

t   1, cr

captured. more speci   cally, the forward computation of a
s-lstm memory block is speci   ed in the following equa-
tions.

 h
t =

it =   (w l
ci cr

+ w r

hihl
t   1 + bi)

t   1 + w r

hihr

t   1 + w l

cicl

t   1

t =   (w l
f l
hf l
cr
t   1 + bfl )

+ w r
cf l

hl
t   1 + w r
hf l

hr
t   1 + w l
cf l

cl
t   1

t =   (w l
f r
hf r
cr
t   1 + bfr )

+ w r
cf r

hl
t   1 + w r
hf r

hr
t   1 + w l
cf r

cl
t   1

xt = w l

t   1 + w r

hxhl
t     cl

hxhr
t     cr

t   1 + bx
t   1 + it     tanh(xt)

ct = f l

t   1 + f r

ot =   (w l

hohl

t   1 + w r

hohr

t   1 + wcoct + bo)

ht = ot     tanh(ct)

(1)

(2)

(3)

(4)

(5)

(6)

(7)

   o
   ht
t     tanh(ct)       (cid:48)(ot)
t     cl
t     cr
t     tanh(xt)       (cid:48)(it)

t   1       (cid:48)(f l
t )
t   1       (cid:48)(f r
t )

  o
t =  h
  fl
t =  c
  fr
t =  c
  i
t =  c

(8)

(9)

(10)

(11)

(12)

where   (cid:48)(x) is the element-wise derivative of the logistic
function over vector x. since it can be computed with the
activation of x, we abuse the notation a bit to write it over
the activated vectors in these equations.  c
t is the derivative
over the cell vector. so if the current node is the left child of
its parent, we use equation (13) to calculate  c
t, otherwise
formula (14) is used:

 c
t = h

t     ot     g(cid:48)(ct) +  c
t+1 + (w l
ci)t   i
(w l
cfl

t+1     f l
)t   fl
t+1+

t+1+

 c
t = h

t+1 + (wco)t   o
t

)t   fr

(w l
cfr
t     ot     g(cid:48)(ct) +  c
ci )t   i
(w r
t+1 + (w r
cfl

t+1+

t+1     f r
)t   fl
t+1+

(13)

where    is the element-wise logistic function used to con-
   ne the gating signals to be in the range of [0, 1]; f l
and f r are the left and right forget gate, respectively; b
is bias and w is network weight matrices; the sign     is
a hadamard product, i.e., element-wise product. the sub-
scripts of the weight matrices indicate what they are used
for. for example, who is a matrix mapping a hidden vector
to an output gate.
id26 over structures during training, the
gradient of the objective function with respect to each pa-
rameter can be calculated ef   ciently via id26
over structures (goller & kuchler, 1996; socher et al.,
2013). the major difference from that of (socher et al.,
2013) is we use lstm-like id26, where unlike
a regular lstm, pass of error needs to discriminate be-
tween the left and right children, or in a topology with more
than two children, needs to discriminate between children.
obtaining the id26 formulas is tedious but we
list them below to facilitate duplication of our work. we
will discuss the speci   c objective function later in exper-
iments. for each memory block, assume that the error
passed to the hidden vector is  h
t . the derivatives of the
t , left forget gate   fl
t , right forget gate   fr
output gate   o
t ,
and input gate   i
t are computed as:

(w r
cfr

)t   fr

t+1 + (wco)t   o
t

(14)

where g(cid:48)(x) is the element-wise derivative of the tanh func-
tion. it can also be directly calculated from the tanh acti-
vation of x. the superscript t over the weight matrices
means matrix transpose.
with derivatives at each gate computed, the derivatives of
the weight matrices used in formula (1)-(7) can be calcu-
lated accordingly, which is omitted here. we checked the
gradient to ensure the correctness of the implementation of
s-lstm.
objective over trees the objective function de   ned over
structures can be complicated, which could consider the
output structures depending on the properties of problem.
following (socher et al., 2013), the overall objective func-
tion we used to learn s-lstm in this paper is simply mini-
mizing the overall cross-id178 errors and a sum of that at
all nodes.

4. experiment set-up
as discussed earlier, recursion is a basic process inherent
to many problems. in this paper, we leverage the proposed

model to solve semantic composition for the meanings of
pieces of text, a fundamental problem in understanding hu-
man languages.
we speci   cally attempt to determine the sentiment of dif-
ferent granularities of phrases in a tree, within the stan-
ford sentiment treebank benchmark data (socher et al.,
2013). in obtaining the sentiment of a long piece of text,
early work often factorized the problem to consider smaller
pieces of component words or phrases with bag-of-words
or bag-of-phrases models (pang & lee, 2008; liu & zhang,
2012). more recent work has started to model compo-
sition (moilanen & pulman, 2007; choi & cardie, 2008;
socher et al., 2012; 2013; kalchbrenner et al., 2014), a
more principled approach to modeling the formation of se-
mantics. in this paper, we put the proposed lstm mem-
ory blocks at tree nodes   we replaced the tensor-enhanced
composition layer at each tree node presented in (socher
et al., 2013) with a s-lstm memory block. we used the
same dataset, the stanford sentiment treebank, to eval-
uate the performances of the models.
in addition to be-
ing a benchmark for much previous work, the data provide
with human annotations at all nodes of the trees, facilitat-
ing a more comprehensive exploration of the properties of
s-lstm.

4.1. data set

the stanford sentiment treebank (socher et al., 2013) con-
tains about 11,800 sentences from the movie reviews that
were originally discussed in (pang & lee, 2005). the sen-
tences were parsed with the stanford parser (klein & man-
ning, 2003). phrases at all the tree nodes were manually
annotated with sentiment values. we use the same split of
the training and test data as in (socher et al., 2013) to pre-
dict the sentiment categories of the roots (sentences) and all
phrases (including sentences). for the root sentiment, the
training, development, and test sentences are 8544, 1101,
and 2210, respectively. the phrase sentiment task includes
318582, 41447, and 82600 phrases for the three sets. fol-
lowing (socher et al., 2013), we also use the classi   cation
accuracy to measure the performances.

4.2. training details

as mentioned before, we follow (socher et al., 2013) to
minimize the cross-id178 error for all nodes or for roots
only, depending on speci   c experiment settings. for all
phrases, the error is calculated as a regularized sum:

e(  ) =

jlogyseni
ti

j +   (cid:107)  (cid:107)2

2

(15)

i

j

where yseni     rc  1 is predicted distribution and ti    
rc  1 the target distribution. c is the number of classes

(cid:88)

(cid:88)

or categories, and j     c denotes the j-th element of the
multinomial target distribution; i iterates over nodes,    are
model parameters, and    is a id173 parameter. we
tuned our model against the development data set as split
in (socher et al., 2013).

5. results
to understand the modeling advantages of s-lstm over
the structures, we conducted four sets of experiments.
default setting in the default setting, we conducted experi-
ments as in (socher et al., 2013). table 1 shows the accura-
cies of different models on the test set of the stanford senti-
ment treebank. we present the results on 5-category senti-
ment prediction at both the sentence level (i.e., the roots
column in the table) and for all phrases including roots (the
phrases column) 2. in table 1, nb and id166 are naive
bayes and support vector machine classi   ers, respectively;
rvnn corresponds to id56 in (socher et al., 2013). as de-
scribed earlier, we refer to id56s to as
rvnn to avoid confusion with recurrent neural networks.
rntn is different from rvnn in that when merging two
nodes to obtain the hidden vector of their parent, tensor is
used to obtain the second-degree polynomial interactions.

table 1. performances (accuracies) of different models on the test
set of stanford sentiment treebank, at the sentence level (roots)
and the phrase level.     shows the performance are statistically
signi   cantly better (p < 0.05) than the corresponding models.

models

nb
id166
rvnn
rntn
s-lstm

roots

phrases

41.0
40.7
43.2
45.7
48.9   

67.2
64.3
79.0
80.7
81.9   

table 1 showed that s-lstm achieved the best predictive
performance, when compared to all the models reported
in (socher et al., 2013). the s-lstm results reported here
were obtained by setting the size of the hidden units to
be 100, batch size to be 10, and learning rate to be 0.1.
in our experiments, we only tuned these hyper-parameters,
and we feel that more    ner tuning, such as discriminating
the classi   cation weights between the leaves (word embed-
ding) and other nodes, using different numbers of hidden
units for the memory blocks (e.g., for the hidden layers of

2the

stanford

package
(http://nlp.stanford.edu/sentiment/code.html)
ap-
proximate accuracies for 2-category sentiment, which are not
included here in the table.

corenlp

gives

only

words), or different initializations of id27, may
further improve the performances reported here.
as discussed earlier in this paper, the model we proposed
can be viewed as considering together a recursive and re-
current neural network. table 1 serves to compare s-lstm
with a directly comparable state-of-the-art recursive model
by replacing its composition layers with the s-lstm mem-
ory blocks (later we will also compare s-lstm with the
recurrent version without considering recursive structures.)
table 2 compares s-lstm with more state-of-the-art mod-
els that reported root-level results. the interesting work
described in (kalchbrenner et al., 2014) uses convolutional
networks instead of recursive nor recurrent networks to
model sentences. without using any additionally learned
id27, the accuracy of our s-lstm is 48.9%,
marginally higher than 48.5% reported in (kalchbrenner
et al., 2014). we also initialized our word representa-
tions using publicly available 300-dimensional glove vec-
tors (pennington et al., 2013), with which s-lstm attained
an accuracy of 50.1% on the root level task, comparable to
49.8% reported in (irsoy & cardie, 2014); the latter pro-
posed a deep recursive network that stacks multiple recur-
sive layers. note that s-lstm may be considered orthog-
onally with such models. while we report here the results
using external embedding to initialize words, comparison
without that may avoid being potentially confounded by the
choices of external embedding and the additional tuning in-
volved.

table 2. comparison with more models on    ve-category classi   -
cation accuracies on the test set of stanford sentiment treebank
at the sentence level (roots).

models

kalchbrenner et al., 2014
s-lstm

irsoy & cardie, 2014
s-lstm (glove-300)

roots

48.5
48.9

49.8
50.1

to evaluate the s-sltm model   s convergence behavior,
figure 3 depicts the converging time during training. more
speci   cally, we show two sub-   gures: one for roots (upper
sub-   gure) and the other for all phrases (lower sub-   gure).
the curves were obtained by using different initial learn-
ing rates corresponding to each best model. from these
   gures, we can observe that s-lstm converges faster than
the rntn. for instance, for the phrase-level task, s-lstm
started to converge after about 20 minutes but the rntn
needed over 180 minutes. s-lstm has much less parame-
ters than rntn and the forward and backward propagation

can be computed ef   ciently. we also observed that con-
verging time was not very sensitive to the initial learning
rates under adagrad used in our experiments.

figure 3. converging time during training for roots (the upper    g-
ure) and for all nodes (the lower    gure).

more real-life settings we further compare s-lstm with
rntn in two more experimental settings. in the    rst set-
ting we only keep the training signals at the roots to train
s-lstm and rntn, depicted as model (1) and (2) in ta-
ble 3. root lbls besides the model names stands for root
labels; that is, only the gold labels of the sentence level are
used to train the model. in most id31 circum-
stances, phrase level annotations are not available: most
nodes in a tree are fragments that may not be that interest-
ing; e.g., the fragment    of a good movie    3. also, annotat-
ing all phrases is expensive. however, these should not be
regarded as comments on the value of the sentiment tree-
bank. detailed annotations in the treebank enable much
interesting work to be possible, e.g., the study of the effect
of negation in changing sentiment (zhu et al., 2014a).
the second setting, corresponding to model (3) and (4) in
table 3, is only slightly different, in which we keep an-
notation for the tree leafs as well, to simulate that a sen-
timent lexicon is available and it covers all leafs (words)
(leaf lbls beside the model names stands for leaf la-
bels), and so there is no out-of-vocabulary concern. using
real sentiment lexicons is expected to have a performance

3phrase-level id31 is often de   ned over a very
small subset of phrases of interest, such as in the phrase and ap-
sect level tasks de   ned and studied in (wilson et al., 2005; mo-
hammad et al., 2013; zhu et al., 2014b; kiritchenko et al., 2014).

2530354045rntns   lstm060120180root accuracy (%)training time (minutes)6065707580rntns   lstm060120180phrase accuracy (%)training time (minutes)between the two settings here.
results in the table show that in both settings, s-lstm out-
performs rntn by a large margin. when only root labels
are used to train the models, s-lstm obtains an accuracy
of 43.5, compared with 29.1 of rntn. when the leaf la-
bels are also used, s-lstm achieves an accuracy of 44.1
and rntn 34.9. all these improvements are statistically
signi   cant (p < 0.05). for the rntn, without supervising
signals from the internal nodes, the composition parameters
may not be learned well, potentially because the tensor has
much more parameters to learn. on the other hand, through
controlling its gates, the s-lstm shows a very good ability
to learn from the trees.

table 3. performances of models trained with only root labels (the
   rst two rows) and models that use both root and leaf labels (the
last two rows).

models

(1) rntn (root lbls)
(2) s-lstm (root lbls)

(3) rntn (root + leaf lbls)
(4) s-lstm (root + leaf lbls)

roots

29.1
43.5   

34.9
44.1   

figure 4. accuracies at different depths (the upper    gure) in the
trees, or by different lengths of the phrases (the lower    gure).

performance over different levels of trees in figure 4,
we further depict the performances of models on different
levels of nodes in the trees. in this    gure, the x-axis corre-
sponds to different depths or lengths and y-axis is accuracy.
the depth here is de   ned as the longest distance between
the root of a phrase and their descendant leafs. the length
is simply the number of words of a node, where depth is
not necessarily to be length   e.g., a balanced tree with 4
leafs has different depths than the unbalanced tree with the
same number of leafs. the trends of the two    gure are sim-
ilar. in both    gures, s-lstm performs better at all depths,
showing its advantages on nodes at depth. as the deeper
levels of the tree tend to have more complicated syntax and
semantics, s-lstm can model such more complicated syn-
tax and semantics better.
explicit structures vs. no structures some research ef-
forts in the literature attempt to learn distributed represen-
tation by utilizing input structures when available, and oth-
ers prefer to assume chain-structured recurrent neural net-
works can actually capture the structures implicitly though
a linear coding process. in this paper, we attempt to give
some empirical evidences in our experiment setting by
comparing several different models. first, a special case
for the s-lstm model is considered, in which no senten-

tial structures are given. instead, words are read from left
to right and combined in that order. we call it left recur-
sive s-lstm, or s-lstm-lr in short. similarly, we also
experimented with a right recursive s-lstm, s-lstm-
rr, in which words are read from right to left instead.
since for these models, phrase-level training signals are not
available   the nodes here do not correspond to that in the
original standford sentiment treebank, but the roots and
leafs annotations are still the same, so we run two versions
of our experiments: one uses only training signals from
roots and the other includes also leaf annotations.
it can be observed from table 4 that the given parsing struc-
ture helps improve the predictive accuracy.
in the case
of using only root labels, the left recursive s-lstm and
right recursive s-lstm have similar performance (40.2
and 40.3, respectively), both inferior to s-lstm (43.5).
when using gold leaf labels, the gaps are smaller, but still,
using the parse structure is better. note that in real appli-
cations, where there is out-of-vocabulary issue (i.e., some
leafs are not seen in the sentiment dictionaries), the differ-
ence between s-lstm and the recursive version without
using the structures are expected to be between the gaps
we observed here.

50607080rntns   lstm234567accuracy (%)depths of nodes50607080rntns   lstm234567accuracy (%)lengths of nodestable 4. performances of models that do not use the given sen-
tence structures. s-lstm-lr is a degenerated version of s-
lstm that reads input words from left to right, and s-lstm-rr
reads words from right to left.

models

s-lstm-lr (root lbls)
s-lstm-rr (root lbls)
s-lstm (root lbls)

s-lstm-lr (root + leaf lbls)
s-lstm-rr (root + leaf lbls)
s-lstm (root + leaf lbls)

roots

40.2
40.3
43.5   

43.1
43.2
44.1   

6. conclusions
we aim to extend the conventional chain-structured long
short-term memory to explicitly consider structures.
in
this paper we particularly study tree structures, in which
the proposed s-lstm memory cell can re   ect the history
memories of multiple descendants through gated copying
of memory vectors. the model provides a principled way
to consider long-distance interplays over the structures. we
leveraged the model to learn distributed sentiment repre-
sentations for texts, and showed that it outperforms a state-
of-the-art recursive model by replacing its tensor-enhanced
composition layers with the s-lstm memory blocks. we
showed that the structure information is useful in helping
s-lstm achieve the state-of-the-art performance.
the research community seems to contain two lines of wis-
dom; one attempts to learn distributed representation by
utilizing structures when available, and the other prefers to
believe neural networks can actually capture the structures
implicitly though a linear coding process. in this paper, we
also attempt to give some empirical evidences toward an-
swering the question. it is at least for the settings of our
experiments that the explicit input structures are helpful in
inferring the high-level (e.g., root) semantics.

references
chater, n; conkey, p.    nding linguistic structure with re-
current neural networks. lawrence erlbaum assoc publ.
in proceedings of the annual conference of the cogni-
tive science scoiety. (pp. 402 - 407, 1992.

cho, kyunghyun, van merrienboer, bart, g  ulc  ehre, c   aglar,
bougares, fethi, schwenk, holger, and bengio, yoshua.
learning phrase representations using id56 encoder-
corr,
decoder for id151.

abs/1406.1078, 2014.

choi, yejin and cardie, claire. learning with composi-
tional semantics as structural id136 for subsentential
id31. in proceedings of emnlp, pp. 793   
801, honolulu, hawaii, 2008.

eck, douglas and schmidhuber, juergen. finding tempo-
ral structure in music: blues improvisation with lstm
recurrent networks. in proceedings of ieee workshop
on neural networks for signal processing, pp. 747   756.
ieee, 2002a.

eck, douglas and schmidhuber, juergen. learning the
in proceedings of

long-term structure of the blues.
icann, pp. 284   289, 2002b.

goller, christoph and kuchler, andreas. learning task-
dependent distributed representations by backpropaga-
tion through structure. in proceedings of iid98, pp. 347   
352, bochum, germany, 1996.

graves, alex. supervised sequence labelling with recur-
rent neural networks. phd thesis, technische universitat
munchen, 2008.

graves, alex, mohamed, abdel-rahman, and hinton, ge-
offrey e. id103 with deep recurrent neural
networks. corr, abs/1303.5778, 2013.

guo, hongyu, zhu, xiaodan, and min, renqiang. a deep
learning model for structured outputs with high-order
interaction. in proceedings of nips workshop on rep-
resentation and learning methods for complex outputs,
2014.

hammer, barbara, micheli, alessio, sperduti, alessandro,
and strickert, marc. a general framework for unsuper-
vised processing of structured data. neurocomputing,
57, 2004.

hermann, karl moritz and blunsom, phil. the role of
syntax in vector space models of compositional se-
in proceedings of acl, pp. 894   904, so   a,
mantics.
bulgaria, august 2013.

hochreiter, s. and schmidhuber, j. long short-term mem-

ory. neural computation, (8):1735   1780, 1997.

irsoy, ozan and cardie, claire. deep recursive neural net-
works for compositionality in language. in proceedings
of nips, pp. 2096   2104. 2014.

kalchbrenner, nal, grefenstette, edward, and blunsom,
phil. a convolutional neural network for modelling sen-
tences. proceedings of acl, june 2014.

kiritchenko, svetlana, zhu, xiaodan, and mohammad,
saif. nrc-canada-2014: detecting aspects and sen-
timent in customer reviews. in proceedings of the in-
ternational workshop on semantic evaluation, 2014.

socher, richard, huval, brody, manning, christopher d.,
and ng, andrew y. semantic compositionality through
in proceedings of
recursive matrix-vector spaces.
emnlp, 2012.

socher, richard, perelygin, alex, wu, jean y., chuang, ja-
son, manning, christopher d., ng, andrew y., and potts,
christopher. recursive deep models for semantic com-
positionality over a sentiment treebank. in proceedings
of emnlp, 2013.

starzyk, janusz a. and he, haibo. anticipation-based tem-
poral sequences learning in hierarchical structure. ieee
transactions on neural networks, 18(2), 2007.

sutskever, ilya, vinyals, oriol, and le, quoc v. sequence
to sequence learning with neural networks. corr,
abs/1409.3215, 2014.

tai, kai sheng, socher, richard, and manning, christo-
pher.
improved semantic representations from tree-
structured id137. in pro-
ceedings of acl (to appear), 2015.

vinyals, oriol, toshev, alexander, bengio, samy, and er-
han, dumitru. show and tell: a neural image caption
generator. corr, abs/1411.4555, 2014.

wilson, theresa, wiebe, janyce, and hoffmann, paul. rec-
ognizing contextual polarity in phrase-level sentiment
analysis. in proceedings of emnlp, vancouver, canada,
2005.

zhu, xiaodan, guo, hongyu, mohammad, saif, and kir-
itchenko, svetlana. an empirical study on the effect of
in proceedings of acl,
negation words on sentiment.
baltimore, maryland, usa, june 2014a.

zhu, xiaodan, kiritchenko, svetlana, and mohammad,
saif. nrc-canada-2014: recent improvements in the
id31 of tweets. in proceedings of the in-
ternational workshop on semantic evaluation, 2014b.

klein, dan and manning, christopher d. accurate unlex-
icalized parsing. in proceedings of acl, pp. 423   430,
sapporo, japan, 2003.

le, phong and zuidema, willem. compositional distri-
butional semantics with long short term memory. in
proceedings of joint conference on lexical and compu-
tational semantics (to appear), 2015.

liu, bing and zhang, lei. a survey of opinion mining and
id31. in mining text data, pp. 415   463.
2012.

liwicki, marcus, graves, alex, bunke, horst, and schmid-
huber, juergen. a novel approach to on-line handwriting
recognition based on bidirectional long short-term mem-
ory networks. in proceedings of icdar, 2007.

manning, christopher d. and sch  utze, hinrich. founda-
tions of statistical natural language processing. mit
press, cambridge, ma, usa, 1999. isbn 0-262-13360-
1.

identifying purpose behind electoral tweets.

mohammad, saif m., kiritchenko, svetlana, and martin,
joel.
in
proceedings of the 2nd international workshop on issues
of sentiment discovery and opinion mining, pp. 1   9,
2013.

moilanen, karo and pulman, stephen. sentiment compo-
in proceedings of ranlp, borovets, bulgaria,

sition.
2007.

pang, bo and lee, lillian. seeing stars: exploiting class
relationships for sentiment categorization with respect to
rating scales. in proceedings of acl, pp. 115   124, 2005.

pang, bo and lee, lillian. opinion mining and sentiment
analysis. foundations and trends in information re-
trieval, 2(1   2):1   135, 2008.

pennington,

jeffrey, socher, richard, and manning,
christopher. glove: global vectors for word representa-
tion. in proceedings of emnlp, august 2013.

pinheiro, p. h. o. and collobert, r. recurrent convolu-
tional neural networks for scene labeling. in proceedings
of icml, 2014.

socher, richard, lin, cliff c., ng, andrew y., and man-
ning, christopher d. parsing natural scenes and natural
language with id56s. in proceed-
ings of icml, 2011.

