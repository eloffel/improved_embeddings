representa)ons		

in	

deep	reinforcement	learning		

	

	

pieter	abbeel	

open	ai		/	berkeley	ai	research	lab	

	

many	slides	made	in	collabora<on	with	john	schulman	and	sergey	levine	

	

	

reinforcement	learning	

ut

[figure	source:	sugon	&	barto,	1998]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

some	reinforcement	learning	success	stories	

kohl	and	stone,	2004	

ng	et	al,	2004	

tedrake	et	al,	2005	

kober	and	peters,	2009	

mnih	et	al,	2015	

(a3c)	

silver	et	al,	2014	

lillicrap	et	al,	2015	

(dpg)	

(ddpg)	

schulman	et	al,	

2016	(trpo	+	gae)	

levine*,	finn*,	et	

al,	2016	
(gps)	

silver*,	huang*,	et	

al,	2016	
(alphago)	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

rl	algorithms	landscape	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

rl	algorithms	landscape	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

talk	outline	

n    classical	rl	

n    algorithms	

n    policy	gradients	
n    actor-cri<c	
n    id24	

n    representa<on	

n    representa<on	in	explora<on	

n    di   erent	approaches	/	architectures	

n    value	itera<on	networks	
n    predictron	
n    modular	networks	
n    op<on-cri<c	
n    feudal	networks	

n    meta	learning	

n    maml	
n    rl2	

policy	op<miza<on	

      (u|s)

ut

[figure	source:	sugon	&	barto,	1998]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

policy	op<miza<on	

n    consider	control	policy	parameterized	

by	parameter	vector	

   

      (u|s)

ut

max

   

e[

hxt=0

r(st)|      ]

	

n    ohen	stochas<c	policy	class	(smooths	

out	the	problem):	
																				:	id203	of	ac<on	u	in	state	s		
      (u|s)

[figure	source:	sugon	&	barto,	1998]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	policy	gradient	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleg,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleg,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleg,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleg,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleg,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleg,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	gradient:	validity	

ru (   )       g =

1
m

mxi=1

r    log p (    (i);    )r(    (i))

n    valid	even	if	r	is	discon<nuous,	and	
unknown,	or	sample	space	(of	paths)	
is	a	discrete	set		

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	gradient:	intui<on	

ru (   )       g =
n    gradient	tries	to:	

1
m

mxi=1

r    log p (    (i);    )r(    (i))

n   

increase	id203	of	paths	with	
posi<ve	r	

n    decrease	id203	of	paths	with	

nega<ve	r	

!	likelihood	ra<o	changes	probabili<es	of	experienced	paths,	
does	not	try	to	change	the	paths	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	ac<ons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	ac<ons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	ac<ons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	ac<ons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ra<o	gradient	es<mate	

  g =

=

=

=

1
m

1
m

1
m

1
m

mxk=1
mxk=1
mxk=1
mxk=1

t

t

|s(k)

r    log       (u(k)

r    log p (    (k);    )r(    (k))
h 1xt=0
h 1xt=0
h 1xt=0

r    log       (u(k)

r    log       (u(k)

|s(k)

|s(k)

t

t

t

t

)r(    (k))

r(k)
t0

)

h 1xt0=t
) h 1xt0=t
h 1xt0=t
mxk=1

b(s(k)

t0 ) =

1
m

r(k)
t0

t0 )!
t0   b(s(k)
r(k)

likelihood	ra<o	gradient	es<mate	

  g =

=

=

=

1
m

1
m

1
m

1
m

mxk=1
mxk=1
mxk=1
mxk=1

t

t

|s(k)

r    log       (u(k)

r    log p (    (k);    )r(    (k))
h 1xt=0
h 1xt=0
h 1xt=0

r    log       (u(k)

r    log       (u(k)

|s(k)

|s(k)

t

t

t

t

)r(    (k))

r(k)
t0

)

h 1xt0=t
) h 1xt0=t
h 1xt0=t
mxk=1

b(s(k)

t0 ) =

1
m

r(k)
t0

t0 )!
t0   b(s(k)
r(k)

likelihood	ra<o	gradient	es<mate	

  g =

=

=

=

1
m

1
m

1
m

1
m

mxk=1
mxk=1
mxk=1
mxk=1

t

t

|s(k)

r    log       (u(k)

r    log p (    (k);    )r(    (k))
h 1xt=0
h 1xt=0
h 1xt=0

r    log       (u(k)

r    log       (u(k)

|s(k)

|s(k)

t

t

t

t

)r(    (k))

r(k)
t0

)

h 1xt0=t
) h 1xt0=t
h 1xt0=t
mxk=1

b(s(k)

t0 ) =

1
m

r(k)
t0

t0 )!
t0   b(s(k)
r(k)

likelihood	ra<o	gradient	es<mate	

  g =

=

=

=

1
m

1
m

1
m

1
m

mxk=1
mxk=1
mxk=1
mxk=1

t

t

|s(k)

r    log       (u(k)

r    log p (    (k);    )r(    (k))
h 1xt=0
h 1xt=0
h 1xt=0

r    log       (u(k)

r    log       (u(k)

|s(k)

|s(k)

t

t

t

t

)r(    (k))

r(k)
t0

)

h 1xt0=t
) h 1xt0=t
h 1xt0=t
mxk=1

b(s(k)

t0 ) =

1
m

r(k)
t0

t0 )!
t0   b(s(k)
r(k)

likelihood	ra<o	policy	gradient	

n   

init	

      0

n    collect	data	{s,	u,	s   ,	r}	

n   

			
   i+1      i +    

1
m

mxk=1

h 1xt=0

|s(k)

t

) h 1xt0=t

t0 )!
t0   b(s(k)
r(k)

t

r    log       i(u(k)
h 1xt0=t

mxk=1

1
m

r(k)
t0

b(s(k)

t0 ) =

  	increase	logprob	of	ac<on	propor<onally	to	how	much	its	returns	are	beger	than	the	
expected	return	under	the	current	policy	
n    can	we	get	a	beger	b	?			

 [->    actor-critic   ] 

		
yes!

v    
 

es<ma<on	of						
v    

n    bellman	equa<on	for		

v    

v    (s) =xu

   (u|s)xs0

n    figed	v	itera<on:	

p (s0|s, u)[r(s, u, s0) +  v    (s0)]

v    
 0

init		

n   
n    collect	data	{s,	u,	s   ,	r}	

  x(s,u,s0,r)

n   

			
 i+1   min

kr + v    

 i(s0)   v (s)k2

2 +  k     ik2

2

actor-cri<c	

n    policy	gradient	+	figed	v	itera<on:	

n   

init		

      0

v    
 0

n    collect	data	{s,	u,	s   ,	r}	

n   

			

 i+1   min

 i(s0)   v (s)k2

2 +  k     ik2

2

  x(s,u,s0,r)
kr + v    
h 1xt=0
mxk=1

1
m

   i+1      i +    

r    log       i(u(k)

t

|s(k)

t

) h 1xt0=t

r(k)
t0   v    

 i(s(k)

t0 )!

n    generalized	advantage	es<ma<on	(schulman	et	al,	2016)	also	uses	learned	

value	func<on	to	beger	es<mate	   rst	term			
h 1xt0=t

r(k)
t0

id24	

n    bellman	equa<on	for	q*:	

q   (s, u) =xs0

n    figed	q	itera<on:	

p (s0|s, u)[r(s, u, s0) + max

u0

q   (s0, u0)]

n   

init		

q 0

n    collect	data	{s,	u,	s   ,	r}	

n   

			
 i+1   min

  x(s,u,s0,r)

kr + max

u0

q i(s0, u0)   q (s, u)k2

2 +  k     ik2

2

rl	algorithms	landscape	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

talk	outline	

n    classical	rl	

n    algorithms	

n    policy	gradients	
n    actor-cri<c	
n    id24	

n    representa<on	

n    representa<on	in	explora<on	

n    di   erent	approaches	/	architectures	

n    value	itera<on	networks	
n    predictron	
n    modular	networks	
n    op<on-cri<c	
n    feudal	networks	

n    meta	learning	

n    maml	
n    rl2	

id25	

[mnih	et	al,	nature	2015]	

conv1:	32	8x8	   lters	w/stride	4	
conv2:	64	4x4	   lters	with	stride	2	
conv3:	64	3x3	   lters	with	stride	1	

q 

id25	

[mnih	et	al,	nature	2015]	

trpo	+	gae	

[schulman	et	al,	2015]	

      

v 

a3c	

[mnih	et	al,	2016]	

       v 

dueling	network	

[wang et al, 2016] 

id25 

dueling 

q 

v 

q 

a = q-v 

trpo+gae	   	con<nuous	control		

n    feedforward:		h100	   	h50	   	h25	for	both	policy	and	value	

func<on	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

learning	locomo<on	(trpo	+	gae)	

[schulman,	moritz,	levine,	jordan,	abbeel,	2016]	

real robots 

[levine*,	finn*,	darrell,	abbeel,	jmlr	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

~ 92,000 
parameters 

guided	policy	search	[levine,	finn,	darrell,	abbeel,	jmlr	2016]	

learned	skills	

[levine*,	finn*,	darrell,	abbeel,	jmlr	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

other	architectures	

n    a3c	with	lstms	[mnih	et	al,	2016]	
n    memory	[oh	et	al,	2016]	

n    auxiliary	losses:		learning	to	navigate	[mirowski,	pascanu	et	al,	

2016];	[jaderberg,	mnih,	czarnecki	et	al,	2016]	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

auxiliary	losses	

unreal agent [jaderberg, mnih, czarnecki et al, 2016] 

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

talk	outline	

n    classical	rl	

n    algorithms	

n    policy	gradients	
n    actor-cri<c	
n    id24	

n    representa<on	

n    representa)on	in	explora)on	

n    di   erent	approaches	/	architectures	

n    value	itera<on	networks	
n    predictron	
n    modular	networks	
n    op<on-cri<c	
n    feudal	networks	

n    meta	learning	

n    rl2	
n    maml	

explora<on	through	reward	bonuses	

n   

idea:	infuse	addi<onal	reward	for	encountering	novelty	/	
learning	something	about	the	environment	
n    vime:	bayesian	neural	net	   	houthooh	et	al,	2016	
n    pixel-id98	density	es<ma<on	   	ostrovski	et	al,	2017	
n       	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

talk	outline	

n    classical	rl	

n    algorithms	

n    policy	gradients	
n    actor-cri<c	
n    id24	

n    representa<on	

n    representa<on	in	explora<on	

n    di   erent	approaches	/	architectures	

n    value	itera<on	networks	
n    predictron	
n    modular	networks	
n    op<on-cri<c	
n    feudal	networks	

n    meta	learning	

n    rl2	
n    maml	

value	itera<on	networks	

[tamar,	wu,	thomas,	levine,	abbeel,	nips	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

value	itera<on	network	(vin)	

n    vin	architecture:	

end-to-end	
di   eren)able	

planner!	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

vin	--	evalua<on	

gridworld	naviga)on	

mars	naviga)on	

0.39 

0.58 

con)nuous	domain	

[tamar,	wu,	thomas,	levine,	abbeel,	nips	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

con<nuous	domain	--	video	

[tamar,	wu,	thomas,	levine,	abbeel,	nips	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

1st	person	mapping	+	naviga<on	with	vin	

[gupta,	davidson,	levine,	sukthankar,	malik,	2017]	

aher	learning	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

most	closely	related	work	

n    embed	to	control	   	wager,	springenberg,	boedecker,	riedmiller,	

2015	

n    hindsight	mpc	   	aviv	tamar	et	al.,	icra	2017	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

talk	outline	

n    classical	rl	

n    algorithms	

n    policy	gradients	
n    actor-cri<c	
n    id24	

n    representa<on	

n    representa<on	in	explora<on	

n    di   erent	approaches	/	architectures	

n    value	itera<on	networks	
n    predictron	
n    modular	networks	
n    op)on-cri)c	
n    feudal	networks	

n    meta	learning	

n    rl2	
n    maml	

predictron	

[silver	et	al,	2016]	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

predictron	

[silver	et	al,	2016]	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

modular	networks	

[devin, gupta et al, 2016] 

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

robot 1: 
3 dof arm 

robot 1 

robot 2: 
4 dof 
arm 

robot 2 

[devin, gupta et al, 2016] 

task 1: 

opening a 

drawer 

task 2: 

pushing a 

block 

task 1 

task 2 

robot 1 

robot 2 

task 1 

task 2 

[devin, gupta et al, 2016] 

robot 1 

robot 2 

task 1 

task 2 

[devin, gupta et al, 2016] 

task 2 

robot 1 

robot 2 

task 1 

task 2 

[devin, gupta et al, 2016] 

robot 2 

task 2 

robot 1 

robot 2 

task 1 

task 2 

[devin, gupta et al, 2016] 

robot 2 

task 2 

observation
s 

robot 1 

robot 2 

task 1 

task 2 

[devin, gupta et al, 2016] 

ac<ons	

robot 2 

task 2 

observation
s 

op<on-cri<c	architecture	

[bacon,	harb,	precup,	2017]	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

feudal	networks	

[dayan	and	hinton,	1993;	vezhnevets	et	al,	2017]	

pieter	abbeel	--	uc	berkeley	/	openai	/	gradescope	

talk	outline	

n    classical	rl	

n    algorithms	

n    policy	gradients	
n    actor-cri<c	
n    id24	

n    representa<on	

n    representa<on	in	explora<on	

n    di   erent	approaches	/	architectures	

n    value	itera<on	networks	
n    predictron	
n    modular	networks	
n    op<on-cri<c	
n    feudal	networks	

n    meta	learning	

n    maml	
n    rl2	

learning	useful	representa<ons	with	deep	learning	

where	are	the	   id163   	features	of	motor	control?	

the	trouble	with	rl	

       large-scale	
       emphasizes	diversity	
       evaluated	on	generaliza<on	

       small-scale	
       emphasizes	mastery	
       evaluated	on	performance	
       can	we	force	rl	to	generalize?	

mul<-task	training	for	adaptability	

xi

li(    +    r   ri(   ))

[finn,	abbeel,	levine,	2017]	

model-agnos<c	meta-learning:	forward/backward	locomo<on	

aher	maml	training	

aher	1	gradient	step	

(forward	reward)	

aher	1	gradient	step	
(backward	reward)	

model-agnos<c	meta-learning	benchmark	results	

model-agnos<c	meta-learning:	forward/backward	locomo<on	

aher	maml	training	

aher	1	gradient	step	
(backward	reward)	

aher	1	gradient	step	

(forward	reward)	

talk	outline	

n    classical	rl	

n    algorithms	

n    policy	gradients	
n    actor-cri<c	
n    id24	

n    representa<on	

n    representa<on	in	explora<on	

n    di   erent	approaches	/	architectures	

n    value	itera<on	networks	
n    predictron	
n    modular	networks	
n    op<on-cri<c	
n    feudal	networks	

n    meta	learning	

n    maml	
n    rl2	

speed	of	learning	

deep	rl	(id25)	

score: 18.9

#experience	
measured	in	real	
<me:	40	days	

   slow   

vs. 

human	

score: 9.3 

#experience	
measured	in	real	
<me:	2	hours	

   fast   

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

star<ng	observa<ons	

n    trpo,	id25,	a3c	are	fully	general	rl	algorithms	

n   

i.e.,	for	any	mdp	that	can	be	mathema<cally	de   ned,	these	algorithms	
are	equally	applicable	

n    mdps	encountered	in	real	world	

	=	<ny,	<ny	subset	of	all	mdps	that	could	be	de   ned	

n    can	we	design	   fast   	rl	algorithms	that	take	advantage	of	

such	knowledge?	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

research	ques<ons	

n    how	to	acquire	a	good	prior	for	real-world	mdps?	

n    or	for	starters,	e.g.,	for	real-games	mdps?	

n    how	to	design	algorithms	that	make	use	of	such	prior	

informa<on?	

key	idea:	learn	a	fast	rl	algorithm	that	encodes	this	prior	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

formula<on	

n    given:	distribu<on	over	relevant	mdps	

n    train	the	fast	rl	algorithm	to	be	fast	on	a	training	set	of	mdps	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

learning	the	fast	rl	algorithm	

n    representa<on	of	the	fast	rl	algorithm:		

n    id56	=	generic	computa<on	architecture	
n    di   erent	weights	in	the	id56	means	di   erent	rl	algorithm	
n    di   erent	ac<va<ons	in	the	id56	means	di   erent	current	policy	

n    training	setup:	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

formula<on	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

alterna<ve	view	on	rl2	

n    id56	=	policy	for	ac<ng	in	a	pomdp	

n    part	of	what   s	not	observed	in	the	pomdp	is	which	mdp	the	agent	is	in	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

related	work	

n    wang	et	al.,	(2016)	learning	to	reinforcement	learn,	in	submission	to	iclr	

2017,	

n    chen	et	al.	(2016)	learning	to	learn	for	global	op<miza<on	of	black	box	

func<ons	

n    andrychowicz	et	al.,	(2016)	learning	to	learn	by	gradient	descent	by	

gradient	descent	

n    santoro	et	al.,	(2016)	one-shot	learning	with	memory-augmented	neural	

networks	

n    larochelle	et	al.,	(2008),	zero-data	learning	of	new	tasks.	
n    younger	et	al.	(2001),	meta	learning	with	backpropaga<on		
n    schmidhuber	et	al.	(1996),	simple	principles	of	metalearning	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on	

n    mul<-armed	bandits	

n    provably	(asympto<cally)	op<mal	
rl	algorithms	have	been	invented	
by	humans:	gizns	index,	ucb1,	
thompson	sampling,	   	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

5-armed	bandit	

(source:	ebay)	

evalua<on	

n    mul<-armed	bandits	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on	

n    mul<-armed	bandits	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on:	tabular	mdps	

n    provably	(asympto<cally)	

op<mal	algorithms:		
n    beb,	psrl,	ucrl2,	   	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on:	tabular	mdps	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on:	tabular	mdps	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on:	visual	naviga<on	

(built	on	top	of	vizdoom)	

agent   s view 

small maze 

large maze 

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on:	visual	naviga<on	

before learning 

after learning 

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

evalua<on	

[duan,	schulman,	chen,	bartleg,	sutskever,	abbeel,	2016]	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

openai	universe	

pieter	abbeel	   	openai	/	uc	berkeley	/	gradescope	

how	to	learn	more	and	get	started?		

n    (1)	deep	rl	courses	

n    cs294-112	deep	reinforcement	learning	(uc	berkeley):	

hgp://rll.berkeley.edu/deeprlcourse/	by	sergey	levine,	john	schulman,	chelsea	
finn	
hgp://www0.cs.ucl.ac.uk/sta   /d.silver/web/teaching.html	by	david	silver	

n    compm050/compgi13	reinforcement	learning	(ucl):	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

how	to	learn	more	and	get	started?	

n    (2)	deep	rl	code	bases	

n   

rllab:	hgps://github.com/openai/rllab			
duan,	chen,	houthooh,	schulman	et	al	

n    gps:	hgp://rll.berkeley.edu/gps/	
finn,	zhang,	fu,	tan,	mccarthy,	
schar   ,	stadie,	levine	

		

	
n    rlpy:	

hgps://rlpy.readthedocs.io/en/latest/	
geramifard,	klein,	dann,	dabney,	how	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

how	to	learn	more	and	get	started?	

	

n    (3)	environments	

n    deepmind	lab	/	labyrinth	(deepmind)	

n    arcade	learning	environment	(ale)	

(bellemare	et	al,	jair	2013)	

	

   	

n    openai	gym:	hgps://gym.openai.com/	

n    mujoco:		hgp://mujoco.org	(todorov)	

n    minecrat	(microsoh)	

n    universe:	hgps://universe.openai.com/	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

current	/	future	direc<ons	

n   

faster	learning	/	hierarchy	

n   

unsupervised	/	semisupervised	learning	

n   

explora<on	(stadie,	levine,	abbeel	2015;	
houthooh,	duan,	chen,	schulman	abbeel,	
2016)	

n    meta-learning:	rl2	(duan,	schulman,	chen,	

bartleg,	sutskever,	abbeel,	2016);	maml	(finn,	
abbeel,	levine,	2017)	

n   

transfer	learning	

n    modular	networks	(devin,	gupta,	darrell,	

abbeel,	levine,	2017)	;	invariant	feature	spaces	
(gupta	devin,	liu,	abbeel,	levine,	2017)	

n   

domain	randomiza<on	(tobin,	fong,	schneider,	
zaremba,	abbeel,	2017)	

n   

n   

infogan	(chen,	duan,	houthooh,	schulman,	sutskever,	abbeel	2016),	
vlae	(chen,	kigma,	salimans,	duan,	dhariwal,	schulman,	sutskever,	
abbeel,	2017)	

semisupervised	rl	(finn,	yu,	fu,	abbeel,	levine,	2017)	

n   

n   

grounded	language	/	mul<-agent	

n   

   inven<ng   	language	(mordatch	&	abbeel,	2017)	

imita<on	

n   

n   

first-person	from	vr	tele-op	(mccarthy,	zhang,	jow,	lee,	goldberg,	
abbeel,	2017)	

third-person	(stadie,	abbeel,	sutskever,	2017)	

n   

safe	learning	

n   

value	alignment	/	ai	safety	

n   

kahn,	villa   or,	pong,	abbeel,	levine,	2017;	
held,	mccarthy,	zhang,	shentu,	abbeel,	2016	

n   

n   

cirl	(had   eld-menell,	dragan,	abbeel,	russell,	2016),	o   -switch	
(had   eld	menell,	dragan,	abbeel,	russell,	2017)	

communica<on	(huang,	held,	abbeel,	dragan,	2017)	

