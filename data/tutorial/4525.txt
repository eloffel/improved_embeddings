tutorial:		part	1

optimization	for	machine	learning

elad hazan

princeton	university

+	help	from	sanjeev	arora,	yoram singer

ml	paradigm

machine

{a}       %

distribution	
over	

this	tutorial	- training	the	machine
   
   

efficiency
generalization

chair/car

    =    )*+*,-.-+/(    )			

label

agenda

1. learning	as	mathematical	optimization

    stochastic	optimization,	erm,	online	regret	minimization
    offline/online/stochastic	gradient	descent

2. id173

    adagrad and	optimal	id173

3. gradient	descent++	

    frank-wolfe,	acceleration,	variance	reduction,	second	order	methods,	
non-convex	optimization

not	touch	upon:

    parallelism/distributed	computation	(asynchronous	optimization,	
hogwild	etc.),	bayesian	id136	in	graphical	models,	markov-chain-
monte-carlo,	partial	information	and	bandit	algorithms

mathematical	optimization

input:			function	    :           ,		for	           8
output:		minimizer		           ,	such	that                    	   	           

accessing	f?		(values,	differentials,	   )

generally	np-hard,	given	full	access	to	function.

learning	=	optimization	over	data	
(a.k.a.	empirical	risk	minimization)

we   re screwed.

but generally speaking...

! local (non global) minima of f0
! all kinds of constraints (even restricting to continuous functions):

what is optimization

fitting	the	parameters	of	the	model	(   training   )	=		optimization	
problem:

h(x) = sin(2  x) = 0

250

200

150

100

50

0

argminb   cd		1     g    i    ,    i,    i

ikl	.m	,

   50
3

+        

duchi (uc berkeley)

2

1

0

   1

   2

   3

   3

   2

3

2

1

0

   1

id76 for machine learning

m	=	#	of	examples			(a,b)	=	(features,	labels)
d	=	dimension

example:	linear	classification

find	hyperplane	(through	the	origin	w.l.o.g)	
such	that:

given	a	sample	    =     l,    l,   ,     ,,    , 	,	
    =argminbql# of	mistakes	=	
argminbql	
    			s.    .	                	(    z    i        i|
for		       ,    i,    i =_1				                  
argmin]qll,      (    ,    i,    i)
0				           =    

i

np	hard!	

sum	of	signs	   global	optimization	np-hard!
but	locally	verifiable   

local	property	that	ensures	global	optimality?

convexity

a	function	    :    8       	 is	convex	if	and	only	if:
     12    +12        12         +12        

    informally:	smiley	j
    alternative	definition:	

fy    fx +        (    )   (           )

    

    

convex	sets

    ,           		   (      +      )       	

set	k	is	convex	if	and	only	if:

loss	functions	       ,    i,    i =   (    z    i       i)

convex	relaxations	for	linear	(&kernel)	
classification

    =argminbql     			s.    .	                	(    z    i        i|

1. ridge	/	linear	regression	              i,    i =            i       i l
              i,    i =max{0,1       i	           i}
              i,    i =log	(1+    qrs   bt*s)

2. id166																			
3. logistic	regression	

we	have:	cast	learning	as	mathematical	optimization,	
argued	convexity	is	algorithmically	important	

next	   algorithms!

gradient	descent,	constrained	set

    .ul	       .                   .
    .ul=argmin]   x|    .ul       |	

 [rf (x)]i =  

@
@xi

f (x)

p*

p3

p2

p1

    .ul	       .                   .
    .ul=argmin]   x|    .ul       |	

convergence	of	gradient	descent

theorem:	for	step	size	    = yz z
     1    g    .
.

   minb      x	            +            
|            .|       
       ,           			.		|           |       

where:
    g	=	upper	bound	on	norm	of	gradients

    d	=	diameter	of	constraint	set

    .ul	       .                   .
    .ul=argmin]   x|    .ul       |	

proof:
1. observation 1: 

x      y(cid:127)ul l= x      x(cid:127)l   2            (    .)(    .          )+    l        (    .)l

x          .ul l    x      y.ul l

2. observation 2: 

this is the pythagorean theorem:

    .ul	       .                   .
    .ul=argmin]   x|    .ul       |	

thus: 

2. observation  2: 

proof:
1. observation  1: 

x      y(cid:127)ull= x      x(cid:127)l   2            (    .)(    .          )+    l        (    .)l
x          .ull    x      y.ull
x      x(cid:127)ull    x      x(cid:127) l   2            (    .)(    .          )+    l    l
   1    g            .     .          
1    g        .               
and hence:        (1    g    .)	                  	
.
.
+    2    l
   1    g12     x      x(cid:127)ull   x      x(cid:127)l
.
    1       2        l	+    2    l               

.

recap

theorem:	for	step	size	    = yz z
     1    g    .
   minb      x	            +            
.
thus,	to	get	    -approximate	solution,	apply	o l(cid:130)(cid:131)

gradient	iterations.	

gradient	descent	- caveat

for	erm	problems	

argminb   cd		1     g    i    ,    i,    i

ikl	.m	,

+        

1. gradient	depends	on	all	data	
2. what	about	generalization?	

p*

p3

p2

p1

next	few	slides:	

simultaneous	optimization	and	generalization
   faster	optimization!	(single	example	per	iteration)

statistical	(pac)	learning

a	      =	{(    ,    )}

nature:	i.i.d from	distribution	 d over	

(a1,b1)

(am,bm)

h1
h2

learner:
hypothesis	h

loss,	e.g.		      ,     ,     =                 l
                =    *,r   y[   (   ,     ,    ]
hypothesis	 class	h:	x	->	y		is	learnable	if		       ,    >0	 exists	algorithm	s.t. after	seeing	m
examples,	for	    =                (    ,    ,                                    (    ))
err(h)     min
h   2h

err(h   ) +    

finds	h s.t. w.p.	1-   :	

hn

more	powerful	setting:
online	learning	in	games
iteratively,	for	t=1,2,   ,    
player:		   .       
adversary:	(    .,    .)       
loss	   (   .,(    .,    .))

h

goal:	minimize	(average,	expected)	regret:

a	x	b

1

t "xt

`(ht, (at, bt)   min

`(h   , (at, bt))#  !t!1
from	this	point	onwards:				    .     =   (    ,    .,    .) =	loss	for	one	example

vanishing	regret	   generalization	in	pac	setting!	(online2batch)

h   2hxt

0

can	we	minimize	regret	efficiently?

online	gradient	descent	[zinkevich    05]
yt+1 = xt      rft(xt)
xt+1 = arg min

x2k kyt+1   xtk

theorem:		regret	=	       .    .           .        =

.

.

         

    .           .(    .)

analysis

observation	1:
kyt+1   x   k2 = kxt   x   k2   2   rt(x      xt) +    2krtk2
observation	2:		(pythagoras)

thus:

convexity:

kxt+1   x   k     kyt+1   x   k
kxt+1   x   k2     kxt   x   k2   2   rt(x      xt) +    2krtk2
xt

[ft(xt)   ft(x   )]    xt rt(xt   x   )
(kxt   x   k2   kxt+1   x   k2) +    xt krtk2
   kx1   x   k2 +    t g = o(pt )

   

   

1
   

1

lower	bound

regret =    (pt )

       =    1,1,    l     =    	,    l     =       

    2	loss	functions,	t iterations:

    second	expert	loss	=	first	*	-1

    expected	loss	=	0		(any	algorithm)
    regret	=	(compared	to	either	-1	or	1)

e[|#10s   #( 1)0s|] =    (pt )

! all kinds of constraints (even restricting to continuous functions):

h(x) = sin(2  x) = 0

stochastic	gradient	descent

duchi (uc berkeley)

learning	problem		argminb   cd         =		    (*s,rs)   i    ,    i,    i
random	example:				    .     =   i    ,    i,    i
1. we	have	proved:	(for	any	sequence	of	    .)
   minb      x	1    g    .          
1    g    .       .
+            
.
.
	          1    g    .
   minb      x	           	         1    g    .   (    .          )
.
.
total	running	time	for	     generalization	error.	

o 8(cid:130)(cid:131) vs.	o ,8(cid:130)(cid:131)

taking	(conditional)	expectation:

one	example	per	step,	same	convergence	as	gd,	&	gives	direct	generalization!		
(formally	needs	martingales)

2.

250

200

150

100

50

0

   50
3

2

1

0

   1

   2

   3

   3

   2

3

2

1

0

   1

id76 for machine learning

]                

stochastic	vs.	full	gradient	descent

id173	&	
gradient	descent++

why	   regularize   ?	

    statistical	learning	theory	/	
occam   s	razor:
#	of	examples	needed	to	learn	
hypothesis	class	~	it   s	   dimension   
    vc	dimension
    fat-shattering	dimension
    rademacher width
    margin/norm	of	linear/kernel	classifier

    pac	theory:	id173	 <->		reduce	complexity	
    regret	minimization:	id173	<->		stability

minimize	regret:	best-in-hindsight

ft(x   )

    most	natural:

regret =xt

x   2kxt
ft(xt)   min
.ql
    .=argminb   x	g    i    
	
ikl
.
=    .ul
    .(cid:158)=argminb   x	g    i    
ikl
    so	if	    .   	    .ul,	we	get	a	regret	bound
    but	instability	    .       .ul can	be	large!

    provably	works	[kalai-vempala   05]:

    add	id173:

fixing	ftl:	follow-the-regularized-leader	
(ftrl)

    linearize:	replace	ft by	a	linear	function,	        .    . z    
    .=argminb   x	 g     .       	+1            	
ikl   .ql
    .       .		       .ul	 =    (    )

    r(x)	is	a	strongly	convex	function,	ensures	stability:

ftrl	vs.	gradient	descent

            =ll          l

xt = arg min

x2k pt 1
=qk       pt 1

i=1 rfi(xi)>x + 1
i=1 rfi(xi)   

    r(x)

    essentially	ogd:	starting	with	y1 =	0,	for t	=	1,	2,	   

xt =qk(yt)

yt+1 = yt      rft(xt)

ftrl	vs.	multiplicative	weights	

    experts	setting:	    	=  % distributions	over	experts
       .     =    .z    ,	where	ct is	the	vector	of	losses
            =	       ilog    i

:	negative	id178

i

xt = arg min

x2kpt 1
= exp       pt 1

i=1 rfi(xi)>x + 1
i=1 ci    /zt

    r(x)

id172	

constant

entrywise
exponential

    gives	the	multiplicative	weights	method!	

ftrl	    online	mirror	descent

xt = arg min

i=1 rfi(xi)>x + 1

    r(x)

x2kpt 1

bregman projection:

k(y) = arg min

qr
br(xky) := r(x)   r(y)   rr(y)>(x   y)

br(xky)

x2k

xt =qr
yt+1 = (rr) 1(rr(yt)      rft(xt))

k(yt)

adaptive	id173:	adagrad

    consider	generalized	linear	model,	prediction	is	function	of	    z    
    ogd	update:	    .ul=    .           .=    .	              .,    .,        .

        .     =       .,    .,        .

    features	treated	equally	in	updating	parameter	vector

    in	typical	text	classification	tasks,	feature	vectors	at are	very	sparse,	
slow	learning!

    adaptive	id173:	per-feature	 learning	rates

optimal	id173

    the	general	rftl	form	

ikl   .ql

    .=argminb   x	 g     i    	+1            	
         =        l					    .    .			       0		,                         =    

    which	regularizer to	pick?		
    adagrad:	treat	this	as	a	learning	problem!
family	of	id173s:	

    objective	in	matrix	world:		best	regret	in	hindsight!

    for	t =	1,	2,   ,	

adagrad (diagonal	form)

    set	    l       	arbitrarily
1. use	    . obtain	ft
2. compute	    .ul as	follows:
gt = diag(pt
yt+1 = xt      g 1/2
xt+1 = arg min
	 ,	can	be	      better	than	sgd

                .,il
i
.

    regret	bound: [duchi,	hazan,	singer	   10]	

x2k

i=1 rfi(xi)rfi(xi)>)
t rft(xt)
(yt+1   x)>gt(yt+1   x)

    infrequently	occurring,	or	small-scale,	features	have	small	influence	
on	regret	(and	therefore,	convergence	to	optimal	parameter)

agenda

1. learning	as	mathematical	optimization

    stochastic	optimization,	erm,	online	regret	minimization
    offline/stochastic/online	gradient	descent

2. id173

    adagrad and	optimal	id173

3. gradient	descent++	

    frank-wolfe,	acceleration,	variance	reduction,	second	order	methods,	
non-convex	optimization

