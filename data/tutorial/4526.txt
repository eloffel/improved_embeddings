tutorial:		part	2

optimization	for	machine	learning	

elad hazan

princeton	university

+	help	from	sanjeev	arora	&	yoram singer

agenda

1. learning	as	mathematical	optimization

    stochastic	optimization,	erm,	online	regret	minimization
    online	gradient	descent

2. id173

    adagrad and	optimal	id173

3. gradient	descent++	

    frank-wolfe,	acceleration,	variance	reduction,	second	order	methods,	
non-convex	optimization

accelerating	gradient	descent?	

1. adaptive	id173	(adagrad)

works	for	non-smooth&non-convex

2. variance	reduction

uses	special	erm	structure
very	effective	for	smooth&convex

3. acceleration/momentum

smooth	convex	only,	general	purpose	optimization	
since	80   s

condition	number	of	convex	functions

defined	as	    =#$,	where	(simplified)
0                  +                    
     =	strong	convexity,	     =	smoothness
                  +                    

non-convex	smooth	functions:	(simplified)

why	do	we	care?	

well-conditioned	functions	exhibit	much	faster	optimization!
(but	equivalent	via	reductions)	

examples

smooth	gradient	descent

the	descent	lemma,	    -smooth	functions:		(algorithm:	    012=    0           4)

        012            0           0    012       0 +        0       012+
      14    >    0 +
0

=       +        +     0 +=   14        0+
thus,	for	m-bounded	functions:	(        0        )
   2               ;        (    2)=>    (    012)           0
0
    0+   8            

thus,	exists	a	t for	which,

smooth	gradient	descent

conclusions:	for		    012=    0           4 and	t=   2c 	,	finds
for	convex	functions	implies	        0                       (    ) (faster	for	

1. holds	even	for	non-convex	functions
2.

    0+       

smooth!)

non-convex	stochastic gradient	descent

the	descent	lemma,	    -smooth	functions:		(algorithm:	    012=    0           0n)
            012            0               4    012       0 +        0       012+
=            p0           4+        4n + =           4++    +            4n +

thus,	for	m-bounded	functions:	(        0        )

=           4++    +    (    4++                p0)
t=     m        + 													   							   0y;	.		    0+       

controlling	the	variance:
interpolating	gd	and	sgd

model:	both	full	and	stochastic	gradients.	estimator	combines	
both	into	lower	variance	rv:		

    012=    0           p        0        p        [ +        (    [)	
every	so	often,	compute	full	gradient	and	restart	at	new	    [.	
	    =        

theorem:		[schmidt,	leroux,	bach	   12;	johnson	and	zhang	   13;		
mahdavi,	zhang,	jin    13]	
variance	reduction	for	well-conditioned	functions

0                  +                    		,
produces	an	     approximate	solution	in	time		
         +        	log	1    

     should	be	
1    

interpreted	as	

acceleration/momentum	[nesterov    83]

    optimal	gradient	complexity	(smooth,	convex)

    modest	practical	improvements,	non-convex	   momentum   	
methods.

    with	variance	reduction,	fastest	possible	running	time	of	first-

order	methods:	      (    +         )	    	log	1    

[woodworth,	srebro    15]	    tight	lower	bound	w.	gradient	oracle

experiments	w.	convex	losses

improve	upon	gradientdescent++	?

next	few	slides:	

move	from	first	order	(gd++)	to	second	order

higher	order	optimization	

    gradient	descent	    direction	of	steepest	descent
    second	order	methods	    use	local	curvature

newton   s	method	(+	trust	region)

    012=    0	       	[    +    (    )]p2	        (    )
for	non-convex	function:	can	move	to	   

solution:	 solve	a	quadratic	approximation	in	a	
local	area	(trust	region)

    2

    +

    m

newton   s	method	(+	trust	region)

    012=    0	       	[    +    (    )]p2	        (    )

d3 time	per	iteration!
infeasible	for	ml!!		

till	recently   

    2

    +

    m

speed	up	the	newton	direction	computation??

    spielman-teng    04:	diagonally	dominant	systems	of	equations	in	
linear	time!	
    2015	godel prize		
    used	by	daitch-speilman for	faster	flow	algorithms

    erdogu-montanari    15:	low	rank	approximation	&	inversion	by	
sherman-morisson
    allow	stochastic	information
    still	prohibitive:		rank	*	d2

    (cid:132)
    012=    0	       	[    +    (    )]p2	        (    )

stochastic	newton?

    erm,	rank-1	loss:				argminr     s   	u[       ;    s,    s +2+|    |+]
    +y=azaz{             ;    s,bz +    							    		~	    [1,   ,    ]

    unbiased	estimator	of	the	hessian:

    clearly		        +y =    +     ,	but         +yp2    	    +    	

p2

circumvent	hessian	creation	and	inversion!	

naturals		i	       

for	any	distribution	on	

    3	steps:		

    (1)	represent	hessian	inverse	as	infinite	series

    p+= >     	       + s

s(cid:134)[	0(cid:135)	(cid:136)

    (2)	sample	from	the	infinite	series	(hessian-gradient	product)	,	once

    +    p2        =>           +     s	    f	=	    s   (cid:137)	           +     s	    f    1pr[    ]	
s
=es   (cid:137),(cid:139)   [s] (cid:140)            +    (cid:139)    f    1pr[    ]

(cid:139)(cid:134)2	0(cid:135)	s

    (3)	estimate	hessian-power	by	sampling	i.i.d.	data	examples

single	example	
vector-vector	
products	only

linear-time	second-order	stochastic	algorithm	
(lissa)

    use	the	estimator	    p+    (cid:141) defined	previously	
    compute	a	full	(large	batch)	gradient		    f	
    move	in	the	direction	    p+    (cid:141)	        
    theoretical	running	time	to	produce	an	     approximate	solution	for	    
             	log	1    +         	    	log	1    

well-conditioned	functions	(convex):	[agarwal,	bullins,	hazan	   15]

1.
2.

faster	than	first-order	methods!
indications	this	is	tight	[arjevani,shamir    16]

what	about	constraints??

next	few	slides	    projection	free	
(frank-wolfe)	methods

recommendation	systems

users

1

1

movies
5

4

2

4

5

2

5

1

3

2

5

1

3
1

rating of user i

for movie j

recommendation	systems

users

movies
5
2
2
2
4
4
3
3
5
1
2
1
1
4
3
3

1
3
1
3
4
2
5
5

5
5
1
3
4
3
1
2

1
3
5
3
2
3
1
4

4
4
2
3
3
4
2
5

complete 

missing entries

recommendation	systems

users

movies
5
2
2
2
4
4
3
3
5
1
2
1
1
4
3
3

1
3
1
3
4
2
5
5

5
5
1
5
4
3
1
2

1
3
5
3
2
3
1
4

4
4
2
3
3
4
2
5

get new data

recommendation	systems

users

movies
5
5
2
3
2
4
2
4
5
2
3
4
1
1
3
3

4
3
3
5
1
2
1
5

4
1
1
5
5
3
1
2

1
2
5
3
3
2
1
4

2
4
2
3
3
4
1
5

re-complete 
missing entries

assume	low	rank	of	   true	matrix   ,	convex	relaxation:	bounded	trace	
norm

bounded	trace	norm	matrices

    trace	norm	of	a	matrix	=	sum	of	singular	values
    k =	{	x |	x is	a	matrix	with	trace	norm	at	most	d }

    computational	bottleneck:	projections	on	k		
require	eigendecomposition:	o(n3) operations

    but:	linear	optimization	over	k is	easier
computing	top	eigenvector;	o(sparsity) time

projections	   linear	optimization

1. matrix	completion	(k =	bounded	 trace	norm	matrices)

eigen decomposition	 								
largest	singular	vector	computation

2. online	routing	(k =	flow	polytope)

conic	optimization	over	flow	polytope
shortest	path	computation

3.

rotations	(k =	rotation	matrices)
conic	optimization	over	rotations	set
wahba   s algorithm	    linear	time

4. matroids (k =	matroid polytope)
convex	opt.	via	ellipsoid	method
greedy	algorithm	    nearly	linear	time!

conditional	gradient	algorithm	[frank,	wolfe	   56]
convex	opt	problem:

min(cid:143)   (cid:145)    (    )

   
   

f is	smooth,	convex
linear	opt	over	k is	easy

vt = arg min

x2k rf (xt)>x
xt+1 = xt +    t(vt   xt)

2. no	learning	rate.	    0   20 (independent	of	diameter,	gradients	etc.)

1. at	iteration	t: convex	comb.	of	at	most	t vertices	(sparsity)

fw	theorem

xt+1 = xt +    t(vt   xt) , vt = arg min

x2k r>t x

theorem:

f (xt)   f (x   ) = o(

1
t

)

vt+1

rf (xt)

xt+1

xt

proof,		main	observation:
f (xt+1)   f (x   ) = f (xt +    t(vt   xt))   f (x   )
    f (xt)   f (x   ) +    t(vt   xt)>rt +    2
    f (xt)   f (x   ) +    t(x      xt)>rt +    2
    f (xt)   f (x   ) +    t(f (x   )   f (xt)) +    2
    (1      t)(f (xt)   f (x   )) +

   2
t  
2

d2.

t

t

 
2kvt   xtk2
 
2kvt   xtk2
 
2kvt   xtk2

t

 -smoothness of f

optimality of vt

convexity of f

thus:		ht =	f(xt)	    f(x*)

ht+1     (1      t)ht + o(   2
t )

   t, ht = o(

1
t

)

   
   

online	conditional	gradient

for	t =	1,	2,   ,	

set	    2       	arbitrarily
1. use	    0,	obtain	ft
compute	    012 as	follows
x2k    pt

2.

vt = arg min
xt+1   (1   t    )xt + t    vt

i=1 rfi(xi) +  txt   >x
theorem:[hazan,	kale	   12]			                        =    (    m/(cid:151))
pt
1. offline:		convergence	after	t	steps:	    p(cid:152)(0)
2. online:		                        =    (     )

i=1 rfi(xi) +  txt

theorem:	[garber,	hazan	   13] for	polytopes,	strongly-convex	and	
smooth	losses,	

xt

xt+1

vt

next	few	slides:
survey	state-of-the-art

non-convex	optimization	in	ml

machine

{a}       (cid:155)

distribution	
over	

label

argmin(cid:143)   (cid:156)(cid:157)		1     >    s    ,    s,    s

s(cid:134)2	0(cid:135)	u

+        

solution	concepts	for	non-convex	optimization?

h(x) = sin(2  x) = 0

! local (non global) minima of f0
! all kinds of constraints (even restricting to continuous functions):

what is optimization

but generally speaking...

we   re screwed.

    global minimization is np hard, even for degree 4 

polynomials. even local minimization up to 4th order 
optimality conditions is np hard. 

250

200

150

100

50

0

   50
3

2

1

0

   1

   2

   3

   3

   2

3

2

1

0

   1

    algorithmic stability is sufficient [hardt, recht, singer    16]

duchi (uc berkeley)

id76 for machine learning

fall 2009

    optimization approaches:

    finding vanishing gradients / local minima efficiently
    graduated optimization / homotopy method
    quasi-convexity
   

structure of local optima (probabilistic assumptions that 
allow alternating minimization,   )

goal:	find	point	x such	that	

gradient/hessian	based	methods

1.                     (approximate	first	order	optimality)
2.	    +                        (approximate	second	order	optimality)	
(we   ve	proved)		gd	algorithm:	    012=    0           4 finds	in	     2c
(we   ve	proved)		sgd	algorithm:	    012=    0           p4 finds	in	     2c(cid:159)
sgd	algorithm	with	noise	finds	in	     2c(cid:160)
4. recent	second	order	methods:	find	in	     2c  /  

(expensive)	iterations	point	(1)

[ge, huang,	jin,	yuan    15]

iterations	point	(1)

2.

3.

1.

(cheap)	iterations	(1&2)	

(expensive)	iterations	

(cheap)	

point	(1&2)	
[carmon,duchi,	hinder,	sidford    16]
[agarwal,	allen-zuo,	bullins,	hazan,	ma	   16]

recap

chair/car

what is optimization

but generally speaking...

we   re screwed.

1.
! local (non global) minima of f0
! all kinds of constraints (even restricting to continuous functions):

h(x) = sin(2  x) = 0

250

200

150

100

50

0

   50
3

2

1

0

   1

   2

   3

   3

   2

3

2

1

0

   1

2.

3.

duchi (uc berkeley)

id76 for machine learning

fall 2009

7 / 53

online	learning	and	stochastic	
optimization
    regret	minimization
    online	gradient	descent
id173
    adagradand	optimal	

id173
advanced	optimization	
    frank-wolfe,	 acceleration,	

variance	reduction,	second	order	
methods,	non-convex	
optimization

bibliography	&	more	information,	see:

http://www.cs.princeton.edu/~ehazan/tutorial/simonstutorial.htm

thank	you!

