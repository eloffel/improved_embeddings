failures of gradient-based deep learning

shai shalev-shwartz, shaked shammah, ohad shamir

the hebrew university and mobileye

representation learning workshop

simons institute, berkeley, 2017

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

1 / 38

deep learning is amazing ...

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

2 / 38

deep learning is amazing ...

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

3 / 38

deep learning is amazing ...

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

4 / 38

deep learning is amazing ...

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

5 / 38

deep learning is amazing ...

intel is buying mobileye for $15.3 billion in biggest
israeli hi-tech deal ever

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

6 / 38

this talk

simple problems where standard deep learning either

does not work well

requires prior knowledge for better architectural/algorithmic choices
requires other than gradient update rule
requiers to decompose the problem and add more supervision

does not work at all

no    local-search    algorithm can work
even for    nice    distributions and well-speci   ed models
even with over-parameterization (a.k.a. improper learning)

mix of theory and experiments

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

7 / 38

outline

1 piece-wise linear curves

2 flat activations

3 end-to-end training

4 learning many orthogonal functions

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

8 / 38

piecewise-linear curves: motivation

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

9 / 38

piecewise-linear curves

problem: train a piecewise-linear curve detector
input: f = (f (0), f (1), . . . , f (n     1)) where

k(cid:88)

f (x) =

ar[x       r]+ ,

  r     {0, . . . , n     1}

r=1

output: curve parameters {ar,   r}k

r=1

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

10 / 38

first try: deep autoencoder

encoding network, ew1: dense(500,relu)-dense(100,relu)-dense(2k)
decoding network, dw2: dense(100,relu)-dense(100,relu)-dense(n)
squared loss: (dw2(ew1(f ))     f )2

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

11 / 38

first try: deep autoencoder

encoding network, ew1: dense(500,relu)-dense(100,relu)-dense(2k)
decoding network, dw2: dense(100,relu)-dense(100,relu)-dense(n)
squared loss: (dw2(ew1(f ))     f )2
doesn   t work well ...

500 iterations

10,000 iterations

50,000 iterations

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

11 / 38

second try: pose as a convex objective

problem: train a piecewise-linear curve detector

input: f     rn where f (x) =(cid:80)k

output: curve parameters {ar,   r}k

r=1

r=1 ar[x       r]+

convex formulation

r=1

let p     rn,n be a k-sparse vector whose k max/argmax elements are
{ar,   r}k
observe: f = w p where w     rn,n is s.t. wi,j = [i     j + 1]+
learning approach     id75: train a one-layer fully
connected network on (f , p) examples:

e(cid:2)(u f     p)2(cid:3) = e(cid:2)(u f     w    1f )2(cid:3)

min

u

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

12 / 38

second try: pose as a convex objective

e(cid:2)(u f     p)2(cid:3) = e(cid:2)(u f     w    1f )2(cid:3)

min

u

convex; realizable; but still doesn   t work well ...

500 iterations

10,000 iterations

50,000 iterations

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

13 / 38

what went wrong?

theorem
the convergence of sgd is governed by the condition number of w (cid:62)w ,
which is large:

    sgd requires    (n3.5) iterations to reach u s.t. (cid:13)(cid:13)e[u ]     w    1(cid:13)(cid:13) < 1/2

=    (n3.5)

  max(w (cid:62)w )
  min(w (cid:62)w )

note: adagrad/adam doesn   t work because they perform diagonal
conditioning

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

14 / 38

3rd try: pose as a convolution

p = w    1f
observation:

                           

      
1
0
0
   2
      
1
0
1    2
      
1
1    2
      
0
1    2       
0
0
...
...
...

0
0
0
1

                           

w    1 =

w    1f is 1d convolution of f with    2nd derivative       lter (1,   2, 1)
can train a one-layer convnet to learn    lter (problem in r3!)

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

15 / 38

better, but still doesn   t work well ...

500 iterations

10,000 iterations

50,000 iterations

theorem: condition number reduced to   (n3). convolutions aid
geometry!
but,   (n3) is very disappointing for a problem in r3 ...

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

16 / 38

4th try: convolution + preconditioning

convnet is equivalent to solving minx e(cid:2)(f x     p)2(cid:3) where f is

n    3 matrix with (f (i     1), f (i), f (i + 1)) at row i
observation: problem is now low-dimensional, so can easily
precondition

compute empirical approximation c of e[f (cid:62)f ]
solve minx e[(f c   1/2x     p)2]
return c 1/2x

condition number of f c   1/2 is close to 1

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

17 / 38

finally, it works ...

500 iterations

10,000 iterations

50,000 iterations

remark:

use of convnet allows for e   cient preconditioning
estimating and manipulating 3    3 rather than n    n matrices.

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

18 / 38

lesson learned

sgd might be extremely slow
prior knowledge allows us to:

choose a better architecture (not for expressivity, but for a better
geometry)
choose a better algorithm (preconditioned sgd)

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

19 / 38

outline

1 piece-wise linear curves

2 flat activations

3 end-to-end training

4 learning many orthogonal functions

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

20 / 38

flat activations

vanishing gradients due to saturating activations (e.g. in id56   s)

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

21 / 38

flat activations

problem: learning x (cid:55)    u((cid:104)w(cid:63), x(cid:105)) where u is a    xed step function

optimization problem:

[(u(nw(x))     u(w(cid:63)(cid:62)x))2]
e
x

min

w

u(cid:48)(z) = 0 almost everywhere     can   t apply gradient-based methods

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

22 / 38

flat activations: smooth approximation

smooth approximation: replace u with   u (similar to using sigmoids as
gates in lstm   s)

u
  u

sometimes works, but slow and only approximate result. often completely
fails

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

23 / 38

flat activations: perhaps i should use a deeper network ...

approach: end-to-end

[(nw(x)     u(w(cid:63)(cid:62)x))2]
e
x

min

w

(3 relu + 1 linear layers; 10000 iterations)

slow train+test time; curve not captured well

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

24 / 38

flat activations: multiclass

approach: multiclass

min

w

e
[(cid:96)(nw(x), y(x))]
x

nw(x): to which step does x belong

problem capturing boundaries

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

25 / 38

flat activations:    forward only    id26

di   erent approach (kalai & sastry 2009, kakade, kalai, kanade, shamir
2011): id119, but replace gradient with something else

(cid:16)

(cid:20) 1
(cid:104)(cid:16)

w

e
x

(u(w(cid:62)x))     u(w(cid:63)(cid:62)x)

2
u(w(cid:62)x)     u(w(cid:63)(cid:62)x)

objective: min
gradient:     = e
non-gradient direction:       = e
interpretation:    forward only    id26

(cid:104)

x

x

(u(w(cid:62)x)     u(w(cid:63)(cid:62)x))x

(cid:17)2(cid:21)
(cid:17)    u(cid:48)(w(cid:62)x)    x

(cid:105)
(cid:105)

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

26 / 38

flat activation

(linear; 5000 iterations)

best results, and smallest train+test time
analysis (ks09, kkks11): needs o(l2/ 2) iterations if u is
l-lipschitz

lesson learned: local search works, but not with the gradient ...

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

27 / 38

outline

1 piece-wise linear curves

2 flat activations

3 end-to-end training

4 learning many orthogonal functions

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

28 / 38

end-to-end vs. decomposition

input x: k-tuple of images of random lines
f1(x): for each image, whether slope is negative/positive
f2(x): return parity of slope signs
goal: learn f2(f1(x))

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

29 / 38

end-to-end vs. decomposition

architecture: concatenation of lenet and 2-layer relu, linked by
sigmoid
end-to-end approach: train overall network on primary objective
decomposition approach: augment objective with loss speci   c to    rst
net, using per-image labels

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

30 / 38

end-to-end vs. decomposition

architecture: concatenation of lenet and 2-layer relu, linked by
sigmoid
end-to-end approach: train overall network on primary objective
decomposition approach: augment objective with loss speci   c to    rst
net, using per-image labels

k = 1

k = 2

k = 3

1

0.3

1

0.3

20000 iterations

1

0.3

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

30 / 38

end-to-end vs. decomposition

why end-to-end training doesn   t work ?

similar experiment by gulcehre and bengio, 2016
they suggest    local minima    problems
we show that the problem is di   erent

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

31 / 38

end-to-end vs. decomposition

why end-to-end training doesn   t work ?

similar experiment by gulcehre and bengio, 2016
they suggest    local minima    problems
we show that the problem is di   erent

signal-to-noise ratio (snr) for random initialization:

end-to-end (red) vs. decomposition (blue), as a function of k
snr of end-to-end for k     3 is below the precision of    oat32

  10   4

0

1

2

3

4

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

31 / 38

outline

1 piece-wise linear curves

2 flat activations

3 end-to-end training

4 learning many orthogonal functions

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

32 / 38

learning many orthogonal functions

let h be a hypothesis class of orthonormal functions:
   h, h(cid:48)     h, e[h(x)h(cid:48)(x)] = 0

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

33 / 38

learning many orthogonal functions

let h be a hypothesis class of orthonormal functions:
   h, h(cid:48)     h, e[h(x)h(cid:48)(x)] = 0
(improper) learning of h using gradient-based deep learning:

learn the parameter vector, w, of some architecture, pw : x     r
for every target h     h, the learning task is to solve:

min

w

fh(w) := e

x

[(cid:96)(pw(x), h(x)]

start with a random w and update the weights based on    fh(w)

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

33 / 38

learning many orthogonal functions

let h be a hypothesis class of orthonormal functions:
   h, h(cid:48)     h, e[h(x)h(cid:48)(x)] = 0
(improper) learning of h using gradient-based deep learning:

learn the parameter vector, w, of some architecture, pw : x     r
for every target h     h, the learning task is to solve:

min

w

fh(w) := e

x

[(cid:96)(pw(x), h(x)]

start with a random w and update the weights based on    fh(w)

analysis tool: how much    fh(w) tells us about the identity of h ?

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

33 / 38

analysis: how much information in the gradient?

min

w

fh(w) := e

x

[(cid:96)(pw(x), h(x)]

theorem: for every w, there are many pairs h, h(cid:48)     h s.t.
ex[h(x)h(cid:48)(x)] = 0 while

(cid:107)   fh(w)        fh(cid:48)(w)(cid:107)2 = o

(cid:19)

(cid:18) 1

|h|

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

34 / 38

analysis: how much information in the gradient?

min

w

fh(w) := e

x

[(cid:96)(pw(x), h(x)]

theorem: for every w, there are many pairs h, h(cid:48)     h s.t.
ex[h(x)h(cid:48)(x)] = 0 while

(cid:107)   fh(w)        fh(cid:48)(w)(cid:107)2 = o

(cid:19)

(cid:18) 1

|h|

proof idea: show that if the functions in h are orthonormal then, for
every w,

(cid:13)(cid:13)(cid:13)(cid:13)   fh(w)     e

(cid:13)(cid:13)(cid:13)(cid:13)2

(cid:19)

(cid:18) 1

|h|

var(h, f, w) := e

h

h(cid:48)    fh(cid:48)(w)

= o

to do so, express every coordinate of    pw(x) using the orthonormal
functions in h

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

34 / 38

proof idea

assume the squared loss, then

   fh(w) = e
= e

x

[(pw(x)     h(x)   pw(x)]
[pw(x)   pw(x)]

    e

(cid:124)

x

(cid:123)(cid:122)

(cid:125)

independent of h

[h(x)   pw(x)]

x

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

35 / 38

proof idea

assume the squared loss, then

x

x

   fh(w) = e
= e

(cid:124)
can expand g =(cid:80)|h|

[(pw(x)     h(x)   pw(x)]
[pw(x)   pw(x)]

    e

[h(x)   pw(x)]

x

(cid:123)(cid:122)

(cid:125)

independent of h

fix some j and denote g(x) =    jpw(x)

i=1(cid:104)hi, g(cid:105)hi + orthogonal component

therefore, eh (ex[h(x) g(x)])2     ex[g(x)2]
it follows that

|h|

var(h, f, w)     ex[(cid:107)   pw(x)(cid:107)2]

|h|

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

35 / 38

example: parity functions

h is the class of parity functions over {0, 1}d and x is uniformly
distributed
there are 2d orthonormal functions, hence there are many pairs
h, h(cid:48)     h s.t. ex[h(x)h(cid:48)(x)] = 0 while
(cid:107)   fh(w)        fh(cid:48)(w)(cid:107)2 = o(2   d)

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

36 / 38

example: parity functions

h is the class of parity functions over {0, 1}d and x is uniformly
distributed
there are 2d orthonormal functions, hence there are many pairs
h, h(cid:48)     h s.t. ex[h(x)h(cid:48)(x)] = 0 while
(cid:107)   fh(w)        fh(cid:48)(w)(cid:107)2 = o(2   d)

remark:

similar hardness result can be shown by combining existing results:

parities on uniform distribution over {0, 1}d is di   cult for statistical
query algorithms (kearns, 1999)
id119 with approximate gradients can be implemented with
statistical queries (feldman, guzman, vempala 2015)

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

36 / 38

visual illustration: linear-periodic functions
fh(w) = ex[(cos(w(cid:62)x)     h(x))2] for h(x) = cos([2, 2](cid:62)x), in 2
dimensions, x     n (0, i):

no local minima/saddle points
however, extremely    at unless very close to optimum

    di   cult for gradient methods, especially stochastic

in fact, di   cult for any local-search method

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

37 / 38

summary

cause of failures: optimization can be di   cult for geometric reasons
other than local minima / saddle points

condition number,    atness
using bigger/deeper networks doesn   t always help

remedies: prior knowledge can still be important

convolution can improve geometry (and not just sample complexity)
   other than gradient    update rule
decomposing the problem and adding supervision can improve
geometry

understanding the limitations: while deep learning is great,
understanding the limitations may lead to better algorithms and/or
better theoretical guarantees
for more information:

   failures of deep learning   : arxiv 1703.07950
github.com/shakedshammah/failures_of_dl

shai shalev-shwartz (huji,me)

failures of gradient-based dl

berkeley   17

38 / 38

