representations for language: 

from id27s to 

sentence meanings

christopher manning

stanford university

@chrmanning (cid:3) @stanfordnlp

simons institute 2017

what   s special about human language?
most important distinctive human characteristic
the only hope for    explainable    intelligence
communication was
central to human
development and
dominance
language forms
come with meanings
a social system

what   s special about human language?
constructed to convey speaker/writer   s meaning
not just an environmental signal; a deliberate communication
using an encoding which little kids learn (amazingly!) quickly
a discrete/symbolic/categorical signaling system

   rocket    = !;    violin    = "
very minor exceptions for expressive signaling        i loooove it   
presumably because of greater signaling reliability
symbols are not just an invention of logic / classical ai!

what   s special about human language?
language symbols are encoded as a continuous
communication signal in several ways:

    sound
    gesture
    writing (images/trajectories)

symbol is invariant across different encodings!

cc by 2.0 david fulmer 2008

national library of nz, no known restrictions

what   s special about human language?
    traditionally, people have extended the symbolic system of 

language into the brain:    the language of thought   

activation, just like the signal used to transmit language 

    but a brain encoding appears to be a continuous pattern of 
    deep learning is exploring a continuous encoding of thought
    cogsci question: whether to assume symbolic representations 

in the brain or to directly model using continuous substrate

lab

talk outline
1. what   s special about human language
2. from symbolic to distributed word representations
3. the bilstm (with attention) hegemony
4. choices for multi-word language representations
5. using tree-structured models: sentiment detection

6

2. from symbolic to distributed word 
representations

sec. 9.2.2

the vast majority of (rule-based and statistical) natural 
language processing and information retrieval (nlp/ir) work 
regarded words as atomic symbols: hotel, conference
in machine learning vector space terms, this is a vector with 
one 1 and a lot of zeroes

[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]

deep learning people call this a    one-hot    representation
it is a localist representation

from symbolic to distributed word 
representations

sec. 9.2.2

its problem, e.g., for web search:

but

    if	user	searches	for	[dell	notebook	battery	size],	we	would	
like	to	match	documents	with	   dell	laptop	battery	capacity   
size       [0 0 0 0 0 0 0 0 0 1 0 0 0 0]t
capacity  [0 0 0 0 0 0 1 0 0 0 0 0 0 0] = 0
our	query	and	document	vectors	are	orthogonal
there	is	no	natural	notion	of	similarity	in	a	set	of	
one-hot	vectors

capturing similarity
there are many things you can do to capture similarity:

id183 with synonym dictionaries
separately learning word similarities from large corpora

but a word representation that encodes similarity wins:

less parameters to learn (per word, not per pair)
more sharing of statistics
more opportunities for id72

a solution via distributional 
similarity-based representations

you can get a lot of value by representing a word 
by means of its neighbors
   you shall know a word by the company it keeps   
(j. r. firth 1957: 11)
one of the most successful ideas of modern nlp

government debt problems turning into banking crises as has happened in

saying that europe needs unified banking regulation to replace the hodgepodge
   these words will represent banking   

basic idea of learning neural network 
id27s (predict!)

we define a model that predicts between a center word 
wt and context words in terms of word vectors, e.g., 

p(context|wt) =    

which has a id168, e.g.,

j = 1     p(w   t |wt) 

we look at many positions t in a big language corpus
we keep adjusting the vector representations of words 
to minimize this loss

id97 skip-gram prediction

training time. the basic skip-gram formulation de   nes p(wt+j|wt) using the softmax function:

details of id97
for                         we choose:

w

wo

(2)

p(wo|wi) =

exp!v   
w=1 exp!v   
#w
where o is the outside (or output) word index, c is the 
w are the    input    and    output    vector representations of w, and w is the num-
ber of words in the vocabulary. this formulation is impractical because the cost of computing
center word index, vc and uo are the    center    and 
    log p(wo|wi ) is proportional to w, which is often large (105   107 terms).
   outside    vectors for word indices c and o
softmax using word c to obtain id203 of word o
co-occurring words are driven to have similar vectors

   vwi"
   vwi"

a computationally ef   cient approximation of the full softmax is the hierarchical softmax. in the
context of neural network language models, it was    rst introduced by morin and bengio [12]. the
main advantage is that instead of evaluating w output nodes in the neural network to obtain the
id203 distribution, it is needed to evaluate only about log2(w ) nodes.
the hierarchical softmax uses a binary tree representation of the output layer with the w words as

word meaning as a vector
the result is a dense vector for each word type, chosen so that 
it is good at predicting other words appearing in its context
    those other words also being represented by vectors

currency =

0.286
0.792
   0.177
   0.107
0.109
   0.542
0.349
0.271

latent semantic analysis (lsa) vs. 
   neural    models

sec. 18.3

comparisons	to	older	work:	lsa	count!	models
    factorize	a	(maybe	weighted,	often	log-
scaled)	term-document	(deerwester et	al.	
1990)	or	word-context	matrix	(sch  tze	1992)	
into	u  vt
    retain	only	k	singular	values,	in	order	to	
generalize

k

svd: intuition of dimensionality 
reduction

6
6

5
5

4
4

3
3

2
2

1
1

pca dimension 1

pca dimension 2

1
1

2
2

3
3

4
4

5
5

6
6

id97 encodes semantic components 
as linear vector differences

coals model (count-modified lsa)
[rohde, gonnerman & plaut, ms., 2005]

encoding meaning in vector differences
[pennington, socher, and manning, emnlp 2014]

crucial insight: 

ratios of co-occurrence probabilities can encode 
meaning components

x = solid

x = gas

x = water   

x = random   

large

small

small

large

large

large

small

small

large

small

~1

~1

encoding meaning in vector differences
[pennington, socher, and manning, emnlp 2014]

crucial insight: 

ratios of co-occurrence probabilities can encode 
meaning components

x = solid

x = gas

x = water   

x = fashion

1.9 x 10-4

6.6 x 10-5

3.0 x 10-3

1.7 x 10-5

2.2 x 10-5

7.8 x 10-4

2.2 x 10-3

1.8 x 10-5

8.9

8.5 x 10-2

1.36

0.96

encoding meaning in vector differences

q: how can we capture ratios of co-occurrence probabilities as 
meaning components in a word vector space?

a: log-bilinear model:

with vector differences

glove word similarities
[pennington et al., emnlp 2014]

nearest words to frog:

1. frogs
2. toad
3. litoria
4. leptodactylidae
5. rana
6. lizard
7. eleutherodactylus

litoria

leptodactylidae

rana
http://nlp.stanford.edu/projects/glove/

eleutherodactylus

glove visualizations: gender pairs

http://nlp.stanford.edu/projects/glove/

glove visualizations: company - ceo

id39 performance
(finding person, organization names in text)

model on 
conll
categorical crf
svd (log tf)
hpca
c&w
cbow
glove

conll    03 
dev
91.0
90.5
92.6
92.2
93.1
93.2

conll    03 
test
85.4
84.8
88.7
87.4
88.2
88.3

f1 score of crf trained on conll 2003 english with 50 dim word vectors

id39 performance
(finding person, organization names in text)

model on 
conll
categorical crf
svd (log tf)
hpca
c&w
cbow
glove

conll    03 
dev
91.0
90.5
92.6
92.2
93.1
93.2

conll    03 
test
85.4
84.8
88.7
87.4
88.2
88.3

ace 2 muc 7

77.4
73.6
81.7
81.7
82.2
82.9

73.4
71.5
80.7
80.2
81.1
82.2

f1 score of crf trained on conll 2003 english with 50 dim word vectors

id27s: conclusion
glove shows the connection between count!
work and predict! work     an appropriate scaling 
and objective gives count! models the 
properties and performance of predict! models

lots of other important recent work in this area:

[levy & goldberg, 2014] 
[arora, li, liang, ma & risteski, 2016]
[hashimoto, alvarez-melis & jaakkola, 2016]

3. the bilstm hegemony

to a first approximation,

the de facto consensus in nlp in 2017 is

that no matter what the task,
you throw a bilstm at it, with

attention if you need information flow

28

an id56 encoder-decoder network
encoder

decoder

je            suis

  tudiant <eos>

0.2
-0.3
-0.1
-0.4
0.2

0.2 
0.4 
0.1 
-0.5 
-0.2 

0.4 
-0.2 
-0.3 
-0.4 
-0.2 

0.2 
0.6 
-0.1 
-0.7 
0.1 

0.2 
0.6 
-0.1 
-0.7 
0.1 

0.2 
0.6 
-0.1 
-0.7 
0.1 

0.2 
0.6 
-0.1 
-0.7 
0.1 

-0.1
0.3
-0.1
-0.7
0.1

i               am                a         student     <eos>   je             suis

  tudiant

ht = tanh(w[xt] + uht   1 + b)

id149        lstms   

equations of the two most widely used id149
gated recurrent unit
long short-term memory 
[cho et al., emnlp2014; 
[hochreiter & schmidhuber, nc1999; 
chung, gulcehre, cho, bengio, dlufl2014]
gers, thesis2001]
ht = ut     ht + (1   ut)   ht 1
ht = ot   tanh(ct)
ct = ft   ct 1 + it     ct
  h = tanh(w [xt] + u (rt   ht 1) + b)
  ct = tanh(wc [xt] + ucht 1 + bc)
ut =  (wu [xt] + uuht 1 + bu)
ot =  (wo [xt] + uoht 1 + bo)
rt =  (wr [xt] + urht 1 + br)
it =  (wi [xt] + uiht 1 + bi)
basic update to memory cell 
ft =  (wf [xt] + uf ht 1 + bf )
(gru h = lstm c) is via a 
standard neural net layer

id149        lstms   

equations of the two most widely used id149
gated recurrent unit
long short-term memory 
[cho et al., emnlp2014; 
[hochreiter & schmidhuber, nc1999; 
chung, gulcehre, cho, bengio, dlufl2014]
gers, thesis2001]
ht = ut     ht + (1   ut)   ht 1
ht = ot   tanh(ct)
ct = ft   ct 1 + it     ct
  h = tanh(w [xt] + u (rt   ht 1) + b)
  ct = tanh(wc [xt] + ucht 1 + bc)
ut =  (wu [xt] + uuht 1 + bu)
ot =  (wo [xt] + uoht 1 + bo)
rt =  (wr [xt] + urht 1 + br)
it =  (wi [xt] + uiht 1 + bi)
bernoulli variable    gates    
ft =  (wf [xt] + uf ht 1 + bf )
control how much history is 
kept & input is attended to

id149        lstms   

equations of the two most widely used id149
gated recurrent unit
long short-term memory 
[cho et al., emnlp2014; 
[hochreiter & schmidhuber, nc1999; 
chung, gulcehre, cho, bengio, dlufl2014]
gers, thesis2001]
ht = ut     ht + (1   ut)   ht 1
ht = ot   tanh(ct)
ct = ft   ct 1 + it     ct
  h = tanh(w [xt] + u (rt   ht 1) + b)
  ct = tanh(wc [xt] + ucht 1 + bc)
ut =  (wu [xt] + uuht 1 + bu)
ot =  (wo [xt] + uoht 1 + bo)
rt =  (wr [xt] + urht 1 + br)
it =  (wi [xt] + uiht 1 + bi)
summing previous & new 
ft =  (wf [xt] + uf ht 1 + bf )
candidate hidden states 
gives direct gradient flow 
& more effective memory

id149        lstms   

equations of the two most widely used id149
gated recurrent unit
long short-term memory 
[cho et al., emnlp2014; 
[hochreiter & schmidhuber, nc1999; 
chung, gulcehre, cho, bengio, dlufl2014]
gers, thesis2001]
ht = ut     ht + (1   ut)   ht 1
ht = ot   tanh(ct)
ct = ft   ct 1 + it     ct
  h = tanh(w [xt] + u (rt   ht 1) + b)
  ct = tanh(wc [xt] + ucht 1 + bc)
ut =  (wu [xt] + uuht 1 + bu)
ot =  (wo [xt] + uoht 1 + bo)
rt =  (wr [xt] + urht 1 + br)
it =  (wi [xt] + uiht 1 + bi)
note that recurrent 
ft =  (wf [xt] + uf ht 1 + bf )
state mixes control and 
memory. good? (freedom to 
represent.) or bad? (mush.)

an lstm encoder-decoder network
[sutskever et al. 2014]

the      protests  escalated    over         the      weekend   <eos>

translation 
generated

0.1
0.3
0.1
-0.4
0.2

0.2
-0.2
-0.1
0.1
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.4
-0.6
0.2
-0.3
0.4

0.4
0.4
0.3
-0.2
-0.3

0.1
0.3
-0.1
-0.7
0.1

0.2
-0.3
-0.1
-0.4
0.2

0.5
0.5
0.9
-0.3
-0.2

0.2
0.6
-0.1
-0.4
0.1

0.2
0.4
0.1
-0.5
-0.2

0.2
0.6
-0.1
-0.5
0.1

0.2
-0.8
-0.1
-0.5
0.1

0.4
-0.2
-0.3
-0.4
-0.2

-0.1
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
-0.1
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

-0.4
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.3
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
0.3
0.1

0.2
0.6
-0.1
-0.7
0.1

0.4
0.4
-0.1
-0.7
0.1

-0.1
0.6
-0.1
0.3
0.1

-0.1
0.3
-0.1
-0.7
0.1

-0.2
0.6
-0.1
-0.7
0.1

0.2
0.4
-0.1
0.2
0.1

-0.2
0.6
0.1
0.3
0.1

-0.4
0.6
-0.1
-0.7
0.1

0.3
0.6
-0.1
-0.5
0.1

-0.4
0.5
-0.5
0.4
0.1

-0.3
0.5
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

0.2
0.6
-0.1
-0.7
0.1

die      proteste    waren am  wochenende eskaliert <eos>   the      protests   escalated   over        the     weekend

decoder

feeding in 
last word

encoder:
builds up 
sentence 
meaning 

source 
sentence

bottleneck

a bilstm encoder and 
lstm-with-attention decoder
encoder

je            suis

  tudiant <eos>

decoder

0.2
-0.3
-0.1
-0.4
0.2

0.2 
0.4 
0.1 
-0.5 
-0.2 

0.4 
-0.2 
-0.3 
-0.4 
-0.2 

0.2 
0.6 
-0.1 
-0.7 
0.1 

0.2 
0.6 
-0.1 
-0.7 
0.1 

0.2 
0.6 
-0.1 
-0.7 
0.1 

0.2 
0.6 
-0.1 
-0.7 
0.1 

-0.1
0.3
-0.1
-0.7
0.1

i               am                a         student     <eos>   je             suis

  tudiant

progress in machine translation

[edinburgh en-de wmt newstest2013 cased id7; id4 2015 from u. montr  al]

25

20

15

10

5

0

phrase-based smt

syntax-based smt

neural mt

2013

2014

2015

2016

from [sennrich 2016, http://www.meta-net.eu/events/meta-forum-2016/slides/09_sennrich.pdf] 

four big wins of neural mt
1. end-to-end training

all parameters are simultaneously optimized to minimize 
a id168 on the network   s output 

2. distributed representations share strength
better exploitation of  word and phrase similarities

3. better exploitation of context

id4 can use a much bigger context     both source and 
partial target text     to translate more accurately

4. more fluent text generation

deep learning text generation is much higher quality

37

bilstms(+attn) not just for neural mt

id52
id39
syntactic parsing (constituency & dependency)
reading comprehension
id53
text summarization
   

reading comprehension on the deepmind 
id98 & daily mail datasets    [hermann et al, 2015]

39

end-to-end neural network
[chen, bolton, & manning, acl 2016]

q

characters in " @placeholder " movies 
have gradually become more diverse

bidirectional id56s

p

   

   

   

attention

a

entity6

40

id98

(hill et al, 2016)

(hermann et al, 2015)

lots of complex models; lots of results
nothing does much better than lstm+attn
daily mail
test
dev
68.0
69.0
n/a
n/a
n/a
n/a
75.0
73.9
75.7
76.7
n/a
n/a
n/a
n/a
77.2
n/a
n/a
n/a
77.6
76.6
80.2
79.2

dev
61.8
63.4
71.3
68.6
acl   16
73.0
2016/6/5
72.6
2016/6/7
2016/6/7
73.4
2016/7/12 n/a
73.1
2016/7/15
73.8
77.2

(kadlec et al, 2016)
(dhingra et al, 2016)
(sodorni et al, 2016)
(trischler et al, 2016)
(weissenborn, 2016)

acl   16
ours: neural net (ensemble) acl   16

test
63.8
66.8
72.9
69.5
73.8
73.3
74.0
73.6
74.4
73.6
77.6

nips   15
iclr   16
naacl   16

(cui et al, 2016)
ours: neural net

(kobayashi et al, 2016)

the standard theory of
natural language interpretation

a cat purred

language 
expressions 

det nsubj

a cat purred

(cid:1)x cat(x)(cid:2)purr(x)

deletion 
map to 
surface 
form

syntax 
trees

semantic 
interpre-
tation

logical 
formulas

semantic
deno-
tation

models 
described

model of:
    most linguistic and philosophical work (till the present)
    most computational linguistic work (till 1990)
    modern    id29    (liang, zettlemoyer, etc.)

semantic interpretation of language 
    not just word vectors
how can we minimally know when larger 
language units are similar in meaning?
    the snowboarder is leaping over a mogul
    a person on a snowboard jumps into the air

people interpret the meaning of larger text units    
entities, descriptive terms, facts, arguments, stories     by 
semantic composition of smaller elements

4. choices for multi-word language 
representations 

44

neural bag-of-words models
    simply average (or just sum) word vectors:

2.1
3.3

0.4
(          +           +          +         +         )/5 = 
0.3
the   country  of      my    birth

7.0
7.0

4.0
4.5

2.3
3.6

3.0
3.7

    can improve effectiveness by putting output 
through 1+ fully connected layers (dans)
    surprisingly effective for many tasks l

    [iyyer, manjunatha, boyd-graber and daum   iii 2015     dans; 
wieting, bansal, gimpel and livescu 2016     periphrastic]

recurrent neural networks
    simple recurrent neural nets do use word order but 

cannot capture phrases without prefix context

    gated lstm/gru units in theory could up to a certain 

depth, but it seems unlikely

    empirically, representations capture too much of last 
words in final vector     focus is lm next word prediction

1
3.5

0.4
0.3
the 

1
5

2.1
3.3

5.5
6.1

7
7

4.5
3.8

4
4.5

2.5
3.8

2.3
3.6

country   of     my      birth

convolutional neural network
    what if we compute vectors for every h-word phrase, 

often for several values of h?
    example:    the country of my birth    computes vectors for:
    the country, country of, of my, my birth, the country of, country of 

my, of my birth, the country of my, country of my birth
    not very linguistic, but you get everything!

1.1

3.5

   

0.4
0.3
the   country    of    my  

4
4.5

2.1
3.3

7
7

2.4

2.3
3.6
birth

0
0

0
0

word vectors, wherein words are projected from a
sparse, 1-of-v encoding (here v is the vocabulary
size) onto a lower dimensional vector space via a
hidden layer, are essentially feature extractors that
encode semantic features of words in their dimen-
sions. in such dense representations, semantically
close words are likewise close   in euclidean or
cosine distance   in the lower dimensional vector

convolutional neural networks (id98) utilize
layers with convolving    lters that are applied to

figure 1: model architecture with two channels for an example sentence.

ferent from the original task for which the feature
extractors were trained.

window of h words to produce a new feature. for
example, a feature ci is generated from a window
necessary) is represented as
of words xi:i+h 1 by

that is kept static throughout training and one that
is    ne-tuned via id26 (section 3.2).2
in the multichannel architecture, illustrated in    g-
ure 1, each    lter is applied to both channels and
the results are added to calculate ci in equation
(2). the model is otherwise equivalent to the sin-
gle channel architecture.

here b 2 r is a bias term and f is a non-linear
2.1 id173
function such as the hyperbolic tangent. this    lter
for id173 we employ dropout on the
2 model
is applied to each possible window of words in the
penultimate layer with a constraint on l2-norms of
necessary) is represented as
convolutional neural network
ci = f(w    xi:i+h 1 + b).
(2)
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
the weight vectors (hinton et al., 2012). dropout
x1:n = x1   x2   . . .   xn,
(1)
the model architecture, shown in    gure 1, is a
prevents co-adaptation of hidden units by ran-
x1:n = x1   x2   . . .   xn,
a feature map
slight variant of the id98 architecture of collobert
in gen-
where   is the concatenation operator.
here b 2 r is a bias term and f is a non-linear
domly dropping out   i.e., setting to zero   a pro-
    word vectors: 
et al. (2011). let xi 2 rk be the k-dimensional
where   is the concatenation operator.
function such as the hyperbolic tangent. this    lter
eral, let xi:i+j refer to the concatenation of words
portion p of the hidden units during foward-
c = [c1, c2, . . . , cn h+1],
    concatenation of words in range:
word vector corresponding to the i-th word in the
eral, let xi:i+j refer to the concatenation of words
is applied to each possible window of words in the
xi, xi+1, . . . , xi+j. a convolution operation in-
id26. that is, given the penultimate
sentence. a sentence of length n (padded where
    convolutional filter:  
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
xi, xi+1, . . . , xi+j. a convolution operation in-
volves a    lter w 2 rhk, which is applied to a
layer z = [  c1, . . . ,   cm] (note that here we have m
with c 2 rn h+1. we then apply a max-over-
a feature map
volves a    lter w 2 rhk, which is applied to a
    id98 layer feature:
window of h words to produce a new feature. for
   lters), instead of using
1https://code.google.com/p/id97/
2.1 id173
time pooling operation (collobert et al., 2011)
window of h words to produce a new feature. for
example, a feature ci is generated from a window
    get feature map:
(3)
over the feature map and take the maximum value
example, a feature ci is generated from a window
for id173 we employ dropout on the
of words xi:i+h 1 by
    max pool (better than ave.):   
  c = max{c} as the feature corresponding to this
of words xi:i+h 1 by
penultimate layer with a constraint on l2-norms of
with c 2 rn h+1. we then apply a max-over-
particular    lter. the idea is to capture the most im-
(2)
   
the weight vectors (hinton et al., 2012). dropout
time pooling operation (collobert et al., 2011)
portant feature   one with the highest value   for
prevents co-adaptation of hidden units by ran-
over the feature map and take the maximum value
here b 2 r is a bias term and f is a non-linear
each feature map. this pooling scheme naturally
domly dropping out   i.e., setting to zero   a pro-
  c = max{c} as the feature corresponding to this
here b 2 r is a bias term and f is a non-linear
function such as the hyperbolic tangent. this    lter
portion p of the hidden units during foward-
deals with variable sentence lengths.
particular    lter. the idea is to capture the most im-
function such as the hyperbolic tangent. this    lter
is applied to each possible window of words in the
portant feature   one with the highest value   for
id26. that is, given the penultimate
we have described the process by which one
is applied to each possible window of words in the
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce
each feature map. this pooling scheme naturally
layer z = [  c1, . . . ,   cm] (note that here we have m
feature is extracted from one    lter. the model
sentence {x1:h, x2:h+1, . . . , xn h+1:n} to produce

where   is the element-wise multiplication opera-
tor and r 2 rm is a    masking    vector of bernoulli
random variables with id203 p of being 1.

for output unit y in forward propagation, dropout
uses

0.4
0.3
the   country    of    my  

ci = f(w    xi:i+h 1 + b).

ci = f(w    xi:i+h 1 + b).

c = [c1, c2, . . . , cn h+1],

2.3
3.6
birth

4
4.5

2.1
3.3

1.1

3.5

2.4

0
0

0
0

7
7

1746

proceedings of the 2014 conference on empirical methods in natural language processing (emnlp), pages 1746   1751,

october 25-29, 2014, doha, qatar. c 2014 association for computational linguistics

1d convolutional neural network 
with max pooling and fc layer

    for more features, use multiple filter weights and 

figure 1: model architecture with two channels for an example sentence.

multiple window sizes

necessary) is represented as

    figure from [kim 2014    convolutional neural networks 
for sentence classification   ]
x1:n = x1   x2   . . .   xn,
(1)
where   is the concatenation operator.
in gen-
eral, let xi:i+j refer to the concatenation of words

that is kept static throughout training and one that
is    ne-tuned via id26 (section 3.2).2
in the multichannel architecture, illustrated in    g-
ure 1, each    lter is applied to both channels and
the results are added to calculate ci in equation

wait for the video and do n't rent it n x k representation of sentence with static and non-static channels convolutional layer with multiple filter widths and feature maps max-over-time pooling fully connected layer with dropout and  softmax output data-dependent composition

language understanding     & artificial 
intelligence     requires being able to 

understand bigger things from 
knowing about smaller parts

53

language structure is recursive
    recursion is natural for describing language

    [the man from [the company that you spoke with about [the 
project] yesterday]]
    noun phrase containing a noun phrase with a noun phrase

    phrases correspond to semantic units of language

relationship between id56s and 
id98s

id98

id56

relationship between id56s and 
id98s

id98

id56

people       there           speak       slowly                people       there        speak       slowly 

5. using tree-structured models: 
sentiment detection

is the tone of a piece of text positive, negative, or neutral?
    sentiment is that sentiment is    easy   
    detection accuracy for longer documents ~90%

        loved                     great                         
impressed                         marvelous                

    but

stanford sentiment treebank
    215,154 phrases labeled in 11,855 sentences
    can train and test compositions

http://nlp.stanford.edu:8080/sentiment/

universal dependencies syntax
http://universaldependencies.org/

root

punct

obl

nsubj

aux

aux

obj

det

det

det

case

det

the cat could have chased all the dogs down the street .

    content words are related by dependency relations
    function words attach to content word they modify
    punctuation attaches to head of phrase or clause 

punct

root

det

nsubj

aux

the

dog

was

chased

by

obl

det

case

the

cat

.

dozat & manning (iclr 2017)

    each word predicts what it is a dependent of as 
a kind of head-dependent attention relation
    we then find the best tree (mst algorithm)

ptb-sd 3.3.0 and ctb 5.1 results

type

model

transition chen & manning (2014)

graph

andor et al. (2016)
kuncoro et al. (2016)
k & g (2016)
cheng et al. (2016)
hashimoto et al. (2016)
ours

ptb-sd

ctb

uas

92.0
94.61
95.8
93.9
94.10
94.67
95.74

las
89.7
92.79
94.6
91.9
91.49
92.90
94.08

uas
83.9
--
--
87.6
88.1
--
89.30

las
82.4
--
--
86.1
86.1
--
88.23

tree-structured long short-term 
memory networks      [tai et al., acl 2015]

tree-structured lstm
generalizes sequential lstm to trees with any branching factor

better dataset helped even simple 
models

positive/negative sentence classification uni+bigram na  ve bayes

95

90

85

80

75

70

training with sentence labels

training with treebank
    but hard negation cases are still mostly incorrect
    we also need a more powerful model!

positive/negative results on treebank
classifying sentences: accuracy improves to 88%

95

90

85

80

75

70

training with sentence labels

training with treebank

bi nb

treelstm

experimental results on treebank
    treeid56 can capture constructions like x but y
    biword na  ve bayes is only 58% on these

results on negating negatives

e.g., sentiment of    not uninteresting   
goal: positive activation should increase

envoi
    deep learning     distributed representations, end-to-

end training, and richer modeling of state     has 
brought great gains to nlp

    at the moment, it seems like we can   t win, or we can 

only barely win, by having more structure than a 
vector space mush

    however, i deeply believe that we do need more 
structure and modularity for language, memory, 
knowledge, and planning; it   ll just take some time

