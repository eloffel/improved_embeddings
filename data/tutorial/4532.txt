large scale hierarchical classi   cation: foundations,

algorithms and applications

huzefa rangwala and azad naik

department of computer science
mlbio+ laboratory
fairfax, virginia, usa

kdd tutorial, halifax, canada

13th aug, 2017

huzefa rangwala and azad naik

george mason university

13th aug, 2017

1 / 117

overview of tutorial coverage

part - i

1 introduction and background

motivation
hierarchical classi   cation (hc) problem description
challenges
methods for solving hc

2 state-of-the-art hc approaches

parent-child id173
cost-sensitive learning

package description/software demo

huzefa rangwala and azad naik

george mason university

13th aug, 2017

2 / 117

overview of tutorial coverage

part - ii

1 inconsistent hierarchy

motivation
methods for resolving inconsistency
optimal hierarchy search in hierarchical space

2 other hc methods

learning using multiple hierarchies
extreme and deep classi   cation

3 conclusion

huzefa rangwala and azad naik

george mason university

13th aug, 2017

3 / 117

motivation

exponential growth in data (image, text, video) over time

big data era - megabytes & gigabytes to terabytes & petabytes
growth in almost all    elds - astronomical, biological, web content

huzefa rangwala and azad naik

george mason university

13th aug, 2017

4 / 117

data organization

organize data into structure

tree, graph [lshtc, bioasq and ilsvrc challenge]

useful in various applications

query search, browsing and categorizing products

huzefa rangwala and azad naik

george mason university

13th aug, 2017

5 / 117

hierarchical structure

classes organized into the hierarchical structure
generic (   ) to speci   c (   ) categories in top-down order

huzefa rangwala and azad naik

george mason university

13th aug, 2017

6 / 117

hierarchical classi   cation

goal

given hierarchy of classes exploit the hierarchical
structure to learn models and classify unlabeled test

examples (instances) to one or more nodes in the

hierarchy

huzefa rangwala and azad naik

george mason university

13th aug, 2017

7 / 117

hierarchical classi   cation

goal

given hierarchy of classes exploit the hierarchical
structure to learn models and classify unlabeled test

examples (instances) to one or more nodes in the

hierarchy

solution

(i) manual classi   cation

(ii) automated classi   cation

huzefa rangwala and azad naik

george mason university

13th aug, 2017

7 / 117

manual classi   cation

requires human understanding and expertise

infeasible for huge data

huzefa rangwala and azad naik

george mason university

13th aug, 2017

8 / 117

automated classi   cation

trained expert (such as computer)

scalable for huge data

huzefa rangwala and azad naik

george mason university

13th aug, 2017

9 / 117

challenges - i

single label vs. multi-label

single label classi   cation - each example belongs exclusively to one
class only
multi-label classi   cation - example may belong to more than one
class

huzefa rangwala and azad naik

george mason university

13th aug, 2017

10 / 117

challenges - ii

mandatory leaf node vs. internal node prediction

example may be assigned to internal nodes

huzefa rangwala and azad naik

george mason university

13th aug, 2017

11 / 117

challenges - ii

mandatory leaf node vs. internal node prediction

example may be assigned to internal nodes
orphan node detection problem

huzefa rangwala and azad naik

george mason university

13th aug, 2017

11 / 117

challenges - iii

rare categories

many classes with very few labeled examples

huzefa rangwala and azad naik

george mason university

13th aug, 2017

12 / 117

challenges - iii

rare categories

many classes with very few labeled examples
more prevalent in large scale datasets -    70% have    10 examples

huzefa rangwala and azad naik

george mason university

13th aug, 2017

12 / 117

challenges - iv

feature selection

all features are not essential to discriminate between classes
identify features to improve classi   cation performance

huzefa rangwala and azad naik

george mason university

13th aug, 2017

13 / 117

challenges - iv

feature selection

all features are not essential to discriminate between classes
identify features to improve classi   cation performance

huzefa rangwala and azad naik

george mason university

13th aug, 2017

14 / 117

other challenges

parameter optimization

incorporate relationships (parent-child, silings) information

huzefa rangwala and azad naik

george mason university

13th aug, 2017

15 / 117

other challenges

parameter optimization

incorporate relationships (parent-child, silings) information

scalability

large # of classes, features and examples require distributed
computation

dataset

dmoz-2010
dmoz-2012

#training #leaf node
examples
128,710
383,408

(classes)
12,294
11,947

#features #parameters

parameter

size (approx)

381,580
348,548

4,652,986,520
4,164,102,956

18.5 gb
16.5 gb

huzefa rangwala and azad naik

george mason university

13th aug, 2017

15 / 117

other challenges

parameter optimization

incorporate relationships (parent-child, silings) information

scalability

large # of classes, features and examples require distributed
computation

dataset

dmoz-2010
dmoz-2012

#training #leaf node
examples
128,710
383,408

(classes)
12,294
11,947

#features #parameters

parameter

size (approx)

381,580
348,548

4,652,986,520
4,164,102,956

18.5 gb
16.5 gb

inconsistent hierarchy

not suitable for classi   cation (more details later)

huzefa rangwala and azad naik

george mason university

13th aug, 2017

15 / 117

notation

n = # of training examples (instances) d = dimension of each instance
n = set of nodes in the hierarchy
c(t) = children of node t

l = set of leaf node (classes)
  (t) = parent of node t

huzefa rangwala and azad naik

george mason university

13th aug, 2017

16 / 117

classi   cation

training - learn mapping function using training data

testing - predict the label of test example

huzefa rangwala and azad naik

george mason university

13th aug, 2017

17 / 117

learning algorithm: general formulation

combination of two terms:

1 empirical loss - controls how well the learnt models    ts the training

data

2 id173 - prevent models from over-   tting and encodes

additional information such as hierarchical relationships

huzefa rangwala and azad naik

george mason university

13th aug, 2017

18 / 117

di   erent approaches for solving hc problem

huzefa rangwala and azad naik

george mason university

13th aug, 2017

19 / 117

flat classi   cation approach

simplest method (ignores hierarchy)

learn discriminant classi   ers for each leaf node in the hierarchy

unlabeled test example classi   ed using the rule:
f (x, y|w)

  y = arg max

y     y

huzefa rangwala and azad naik

george mason university

13th aug, 2017

20 / 117

local classi   cation approach - i

local classi   er per node (lcn)

learn binary classi   ers for all non-root nodes

goal is to e   ectively discriminate between the siblings

top-down approach is followed for classifying unlabeled test examples

huzefa rangwala and azad naik

george mason university

13th aug, 2017

21 / 117

local classi   cation approach - ii

local classi   er per parent node (lcpn)

learn multi-class classi   ers for all non-leaf nodes
like lcn goal is to e   ectively discriminate between the siblings
top-down approach is followed for classifying unlabeled test examples

huzefa rangwala and azad naik

george mason university

13th aug, 2017

22 / 117

local classi   cation approach - iii

local classi   er per level (lcl)

learn multi-class classi   ers for all levels in the hierarchy

least popular among local approaches

prediction inconsistency may occur and hence post-processing step is
required

huzefa rangwala and azad naik

george mason university

13th aug, 2017

23 / 117

global classi   cation approach

learn global function considering all hierarchical relationships

often referred as big-bang approach

unlabeled test instance is classi   ed using an approach similar to    at
or local methods

huzefa rangwala and azad naik

george mason university

13th aug, 2017

24 / 117

id74 - i

flat evaluation measures

misclassi   cations treated equally

common id74:

micro-f1 - gives equal weightage to all examples, dominated by
common class
macro-f1 - gives equal weightage to each class

huzefa rangwala and azad naik

george mason university

13th aug, 2017

25 / 117

id74 - ii

hierarchical evaluation measures

hierarchical distance between the true and predicted class taken into
consideration for performance evaluation

common id74:

hierarchical-f1 - common ancestors between true and predicted class
tree error - average hierarchical distance b/w true and predicted class

huzefa rangwala and azad naik

george mason university

13th aug, 2017

26 / 117

id72 (mtl)

involves joint training of multiple related tasks to improve
generalization performance

independent learning problems can utilize the shared knowledge

exploits inductive biases that are helpful to all the related tasks

similar set of parameters
common feature space

examples

personal email spam classi   cation - many person with same spam
automated driving - brakes and accelerator

huzefa rangwala and azad naik

george mason university

13th aug, 2017

27 / 117

parent-child id173, gopal and yang, sigkdd   13

motivation

traditional approach learn classi   ers for each leaf
node (task) to discriminate one class from other

(cid:104)

n(cid:80)

i=1

(cid:105)

+

1     yitwt
t xi

1
2
works well if:

min
wt

||wt||2

2 + c

dataset is small
balanced
su   cient positive examples per class to learn
generalized discriminant function

drawbacks

real world datasets su   ers from rare categories issue
remember: 70% classes have less than 10 examples per class
large number of classes (scalability issue)

huzefa rangwala and azad naik

george mason university

13th aug, 2017

28 / 117

motivation - ii

can we improve the performance of data sparse
leaf nodes by taking advantage of data rich nodes
at higher levels?
incorporate inter-class dependencies to improve
classi   cation

min
wt

1
2

objective

||wt     w  (t)||2

examples belonging to soccer category is less
likely to belong to software category

2 + c (cid:80)

(cid:104)

n(cid:80)

k   c(t)

i=1

(cid:105)

+

1    yik wt
t xi

how to e   ectively incorporate the hierarchical relationships into the
objective function to improve generalization performance

make it scalable for larger datasets

huzefa rangwala and azad naik

george mason university

13th aug, 2017

29 / 117

proposed formulation

enforces model parameters (weights) to be similar to the parent in
id173
proposed state-of-the-art: hr-id166 and hr-lr global formulation

||wt     w  (t)||2

1     yik wt
k xi

(cid:105)

+

(cid:80)

t   n

1
2

hr-id166

min
w

internal node

1
2

min
wt

||wt     w  (t)||2

||wc     wt||2

2

leaf node

1
2

min
wt

||wt     w  (t)||2

2 +

1     yitwt
t xi

(cid:105)

+

(cid:104)

i=1

k   l

2 + c (cid:80)
n(cid:80)
(cid:80)
(cid:104)

c   c(t)

n(cid:80)

2 +

1
2

1
2

i=1

huzefa rangwala and azad naik

george mason university

13th aug, 2017

30 / 117

hr-lr models

similar formulation as hr-id166
logistic loss instead of hinge loss

hr-lr

min
w

(cid:80)

t   n

1
2

internal node

||wt     w  (t)||2

2 + c (cid:80)

k   l

n(cid:80)

i=1

log (1 + exp(   yik wt

k xi ))

1
2

min
wt

||wt     w  (t)||2

2 +

1
2

(cid:80)

c   c(t)

||wc     wt||2

2

leaf node

1
2

min
wt

||wt     w  (t)||2

2 +

n(cid:80)

i=1

1
2

log (1 + exp(   yitwt

t xi ))

huzefa rangwala and azad naik

george mason university

13th aug, 2017

31 / 117

proposed parallel implementation

each node is independent of all other nodes except its neighbours
objective function is block separable. therefore, parallel block
coordinate descent (cd) can be used for optimization

1 fix odd-levels parameters,

optimize even-levels in parallel

2 fix even-levels parameters,

optimize odd-levels in parallel

3 repeat untill convergence

extended to graph by    rst    nding the minimum graph coloring
[np-hard] and repeatedly optimizing nodes with the same color in
paralle during each iteration

huzefa rangwala and azad naik

george mason university

13th aug, 2017

32 / 117

experiments

dataset description

wide range of single and multi-label dataset with varying number of
features and categories were used for model evaluation

datasets

# features # categories

type

89

clef
rcv1
48,734
ipc
541,869
dmoz-small 51,033
dmoz-2010
381,580
dmoz-2012
348,548
dmoz-2011
594,158
swiki-2011
346,299
lwiki
1,617,899

87
137
552
1,563
15,358
13,347
27,875
50,312
614,428

single-label
multi-label
single-label
single-label
single-label
single-label
multi-label
multi-label
multi-label

table: dataset statistics

avg # labels
(per instance)

1

3.18

1
1
1
1

1.03
1.85
3.26

huzefa rangwala and azad naik

george mason university

13th aug, 2017

33 / 117

comparison methods

flat baselines

id166 - one-vs-rest binary support vector machines
lr - one-vs-rest regularized id28

hierarchical baselines

top-down id166 (td)[liu et al., sigkdd   05] - a pachinko
machine style id166
hierarchical id166 (hid166)[tsochantaridis et al., jmlr   05] - a
large-margin discriminative method with path dependent discriminant
function
hierarchical orthogonal transfer (ot)[lin et al., icml   11] - a
large-margin method enforcing orthogonality between the parent and
the children
hierarchical bayesian id28 (hblr)[gopal et al.,
nips   12]- a bayesian methods to model hierarchical dependencies
among class labels using multivariate id28

huzefa rangwala and azad naik

george mason university

13th aug, 2017

34 / 117

flat baselines comparison - i

figure: performance improvement: hr-id166 vs. id166

huzefa rangwala and azad naik

george mason university

13th aug, 2017

35 / 117

flat baselines comparison - ii

figure: performance improvement: hr-lr vs. lr

huzefa rangwala and azad naik

george mason university

13th aug, 2017

36 / 117

hierarchical baselines comparison

datasets
clef
rcv1
ipc
dmoz-small
dmoz-2010
dmoz-2012
dmoz-2011
swiki-2011
lwiki

hr-id166 hr-lr
80.12
81.23
55.37
45.11
45.84
53.18
42.27
40.99
37.67

80.02
81.66
54.26
45.31
46.02
57.17
43.73
41.79
38.08

td
70.11
71.34
50.34
38.48
38.64
55.14
35.91
36.65
na

hid166 ot
79.72
73.84
na
ns

ns
ns

39.66

37.12

hblr
81.41

na

56.02
46.03

ns
ns
na
na
na

ns
ns
ns
na
na

ns
ns
na
na
na

[na - not applicable; ns - not scalable]

table: micro-f1 performance comparison

huzefa rangwala and azad naik

george mason university

13th aug, 2017

37 / 117

runtime comparison -    at baselines
hr-id166 vs. id166

hr-lr vs. lr

huzefa rangwala and azad naik

george mason university

13th aug, 2017

38 / 117

runtime comparison - hierarchical baselines

datasets
clef
rcv1
ipc
dmoz-small
dmoz-2010
dmoz-2012
dmoz-2011
swiki-2011
lwiki

hr-id166 hr-lr

0.42
0.55
6.81
0.52
8.23
36.66
58.31
89.23
2230.54

1.02
11.74
15.91
3.73
123.22
229.73
248.07
296.87
7282.09

td
0.13
0.21
2.21
0.11
3.97
12.49
16.39
21.34
na

hid166

3.19
na
ns

ot
1.31
ns
ns

289.60

132.34

ns
ns
na
na
na

ns
ns
ns
na
na

hblr
3.05
na
31.20
5.22
ns
ns
na
na
na

[na - not applicable; ns - not scalable]

table: training runtime comparison (in mins)

huzefa rangwala and azad naik

george mason university

13th aug, 2017

39 / 117

cost-sensitive learning, charuvaka & rangwala, ecml   15

motivation

drawbacks of recursive id173

scalable, but more expensive to train than    at classi   cation
requires specialized implementation and communication between
processing node
does not deal with class imbalance directly

objective

decouple models so that they can be trained in parallel without
dependencies between models

account for class imbalance in the optimization framework

huzefa rangwala and azad naik

george mason university

13th aug, 2017

40 / 117

hierarchical id173 re-examination - i

huzefa rangwala and azad naik

george mason university

13th aug, 2017

41 / 117

hierarchical id173 re-examination - ii

opposing learning in   uences:

loss term - model for a node is forced to be dissimilar to all other
nodes
id173 term - model is forced to be similar to its neighbors;
greater similarity to nearer neighbors

resultant e   ect:

mistakes on negative examples that come from near nodes is less
severe than those coming from far nodes while still taking advantage of
the hierarchy

huzefa rangwala and azad naik

george mason university

13th aug, 2017

42 / 117

cost-sensitive loss

consider the loss term for class    t    which is separable over examples

(cid:80)

i

loss(yi , wt

i xi )

each loss value is multiplied by importance of the example for this
class

loss(yi , wt

i xi )      (t, yi )

(cid:80)

i

this is an example of    instance-based    cost sensitive learning

c t
i =   (t, y1)

huzefa rangwala and azad naik

george mason university

13th aug, 2017

43 / 117

hierarchical costs

how to de   ne costs based on hierarchy?

tree distance (trd) - undirected graph distance between between
nodes

number common ancestors (nca) - the number of ancestors in
common to target class and class label

exponentiated tree distance (extrd) - squash tree distance into
a suitable range using validation

huzefa rangwala and azad naik

george mason university

13th aug, 2017

44 / 117

ci = 1 + l/[1 + exp|n     n0|]

imbalance costs

using the same formulation of
cost-sensitive learning, data
imbalance can also be addressed

due to very large skew, inverse
class size can result in extremely
large weights. fix using
squashing function shown in fig.

multiply to combine with
hierarchical costs

huzefa rangwala and azad naik

george mason university

13th aug, 2017

45 / 117

ni = num examples
n0, l = user de   ned constants

experiments

dataset

for comparison purpose same dataset has been used as proposed in
the paper [gopal and yang, sigkdd   13]

comparison methods
flat baseline

lr - one-vs-rest binary id28 is used in the conventional
   at classi   cation setting

hierarchical baselines

top-down id28 (td-lr) - one-vs-rest multi-class
classi   er trained at each internal node
hr-lr [gopal and yang, sigkdd   13] - a recursive id173
approach based on hierarchical relationships

huzefa rangwala and azad naik

george mason university

13th aug, 2017

46 / 117

results (hierarchical costs)

datasets

clef

dmoz-small

ipc

rcv1

lr
trd
nca
extrd
lr
trd
nca
extrd
lr
trd
nca
extrd
lr
trd
nca
extrd

micro-f1 (   ) macro-f1 (   )

79.82
80.02
80.02
80.22
46.39
47.52   
47.36   
47.36   
55.04
55.24   
55.33   
55.31   
78.43
79.46   
79.74   
79.33   

53.45
55.51
57.48
57.55   
30.20
31.37   
31.20   
31.19   
48.99
50.20   
50.29   
50.29   
60.37
60.61
60.76
61.74   

hf1 (   )
85.24
85.39
85.34
85.34
67.00
68.26
68.12
68.20
72.82
73.21
73.28
73.26
80.16
82.83
83.11
82.91

te (   )
0.994
0.984
0.986
0.982
3.569
3.449
3.460
3.456
1.974
1.954
1.949
1.951
0.534
0.451
0.442
0.466

table: performance comparison of hierarchical costs

huzefa rangwala and azad naik

george mason university

13th aug, 2017

47 / 117

results (imbalance costs)

datasets

clef

dmoz-small

ipc

rcv1

micro-f1 (   ) macro-f1 (   )

imb + lr
imb + trd
imb + nca
imb + extrd
imb + lr
imb + trd
imb + nca
imb + extrd
imb + lr
imb + trd
imb + nca
imb + extrd
imb + lr
imb + trd
imb + nca
imb + extrd

79.52
79.92
79.62
80.32
48.55   
49.03   
48.87   
tbf49.03   
55.04
55.60   
55.33
55.67   
78.59   
79.63   
79.61
79.22

53.11
52.84
51.89
58.45
32.72   
33.21   
33.27   
33.34   
49.00
50.45   
50.29
50.42
60.77
61.04
61.04
61.33

hf1 (   )
85.19
85.59
85.34
85.69
68.62
69.41
69.37
69.54
72.82
73.56
73.28
73.58
81.27
83.13
82.65
82.89

te (   )
1.002
0.978
0.994
0.966
3.406
3.334
3.335
3.322
1.974
1.933
1.949
1.931
0.511
0.435
0.458
0.469

table: peformance comparison with imbalance cost included

huzefa rangwala and azad naik

george mason university

13th aug, 2017

48 / 117

results (our best with other methods)

datasets

clef

dmoz-small

ipc

rcv1

td-lr
lr
hr-lr
hiercost
td-lr
lr
hr-lr
hiercost
td-lr
lr
hr-lr
hiercost
td-lr
lr
hr-lr
hiercost

micro-f1 (   ) macro-f1 (   )

73.06
79.82
80.12
80.32
40.90
46.39
45.11
49.03   
50.22
55.04
55.37
55.67   
77.85
78.43
81.23
79.22   

34.47
53.45
55.83
58.45   
24.15
30.20
28.48
33.34   
43.87
48.99
49.60
50.42   
57.80
60.37
55.81
61.33

hf1 (   )
79.32
85.24
na

te (   )
1.366
0.994
na

85.69
69.99
67.00
na
69.54
69.33
72.82
na
73.58
88.78
80.16
na
82.89

0.966
3.147
3.569
na
3.322
2.210
1.974
na

1.931
0.524
0.534
na

0.469

table: performance comparison of hiercost with other baseline methods

huzefa rangwala and azad naik

george mason university

13th aug, 2017

49 / 117

runtime comparison

datasets
clef
dmoz-small
ipc
rcv1
dmoz-2010
dmoz-2012

td-lr

<1
4
27
20
196
384

lr
<1
41
643
29

hiercost

<1
40
453
48

15191
46044

20174
50253

table: total training runtimes (in mins)

huzefa rangwala and azad naik

george mason university

13th aug, 2017

50 / 117

demo of software

freely available for research and education purpose at:

https://cs.gmu.edu/   mlbio/hiercost/

software: implemented in python using scikit-learn machine learning
and id166light-loader package
other prerequisite package:

numpy
scipy
networkx
pandas

huzefa rangwala and azad naik

george mason university

13th aug, 2017

51 / 117

command line interface (cli) options

two main cli is exposed for easy training and classi   cation

train.py

-d : train data    le path
-t : hierarchy    le path
-m : path to save learned model parameters
-f : number of features
-r : id173 parameter ( 0; default = 1)
-i : to incorporate imbalance cost
-c : cost function to use (lr, trd, nca, etrd) (default =    lr   )
-u : for multi-label classi   cation (default = single-label classi   cation)
-n: set of nodes to train (for parallelization)

predict.py

-p : path to save prediction for test examples
-m, -d, -t, -f, -u : similar functionalities

huzefa rangwala and azad naik

george mason university

13th aug, 2017

52 / 117

part ii

inconsistent hierarchy

(30 minutes break)

huzefa rangwala and azad naik

george mason university

13th aug, 2017

53 / 117

motivation

prede   ned hierarchy

hierachy de   ned by the domain experts

re   ects human-view of the domain - may not be optimal for machine
learning classi   cation algorithms

huzefa rangwala and azad naik

george mason university

13th aug, 2017

54 / 117

motivation

flat classi   cation

works well for well-balanced datasets with smaller number of
categories

expensive train/prediction cost

hierarchical classi   cation

performs well for rare categories by leveraging hierarchical structure

computationally e   cient

preferable for large-scale datasets

some benchmark datasets have good performance with    at method (and
its variant). can we improve upon that using hierarchical settings?

huzefa rangwala and azad naik

george mason university

13th aug, 2017

55 / 117

case study: ng dataset

di   erent hierarchical structures results in completely di   erent
classi   cation performance

  f1=77.04, mf1=77.94   f1=79.42, mf1=79.82   f1=81.24, mf1=81.94
huzefa rangwala and azad naik
56 / 117

george mason university

13th aug, 2017

well-known hc methods

[hr     lr] min

w

parent-child id173 [gopal and yang, kdd   13]

(cid:80)

k   n

(cid:1)(cid:1)

i wt

l xi

2 + c(cid:80)

n(cid:80)

l   l

i=1

log(cid:0)1 + exp(cid:0)   y l
(cid:1)(cid:1)(cid:35)

i wt

l xi

  i log(cid:0)1 + exp(cid:0)   y l

1
||wk     w  (k)||2
2
(cid:34)

||wl||2

2 + c

1
2

n(cid:80)

i=1

cost-sensitive learning [charuvaka and rangwala, ecml   15]

[hiercost] min
wl

hc methods uses hierarchical structure. performance can deteriorate if
hierarchy used is not consistent.

huzefa rangwala and azad naik

george mason university

13th aug, 2017

57 / 117

reason for inconsistencies within prede   ned hierarchy - i

hierarchy is designed for the sole purpose of easy search and
navigation without taking classi   cation into consideration

huzefa rangwala and azad naik

george mason university

13th aug, 2017

58 / 117

reason for inconsistencies within prede   ned hierarchy - i

hierarchy is designed for the sole purpose of easy search and
navigation without taking classi   cation into consideration

hierarchy is created based on semantics which is independent of data

whereas

classi   cation depends on data characteristics such as term frequency

   our expectation: data-driven hierarchy can be much powerful   

huzefa rangwala and azad naik

george mason university

13th aug, 2017

58 / 117

reason for inconsistencies within prede   ned hierarchy - i

hierarchy is designed for the sole purpose of easy search and
navigation without taking classi   cation into consideration

hierarchy is created based on semantics which is independent of data

whereas

classi   cation depends on data characteristics such as term frequency

   our expectation: data-driven hierarchy can be much powerful   

apriori it is not clear to domain experts when to generate new nodes
(hierarchy expansion) or merge two or more nodes (link creation) in
the hierarchy

huzefa rangwala and azad naik

george mason university

13th aug, 2017

58 / 117

reason for inconsistencies within prede   ned hierarchy - ii

given list of categories: di   erent experts may come up with di   erent
hierarchies with completely di   erent classi   cation results

huzefa rangwala and azad naik

george mason university

13th aug, 2017

59 / 117

reason for inconsistencies within prede   ned hierarchy - ii

given list of categories: di   erent experts may come up with di   erent
hierarchies with completely di   erent classi   cation results

large number of classes with confusing labels pose a unique challenge
for manual design of good hierarchy

huzefa rangwala and azad naik

george mason university

13th aug, 2017

59 / 117

reason for inconsistencies within prede   ned hierarchy - iii

dynamic changes can a   ect hierarchical relationships
   flood    is the sub-group of geography class but during chennai    ood
it becomes political news

huzefa rangwala and azad naik

george mason university

13th aug, 2017

60 / 117

what we want?

   prede   ned hierarchy   

huzefa rangwala and azad naik

george mason university

13th aug, 2017

61 / 117

what we want?

   prede   ned hierarchy   

   

   data-driven hierarchy   

(for improving classi   cation performance)

huzefa rangwala and azad naik

george mason university

13th aug, 2017

61 / 117

literature overview

huzefa rangwala and azad naik

george mason university

13th aug, 2017

62 / 117

flattening strategy

motivation

for large scale datasets top-down (td) id187 are
preferred over    at models due to computational bene   te (training and
prediction time)
td models performance su   ers due to error propagation i.e.
compounding of errors from misclassi   cations at higher levels which
cannot be recti   ed at the lower levels

objective

modify prede   ned hierarchy by removing (   attening) inconsistent
nodes to improve the classi   cation performance of td models

reduces top-down error propagation due to less number of decisions
for classifying unlabeled example

huzefa rangwala and azad naik

george mason university

13th aug, 2017

63 / 117

flatten hierarchies, wang and lu, icdim   10

level flattening techniques

single or multiple levels within the hierarchy is    attened

based on level(s)    attened various methods exist, for e.g . tlf, blf,
mlf

drawback - all nodes in the level(s) is identi   ed as inconsistent which
may not be true; resulting in poor classi   cation performance

huzefa rangwala and azad naik

george mason university

13th aug, 2017

64 / 117

selected inconsistent node removal (flattening)

rather than    attening entire level(s) only subset of the inconsistent
nodes are removed from the hierarchy

criterion to decide inconsistent nodes - degree of error made at the
node, margin-based or learning-based strategy

comparatively better performance than level    attening methods

huzefa rangwala and azad naik

george mason university

13th aug, 2017

65 / 117

inconsistent node removal - i

local approach for inr (level-inr)

inconsistent set of nodes determined for each level based on loss
function values (such as logistic loss) obtained for nodes at that level

criterion for    attening nodes - mean and standard deviation per level

di   erent levels have di   erent threshold for node    attening

huzefa rangwala and azad naik

george mason university

13th aug, 2017

66 / 117

inconsistent node removal - ii

global approach for inr (global-inr)

inconsistent node determined by considering id168 of all
internal nodes in the hierarchy

criterion for    attening nodes - mean and standard deviation of all
nodes

all levels have same threshold for node    attening

huzefa rangwala and azad naik

george mason university

13th aug, 2017

67 / 117

comparison methods

one-vs-rest models is trained for each node (except root) in the
hierarchy

predictions are made starting from the root node and recursively
selecting the best child nodes until a leaf node is reached

hierarchical baselines

top-down id28 (td-lr) - prede   ned hierarchy
used for training the models
level flattening [wang and lu, icdim   10] -    attened hierarchy
used for training the models, based on level    attened we have:

tlf - top level    attened
blf - bottom level    attened and
mlf - multiple level    attened

mta [babbar et al., nip   13] - hierarchy is modi   ed using the
margin value computed at each node

huzefa rangwala and azad naik

george mason university

13th aug, 2017

68 / 117

comparison against other flattening methods

level-inf and global-inf performed comparatively better than other
approaches
global approach has better performance as compared to local
approach

name

clef

diatoms

ipc

dmoz-small

dmoz-2010

dmoz-2012

x
x
x
x
x
x

top-down hierarchical baselines
metrics
mlf mta
tlf
  f1(   ) 75.84
74.48
mf1(   ) 38.45
39.53
  f1(   ) 56.93
58.36
mf1(   ) 45.17
45.21
  f1(   ) 51.28
51.36
mf1(   ) 44.99
42.80
  f1(   ) 45.48
46.01
mf1(   ) 30.60
30.82
  f1(   ) 41.32
41.82
mf1(   ) 29.05
29.18
  f1(   ) 50.32
50.31
mf1(   ) 29.89
30.04

blf
73.76
40.93
53.27
44.30
50.36
43.74
44.34
30.94
40.34
28.41
50.11
29.73

45.80
30.62
41.77
29.11
48.05
27.65

proposed models
level-inf global-inf
77.14(cid:77)
46.54(cid:78)
61.31(cid:78)
51.85(cid:78)
52.30(cid:77)
45.65(cid:77)
46.61(cid:77)
31.86(cid:78)
42.37
30.41
50.64
30.58

75.25
39.89
58.32
48.77
50.40
43.26
45.43
30.34
40.71
28.66
49.90
30.52

(cid:78) (and (cid:77)) indicates that improvements are statistically signi   cant with
p-value <0.01 (and <0.05).

huzefa rangwala and azad naik

george mason university

13th aug, 2017

69 / 117

comparison against flat method

name

dmoz-small

dmoz-2010

dmoz-2012

# train
example
per class

   5
6-10
11-50
>50
avg.
   5
6-10
11-50
>50
avg.
   5
6-10
11-50
>50
avg.

best proposed
(global-inf)
hf1
mf1
51.86
28.77
67.47
55.55
72.26
78.74
69.43
86.70
31.86(cid:77)
63.37
53.59
18.23
55.76
23.03
62.39
42.56
70.74
77.51
28.41(cid:77)
56.17
50.56
10.28
50.71
20.37
37.19
73.16
53.20
79.73
29.14(cid:78)
68.24

flat baseline

(lr)

mf1
27.02
54.76
72.60
71.44
30.80
14.35
22.62
43.26
73.20
27.06
8.78
18.84
37.98
55.72
27.04

hf1
46.81
65.40
80.12
88.95
60.87
48.13
51.84
61.85
81.51
53.94
48.01
48.82
73.24
84.92
66.45

huzefa rangwala and azad naik

george mason university

13th aug, 2017

70 / 117

runtime comparison (in mins)

training time

clef diatoms ipc dmoz-small dmoz-2010 dmoz-2012

global-inf 3
1

lr

10
3

830
658

68
46

25,462
15,248

63,000
46,124

prediction time

huzefa rangwala and azad naik

george mason university

13th aug, 2017

71 / 117

drawbacks of flattening strategy

flattening strategy although useful upto certain extent has few
limitations

inability to deal with inconsistencies in di   erent branches of the
hierarchy

rewiring strategy can be used to resolve inconsistencies that occurs in
di   erent branch

huzefa rangwala and azad naik

george mason university

13th aug, 2017

72 / 117

hierarchy adjustment using elementary operation - i,
tang et al., sigkdd   06

elementary operation: promote, demote, merge

huzefa rangwala and azad naik

george mason university

13th aug, 2017

73 / 117

hierarchy adjustment using elementary operation - ii

assumption: the optimal hierarchy is near the neighborhood of
prede   ned taxonomy
search for constrained optimal hierarchy by applying sequence of
elementary operations and searching in the hierarchy space

huzefa rangwala and azad naik

george mason university

13th aug, 2017

74 / 117

proposed hierarchy adjustment algorithm

wrapper based approach for hierarchy modi   cation; requires hierarchy
evaluation after each modi   cation which is computationally expensive

input: prede   ned hierarchy (h0), training
data (dt), validation data (dv )
1 generate neighbor hierarchies for h0
2 train hierarchical classi   cation models

for each neighbor on dt

3 evaluate hierarchical classi   ers on dv
4 pick the best neighbor hierarchy as h0
5 until no improvement, repeat from

step 1

huzefa rangwala and azad naik

george mason university

13th aug, 2017

75 / 117

dataset for experimental evaluation

sub-branch from aol database hierarchy is used for evaluation

data is small w .r .t no. of features and classes

datasets #total node #leaf node height #training #features
soc
kids

34,003
48,115

5,248
15,795

69
244

83
299

4
5

table: dataset statistics

huzefa rangwala and azad naik

george mason university

13th aug, 2017

76 / 117

performance results

adjusted hierarchy shows signi   cant performance improvement in
comparison to prede   ned (original) hierarchy and hierarchy generated
using id91 approach

figure: soc (left) and kids (right) dataset results

huzefa rangwala and azad naik

george mason university

13th aug, 2017

77 / 117

filter based rewiring strategy

motivation

for large scale datasets wrapper based approaches are intractable due
to multiple hierarchy evaluations

objective

modify prede   ned hierarchy using    lter based rewiring strategy that
does not requires multiple hierarchy evaluations

without signi   cant loss in performance

huzefa rangwala and azad naik

george mason university

13th aug, 2017

78 / 117

proposed rewiring strategy

elementary operation: node creation, parent-child rewiring, node
deletion

huzefa rangwala and azad naik

george mason university

13th aug, 2017

79 / 117

proposed rewiring strategy algorithm

filter based approach for hierarchy modi   cation

input: prede   ned hierarchy (h0), train data
(dt)
1 compute pairwise similarity between classes

de   ned in h0 on dt

2 group together most similar classes

3 identify inconsistencies within the hierarchy

4 apply elementary operations: node creation

or parent-child rewiring to correct
inconsistencies and obtain new hierarchy h1
5 perform post-processing step (node deletion)

on h1 to obtain new hierarchy h2

6 train and evaluate hierarchical classi   cation

models on h2

huzefa rangwala and azad naik

george mason university

13th aug, 2017

80 / 117

case study: ng dataset

recap: figure shows di   erent hierarchical structures obtained using
   attening and rewiring approaches

  f1=77.04, mf1=77.94   f1=79.42, mf1=79.82   f1=81.24, mf1=81.94
huzefa rangwala and azad naik
81 / 117

george mason university

13th aug, 2017

performance results - flat measure

t-easy method is slightly better due to the brute-force method to
   nd optimal hierarchy
t-easy is very expensive; not scalable for large-scale datasets

td-lr

agglomerative

id91

flattening
global-inf

name

clef

diatoms

ipc

dmoz-small

dmoz-2010

dmoz-2012

evaluation
metrics
  f1(   )
mf1(   )
  f1(   )
mf1(   )
  f1(   )
mf1(   )
  f1(   )
mf1(   )
  f1(   )
mf1(   )
  f1(   )
mf1(   )

72.74
35.92
53.27
44.46
49.32
42.51
45.10
30.65
40.22
28.37
50.13
29.89

73.24
38.27
56.08
44.78
49.83
44.50
45.94
30.75

not scalable
not scalable
not scalable
not scalable

77.14
46.54
61.31
51.85
52.30
45.65
46.61
31.86
42.37
30.41
50.64
30.58

rewiring methods
t-easy
78.12
48.83(cid:78)
62.34(cid:78)
53.81(cid:78)
53.94(cid:77)
46.10(cid:77)

rewhier
78.00
47.10(cid:78)
62.05(cid:78)
52.14(cid:78)
54.28(cid:77)
46.04(cid:77)
48.25(cid:77)
32.92(cid:78)
43.10
31.21
51.82
31.24

not scalable
not scalable
not scalable
not scalable
not scalable
not scalable

(cid:78) (and (cid:77)) indicates that improvements are statistically signi   cant with
p-value <0.01 (and <0.05).

huzefa rangwala and azad naik

george mason university

13th aug, 2017

82 / 117

performance results - hierarchical measure

name

clef

diatoms

ipc

dmoz-small

dmoz-2012

hierarchy flattening rewiring methods
rewhier

used global-inf t-easy
81.43
81.82
64.28
66.35
67.23
68.10

79.06
80.14
80.87
81.28
62.80
63.24
63.88
64.27
64.73
68.34
66.29
68.36
63.37 not scalable 66.18
64.97 not scalable 66.30
73.19 not scalable 74.21

original
modi   ed
original
modi   ed
original
modi   ed
original
modi   ed
original

huzefa rangwala and azad naik

george mason university

13th aug, 2017

83 / 117

runtime comparison (in mins)

t-easy method is very expensive (   20 times more expensive for ipc
dataset)

name

baseline flattening rewiring methods
rewhier
td-lr global-inf t-easy

clef
diatoms
ipc
dmoz-small
dmoz-2010
dmoz-2012

2.5
8.5
607
52

20190
50040

3.5
10
830
65

59
268
26432

7.5
24
1284
168
25600 not scalable 42000
63000 not scalable 94800

not scalable

huzefa rangwala and azad naik

george mason university

13th aug, 2017

84 / 117

# elementary operation comparisons

# elementary operation executed

clef diatoms ipc

tang et al.

(promote, demote, merge)

proposed rewiring filter model

(node creation, pcrewire, node deletion)

52

25

156

34

412

42

huzefa rangwala and azad naik

george mason university

13th aug, 2017

85 / 117

e   ect of varying % of training size

our proposed rewhier method performs well for smaller % of training
dataset

huzefa rangwala and azad naik

george mason university

13th aug, 2017

86 / 117

comparison to flat, td-lr, hiercost approach

our proposed hierarchy modi   cation results in best performance
irrespective of the model trained

(a) macro-f1

(b) hf1

huzefa rangwala and azad naik

george mason university

13th aug, 2017

87 / 117

comparison to flat, td-lr, hiercost approach

figure shows percentage of classes improved over    at approach
80% of the classes showed improved performance with our rewhier
hierarchy and hiercost approach

huzefa rangwala and azad naik

george mason university

13th aug, 2017

88 / 117

conclusion

proposed di   erent approach for    attening inconsistent nodes

local approach
global approach

proposed    lter-based data-driven rewiring approach. works well,
especially for classes with rare categories.

works well for large-scale datasets due to embarassingly parallel steps.

huzefa rangwala and azad naik

george mason university

13th aug, 2017

89 / 117

learning using multiple hierarchies (mtl), charuvaka and
rangwala, icdm   12

motivation

hierarchies are so common that sometimes multiple hierarchies
classify similar data

heterogenous label view provide additional knowledge which should
be exploited by learners
examples

protein structure classi   cation - several hierarchical schemes for
organizing proteins based on curation process or 3d structure
web-page classi   cation - several hierarchy exist for categorizing such
as dmoz and wikipedia datasets

objective

utilize multiple hierarchical label views in id72 context
to improve classi   cation performance

huzefa rangwala and azad naik

george mason university

13th aug, 2017

90 / 117

three di   erent learning settings - i

(i) single task learning (stl) - each task model parameters learned
independently

huzefa rangwala and azad naik

george mason university

13th aug, 2017

91 / 117

three di   erent learning settings - ii

(ii) single hierarchy id72 (shmtl) - relationship
between tasks within a hierarchy are combined individually

huzefa rangwala and azad naik

george mason university

13th aug, 2017

92 / 117

three di   erent learning settings - iii

(iii) multiple hierarchy id72 (mhmtl) - relationship
between tasks from di   erent hierarchies are extracted using common
examples

huzefa rangwala and azad naik

george mason university

13th aug, 2017

93 / 117

mtl formulations

general mtl formulation:

di   erent mtl formulation based on id173:

sparse - all tasks share a single set of useful features

graph id173 - related tasks have similar parameters

trace - task parameters are drawn from a low dimensional sub-space

   (w) = ||w||2,1

   (w) =(cid:80)

(a,b)   e ||wa     wb||2
   (w) = ||w||    = tracenorm(w)

2

huzefa rangwala and azad naik

george mason university

13th aug, 2017

94 / 117

performance: auc comparison

huzefa rangwala and azad naik

george mason university

13th aug, 2017

95 / 117

stl, shmtl and mhmtl comparison

huzefa rangwala and azad naik

george mason university

13th aug, 2017

96 / 117

extreme classi   cation

motivation

many real world problems with multi-class and multi-label involving
an extremely large number of labels or output space

learning classi   ers corresponding to each labels is almost an
impossible task

inter-label dependency not available
examples

predict hashtags from tweets
lshtc (kaggle competition): predict wikipedia tags from documents

objective

given huge set of labels, identify the labels that can be assigned to
unlabeled instances (examples), e   ciently and accurately

huzefa rangwala and azad naik

george mason university

13th aug, 2017

97 / 117

extreme classi   cation challenges

statistical challenges

increase in number of classes is prone to decrease in accuracy due to
complexity in discriminating between di   erent classes

computational challenges

training classi   ers for large number of classes is computationally
infeasible
predicting label for unlabeled test instances is also compute intensive
task

huzefa rangwala and azad naik

george mason university

13th aug, 2017

98 / 117

eigenpartition trees, mineiro and karampatziakis, nips
workshop   15

compute small set of plausible labels using eigenpartition
decomposition at each node in the tree

at each node try to send each classs examples
exclusively left or right
while sending roughly the same number of
examples left or right in aggregate

can be achieved through tree decomposition
[choromanska and langford, nips   15]

optimization function
maximize w t (x t x )w

s.t. w t w     1
1t x t w = 0

invoke expensive classi   ers on set of plausible labels only

huzefa rangwala and azad naik

george mason university

13th aug, 2017

99 / 117

experiments

dataset description

largs-scale text dataset used for model evaluation

huzefa rangwala and azad naik

george mason university

13th aug, 2017

100 / 117

results - lshtc dataset

fastxml [prabhu and varma, sigkdd   14] - node partitioning
formulation which optimized an ndcg based ranking loss over all the
labels
x1 [bhatia et al., nips   15] - e   ective number of labels is reduced
by projecting the high dimensional label vectors onto a low
dimensional linear subspace

huzefa rangwala and azad naik

george mason university

13th aug, 2017

101 / 117

deep classi   cation, xue et al., sigir   08

motivation

large scale taxonomies are more prevelant due to the more speci   c
topic related class information that is bene   cial in several domains
examples

web search browsing -    nding documents relevant to query
modeling user   s for personalized web search -    java    means
di   erent to tourist and programmer
advertisement matching -    nding related ads corresponding to
web-page

traditional algorithms cannot be directly scaled to large scale
problems due to several drawbacks

large scale hierarchies
longer training time and
incorporating structural information into learning framework

huzefa rangwala and azad naik

george mason university

13th aug, 2017

102 / 117

motivation - ii

observations

related categories corresponding to the query document are smaller
than the number of unrelated categories

performance on smaller set of categories is easier and much better
compared to the large set of categories

objective

given large and deep hierarchies identify the relevant subset of
categories for e   ectively    nding the label of unlabeled test instances

huzefa rangwala and azad naik

george mason university

13th aug, 2017

103 / 117

two stages for classi   cation

first stage - search stage

identify related candidate categories corresponding to the test example

second stage - classi   cation stage

select the best candidate categories using classi   cation algorithm as
the label for test document

huzefa rangwala and azad naik

george mason university

13th aug, 2017

104 / 117

first stage

large hierarchy is pruned into smaller subset of hierarchy with
candidate categories and its ancestors only

huzefa rangwala and azad naik

george mason university

13th aug, 2017

105 / 117

second stage

classi   ers are trained on candidate categories

best category for test example is selected using trained classi   ers

huzefa rangwala and azad naik

george mason university

13th aug, 2017

106 / 117

dataset

open directory projects (odp) dataset used for evaluation
dataset statistics

trainining dataset - 1,174,586 web pages, 130,000 categories organized
into 15 levels
test dataset - 130,000 web pages

huzefa rangwala and azad naik

george mason university

figure: data distribution at di   erent levels in the hierarchy

13th aug, 2017

107 / 117

results - micro-f1 performance comparison

search based strategy - best neighbor is chosen as the label for test
example
hierarchical id166 [liu et al., sigkdd   05] - a pachinko machine
style id166
deep classi   cation - top ten neighbors are used as the candidate
categories

huzefa rangwala and azad naik

george mason university

13th aug, 2017

108 / 117

number of candidate categories selection

as the number of candidate categories chosen by the search stage
increases; chances for    nding the correct label for test example in the
classi   cation stage increases
evaluation time increases with increasing number of category selection

huzefa rangwala and azad naik

george mason university

13th aug, 2017

109 / 117

conclusion

large scale hierarchical classi   cation is an important research problem
in machine learning community due to its wide applicability across
several domains

discussed various challenges associated with the hierarchical
classi   cation

discussed various state-of-the-art existing approaches; demo of the
software package developed by the author
emerging topics:

large-scale classi   cation with deep hierarchies
orphan node prediction

huzefa rangwala and azad naik

george mason university

13th aug, 2017

110 / 117

references - i

gopal, siddharth, and yiming yang.    recursive id173 for large-scale
classi   cation with hierarchical and graphical dependencies.    sigkdd, 2013.
charuvaka, anveshi, and huzefa rangwala.    hiercost: improving large
scale hierarchical classi   cation with cost sensitive learning.    ecml, 2015.
tang, lei, jianping zhang, and huan liu.    acclimatizing taxonomic
semantics for hierarchical content classi   cation.    sigkdd, 2006.
li, tao, shenghuo zhu, and mitsunori ogihara.    hierarchical document
classi   cation using automatically generated hierarchy.    journal of intelligent
information systems 29.2 (2007): 211-230.
charuvaka, anveshi, and huzefa rangwala.    id72 for
classifying proteins using dual hierarchies.    icdm, 2012.
punera, kunal, suju rajan, and joydeep ghosh.    automatically learning
document taxonomies for hierarchical classi   cation.    www, 2005.
qi, xiaoguang, and brian d. davison.    hierarchy evolution for improved
classi   cation.    cikm, 2011.
bennett, paul n., and nam nguyen.    re   ned experts: improving
classi   cation in large taxonomies.    sigir, 2009.

huzefa rangwala and azad naik

george mason university

13th aug, 2017

111 / 117

references - ii

silla jr, carlos n., and alex a. freitas.    a survey of hierarchical
classi   cation across di   erent application domains.    dmkd, 2011.
naik, azad, a. charuvaka, and h. rangwala.    classifying documents within
multiple hierarchical datasets using id72.    ictai, 2013.
babbar, rohit, et al.    on    at versus hierarchical classi   cation in large-scale
taxonomies.    nips, 2013.
wang, xiao-lin, and bao-liang lu.    flatten hierarchies for large-scale
hierarchical text categorization.    icdim, 2010.
chuang, shui-lung, and lee-feng chien.    a practical web-based approach
to generating topic hierarchy for text segments.    cikm, 2004.
fagni, tiziano, and fabrizio sebastiani.    on the selection of negative
examples for hierarchical text categorization.    ltc, 2007.
clare, amanda, and ross d. king.    predicting gene function in
saccharomyces cerevisiae.    bioinformatics, 2003.
koller, daphne, and mehran sahami.    hierarchically classifying documents
using very few words.    icml, 1997.

huzefa rangwala and azad naik

george mason university

13th aug, 2017

112 / 117

references - iii

xue et al.    deep classi   cation in large-scale text hierarchies.    sigir, 2008.
tzanetakis, g., and p. cook.    musical genre classi   cation of audio signals.   
ieee transactions on speech and audio processing, 2007.
gopal, siddharth, et al.    bayesian models for large-scale hierarchical
classi   cation.    nips, 2012.
xiao, lin, dengyong zhou, and mingrui wu.    hierarchical classi   cation via
orthogonal transfer.    icml, 2011.
naik, azad, and huzefa rangwala.    a ranking-based approach for
hierarchical classi   cation.    dsaa, 2015.
tsochantaridis, ioannis, et al.    large margin methods for structured and
interdependent output variables.    jmlr, 2005.
liu, tie-yan, et al.    support vector machines classi   cation with a very
large-scale taxonomy.    sigkdd, 2005.
caruana, rich.    multitask learning.    machine learning, 1997.
anveshi charuvaka and huzefa rangwala.    approximate block coordinate
descent for large scale hierarchical classi   cation.    sac, 2015.

huzefa rangwala and azad naik

george mason university

13th aug, 2017

113 / 117

references - iv

dumais, susan, and hao chen.    hierarchical classi   cation of web content.   
sigir, 2000.
mineiro, paul, and karampatziakis, nikos.    a hierarchical spectral method
for extreme classi   cation.    eprint arxiv:1511.03260 (nips workshop), 2015.
choromanska, anna, et al.    extreme multi class classi   cation.    nips
workshop: extreme classi   cation, 2013.
mccallum, andrew, et al.    improving text classi   cation by shrinkage in a
hierarchy of classes.    icml, 1998.
babbar, rohit, et al.    maximum-margin framework for training data
synchronization in large-scale hierarchical classi   cation.    nip, 2013.
choromanska, anna e., and john langford.    logarithmic time online
multiclass prediction.    nips, 2015.
prabhu, yashoteja, and manik varma.    fastxml: a fast, accurate and
stable tree-classi   er for extreme multi-label learning.    sigkdd, 2014.
bhatia, kush, et al.    sparse local embeddings for extreme multi-label
classi   cation.    nips, 2015.

huzefa rangwala and azad naik

george mason university

13th aug, 2017

114 / 117

references - v

naik, a., and rangwala, h.    filter based taxonomy modi   cation for
improving hierarchical classi   cation.    http://arxiv.org/abs/1603.00772,
2016.
peng, hanchuan, fuhui long, and chris ding.    feature selection based on
mutual information criteria of max-dependency, max-relevance, and
min-redundancy.    pami, 2005.
ding, chris, and hanchuan peng.    minimum redundancy feature selection
from microarray gene expression data.    journal of bioinformatics and
computational biology, 2005.

huzefa rangwala and azad naik

george mason university

13th aug, 2017

115 / 117

acknowledgement

presenter:

huzefa rangwala

azad naik

slides available for download at:

http://cs.gmu.edu/ mlbio/kdd2017tutorial.html

huzefa rangwala and azad naik

george mason university

13th aug, 2017

116 / 117

thank you!

huzefa rangwala and azad naik

george mason university

13th aug, 2017

117 / 117

