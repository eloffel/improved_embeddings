id203 and statistics for data science

carlos fernandez-granda

preface

these notes were developed for the course id203 and statistics for data science at the
center for data science in nyu. the goal is to provide an overview of fundamental concepts
in id203 and statistics from    rst principles. i would like to thank levent sagun and vlad
kobzar, who were teaching assistants for the course, as well as brett bernstein and david
rosenberg for their useful suggestions.
i am also very grateful to all my students for their
feedback.

while writing these notes, i was supported by the national science foundation under nsf
award dms-1616340.

new york, august 2017

ii

contents

1 basic id203 theory

1.1 id203 spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 id155 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3
independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 random variables

2.1 de   nition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 continuous random variables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 conditioning on an event
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 functions of random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6 generating random variables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7 proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 multivariate random variables

3.1 discrete random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 continuous random variables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3 joint distributions of discrete and continuous variables . . . . . . . . . . . . . . .
3.4
independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.5 functions of several random variables
. . . . . . . . . . . . . . . . . . . . . . . .
3.6 generating multivariate random variables . . . . . . . . . . . . . . . . . . . . . .
3.7 rejection sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 expectation

4.1 expectation operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 mean and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 covariance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 conditional expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1
1
3
6

9
9
10
17
25
27
28
31

33
33
37
45
49
58
61
62

67
67
70
76
84
86

5 random processes

92
92
5.1 de   nition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
5.2 mean and autocovariance functions . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.3
independent identically-distributed sequences . . . . . . . . . . . . . . . . . . . .
98
5.4 gaussian process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 poisson process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.6 random walk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102

iii

contents

iv

5.7 proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

6 convergence of random processes

106
6.1 types of convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
6.2 law of large numbers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
6.3 central limit theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
6.4 monte carlo simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

7 markov chains

120
7.1 time-homogeneous discrete-time markov chains . . . . . . . . . . . . . . . . . . . 120
7.2 recurrence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
7.3 periodicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7.4 convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
7.5 markov-chain monte carlo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134

8 descriptive statistics

139
8.1 histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
8.2 sample mean and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
8.3 order statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
8.4 sample covariance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
8.5 sample covariance matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

9 frequentist statistics

151
independent identically-distributed sampling . . . . . . . . . . . . . . . . . . . . 151
9.1
9.2 mean square error
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
9.3 consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
9.4 con   dence intervals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
9.5 nonparametric model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
9.6 parametric model estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
9.7 proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

10 bayesian statistics

176
10.1 bayesian parametric models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
10.2 conjugate prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
10.3 bayesian estimators
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180

11 hypothesis testing

186
11.1 the hypothesis-testing framework . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
11.2 parametric testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
11.3 nonparametric testing: the permutation test . . . . . . . . . . . . . . . . . . . . 193
11.4 multiple testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197

12 id75

199
12.1 linear models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
12.2 least-squares estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
12.3 over   tting
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
12.4 global warming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
12.5 proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206

contents

v

a set theory

a.1 basic de   nitions
a.2 basic operations

210
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210

b id202

212
b.1 vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
b.2 inner product and norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
b.3 orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
b.4 projections
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
b.5 matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
b.6 eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
b.7 eigendecomposition of symmetric matrices . . . . . . . . . . . . . . . . . . . . . . 226
b.8 proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228

chapter 1

basic id203 theory

in this chapter we introduce the mathematical framework of id203 theory, which makes it
possible to reason about uncertainty in a principled way using set theory. appendix a contains
a review of basic set-theory concepts.

1.1 id203 spaces

our goal is to build a mathematical framework to represent and analyze uncertain phenomena,
such as the result of rolling a die, tomorrow   s weather, the result of an nba game, etc. to this
end we model the phenomenon of interest as an experiment with several (possibly in   nite)
mutually exclusive outcomes. these outcomes are grouped into sets called events each of
which is assigned a certain value by a measure (recall that a measure is a function that maps
sets to real numbers). the value of the measure on a particular event quanti   es how likely it is
for the actual outcome to belong to that event.

more formally, the experiment is characterized by constructing a id203 space.

de   nition 1.1.1 (id203 space). a id203 space is a triple (   ,f, p) consisting of:

    a sample space    , which contains all possible outcomes of the experiment.
    a set of events f, which must be a   -algebra (see de   nition 1.1.2 below).
    a id203 measure p that assigns probabilities to the events in f (see de   nition 1.1.4

below).

sample spaces may be discrete or continuous. examples of discrete sample spaces include the
possible outcomes of a coin toss, the score of a basketball game, the number of people that show
up at a party, etc. continuous sample spaces are usually intervals of r or rn used to model
time, position, temperature, etc.

the term   -algebra is used in measure theory to denote a collection of sets that satisfy certain
conditions listed below. don   t be too intimidated by it. it is just a sophisticated way of stating
that if we assign a id203 to certain events (for example it will rain tomorrow or it will
snow tomorrow ) we also need to assign a id203 to their complements (i.e. it will not rain
tomorrow or it will not snow tomorrow ) and to their union (it will rain or snow tomorrow ).

1

chapter 1. basic id203 theory

2

de   nition 1.1.2 (  -algebra). a   -algebra f is a collection of sets in     such that:

1. if a set s     f then sc     f.
2. if the sets s1, s2     f, then s1     s2     f. this also holds for in   nite sequences; if

s1, s2, . . .     f then       i=1si     f.

3.         f.

if our sample space is discrete, a possible choice for the   -algebra is the power set of the sample
space, which consists of all possible sets of elements in the sample space. if we are tossing a coin
and the sample space is

then the power set is a valid   -algebra

    := {heads, tails} ,

f := {heads or tails, heads, tails,   } ,

(1.1)

(1.2)

where     denotes the empty set. however, in other cases we might decide to construct   -algebras
that do not contain every possible set of outcomes, as in the following example.

example 1.1.3 (basketball game). the cleveland cavaliers are playing the golden state
warriors. we want to model the game probabilistically. the choice of sample space is very
natural: it is just the set of every possible score.

    := {cavs 1     warriors 0, cavs 0     warriors 1, . . . , cavs 101     warriors 97, . . .} .

(1.3)

we could choose the   -algebra of the model to be the collection of all subsets of    . this provides
a lot of    exibility, since we can reason about events such as the warriors win by 1 or the cavs
lose and the warriors score less than 100 points. however, it might be overkill to consider so
many possibilities. imagine we only care about whether the cavs win or lose. in that case, we
can choose the   -algebra

f := {cavs win, warriors win, cavs or warriors win,   } .

(1.4)

this is a valid choice (check the conditions!), which simpli   es the modeling substantially. the
downside is that it does not allow to reason about events that are not contained in it, such as
the warriors win by 1.

(cid:52)
the role of the id203 measure p is to quantify how likely we are to encounter each of the
events in the   -algebra.

de   nition 1.1.4 (id203 measure). a id203 measure is a function de   ned over the
sets in a   -algebra f such that:

1. p (s)     0 for any event s     f.

chapter 1. basic id203 theory

2. if the sets s1, s2, . . . , sn     f are disjoint (i.e. si     sj =     for i (cid:54)= j) then

p (   n

i=1si) =

p (si) .

n(cid:88)i=1

similarly, for a countably in   nite sequence of disjoint sets s1, s2, . . .     f

p(cid:16) lim
n         n

i=1si(cid:17) = lim

n      

p (si) .

n(cid:88)i=1

3

(1.5)

(1.6)

3. p (   ) = 1.

the two    rst axioms capture the intuitive idea that the id203 of an event is a measure
such as mass (or length or volume):
just like the mass of any object is nonnegative and the
total mass of several distinct objects is the sum of their masses, the id203 of any event
is nonnegative and the id203 of the union of several disjoint objects is the sum of their
probabilities. however, in contrast to mass, the amount of id203 in an experiment cannot
be unbounded.
if it is highly likely that it will rain tomorrow, then it cannot be also very
likely that it will not rain.
if the id203 of an event s is large, then the id203 of
its complement sc must be small. this is captured by the third axiom, which normalizes the
id203 measure (and implies that p (sc) = 1     p (s)).
to simplify notation, we write the id203 of the intersection of several events in the following
form

de   nition 1.1.4 has the following consequences:

p (a, b, c) := p (a     b     c) .

p (   ) = 0,
a     b implies p (a)     p (b) ,
p (a     b) = p (a) + p (b)     p (a     b) .

(1.7)

(1.8)

(1.9)

(1.10)

we omit the proofs (try proving them on your own).

it is important to stress that the id203 measure does not assign probabilities to individual
outcomes, but rather to events in the   -algebra. in some cases, the   -algebra may include sets
that consist of just one outcome (this is the case in the coin-toss example), but this need not
be the case. in example 1.1.3, for instance, we don   t need to model the id203 of every
possible score, because the   -algebra does not include such events. a valid id203 measure
for that example would be

p (cavs win) =

1
2

, p (warriors win) =

1
2

, p (cavs or warriors win) = 1, p (   ) = 0.

1.2 id155

id155 is a crucial concept in probabilistic modeling. it allows us to update
probabilistic models when additional information is revealed. consider a probabilistic space

chapter 1. basic id203 theory

4

(   ,f, p) where we    nd out that the outcome of the experiment belongs to a certain event
s     f. this obviously a   ects how likely it is for any other event s(cid:48)     f to have occurred: for
example, we can rule out events that don   t intersect with s. the updated id203 of each
event is known as the id155 of s(cid:48) given s:

p(cid:0)s(cid:48)|s(cid:1) :=

p (s(cid:48)     s)

p (s)

,

(1.11)

where we assume that p (s) (cid:54)= 0 (later on we will have to deal with the case when s has
zero id203, which often occurs in continuous id203 spaces). the de   nition is rather
intuitive: s is now the new sample space, so if the outcome is in s(cid:48) then it must belong to
s(cid:48)     s. however, just using the id203 of the intersection would underestimate how likely it
is for s(cid:48) to occur because the sample space has been reduced to s. therefore we normalize by
the id203 of s. as a sanity check, we have p (s|s) = 1 and if s and s(cid:48) are disjoint then
p (s(cid:48)|s) = 0.
the id155 p (  |s) is a valid id203 measure in the id203 space
(s,fs, p (  |s)), where fs is a   -algebra that contains the intersection of s and the sets in
f. to simplify notation, when we condition on an intersection of sets we write the conditional
id203 as

p (s|a, b, c) := p (s|a     b     c) ,

(1.12)

for any events s, a, b, c.

example 1.2.1 (flights and rain). jfk airport hires you to estimate how the punctuality of
   ight arrivals is a   ected by the weather. you begin by de   ning a id203 space for which
the sample space is

    = {late and rain, late and no rain, on time and rain, on time and no rain}

(1.13)

and the   -algebra is the power set of    . from data of past    ights you determine that a reasonable
estimate for the id203 measure of the id203 space is

p (late, no rain) =

p (late, rain) =

2
20
3
20

,

,

p (on time, no rain) =

p (on time, rain) =

14
20
1
20

,

.

(1.14)

(1.15)

the airport is interested in the id203 of a    ight being late if it rains, so you de   ne a new
id203 space conditioning on the event rain. the sample space is the set of all outcomes
such that rain occurred, the   -algebra is the power set of {on time, late} and the id203
measure is p (  |rain). in particular,

p (late|rain) =

p (late, rain)

p (rain)

=

3/20

3/20 + 1/20

=

3
4

and similarly p (late|no rain) = 1/8.

(1.16)

(cid:52)

chapter 1. basic id203 theory

5

conditional probabilities can be used to compute the intersection of several events in a structured
way. by de   nition, we can express the id203 of the intersection of two events a, b     f as
follows,

p (a     b) = p (a) p (b|a)
= p (b) p (a|b) .

(1.17)

(1.18)

in this formula p (a) is known as the prior id203 of a, as it captures the information we
have about a before anything else is revealed. analogously, p (a|b) is known as the posterior
id203. these are fundamental quantities in bayesian models, discussed in chapter 10.
generalizing (1.17) to a sequence of events gives the chain rule, which allows to express the
id203 of the intersection of multiple events in terms of conditional probabilities. we omit
the proof, which is a straightforward application of induction.

theorem 1.2.2 (chain rule). let (   ,f, p) be a id203 space and s1, s2, . . . a collection of
events in f,

p (   isi) = p (s1) p (s2|s1) p (s3|s1     s2)      

=(cid:89)i

p(cid:16)si|    i   1

j=1 sj(cid:17) .

(1.19)

(1.20)

sometimes, estimating the id203 of a certain event directly may be more challenging than
estimating its id203 conditioned on simpler events. a collection of disjoint sets a1, a2, . . .
such that     =    iai is called a partition of    . the law of total id203 allows us to pool
conditional probabilities together, weighting them by the id203 of the individual events in
the partition, to compute the id203 of the event of interest.

theorem 1.2.3 (law of total id203). let (   ,f, p) be a id203 space and let the
collection of disjoint sets a1, a2, . . .    f be any partition of    . for any set s     f

p (s) =(cid:88)i
=(cid:88)i

p (s     ai)

p (ai) p (s|ai) .

(1.21)

(1.22)

proof. this is an immediate consequence of the chain rule and axiom 2 in de   nition 1.1.4, since
s =    i s     ai and the sets s     ai are disjoint.
example 1.2.4 (aunt visit). your aunt is arriving at jfk tomorrow and you would like to
know how likely it is for her    ight to be on time. from example 1.2.1, you recall that

p (late|rain) = 0.75, p (late|no rain) = 0.125.
after checking out a weather website, you determine that p (rain) = 0.2.

(1.23)

now, how can we integrate all of this information? the events rain and no rain are disjoint and
cover the whole sample space, so they form a partition. we can consequently apply the law of
total id203 to determine

p (late) = p (late|rain) p (rain) + p (late|no rain) p (no rain)

= 0.75    0.2 + 0.125    0.8 = 0.25.

(1.24)

(1.25)

chapter 1. basic id203 theory

6

so the id203 that your aunt   s plane is late is 1/4.

(cid:52)
it is crucial to realize that in general p (a|b) (cid:54)= p (b|a): most players in the nba probably
own a basketball (p (owns ball|nba) is large) but most people that own basketballs are not in
the nba (p (nba|owns ball) is small). the reason is that the prior probabilities are very di   er-
ent: p (nba) is much smaller than p (owns ball). however, it is possible to invert conditional
probabilities, i.e.    nd p (a|b) from p (b|a), as long as we take into account the priors. this
straightforward consequence of the de   nition of id155 is known as bayes    rule.
theorem 1.2.5 (bayes    rule). for any events a and b in a id203 space (   ,f, p)

p (a|b) =

p (a) p (b|a)

p (b)

,

(1.26)

as long as p (b) > 0.

example 1.2.6 (aunt visit (continued)). you explain the probabilistic model described in
example 1.2.4 to your cousin marvin who lives in california. a day later, you tell him that your
aunt arrived late but you don   t mention whether it rained or not. after he hangs up, marvin
wants to    gure out the id203 that it rained. recall that the id203 of rain was 0.2,
but since your aunt arrived late he should update the estimate. applying bayes    rule and the
law of total id203:

p (rain|late) =

p (late|rain) p (rain)

p (late)

=

=

p (late|rain) p (rain)

p (late|rain) p (rain) + p (late|no rain) p (no rain)

0.75    0.2

0.75    0.2 + 0.125    0.8

= 0.6.

(1.27)

(1.28)

(1.29)

as expected, the id203 that it rained increases under the assumption that your aunt is
late.

(cid:52)

1.3 independence

as discussed in the previous section, conditional probabilities quantify the extent to which the
knowledge of the occurrence of a certain event a   ects the id203 of another event. in some
cases, it makes no di   erence: the events are independent. more formally, events a and b are
independent if and only if

p (a|b) = p (a) .

(1.30)

this de   nition is not valid if p (b) = 0. the following de   nition covers this case but is otherwise
equivalent.
de   nition 1.3.1 (independence). let (   ,f, p) be a id203 space. two events a, b     f
are independent if and only if

p (a     b) = p (a) p (b) .

(1.31)

chapter 1. basic id203 theory

7

similarly, we can de   ne conditional independence between two events given a third event.
a and b are conditionally independent given c if and only if

where p (a|b, c) := p (a|b     c).
a   ected by whether a occurs or not, as long as c occurs.

p (a|b, c) = p (a|c) ,
intuitively, this means that the id203 of b is not

(1.32)

de   nition 1.3.2 (conditional independence). let (   ,f, p) be a id203 space. two events
a, b     f are conditionally independent given a third event c     f if and only if

p (a     b|c) = p (a|c) p (b|c) .

(1.33)

independence does not imply conditional independence or vice versa. this is illustrated by the
following examples.

example 1.3.3 (conditional independence does not imply independence). your cousin marvin
from exercise 1.2.6 always complains about taxis in new york. from his many visits to jfk he
has calculated that

p (taxi|rain) = 0.1, p (taxi|no rain) = 0.6,

(1.34)

where taxi denotes the event of    nding a free taxi after picking up your luggage. given the
events rain and no rain, it is reasonable to model the events plane arrived late and taxi as
conditionally independent,

p (taxi, late|rain) = p (taxi|rain) p (late|rain) ,
p (taxi, late|no rain) = p (taxi|no rain) p (late|no rain) .

(1.35)

(1.36)

the logic behind this is that the availability of taxis after picking up your luggage depends
on whether it   s raining or not, but not on whether the plane is late or not (we assume that
availability is constant throughout the day). does this assumption imply that the events are
independent?

if they were independent, then knowing that your aunt was late would give no information to
marvin about taxi availability. however,

p (taxi) = p (taxi, rain) + p (taxi, no rain)

(by the law of total id203)

= p (taxi|rain) p (rain) + p (taxi|no rain) p (no rain)
= 0.1    0.2 + 0.6    0.8 = 0.5,

p (taxi, late, rain) + p (taxi, late, no rain)

p (taxi|late) =

=

=

p (late)

(by the law of total id203)

p (taxi|rain) p (late|rain) p (rain) + p (taxi|no rain) p (late|no rain) p (no rain)

p (late)

0.1    0.75    0.2 + 0.6    0.125    0.8

0.25

= 0.3.

(1.40)

p (taxi) (cid:54)= p (taxi|late) so the events are not independent. this makes sense, since if the airplane
is late, it is more probable that it is raining, which makes taxis more di   cult to    nd.

(cid:52)

(1.37)

(1.38)

(1.39)

chapter 1. basic id203 theory

8

example 1.3.4 (independence does not imply conditional independence). after looking at your
probabilistic model from example 1.2.1 your contact at jfk points out that delays are often
caused by mechanical problems in the airplanes. you look at the data and determine that

p (problem) = p (problem|rain) = p (problem|no rain) = 0.1,

(1.41)

so the events mechanical problem and rain in nyc are independent, which makes intuitive
sense. after some more analysis of the data, you estimate

p (late|problem) = 0.7, p (late|no problem) = 0.2, p (late|no rain, problem) = 0.5.

the next time you are waiting for marvin at jfk, you start wondering about the id203 of
his plane having had some mechanical problem. without any further information, this proba-
bility is 0.1. it is a sunny day in new york, but this is of no help because according to the data
(and common sense) the events problem and rain are independent.

suddenly they announce that marvin   s plane is late. now, what is the id203 that his
plane had a mechanical problem? at    rst thought you might apply bayes    rule to compute
p (problem|late) = 0.28 as in example 1.2.6. however, you are not using the fact that it is
sunny. this means that the rain was not responsible for the delay, so intuitively a mechanical
problem should be more likely. indeed,

p (problem|late, no rain) =

p (late, no rain, problem)

p (late, no rain)

=

=

p (late|no rain, problem) p (no rain) p (problem)

p (late|no rain) p (no rain)

0.5    0.1
0.125

= 0.4.

(1.42)

(by the chain rule)

(1.43)

since p (problem|late, no rain) (cid:54)= p (problem|late) the events mechanical problem and rain in
nyc are not conditionally independent given the event plane is late.

(cid:52)

chapter 2

random variables

random variables are a fundamental tool in probabilistic modeling. they allow us to model
numerical quantities that are uncertain: the temperature in new york tomorrow, the time of
arrival of a    ight, the position of a satellite... reasoning about such quantities probabilistically
allows us to structure the information we have about them in a principled way.

2.1 de   nition

formally, we de   ne a random variables as a function mapping each outcome in a id203
space to a real number.

de   nition 2.1.1 (random variable). given a id203 space (   ,f, p), a random variable
x is a function from the sample space     to the real numbers r. once the outcome            of
the experiment is revealed, the corresponding x (  ) is known as a realization of the random
variable.

remark 2.1.2 (rigorous de   nition). if we want to be completely rigorous, de   nition 2.1.1 is
missing some details. consider two sample spaces    1 and    2, and a   -algebra f2 of sets in    2.
then, for x to be a random variable, there must exist a   -algebra f1 in    1 such that for any
set s in f2 the inverse image of s, de   ned by

x   1 (s) := {   | x (  )     s} ,

(2.1)

belongs to f1. usually, we take    2 to be the reals r and f2 to be the borel   -algebra, which is

de   ned as the smallest   -algebra de   ned on the reals that contains all open intervals (amazingly,
it is possible to construct sets of real numbers that do not belong to this   -algebra). in any case,
for the purpose of these notes, de   nition 2.1.1 is su   cient (more information about the formal
foundations of id203 can be found in any book on measure theory and advanced id203
theory).

remark 2.1.3 (notation). we often denote events of the form

for some random variable x and some set s as

{x (  )     s :           }

{x     s}

9

(2.2)

(2.3)

chapter 2. random variables

10

to alleviate notation, since the underlying id203 space is often of no signi   cance once we
have speci   ed the random variables of interest.

a random variable quanti   es our uncertainty about the quantity it represents, not the value
that it happens to    nally take once the outcome is revealed. you should never think of a random
variable as having a    xed numerical value. if the outcome is known, then that determines a
realization of the random variable. in order to stress the di   erence between random variables
and their realizations, we denote the former with uppercase letters (x, y , . . . ) and the latter
with lowercase letters (x, y, . . . ).
if we have access to the id203 space (   ,f, p) in which the random variable is de   ned, then
it is straightforward to compute the id203 of a random variable x belonging to a certain
set s:1 it is the id203 of the event that comprises all outcomes in     which x maps to s,

p (x     s) = p ({   | x (  )     s}) .

(2.4)

however, we almost never model the id203 space directly, since this requires estimating the
id203 of every possible event in the corresponding   -algebra. as we explain in sections 2.2
and 2.3, there are more practical methods to specify random variables, which automatically
imply that a valid underlying id203 space exists. the existence of this id203 space
ensures that the whole framework is mathematically sound, but you don   t really have to worry
about it.

2.2 discrete random variables
discrete random variables take values on a    nite or countably in   nite subset of r such as the
integers. they are used to model discrete numerical quantities: the outcome of the roll of a die,
the score in a basketball game, etc.

2.2.1 id203 mass function

to specify a discrete random variable it is enough to determine the id203 of each value
that it can take. in contrast to the case of continuous random variables, this is tractable because
these values are countable by de   nition.

de   nition 2.2.1 (id203 mass function). let (   ,f, p) be a id203 space and x :        
z a random variable. the id203 mass function (pmf ) of x is de   ned as

in words, px (x) is the id203 that x equals x.

px (x) := p ({   | x (  ) = x}) .

(2.5)

we usually say that a random variable is distributed according to a certain pmf.

if the discrete range of x is denoted by d, then the triplet (cid:0)d, 2d, px(cid:1) is a valid id203

space (recall that 2d is the power set of d). in particular, px is a valid id203 measure

1strictly speaking, s needs to belong to the borel   -algebra. again, this comprises essentially any subset of the
reals that you will ever encounter in probabilistic modeling

chapter 2. random variables

11

figure 2.1: id203 mass function of the random variable x in example 2.2.2.

which satis   es

px (x)     0
px (x) = 1.

for any x     d,

(cid:88)x   d

(2.6)

(2.7)

the converse is also true, if a function de   ned on a countable subset d of the reals is nonnegative
and adds up to one, then it may be interpreted as the pmf of a random variable. in fact, in
practice we usually de   ne discrete random variables by just specifying their pmf.

to compute the id203 that a random variable x is in a certain set s we take the sum of
the pmf over all the values contained in s:

p (x     s) =(cid:88)x   s

px (x) .

(2.8)

example 2.2.2 (discrete random variable). figure 2.1 shows the id203 mass function of
a discrete random variable x (check that it adds up to one). to compute the id203 of x
belonging to di   erent sets we apply (2.8):

p (x     {1, 4}) = px (1) + px (4) = 0.5,
p (x > 3) = px (4) + px (5) = 0.6.

(2.9)

(2.10)

(cid:52)

2.2.2

important discrete random variables

in this section we describe several discrete random variables that are useful for probabilistic
modeling.

1234500.10.20.30.4xpx(x)chapter 2. random variables

12

bernoulli

bernoulli random variables are used to model experiments that have two possible outcomes.
by convention we usually represent an outcome by 0 and the other outcome by 1. a canonical
example is    ipping a biased coin, such that the id203 of obtaining heads is p. if we encode
heads as 1 and tails as 0, then the result of the coin    ip corresponds to a bernoulli random
variable with parameter p.

de   nition 2.2.3 (bernoulli). the pmf of a bernoulli random variable with parameter p     [0, 1]
is given by

px (0) = 1     p,
px (1) = p.

(2.11)

(2.12)

a special kind of bernoulli random variable is the indicator random variable of an event. this
random variable is particularly useful in proofs.

de   nition 2.2.4 (indicator). let (   ,f, p) be a id203 space. the indicator random vari-
able of an event s     f is de   ned as

1s (  ) =(cid:40)1,

0,

if        s,
otherwise.

(2.13)

by de   nition the distribution of an indicator random variable is bernoulli with parameter p (s).

geometric

imagine that we take a biased coin and    ip it until we obtain heads.
if the id203 of
obtaining heads is p and the    ips are independent then the id203 of having to    ip k times
is

p (k    ips) = p (1st    ip = tails, . . . , k     1th    ip = tails, kth    ip = heads)

= p (1st    ip = tails)       p (k     1th    ip = tails) p (kth    ip = heads)
= (1     p)k   1 p.

(2.14)

(2.15)

(2.16)

this reasoning can be applied to any situation in which a random experiment with a    xed prob-
ability p is repeated until a particular outcome occurs, as long as the independence assumption
is met. in such cases the number of repetitions is modeled as a geometric random variable.

de   nition 2.2.5 (geometric). the pmf of a geometric random variable with parameter p is
given by

px (k) = (1     p)k   1 p,

k = 1, 2, . . .

(2.17)

figure 2.2 shows the id203 mass function of geometric random variables with di   erent
parameters. the larger p is, the more the distribution concentrates around smaller values of k.

chapter 2. random variables

13

p = 0.2

p = 0.5

p = 0.8

figure 2.2: id203 mass function of three geometric random variables with di   erent parameters.

binomial

binomial random variables are extremely useful in probabilistic modeling. they are used to
model the number of positive outcomes of n trials modeled as independent bernoulli random
variables with the same parameter. the following example illustrates this with coin    ips.

example 2.2.6 (coin    ips). if we    ip a biased coin n times, what is the id203 that we
obtain exactly k heads if the    ips are independent and the id203 of heads is p?

let us    rst consider a simpler problem: what is the id203 of    rst obtaining k heads and
then n     k tails? by independence, the answer is

p (k heads, then n     k tails)
= p (1st    ip = heads, . . . , kth    ip = heads, k + 1th    ip = tails,. . . , nth    ip = tails)
= p (1st    ip = heads)       p (kth    ip = heads) p (k + 1th    ip = tails)       p (nth    ip = tails)
= pk (1     p)n   k .

(2.18)

(2.19)

note that the same reasoning implies that this is also the id203 of obtaining exactly k
heads in any    xed order. the id203 of obtaining exactly k heads is the union of all of these
events. because these events are disjoint (we cannot obtain exactly k heads in two di   erent
orders simultaneously) we can add their individual to compute the id203 of our event of
interest. we just need to know the number of possible orderings. by basic combinatorics, this

is given by the binomial coe   cient(cid:0)n

k(cid:1), de   ned as
(cid:18)n
k(cid:19) :=

n!

.

k! (n     k)!

we conclude that

p (k heads out of n    ips) =(cid:18)n

k(cid:19) pk (1     p)(n   k) .

(2.20)

(2.21)

(cid:52)
the random variable representing the number of heads in the example is called a binomial
random variable.

24681000.20.40.60.8kpx(k)24681000.20.40.60.8k24681000.20.40.60.8kchapter 2. random variables

14

p = 0.2

p = 0.5

p = 0.8

figure 2.3: id203 mass function of three binomial random variables with di   erent values of p and
n = 20.

   = 10

   = 20

   = 30

figure 2.4: id203 mass function of three poisson random variables with di   erent parameters.

de   nition 2.2.7 (binomial). the pmf of a binomial random variable with parameters n and p
is given by

px (k) =(cid:18)n

k(cid:19) pk (1     p)(n   k) ,

k = 0, 1, 2, . . . , n.

(2.22)

figure 2.3 shows the id203 mass function of binomial random variables with di   erent values
of p.

poisson

we motivate the de   nition of the poisson random variable using an example.

example 2.2.8 (call center). a call center wants to model the number of calls they receive
over a day in order to decide how many people to hire. they make the following assumptions:

1. each call occurs independently from every other call.

2. a given call has the same id203 of occurring at any given time of the day.

3. calls occur at a rate of    calls per day.

0510152005  10   20.10.150.20.25kpx(k)0510152005  10   20.10.150.20.25k0510152005  10   20.10.150.20.25k0102030405005  10   20.10.15kpx(k)0102030405005  10   20.10.15k0102030405005  10   20.10.15kchapter 2. random variables

15

in chapter 5, we will see that these assumptions de   ne a poisson process.

our aim is to compute the id203 of receiving exactly k calls during a given day. to do this
we discretize the day into n intervals, compute the desired id203 assuming each interval is
very small and then let n        .
the id203 that a call occurs in an interval of length 1/n is   /n by assumptions 2 and 3.
the id203 that m > 1 calls occur is (  /n)m. if n is very large this id203 is negligible
compared to the id203 that either one or zero calls are received in the interval, in fact it
tends to zero when we take the limit n        . the total number of calls occurring over the whole
hour can consequently be approximated by the number of intervals in which a call occurs, as
long as n is large enough. since a call occurs in each interval with the same id203 and
calls happen independently, the number of calls over a whole day can be modeled as a binomial
random variable with parameters n and p :=   /n.

we now compute the distribution of calls when the intervals are arbitrarily small, i.e. when
n        :

p (k calls during the day ) = lim
n      
= lim

p (k calls in n small intervals)

n      (cid:18)n
n      (cid:18)n

k(cid:19) pk (1     p)(n   k)
k(cid:19)(cid:18)   
n(cid:19)k(cid:18)1    
n(cid:19)(n   k)
n(cid:19)n
k! (n     k)! (n       )k(cid:18)1    

n!   k

  

  

= lim

= lim
n      
  k e     

=

.

k!

the last step follows from the following lemma proved in section 2.7.1.

lemma 2.2.9.

n!

(n     k)! (n       )k(cid:18)1    

  

n(cid:19)n

lim
n      

= e     .

(2.23)

(2.24)

(2.25)

(2.26)

(2.27)

(2.28)

(cid:52)
random variables with the pmf that we have derived in the example are called poisson random
variables. they are used to model situations where something happens from time to time at a
constant rate: packets arriving at an internet router, earthquakes, tra   c accidents, etc. the
number of such events that occur over a    xed interval follows a poisson distribution, as long as
the assumptions we listed in the example hold.

de   nition 2.2.10 (poisson). the pmf of a poisson random variable with parameter    is given
by

px (k) =

  k e     

k!

,

k = 0, 1, 2, . . .

(2.29)

chapter 2. random variables

16

binomial: n = 40, p = 20
40

binomial: n = 80, p = 20
80

binomial: n = 400, p = 20
400

poisson:    = 20

figure 2.5: convergence of the binomial pmf with p =   /n to a poisson pmf of parameter    as n grows.

figure 2.4 shows the id203 mass function of poisson random variables with di   erent values
of   . in example 2.2.8 we prove that as n         the pmf of a binomial random variable with
parameters n and   /n tends to the pmf of a poisson with parameter    (as we will see later in
the course, this is an example of convergence in distribution). figure 2.5 shows an example of
this phenomenon numerically; the convergence is quite fast.

you might feel a bit skeptical about example 2.2.8: the id203 of receiving a call surely
changes over the day and it must be di   erent on weekends! that is true, but the model is
actually very useful if we restrict our attention to shorter periods of time. in figure 2.6 we show
the result of modeling the number of calls received by a call center in israel2 over an interval of
four hours (8 pm to midnight) using a poisson random variable. we plot the histogram of the
number of calls received during that interval for two months (september and october of 1999)
together with a poisson pmf    tted to the data (we will learn how to    t distributions to data
later on in the course). despite the fact that our assumptions do not hold exactly, the model

2the data is available here.

01020304005  10   20.10.15k01020304005  10   20.10.15k01020304005  10   20.10.15k01020304005  10   20.10.15kchapter 2. random variables

17

figure 2.6: in blue, we see the histogram of the number of calls received during an interval of four
hours over two months at a call center in israel. a poisson pmf approximating the distribution of the
data is plotted in orange.

produces a reasonably good    t.

2.3 continuous random variables

physical quantities are often best described as continuous: temperature, duration, speed, weight,
etc. in order to model such quantities probabilistically we could discretize their domain and
represent them as discrete random variables. however, we may not want our conclusions to
depend on how we choose the discretization grid. constructing a continuous model allows us to
obtain insights that are valid for su   ciently    ne grids without worrying about discretization.

precisely because continuous domains model the limit when discrete outcomes have an arbitrar-
ily    ne granularity, we cannot characterize the probabilistic behavior of a continuous random
variable by just setting values for the id203 of x being equal to individual outcomes, as we
do for discrete random variables. in fact, we cannot assign nonzero probabilities to speci   c out-
comes of an uncertain continuous quantity. this would result in uncountable disjoint outcomes
with nonzero id203. the sum of an uncountable number of positive values is in   nite, so
the id203 of their union would be greater than one, which does not make sense.

more rigorously, it turns out that we cannot de   ne a valid id203 measure on the power set
of r (justifying this requires measure theory and is beyond the scope of these notes). instead,
we consider events that are composed of unions of intervals of r. such events form a   -algebra
called the borel   -algebra. this   -algebra is granular enough to represent any set that you
might be interested in (try thinking of a set that cannot be expressed as a countable union of
intervals), while allowing for valid id203 measures to be de   ned on it.

0510152025303540number of calls0.000.020.040.060.080.100.120.14real datapoisson distributionchapter 2. random variables

18

2.3.1 cumulative distribution function

to specify a random variable on the borel   -algebra it su   ces to determine the id203 of

the random variable belonging to all intervals of the form (      , x) for any x     r.
de   nition 2.3.1 (cumulative distribution function). let (   ,f, p) be a id203 space and
x :         r a random variable. the cumulative distribution function (cdf ) of x is de   ned as

fx (x) := p (x     x) .

(2.30)

in words, fx (x) is the id203 of x being smaller than x.

note that the cumulative distribution function can be de   ned for both continuous and discrete
random variables.

the following lemma describes some basic properties of the cdf. you can    nd the proof in
section 2.7.2.

lemma 2.3.2 (properties of the cdf). for any continuous random variable x

fx (x) = 0,

lim
x         
lim
fx (x) = 1,
x      
fx (b)     fx (a)

if b > a,

i.e. fx is nondecreasing.

(2.31)

(2.32)

(2.33)

to see why the cdf completely determines a random variable recall that we are only considering
sets that can be expressed as unions of intervals. the id203 of a random variable x
belonging to an interval (a, b] is given by

p (a < x     b) = p (x     b)     p (x     a)

= fx (b)     fx (a) .

(2.34)

(2.35)

remark 2.3.3. since individual points have zero id203, for any continuous random vari-
able x

p (a < x     b) = p (a     x     b) = p (a < x < b) = p (a     x < b) .

(2.36)

now, to    nd the id203 of x belonging to any particular set, we only need to decompose it
into disjoint intervals and apply (2.35), as illustrated by the following example.

example 2.3.4 (continuous random variable). consider a continuous random variable x with
a cdf given by

fx (x) :=

for x < 0,
for 0     x     1,
for 1     x     2,

0.5(cid:16)1 + (x     2)2(cid:17) for 2     x     3,

for x > 3.

1

0
0.5 x
0.5

                                                   

(2.37)

chapter 2. random variables

19

figure 2.7: cumulative distribution function of the random variable in examples 2.3.4 and 2.3.7.

figure 2.7 shows the cdf on the left image. you can check that it satis   es the properties in
lemma 2.3.2. to determine the id203 that x is between 0.5 and 2.5, we apply (2.35),

p (0.5 < x     2.5) = fx (2.5)     fx (0.5) = 0.375,

as illustrated in figure 2.7.

(2.38)

(cid:52)

2.3.2 id203 density function

if the cdf of a continuous random variable is di   erentiable, its derivative can be interpreted as
a density function. this density can then be integrated to obtain the id203 of the random
variable belonging to an interval or a union of intervals (and hence to any borel set).

de   nition 2.3.5 (id203 density function). let x :         r be a random variable with cdf

fx . if fx is di   erentiable then the id203 density function or pdf of x is de   ned as

fx (x) :=

dfx (x)

d x

.

(2.39)

intuitively, fx (x)     is the id203 of x belonging to an interval of width     around x as
        0. by the fundamental theorem of calculus, the id203 of a random variable x
belonging to an interval is given by

p (a < x     b) = fx (b)     fx (a)

=(cid:90) b

a

fx (x) dx.

(2.40)

(2.41)

our sets of interest belong the borel   -algebra, and hence can be decomposed into unions of
intervals, so we can obtain the id203 of x belonging to any such set s by integrating its
pdf over s

p (x     s) =(cid:90)s

fx (x) dx.

(2.42)

   10123400.510.52.5p(x   (0.5,2.5])xfx(x)chapter 2. random variables

20

figure 2.8: id203 density function of the random variable in examples 2.3.4 and 2.3.7.

in particular, since x belongs to r by de   nition

it follows from the monotonicity of the cdf (2.33) that the pdf is nonnegative

(cid:90)    

      

fx (x) dx = p (x     r) = 1.

fx (x)     0,

(2.43)

(2.44)

since otherwise we would be able to    nd two points x1 < x2 for which fx (x2) < fx (x1).

remark 2.3.6 (the pdf is not a id203 measure). the pdf is a density which must be
integrated to yield a id203. in particular, it is not necessarily smaller than one (for example,
take a = 0 and b = 1/2 in de   nition 2.3.8 below).

finally, just as in the case of discrete random variables, we often say that a random variable is
distributed according to a certain pdf or cdf, or that we know its distribution. the reason is
that the pmf, pdf or cdf su   ce to characterize the underlying id203 space.

example 2.3.7 (continuous random variable (continued)). to compute the pdf of the random
variable in example 2.3.4 we di   erentiate its cdf, to obtain

fx (x) =

0
0.5
0
x     2
0

for x < 0,
for 0     x     1
for 1     x     2
for 2     x     3
for x > 3.

(2.45)

                                             

figure 2.8 shows the pdf. you can check that it integrates to one. to determine the id203
that x is between 0.5 and 2.5, we can just integrate over that interval to obtain the same answer
as in example 2.3.4,

p (0.5 < x     2.5) =(cid:90)0.5
=(cid:90) 1

0.5

fx (x) dx

0.5 dx +(cid:90) 2.5

2

x     2 dx = 0.375.

(2.46)

(2.47)

   0.500.511.522.533.500.51xfx(x)p(x   (0.5,2.5])chapter 2. random variables

21

figure 2.9: id203 density function (left) and cumulative distribution function (right) of a uniform
random variable x.

figure 2.8 illustrates that the id203 of an event is equal to the area under the pdf once we
restrict it to the corresponding subset of the real line.

(cid:52)

2.3.3

important continuous random variables

in this section we describe several continuous random variables that are useful in probabilistic
modeling and statistics.

uniform

a uniform random variable models an experiment in which every outcome within a continuous
interval is equally likely. as a result the pdf is constant over the interval. figure 2.9 shows the
pdf and cdf of a uniform random variable.

de   nition 2.3.8 (uniform). the pdf of a uniform random variable with domain [a, b], where
b > a are real numbers, is given by

fx (x) =(cid:40) 1

b   a ,
0,

if a     x     b,
otherwise.

(2.48)

exponential

exponential random variables are often used to model the time that passes until a certain event
occurs. examples include decaying radioactive particles, telephone calls, earthquakes and many
others.

de   nition 2.3.9 (exponential). the pdf of an exponential random variable with parameter   
is given by

fx (x) =(cid:40)  e     x,

0,

if x     0,
otherwise.

(2.49)

ab01b   axfx(x)ab01xfx(x)chapter 2. random variables

22

figure 2.10: id203 density functions of exponential random variables with di   erent parameters.

figure 2.10 shows the pdf of three exponential random variables with di   erent parameters. in
order to illustrate that the potential of exponential distributions for modeling real data, in
figure 2.11 we plot the histogram of inter-arrival times of calls at the same call center in israel
we mentioned earlier. in more detail, these inter-arrival times are the times between consecutive
calls occurring between 8 pm and midnight over two days in september 1999. an exponential
model    ts the data quite well.

an important property of an exponential random variable is that it is memoryless. we elaborate
on this property, which is shared by the geometric distribution, in section 2.4.

gaussian or normal

the gaussian or normal random variable is arguably the most popular random variable in all
of id203 and statistics. it is often used to model variables with unknown distributions in
the natural sciences. this is motivated by the fact that sums of independent random variables
often converge to gaussian distributions. this phenomenon is captured by the central limit
theorem, which we discuss in chapter 6.

de   nition 2.3.10 (gaussian). the pdf of a gaussian or normal random variable with mean   
and standard deviation    is given by

fx (x) =

e    (x     )2

2  2

.

1

   2    

(2.50)

a gaussian distribution with mean    and standard deviation    is usually denoted by n(cid:0)  ,   2(cid:1).

we provide formal de   nitions of the mean and the standard deviation of a random variable in
chapter 4. for now, you can just think of them as quantities that parametrize the gaussian
pdf.

it is not immediately obvious that the pdf of the gaussian integrates to one. we establish this
in the following lemma.

lemma 2.3.11 (proof in section 2.7.3). the pdf of a gaussian random variable integrates to
one.

012345600.511.5xfx(x)  =0.5  =1.0  =1.5chapter 2. random variables

23

figure 2.11: histogram of inter-arrival times of calls at a call center in israel (red) compared to its
approximation by an exponential pdf.

figure 2.12: gaussian random variable with di   erent means and standard deviations.

0123456789interarrival times (s)0.00.10.20.30.40.50.60.70.80.9exponential distributionreal data   10   8   6   4   2024681000.20.4xfx(x)  =2  =1  =0  =2  =0  =4chapter 2. random variables

24

figure 2.13: histogram of heights in a population of 25,000 people (blue) and its approximation using
a gaussian distribution (orange).

figure 2.12 shows the pdfs of two gaussian random variables with di   erent values of    and   .
figure 2.13 shows the histogram of the heights in a population of 25,000 people and how it is
very well approximated by a gaussian random variable3.

an annoying feature of the gaussian random variable is that its cdf does not have a closed form
solution, in contrast to the uniform and exponential random variables. this complicates the
task of determining the id203 that a gaussian random variable is in a certain interval. to
tackle this problem we use the fact that if x is a gaussian random variable with mean    and
standard deviation   , then

u :=

x       

  

(2.51)

is a standard gaussian random variable, which means that its mean is zero and its standard
deviation equals one. see lemma 2.5.1 for the proof. this allows us to express the id203
of x being in an interval [a, b] in terms of the cdf of a standard gaussian, which we denote by
  ,

p (x     [a, b]) = p(cid:18) x       
=   (cid:18) b       

   (cid:20) a       
   (cid:21)(cid:19)
   (cid:19)       (cid:18) a       
   (cid:19) .

b       

  

  

,

(2.52)

(2.53)

as long as we can evaluate   , this formula allows us to deal with arbitrary gaussian random
variables. to evaluate    people used to resort to lists of tabulated values, compiled by computing
the corresponding integrals numerically. nowadays you can just use matlab, wolframalpha,
scipy, etc.

3the data is available available here.

606264666870727476height (inches)0.050.100.150.200.25gaussian distributionreal datachapter 2. random variables

25

figure 2.14: pdfs of beta random variables with di   erent values of the a and b parameters.

beta

beta distributions allow us to parametrize unimodal continuous distributions supported on the
unit interval. this is useful in bayesian statistics, as we discuss in chapter 10.

de   nition 2.3.12 (beta distribution). the pdf of a beta distribution with parameters a and b
is de   ned as

f   (  ; a, b) :=(cid:40)   a   1(1     )b   1

  (a,b)

0

,

if 0            1,
otherwise,

where

   (a, b) :=(cid:90)u

ua   1 (1     u)b   1 du.

(2.54)

(2.55)

   (a, b) is a special function called the beta function or euler integral of the    rst kind, which
must be computed numerically. the uniform distribution is an example of a beta distribution
(where a = 1 and b = 1). figure 2.14 shows the pdf of several di   erent beta distributions.

2.4 conditioning on an event

in section 1.2 we explain how to modify the id203 measure of a id203 space to
incorporate the assumption that a certain event has occurred. in this section, we review this
situation when random variables are involved. in particular, we consider a random variable x
with a certain distribution represented by a pmf, cdf or pdf and explain how its distribution
changes if we assume that x     s, for any set s belonging to the borel   -algebra (remember
that this includes essentially any useful set you can think of).

00.20.40.60.810246xfx(x)a=1b=1a=1b=2a=3b=3a=6b=2a=3b=15chapter 2. random variables

if x is discrete with pmf px , the conditional pmf of x given x     s is

px|x   s (x) := p (x = x|x     s)

(cid:80)

=(cid:40) px (x)

0

s   s px (s)

otherwise.

if x     s

26

(2.56)

(2.57)

this is a valid pmf in the new id203 space restricted to the event {x     s}.
similarly if x is continuous with pdf fx , the conditional cdf of x given the event x     s is

fx|x   s (x) := p (x     x|x     s)
p (x     x, x     s)

=

p (x     s)

fx (u) du

fx (u) du

= (cid:82)u   x,u   s
(cid:82)u   s

(2.58)

(2.59)

(2.60)

,

again by the de   nition of id155. one can check that this is a valid cdf in the
new id203 space. to obtain the conditional pdf we just di   erentiate this cdf,

fx|x   s (x) :=

dfx|x   s (x)

dx

.

(2.61)

we now apply this ideas to show that the geometric and exponential random variables are
memoryless.

example 2.4.1 (geometric random variables are memoryless). we    ip a coin repeatedly until
we obtain heads, but pause after a couple of    ips (which were tails). let us assume that the
   ips are independent and have the same bias p (i.e. the id203 of obtaining heads in every
   ip is p). what is the id203 of obtaining heads in k more    ips? perhaps surprisingly, it is
exactly the same as the id203 of obtaining a heads after k    ips from the beginning.

to establish this rigorously we compute the conditional pmf of a geometric random variable x
conditioned on the event {x > k0} (i.e. the    rst k0 were tails in our example). applying (2.56)
we have

if k > k0 and zero otherwise. we have used the fact that the geometric series

px|x>k0 (k) =

px (k)

=

(cid:80)   m=k0+1 px (m)
(1     p)k   1 p
(cid:80)   m=k0+1 (1     p)m   1 p
= (1     p)k   k0   1 p
   (cid:88)m=k0+1

  k0
1       

  m =

(2.62)

(2.63)

(2.64)

(2.65)

for any    < 1.

in the new id203 space where the count starts at k0 + 1 the conditional pmf is that of a
geometric random variable with the same parameter as the original one. the    rst k0    ips don   t
a   ect the future, once it is revealed that they were tails.

(cid:52)

chapter 2. random variables

27

example 2.4.2 (exponential random variables are memoryless). let us assume that the inter-
arrival times of your emails follow an exponential distribution (over intervals of several hours
this is probably a good approximation, let us know if you check). you receive an email. the
time until you receive your next email is exponentially distributed with a certain parameter   .
no email arrives in the next t0 minutes. surprisingly, the time from then until you receive your
next email is again exponentially distributed with the same parameter, no matter the value of
t0. just like geometric random variables, exponential random variables are memoryless.
let us prove this rigorously. we compute the conditional cdf of an exponential random variable t
with parameter    conditioned on the event {t > t0}    for an arbitrary t0 > 0    by applying (2.60)

t0

ft|t >t0 (t) = (cid:82) t
ft (u) du
(cid:82)    t0
ft (u) du
e     t     e     t0
=
= 1     e     (t   t0).

   e     t0

(2.66)

(2.67)

(2.68)
di   erentiating with respect to t yields an exponential pdf ft|t >t0 (t) =   e     (t   t0) starting at t0.
(cid:52)

2.5 functions of random variables

computing the distribution of a function of a random variable is often very useful in probabilistic
modeling. for example, if we model the current in a circuit using a random variable x, we might
be interested in the power y := rx 2 dissipated across a resistor with deterministic resistance
if we apply a deterministic function g : r     r to a random variable x, then the result
r.
y := g (x) is not a deterministic quantity. recall that random variables are functions from a
sample space     to r. if x maps elements of     to r, then so does y since y (  ) = g (x (  )).
this means that y is also a random variable. in this section we explain how to characterize the
distribution of y when the distribution of x is known.

if x is discrete, then it is straightforward to compute the pmf of g (x) from the pmf of x,

py (y) = p (y = y)

= p (g (x) = y)

= (cid:88){x | g(x)=y}

px (x) .

(2.69)

(2.70)

(2.71)

if x is continuous, the procedure is more subtle. we    rst compute the cdf of y by applying the
de   nition,

fy (y) = p (y     y)

= p (g (x)     y)

=(cid:90){x | g(x)   y}

fx (x) dx,

(2.72)

(2.73)

(2.74)

where the last equality obviously only holds if x has a pdf. we can then obtain the pdf of y
from its cdf if it is di   erentiable. this idea can be used to prove a useful result about gaussian
random variables.

chapter 2. random variables

28

lemma 2.5.1 (gaussian random variable). if x is a gaussian random variable with mean   
and standard deviation   , then

u :=

x       

  

is a standard gaussian random variable.

proof. we apply (2.74) to obtain

       u(cid:19)

fu (u) = p(cid:18) x       
=(cid:90)(x     )/     u
=(cid:90) u

1
   2  

      

1

   2    
e    w2

2 dw

e    (x     )2

2  2 dx

by the change of variables w =

x       

  

.

di   erentiating with respect to u yields

1
   2  
so u is indeed a standard gaussian random variable.

fu (u) =

e    u2
2 ,

(2.75)

(2.76)

(2.77)

(2.78)

(2.79)

2.6 generating random variables

simulation is a fundamental tool in probabilistic modeling. simulating the outcome of a model
requires sampling from the random variables included in it. the most widespread strategy for
generating samples from a random variable decouples the process into two steps:

1. generating samples uniformly from the unit interval [0, 1].

2. transforming the uniform samples so that they have the desired distribution.

here we focus on the second step, assuming that we have access to a random-number generator
that produces independent samples following a uniform distribution in [0, 1]. the construction
of good uniform random generators is an important problem, which is beyond the scope of these
notes.

2.6.1 sampling from a discrete distribution

let x be a discrete random variable with pmf px and u a uniform random variable in [0, 1].
our aim is to transform a sample from u so that it is distributed according to px . we denote
the values that have nonzero id203 under px by x1, x2, . . .
for a    xed i, assume that we assign all samples of u within an interval of length px (xi)
to xi. then the id203 that a given sample from u is assigned to xi is exactly px (xi)!

chapter 2. random variables

29

figure 2.15:
illustration of the method to generate samples from an arbitrary discrete distribution
described in section 2.6.1. the cdf of a discrete random variable is shown in blue. the samples u4 and
u2 from a uniform distribution are mapped to x1 and x3 respectively, whereas u1, u3 and u5 are mapped
to x3.

very conveniently, the unit interval can be partitioned into intervals of length px (xi). we can
consequently generate x by sampling from u and setting

x =

x1
x2
. . .
xi
. . .

                                             

if 0     u     px (x1) ,
if px (x1)     u     px (x1) + px (x2) ,

if (cid:80)i   1

j=1 px (xj)     u    (cid:80)i

j=1 px (xj) ,

recall that the cdf of a discrete random variable equals

fx (x) = p (x     x)

px (xi) ,

= (cid:88)xi   x

(2.80)

(2.81)

(2.82)

so our algorithm boils down to obtaining a sample u from u and then outputting the xi such
that fx (xi   1)     u     fx (xi). this is illustrated in figure 2.15.

2.6.2

inverse-transform sampling

inverse-transform sampling makes it possible to sample from an arbitrary distribution with a
known cdf by applying a deterministic transformation to uniform samples. intuitively, we can
interpret it as a generalization of the method in section 2.6.1 to continuous distributions.

algorithm 2.6.1 (inverse-transform sampling). let x be a continuous random variable with
cdf fx and u a random variable that is uniformly distributed in [0, 1] and independent of x.

1. obtain a sample u of u .

2. set x := f    1

x (u).

0fx(x1)fx(x2)1u4u1u5u3u2x1x2x3chapter 2. random variables

30

figure 2.16: samples from an exponential distribution with parameter    = 1 obtained by inverse-
transform sampling as described in example 2.6.4. the samples u1, . . . , u5 are generated from a uniform
distribution.

the careful reader will point out that fx may not be invertible at every point. to avoid this
problem we de   ne the generalized inverse of the cdf as

f    1

x (u) := min

x {fx (x) = u} .

(2.83)

the function is well de   ned because all cdfs are non-increasing, so fx is equal to a constant c
in any interval [x1, x2] where it is not invertible.
we now prove that algorithm 2.6.1 works.

theorem 2.6.2 (inverse-transform sampling works). the distribution of y = f    1
same as the distribution of x.

x (u ) is the

proof. we just need to show that the cdf of y is equal to fx . we have

fy (y) = p (y     y)

x (u )     y(cid:1)

= p (u     fx (y))

= p(cid:0)f    1
=(cid:90) fx (y)

u=0

= fx (y) ,

du

(2.84)

(2.85)

(2.86)

(2.87)

(2.88)

where in step (2.86) we have to take into account that we are using the generalized inverse of
the cdf. this is resolved by the following lemma proved in section 2.7.4.

lemma 2.6.3. the events(cid:8)f    1

x (u )     y(cid:9) and {u     fx (y)} are equivalent.

0u2u5u4u3u11f   1x(u2)f   1x(u5)f   1x(u4)f   1x(u3)f   1x(u1)chapter 2. random variables

31

example 2.6.4 (sampling from an exponential distribution). let x be an exponential random
variable with parameter   . its cdf fx (x) := 1     e     x is invertible in [0,   ]. its inverse equals

f    1

x (u) =

1
  

log(cid:18) 1

1     u(cid:19) .

x (u ) is an exponential random variable with parameter    by theorem 2.6.2. figure 2.16

f    1
shows how the samples of u are transformed into samples of x.

2.7 proofs

2.7.1 proof of lemma 2.2.9

for any    xed constants c1 and c2

lim
n      

n     c1
n     c2

= 1,

so that

lim
n      

the result follows from the following basic calculus identity:

n     1
n              

n     k + 1
n       

= 1.

n!

n

(n     k)! (n       )k =
n      (cid:18)1    

n          
n(cid:19)n

lim

  

= e     .

2.7.2 proof of lemma 2.3.2

to establish (2.31)

lim
x         

fx (x) = 1     lim
x         

p (x > x)

= 1     p (x > 0) + lim
n      

p (   i     x >     (i + 1))

n(cid:88)i=0
i=0 {   i     x >     (i + 1)}(cid:17)
n      {x > 0}        n

= 1     p(cid:16) lim

= 1     p (   ) = 0.

the proof of (2.32) follows from this result. let y =    x, then

lim
x      

fx (x) = lim
x      

p (x     x)

p (x > x)

= 1     lim
x      
= 1     lim
x         
= 1     lim
x         
finally, (2.33) holds because {x     a}     {x     b}.

p (   x < x)
fy (x) = 1 by (2.32).

(2.89)

(cid:52)

(2.90)

(2.91)

(2.92)

(2.93)

(2.94)

(2.95)

(2.96)

(2.97)

(2.98)

(2.99)

(2.100)

chapter 2. random variables

32

2.7.3 proof of lemma 2.3.11

the result is a consequence of the following lemma.

lemma 2.7.1.

proof. let us de   ne

e   t2

dt =      .

(cid:90)    

      

i =(cid:90)    

      

e   x2

dx.

now taking the square and changing to polar coordinates,

dx(cid:90)    

e   y2

dy

      
e   (x2+y2)dxdy

re   (r2)d  dr

      

e   x2

i 2 =(cid:90)    
=(cid:90)    
x=      (cid:90)    
=(cid:90) 2  
  =0(cid:90)    

r=      

y=      

=   e   (r2)]   0 =   .

(2.101)

(2.102)

(2.103)

(2.104)

(2.105)

(2.106)

to complete the proof we use the change of variables t = (x       ) /   2  .

2.7.4 proof of lemma 2.6.3

x (u )     y(cid:9) implies {u     fx (y)}
(cid:8)f    1

assume that u > fx (y), then for all x, such that fx (x) = u , x > y because the cdf is nonde-
creasing. in particular minx {fx (x) = u} > y.

{u     fx (y)} implies(cid:8)f    1

x (u )     y(cid:9)

assume that minx {fx (x) = u} > y, then u > fx (y) because the cdf is nondecreasing. the
inequality is strict because u = fx (y) would imply that y belongs to {fx (x) = u}, which
cannot be the case as we are assuming that it is smaller than the minimum of that set.

chapter 3

multivariate random variables

probabilistic models usually include multiple uncertain numerical quantities. in this chapter we
describe how to specify random variables to represent such quantities and their interactions. in
some occasions, it will make sense to group these random variables as random vectors, which
we write using uppercase letters with an arrow on top: (cid:126)x. realizations of these random vectors
are denoted with lowercase letters: (cid:126)x.

3.1 discrete random variables

recall that discrete random variables are numerical quantities that take either    nite or countably
in   nite values. in this section we explain how to manipulate multiple discrete random variables
that share a common id203 space.

3.1.1 joint id203 mass function

if several discrete random variables are de   ned on the same id203 space, we specify their
probabilistic behavior through their joint id203 mass function, which is the id203
that each variable takes a particular value.
de   nition 3.1.1 (joint id203 mass function). let x :         rx and y :         ry be
discrete random variables (rx and ry are discrete sets) on the same id203 space (   ,f, p).
the joint pmf of x and y is de   ned as

px,y (x, y) := p (x = x, y = y)

.

(3.1)

in words, px,y (x, y) is the id203 of x and y being equal to x and y respectively.
similarly, the joint pmf of a discrete random vector of dimension n

(cid:126)x :=            

x1
x2
      
xn

            

(3.2)

with entries xi :         rxi (r1, . . . , rn are all discrete sets) belonging to the same id203
space is de   ned as

p (cid:126)x ((cid:126)x) := p (x1 = (cid:126)x1, x2 = (cid:126)x2, . . . , xn = (cid:126)xn) .

(3.3)

33

chapter 3. multivariate random variables

34

as in the case of the pmf of a single random variable, the joint pmf is a valid id203 measure
1 (or rx1    rx2          rxn
if we consider a id203 space where the sample space is rx    ry
in the case of a random vector) and the   -algebra is just the power set of the sample space. this
implies that the joint pmf completely characterizes the random variables or the random vector,
we don   t need to worry about the underlying id203 space.

by the de   nition of id203 measure, the joint pmf must be nonnegative and its sum over
all its possible arguments must equal one,

px,y (x, y)     0

for any x     rx , y     ry ,

px,y (x, y) = 1.

(cid:88)x   rx (cid:88)y   ry

(3.4)

(3.5)

by the law of total id203, the joint pmf allows us to obtain the id203 of x and y
belonging to any set s     rx    ry ,

p ((x, y )     s) = p(cid:0)   (x,y)   s {x = x, y = y}(cid:1)

p (x = x, y = y)

= (cid:88)(x,y)   s
= (cid:88)(x,y)   s

px,y (x, y) .

(union of disjoint events)

(3.6)

(3.7)

(3.8)

these properties also hold for random vectors (and groups of more than two random variables).
for any random vector (cid:126)x,

the id203 that (cid:126)x belongs to a discrete set s     rn is given by

p (cid:126)x ((cid:126)x)     0,

(cid:88)(cid:126)x1   r1 (cid:88)(cid:126)x2   r2
       (cid:88)(cid:126)xn   rn
p(cid:16) (cid:126)x     s(cid:17) =(cid:88)(cid:126)x   s

p (cid:126)x ((cid:126)x) = 1.

p (cid:126)x ((cid:126)x) .

(3.9)

(3.10)

(3.11)

3.1.2 marginalization

assume we have access to the joint pmf of several random variables in a certain id203
space, but we are only interested in the behavior of one of them. to compute the value of its
pmf for a particular value, we    x that value and sum over the remaining random variables.
indeed, by the law of total id203

px (x) = p (x = x)

= p (   y   ry {x = x, y = y})

= (cid:88)y   ry
= (cid:88)y   ry

p (x = x, y = y)

px,y (x, y) .

(union of disjoint events)

(3.12)

(3.13)

(3.14)

(3.15)

1this is the cartesian product of the two sets, de   ned in section a.2, which contains all possible pairs (x, y)
where x     rx and y     ry.

chapter 3. multivariate random variables

35

when the joint pmf involves more than two random variables the argument is exactly the same.
this is called marginalizing over the other random variables. in this context, the pmf of a
single random variable is called its marginal pmf. table 3.1 shows an example of a joint pmf
and the corresponding marginal pmfs.

if we are interested in computing the joint pmf of several entries in a random vector, instead
of just one, the marginalization process is essentially the same. the pmf is again obtained
by summing over the rest of the entries. let i     {1, 2, . . . , n} be a subset of m < n entries
of an n-dimensional random vector (cid:126)x and (cid:126)xi the corresponding random subvector. to com-
pute the joint pmf of (cid:126)xi we sum over all the entries that are not in i, which we denote by
{j1, j2, . . . , jn   m} := {1, 2, . . . , n} /i

p (cid:126)xi ((cid:126)xi) = (cid:88)(cid:126)xj1   rj1 (cid:88)(cid:126)xj2   rj2

       (cid:88)(cid:126)xjn   m   rjn   m

p (cid:126)x ((cid:126)x) .

(3.16)

3.1.3 conditional distributions

conditional probabilities allow us to update our uncertainty about the quantities in a probabilis-
tic model when new information is revealed. the conditional distribution of a random variable
speci   es the behavior of the random variable when we assume that other random variables in
the id203 space take a    xed value.

de   nition 3.1.2 (id155 mass function). the id155 mass
function of y given x, where x and y are discrete random variables de   ned on the same
id203 space, is given by

py |x (y|x) = p (y = y|x = x)

px,y (x, y)

px (x)

if px (x) > 0

and is unde   ned otherwise.

=

(3.17)

(3.18)

the conditional pmf px|y (  |y) characterizes our uncertainty about x conditioned on the event
{y = y}. this object is a valid pmf of x, so that if rx is the range of x

(cid:88)x   rx

px|y (x|y) = 1

(3.19)

for any y for which it is well de   ned. however, it is not a pmf for y . in particular, there is no

reason for(cid:80)y   ry

px|y (x|y) to add up to one!

we now de   ne the joint conditional pmf of several random variables (equivalently of a subvector
of a random vector) given other random variables (or entries of the random vector).
de   nition 3.1.3 (conditional pmf). the conditional pmf of a discrete random subvector (cid:126)xi,
i     {1, 2, . . . , n}, given another subvector (cid:126)xj is

p (cid:126)xi| (cid:126)xj ((cid:126)xi|(cid:126)xj ) :=

p (cid:126)x ((cid:126)x)
p (cid:126)xj ((cid:126)xj )

,

(3.20)

where {j1, j2, . . . , jn   m} := {1, 2, . . . , n} /i.

chapter 3. multivariate random variables

36

pl

15
20

5
20

pl|r (  |0)

pl|r (  |1)

7
8

1
8

1
4

3
4

pl,r

0

1

l

r

0

14
20

2
20

1

1
20

3
20

pr

16
20

4
20

pr|l (  |0)

14
15

1
15

pr|l (  |1)

2
5

3
5

table 3.1: joint, marginal and conditional pmfs of the random variables l and r de   ned in exam-
ple 3.1.5.

the conditional pmfs py |x (  |x) and p (cid:126)xi| (cid:126)xj (  |(cid:126)xj ) are valid pmfs in the id203 space where
x = x or (cid:126)xj = (cid:126)xj respectively. for instance, they must be nonnegative and add up to one.
from the de   nition of conditional pmfs we derive a chain rule for discrete random variables and
vectors.

lemma 3.1.4 (chain rule for discrete random variables and vectors).

px,y (x, y) = px (x) py |x (y|x) ,

p (cid:126)x ((cid:126)x) = px1 ((cid:126)x1) px2|x1 ((cid:126)x2|(cid:126)x1) . . . pxn|x1,...,xn   1 ((cid:126)xn|(cid:126)x1, . . . , (cid:126)xn   1)

=

n(cid:89)i=1

pxi| (cid:126)x{1,...,i   1}(cid:0)(cid:126)xi|(cid:126)x{1,...,i   1}(cid:1) ,

where the order of indices in the random vector is arbitrary (any order works).

(3.21)

(3.22)

(3.23)

the following example illustrates the de   nitions of marginal and conditional pmfs.

example 3.1.5 (flights and rains (continued)). within the id203 space described in
example 1.2.1 we de   ne a random variable

l =(cid:40)1

0

if plane is late,
otherwise,

to represent whether the plane is late or not. similarly,

r =(cid:40)1

0

it rains,
otherwise,

(3.24)

(3.25)

chapter 3. multivariate random variables

37

represents whether it rains or not. equivalently, these random variables are just the indicators
r = 1rain and l = 1late. table 3.1 shows the joint, marginal and conditional pmfs of l and r.
(cid:52)

3.2 continuous random variables

continuous random variables allow us to model continuous quantities without having to worry
about discretization. in exchange, the mathematical tools to manipulate them are somewhat
more complicated than in the discrete case.

3.2.1 joint cdf and joint pdf

as in the case of univariate continuous random variables, we characterize the behavior of sev-
eral continuous random variables de   ned on the same id203 space through the id203
that they belong to borel sets (or equivalently unions of intervals). in this case we are con-
sidering multidimensional borel sets, which are cartesian products of one-dimensional borel
sets. multidimensional borel sets can be represented as unions of multidimensional intervals
or hyperrectangles (de   ned as cartesian products of one-dimensional intervals). the joint cdf
compiles the id203 that the random variables belong to the cartesian product of intervals

of the form (      , r] for every r     r.
de   nition 3.2.1 (joint cumulative distribution function). let (   ,f, p) be a id203 space
and x, y :         r random variables. the joint cdf of x and y is de   ned as

fx,y (x, y) := p (x     x, y     y) .

(3.26)

in words, fx,y (x, y) is the id203 of x and y being smaller than x and y respectively.

let (cid:126)x :         rn be a random vector of dimension n on a id203 space (   ,f, p). the joint
cdf of (cid:126)x is de   ned as

f (cid:126)x ((cid:126)x) := p(cid:16) (cid:126)x1     (cid:126)x1, (cid:126)x2     (cid:126)x2, . . . , (cid:126)xn     (cid:126)xn(cid:17) .
in words, f (cid:126)x ((cid:126)x) is the id203 that (cid:126)xi     (cid:126)xi for all i = 1, 2, . . . , n.
we now record some properties of the joint cdf.

lemma 3.2.2 (properties of the joint cdf).

fx,y (x, y) = 0,

fx,y (x, y) = 0,

lim
x         
lim
y         
lim

fx,y (x, y) = 1,

x      ,y      
fx,y (x1, y1)     fx,y (x2, y2)

if x2     x1, y2     y1,

i.e. fx,y is nondecreasing.

proof. the proof follows along the same lines as that of lemma 2.3.2.

(3.27)

(3.28)

(3.29)

(3.30)

(3.31)

chapter 3. multivariate random variables

38

the joint cdf completely speci   es the behavior of the corresponding random variables. indeed,
we can decompose any borel set into a union of disjoint n-dimensional intervals and compute
their id203 by evaluating the joint cdf. let us illustrate this for the bivariate case:

p (x1     x     x2, y1     y     y2) = p ({x     x2, y     y2}     {x > x1}     {y > y1})

= p (x     x2, y     y2)     p (x     x1, y     y2)

    p (x     x2, y     y1) + p (x     x1, y     y1)

(3.34)
= fx,y (x2, y2)     fx,y (x1, y2)     fx,y (x2, y1) + fx,y (x1, y1) .
this means that, as in the univariate case, to de   ne a random vector or a group of random
variables all we need to do is de   ne their joint cdf. we don   t have to worry about the underlying
id203 space.

(3.32)

(3.33)

if the joint cdf is di   erentiable, we can di   erentiate it to obtain the joint id203 density
function of x and y . as in the case of univariate random variables, this is often a more
convenient way of specifying the joint distribution.

de   nition 3.2.3 (joint id203 density function). if the joint cdf of two random variables
x, y is di   erentiable, then their joint pdf is de   ned as

fx,y (x, y) :=

   2fx,y (x, y)

   x   y

.

if the joint cdf of a random vector (cid:126)x is di   erentiable, then its joint pdf is de   ned as

f (cid:126)x ((cid:126)x) :=

   nf (cid:126)x ((cid:126)x)

   (cid:126)x1    (cid:126)x2           (cid:126)xn

.

(3.35)

(3.36)

the joint pdf should be understood as an n-dimensional density, not as a id203 (for
instance, it can be larger than one). in the two-dimensional case,

lim

   x   0,   y   0

p (x     x     x +    x, y     y     y +    y) = fx,y (x, y)    x   y.

(3.37)

due to the monotonicity of joint cdfs in every variable, joint pmfs are always nonnegative.

the joint pdf of x and y allows us to compute the id203 of any borel set s     r2 by
integrating over s

fx,y (x, y) dx dy.

(3.38)

p ((x, y )     s) =(cid:90)(x,y)   s

p(cid:16) (cid:126)x     s(cid:17) =(cid:90)(cid:126)x   s

similarly, the joint pdf of an n-dimensional random vector (cid:126)x allows to compute the id203
that (cid:126)x belongs to a set borel set s     rn,

in particular, if we integrate a joint pdf over the whole space rn, then it must integrate to one
by the law of total id203.

f (cid:126)x ((cid:126)x) d(cid:126)x.

(3.39)

chapter 3. multivariate random variables

39

figure 3.1: triangle lake in example 3.2.12.

example 3.2.4 (triangle lake). a biologist is tracking an otter that lives in a lake. she decides
to model the location of the otter probabilistically. the lake happens to be triangular as shown
in figure 3.1, so that we can represent it by the set

lake := {(cid:126)x | (cid:126)x1     0, (cid:126)x2     0, (cid:126)x1 + (cid:126)x2     1} .

(3.40)

the biologist has no idea where the otter is, so she models the position as a random vector (cid:126)x
which is uniformly distributed over the lake. in other words, the joint pdf of (cid:126)x is constant,

f (cid:126)x ((cid:126)x) =(cid:40)c

0

if (cid:126)x     lake,
otherwise.

(3.41)

to    nd the normalizing constant c we use the fact that to be a valid joint pdf f (cid:126)x should integrate
to 1.

(cid:90)    
x1=      (cid:90)    

x2=      

c dx1 dx2 =(cid:90) 1
x2=0(cid:90) 1   x2
= c(cid:90) 1

x1=0

x2=0

=

= 1,

c
2

c dx1 dx2

(1     x2) dx2

(3.42)

(3.43)

(3.44)

so c = 2.
we now compute the cdf of (cid:126)x. f (cid:126)x ((cid:126)x) represents the id203 that the otter is southwest of
the point (cid:126)x. computing the joint cdf requires dividing the range into the sets shown in figure 3.1

and integrating the joint pdf. if (cid:126)x     a then f (cid:126)x ((cid:126)x) = 0 because p(cid:16) (cid:126)x     (cid:126)x(cid:17) = 0. if ((cid:126)x)     b,

f (cid:126)x ((cid:126)x) =(cid:90) (cid:126)x2

u=0(cid:90) (cid:126)x1

v=0

2 dv du = 2(cid:126)x1(cid:126)x2.

(3.45)

   0.500.511.5   0.500.511.5abcdefchapter 3. multivariate random variables

40

if (cid:126)x     c,

v=0

v=0

if (cid:126)x     d,

u=0 (cid:90) (cid:126)x1

u=1   (cid:126)x1(cid:90) 1   u

2 dv du = 2(cid:126)x1 + 2(cid:126)x2     (cid:126)x2

2 dv du +(cid:90) (cid:126)x2

f (cid:126)x ((cid:126)x) =(cid:90) 1   (cid:126)x1
f (cid:126)x ((cid:126)x) = p(cid:16) (cid:126)x1     (cid:126)x1, (cid:126)x2     (cid:126)x2(cid:17) = p(cid:16) (cid:126)x1     1, (cid:126)x2     (cid:126)x2(cid:17) = f (cid:126)x (1, (cid:126)x2) = 2(cid:126)x2     (cid:126)x2
where the last step follows from (3.46). exchanging (cid:126)x1 and (cid:126)x2, we obtain f (cid:126)x ((cid:126)x) = 2(cid:126)x1     (cid:126)x2
(cid:126)x     e by the same reasoning. finally, for (cid:126)x     f f (cid:126)x ((cid:126)x) = 1 because p(cid:16) (cid:126)x1     x1, (cid:126)x2     x2(cid:17) = 1.

putting everything together,

2     (cid:126)x2

1     1.

(3.46)

(3.47)

1 for

2,

f (cid:126)x ((cid:126)x) =

2     (cid:126)x2

1     1,

0
2(cid:126)x1(cid:126)x2,
2(cid:126)x1 + 2(cid:126)x2     (cid:126)x2
2(cid:126)x2     (cid:126)x2
2,
2(cid:126)x1     (cid:126)x2
1,
1,

                                                               

if (cid:126)x1 < 0 or (cid:126)x2 < 0,
if (cid:126)x1     0, (cid:126)x2     0, (cid:126)x1 + (cid:126)x2     1,
if (cid:126)x1     1, (cid:126)x2     1, (cid:126)x1 + (cid:126)x2     1,
if (cid:126)x1     1, 0     (cid:126)x2     1,
if 0     (cid:126)x1     1, (cid:126)x2     1,
if (cid:126)x1     1, (cid:126)x2     1.

(3.48)

(cid:52)

3.2.2 marginalization

we now discuss how to characterize the marginal distributions of individual random variables
from a joint cdf or a joint pdf. consider the joint cdf fx,y (x, y). when x         the limit
of fx,y (x, y) is by de   nition the id203 of y being smaller than y, which is precisely the
marginal cdf of y . more formally,

lim
x      

fx,y (x, y) = lim
n      

i=1 {x     i, y     y})

p (   n
n      {x     n, y     y}(cid:17)
= p(cid:16) lim

= p (y     y)
= fy (y) .

if the random variables have a joint pdf, we can also compute the marginal cdf by integrating
over x

di   erentiating the latter equation with respect to y, we obtain the marginal pdf of x

fy (y) = p (y     y)

x=      

=(cid:90) y
u=      (cid:90)    
fx (x) =(cid:90)    

y=      

fx,y (x, u) dx dy.

fx,y (x, y) du.

(3.49)

(3.50)

(3.51)

(3.52)

(3.53)

(3.54)

(3.55)

chapter 3. multivariate random variables

41

similarly, the marginal pdf of a subvector (cid:126)xi of a random vector (cid:126)x indexed by i := {i1, i2, . . . , im}
is obtained by integrating over the rest of the components {j1, j2, . . . , jn   m} := {1, 2, . . . , n} /i,

f (cid:126)xi ((cid:126)xi) =(cid:90)(cid:126)xj1(cid:90)(cid:126)xj2

      (cid:90)(cid:126)xjn   m

f (cid:126)x ((cid:126)x) d(cid:126)xj1 d(cid:126)xj2        d(cid:126)xjn   m.

(3.56)

example 3.2.5 (triangle lake (continued)). the biologist is interested in the id203 that
the otter is south of x1. this information is encoded in the cdf of the random vector, we just
need to take the limit when x2         to marginalize over x2.
if x1 < 0,
if 0     x1     1,
if x1     1.

0
2x1     x2
1

(3.57)

1

fx1 (x1) =               

to obtain the marginal pdf of x1, which represents the latitude of the otter   s position, we
di   erentiate the marginal cdf

fx1 (x1) =

dfx1 (x1)

dx1

=(cid:40)2 (1     x1)

0,

if 0     x1     1,
otherwise.

.

(3.58)

alternatively, we could have integrated the joint uniform pdf over x2 (we encourage you to check
that the result is the same).

(cid:52)

3.2.3 conditional distributions

in this section we discuss how to obtain the conditional distribution of a random variable given
information about other random variables in the id203 space. to begin with, we consider
the case of two random variables. as in the case of univariate distributions, we can de   ne the
joint cdf and pdf of two random variables given events of the form {(x, y )     s} for any borel
set in r2 by applying the de   nition of id155.

de   nition 3.2.6 (joint conditional cdf and pdf given an event). let x, y be random variables

with joint pdf fx,y and let s     r2 be any borel set with nonzero id203, the conditional cdf
and pdf of x and y given the event (x, y )     s is de   ned as

fx,y |(x,y )   s (x, y) := p (x     x, y     y| (x, y )     s)
p (x     x, y     y, (x, y )     s)

=

p ((x, y )     s)

= (cid:82)u   x,v   y,(u,v)   s
(cid:82)(u,v)   s

   2fx,y |(x,y )   s (x, y)

.

   x   y

fx,y (u, v) du dv

fx,y (u, v) du dv

fx,y |(x,y )   s (x, y) :=

,

(3.59)

(3.60)

(3.61)

(3.62)

this de   nition only holds for events with nonzero id203. however, events of the form
{x = x} have id203 equal to zero because the random variable is continuous. indeed, the

chapter 3. multivariate random variables

42

range of x is uncountable, so the id203 of almost every event {x = x} must be zero, as
otherwise the id203 their union would be unbounded.

how can we characterize our uncertainty about y given x = x then? we de   ne a conditional
pdf that captures what we are trying to do in the limit and then integrate it to obtain a
conditional cdf.

de   nition 3.2.7 (conditional pdf and cdf). if fx,y is di   erentiable, then the conditional pdf
of y given x is de   ned as

fy |x (y|x) :=

fx,y (x, y)

fx (x)

if fx (x) > 0

(3.63)

and is unde   ned otherwise.

the conditional cdf of y given x is de   ned as

fy |x (y|x) :=(cid:90) y

u=      

fy |x (u|x) du

if fx (x) > 0

(3.64)

and is unde   ned otherwise.

we now justify this de   nition, beyond the analogy with (3.18). assume that fx (x) > 0. let us
write the de   nition of the conditional pdf in terms of limits. we have

fx (x) = lim
   x   0
fx,y (x, y) = lim
   x   0

p (x     x     x +    x)

   x

,

   p (x     x     x +    x, y     y)

   y

.

(3.65)

(3.66)

1
   x

1

this implies

fx,y (x, y)

fx (x)

=

lim

   x   0,   y   0

we can now write the conditional cdf as

p (x     x     x +    x)

   p (x     x     x +    x, y     y)

   y

.

(3.67)

fy |x (y|x) =(cid:90) y

u=      

1

lim

p (x     x     x +    x)

   x   0,   y   0

1

p (x     x     x +    x)(cid:90) y

u=      
p (x     x     x +    x, y     y)

p (x     x     x +    x)

p (y     y|x     x     x +    x) .

= lim
   x   0

= lim
   x   0
= lim
   x   0

   p (x     x     x +    x, y     u)

   y

du (3.68)

   p (x     x     x +    x, y     u)

   y

du

(3.69)

(3.70)

(3.71)

we can therefore interpret the conditional cdf as the limit of the cdf of y at y conditioned on
x belonging to an interval around x when the width of the interval tends to zero.

remark 3.2.8. interchanging limits and integrals as in (3.69) is not necessarily justi   ed in
general.
in this case it is, as long as the integral converges and the quantities involved are
bounded.

chapter 3. multivariate random variables

43

an immediate consequence of de   nition 3.2.7 is the chain rule for continuous random variables.

lemma 3.2.9 (chain rule for continuous random variables).

fx,y (x, y) = fx (x) fy |x (y|x) .

(3.72)

applying the same ideas as in the bivariate case, we de   ne the conditional distribution of a
subvector given the rest of the random vector.
de   nition 3.2.10 (conditional pdf). the conditional pdf of a random subvector (cid:126)xi, i    
{1, 2, . . . , n}, given the subvector (cid:126)x{1,...,n}/i is

f (cid:126)xi| (cid:126)x{1,...,n}/i(cid:0)(cid:126)xi|(cid:126)x{1,...,n}/i(cid:1) :=

f (cid:126)x ((cid:126)x)

f (cid:126)x{1,...,n}/i(cid:0)(cid:126)x{1,...,n}/i(cid:1) .

(3.73)

it is often useful to represent the joint pdf of a random vector by factoring it into conditional
pdfs using the chain rule for random vectors.

lemma 3.2.11 (chain rule for random vectors). the joint pdf of a random vector (cid:126)x can be
decomposed into

f (cid:126)x ((cid:126)x) = f (cid:126)x1

((cid:126)x1) f (cid:126)x2| (cid:126)x1

((cid:126)x2|(cid:126)x1) . . . f (cid:126)xn| (cid:126)x1,..., (cid:126)xn   1

=

n(cid:89)i=1

f (cid:126)xi| (cid:126)x{1,...,i   1}(cid:0)(cid:126)xi|(cid:126)x{1,...,i   1}(cid:1) .

((cid:126)xn|(cid:126)x1, . . . , (cid:126)xn   1)

(3.74)

(3.75)

note that the order is arbitrary, you can reorder the components of the vector in any way you
like.

proof. the result follows from applying the de   nition of conditional pdf recursively.

example 3.2.12 (triangle lake (continued)). the biologist spots the otter from the shore of
the lake. she is standing on the west side of the lake at a latitude of x1 = 0.75 looking east and
the otter is right in front of her. the otter is consequently also at a latitude of x1 = 0.75, but
she cannot tell at what distance. the distribution of the location of the otter given its latitude
x1 is characterized by the conditional pdf of the longitude x2 given x1,

the biologist is interested in the id203 that the otter is closer than x2 to her. this
id203 is given by the conditional cdf

fx2|x1 (x2|x1) =

=

fx1,x2 (x1, x2)

fx1 (x1)
1

,

0     x2     1     x1.

1     x1

fx2|x1 (x2|x1) =(cid:90) x2

fx2|x1 (u|x1) du
.

=

      
x2
1     x1

the id203 that the otter is less than x2 away is 4x2 for 0     x2     1/4.

(3.76)

(3.77)

(3.78)

(3.79)

(cid:52)

chapter 3. multivariate random variables

44

figure 3.2: joint pdf of a bivariate gaussian random variable (x, y ) together with the marginal pdfs
of x and y .

3.2.4 gaussian random vectors

gaussian random vectors are a multidimensional generalization of gaussian random variables.
they are parametrized by a vector and a matrix that correspond to their mean and covariance
matrix (we de   ne these quantities for general multivariate random variables in chapter 4).

de   nition 3.2.13 (gaussian random vector). a gaussian random vector (cid:126)x is a random vector
with joint pdf

(3.80)

f (cid:126)x ((cid:126)x) =

1

(cid:112)(2  )n |  |

exp(cid:18)   

1
2

((cid:126)x     (cid:126)  )t      1 ((cid:126)x     (cid:126)  )(cid:19)

where the mean vector (cid:126)       rn and the covariance matrix   , which is symmetric and positive
de   nite, parametrize the distribution. a gaussian distribution with mean (cid:126)   and covariance
matrix    is usually denoted by n ((cid:126)  ,   ).
a fundamental property of gaussian random vectors is that performing linear transformations
on them always yields vectors with joint distributions that are also gaussian. we will not prove
this result formally, but the proof is similar to lemma 2.5.1 (in fact this is a multidimensional
generalization of that result).

theorem 3.2.14 (linear transformations of gaussian random vectors are gaussian). let (cid:126)x
be a gaussian random vector of dimension n with mean (cid:126)   and covariance matrix   . for any
matrix a     rm  n and (cid:126)b     rm (cid:126)y = a (cid:126)x +(cid:126)b is a gaussian random vector with mean a(cid:126)   +(cid:126)b and
covariance matrix a  at .

   3   2   10123   20200.10.2xyfx,y(x,y)fy(y)fx(x)chapter 3. multivariate random variables

45

a corollary of this result is that the joint pdf of a subvector of a gaussian random vector is also
a gaussian vector.

corollary 3.2.15 (marginals of gaussian random vectors are gaussian). the joint pdf of any
subvector of a gaussian random vector is gaussian. without loss of generality, assume that the
subvector (cid:126)x consists of the    rst m entries of the gaussian random vector,

(cid:126)z :=(cid:34) (cid:126)x
(cid:126)y(cid:35) ,

with mean

(cid:126)   :=(cid:20)   (cid:126)x
  (cid:126)y(cid:21)

and covariance matrix

   (cid:126)z =(cid:20)    (cid:126)x    (cid:126)x (cid:126)y
  (cid:126)y (cid:21) .

  t

(cid:126)x (cid:126)y

then (cid:126)x is a gaussian random vector with mean    (cid:126)x and covariance matrix    (cid:126)x .

proof. note that

(cid:126)x =(cid:20)

im

0n   m  m 0n   m  n   m(cid:21)(cid:34) (cid:126)x

0m  n   m

(cid:126)y(cid:35) =(cid:20)

im

0n   m  m 0n   m  n   m(cid:21) (cid:126)z,

0m  n   m

(3.81)

(3.82)

(3.83)

where i     rm  m is an identity matrix and 0c  d represents a matrix of zeros of dimensions c   d.

the result then follows from theorem 3.2.14.

figure 3.2 shows the joint pdf of a bivariate gaussian random variable along with its marginal
pdfs.

3.3 joint distributions of discrete and continuous variables

probabilistic models often include both discrete and continuous random variables. however, the
joint pmf or pdf of a discrete and a continuous random variable is not well de   ned. in order to
specify the joint distribution in such cases we use their marginal and conditional pmfs and pdfs.

assume that we have a continuous random variable c and a discrete random variable d with
range rd. we de   ne the conditional cdf and pdf of c given d as follows.

de   nition 3.3.1 (conditional cdf and pdf of a continuous random variable given a discrete
random variable). let c and d be a continuous and a discrete random variable de   ned on the
same id203 space. then, the conditional cdf and pdf of c given d are of the form

fc|d (c|d) := p (c     c|d) ,
dfc|d (c|d)
fc|d (c|d) :=
.

dc

(3.84)

(3.85)

we obtain the marginal cdf and pdf of c from the conditional cdfs and pdfs by computing a
weighted sum.

chapter 3. multivariate random variables

46

figure 3.3: conditional and marginal distributions of the weight of the bears w in example 9.4.3.

lemma 3.3.2. let fc|d and fc|d be the conditional cdf and pdf of a continuous random variable
c given a discrete random variable d. then,

fc (c) = (cid:88)d   rd
fc (c) = (cid:88)d   rd

pd (d) fc|d (c|d) ,

pd (d) fc|d (c|d) .

(3.86)

(3.87)

proof. the events {d = d} are a partition of the whole id203 space (one of them must
happen and they are all disjoint), so

fc (c) = p (c     c)

= (cid:88)d   rd
= (cid:88)d   rd

p (d = d) p (c     c|d)

by the law of total id203

pd (d) fc|d (c|d) .

(3.88)

(3.89)

(3.90)

now, (3.87) follows by di   erentiating.

combining a discrete marginal pmf with a continuous conditional distribution allows us to de   ne
mixture models where the data is drawn from a continuous distribution whose parameters are
chosen from a discrete set. if a gaussian is used as the continuous distribution, this yields a
gaussian mixture model. fitting gaussian mixture models is a popular technique for id91
data.

example 3.3.3 (grizzlies in yellowstone). a scientist is gathering data on the bears in yel-
lowstone.
it turns out that the weight of the males is well modeled by a gaussian random
variable with mean 240 kg and standard variation 40 kg, whereas the weight of the females is
well modeled by a gaussian with mean 140 kg and standard deviation 20 kg. there are about
the same number of females and males.

050100150200250300350400012  10   2fw|s(  |0)fw|s(  |1)fw(  )chapter 3. multivariate random variables

47

the distribution of the weights of all the grizzlies can consequently be modeled by a gaussian
mixture that includes a continuous random variable w to represent the weight and a discrete
random variable s to represent the sex of the bears. s is bernoulli with parameter 1/2, w given
s = 0 (male) is n (240, 1600) and w given s = 1 (female) is n (140, 400). by (3.87) the pdf of
w is consequently of the form

fw (w) =

=

ps (s) fw|s (w|s)

1(cid:88)s=0
2   2         e    (w   240)2

40

3200

1

+

e    (w   140)2

800

20        .

figure 3.3 shows the conditional and marginal distributions of w .

(3.91)

(3.92)

(cid:52)
de   ning the conditional pmf of a discrete random variable d given a continuous random variable
c is challenging because the id203 of the event {c = c} is zero. we follow the same
approach as in de   nition 3.2.7 and de   ne the conditional pmf as a limit.

de   nition 3.3.4 (conditional pmf of a discrete random variable given a continuous random
variable). let c and d be a continuous and a discrete random variable de   ned on the same
id203 space. then, the conditional pmf of d given c is de   ned as

pd|c (d|c) := lim
      0

p (d = d, c     c     c +    )

p (c     c     c +    )

.

(3.93)

analogously to lemma 3.3.2, we obtain the marginal pmf of d from the conditional pmfs by
computing a weighted sum.
lemma 3.3.5. let pd|c be the conditional pmf of a discrete random variable d given a con-
tinuous random variable c. then,

pd (d) =(cid:90)    

c=      

   (cid:88)i=      

fc (c) pd|c (d|c) dc.

(3.94)

proof. we will not give a formal proof but rather an intuitive argument that can be made
rigorous. if we take a grid of values for c which are on a grid . . . , c   1, c0, c1, . . . of width    , then

pd (d) =

p (d = d, ci     c     ci +    )

(3.95)

by the law of total id203. taking the limit as         0 the sum becomes an integral and
we have

lim
      0

c=      

pd (d) =(cid:90)    
=(cid:90)    
=(cid:90)    

c=      

c=      
p(c   c   c+   )

.

   

since fc (c) = lim      0

p (d = d, c     c     c +    )

   

dc

p (c     c     c +    )

   

lim
      0
fc (c) pd|c (d|c) dc.

  

p (d = d, c     c     c +    )

p (c     c     c +    )

dc

(3.96)

(3.97)

(3.98)

chapter 3. multivariate random variables

48

combining continuous marginal distributions with discrete conditional distributions is particu-
larly useful in bayesian statistical models, as illustrated in the following example (see chapter 10
for more information). the continuous distribution is used to quantify our uncertainty about
the parameter of a discrete distribution.

example 3.3.6 (bayesian coin    ip). your uncle bets you ten dollars that a coin    ip will turn
out heads. you suspect that the coin is biased, but you are not sure to what extent. to model
this uncertainty you represent the bias as a continuous random variable b with the following
pdf:

fb (b) = 2b

for b     [0, 1] .

(3.99)

you can now compute the id203 that the coin lands on heads denoted by x using lemma 3.3.5.
conditioned on the bias b, the result of the coin    ip is bernoulli with parameter b.

fb (b) px|b (1|b) db

px (1) =(cid:90)    
=(cid:90) 1

b=0
2
3

.

=

b=      

2b2 db

(3.100)

(3.101)

(3.102)

according to your model the id203 that the coin lands heads is 2/3.

(cid:52)
the following lemma provides an analogue to the chain rule for jointly distributed continuous
and discrete random variables.

lemma 3.3.7 (chain rule for jointly distributed continuous and discrete random variables). let
c be a continuous random variable with conditional pdf fc|d and d a discrete random variable
with conditional pmf pd|c. then,

proof. applying the de   nitions,

pd (d) fc|d (c|d) = fc (c) pd|c (d|c) .

p (c     c     c +    |d = d)

pd (d) fc|d (c|d) = lim
      0
= lim
      0
= lim
      0

p (d = d)
   
p (d = d, c     c     c +    )
p (c     c     c +    )

   

   
= fc (c) pd|c (d|c) .

  

p (d = d, c     c     c +    )

p (c     c     c +    )

(3.103)

(3.104)

(3.105)

(3.106)

(3.107)

example 3.3.8 (grizzlies in yellowstone (continued)). the scientist observes a bear with her
binoculars. from their size she estimates that its weight is 180 kg. what is the id203 that
the bear is male?

chapter 3. multivariate random variables

49

figure 3.4: conditional and marginal distributions of the bias of the coin    ip in example 3.3.9.

we apply lemma 3.3.7 to compute

ps|w (0|180) =

=

ps (0) fw|s (180|0)

fw (180)

1

3200(cid:17)
40 exp(cid:16)    602
20 exp(cid:16)    402
3200(cid:17) + 1
800(cid:17)

1

40 exp(cid:16)    602

= 0.545.

(3.108)

(3.109)

(3.110)

according to the probabilistic model, the id203 that it   s a male is 0.545.

(cid:52)
example 3.3.9 (bayesian coin    ip (continued)). the coin lands on tails. you decide to recom-
pute the distribution of the bias conditioned on this information. by lemma 3.3.7

fb|x (b|0) =

=

fb (b) px|b (0|b)

px (0)
2b (1     b)

1/3

= 6b (1     b) .

(3.111)

(3.112)

(3.113)

conditioned on the outcome, the pdf of the bias is now centered instead of concentrated near
one as before, as shown in figure 3.4.

(cid:52)

3.4 independence

in this section we de   ne independence and conditional independence for random variables and
vectors.

00.20.40.60.8100.511.52fb(  )fb|x(  |0)chapter 3. multivariate random variables

50

3.4.1 de   nition

when knowledge about a random variable x does not a   ect our uncertainty about another
random variable y , we say that x and y are independent. formally, this is re   ected by the
marginal and conditional cdf and the conditional pmf or pdf which must be equal, i.e.

and

fy (y) = fy |x (y|x)

py (y) = py |x (y|x)

or

fy (y) = fy |x (y|x) ,

(3.114)

(3.115)

depending on whether the variable is discrete or continuous, for any x and any y for which the
conditional distributions are well de   ned. equivalently, the joint cdf and the conditional pmf or
pdf factors into the marginals.

de   nition 3.4.1 (independent random variables). two random variables x and y are inde-
pendent if and only if

fx,y (x, y) = fx (x) fy (y) ,

for all (x, y)     r2.

if the variables are discrete, the following condition is equivalent

px,y (x, y) = px (x) py (y) ,

for all x     rx , y     ry .

(3.116)

(3.117)

if the variables are continuous have joint and marginal pdfs, the following condition is equivalent

fx,y (x, y) = fx (x) fy (y) ,

for all (x, y)     r2.

(3.118)

when now extend the de   nition to account for several random variables (or equivalently several
entries in a random vector) that do not provide information about each other.

de   nition 3.4.2 (independent random variables). the n entries x1, x2, . . . , xn in a random
vector (cid:126)x are independent if and only if

which is equivalent to

for discrete vectors and

f (cid:126)x ((cid:126)x) =

n(cid:89)i=1

fxi ((cid:126)xi) ,

p (cid:126)x ((cid:126)x) =

f (cid:126)x ((cid:126)x) =

pxi ((cid:126)xi)

fxi ((cid:126)xi)

n(cid:89)i=1

n(cid:89)i=1

for continuous vectors, if the joint pdf exists.

the following example shows that pairwise independence does not imply independence.

(3.119)

(3.120)

(3.121)

chapter 3. multivariate random variables

51

example 3.4.3 (pairwise independence does not imply joint independence). let x1 and x2
be the outcomes of independent unbiased coin    ips. let x3 be the indicator of the event
{x1 and x2 have the same outcome},

the pmf of x3 is

x3 =(cid:40)1

0

if x1 = x2,
if x1 (cid:54)= x2.

px3 (1) = px1,x2 (1, 1) + px1,x2 (0, 0) =

px3 (0) = px1,x2 (0, 1) + px1,x2 (1, 0) =

1
2
1
2

,

.

x1 and x2 are independent by assumption. x1 and x3 are independent because

px1,x3 (0, 0) = px1,x2 (0, 1) =

px1,x3 (1, 0) = px1,x2 (1, 0) =

px1,x3 (0, 1) = px1,x2 (0, 0) =

px1,x3 (1, 1) = px1,x2 (1, 1) =

1
4
1
4
1
4
1
4

= px1 (0) px3 (0) ,

= px1 (1) px3 (0) ,

= px1 (0) px3 (1) ,

= px1 (1) px3 (1) .

(3.122)

(3.123)

(3.124)

(3.125)

(3.126)

(3.127)

(3.128)

x2 and x3 are independent too (the reasoning is the same).
however, are x1, x2 and x3 all independent?

px1,x2,x3 (1, 1, 1) = p (x1 = 1, x2 = 1) =

1
4 (cid:54)= px1 (1) px2 (1) px3 (1) =

1
8

.

(3.129)

they are not, which makes sense since x3 is a function of x1 and x2.

(cid:52)
conditional independence indicates that two random variables do not depend on each other,
as long as an additional random variable is known.

de   nition 3.4.4 (conditionally independent random variables). two random variables x and
y are independent with respect to another random variable z if and only if

fx,y | z (x, y | z) = fx | z (x| z) fy | z (y | z) ,

and any z for which the conditional cdfs are well de   ned.
following condition is equivalent

for all (x, y)     r2,

(3.130)

if the variables are discrete, the

px,y | z (x, y | z) = px | z (x| z) py | z (y | z) ,

for all x     rx , y     ry ,

(3.131)

and any z for which the conditional pmfs are well de   ned. if the variables are continuous have
joint and marginal pdfs, the following condition is equivalent

fx,y | z (x, y | z) = fx | z (x| z) fy | z (y | z) ,
and any z for which the conditional pmfs are well de   ned.

for all (x, y)     r2,

(3.132)

chapter 3. multivariate random variables

52

the de   nition can be extended to condition on several random variables.

de   nition 3.4.5 (conditionally independent random variables). the components of a sub-
vector (cid:126)xi, i     {1, 2, . . . , n} are conditionally independent given another subvector (cid:126)xj , j    
{1, 2, . . . , n}, if and only if

which is equivalent to

for discrete vectors and

f (cid:126)xi| (cid:126)xj ((cid:126)xi|(cid:126)xj ) =(cid:89)i   i
p (cid:126)xi| (cid:126)xj ((cid:126)xi|(cid:126)xj ) =(cid:89)i   i
f (cid:126)xi| (cid:126)xj ((cid:126)xi|(cid:126)xj ) =(cid:89)i   i

fxi| (cid:126)xj ((cid:126)xi|(cid:126)xj ) ,

pxi| (cid:126)xj ((cid:126)xi|(cid:126)xj )

fxi| (cid:126)xj ((cid:126)xi|(cid:126)xj )

(3.133)

(3.134)

(3.135)

for continuous vectors if the conditional joint pdf exists.

as established in examples 1.3.3 and 1.3.4, independence does not imply conditional indepen-
dence or vice versa.

3.4.2 variable dependence in probabilistic modeling

a fundamental consideration when designing a probabilistic model is the dependence between
the di   erent variables, i.e. what variables are independent or conditional independent from each
other. although it may sound surprising, if the number of variables is large, introducing some
independence assumptions may be necessary to make the model tractable, even if we know that
all the variables are dependent. to illustrate this, consider a model for the us presidential
election where there are 50 random variables, each representing a state. if the variables only
take two possible values (representing what candidate wins that state), the joint pmf of their
distribution has 250     1     1015 degrees of freedom. we wouldn   t be able to store the pmf
with all the computer memory in the world! in contrast, if we assume that all the variables are
independent, then the distribution only has 50 free parameters. of course, this is not necessarily
a good idea because failing to represent dependencies may severely a   ect the prediction accuracy
of a model, as illustrated in example 3.5.1 below. striking a balance between tractability and
accuracy is a crucial challenge in probabilistic modeling.

we nos illustrate how the dependence structure of the random variables in a probabilistic model
can be exploited to reduce the number of parameters describing the distribution through an
appropriate factorization of their joint pmf or pdf. consider three bernoulli random variables
a, b and c. in general, we need 7 = 23     1 parameters to describe the pmf. however, if b and
c are conditionally independent given a we can perform the following factorization

pa,b,c = pa pb | a pc | a

(3.136)

it is
which only depends on    ve parameters (one for pa and two each for pb | a and pc | b).
important to note that there are many other possible factorizations that do not exploit the
dependence assumptions, such as for example

pa,b,c = pb pa | b pc | a,b.

(3.137)

chapter 3. multivariate random variables

53

figure 3.5: example of a directed acyclic graphic representing a probabilistic model.

for large probabilistic models it is crucial to    nd factorizations that reduce the number of
parameters as much as possible.

3.4.3 id114

id114 are a tool for characterizing the dependence structure of probabilistic models.
in this section we give a brief description of directed id114, which are also called
id110s. undirected id114, known as markov random    elds, are out of
the scope of these notes. we refer the interested reader to more advanced texts in probabilistic
modeling and machine learning for a more in-depth treatment of id114.

directed acyclic graphs, known as dags, can be interpreted as diagrams representing a factor-
ization of the joint pmf or pdf of a probabilistic model. in order to specify a valid factorization,
the graphs are constrained to not have any cycles (hence the term acyclic). each node in the
dag represents a random variable. the edges between the nodes indicate the dependence
between the variables. the factorization corresponding to a dag contains:

    the marginal pmf or pdf of the variables corresponding to all nodes with no incoming

edges.

    the conditional pmf or pdf of the remaining random variables given their parents. a is a
parent of b if there is a directed edge from (the node assigned to) a to (the node assigned
to) b.

to be concrete, consider the dag in figure 3.5. for simplicity we denote each node using the
corresponding random variable and assume that they are all discrete. nodes x1 and x4 have
no parents, so the factorization of the joint pmf includes their marginal pmfs. node x2 only
descends from x4 so we include px2 | x4. node x3 descends from x2 so we include px3 | x2.
finally, node x5 descends from x3 and x4 so we include px5 | x3,x4. the factorization is of the
form

px1,x2,x3,x4,x5 = px1 px4 px2 | x4 px3 | x2 px5 | x3,x4.

(3.138)

this factorization reveals some dependence assumptions. by the chain rule another valid fac-
torization of the joint pmf is

px1,x2,x3,x4,x5 = px1 px4 | x1 px2 | x1,x4 px3 | x1,x2,x4 px5 | x1,x2,x3,x4.

(3.139)

x1x2x3x4x5chapter 3. multivariate random variables

54

pl,r,t = pr pl | r pt | r

pm,r,t = pm pr pl | m,r

figure 3.6: directed id114 corresponding to the variables in examples 1.3.3 and 1.3.4.

comparing both expressions, we see that x1 and all the other variables are independent, since
px4 | x1 = px4, px2 | x1,x4 = px2 | x4 and so on. in addition, x3 is conditionally independent of
x4 given x2 since px3 | x2,x4 = px3 | x2. these dependence assumptions can be read directly
from the graph, using the following property.

theorem 3.4.6 (local markov property). the factorization of the joint pmf or pdf represented
by a dag satis   es the local markov property: each variable is conditionally independent of its
non-descendants given all its parent variables. in particular, if it has no parents, it is independent
of its non-descendants. to be clear, b is a non-descendant of a if there is no directed path from
a to b.

proof. let xi be an arbitrary variable. we denote by xn the set of non-descendants of xi, by
xp the set of parents and by xd the set of descendants. the factorization represented by the
graphical model is of the form

by the chain rule another valid factorization is

px1,...,xn = pxn pxp |xn pxi|xp pxd|xi.

px1,...,xn = pxn pxp |xn pxi|xp ,xn pxd|xi,xp ,xn .

(3.140)

(3.141)

comparing both expressions we conclude that pxi|xp ,xn = pxi|xp so xi is conditionally inde-
pendent of xn given xp .

we illustrate these ideas by showing the dags for examples 1.3.3 and 1.3.4.

example 3.4.7 (graphical model for example 1.3.3). we model the di   erent events in ex-
ample 1.3.3 using indicator random variables. t represents whether a taxi is available (t = 1)
or not (t = 0), l whether the plane is late (l = 1) or not (l = 0), and r whether it rains
(r = 1) or not (r = 0). in the example, t and l are conditionally independent given r. we
can represent the corresponding factorization using the graph on the left of figure 3.6.

(cid:52)

example 3.4.8 (graphical model for example 1.3.4). we model the di   erent events in exam-
ple 1.3.4 using indicator random variables. m represents whether a mechanical problem occurs

rltlrmchapter 3. multivariate random variables

55

figure 3.7: fictitious country considered in example 3.4.9.

(m = 1) or not (m = 0) and l and r are the same as in example 3.4.7. in the example, m
and r are independent, but l depends on both of them. we can represent the corresponding
factorization using the graph on the right of figure 3.6.

(cid:52)
the following example that introduces an important class of id114 called markov
chains, which we will discuss at length in chapter 7.

example 3.4.9 (election). in the country shown in figure 3.7 the presidential election follows
the same system as in the united states. citizens cast ballots for electors in the electoral
college. each state is entitled to a number of electors (in the us this is usually the same as the
members of congress). in every state, the electors are pledged to the candidate that wins the
state. our goal is to model the election probabilistically. we assume that there are only two
candidates a and b. each state is represented by a random variable si, 1     i     4,

si =(cid:40)1

   1

if candidate a wins state i,
if candidate b wins state i.

(3.142)

an important decision to make is what independence assumptions to assume about the model.
figure 3.8 shows three di   erent options. if we model each state as independent, then we only
need to estimate a single parameter for each state. however, the model may not be accurate,
as the outcome in states with similar demographics is bound to be related. another option is
to estimate the full joint pmf. the problem is that it may be quite challenging to compute
the parameters. we can estimate the marginal pmfs of the individual states using poll data,
but conditional probabilities are more di   cult to estimate. in addition, for larger models it is
not tractable to consider fully dependent models (for instance in the case of the us election,
as mentioned previously). a reasonable compromise could be to model the states that are
not adjacent as conditionally independent given the states between them. for example, we
assume that the outcome of states 1 and 3 are only related through state 2. the corresponding
graphical model, depicted on the right of figure 3.8, is called a markov chain. it corresponds

s4(3electors)s3(7electors)s2(3electors)s1(5electors)chapter 3. multivariate random variables

56

fully independent

fully dependent

markov chain

figure 3.8: id114 capturing di   erent assumptions about the distribution of the random
variables considered in example 3.4.9.

to a factorization of the form

ps1,s2,s3,s4 = ps1 ps2 | s1 ps3 | s2 ps4 | s3.

(3.143)

under this model we only need to worry about estimating pairwise conditional probabilities, as
opposed to the full joint pmf. we discuss markov chains at length in chapter 7.

(cid:52)

we conclude the section with an example involving continuous variables.

example 3.4.10 (desert). dani and felix are traveling through the desert in arizona. they
become concerned that their car might break down and decide to build a probabilistic model
to evaluate the risk. they model the time until the car breaks down as an exponential random
variable t with a parameter that depends on the state of the motor m and the state of the road
r. these three quantities are represented by random variables in the same id203 space.

unfortunately they have no idea what the state of the motor is so they assume that it is uniform
between 0 (no problem with the motor) and 1 (the motor is almost dead). similarly, they have
no information about the road, so they also assume that its state is a uniform random variable
between 0 (no problem with the road) and 1 (the road is terrible). in addition, they assume that
the states of the road and the car are independent and that the parameter of the exponential
random variable that represents the time in hours until there is a breakdown is equal to m + r.
the corresponding graphical model is shown in figure 3.9

to    nd the joint distribution of the random variables, we apply the chain rule to obtain,

fm,r,t (m, r, t) = fm (m) fr|m (r|m) ft|m,r (t|m, r)

= fm (m) fr (r) ft|m,r (t|m, r)

=(cid:40)(m + r) e   (m+r)t

otherwise.

0

for t     0, 0     m     1, 0     r     1,

(by independence of m and r)

(3.144)

(3.145)

(3.146)

note that we start with m and r because we know their marginal distribution, whereas we only
know the conditional distribution of t given m and r.

after 15 minutes, the car breaks down. the road seems ok, about a 0.2 in the scale they
de   ned for the value of r, so they naturally wonder about the state of the motor. given their

s1s2s3s4s1s2s3s4s1s2s3s4chapter 3. multivariate random variables

57

figure 3.9: the left image is a graphical model representing the random variables in example 3.4.10.
the right plot shows the conditional pdf of m given t = 0.25 and r = 0.2.

probabilistic model, their uncertainty about the motor given all of this information is captured
by the conditional distribution of m given t and r.

to compute the conditional pdf, we    rst need to compute the joint marginal distribution of t
and r by marginalizing over m . in order to simplify the computations, we use the following
simple lemma.

lemma 3.4.11. for any constant c > 0,

0

(cid:90) 1
(cid:90) 1

0

e   cx dx =

xe   cx dx =

,

c

1     e   c
1     (1 + c) e   c

c2

.

(3.147)

(3.148)

proof. equation (3.147) is obtained using the antiderivative of the exponential function (itself),
(cid:52)
whereas integrating by parts yields (3.148).

we have

fr,t (r, t) =(cid:90) 1

m=0

fm,r,t (m, r, t) dm

m=0

m=0

me   tm dm + r(cid:90) 1

= e   tr(cid:18)(cid:90) 1
= e   tr(cid:32) 1     (1 + t) e   t
r(cid:0)1     e   t(cid:1)
t2 (cid:0)1 + tr     e   t (1 + t + tr)(cid:1) ,

e   tr

t2

=

+

t

for t     0, 0     r     1.

e   tm dm(cid:19)
(cid:33) by (3.147) and (3.148)

(3.149)

(3.150)

(3.151)

(3.152)

trm00.20.40.60.8100.511.5mfm|r,t(m|0.2,0.25)chapter 3. multivariate random variables

58

the conditional pdf of m given t and r is

fm|r,t (m|r, t) =

fm,r,t (m, r, t)

fr,t (r, t)

=

=

(m + r) e   (m+r)t

e   tr
t2 (1 + tr     e   t (1 + t + tr))

(m + r) t2e   tm

1 + tr     e   t (1 + t + tr)

,

(3.153)

(3.154)

(3.155)

for t     0, 0     m     1, 0     r     1. plugging in the observed values, the conditional pdf is equal
to

fm|r,t (m|0.2, 0.25) =

(m + 0.2) 0.252e   0.25m

1 + 0.25    0.2     e   0.25 (1 + 0.25 + 0.25    0.2)

= 1.66 (m + 0.2) e   0.25m.

(3.156)

(3.157)

for 0     m     1 and to zero otherwise. the pdf is plotted in figure 3.9. according to the model,
(cid:52)
it seems quite likely that the state of the motor was not good.

3.5 functions of several random variables
the pmf of a random variable y := g (x1, . . . , xn) de   ned as a function g : rn     r of several

discrete random variables x1, . . . , xn is given by

py (y) = (cid:88)y=g(x1,...,xn)

px1,...,xn (x1, . . . , xn) .

(3.158)

this follows directly from (3.11). in words, the id203 that g (x1, . . . , xn) = y is the sum
of the joint pmf over all possible values such that y = g (x1, . . . , xn).

example 3.5.1 (election). in example 3.4.9 we discussed several possible models for a presi-
dential election for a country with four states. imagine that you are trying to predict the result
of the election using poll data from individual states. the goal is to predict the outcome of the
election, represented by the random variable

o :=(cid:40)1

0

i=1 ni si > 0,

if (cid:80)4

otherwise,

(3.159)

where ni denotes the number of electors in state i (notice that the sum can never be zero).
from analyzing the poll data you conclude that the id203 that candidate a wins each of the
states is 0.15. if you assume that all the states are independent, this is enough to characterize
the joint pmf. table 3.2 lists the id203 of all possible outcomes for this model. by (3.158)
we only need to add up the outcomes for which o = 1. under the full-independence assumption,
the id203 that candidate a wins is 6%.

you are not satis   ed by the result because you suspect that the outcomes in di   erent states
are highly dependent. from past elections, you determine that the id155 of a

chapter 3. multivariate random variables

59

s1 s2 s3 s4 o prob. (indep.) prob. (markov)
-1
-1
-1
-1
-1
-1
-1
-1
1
1
1
1
1
1
1
1

0.5220
0.0921
0.0921
0.0163
0.0921
0.0163
0.0163
0.0029
0.0921
0.0163
0.0163
0.0029
0.0163
0.0029
0.0029
0.0005

0.6203
0.0687
0.0431
0.0332
0.0431
0.0048
0.0208
0.0160
0.0687
0.0077
0.0048
0.0037
0.0332
0.0037
0.0160
0.0123

-1
-1
-1
-1
1
1
1
1
-1
-1
-1
-1
1
1
1
1

-1
-1
1
1
-1
-1
1
1
-1
-1
1
1
-1
-1
1
1

-1
1
-1
1
-1
1
-1
1
-1
1
-1
1
-1
1
-1
1

0
0
0
1
0
0
1
1
0
0
1
1
0
1
1
1

table 3.2: table of auxiliary values for example 3.5.1.

candidate winning a state if they win an adjacent state is indeed very high. you incorporate
your estimate of the conditional probabilities into a markov-chain model described by (3.143):

ps1 (1) = 0.15,
psi+1 | si (1| 1) = 0.435,
psi+1 | si (   1|     1) = 0.900

2     i     4,
2     i     4.

(3.160)

(3.161)

(3.162)

this means that if candidate b wins a state, they are very likely to win the adjacent one. if
candidate a wins a state, their chance to win an adjacent state is signi   cantly higher than if
they don   t (but still lower than candidate b). under this model the marginal id203 that
candidate a wins each state is still 0.15. table 3.2 lists the id203 of all possible outcomes.
the id203 that candidate a wins is now 11%, almost double the id203 than that
obtained under the fully-independent model. this illustrates the danger of not accounting for
dependencies between states, which for example may have been one of the reasons why many
forecasts severely underestimated donald trump   s chances in the 2016 election.

(cid:52)
section 2.5 explains how to derive the distribution of functions of univariate random variables by
   rst computing their cdf and then di   erentiating it to obtain their pdf. this directly extends to
multivariable random functions. let x, y be random variables de   ned on the same id203

chapter 3. multivariate random variables

60

space, and let u = g (x, y ) and v = h (x, y ) for two arbitrary functions g, h : r2     r. then,

fu,v (u, v) = p (u     u, v     v)

= p (g (x, y )     u, h (x, y )     v)

=(cid:90){(x,y) | g(x,y)   u,h(x,y)   v}

fx,y (x, y) dx dy,

(3.163)

(3.164)

(3.165)

where the last equality only holds if the joint pdf of x and y exists. the joint pdf can then be
obtained by di   erentiation.

theorem 3.5.2 (pdf of the sum of two independent random variables). the pdf of z = x + y ,
where x and y are independent random variables is equal to the convolution of their respective
pdfs fx and fy ,

fz (z) =(cid:90)    

u=      

fx (z     u) fy (u) du.

proof. first we derive the cdf of z

fz (z) = p (x + y     z)

=(cid:90)    
y=      (cid:90) z   y
=(cid:90)    

y=      

fx (x) fy (y) dx dy

x=      
fx (z     y) fy (y) dy.

(3.166)

(3.167)

(3.168)

(3.169)

note that the joint pdf of x and y is the product of the marginal pdfs because the random
variables are independent. we now di   erentiate the cdf to obtain the pdf. note that this requires
an interchange of a limit operator with a di   erentiation operator and another interchange of
an integral operator with a di   erentiation operator, which are justi   ed because the functions
involved are bounded and integrable.

fz (z) =

= lim
u      

= lim

d

lim

d
dz

u      (cid:90) u
dz(cid:90) u
u      (cid:90) u
u      (cid:90) u

y=   u

y=   u

= lim

y=   u

y=   u
d
dz

fx (z     y) fy (y) dy

fx (z     y) fy (y) dy

fx (z     y) fy (y) dy

fx (z     y) fy (y) dy.

(3.170)

(3.171)

(3.172)

(3.173)

example 3.5.3 (co   ee beans). a company that makes co   ee buys beans from two small local
producers in colombia and vietnam. the amount of beans they can buy from each producer
varies depending on the weather. the company models these quantities c and v as independent
random variables (assuming that the weather in colombia is independent from the weather in
vietnam) which have uniform distributions in [0, 1] and [0, 2] (the unit is tons) respectively.

chapter 3. multivariate random variables

61

figure 3.10: id203 density functions in example 3.5.3.

we now compute the pdf of the total amount of co   ee beans b := e +v applying theorem 3.5.2,

fc (b     u) fv (u) du

=

u=      
1

fc (b     u) du

fb (b) =(cid:90)    
2(cid:90) 2
=               
2(cid:82) b
u=0 du = b
2(cid:82) b
u=b   1 du = 1
2(cid:82) 2
u=b   1 du = 3   b

u=0

1

2

1

2

1

2

if b     1
if 1     b     2
if 2     b     3.

the pdf of b is shown in figure 3.10.

(3.174)

(3.175)

(3.176)

(cid:52)

3.6 generating multivariate random variables

in section 2.6 we consider the problem of generating independent samples from an arbitrary
univariate distribution. assuming that a procedure to achieve this is available, we can use it to
sample from an arbitrary multivariate distribution by generating samples from the appropriate
conditional distributions.

algorithm 3.6.1 (sampling from a multivariate distribution). let x1, x2, . . . , xn be ran-
dom variables belonging to the same id203 space. to generate samples from their joint
distribution we sequentially sample from their conditional distributions:

1. obtain a sample x1 of x1.
2. for i = 2, 3, . . . , n, obtain a sample xi of xi given the event {x1 = x1, . . . , xi   1 = xi   1}

by sampling from fxi|x1,...,xi   1 (  |x1, . . . , xi   1).

the chain rule implies that the output x1, . . . , xn of this procedure are samples from the joint
distribution of the random variables. the following example considers the problem of sampling
from a mixture of exponential random variables.

00.511.522.5300.51fcfv00.511.522.5300.51fbchapter 3. multivariate random variables

62

example 3.6.2 (mixture of exponentials). let b be a bernoulli random variable with parame-
ter p and x an exponential random variable with parameter 1 if b = 0 and 2 if b = 1. assume
that we have access to two independent samples u1 and u2 from a uniform distribution in [0, 1].
to obtain samples from b and x:

1. we set b := 1 if u1     p and b := 0 otherwise. this ensures that b is a bernoulli sample

with the right parameter.

2. then, we set

x :=

1
  

log(cid:18) 1

1     u2(cid:19)

(3.177)

where    := 1 if b = 0 and    := 2 if b = 1. by example 2.6.4 x is distributed as an
exponential with parameter   .

(cid:52)

3.7 rejection sampling

we end the chapter by describing rejection sampling, an alternative procedure for sampling from
univariate distributions. the reason we have deferred it to this chapter is that analyzing this
technique requires an understanding of multivariate distributions. recall that inverse-transform
sampling, the technique for sampling from a continuous univariate distribution described in
section 2.6.2 assumes that we have access to the inverse of the cdf of the distribution of interest.
unfortunately this is not always available. for instance, in the case of the gaussian random
variable the cdf does not even have a closed-form expression. rejection sampling, also known
as the accept-reject method, only requires knowing the pdf of the distribution of interest. it
allows to obtain samples according to a target pdf fy by choosing samples obtained according
to a di   erent pdf fx that satis   es

fy (y)     c fx (y)

(3.178)

for all y, where c is a    xed positive constant. in words, the pdf of y must be bounded by a
scaled version of the pdf of x.

algorithm 3.7.1 (rejection sampling). let x be a random variable with pdf fx and u a ran-
dom variable that is uniformly distributed in [0, 1] and independent of x. we assume that (3.178)
holds.

1. obtain a sample y of x.

2. obtain a sample u of u .

3. declare y to be a sample of y if

u    

fy (y)
c fx (y)

.

(3.179)

chapter 3. multivariate random variables

63

the following theorem establishes that the samples obtained by rejection sampling have the
desired distribution.

theorem 3.7.2 (rejection sampling works). if assumption (3.178) holds, then the samples
produced by rejection sampling are distributed according to fy .

proof. let z denote the random variable produced by rejection sampling. the cdf of z is equal
to

to compute the numerator we integrate the joint pdf of u and x over the region of interest

.

=

fy (x)

fz (y) = p(cid:18)x     y | u    
c fx (x)(cid:19)
p(cid:16)x     y, u     fy (x)
c fx (x)(cid:17)
p(cid:16)u     fy (x)
c fx (x)(cid:17)
c fx (x)(cid:19) =(cid:90) y
x=      (cid:90) fy (x)
=(cid:90) y
c(cid:90) y

u=0
fy (x)
c fx (x)

x=      
1

fy (x)

c fx (x)

=

fy (x) dx

x=      
fy (y) .

1
c

fx (x) du dx

fx (x) dx

p(cid:18)x     y, u    

the denominator is obtained in a similar way

=

p(cid:18)u    

c fx (x)

fy (x)

c fx (x)(cid:19) =(cid:90)    
x=      (cid:90) fy (x)
=(cid:90)    
c(cid:90)    

u=0
fy (x)
c fx (x)

x=      
1

x=      

=

fy (x) dx

fx (x) du dx

fx (x) dx

=

1
c

.

we conclude that

so the method produces samples from the distribution of y .

fz (y) = fy (y) ,

(3.180)

(3.181)

(3.182)

(3.183)

(3.184)

(3.185)

(3.186)

(3.187)

(3.188)

(3.189)

(3.190)

we now illustrate the method by applying it to produce a gaussian random variable from an
exponential and a uniform random variable.

example 3.7.3 (generating a gaussian random variable). in example 2.6.4 we learned how
to generate an exponential random variables using samples from a uniform distribution. in this
example we will use samples from an exponential distribution to generate a standard gaussian
random variable applying rejection sampling.

the following lemma shows that we can generate a standard gaussian random variable y by:

chapter 3. multivariate random variables

64

figure 3.11: bound on the pdf of the target distribution in example 3.7.3.

1. generating a random variable h with pdf

fh (h) :=(cid:40) 2   2  

0

exp(cid:16)    h2

2(cid:17) if h     0,

otherwise.

(3.191)

2. generating a random variable s which is equal to 1 or -1 with id203 1/2, for example

by applying the method described in section 2.6.1.

3. setting y := sh.

lemma 3.7.4. let h be a continuous random variable with pdf given by (3.191) and s a discrete
random variable which equals 1 with id203 1/2 and    1 with id203 1/2. the random
variable of y := sh is a standard gaussian.

proof. the conditional pdf of y given s is given by

fy |s (y|1) =(cid:40)fh (y)
fy |s (y|     1) =(cid:40)fh (   y)

0

0

if y     0,
otherwise,

if y < 0,
otherwise.

by lemma 3.3.5 we have

fy (y) = ps (1) fy |s (y|1) + ps (   1) fy |s (y|     1)

=

1
   2  

exp(cid:18)   

y2

2(cid:19) .

(3.192)

(3.193)

(3.194)

(3.195)

(cid:52)
the reason why we reduce the problem to generating h is that its pdf is only nonzero on the
positive axis, which allows us to bound it with the exponential pdf of an exponential random

01234500.51xcfx(x)fh(x)chapter 3. multivariate random variables

65

histogram of 50,000 iid
samples from x (fx is
shown in black)

scatterplot of
samples
from x and samples
from
(accepted
samples are colored red,
is
exp

(cid:17)
    (x     1)2 /2

(cid:16)

u

shown in black)

histogram of accepted
samples (fh is shown in
black)

figure 3.12: illustration of how to generate 50,000 samples from the random variable h de   ned in
example 3.7.3 via rejection sampling.

012345x0.00.20.40.60.81.0012345x0.00.20.40.60.81.0u012345h0.00.10.20.30.40.50.60.70.80.9chapter 3. multivariate random variables

66

variable x with parameter 1. if we set c :=(cid:112)2e/   then fh (x)     cfx (x) for all x, as illustrated

in figure 3.11. indeed,

exp(cid:16)    x2
2(cid:17)
exp(cid:32)    (x     1)2

exp (   x)

2

(cid:33)

2   2  

fh (x)
fx (x)

=

  

=(cid:114) 2e
   (cid:114) 2e

  

.

we can now apply rejection sampling to generate h. the steps are

1. obtain a sample x from an exponential random variable x with parameter one

2. obtain a sample u from u , which is uniformly distributed in [0, 1].

3. accept x as a sample of h if

u     exp(cid:32)    (x     1)2

2

(cid:33) .

this procedure is illustrated in figure 3.12. the rejection mechanism ensures that the accepted
(cid:52)
samples have the right distribution.

(3.196)

(3.197)

(3.198)

(3.199)

chapter 4

expectation

in this section we introduce some quantities that describe the behavior of random variables
very succinctly. the mean is the value around which the distribution of a random variable is
centered. the variance quanti   es the extent to which a random variable    uctuates around the
mean. the covariance of two random variables indicates whether they tend to deviate from
their means in a similar way. in multiple dimensions, the covariance matrix of a random vector
encodes its variance in every possible direction. these quantities do not completely characterize
the distribution of a random variable or vector, but they provide a useful summary of their
behavior with just a few numbers.

4.1 expectation operator

the expectation operator allows us to de   ne the mean, variance and covariance rigorously. it
maps a function of a random variable or of several random variables to an average weighted by
the corresponding pmf or pdf.

de   nition 4.1.1 (expectation for discrete random variables). let x be a discrete random

variable with range r. the expected value of a function g (x), g : r     r, of x is

g (x) px (x) .

(4.1)

similarly, if x, y are both discrete random variables with ranges rx and ry then the expected

value of a function g (x, y ), g : r2     r, of x and y is

g (x, y) px,y (x, y) .

(4.2)

e (g (x)) :=(cid:88)x   r

e (g (x, y )) := (cid:88)x   rx (cid:88)x   ry

if (cid:126)x is an n-dimensional discrete random vector, the expected value of a function g( (cid:126)x), g :
rn     r, of (cid:126)x is

e(cid:16)g( (cid:126)x)(cid:17) :=(cid:88)(cid:126)x1 (cid:88)(cid:126)x2

      (cid:88)(cid:126)xn

g ((cid:126)x) p (cid:126)x ((cid:126)x) .

(4.3)

67

chapter 4. expectation

68

de   nition 4.1.2 (expectation for continuous random variables). let x be a continuous random

variable. the expected value of a function g (x), g : r     r, of x is

e (g (x)) :=(cid:90)    

x=      

g (x) fx (x) dx.

(4.4)

similarly, if x, y are both continuous random variables then the expected value of a function

g (x, y ), g : r2     r, of x and y is
e (g (x, y )) :=(cid:90)    

x=      (cid:90)    

y=      

g (x, y) fx,y (x, y) dx dy.

(4.5)

if (cid:126)x is an n-dimensional random vector, the expected value of a function g (x), g : rn     r, of
(cid:126)x is

e(cid:16)g( (cid:126)x)(cid:17) :=(cid:90)    

x1=      (cid:90)    

x2=      

      (cid:90)    

xn=      

g ((cid:126)x) f (cid:126)x ((cid:126)x) dx1 dx2 . . . dxn

(4.6)

in the case of quantities that depend on both continuous and discrete random variables, the
product of the marginal and conditional distributions plays the role of the joint pdf or pmf.

de   nition 4.1.3 (expectation with respect to continuous and discrete random variables). if
c is a continuous random variable and d a discrete random variable with range rd de   ned on
the same id203 space, the expected value of a function g (c, d) of c and d is

e (g (c, d)) :=(cid:90)    
c=       (cid:88)d   rd
= (cid:88)d   rd(cid:90)    

c=      

g (c, d) fc (c) pd|c (d|c) dc

g (c, d) pd (d) fc|d (c|d) dc.

(4.7)

(4.8)

the expected value of a certain quantity may be in   nite or not even exist if the correspond-
ing sum or integral tends towards in   nity or has an unde   ned value. this is illustrated by
examples 4.1.4 and 4.2.2 below.

example 4.1.4 (st petersburg paradox). a casino o   ers you the following game. you will    ip
an unbiased coin until it lands on heads and the casino will pay you 2k dollars where k is the
number of    ips. how much are you willing to pay in order to play?

let us compute the expected gain. if the    ips are independent, the total number of    ips x is a
geometric random variable, so px (k) = 1/2k. the gain is 2x which means that

e (gain) =

2k   

1
2k =    .

   (cid:88)k=1

(4.9)

the expected gain is in   nite, but since you only get to play once, the amount of money that
you are willing to pay is probably bounded. this is known as the st petersburg paradox.

(cid:52)

a fundamental property of the expectation operator is that it is linear.

chapter 4. expectation

69

theorem 4.1.5 (linearity of expectation). for any constant a     r, any function g : r     r

and any continuous or discrete random variable x

e (a g (x)) = a e (g (x)) .

(4.10)

for any constants a, b     r, any functions g1, g2 : rn     r and any continuous or discrete

random variables x and y

e (a g1 (x, y ) + b g2 (x, y )) = a e (g1 (x, y )) + b e (g2 (x, y )) .

(4.11)

proof. the theorem follows immediately from the linearity of sums and integrals.

linearity of expectation makes it very easy to compute the expectation of linear functions of
random variables. in contrast, computing the joint pdf or pmf is usually much more complicated.

example 4.1.6 (co   ee beans (continued from example 3.5.3)). let us compute the expected
total amount of beans that can be bought. c is uniform in [0, 1], so e (c) = 1/2. v is uniform
in [0, 2], so e (v ) = 1. by linearity of expectation

e (c + v ) = e (c) + e (v )

= 1.5 tons.

(4.12)

(4.13)

note that this holds even if the two quantities are not independent.

(cid:52)
if two random variables are independent, then the expectation of the product factors into a
product of expectations.

theorem 4.1.7 (expectation of functions of independent random variables). if x, y are inde-

pendent random variables de   ned on the same id203 space, and g, h : r     r are univariate

real-valued functions, then

e (g (x) h (y )) = e (g (x)) e (h (y )) .

(4.14)

proof. we prove the result for continuous random variables, but the proof for discrete random
variables is essentially the same.

e (g (x) h (y )) =(cid:90)    
x=      (cid:90)    
=(cid:90)    
x=      (cid:90)    

y=      

y=      

g (x) h (y) fx,y (x, y) dx dy

(4.15)

g (x) h (y) fx (x) fy (y) dx dy by independence

(4.16)

= e (g (x)) e (h (y )) .

(4.17)

chapter 4. expectation

70

figure 4.1: id203 density function of a cauchy random variable.

4.2 mean and variance

4.2.1 mean

the mean of a random variable is equal to its expected value.

de   nition 4.2.1 (mean). the mean or    rst moment of x is the expected value of x: e (x).

table 4.1 lists the means of some important random variables. the derivations can be found in
section 4.5.1. as illustrated by figure 4.3, the mean is the center of mass of the pmf or the pdf
of the corresponding random variable.

if the distribution of a random variable is very heavy tailed, which means that the id203 of
the random variable taking large values decays slowly, its mean may be in   nite. this is the case
of the random variable representing the gain in example 4.1.4. the following example shows
that the mean may not exist if the value of the corresponding sum or integral is not well de   ned.

example 4.2.2 (cauchy random variable). the pdf of the cauchy random variable, which is
shown in figure 4.1, is given by

fx (x) =

1

  (1 + x2)

.

(4.18)

by the de   nition of expected value,

x

  (1 + x2)

dx =(cid:90)    

0

x

  (1 + x2)

dx    (cid:90)    

0

x

  (1 + x2)

dx.

(4.19)

now, by the change of variables t = x2,

dx =(cid:90)    

0

1

2  (1 + t)

dt = lim
t      

log(1 + t)

2  

=    ,

so e(x) does not exist, as it is the di   erence of two limits that tend to in   nity.

      

e(x) =(cid:90)    
(cid:90)    

0

x

  (1 + x2)

(4.20)

(cid:52)

   10   5051000.10.20.3xfx(x)chapter 4. expectation

71

the mean of a random vector is de   ned as the vector formed by the means of its components.

de   nition 4.2.3 (mean of a random vector). the mean of a random vector (cid:126)x is

e( (cid:126)x) :=

                     

e(cid:16) (cid:126)x1(cid:17)
e(cid:16) (cid:126)x2(cid:17)
e(cid:16) (cid:126)xn(cid:17)

      

.

                     

(4.21)

as in the univariate case, the mean can be interpreted as the value around which the distribution
of the random vector is centered.

it follows immediately from the linearity of the expectation operator in one dimension that the
mean operator is linear.

theorem 4.2.4 (mean of linear transformation of a random vector). for any random vector

(cid:126)x of dimension n, any matrix a     rm  n and (cid:126)b     rm

proof.

e(cid:16)a (cid:126)x + (cid:126)b(cid:17) =

=

e(cid:16)a (cid:126)x + (cid:126)b(cid:17) = a e( (cid:126)x) + (cid:126)b.
i=1 a1i (cid:126)xi + b1(cid:17)
e(cid:16)(cid:80)n
e(cid:16)(cid:80)n
i=1 a2i (cid:126)xi + b2(cid:17)
e(cid:16)(cid:80)n
i=1 ami (cid:126)xi + bn(cid:17)
i=1 a1ie(cid:16) (cid:126)xi(cid:17) + b1
(cid:80)n
i=1 a2ie(cid:16) (cid:126)xi(cid:17) + b2
(cid:80)n
i=1 amie(cid:16) (cid:126)xi(cid:17) + bn
(cid:80)n

                     
                     

      

      

                     
                     

= a e( (cid:126)x) + (cid:126)b.

by linearity of expectation

(4.22)

(4.23)

(4.24)

(4.25)

4.2.2 median

the mean is often interpreted as representing a typical value taken by the random variable.
however, the id203 of a random variable being equal to its mean may be zero! for instance,
a bernoulli random variable cannot equal 0.5. in addition, the mean can be severely distorted
by a small subset of extreme values, as illustrated by example 4.2.6 below. the median is an
alternative characterization of a typical value taken by the random variable, which is designed
to be more robust to such situations. it is de   ned as the midpoint of the pmf or pdf of the
random variable. if the random variable is continuous, the id203 that it is either larger or
smaller than the median is equal to 1/2.

chapter 4. expectation

72

figure 4.2: uniform pdf in [   4.5, 4.5]     [99.5, 100.5]. the mean is 10 and the median is 0.5.

de   nition 4.2.5 (median). the median of a discrete random variable x is a number m such
that

p (x     m)    

1
2

and p (x     m)    

1
2

.

the median of a continuous random variable x is a number m such that

fx (m) =(cid:90) m

      

fx (x) dx =

1
2

.

(4.26)

(4.27)

the following example illustrates the robustness of the median to the presence of a small subset
of extreme values with nonzero id203.

example 4.2.6 (mean vs median). consider a uniform random variable x with support
[   4.5, 4.5]     [99.5, 100.5]. the mean of x equals

e (x) =(cid:90) 4.5

xfx (x) dx +(cid:90) 100.5

x=99.5

=

x=   4.5
100.52     99.52
1
10
= 10.

2

the cdf of x between -4.5 and 4.5 is equal to

fx (m) =(cid:90) m

   4.5
m + 4.5

.

10

=

fx (x) dx

xfx (x) dx

(4.28)

(4.29)

(4.30)

(4.31)

(4.32)

setting this equal to 1/2 allows to compute the median which is equal to 0.5. figure 4.2 shows
the pdf of x and the location of the median and the mean. the median provides a more realistic
measure of the center of the distribution.

(cid:52)

   10010203040506070809010011000.1xfx(x)meanmedianchapter 4. expectation

73

random variable parameters mean variance

bernoulli

geometric

binomial

poisson

uniform

exponential

gaussian

p

p

n, p

  

a, b

  

  ,   

p

1
p

np

  

a+b

2

1
  

  

p (1     p)

1   p
p2

np (1     p)

  

(b   a)2

12

1
  2

  2

table 4.1: means and variance of common random variables, derived in section 4.5.1 of the appendix.

4.2.3 variance and standard deviation

the expected value of the square of a random variable is sometimes used to quantify the energy
of the random variable.

de   nition 4.2.7 (second moment). the mean square or second moment of a random variable

x is the expected value of x 2: e(cid:0)x 2(cid:1).

the de   nition generalizes to higher moments, de   ned as e (x p) for integers larger than two. the
mean square of the di   erence between the random variable and its mean is called the variance
of the random value. it quanti   es the variation of the random variable around its mean and
is also referred to as the second centered moment of the distribution. the square root of this
quantity is the standard deviation of the random variable.

de   nition 4.2.8 (variance and standard deviation). the variance of x is the mean square
deviation from the mean

the standard deviation   x of x is

var (x) := e(cid:16)(x     e (x))2(cid:17)
= e(cid:0)x 2(cid:1)     e2 (x) .
  x :=(cid:112)var (x).

we have compiled the variances of some important random variables in table 4.1. the deriva-
tions can be found in section 4.5.1. in figure 4.3 we plot the pmfs and pdfs of these random
variables and display the range of values that fall within one standard deviation of the mean.

the variance operator is not linear, but it is straightforward to determine the variance of a linear
function of a random variable.

(4.33)

(4.34)

(4.35)

chapter 4. expectation

74

geometric (p = 0.2)

binomial (n = 20, p = 0.5)

poisson (   = 25)

uniform [0, 1]

exponential (   = 1)

gaussian (   = 0,    = 1)

figure 4.3: pmfs of discrete random variables (top row) and pdfs of continuous random variables
(bottom row). the mean of the random variable is marked in red. values that are within one standard
deviation of the mean are marked in pink.

lemma 4.2.9 (variance of linear functions). for any constants a and b

var (a x + b) = a2 var (x) .

proof.

var (a x + b) = e(cid:16)(a x + b     e (a x + b))2(cid:17)
= e(cid:16)(a x + b     ae (x)     b)2(cid:17)
= a2 e(cid:16)(x     e (x))2(cid:17)

= a2 var (x) .

(4.36)

(4.37)

(4.38)

(4.39)

(4.40)

this result makes sense: if we change the center of the random variable by adding a constant,
then the variance is not a   ected because the variance only measures the deviation from the
mean. if we multiply a random variable by a constant, the standard deviation is scaled by the
same factor.

0510152005  10   20.10.150.2kpx(k)0510152005  10   20.10.150.2k1020304002468  10   2k   0.500.511.500.51xfx(x)02400.51x   4   202400.10.20.30.4xchapter 4. expectation

75

4.2.4 bounding probabilities using the mean and variance

in this section we introduce two inequalities that allow to characterize the behavior of a random
valuable to some extent just from knowing its mean and variance. the    rst is the markov
inequality, which quanti   es the intuitive idea that if a random variable is nonnegative and small
then the id203 that it takes large values must be small.

theorem 4.2.10 (markov   s inequality). let x be a nonnegative random variable. for any
positive constant a > 0,

p (x     a)    

e (x)

a

.

proof. consider the indicator variable 1x   a. we have
x     a 1x   a     0.

(4.41)

(4.42)

in particular its expectation is nonnegative (as it is the sum or integral of a nonnegative quantity
over the positive real line). by linearity of expectation and the fact that 1x   a is a bernoulli
random variable with expectation p (x     a) we have

e (x)     a e (1x   a) = a p (x     a) .

(4.43)

example 4.2.11 (age of students). you hear that the mean age of nyu students is 20 years,
but you know quite a few students that are older than 30. you decide to apply markov   s
inequality to bound the fraction of students above 30 by modeling age as a nonnegative random
variable a.

p(a     30)    
at most two thirds of the students are over 30.

e (a)

30

=

2
3

.

(4.44)

(cid:52)
as illustrated example 4.2.11, markov   s inequality can be rather loose. the reason is that it
barely uses any information about the distribution of the random variable.

chebyshev   s inequality controls the deviation of the random variable from its mean. intuitively,
if the variance (and hence the standard deviation) is small, then the id203 that the random
variable is far from its mean must be low.

theorem 4.2.12 (chebyshev   s inequality). for any positive constant a > 0 and any random
variable x with bounded variance,

p (|x     e (x)|     a)    

var (x)

a2

.

(4.45)

proof. applying markov   s inequality to the random variable y = (x     e (x))2 yields the result.

chapter 4. expectation

76

an interesting corollary to chebyshev   s inequality shows that if the variance of a random variable
is zero, then the random variable is a constant or, to be precise, the id203 that it deviates
from its mean is zero.

corollary 4.2.13. if var (x) = 0 then p (x (cid:54)= e (x)) = 0.
proof. take any   > 0, by chebyshev   s inequality

p (|x     e (x)|      )    

var (x)

 2

= 0.

(4.46)

example 4.2.14 (age of students (continued)). you are not very satis   ed with your bound on
the number of students above 30. you    nd out that the standard deviation of student age is
actually just 3 years. applying chebyshev   s inequality, this implies that

p(a     30)     p (|a     e (a)|     10)

var (a)

100

=

9
100

.

   

so actually at least 91% of the students are under 30 (and above 10).

(4.47)

(4.48)

(cid:52)

4.3 covariance

4.3.1 covariance of two random variables

the covariance of two random variables describes their joint behavior. it is the expected value
of the product between the di   erence of the random variables and their respective means. in-
tuitively, it measures to what extent the random variables    uctuate together.

de   nition 4.3.1 (covariance). the covariance of x and y is

cov (x, y ) := e ((x     e (x)) (y     e (y )))

= e (xy )     e (x) e (y ) .

(4.49)

(4.50)

if cov (x, y ) = 0, x and y are uncorrelated.

figure 4.4 shows samples from bivariate gaussian distributions with di   erent covariances. if
the covariance is zero, then the joint pdf has a spherical form. if the covariance is positive and
large, then the joint pdf becomes skewed so that the two variables tend to have similar values.
if the covariance is large and negative, then the two variables will tend to have similar values
with opposite sign.

the variance of the sum of two random variables can be expressed in terms of their individ-
ual variances and their covariance. as a result, their    uctuations reinforce each other if the
covariance is positive and cancel each other if it is negative.

theorem 4.3.2 (variance of the sum of two random variables).

var (x + y ) = var (x) + var (y ) + 2 cov (x, y ) .

(4.51)

chapter 4. expectation

77

cov (x, y )

0.5

0.9

0.99

cov (x, y )

0

-0.9

-0.99

figure 4.4: samples from 2d gaussian vectors (x, y ), where x and y are standard gaussian random
variables with zero mean and unit variance, for di   erent values of the covariance between x and y .

proof.

var (x + y ) = e(cid:16)(x + y     e (x + y ))2(cid:17)

= e(cid:16)(x     e (x))2(cid:17) + e(cid:16)(y     e (y ))2(cid:17) + 2e ((x     e (x)) (y     e (y )))

= var (x) + var (y ) + 2 cov (x, y ) .

(4.53)

(4.52)

an immediate consequence is that if two random variables are uncorrelated, then the variance
of their sum equals the sum of their variances.

corollary 4.3.3. if x and y are uncorrelated, then

var (x + y ) = var (x) + var (y ) .

(4.54)

the following lemma and example show that independence implies uncorrelation, but uncorre-
lation does not always imply independence.

lemma 4.3.4 (independence implies uncorrelation). if two random variables are independent,
then they are uncorrelated.

proof. by theorem 4.1.7, if x and y are independent

cov (x, y ) = e (xy )     e (x) e (y ) = e (x) e (y )     e (x) e (y ) = 0.

(4.55)

chapter 4. expectation

78

example 4.3.5 (uncorrelation does not imply independence). let x and y be two independent
bernoulli random variables with parameter 1/2. consider the random variables

u = x + y,
v = x     y.

note that

pu (0) = p (x = 0, y = 0) =

1
4

,

pv (0) = p (x = 1, y = 1) + p (x = 0, y = 0) =

pu,v (0, 0) = p (x = 0, y = 0) =

1
4 (cid:54)= pu (0) pv (0) =

1
2
1
8

,

,

so u and v are not independent. however, they are uncorrelated as

cov (u, v ) = e (u v )     e (u ) e (v )

= e ((x + y ) (x     y ))     e (x + y ) e (x     y )

= e(cid:0)x 2(cid:1)     e(cid:0)y 2(cid:1)     e2 (x) + e2 (y ) = 0.

the    nal equality holds because x and y have the same distribution.

(4.56)

(4.57)

(4.58)

(4.59)

(4.60)

(4.61)

(4.62)

(4.63)

(cid:52)

4.3.2 correlation coe   cient

the covariance does not take into account the magnitude of the variances of the random variables
involved. the pearson correlation coe   cient is obtained by normalizing the covariance using
the standard deviations of both variables.

de   nition 4.3.6 (pearson correlation coe   cient). the pearson correlation coe   cient of two
random variables x and y is

  x,y :=

cov (x, y )

  x   y

.

(4.64)

the correlation coe   cient between x and y is equal to the covariance between x/  x and
y /  y . figure 4.5 compares samples of bivariate gaussian random variables that have the same
correlation coe   cient, but di   erent covariance and vice versa.

although it might not be immediately obvious, the magnitude of the correlation coe   cient is
bounded by one because the covariance of two random variables cannot exceed the product
of their standard deviations. a useful interpretation of the correlation coe   cient is that it
quanti   es to what extent x and y are linearly related. in fact, if it is equal to 1 or -1 then one
of the variables is a linear function of the other! all of this follows from the cauchy-schwarz
inequality. the proof is in section 4.5.3.

theorem 4.3.7 (cauchy-schwarz inequality). for any random variables x and y de   ned on
the same id203 space

|e (xy )|    (cid:112)e (x 2) e (y 2).

(4.65)

chapter 4. expectation

79

  y = 1, cov (x, y ) = 0.9,

  y = 3, cov (x, y ) = 0.9,

  y = 3, cov (x, y ) = 2.7,

  x,y = 0.9

  x,y = 0.3

  x,y = 0.9

figure 4.5: samples from 2d gaussian vectors (x, y ), where x is a standard gaussian random variables
with zero mean and unit variance, for di   erent values of the standard deviation   y of y (which is mean
zero) and of the covariance between x and y .

assume e(cid:0)x 2(cid:1) (cid:54)= 0,

e (xy ) =(cid:112)e (x 2) e (y 2)        y =(cid:115) e (y 2)
e (xy ) =    (cid:112)e (x 2) e (y 2)        y =    (cid:115) e (y 2)

e (x 2)

e (x 2)

x,

x.

corollary 4.3.8. for any random variables x and y ,

cov (x, y )       x   y .
equivalently, the pearson correlation coe   cient satis   es

with equality if and only if there is a linear relationship between x and y

|  x,y |     1,

where

proof. let

c :=(cid:40)   y
      y

  x

  x

|  x,y | = 1        y = c x + d.

if   x,y = 1,
if   x,y =    1,

d := e (y )     c e (x) .

u := x     e (x) ,
v := y     e (y ) .

from the de   nition of the variance and the correlation coe   cient,

e(cid:0)u 2(cid:1) = var (x) ,
e(cid:0)v 2(cid:1) = var (y )

  x,y =

e (u v )

(cid:112)e (u 2) e (v 2)

.

the result now follows from applying theorem 4.3.7 to u and v .

(4.66)

(4.67)

(4.68)

(4.69)

(4.70)

(4.71)

(4.72)

(4.73)

(4.74)

(4.75)

(4.76)

chapter 4. expectation

80

4.3.3 covariance matrix of a random vector

the covariance matrix of a random vector captures the interaction between the components
of the vector. it contains the variance of each component in the diagonal and the covariances
between di   erent components in the o    diagonals.

de   nition 4.3.9. the covariance matrix of a random vector (cid:126)x is de   ned as

   (cid:126)x :=

                        

var(cid:16) (cid:126)x1(cid:17)
cov(cid:16) (cid:126)x2, (cid:126)x1(cid:17)
cov(cid:16) (cid:126)xn, (cid:126)x1(cid:17) cov(cid:16) (cid:126)xn, (cid:126)x2(cid:17)       

cov(cid:16) (cid:126)x1, (cid:126)x2(cid:17)        cov(cid:16) (cid:126)x1, (cid:126)xn(cid:17)
var(cid:16) (cid:126)x2(cid:17)
       cov(cid:16) (cid:126)x2, (cid:126)xn(cid:17)
var(cid:16) (cid:126)xn(cid:17)

. . .

...

...

...

                        

(4.77)

(4.78)

= e(cid:16) (cid:126)x (cid:126)x t(cid:17)     e( (cid:126)x)e( (cid:126)x)t .

note that if all the entries of a vector are uncorrelated, then its covariance matrix is diagonal.

from theorem 4.2.4 we obtain a simple expression for the covariance matrix of the linear
transformation of a random vector.

theorem 4.3.10 (covariance matrix after a linear transformation). let (cid:126)x be a random vector
of dimension n with covariance matrix   . for any matrix a     rm  n and (cid:126)b     rm,

proof.

  a (cid:126)x+(cid:126)b = a   (cid:126)x at .

  a (cid:126)x+(cid:126)b = e(cid:18)(cid:16)a (cid:126)x + (cid:126)b(cid:17)(cid:16)a (cid:126)x + (cid:126)b(cid:17)t(cid:19)     e(cid:16)a (cid:126)x + (cid:126)b(cid:17) e(cid:16)a (cid:126)x + (cid:126)b(cid:17)t

= a e(cid:16) (cid:126)x (cid:126)x t(cid:17) at + (cid:126)b e(cid:16) (cid:126)x(cid:17)t
at + a e( (cid:126)x)(cid:126)bt + (cid:126)b(cid:126)bt
    a e( (cid:126)x)e( (cid:126)x)t at     a e( (cid:126)x)(cid:126)bt     (cid:126)b e( (cid:126)x)t at     (cid:126)b(cid:126)bt
= a(cid:16)e(cid:16) (cid:126)x (cid:126)x t(cid:17)     e( (cid:126)x)e( (cid:126)x)t(cid:17) at

= a   (cid:126)x at .

(4.79)

(4.80)

(4.81)

(4.82)

(4.83)

an immediate corollary of this result is that we can easily decode the variance of the random
vector in any direction from the covariance matrix. mathematically, the variance of the random
vector in the direction of a unit vector (cid:126)v is equal to the variance of its projection onto (cid:126)v.

corollary 4.3.11. let (cid:126)v be a unit vector,

var(cid:16)(cid:126)v t (cid:126)x(cid:17) = (cid:126)v t    (cid:126)x(cid:126)v.

(4.84)

chapter 4. expectation

81

consider the eigendecomposition of the covariance matrix of an n-dimensional random vector
(cid:126)x

   (cid:126)x = u   u t

=(cid:2)(cid:126)u1 (cid:126)u2

       (cid:126)un(cid:3)            

  1
0

0
  2

0

0

0
0

      
      
      
         n

            (cid:2)(cid:126)u1 (cid:126)u2

(4.85)

(4.86)

       (cid:126)un(cid:3)t

,

where the eigenvalues are ordered   1       2     . . .       n. covariance matrices are symmetric
by de   nition, so by theorem b.7.1 the eigenvectors (cid:126)u1, (cid:126)u2, . . . , (cid:126)un can be chosen to be or-
thogonal. these eigenvectors and the eigenvalues completely characterize the variance of the
random vector in di   erent directions. the theorem is a direct consequence of corollary 4.3.11
and theorem b.7.2.

theorem 4.3.12. let (cid:126)x be a random vector of dimension n with covariance matrix    (cid:126)x . the
eigendecomposition of    (cid:126)x given by (4.86) satis   es

var(cid:16)(cid:126)v t (cid:126)x(cid:17) ,
var(cid:16)(cid:126)v t (cid:126)x(cid:17) ,

  1 = max
||(cid:126)v||2=1

(cid:126)u1 = arg max
||(cid:126)v||2=1
max

  k =

||(cid:126)v||2=1,(cid:126)v   (cid:126)u1,...,(cid:126)uk   1

(cid:126)uk = arg

max

||(cid:126)v||2=1,(cid:126)v   (cid:126)u1,...,(cid:126)uk   1

var(cid:16)(cid:126)v t (cid:126)x(cid:17) ,
var(cid:16)(cid:126)v t (cid:126)x(cid:17) .

(4.87)

(4.88)

(4.89)

(4.90)

in words, (cid:126)u1 is the direction of maximum variance. the eigenvector (cid:126)u2 corresponding to the
second largest eigenvalue   2 is the direction of maximum variation that is orthogonal to (cid:126)u1. in
general, the eigenvector (cid:126)uk corresponding to the kth largest eigenvalue   k reveals the direction
of maximum variation that is orthogonal to (cid:126)u1, (cid:126)u2, . . . , (cid:126)uk   1. finally, (cid:126)un is the direction of
minimum variance. figure 4.6 illustrates this with an example, where n = 2. as we discuss
in chapter 8, principal component analysis    a popular method for unsupervised learning and
id84    applies the same principle to determine the directions of variation of
a data set.

to conclude the section, we describe an algorithm to transform samples from an uncorrelated
random vector so that they have a prescribed covariance matrix. the process of transforming
uncorrelated samples for this purpose is called coloring because uncorrelated samples are usually
described as being white noise. as we will see in the next section, coloring allows to simulate
gaussian random vectors.

algorithm 4.3.13 (coloring uncorrelated samples). let (cid:126)x be a realization from an n-dimensional
random vector with covariance matrix i. to generate samples with covariance matrix   , we:

1. compute the eigendecomposition    = u   u t .

chapter 4. expectation

82

     1 = 1.22,      2 = 0.71

     1 = 1,      2 = 1

     1 = 1.38,      2 = 0.32

figure 4.6: samples from bivariate gaussian random vectors with di   erent covariance matrices are
shown in gray. the eigenvectors of the covariance matrices are plotted in red. each is scaled by the
square roof of the corresponding eigenvalue   1 or   2.

2. set (cid:126)y := u     (cid:126)x, where       is a diagonal matrix containing the square roots of the eigen-

values of   ,

     1
0
0      2

0

0

      :=            

0
0

      
      
      
            n

             .

by theorem 4.3.10 the covariance matrix of y := u     (cid:126)x indeed equals   .

t

  (cid:126)y = u        (cid:126)x
     
= u     i     
u t
=   .

t

u t

(4.91)

(4.92)

(4.93)

(4.94)

figure 4.7 illustrates the two steps of coloring in 2d: first the samples are stretched according to
the eigenvalues of    and then they are rotated to align them with the corresponding eigenvectors.

4.3.4 gaussian random vectors

we have mostly used gaussian vectors to visualize the di   erent properties of the covariance
operator. as opposed to other random vectors, gaussian random vectors are completely de-
termined by their mean and their covariance matrix. an important consequence, is that if the
entries of a gaussian random vector are uncorrelated then they are also mutually independent.

lemma 4.3.14 (uncorrelation implies mutual independence for gaussian random vectors). if
all the components of a gaussian random vector (cid:126)x are uncorrelated, this implies that they are
mutually independent.

proof. the parameter    of the joint pdf of a gaussian random vector is its covariance matrix (one
can verify this by applying the de   nition of covariance and integrating). if all the components

chapter 4. expectation

83

(cid:126)x

      (cid:126)x

u      (cid:126)x

figure 4.7: when we color two-dimensional uncorrelated samples (left),    rst the diagonal matrix      
stretches them di   erently along di   erent directions according to the eigenvalues of the desired covariance
matrix (center) and then u rotates them so that they are aligned with the correspondent eigenvectors
(right).

are uncorrelated then

   (cid:126)x =               
                  

=

     1
(cid:126)x

  2
1
0
...
0

0
  2
2
...
0

      
0
      
0
...
. . .
         2

n

1
  2
1
0
...
0

0
1
  2
2
...
0

      
      
. . .
      

0

0
...

1
  2
n

,

               
                  

where   i is the standard deviation of the ith component. now, the inverse of this diagonal
matrix is just

,

(4.96)

i so that

f (cid:126)x ((cid:126)x) =

i=1   2
1

and its determinant is |  | =(cid:81)n
(cid:112)(2  )n |  |
n(cid:89)i=1
(cid:112)(2  )  i
n(cid:89)i=1

((cid:126)xi) .

f (cid:126)xi

=

=

1

exp(cid:18)   
exp(cid:32)   

((cid:126)x     (cid:126)  )t      1 ((cid:126)x     (cid:126)  )(cid:19)
1
2
((cid:126)xi       i)2

(cid:33)

2  2
i

since the joint pdf factors into a product of the marginals, the components are all mutually
independent.

the following algorithm generates samples from a gaussian random vector with an arbitrary
mean and covariance matrix by coloring (and centering) a vector of independent samples from
a standard gaussian distribution.

(4.95)

(4.97)

(4.98)

(4.99)

chapter 4. expectation

84

algorithm 4.3.15 (generating a gaussian random vector). to sample from an n-dimensional
gaussian random vector with mean (cid:126)   and covariance matrix   , we:

1. generate a vector (cid:126)x containing n independent standard gaussian samples.

2. compute the eigendecomposition    = u   u t .
3. set (cid:126)y := u     (cid:126)x + (cid:126)  , where       is de   ned by (8.20).

the algorithm just centers and colors the random vector (cid:126)y := u      (cid:126)x + (cid:126)  . by linearity of
expectation its mean is

e((cid:126)y ) = u     e( (cid:126)x) + (cid:126)  

= (cid:126)  

(4.100)

(4.101)

since the mean of (cid:126)x is zero. the same argument used in equation (4.94) shows that the covari-
ance matrix of (cid:126)x is   . since coloring and centering are linear operations, by theorem 3.2.14
(cid:126)y is gaussian with the desired mean and covariance matrix. for example, in figure 4.7 the
generated samples are gaussian. for non-gaussian random vectors, coloring will modify the
covariance matrix, but not necessarily preserve the distribution.

4.4 conditional expectation

conditional expectation is a useful tool for manipulating random variables. unfortunately, it can
be somewhat confusing (as we see below it   s a random variable not an expectation!). consider
a function g of two random variables x and y . the expectation of g conditioned on the event
x = x for any    xed value x can be computed using the conditional pmf or pdf of y given x.

if y is discrete and has range r, whereas

e (g (x, y )|x = x) =(cid:88)y   r
e (g (x, y )|x = x) =(cid:90)    

y=      

g (x, y) py |x (y|x) ,

(4.102)

g (x, y) fy |x (y|x) dy,

(4.103)

if y is continuous.
note that e (g (x, y )|x = x) can actually be interpreted as a function of x since it maps every
value of x to a real number. this allows to de   ne the conditional expectation of g (x, y ) given
x as follows.

de   nition 4.4.1 (conditional expectation). the conditional expectation of g (x, y ) given x
is

where

e (g (x, y )|x) := h (x) ,

h (x) := e (g (x, y )|x = x) .

(4.104)

(4.105)

chapter 4. expectation

85

beware the confusing de   nition, the conditional expectation is actually a random variable!

one of the main uses of conditional expectation is applying iterated expectation for computing
expected values. the idea is that the expected value of a certain quantity can be expressed as
the expectation of the conditional expectation of the quantity.

theorem 4.4.2 (iterated expectation). for any random variables x and y and any function

g : r2     r

e (g (x, y )) = e (e (g (x, y )|x)) .

(4.106)

proof. we prove the result for continuous random variables, the proof for discrete random
variables, and for quantities that depend on both continuous and discrete random variables, is
almost identical. to make the explanation clearer, we de   ne

h (x) := e (g (x, y )|x = x)

=(cid:90)    

y=      

g (x, y) fy |x (y|x) dy.

now,

e (e (g (x, y )|x)) = e (h (x))

h (x) fx (x) dx

=(cid:90)    
=(cid:90)    
x=      (cid:90)    

x=      

y=      
= e (g (x, y )) .

fx (x) fy |x (y|x) g (x, y) dy dx

(4.107)

(4.108)

(4.109)

(4.110)

(4.111)

(4.112)

iterated expectation allows to obtain the expectation of quantities that depend on several quan-
tities very easily if we have access to the marginal and conditional distributions. we illustrate
this with several examples taken from the previous chapters.

example 4.4.3 (desert (continued from example 3.4.10)). let us compute the mean time at
which the car breaks down, i.e. the mean of t . by iterated expectation

e (t ) = e (e (t|m, r))

m + r(cid:19) because t is exponential when conditioned on m and r
= e(cid:18) 1
0 (cid:90) 1
=(cid:90) 1
=(cid:90) 1

log (r + 1)     log (r) dr

dm dr

m + r

1

0

0

= log 4     1.39

integrating by parts.

(4.113)

(4.114)

(4.115)

(4.116)

(4.117)

(cid:52)

chapter 4. expectation

86

example 4.4.4 (grizzlies in yellowstone (continued from example 9.4.3)). let us compute the
mean weight of a bear in yosemite. by iterated expectation

e (w ) = e (e (w|s))

e (w|s = 0) + e (w|s = 1)

=

2

= 180 kg.

(4.118)

(4.119)

(4.120)

(cid:52)
example 4.4.5 (bayesian coin    ip (continued from example 3.3.6). let us compute the mean
of the coin-   ip outcome x. by iterated expectation

= e (b)

because x is bernoulli when conditioned on b

e (x) = e (e (x|b))

2b2 db

=(cid:90) 1

0
2
3

.

=

4.5 proofs

4.5.1 derivation of means and variances in table 4.1

bernoulli

geometric

e (x) = px (1) = p,

e(cid:0)x 2(cid:1) = px (1) ,
var (x) = e(cid:0)x 2(cid:1)     e2 (x) = p (1     p) .

(4.121)

(4.122)

(4.123)

(4.124)

(cid:52)

(4.125)

(4.126)

(4.127)

to compute the mean of a geometric random variable, we need to deal with a geometric series.
by lemma 4.5.3 in section 4.5.2 below we have:

   (cid:88)k=1
   (cid:88)k=1

e (x) =

=

=

k px (k)

k p (1     p)k   1
   (cid:88)k=1

p
1     p

k (1     p)k =

(4.128)

(4.129)

(4.130)

1
p

.

chapter 4. expectation

to compute the mean square value we apply lemma 4.5.4 in the same section:

e(cid:0)x 2(cid:1) =

=

=

=

   (cid:88)k=1
   (cid:88)k=1

p
1     p
2     p
p2

.

k2 px (k)

k2 p (1     p)k   1
   (cid:88)k=1

k2 (1     p)k

87

(4.131)

(4.132)

(4.133)

(4.134)

binomial

as shown in example 2.2.6, we can express a binomial random variable with parameters n and
p as the sum of n independent bernoulli random variables b1, b2, . . . with parameter p

since the mean of the bernoulli random variables is p, by linearity of expectation

note that e(cid:0)b2

bi.

x =

e (x) =

e (bi) = np.

n(cid:88)i=1
n(cid:88)i=1
i(cid:1) = p and e (bibj) = p2 by independence, so
e(cid:0)x 2(cid:1) = e      
bibj      
n(cid:88)j=1
n(cid:88)i=1
n   1(cid:88)i=1
n(cid:88)i=1
i(cid:1) + 2
e(cid:0)b2

n(cid:88)i=j+1

=

e (bibj) = np + n (n     1) p2.

poisson

from calculus we have

which is the taylor series expansion of the exponential function. this implies

e (x) =

kpx (k)

= e  ,

  k
k!

   (cid:88)k=0
   (cid:88)k=1
   (cid:88)k=1
= e         (cid:88)m=0

=

  ke     
(k     1)!

  m+1
m!

=   ,

(4.135)

(4.136)

(4.137)

(4.138)

(4.139)

(4.140)

(4.141)

(4.142)

chapter 4. expectation

and

e(cid:0)x 2(cid:1) =

k2px (k)

k  ke     
(k     1)!

=

   (cid:88)k=1
   (cid:88)k=1
= e     (cid:32)    (cid:88)k=1
= e     (cid:32)    (cid:88)m=1

(k     1)   k
(k     1)!
  m+2
m!

+

+

   (cid:88)m=1

k  k

(k     1)!(cid:33)
m! (cid:33) =   2 +   .

  m+1

uniform

we apply the de   nition of expected value for continuous random variables to obtain

e (x) =(cid:90)    
xfx (x) dx =(cid:90) b
      
b2     a2
2 (b     a)

a + b

=

=

2

a

.

similarly,

e(cid:0)x 2(cid:1) =(cid:90) b

a

=

dx

x2
b     a
b3     a3
3 (b     a)
a2 + ab + b2

3

=

exponential

applying integration by parts,

x
b     a

dx

.

      

xfx (x) dx

e (x) =(cid:90)    
=(cid:90)    
= xe     x]   0 +(cid:90)    

x  e     xdx

0

0

e     xdx =

1
  

.

similarly,

e(cid:0)x 2(cid:1) =(cid:90)    

0

x2  e     xdx

= x2e     x]   0 + 2(cid:90)    

0

xe     xdx =

2
  2 .

88

(4.143)

(4.144)

(4.145)

(4.146)

(4.147)

(4.148)

(4.149)

(4.150)

(4.151)

(4.152)

(4.153)

(4.154)

(4.155)

(4.156)

chapter 4. expectation

gaussian
we apply the change of variables t = (x       ) /  .
xfx (x) dx

      

e (x) =(cid:90)    
=(cid:90)    
   2  (cid:90)    

      
  

=

x

      

   2    

=   ,

e    (x     )2

2  2 dx

te    t2

2 dt +

  

   2  (cid:90)    

      

89

(4.157)

(4.158)

(4.159)

(4.160)

e    t2

2 dt

where the last step follows from the fact that the integral of a bounded odd function over a
symmetric interval is zero.
applying the change of variables t = (x       ) /   and integrating by parts, we obtain that

e    (x     )2

2  2 dx

x2fx (x) dx

      

x2
   2    

e(cid:0)x 2(cid:1) =(cid:90)    
=(cid:90)    
   2  (cid:90)    
   2  (cid:18)t2e    t2

      
  2

      

  2

=

=

=   2 +   2.

t2e    t2

2 dt +

te    t2

2 dt +

2    

   2  (cid:90)    

      

2 ]   

       +(cid:90)    

      

e    t2

2 dt(cid:19) +   2

4.5.2 geometric series
lemma 4.5.1. for any    (cid:54)= 0 and any integers n1 and n2
  n1       n2+1

  k =

.

n2(cid:88)k=n1

1       

corollary 4.5.2. if 0 <    < 1

   (cid:88)k=0

  k =

  
1       

.

  2

   2  (cid:90)    

      

e    t2

2 dt

(4.161)

(4.162)

(4.163)

(4.164)

(4.165)

(4.166)

(4.167)

proof. we just multiply the sum by the factor (1       ) / (1       ) which obviously equals one,

  n1 +   n1+1 +        +   n2   1 +   n2 =
=

=

1       (cid:0)  n1 +   n1+1 +        +   n2   1 +   n2(cid:1)
1       
  n1       n1+1 +   n1+1 +              n2 +   n2       n2+1
  n1       n2+1

1       

(4.168)

(4.169)

1       

chapter 4. expectation

lemma 4.5.3. for 0 <    < 1

proof. by corollary 4.5.2,

   (cid:88)k=1

k   k =

1

(1       )2 .

   (cid:88)k=0

  k =

1

1       

.

since the left limit converges, we can di   erentiate on both sides to obtain

lemma 4.5.4. for 0 <    < 1

proof. by lemma 4.5.3,

   (cid:88)k=0

   (cid:88)k=1

   (cid:88)k=1

k  k   1 =

1

(1       )2 .

k2   k =

   (1 +   )

(1       )3 .

k2  k =

   (1 +   )

(1       )3 .

since the left limit converges, we can di   erentiate on both sides to obtain

   (cid:88)k=1

k2  k   1 =

1 +   

(1       )3 .

4.5.3 proof of theorem 4.3.7

90

(4.170)

(4.171)

(4.172)

(4.173)

(4.174)

(4.175)

if e(cid:0)x 2(cid:1) = 0 then x = 0 by corollary 4.2.13 x = 0 with id203 one, which implies
e (xy ) = 0 and consequently that equality holds in (4.65). the same is true if e(cid:0)y 2(cid:1) = 0.
now assume that e(cid:0)x 2(cid:1) (cid:54)= 0 and e(cid:0)y 2(cid:1) (cid:54)= 0. let us de   ne the constants a =(cid:112)e (y 2) and
b =(cid:112)e (x 2). by linearity of expectation,

e(cid:16)(ax + by )2(cid:17) = a2e(cid:0)x 2(cid:1) + b2e(cid:0)y 2(cid:1) + 2 a b e (xy )
e(cid:16)(ax     by )2(cid:17) = a2e(cid:0)x 2(cid:1) + b2e(cid:0)y 2(cid:1)     2 a b e (xy )

= 2(cid:16)e(cid:0)x 2(cid:1) e(cid:0)y 2(cid:1) +(cid:112)e (x 2) e (y 2)e (xy )(cid:17) ,
= 2(cid:16)e(cid:0)x 2(cid:1) e(cid:0)y 2(cid:1)    (cid:112)e (x 2) e (y 2)e (xy )(cid:17) .

(4.176)

(4.177)

(4.178)

(4.179)

chapter 4. expectation

91

the expectation of a nonnegative quantity is nonzero because the integral or sum of a non-
negative quantity is nonnegative. consequently, the left-hand side of (4.176) and (4.178) is
nonnegative, so (b.117) and (b.118) are both nonnegative, which implies (4.65).

let us prove (b.21) by proving both implications.

(   ). assume e (xy ) =    (cid:112)e (x 2) e (y 2). then (b.117) equals zero, so

e(cid:18)(cid:16)(cid:112)e (x 2)x +(cid:112)e (x 2)y(cid:17)2(cid:19) = 0,

which by corollary 4.2.13 means that(cid:112)e (y 2)x =    (cid:112)e (x 2)y with id203 one.
implies e (xy ) =    (cid:112)e (x 2) e (y 2).

the proof of (b.22) is almost identical (using (4.176) instead of (b.117)).

e(y 2)
e(x 2) x. then one can easily check that (b.117) equals zero, which

(   ). assume y =    

(4.180)

chapter 5

random processes

random processes, also known as stochastic processes, are used to model uncertain quantities
that evolve in time: the trajectory of a particle, the price of oil, the temperature in new york, the
national debt of the united states, etc. in these notes we introduce a mathematical framework
that makes it possible to reason probabilistically about such quantities.

5.1 de   nition

random process.

valued functions.

where t is a discrete or continuous set.

notation, but we want to emphasize the di   erence with random variables and random vectors.

we denote random processes using a tilde over an upper case letter (cid:101)x. this is not standard
formally, a random process (cid:101)x is a function that maps elements in a sample space     to real-
de   nition 5.1.1 (random process). given a id203 space (   ,f, p), a random process (cid:101)x
is a function that maps each element    in the sample space     to a function (cid:101)x (  ,  ) : t     r,
there are two possible interpretations for (cid:101)x (  , t):
    if we    x   , then (cid:101)x (  , t) is a deterministic function of t known as a realization of the
    if we    x t then (cid:101)x (  , t) is a random variable, which we usually just denote by (cid:101)x (t).
we can consequently interpret (cid:101)x as an in   nite collection of random variables indexed by t. the
set of possible values that the random variable (cid:101)x (t) can take for    xed t is called the state
    if the indexing variable t is de   ned on r, or on a semi-in   nite interval (t0,   ) for some
t0     r, then (cid:101)x is a continuous-time random process.
numbers, then (cid:101)x is a discrete-time random process. in such cases we often use a di   erent

space of the random process. random processes can be classi   ed according to the indexing
variable or to their state space.

    if the indexing variable t is de   ned on a discrete set, usually the integers or the natural

letter from t, such as i, as an indexing variable.

92

chapter 5. random processes

93

figure 5.1: realizations of the continuous-time (left) and discrete-time (right) random process de   ned
in example 5.1.2.

if the discrete random variable takes a    nite number of values that is the same for all t,

    if (cid:101)x (t) is a discrete random variable for all t, then (cid:101)x is a discrete-state random process.
then (cid:101)x is a    nite-state random process.
    if (cid:101)x (t) is a continuous random variable for all t, then (cid:101)x is a continuous-state random

process.

note that there are continuous-state discrete-time random processes and discrete-state continuous-
time random processes. any combination is possible.
the underlying id203 space (   ,f, p) mentioned in the de   nition completely determines
the stochastic behavior of the random process. in principle we can specify random processes
by de   ning (1) a id203 space (   ,f, p) and (2) a mapping that assigns a function to each
element of    , as illustrated in the following example. this way of specifying random processes
is only tractable for very simple cases.

example 5.1.2 (puddle). bob asks mary to model a puddle probabilistically. when the puddle
is formed, it contains an amount of water that is distributed uniformly between 0 and 1 gallon.
as time passes, the water evaporates. after a time interval t the water that is left is t times less
than the initial quantity.

mary models the water in the puddle as a continuous-state continuous-time random process

(cid:101)c. the underlying sample space is (0, 1), the    algebra is the corresponding borel    algebra

(all possible countable unions of intervals in (0, 1)) and the id203 measure is the uniform
id203 measure on (0, 1). for a particular element in the sample space        (0, 1)

  
t

,

t     [1,   ),

(5.1)

(cid:101)c (  , t) :=

where the unit of t is days in this example. figure 6.1 shows di   erent realizations of the random
process. each realization is a deterministic function on [1,   ).
bob points out that he only cares what the state of the puddle is each day, as opposed to at any
time t. mary decides to simplify the model by using a continuous-state discrete-time random

24681000.20.40.60.81tec(  ,t)  =0.62  =0.91  =0.121234567891000.20.40.60.8ied(  ,i)  =0.31  =0.89  =0.52chapter 5. random processes

94

  
i

(cid:101)d (  , i) :=

process (cid:101)d. the underlying id203 space is exactly the same as before, but the time index

is now discrete. for a particular element in the sample space        (0, 1)

,

i = 1, 2, . . .

(5.2)

figure 6.1 shows di   erent realizations of the continuous random process. note that each real-
ization is just a deterministic discrete sequence.

(cid:52)
recall that the value of the random process at a speci   c time is a random variable. we can
therefore characterize the behavior of the process at that time by computing the distribution
of the corresponding random variable. similarly, we can consider the joint distribution of the
process sampled at n    xed times. this is given by the nth-order distribution of the random
process.

{t1, t2, . . . , tn} of the time index t.

de   nition 5.1.3 (nth-order distribution). the nth-order distribution of a random process (cid:101)x
is the joint distribution of the random variables (cid:101)x (t1), (cid:101)x (t2), . . . , (cid:101)x (tn) for any n samples
example 5.1.4 (puddle (continued)). the    rst-order cdf of (cid:101)c (t) in example 5.1.2 is

(5.3)

f(cid:101)c(t) (x) := p(cid:16)(cid:101)c (t)     x(cid:17)

= p (       t x)

u=0 du = t x
1
0

=               
(cid:82) t x
f(cid:101)c(t) (x) =(cid:40)t

0

if 0     x     1
t ,
if x > 1
t ,
if x < 0.

if 0     x     1
t ,
otherwise.

(5.4)

(5.5)

(5.6)

we obtain the    rst-order pdf by di   erentiating.

(cid:52)
if the nth order distribution of a random process is shift-invariant, then the process is said to
be strictly or strongly stationary.

de   nition 5.1.5 (strictly/strongly stationary process). a process is stationary in a strict or
strong sense if for any n     0 if we select n samples t1, t2, . . . , tn and any displacement   

the random variables (cid:101)x (t1), (cid:101)x (t2), . . . , (cid:101)x (tn) have the same joint distribution as (cid:101)x (t1 +    ),
(cid:101)x (t2 +    ), . . . , (cid:101)x (tn +    ).

the random processes in example 5.1.2 are clearly not strictly stationary because their    rst-
order pdf and pmf are not the same at every point. an important example of strictly stationary
processes are independent identically-distributed sequences, presented in section 5.3.

as in the case of random variables and random vectors, de   ning the underlying id203
space in order to specify a random process is usually not very practical, except for very simple

chapter 5. random processes

95

cases like the one in example 5.1.2. the reason is that it is challenging to come up with a
id203 space that gives rise to a given n-th order distribution of interest. fortunately, we
can also specify a random process by directly specifying its n-th order distribution for all values
of n = 1, 2, . . . this completely characterizes the random process. most of the random processes
described in this chapter, e.g.
independent identically-distributed sequences, markov chains,
poisson processes and gaussian processes, are speci   ed in this way.

finally, random processes can also be speci   ed by expressing them as functions of other random

processes. a function (cid:101)y := g((cid:101)x) of a random process (cid:101)x is also a random process, as it maps
any element    in the sample space     to a function (cid:101)y (  ,  ) := g((cid:101)x (  ,  )). in section 5.6 we

de   ne id93 in this way.

5.2 mean and autocovariance functions

as in the case of random variables and random vectors, the expectation operator allows to derive
quantities that summarize the behavior of the random process. the mean of the random vector

is the mean of (cid:101)x (t) at any    xed time t.

de   nition 5.2.1 (mean). the mean of a random process is the function

note that the mean is a deterministic function of t. the autocovariance of a random process is

another deterministic function that is equal to the covariance of (cid:101)x (t1) and (cid:101)x (t2) for any two

points t1 and t2. if we set t1 := t2, then the autocovariance equals the variance at t1.

de   nition 5.2.2 (autocovariance). the autocovariance of a random process is the function

  (cid:101)x (t) := e(cid:16)(cid:101)x (t)(cid:17) .

r(cid:101)x (t1, t2) := cov(cid:16)(cid:101)x (t1) , (cid:101)x (t2)(cid:17) .
r(cid:101)x (t, t) := var(cid:16)(cid:101)x (t)(cid:17) .

intuitively, the autocovariance quanti   es the correlation between the process at two di   erent
time points. if this correlation only depends on the separation between the two points, then the
process is said to be wide-sense stationary.

de   nition 5.2.3 (wide-sense/weakly stationary process). a process is stationary in a wide or
weak sense if its mean is constant

  (cid:101)x (t) :=   

and its autocovariance function is shift invariant, i.e.

r(cid:101)x (t1, t2) := r(cid:101)x (t1 +   , t2 +    )

for any t1 and t2 and any shift    . for weakly stationary processes, the autocovariance is usually
expressed as a function of the di   erence between the two time points,

r(cid:101)x (s) := r(cid:101)x (t, t + s)

for any t.

(5.12)

(5.7)

(5.8)

(5.9)

(5.10)

(5.11)

in particular,

chapter 5. random processes

96

autocovariance function

figure 5.2: realizations (bottom three rows) of gaussian processes with zero mean and the autocovari-
ance functions shown on the top row.

15105051015s0.20.00.20.40.60.81.01.2r(s)15105051015s0.20.00.20.40.60.81.01.2r(s)15105051015s1.00.50.00.51.0r(s)02468101214i321012302468101214i321012302468101214i321012302468101214i321012302468101214i321012302468101214i321012302468101214i321012302468101214i321012302468101214i3210123chapter 5. random processes

97

note that any strictly stationary process is necessarily weakly stationary because its    rst and
second-order distributions are shift invariant.

figure 5.2 shows several stationary random processes with di   erent autocovariance functions. if
the autocovariance function is zero everywhere except at the origin, then the values of the random
processes at di   erent points are uncorrelated. this results in erratic    uctuations. when the
autocovariance at neighboring times is high, the trajectory random process becomes smoother.
the autocorrelation can also induce more structured behavior, as in the right column of the
   gure.

in that example (cid:101)x (i) is negatively correlated with its two neighbors (cid:101)x (i     1) and
(cid:101)x (i + 1), but positively correlated with (cid:101)x (i     2) and (cid:101)x (i + 2). this results in rapid periodic

   uctuations.

5.3 independent identically-distributed sequences

an independent identically-distributed (iid) sequence (cid:101)x is a discrete-time random process where
(cid:101)x (i) has the same distribution for any    xed i and (cid:101)x (i1), (cid:101)x (i2), . . . , (cid:101)x (in) are mutually
independent for any n    xed indices and any n     2. if (cid:101)x (i) is a discrete random variable (or
to the distribution of each entry by p(cid:101)x . this pdf completely characterizes the random process,

equivalently the state space of the random process is discrete), then we denote the pmf associated

since for any n indices i1, i2, . . . , in and any n:

p(cid:101)x(i1),(cid:101)x(i2),...,(cid:101)x(in) (xi1, xi2, . . . , xin) =

p(cid:101)x (xi) .

(5.13)

n(cid:89)i=1

note that the distribution that does not vary if we shift every index by the same amount, so
the process is strictly stationary.

similarly, if (cid:101)x (i) is a continuous random variable, then we denote the pdf associated to the
distribution by f(cid:101)x . for any n indices i1, i2, . . . , in and any n we have
f(cid:101)x (xi) .

f(cid:101)x(i1),(cid:101)x(i2),...,(cid:101)x(in) (xi1, xi2, . . . , xin) =

(5.14)

n(cid:89)i=1

figure 5.3 shows several realizations from iid sequences which follow a uniform and a geometric
distribution.

the mean of an iid random sequence is constant and equal to the mean of its associated distri-
bution, which we denote by   ,

  (cid:101)x (i) := e(cid:16)(cid:101)x (i)(cid:17)

=   .

let us denote the variance of the distribution associated to the iid sequence by   2. the auto-
covariance function is given by

r(cid:101)x (i, j) := e(cid:16)(cid:101)x (i) (cid:101)x (j)(cid:17)     e(cid:16)(cid:101)x (i)(cid:17) e(cid:16)(cid:101)x (j)(cid:17)

=(cid:40)  2,

0.

this is not surprising, (cid:101)x (i) and (cid:101)x (j) are independent for all i (cid:54)= j, so they are also uncorrelated.

(5.15)

(5.16)

(5.17)

(5.18)

chapter 5. random processes

98

uniform in (0, 1) (iid)

geometric with p = 0.4 (iid)

figure 5.3: realizations of an iid uniform sequence in (0, 1) (   rst row) and an iid geometric sequence
with parameter p = 0.4 (second row).

5.4 gaussian process

a random process (cid:101)x is gaussian if any set of samples is a gaussian random vector. a gaussian
process (cid:101)x is fully characterized by its mean function   (cid:101)x and its autocovariance function r(cid:101)x .

for all t1, t2, . . . , tn and any n     1, the random vector

      

               
(cid:126)x :=               
(cid:101)x (t1)
(cid:101)x (t2)
(cid:101)x (tn)
  (cid:101)x (t1)
            
(cid:126)   (cid:126)x :=            
  (cid:101)x (t2)
  (cid:101)x (tn)
      

(5.19)

(5.20)

is a gaussian random vector with mean

02468101214i0.20.00.20.40.60.81.01.202468101214i0.20.00.20.40.60.81.01.202468101214i0.20.00.20.40.60.81.01.202468101214i02468101202468101214i02468101202468101214i024681012chapter 5. random processes

and covariance matrix

r(cid:101)x (t1, t1) r(cid:101)x (t1, t2)
r(cid:101)x (t1, t2) r(cid:101)x (t2, t2)
r(cid:101)x (t2, tn) r(cid:101)x (t2, tn)

...

...

   (cid:126)x :=               

       r(cid:101)x (t1, tn)
       r(cid:101)x (t2, tn)
. . .
       r(cid:101)x (tn, tn)

...

               

99

(5.21)

figure 5.2 shows realizations of several discrete gaussian processes with di   erent autocovariance
functions. sampling from a gaussian random process boils down to sampling a gaussian random
vector with the appropriate mean and covariance matrix.

algorithm 5.4.1 (generating a gaussian random process). to sample from an gaussian ran-

dom process with mean function   (cid:101)x and autocovariance function   (cid:101)x at n points t1,. . . , tn we:

1. compute the mean vector (cid:126)   (cid:126)x given by (5.20) and the covariance matrix    (cid:126)x given by (5.21).

2. generate n independent samples from a standard gaussian.

3. color the samples according to    (cid:126)x and center them around (cid:126)   (cid:126)x , as described in algo-

rithm 4.3.15.

5.5 poisson process

in example 2.2.8 we motivate the de   nition of poisson random variable by deriving the distri-
bution of the number of events that occur in a    xed time interval under the following conditions:

1. each event occurs independently from every other event.

2. events occur uniformly.

3. events occur at a rate of    events per time interval.

we now assume that these conditions hold in the semi-in   nite interval [0,   ) and de   ne a random
0 and t.

process (cid:101)n that counts the events. to be clear (cid:101)n (t) is the number of events that happen between
by the same reasoning as in example 2.2.8, the distribution of the random variable (cid:101)n (t2)    
(cid:101)n (t1), which represents the number of events that occur between t1 and t2, is a poisson random
variables (cid:101)n (t2)    (cid:101)n (t1) and (cid:101)n (t4)    (cid:101)n (t3) are independent as along as the intervals [t1, t2] and

variable with parameter    (t2     t1). this holds for any t1 and t2.
(t3, t4) do not overlap by condition 1. a poisson process is a discrete-state continuous random
process that satis   es these two properties.

in addition the random

poisson processes are often used to model events such as earthquakes, telephone calls, decay of
radioactive particles, neural spikes, etc. figure 2.6 shows an example of a real scenario where
the number of calls received at a call center is well approximated as a poisson process (as long as
we only consider a few hours). note that here we are using the word event to mean something
that happens, such as the arrival of an email, instead of a set within a sample space, which is
the meaning that it usually has elsewhere in these notes.

chapter 5. random processes

100

   = 0.2

   = 1

   = 2

de   nition 5.5.1 (poisson process). a poisson process with parameter    is a discrete-state

figure 5.4: events corresponding to the realizations of a poisson process (cid:101)n for di   erent values of the
parameter   . (cid:101)n (t) equals the number of events up to time t.
continuous random process (cid:101)n such that
1. (cid:101)n (0) = 0.
2. for any t1 < t2 < t3 < t4 (cid:101)n (t2)     (cid:101)n (t1) is a poisson random variable with parameter
3. for any t1 < t2 < t3 < t4 the random variables (cid:101)n (t2)     (cid:101)n (t1) and (cid:101)n (t4)     (cid:101)n (t3) are

   (t2     t1).

independent.

0246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.00246810t0.00.20.40.60.81.0chapter 5. random processes

101

we now check that the random process is well de   ned, by proving that we can derive the joint

pmf of (cid:101)n at any n points t1 < t2 < . . . < tn for any n     0. to alleviate notation let p(cid:16)    , x(cid:17) be

the value of the pmf of a poisson random variable with parameter      at x, i.e.

p(cid:16)    , x(cid:17) :=

    x e       

x!

.

(5.22)

we have

(5.24)

(5.23)

p(cid:101)n (t1),...,(cid:101)n (tn) (x1, . . . , xn)
= p(cid:16)(cid:101)n (t1) = x1, . . . , (cid:101)n (tn) = xn(cid:17)
= p(cid:16)(cid:101)n (t1) = x1, (cid:101)n (t2)     (cid:101)n (t1) = x2     x1, . . . , (cid:101)n (tn)     (cid:101)n (tn   1) = xn     xn   1(cid:17)
= p(cid:16)(cid:101)n (t1) = x1(cid:17) p(cid:16)(cid:101)n (t2)     (cid:101)n (t1) = x2     x1(cid:17) . . . p(cid:16)(cid:101)n (tn)     (cid:101)n (tn   1) = xn     xn   1(cid:17)
in words, we have expressed the event that (cid:101)n (ti) = xi for 1     i     n in terms of the random
variables (cid:101)n (t1) and (cid:101)n (ti)   (cid:101)n (ti   1), 2     i     n, which are independent poisson random variables
process (cid:101)n for di   erent values of the parameter    ((cid:101)n (t) equals the number of events up to time

with parameters   t1 and    (ti     ti   1) respectively.
figure 5.4 shows several sequences of events corresponding to the realizations of a poisson

t). interestingly, the interarrival time of the events, i.e. the time between contiguous events,
always has the same distribution: it is an exponential random variable.

= p (  t1, x1) p (   (t2     t1) , x2     x1) . . . p (   (tn     tn   1) , xn     xn   1) .

(5.26)

(5.25)

lemma 5.5.2 (interarrival times of a poisson process are exponential). let t denote the time
between two contiguous events in a poisson process with parameter   . t is an exponential
random variable with parameter   .

the proof is in section 5.7.1 of the appendix. figure 2.11 shows that the interarrival times of
telephone calls at a call center are indeed well modeled as exponential.

lemma 5.5.2 suggests that to simulate a poisson process all we need to do is sample from an
exponential distribution.

algorithm 5.5.3 (generating a poisson random process). to sample from a poisson random
process with parameter    we:

1. generate independent samples from an exponential random variable with parameter    t1,

t2, t3, . . .

2. set the events of the poisson process to occur at t1, t1 + t2, t1 + t2 + t3, . . .

figure 5.4 was generated in this way. to con   rm that the algorithm allows to sample from a
poisson process, we would have to prove that the resulting process satis   es the conditions in
de   nition 5.5.1. this is indeed the case, but we omit the proof.

the following lemma, which derives the mean and autocovariance functions of a poisson process
is proved in section 5.7.2.

chapter 5. random processes

102

lemma 5.5.4 (mean and autocovariance of a poisson process). the mean and autocovariance
of a poisson process equal

e(cid:16)(cid:101)x (t)(cid:17) =    t,
r(cid:101)x (t1, t2) =    min{t1, t2} .

(5.27)

(5.28)

the mean of the poisson process is not constant and its autocovariance is not shift-invariant, so
the process is neither strictly nor wide-sense stationary.

example 5.5.5 (earthquakes). the number of earthquakes with intensity at least 3 on the
richter scale occurring in the san francisco peninsula is modeled using a poisson process with
parameter 0.3 earthquakes/year. what is the id203 that there are no earthquakes in the
next ten years and then at least one earthquake over the following twenty years?

we de   ne a poisson process (cid:101)x with parameter 0.3 to model the problem. the number of
earthquakes in the next 10 years, i.e. (cid:101)x (10), is a poisson random variable with parameter
0.3    10 = 3. the earthquakes in the following 20 years, (cid:101)x (30)     (cid:101)x (10), are poisson with

parameter 0.3    20 = 6. the two random variables are independent because the intervals do not
overlap.

p(cid:16)(cid:101)x (10) = 0, (cid:101)x (30)     1(cid:17) = p(cid:16)(cid:101)x (10) = 0, (cid:101)x (30)     (cid:101)x (10)     1(cid:17)
= p(cid:16)(cid:101)x (10) = 0(cid:17) p(cid:16)(cid:101)x (30)     (cid:101)x (10)     1(cid:17)
= p(cid:16)(cid:101)x (10) = 0(cid:17)(cid:16)1     p(cid:16)(cid:101)x (30)     (cid:101)x (10) = 0(cid:17)(cid:17)
= e   3(cid:0)1     e   6(cid:1) = 4.97 10   2.

the id203 is 4.97%.

(5.29)

(5.30)

(5.31)

(5.32)

(cid:52)

5.6 random walk

a random walk is a discrete-time random process that models a sequence of steps in random

directions. to specify a random walk formally, we    rst de   ne an iid sequence of steps (cid:101)s such

that

we de   ne a random walk (cid:101)x as the discrete-state discrete-time random process

for i = 0,
for i = 1, 2, . . .

with id203 1
2 ,
with id203 1
2 .

   1

(cid:101)s (i) =(cid:40)+1
(cid:101)x (i) :=(cid:40)0
(cid:80)i
j=1(cid:101)s (j)

several realizations of the random walk.

we have speci   ed (cid:101)x as a function of an iid sequence, so it is well de   ned. figure 5.5 shows
(cid:101)x is symmetric (there is the same id203 of taking a positive step and a negative step) and

begins at the origin. it is easy to de   ne variations where the walk is non-symmetric and begins

(5.33)

(5.34)

chapter 5. random processes

103

figure 5.5: realizations of the random walk de   ned in section 5.5.

at another point. generalizations to higher dimensional spaces    for instance to model random
processes on a 2d surface    are also possible.

we derive the    rst-order pmf of the random walk in the following lemma, proved in section 5.7.3
of the appendix.

lemma 5.6.1 (first-order pmf of a random walk). the    rst-order pmf of the random walk (cid:101)x

is

p(cid:101)x(i) (x) =(cid:40)(cid:0) i
2 (cid:1) 1

i+x

0

2i

if i + x is even and    i     x     i
otherwise.

(5.35)

the    rst-order distribution of the random walk is clearly time-dependent, so the random process
is not strictly stationary. by the following lemma, the mean of the random walk is constant (it
equals zero). the autocovariance, however, is not shift invariant, so the process is not weakly
stationary either.

lemma 5.6.2 (mean and autocovariance of a random walk). the mean and autocovariance of

the random walk (cid:101)x are

  (cid:101)x (i) = 0,
r(cid:101)x (i, j) = min{i, j} .

proof.

  (cid:101)x (i) := e(cid:16)(cid:101)x (i)(cid:17)
i(cid:88)j=1(cid:101)s (j)      
= e      
e(cid:16)(cid:101)s (j)(cid:17)
i(cid:88)j=1

=

= 0.

by linearity of expectation

(5.36)

(5.37)

(5.38)

(5.39)

(5.40)

(5.41)

02468101214i642024602468101214i642024602468101214i6420246chapter 5. random processes

r(cid:101)x (i, j) := e(cid:16)(cid:101)x (i) (cid:101)x (j)(cid:17)     e(cid:16)(cid:101)x (i)(cid:17) e(cid:16)(cid:101)x (j)(cid:17)
j(cid:88)l=1 (cid:101)s (k)(cid:101)s (l)(cid:33)
= e(cid:32) i(cid:88)k=1
= e            
l(cid:54)=k (cid:101)s (k)(cid:101)s (l)            
min{i,j}(cid:88)k=1 (cid:101)s (k)2 +
j(cid:88)l=1
i(cid:88)k=1
min{i,j}(cid:88)k=1
e(cid:16)(cid:101)s (k)(cid:17) e(cid:16)(cid:101)s (l)(cid:17)
j(cid:88)l=1
i(cid:88)k=1

l(cid:54)=k

1 +

=

= min{i, j} ,

104

(5.42)

(5.43)

(5.44)

(5.45)

(5.46)

where (5.45) follows from linearity of expectation and independence.

the variance of (cid:101)x at i equals r(cid:101)x (i, i) = i which means that the standard deviation of the

random walk scales as    i.
example 5.6.3 (gambler). a gambler is playing the following game. a fair coin is    ipped
sequentially. every time the result is heads the gambler wins a dollar, every time it lands on
tails she loses a dollar. we can model the amount of money earned (or lost) by the gambler as a
random walk, as long as the    ips are independent. this allows us to estimate that the expected
gain equals zero or that the id203 that the gambler is up 6 dollars or more after the    rst
10    ips is

p (gambler is up $6 or more) = p(cid:101)x(10) (6) + p(cid:101)x(10) (8) + p(cid:101)x(10) (10)

8(cid:19) 1
=(cid:18)10
210 +(cid:18)10
9(cid:19) 1

210 +

1
210

= 5.47 10   2.

5.7 proofs

5.7.1 proof of lemma 5.5.2

we begin by deriving the cdf of t ,

ft (t) := p (t     t)

= 1     p (t > t)
= 1     p (no events in an interval of length t)
= 1     e      t

because the number of points in an interval of length t follows a poisson distribution with
parameter    t. di   erentiating we conclude that

ft (t) =   e      t.

(5.54)

(5.47)

(5.48)

(5.49)

(cid:52)

(5.50)

(5.51)

(5.52)

(5.53)

chapter 5. random processes

105

5.7.2 proof of lemma 5.5.4

by de   nition the number of events between 0 and t is distributed as a poisson random variables
with parameter    t and hence its mean is equal to    t.

the autocovariance equals

r(cid:101)x (t1, t2) := e(cid:16)(cid:101)x (t1) (cid:101)x (t2)(cid:17)     e(cid:16)(cid:101)x (t1)(cid:17) e(cid:16)(cid:101)x (t2)(cid:17)

= e(cid:16)(cid:101)x (t1) (cid:101)x (t2)(cid:17)       2t1t2.

by assumption (cid:101)x (t1) and (cid:101)x (t2)     (cid:101)x (t1) are independent so that

e(cid:16)(cid:101)x (t1) (cid:101)x (t2)(cid:17) = e(cid:16)(cid:101)x (t1)(cid:16)(cid:101)x (t2)     (cid:101)x (t1)(cid:17) + (cid:101)x (t1)2(cid:17)

= e(cid:16)(cid:101)x (t1)(cid:17) e(cid:16)(cid:101)x (t2)     (cid:101)x (t1)(cid:17) + e(cid:16)(cid:101)x (t1)2(cid:17)

=   2t1 (t2     t1) +   t1 +   2t2
=   2t1t2 +   t1.

1

(5.55)

(5.56)

(5.57)

(5.58)

(5.59)

(5.60)

5.7.3 proof of lemma 5.6.1

let us de   ne the number of positive steps s+ that the random walk takes. given the assumptions

on (cid:101)s, this is a binomial random variable with parameters i and 1/2. the number of negative
steps is s    := i    s+. in order for (cid:101)x (i) to equal x we need for the net number of steps to equal

x, which implies

this means that s+ must equal i+x

x = s+     s   
= 2s+     i.
2 . we conclude that

p(cid:101)x(i) (i) = p      
i(cid:88)j=0(cid:101)s (i) = x      
2 (cid:19) 1
=(cid:18) i

i+x

2i

if

2

i + x

(5.61)

(5.62)

(5.63)

is an integer between 0 and i.

(5.64)

chapter 6

convergence of random processes

in this chapter we study the convergence of discrete random processes. this allows to charac-
terize two phenomena that are fundamental in statistical estimation and probabilistic modeling:
the law of large numbers and the central limit theorem.

6.1 types of convergence

let us quickly recall the concept of convergence for a deterministic sequence of real numbers x1,
x2, . . . we have

xi = x

lim
i      

(6.1)

recall that any realization of a discrete-time random process (cid:101)x (  , i) where we    x the outcome

if xi is arbitrarily close to x as the index i grows. more formally, the sequence converges to x if
for any   > 0 there is an index i0 such that for all indices i greater than i0 we have |xi     x| <  .
   is a deterministic sequence. establishing convergence of such realizations to a    xed number
can therefore be achieved by computing the corresponding limit. however, if we consider the
random process itself instead of a realization and we want to determine whether it eventually
converges to a random variable x, then deterministic convergence no longer makes sense. in
this section we describe several alternative de   nitions of convergence, which allow to extend this
concept to random quantities.

6.1.1 convergence with id203 one

consider a discrete random process (cid:101)x and a random variable x de   ned on the same id203
space. if we    x an element    of the sample space    , then (cid:101)x (i,   ) is a deterministic sequence
and x (  ) is a constant. it is consequently possible to verify whether (cid:101)x (i,   ) converges deter-

ministically to x (  ) as i         for that particular value of   . in fact, we can ask: what is the
id203 that this happens? to be precise, this would be the id203 that if we draw   
we have

lim

i       (cid:101)x (i,   ) = x (  ) .

if this id203 equals one then we say that (cid:101)x (i) converges to x with id203 one.

106

(6.2)

chapter 6. convergence of random processes

107

figure 6.1: convergence to zero of the discrete random process (cid:101)d de   ned in example 5.1.2.

de   nition 6.1.1 (convergence with id203 one). a discrete random vector (cid:101)x converges

with id203 one to a random variable x belonging to the same id203 space (   ,f, p) if

p(cid:18)(cid:26)   |           ,

lim

i       (cid:101)x (  , i) = x (  )(cid:27)(cid:19) = 1.

recall that in general the sample space     is very di   cult to de   ne and manipulate explicitly,
except for very simple cases.

example 6.1.2 (puddle (continued from example 5.1.2)). let us consider the discrete random

process (cid:101)d de   ned in example 5.1.2. if we    x        (0, 1)
i      (cid:101)d (  , i) = lim
implies that (cid:101)d converges to zero with id203 one.

= 0.

lim

i      

  
i

(6.4)

(6.5)

it turns out the realizations tend to zero for all possible values of    in the sample space. this

(6.3)

(cid:52)

6.1.2 convergence in mean square and in id203

to verify convergence with id203 one we    x the outcome    and check whether the corre-
sponding realizations of the random process converge deterministically. an alternative viewpoint

random variable x as we increase i.

is to    x the indexing variable i and consider how close the random variable (cid:101)x (i) is to another
a possible measure of the distance between two random variables is the mean square of their
di   erence. if e((x     y )2) = 0 then x = y with id203 one by chebyshev   s inequality.
the mean square deviation between (cid:101)x (i) and x is a deterministic quantity (a number), so we
can evaluate its convergence as i        . if it converges to zero then we say that the random
sequence converges in mean square.

0510152025303540455000.51ied(  ,i)(6.6)

(6.7)

chapter 6. convergence of random processes

108

de   nition 6.1.3 (convergence in mean square). a discrete random process (cid:101)x converges in

mean square to a random variable x belonging to the same id203 space if

lim
i      

e(cid:18)(cid:16)x     (cid:101)x (i)(cid:17)2(cid:19) = 0.

lim
i      

p(cid:16)(cid:12)(cid:12)(cid:12)x     (cid:101)x (i)(cid:12)(cid:12)(cid:12) >  (cid:17) = 0.

  > 0. if for any  , no matter how small, this id203 converges to zero as i         then we
say that the random sequence converges in id203.

alternatively, we can consider the id203 that (cid:101)x (i) is separated from x by a certain    xed
de   nition 6.1.4 (convergence in id203). a discrete random process (cid:101)x converges in prob-

ability to another random variable x belonging to the same id203 space if for any   > 0

note that as in the case of convergence in mean square, the limit in this de   nition is deterministic,
as it is a limit of probabilities, which are just real numbers.

as a direct consequence of markov   s inequality, convergence in mean square implies convergence
in id203.

theorem 6.1.5. convergence in mean square implies convergence in id203.

proof. we have

lim
i      

p(cid:16)(cid:12)(cid:12)(cid:12)x     (cid:101)x (i)(cid:12)(cid:12)(cid:12) >  (cid:17) = lim

i      

    lim
i      
= 0,

p(cid:18)(cid:16)x     (cid:101)x (i)(cid:17)2
>  2(cid:19)
e(cid:18)(cid:16)x     (cid:101)x (i)(cid:17)2(cid:19)

 2

by markov   s inequality

(6.8)

(6.9)

(6.10)

if the sequence converges in mean square.

it turns out that convergence with id203 one also implies convergence in id203. con-
vergence in id203 one does not imply convergence in mean square or vice versa. the
di   erence between these three types of convergence is not very important for the purposes of
this course.

6.1.3 convergence in distribution
in some cases, a random process   x does not converge to the value of any random variable, but
the cdf of   x (i) converges pointwise to the cdf of another random variable x.
in that case,
the actual values of   x (i) and x are not necessarily close, but in the limit they have the same

to a random variable x belonging to the same id203 space if

distribution. in this case, we say that (cid:101)x converges in distribution to x.
de   nition 6.1.6 (convergence in distribution). a random process (cid:101)x converges in distribution
lim
i      
for all x     r where fx is continuous.

f(cid:101)x(i) (x) = fx (x)

(6.11)

chapter 6. convergence of random processes

109

note that convergence in distribution is a much weaker notion than convergence with id203

one, in mean square or in id203. if a discrete random process (cid:101)x converges to a random
variable x in distribution, this only means that as i becomes large the distribution of (cid:101)x (i) tends

to the distribution of x, not that the values of the two random variables are close. however,
convergence in id203 (and hence convergence with id203 one or in mean square) does
imply convergence in distribution.

example 6.1.7 (binomial converges to poisson). let us de   ne a discrete random process (cid:101)x (i)
such that the distribution of (cid:101)x (i) is binomial with parameters i and p :=   /i. (cid:101)x (i) and (cid:101)x (j)
are independent for i (cid:54)= j, which completely characterizes the n-order distributions of the process
for all n > 1. consider a poisson random variable x with parameter    that is independent of
  x (i) for all i. do you expect the values of x and   x (i) to be close as i        ?
no!
distribution to x, as established in example 2.2.8:

in fact even   x (i) and   x (i + 1) will not be close in general. however,   x converges in

lim
i      

i      (cid:18) i
p(cid:101)x(i) (x) = lim

x(cid:19) px (1     p)(i   x)

=

  x e     

x!

= px (x) .

6.2 law of large numbers

let us de   ne the average of a discrete random process.

(6.12)

(6.13)

(6.14)

(cid:52)

de   nition 6.2.1 (moving average). the moving or running average (cid:101)a of a discrete random
process (cid:101)x, de   ned for i = 1, 2, . . . (i.e. 1 is the starting point), is equal to

(6.15)

(cid:101)a (i) :=

1
i

i(cid:88)j=1 (cid:101)x (j) .

consider an iid sequence. a very natural interpretation for the moving average is that it is a
real-time estimate of the mean. in fact, in statistical terms the moving average is the sample
mean of the process up to time i (the sample mean is de   ned in chapter 8). the law of large
numbers establishes that the average does indeed converge to the mean of the iid sequence.

theorem 6.2.2 (weak law of large numbers). let (cid:101)x be an iid discrete random process with
mean   (cid:101)x :=    such that the variance of (cid:101)x (i)   2 is bounded. then the average (cid:101)a of (cid:101)x converges

in mean square to   .

chapter 6. convergence of random processes

110

proof. first, we establish that the mean of (cid:101)a (i) is constant and equal to   ,

e(cid:16)(cid:101)a (i)(cid:17) = e       1
i(cid:88)j=1

1
i

=

i

=   .

i(cid:88)j=1 (cid:101)x (j)      
e(cid:16)(cid:101)x (j)(cid:17)

(6.16)

(6.17)

(6.18)

due to the independence assumption, the variance scales linearly in i. recall that for indepen-
dent random variables the variance of the sum equals the sum of the variances,

var(cid:16)(cid:101)a (i)(cid:17) = var       1
i(cid:88)j=1

1
i2

=

i

i(cid:88)j=1 (cid:101)x (j)      
var(cid:16)(cid:101)x (j)(cid:17)

=

  2
i

.

we conclude that

lim
i      

e(cid:18)(cid:16)(cid:101)a (i)       (cid:17)2(cid:19) = lim

i      
= lim
i      
= lim
i      

= 0.

e(cid:18)(cid:16)(cid:101)a (i)     e(cid:16)(cid:101)a (i)(cid:17)(cid:17)2(cid:19) by (6.18)
var(cid:16)(cid:101)a (i)(cid:17)

by (6.21)

  2
i

(6.19)

(6.20)

(6.21)

(6.22)

(6.23)

(6.24)

(6.25)

by theorem 6.1.5 the average also converges to the mean of the iid sequence in id203. in
fact, one can also prove convergence with id203 one under the same assumptions. this
result is known as the strong law of large numbers, but the proof is beyond the scope of these
notes. we refer the interested reader to more advanced texts in id203 theory.

figure 6.2 shows averages of realizations of several iid sequences. when the iid sequence is
gaussian or geometric we observe convergence to the mean of the distribution, however when the
sequence is cauchy the moving average diverges. the reason is that, as shown in example 4.2.2,
the cauchy distribution does not have a well de   ned mean! intuitively, extreme values have
non-negligeable id203 under the cauchy distribution so from time to time the iid sequence
takes values with very large magnitudes and this prevents the moving average from converging.

6.3 central limit theorem

in the previous section we established that the moving average of a sequence of iid random
variables converges to the mean of their distribution (as long as the mean is well de   ned and

chapter 6. convergence of random processes

111

standard gaussian (iid)

geometric with p = 0.4 (iid)

cauchy (iid)

figure 6.2: realization of the moving average of an iid standard gaussian sequence (top), an iid
geometric sequence with parameter p = 0.4 (center) and an iid cauchy sequence (bottom).

01020304050i2.01.51.00.50.00.51.01.52.0moving averagemean of iid seq.0100200300400500i2.01.51.00.50.00.51.01.52.0moving averagemean of iid seq.010002000300040005000i2.01.51.00.50.00.51.01.52.0moving averagemean of iid seq.01020304050i024681012moving averagemean of iid seq.0100200300400500i024681012moving averagemean of iid seq.010002000300040005000i024681012moving averagemean of iid seq.01020304050i5051015202530moving averagemedian of iid seq.0100200300400500i1050510moving averagemedian of iid seq.010002000300040005000i6050403020100102030moving averagemedian of iid seq.chapter 6. convergence of random processes

112

is very useful in statistics as we will see later on.

the variance is    nite). in this section, we characterize the distribution of the average (cid:101)a (i) as i
increases. it turns out that (cid:101)a converges to a gaussian random variable in distribution, which

this result, known as the central limit theorem, justi   es the use of gaussian distributions
to model data that are the result of many di   erent independent factors. for example, the
distribution of height or weight of people in a certain population often has a gaussian shape   
as illustrated by figure 2.13    because the height and weight of a person depends on many
di   erent factors that are roughly independent. in many signal-processing applications noise is
well modeled as having a gaussian distribution for the same reason.

theorem 6.3.1 (central limit theorem). let (cid:101)x be an iid discrete random process with mean
  (cid:101)x :=    such that the variance of (cid:101)x (i)   2 is bounded. the random process    n((cid:101)a       ), which
corresponds to the centered and scaled moving average of (cid:101)x, converges in distribution to a

gaussian random variable with mean 0 and variance   2.

proof. the proof of this remarkable result is beyond the scope of these notes. it can be found
in any advanced text on id203 theory. however, we would still like to provide some
intuition as to why the theorem holds. theorem 3.5.2 establishes that the pdf of the sum of two
independent random variables is equal to the convolutions of their individual pdfs. the same
holds for discrete random variables: the pmf of the sum is equal to the convolution of the pmfs,
as long as the random variables are independent.

if each of the entries of the iid sequence has pdf f , then the pdf of the sum of the    rst i elements
can be obtained by convolving f with itself i times

if the sequence has a discrete state and each of the entries has pmf p, the pmf of the sum of the
   rst i elements can be obtained by convolving p with itself i times

f(cid:80)1
j=1 (cid:101)x(j) (x) = (f     f                f ) (x) .

p(cid:80)1
j=1 (cid:101)x(j) (x) = (p     p                p) (x) .

(6.26)

(6.27)

normalizing by i just results in scaling the result of the convolution, so the pmf or pdf of the

moving mean (cid:101)a is the result of repeated convolutions of a    xed function. these convolutions

have a smoothing e   ect, which eventually transforms the pmf/pdf into a gaussian! we show
this numerically in figure 6.3 for two very di   erent distributions: a uniform distribution and a
very irregular one. both converge to gaussian-like shapes after just 3 or 4 convolutions. the
central limit theorem makes this precise, establishing that the shape of the pmf or pdf becomes
gaussian asymptotically.

in statistics the central limit theorem is often invoked to justify treating averages as if they have a

gaussian distribution. the idea is that for large enough n    n((cid:101)a      ) is approximately gaussian
with mean 0 and variance   2, which implies that (cid:101)a is approximately gaussian with mean    and

variance   2/n. it   s important to remember that we have not established this rigorously. the
rate of convergence will depend on the particular distribution of the entries of the iid sequence.

in practice convergence is usually very fast. figure 6.4 shows the empirical distribution of the
moving average of an exponential and a geometric iid sequence. in both cases the approximation
obtained by the central limit theory is very accurate even for an average of 100 samples. the

chapter 6. convergence of random processes

113

figure 6.3: result of convolving two di   erent distributions with themselves several times. the shapes
quickly become gaussian-like.

i = 1i = 2i = 3i = 4i = 5i = 1i = 2i = 3i = 4i = 5chapter 6. convergence of random processes

114

exponential with    = 2 (iid)

i = 102

i = 103

i = 104

geometric with p = 0.4 (iid)

i = 102

i = 103

cauchy (iid)

i = 104

i = 102

i = 103

i = 104

figure 6.4: empirical distribution of the moving average of an iid standard gaussian sequence (top),
an iid geometric sequence with parameter p = 0.4 (center) and an iid cauchy sequence (bottom). the
empirical distribution is computed from 104 samples in all cases. for the two    rst rows the estimate
provided by the central limit theorem is plotted in red.

0.300.350.400.450.500.550.600.651234567890.300.350.400.450.500.550.600.65510152025300.300.350.400.450.500.550.600.651020304050607080901.82.02.22.42.62.83.03.20.51.01.52.02.51.82.02.22.42.62.83.03.212345671.82.02.22.42.62.83.03.251015202520151050510150.050.100.150.200.250.3020151050510150.050.100.150.200.250.3020151050510150.050.100.150.200.250.30chapter 6. convergence of random processes

115

   gure also shows that for a cauchy iid sequence, the distribution of the moving average does
not become gaussian, which does not contradict the central limit theorem as the distribution
does not have a well de   ned mean. to close this section we derive a useful approximation to
the binomial distribution using the central limit theorem.

example 6.3.2 (gaussian approximation to the binomial distribution). let x have a binomial
distribution with parameters n and p, such that n is large. computing the id203 that x is
in a certain interval requires summing its pmf over all the values in that interval. alternatively,
we can obtain a quick approximation using the fact that for large n the distribution of a binomial
random variable is approximately gaussian. indeed, we can write x as the sum of n independent
bernoulli random variables with parameter p,

x =

bi.

n(cid:88)i=1

(6.28)

the mean of bi is p and its variance is p (1     p). by the central limit theorem 1
n x is approx-
imately gaussian with mean p and variance p (1     p) /n. equivalently, by lemma 2.5.1, x is
approximately gaussian with mean np and variance np (1     p).
if we
assume that a basketball player makes each shot she takes with id203 p = 0.4.
assume that each shot is independent, what is the id203 that she makes more than 420
shots out of 1000? we can model the shots made as a binomial x with parameters 1000 and
0.4. the exact answer is

p (x     420) =

=

px (x)

1000(cid:88)x=420
1000(cid:88)x=420(cid:18)1000

= 10.4 10   2.

x (cid:19) 0.4x0.6(n   x)

if we apply the gaussian approximation, by lemma 2.5.1 x being larger than 420 is the same
as a standard gaussian u being larger than 420     
   where    and    are the mean and standard

deviation of x, equal to np = 400 and(cid:112)np(1     p) = 15.5 respectively.
p (x     420)     p(cid:16)(cid:112)np (1     p)u + np     420(cid:17)

= p (u     1.29)
= 1        (1.29)
= 9.85 10   2.

6.4 monte carlo simulation

simulation is a powerful tool in id203 and statistics. probabilistic models are often too
complex for us to derive closed-form solutions of the distribution or expectation of quantities of
interest, as we do in homework problems.

(6.29)

(6.30)

(6.31)

(6.32)

(6.33)

(6.34)

(6.35)

(cid:52)

chapter 6. convergence of random processes

116

as an example, imagine that you set up a probabilistic model to determine the id203 of
winning a game of solitaire. if the cards are well shu   ed, this id203 equals

p (win) =

number of permutations that lead to a win

total number

.

(6.36)

the problem is that characterizing what permutations lead to a win is very di   cult without
actually playing out the game to see the outcome. doing this for every possible permutation is
computationally intractable, since there are 52!     8 1067 of them. however, there is a simple way
to approximate the id203 of interest: simulating a large number of games and recording
what fraction result in wins. the game of solitaire was precisely what inspired stanislaw ulam
to propose simulation-based methods, known as the monte carlo method (a code name, inspired
by the monte carlo casino in monaco), in the context of nuclear-weapon research in the 1940s:

the    rst thoughts and attempts i made to practice (the monte carlo method) were suggested
by a question which occurred to me in 1946 as i was convalescing from an illness and playing
solitaires. the question was what are the chances that a can   eld solitaire laid out with 52
cards will come out successfully? after spending a lot of time trying to estimate them by pure
combinatorial calculations, i wondered whether a more practical method than    abstract thinking   
might not be to lay it out say one hundred times and simply observe and count the number of
successful plays.

this was already possible to envisage with the beginning of the new era of fast computers, and
i immediately thought of problems of neutron di   usion and other questions of mathematical
physics, and more generally how to change processes described by certain di   erential equations
into an equivalent form interpretable as a succession of random operations. later, i described
the idea to john von neumann, and we began to plan actual calculations.1

monte carlo methods use simulation to estimate quantities that are challenging to compute
exactly. in this section, we consider the problem of approximating the id203 of an event
e, as in the game of solitaire example.
algorithm 6.4.1 (monte carlo approximation). to approximate the id203 of an event e,
we:

1. generate n independent samples from the indicator function 1e associated to the event:

i1, i2, . . . , in.

2. compute the average of the n samples

which is the estimate for the id203 of e

(cid:101)a (n) :=

1
n

n(cid:88)i=1

ii

(6.37)

the id203 of interest can be interpreted as the expectation of the indicator function 1e
associated to the event,

e (1e ) = p (e) .

(6.38)

by the law of large numbers, the estimate (cid:101)a converges to the true id203 as n        . the

following example illustrates the power of this simple technique.

1http://en.wikipedia.org/wiki/monte_carlo_method#history

chapter 6. convergence of random processes

117

rank

game outcomes
1-2
1
1
1
1
2
2
2
2

1-3
1
1
3
3
1
1
3
3

2-3 r1 r2 r3
3
2
3
2
1
2
1
3
3
2
3
1
2
2
3
1

1
1
1
2
2
1
3
3

2
3
1
3
1
1
1
2

id203

1/6
1/6
1/12
1/12
1/6
1/6
1/12
1/12

id203 mass function

r1
7/12
1/4
1/6

r2
1/2
1/4
1/4

1
2
3

r3
5/12
1/4
1/3

table 6.1: the table on the left shows all possible outcomes in a league of three teams (m = 3), the
resulting ranks for each team and the corresponding id203. the table on the right shows the pmf
of the ranks of each of the teams.

example 6.4.2 (basketball league). in an intramural basketball league m teams play each other
once every season. the teams are ordered according to their past results: team 1 being the best
and team m the worst. we model the id203 that team i beats team j, for 1     i < j     m
as

p (team j beats team i) :=

1

j     i + 1

.

(6.39)

the best team beats the second with id203 1/2 and the third with id203 2/3, the
second beats the third with id203 1/2, the fourth with id203 2/3 and the    fth with
id203 3/4, and so on. we assume that the outcomes of the di   erent games are independent.

at the end of the season, after every team has played with every other team, the teams are
ranked according to their number of wins. if several teams have the same number of wins, then
they share the same rank. for example, if two teams have the most wins, they both have rank
1, and the next team has rank 3. the goal is to compute the distribution of the    nal rank of
each team in the league, which we model as the random variables r1, r2, . . . , rm. we have all
the information to compute the joint pmf of these random variables by applying the law of total
id203. as shown in table 6.1 for m = 3, all we need to do is enumerate all the possible
outcomes of the games and sum the probabilities of the outcomes that result in a particular
rank.

unfortunately, the number of possible outcomes grows dramatically with m. the number of
games equals m (m     1) /2, so the possible outcomes are 2m(m   1)/2. when there are just
10 teams, this is larger than 1013. computing the exact distribution of the    nal ranks for
leagues that are not very small is therefore very computationally demanding. fortunately, al-
gorithm 6.4.1 o   ers a more tractable alternative: we can sample a large number of seasons n by
simulating each game as a bernoulli random variable with a parameter given by equation (6.39)
and approximate the pmf using the fraction of times that each team ends up in each position.
simulating a whole season only requires sampling m (m     1) /2 games, which can be done very
fast.

table 6.2 illustrates the monte carlo approach for m = 3. the approximation is quite coarse if
we only use n = 10 simulated seasons, but becomes very accurate when n = 2, 000. figure 6.5

chapter 6. convergence of random processes

118

rank

estimated pmf (n = 10)

game outcomes
1-2
1
1
2
2
2
1
2
2
1
2

1-3
3
1
1
3
1
1
1
3
1
3

2-3 r1 r2 r3
1
2
2
3
2
3
2
2
1
3
2
3
1
3
2
2
3
2
2
2

1
1
2
3
1
1
1
3
1
3

1
3
1
1
1
2
1
1
2
1

r1

0.6 (0.583)
0.1 (0.25)
0.3 (0.167)

r2

0.7 (0.5)
0.2 (0.25)
0.1 (0.25)

r3

0.3 (0.417)
0.4 (0.25)
0.3 (0.333)

estimated pmf (n = 2, 000)

r1

r2

r3

0.582 (0.583)
0.248 (0.25)
0.171 (0.167)

0.496 (0.5)
0.261 (0.25)
0.245 (0.25)

0.417 (0.417)
0.244 (0.25)
0.339 (0.333)

1
2
3

1
2
3

table 6.2: the table on the left shows 10 simulated outcomes of a league of three teams (m = 3) and
the resulting ranks. the tables on the right show the estimated pmf obtained by monte carlo simulation
from the simulated outcomes on the left (top) and from 2,000 simulated outcomes (bottom). the exact
values are included in brackets for comparison.

monte carlo error

m average error
3
4
5
6
7

9.28 10   3
12.7 10   3
7.95 10   3
7.12 10   3
7.12 10   3

figure 6.5: the graph on the left shows the time needed to obtain the exact pmf of the    nal ranks
in example 6.4.2 and to approximate them by monte carlo approximation using 2,000 simulated league
outcomes. the table on the right shows the average error per entry of the monte carlo approximation.

246810121416182010   310   210   1100101102103numberofteamsmrunningtime(seconds)exactcomputationmontecarloapprox.chapter 6. convergence of random processes

119

m = 5

m = 20

m = 100

figure 6.6: approximate pmf of the    nal ranks in example 6.4.2 using 2,000 simulated league outcomes.

shows the running time needed to compute the exact pmf and to approximate it with the monte
carlo approach for di   erent numbers of teams. when the number of teams is very small the
exact computation is very fast, but the running time increases exponentially with m as expected,
so that for 7 teams the computation already takes 5 and a half minutes. in contrast, the monte
carlo approximation is dramatically faster. for m = 20 it just takes half a second. figure 6.6
shows the approximate pmf of the    nal ranks for 5, 20 and 100 teams. higher ranks have higher
probabilities because when two teams are tied they are awarded the higher rank.

(cid:52)

chapter 7

markov chains

the markov property is satis   ed by any random process for which the future is conditionally
independent from the past given the present.

de   nition 7.0.1 (markov property). a random process satis   es the markov property if (cid:101)x (ti+1)
is conditionally independent of (cid:101)x (t1) , . . . , (cid:101)x (ti   1) given (cid:101)x (ti) for any t1 < t2 < . . . < ti < ti+1.

if the state space of the random process is discrete, then for any x1, x2, . . . , xi+1

p(cid:101)x(tn+1)|(cid:101)x(t1),(cid:101)x(t2),...,(cid:101)x(ti) (xn+1|x1, x2, . . . , xn) = p(cid:101)x(ti+1)|(cid:101)x(ti) (xi+1|xi) .
f(cid:101)x(ti+1)|(cid:101)x(t1),(cid:101)x(t2),...,(cid:101)x(ti) (xi+1|x1, x2, . . . , xi) = f(cid:101)x(ti+1)|(cid:101)x(ti) (xi+1|xi) .

if the state space of the random process is continuous (and the distribution has a joint pdf ),

(7.1)

(7.2)

figure 7.1 shows the directed graphical model that corresponds to the dependence assumptions
implied by the markov property. any iid sequence satis   es the markov property, since all
conditional pmfs or pdfs are just equal to the marginals (in this case there would be no edges
in the directed acyclic graph of figure 7.1). the random walk also satis   es the property, since
once we    x where the walk is at a certain time i the path that it took before i has no in   uence
in its next steps.

lemma 7.0.2. the random walk satis   es the markov property.

proof. let (cid:101)x denote the random walk de   ned in section 5.6. conditioned on (cid:101)x (j) = xi for j    
i, (cid:101)x (i + 1) equals xi +(cid:101)s (i + 1). this does not depend on x1, . . . , xi   1, which implies (7.1).

7.1 time-homogeneous discrete-time markov chains

a markov chain is a random process that satis   es the markov property. here we consider
discrete-time markov chains with a    nite state space, which means that the process can
only take a    nite number of values at any given time point. to specify a markov chain, we only
need to de   ne the pmf of the random process at its starting point (which we will assume is at
i = 0) and its transition probabilities. this follows from the markov property, since for any

120

chapter 7. markov chains

121

figure 7.1: directed graphical model describing the dependence assumptions implied by the markov
property.

n     0

p(cid:101)x(0),(cid:101)x(1),...,(cid:101)x(n) (x0, x1, . . . , xn) :=
p(cid:101)x(i)|(cid:101)x(0),...,(cid:101)x(i   1) (xi|x0, . . . , xi   1)
n(cid:89)i=0
p(cid:101)x(i)|(cid:101)x(i   1) (xi|xi   1) .
n(cid:89)i=0

=

(7.3)

(7.4)

if these transition probabilities are the same at every time step (i.e. they are constant and do
not depend on i), then the markov chain is said to be time homogeneous. in this case, we can

store the id203 of each possible transition in an s   s matrix t(cid:101)x , where s is the number of

states.

(cid:0)t(cid:101)x(cid:1)jk := p(cid:101)x(i+1)|(cid:101)x(i) (xj|xk) .

(7.5)

in this chapter we focus on time-homogeneous    nite-state markov chains. the transition prob-
abilities of these chains can be visualized using a state diagram, which shows each state and the
id203 of every possible transition. see figure 7.2 below for an example. the state diagram
should not be confused with the directed acyclic graph (dag) that represents the dependence
structure of the model, illustrated in figure 7.1. in the state diagram, each node corresponds to
a state and the edges to transition probabilities between states, whereas the dag just indicates
the dependence structure of the random process in time and is usually the same for all markov
chains.

to simplify notation we de   ne an s-dimensional vector (cid:126)p(cid:101)x(i) called the state vector, which

contains the marginal pmf of the markov chain at each time i,

(cid:126)p(cid:101)x(i) :=

p(cid:101)x(i) (x1)
p(cid:101)x(i) (x2)
p(cid:101)x(i) (xs)

      

                        

                        

.

(7.6)

each entry in the state vector contains the id203 that the markov chain is in that particular
state at time i. it is not the value of the markov chain, which is a random variable.

the initial state space (cid:126)p(cid:101)x(0) and the transition matrix t(cid:101)x su   ce to completely specify a time-
chain at any n time points i1, i2, . . . , in for any n     1 from (cid:126)p(cid:101)x(0) and t(cid:101)x by applying (7.4) and

homogeneous    nite-state markov chain. indeed, we can compute the joint distribution of the

marginalizing over any times that we are not interested in. we illustrate this in the following
example.

x1x2x3x4x5...chapter 7. markov chains

122

figure 7.2: state diagram of the markov chain described in example (7.1.1) (top). each arrow shows
the id203 of a transition between the two states. below we show three realizations of the markov
chain.

example 7.1.1 (car rental). a car-rental company hires you to model the location of their
cars. the company operates in los angeles, san francisco and san jose. customers regularly
take a car in a city and drop it o    in another. it would be very useful for the company to be
able to compute how likely it is for a car to end up in a given city. you decide to model the
location of the car as a markov chain, where each time step corresponds to a new customer
taking the car. the company allocates new cars evenly between the three cities. the transition
probabilities, obtained from past data, are given by

san francisco los angeles san jose

(cid:32)

0.6
0.2
0.2

0.1
0.8
0.1

0.3
0.3
0.4

(cid:33)

san francisco
los angeles

san jose

to be clear, the id203 that a customer moves the car from san francisco to la is 0.2, the

sflasj0.20.20.60.10.80.10.30.30.4051015customersf la sj 051015customersf la sj 051015customersf la sj chapter 7. markov chains

123

id203 that the car stays in san francisco is 0.6, and so on.

the initial state vector and the transition matrix of the markov chain are

(cid:126)p(cid:101)x(0) :=               

1/3

1/3

,

1/3

               

t(cid:101)x :=               

0.6 0.1 0.3

0.2 0.1 0.4

               

0.2 0.8 0.3

.

(7.7)

state 1 is assigned to san francisco, state 2 to los angeles and state 3 to san jose. figure 7.2
shows a state diagram of the markov chain. figure 7.2 shows some realizations of the markov
chain.

the company wants to    nd out the id203 that the car starts in san francisco, but is in
san jose right after the second customer. this is given by

p(cid:101)x(0),(cid:101)x(2) (1, 3) =

=

3(cid:88)i=1
3(cid:88)i=1
=(cid:16)(cid:126)p(cid:101)x(0)(cid:17)1

p(cid:101)x(0),(cid:101)x(1),(cid:101)x(2) (1, i, 3)
p(cid:101)x(0) (1) p(cid:101)x(1)|(cid:101)x(0) (i|1) p(cid:101)x(2)|(cid:101)x(1) (3|i)

3(cid:88)i=1(cid:0)t(cid:101)x(cid:1)i1(cid:0)t(cid:101)x(cid:1)3i

0.6    0.2 + 0.2    0.1 + 0.2    0.4

3

    7.33 10   2.

=

the id203 is 7.33%.

(cid:52)
the following lemma provides a simple expression for the state vector at time i (cid:126)p(cid:101)x(i) in terms
of t(cid:101)x and the previous state vector.
lemma 7.1.2 (state vector and transition matrix). for a markov chain (cid:101)x with transition
matrix t(cid:101)x

if the markov chain starts at time 0 then

(cid:126)p(cid:101)x(i) = t(cid:101)x (cid:126)p(cid:101)x(i   1).
(cid:126)p(cid:101)x(0),
(cid:126)p(cid:101)x(i) = t i(cid:101)x
denotes multiplying i times by matrix t(cid:101)x .

where t i(cid:101)x

(7.8)

(7.9)

(7.10)

(7.11)

(7.12)

(7.13)

chapter 7. markov chains

proof. the proof follows directly from the de   nitions,

(cid:126)p(cid:101)x(i) :=

=

      

                        

                        

j=1 p(cid:101)x(i   1) (xj) p(cid:101)x(i)|(cid:101)x(i   1) (x1|xj)
(cid:80)s
j=1 p(cid:101)x(i   1) (xj) p(cid:101)x(i)|(cid:101)x(i   1) (x2|xj)
(cid:80)s
j=1 p(cid:101)x(i   1) (xj) p(cid:101)x(i)|(cid:101)x(i   1) (xs|xj)
(cid:80)s

p(cid:101)x(i) (x1)
p(cid:101)x(i) (x2)
p(cid:101)x(i) (xs)
p(cid:101)x(i)|(cid:101)x(i   1) (x1|x1) p(cid:101)x(i)|(cid:101)x(i   1) (x1|x2)
p(cid:101)x(i)|(cid:101)x(i   1) (x2|x1) p(cid:101)x(i)|(cid:101)x(i   1) (x2|x2)
p(cid:101)x(i)|(cid:101)x(i   1) (xs|x1) p(cid:101)x(i)|(cid:101)x(i   1) (xs|x2)

      

       p(cid:101)x(i)|(cid:101)x(i   1) (x1|xs)
       p(cid:101)x(i)|(cid:101)x(i   1) (x2|xs)
      
       p(cid:101)x(i)|(cid:101)x(i   1) (xs|xs)

                        

                        
                        

=

= t(cid:101)x (cid:126)p(cid:101)x(i   1)

124

(7.14)

                        

                        

p(cid:101)x(i   1) (x1)
p(cid:101)x(i   1) (x2)
p(cid:101)x(i   1) (xs)

      

                        

(7.15)

equation (7.13) is obtained by applying (7.12) i times and taking into account the markov
property.

example 7.1.3 (car rental (continued)). the company wants to estimate the distribution of
locations right after the 5th customer has used a car. applying lemma 7.1.2 we obtain

(cid:126)p(cid:101)x(5) = t 5(cid:101)x

(cid:126)p(cid:101)x(0)

=               

0.281

0.534

0.185

               

.

(7.16)

(7.17)

the model estimates that after 5 customers more than half of the cars are in los angeles.

7.2 recurrence

the states of a markov chain can be classi   ed depending on whether the markov chain is
guaranteed to always return to them or whether it may eventually stop visiting those states.

(cid:52)

de   nition 7.2.1 (recurrent and transient states). let (cid:101)x be a time-homogeneous    nite-state

markov chain. we consider a particular state x. if

then the state is recurrent. in words, given that the markov chain is at x, the id203 that
it returns to x is one. in contrast, if

p(cid:16)(cid:101)x (j) = s for some j > i | (cid:101)x (i) = s(cid:17) = 1
p(cid:16)(cid:101)x (j) (cid:54)= s for all j > i | (cid:101)x (i) = s(cid:17) > 0

the state is transient. given that the markov chain is at x, there is nonzero id203 that it
will never return.

(7.18)

(7.19)

chapter 7. markov chains

125

figure 7.3: state diagram of the markov chain described in example (7.2.2) (top). below we show
three realizations of the markov chain.

studentemployedinternunemployed0.10.80.10.90.10.40.60.50.5202530agestud.int.emp.unemp.202530agestud.int.emp.unemp.202530agestud.int.emp.unemp.chapter 7. markov chains

126

the following example illustrates the di   erence between recurrent and transient states.

example 7.2.2 (employment dynamics). a researcher is interested in modeling the employ-
ment dynamics of young people using a markov chain.

she determines that at age 18 a person is either a student with id203 0.9 or an intern with
id203 0.1. after that she estimates the following transition probabilities:

student

intern employed unemployed

0.8
0.1
0.1
0

0.5
0.5
0
0

0
0
0.9
0.1

0
0
0.4
0.6

         

student
intern

employed

unemployed

         

the markov assumption is obviously not completely precise, someone who has been a student
for longer is probably less likely to remain a student, but such markov models are easier to    t
(we only need to estimate the transition probabilities) and often yield useful insights.

the initial state vector and the transition matrix of the markov chain are

(cid:126)p(cid:101)x(0) :=

t(cid:101)x :=

,

.

(7.20)

0.9

0.1

0

0

                        

                        

                        

0.8 0.5

0.1 0.5

0

0

0

0

0.1

0

0

0

0.9 0.4

0.1 0.6

                        

figure 7.3 shows the state diagram and some realizations of the markov chain.

states 1 (student) and 2 (intern) are transient states. note that the id203 that the markov
chain returns to those states after visiting state 3 (employed) is zero, so

p(cid:16)(cid:101)x (j) (cid:54)= 1 for all j > i | (cid:101)x (i) = 1(cid:17)     p(cid:16)(cid:101)x (i + 1) = 3 |(cid:101)x (i) = 1(cid:17)
p(cid:16)(cid:101)x (j) (cid:54)= 2 for all j > i | (cid:101)x (i) = 2(cid:17)     p(cid:16)(cid:101)x (i + 2) = 3 |(cid:101)x (i) = 2(cid:17)

= 0.5    0.1 > 0.

= 0.1 > 0,

(7.21)

(7.22)

(7.23)

(7.24)

in contrast, states 3 and 4 (unemployed) are recurrent. we prove this for state 3 (the argument
for state 4 is exactly the same):

p(cid:16)(cid:101)x (j) (cid:54)= 3 for all j > i | (cid:101)x (i) = 3(cid:17)
= p(cid:16)(cid:101)x (j) = 4 for all j > i |(cid:101)x (i) = 3(cid:17)
p(cid:16)(cid:101)x (i + 1) = 4 |(cid:101)x (i) = 3(cid:17) k(cid:89)j=1

0.1    0.6k

= lim
k      
= lim
k      

= 0.

p(cid:16)(cid:101)x (i + j + 1) = 4 |(cid:101)x (i + j) = 4(cid:17)

(7.25)

(7.26)

(7.27)

(7.28)

(7.29)

chapter 7. markov chains

127

(cid:52)
in this example, it is not possible to reach the states student and intern from the states employed
or unemployed. markov chains for which there is a possible transition between any two states
(even if it is not direct) are called irreducible.

de   nition 7.2.3 (irreducible markov chain). a time-homogeneous    nite-state markov chain
is irreducible if for any state x, the id203 of reaching every other state y (cid:54)= x in a    nite
number of steps is nonzero, i.e. there exists m     0 such that

p(cid:16)(cid:101)x (i + m) = y | (cid:101)x (i) = x(cid:17) > 0.

one can easily check that the markov chain in example 7.1.1 is irreducible, whereas the one
in example 7.2.2. an important result is that all states in an irreducible markov chain are
recurrent.

theorem 7.2.4 (irreducible markov chains). all states in an irreducible markov chain are
recurrent.

proof. in any    nite-state markov chain there must be at least one state that is recurrent. if
all the states are transient there is a nonzero id203 that it leaves all of the states forever,
which is not possible. without loss of generality let us assume that state x is recurrent. we now
provide a sketch of a proof that another arbitrary state y must also be recurrent. to alleviate
notation let

px,x := p(cid:16)(cid:101)x (j) = x for some j > i | (cid:101)x (i) = x(cid:17) ,
px,y := p(cid:16)(cid:101)x (j) = y for some j > i | (cid:101)x (i) = x(cid:17) ,
py,x := p(cid:16)(cid:101)x (k) = x for some j > i | (cid:101)x (i) = y(cid:17) .

the chain is irreducible so there is a nonzero id203 pm > 0 of reaching y from x in at most
m steps for some m > 0. the id203 that the chain goes from x to y and never goes back
to x is consequently at least pm (1     py,x). however, x is recurrent, so this id203 must be
zero! since pm > 0 this implies py,x = 1.
consider the following event:

1. (cid:101)x goes from y to x.
2. (cid:101)x does not return to y in m steps after reaching x.
3. (cid:101)x eventually reaches x again at a time m(cid:48) > m.
the id203 of this event is equal to py,x (1     pm) px,x = 1    pm (recall that x is recurrent so
px,x = 1). now imagine that steps 2 and 3 repeat k times, i.e. that (cid:101)x fails to go from x to y in
m steps k times. the id203 of this event is py,x (1     pm)k pk
x,x = (1     pm)k. taking k        
this is equal to zero for any m so the id203 that (cid:101)x does not eventually return to x must

be zero (this can be made rigorous, but the details are beyond the scope of these notes).

(7.30)

(7.31)

(7.32)

(7.33)

chapter 7. markov chains

128

figure 7.4: state diagram of a markov chain where states the states have period two.

7.3 periodicity

another important consideration is whether the markov chain always visits a given state at
regular intervals. if this is the case, then the state has a period greater than one.

de   nition 7.3.1 (period of a state). let (cid:101)x be a time-homogeneous    nite-state markov chain

and x a state of the markov chain. the period m of x is the largest integer such that it is only
possible to return to x in a number of steps that is a multiple of m, i.e. we can only return in
km steps with nonzero id203 where k is a positive integer.

figure 7.4 shows a markov chain where the states have a period equal to two. aperiodic markov
chains do not contain states with periods greater than one.

de   nition 7.3.2 (aperiodic markov chain). a time-homogeneous    nite-state markov chain (cid:101)x

is aperiodic if all states have period equal to one.

the markov chains in examples 7.1.1 and 7.2.2 are both aperiodic.

7.4 convergence

in this section we study under what conditions a    nite-state time-homogeneous markov chain

(cid:101)x converges in distribution. if a markov chain converges in distribution, then its state vector
(cid:126)p(cid:101)x(i), which contains the    rst order pmf of (cid:101)x, converges to a    xed vector (cid:126)p   ,

(7.34)

(cid:126)p(cid:101)x(i).

(cid:126)p    := lim
i      

in that case the id203 of the markov chain being in each state eventually tends to a    xed
value (which does not imply that the markov chain will stay at a given state!).

by lemma 7.1.2 we can express (7.34) in terms of the initial state vector and the transition
matrix of the markov chain

t i(cid:101)x

(cid:126)p(cid:101)x(0).

(cid:126)p    = lim
i      

computing this limit analytically for a particular t(cid:101)x and (cid:126)p(cid:101)x(0) may seem challenging at    rst
sight. however, it is often possible to leverage the eigendecomposition of the transition matrix
(if it exists) to    nd (cid:126)p   . this is illustrated in the following example.

(7.35)

abc10.10.91chapter 7. markov chains

129

figure 7.5: state diagram of the markov chain described in example (7.4.1) (top). below we show
three realizations of the markov chain.

example 7.4.1 (mobile phones). a company that makes mobile phones wants to model the
sales of a new model they have just released. at the moment 90% of the phones are in stock,
10% have been sold locally and none have been exported. based on past data, the company
determines that each day a phone is sold with id203 0.2 and exported with id203 0.1.
the initial state vector and the transition matrix of the markov chain are

(cid:126)a :=               

0.9

0.1

0

,

               

t(cid:101)x =               

0.7 0 0

0.1 0 1

               

0.2 1 0

.

(7.36)

we have used (cid:126)a to denote (cid:126)p(cid:101)x(0) because later we will consider other possible initial state vectors.

figure 7.6 shows the state diagram and some realizations of the markov chain.

the company is interested in the fate of the new model. in particular, it would like to compute
what fraction of mobile phones will end up exported and what fraction will be sold locally. this

instocksoldexported0.20.10.71105101520dayin stocksoldexported05101520dayin stocksoldexported05101520dayin stocksoldexportedchapter 7. markov chains

130

(cid:126)a

(cid:126)b

(cid:126)c

figure 7.6: evolution of the state vector of the markov chain in example (7.4.1) for di   erent values of

the initial state vector (cid:126)p(cid:101)x(0).

is equivalent to computing

lim
i      

(cid:126)p(cid:101)x(i) = lim

i      
= lim
i      

(cid:126)p(cid:101)x(0)

(cid:126)a.

t i(cid:101)x
t i(cid:101)x

the transition matrix t(cid:101)x has three eigenvectors

(cid:126)q1 :=      

0
0
1

       ,

(cid:126)q2 :=      

0
1
0

(cid:126)q3 :=      

0.80
   0.53
   0.27

       ,
   :=      

       .
       ,

q :=(cid:2)(cid:126)q1 (cid:126)q2 (cid:126)q3(cid:3) ,

  1
0
0

0
  2
0

0
0
  3

so that the eigendecomposition of t(cid:101)x is

t(cid:101)x := q  q   1.

the corresponding eigenvalues are   1 := 1,   2 := 1 and   3 := 0.7. we gather the eigenvectors
and eigenvalues into two matrices

(7.37)

(7.38)

(7.39)

(7.40)

(7.41)

it will be useful to express the initial state vector (cid:126)a in terms of the di   erent eigenvectors. this
is achieved by computing

so that

q   1(cid:126)p(cid:101)x(0) =      

0.3
0.7

1.122

       ,

(cid:126)a = 0.3 (cid:126)q1 + 0.7 (cid:126)q2 + 1.122 (cid:126)q3.

(7.42)

(7.43)

05101520day0.00.20.40.60.81.005101520day0.00.20.40.60.81.0in stocksoldexported05101520day0.00.20.40.60.81.0chapter 7. markov chains

we conclude that

t i(cid:101)x

lim
i      

this means that eventually the id203 that each phone has been sold locally is 0.7 and the
id203 that it has been exported is 0.3. the left graph in figure 7.6 shows the evolution of
the state vector. as predicted, it eventually converges to the vector in equation (7.49).

in general, because of the special structure of the two eigenvectors with eigenvalues equal to one
in this example, we have

this is illustrated in figure 7.6 where you can see the evolution of the state vector if it is
initialized to these other two distributions:

t i(cid:101)x
0.3 t i(cid:101)x

0.3    i

(0.3 (cid:126)q1 + 0.7 (cid:126)q2 + 1.122 (cid:126)q3)

(cid:126)q1 + 0.7 t i(cid:101)x
1 (cid:126)q1 + 0.7    i

(cid:126)q2 + 1.122 t i(cid:101)x
2 (cid:126)q2 + 1.122    i

3 (cid:126)q3

(cid:126)q3

0.3 (cid:126)q1 + 0.7 (cid:126)q2 + 1.122 0.5 i (cid:126)q3

(cid:126)a = lim
i      
= lim
i      
= lim
i      
= lim
i      

= 0.3 (cid:126)q1 + 0.7 (cid:126)q2

=      

0
0.7
0.3

       .

0

(cid:126)p(cid:101)x(0) =            
       ,
       ,

             .
(cid:16)q   1(cid:126)p(cid:101)x(0)(cid:17)2
(cid:16)q   1(cid:126)p(cid:101)x(0)(cid:17)1
       ,
q   1(cid:126)b =      
       .
q   1(cid:126)c =      

0.23
0.77
0.50

0.6
0.4
0.75

0.6
0
0.4

0.4
0.5
0.1

t i(cid:101)x

lim
i      

(cid:126)b :=      
(cid:126)c :=      

(cid:52)
the transition matrix of the markov chain in example 7.4.1 has two eigenvectors with eigenvalue
equal to one. if we set the initial state vector to equal either of these eigenvectors (note that we
must make sure to normalize them so that the state vector contains a valid pmf) then

t(cid:101)x (cid:126)p(cid:101)x(0) = (cid:126)p(cid:101)x(0),

so that

(cid:126)p(cid:101)x(0)

(cid:126)p(cid:101)x(i) = t i(cid:101)x
= (cid:126)p(cid:101)x(0)

131

(7.44)

(7.45)

(7.46)

(7.47)

(7.48)

(7.49)

(7.50)

(7.51)

(7.52)

(7.53)

(7.54)

(7.55)

132

(7.56)

chapter 7. markov chains

for all i. in particular,

(cid:126)p(cid:101)x(i) = (cid:126)p(cid:101)x(0),

lim
i      

is an eigenvector associated to an eigenvalue equal to one, so that

   es (7.56) is called a stationary distribution of the markov chain.

so (cid:126)x converges to a random variable with pmf (cid:126)p(cid:101)x(0) in distribution. a distribution that satis-
de   nition 7.4.2 (stationary distribution). let (cid:101)x be a    nite-state time-homogeneous markov
chain and let (cid:126)pstat be a state vector containing a valid pmf over the possible states of (cid:101)x. if (cid:126)pstat
then the distribution corresponding to (cid:126)pstat is a stationary or steady-state distribution of (cid:101)x.

establishing whether a distribution is stationary by checking whether (7.57) holds may be chal-
lenging computationally if the state space is very large. we now derive an alternative condition
that implies stationarity. let us    rst de   ne reversibility of markov chains.

t(cid:101)x (cid:126)pstat = (cid:126)pstat,

(7.57)

de   nition 7.4.3 (reversibility). let (cid:101)x be a    nite-state time-homogeneous markov chain with
s states and transition matrix t(cid:101)x . assume that (cid:101)x (i) is distributed according to the state vector
(cid:126)p     rs. if
p(cid:16)(cid:101)x (i) = xj, (cid:101)x (i + 1) = xk(cid:17) = p(cid:16)(cid:101)x (i) = xk, (cid:101)x (i + 1) = xj(cid:17) ,
then we say that (cid:101)x is reversible with respect to (cid:126)p. this is equivalent to the detailed-balance

for all 1     j, k     s, (7.58)

condition

for all 1     j, k     s.

(7.59)

(cid:0)t(cid:101)x(cid:1)kj (cid:126)pj =(cid:0)t(cid:101)x(cid:1)jk (cid:126)pk,

as proved in the following theorem, reversibility implies stationarity, but the converse does not
hold. a markov chain is not necessarily reversible with respect to a stationary distribution (and
often will not be). the detailed-balance condition therefore only provides a su   cient condition
for stationarity.

theorem 7.4.4 (reversibility implies stationarity). if a time-homogeneous markov chain (cid:101)x is
reversible with respect to a distribution px , then px is a stationary distribution of (cid:101)x.
proof. let (cid:126)p be the state vector containing px . by assumption t(cid:101)x and (cid:126)p satisfy (7.59), so for
1     j     s

(cid:0)t(cid:101)x (cid:126)p(cid:1)j =

=

s(cid:88)k=1(cid:0)t(cid:101)x(cid:1)jk (cid:126)pk
s(cid:88)k=1(cid:0)t(cid:101)x(cid:1)kj (cid:126)pj
s(cid:88)k=1(cid:0)t(cid:101)x(cid:1)kj

= (cid:126)pj

= (cid:126)pj.

(7.60)

(7.61)

(7.62)

(7.63)

chapter 7. markov chains

133

(cid:126)p(cid:101)x(0) =      

1/3
1/3
1/3

      

(cid:126)p(cid:101)x(0) =      

1
0
0

      

(cid:126)p(cid:101)x(0) =      

0.1
0.2
0.7

      

figure 7.7: evolution of the state vector of the markov chain in example (7.4.7).

the last step follows from the fact that the columns of a valid transition matrix must add to
one (the chain always has to go somewhere).

in example 7.4.1 the markov chain has two stationary distributions. it turns out that this is
not possible for irreducible markov chains.

theorem 7.4.5. irreducible markov chains have a single stationary distribution.

proof. this follows from the perron-frobenius theorem, which states that the transition ma-
trix of an irreducible markov chain has a single eigenvector with eigenvalue equal to one and
nonnegative entries.

if in addition, the markov chain is aperiodic, then it is guaranteed to converge in distribution
to a random variable with its stationary distribution for any initial state vector. such markov
chains are called ergodic.

theorem 7.4.6 (convergence of markov chains). if a discrete-time time-homogeneous markov

chain (cid:101)x is irreducible and aperiodic its state vector converges to the stationary distribution (cid:126)pstat
of (cid:101)x for any initial state vector (cid:126)p(cid:101)x(0). this implies that (cid:101)x converges in distribution to a random

variable with pmf given by (cid:126)pstat.

the proof of this result is beyond the scope of these notes.

example 7.4.7 (car rental (continued)). the markov chain in the car rental example is irre-
ducible and aperiodic. we will now check that it indeed converges in distribution. its transition
matrix has the following eigenvectors

(cid:126)q1 :=      

0.273
0.545
0.182

       ,

(cid:126)q2 :=      

   0.577
0.789
   0.211

       ,

(cid:126)q3 :=      

   0.577
   0.211
0.789

       .

(7.64)

the corresponding eigenvalues are   1 := 1,   2 := 0.573 and   3 := 0.227. as predicted by
theorem 7.4.5 the markov chain has a single stationary distribution.

05101520customer0.00.20.40.60.81.0sflasj05101520customer0.00.20.40.60.81.0sflasj05101520customer0.00.20.40.60.81.0sflasjchapter 7. markov chains

134

for any initial state vector, the component that is collinear with (cid:126)q1 will be preserved by the
transitions of the markov chain, but the other two components will become negligible after
a while. the chain consequently converges in distribution to a random variable with pmf (cid:126)q1
(note that (cid:126)q1 has been normalized to be a valid pmf), as predicted by theorem 7.4.6. this is
illustrated in figure 7.7. no matter how the company allocates the new cars, eventually 27.3%
(cid:52)
will end up in san francisco, 54.5% in la and 18.2% in san jose.

7.5 markov-chain monte carlo

the convergence of markov chains to a stationary distribution is very useful for simulating
random variables. markov-chain monte carlo (mcmc) methods generate samples from a target
distribution by constructing a markov chain in such a way that the stationary distribution equals
the desired distribution. these techniques are of huge importance in modern statistics and in
particular in bayesian modeling. in this section we describe one of the most popular mcmc
methods and illustrate it with a simple example.

the key challenge in mcmc methods is to design an irreducible aperiodic markov chain for
which the target distribution is stationary. the metropolis-hastings algorithm uses an auxiliary
markov chain to achieve this.

algorithm 7.5.1 (metropolis-hastings algorithm). we store the pmf px of the target distri-

bution in a vector (cid:126)p     rs, such that

(cid:126)pj := px (xj) ,

1     j     s.

(7.65)

let t denote the transition matrix of an irreducible markov chain with the same state space
{x1, . . . , xs} as (cid:126)p.

initialize (cid:101)x (0) randomly or to a    xed state, then repeat the following steps for i = 1, 2, 3, . . ..
1. generate a candidate random variable c from (cid:101)x (i     1) by using the transition matrix t ,

i.e.

1     j, k     s.

with id203 pacc(cid:16)(cid:101)x (i     1) , c(cid:17),

p(cid:16)c = k | (cid:101)x (i     1) = j(cid:17) = tkj,
(cid:101)x (i) :=(cid:40)c
(cid:101)x (i     1) otherwise,
pacc (j, k) := min(cid:26) tjk (cid:126)pk
, 1(cid:27)

tkj (cid:126)pj

1     j, k     s.

(7.66)

(7.67)

(7.68)

where the acceptance id203 is de   ned as

2. set

it turns out that this algorithm yields a markov chain that is reversible with respect to the
distribution of interest, which ensures that the distribution is stationary.

theorem 7.5.2. the pmf in (cid:126)p corresponds to a stationary distribution of the markov chain (cid:101)x

obtained by the metropolis-hastings algorithm.

chapter 7. markov chains

proof. we show that the markov chain (cid:101)x is reversible with respect to (cid:126)p, i.e. that

(cid:0)t(cid:101)x(cid:1)kj (cid:126)pj =(cid:0)t(cid:101)x(cid:1)jk (cid:126)pk,

holds for all 1     j, k     s. this establishes the result by theorem 7.4.4. the detailed-balanced
condition holds trivially if j = k. if j (cid:54)= k we have

(cid:0)t(cid:101)x(cid:1)kj := p(cid:16)(cid:101)x (i) = k | (cid:101)x (i     1) = j(cid:17)

= p(cid:16)(cid:101)x (i) = c, c = k | (cid:101)x (i     1) = j(cid:17)
= p(cid:16)(cid:101)x (i) = c | c = k, (cid:101)x (i     1) = j(cid:17) p(cid:16)c = k | (cid:101)x (i     1) = j(cid:17)
and by exactly the same argument(cid:0)t(cid:101)x(cid:1)jk = pacc (k, j) tjk. we conclude that

= pacc (j, k) tkj

tkj (cid:126)pj
= min{tjk (cid:126)pk, tkj (cid:126)pj}
tkj (cid:126)pj

(cid:0)t(cid:101)x(cid:1)kj (cid:126)pj = pacc (j, k) tkj (cid:126)pj
= tkj (cid:126)pj min(cid:26) tjk (cid:126)pk
= tjk (cid:126)pk min(cid:26)1,
=(cid:0)t(cid:101)x(cid:1)jk (cid:126)pk.

, 1(cid:27)
tjk (cid:126)pk(cid:27)

= pacc (k, j) tjk (cid:126)pk

135

(7.69)

(7.70)

(7.71)

(7.72)

(7.73)

(7.74)

(7.75)

(7.76)

(7.77)

(7.78)

(7.79)

the following example is taken from hastings   s seminal paper monte carlo sampling methods
using markov chains and their applications.

example 7.5.3 (generating a poisson random variable). our aim is to generate a poisson
random variable x. note that we don   t need to know the normalizing constant in the poisson
pmf, which equals to e  , as long as we know that it is proportional to

px (x)    

  x
x!

(7.80)

the auxiliary markov chain must be able to reach any possible value of x, i.e. all positive
integers. we will use a modi   ed random walk that takes steps upwards and downwards with
id203 1/2, but never goes below 0. its transition matrix equals

tkj :=

1
2
1
2
1
2
0

                                 

if j = 0 and k = 0,
if k = j + 1,
if j > 0 and k = j     1,
otherwise.

(7.81)

chapter 7. markov chains

t is symmetric so the acceptance id203 is equal to the ratio of the pmfs:

pacc (j, k) := min(cid:26) tjk px (k)
, 1(cid:27)
= min(cid:26) px (k)
, 1(cid:27) .

tkj px (j)

px (j)

136

(7.82)

(7.83)

to compute the acceptance id203, we only consider transitions that are possible under the
random walk. if j = 0 and k = 0

if k = j + 1

if k = j     1

pacc (j, k) = 1.

  j
j!

j + 1

  j+1
(j+1)!

pacc (j, j + 1) = min         
= min(cid:26)   
pacc (j, j     1) = min         
= min(cid:26) j

, 1         
, 1(cid:27) .
, 1         
, 1(cid:27) .

  j   1
(j   1)!

  j
j!

  

(7.84)

(7.85)

(7.86)

(7.87)

(7.88)

we now spell out the steps of the metropolis-hastings method. to simulate the auxiliary random
walk we use a sequence of bernoulli random variables that indicate whether the random walk
is trying to go up or down (or stay at zero). we initialize the chain at x0 = 0. then, for
i = 1, 2, . . ., we

    generate a sample b from a bernoulli distribution with parameter 1/2 and a sample u

uniformly distributed in [0, 1].

    if b = 0:

    if xi   1 = 0, xi := 0.
    if xi   1 > 0:

    if u < xi   1
    otherwise xi := xi   1.

   , xi := xi   1     1.

    if b = 1:

xi   1+1 , xi := xi   1 + 1.

    if u <   
    otherwise xi := xi   1.

chapter 7. markov chains

137

figure 7.8: convergence in distribution of the markov chain constructed in example 7.8 for    := 6.
to prevent clutter we only plot the empirical distribution of 6 states, computed by running the markov
chain 104 times.

the markov chain that we have built is irreducible: there is nonzero id203 of going from
any nonnegative integer to any other nonnegative integer (although it could take a while!). we
have not really proved that the chain should converge to the desired distribution, since we have
not discussed convergence of markov chains with in   nite state spaces, but figure 7.8 shows that
the method indeed allows to sample from a poisson distribution with    := 6.

(cid:52)
for the example in figure 7.8, approximate convergence in distribution occurs after around 100
iterations. this is called the mixing time of the markov chain. to account for it, mcmc
methods usually discard the samples from the chain over an initial period known as burn-in
time.

the careful reader might be wondering about the point of using mcmc methods if we already
have access to the desired distribution. it seems much simpler to just apply the method described
in section 2.6.1 instead. however, the metropolis-hastings method can be applied to discrete
distributions with in   nite supports and also to continuous distributions (justifying this is beyond
the scope of these notes). crucially, in contrast with inverse-transform and rejection sampling,
metropolis-hastings does not require having access to the pmf px or pdf fx of the target
distribution, but rather to the ratio px (x) /px (y) or fx (x) /fx (y) for every x (cid:54)= y. this is
very useful when computing conditional distributions within probabilistic models.

imagine that we have access to the marginal distribution of a continuous random variable a and
the conditional distribution of another continuous random variable b given a. computing the

100101102103iterations0.000.050.100.150.200.250.300.35distribution012345chapter 7. markov chains

conditional pdf

fa|b (a|b) =

fa (a) fb|a (b|a)
fa (u) fb|a (b|u) du

(cid:82)    u=      

is not necessary feasible due to the integral in the denominator. however, if we apply metropolis-
hastings to sample from fa|b we don   t need to compute the normalizing factor since for any
a1 (cid:54)= a2

fa|b (a1|b)
fa|b (a2|b)

=

fa (a1) fb|a (b|a1)
fa (a2) fb|a (b|a2)

.

(7.90)

138

(7.89)

chapter 8

descriptive statistics

in this chapter we describe several techniques for visualizing data, as well as for computing
quantities that summarize it e   ectively. such quantities are known as descriptive statistics. as
we will see in the following chapters, these statistics can often be interpreted within a proba-
bilistic framework, but they are also useful when probabilistic assumptions are not warranted.
because of this, we present them from a deterministic point of view.

8.1 histogram

we begin by considering data sets containing one-dimensional data. one of the most natural
ways of visualizing 1d data is to plot their histogram. the histogram is obtained by binning the
range of the data and counting the number of instances that fall within each bin. the width of
the bins is a parameter that can be adjusted to yield higher or lower resolution. if we interpret
the data as corresponding to samples from a random variable, then the histogram would be a
piecewise constant approximation to their pmf or pdf.

figure 8.1 shows two histograms computed from temperature data gathered at a weather station
in oxford over 150 years.1 each data point represents the maximum temperature recorded in
january or august of a particular year. figure 8.2 shows a histogram of the gdp per capita of
all countries in the world in 2014 according to the united nations.2

8.2 sample mean and variance

averaging the elements in a one-dimensional data set provides a one-number summary of the
data, which is a deterministic counterpart to the mean of a random variable (recall that we are
making no probabilistic assumptions in this chapter). this can be extended to multi-dimensional
data by averaging over each dimension separately.

de   nition 8.2.1 (sample mean). let {x1, x2, . . . , xn} be a set of real-valued data. the sample
1the data
oxforddata.txt.
2the data is available at http://unstats.un.org/unsd/snaama/selbasicfast.asp.

at http://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/

available

is

139

chapter 8. descriptive statistics

140

figure 8.1: histograms of temperature data taken in a weather station in oxford over 150 years. each
data point equals the maximum temperature recorded in a certain month in a particular year.

figure 8.2: histogram of the gdp per capita of all countries in the world in 2014.

051015202530degrees (celsius)051015202530354045januaryaugust050100150200thousands of dollars0102030405060708090chapter 8. descriptive statistics

141

uncentered data

centered data

figure 8.3: e   ect of centering a two-dimensional data set. the axes are depicted using dashed lines.

mean of the data is de   ned as

av (x1, x2, . . . , xn) :=

1
n

n(cid:88)i=1

xi.

(8.1)

let {(cid:126)x1, (cid:126)x2, . . . , (cid:126)xn} be a set of d-dimensional real-valued data vectors. the sample mean is

av ((cid:126)x1, (cid:126)x2, . . . , (cid:126)xn) :=

1
n

n(cid:88)i=1

(cid:126)xi.

(8.2)

the sample mean of the data in figure 8.1 is 6.73    c in january and 21.3    c in august. the
sample mean of the gdps per capita in figure 8.2 is $16,500.

geometrically, the average, also known as the sample mean, is the center of mass of the data. a
common preprocessing step in data analysis is to center a set of data by subtracting its sample
mean. figure 8.3 shows an example.

algorithm 8.2.2 (centering). let (cid:126)x1, . . . , (cid:126)xn be a set of d-dimensional data. to center the
data set we:

1. compute the sample mean following de   nition 8.2.1.

2. subtract the sample mean from each vector of data. for 1     i     n

(cid:126)yi := (cid:126)xi     av ((cid:126)x1, (cid:126)x2, . . . , (cid:126)xn) .

(8.3)

the resulting data set (cid:126)y1, . . . , (cid:126)yn has sample mean equal to zero; it is centered at the origin.

the sample variance is the average of the squared deviations from the sample mean. geomet-
rically, it quanti   es the average variation of the data set around its center. it is a deterministic
counterpart to the variance of a random variable.

chapter 8. descriptive statistics

142

de   nition 8.2.3 (sample variance and standard deviation). let {x1, x2, . . . , xn} be a set of
real-valued data. the sample variance is de   ned as

var (x1, x2, . . . , xn) :=

1

n     1

n(cid:88)i=1

(xi     av (x1, x2, . . . , xn))2

the sample standard deviation is the square root of the sample variance

std (x1, x2, . . . , xn) :=(cid:112)var (x1, x2, . . . , xn).

(8.4)

(8.5)

you might be wondering why the normalizing constant is 1/ (n     1) instead of 1/n. the reason
is that this ensures that the expectation of the sample variance equals the true variance when
the data are iid (see lemma 9.2.5). in practice there is not much di   erence between the two
id172s.
the sample standard deviation of the temperature data in figure 8.1 is 1.99    c in january and
1.73    c in august. the sample standard deviation of the gdp data in figure 8.2 is $25,300.

8.3 order statistics

in some cases, a data set is well described by its mean and standard deviation.
in january the temperature in oxford is around 6.73    c give or take 2    c.
this a pretty accurate account of the temperature data from the previous section. however,
imagine that someone describes the gdp data set in figure 8.2 as:

countries typically have a gdp per capita of about $16 500 give or take $25 300.

this description is pretty terrible. the problem is that most countries have very small gdps
per capita, whereas a few have really large ones and the sample mean and standard deviation
don   t really convey this information. order statistics provide an alternative description, which
is usually more informative when there are extreme values in the data.

de   nition 8.3.1 (quantiles and percentiles). let x(1)     x(2)     . . .     x(n) denote the ordered
elements of a set of data {x1, x2, . . . , xn}. the q quantile of the data for 0 < q < 1 is x([q(n+1)]),
where [q (n + 1)] is the result of rounding q (n + 1) to the closest integer. the 100 p quantile is
known as the p percentile.

the 0.25 and 0.75 quantiles are known as the    rst and third quartiles, whereas the 0.5 quantile
is known as the sample median. a quarter of the data are smaller than the 0.25 quantile, half
are smaller (or larger) than the median and three quarters are smaller than the 0.75 quartile. if
n is even, the sample median is usually set to

x(n/2) + x(n/2+1)

2

.

(8.6)

the di   erence between the third and the    rst quartile is known as the interquartile range
(iqr).

it turns out that for the temperature data set in figure 8.1 the sample median is 6.80    c in
january and 21.2    c in august, which is essentially the same as the sample mean. the iqr is

chapter 8. descriptive statistics

143

figure 8.4: box plots of the oxford temperature data set used in figure 8.1. each box plot corresponds
to the maximum temperature in a particular month (january, april, august and november) over the
last 150 years.

2.9    c in january and 2.1    c in august. this gives a very similar spread around the median, as
the sample mean. in this particular example, there does not seem to be an advantage in using
order statistics.

for the gdp data set, the median is $6,350. this means that half of the countries have a gdp
of less than $6,350. in contrast, 71% of the countries have a gdp per capita lower than the
sample mean! the iqr of these data is $18,200. to provide a more complete description of the
data set, we can list a    ve-number summary of order statistics: the minimum x(1), the    rst
quartile, the sample median, the third quartile and the maximum x(n). for the gdp data set
these are $130, $1,960, $6,350, $20,100, and $188,000 respectively.

we can visualize the main order statistics of a data set by using a box plot, which shows the
median value of the data enclosed in a box. the bottom and top of the box are the    rst and
third quartiles. this way of visualizing a data set was proposed by the mathematician john
tukey. tukey   s box plot also includes whiskers. the lower whisker is a line extending from the
bottom of the box to the smallest value within 1.5 iqr of the    rst quartile. the higher whisker
extends from the top of the box to the highest value within 1.5 iqr of the third quartile. values
beyond the whiskers are considered outliers and are plotted separately.

figure 8.4 applies box plots to visualize the temperature data set used in figure 8.1. each box
plot corresponds to the maximum temperature in a particular month (january, april, august
and november) over the last 150 years. the box plots allow us to quickly compare the spread
of temperatures in the di   erent months. figure 8.5 shows a box plot of the gdp data from
figure 8.2. from the box plot it is immediately apparent that most countries have very small
gdps per capita, that the spread between countries increases for larger gdps per capita and
that a small number of countries have very large gdps per capita.

januaryaprilaugustnovember5051015202530degrees (celsius)chapter 8. descriptive statistics

144

figure 8.5: box plot of the gdp per capita of all countries in the world in 2014. not all of the outliers
are shown.

8.4 sample covariance

in the previous sections we mostly considered data sets consisting of one-dimensional data
(except when we discussed the sample mean of a multidimensional data set).
in machine-
learning lingo, there was only one feature per data point. we now study a multidimensional
scenario, where there are several features associated to each data point.

if the dimension of the data set equals to two (i.e. there are two features per data point), we
can visualize the data using a scatter plot, where each axis represents one of the features.
figure 8.6 shows several scatter plots of temperature data. these data are the same as in
figure 8.1, but we have now arranged them to form two-dimensional data sets. in the plot on
the left, one dimension corresponds to the temperature in january and the other dimension to
the temperature in august (there is one data point per year).
in the plot on the right, one
dimension represents the minimum temperature in a particular month and the other dimension
represents the maximum temperature in the same month (there is one data point per month).
the sample covariance quanti   es whether the two features of a two-dimensional data set tend
to vary in a similar way on average, just as the covariance quanti   es the expected joint variation
of two random variables.
de   nition 8.4.1 (sample covariance). let {(x1, y1) , (x2, y2) , . . . , (xn, yn)} be a data set where
each example consists of a measurement of two di   erent features. the sample covariance is
de   ned as

cov ((x1, y1) , . . . , (xn, yn)) :=

1

n     1

n(cid:88)i=1

(xi     av (x1, . . . , xn)) (yi     av (y1, . . . , yn)) .

(8.7)

in order to take into account that each individual feature may vary on a di   erent scale, a common
preprocessing step is to normalize each feature, dividing it by its sample standard deviation.

0102030405060thousands of dollarschapter 8. descriptive statistics

145

   = 0.269

   = 0.962

figure 8.6: scatterplot of the temperature in january and in august (left) and of the maximum and
minimum monthly temperature (right) in oxford over the last 150 years.

if we normalize before computing the covariance, we obtain the sample correlation coe   cient
of the two features. one of the advantages of the correlation coe   cient is that we don   t need
to worry about the units in which the features are measured. in contrast, measuring a feature
representing distance in inches or miles can severely distort the covariance, if we don   t scale the
other feature accordingly.

de   nition 8.4.2 (sample correlation coe   cient). let {(x1, y1) , (x2, y2) , . . . , (xn, yn)} be a data
set where each example consists of two features. the sample correlation coe   cient is de   ned as

   ((x1, y1) , . . . , (xn, yn)) :=

cov ((x1, y1) , . . . , (xn, yn))

std (x1, . . . , xn) std (y1, . . . , yn)

.

(8.8)

by the cauchy-schwarz inequality (theorem b.2.4), which states that for any vectors (cid:126)a and (cid:126)b

   1    

(cid:126)at(cid:126)b

||a||2 ||b||2     1,

(8.9)

the magnitude of the sample correlation coe   cient is bounded by one. if it is equal to 1 or
-1, then the two centered data sets are collinear. the cauchy-schwarz inequality is related
to the cauchy-schwarz inequality for random variables (theorem 4.3.7), but here it applies to
deterministic vectors.

figure 8.6 is annotated with the sample correlation coe   cients corresponding to the two plots.
maximum and minimum temperatures within the same month are highly correlated, whereas
the maximum temperature in january and august within the same year are only somewhat
correlated.

16182022242628august8101214161820april5051015202530maximum temperature10505101520minimum temperaturechapter 8. descriptive statistics

146

8.5 sample covariance matrix

8.5.1 de   nition

we now consider sets of multidimensional data. in particular, we are interested in analyzing the
variation in the data. the sample covariance matrix of a data set contains the pairwise sample
covariance between every pair of features.
de   nition 8.5.1 (sample covariance matrix). let {(cid:126)x1, (cid:126)x2, . . . , (cid:126)xn} be a set of d-dimensional
real-valued data vectors.the sample covariance matrix of these data is the d    d matrix

   ((cid:126)x1, . . . , (cid:126)xn) :=

1

n     1

n(cid:88)i=1

((cid:126)xi     av ((cid:126)x1, . . . , (cid:126)xn)) ((cid:126)xi     av ((cid:126)x1, . . . , (cid:126)xn))t .

(8.10)

the (i, j) entry of the covariance matrix, where 1     i, j     d, is given by

   ((cid:126)x1, . . . , (cid:126)xn)ij =(cid:40)var (((cid:126)x1)i , . . . , ((cid:126)xn)i)

cov(cid:16)(cid:16)((cid:126)x1)i , ((cid:126)x1)j(cid:17) , . . . ,(cid:16)((cid:126)xn)i , ((cid:126)xn)j(cid:17)(cid:17)

if i = j,
if i (cid:54)= j.

(8.11)

in order to characterize the variation of a multidimensional data set around its center, we
consider its variation in di   erent directions. the average variation of the data in a certain
direction is quanti   ed by the sample variance of the projections of the data onto that direction.
let (cid:126)v be a unit-norm vector aligned with a direction of interest, the sample variance of the data
set in the direction of (cid:126)v is given by

var(cid:0)(cid:126)v t (cid:126)x1, . . . , (cid:126)v t (cid:126)xn(cid:1) =

=

1

n     1

1

n     1

n(cid:88)i=1(cid:0)(cid:126)v t (cid:126)xi     av(cid:0)(cid:126)v t (cid:126)x1, . . . , (cid:126)v t (cid:126)xn(cid:1)(cid:1)2
n(cid:88)i=1(cid:0)(cid:126)v t ((cid:126)xi     av ((cid:126)x1, . . . , (cid:126)xn))(cid:1)2

= (cid:126)v t(cid:32) 1

n     1

n(cid:88)i=1

= (cid:126)v t    ((cid:126)x1, . . . , (cid:126)xn) (cid:126)v.

((cid:126)xi     av ((cid:126)x1, . . . , (cid:126)xn)) ((cid:126)xi     av ((cid:126)x1, . . . , (cid:126)xn))t(cid:33) (cid:126)v

(8.12)

(8.13)

(8.14)

using the sample covariance matrix we can express the variation in every direction! this is
a deterministic analog of the fact that the covariance matrix of a random vector encodes its
variance in every direction.

8.5.2 principal component analysis

consider the eigendecomposition of the covariance matrix

   ((cid:126)x1, . . . , (cid:126)xn) =(cid:2)(cid:126)u1 (cid:126)u2

       (cid:126)un(cid:3)            

  1
0

0
  2

0

0

0
0

      
      
      
         n

            (cid:2)(cid:126)u1 (cid:126)u2

       (cid:126)un(cid:3)t

.

(8.15)

by de   nition,    ((cid:126)x1, . . . , (cid:126)xn) is symmetric, so its eigenvectors u1, u2, . . . , un are orthogonal. by
equation (8.14) and theorem b.7.2, the eigenvectors and eigenvalues completely characterize
the variation of the data in every direction.

chapter 8. descriptive statistics

147

  1/n = 0.497
  2/n = 0.476

  1/n = 0.967
  2/n = 0.127

  1/n = 1.820
  2/n = 0.021

figure 8.7: pca of a set consisting of n = 100 two-dimensional data points with di   erent con   gurations.

theorem 8.5.2. let the sample covariance of a set of vectors    ((cid:126)x1, . . . , (cid:126)xn) have an eigende-
composition given by (8.15) where the eigenvalues are ordered   1       2     . . .       n. then,

  1 = max
||(cid:126)v||2=1

(cid:126)u1 = arg max
||(cid:126)v||2=1
max

  k =

var(cid:0)(cid:126)v t (cid:126)x1, . . . , (cid:126)v t (cid:126)xn(cid:1) ,
var(cid:0)(cid:126)v t (cid:126)x1, . . . , (cid:126)v t (cid:126)xn(cid:1) ,

||(cid:126)v||2=1,(cid:126)u   (cid:126)u1,...,(cid:126)uk   1

(cid:126)uk = arg

max

||(cid:126)v||2=1,(cid:126)u   (cid:126)u1,...,(cid:126)uk   1

var(cid:0)(cid:126)v t (cid:126)x1, . . . , (cid:126)v t (cid:126)xn(cid:1) ,
var(cid:0)(cid:126)v t (cid:126)x1, . . . , (cid:126)v t (cid:126)xn(cid:1) .

(8.16)

(8.17)

(8.18)

(8.19)

this means that (cid:126)u1 is the direction of maximum variation. the eigenvector (cid:126)u2 corresponding
to the second largest eigenvalue   2 is the direction of maximum variation that is orthogonal
to (cid:126)u1.
in general, the eigenvector (cid:126)uk corresponding to the kth largest eigenvalue   k reveals
the direction of maximum variation that is orthogonal to (cid:126)u1, (cid:126)u2, . . . , (cid:126)uk   1. finally, (cid:126)un is the
direction of minimum variation.

in data analysis, the eigenvectors of the sample covariance matrix are usually called principal
directions. computing these eigenvectors to quantify the variation of a data set in di   erent
directions is called principal component analysis (pca). figure 8.7 shows the principal
directions for several 2d examples.

figure 8.8 illustrates the importance of centering before applying pca. theorem 8.5.2 still holds
if the data are not centered. however, the norm of the projection onto a certain direction no
longer re   ects the variation of the data. in fact, if the data are concentrated around a point
that is far from the origin, the    rst principal direction tends be aligned with that point. this
makes sense as projecting onto that direction captures more energy. as a result, the principal
directions do not re   ect the directions of maximum variation within the cloud of data. centering
the data set before applying pca solves the issue.

the following example explains how to apply principal component analysis to dimensionality re-
duction. the motivation is that in many cases directions of higher variation are more informative
about the structure of the data set.

~u1~u2~u1~u2~u1~u2chapter 8. descriptive statistics

148

  1/n = 25.78
  2/n = 0.790

  1/n = 1.590
  2/n = 0.019

uncentered data

centered data

figure 8.8: pca applied to n = 100 2d data points. on the left the data are not centered. as a result
the dominant principal direction u1 lies in the direction of the mean of the data and pca does not re   ect
the actual structure. once we center, u1 becomes aligned with the direction of maximal variation.

figure 8.9: projection of 7-dimensional vectors describing di   erent wheat seeds onto the    rst two (left)
and the last two (right) principal directions of the data set. each color represents a variety of wheat.

u1u2u2u12.52.01.51.00.50.00.51.01.52.0projection onto first pc2.52.01.51.00.50.00.51.01.52.0projection onto second pc2.52.01.51.00.50.00.51.01.52.0projection onto (d-1)th pc2.52.01.51.00.50.00.51.01.52.0projection onto dth pcchapter 8. descriptive statistics

149

(cid:126)x1, . . . , (cid:126)xn

u t (cid:126)x1, . . . , u t (cid:126)xn

        1u t (cid:126)x1, . . . ,         1u t (cid:126)xn

figure 8.10: e   ect of whitening a set of data. the original data are dominated by a linear skew (left).
applying u t aligns the axes with the eigenvectors of the sample covariance matrix (center). finally,
        1 reweights the data along those axes so that they have the same average variation, revealing the
nonlinear structure that was obscured by the linear skew (right).

example 8.5.3 (id84 via pca). we consider a data set where each data
point corresponds to a seed which has seven features: area, perimeter, compactness, length of
kernel, width of kernel, asymmetry coe   cient and length of kernel groove. the seeds belong to
three di   erent varieties of wheat: kama, rosa and canadian.3 our aim is to visualize the data
by projecting the data down to two dimensions in a way that preserves as much variation as
possible. this can be achieved by projecting each point onto the two    rst principal dimensions
of the data set.

figure 8.9 shows the projection of the data onto the    rst two and the last two principal directions.
in the latter case, there is almost no discernible variation. the structure of the data is much
better conserved by the two    rst directions, which allow to clearly visualize the di   erence between
the three types of seeds. note however that projection onto the    rst principal directions only
ensures that we preserve as much variation as possible, but it does not necessarily preserve useful
(cid:52)
features for tasks such as classi   cation.

8.5.3 whitening

whitening is a useful procedure for preprocessing data that contains nonlinear patterns. the
goal is to eliminate the linear skew in the data by rotating and contracting the data along
di   erent directions in order to reveal its underlying nonlinear structure. this can be achieved
by applying a linear transformation that essentially inverts the sample covariance matrix, so that
the result is uncorrelated. the process is known as whitening, because random vectors with
uncorrelated entries are often referred to as white noise. it is closely related to algorithm 8.5.4
for coloring random vectors.

algorithm 8.5.4 (whitening). let (cid:126)x1, . . . , (cid:126)xn be a set of d-dimensional data, which we assume
to be centered and to have a full-rank covariance matrix. to whiten the data set we:

1. compute the eigendecomposition of the sample covariance matrix    ((cid:126)x1, . . . , (cid:126)xn) = u   u t .

3the data can be found at https://archive.ics.uci.edu/ml/datasets/seeds.

chapter 8. descriptive statistics

2. set (cid:126)yi :=         1

u t (cid:126)xi, for i = 1, . . . , n, where

     1
0
0      2

0

0

      :=            

so that    ((cid:126)x1, . . . , (cid:126)xn) = u          u t .

0
0

      
      
      
            n

             ,

the whitened data set (cid:126)y1, . . . , (cid:126)yn has a sample covariance matrix equal to the identity,

   ((cid:126)y1, . . . , (cid:126)yn) :=

(cid:126)yi(cid:126)yt
i

n(cid:88)i=1
n(cid:88)i=1
        1
u t(cid:32) 1
u t    ((cid:126)x1, . . . , (cid:126)xn) u        1
u t u          u t u        1

u t (cid:126)xi(cid:16)        1
n(cid:88)i=1

n     1

(cid:126)xi(cid:126)xt

u t (cid:126)xi(cid:17)t
i(cid:33) u        1

1

n     1
1

=

n     1
=         1
=         1
=         1
= i.

150

(8.20)

(8.21)

(8.22)

(8.23)

(8.24)

(8.25)

(8.26)

intuitively, whitening    rst rotates the data and then shrinks or expands it so that the average
variation is the same in every direction. as a result, nonlinear patterns become more apparent,
as illustrated by figure 8.10.

chapter 9

frequentist statistics

the goal of statistical analysis is to extract information from data by computing statistics,
which are deterministic functions of the data. in chapter 8 we describe several statistics from
a deterministic and geometric point of view, without making any assumptions about the data-
generation process. this makes it very challenging to evaluate the accuracy of the acquired
information.

in this chapter we model the data-acquisition process probabilistically. this allows to ana-
lyze statistical techniques and derive theoretical guarantees on their performance. the data
are interpreted as realizations of random variables, vectors or processes (depending on the
dimensionality). the information that we want to extract can then be expressed in terms of the
joint distribution of these quantities. we consider this distribution to be unknown but    xed,
taking a frequentist perspective. the alternative framework of bayesian statistics is described
in chapter 10.

9.1

independent identically-distributed sampling

in this chapter we consider one-dimensional real-valued data, modeled as the realization of an
iid sequence. figure 9.1 depicts the corresponding graphical model. this is a very popular
assumption, which holds for controlled experiments, such as randomized trials to test drugs,
and can often be a good approximation in other settings. however, in practice it is crucial to
evaluate to what extent the independence assumptions of a model actually hold.

the following example shows that measuring a quantity by sampling a subset of individuals
randomly from a large population produces data satisfying the iid assumption, as long as we
sample with replacement (if the population is large, sampling without replacement will have a
negligible e   ect).

example 9.1.1 (sampling from a population). assume that we are studying a population of
m individuals. we are interested in a certain quantity associated to each person, e.g. their
cholesterol level, their salary or who they are voting for in an election. there are k possible
values for the quantity {z1, z2, . . . , zk}, where k can be equal to m or much smaller. we denote
by mj the number of people for whom the quantity is equal to zj, 1     j     k. in the case of
an election with two candidates, k would equal two and m1 and m2 would represent the people
voting for each of the candidates.

151

chapter 9. frequentist statistics

152

figure 9.1: directed graphical model corresponding to an independent sequence. if the sequence is also
identically distributed, then x1, x2, . . . , xn all have the same distribution.

let us assume that we select n individuals independently at random with replacement, which
means that one individual could be chosen more than once, and record the value of the quantity
of interest. under these assumptions the measurements can be modeled as a random sequence

of independent variables (cid:101)x. since the id203 of choosing any individual is the same every

time we make a selection, the    rst-order pmf of the sequence is

p(cid:101)x(i) (zj) = p (the ith measurement equals zj)

=

=

people such that the quantity equals zj

mj
m

,

total number of people
1     j     k,

(9.1)

(9.2)

(9.3)

for 1     i     n by the law of total id203. we conclude that the data can be modeled as a
(cid:52)
realization of an iid sequence.

9.2 mean square error

we de   ne an estimator as a deterministic function of the available data x1, x2, . . . , xn which
provides an approximation to a quantity associated to the distribution that generates the data

y := h (x1, x2, . . . , xn) .

(9.4)

for example, as we will see, if we want to estimate the expectation of the underlying distribution,
a reasonable estimator is the average of the data. since we are taking a frequentist viewpoint,
the quantity of interest is modeled as deterministic (in contrast to the bayesian viewpoint which
would model it as a random variable). for a    xed data set, the estimator is a deterministic
function of the data. however, if we model the data as realizations of a sequence of random
variables, then the estimator is also a realization of the random variable

y := h (x1, x2, . . . , xn) .

(9.5)

this allows to evaluate the estimator probabilistically (usually under some assumptions on the
underlying distribution). for instance, we can measure the error incurred by the estimator by
computing the mean square of the di   erence between the estimator and the true quantity of
interest.

de   nition 9.2.1 (mean square error). the mean square error (mse) of an estimator y that

approximates a deterministic quantity        r is

mse (y ) := e(cid:16)(y       )2(cid:17) .

(9.6)

x1x2x3x4...xnchapter 9. frequentist statistics

153

the mse can be decomposed into a bias term and a variance term. the bias term is the
di   erence between the quantity of interest and the expected value of the estimator. the variance
term corresponds to the variation of the estimator around its expected value.

lemma 9.2.2 (bias-variance decomposition). the mse of an estimator y that approximates

       r satis   es

mse (y ) = e(cid:16)(y     e (y ))2(cid:17)
(cid:125)

(cid:123)(cid:122)

variance

(cid:124)

+ (e (y )       )2
(cid:125)
(cid:124)

(cid:123)(cid:122)

bias

proof. the lemma is a direct consequence of linearity of expectation.

.

(9.7)

if the bias is zero, then the estimator equals the quantity of interest on average.

de   nition 9.2.3 (unbiased estimator). an estimator y that approximates        r is unbiased

if its bias is equal to zero, i.e. if and only if

e (y ) =   .

(9.8)

an estimator may be unbiased but still incur in a large mean square error due to its variance.

the following lemmas establish that the sample mean and variance are unbiased estimators of
the true mean and variance of an iid sequence of random variables.

lemma 9.2.4 (the sample mean is unbiased). the sample mean is an unbiased estimator of
the mean of an iid sequence of random variables.

proof. we consider the sample mean of an iid sequence (cid:101)x with mean   ,

(cid:101)y (n) :=
e(cid:16)(cid:101)y (n)(cid:17) =

1
n

1
n

n(cid:88)i=1 (cid:101)x (i) .
e(cid:16)(cid:101)x (i)(cid:17)
n(cid:88)i=1

=   .

(9.9)

(9.10)

(9.11)

by linearity of expectation

lemma 9.2.5 (the sample variance is unbiased). the sample variance is an unbiased estimator
of the variance of an iid sequence of random variables.

the proof of this result is in section 9.7.1.

chapter 9. frequentist statistics

154

9.3 consistency

if we are estimating a scalar quantity, the estimate should improve as we gather more data.
ideally the estimate should converge to the true value in the limit when the number of data
n        . estimators that achieve this are said to be consistent.

de   nition 9.3.1 (consistency). an estimator (cid:101)y (n) := h(cid:16)(cid:101)x (1) , (cid:101)x (2) , . . . , (cid:101)x (n)(cid:17) that ap-
proximates        r is consistent if it converges to    as n         in mean square, with id203

one or in id203.

the following theorem shows that the mean is consistent.

theorem 9.3.2 (the sample mean is consistent). the sample mean is a consistent estimator
of the mean of an iid sequence of random variables as long as the variance of the sequence is
bounded.

proof. we consider the sample mean of an iid sequence (cid:101)x with mean   ,

(cid:101)y (n) :=

1
n

n(cid:88)i=1 (cid:101)x (i) .

the estimator is equal to the moving average of the data. as a result it converges to    in mean
square (and with id203 one) by the law of large numbers (theorem 6.2.2), as long as the
variance   2 of each of the entries in the iid sequence is bounded.

example 9.3.3 (estimating the average height). in this example we illustrate the consistency
of the sample mean. imagine that we want to estimate the mean height in a population. to be
concrete we consider a population of m := 25000 people. figure 9.2 shows a histogram of their
heights.1 as explained in example 9.1.1 if we sample n individuals from this population with

replacement, then their heights form an iid sequence (cid:101)x. the mean of this sequence is

p (person j is chosen)    height of person j

e(cid:16)(cid:101)x (i)(cid:17) :=

m(cid:88)j=1
m(cid:88)j=1

1
m

=

hj

= av (h1, . . . , hm)

(9.12)

(9.13)

(9.14)

(9.15)

for 1     i     n, where h1, . . . , hm are the heights of the people.
in addition, the variance
is bounded because the heights are    nite. by theorem 9.3.2 the sample mean of the n data
should converge to the mean of the iid sequence and hence to the average height over the whole
population. figure 9.3 illustrates this numerically.

(cid:52)
if the mean of the underlying distribution is not well de   ned, or its variance is unbounded,
then the sample mean is not necessarily a consistent estimator. this is related to the fact that

1the data are available here: wiki.stat.ucla.edu/socr/index.php/socr_data_dinov_020108_heightsweights.

chapter 9. frequentist statistics

155

figure 9.2: histogram of the heights of a group of 25 000 people.

figure 9.3: di   erent realizations of the sample mean when individuals from the population in figure 9.2
are sampled with replacement.

606264666870727476height (inches)0.050.100.150.200.25100101102103n646566676869707172height (inches)true meanempirical meanchapter 9. frequentist statistics

156

sample mean

sample median

figure 9.4: realization of the moving average of an iid cauchy sequence (top) compared to the moving
median (bottom).

the sample mean can be severely a   ected by the presence of extreme values, as we discussed
in section 8.2. the sample median, in contrast, tends to be more robust in such situations, as
discussed in section 8.3. the following theorem establishes that the sample median is consistent
under the iid assumption, even if the mean is not well de   ned or the variance is unbounded.
the proof is in section 9.7.2.

theorem 9.3.4 (sample median as an estimator of the median). the sample median is a
consistent estimator of the median of an iid sequence of random variables.

figure 9.4 compares the moving average and the moving median of an iid sequence of cauchy
random variables for three di   erent realizations. the moving average is unstable and does not
converge no matter how many data are available, which is not surprising because the mean is
not well de   ned. in contrast, the moving median does eventually converge to the true median
as predicted by theorem 9.3.4.

the sample variance and covariance are consistent estimators of the variance and covariance
respectively, under certain assumptions on the higher moments of the underlying distributions.
this provides an intuitive interpretation for principal component analysis (see section 8.5.2) un-
der the assumption that the data are realizations of an iid sequence of random vectors: the prin-
cipal components approximate the eigenvectors of the true covariance matrix (see section 4.3.3),
and hence the directions of maximum variance of the multidimensional distribution. figure 9.5

01020304050i5051015202530moving averagemedian of iid seq.0100200300400500i1050510moving averagemedian of iid seq.010002000300040005000i6050403020100102030moving averagemedian of iid seq.01020304050i3210123moving medianmedian of iid seq.0100200300400500i3210123moving medianmedian of iid seq.010002000300040005000i3210123moving medianmedian of iid seq.chapter 9. frequentist statistics

157

n = 5

n = 20

n = 100

figure 9.5: principal directions of n samples from a bivariate gaussian distribution (red) compared to
the eigenvectors of the covariance matrix of the distribution (black).

illustrates this with a numerical example, where the principal components indeed converge to
the eigenvectors as the number of data increases.

9.4 con   dence intervals

consistency implies that an estimator will be perfect if we acquire in   nite data, but this is of
course impossible in practice. it is therefore important to quantify the accuracy of an estimator
for a    xed number of data. con   dence intervals allow to do this from a frequentist point of
view. a con   dence interval can be interpreted as a soft estimate of the deterministic quantity
of interest, which guarantees that the true value will belong to the interval with a certain
id203.

de   nition 9.4.1 (con   dence interval). a 1        con   dence interval i for        r satis   es

p (       i)     1       ,

(9.16)

where 0 <    < 1.

con   dence intervals are usually of the form [y     c, y + c] where y is an estimator of the quantity
of interest and c is a constant that depends on the number of data. the following theorem derives
a con   dence interval for the mean of an iid sequence. the con   dence interval is centered at the
sample mean.

theorem 9.4.2 (con   dence interval for the mean of an iid sequence). let (cid:101)x be an iid sequence
with mean    and variance   2     b2 for some b > 0. for any 0 <    < 1

in :=(cid:20)yn    

b

      n

, yn +

b

      n(cid:21) ,

is a 1        con   dence interval for   .

yn := av(cid:16)(cid:101)x (1) , (cid:101)x (2) , . . . , (cid:101)x (n)(cid:17) ,

(9.17)

truecovariancesamplecovariancechapter 9. frequentist statistics

158

of theorem 6.2.2). we have

proof. recall that the variance of yn equals var(cid:0)   xn(cid:1) =   2/n (see equation (6.21) in the proof
p(cid:18)      (cid:20)yn    

      n(cid:21)(cid:19) = 1     p(cid:18)|yn       | >

      n(cid:19)

      n

, yn +

(9.18)

  

b

b

   nvar (yn)

b2

    1    
     2
= 1    
b2
    1       .

by chebyshev   s inequality

(9.19)

(9.20)

(9.21)

the width of the interval provided in the theorem decreases with n for    xed   , which makes
sense as incorporating more data reduces the variance of the estimator and hence our uncertainty
about it.

example 9.4.3 (bears in yosemite). a scientist is trying to estimate the average weight of the
black bears in yosemite national park. she manages to capture 300 bears. we assume that the
bears are sampled uniformly at random with replacement (a bear can be weighed more than
once). under this assumptions, in example 9.1.1 we show that the data can be modeled as iid
samples and in example 9.3.3 we show the sample mean is a consistent estimator of the mean
of the whole population.

the average weight of the 300 captured bears is y := 200 lbs. to derive a con   dence interval
from this information we need a bound on the variance. the maximum weight recorded for a
black bear ever is 880 lbs. let    and   2 be the (unknown) mean and variance of the weights of
the whole population. if x is the weight of a bear chosen uniformly at random from the whole
population then x has mean    and variance   2, so

  2 = e(cid:0)x 2(cid:1)     e2 (x)
    e(cid:0)x 2(cid:1)
    8802 because x     880.

as a result, 880 is an upper bound for the standard deviation. applying theorem 9.4.2,

(cid:20)y    

b

      n

, y +

b

      n(cid:21) = [   27.2, 427.2]

(9.22)

(9.23)

(9.24)

(9.25)

is a 95% con   dence interval for the average weight of the whole population. the interval is not
(cid:52)
very precise because n is not very large.
as illustrated by this example, con   dence intervals derived from chebyshev   s inequality tend to
be very conservative. an alternative is to leverage the central limit theorem (clt). the clt
characterizes the distribution of the sample mean asymptotically, so con   dence intervals derived
from it are not guaranteed to be precise. however, the clt often provides a very accurate
approximation to the distribution of the sample mean for    nite n, as we show through some
numerical examples in chapter 6. in order to obtain con   dence intervals for the mean of an iid
sequence from the clt as stated in theorem 6.3.1 we would need to know the true variance of

chapter 9. frequentist statistics

159

the sequence, which is unrealistic in practice. however, the following result states that we can
substitute the true variance with the sample variance. the proof is beyond the scope of these
notes.

theorem 9.4.4 (central limit theorem with sample standard deviation). let (cid:101)x be an iid
discrete random process with mean   (cid:101)x :=    such that its variance and fourth moment e((cid:101)x (i)4)

are bounded. the sequence

   n(cid:16)av(cid:16)(cid:101)x (1) , . . . , (cid:101)x (n)(cid:17)       (cid:17)
std(cid:16)(cid:101)x (1) , . . . , (cid:101)x (n)(cid:17)

converges in distribution to a standard gaussian random variable.

recall that the cdf of a standard gaussian does not have a closed-form expression. to simplify
notation we express the con   dence interval in terms of the q function.

de   nition 9.4.5 (q function). q (x) is the id203 that a standard gaussian random vari-
able is greater than x for positive x,

q (x) :=(cid:90)    

u=x

1
   2  

exp(cid:18)   

u2

2(cid:19) du,

x > 0.

(9.27)

by symmetry, if u is a standard gaussian random variable and y < 0

p (u < y) = q (   y) .

(9.28)

corollary 9.4.6 (approximate con   dence interval for the mean). let (cid:101)x be an iid sequence that

satis   es the conditions of theorem 9.4.4. for any 0 <    < 1

2(cid:17)(cid:21) ,
q   1(cid:16)   

sn   n

q   1(cid:16)   

in :=(cid:20)yn    
2(cid:17) , yn +
sn   n
yn := av(cid:16)(cid:101)x (1) , (cid:101)x (2) , . . . , (cid:101)x (n)(cid:17) ,
sn := std(cid:16)(cid:101)x (1) , (cid:101)x (2) , . . . , (cid:101)x (n)(cid:17) ,

p (       in)     1       .

is an approximate 1        con   dence interval for   , i.e.

(9.26)

(9.29)

(9.30)

(9.31)

(9.32)

proof. by the central limit theorem, when n           xn is distributed as a gaussian random
variable with mean    and variance   2. as a result
p (       in) = 1     p(cid:18)yn >    +
q   1(cid:16)   
sn   n
= 1     p(cid:18)   n (yn       )
> q   1(cid:16)   
    1     2q(cid:16)q   1(cid:16)   

2(cid:17)(cid:19)     p(cid:18)yn <       
sn   n
2(cid:17)(cid:19)     p(cid:18)   n (yn       )

2(cid:17)(cid:19)
q   1(cid:16)   
<    q   1(cid:16)   

2(cid:17)(cid:17) by theorem 9.4.4

2(cid:17)(cid:19) (9.34)

(9.33)

(9.35)

sn

sn

(9.36)

= 1       .

chapter 9. frequentist statistics

160

it is important to stress that the result only provides an accurate con   dence interval if n is large
enough for the sample variance to converge to the true variance and for the clt to take e   ect.

example 9.4.7 (bears in yosemite (continued)). the sample standard deviation of the bears
captured by the scientist equals 100 lbs. we apply corollary 9.4.6 to derive an approximate
con   dence interval that is tighter than the one obtained applying chebyshev   s inequality. given
that q (1.95)     0.025,

(cid:20)y    

  
   n

q   1(cid:16)   

2(cid:17) , y +

  
   n

q   1(cid:16)   

2(cid:17)(cid:21)     [188.8, 211.3]

(9.37)

is an approximate 95% con   dence interval for the mean weight of the population of bears.

(cid:52)
interpreting con   dence intervals is somewhat tricky. after computing the con   dence interval in
example 9.4.7 one is tempted to state:

the id203 that the average weight is between 188.8 and 211.3 lbs is 0.95.

however we are modeling the average weight as a deterministic quantity, so there are no random
quantities in this statement! the correct interpretation is that if we repeat the process of
sampling the population and compute the con   dence interval many times, then the true value
will lie in the interval 95% of the time. this is illustrated in the following example and figure 9.6.

example 9.4.8 (estimating the average height (continued)). figure 9.6 shows several 95%
con   dence intervals for the average of the height population in example 9.3.3. to compute
each interval we select n individuals and then apply corollary 9.4.6. the width of the intervals
decreases as n grows, but because they are all 95% con   dence intervals they all contain the true
average with id203 0.95. indeed this is the case for 113 out of 120 (94%) of the intervals
that are plotted.

(cid:52)

9.5 nonparametric model estimation

in this section we consider the problem of estimating a distribution from multiple iid samples.
this requires approximating the cdf, pmf or pdf of the distribution. if we assume that the dis-
tribution belongs to a prede   ned family, then the problem reduces to estimating the parameters
that characterize that particular family, as we explain in detail in section 9.6. here we do not
make such an assumption. estimating a distribution directly is very challenging; clearly many
(in   nite!) di   erent distributions could have generated the data. however with enough samples
it is often possible to obtain models that produce an accurate approximation, as long as the iid
assumption holds.

9.5.1 empirical cdf

under the assumption that a data set corresponds to iid samples from a certain distribution, a
reasonable estimate for the cdf of the distribution at a given point x is the fraction of samples
that are smaller than x. this results in a piecewise constant estimator known as the empirical
cdf.

chapter 9. frequentist statistics

161

n = 50

n = 200

n = 1000

figure 9.6: 95% con   dence intervals for the average of the height population in example 9.3.3.

de   nition 9.5.1 (empirical cdf). the empirical cdf corresponding to data x1, . . . , xn is

where x     r.

1
n

n(cid:88)i=1

1xi   x,

(cid:98)fn (x) :=

(9.38)

the empirical cdf is an unbiased and consistent estimator of the true cdf. this is established
rigorously in theorem 9.5.2 below and illustrated empirically in figure 9.7. the cdf of the height
data from 25,000 people is compared to three realizations of the empirical cdf computed from
di   erent numbers of iid samples. as the number of available samples grows, the approximation
becomes very accurate.

theorem 9.5.2. let (cid:101)x be an iid sequence with marginal cdf fx . for any    xed x     r (cid:98)fn (x)
is an unbiased and consistent estimator of fx (x). in fact, (cid:98)fn (x) converges in mean square to

fx (x).

proof. first, we verify

e(cid:16)(cid:98)fn (x)(cid:17) = e(cid:32) 1
n(cid:88)i=1

1
n

=

n

1(cid:101)x(i)   x(cid:33)
n(cid:88)i=1
p(cid:16)(cid:101)x (i)     x(cid:17) by linearity of expectation

= fx (x) ,

(9.39)

(9.40)

(9.41)

true meantrue meantrue meanchapter 9. frequentist statistics

162

n = 10

n = 100

n = 1000

figure 9.7: cdf of the height data in figure 2.13 along with three realizations of the empirical cdf
computed with n iid samples for n = 10, 100, 1000.

606264666870727476height (inches)0.00.20.40.60.8true cdfempirical cdf606264666870727476height (inches)0.00.20.40.60.8true cdfempirical cdf606264666870727476height (inches)0.00.20.40.60.8true cdfempirical cdfchapter 9. frequentist statistics

so the estimator is unbiased. we now estimate its mean square

n (x)(cid:17) = e       1
e(cid:16)(cid:98)f 2
n(cid:88)i=1

1
n2

n2

=

=

=

fx (x)

n

fx (x)

n

1(cid:101)x(i)   x1(cid:101)x(j)   x      
n(cid:88)j=1
n(cid:88)i=1
p(cid:16)(cid:101)x (i)     x(cid:17) +
n(cid:88)j=1,i(cid:54)=j
n(cid:88)i=1
f(cid:101)x(i) (x) f(cid:101)x(j) (x)
n(cid:88)i=1
n(cid:88)j=1,i(cid:54)=j

1
n2

1
n2

+

+

n     1
n

f 2

x (x) .

the variance is consequently equal to

=

fx (x) (1     fx (x))

var(cid:16)(cid:98)fn (x)(cid:17) = e(cid:16)(cid:98)fn (x)2(cid:17)     e2(cid:16)(cid:98)fn (x)(cid:17)
e(cid:18)(cid:16)fx (x)     (cid:98)fn (x)(cid:17)2(cid:19) = lim

n      

var(cid:16)(cid:98)fn (x)(cid:17) = 0.

n

.

we conclude that

lim
n      

p(cid:16)(cid:101)x (i)     x, (cid:101)x (j)     x(cid:17)

by independence

(9.44)

9.5.2 density estimation

estimating the pdf of a continuous quantity is much more challenging that estimating the cdf.
if we have su   cient data, the fraction of samples that are smaller than a certain x provide a
good estimate for the cdf at that point. however, no matter how much data we have, there is
negligible id203 that we will see any samples exactly at x: a pointwise empirical density
estimator would equal zero almost everywhere (except at the available samples).

our only hope to produce an accurate estimator is if the pdf that we aim to estimate is smooth.
in that case, we can estimate its value at a point x from observed samples that are situated at
neighboring locations. if there are many samples close to x then this suggests that the estimate
at x should be large, whereas if all the samples are far away, then it should be small. kernel
density estimation achieves this by averaging the samples.

de   nition 9.5.3 (kernel density estimator). the kernel density estimate with bandwidth h of

the distribution of x1, . . . , xn at x     r is
(cid:98)fh,n (x) :=

1
n h

where k is a id81 centered at the origin that satis   es

k(cid:18) x     xi
h (cid:19) ,

n(cid:88)i=1
for all x     r,

k (x)     0
k (x) dx = 1.

(cid:90)r

163

(9.42)

(9.43)

(9.45)

(9.46)

(9.47)

(9.48)

(9.49)

(9.50)

(9.51)

chapter 9. frequentist statistics

164

h = 0.1

h = 0.5

n = 5

n = 102

n = 104

figure 9.8: kernel density estimation for the gaussian mixture described in example 9.6.5 for di   erent
number of iid samples and di   erent values of the kernel bandwidth h.

5050.20.00.20.40.60.81.01.21.4true distributiondatakernel-density estimate5050.050.000.050.100.150.200.250.300.35true distributiondatakernel-density estimate5050.10.00.10.20.30.40.5true distributiondatakernel-density estimate5050.050.000.050.100.150.200.250.300.35true distributiondatakernel-density estimate5050.050.000.050.100.150.200.250.300.35true distributiondatakernel-density estimate5050.050.000.050.100.150.200.250.300.35true distributiondatakernel-density estimatechapter 9. frequentist statistics

165

the e   ect of the kernel is to weight each sample according to their distance to the point at which
we are estimating the pdf x. choosing a rectangular kernel yields an empirical density estimate
that is piecewise constant and roughly looks like a histogram (the corresponding weights are

constant or equal to zero). a popular alternative is the gaussian kernel k (x) = exp(cid:0)   x2(cid:1) /     ,

which produces a smooth density estimate. the kernel should decay so that k ((x     xi) /h) is
large when the sample xi is close to x and small when it is far. this decay is governed by the
bandwidth h, which is chosen before hand based on our expectations about the smoothness of
the pdf and on the amount of available data. if the bandwidth is very small, individual samples
have a large in   uence on the density estimate. this allows to reproduce irregular shapes more
easily, but also yields spurious    uctuations that are not present in the true curve, especially if we
don   t have a lot of samples. increasing the bandwidth smooths out such    uctuations and yields
more stable estimates when the number of data is small. however, it may also over-smooth the
estimate. as a rule of thumb, we should decrease the bandwidth of the kernel as the number of
data increases.

figures 9.8 and 9.9 illustrate the e   ect of varying the bandwidth h at di   erent sampling rates.
in figure 9.8 gaussian kernel density estimation is applied to estimate the gaussian mixture
described in example 9.6.5. figure 9.9 shows an example where the same technique is used on
real data: the aim is to estimate the density of the weight of a sea-snail population.2 the whole
population consists of 4,177 individuals. the kernel density estimate is computed from 200 iid
samples for di   erent values of the kernel bandwidth.

9.6 parametric model estimation

in the previous section, we describe how to estimate a distribution by directly estimating the
cdf or pdf generating the data. in this section, we discuss an alternative route based on the
assumption that the type of distribution generating the data is known beforehand. if this is
the case, the problem boils down to    tting the parameters characterizing the distribution to the
data. recall that from a frequentist viewpoint, the true distribution is    xed, so the corresponding
parameters are modeled as deterministic quantities (in contrast, in a bayesian framework they
are modeled as random variables).

9.6.1 the method of moments

the method of moments adjusts the parameters of a distribution so that the moments of the
distribution coincide with the sample moments of the data (i.e.
its mean, mean square or
variance, etc.). if the distribution only depends on one parameter, then we use the sample mean
as a surrogate for the true mean and compute the corresponding value of the parameter. for an
exponential with parameter    and mean    we have

   =

1
  

.

(9.52)

assuming that we have access to n iid samples x1, . . . , xn from the exponential distribution, the
method-of-moments estimate of    equals

  mm :=

1

av (x1, . . . , xn)

.

(9.53)

2the data are available at archive.ics.uci.edu/ml/datasets/abalone

chapter 9. frequentist statistics

166

figure 9.9: kernel density estimate for the weight of a population of abalone, a species of sea snail. in
the plot above the density is estimated from 200 iid samples using a gaussian kernel with three di   erent
bandwidths. black crosses representing the individual samples are shown underneath. in the plot below
we see the result of repeating the procedure three times using a    xed bandwidth equal to 0.25.

101234weight (grams)0.00.20.40.60.81.0kde bandwidth: 0.05kde bandwidth: 0.25kde bandwidth: 0.5true pdf101234weight (grams)0.00.10.20.30.40.50.60.70.80.9chapter 9. frequentist statistics

167

figure 9.10: exponential distribution    tted to data consisting of inter-arrival times of calls at a call
center in israel (left). gaussian distribution    tted to height data (right).

the graph on the right of figure 9.10 shows the result of    tting an exponential to the call-center
data in figure 2.11. similarly, to    t a gaussian using the method of moments we set the mean
equal to its sample mean and the variance equal to the sample variance, as illustrated by the
graph on the right of figure 9.10 using the data from figure 2.13.

9.6.2 maximum likelihood

the most popular method for learning parametric models is maximum-likelihood    tting. the
likelihood function is the joint pmf or pdf of the data, interpreted as a function of the unknown
parameters.
in more detail, let us denote the data by x1, . . . , xn and assume that they are
realizations of a set of discrete random variables x1, . . . , xn which have a joint pmf that depends
on a vector of parameters (cid:126)  . to emphasize that the joint pmf depends on (cid:126)   we denote it by
p(cid:126)   := px1,...,xn. this pmf evaluated at the observed data

is the likelihood function, when we interpret it as a function of (cid:126)  . for continuous random
variables, we use the joint pdf of the data instead.

p(cid:126)   (x1, . . . , xn)

(9.54)

de   nition 9.6.1 (likelihood function). given a realization x1, . . . , xn of a set of discrete ran-

dom variables x1, . . . , xn with joint pmf p(cid:126)  , where (cid:126)       rm is a vector of parameters, the

likelihood function is

if the random variables are continuous with pdf f(cid:126)  , where (cid:126)       rm, the likelihood function is

lx1,...,xn(cid:16)(cid:126)  (cid:17) := p(cid:126)   (x1, . . . , xn) .
lx1,...,xn(cid:16)(cid:126)  (cid:17) := f(cid:126)   (x1, . . . , xn) .

(9.55)

(9.56)

the log-likelihood function is equal to the logarithm of the likelihood function log lx1,...,xn(cid:16)(cid:126)  (cid:17).

0123456789interarrival times (s)0.00.10.20.30.40.50.60.70.80.9exponential distributionreal data606264666870727476height (inches)0.050.100.150.200.25gaussian distributionreal datachapter 9. frequentist statistics

168

when the data are modeled as iid samples, the likelihood factors into a product of the marginal
pmf or pdf, so the log likelihood can be decomposed into a sum.
in the case of discrete distributions, for a    xed (cid:126)   the likelihood is the id203 that x1, . . . , xn
equal the observed data. if we don   t know (cid:126)  , it makes sense to choose a value for (cid:126)   such that this
id203 is as high as possible, i.e. to maximize the likelihood. for continuous distributions
we apply the same principle to the joint pdf of the data.

de   nition 9.6.2 (maximum-likelihood estimator). the maximum likelihood (ml) estimator

for the vector of parameters (cid:126)       rm is

(cid:126)  ml (x1, . . . , xn) := arg max

(cid:126)   lx1,...,xn(cid:16)(cid:126)  (cid:17)
log lx1,...,xn(cid:16)(cid:126)  (cid:17) .

(cid:126)  

= arg max

(9.57)

(9.58)

the maximum of the likelihood function and that of the log-likelihood function are at the same
location because the logarithm is monotone.

under certain conditions, one can show that the maximum-likelihood estimator is consistent: it
converges in id203 to the true parameter as the number of data increases. one can also
show that its distribution converges to that of a gaussian random variable (or vector), just like
the distribution of the sample mean. these results are beyond the scope of the course. bear in
mind, however, that they only hold if the data are indeed generated by the type of distribution
that we are considering.

we now show how to derive the maximum-likelihood for a bernoulli and a gaussian distribution.
the resulting estimators for the parameters are the same as the method-of-moments estimators
(except for a slight di   erence in the estimate of the gaussian variance parameter).

example 9.6.3 (ml estimator of the parameter of a bernoulli distribution). we model a set
of data x1, . . . , xn as iid samples from a bernoulli distribution with parameter    (in this case
there is only one parameter). the likelihood function is equal to

lx1,...,xn (  ) = p   (x1, . . . , xn)

(1xi=1   + 1xi=0 (1       ))

=(cid:89)i=1
=   n1 (1       )n0

and the log-likelihood function to

log lx1,...,xn (  ) = n1 log    + n0 log (1       ) ,

(9.59)

(9.60)

(9.61)

(9.62)

where n1 are the number of samples equal to one and n0 the number of samples equal to zero.
the ml estimator of the parameter    is

  ml = arg max

  

= arg max

  

log lx1,...,xn (  )
n1 log    + n0 log (1       ) .

(9.63)

(9.64)

chapter 9. frequentist statistics

we compute the derivative and second derivative of the log-likelihood function,

d log lx1,...,xn (  )
d2 log lx1,...,xn (  )

d  

d  2

=

n1
      
n1
  2    

,

n0
1       
n0
(1       )2 < 0.

=    

the function is concave, as the second derivative is negative. the maximum is consequently at
the point where the    rst derivative equals zero, namely

  ml =

n1

n0 + n1

.

(9.67)

the estimate is equal to the fraction of samples that are equal to one.

(cid:52)
example 9.6.4 (ml estimator of the parameters of a gaussian distribution). let x1, x2, . . .
be data that we wish to model as iid samples from a gaussian distribution with mean    and
standard deviation   . the likelihood function is equal to

and the log-likelihood function to

=

lx1,...,xn (  ,   ) = f  ,   (x1, . . . , xn)

(xi     )2

2  2

1

   2    

e   

n(cid:89)i=1

log lx1,...,xn (  ,   ) =    

n log (2  )

2

    n log       

(xi       )2

2  2

.

n(cid:88)i=1

the ml estimator of the parameters    and    is
{  ml,   ml} = arg max
{  ,  }
{  ,  }   n log       

= arg max

log lx1,...,xn (  ,   )

we compute the partial derivatives of the log-likelihood function,

(xi       )2

2  2

.

n(cid:88)i=1

    log lx1,...,xn (  ,   )

     

    log lx1,...,xn (  ,   )

     

=    

=    

n(cid:88)i=1

n
  

+

xi       
  2

,

(xi       )2

  3

.

n(cid:88)i=1

the function we are trying to maximize is strictly concave in {  ,   }. to prove this, we would
have to show that the hessian of the function is positive de   nite. we omit the calculations that
show that this is the case. setting the partial derivatives to zero we obtain

  ml =

  2
ml =

1
n

1
n

n(cid:88)i=1
n(cid:88)i=1

xi,

(xi       ml)2 .

(9.75)

(9.76)

169

(9.65)

(9.66)

(9.68)

(9.69)

(9.70)

(9.71)

(9.72)

(9.73)

(9.74)

chapter 9. frequentist statistics

170

data

log-likelihood function

figure 9.11: the left column shows histograms of 50 iid samples from a gaussian distribution, together
with the pdf of the original distribution, as well as the maximum-likelihood estimate. the right column
shows the log-likelihood function corresponding to the data and the location of its maximum and of the
point corresponding to the true parameters.

1050510150.000.050.100.15estimated distributiontrue distributiondata2.02.53.03.54.04.55.0  3.03.54.04.55.05.56.0  estimated parameterstrue parameters12312011711411110810510299961050510150.000.050.100.15estimated distributiontrue distributiondata2.02.53.03.54.04.55.0  3.03.54.04.55.05.56.0  estimated parameterstrue parameters120.8118.4116.0113.6111.2108.8106.4104.0101.61050510150.000.050.100.15estimated distributiontrue distributiondata2.02.53.03.54.04.55.0  3.03.54.04.55.05.56.0  estimated parameterstrue parameters107.4105.9104.4102.9101.499.998.496.995.493.9chapter 9. frequentist statistics

171

data

log-likelihood function

figure 9.12: the left image shows a histogram of 40 iid samples from the gaussian mixture de   ned in
example 9.6.5, together with the pdf of the original distribution. the right image shows the log-likelihood
function corresponding to the data, which has a local maximum apart from the global maximum. the
density estimates corresponding to the two maxima are shown on the left.

the estimator for the mean is just the sample mean. the estimator for the variance is a rescaled
sample variance.

(cid:52)
figure 9.11 displays the log-likelihood function corresponding to 50 iid samples from a gaussian
distribution with    := 3 and    := 4. it also shows the approximation to the true pdf obtained by
maximum likelihood. in examples 9.6.3 and 9.6.4 the log-likelihood function is strictly concave.
this means that the function has a unique maximum that can be located by setting the gradient
to zero. when this yields nonlinear equations that cannot be solved directly, we can leverage
optimization methods such as gradient ascent that will converge to the maximum. however,
the log-likelihood function is not always concave. as illustrated by the following example, in
such cases it can have multiple local maxima, which may make it intractable to compute the
maximum-likelihood estimator.

example 9.6.5 (log-likelihood function of a gaussian mixture). let x be a gaussian mixture
de   ned as

x :=(cid:40)g1 with id203 1

5 ,
g2 with id203 4
5 ,

(9.77)

where g1 is a gaussian random variable with mean       and variance   2, whereas g2 is also
gaussian with mean    and variance   2. we have parameterized the mixture with just two
parameters so that we can visualize the log-likelihood in two dimensions. let x1, x2, . . . be data

105051015200.000.050.100.150.200.250.300.35estimate (maximum)estimate (local max.)true distributiondata6420246  0.51.01.52.02.53.0  global maximumlocal maximumtrue parameters1200107595082570057545032520075172

(9.78)

(9.79)

(9.80)

chapter 9. frequentist statistics

modeled as iid samples from x. the likelihood function is equal to

lx1,...,xn (  ,   ) = f  ,   (x1, . . . , xn)

(xi+  )2

2  2 +

e   

4

5   2    

e   

(xi     )2

2  2

1

5   2    

n(cid:89)i=1
log(cid:18) 1

5   2    

=

n(cid:88)i=1

and the log-likelihood function to

log lx1,...,xn (  ,   ) =

(xi+  )2

2  2 +

e   

4

5   2    

e   

(xi     )2

2  2 (cid:19) .

figure 9.12 shows the log-likelihood function for 40 iid samples of the distribution when    := 4
and    := 1. the function has a local maximum away from the global maximum. this means
that if we use a local ascent method to    nd the ml estimator, we might not    nd the global
maximum, but remain stuck at the local maximum instead. the estimate corresponding to the
local maximum (shown on the left) has the same variance as the global maximum but    is close
to    4 instead of 4. although the estimate doesn   t    t the data very well, it is locally optimal,
small shifts of    and    yield worse    ts (in terms of the likelihood).

(cid:52)
to    nish this section, we describe a machine-learning algorithm for supervised learning based
on parametric    tting using ml estimation.

example 9.6.6 (quadratic discriminant analysis). quadratic discriminant analysis is an algo-
rithm for supervised learning. the input to the algorithm are two sets of training data, consist-
ing of d-dimensional vectors (cid:126)a1, . . . , (cid:126)an and (cid:126)b1, . . . ,(cid:126)bn which belong to two di   erent classes (the
method can easily be extended to deal with more classes). the goal is to classify new instances
based on the structure of the data.

to perform quadratic discriminant analysis we    rst    t a d-dimensional gaussian distribution
to the data of each class using the ml estimator for the mean and covariance matrix, which
correspond to the sample mean and covariance matrix of the training data (up to a slight
rescaling of the sample covariance). in more detail, (cid:126)a1, . . . , (cid:126)an are used to estimate a mean (cid:126)  a
and covariance matrix   a, whereas (cid:126)b1, . . . ,(cid:126)bn are used to estimate (cid:126)  b and   b,

{(cid:126)  a,   a} := arg max
{(cid:126)  b,   b} := arg max

(cid:126)  ,   l(cid:126)a1,...,(cid:126)an ((cid:126)  ,   ) ,
(cid:126)  ,   l(cid:126)b1,...,(cid:126)bn
((cid:126)  ,   ) .

(9.81)

(9.82)

then for each new example (cid:126)x, the value of the density function at the example for both classes
is evaluated. if

f(cid:126)  a,  a ((cid:126)x) > f(cid:126)  b,  b ((cid:126)x)

(9.83)

then (cid:126)x is declared to belong to the    rst class, otherwise it is declared to belong to the second
class. figure 9.13 shows the results of applying the method to data simulated using two gaussian
distributions.

(cid:52)

chapter 9. frequentist statistics

173

figure 9.13: quadratic-discriminant analysis applied to data from two di   erent classes (left). the data
corresponding to the two di   erent classes are colored orange and blue. three new examples are colored
in black. two bivariate gaussians are    t to the data. their contour lines are shown in the respective
color of each class on the right. these distributions are used to classify the new examples, which are
colored according to their estimated class.

9.7 proofs

9.7.1 proof of lemma 9.2.5

we consider the sample variance of an iid sequence (cid:101)x with mean    and variance   2,

(cid:101)y (n) :=

1

1

1
n

n     1

      (cid:101)x (i)    
n(cid:88)i=1
n     1      (cid:101)x (i)    
n     1      (cid:101)x (i)2 +

1(cid:88)j=1 (cid:101)x (j)      
n(cid:88)j=1 (cid:101)x (j)      
n(cid:88)k=1 (cid:101)x (j) (cid:101)x (k)    
n(cid:88)j=1

1
n2

1
n

1

2

=

=

2
n

n(cid:88)j=1 (cid:101)x (i) (cid:101)x (j)      

(9.84)

(9.85)

(9.86)

chapter 9. frequentist statistics

to simplify notation, we denote the mean square e(cid:16)(cid:101)x (i)2(cid:17) =   2 +   2 by   . we have
e(cid:16)(cid:101)y (n)(cid:17) =
e(cid:16)(cid:101)x (j) (cid:101)x (k)(cid:17)

n(cid:88)j=1

n(cid:88)k=1

n     1

1
n2

k(cid:54)=j

1

1
n2

e(cid:16)(cid:101)x (j)2(cid:17) +
n(cid:88)j=1
e(cid:16)(cid:101)x (i) (cid:101)x (j)(cid:17)

j(cid:54)=i
n (n     1)   2

n2

2   
n    

   

2 (n     1)   2

n

2
n

e(cid:16)(cid:101)x (i)2(cid:17) +
n(cid:88)j=1

n(cid:88)i=1
e(cid:16)(cid:101)x (i)2(cid:17)    
n(cid:88)i=1
n(cid:88)i=1

n     1

n (cid:0)          2(cid:1)

n   
n2 +

   +

2
n

   

n     1

1

1

=

=

n     1

=   2.

9.7.2 proof of theorem 9.3.4

we denote the sample median by (cid:101)y (n). our aim is to show that for any   > 0

we will prove that

lim
n      
the same argument allows to establish

lim
n      

lim
n      

p(cid:16)(cid:12)(cid:12)(cid:12)(cid:101)y (n)       (cid:12)(cid:12)(cid:12)      (cid:17) = 0.
p(cid:16)(cid:101)y (n)        +  (cid:17) = 0.
p(cid:16)(cid:101)y (n)             (cid:17) = 0.

174

(9.87)

(9.88)

(9.89)

(9.90)

(9.91)

(9.92)

(9.93)

(9.94)

therefore implies that at least (n + 1) /2 of the elements are larger than    +  .

if we order the set (cid:110)(cid:101)x (1) , . . . , (cid:101)x (n)(cid:111), then (cid:101)y (n) equals the (n + 1) /2th element if n is odd
and the average of the n/2th and the (n/2 + 1)th element if n is even. the event (cid:101)y (n)        +  
for each individual (cid:101)x (i), the id203 that (cid:101)x (i) >    +   is
p := 1     f(cid:101)x(i) (   +  ) = 1/2      (cid:48)
the median is not well de   ned. the number of random variables in the set(cid:110)(cid:101)x (1) , . . . , (cid:101)x (n)(cid:111)

where we assume that  (cid:48) > 0. if this is not the case then the cdf of the iid sequence is    at at    and

which are larger than    +   is distributed as a binomial random variable bn with parameters n

(9.95)

chapter 9. frequentist statistics

and p. as a result, we have

2

n + 1

n + 1

p(cid:16)(cid:101)y (n)        +  (cid:17)     p(cid:18) n + 1
or more samples are greater or equal to    +  (cid:19)
2 (cid:19)
= p(cid:18)bn    
= p(cid:18)bn     np    
    p(cid:18)|bn     np|     n (cid:48) +
(cid:0)n (cid:48) + 1
2(cid:1)2
n2(cid:0) (cid:48) + 1
2n(cid:1)2
2n(cid:1)2 ,
n(cid:0) (cid:48) + 1

2     np(cid:19)
2(cid:19)

by chebyshev   s inequality

np (1     p)

p (1     p)

var (bn)

   

=

=

1

which converges to zero as n        . this establishes (9.93).

175

(9.96)

(9.97)

(9.98)

(9.99)

(9.100)

(9.101)

(9.102)

chapter 10

bayesian statistics

in the frequentist paradigm we model the data as realizations from a distribution that is    xed. in
particular, if the model is parametric, the parameters are deterministic quantities. in contrast,
in bayesian parametric modeling the parameters are modeled as random variables. the goal is
to have the    exibility to quantify our uncertainty about the underlying distribution beforehand,
for example in order to integrate available prior information about the data.

10.1 bayesian parametric models

in this section we describe how to    t a parametric model to a data set within a bayesian
framework. as in section 9.6, we assume that the data are generated by sampling from known
distributions with unknown parameters. the crucial di   erence is that we model the parameters
as being random instead of deterministic. this requires selecting their prior distribution before
   tting the data, which allows to quantify our uncertainty about the value of the parameters
beforehand. a bayesian parametric model is speci   ed by:

1. the prior distribution is the distribution of (cid:126)  , which encodes our uncertainty about the

model before seeing the data.

2. the likelihood is the conditional distribution of (cid:126)x given (cid:126)  , which speci   es how the data
depend on the parameters. in contrast to the frequentist framework, the likelihood is not
interpreted as a deterministic function of the parameters.

our goal when learning a bayesian model is to compute the posterior distribution of the
parameters    given (cid:126)x. evaluating this posterior distribution at the realization (cid:126)x allows to
update our uncertainty about    using the data.

the following example    ts a bayesian model to iid samples from a bernoulli random variable.

example 10.1.1 (bernoulli distribution). let (cid:126)x be a vector of data that we wish to model as
iid samples from a bernoulli distribution. since we are taking a bayesian approach we choose
a prior distribution for the parameter of the bernoulli. we will consider two di   erent bayesian
estimators   1 and   2:

176

chapter 10. bayesian statistics

177

prior distribution

n0 = 1, n1 = 3

n0 = 3, n1 = 1

n0 = 91, n1 = 9

figure 10.1: the prior distribution of   1 (blue) and   2 (dark red) in example 10.1.1 are shown in the
top-left graph. the rest of the graphs show the corresponding posterior distributions for di   erent data
sets.

1.   1 represents a conservative estimator in terms of prior information. we assign a uniform

pdf to the parameter. any value in the unit interval has the same id203 density:

f  1 (  ) =(cid:40)1

0

for 0            1,
otherwise.

(10.1)

2.   2 is an estimator that assumes that the parameter is closer to 1 than to 0. we could use
it for instance to capture the suspicion that a coin is biased towards heads. we choose a
skewed pdf that increases linearly from zero to one,

f  2 (  ) =(cid:40)2   

0

for 0            1,
otherwise.

(10.2)

0.00.20.40.60.81.00.00.51.01.52.00.00.20.40.60.81.00.00.51.01.52.02.50.00.20.40.60.81.00.00.51.01.52.00.00.20.40.60.81.002468101214posterior mean (uniform prior)posterior mean (skewed prior)ml estimatorchapter 10. bayesian statistics

178

by the iid assumption, the likelihood, which is just the conditional pmf of the data given the
parameter of the bernoulli, equals

p (cid:126)x|   ((cid:126)x|  ) =   n1 (1       )n0 ,

(10.3)

where n1 is the number of ones in the data and n0 the number of zeros (see example 9.6.3).
the posterior pdfs of the two estimators are consequently equal to

f  1| (cid:126)x (  |(cid:126)x) =

f  1 (  ) p (cid:126)x|  1
p (cid:126)x ((cid:126)x)

((cid:126)x|  )

=

=

=

f  2| (cid:126)x (  |(cid:126)x) =

((cid:126)x|  )
((cid:126)x|u) du

f  1 (  ) p (cid:126)x|  1

(cid:82)u f  1 (u) p (cid:126)x|  1
  n1 (1       )n0
(cid:82)u un1 (1     u)n0 du
  n1 (1       )n0

   (n1 + 1, n0 + 1)
((cid:126)x|  )
f  2 (  ) p (cid:126)x|  2
p (cid:126)x ((cid:126)x)

,

where

=

=

  n1+1 (1       )n0
(cid:82)u un1+1 (1     u)n0 du
  n1+1 (1       )n0
   (n1 + 2, n0 + 1)

,

   (a, b) :=(cid:90)u

ua   1 (1     u)b   1 du

(10.4)

(10.5)

(10.6)

(10.7)

(10.8)

(10.9)

(10.10)

(10.11)

(10.12)

is a special function called the beta function or euler integral of the    rst kind, which is tabulated.

figure 10.1 shows the plot of the posterior distribution for di   erent values of n1 and n0.
it
also shows the maximum-likelihood estimator of the parameter, which is just n1/ (n0 + n1) (see
example 9.6.3). for a small number of    ips, the posterior pdf of   2 is skewed to the right with
respect to that of   1, re   ecting the prior belief that the parameter is closer to 1. however for
a large number of    ips both posterior densities are very close.

(cid:52)

10.2 conjugate prior

both posterior distributions in example 10.1.1 are beta distributions (see de   nition 2.3.12), and
so are the priors. the uniform prior of   1 is beta with parameters a = 1 and b = 1, whereas the
skewed prior of   2 is beta distribution with parameters a = 2 and b = 1. since the prior and
the posterior belong to the same family, computing the posterior is equivalent to just updating
the parameters. when the prior and posterior are guaranteed to belong to the same family of
distributions for a particular likelihood, the distributions are called conjugate priors.

chapter 10. bayesian statistics

179

de   nition 10.2.1 (conjugate priors). a conjugate family of distributions for a certain likeli-
hood satis   es the following property: if the prior belongs to the family, then the posterior also
belongs to the family.

beta distributions are conjugate priors when the likelihood is binomial.

theorem 10.2.2 (the beta distribution is conjugate to the binomial likelihood). if the prior
distribution of    is a beta distributions with parameters a and b and the likelihood of the data
x given    is binomial with parameters n and x, then the posterior distribution of    given x is
a beta distribution with parameters x + a and n     x + b.
proof.

f   | x (   | x) =

f   (  ) px |    (x|   )

px (x)

=

=

=

f   (  ) px |    (x|   )

(cid:82)u f   (u) px |    (x| u) du
  a   1 (1       )b   1(cid:0)n
(cid:82)u ua   1 (1     u)b   1(cid:0)n
  x+a   1 (1       )n   x+b   1
(cid:82)u ux+a   1 (1     u)n   x+b   1 du

= f   (  ; x + a, n     x + b) .

x(cid:1)  x (1       )n   x
x(cid:1)ux (1     u)n   x du

(10.13)

(10.14)

(10.15)

(10.16)

(10.17)

note that the posteriors obtained in example 10.1.1 follow immediately from the theorem.

example 10.2.3 (poll in new mexico). in a poll in new mexico for the 2016 us election, 429
participants, 227 people intend to vote for clinton and 202 for trump (the data are from a real
poll1, but for simplicity we are ignoring the other candidates and people that were undecided).
our aim is to use a bayesian framework to predict the outcome of the election in new mexico
using these data.

we model the fraction of people that vote for trump as a random variable   . we assume that
the n people in the poll are chosen uniformly at random with replacement from the population,
so given    =    the number of trump voters is a binomial with parameters n and   . we don   t
have any additional information about the possible value of   , so we assume it is uniform or
equivalently a beta distribution with parameters a := 1 and b := 1.

by theorem 10.2.2 the posterior distribution of    given the data that we observe is a beta
distribution with parameters a := 203 and b := 228, depicted in figure 10.2. the corresponding
id203 that        0.5 is 11.4%, which is our estimate for the id203 that trump wins in
new mexico.

1the poll results are taken from
https://www.abqjournal.com/883092/clinton-still-ahead-in-new-mexico.html

(cid:52)

chapter 10. bayesian statistics

180

figure 10.2: posterior distribution of the fraction of trump voters in new mexico conditioned on the
poll data in example 10.2.3.

10.3 bayesian estimators

the bayesian approach to learning probabilistic models yields the whole posterior distribution
of the parameters of interest. in this section we describe two alternatives for deriving a single
estimate of the parameters from the posterior distribution.

10.3.1 minimum mean-square-error estimation

the mean of the posterior distribution is the conditional expectation of the parameters given
the data. choosing the posterior mean as an estimator for the parameters (cid:126)   has a strong
theoretical justi   cation:
it is guaranteed to achieve the minimum mean square error (mse)
among all possible estimators. of course, this only holds if all of the assumptions hold, i.e. the
parameters are generated according to the prior and the data are then generated according to
the likelihood, which may not be the case for real data.

theorem 10.3.1 (the posterior mean minimizes the mse). the posterior mean is the min-
imum mean-square-error (mmse) estimate of the parameter (cid:126)   given the data (cid:126)x. to be more
precise, let us de   ne

for any arbitrary estimator   other ((cid:126)x),

  mmse ((cid:126)x) := e(cid:0)(cid:126)  | (cid:126)x = (cid:126)x(cid:1).

e(cid:18)(cid:16)  other( (cid:126)x)     (cid:126)  (cid:17)2(cid:19)     e(cid:18)(cid:16)  mmse( (cid:126)x)     (cid:126)  (cid:17)2(cid:19) .

(10.18)

(10.19)

proof. we begin by computing the mse of the arbitrary estimator conditioned on (cid:126)x = (cid:126)x in

0.350.400.450.500.550.6002468101214161888.6%11.4%181

(10.20)

(10.21)

(10.22)

(10.23)

(10.24)

chapter 10. bayesian statistics

terms of the conditional expectation of    given (cid:126)x,

e(cid:18)(cid:16)  other( (cid:126)x)     (cid:126)  (cid:17)2(cid:12)(cid:12)(cid:12) (cid:126)x = (cid:126)x(cid:19)
= e(cid:18)(cid:16)  other( (cid:126)x)       mmse( (cid:126)x) +   mmse( (cid:126)x)     (cid:126)  (cid:17)2(cid:12)(cid:12)(cid:12) (cid:126)x = (cid:126)x(cid:19)
= (  other((cid:126)x)       mmse((cid:126)x))2 + e(cid:16)(cid:16)  mmse( (cid:126)x)     (cid:126)  (cid:17)2(cid:12)(cid:12)(cid:12) (cid:126)x = (cid:126)x(cid:17)
+ 2 (  other((cid:126)x)       mmse((cid:126)x)) e(cid:16)  mmse((cid:126)x)     e(cid:16)(cid:126)  (cid:12)(cid:12)(cid:12) (cid:126)x = (cid:126)x(cid:17)(cid:17)
= (  other((cid:126)x)       mmse((cid:126)x))2 + e(cid:16)(cid:16)  mmse( (cid:126)x)     (cid:126)  (cid:17)2(cid:12)(cid:12)(cid:12) (cid:126)x = (cid:126)x(cid:17).

by iterated expectation,

e(cid:18)(cid:16)  other( (cid:126)x)       (cid:17)2(cid:19) = e(cid:18)e(cid:18)(cid:16)  other( (cid:126)x)     (cid:126)  (cid:17)2(cid:12)(cid:12)(cid:12) (cid:126)x(cid:19)(cid:19)

= e(cid:18)(cid:16)  other( (cid:126)x)       mmse( (cid:126)x)(cid:17)2(cid:19) + e(cid:18)e(cid:16)(cid:16)  mmse( (cid:126)x)     (cid:126)  (cid:17)2(cid:12)(cid:12)(cid:12) (cid:126)x(cid:17)(cid:19)
= e(cid:18)(cid:16)  other( (cid:126)x)       mmse( (cid:126)x)(cid:17)2(cid:19) + e(cid:18)(cid:16)  mmse( (cid:126)x)     (cid:126)  (cid:17)2(cid:19) (10.25)
    e(cid:18)(cid:16)  mmse( (cid:126)x)     (cid:126)  (cid:17)2(cid:19) ,

(10.26)

since the expectation of a nonnegative quantity is nonnegative.

example 10.3.2 (bernoulli distribution (continued)). in order to obtain point estimates for
the parameter in example 10.1.1 we compute the posterior means:

e(cid:16)  1| (cid:126)x = (cid:126)x(cid:17) =(cid:90) 1
= (cid:82) 1
0   n1+1 (1       )n0 d  
   (n1 + 1, n0 + 1)

  f  1| (cid:126)x (  |(cid:126)x) d  

0

e(cid:16)  2| (cid:126)x = (cid:126)x(cid:17) =(cid:90) 1

=

   (n1 + 2, n0 + 1)
   (n1 + 1, n0 + 1)

,

  f  2| (cid:126)x (  |(cid:126)x) d  
0
   (n1 + 3, n0 + 1)
.
   (n1 + 2, n0 + 1)

=

figure 10.1 shows the posterior means for di   erent values of n0 and n1.

(10.27)

(10.28)

(10.29)

(10.30)

(10.31)

(cid:52)

10.3.2 maximum-a-posteriori estimation

an alternative to the posterior mean is the posterior mode, which is the maximum of the pdf
or the pmf of the posterior distribution.

chapter 10. bayesian statistics

182

de   nition 10.3.3 (maximum-a-posteriori estimator). the maximum-a-posteriori (map) esti-
mator of a parameter (cid:126)   given data (cid:126)x modeled as a realization of a random vector (cid:126)x is

  map ((cid:126)x) := arg max

(cid:126)  

if (cid:126)   is modeled as a discrete random variable and

if it is modeled as a continuous random variable.

  map ((cid:126)x) := arg max

(cid:126)  

p(cid:126)   | (cid:126)x(cid:16)(cid:126)   | (cid:126)x(cid:17)
f(cid:126)   | (cid:126)x(cid:16)(cid:126)   | (cid:126)x(cid:17)

(10.32)

(10.33)

in figure 10.1 the ml estimator of    is the mode (maximum value) of the posterior distribution
when the prior is uniform. this is not a coincidence, under a uniform prior the map and ml
estimates are the same.

lemma 10.3.4. the maximum-likelihood estimator of a parameter    is the mode (maximum
value) of the pdf of the posterior distribution given the data (cid:126)x if its prior distribution is uniform.

proof. we prove the result when the model for the data and the parameters is continuous, if
any or both of them are discrete the proof is identical (in that case the ml estimator is the
mode of the pmf of the posterior). if the prior distribution of the parameters is uniform, then

f(cid:126)  (cid:0)(cid:126)  (cid:1) is constant for any (cid:126)  , which implies

arg max

(cid:126)  

f(cid:126)   | (cid:126)x(cid:16)(cid:126)  |(cid:126)x(cid:17) = arg max

(cid:126)  

f(cid:126)  (cid:0)(cid:126)  (cid:1)f (cid:126)x|(cid:126)  (cid:16)(cid:126)x|(cid:126)  (cid:17)
(cid:82)u f(cid:126)   (u) f (cid:126)x|(cid:126)   ((cid:126)x|u) du
f (cid:126)x|(cid:126)  (cid:16)(cid:126)x|(cid:126)  (cid:17)
(cid:126)   l(cid:126)x(cid:0)(cid:126)  (cid:1).

(cid:126)  

= arg max

= arg max

(the rest of the terms do not depend on (cid:126)  )

note that uniform priors are only well de   ned in situations where the parameter is restricted to
a bounded set.

we now describe a situation in which the map estimator is optimal. if the parameter    can
only take a discrete set of values, then the map estimator minimizes the id203 of making
the wrong choice.

theorem 10.3.5 (map estimator minimizes the id203 of error). let (cid:126)   be a discrete
random vector and let (cid:126)x be a random vector modeling the data. we de   ne

for any arbitrary estimator   other ((cid:126)x),

(cid:126)  

  map ((cid:126)x) := arg max

p(cid:126)   | (cid:126)x(cid:0)(cid:126)   | (cid:126)x = (cid:126)x(cid:1).
p(cid:16)  other( (cid:126)x) (cid:54)= (cid:126)  (cid:17)     p(cid:16)  map( (cid:126)x) (cid:54)= (cid:126)  (cid:17) .

in words, the map estimator minimizes the id203 of error.

(10.34)

(10.35)

(10.36)

(10.37)

chapter 10. bayesian statistics

183

proof. we assume that (cid:126)x is a continuous random vector, but the same argument applies if it is
discrete. we have

f (cid:126)x ((cid:126)x) p(cid:16)   =   other((cid:126)x)| (cid:126)x = (cid:126)x(cid:17) d(cid:126)x

p(cid:16)   =   other( (cid:126)x)(cid:17) =(cid:90)(cid:126)x
=(cid:90)(cid:126)x
   (cid:90)(cid:126)x
= p(cid:16)   =   map( (cid:126)x)(cid:17) ,

f (cid:126)x ((cid:126)x) p(cid:126)   | (cid:126)x (  other((cid:126)x)| (cid:126)x) d(cid:126)x
f (cid:126)x ((cid:126)x) p(cid:126)   | (cid:126)x (  map((cid:126)x)| (cid:126)x) d(cid:126)x

(10.38)

(10.39)

(10.40)

(10.41)

where (10.40) follows from the de   nition of the map estimator as the mode of the posterior.

example 10.3.6 (sending bits). we consider a very simple model for a communication channel
in which we aim to send a signal    consisting of a single bit. our prior knowledge indicates that
the signal is equal to one with id203 1/4.

p   (1) =

1
4

,

p   (0) =

3
4

.

(10.42)

due to the presence of noise in the channel, we send the signal n times. at the receptor we
observe

(cid:126)xi =    + (cid:126)zi,

1     i     n,

(10.43)

where (cid:126)z contains n iid standard gaussian random variables. modeling perturbations as gaus-
sian is a popular choice in communications. it is justi   ed by the central limit theorem, under
the assumption that the noise is a combination of many small e   ects that are approximately
independent.

we will now compute and compare the ml and map estimators of    given the observations.

the likelihood is equal to

it is easier to deal with the log-likelihood function,

since    only takes two values, we can compare directly. we will choose   ml ((cid:126)x) = 1 if

=

n(cid:89)i=1
n(cid:89)i=1
n(cid:88)i=1
n(cid:88)i=1
n(cid:88)i=1

l(cid:126)x (  ) =

f (cid:126)xi|   ((cid:126)xi |   )
((cid:126)xi     )2
1
   2  

e   

2

.

log l(cid:126)x (  ) =    

((cid:126)xi       )2

2

n
2

   

log 2  .

log l(cid:126)x (1) =    

(cid:126)x 2
i     2(cid:126)xi + 1

2

n
2

   

log 2  

n
2

log 2  

(cid:126)x 2
i
       
2    
= log l(cid:126)x (0) .

(10.44)

(10.45)

(10.46)

(10.47)

(10.48)

(10.49)

chapter 10. bayesian statistics

equivalently,

  ml ((cid:126)x) =(cid:40)1

0

i=1 (cid:126)xi > 1
2 ,

if 1
otherwise.

n(cid:80)n

184

(10.50)

the rule makes a lot of sense: if the sample mean of the data is closer to 1 than to 0 then our
estimate is equal to 1. by the law of total id203, the id203 of error of this estimator
is equal to

p(cid:16)   (cid:54)=   ml( (cid:126)x)(cid:17) = p(cid:16)   (cid:54)=   ml( (cid:126)x)(cid:12)(cid:12)   = 0(cid:17) p (   = 0) + p(cid:16)   (cid:54)=   ml( (cid:126)x)(cid:12)(cid:12)   = 1(cid:17) p (   = 1)
2(cid:12)(cid:12)(cid:12)(cid:12)   = 1(cid:33) p (   = 1)

2(cid:12)(cid:12)(cid:12)(cid:12)   = 0(cid:33) p (   = 0) + p(cid:32) 1

n(cid:88)i=1

(cid:126)xi >

(cid:126)xi <

(10.51)

n

1

1

n

= p(cid:32) 1
n(cid:88)i=1
= q(cid:0)   n/2(cid:1) ,

where the last equality follows from the fact that if we condition on    =    the empirical mean
is gaussian with variance   2/n and mean    (see the proof of theorem 6.2.2).

to compute the map estimate we must    nd the maximum of the posterior pdf of    given the
observed data. equivalently, we    nd the maximum of its logarithm (this is equivalent because
the logarithm is a monotone function),

i=1 f (cid:126)xi|   ((cid:126)xi|  ) p   (  )

f (cid:126)x ((cid:126)x)

log f (cid:126)xi|   ((cid:126)xi|  ) p   (  )     log f (cid:126)x ((cid:126)x)

log p  | (cid:126)x (  |(cid:126)x) = log(cid:81)n
n(cid:88)i=1
n(cid:88)i=1

=    

=

(cid:126)x 2
i     2(cid:126)xi   +   2

2

n
2

   

log 2   + log p   (  )     log f (cid:126)x ((cid:126)x) .

we compare the value of this function for the two possible values of   : 0 and 1. we choose
  map ((cid:126)x) = 1 if

log p  | (cid:126)x (1|(cid:126)x) + log f (cid:126)x ((cid:126)x) =    

(cid:126)x 2
i     2(cid:126)xi + 1

2

n
2

   

log 2       log 4

n(cid:88)i=1
n(cid:88)i=1

(cid:126)x 2
i
2    

n
2

log 2       log 4 + log 3

       
= log p  | (cid:126)x (0|(cid:126)x) + log f (cid:126)x ((cid:126)x) .

equivalently,

  map ((cid:126)x) =(cid:40)1

0

i=1 (cid:126)xi > 1

2 + log 3
n ,

if 1
otherwise.

n(cid:80)n

the map estimate shifts the threshold with respect to the ml estimate to take into account
that    is more prone to equal zero. however, the correction term tends to zero as we gather
more evidence, so if a lot of data is available the two estimators will be very similar.

(10.52)

(10.53)

(10.54)

(10.55)

(10.56)

(10.57)

(10.58)

chapter 10. bayesian statistics

185

figure 10.3: id203 of error of the ml and map estimators in example 10.3.6 for di   erent values
of n.

the id203 of error of the map estimator is equal to

p(cid:16)   (cid:54)=   map( (cid:126)x)(cid:17) = p(cid:16)   (cid:54)=   map( (cid:126)x)|   = 0(cid:17) p (   = 0) + p(cid:16)   (cid:54)=   map( (cid:126)x)|   = 1(cid:17) p (   = 1)

(cid:126)xi >

1
2

+

log 3

n

= p(cid:32) 1
n(cid:88)i=1
+ p(cid:32) 1
n(cid:88)i=1
q(cid:18)   n/2 +

3
4

=

n

(cid:126)xi <

1
2

+

log 3

   n(cid:19) +

log 3

n (cid:12)(cid:12)(cid:12)(cid:12)   = 0(cid:33) p (   = 0)
n (cid:12)(cid:12)(cid:12)(cid:12)   = 1(cid:33) p (   = 1)
   n(cid:19) .
q(cid:18)   n/2    

log 3

1
4

(10.59)

(10.60)

we compare the id203 of error of the ml and map estimators in figure 10.3. map
estimation results in better performance, but the di   erence becomes small as n increases.

(cid:52)

05101520n0.000.050.100.150.200.250.300.35id203 of errorml estimatormap estimatorchapter 11

hypothesis testing

in a medical study we observe that 10% of the women and 12.5% of the men su   er from heart
disease.
if there are 20 people in the study, we would probably be hesitant to declare that
women are less prone to su   er from heart disease than men; it is very possible that the results
occurred by chance. however, if there are 20,000 people in the study, then it seems more likely
that we are observing a real phenomenon. hypothesis testing makes this intuition precise; it is
a framework that allows us to decide whether patterns that we observe in our data are likely to
be the result of random    uctuations or not.

11.1 the hypothesis-testing framework

the aim of hypothesis testing is to evaluate a prede   ned conjecture.
in the example above,
this could be that heart disease is more prevalent in men than in women. the hypothesis that
our conjecture is false is called the null hypothesis, denoted by h0. in our example, the null
hypothesis would be that heart disease is at least as prevalent in men as in women. if the null
hypothesis holds, then whatever pattern we are detecting in our data that seems to support our
conjecture is just a    uke. there just happen to be a lot of men with heart disease (or women
without) in the study. in contrast, the hypothesis under which our conjecture is true is known as
the alternative hypothesis, denoted by h1. in this chapter we take a frequentist perspective:
the hypotheses either hold or not, they are not modeled probabilistically.

a test is a procedure to determine whether we should reject the null hypothesis or not based on
the data. rejecting the null hypothesis means that we consider unlikely that it happened, which
is evidence in favor of the alternative hypothesis. if we fail to reject the null hypothesis, this
does not mean that we consider it likely, we just don   t have enough information to discard it.
most tests produce a decision by thresholding a test statistic, which is a function that maps
the data (i.e. a vector in rn) to a single number. the test rejects the null hypothesis if the test
statistic belongs to a rejection region r. for example, we could have

r := {t | t       } ,

(11.1)

where t is the test statistic computed from the data and    is a prede   ned threshold. in this
case, we would reject the null hypothesis only if t is larger than   .

as shown in table 11.1, there are two possible errors that we can make. a type i error is
a false positive: our conjecture is false, but we reject the null hypothesis. a type ii error is

186

chapter 11. hypothesis testing

187

reject h0?

no

yes

h0 is true

h1 is true type ii error

(cid:44)

type i error

(cid:44)

table 11.1: type i and ii errors.

a false negative: our conjecture holds, but we do not reject the null hypothesis. in hypothesis
testing, our priority is to control type i errors. when you read in a study that a result is
statistically signi   cant at a level of 0.05, this means that the id203 of committing a
type i error is bounded by 5%.

de   nition 11.1.1 (signi   cance level and size). the size of a test is the id203 of making
a type i error. the signi   cance level of a test is an upper bound on the size.

rejecting the null hypothesis does not give a quantitative sense of the extent to which the data
are incompatible with the null hypothesis. the p value is a function of the data that plays this
role.

de   nition 11.1.2 (p value). the p value is the smallest signi   cance level at which we would
reject the null hypothesis for the data we observe.

for a    xed signi   cance level, it is desirable to select a test that minimizes the id203 of
making a type ii error. equivalently, we would like to maximize the id203 of rejecting
the null hypothesis when it does not hold. this id203 is known as the power of the test.

de   nition 11.1.3 (power). the power of a test is the id203 of rejecting the null hypothesis
if it does not hold.

note that in order to characterize the power of a test we need to know the distribution of
the data under the alternative hypothesis, which is often unrealistic (recall that the alternative
hypothesis is just the complement of the null hypothesis and consequently encompasses many
di   erent possibilities).

the standard procedure to apply hypothesis testing in the applied sciences is the following:

1. choose a conjecture.

2. determine the corresponding null hypothesis.

3. choose a test.

4. gather the data.

5. compute the test statistic from the data.

chapter 11. hypothesis testing

188

6. compute the p value and reject the null hypothesis if it is below a prede   ned limit (typically

1% or 5%).

example 11.1.4 (clutch). we want to test the conjecture that a certain player in the nba
is clutch, i.e. that he scores more points at the end of close games than during the rest of the
game. the null hypothesis is that there is no di   erence in his performance. the test statistic t
that we choose is whether he makes more or less points per minute in the last quarter than in
the rest of the game

t ((cid:126)x) =

1(cid:126)xi>0,

n(cid:88)i=1

(11.2)

where (cid:126)xi is the di   erence between the points per minute he scores in the 4th quarter and in the
rest of the quarters of game i for 1     i     n.
the rejection region of the test is of the form

r := {t ((cid:126)x) | t ((cid:126)x)       } ,

(11.3)

for a    xed threshold   . under the null hypothesis the id203 of scoring more points per
minute in the 4th quarter is 1/2 (for simplicity we ignore the possibility that he scores the same
number of points), so we can model the test statistic under the null hypothesis as a binomial
random variable with parameters n and 1/2.
if    is an integer between 0 and n, then the
id203 that the test statistic is in the rejection region if the null hypothesis holds is

p (t0       ) =

1
2n

n(cid:88)k=  (cid:18)n
k(cid:19).

(11.4)

so the size of the test is 1
we want a signi   cance level of 1% or 5% then we need to set the threshold at    = 16 or    = 15
respectively.

k(cid:1). table 11.2 shows this value for all possible values of   . if

2n(cid:80)n

k=  (cid:0)n

we gather the data from 20 games (cid:126)x and compute the value of the test statistic t ((cid:126)x) (note that
we use a lowercase letter because it is a speci   c realization), which turns out to be 14 (he scores
more points per minute in the fourth quarter in 14 of the games). this is not enough to reject
the null hypothesis for our prede   ned level of 1% or 5%. therefore the result is not statistically
signi   cant.

in any case, we compute the p value, which is the smallest level at which the result would have
been signi   cant. from the table it is equal to 0.058. note that under a frequentist framework we
cannot interpret this as the id203 that the null hypothesis holds (i.e. that the player is not
better in the fourth quarter) because the hypothesis is not random, it either holds or it doesn   t.
our result is almost signi   cant and although we do not have enough evidence to support our
conjecture, it does seem plausible that the player performs better in the fourth quarter.

(cid:52)

11.2 parametric testing

in this section we discuss hypothesis testing under the assumption that our data are sampled
from a known distribution with unknown parameters. we again take a frequentist perspective,

chapter 11. hypothesis testing

189

  

1

2

3

4

5

6

7

8

9

10

p (t0       )

1.000

1.000

1.000

0.999

0.994

0.979

0.942

0.868

0.748

0.588

  

11

12

13

14

15

16

17

18

19

20

p (t0       )

0.412

0.252

0.132

0.058

0.021

0.006

0.001

0.000

0.000

0.000

table 11.2: id203 of committing a type i error depending on the value of the threshold in
example 11.1.4. the values are rounded to three decimal points.

as is usually done in most studies in the applied sciences. the parameter is consequently
deterministic and so are the hypotheses: the null hypothesis is true or not, there is no such
thing as the id203 that the null hypothesis holds.

to simplify the exposition, we assume that the id203 distribution depends only on one
parameter that we denote by   . p   is the id203 measure of our id203 space if    is
the value of the parameter. (cid:126)x is a random vector distributed according to p  . the actual data
that we observe, which we denote by (cid:126)x is assumed to be a realization from this random vector.

assume that the null hypothesis is    =   0. in that case, the size of a test with test statistic t
and rejection region r is equal to

for a rejection region of the form (11.1) we have

if the realization of the test statistic is t (x1, . . . , xn) then the signi   cance level at which we
would reject h0 would be

which is the p value if we observe (cid:126)x. the p value can consequently be interpreted as the
id203 of observing a result that is more extreme than what we observe in the data if the
null hypothesis holds.
a hypothesis of the form    =   0 is known as a simple hypothesis. if a hypothesis is of the form
       s for a certain set s then the hypothesis is composite. for a composite null hypothesis
       h0 we rede   ne the size and the p value in the following way,

   = p  0(cid:16)t ( (cid:126)x)     r(cid:17) .
   := p  0(cid:16)t ( (cid:126)x)       (cid:17) .

p = p  0(cid:16)t ( (cid:126)x)     t ((cid:126)x)(cid:17) ,

   = sup
     h0
p = sup
     h0

p  (cid:16)t ( (cid:126)x)       (cid:17) ,
p  (cid:16)t ( (cid:126)x)     t ((cid:126)x)(cid:17) .

(11.5)

(11.6)

(11.7)

(11.8)

(11.9)

in order to characterize the power of the test for a certain signi   cance level, we compute the
power function.

chapter 11. hypothesis testing

190

de   nition 11.2.1 (power function). let p   be the id203 measure parametrized by    and
let r the rejection region for a test based on the test statistic t ((cid:126)x). the power function of the
test is de   ned as

   (  ) := p  (cid:16)t ( (cid:126)x)     r(cid:17)

(11.10)

ideally we would like    (  )     0 for        h0 and    (  )     1 for        h1.
example 11.2.2 (coin    ip). we are interested in checking whether a coin is biased towards
heads. the null hypothesis is that for each coin    ip the id203 of obtaining heads is        1/2.
consequently, the alternative hypothesis is    > 1/2. let us consider a test statistic equal to the
number of heads observed in a sequence of n iid    ips,

t ((cid:126)x) =

1(cid:126)xi=1,

n(cid:88)i=1

(11.11)

where (cid:126)xi is one if the ith coin    ip is heads and zero otherwise. a natural rejection region is

in particular, we consider two possible thresholds

t ((cid:126)x)       .

(11.12)

1.    = n, i.e. we only reject the null hypothesis if all the coin    ips are heads,

2.    = 3n/5, i.e. we reject the null hypothesis if at least three    fths of the coin    ips are

heads.

what test should we use if the number of coin    ips is 5, 50 or 100? do the tests have a 5%
signi   cance level? what is the power of the tests for these values of n?

to answer these questions, we compute the power function of the test for both options. if    = n,

if    = 3n/5,

  1 (  ) = p  (cid:16)t ( (cid:126)x)     r(cid:17)

=   n.

  2 (  ) =

n(cid:88)k=3n/5(cid:18)n

k(cid:19)  k (1       )n   k .

(11.13)

(11.14)

(11.15)

figure 11.1 shows the two power functions. if    = n, then the test has a signi   cance level of
5% for the three values of n. however the power is very low, especially for large n. this makes
sense: even if the coin is pretty biased the id203 of n heads is extremely low. if    = 3n/5,
then for n = 5 the test has a signi   cance level way above 5%, since even if the coin is not biased
the id203 of observing 3 heads out of 5    ips is quite high. however for large n the test has
much higher power than the    rst option. if the bias of the coin is above 0.7 we reject the null
hypothesis with high id203.

(cid:52)

chapter 11. hypothesis testing

191

   = n

   = 3n
5

figure 11.1: power functions for the tests described in example 11.2.2.

a systematic method for building tests under parametric assumptions is to threshold the ratio
between the likelihood of the data under the null hypothesis and the likelihood of the data under
the alternative hypothesis. if this ratio is high, the data are compatible with the null hypothesis,
so it should not be rejected.

de   nition 11.2.3 (likelihood-ratio test). let l(cid:126)x (  ) denote the likelihood function correspond-
ing to a data vector (cid:126)x. h0 and h1 are the sets corresponding to the null and alternative hy-
potheses respectively. the likelihood ratio is

   ((cid:126)x) :=

sup     h0 l(cid:126)x (  )
sup     h1 l(cid:126)x (  )

.

(11.16)

a likelihood-ratio test has a rejection region of the form {   ((cid:126)x)       }, for a constant threshold   .
example 11.2.4 (gaussian with known variance). imagine that you have some data that are
well modeled as iid gaussian with a known variance   . the mean is unknown and we are
interested in establishing that it is not equal to a certain value   0. what is the corresponding
likelihood-ratio test and how should be set the threshold so that we have a signi   cance level   ?

first, from example 9.6.4 the sample mean achieves the maximum of the likelihood function of
a gaussian

for any value of   . using this result, we have

av ((cid:126)x) := arg max

   l(cid:126)x (  ,   )

   ((cid:126)x) =

sup     h0 l(cid:126)x (  )
sup     h1 l(cid:126)x (  )

= l(cid:126)x (  0)
l(cid:126)x (av ((cid:126)x))

.

(11.17)

(11.18)

(11.19)

0.250.500.75  0.050.250.500.75  (  )n = 5n = 50n = 1000.250.500.75  0.050.250.500.75  (  )n = 5n = 50n = 100chapter 11. hypothesis testing

plugging in the expressions for the likelihood we obtain,

   ((cid:126)x) = exp(cid:40)   
= exp(cid:40)   
= exp(cid:40)   

1
2  2

n(cid:88)i=1(cid:16)((cid:126)xi     av ((cid:126)x))2     ((cid:126)xi       0)2(cid:17)(cid:41)
2  2(cid:32)   2 av ((cid:126)x)
n (av ((cid:126)x)       0)2

(cid:126)xi + n av ((cid:126)x)2     2  0

1

n(cid:88)i=1
(cid:41) .

2  2

192

(11.20)

(11.21)

(11.22)

(cid:126)xi + n  2

0(cid:33)(cid:41)

n(cid:88)i=1

taking logarithms, the test is of the form

|av ((cid:126)x)       0|

  

.

(11.23)

the sample mean of n independent gaussian random variables with mean   0 and variance   2
is gaussian with mean   0 and variance   2/n, which implies

n

   (cid:114)   2 log   
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

   (cid:112)   2 log         

av(cid:16) (cid:126)x(cid:17)       0
   = p  0      (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= 2 q(cid:16)(cid:112)   2 log   (cid:17) .

  /   n

if we    x a desired size    then the test becomes
|av ((cid:126)x)       0|

q   1 (  /2)

   n

.

   

  

(cid:52)
a motivating argument to employ the likelihood-ratio test is that if the null and alternative
hypotheses are simple, then it is optimal in terms of power.

lemma 11.2.5 (neyman-pearson lemma). if both the null hypothesis and the alternative hy-
pothesis are simple, i.e. the parameter    can only have two values   0 and   1, then the likelihood-
ratio test has the highest power among all tests with a    xed size.

proof. recall that the power is the id203 of rejecting the null hypothesis if it does not
hold. if we denote the rejection region of the likelihood-ratio test by rlr then its power is

assume that we have another test with rejection region r. its power is equal to

(11.24)

(11.25)

(11.26)

(11.27)

(11.28)

p  1(cid:16) (cid:126)x     rlr(cid:17) .
p  1(cid:16) (cid:126)x     r(cid:17) .

to prove that the power of the likelihood-ratio test is larger we only need to establish that

p  1(cid:16) (cid:126)x     rc     rlr(cid:17)     p  1(cid:16) (cid:126)x     rc

lr     r(cid:17) .

(11.29)

chapter 11. hypothesis testing

193

let us assume that the data are continuous random variables (the argument for discrete random
variables is practically the same) and that the pdf when the null and alternative hypotheses
hold are f  0 and f  1 respectively. by the de   nition of the rejection region of the likelihood-ratio
test, if    ((cid:126)x)     rlr

whereas if    ((cid:126)x)     rc

lr

if both tests have size    then

f  1 ((cid:126)x)    

f  0 ((cid:126)x)

  

f  1 ((cid:126)x)    

f  0 ((cid:126)x)

  

,

.

p  0(cid:16) (cid:126)x     r(cid:17) =    = p  0(cid:16) (cid:126)x     rlr(cid:17) .

and consequently

p  0(cid:16) (cid:126)x     rc     rlr(cid:17) = p  0(cid:16) (cid:126)x     rlr(cid:17)     p  0(cid:16) (cid:126)x     r     rlr(cid:17)
= p  0(cid:16) (cid:126)x     r(cid:17)     p  0(cid:16) (cid:126)x     r     rlr(cid:17)
= p  0(cid:16) (cid:126)x     r     rc

lr(cid:17) .

now let us prove that (11.29) holds,

(11.30)

(11.31)

(11.32)

(11.33)

(11.34)

(11.35)

(11.36)

1

=

   

f  0 ((cid:126)x) d(cid:126)x

f  1 ((cid:126)x) d(cid:126)x

p  1(cid:16) (cid:126)x     rc     rlr(cid:17) =(cid:90)(cid:126)x   rc   rlr
  (cid:90)(cid:126)x   rc   rlr
p  0(cid:16) (cid:126)x     rc     rlr(cid:17)
p  0(cid:16) (cid:126)x     r     rc
lr(cid:17)
  (cid:90)(cid:126)x   r   rc
   (cid:90)(cid:126)x   r   rc
= p  1(cid:16) (cid:126)x     r     rc

lr(cid:17) .

f  0 ((cid:126)x) d(cid:126)x

f  1 ((cid:126)x) d(cid:126)x

1
  
1
  
1

=

=

lr

lr

by (11.30)

(11.37)

(11.38)

by (11.35)

(11.39)

by (11.31)

(11.40)

(11.41)

(11.42)

11.3 nonparametric testing: the permutation test

in practical situations we may not able to design a parametric model that is adequate for
our data. nonparametric tests are hypothesis tests that do not assume that the data follow

chapter 11. hypothesis testing

194

any distribution with a prede   ned form. in this section we describe the permutation test, a
nonparametric test that can be used to compare two data sets (cid:126)xa and (cid:126)xb in order to evaluate
conjectures of the form (cid:126)xa is sampled from a distribution that has a higher mean than (cid:126)xb or (cid:126)xb
is sampled from a distribution that has a higher variance than (cid:126)xa. the null hypothesis is that
the two data sets are actually sampled from the same distribution.

the test statistic in a permutation test is the di   erence between the values of a test statistic of
interest t evaluated on the two data sets

tdi    ((cid:126)x) := t ((cid:126)xa)     t ((cid:126)xb) ,

(11.43)

where (cid:126)x are all the data merged together. our goal is to test whether t ((cid:126)xa) is larger than t ((cid:126)xb)
at a certain signi   cance level. the corresponding rejection region is of the form r := {t | t       }.
the problem is how to    x the threshold so that the test has the desired signi   cance level.

imagine that we randomly permute the labels a and b in the merged data set (cid:126)x. as a result,
some of the data that were labeled as a will be labeled as b and vice versa. if we recompute
tdi    ((cid:126)x) we will obviously obtain a di   erent value. however, the distribution of the random
variable tdi   ( (cid:126)x) under the hypothesis that the data are sampled from the same distribution
has not changed. indeed, the null hypothesis implies that the distribution of any function of
(cid:126)x1, (cid:126)x2, . . . , (cid:126)xn that only depends on the class assigned to each variable is invariant to permu-
tations. more formally, the random sequence is exchangeable with respect to such functions.

consider the value of tdi    for all the possible permutations of the labels: tdi   ,1, tdi   ,2, . . . tdi   ,n!. if
the null hypothesis holds, then it would be surprising to    nd that tdi    ((cid:126)x) is larger than most of
the tdi   ,i. in fact, under the null hypothesis, the random variable tdi   ( (cid:126)x) is uniformly distributed
in the set {tdi   ,1, tdi   ,2, . . . tdi   ,n!}, so that

p(cid:16)tdi   ( (cid:126)x)       (cid:17) =

1
n!

n!(cid:88)i=1

1tdi   ,i     .

(11.44)

this is exactly to the size of the test. we can therefore compute the p value of the observed
statistic tdi    ((cid:126)x) as

p = p(cid:16)tdi   ( (cid:126)x)     tdi    ((cid:126)x)(cid:17)

=

1
n!

n!(cid:88)i=1

1tdi   ,i   tdi   ((cid:126)x).

(11.45)

(11.46)

in words, the p value is the fraction of permutations that yield a more extreme test statistic
than the one we observe. unfortunately, it is often challenging to compute (11.46) exactly.
even for moderately sized data sets the number of possible permutations is usually too large
(for example, 40! > 8 1047) for it to be computationally tractable. in such cases the p value
can be approximated by sampling a large number of permutations and making a monte carlo
approximation of (11.46) with its average.

before looking at an example, let us review the steps to be followed when applying a permutation
test.

1. choose a conjecture as to how (cid:126)xa and (cid:126)xb are di   erent.

chapter 11. hypothesis testing

195

cholesterol

blood pressure

figure 11.2: histograms of the cholesterol and blood-pressure for men and women in example 11.3.1.

2. choose a test statistic tdi   .

3. compute tdi    ((cid:126)x).

4. permute the labels m times and compute the corresponding values of tdi   : tdi   ,1, tdi   ,2,

. . . tdi   ,m.

5. compute the approximate p value

p = p(cid:16)tdi   ( (cid:126)x)     tdi    ((cid:126)x)(cid:17)

1tdi   ,i   tdi   ((cid:126)x)

=

1
m

m(cid:88)i=1

(11.47)

(11.48)

and reject the null hypothesis if it is below a prede   ned limit (typically 1% or 5%).

example 11.3.1 (cholesterol and blood pressure). a scientist want to determine whether men
have higher cholesterol and blood pressure. she gathers data from 86 men and 182 women.
figure 11.2 shows the histograms of the cholesterol and blood-pressure for men and women.
from the histograms it seems that men have higher levels of cholesterol and blood pressure.
the sample mean for cholesterol is 261.3 mg/dl amongst men and 242.0 mg/dl amongst women.
the sample mean for blood pressure is 133.2 mmhg amongst men and 130.6 mmhg amongst
women.

in order to quantify whether these di   erences are signi   cant we compute the sample permutation
distribution of the di   erence between the sample means using 106 permutations. to make sure
that the results are stable, we repeat the procedure three times. the results are shown in
figure 11.3. for cholesterol, the p value is around 0.1%, so we have very strong evidence against
the null hypothesis. in contrast, the p value for blood pressure is 13%, so the results are not
very conclusive, we cannot reject the possibility that the di   erence is merely due to random
   uctuations.

1001502002503003504004500246810121416menwomen80100120140160180200220051015202530menwomenchapter 11. hypothesis testing

196

cholesterol

blood pressure

p value = 0.119%

p value = 13.48%

p value = 0.112%

p value = 13.56%

p value = 0.115%

p value = 13.50%

figure 11.3: approximate distribution under the null hypothesis of the di   erence between the sample
means of cholesterol and blood pressure in men and women. the observed value for the test statistic is
marked by a dashed line.

20.0010.000.0010.0019.220.000.010.020.030.040.050.060.075.00.05.02.60.000.050.100.150.2020.0010.000.0010.0019.220.000.010.020.030.040.050.060.075.00.05.02.60.000.050.100.150.2020.0010.000.0010.0019.220.000.010.020.030.040.050.060.075.00.05.02.60.000.050.100.150.20chapter 11. hypothesis testing

197

(cid:52)

11.4 multiple testing

in some applications, it is common to conduct many simultaneous hypothesis tests. for example,
in computational genomics a researcher might be interested in testing whether any gene within
a group of several thousand is relevant to a certain disease. if we apply a hypothesis test with
size    in this setting, then the id203 of obtaining a false positive for a particular gene is
  . now, assume that we test n genes and that the events gene i is a false positive, 1     i     n
are all mutually independent. the id203 of obtaining at least one false positive is

p (at least one false positive) = 1     p (no false positives)

= 1     (1       )n .

(11.49)

(11.50)

for    = 0.01 and n = 500 this id203 is equal to 0.99! if we want to control the id203
of making a type i error we must take into account that we are carrying out multiple tests at
the same time. a popular procedure to do this is bonferroni   s method.

de   nition 11.4.1 (bonferroni   s method). given n hypothesis tests, compute the corresponding
p values p1, . . . , pn. for a    xed signi   cance level    reject the ith null hypothesis if

pi    

  
n

.

(11.51)

the following lemma shows that the method guarantees that the desired signi   cance level holds
simultaneously for all the tests.

lemma 11.4.2. if we apply bonferroni   s method, the id203 of making a type i error is
bounded by   .

proof. the result follows directly from the union bound, which controls the id203 of a
union of events with the sum of their individual probabilities.
theorem 11.4.3 (union bound). let (   ,f, p) be a id203 space and s1, s2, . . . a collection
of events in f. then

p (si) .

(11.52)

p (   isi)    (cid:88)i

proof. let us de   ne the sets:

it is straightforward to show by induction that    n
sets   s1,   s2, . . . are disjoint by construction, so

  si = si        i   1

j=1sc
j .
j=1sj =    n

j=1

(11.53)
  sj for any n, so    isi =    i   si. the

p (   isi) = p(cid:16)   i   si(cid:17) =(cid:88)i
   (cid:88)i

p(cid:16)   si(cid:17) by axiom 2 in de   nition 1.1.4

p (si)

because   si     si.

(11.54)

(11.55)

chapter 11. hypothesis testing

applying the bound,

p (type i error) = p (   n
n(cid:88)i=1

   

= n   

  
n

i=1type i error for test i)

p (type i error for test i)

198

(11.56)

by the union bound

(11.57)

=   .

(11.58)

example 11.4.4 (clutch (continued)). if we apply the test in example 11.1.4 to 10 players,
the id203 that one of them seems to be clutch just due to chance increases substantially.
to control for this, by bonferroni   s method we must divide the p values of the individual tests
by 10. as a result, to maintain a signi   cance level of 0.05 we would require that each player
score more points per minute during the last quarter in 17 of the 20 games instead of 15 (see
table 11.2) in order to reject the null hypothesis.

(cid:52)

chapter 12

id75

in statistics, regression is the problem of characterizing the relation between a certain quantity
of interest y, called the response or the dependent variable, to several observed variables
x1, x2, . . . , xp, known as covariates, features or independent variables. for example, the
response could be price of a house and the covariates could correspond to the extension, the
number of rooms, the year it was built, etc. a regression model would describe how house prices
are a   ected by all of these factors.

more formally, the main assumption in regression models is that the predictor is generated
according to a function h applied to the features and then perturbed by some unknown noise z,
which is often additive,

y = h ((cid:126)x) + z.

the aim is to learn h from n examples of responses and their corresponding features

in this chapter we focus on the case where h is a linear function.

(cid:16)y(1), (cid:126)x (1)(cid:17) ,(cid:16)y(2), (cid:126)x (2)(cid:17) , . . . ,(cid:16)y(n), (cid:126)x (n)(cid:17) .

(12.1)

(12.2)

12.1 linear models

if the regression function h in a model of the form 12.1 is linear, then the response is modeled
as a linear combination of the predictors:

y(i) = (cid:126)x (i) t (cid:126)      + z(i),

1     i     n,

(12.3)

where z(i) is an entry of the unknown noise vector. the function is parametrized by a vector of
weights (cid:126)          rp. all we need to    t the linear model to the data is to estimate these weights.

expressing the linear system (12.3) in matrix form yields the following representation of the
linear-regression model

y(1)

y(2)
      
y(n)

                        

                        

=

(cid:126)x (1)
1
(cid:126)x (2)
1
      
(cid:126)x (n)
1

                        

                        

                        

(cid:126)     1
(cid:126)     2
      
(cid:126)     p

                        

z(1)

z(2)
      
z(n)

                        

                        

(cid:126)x (1)
2
(cid:126)x (2)
2
      
(cid:126)x (n)
2

p

p

       (cid:126)x (1)
       (cid:126)x (2)
      
      
       (cid:126)x (n)

p

199

+

.

(12.4)

200

(12.5)

chapter 12. id75

equivalently,

(cid:126)y = x (cid:126)      + (cid:126)z,

where x is a n   p matrix containing the features, (cid:126)y contains the response and (cid:126)z     rn represents

the noise.

example 12.1.1 (linear model for gdp). we consider the problem of building a linear model
to predict the gross domestic product (gdp) of a state in the us from its population and
unemployment rate. we have available the following data:

gdp

population unemployment

(usd millions)

rate (%)

north dakota

alabama

mississippi

arkansas

kansas

georgia

iowa

west virginia

kentucky

tennessee

                                                                              

52 089

204 861

107 680

120 689

153 258

757 952

4 863 300

2 988 726

2 988 248

2 907 289

525 360

10 310 371

178 766

3 134 693

73 374

1 831 102

197 043

4 436 974

???

6 651 194

2.4

3.8

5.2

3.5

3.8

4.5

3.2

5.1

5.2

3.0

                                                                              

in this example, the gdp is the response, and the population and the unemployment rate are
the features. our goal is to    t a linear model to the data so that we can predict the gdp of
tennessee, using a linear model. we begin by centering and normalizing the data. the averages
of the response and of the features are

av ((cid:126)y) = 179 236,

the empirical standard deviations are

std ((cid:126)y) = 396 701,

av (x) =(cid:104)3 802 073 4.1(cid:105) .
std (x) =(cid:104)7 720 656 2.80(cid:105) .

(12.6)

(12.7)

we subtract the average and divide by the standard deviations so that both the response and

chapter 12. id75

201

the features are centered and on the same scale,

(cid:126)y =

,

x =

   0.321
0.065
   0.180
   0.148
   0.065
0.872
   0.001
   0.267
0.045

                                                                        

                                                                        

   0.394    0.600
0.137    0.099
   0.105
0.401
   0.105    0.207
   0.116    0.099
0.843
0.151
   0.086    0.314
   0.255
0.366
0.082

0.401

                                                                        

.

                                                                        

(12.8)

(12.9)

(cid:126)y     x (cid:126)  ,

to obtain the estimate for the gdp of tennessee we    t the model

rescale according to the standard deviations (12.7) and recenter using the averages (12.6). the
   nal estimate is

(cid:126)yten = av ((cid:126)y) + std ((cid:126)y)(cid:68)(cid:126)xten

norm, (cid:126)  (cid:69)

where (cid:126)xten

norm is centered using av (x) and normalized using std (x).

(12.10)

(cid:52)

12.2 least-squares estimation

to calibrate the id75 model, we need to estimate the weight vector so that it yields

a good    t to the data. we can evaluate the    t for a speci   c choice of (cid:126)       rp using the sum of

the squares of the error,

the least-squares estimate (cid:126)  ls is the vector of weights that minimizes this cost function,

n(cid:88)i=1(cid:16)y(i)     (cid:126)x (i) t (cid:126)  (cid:17)2

(cid:126)  ls := arg min
(cid:126)  

=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)y     x (cid:126)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)y     x (cid:126)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

.

2

2

.

(12.11)

(12.12)

the least-squares cost function is convenient from a computational view, since it is convex and
can be minimized e   ciently (in fact, as we will see in a moment it has a closed-form solution).
in addition, it has intuitive geometric and probabilistic interpretations. figure 12.1 shows the
linear model learnt using least squares in a simple example where there is just one feature (p = 1)
and 40 examples (n = 40).

chapter 12. id75

202

figure 12.1: linear model learnt via least-squares    tting for a simple example where there is just one
feature (p = 1) and 40 examples (n = 40).

example 12.2.1 (linear model for gdp (continued)). the least-squares estimate for the re-
gression coe   cients in the linear gdp model is equal to

(cid:126)  ls =      

1.019

   0.111       .

(12.13)

the gdp seems to be proportional to the population and inversely proportional to the unem-
ployment rate. we now compare the    t provided by the linear model to the original data, as
well as its prediction of the gdp of tennessee:

north dakota

alabama

mississippi

arkansas

kansas

georgia

iowa

west virginia

kentucky

tennessee

gdp

estimate

52 089

46 241

204 861

107 680

120 689

239 165

119 005

145 712

153 258

136 756

525 360

513 343

178 766

158 097

73 374

59 969

197 043

194 829

328 770

345 352

                                                                              

                                                                              

0.00.20.40.60.81.01.2x0.00.20.40.60.81.01.2ydataleast-squares fitchapter 12. id75

203

figure 12.2: illustration of corollary 12.2.3. the least-squares solution is a projection of the data onto
the subspace spanned by the columns of x , denoted by x1 and x2.

12.2.1 geometric interpretation

the following theorem, proved in section 12.2.2, shows that the least-squares problem has a
closed form solution.
theorem 12.2.2 (least-squares solution). for p     n, if x is full rank then the solution to the
least-squares problem (12.12) is

(cid:52)

(cid:126)  ls :=(cid:0)x tx(cid:1)   1

x t (cid:126)y.

(12.14)

a corollary to this result provides a geometric interpretation for the least-squares estimate of
(cid:126)y: it is obtained by projecting the response onto the column space of the matrix formed by the
predictors.
corollary 12.2.3. for p     n, if x is full rank then x (cid:126)  ls is the projection of (cid:126)y onto the column
space of x .
we provide a formal proof in section 12.5.2 of the appendix, but the result is very intuitive.
any vector of the form x (cid:126)   is in the span of the columns of x . by de   nition, the least-squares
estimate is the closest vector to (cid:126)y that can be represented in this way, so it is the projection of
(cid:126)y onto the column space of x . this is illustrated in figure 12.2.

12.2.2 probabilistic interpretation
if we model the noise in (12.5) as a realization from a random vector (cid:126)z which has entries that are
independent gaussian random variables with mean zero and a certain variance   2, then we can

chapter 12. id75

204

interpret the least-squares estimate as a maximum-likelihood estimate. under that assumption,
the data are a realization of the random vector

which is an iid gaussian random vector with mean x (cid:126)   and covariance matrix   2i. the joint
pdf of (cid:126)y is equal to

(cid:126)y := x (cid:126)   + (cid:126)z,

(12.15)

the likelihood is the id203 density function of (cid:126)y evaluated at the observed data (cid:126)y and
interpreted as a function of the weight vector (cid:126)  ,

to    nd the ml estimate, we maximize the log likelihood. we conclude that it is given by the
solution to the least-squares problem, since

f(cid:126)y ((cid:126)a) =

=

l(cid:126)y(cid:16)(cid:126)  (cid:17) =

2

1

1

1

1

1

2  2(cid:16)(cid:126)ai    (cid:16)x (cid:126)  (cid:17)i(cid:17)2(cid:19)
2(cid:19) .
2  2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)a     x (cid:126)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
2(cid:19) .
2(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)y     x (cid:126)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

   2    
1

exp(cid:18)   
exp(cid:18)   

n(cid:89)i=1
(cid:112)(2  )n  n
(cid:112)(2  )n exp(cid:18)   
(cid:126)   l(cid:126)y(cid:16)(cid:126)  (cid:17)
log l(cid:126)y(cid:16)(cid:126)  (cid:17)
(cid:126)   (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)y     x (cid:126)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:126)  ml = arg max

= arg max

= arg min

= (cid:126)  ls.

(cid:126)  

2

2

2

(12.16)

(12.17)

(12.18)

(12.19)

(12.20)

(12.21)

(12.22)

12.3 over   tting

imagine that a friend tells you:

i found a cool way to predict the temperature in new york: it   s just a linear combination of the
temperature in every other state. i    t the model on data from the last month and a half and it   s
perfect!

your friend is not lying, but the problem is that she is using a number of data points to    t the
linear model that is roughly the same as the number of parameters. if n     p we can    nd a (cid:126)  
such that (cid:126)y = x (cid:126)   exactly, even if (cid:126)y and x have nothing to do with each other! this is called
over   tting and is usually caused by using a model that is too    exible with respect to the number
of data that are available.

to evaluate whether a model su   ers from over   tting we separate the data into a training set
and a test set. the training set is used to    t the model and the test set is used to evaluate the
error. a model that over   ts the training set will have very low error when evaluated on the
training examples, but will not generalize well to the test examples.

chapter 12. id75

205

figure 12.3 shows the result of evaluating the training error and the test error of a linear model
with p = 50 parameters    tted from n training examples. the training and test data are generated
by    xing a vector of weights (cid:126)      and then computing

(cid:126)ytrain = xtrain (cid:126)      + (cid:126)ztrain,
(cid:126)ytest = xtest (cid:126)     ,

(12.23)

(12.24)

where the entries of xtrain, xtest, (cid:126)ztrain and (cid:126)      are sampled independently at random from
a gaussian distribution with zero mean and unit variance. the training and test errors are
de   ned as

errortrain = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xtrain (cid:126)  ls     (cid:126)ytrain(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
errortest = (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xtest (cid:126)  ls     (cid:126)ytest(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

||(cid:126)ytrain||2

||(cid:126)ytest||2

.

,

(12.25)

(12.26)

note that even the true (cid:126)      does not achieve zero training error because of the presence of the
noise, but the test error is actually zero if we manage to estimate (cid:126)      exactly.
the training error of the linear model grows with n. this makes sense as the model has to    t
more data using the same number of parameters. when n is close to p := 50, the    tted model
is much better than the true model at replicating the training data (the error of the true model
is shown in green). this is a sign of over   tting: the model is adapting to the noise and not
learning the true linear structure. indeed, in that regime the test error is extremely high. at
larger n, the training error rises to the level achieved by the true linear model and the test error
decreases, indicating that we are learning the underlying model.

12.4 global warming

in this section we describe an application of id75 to climate data. in particular, we
analyze temperature data taken in a weather station in oxford over 150 years.1 our objective is
not to perform prediction, but rather to determine whether temperatures have risen or decreased
during the last 150 years in oxford.

in order to separate the temperature into di   erent components that account for seasonal e   ects
we use a simple linear model with three predictors and an intercept

(cid:126)yt     (cid:126)  0 + (cid:126)  1 cos(cid:18) 2  t

12(cid:19) + (cid:126)  2 sin(cid:18) 2  t

12(cid:19) + (cid:126)  3 t

(12.27)

where 1     t     n denotes the time in months (n equals 12 times 150). the corresponding matrix
1the data
oxforddata.txt.

at http://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/

available

is

chapter 12. id75

206

figure 12.3: relative (cid:96)2-norm error in estimating the response achieved using least-squares regression
for di   erent values of n (the number of training data). the training error is plotted in blue, whereas the
test error is plotted in red. the green line indicates the training error of the true model used to generate
the data.

.

(12.28)

of predictors is

x :=

1

1
      
1

                        

cos(cid:0) 2  t1
12 (cid:1)
sin(cid:0) 2  t1
12 (cid:1)
sin(cid:0) 2  t2
cos(cid:0) 2  t2
12 (cid:1)
12 (cid:1)
12 (cid:1) sin(cid:0) 2  tn
cos(cid:0) 2  tn
12 (cid:1)

      

      

t1

t2
      
tn

                        

the intercept (cid:126)  0 represents the mean temperature, (cid:126)  1 and (cid:126)  2 account for periodic yearly    uctu-
ations and (cid:126)  3 is the overall trend. if (cid:126)  3 is positive then the model indicates that temperatures
are increasing, if it is negative then it indicates that temperatures are decreasing.

the results of    tting the linear model are shown in figures 12.4 and 12.5. the    tted model
indicates that both the maximum and minimum temperatures have an increasing trend of about
0.8 degrees celsius (around 1.4 degrees fahrenheit).

12.5 proofs

12.5.1 proof of proposition 12.2.2
let x = u    vt be the id166 (svd) of x . under the conditions of the

theorem,(cid:0)x tx(cid:1)   1 x t y = v    u t . we begin by separating (cid:126)y into two components

(12.29)

y = u u t y +(cid:0)i     u u t(cid:1) y

10020030040050050n0.00.10.20.30.40.5relative error (l2 norm)error (training)error (test)noise level (training)chapter 12. id75

207

maximum temperature

minimum temperature

figure 12.4: temperature data together with the linear model described by (12.27) for both maximum
and minimum temperatures.

18601880190019201940196019802000051015202530temperature (celsius)datamodel1860188019001920194019601980200010505101520temperature (celsius)datamodel1900190119021903190419050510152025temperature (celsius)datamodel190019011902190319041905202468101214temperature (celsius)datamodel19601961196219631964196550510152025temperature (celsius)datamodel196019611962196319641965105051015temperature (celsius)datamodelchapter 12. id75

208

maximum temperature

minimum temperature

+ 0.75    c / 100 years

+ 0.88    c / 100 years

figure 12.5: temperature trend obtained by    tting the model described by (12.27) for both maximum
and minimum temperatures.

where u u t y is the projection of (cid:126)y onto the column space of x . note that (cid:0)i     u u t(cid:1) y is
orthogonal to the column space of x and consequently to both u u t y and x (cid:126)   for any (cid:126)  . by
pythagoras   s theorem

2

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)y     x (cid:126)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

=(cid:12)(cid:12)(cid:12)(cid:12)(cid:0)i     u u t(cid:1) y(cid:12)(cid:12)(cid:12)(cid:12)2

2 +(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)u u t y     x (cid:126)  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

2

2

.

(12.30)

the minimum value of this cost function that can be achieved by optimizing over   beta is ||(cid:126)yx
this can be achieved by solving the system of equations

   ||2
2.

since u t u = i because p     n, multiplying both sides of the equality yields the equivalent
system

u u t y = x (cid:126)   = u    vt (cid:126)  .

(12.31)

since x is full rank,    and v are square and invertible (and by de   nition of the svd v    1 = v t ),
so

u t y =    vt (cid:126)  .

(12.32)

(cid:126)  ls = v    u t y

(12.33)

is the unique solution to the system and consequently also of the least-squares problem.

18601880190019201940196019802000051015202530temperature (celsius)datatrend1860188019001920194019601980200010505101520temperature (celsius)datatrendchapter 12. id75

209

12.5.2 proof of corollary 12.2.3
let x = u    v t be the id166 of x . since x is full rank and p     n we
have u t u = i, v t v = i and    is a square invertible matrix, which implies

x (cid:126)  ls = x(cid:0)x tx(cid:1)   1

x t y

= u    v t(cid:0)v    u t u    v t(cid:1) v    u t y

= u u t y.

(12.34)

(12.35)

(12.36)

appendix a

set theory

this chapter provides a review of basic concepts in set theory.

a.1 basic de   nitions

a set is a collection of objects. the set containing every possible object that we consider in a
certain situation is called the universe and is usually denoted by    . if an object x in     belongs
to set s, we say that x is an element of s and write x     s. if x is not an element of s then
we write x /    s. the empty set, usually denoted by    , is a set such that x /    for all x         (i.e.
it has no elements). if all the elements in a set b also belong to a set a then b is a subset of
a, which we denote by b     a. if in addition there is at least one element of a that does not
belong to b then b is a proper subset of a, denoted by b     a.
the elements of a set can be arbitrary objects and in particular they can be sets themselves.
this is the case for the power set of a set, de   ned in the next section.

a useful way of de   ning a set is through a statement concerning its elements. let s be the set
of elements such that a certain statement s(x) holds, to de   ne s we write

for example, a := {x | 1 < x < 3} is the set of all elements greater than 1 and smaller than 3.
let us de   ne some important sets and set operations using this notation.

s := {x | s(x)} .

(a.1)

a.2 basic operations

de   nition a.2.1 (set operations).

    the complement sc of a set s contains all elements that are not in s.

sc := {x | x /    s} .

    the union of two sets a and b contains the objects that belong to a or b.

a     b := {x | x     a or x     b} .

210

(a.2)

(a.3)

appendix a. set theory

this can be generalized to a sequence of sets a1, a2, . . .

(cid:91)n

an := {x | x     an for some n} ,

where the sequence may be in   nite.

211

(a.4)

    the intersection of two sets a and b contains the objects that belong to a and b.

again, this can be generalized to a sequence,

a     b := {x | x     a and x     b} .

(cid:92)n

an := {x | x     an for all n} .

    the di   erence of two sets a and b contains the elements in a that are not in b.

a/b := {x | x     a and x /    b} .

(a.5)

(a.6)

(a.7)

    the power set 2s of a set s is the set of all possible subsets of s, including     and s.

    the cartesian product of two sets s1 and s2 is the set of all ordered pairs of elements

in the sets

2s :=(cid:8)s(cid:48) | s(cid:48)     s(cid:9) .

(a.8)

s1    s2 := {(x1, x2) | x1     s1, x2     s2} .
an example is r2 = r    r, the set of all possible pairs of real numbers.
two sets are equal if they have the same elements, i.e. a = b if and only if a     b and b     a.
it is easy to verify for instance that (ac)c = a, s         =    , s         = s or the following identities
which are known as de morgan   s laws.

(a.9)

theorem a.2.2 (de morgan   s laws). for any two sets a and b

(a     b)c = ac     bc,
(a     b)c = ac     bc.

(a.10)

(a.11)

proof. let us prove the    rst identity; the proof of the second is almost identical.
first we prove that (a     b)c     ac   bc. a standard way to prove the inclusion of a set in another
set is to show that if an element belongs to the    rst set then it must also belong to the second.
any element x in (a     b)c (if the set is empty then the inclusion holds trivially, since         s for
any set s) is in ac; otherwise it would belong to a and consequently to a    b. similarly, x also
belongs to bc. we conclude that x belongs to ac     bc, which proves the inclusion.
to complete the proof we establish ac     bc     (a     b)c. if x     ac     bc, then x /    a and x /    b,
so x /    a     b and consequently x     (a     b)c.

appendix b

id202

this chapter provides a review of basic concepts in id202.

b.1 vector spaces
you are no doubt familiar with vectors in r2 or r3, i.e.

(cid:126)x =      

2.2

3        ,

(cid:126)y =               

   1
0

5

               

.

(b.1)

from the point of view of algebra, vectors are much more general objects. they are elements of
sets called vector spaces that satisfy the following de   nition.

de   nition b.1.1 (vector space). a vector space consists of a set v and two operations + and
   satisfying the following conditions.

1. for any pair of elements (cid:126)x, (cid:126)y     v the vector sum (cid:126)x + (cid:126)y belongs to v.
2. for any (cid:126)x     v and any scalar        r the scalar multiple       (cid:126)x     v.
3. there exists a zero vector or origin (cid:126)0 such that (cid:126)x + (cid:126)0 = (cid:126)x for any (cid:126)x     v.
4. for any (cid:126)x     v there exists an additive inverse (cid:126)y such that (cid:126)x + (cid:126)y = (cid:126)0, usually denoted as

   (cid:126)x.

5. the vector sum is commutative and associative, i.e. for all (cid:126)x, (cid:126)y     v
((cid:126)x + (cid:126)y) + z = (cid:126)x + ((cid:126)y + z).

(cid:126)x + (cid:126)y = (cid:126)y + (cid:126)x,

6. scalar multiplication is associative, for any   ,        r and (cid:126)x     v

   (      (cid:126)x) = (     )    (cid:126)x.

7. scalar and vector sums are both distributive, i.e. for all   ,        r and (cid:126)x, (cid:126)y     v

(   +   )    (cid:126)x =       (cid:126)x +       (cid:126)x,       ((cid:126)x + (cid:126)y) =       (cid:126)x +       (cid:126)y.

(b.2)

(b.3)

(b.4)

212

appendix b. id202

213

a subspace of a vector space v is a subset of v that is also itself a vector space.
from now on, for ease of notation we will ignore the symbol for the scalar product   , writing
      (cid:126)x as    (cid:126)x.
remark b.1.2 (more general de   nition). we can de   ne vector spaces over an arbitrary    eld,
instead of r, such as the complex numbers c. we refer to any id202 text for more
details.

vector-scalar product. in this case the zero vector is the all-zero vector(cid:104)0 0 0 . . .(cid:105)t

we can easily check that rn is a valid vector space together with the usual vector addition and
. when
thinking about vector spaces it is a good idea to have r2 or r3 in mind to gain intuition, but
it is also important to bear in mind that we can de   ne vector sets over many other objects,
such as in   nite sequences, polynomials, functions and even random variables as in the following
example.

the de   nition of vector space guarantees that any linear combination of vectors in a vector
space v, obtained by adding the vectors after multiplying by scalar coe   cients, belongs to v.
given a set of vectors, a natural question to ask is whether they can be expressed as linear
combinations of each other, i.e. if they are linearly dependent or independent.

de   nition b.1.3 (linear dependence/independence). a set of m vectors (cid:126)x1, (cid:126)x2, . . . , (cid:126)xm is lin-
early dependent if there exist m scalar coe   cients   1,   2, . . . ,   m which are not all equal to zero
and such that

  i (cid:126)xi = (cid:126)0.

m(cid:88)i=1

(b.5)

otherwise, the vectors are linearly independent.

equivalently, at least one vector in a linearly dependent set can be expressed as the linear com-
bination of the rest, whereas this is not the case for linearly independent sets.

let us check the equivalence. equation (b.5) holds with   j (cid:54)= 0 for some j if and only if

(cid:126)xj =

1

  j (cid:88)i   {1,...,m}/{j}

  i (cid:126)xi.

(b.6)

we de   ne the span of a set of vectors {(cid:126)x1, . . . , (cid:126)xm} as the set of all possible linear combinations
of the vectors:

span ((cid:126)x1, . . . , (cid:126)xm) :=(cid:40)(cid:126)y | (cid:126)y =

  i (cid:126)xi

m(cid:88)i=1

for some   1,   2, . . . ,   m     r(cid:41) .

(b.7)

this turns out to be a vector space.

lemma b.1.4. the span of any set of vectors (cid:126)x1, . . . , (cid:126)xm belonging to a vector space v is a
subspace of v.

appendix b. id202

214

proof. the span is a subset of v due to conditions 1 and 2 in de   nition b.1.1. we now show
that it is a vector space. conditions 5, 6 and 7 in de   nition b.1.1 hold because v is a vector
space. we check conditions 1, 2, 3 and 4 by proving that for two arbitrary elements of the span

(cid:126)y1 =

  i (cid:126)xi,

(cid:126)y2 =

  i (cid:126)xi,   1, . . . ,   m,   1, . . . ,   m     r,

m(cid:88)i=1

m(cid:88)i=1

  1 (cid:126)y1 +   2 (cid:126)y2 also belongs to the span. this holds because

(b.8)

(b.9)

  1 (cid:126)y1 +   2 (cid:126)y2 =

(  1   i +   2   i) (cid:126)xi,

m(cid:88)i=1

so   1 (cid:126)y1 +   2 (cid:126)y2 is in span ((cid:126)x1, . . . , (cid:126)xm). now to prove condition 1 we set   1 =   2 = 1, for
condition 2   2 = 0, for condition 3   1 =   2 = 0 and for condition 4   1 =    1,   2 = 0.
when working with a vector space, it is useful to consider the set of vectors with the smallest
cardinality that spans the space. this is called a basis of the vector space.
de   nition b.1.5 (basis). a basis of a vector space v is a set of independent vectors {(cid:126)x1, . . . , (cid:126)xm}
such that

v = span ((cid:126)x1, . . . , (cid:126)xm) .

(b.10)

an important property of all bases in a vector space is that they have the same cardinality.
theorem b.1.6. if a vector space v has a basis with    nite cardinality then every basis of v
contains the same number of vectors.

this theorem, which is proved in section b.8.1, allows us to de   ne the dimension of a vector
space.
de   nition b.1.7 (dimension). the dimension dim (v) of a vector space v is the cardinality
of any of its bases, or equivalently the smallest number of linearly independent vectors that span
v.
this de   nition coincides with the usual geometric notion of dimension in r2 and r3: a line
has dimension 1, whereas a plane has dimension 2 (as long as they contain the origin). note
that there exist in   nite-dimensional vector spaces, such as the continuous real-valued functions
de   ned on [0, 1].

the vector space that we use to model a certain problem is usually called the ambient space
and its dimension the ambient dimension. in the case of rn the ambient dimension is n.
lemma b.1.8 (dimension of rn). the dimension of rn is n.
proof. consider the set of vectors (cid:126)e1, . . . , (cid:126)en     rn de   ned by

(cid:126)e1 =

,

(cid:126)e2 =

,

. . . ,

(cid:126)en =

.

(b.11)

1

0
      
0

                        

                        

0

1
      
0

                        

                        

0

0
      
1

                        

                        

appendix b. id202

215

one can easily check that this set is a basis. it is in fact the standard basis of rn.

b.2

inner product and norm

up to now, the only operations we have considered are addition and multiplication by a scalar.
in this section, we introduce a third operation, the inner product between two vectors.

de   nition b.2.1 (inner product). an inner product on a vector space v is an operation (cid:104)  ,  (cid:105)
that maps pairs of vectors to r and satis   es the following conditions.

    it is symmetric, for any (cid:126)x, (cid:126)y     v

(cid:104)(cid:126)x, (cid:126)y(cid:105) = (cid:104)(cid:126)y, (cid:126)x(cid:105) .
    it is linear, i.e. for any        r and any (cid:126)x, (cid:126)y, (cid:126)z     v
(cid:104)   (cid:126)x, (cid:126)y(cid:105) =   (cid:104)(cid:126)y, (cid:126)x(cid:105) ,

(cid:104)(cid:126)x + (cid:126)y, (cid:126)z(cid:105) = (cid:104)(cid:126)x, (cid:126)z(cid:105) + (cid:104)(cid:126)y, (cid:126)z(cid:105) .

(b.12)

(b.13)

(b.14)

    it is positive semide   nite: (cid:104)(cid:126)x, (cid:126)x(cid:105) is nonnegative for all (cid:126)x     v and if (cid:104)(cid:126)x, (cid:126)x(cid:105) = 0 then (cid:126)x = 0.
a vector space endowed with an inner product is called an inner-product space. an important

instance of an inner product is the dot product between two vectors (cid:126)x, (cid:126)y     rn as

(cid:126)x [i] (cid:126)y [i] ,

(b.15)

(cid:126)x    (cid:126)y :=(cid:88)i

where (cid:126)x [i] is the ith entry of (cid:126)x. in this section we use (cid:126)xi to denote a vector, but in some other
parts of the notes it may also denote an entry of a vector (cid:126)x; this will be clear from the context.
it is easy to check that the dot product is a valid inner product. rn endowed with the dot
product is usually called a euclidean space of dimension n.

the norm of a vector is a generalization of the concept of length.

de   nition b.2.2 (norm). let v be a vector space, a norm is a function ||  || from v to r that

satis   es the following conditions.

    it is homogeneous. for all        r and (cid:126)x     v

||   (cid:126)x|| = |  |||(cid:126)x|| .

    it satis   es the triangle inequality

||(cid:126)x + (cid:126)y||     ||(cid:126)x|| + ||(cid:126)y|| .

in particular, it is nonnegative (set (cid:126)y =    (cid:126)x).
    ||(cid:126)x|| = 0 implies that (cid:126)x is the zero vector (cid:126)0.

(b.16)

(b.17)

a vector space equipped with a norm is called a normed space. distances in a normed space
can be measured using the norm of the di   erence between vectors.

appendix b. id202

216

de   nition b.2.3 (distance). the distance between two vectors (cid:126)x and (cid:126)y in a normed space with
norm ||  || is

d ((cid:126)x, (cid:126)y) := ||(cid:126)x     (cid:126)y|| .

(b.18)

inner-product spaces are normed spaces because we can de   ne a valid norm using the inner
product. the norm induced by an inner product is obtained by taking the square root of the
inner product of the vector with itself,

||(cid:126)x||(cid:104)  ,  (cid:105) :=(cid:112)(cid:104)(cid:126)x, (cid:126)x(cid:105).

(b.19)

the norm induced by an inner product is clearly homogeneous by linearity and symmetry of
the inner product. ||(cid:126)x||(cid:104)  ,  (cid:105) = 0 implies (cid:126)x = 0 because the inner product is positive semide   nite.
we only need to establish that the triangle inequality holds to ensure that the inner-product
is a valid norm. this follows from a classic inequality in id202, which is proved in
section b.8.2.

theorem b.2.4 (cauchy-schwarz inequality). for any two vectors (cid:126)x and (cid:126)y in an inner-product
space

assume ||(cid:126)x||(cid:104)  ,  (cid:105) (cid:54)= 0,

|(cid:104)(cid:126)x, (cid:126)y(cid:105)|     ||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105) .

||(cid:126)y||(cid:104)  ,  (cid:105)
(cid:104)(cid:126)x, (cid:126)y(cid:105) =    ||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105)        (cid:126)y =    
||(cid:126)x||(cid:104)  ,  (cid:105)
(cid:104)(cid:126)x, (cid:126)y(cid:105) = ||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105)        (cid:126)y = ||(cid:126)y||(cid:104)  ,  (cid:105)
(cid:126)x.
||(cid:126)x||(cid:104)  ,  (cid:105)

(cid:126)x,

(b.20)

(b.21)

(b.22)

corollary b.2.5. the norm induced by an inner product satis   es the triangle inequality.

proof.

||(cid:126)x + (cid:126)y||2

(cid:104)  ,  (cid:105) + ||(cid:126)y||2
(cid:104)  ,  (cid:105) + ||(cid:126)y||2

(cid:104)  ,  (cid:105) = ||(cid:126)x||2
    ||(cid:126)x||2
=(cid:16)||(cid:126)x||(cid:104)  ,  (cid:105) + ||(cid:126)y||(cid:104)  ,  (cid:105)(cid:17)2

(cid:104)  ,  (cid:105) + 2(cid:104)(cid:126)x, (cid:126)y(cid:105)
(cid:104)  ,  (cid:105) + 2||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105)

by the cauchy-schwarz inequality

(b.23)

.

(b.24)

the euclidean or (cid:96)2 norm is the norm induced by the dot product in rn,

||(cid:126)x||2 :=    (cid:126)x    (cid:126)x =(cid:118)(cid:117)(cid:117)(cid:116)
n(cid:88)i=1

(cid:126)x[i]2.

(b.25)

in the case of r2 or r3 it is what we usually think of as the length of the vector.

appendix b. id202

b.3 orthogonality

an important concept in id202 is orthogonality.

de   nition b.3.1 (orthogonality). two vectors (cid:126)x and (cid:126)y are orthogonal if

(cid:104)(cid:126)x, (cid:126)y(cid:105) = 0.

a vector (cid:126)x is orthogonal to a set s, if

(cid:104)(cid:126)x, (cid:126)s(cid:105) = 0,

for all (cid:126)s     s.

two sets of s1,s2 are orthogonal if for any (cid:126)x     s1, (cid:126)y     s2

(cid:104)(cid:126)x, (cid:126)y(cid:105) = 0.

the orthogonal complement of a subspace s is

s    := {(cid:126)x | (cid:104)(cid:126)x, (cid:126)y(cid:105) = 0

for all (cid:126)y     s} .

217

(b.26)

(b.27)

(b.28)

(b.29)

distances between orthogonal vectors measured in terms of the norm induced by the inner
product are easy to compute.

theorem b.3.2 (pythagorean theorem). if (cid:126)x and (cid:126)y are orthogonal vectors

||(cid:126)x + (cid:126)y||2
proof. by linearity of the inner product

(cid:104)  ,  (cid:105) = ||(cid:126)x||2

(cid:104)  ,  (cid:105) + ||(cid:126)y||2

(cid:104)  ,  (cid:105) .

||(cid:126)x + (cid:126)y||2

(cid:104)  ,  (cid:105) = ||(cid:126)x||2
= ||(cid:126)x||2

(cid:104)  ,  (cid:105) + ||(cid:126)y||2
(cid:104)  ,  (cid:105) + ||(cid:126)y||2

(cid:104)  ,  (cid:105) + 2(cid:104)(cid:126)x, (cid:126)y(cid:105)
(cid:104)  ,  (cid:105) .

(b.30)

(b.31)

(b.32)

if we want to show that a vector is orthogonal to a certain subspace, it is enough to show that
it is orthogonal to every vector in a basis of the subspace.
lemma b.3.3. let (cid:126)x be a vector and s a subspace of dimension n. if for any basis (cid:126)b1,(cid:126)b2, . . . ,(cid:126)bn
of s,

(cid:68)(cid:126)x,(cid:126)bi(cid:69) = 0,

1     i     n,

(b.33)

then (cid:126)x is orthogonal to s.

proof. any vector v     s can be represented as v =(cid:80)i   n
(cid:126)bi(cid:43) =(cid:88)i

(cid:104)(cid:126)x, v(cid:105) =(cid:42)(cid:126)x,(cid:88)i

  n

  n

i=1

i=1

(cid:126)bi for   1, . . . ,   n     r, from (b.33)
i=1(cid:68)(cid:126)x,(cid:126)bi(cid:69) = 0.

(b.34)

appendix b. id202

218

we now introduce orthonormal bases.

de   nition b.3.4 (orthonormal basis). a basis of mutually orthogonal vectors with norm equal
to one is called an orthonormal basis.

it is very easy to    nd the coe   cients of a vector in an orthonormal basis: we just need to
compute the dot products with the basis vectors.

lemma b.3.5 (coe   cients in an orthonormal basis). if {(cid:126)u1, . . . , (cid:126)un} is an orthonormal basis
of a vector space v, for any vector (cid:126)x     v

(cid:126)x =

n(cid:88)i=1

(cid:104)(cid:126)ui, (cid:126)x(cid:105) (cid:126)ui.

proof. since {(cid:126)u1, . . . , (cid:126)un} is a basis,

immediately,

(cid:126)x =

m(cid:88)i=1

  i (cid:126)ui

for some   1,   2, . . . ,   m     r.

(cid:104)(cid:126)ui, (cid:126)x(cid:105) =(cid:42)(cid:126)ui,

  i (cid:126)ui(cid:43) =

m(cid:88)i=1

m(cid:88)i=1

  i (cid:104)(cid:126)ui, (cid:126)ui(cid:105) =   i

(b.35)

(b.36)

(b.37)

because (cid:104)(cid:126)ui, (cid:126)ui(cid:105) = 1 and (cid:104)(cid:126)ui, (cid:126)uj(cid:105) = 0 for i (cid:54)= j.
for any subspace of rn we can obtain an orthonormal basis by applying the gram-schmidt
method to a set of linearly independent vectors spanning the subspace.

algorithm b.3.6 (gram-schmidt). consider a set of linearly independent vectors (cid:126)x1, . . . , (cid:126)xm
in rn. to obtain an orthonormal basis of the span of these vectors we:

1. set (cid:126)u1 := (cid:126)x1/||(cid:126)x1||2.
2. for i = 1, . . . , m, compute

and set (cid:126)ui := (cid:126)vi/||(cid:126)vi||2.

(cid:126)vi := (cid:126)xi    

i   1(cid:88)j=1

(cid:104)(cid:126)uj, (cid:126)xi(cid:105) (cid:126)uj.

(b.38)

it is not di   cult to show that the resulting set of vectors (cid:126)u1, . . . , (cid:126)um is an orthonormal basis for
the span of (cid:126)x1, . . . , (cid:126)xm. this implies in particular that we can always assume that a subspace
has an orthonormal basis.

theorem b.3.7. every    nite-dimensional vector space has an orthonormal basis.

proof. to see that the gram-schmidt method produces an orthonormal basis for the span of
the input vectors we can check that span ((cid:126)x1, . . . , (cid:126)xi) = span ((cid:126)u1, . . . , (cid:126)ui) and that (cid:126)u1, . . . , (cid:126)ui is
set of orthonormal vectors.

appendix b. id202

219

b.4 projections

the projection of a vector (cid:126)x onto a subspace s is the vector in s that is closest to (cid:126)x. in order
to de   ne this rigorously, we start by introducing the concept of direct sum. if two subspaces are
disjoint, i.e. their only common point is the origin, then a vector that can be written as a sum
of a vector from each subspace is said to belong to their direct sum.

de   nition b.4.1 (direct sum). let v be a vector space. for any subspaces s1,s2     v such
that

the direct sum is de   ned as

s1     s2 = {0}

s1     s2 := {(cid:126)x | (cid:126)x = (cid:126)s1 + (cid:126)s2

(cid:126)s1     s1, (cid:126)s2     s2} .

the representation of a vector in the direct sum of two subspaces is unique.

lemma b.4.2. any vector (cid:126)x     s1     s2 has a unique representation

(cid:126)x = (cid:126)s1 + (cid:126)s2

(cid:126)s1     s1, (cid:126)s2     s2.

(b.39)

(b.40)

(b.41)

proof. if (cid:126)x     s1     s2 then by de   nition there exist (cid:126)s1     s1, (cid:126)s2     s2 such that (cid:126)x = (cid:126)s1 + (cid:126)s2.
assume (cid:126)x = (cid:126)v1 + (cid:126)v2, (cid:126)v1     s1, (cid:126)v2     s2, then (cid:126)s1     (cid:126)v1 = (cid:126)s2     (cid:126)v2. this implies that (cid:126)s1     (cid:126)v1 and
(cid:126)s2   (cid:126)v2 are in s1 and also in s2. however, s1   s2 = {0}, so we conclude (cid:126)s1 = (cid:126)v1 and (cid:126)s2 = (cid:126)v2.
we can now de   ne the projection of a vector (cid:126)x onto a subspace s by separating the vector into
a component that belongs to s and another that belongs to its orthogonal complement.
de   nition b.4.3 (orthogonal projection). let v be a vector space. the orthogonal projection
of a vector (cid:126)x     v onto a subspace s     v is a vector denoted by ps (cid:126)x such that (cid:126)x     ps (cid:126)x     s   .
theorem b.4.4 (properties of orthogonal projections). let v be a vector space. every vector
(cid:126)x     v has a unique orthogonal projection ps (cid:126)x onto any subspace s     v of    nite dimension.
in particular (cid:126)x can be expressed as

for any vector (cid:126)s     s

(cid:126)x = ps (cid:126)x + ps

    (cid:126)x.

(cid:104)(cid:126)x, (cid:126)s(cid:105) = (cid:104)ps (cid:126)x, (cid:126)s(cid:105) .

for any orthonormal basis (cid:126)b1, . . . ,(cid:126)bm of s,

ps (cid:126)x =

m(cid:88)i=1(cid:68)(cid:126)x,(cid:126)bi(cid:69)(cid:126)bi.

(b.42)

(b.43)

(b.44)

appendix b. id202

220

proof. let us denote the dimension of s by m. since m is    nite, the exists an orthonormal basis
s: (cid:126)b(cid:48)1, . . . ,(cid:126)b(cid:48)m. consider the vector

it turns out that (cid:126)x     (cid:126)p is orthogonal to every vector in the basis. for 1     j     m,

(cid:126)p :=

m(cid:88)i=1(cid:68)(cid:126)x,(cid:126)b(cid:48)i(cid:69)(cid:126)b(cid:48)i.
m(cid:88)i=1(cid:68)(cid:126)x,(cid:126)b(cid:48)i(cid:69)(cid:126)b(cid:48)i,(cid:126)b(cid:48)j(cid:43)
m(cid:88)i=1(cid:68)(cid:126)x,(cid:126)b(cid:48)i(cid:69)(cid:68)(cid:126)b(cid:48)i,(cid:126)b(cid:48)j(cid:69)

(cid:68)(cid:126)x     (cid:126)p,(cid:126)b(cid:48)j(cid:69) =(cid:42)(cid:126)x    
=(cid:68)(cid:126)x,(cid:126)b(cid:48)j(cid:69)    
=(cid:68)(cid:126)x,(cid:126)b(cid:48)j(cid:69)    (cid:68)(cid:126)x,(cid:126)b(cid:48)j(cid:69) = 0,

so (cid:126)x     (cid:126)p     s    and (cid:126)p is an orthogonal projection. since s     s    = {0} 1 there cannot be two
other vectors (cid:126)x1     s, (cid:126)x1     s    such that (cid:126)x = (cid:126)x1 + (cid:126)x2 so the orthogonal projection is unique.
notice that (cid:126)o := (cid:126)x     (cid:126)p is a vector in s    such that (cid:126)x     (cid:126)o = (cid:126)p is in s and therefore in (cid:0)s   (cid:1)   .
this implies that (cid:126)o is the orthogonal projection of (cid:126)x onto s    and establishes (b.42).
equation (b.43) follows immediately from the orthogonality of any vector (cid:126)s     s and ps (cid:126)x.
equation (b.44) follows from (b.43).

computing the norm of the projection of a vector onto a subspace is easy if we have access to
an orthonormal basis (as long as the norm is induced by the inner product).

lemma b.4.5 (norm of the projection). the norm of the projection of an arbitrary vector
(cid:126)x     v onto a subspace s     v of dimension d can be written as

||ps (cid:126)x||(cid:104)  ,  (cid:105) =(cid:118)(cid:117)(cid:117)(cid:116)

d(cid:88)i (cid:68)(cid:126)bi, (cid:126)x(cid:69)2

for any orthonormal basis (cid:126)b1, . . . ,(cid:126)bd of s.
proof. by (b.44)

||ps (cid:126)x||2

(cid:104)  ,  (cid:105) = (cid:104)ps (cid:126)x,ps (cid:126)x(cid:105)

=

d(cid:88)j (cid:68)(cid:126)bj, (cid:126)x(cid:69)(cid:126)bj(cid:43)
=(cid:42) d(cid:88)i (cid:68)(cid:126)bi, (cid:126)x(cid:69)(cid:126)bi,
d(cid:88)j (cid:68)(cid:126)bi, (cid:126)x(cid:69)(cid:68)(cid:126)bj, (cid:126)x(cid:69)(cid:68)(cid:126)bi,(cid:126)bj(cid:69)
d(cid:88)i
d(cid:88)i (cid:68)(cid:126)bi, (cid:126)x(cid:69)2

=

.

1for any vector (cid:126)v that belongs to both s and s    (cid:104)(cid:126)v, (cid:126)v(cid:105) = ||(cid:126)v||2

2 = 0, which implies (cid:126)v = 0.

(b.45)

(b.46)

(b.47)

(b.48)

(b.49)

(b.50)

(b.51)

(b.52)

(b.53)

appendix b. id202

221

example b.4.6 (projection onto a one-dimensional subspace). to compute the projection
of a vector (cid:126)x onto a one-dimensional subspace spanned by a vector (cid:126)v, we use the fact that

(cid:110)(cid:126)v/||(cid:126)v||(cid:104)  ,  (cid:105)(cid:111) is a basis for span ((cid:126)v) (it is a set containing a unit vector that spans the subspace)

and apply (b.44) to obtain

pspan((cid:126)v) (cid:126)x = (cid:104)(cid:126)v, (cid:126)x(cid:105)
||(cid:126)v||2
(cid:104)  ,  (cid:105)

(cid:126)v.

(b.54)

(cid:52)
finally, we prove that the projection of a vector (cid:126)x onto a subspace s is indeed the vector in s
that is closest to (cid:126)x in the distance induced by the inner-product norm.

theorem b.4.7 (the orthogonal projection is closest). the orthogonal projection of a vector
(cid:126)x onto a subspace s belonging to the same inner-product space is the closest vector to (cid:126)x that
belongs to s in terms of the norm induced by the inner product. more formally, ps (cid:126)x is the
solution to the optimization problem

minimize

(cid:126)u

subject to

||(cid:126)x     (cid:126)u||(cid:104)  ,  (cid:105)
(cid:126)u     s.

proof. take any point (cid:126)s     s such that (cid:126)s (cid:54)= ps (cid:126)x

||(cid:126)x     (cid:126)s||2

(cid:104)  ,  (cid:105) = ||x     ps (cid:126)x + ps (cid:126)x     (cid:126)s||2
(cid:104)  ,  (cid:105)

(cid:104)  ,  (cid:105) + ||ps (cid:126)x     (cid:126)s||2
= ||x     ps (cid:126)x||2
(cid:104)  ,  (cid:105)
> ||x     ps (cid:126)x||2
(cid:104)  ,  (cid:105)

because (cid:126)s (cid:54)= ps (cid:126)x,

(b.55)

(b.56)

(b.57)

(b.58)

(b.59)

where (b.58) follows from the pythagorean theorem since because ps
s    and ps (cid:126)x     (cid:126)s to s.

    (cid:126)x := x   ps (cid:126)x belongs to

b.5 matrices

a matrix is a rectangular array of numbers. we denote the vector space of m    n matrices by
rm  n. we denote the ith row of a matrix a by ai:, the jth column by a:j and the (i, j) entry
by aij. the transpose of a matrix is obtained by switching its rows and columns.

de   nition b.5.1 (transpose). the transpose at of a matrix a     rm  n is a matrix in
a     rm  n

(b.60)

(cid:0)at(cid:1)ij = aji.

a symmetric matrix is a matrix that is equal to its transpose.

matrices map vectors to other vectors through a linear operation called matrix-vector product.

appendix b. id202

222

de   nition b.5.2 (matrix-vector product). the product of a matrix a     rm  n and a vector
(cid:126)x     rn is a vector a(cid:126)x     rn, such that

(a(cid:126)x)i =

aij(cid:126)x [j]

n(cid:88)j=1

= (cid:104)ai:, (cid:126)x(cid:105) ,

(b.61)

(b.62)

(b.63)

i.e. the ith entry of a(cid:126)x is the dot product between the ith row of a and (cid:126)x.

equivalently,

a(cid:126)x =

n(cid:88)j=1

a:j(cid:126)x [j] ,

i.e. a(cid:126)x is a linear combination of the columns of a weighted by the entries in (cid:126)x.

one can easily check that the transpose of the product of two matrices a and b is equal to the
transposes multiplied in the inverse order,

(ab)t = bt at .

we can express the dot product between two vectors (cid:126)x and (cid:126)y as

(cid:104)(cid:126)x, (cid:126)y(cid:105) = (cid:126)xt (cid:126)y = (cid:126)yt (cid:126)x.

the identity matrix is a matrix that maps any vector to itself.
de   nition b.5.3 (identity matrix). the identity matrix in rn  n is

(b.64)

(b.65)

i =

.

(b.66)

                        

1 0        0
0 1        0

      

0 0        1

                        

clearly, for any (cid:126)x     rn i(cid:126)x = (cid:126)x.
de   nition b.5.4 (id127). the product of two matrices a     rm  n and b    
rn  p is a matrix ab     rm  p, such that
(ab)ij =

(b.67)

aikbkj = (cid:104)ai:, b:,j(cid:105) ,

n(cid:88)k=1

i.e. the (i, j) entry of ab is the dot product between the ith row of a and the jth column of b.

equivalently, the jth column of ab is the result of multiplying a and the jth column of b

ab =

n(cid:88)k=1

aikbkj = (cid:104)ai:, b:,j(cid:105) ,

(b.68)

and ith row of ab is the result of multiplying the ith row of a and b.

appendix b. id202

223

square matrices may have an inverse. if they do, the inverse is a matrix that reverses the e   ect
of the matrix of any vector.

de   nition b.5.5 (matrix inverse). the inverse of a square matrix a     rn  n is a matrix
a   1     rn  n such that

lemma b.5.6. the inverse of a matrix is unique.

aa   1 = a   1a = i.

proof. let us assume there is another matrix m such that am = i, then

m = a   1am by (b.69)

= a   1.

(b.69)

(b.70)

(b.71)

an important class of matrices are orthogonal matrices.

de   nition b.5.7 (orthogonal matrix). an orthogonal matrix is a square matrix such that its
inverse is equal to its transpose,

u t u = u u t = i

(b.72)

by de   nition, the columns u:1, u:2, . . . , u:n of any orthogonal matrix have unit norm and orthog-
onal to each other, so they form an orthonormal basis (it   s somewhat confusing that orthogonal
matrices are not called orthonormal matrices instead). we can interpret applying u t to a vector
(cid:126)x as computing the coe   cients of its representation in the basis formed by the columns of u .
applying u to u t (cid:126)x recovers (cid:126)x by scaling each basis vector with the corresponding coe   cient:

(cid:126)x = u u t (cid:126)x =

n(cid:88)i=1

(cid:104)u:i, (cid:126)x(cid:105) u:i.

(b.73)

applying an orthogonal matrix to a vector does not a   ect its norm, it just rotates the vector.

lemma b.5.8 (orthogonal matrices preserve the norm). for any orthogonal matrix u     rn  n
and any vector (cid:126)x     rn,

proof. by the de   nition of an orthogonal matrix

||u(cid:126)x||2 = ||(cid:126)x||2 .

||u(cid:126)x||2

2 = (cid:126)xt u t u(cid:126)x
= (cid:126)xt (cid:126)x
= ||(cid:126)x||2
2 .

(b.74)

(b.75)

(b.76)

(b.77)

appendix b. id202

224

b.6 eigendecomposition

an eigenvector (cid:126)v of a matrix a satis   es

a(cid:126)v =   (cid:126)v

(b.78)

for a scalar    which is the corresponding eigenvalue. even if a is real, its eigenvectors and
eigenvalues can be complex.

lemma b.6.1 (eigendecomposition). if a square matrix a     rn  n has n linearly independent

eigenvectors (cid:126)v1, . . . , (cid:126)vn with eigenvalues   1, . . . ,   n it can be expressed in terms of a matrix q,
whose columns are the eigenvectors, and a diagonal matrix containing the eigenvalues,

a =(cid:104)(cid:126)v1 (cid:126)v2

       (cid:126)vn(cid:105)

= q  q   1

proof.

  1

0

0

  2

0

0

                        

0

0

      
      
      
         n

                        

(cid:104)(cid:126)v1 (cid:126)v2

       (cid:126)vn(cid:105)   1

aq =(cid:104)a(cid:126)v1 a(cid:126)v2
=(cid:104)  1(cid:126)v1   2(cid:126)v2

= q  .

       a(cid:126)vn(cid:105)
         2(cid:126)vn(cid:105)

(b.79)

(b.80)

(b.81)

(b.82)

(b.83)

if the columns of a square matrix are all linearly independent, then the matrix has an inverse,
so multiplying the expression by q   1 on both sides completes the proof.

lemma b.6.2. not all matrices have an eigendecomposition

proof. consider for example the matrix

assume    has a nonzero eigenvalue corresponding to an eigenvector with entries (cid:126)v[1] and (cid:126)v[2],
then

0 1

      
0 0       .
0 0            
(cid:126)v[2]       =      

0 1

(cid:126)v[1]

  (cid:126)v[2]

  (cid:126)v[2]       ,

      

(cid:126)v[2]

0        =      

(b.84)

(b.85)

which implies that (cid:126)v[2] = 0 and hence (cid:126)v[1] = 0, since we have assumed that    (cid:54)= 0. this implies
that the matrix does not have nonzero eigenvalues associated to nonzero eigenvectors.

appendix b. id202

225

an interesting use of the eigendecomposition is computing successive matrix products very fast.
assume that we want to compute

i.e. we want to apply a to (cid:126)x k times. ak cannot be computed by taking the power of its entries
(try out a simple example to convince yourself). however, if a has an eigendecomposition,

aa       a(cid:126)x = ak(cid:126)x,

(b.86)

ak = q  q   1q  q   1        q  q   1

= q  kq   1

= q

q   1,

  k
1

0

0

                        

0

  k
2

0

0

0

      
      
      
         k

n

                        

(b.87)

(b.88)

(b.89)

using the fact that for diagonal matrices applying the matrix repeatedly is equivalent to taking
the power of the diagonal entries. this allows to compute the k matrix products using just 3
matrix products and taking the power of n numbers.

from high-school or undergraduate algebra you probably remember how to compute eigenvectors
using determinants. in practice, this is usually not a viable option due to stability issues. a

popular technique to compute eigenvectors is based on the following insight. let a     rn  n be
a matrix with eigendecomposition q  q   1 and let (cid:126)x be an arbitrary vector in rn. since the
columns of q are linearly independent, they form a basis for rn, so we can represent (cid:126)x as

(cid:126)x =

n(cid:88)i=1

  iq:i,   i     r, 1     i     n.

now let us apply a to (cid:126)x k times,

ak(cid:126)x =

=

n(cid:88)i=1
n(cid:88)i=1

  iakq:i

  i  k

i q:i.

(b.90)

(b.91)

(b.92)

if we assume that the eigenvectors are ordered according to their magnitudes and that the
magnitude of one of them is larger than the rest, |  1| > |  2|     . . ., and that   1 (cid:54)= 0 (which
happens with high id203 if we draw a random (cid:126)x) then as k grows larger the term   1  k
1q:1
dominates. the term will blow up or tend to zero unless we normalize every time before applying
a. adding the id172 step to this procedure results in the power method or power
iteration, an algorithm of great importance in numerical id202.

algorithm b.6.3 (power method).
input: a matrix a.
output: an estimate of the eigenvector of a corresponding to the largest eigenvalue.

appendix b. id202

226

figure b.1: illustration of the    rst three iterations of the power method for a matrix with eigenvectors
(cid:126)v1 and (cid:126)v2, whose corresponding eigenvalues are   1 = 1.05 and   2 = 0.1661.

initialization: set (cid:126)x1 := (cid:126)x/||(cid:126)x||2, where the entries of (cid:126)x are drawn at random.
for i = 1, . . . , k, compute

(cid:126)xi :=

a(cid:126)xi   1
||a(cid:126)xi   1||2

.

(b.93)

figure b.1 illustrates the power method on a simple example, where the matrix is equal to

0.930 0.388

a =      
0.237 0.286       .

(b.94)

the convergence to the eigenvector corresponding to the eigenvalue with the largest magnitude
is very fast.

b.7 eigendecomposition of symmetric matrices

real symmetric matrices always have an eigendecomposition. in addition, their eigenvalues are
real and their eigenvectors are all orthogonal.

theorem b.7.1 (spectral theorem for real symmetric matrices). if a     rn  n is symmetric,

then it has an eigendecomposition of the form

a =(cid:104)(cid:126)u1 (cid:126)u2

       (cid:126)un(cid:105)

(cid:104)(cid:126)u1 (cid:126)u2

       (cid:126)un(cid:105)t

,

(b.95)

  1

0

0

  2

0

0

                        

0

0

      
      
      
         n

                        

where the eigenvalues (cid:126)u1, (cid:126)u2, . . . , (cid:126)un are real and the eigenvectors (cid:126)u1, (cid:126)u2, . . . , (cid:126)un are real and
orthogonal.

proof. the proof that every real symmetric matrix has n eigenvectors is beyond the scope of
these notes. under the assumption that this is the case, we begin by proving that the eigenvalues
are real. consider an arbitrary eigenvalue   i and the corresponding normalized eigenvector (cid:126)vi,
we have

(cid:126)v    i a(cid:126)vi =   (cid:126)v    i (cid:126)vi =   ,
(cid:126)v    i a(cid:126)vi = (a(cid:126)vi)    (cid:126)vi = (  (cid:126)vi)    =     (cid:126)v    i (cid:126)vi =     .

(b.96)

(b.97)

~v1~x1~v2~v1~x2~v2~v1~x3~v2appendix b. id202

227

this implies that    is real because    =     , so we can restrict the eigenvectors to be real (since the
eigenvalue is real, both the real and imaginary parts of the eigenvector are eigenvectors them-
selves and at least one of them must be nonzero). if several linearly independent eigenvectors
have the same eigenvalue, an orthonormal basis of their span will also consist of eigenvectors of
the matrix. all that is left to prove is that eigenvectors corresponding to di   erent eigenvalues are
orthogonal. assume (cid:126)vi and (cid:126)vj are eigenvectors corresponding to di   erent eigenvalues   i (cid:54)=   j,
then

(cid:126)ut
i (cid:126)uj =

=

=

=

1
  i
1
  i
1
  i
  j
  i

(a(cid:126)ui)t (cid:126)uj

(cid:126)ut
i at (cid:126)uj

(cid:126)ut
i a(cid:126)uj

(cid:126)ut
i (cid:126)uj.

this is only possible if (cid:126)ut

i (cid:126)uj = 0.

the eigenvalues of a symmetric matrix determine the value of the quadratic form:

q ((cid:126)x) := (cid:126)xt a(cid:126)x =

n(cid:88)i=1

  i(cid:0)(cid:126)xt (cid:126)ui(cid:1)2

(b.98)

(b.99)

(b.100)

(b.101)

(b.102)

if we order the eigenvalues   1       2     . . .       n then the    rst eigenvalue is the maximum value
attained by the quadratic if its input has unit (cid:96)2 norm, the second eigenvalue is the maximum
value attained by the quadratic form if we restrict its argument to be normalized and orthogonal
to the    rst eigenvector, and so on.

theorem b.7.2. for any symmetric matrix a     rn with normalized eigenvectors (cid:126)u1, (cid:126)u2, . . . , (cid:126)un

with corresponding eigenvalues   1,   2, . . . ,   n

  1 = max
||(cid:126)u||2=1

(cid:126)ut a(cid:126)u,

(cid:126)u1 = arg max
||(cid:126)u||2=1
max

  k =

(cid:126)ut a(cid:126)u,

||(cid:126)u||2=1,(cid:126)u   (cid:126)u1,...,(cid:126)uk   1

(cid:126)ut a(cid:126)u,

(cid:126)uk = arg

max

||(cid:126)u||2=1,(cid:126)u   (cid:126)u1,...,(cid:126)uk   1

(cid:126)ut a(cid:126)u.

(b.103)

(b.104)

(b.105)

(b.106)

proof. the eigenvectors are an orthonormal basis (they are mutually orthogonal and we assume
that they have been normalized), so we can represent any unit-norm vector (cid:126)hk that is orthogonal
to (cid:126)u1, . . . , (cid:126)uk   1 as

(cid:126)hk =

  i(cid:126)ui

m(cid:88)i=k

(b.107)

appendix b. id202

where

228

(b.108)

2

2

=

m(cid:88)i=k

  2

i = 1,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:126)hk(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

by lemma b.4.5. note that (cid:126)h1 is just an arbitrary unit-norm vector.
now we will show that the value of the quadratic form when the normalized input is restricted
to be orthogonal to (cid:126)u1, . . . , (cid:126)uk   1 cannot be larger than   k,

2

by (b.102) and (b.107)

(b.109)

  j(cid:126)ut

i (cid:126)uj      

(cid:126)ht
k a(cid:126)hk =

=

  i      
m(cid:88)j=k
n(cid:88)i=1
n(cid:88)i=1
m(cid:88)i=k

  i  2
i

  2
i

      k
=   k,

by (b.108).

because (cid:126)u1, . . . , (cid:126)um is an orthonormal basis

because   k       k+1     . . .       m

(b.110)

(b.111)

(b.112)

this establishes (b.103) and (b.105). to prove (b.104) and (b.106) we just need to show that
(cid:126)uk achieves the maximum

(cid:126)ut
k a(cid:126)uk =

n(cid:88)i=1

=   k.

  i(cid:0)(cid:126)ut

i (cid:126)uk(cid:1)2

(b.113)

(b.114)

b.8 proofs

b.8.1 proof of theorem b.1.6
we prove the claim by contradiction. assume that we have two bases {(cid:126)x1, . . . , (cid:126)xm} and {(cid:126)y1, . . . , (cid:126)yn}
such that m < n (or the second set has in   nite cardinality). the proof follows from applying
the following lemma m times (setting r = 0, 1, . . . , m     1) to show that {(cid:126)y1, . . . , (cid:126)ym} spans v
and hence {(cid:126)y1, . . . , (cid:126)yn} must be linearly dependent.
lemma b.8.1. under the assumptions of the theorem, if {(cid:126)y1, (cid:126)y2, . . . , (cid:126)yr, (cid:126)xr+1, . . . , (cid:126)xm} spans v
then {(cid:126)y1, . . . , (cid:126)yr+1, (cid:126)xr+2, . . . , (cid:126)xm} also spans v (possibly after rearranging the indices r+1, . . . , m)
for r = 0, 1, . . . , m     1.
proof. since {(cid:126)y1, (cid:126)y2, . . . , (cid:126)yr, (cid:126)xr+1, . . . , (cid:126)xm} spans v

(cid:126)yr+1 =

  i (cid:126)yi +

r(cid:88)i=1

m(cid:88)i=r+1

  i (cid:126)xi,

  1, . . . ,   r,   r+1, . . . ,   m     r,

(b.115)

appendix b. id202

229

where at least one of the   j is non zero, as {(cid:126)y1, . . . , (cid:126)yn} is linearly independent by assumption.
without loss of generality (here is where we might need to rearrange the indices) we assume
that   r+1 (cid:54)= 0, so that

(cid:126)xr+1 =

1

  r+1(cid:32) r(cid:88)i=1

  i (cid:126)yi    

m(cid:88)i=r+2

  i(cid:126)xi(cid:33) .

(b.116)

this implies that any vector in the span of {(cid:126)y1, (cid:126)y2, . . . , (cid:126)yr, (cid:126)xr+1, . . . , (cid:126)xm}, i.e.
in v, can be
represented as a linear combination of vectors in {(cid:126)y1, . . . , (cid:126)yr+1, (cid:126)xr+2, . . . , (cid:126)xm}, which completes
the proof.

b.8.2 proof of theorem b.2.4
if ||(cid:126)x||(cid:104)  ,  (cid:105) = 0 then (cid:126)x = (cid:126)0 because the inner product is positive semide   nite, which implies
(cid:104)(cid:126)x, (cid:126)y(cid:105) = 0 and consequently that (b.20) holds with equality. the same is true if ||(cid:126)y||(cid:104)  ,  (cid:105) = 0.
now assume that ||(cid:126)x||(cid:104)  ,  (cid:105) (cid:54)= 0 and ||(cid:126)y||(cid:104)  ,  (cid:105) (cid:54)= 0. by semide   niteness of the inner product,

0    (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)||(cid:126)y||(cid:104)  ,  (cid:105) (cid:126)x + ||(cid:126)x||(cid:104)  ,  (cid:105) (cid:126)y(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
0    (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)||(cid:126)y||(cid:104)  ,  (cid:105) (cid:126)x     ||(cid:126)x||(cid:104)  ,  (cid:105) (cid:126)y(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

these inequalities establish (b.20).

2

2

= 2||(cid:126)x||2
= 2||(cid:126)x||2

(cid:104)  ,  (cid:105) ||(cid:126)y||2
(cid:104)  ,  (cid:105) ||(cid:126)y||2

(cid:104)  ,  (cid:105) + 2||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105) (cid:104)(cid:126)x, (cid:126)y(cid:105) ,
(cid:104)  ,  (cid:105)     2||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105) (cid:104)(cid:126)x, (cid:126)y(cid:105) .

(b.117)

(b.118)

let us prove (b.21) by proving both implications.
(   ) assume (cid:104)(cid:126)x, (cid:126)y(cid:105) =    ||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105). then (b.117) equals zero, so ||(cid:126)y||(cid:104)  ,  (cid:105) (cid:126)x =    ||(cid:126)x||(cid:104)  ,  (cid:105) (cid:126)y
because the inner product is positive semide   nite.
(   ) assume ||(cid:126)y||(cid:104)  ,  (cid:105) (cid:126)x =    ||(cid:126)x||(cid:104)  ,  (cid:105) (cid:126)y. then one can easily check that (b.117) equals zero, which
implies (cid:104)(cid:126)x, (cid:126)y(cid:105) =    ||(cid:126)x||(cid:104)  ,  (cid:105) ||(cid:126)y||(cid:104)  ,  (cid:105).
the proof of (b.22) is identical (using (b.118) instead of (b.117)).

