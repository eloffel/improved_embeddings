learning latent variable models

using tensor decompositions

daniel hsu

columbia university

january 27, 2017

1

subject matter

learning algorithms
for latent variable models
based on decompositions of moment tensors.

2

subject matter

learning algorithms (parameter estimation)
for latent variable models
based on decompositions of moment tensors.

   method-of-moments    (pearson, 1894)

2

example #1: summarizing a corpus of documents

observation: documents express one or more thematic topics.

3

example #1: summarizing a corpus of documents

observation: documents express one or more thematic topics.

(cid:73) what topics are expressed in a corpus of documents?
(cid:73) how prevalent is each topic in the corpus?

3

topic model (e.g., id44)

k topics (distributions over vocab words).
document     mixture of topics.
word tokens in doc. iid    mixture distribution.

4

sportssciencebusinesspoliticstopic model (e.g., id44)

k topics (distributions over vocab words).
document     mixture of topics.
word tokens in doc. iid    mixture distribution.

e.g.,

iid    0.7    p sports + 0.3    p business.

4

sportssciencebusinesspoliticstopic model (e.g., id44)

k topics (distributions over vocab words).
document     mixture of topics.
word tokens in doc. iid    mixture distribution.

e.g.,

iid    0.7    p sports + 0.3    p business.

given corpus of documents (and    hyper-parameters   , e.g., k),
produce estimates of model parameters, e.g.:

(cid:73) distribution p t over vocab words, for each t     [k].
(cid:73) weight wt of topic t in document corpus, for each t     [k].

4

sportssciencebusinesspoliticslabels / annotations

(cid:73) suppose each word token x in document is annotated with

source topic tx     {1, 2, . . . , k}.

team relocations keep n.f.l. moving up
4

1

1

4

1

1

financially

4

5

labels / annotations

(cid:73) suppose each word token x in document is annotated with

source topic tx     {1, 2, . . . , k}.

1

team relocations keep n.f.l. moving up
4

1
4
then estimating the {(p t, wt)}k
t=1 can be done    directly   .

1

4

1

financially

5

labels / annotations

(cid:73) suppose each word token x in document is annotated with

source topic tx     {1, 2, . . . , k}.

1

team relocations keep n.f.l. moving up
4

1
4
then estimating the {(p t, wt)}k
t=1 can be done    directly   .

1

4

1

financially

(cid:73) unfortunately, we often don   t have such annotations

(i.e., data are unlabeled / topics are hidden).
   direct    approach to estimation unavailable.

5

example #2: subpopulations in data

data studied by pearson (1894):
ratio of forehead-width to body-length for 1000 crabs.

6

ratio0.560.580.60.620.640.660.680.7count (or density)020406080100countssubpop 1subpop 2full popexample #2: subpopulations in data

data studied by pearson (1894):
ratio of forehead-width to body-length for 1000 crabs.

sample may be comprised of di   erent sub-species of crabs.

6

ratio0.560.580.60.620.640.660.680.7count (or density)020406080100countssubpop 1subpop 2full popgaussian mixture model

h     discrete(  1,   2, . . . ,   k) ;

x | h = t     normal(  t,   t) ,

t     [k] .

7

gaussian mixture model

h     discrete(  1,   2, . . . ,   k) ;

x | h = t     normal(  t,   t) ,

t     [k] .

estimate mean vector, covariance matrix, and mixing weight of
each subpopulation from unlabeled data.

7

id113

(cid:73) no    direct    estimators when some variables are hidden.

8

id113

(cid:73) no    direct    estimators when some variables are hidden.
(cid:73) maximum likelihood estimator (id113):

  id113 := arg max

       

log pr   (data) .

8

id113

(cid:73) no    direct    estimators when some variables are hidden.
(cid:73) maximum likelihood estimator (id113):

  id113 := arg max

       

log pr   (data) .

(cid:73) note: log-likelihood is not necessarily concave function of   .

8

id113

(cid:73) no    direct    estimators when some variables are hidden.
(cid:73) maximum likelihood estimator (id113):

  id113 := arg max

       

log pr   (data) .

(cid:73) note: log-likelihood is not necessarily concave function of   .
(cid:73) for latent variable models, often use local optimization,

most notably via expectation-maximization (em)
(dempster, laird, & rubin, 1977).

8

id113 for gaussian mixture models

given data {xi}n

i=1,    nd {(  t,   t,   t)}k

t=1 to maximize

(cid:26)

  t   

1

det(  t)1/2

exp

    1
2

(xi       t)(cid:62)     1

t (xi       t)

n(cid:88)

log

       k(cid:88)

i=1

t=1

(cid:27)       .

9

id113 for gaussian mixture models

given data {xi}n

i=1,    nd {(  t,   t,   t)}k

t=1 to maximize

(cid:26)

  t   

1

det(  t)1/2

exp

    1
2

(xi       t)(cid:62)     1

t (xi       t)

n(cid:88)

log

       k(cid:88)

i=1

t=1

(cid:73) sensible with restrictions on   t (e.g.,   t (cid:23)   2i).

(cid:27)       .

9

id113 for gaussian mixture models

given data {xi}n

i=1,    nd {(  t,   t,   t)}k

t=1 to maximize

(cid:26)

  t   

1

det(  t)1/2

exp

    1
2

(xi       t)(cid:62)     1

t (xi       t)

n(cid:88)

log

       k(cid:88)

i=1

t=1

(cid:27)       .

(cid:73) sensible with restrictions on   t (e.g.,   t (cid:23)   2i).
(cid:73) similar to euclidean id116 problem, which is np-hard

(dasgupta, 2008; aloise, deshpande, hansen, & popat, 2009; mahajan,
nimbhorkar, & varadarajan, 2009; vattani, 2009; awasthi, charikar,
krishnaswamy, & sinop, 2015).

9

parameter learning objective

suppose iid sample of size n is generated by distribution from
model with (unknown) parameters               rp

(p = # params).

10

parameter learning objective

suppose iid sample of size n is generated by distribution from
model with (unknown) parameters               rp

(p = # params).

task: produce estimate      of    such that

e(cid:107)           (cid:107)     0 as n        

(i.e.,      is consistent).

10

parameter learning objective

suppose iid sample of size n is generated by distribution from
model with (unknown) parameters               rp

(p = # params).

task: produce estimate      of    such that

e(cid:107)           (cid:107)     0 as n        

(i.e.,      is consistent).

(cid:73) e.g., for spherical gaussian mixtures (as n        ):

(cid:73) for k = 2 (and   t = 1/2,   t = i): em is consistent

(xu, h., & maleki, 2016; daskalakis, tzamos, & zampetakis, 2016).

10

parameter learning objective

suppose iid sample of size n is generated by distribution from
model with (unknown) parameters               rp

(p = # params).

task: produce estimate      of    such that

e(cid:107)           (cid:107)     0 as n        

(i.e.,      is consistent).

(cid:73) e.g., for spherical gaussian mixtures (as n        ):

(cid:73) for k = 2 (and   t = 1/2,   t = i): em is consistent

(xu, h., & maleki, 2016; daskalakis, tzamos, & zampetakis, 2016).
(cid:73) larger k: easily trapped in local maxima, far from global max

(jin, zhang, balakrishnan, wainwright, & jordan, 2016).

10

parameter learning objective

suppose iid sample of size n is generated by distribution from
model with (unknown) parameters               rp

(p = # params).

task: produce estimate      of    such that

e(cid:107)           (cid:107)     0 as n        

(i.e.,      is consistent).

(cid:73) e.g., for spherical gaussian mixtures (as n        ):

(cid:73) for k = 2 (and   t = 1/2,   t = i): em is consistent

(xu, h., & maleki, 2016; daskalakis, tzamos, & zampetakis, 2016).
(cid:73) larger k: easily trapped in local maxima, far from global max

(jin, zhang, balakrishnan, wainwright, & jordan, 2016).
practitioners often use em with many (random) restarts . . .
but may take a long time to get near the global max.

10

parameter learning objective

suppose iid sample of size n is generated by distribution from
model with (unknown) parameters               rp

(p = # params).

task: produce estimate      of    such that

(cid:16)(cid:107)           (cid:107)      

(cid:17)     1       

pr

with poly(p, 1/ , 1/  , . . . ) sample size and running time.

(cid:73) e.g., for spherical gaussian mixtures (as n        ):

(cid:73) for k = 2 (and   t = 1/2,   t = i): em is consistent

(xu, h., & maleki, 2016; daskalakis, tzamos, & zampetakis, 2016).
(cid:73) larger k: easily trapped in local maxima, far from global max

(jin, zhang, balakrishnan, wainwright, & jordan, 2016).
practitioners often use em with many (random) restarts . . .
but may take a long time to get near the global max.

10

barriers

hard to learn model parameters,

even when data is generated by a model distribution.

11

barriers

hard to learn model parameters,

even when data is generated by a model distribution.

cryptographic hardness

(e.g., mossel & roch, 2006)

information-theoretic hardness
(e.g., moitra & valiant, 2010)

may require 2   (k) running time or 2   (k) sample size.

11

ways around the barriers

(cid:73) separation conditions.

e.g., assume min
i(cid:54)=j

(cid:107)  i       j(cid:107)2
  2
i +   2
j

is su   ciently large.

(dasgupta, 1999; arora & kannan, 2001; vempala & wang, 2002; . . . )

12

ways around the barriers

(cid:73) separation conditions.

e.g., assume min
i(cid:54)=j

(cid:107)  i       j(cid:107)2
  2
i +   2
j

is su   ciently large.

(dasgupta, 1999; arora & kannan, 2001; vempala & wang, 2002; . . . )

(cid:73) structural assumptions.

e.g., sparsity, anchor words.
(spielman, wang, & wright, 2012; arora, ge, & moitra, 2012; . . . )

12

ways around the barriers

(cid:73) separation conditions.

e.g., assume min
i(cid:54)=j

(cid:107)  i       j(cid:107)2
  2
i +   2
j

is su   ciently large.

(dasgupta, 1999; arora & kannan, 2001; vempala & wang, 2002; . . . )

(cid:73) structural assumptions.

e.g., sparsity, anchor words.
(spielman, wang, & wright, 2012; arora, ge, & moitra, 2012; . . . )

(cid:73) non-degeneracy conditions.

e.g., assume   1,   2, . . . ,   k are in general position.

12

ways around the barriers

(cid:73) separation conditions.

e.g., assume min
i(cid:54)=j

(cid:107)  i       j(cid:107)2
  2
i +   2
j

is su   ciently large.

(dasgupta, 1999; arora & kannan, 2001; vempala & wang, 2002; . . . )

(cid:73) structural assumptions.

e.g., sparsity, anchor words.
(spielman, wang, & wright, 2012; arora, ge, & moitra, 2012; . . . )

(cid:73) non-degeneracy conditions.

e.g., assume   1,   2, . . . ,   k are in general position.

this talk:
method-of-moments.

learning algorithms for non-degenerate instances via

12

method-of-moments at a glance

1. determine function of model parameters    estimatable from

observable data:

e  [f (x)]

(   moments   ) .

2. form estimates of moments using data (e.g., iid sample):

(cid:98)e[f (x)]

(   empirical moments   ) .

3. approximately solve equations for parameters   :

e  [f (x)] = (cid:98)e[f (x)] .

4. (   fine-tune    estimated parameters with local optimization.)

13

method-of-moments at a glance

1. determine function of model parameters    estimatable from

observable data:

e  [f (x)]

(   moments   ) .

which moments?

2. form estimates of moments using data (e.g., iid sample):

(cid:98)e[f (x)]

(   empirical moments   ) .

3. approximately solve equations for parameters   :

e  [f (x)] = (cid:98)e[f (x)] .

how?

4. (   fine-tune    estimated parameters with local optimization.)

13

method-of-moments at a glance

1. determine function of model parameters    estimatable from

observable data:

e  [f (x)]

(   moments   ) .

which moments? often third-order moments su   ce.
2. form estimates of moments using data (e.g., iid sample):

(cid:98)e[f (x)]

(   empirical moments   ) .

3. approximately solve equations for parameters   :

e  [f (x)] = (cid:98)e[f (x)] .

how? algorithms for tensor decomposition.

4. (   fine-tune    estimated parameters with local optimization.)

13

unresolved issues

(cid:73) handle model misspeci   cation, increase robustness.

(cid:73) can tolerate some independence assumptions but not others?

(cid:73) general methodology.

(cid:73) at present, ad hoc to instantiate; guided by examples.

(cid:73) incorporate general prior knowledge.
(cid:73) incorporate user feedback interactively.

14

outline

1. warm-up: topic model for single-topic documents.

(cid:73) identi   ability.
(cid:73) parameter recovery via decompositions of exact moments.

2. moment decompositions for other models.

(cid:73) mixtures of gaussians and id75s.
(cid:73) multi-view models.

3. error-tolerant algorithms for tensor decompositions.

15

other models amenable to moment tensor decomposition

(cid:73) models for independent components analysis (comon, 1994; frieze,

jerrum, & kannan, 1996; arora, ge, moitra & sachdeva, 2012;
anandkumar, foster, h., kakade, & liu, 2012, 2015; belkin,
rademacher, & voss, 2013; etc.)

(cid:73) id44 (anandkumar, foster, h., kakade, & liu,

2012, 2015; anderson, goyal, & rademacher, 2013)

(cid:73) mixed-membership stochastic blockmodels (anandkumar, ge, h., &

kakade, 2013, 2014)

(cid:73) simple probabilistic grammars (h., kakade, & liang, 2012)
(cid:73) noisy-or networks (halpern & sontag, 2013; jernite, halpern & sontag,

2013; arora, ge, ma, & risteski, 2016)

(cid:73) indian bu   et process (tung & smola, 2014)
(cid:73) mixed multinomial logit model (oh & shah, 2014)
(cid:73) dawid-skene model (zhang, chen, zhou, & jordan, 2014)
(cid:73) multi-task bandits (azar, lazaric, & brunskill, 2013)
(cid:73) partially obs. mdps (azizzadenesheli, lazaric, & anandkumar, 2016)
(cid:73) . . .

16

1. warm-up: topic model for single-topic documents

17

topic model

general topic model (e.g., id44)

k topics (dists. over words) {p t}k
document     mixture of topics (hidden).
word tokens in doc. iid    mixture distribution.

t=1.

18

sportssciencebusinesspoliticstopic model

topic model for single-topic documents

k topics (dists. over words) {p t}k
t=1.
pick topic t with prob. wt (hidden).
word tokens in doc. iid    p t.

18

sportssciencebusinesspoliticstopic model

topic model for single-topic documents

k topics (dists. over words) {p t}k
t=1.
pick topic t with prob. wt (hidden).
word tokens in doc. iid    p t.

given iid sample of documents of length l,
produce estimates of model parameters {(p t, wt)}k

t=1.

18

sportssciencebusinesspoliticstopic model

topic model for single-topic documents

k topics (dists. over words) {p t}k
t=1.
pick topic t with prob. wt (hidden).
word tokens in doc. iid    p t.

given iid sample of documents of length l,
produce estimates of model parameters {(p t, wt)}k

t=1.

how long must the documents be?

18

sportssciencebusinesspoliticsidenti   ability

(cid:73) generative process:

pick t     discrete(w1, w2, . . . , wk).
given t, pick l words from p t.

19

ijpr[wordsi,j]ptp(cid:62)t=identi   ability

(cid:73) generative process:

pick t     discrete(w1, w2, . . . , wk).
given t, pick l words from p t.

(cid:73) l = 1: random document     (cid:80)k

t=1 wtp t

19

ijpr[wordsi,j]ptp(cid:62)t=identi   ability

(cid:73) generative process:

pick t     discrete(w1, w2, . . . , wk).
given t, pick l words from p t.

(cid:73) l = 1: random document     (cid:80)k

t=1 wtp t

parameters not identi   able from such observations.

19

ijpr[wordsi,j]ptp(cid:62)t=identi   ability

(cid:73) generative process:

pick t     discrete(w1, w2, . . . , wk).
given t, pick l words from p t.

(cid:73) l = 1: random document     (cid:80)k

t=1 wtp t

parameters not identi   able from such observations.

(cid:73) l = 2:

regard p t as id203 vector.
joint distribution of word pairs (for topic t) is given by matrix:

random document     (cid:80)k

t=1 wtp tp (cid:62)
t .

19

ijpr[wordsi,j]ptp(cid:62)t=identi   ability

(cid:73) generative process:

pick t     discrete(w1, w2, . . . , wk).
given t, pick l words from p t.

(cid:73) l = 1: random document     (cid:80)k

t=1 wtp t

parameters not identi   able from such observations.

(cid:73) l = 2:

regard p t as id203 vector.
joint distribution of word pairs (for topic t) is given by matrix:

random document     (cid:80)k

are parameters {(p t, wt)}k

t=1 wtp tp (cid:62)
t .
t=1 identi   able?

19

ijpr[wordsi,j]ptp(cid:62)t=identi   ability: l = 2

parameters {(p 1, w1), (p 2, w2)} and {((cid:101)p 1,   w1), ((cid:101)p 2,   w2)}
(cid:35)
(cid:35)

(p 1, w1) =

(p 2, w2) =

(cid:35)
(cid:35)

0.40
0.60

0.60
0.40

, 0.5

, 0.5

((cid:101)p 2,   w2) =

0.30
0.70

, 0.2

((cid:101)p 1,   w1) =

       ,
       ,

      (cid:34)
      (cid:34)

      (cid:34)
      (cid:34)

0.55
0.45

, 0.8

       ;
      

20

identi   ability: l = 2

parameters {(p 1, w1), (p 2, w2)} and {((cid:101)p 1,   w1), ((cid:101)p 2,   w2)}
(cid:35)
(cid:35)

(p 1, w1) =

(p 2, w2) =

(cid:35)
(cid:35)

0.40
0.60

0.60
0.40

, 0.5

((cid:101)p 2,   w2) =

0.30
0.70

, 0.2

      (cid:34)
      (cid:34)

      (cid:34)
      (cid:34)

, 0.5

       ,
       ,
2 =   w1(cid:101)p 1(cid:101)p

, 0.8

0.55
0.45

((cid:101)p 1,   w1) =

satisfy

w1p 1p (cid:62)

1 + w2p 2p (cid:62)

(cid:34)

1 +   w2(cid:101)p 2(cid:101)p

(cid:62)

(cid:62)
2 =

0.26 0.24
0.24 0.26

       ;
      
(cid:35)

.

20

identi   ability: l = 2

parameters {(p 1, w1), (p 2, w2)} and {((cid:101)p 1,   w1), ((cid:101)p 2,   w2)}
(cid:35)
(cid:35)

(p 1, w1) =

(p 2, w2) =

(cid:35)
(cid:35)

0.40
0.60

0.60
0.40

, 0.5

((cid:101)p 2,   w2) =

0.30
0.70

, 0.2

      (cid:34)
      (cid:34)

      (cid:34)
      (cid:34)

, 0.5

       ,
       ,
2 =   w1(cid:101)p 1(cid:101)p

, 0.8

0.55
0.45

((cid:101)p 1,   w1) =

satisfy

w1p 1p (cid:62)

1 + w2p 2p (cid:62)

(cid:34)

1 +   w2(cid:101)p 2(cid:101)p

(cid:62)

(cid:62)
2 =

0.26 0.24
0.24 0.26

       ;
      
(cid:35)

cannot identify parameters from length-two documents.

.

20

identi   ability: l = 3

documents of length l = 3
joint distribution of word triple (for topic t) is given by tensor:

random document     (cid:80)k

t=1 wtp t     p t     p t.

21

pr[wordsi,j,k]pt   pt   pt=identi   ability from documents of length three

claim: if {p t}k
parameters {(p t, wt)}k

t=1 are linearly independent and all wt > 0, then

t=1 are identi   able from word triples.

22

identi   ability from documents of length three

claim: if {p t}k
parameters {(p t, wt)}k

t=1 are linearly independent and all wt > 0, then

t=1 are identi   able from word triples.

(cid:73) claim implied by uniqueness of certain tensor decompositions.

22

identi   ability from documents of length three

claim: if {p t}k
parameters {(p t, wt)}k

t=1 are linearly independent and all wt > 0, then

t=1 are identi   able from word triples.

(cid:73) claim implied by uniqueness of certain tensor decompositions.
(cid:73) algorithmic proof via special case of jennrich   s algorithm

(harshman, 1970).

22

identi   ability from documents of length three

claim: if {p t}k
parameters {(p t, wt)}k

t=1 are linearly independent and all wt > 0, then

t=1 are identi   able from word triples.

(cid:73) claim implied by uniqueness of certain tensor decompositions.
(cid:73) algorithmic proof via special case of jennrich   s algorithm

(harshman, 1970).

next: brief overview of tensors.

22

tensors of order two

matrices (tensors of order two): m     rd  d.

(cid:73) think of as bilinear function m : rd    rd     r.

23

tensors of order two

matrices (tensors of order two): m     rd  d.

(cid:73) think of as bilinear function m : rd    rd     r.
(cid:73) formula using matrix representation:

m (x, y) = x(cid:62)m y =

mi,j    xiyj .

(cid:88)

i,j

23

tensors of order two

matrices (tensors of order two): m     rd  d.

(cid:73) think of as bilinear function m : rd    rd     r.
(cid:73) formula using matrix representation:

(cid:88)

m (x, y) = x(cid:62)m y =

(cid:73) describe m by d2 values m (ei, ej).

i,j

mi,j    xiyj .

23

tensors of order two

matrices (tensors of order two): m     rd  d.

(cid:73) think of as bilinear function m : rd    rd     r.
(cid:73) formula using matrix representation:

(cid:88)

m (x, y) = x(cid:62)m y =

i,j

(cid:73) describe m by d2 values m (ei, ej).

tensors are multi-linear generalization.

mi,j    xiyj .

23

tensors of order p

p-linear functions: t : rd    rd              rd     r.

24

tensors of order p

p-linear functions: t : rd    rd              rd     r.
(cid:73) describe t by dp values t (ei1, ei2, . . . , eip).

24

tensors of order p

p-linear functions: t : rd    rd              rd     r.
(cid:73) describe t by dp values t (ei1, ei2, . . . , eip).
(cid:73) identify t with multi-index array t     rd  d          d.

24

tensors of order p

p-linear functions: t : rd    rd              rd     r.
(cid:73) describe t by dp values t (ei1, ei2, . . . , eip).
(cid:73) identify t with multi-index array t     rd  d          d.

formula for function value:

t (x(1), x(2), . . . , x(p)) =

(cid:88)

i1,i2,...,ip

ti1,i2,...,ip    x(1)

i1

x(2)
i2

       x(p)

ip

.

24

tensors of order p

p-linear functions: t : rd    rd              rd     r.
(cid:73) describe t by dp values t (ei1, ei2, . . . , eip).
(cid:73) identify t with multi-index array t     rd  d          d.

formula for function value:

t (x(1), x(2), . . . , x(p)) =

(cid:88)

ti1,i2,...,ip    x(1)

i1

x(2)
i2

       x(p)

ip

.

(cid:73) rank-1 tensor: t = v(1)     v(2)                v(p),

i1,i2,...,ip

t (x(1), x(2), . . . , x(p)) = (cid:104)v(1), x(1)(cid:105)(cid:104)v(2), x(2)(cid:105)      (cid:104)v(p), x(p)(cid:105) .

24

tensors of order p

p-linear functions: t : rd    rd              rd     r.
(cid:73) describe t by dp values t (ei1, ei2, . . . , eip).
(cid:73) identify t with multi-index array t     rd  d          d.

formula for function value:

t (x(1), x(2), . . . , x(p)) =

(cid:88)

ti1,i2,...,ip    x(1)

i1

x(2)
i2

       x(p)

ip

.

(cid:73) rank-1 tensor: t = v(1)     v(2)                v(p),

i1,i2,...,ip

t (x(1), x(2), . . . , x(p)) = (cid:104)v(1), x(1)(cid:105)(cid:104)v(2), x(2)(cid:105)      (cid:104)v(p), x(p)(cid:105) .
symmetric rank-1 tensor: t = v   p = v     v                v,

t (x(1), x(2), . . . , x(p)) = (cid:104)v, x(1)(cid:105)(cid:104)v, x(2)(cid:105)      (cid:104)v, x(p)(cid:105) .

24

usual caveat
(hillar & lim, 2013)

25

                        45mosttensorproblemsarenp-hardchristopherj.hillar,mathematicalsciencesresearchinstitutelek-henglim,universityofchicagoweprovethatmultilinear(tensor)analoguesofmanyef   cientlycomputableproblemsinnumericallinearalgebraarenp-hard.ourlistincludes:determiningthefeasibilityofasystemofbilinearequations,de-cidingwhethera3-tensorpossessesagiveneigenvalue,singularvalue,orspectralnorm;approximatinganeigenvalue,eigenvector,singularvector,orthespectralnorm;anddeterminingtherankorbestrank-1approximationofa3-tensor.furthermore,weshowthatrestrictingtheseproblemstosymmetrictensorsdoesnotalleviatetheirnp-hardness.wealsoexplainhowdecidingnonnegativede   nitenessofasymmetric4-tensorisnp-hardandhowcomputingthecombinatorialhyperdeterminantisnp-,#p-,andvnp-hard.categoriesandsubjectdescriptors:g.1.3[numericalanalysis]:numericallinearalgebrageneralterms:algorithms,theoryadditionalkeywordsandphrases:numericalmultilinearalgebra,tensorrank,tensoreigenvalue,tensorsingularvalue,tensorspectralnorm,systemofmultilinearequations,hyperdeterminants,symmetricten-sors,nonnegativede   nitetensors,bivariatematrixpolynomials,np-hardness,#p-hardness,vnp-hardness,undecidability,polynomialtimeapproximationschemesacmreferenceformat:hillar,c.j.andlim,l.-h.2013.mosttensorproblemsarenp-hard.j.acm60,6,article45(november2013),39pages.doi:http://dx.doi.org/10.1145/25123291.introductionfrequentlyaprobleminscienceorengineeringcanbereducedtosolvingalinear(matrix)systemofequationsandinequalities.othertimes,solutionsinvolvetheex-tractionofcertainquantitiesfrommatricessuchaseigenvectorsorsingularvalues.incomputervision,forinstance,segmentationsofadigitalpicturealongobjectbound-ariescanbefoundbycomputingthetopeigenvectorsofacertainmatrixproducedfromtheimage[shiandmalik2000].anothercommonproblemformulationisto   ndlow-rankmatrixapproximationsthatexplainagiventwo-dimensionalarrayofdata,accomplished,asisnowstandard,byzeroingthesmallestsingularvaluesinasingularvaluedecompositionofthearray[golubandkahan1965;golubandreinsch1970].inc.j.hillarwaspartiallysupportedbyannsayounginvestigatorsgrantandannsfall-institutespost-doctoralfellowshipadministeredbythemathematicalsciencesresearchinstitutethroughitscoregrantdms-0441170.l.-h.limwaspartiallysupportedbyannsfcareerawarddms-1057064,andnsfcollaborativeresearchgrantdms1209136,andanafosryounginvestigatorawardfa9550-13-1-0133.authors   addresses:c.j.hillar,mathematicalsciencesresearchinstitute,berkeley,ca94720;email:chillar@msri.org;l.-h.lim,computationalandappliedmathematicsinitiative,departmentofstatistics,universityofchicago,chicago,il60637;email:lekheng@galton.uchicago.edu.permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalorclassroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributedforpro   torcommercialadvantageandthatcopiesshowthisnoticeonthe   rstpageorinitialscreenofadisplayalongwiththefullcitation.copyrightsforcomponentsofthisworkownedbyothersthanacmmustbehonored.abstractingwithcreditisper-mitted.tocopyotherwise,torepublish,topostonservers,toredistributetolists,ortouseanycomponentofthisworkinotherworksrequirespriorspeci   cpermissionand/orafee.permissionsmayberequestedfrompublicationsdept.,acm,inc.,2pennplaza,suite701,newyork,ny10121-0701usa,fax+1(212)869-0481,orpermissions@acm.org.c   2013acm0004-5411/2013/11-art45$15.00doi:http://dx.doi.org/10.1145/2512329journaloftheacm,vol.60,no.6,article45,publicationdate:november2013.jennrich   s algorithm (simpli   ed)

task: given tensor t =(cid:80)k

components {vt}k

t=1 v   3

t with linearly independent

t=1,    nd the components (up to scaling).

26

jennrich   s algorithm (simpli   ed)

task: given tensor t =(cid:80)k

components {vt}k

t=1 v   3

t with linearly independent

t=1,    nd the components (up to scaling).

jennrich   s algorithm: based on    collapsing    the tensor.

26

jennrich   s algorithm (simpli   ed)

task: given tensor t =(cid:80)k

components {vt}k

t=1 v   3

t with linearly independent

t=1,    nd the components (up to scaling).

jennrich   s algorithm: based on    collapsing    the tensor.

(cid:73) think of t : rd    rd    rd     r as t : rd     rd  d:

[t (x)]j,k = t (x, ej, ek) .

(like    currying    in functional programming.)

26

jennrich   s algorithm (simpli   ed)

task: given tensor t =(cid:80)k

components {vt}k

t=1 v   3

t with linearly independent

t=1,    nd the components (up to scaling).

jennrich   s algorithm: based on    collapsing    the tensor.

(cid:73) think of t : rd    rd    rd     r as t : rd     rd  d:

[t (x)]j,k = t (x, ej, ek) .

(like    currying    in functional programming.)

input tensor t     rd  d  d.
1: pick x, y independently & uniformly at random from sd   1.
2: compute and return eigenvectors of t (x)t (y)   

(with non-zero eigenvalues).

26

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

(vt     vt     vt)(x)

t (x) =

t=1

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t

t (x) =

t=1

t=1

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t (x) =

t = v dxv (cid:62)

t=1

t=1

where v = [v1|      |vk] and dx = diag((cid:104)v1, x(cid:105), . . . ,(cid:104)vk, x(cid:105)).

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t (x) =

t = v dxv (cid:62)

t=1

t=1

where v = [v1|      |vk] and dx = diag((cid:104)v1, x(cid:105), . . . ,(cid:104)vk, x(cid:105)).

by linear independence of {vt}k

t=1 and random choice of x and y:

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t (x) =

t = v dxv (cid:62)

t=1

t=1

where v = [v1|      |vk] and dx = diag((cid:104)v1, x(cid:105), . . . ,(cid:104)vk, x(cid:105)).

by linear independence of {vt}k
1. v has rank k;

t=1 and random choice of x and y:

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t (x) =

t = v dxv (cid:62)

t=1

t=1

where v = [v1|      |vk] and dx = diag((cid:104)v1, x(cid:105), . . . ,(cid:104)vk, x(cid:105)).

by linear independence of {vt}k
1. v has rank k;
2. dx and dy are invertible (a.s.);

t=1 and random choice of x and y:

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t (x) =

t = v dxv (cid:62)

t=1

t=1

where v = [v1|      |vk] and dx = diag((cid:104)v1, x(cid:105), . . . ,(cid:104)vk, x(cid:105)).

by linear independence of {vt}k
1. v has rank k;
2. dx and dy are invertible (a.s.);
3. diagonal entries of dxd   1

y are distinct (a.s.);

t=1 and random choice of x and y:

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t (x) =

t = v dxv (cid:62)

t=1

t=1

where v = [v1|      |vk] and dx = diag((cid:104)v1, x(cid:105), . . . ,(cid:104)vk, x(cid:105)).

t=1 and random choice of x and y:

by linear independence of {vt}k
1. v has rank k;
2. dx and dy are invertible (a.s.);
3. diagonal entries of dxd   1
4. t (x)t (y)    = v (dxd   1

y are distinct (a.s.);
y )v     (a.s.).

27

analysis of jennrich   s algorithm

for t =(cid:80)k
t=1 vt     vt     vt, linearity of    collapsing    implies
k(cid:88)

k(cid:88)

(vt     vt     vt)(x) =

(cid:104)vt, x(cid:105)vtv(cid:62)

t (x) =

t = v dxv (cid:62)

t=1

t=1

where v = [v1|      |vk] and dx = diag((cid:104)v1, x(cid:105), . . . ,(cid:104)vk, x(cid:105)).

t=1 and random choice of x and y:

by linear independence of {vt}k
1. v has rank k;
2. dx and dy are invertible (a.s.);
3. diagonal entries of dxd   1
4. t (x)t (y)    = v (dxd   1
so {vt}k
non-zero eigenvalues.

y are distinct (a.s.);
y )v     (a.s.).

t=1 are the eigenvectors of t (x)t (y)    with distinct

27

application to topic model parameters

probabilities of word triples as third-order tensor:

k(cid:88)

k(cid:88)

t =

wtp t     p t     p t =

t=1

t=1

for vt = w1/3

t p t.

vt     vt     vt

28

application to topic model parameters

probabilities of word triples as third-order tensor:

k(cid:88)

k(cid:88)

t =

wtp t     p t     p t =

vt     vt     vt

t=1

t=1

for vt = w1/3

t p t.

(cid:73) about pre-condition for jennrich   s algorithm:

{vt}k
    {p t}k

t=1 are linearly independent
t=1 are linearly independent and all wt > 0.

28

application to topic model parameters

probabilities of word triples as third-order tensor:

k(cid:88)

k(cid:88)

t =

wtp t     p t     p t =

vt     vt     vt

t=1

t=1

for vt = w1/3

t p t.

(cid:73) about pre-condition for jennrich   s algorithm:

{vt}k
    {p t}k
(cid:73) can recover {p t}k

t=1 are linearly independent
t=1 are linearly independent and all wt > 0.
t=1 from {ctvt}k

t=1 for any ct (cid:54)= 0.

28

application to topic model parameters

probabilities of word triples as third-order tensor:

k(cid:88)

k(cid:88)

t =

wtp t     p t     p t =

vt     vt     vt

t=1

t=1

for vt = w1/3

t p t.

(cid:73) about pre-condition for jennrich   s algorithm:

{vt}k
    {p t}k
(cid:73) can recover {p t}k
(cid:73) can recover {(p t, wt)}k

t=1 are linearly independent
t=1 are linearly independent and all wt > 0.
t=1 from {ctvt}k

t=1 for any ct (cid:54)= 0.

t=1 from {p t}k

t=1 and t .

28

recap

(cid:73) parameters of topic model for single-topic documents
(satisfying linear independence condition) can be e   ciently
recovered from distribution of three-word documents.

29

recap

(cid:73) parameters of topic model for single-topic documents
(satisfying linear independence condition) can be e   ciently
recovered from distribution of three-word documents.

(cid:73) two-word documents not su   cient.

29

illustrative empirical results

(cid:73) corpus: 300, 000 new york times articles.
(cid:73) vocabulary size: 102, 660 words.
(cid:73) set number of topics k := 50.

model predictive performance:
    4   8   speed-up over id150 for lda;
comparable to    fastlda    (porteous, newman, ihler, asuncion, smyth, &
welling, 2008).

method-of-moments

id150

fastlda

s
s
o

l

g
o
l

8.6

8.4

0

0.5
1.5
training time (  104 sec)

1

2

30

illustrative empirical results

sample topics: (showing top 10 words for each topic)

econ.
sales

economic
consumer

major
home

indicator
weekly
order
claim

scheduled

baseball

run
inning

hit
game
season
home
right
games
dodger

left

edu.
school
student
teacher
program
o   cial
public
children

high

education
district

health care

drug
patient
million
company
doctor

companies
percent

cost

program
health

tiger_wood

golf
player

won
shot
play
round
win

tour
right

tournament

31

illustrative empirical results

sample topics: (showing top 10 words for each topic)

invest.
percent
stock
market
fund

investor
companies

analyst
money

investment
economy

election
al_gore
campaign
president

george_bush

bush
clinton

vice

presidential

million

democratic

auto race child   s lit. afghan war

car
race
driver
team
won
win
racing
track
season

lap

book
children

ages
author
read

web
writer
written
sales

newspaper

afghanistan

taliban
attack

o   cial
military

u_s

united_states

terrorist

war
bin

32

illustrative empirical results

sample topics: (showing top 10 words for each topic)

web
com
www
site
web
sites

online
mail

internet
telegram

antitrust

court
case
law
lawyer
federal

decision

trial

microsoft

right

information

government

tv
show
network
season
nbc
cb

program
television

series
night

new_york

movies

   lm
movie
director

play

character

actor
show
movies
million
part

music
music
song
group
part

new_york
company
million
band
show
album

etc.

33

learning algorithms

(cid:73) estimation via method-of-moments:

1. estimate distribution of three-word documents     (cid:98)t
2. approximately decompose (cid:98)t     estimates {((cid:98)p t,   wt)}k

(empirical moment tensor).

t=1.

34

learning algorithms

(cid:73) estimation via method-of-moments:

1. estimate distribution of three-word documents     (cid:98)t
2. approximately decompose (cid:98)t     estimates {((cid:98)p t,   wt)}k

(empirical moment tensor).

t=1.

(cid:73) issues:

1. accuracy of moment estimates?

2. robustness of (approximate) tensor decomposition?

3. generality beyond simple topic models?

34

learning algorithms

(cid:73) estimation via method-of-moments:

1. estimate distribution of three-word documents     (cid:98)t
2. approximately decompose (cid:98)t     estimates {((cid:98)p t,   wt)}k

(empirical moment tensor).

t=1.

(cid:73) issues:

1. accuracy of moment estimates?

can more reliably estimate lower-order moments;
distribution-speci   c sample complexity bounds.

2. robustness of (approximate) tensor decomposition?

instead of jennrich   s algorithm, use more error-tolerant
decomposition algorithm (also computationally e   cient).

3. generality beyond simple topic models?

34

learning algorithms

(cid:73) estimation via method-of-moments:

1. estimate distribution of three-word documents     (cid:98)t
2. approximately decompose (cid:98)t     estimates {((cid:98)p t,   wt)}k

(empirical moment tensor).

t=1.

(cid:73) issues:

1. accuracy of moment estimates?

can more reliably estimate lower-order moments;
distribution-speci   c sample complexity bounds.

2. robustness of (approximate) tensor decomposition?

instead of jennrich   s algorithm, use more error-tolerant
decomposition algorithm (also computationally e   cient).

3. generality beyond simple topic models?

next: moment decompositions for other models.

34

2. moment decompositions for other models

35

moment decompositions

some examples of usable moment decompositions.

1. two classical mixture models.
2. models with multi-view structure.

36

mixtures of spherical gaussians

h     discrete(  1,   2, . . . ,   k)

(hidden) ;

x | h = t     normal(  t,   2

t i d) ,

t     [k] .

37

  1  2  3rdmixtures of spherical gaussians

h     discrete(  1,   2, . . . ,   k)

(hidden) ;

x | h = t     normal(  t,   2i d) ,
(for simplicity, restrict   1 =   2 =        =   k =   .)

t     [k] .

37

  1  2  3rdmixtures of spherical gaussians

h     discrete(  1,   2, . . . ,   k)

(hidden) ;

x | h = t     normal(  t,   2i d) ,
(for simplicity, restrict   1 =   2 =        =   k =   .)

t     [k] .

generative process:

x = y +   z

where pr(y =   t) =   t, and
z     normal(0, i d)

(indep. of y ).

37

  1  2  3rdmoments for spherical gaussian mixtures

first- and second-order moments:

k(cid:88)
k(cid:88)

t=1

e(x) =

e(x     x) =

  t      t ,

  t      t       t +   2i d .

t=1

38

moments for spherical gaussian mixtures

first- and second-order moments:

k(cid:88)
k(cid:88)

t=1

e(x) =

e(x     x) =

  t      t ,

  t      t       t +   2i d .

t=1

(vempala & wang, 2002):
span of top k eigenvectors of e(x     x) contains {  t}k
t=1.
    principal component analysis (pca).

38

use of moments for mixtures of spherical gaussians

separation (dasgupta, 1999):
# standard deviations between component means

sep := min
i(cid:54)=j

(cid:107)  i       j(cid:107)

  

.

39

use of moments for mixtures of spherical gaussians

separation (dasgupta, 1999):
# standard deviations between component means

sep := min
i(cid:54)=j

(cid:107)  i       j(cid:107)

  

.

(cid:73) (dasgupta & schulman, 2000, 2007):

distance-based id91 (e.g., em) works when sep (cid:38) d1/4.

39

use of moments for mixtures of spherical gaussians

separation (dasgupta, 1999):
# standard deviations between component means

sep := min
i(cid:54)=j

(cid:107)  i       j(cid:107)

  

.

(cid:73) (dasgupta & schulman, 2000, 2007):

distance-based id91 (e.g., em) works when sep (cid:38) d1/4.

(cid:73) (vempala & wang, 2002):

problem becomes k-dimensional via pca (assume k     d).
required separation reduced to sep (cid:38) k1/4.

39

use of moments for mixtures of spherical gaussians

separation (dasgupta, 1999):
# standard deviations between component means

sep := min
i(cid:54)=j

(cid:107)  i       j(cid:107)

  

.

(cid:73) (dasgupta & schulman, 2000, 2007):

distance-based id91 (e.g., em) works when sep (cid:38) d1/4.

(cid:73) (vempala & wang, 2002):

problem becomes k-dimensional via pca (assume k     d).
required separation reduced to sep (cid:38) k1/4.

third-order moments identify the mixture distribution when
{  t}k

t=1 are lin. indpt.; sep may be arbitrarily close to zero.

39

use of moments for mixtures of spherical gaussians

separation (dasgupta, 1999):
# standard deviations between component means

sep := min
i(cid:54)=j

(cid:107)  i       j(cid:107)

  

.

(cid:73) (dasgupta & schulman, 2000, 2007):

distance-based id91 (e.g., em) works when sep (cid:38) d1/4.

(cid:73) (vempala & wang, 2002):

problem becomes k-dimensional via pca (assume k     d).
required separation reduced to sep (cid:38) k1/4.

t=1 are lin. indpt.; sep may be arbitrarily close to zero.

third-order moments identify the mixture distribution when
{  t}k
(belkin & sinha, 2010; moitra & valiant, 2010):
general gaussians & no minimum sep, but    (k)th-order moments.

39

third-order moments of spherical gaussian mixtures

generative process:

x = y +   z

where pr(y =   t) =   t, and z     normal(0, i d) (indep. of y ).

third-order moment tensor:

e(cid:0)x   3(cid:1) = e(cid:0){y +   z}   3(cid:1)

40

third-order moments of spherical gaussian mixtures

generative process:

x = y +   z

where pr(y =   t) =   t, and z     normal(0, i d) (indep. of y ).

third-order moment tensor:

e(cid:0)x   3(cid:1) = e(cid:0){y +   z}   3(cid:1)
= e(cid:0)y    3(cid:1) +   2 e(cid:0)y     z     z + z     y     z + z     z     y(cid:1)

40

third-order moments of spherical gaussian mixtures

generative process:

x = y +   z

where pr(y =   t) =   t, and z     normal(0, i d) (indep. of y ).

third-order moment tensor:

e(cid:0)x   3(cid:1) = e(cid:0){y +   z}   3(cid:1)
= e(cid:0)y    3(cid:1) +   2 e(cid:0)y     z     z + z     y     z + z     z     y(cid:1)
k(cid:88)
(cid:124) (cid:123)(cid:122) (cid:125)

  t         3

t +    (  2,   )
some tensor

t=1

=

.

40

tensor decomposition for spherical gaussian mixtures
(h. & kakade, 2013)

e(cid:0)x   3(cid:1) =

k(cid:88)

t=1

  t         3

(cid:124) (cid:123)(cid:122) (cid:125)

t +    (  2,   )
some tensor

.

41

tensor decomposition for spherical gaussian mixtures
(h. & kakade, 2013)

e(cid:0)x   3(cid:1) =

k(cid:88)

t=1

  t         3

(cid:124) (cid:123)(cid:122) (cid:125)

t +    (  2,   )
some tensor

.

claim:    and   2 are functions of e(x) and e(x     x).

41

tensor decomposition for spherical gaussian mixtures
(h. & kakade, 2013)

e(cid:0)x   3(cid:1) =

k(cid:88)

t=1

  t         3

(cid:124) (cid:123)(cid:122) (cid:125)

t +    (  2,   )
some tensor

.

claim:    and   2 are functions of e(x) and e(x     x).

claim: if {  t}k
{(  t,   t)}k

t=1 are identi   able from

t=1 are linearly independent and all   t > 0, then

t := e(x   3)        (  2,   ) =

  t         3

t

.

k(cid:88)

t=1

41

tensor decomposition for spherical gaussian mixtures
(h. & kakade, 2013)

e(cid:0)x   3(cid:1) =

k(cid:88)

t=1

  t         3

(cid:124) (cid:123)(cid:122) (cid:125)

t +    (  2,   )
some tensor

.

claim:    and   2 are functions of e(x) and e(x     x).

claim: if {  t}k
{(  t,   t)}k

t=1 are identi   able from

t=1 are linearly independent and all   t > 0, then

t := e(x   3)        (  2,   ) =

  t         3

t

.

t=1

can use, e.g., jennrich   s algorithm to recover {(  t,   t)}k

t=1 from t .

41

k(cid:88)

even more gaussian mixtures

note: linear independence condition on {  t}k

t=1 requires k     d.

42

even more gaussian mixtures

note: linear independence condition on {  t}k
(cid:73) (anderson, belkin, goyal, rademacher, & voss, 2014),
(bhaskara, charikar, moitra, & vijayaraghavan, 2014)
mixtures of do(1) gaussians (w/ simple or known covariance)
via smoothed analysis and o(1)-order moments.

t=1 requires k     d.

42

even more gaussian mixtures

note: linear independence condition on {  t}k
(cid:73) (anderson, belkin, goyal, rademacher, & voss, 2014),
(bhaskara, charikar, moitra, & vijayaraghavan, 2014)
mixtures of do(1) gaussians (w/ simple or known covariance)
via smoothed analysis and o(1)-order moments.

t=1 requires k     d.

(cid:73) (ge, huang, & kakade, 2015)

also with arbitrary unknown covariances.

42

mixtures of id75s

h     discrete(  1,   2, . . . ,   k)
x     normal(  ,   ) ;

y | h = t, x = x     normal((cid:104)  t, x(cid:105),   2) .

(hidden) ;

43

mixtures of id75s

h     discrete(  1,   2, . . . ,   k)
x     normal(  ,   ) ;

y | h = t, x = x     normal((cid:104)  t, x(cid:105),   2) .

(hidden) ;

43

x-4-202468y-505mixtures of id75s

h     discrete(  1,   2, . . . ,   k)
x     normal(  ,   ) ;

y | h = t, x = x     normal((cid:104)  t, x(cid:105),   2) .

(hidden) ;

43

x-4-202468y-505use of moments for mixtures of id75s

second-order moments (assume x     normal(0, i d)):

k(cid:88)

        2 +

k(cid:88)

e(y 2xx(cid:62)) = 2

  t      t  (cid:62)

t +

  t    (cid:107)  t(cid:107)2

t=1

t=1

       i d .

44

use of moments for mixtures of id75s

second-order moments (assume x     normal(0, i d)):

k(cid:88)

        2 +

k(cid:88)

       i d .

e(y 2xx(cid:62)) = 2

  t      t  (cid:62)

t +

  t    (cid:107)  t(cid:107)2

t=1

t=1

(cid:73) span of top k eigenvectors of e(y 2xx(cid:62)) contains {  t}k
t=1.

44

use of moments for mixtures of id75s

second-order moments (assume x     normal(0, i d)):

k(cid:88)

        2 +

k(cid:88)

       i d .

e(y 2xx(cid:62)) = 2

  t      t  (cid:62)

t +

  t    (cid:107)  t(cid:107)2

t=1

t=1

(cid:73) span of top k eigenvectors of e(y 2xx(cid:62)) contains {  t}k
t=1.
(cid:73) using stein   s identity (1973), similar approach works for glms

(sun, ioannidis, & montanari, 2013).

44

use of moments for mixtures of id75s

second-order moments (assume x     normal(0, i d)):

k(cid:88)

        2 +

k(cid:88)

       i d .

e(y 2xx(cid:62)) = 2

  t      t  (cid:62)

t +

  t    (cid:107)  t(cid:107)2

t=1

t=1

(cid:73) span of top k eigenvectors of e(y 2xx(cid:62)) contains {  t}k
t=1.
(cid:73) using stein   s identity (1973), similar approach works for glms

(sun, ioannidis, & montanari, 2013).

tensor decomposition approach:
can recover parameters {(  t,   t)}k
(chaganty & liang, 2013; yi, caramanis, & sanghavi, 2014, 2016).

t=1 with higher-order moments

44

use of moments for mixtures of id75s

second-order moments (assume x     normal(0, i d)):

k(cid:88)

        2 +

k(cid:88)

       i d .

e(y 2xx(cid:62)) = 2

  t      t  (cid:62)

t +

  t    (cid:107)  t(cid:107)2

t=1

t=1

(cid:73) span of top k eigenvectors of e(y 2xx(cid:62)) contains {  t}k
t=1.
(cid:73) using stein   s identity (1973), similar approach works for glms

(sun, ioannidis, & montanari, 2013).

tensor decomposition approach:
can recover parameters {(  t,   t)}k
(chaganty & liang, 2013; yi, caramanis, & sanghavi, 2014, 2016).
also for glms, via stein   s identity (sedghi & anandkumar, 2014).

t=1 with higher-order moments

44

simpler setting: mixed random linear equations
(yi, caramanis, & sanghavi, 2016)

h     discrete(  1,   2, . . . ,   k)
x     normal(0, i d) ;
y = (cid:104)  h , x(cid:105) .

(hidden) ;

45

simpler setting: mixed random linear equations
(yi, caramanis, & sanghavi, 2016)

h     discrete(  1,   2, . . . ,   k)
x     normal(0, i d) ;
y = (cid:104)  h , x(cid:105) .

(hidden) ;

t=1 are linearly independent and all   t > 0, then

t=1 are identi   able from

claim: if {  t}k
parameters {(  t,   t)}k

t := e(cid:0)y 3x   3(cid:1) = 6

k(cid:88)

t=1

  t         3

t +    (e y 3x)
some tensor

(cid:124)

(cid:123)(cid:122)

(cid:125)

.

45

recap: mixtures of gaussians and id75s

(cid:73) parameters of gaussian mixture models and related models

(satisfying linear independence condition) can be e   ciently
recovered from o(1)-order moments.

46

recap: mixtures of gaussians and id75s

(cid:73) parameters of gaussian mixture models and related models

(satisfying linear independence condition) can be e   ciently
recovered from o(1)-order moments.

(cid:73) exploit distributional properties to determine usable moments.

46

recap: mixtures of gaussians and id75s

(cid:73) parameters of gaussian mixture models and related models

(satisfying linear independence condition) can be e   ciently
recovered from o(1)-order moments.

(cid:73) exploit distributional properties to determine usable moments.
(cid:73) smoothed analysis: avoid linear independence condition for

   most    mixture distributions.

46

recap: mixtures of gaussians and id75s

(cid:73) parameters of gaussian mixture models and related models

(satisfying linear independence condition) can be e   ciently
recovered from o(1)-order moments.

(cid:73) exploit distributional properties to determine usable moments.
(cid:73) smoothed analysis: avoid linear independence condition for

   most    mixture distributions.

next: multi-view approach to    nding usable moments.

46

multi-view interpretation of topic model

recall: topic model for single-topic documents

h

x 1 x 2

       x l

k topics (dists. over words) {p t}k
pick topic h = t with prob. wt (hidden).
word tokens x 1, x 2, . . . , x l

iid    p h.

t=1.

47

multi-view interpretation of topic model

recall: topic model for single-topic documents

h

x 1 x 2

       x l

k topics (dists. over words) {p t}k
pick topic h = t with prob. wt (hidden).
word tokens x 1, x 2, . . . , x l

iid    p h.

t=1.

key property:
x 1, x 2, . . . , x l conditionally independent given h.

47

multi-view interpretation of topic model

recall: topic model for single-topic documents

h

x 1 x 2

       x l

k topics (dists. over words) {p t}k
pick topic h = t with prob. wt (hidden).
word tokens x 1, x 2, . . . , x l

iid    p h.

t=1.

key property:
x 1, x 2, . . . , x l conditionally independent given h.
each word token x i provides new    view    of hidden variable h.

47

multi-view interpretation of topic model

recall: topic model for single-topic documents

h

x 1 x 2

       x l

k topics (dists. over words) {p t}k
pick topic h = t with prob. wt (hidden).
word tokens x 1, x 2, . . . , x l

iid    p h.

t=1.

key property:
x 1, x 2, . . . , x l conditionally independent given h.
each word token x i provides new    view    of hidden variable h.
some previous theoretical analysis:

(cid:73) (blum & mitchell, 1998)

co-training in semi-supervised learning.

(cid:73) (chaudhuri, kakade, livescu, & sridharan, 2009)

multi-view gaussian mixture models.

47

multi-view mixture model

h

x 1

x 2

x 3

view 1: x 1 view 2: x 2 view 3: x 3

48

multi-view mixture model

h

x 1

x 2

x 3

view 1: x 1 view 2: x 2 view 3: x 3

48

multi-view mixture model

h

x 2

k(cid:88)

x 1

e (x 1     x 2     x 3) =

x 3

  t      (1)

t       (2)

t       (3)

t

where   (i)

t=1

t = e[x i | h = t] ,
  t = pr(h = t) .

48

multi-view mixture model

h

x 2

k(cid:88)

x 1

e (x 1     x 2     x 3) =

x 3

  t      (1)

t       (2)

t       (3)

t

where   (i)

t=1

t = e[x i | h = t] ,
  t = pr(h = t) .

jennrich   s algorithm works in this asymmetric case provided
{  (j)
t=1 are linearly independent for each j, and all   t > 0.

t }k

48

multi-view mixture model

h

x 2

k(cid:88)

x 1

e (x 1     x 2     x 3) =

x 3

  t      (1)

t       (2)

t       (3)

t

where   (i)

t=1

t = e[x i | h = t] ,
  t = pr(h = t) .

t }k

jennrich   s algorithm works in this asymmetric case provided
{  (j)
t=1 are linearly independent for each j, and all   t > 0.
(also possible to    symmetrize    using second-order moments.)

48

examples of multi-view mixture models
(mossel & roch, 2006; anandkumar, h., & kakade, 2012)

1. mixtures of high-dimensional product distributions.

(e.g., mixtures of axis-aligned gaussians.)

49

examples of multi-view mixture models
(mossel & roch, 2006; anandkumar, h., & kakade, 2012)

1. mixtures of high-dimensional product distributions.

(e.g., mixtures of axis-aligned gaussians.)

2. id48.

h1

h2

h3

      

h2

x 1 x 2 x 3

x 1 x 2 x 3

49

examples of multi-view mixture models
(mossel & roch, 2006; anandkumar, h., & kakade, 2012)

1. mixtures of high-dimensional product distributions.

(e.g., mixtures of axis-aligned gaussians.)

2. id48.

h1

h2

h3

      

h2

x 1 x 2 x 3

x 1 x 2 x 3

3. phylogenetic trees.

(cid:73) x 1, x 2, x 3: genes of three extant species.
(cid:73) h: lca of most closely related pair of species.

49

examples of multi-view mixture models
(mossel & roch, 2006; anandkumar, h., & kakade, 2012)

1. mixtures of high-dimensional product distributions.

(e.g., mixtures of axis-aligned gaussians.)

2. id48.

h1

h2

h3

      

h2

x 1 x 2 x 3

x 1 x 2 x 3

3. phylogenetic trees.

(cid:73) x 1, x 2, x 3: genes of three extant species.
(cid:73) h: lca of most closely related pair of species.

4. . . .

49

recap

(cid:73) parameters of many latent variable models

(satisfying non-degeneracy conditions) can be e   ciently
recovered from o(1)-order moments.

50

recap

(cid:73) parameters of many latent variable models

(satisfying non-degeneracy conditions) can be e   ciently
recovered from o(1)-order moments.

(cid:73) exploit distributional properties, multi-view structure, and

other structure to determine usable moments.

50

recap

(cid:73) parameters of many latent variable models

(satisfying non-degeneracy conditions) can be e   ciently
recovered from o(1)-order moments.

(cid:73) exploit distributional properties, multi-view structure, and

other structure to determine usable moments.

(cid:73) estimation via method-of-moments:

1. estimate moments     empirical moment tensor (cid:98)t .
2. approximately decompose (cid:98)t     parameter estimate     .

50

recap

(cid:73) parameters of many latent variable models

(satisfying non-degeneracy conditions) can be e   ciently
recovered from o(1)-order moments.

(cid:73) exploit distributional properties, multi-view structure, and

other structure to determine usable moments.

(cid:73) estimation via method-of-moments:

1. estimate moments     empirical moment tensor (cid:98)t .
2. approximately decompose (cid:98)t     parameter estimate     .

next: error-tolerant (approximate) tensor decomposition.

50

3. error-tolerant algorithms for tensor

decompositions

51

moment estimates

estimation of e[x   3] (say) from iid sample {xi}n

i=1:

(cid:98)e[x   3]

n(cid:88)

:=

1
n

x   3

i

.

i=1

52

moment estimates

estimation of e[x   3] (say) from iid sample {xi}n

i=1:

(cid:98)e[x   3]

n(cid:88)

i=1

:=

1
n

x   3

i

.

inevitably expect error of order n   1/2 in some norm, e.g.,

t (x, y, z)

(operator norm) ,

(cid:107)t(cid:107) :=

(cid:107)t(cid:107)f :=

sup

x,y,z   sd   1

      (cid:88)

i,j,k

t 2
i,j,k

      1/2

(frobenius norm) .

52

using jennrich   s algorithm

recall: jennrich   s algorithm (simpli   ed)

goal: given tensor t =(cid:80)k

t=1 v   3

t

,    nd components {vt}k

t=1.

input tensor t     rd  d  d.
1: pick x, y independently & uniformly at random from sd   1.
2: compute and return eigenvectors of t (x)t (y)   

(with non-zero eigenvalues).

53

using jennrich   s algorithm

recall: jennrich   s algorithm (simpli   ed)

goal: given tensor t =(cid:80)k

t=1 v   3

t

,    nd components {vt}k

t=1.

input tensor t     rd  d  d.
1: pick x, y independently & uniformly at random from sd   1.
2: compute and return eigenvectors of t (x)t (y)   

(with non-zero eigenvalues).

but we only have (cid:98)t , an estimate of t =(cid:80)k
(cid:107)(cid:98)t     t(cid:107) (cid:46) n   1/2 .

t=1 v   3

t with (say)

53

stability of jennrich   s algorithm

stability of eigenvectors requires eigenvalue gaps.

54

stability of jennrich   s algorithm

stability of eigenvectors requires eigenvalue gaps.

(cid:73) eigenvalue gaps for t (x)t (y)   :

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)vi, x(cid:105)
(cid:104)vi, y(cid:105)     (cid:104)vj, x(cid:105)
(cid:104)vj, y(cid:105)

    := min
i(cid:54)=j

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

54

stability of jennrich   s algorithm

stability of eigenvectors requires eigenvalue gaps.

(cid:73) eigenvalue gaps for t (x)t (y)   :

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)vi, x(cid:105)
(cid:104)vi, y(cid:105)     (cid:104)vj, x(cid:105)
(cid:104)vj, y(cid:105)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

    := min
i(cid:54)=j

(cid:73) need (cid:107)(cid:98)t (x)(cid:98)t (y)        t (x)t (y)   (cid:107) (cid:28)     so that (cid:98)t (x)(cid:98)t (y)   

also has su   cient eigenvalue gaps.

54

stability of jennrich   s algorithm

stability of eigenvectors requires eigenvalue gaps.

    := min
i(cid:54)=j

(cid:73) eigenvalue gaps for t (x)t (y)   :

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)vi, x(cid:105)
(cid:104)vi, y(cid:105)     (cid:104)vj, x(cid:105)
(cid:104)vj, y(cid:105)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:73) need (cid:107)(cid:98)t (x)(cid:98)t (y)        t (x)t (y)   (cid:107) (cid:28)     so that (cid:98)t (x)(cid:98)t (y)   
(cid:73) ultimately, appears to need (cid:107)(cid:98)t     t(cid:107)f (cid:28) 1

also has su   cient eigenvalue gaps.

poly(d).

54

stability of jennrich   s algorithm

stability of eigenvectors requires eigenvalue gaps.

    := min
i(cid:54)=j

(cid:73) eigenvalue gaps for t (x)t (y)   :

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:104)vi, x(cid:105)
(cid:104)vi, y(cid:105)     (cid:104)vj, x(cid:105)
(cid:104)vj, y(cid:105)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:73) need (cid:107)(cid:98)t (x)(cid:98)t (y)        t (x)t (y)   (cid:107) (cid:28)     so that (cid:98)t (x)(cid:98)t (y)   
(cid:73) ultimately, appears to need (cid:107)(cid:98)t     t(cid:107)f (cid:28) 1

also has su   cient eigenvalue gaps.

poly(d).

next: a di   erent approach.

54

reduction to orthonormal case

k(cid:88)
k(cid:88)

t=1

in many (all?) applications, we can estimate moments of the form

m =

and

t =

vt     vt ,

(e.g., word pairs)

  t    vt     vt     vt .

(e.g., word triples)

t=1

(here, we assume {vt}k

t=1 are linearly independent, and {  t}k

t=1 are positive.)

55

reduction to orthonormal case

k(cid:88)
k(cid:88)

t=1

in many (all?) applications, we can estimate moments of the form

m =

and

t =

vt     vt ,

(e.g., word pairs)

  t    vt     vt     vt .

(e.g., word triples)

t=1

(here, we assume {vt}k

t=1 are linearly independent, and {  t}k

t=1 are positive.)

(cid:73) m is positive semide   nite of rank k.

55

reduction to orthonormal case

k(cid:88)
k(cid:88)

t=1

in many (all?) applications, we can estimate moments of the form

m =

and

t =

vt     vt ,

(e.g., word pairs)

  t    vt     vt     vt .

(e.g., word triples)

t=1

(here, we assume {vt}k

t=1 are linearly independent, and {  t}k

t=1 are positive.)

(cid:73) m is positive semide   nite of rank k.
(cid:73) m determines inner product system on span{vt}k

t=1 s.t.

{vt}k

t=1 are orthonormal.

55

(nearly) orthogonally decomposable tensors (d = k)

goal: given tensor (cid:98)t     rd  d  d such that (cid:107)(cid:98)t     t(cid:107)        for
some t =(cid:80)d

t where {vt}d

t=1 are orthonormal and

t=1   t    v   3

all   t > 0, approximately recover {(vt,   t)}d

t=1.

56

(nearly) orthogonally decomposable tensors (d = k)

goal: given tensor (cid:98)t     rd  d  d such that (cid:107)(cid:98)t     t(cid:107)        for
some t =(cid:80)d

t where {vt}d

t=1 are orthonormal and

t=1   t    v   3

all   t > 0, approximately recover {(vt,   t)}d

t=1.

analogous matrix problems:
(cid:73)    = 0: eigendecomposition.

(   promised    decomposition always exists by symmetry.)

56

(nearly) orthogonally decomposable tensors (d = k)

goal: given tensor (cid:98)t     rd  d  d such that (cid:107)(cid:98)t     t(cid:107)        for
some t =(cid:80)d

t where {vt}d

t=1 are orthonormal and

t=1   t    v   3

all   t > 0, approximately recover {(vt,   t)}d

t=1.

analogous matrix problems:
(cid:73)    = 0: eigendecomposition.

(   promised    decomposition always exists by symmetry.)
decomposition is unique i    the {  t}d

t=1 are distinct.

56

(nearly) orthogonally decomposable tensors (d = k)

goal: given tensor (cid:98)t     rd  d  d such that (cid:107)(cid:98)t     t(cid:107)        for
some t =(cid:80)d

t where {vt}d

t=1 are orthonormal and

t=1   t    v   3

all   t > 0, approximately recover {(vt,   t)}d

t=1.

analogous matrix problems:
(cid:73)    = 0: eigendecomposition.

(   promised    decomposition always exists by symmetry.)
decomposition is unique i    the {  t}d

t=1 are distinct.

(cid:73)    > 0: perturbation theory for eigenvalues (weyl) and

eigenvectors (davis & kahan).

56

exact orthogonally decomposable tensor
(zhang & golub, 2001)

for now assume    = 0, so (cid:98)t = t .

matching moments:

{(  vt,     t)}d

t=1 := arg min
{(xt,  t)}d

t=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)t     d(cid:88)

t=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

f

.

  t    x   3

t

57

exact orthogonally decomposable tensor
(zhang & golub, 2001)

for now assume    = 0, so (cid:98)t = t .

matching moments:

{(  vt,     t)}d

t=1 := arg min
{(xt,  t)}d

t=1

(cid:73) greedy approach:

(cid:73) find best rank-1 approximation:

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

f

.

  t    x   3

t

t=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)t     d(cid:88)
(cid:13)(cid:13)(cid:13)t           x   3(cid:13)(cid:13)(cid:13)2

f

(  v,     ) :=

arg min
(cid:73)    de   ate    t := t               v

(x,  )   sd   1  r+

   3 and repeat.

.

57

exact orthogonally decomposable tensor
(zhang & golub, 2001)

for now assume    = 0, so (cid:98)t = t .

matching moments:

{(  vt,     t)}d

t=1 := arg min
{(xt,  t)}d

t=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)t     d(cid:88)

t=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

f

.

  t    x   3

t

(cid:73) greedy approach:

(cid:73) find best rank-1 approximation:

  v := arg max
x   sd   1

t (x, x, x) ,

     := t (  v,   v,   v) .

(cid:73)    de   ate    t := t               v

   3 and repeat.

57

rank-1 approximation problem

claim: local maximizers of the function

x (cid:55)    t (x, x, x) =

ti,j,k    xixjxk

(cid:88)

i,j,k

(over the unit ball) are {vt}d

t=1, and

t (vt, vt, vt) =   t ,

t     [d] .

58

rank-1 approximation problem

(cid:88)

claim: local maximizers of the function

x (cid:55)    t (x, x, x) =

ti,j,k    xixjxk =

i,j,k

(over the unit ball) are {vt}d

t=1, and

d(cid:88)

t=1

  t    (cid:104)vt, x(cid:105)3

t (vt, vt, vt) =   t ,

t     [d] .

58

rank-1 approximation problem

(cid:88)

claim: local maximizers of the function

x (cid:55)    t (x, x, x) =

ti,j,k    xixjxk =

i,j,k

(over the unit ball) are {vt}d

t=1, and

d(cid:88)

t=1

  t    (cid:104)vt, x(cid:105)3

t (vt, vt, vt) =   t ,

t     [d] .

algorithm: use gradient ascent to    nd each component vt.

58

rank-1 approximation problem

(cid:88)

claim: local maximizers of the function

x (cid:55)    t (x, x, x) =

ti,j,k    xixjxk =

i,j,k

(over the unit ball) are {vt}d

t=1, and

d(cid:88)

t=1

  t    (cid:104)vt, x(cid:105)3

t (vt, vt, vt) =   t ,

t     [d] .

algorithm: use gradient ascent to    nd each component vt.
next:    parameter-free       xed-point algorithm.

58

fixed-point algorithm
(de lathauwer, de moore, & vandewalle, 2000)

first-order (necessary but not su   cient) optimality condition:

   xt (x, x, x) =   x .

59

fixed-point algorithm
(de lathauwer, de moore, & vandewalle, 2000)

first-order (necessary but not su   cient) optimality condition:

   xt (x, x, x) =   x .

gradient is    partial evaluation    of t :

   xt (x, x, x) = 3

(cid:88)

i,j

ti,j,k    xixjek = 3t (x, x,  ) .

59

fixed-point algorithm
(de lathauwer, de moore, & vandewalle, 2000)

first-order (necessary but not su   cient) optimality condition:

   xt (x, x, x) =   x .

gradient is    partial evaluation    of t :

(cid:88)

   xt (x, x, x) = 3

ti,j,k    xixjek = 3t (x, x,  ) .

i,j

(third-order) tensor power iteration:

for i = 1, 2, . . . : x(i+1)

:=

t (x(i), x(i),  )
(cid:107)t (x(i), x(i),  )(cid:107) .

59

comparison to matrix power iteration

matrix power iteration x(i+1) = m x(i)

(cid:107)m x(i)(cid:107) for m =(cid:80)

t   tvtv(cid:62)
t .

60

comparison to matrix power iteration

matrix power iteration x(i+1) = m x(i)

(cid:73) requires gap mini(cid:54)=1 1       i/  1 > 0 to converge to v1.

(cid:107)m x(i)(cid:107) for m =(cid:80)

t   tvtv(cid:62)
t .

60

comparison to matrix power iteration

matrix power iteration x(i+1) = m x(i)

(cid:73) requires gap mini(cid:54)=1 1       i/  1 > 0 to converge to v1.

(cid:107)m x(i)(cid:107) for m =(cid:80)

t   tvtv(cid:62)
t .

tensor power iteration:
no gap required.

60

comparison to matrix power iteration

matrix power iteration x(i+1) = m x(i)

(cid:73) requires gap mini(cid:54)=1 1       i/  1 > 0 to converge to v1.

(cid:107)m x(i)(cid:107) for m =(cid:80)

t   tvtv(cid:62)
t .

tensor power iteration:
no gap required.

(cid:73) if (cid:104)v1, x(0)(cid:105) (cid:54)= 0 (and gap > 0), converges to v1.

60

comparison to matrix power iteration

matrix power iteration x(i+1) = m x(i)

(cid:73) requires gap mini(cid:54)=1 1       i/  1 > 0 to converge to v1.

(cid:107)m x(i)(cid:107) for m =(cid:80)

t   tvtv(cid:62)
t .

tensor power iteration:
no gap required.

(cid:73) if (cid:104)v1, x(0)(cid:105) (cid:54)= 0 (and gap > 0), converges to v1.
tensor power iteration:
if t := arg maxt(cid:48)   t(cid:48)|(cid:104)vt(cid:48), x(0)(cid:105)|, converges to vt.

60

comparison to matrix power iteration

matrix power iteration x(i+1) = m x(i)

(cid:73) requires gap mini(cid:54)=1 1       i/  1 > 0 to converge to v1.

(cid:107)m x(i)(cid:107) for m =(cid:80)

t   tvtv(cid:62)
t .

tensor power iteration:
no gap required.

(cid:73) if (cid:104)v1, x(0)(cid:105) (cid:54)= 0 (and gap > 0), converges to v1.
tensor power iteration:
if t := arg maxt(cid:48)   t(cid:48)|(cid:104)vt(cid:48), x(0)(cid:105)|, converges to vt.

(cid:73) converges at linear rate.

60

comparison to matrix power iteration

matrix power iteration x(i+1) = m x(i)

(cid:73) requires gap mini(cid:54)=1 1       i/  1 > 0 to converge to v1.

(cid:107)m x(i)(cid:107) for m =(cid:80)

t   tvtv(cid:62)
t .

tensor power iteration:
no gap required.

(cid:73) if (cid:104)v1, x(0)(cid:105) (cid:54)= 0 (and gap > 0), converges to v1.
tensor power iteration:
if t := arg maxt(cid:48)   t(cid:48)|(cid:104)vt(cid:48), x(0)(cid:105)|, converges to vt.

(cid:73) converges at linear rate.
tensor power iteration:
converges at quadratic rate.

60

nearly orthogonally decomposable tensor
(mu, h., & goldfarb, 2015)

now allow    = (cid:107)e(cid:107) > 0, for e := (cid:98)t     t .

claim: let   v := arg max
x   sd   1

then

|           t|        ,

(cid:98)t (x, x, x) and      := (cid:98)t (  v,   v,   v).
(cid:19)2(cid:33)

(cid:32)

(cid:18)   

(cid:107)  v     vt(cid:107)     o

+

  
  t

for some t     [d] with   t     maxt(cid:48)   t(cid:48)     2  .

  t

61

nearly orthogonally decomposable tensor
(mu, h., & goldfarb, 2015)

now allow    = (cid:107)e(cid:107) > 0, for e := (cid:98)t     t .

claim: let   v := arg max
x   sd   1

then

|           t|        ,

(cid:98)t (x, x, x) and      := (cid:98)t (  v,   v,   v).
(cid:19)2(cid:33)

(cid:32)

(cid:18)   

(cid:107)  v     vt(cid:107)     o

+

  
  t

  t

for some t     [d] with   t     maxt(cid:48)   t(cid:48)     2  .

many e   cient algorithms for solving this approximately, when    is
small enough, like 1/d or 1/
d (e.g., anandkumar, ge, h., kakade, &
telgarsky, 2014; ma, shi, & steurer, 2016).

   

61

errors from de   ation

(for simplicity, assume   t = 1 for all t, so t =(cid:80)

t v   3
to (cid:98)t satis   es (cid:107)  v1     v1(cid:107)        (say).

t

.)

first greedy step:
rank-1 approx.   v   3

1

62

errors from de   ation

(for simplicity, assume   t = 1 for all t, so t =(cid:80)

t v   3
to (cid:98)t satis   es (cid:107)  v1     v1(cid:107)        (say).

t

.)

first greedy step:
rank-1 approx.   v   3
de   ation: to    nd next vt, use

1

(cid:98)t       v   3

1 = t + e       v   3

1

=

v   3
t + e +

d(cid:88)

t=2

(cid:16)

(cid:17)

.

v   3
1       v   3

1

62

errors from de   ation

(for simplicity, assume   t = 1 for all t, so t =(cid:80)

t v   3
to (cid:98)t satis   es (cid:107)  v1     v1(cid:107)        (say).

t

.)

first greedy step:
rank-1 approx.   v   3
de   ation: to    nd next vt, use

1

(cid:98)t       v   3

1 = t + e       v   3

1

=

v   3
t + e +

d(cid:88)

(cid:16)

(cid:17)

.

v   3
1       v   3

1

now error seems to have doubled (i.e., of size 2  ) . . .

t=2

62

e   ect of de   ation errors

for any unit vector x orthogonal to v1:

(cid:17)

(cid:111)(cid:13)(cid:13)(cid:13) =

(cid:13)(cid:13)(cid:13)(cid:104)v1, x(cid:105)2v1     (cid:104)  v1, x(cid:105)2  v1

   x

v   3
1      v   3

1

(x, x, x)

(cid:110)(cid:16)

(cid:13)(cid:13)(cid:13) 1

3

(cid:13)(cid:13)(cid:13)

63

e   ect of de   ation errors

for any unit vector x orthogonal to v1:

(cid:110)(cid:16)

(cid:13)(cid:13)(cid:13) 1

3

   x

v   3
1      v   3

1

(x, x, x)

(cid:17)

(cid:111)(cid:13)(cid:13)(cid:13) =

(cid:13)(cid:13)(cid:13)(cid:104)v1, x(cid:105)2v1     (cid:104)  v1, x(cid:105)2  v1

= (cid:104)  v1, x(cid:105)2

(cid:13)(cid:13)(cid:13)

63

e   ect of de   ation errors

for any unit vector x orthogonal to v1:

(cid:110)(cid:16)

(cid:13)(cid:13)(cid:13) 1

3

   x

v   3
1      v   3

1

(x, x, x)

(cid:17)

(cid:111)(cid:13)(cid:13)(cid:13) =

(cid:13)(cid:13)(cid:13)(cid:104)v1, x(cid:105)2v1     (cid:104)  v1, x(cid:105)2  v1

= (cid:104)  v1, x(cid:105)2
    (cid:107)v1       v1(cid:107)2       2 .

(cid:13)(cid:13)(cid:13)

63

e   ect of de   ation errors

for any unit vector x orthogonal to v1:

(cid:17)

(cid:111)(cid:13)(cid:13)(cid:13) =

(cid:13)(cid:13)(cid:13)(cid:104)v1, x(cid:105)2v1     (cid:104)  v1, x(cid:105)2  v1

   x

v   3
1      v   3

1

(x, x, x)

(cid:110)(cid:16)

(cid:13)(cid:13)(cid:13) 1

3

= (cid:104)  v1, x(cid:105)2
    (cid:107)v1       v1(cid:107)2       2 .
v   3
1       v   3
so e   ect of errors (original and from de   ation) e +
in directions orthogonal to v1 is (1 + o(1))   rather than 2  .

(cid:16)

1

(cid:13)(cid:13)(cid:13)
(cid:17)

63

e   ect of de   ation errors

for any unit vector x orthogonal to v1:

(cid:17)

(cid:111)(cid:13)(cid:13)(cid:13) =

(cid:13)(cid:13)(cid:13)(cid:104)v1, x(cid:105)2v1     (cid:104)  v1, x(cid:105)2  v1

   x

v   3
1      v   3

1

(x, x, x)

(cid:110)(cid:16)

(cid:13)(cid:13)(cid:13) 1

3

= (cid:104)  v1, x(cid:105)2
    (cid:107)v1       v1(cid:107)2       2 .
v   3
1       v   3
so e   ect of errors (original and from de   ation) e +
in directions orthogonal to v1 is (1 + o(1))   rather than 2  .

(cid:16)

1

(cid:13)(cid:13)(cid:13)
(cid:17)

(cid:73) de   ation errors have lower-order e   ect on    nding other vt.

(analogous statement for de   ation with matrices does not hold.)

63

recap

(cid:73) reduction to (nearly) orthogonally decomposable tensor

permits simple and error-tolerant algorithms.
lots of on-going work on non-orthogonal / over-complete
tensor decompositions (e.g., goyal, vempala, & xiao, 2014; ge &
ma, 2015; barak, kelner, & steurer, 2015; ma, shi, & steurer, 2016).
(cid:73) many similarities to matrix decompositions and algorithms,

but di   erences due to non-linearity are crucial.

64

summary

(cid:73) using method-of-moments with o(1)-order moments, can

e   ciently estimate parameters for many latent variable models.

(cid:73) exploit distributional properties, multi-view structure, and

other structure to determine usable moments tensors.
(cid:73) some e   cient algorithms for carrying out the tensor

decomposition to obtain parameter estimates.

65

summary

(cid:73) using method-of-moments with o(1)-order moments, can

e   ciently estimate parameters for many latent variable models.

(cid:73) exploit distributional properties, multi-view structure, and

other structure to determine usable moments tensors.
(cid:73) some e   cient algorithms for carrying out the tensor

decomposition to obtain parameter estimates.

(cid:73) many issues to resolve!

(cid:73) handle model misspeci   cation, increase robustness.
(cid:73) general methodology.
(cid:73) incorporate general prior knowledge.
(cid:73) incorporate user feedback interactively.

65

acknowledgements

collaborators: anima anandkumar (uci/amazon), dean foster (amazon),
rong ge (duke), don goldfarb (columbia), sham kakade (uw),
percy liang (stanford), yi-kai liu (nist), cun mu (columbia),
matus telgarsky (uiuc), tong zhang (tencent)

funding: nsf (dmr-1534910, iis-1563785), sloan foundation

further reading:
(cid:73) anandkumar, ge, h., kakade, & telgarsky.

tensor decompositions for learning latent variable models.
journal of machine learning research, 15(aug):2773   2831, 2014.
https://goo.gl/f8hudn

(cid:73) moitra. algorithmic aspects of machine learning. 2014.

http://people.csail.mit.edu/moitra/docs/bookex.pdf (chapter 3)

thank you

66

