deep learning for robotics

sergey levine

deep learning: end-to-end vision

standard
computer
vision

deep
learning

features
(e.g. hog)

mid-level features
(e.g. dpm)
felzenszwalb    08

classifier
(e.g. id166)

krizhevsky    12

perception

action

(run away)

action

sensorimotor loop

action

(run away)

   when a man throws a ball high in the
air and catches it again, he behaves as if
he had solved a set of differential
equations in predicting the trajectory of
the ball     at some subconscious level,
something functionally equivalent to the
mathematical calculations is going on.   

-- richard dawkins

mcleod & dienes. do fielders know where to go to catch 
the ball or only how to get there? journal of experimental 
psychology 1996, vol. 22, no. 3, 531-543

philipp krahenbuhl, stanford university

end-to-end vision

features
(e.g. hog)

mid-level features
(e.g. dpm)
felzenszwalb    08

classifier
(e.g. id166)

standard
computer
vision

deep
learning

end-to-end robotic control

krizhevsky    12

standard
robotic
control

deep
sensorimotor
learning

observations

state 

estimation
(e.g. vision)

modeling & 
prediction

motion 
planning

low-level 
controller 
(e.g. pd)

motor 
torques

observations

motor 
torques

indirect supervision 
actions have consequences

why should we care?

action

why should we care?

goals of this lecture

   

introduce formalisms of decision making
    states, actions, time, cost
    id100

    survey recent research

    imitation learning
    id23

    outstanding research challenges

    what is easy
    what is hard
    where could we go next?

contents

imitation learning

imitation without a human

id23

research frontiers

terminology & notation

1. run away
2. ignore
3. pet

terminology & notation

1. run away
2. ignore
3. pet

contents

imitation learning

imitation without a human

id23

research frontiers

imitation learning

training

data

supervised

learning

images: bojarski et al.    16, nvidia

does it work?

no!

does it work?

yes!

video: bojarski et al.    16, nvidia

why did that work?

bojarski et al.    16, nvidia

can we make it work more often?

t
s
o
c

stability

learning from a stabilizing 
controller

(more on this later)

can we make it work more often?

can we make it work more often?

dagger: dataset aggregation

ross et al.    11

dagger example

ross et al.    11

what   s the problem?

ross et al.    11

imitation learning: recap

training

data

supervised

learning

    usually (but not always) insufficient by itself

    distribution mismatch problem

    sometimes works well

    hacks (e.g. left/right images)
    samples from a stable trajectory distribution
    add more on-policy data, e.g. using dagger

imitation learning: questions

training

data

supervised

learning

    distribution mismatch does not seem to be 

the whole story
    imitation often works without dagger
    can we think about how stability factors in?

    do we need to add data for imitation to work?

    can we just use existing data more cleverly?

contents

imitation learning

imitation without a human

id23

research frontiers

terminology & notation

1. run away
2. ignore
3. pet

trajectory optimization

probabilistic version

probabilistic version (in pictures)

dagger without humans

ross et al.    11

another problem

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

path replanned!

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

plato: policy learning with 
adaptive trajectory optimization

avoids high cost!

kahn, zhang, levine, abbeel    16

input substitution trick
need state at training time
but not at test time!

plato: policy learning with 
adaptive trajectory optimization

kahn, zhang, levine, abbeel    16

beyond driving & flying

trajectory optimization with unknown dynamics

[l. et al. nips    14]

trajectory optimization with unknown dynamics

new old

[l. et al. nips    14]

learning on pr2

[l. et al. icra    15]

combining with policy learning

expectation under

current policy

trajectory distribution(s)

l. et al. icml    14 (dual descent)

can also use badmm (l. et al.   15)

lagrange multiplier

guided policy search

trajectory-centric rl

supervised learning

[see l. et al. nips    14 for details]

training time

test time

l.*, finn*, darrell, abbeel    16

~ 92,000 
parameters

experimental tasks

imitating optimal control: questions

    any difference from standard imitation 

learning?
    can change behavior of the    teacher    

programmatically

    can the policy help optimal control (rather 

than just the other way around?)

contents

imitation learning

imitation without a human

id23

research frontiers

can we avoid dynamics completely?

a simple id23 
algorithm

reinforce
likelihood ratio policy gradient

williams    92

what the heck did we just do?

one more piece   

policy gradient challenges

    high variance in gradient 

estimate
    smarter baselines
    poor conditioning

    use higher order methods (see: 

natural gradient)

    very hard to choose step size
    trust region policy optimization 

(trpo)

schulman, l., moritz, jordan, abbeel    15

value functions

value functions

value functions challenges

high bias?

high variance?

    the usual problems with policy gradient

    poor conditioning (use natural gradient)
    hard to choose step size (use trpo)

    bias/variance tradeoff

    combine monte carlo and function approximation: generalized advantage estimation (gae)

   

instability/overfitting

    limit how much the value function estimate changes per iteration

schulman, moritz, l. jordan, abbeel    16

generalized advantage estimation

schulman, moritz, l., jordan, abbeel    16

online actor-critic methods

deep deterministic policy gradient (ddpg)

continuous control with deep id23 (lillicrap et al.    15)

just the q function?

discrete id24

human-level control through deep id23 (mnih et al.    15)

continuous id24

normalized advantage functions

*random, ddpg, naf policies: final rewards and 
episodes to converge

policy learning with multiple robots: deep rl with naf

shane gu

ethan holly

tim lillicrap

gu*, holly*, lillicrap, l.,    16

deep rl with policy gradients

    unbiased but high-variance gradient
    stable
    requires many samples
    example: trpo [schulman et al.    15]

deep rl with off-policy q-function critic

    low-variance but biased gradient
    much more efficient (because off-policy)
    much less stable (because biased)
    example: ddpg [lillicrap et al.    16]

improving efficiency & stability with q-prop

    unbiased gradient, stable
    efficient (uses off-policy samples)
    critic comes from off-policy data
    gradient comes from on-policy data
    automatic variance-based 

adjustment

policy gradient:

q-function critic:

q-prop:

shane gu

comparisons

    works with smaller batches than trpo
    more efficient than trpo
    more stable than ddpg with respect to hyperparameters

    likely responsible for the better performance on harder task

sample complexity
    deep id23 is very data-hungry

    id25: about 100 hours to learn breakout
    gae: about 50 hours to learn to walk
    ddpg/naf: 4-5 hours to learn basic manipulation, walking

    model-based methods are more efficient

    time-varying linear models: 3 minutes for real world

manipulation

    gps with vision: 30-40 minutes for real world visuomotor

policies

id23 tradeoffs

    id23 (for the purpose of 

this slide) = model-free rl

    fewer assumptions

    don   t need to model dynamics
    don   t (in general) need state definition, only 

observations

    fully general stochastic environments

    much slower (model-based acceleration?)
    hard to stabilize

    few convergence results with neural networks

contents

imitation learning

imitation without a human

id23

research frontiers

ingredients for success in learning:

supervised learning:

learning sensorimotor skills:

computation
algorithms

data

computation
algorithms

~ data
?

l., pastor, krizhevsky, quillen    16

grasping with learned hand-eye coordination

    800,000 grasp 

attempts for training 
(3,000 robot-hours)
    monocular camera 

(no depth)

    2-5 hz update
    no prior knowledge

monocular
rgb camera

7 dof arm

2-finger
gripper

object
bin

l., pastor, krizhevsky, quillen    16

using grasp success prediction

training

testing

l., pastor, krizhevsky, quillen    16

open-loop vs. closed-loop grasping

open-loop grasping

closed-loop grasping

failure rate: 33.7%

depth + segmentation
failure rate: 35%

failure rate: 17.5%

pinto & gupta, 2015

l., pastor, krizhevsky, quillen    16

grasping experiments

l., pastor, krizhevsky, quillen    16

learning what success means

can we learn the cost
with visual features?

learning what success means

with c. finn, p. abbeel

learning what success means

challenges & frontiers

    algorithms

    sample complexity
    safety
    scalability
    supervision

    automatically evaluate success
    learn cost functions

acknowledgements

greg kahn

tianhao zhang

chelsea finn

trevor darrell

pieter abbeel

john schulman

philipp moritz michael jordan

shane gu

peter pastor

alex krizhevsky

deirdre quillen

