deep learning for web search and 

natural language processing

jianfeng gao

deep learning technology center (dltc)

microsoft research, redmond, usa

wsdm 2015, shanghai, china

*thank li deng and xiaodong he, with whom we participated in the 

previous icassp2014 and cikm2014 versions of this tutorial

mission of machine (deep) learning

data (collected/labeled)

model (architecture)

training (algorithm)

2

outline

    the basics

    background of deep learning
    a query classification problem
    a single neuron model
    a deep neural network (dnn) model
    potentials and problems of dnn
    the breakthrough after 2006

    deep semantic similarity models (dssm) for text processing
    recurrent neural networks

3

4

scientists see promise in deep-learning programs
john markoff november 23, 2012

rick rashid in tianjin, china, october, 25, 2012

geoff hinton

the universal translator on 
   star trek    comes true   

a voice recognition program translated a speech given by richard f. 
rashid, microsoft   s top scientist, into chinese. 

5

6

impact of deep learning in speech technology

cortana

7

8

a query classification problem

    given a search query     , e.g.,    denver sushi downtown   
    identify its domain      e.g.,

    restaurant 
    hotel
    nightlife 
    flight
    etc.

    so that a search engine can tailor the interface and result to provide a 

richer personalized user experience

9

a single neuron model

    for each domain     , build a binary classifier

    input: represent a query      as a vector of features      = [    1,             ]    
    output:      =               
         is labeled      is                > 0.5

    input feature vector, e.g., a bag of words vector

    regards words as atomic symbols: denver, sushi, downtown
    each word is represented as a one-hot vector:  0,     , 0,1,0,     , 0     
    bag of words vector = sum of one-hot vectors
    we may use other features, such as id165s, phrases, (hidden) topics

10

a single neuron model

input features     

     =      =0

                     

output:     (    |    )
     =           =

1

1+exp(       )

        : weight vector to be learned
        : weighted sum of input features
        : the logistic function

    turn a score to a id203
    a sigmoid non-linearlity (activation function), essential 

in multi-layer/deep neural network models

11

model training: how to assign     

    training data: a set of            ,          

    input                       
    output           = {0,1}

    ={1,2,   ,    }

pairs

    goal: learn function     :               to predict correctly on new input     

    step 1: choose a function family, e.g.,

    neural networks, id28, support vector machine, in our case
              =           =0

                      =     (            )

    step 2: optimize parameters      on training data, e.g.,

    minimize a id168 (mean square error loss)
    min

             

     =1

    

    where     (    ) =

1
2

                                

2

12

training the single neuron model,     

    stochastic id119 (sgd) algorithm

    initialize      randomly
    update for each training sample until convergence:                  =                          

        
        

    mean square error loss:      =

1
2

                           2

        
        

=                      

    gradient: 
         =             
    error:      =                   
    derivative of sigmoid        (    ) =           1              

13

sgd vs. id119

    id119 is a batch training algorithm

    update      per batch of training samples
    goes in steepest descent direction

    sgd is noisy descent (but faster per iteration)
    id168 contour plot (duh 2014) 

     1
         =1
2

                           2 +     

14

multi-layer (deep) neural networks

output layer          =     (            2)

vector     

this is exactly the single neuron model
with hidden features.

2st hidden layer     2 =     (    2    1)

projection matrix     2

1st hidden layer     1 =     (    1    )

projection matrix     1

input features     

feature generation: project raw input 
features (bag of words) to hidden
features (topics).

15

standard machine 
learning process

deep learning

adapted from [duh 2014]

16

revisit the activation function:     

    assuming a l-layer neural network

         =                           2         1    

, where      is the output vector

    if      is a linear function, then l-layer neural network is compiled down 

into a single linear transform
        : map scores to probabilities

    useful in prediction as it transforms the neuron weighted sum into the 

interval [0..1] 

    unnecessary for model training except in the boltzman machine or graphical 

models

17

training a two-layer neural net

    training data: a set of            ,          

    ={1,2,   ,    }

pairs

    input                       
    output           = {0,1}

    goal: learn function     :               to predict correctly 

on new input     

              =                             (                          )
    optimize parameters      on training data via

    minimize a id168: min

    

    where     (    ) =

1
2

                                

             

     =1
2

18

training neural nets: back-propagation 

    stochastic id119 (sgd) algorithm

                     =                          

        
        

   

        
        

: sample-wise loss w.r.t. parameters

    need to apply the derivative chain rule correctly

         =          
         =          
        
        

        
        

=

   

        
        

    a detailed discussion in [socher & manning 2013]

19

simple chain rule

[socher & manning 2013]

20

multiple paths chain rule

[socher & manning 2013]

21

chain rule in flow graph

[socher & manning 2013]

22

training neural nets: back-propagation 

assume two outputs (    1,     2) per input     , and 

loss per sample:      =      

1
2

                          

2

forward pass:

         =     (        ),          =                          
        =     (        ),          =                           

derivatives of the weights

    (                         )

        

=

=

=         

                
        
                
         =

        
            
        
            
        
            
         =      

                
    (                          )

            
                
            
                
=                                            
        
            

=               

            
            

    
            

=         

                

=                

=                 

                                =                                   (        )

adapted from [duh 2014]

23

training neural nets: back-propagation 

    all updates involve some scaled error from output    input feature:

   

   

        

                
        
                

=                 where          =                                            

=                  where          =                                   (        )

    first compute          from output layer, then          for other layers and iterate.

        =    1

        =    2

    31

    32

        =   3 =         =    1    31 +         =    2    32        (        =   3)

adapted from (duh 2014)

24

potential of dnn

this is exactly the single neuron model
with hidden features.

project raw input features to hidden
features (high level representation).

[bengio, 2009]

25

dnn is difficult to training

    vanishing gradient problem in id26

   

        
                

=

        
            

            
                

=                 

             =                                   (        )
             may vanish after repeated multiplication

    scalability problem

26

many, but not all, limitations of early dnns have 
been overcome
   better learning algorithms and different nonlinearities. 

   sgd can often allow the training to jump out of local optima due to the noisy 

gradients estimated from a small batch of samples.

   sgd effective for parallelizing over many machines with an asynchronous mode

    vanishing gradient problem?
    try deep belief net (dbn) to initialize it     layer-wise pre-training 

(hinton et al. 2006)
    scalability problem
    computational power due to the use of gpu and large-scale cpu 
clusters

27

dnn: (fully-connected) deep neural  networks
hinton, deng, yu, etc., dnn for am in id103, ieee spm, 2012

geoff hinton

li deng

dong yu

first train a stack of n models each of 
which has one hidden layer. each model in 
the stack treats the hidden variables of the 
previous model as data.

then compose them into 
a single deep belief 
network.

then add outputs 
and train the dnn 
with backprop.

28

cd-dnn-id48 

dahl, yu, deng, and acero,    context-dependent pre-
trained deep neural networks for large vocabulary 
id103,    ieee trans. aslp, jan. 2012

after no improvement for 10+ years by the 
research community   

   msr reduced error from ~23% to <13% 
(and under 7% for rick rashid   s s2s demo)!

29

deep convolutional neural network for images

id98: local connections with weight sharing;

pooling for translation invariance 

image

[lecun et al., 1998]

output

30

a basic module of the id98

pooling

convolution

image

31

deep convolutional nn for images

earlier

id166

pooling

histogram oriented grads

image

2012-2014

fully connected

fully connected

fully connected

convolution/pooling

convolution/pooling

convolution/pooling

convolution/pooling

convolution/pooling

raw image pixels

32

id163 1k competition

krizhevsky, sutskever, hinton,    id163 classification with 
deep convolutional neural networks.    nips, dec. 2012

deep id98
univ. toronto team

33

gartner hyper cycle graph for nn history

[deng and yu 2014]

34

useful sites on deep learning

    http://www.cs.toronto.edu/~hinton/

    http://ufldl.stanford.edu/wiki/index.php/ufldl_recommended_readings
    http://ufldl.stanford.edu/wiki/index.php/ufldl_tutorial (andrew ng   s group)
    http://deeplearning.net/reading-list/ (bengio   s group)

    http://deeplearning.net/tutorial/
    http://deeplearning.net/deep-learning-research-groups-and-labs/

    google+ deep learning community

35

outline

    the basics
    deep semantic similarity models (dssm) for text processing

    what is dssm
    dssm for web search ranking
    dssm for recommendation
    dssm for automatic image captioning

    recurrent neural networks

36

computing semantic similarity

    fundamental to almost all web search and nlp tasks, e.g.,

    machine translation: similarity between sentences in different languages
    web search: similarity between queries and documents

    problems of the existing approaches

    lexical matching cannot handle language discrepancy.
    unsupervised id27 or topic models are not optimal for the task of 

interest.

37

deep semantic similarity model (dssm) 
[huang et al. 2013; gao et al. 2014a; gao et al. 2014b; shen et al. 2014]

    compute semantic similarity between two text strings x and y

    map x and y to feature vectors in a latent semantic space via deep neural net 
    compute the cosine similarity between the feature vectors
    also called    deep structured similarity model    in huang et al. (2013)

    dssm for nlp tasks

tasks

web search

x

y

search query

web document

automatic highlighting

doc in reading

key phrases to be highlighted

contextual entity search

key phrase and context

entity and its corresponding page

machine translation

sentence in language a

translations in language b

38

from common deep models to dssm

    common deep models
    mainly for classification
    target: one-hot vector
    example of dnn:

dist=xid178

one-hot target

w4

w3

w2

w1

h3
h3

h2

h1

input 1

text string s

39

from dnn to dssm

    dssm

    deep-structured semantic model, or
    deep semantic similarity model
    for ranking (not classification with dnn)
    step 1: target from    one-hot   
to continuous-valued vectors

   vector   -valued    target   

dist   xid178

w4

w3

w2

w1

h3
h3

h2

h1

input 1

text string s

40

from dnn to dssm

    to construct a dssm 

    step 1: target from    one-hot   
to continuous-valued vectors

    step 2: derive the    target    vector

using a deep net

semantic representation   

   vector   -valued    target   

distance(s,t)

w4

w3

w2

w1

h3
h3

h2

h1

w4

w3

w2

w1

h3
h3

h2

h1

input s

text string s

input t1
text string t

41

from dnn to dssm

    to construct a dssm 
    step 1: target from    one-hot   
to a continuous-valued vector

    step 2: derive the    target    vector

using a deep net

    step 3: normalize two    semantic    
vectors & computer their similarity

use semantic similarity to rank
documents/entities
cos(s,t1)
cos(s,t2)
cos(s,t3)
      

distance(s,t1)

       

w4

w3

w2

w1

h3
h3

h2

h1

       

       

w4

h3
h3

h2

h1

w3

w2

w1

input s

text string s

input t1
text string t

42

dssm for web search ranking

    task
    model architecture
    model training
    evaluation
    analysis

[huang et al. 2013; shen et al. 2014]

43

an example of web search

    cold home remedy 
    cold remeedy
    flu treatment
    how to deal with stuffy nose

44

semantic matching between q and d

r&d progress

    fuzzy keyword matching 

    q: cold home remedy
    d: best home remedies for cold and flu

    id147
    q: cold remeedies
    d: best home remedies for cold and flu

    query alteration/expansion

    q: flu treatment
    d: best home remedies for cold and flu

    query/document semantic matching

    q: how to deal with stuffy nose
    d: best home remedies for cold and flu
    q: auto body repair cost calculator software
    d: free online car body shop repair estimates

45

dssm: compute similarity in semantic space

learning: maximize the similarity 
between x (source) and y (target)

    (. )                 

    (. )

46

xtftctvhword sequenceword hashing layerconvolutional layersemantic layerrelevance measured by cosine similaritymax pooling layerw1,w2,  ,wtqf1 , f2 ,  ,  ftq300300128...sim(x, y)w1,w2,  ,wtdf1 , f2 ,  ,  ftd1300300128...xydssm: compute similarity in semantic space

learning: maximize the similarity 
between x (source) and y (target)

representation: use dnn to extract 
abstract semantic representations

    (. )

    (. )

47

xtftctvhword sequenceword hashing layerconvolutional layersemantic layerrelevance measured by cosine similaritymax pooling layerw1,w2,  ,wtqf1 , f2 ,  ,  ftq300300128...sim(x, y)w1,w2,  ,wtdf1 , f2 ,  ,  ftd1300300128...xydssm: compute similarity in semantic space

learning: maximize the similarity 
between x (source) and y (target)

representation: use dnn to extract 
abstract semantic representations

convolutional and max-pooling layer:
identify key words/concepts in x and y

word hashing: use sub-word unit (e.g., 
letter     -gram) as raw input to handle 
very large vocabulary

48

xtftctvhword sequenceword hashing layerconvolutional layersemantic layerrelevance measured by cosine similaritymax pooling layerw1,w2,  ,wtqf1 , f2 ,  ,  ftq300300128...sim(x, y)w1,w2,  ,wtdf1 , f2 ,  ,  ftd1300300128...xyletter-trigram representation

    control the dimensionality of the input space

    e.g.,  cat     #cat#     #-c-a, c-a-t, a-t-#
    only ~50k letter-trigrams in english; no oov issue

    capture sub-word semantics (e.g., prefix & suffix)
    words with small typos have similar raw representations

    collision: different words with same letter-trigram representation?

vocabulary size

# of unique letter-trigrams

# of collisions

40k
500k
5m

10,306
30,621
49,292

2
22
179

collision rate
0.0050%
0.0044%
0.0036%

49

convolutional layer

    extract local features using convolutional layer

    {w1, w2, w3}     topic 1
    {w2, w3, w4}     topic 4

50

u1u2u3u4u5w1w2w3w4w52341##max-pooling layer

    extract local features using convolutional layer

    {w1, w2, w3}     topic 1
    {w2, w3, w4}     topic 4

    generate global features using max-pooling

    key topics of the text     topics 1 and 3
    keywords of the text: w2 and w5

51

u1u2u3u4u5w1w2w3w4w52341##w1w2w3w4w5v2341##max-pooling layer

    extract local features using convolutional layer

    {w1, w2, w3}     topic 1
    {w2, w3, w4}     topic 4

    generate global features using max-pooling

    key topics of the text     topics 1 and 3
    keywords of the text: w2 and w5

as

us

the

comedy

festival

comedy
the

formerly
   
arts
known
festival is a comedy festival held
each year in las vegas nevada from
its 1985 inception to 2008 . it
was held annually at the wheeler
opera house and other venues in
aspen
primary
sponsor of the festival was hbo
with
caesars
palace . the primary venue tbs
geico insurance twix candy bars
and smirnoff vodka hbo exited the
festival business in 2007     52

co-sponsorship

colorado

the

by

.

u1u2u3u4u5w1w2w3w4w52341##w1w2w3w4w5v2341##intent matching via convolutional-pooling

    semantic matching of query and document

auto body repair cost calculator software

264

170

294

209

132

231

224

186

264

170

294

209

132

231

224

186

most active neurons at 
the max-pooling layers of 
the query and document 
nets, respectively

free online car body shop repair estimates

53

more examples

54

learning dssm from labeled x-y pairs

    consider a query      and two docs     + and        

    assume     + is more relevant than         with respect to     

    sim         ,      is the cosine similarity of      and      in semantic space, 

mapped by dssm parameterized by     

55

learning dssm from labeled x-y pairs

    consider a query      and two docs     + and        

    assume     + is more relevant than         with respect to     

    sim         ,      is the cosine similarity of      and      in semantic space, 

mapped by dssm parameterized by     

       = sim         ,     +     sim         ,        

    we want to maximize   

                       ;      = log(1 + exp           )
    optimize      using mini-batch sgd on gpu

20

15

10

5

0

-2

-1

0

1

2

56

mine    labeled    x-y pairs from search logs

how to deal with stuffy nose?

no click

stuffy nose treatment

no click

cold home remedies

http://www.agelessherbs.com/besthome
remediescoldflu.html

[gao, he, nie, 2010] 

57

mine    labeled    x-y pairs from search logs

how to deal with stuffy nose?

stuffy nose treatment

cold home remedies

[gao, he, nie, 2010] 

58

mine    labeled    x-y pairs from search logs

how to deal with stuffy nose?

stuffy nose treatment

cold home remedies

query (q)

title (t)

how to deal with stuffy nose

best home remedies for cold and flu

stuffy nose treatment

cold home remedies

       

go israel

best home remedies for cold and flu

best home remedies for cold and flu

       

forums goisrael community

skate at wholesale at pr

wholesale skates southeastern skate supply

breastfeeding nursing blister baby

clogged milk ducts babycenter

thank you teacher song

immigration canada lacolle

lyrics for teaching educational children s music

cbsa office detailed information

[gao, he, nie, 2010] 

59

learning dssm from labeled x-y pairs

semantic space

y1: free online car body shop repair estimates 

y2: online body fat percentage calculator 

implicit supervised information

y3: body language online courses shop

x: auto body repair cost 

calculator software 

    positive x-y pairs are extracted from search click logs
    negative x-y pairs are randomly sampled
    map x and y into the same semantic space via deep neural net

60

learning dssm from labeled x-y pairs

semantic space

y1: free online car body shop repair estimates 

y2: online body fat percentage calculator 

implicit supervised information

y3: body language online courses shop

x: auto body repair cost 

calculator software 

    positive x-y pairs are extracted from search click logs
    negative x-y pairs are randomly sampled
    map x and y into the same semantic space via deep neural net
    positive y are closer to x than negative y in that space

61

learning dssm on x-y pairs via sgd

initialization:

neural networks are initialized with random weights

semantic vector

        

        +

           

d=300

w4

d=500

d=500

w3

w2

dim = 50k

w1
dim = 5m

s:    hot dog   

d=300

d=500

d=500

d=300

d=500

d=500

dim = 50k

dim = 50k

dim = 5m

t+:    fast food   

dim = 5m

t -:    dog racing   

letter-trigram 
embedding matrix

letter-trigram enco.
matrix (fixed)

bag-of-words vector
input word/phrase

62

learning dssm on x-y pairs via sgd

training (back propagation):
compute cosine similarity between semantic vectors 

compute 
gradients

    

            (                      ,         + )

 
        ={    +,       }             (                      ,             )

        

cos(        ,         +)

cos(        ,            )

semantic vector

letter-trigram 
embedding matrix

letter-trigram enco.
matrix (fixed)

bag-of-words vector
input word/phrase

        

d=300

w4

d=500

d=500

w3

w2

dim = 50k

w1
dim = 5m

s:    hot dog   

        +

           

d=300

d=500

d=500

d=300

d=500

d=500

dim = 50k

dim = 50k

dim = 5m

t+:    fast food   

dim = 5m

t -:    dog racing   

63

learning dssm on x-y pairs via sgd

after training converged:

cosine similarity between 
semantic vectors

similar

apart

semantic vector

letter-trigram 
embedding matrix

letter-trigram enco.
matrix (fixed)

bag-of-words vector
input word/phrase

d=300

w4

d=500

d=500

w3

w2

dim = 50k

w1
dim = 5m

   hot dog   

d=300

d=500

d=500

d=300

d=500

d=500

dim = 50k

dim = 50k

dim = 5m

   fast food   

dim = 5m

   dog racing   

64

evaluation methodology

    measurement: ndcg, t-test
    test set: 

    12,071 english queries sampled from 1-y log
    5-level relevance label for each query-doc pair

    training data for translation models:

    82,834,648 query-title pairs

    baselines

    lexicon matching models: bm25, ulm
    translation models
    topic models
    deep auto-encoder [hinton & salakhutdinov 2010]

65

translation models for web search

    leverage id151 (smt) technologies and 

infrastructures to improve search relevance

    model documents and queries as different languages, cast mapping 

queries to documents as bridging the language gap via translation
    given a q, d can be ranked by how likely it is that q is    translated    

from d,     (q|d)

    word translation model
    phrase translation model

[gao, he, nie, 2010] 

66

generative topic models

q: stuffy nose treatment

d: cold home remedies

q: stuffy nose treatment

topic

d: cold home remedies

    probabilistic latent semantic analysis (plsa) 

         q d =         q                              (    |d,     )
    d is assigned a single most likely topic vector
    q is generated from the topic vectors

    id44 (lda) generalizes plsa

    a posterior distribution over topic vectors is used
    plsa = lda with map id136

67

bilingual topic model for web search

q,         

d ~ dir(    )

    for each topic z:          
    for each q-d pair:      ~ dir(    )
q
    each q is generated by      ~      and      ~         
d
    each w is generated by      ~      and      ~         

[gao, toutanova, yih, 2011]

68

web doc ranking results

37

35

33

31

29

27

32.8

33.5

30.5

30.5

34.4

34.2

34.7

31.6

31.5

31.9

32

37.4

35.6

34.2

bm25

plsa

bltm

word translation

phrase translation

dssm_bow

dssm

model

model

ndcg@1

ndcg@3

69

analysis: dssm for semantic word id91 and analogy

    learn id27 by means of its neighbors (context)

    construct context <-> word training pair for dssm
    similar words with similar context -> higher cosine

    training setting:

    30k vocabulary size
    10m words from wikipedia 
    50-dimentional vector

d=50

d=500

dim = 120k

s:    w(t-2) w(t-1) w(t+1) w(t+2)   

similar

d=50

dim = 30k

t:    w(t)   

[song et al. 2014]

70

plotting 3k words in 2d

71

plotting 3k words in 2d

72

plotting 3k words in 2d

73

dssm: semantic similarity vs. semantic reasoning

semantic id91 examples (how similar words are) 
top 3 neighbors of each word
king
earl (0.77)
person (0.79)
woman
spain (0.94)
france
rome
constantinople (0.81)
summer (0.83)
winter

pope (0.77)
girl (0.77)
italy (0.93)
paris (0.79)
autumn (0.79)

lord (0.74)
man (0.76)
belgium (0.88)
moscow (0.77)
spring (0.74)

semantic reasoning examples (how words relate to one another)
    1:     2 =     3                       =     3         1 +     2
summer : rain = winter :      snow (0.79)
italy : rome = france :     
paris (0.78)
man : eye = car :     
motor (0.64)
man : woman = king :     
mary (0.70)
read : book = listen :     
sequel (0.65)

rainfall (0.73)
constantinople (0.74) egypt (0.73)
brake (0.58)
prince (0.70)
tale (0.63)

overhead (0.58)
queen (0.68)
song (0.60)

wet (0.71)

*note that the dssm used in these examples are trained in an unsupervised manner, as google   s id97.74

dssm: semantic similarity vs. semantic reasoning

semantic id91 examples (how similar words are) 
top 3 neighbors of each word
king
earl (0.77)
person (0.79)
woman
spain (0.94)
france
rome
constantinople (0.81)
summer (0.83)
winter

pope (0.77)
girl (0.77)
italy (0.93)
paris (0.79)
autumn (0.79)

lord (0.74)
man (0.76)
belgium (0.88)
moscow (0.77)
spring (0.74)

semantic reasoning examples (how words relate to one another)
    1:     2 =     3                       =     3         1 +     2
summer : rain = winter :      snow (0.79)
italy : rome = france :     
paris (0.78)
man : eye = car :     
motor (0.64)
man : woman = king :     
mary (0.70)
read : book = listen :     
sequel (0.65)

rainfall (0.73)
constantinople (0.74) egypt (0.73)
brake (0.58)
prince (0.70)
tale (0.63)

overhead (0.58)
queen (0.68)
song (0.60)

wet (0.71)

*note that the dssm used in these examples are trained in an unsupervised manner, as google   s id97.75

summary

    map the queries and documents into the same latent semantic space
    doc ranking score is the cosine distance of q/d vectors in that space
    dssm outperforms all the competing models
    the learning dssm vectors capture semantic similarities and relations 

btw words

76

dssm for recommendation

    two interestingness tasks for recommendation
    modeling interestingness via dssm
    training data acquisition
    evaluation
    summary

[gao et al. 2014b]

77

two tasks of modeling interestingness

    automatic highlighting

    highlight the key phrases which represent the entities (person/loc/org) that 

interest a user when reading a document

    doc semantics influences what is perceived as interesting to the user
    e.g., article about movie     articles about an actor/character

    contextual entity search

    given the highlighted key phrases, recommend new, interesting documents 

by searching the web for supplementary information about the entities
    a key phrase may refer to different entities; need to use the contextual 

information to disambiguate

78

the einstein theory of relativity

79

the einstein theory of relativity

80

the einstein theory of relativity

81

the einstein theory of relativity

entity

82

the einstein theory of relativity

context

entity

83

the einstein theory of relativity

context

entity

84

dssm for modeling interestingness

context

key phrase

entity page 

(reference doc)

tasks

x (source text)

y (target text)

automatic highlighting

doc in reading

key phrases to be highlighted

contextual entity search

key phrase and context

entity and its corresponding (wiki) page

85

dssm for modeling interestingness

context

key phrase

entity page 

(reference doc)

tasks

x (source text)

y (target text)

automatic highlighting

doc in reading

key phrases to be highlighted

contextual entity search

key phrase and context entity and its corresponding (wiki) page

86

learning dssm from labeled x-y pairs

the einstein theory of relativity

ray of light (experiment)

ray of light

ray of light (song)

87

learning dssm from labeled x-y pairs

the einstein theory of relativity

ray of light (experiment)

ray of light

ray of light (song)

88

dssm for recommendation

    two interestingness tasks for recommendation
    modeling interestingness via dssm
    training data acquisition
    evaluation
    summary

89

extract labeled pairs from web browsing logs
automatic highlighting

    when reading a page     , the user clicks a hyperlink     

    

http://runningmoron.blogspot.in/

   

i spent a lot of time finding music that was motivating and 
that i'd also want to listen to through my phone. i could 
find none. none! i wound up downloading three metallica 
songs, a judas priest song and one from bush.

   

    

    (text in     , anchor text of     )

90

extract labeled pairs from web browsing logs
contextual entity search

    when a hyperlink      points to a wikipedia        

http://runningmoron.blogspot.in/

   

i spent a lot of time finding music that was motivating and 
that i'd also want to listen to through my phone. i could 
find none. none! i wound up downloading three metallica 
songs, a judas priest song and one from bush.

   

http://en.wikipedia.org/wiki/bush_(band)

    (anchor text of      & surrounding words, text in        )

91

automatic highlighting: settings

    simulation

    use a set of anchors as candidate key phrases to be highlighted
    gold standard rank of key phrases     determined by # user clicks
    model picks top-     keywords from the candidates
    evaluation metric: ndcg

    data

    18 million occurrences of user clicks from a wiki page to another,  

collected from 1-year web browsing logs

    60/20/20 split for training/validation/evaluation

92

automatic highlighting results: baselines

0.6

0.5

0.4

0.3

0.2

0.1

0

0.253

0.215

0.041

0.062

random

basic feat

ndcg@1

ndcg@5

    random: random baseline
    basic feat: boosted decision tree learner with document features, such as 

anchor position, freq. of anchor, anchor density, etc.

93

automatic highlighting results: semantic features

0.505

0.475

0.554

0.524

0.6

0.5

0.4

0.3

0.2

0.1

0

0.380

0.345

0.253

0.215

0.041

0.062

random

basic feat

+ lda vec

+ wiki cat

+ dssm vec

ndcg@1

ndcg@5

    + lda vec: basic + topic model (lda) vectors [gamon+ 2013]
    + wiki cat: basic + wikipedia categories (do not apply to general documents)
    + dssm vec: basic + dssm vectors

94

contextual entity search: settings

    training/validation data: same as in automatic highlighting
    evaluation data

    sample 10k web documents as the source documents
    use named entities in the doc as query; retain up to 100 returned 

documents as target documents

    manually label whether each target document is a good page 

describing the entity

    870k labeled pairs in total

    evaluation metric: ndcg and auc

95

contextual entity search results: baselines

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.253

0.215

0.041

0.062

bm25

bltm

ndcg@1

auc

    bm25: the classical document model in ir [robertson+ 1994]
    bltm: bilingual topic model [gao+ 2011]

96

contextual entity search results: dssm

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

0.699

0.711

0.253

0.215

0.223

0.259

0.041

0.062

bm25

bltm

dssm-bow

dssm

ndcg@1

auc

    dssm-bow: dssm without convolutional layer and max-pooling structure

    dssm outperforms classic doc model and state-of-the-art topic model

97

summary

    extract labeled pairs from web browsing logs
    dssm outperforms state-of-the-art topic models
    dssm learned semantic features outperform the thousands of 

features coming from the manually assigned semantic labels

98

multi-task dssm for scalable intent modeling

query classification for 
different domains 

p(c|q)

p(c|q)

p(c|q)

compute cosine similarity 
between semantic vectors

cosine(q,d1)

cosine(q,d2)

semantic vector

d=300

d=300

d=300

d=300

d=300

d=300

multi-layer 
non-linear 
projection

word hashing

dim = 50k

dim = 50k

dim = 50k

shared layers 

dim = 5m

q:    hot dog   

dim = 5m

d1:    fast food   

dim = 5m

d2:    dog racing   

deep semantic similarity model (dssm): 
learning semantic similarity between x and y

tasks

web search

ad selection

entity ranking

x

search query

search query

mention (highlighted)

y

web documents

ad keywords

entities

recommendation

doc in reading

interesting things in doc or other docs

machine translation

sentence in language a

translations in language b

nature user interface

command (text/speech)

summarization 

query rewriting

image captioning

   

document

query

text string

   

action

summary

rewrite

images

   

[huang et al. 2013; shen et al. 2014; gao et al. 2014a; gao et al. 2014b]

100

go beyond text
dssm for multi-modal representation 
learning

    recall dssm for text inputs:  s, t1, t2, t3,    

    now: replace text s by image s

    using dnn/id98 features of image

    can rank/generate text   s given image or can rank images given text.

x

softmax layer

fully connected

distance(s,t)

w4

w3

w2

h3
h3

h2

h1

w1

input s

       

w4

w3

w2

h3
h3

h2

h1

w1
input t1

image features s

text: a parrot rides a tricycle

fully connected

convolution/pooling

convolution/pooling

convolution/pooling

convolution/pooling

convolution/pooling

raw image pixels

101

sip: automatic image captioning at a human-level of 
performance 

detector models,
deep neural net 

features,    

computer 

vision 
system

stree

signs
unde

r
stop

t

on

light

red

sign

bus

city

pole

building

traffi

c

a stop sign at an intersection on a city street

language 

model

a red stop sign sitting under a traffic light on a city street
a stop sign at an intersection on a street
a stop sign with two street signs on a pole on a sidewalk
a stop sign at an intersection on a city street
   
a stop sign
a red traffic light

caption 

generation 

system

dssm model

semantic 
ranking 
system

fang, gupta, iandola, srivastava, deng, dollar, 
gao, he, mitchell, platt, zitnick, zweig, 
   automatic image captioning at a human-level of 
performance    to appear

102

103

outline

    the basics
    deep semantic similarity models (dssm) for text processing
    recurrent neural networks (id56)

    id165 language models
    id56 language models
    potentials and difficulties of id56

104

statistical id38

    goal: how to incorporate language structure into a probabilistic 

model

    task: next word prediction

    fill in the blank:    the dog of our neighbor ___   

    starting point: word id165 model
    very simple, yet surprisingly effective
    words are generated from left-to-right
    assumes no other structure than words themselves

105

word-based id165 model

    using chain rule on its history i.e., preceding words

                                                                                                         =                  bos

                     bos ,            
                 bos ,            ,             
       
                             bos ,            ,             ,         ,             ,                                
        eos bos ,            ,             ,         ,             ,                                ,                     

         1    2              =          1          2     1          3     1    2    
=     (    1)      =2            (        |    1                1)

106

word-based id165 model

    how do we get id165 id203 estimates?

    get text and count:           2     1 =             (    1    2)/            (    1)
    smoothing to ensure non-zero probabilities

    problem of using long history 

    rare events: unreliable id203 estimates
    assuming a vocabulary of 20,000 words,  

model
unigram    p(w1)
bigram      p(w2|w1)
trigram      p(w3|w1w2)
fourgram p(w4|w1w2w3)

# parameters

20,000

400m
8 x 1012
1.6 x 1017

from manning and sch  tze 1999: 194

107

word-based id165 model 

    markov independence assumption

    a word depends only on n-1 preceding words, e.g.,

    word-based tri-gram model

         1    2              =          1          2     1          3     2    

=     (    1)      =2            (        |           2           1)

    cannot capture any long-distance dependency

the dog of our neighbor barks

108

recurrent neural network for id38

mt

yt

dog

u

v

w ht

barks

   
   

 

   
   

 

runs

ht-1

m    : input one-hot vector at time step     
h    : encodes the history of all words up to time step     
y    : distribution of output words at time step     

         =              +                1
         =     (        )
         =     (            )

where
          =

1

1+exp(       )

,               =

exp(        )
      exp(        )

    (. ) is called the softmax function

[mikolov et al., 2011]

109

id56 unfolds into a dnn over time

wt+1
yt

   
   

 

u

v

w ht

         =              +                1
         =     (        )
         =     (            )

where
          =

1

1+exp(       )

,               =

exp(        )
      exp(        )

wt-2

wt-1

wt
mt

mt-1

u

w

ht-1

u

w

ht-2

mt-2

ht-3

110

training id56-lm by id26 through time

mt

yt

dt

forward pass:

mt-1

v

   
   

 

u

w

ht

 

 

 

0
0
   
0
1
0
   
0
0

 

 

 

 

u

w

ht-1

              =                      

             =                          (        )

                 1 =            (    )        (           1)

u

w

ht-2

mt-2

ht-3

         =              +                1
         =     (        )
         =     (            )
where
          =

1

1+exp(       )

,               =

exp(        )
      exp(        )

parameter updates in id26:

    
                 =                                                
                 =                                =0
                 =                                =0

                                          
    
                                             1

    

111

pseudo code for bptt

112

potentials and difficulties of id56

    in theory, id56 can    store    in h all 

information about past inputs. 

    but in practice, standard id56 cannot 

capture very long distance dependency

    vanishing gradient problem in 

id26

         may vanish after repeated multiplication 

with        (. )

    solution: long short-term memory (lstm)

yt

   
   

 

u

v

w ht

mt

ht-1

ht

u

v

delayed

   
   

 

113

a long short-term memory cell in lstm-id56

information flow in an lstm unit of the id56, with both diagrammatic and mathematical descriptions. w   s are
weight matrices, not shown but can easily be inferred in the diagram (graves et al., 2013).

114

lstm for machine translation (mt)

       a b c    is source sentence;    w x y z    is target sentence

    treat mt as general sequence-to-sequence transduction

    read source; accumulate hidden state; generate target
    <eos> token stops the recurrent process
    in practice, read source sentence in reverse leads to better mt results

    train on bitext; optimize target likelihood

[sutskever et al. 2014]

115

mission of machine (deep) learning

data (collected/labeled)

model (architecture)

training (algorithm)

116

q&a

    http://research.microsoft.com/en-us/um/people/jfgao/
    jfgao@microsoft.com

    we are hiring!
    http://research.microsoft.com/en-us/groups/dltc/
    http://research.microsoft.com/en-us/projects/dssm/

117

references

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

auli, m., galley, m., quirk, c. and zweig, g., 2013. joint language and translation modeling with recurrent neural networks.  in emnlp.

auli, m., and gao, j., 2014. decoder integration and expected id7 training for recurrent neural network language models. in acl.

bengio, y., 2009. learning deep architectures for ai. foundations and trends in machine learning, vol. 2.

bengio, y., courville, a., and vincent, p. 2013. representation learning: a review and new perspectives. ieee trans. pami, vol. 38, pp. 1798-1828.

bengio, y., ducharme, r., and vincent, p., 2000. a neural probabilistic language model, in nips. 

collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k., and kuksa, p., 2011. natural language processing (almost) from scratch. in jmlr, vol. 12.

dahl, g., yu, d., deng, l., and acero, 2012. a. context-dependent, pre-trained deep neural networks for large vocabulary id103, ieee trans. audio, speech, & language proc., vol. 20 
(1), pp. 30-42.

deerwester, s., dumais, s. t., furnas, g. w., landauer, t., and harshman, r. 1990. indexing by latent semantic analysis. j. american society for information science, 41(6): 391-407

deng, l., seltzer, m., yu, d., acero, a., mohamed, a., and hinton, g., 2010. binary coding of speech spectrograms using a deep auto-encoder, in interspeech.

deng, l. and yu, d. 2014. deeping learning methods and applications. foundations and trends in signal processing 7:3-4.

deng, l., yu, d., and platt, j. 2012. scalable stacking and learning for building deep architectures, proc. icassp.

deoras, a., and sarikaya, r., 2013. deep belief network based semantic taggers for spoken language understanding, in interspeech.

devlin, j., zbib, r., huang, z., lamar, t., schwartz, r., and makhoul, j., 2014. fast and robust neural network joint models for id151, acl.

duh, k. 2014. deep learning for natural language processing and machine translation. tutorial. 2014.

frome, a., corrado, g., shlens, j., bengio, s.,  dean, j.,   ranzato, m., and mikolov, t., 2013. devise: a deep visual-semantic embedding model, proc. nips.

hochreiter, s. and schmidhuber, j. 1997. long short-term memory. neural computation, 9(8): 1735-1780, 1997.

gao, j., he, x., yih, w-t., and deng, l. 2014a. learning continuous phrase representations for translation modeling. in acl.

gao, j., he, x., and nie, j-y. 2010. clickthrough-based translation models for web search: from word models to phrase models. in cikm.

gao, j., pantel, p., gamon, m., he, x., and deng, l. 2014b. modeling interestingness with deep neural networks.  emnlp.

gao, j., toutanova, k., yih., w-t. 2011. clickthrough-based latent semantic models for web search. in sigir.          

    gao, j., yuan, w., li, x., deng, k., and nie, j-y. 2009. smoothing clickthrough data for web search ranking. in sigir.

    gao, j., and he, x. 2013. training mrf-based translation models using gradient ascent. in naacl-hlt.

    graves, a., jaitly, n., and mohamed, a., 2013a. hybrid id103 with deep bidirectional lstm, proc. asru.

118

references

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

he, x. and deng, l., 2013. speech-centric information processing: an optimization-oriented approach, in proceedings of the ieee.

he, x., deng, l., and chou, w., 2008. discriminative learning in sequential pattern recognition, sept. ieee sig. proc. mag. 

hinton, g., deng, l., yu, d., dahl, g., mohamed, a., jaitly, n., senior, a., vanhoucke, v., nguyen, p., sainath, t., and kingsbury, b., 2012. deep neural networks for acoustic modeling in speech 
recognition, ieee signal processing magazine, vol. 29, no. 6, pp. 82-97.

hinton, g., osindero, s., and the, y-w. 2006. a fast learning algorithm for deep belief nets. neural computation, 18: 1527-1554.

hinton, g., and salakhutdinov, r., 2010. discovering binary codes for documents by learning deep generative models. topics in cognitive science.

hu, y., auli, m., gao, q., and gao, j. 2014. minimum translation modeling with recurrent neural networks. in eacl.

huang, e., socher, r., manning, c, and ng, a. 2012. improving word representations via global context and multiple word prototypes, proc. acl.

huang, p., he, x., gao, j., deng, l., acero, a., and heck, l. 2013. learning deep structured semantic models for web search using clickthrough data. in cikm.

hutchinson, b., deng, l., and yu, d., 2012. a deep architecture with bilinear modeling of hidden representations: applications to phonetic recognition, proc. icassp.

hutchinson, b., deng, l., and yu, d., 2013. tensor deep stacking networks, ieee trans. pattern analysis and machine intelligence, vol. 35, pp. 1944 - 1957.

kiros, r., zemel, r., and salakhutdinov, r. 2013. multimodal neural language models, proc. nips deep learning workshop.

krizhevsky, a., sutskever, i, and hinton, g., 2012. id163 classification with deep convolutional neural networks, nips.

le, h-s, oparin, i., allauzen, a., gauvain, j-l., yvon, f., 2013. structured output layer neural network language models for id103, ieee transactions on audio, speech and language 
processing.

lecun, y., bottou, l., bengio, y., and haffner, p. 1998. gradient-based learning applied to document recognition, proceedings of the ieee, vol. 86, pp. 2278-2324.

li, p., hastie, t., and church, k.. 2006. very sparse random projections, in proc. sigkdd.

    manning, c. and schutze, h. 1999. foundations of statistical natural language processing. the mit press. 

    mikolov, t. 2012. statistical language models based on neural networks, ph.d. thesis, brno university of technology.

    mikolov, t., chen, k., corrado, g., and dean, j. 2013. efficient estimation of word representations in vector space, proc. iclr.

    mikolov, t., kombrink,. s., burget, l., cernocky, j.,  khudanpur, s., 2011. extensions of recurrent neural network lm. icassp.

    mikolov, t., yih, w., zweig, g., 2013. linguistic regularities in continuous space word representations. in naacl-hlt.

119

references

    mohamed, a., yu, d., and deng, l. 2010. investigation of full-sequence training of id50 for id103, proc. interspeech.

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

ngiam, j., khosla, a., kim, m., nam, j., lee, h., and ng, a. 2011. multimodal deep learning, proc. icml.

sainath, t., mohamed, a., kingsbury, b., and ramabhadran, b. 2013. convolutional neural networks for lvcsr, proc. icassp.

salakhutdinov r., and hinton, g., 2007 semantic hashing. in proc. sigir workshop information retrieval and applications of id114

sarikaya, r., hinton, g., and ramabhadran, b., 2011. deep belief nets for natural language call-routing, in proceedings of the icassp.

schwenk, h., dchelotte, d., gauvain, j-l., 2006. continuous space language models for id151, in coling-acl

seide, f., li, g., and yu, d. 2011. conversational speech transcription using context-dependent deep neural networks, proc. interspeech

shen, y., he, x., gao, j., deng, l., and mesnil, g. 2014. a convolutional latent semantic model for web search. cikm 2014.

socher, r., huval, b., manning, c., ng, a., 2012. semantic compositionality through recursive matrix-vector spaces. in emnlp.

socher, r., and manning, c. 2013. deep learning for nlp (without magic). tutorial in naacl.

socher, r., lin, c., ng, a., and manning, c. 2011. learning continuous phrase representations and syntactic parsing with recursive  neural networks, proc. icml.

socher, r., perelygin, a., wu, j., chuang, j., manning, c., ng a., and potts. c. 2013. recursive deep models for semantic compositionality over a sentiment treebank, proc. emnlp

song, x. he, x., gao. j., and deng, l. 2014. learning id27 using the dssm. msr tech report.

sutskever, i., vinyals, o., and le, q. 2014. sequence to sequence learning with neural networks. in nips.

xu, p., and sarikaya, r., 2013. convolutional neural network based triangular crf for joint intent detection and slot filling, in ieee asru.

yann, d., tur, g., hakkani-tur, d., heck, l., 2014. zero-shot learning and id91 for semantic utterance classification using deep learning, in iclr.

yih, w., toutanova, k., platt, j., and meek, c. 2011. learning discriminative projections for text similarity measures. in conll.

zeiler, m. and fergus, r. 2013. visualizing and understanding convolutional networks, arxiv:1311.2901, pp. 1-11.

120

