neural networks for information retrieval   

tom kenter   
booking.com

amsterdam, the netherlands

tom.kenter@gmail.com
mostafa dehghani
university of amsterdam

amsterdam, the netherlands

dehghani@uva.nl

alexey borisov

yandex

moscow, russia

alborisov@yandex-team.ru

maarten de rijke

university of amsterdam

amsterdam, the netherlands

derijke@uva.nl

christophe van gysel
university of amsterdam

amsterdam, the netherlands

cvangysel@uva.nl
bhaskar mitra

microsoft, university college london

cambridge, uk

bmitra@microsoft.com

8
1
0
2

 

n
a
j
 

7

 
 
]

r

i
.
s
c
[
 
 

1
v
8
7
1
2
0

.

1
0
8
1
:
v
i
x
r
a

acm reference format:
tom kenter, alexey borisov, christophe van gysel, mostafa dehghani,
maarten de rijke, and bhaskar mitra. 2018. neural networks for information
retrieval. in wsdm 2018: wsdm 2018: the eleventh acm international
conference on web search and data mining , february 5   9, 2018, marina del
rey, ca, usa. acm, new york, ny, usa, 2 pages. https://doi.org/10.1145/
3159652.3162009

abstract
machine learning plays a role in many aspects of modern ir sys-
tems, and deep learning is applied in all of them. the fast pace of
modern-day research has given rise to many approaches to many
ir problems. the amount of information available can be over-
whelming both for junior students and for experienced researchers
looking for new research topics and directions. the aim of this full-
day tutorial is to give a clear overview of current tried-and-trusted
neural methods in ir and how they benefit ir.

motivation
prompted by advances of deep learning in id161, neural
networks (nns) have resurfaced as a popular machine learning
paradigm in many other directions of research, including ir. recent
years have seen nns being applied to all key parts of the typical
modern ir pipeline, such as click models, core ranking algorithms,
dialogue systems, entity retrieval, id13s, language
modeling, id53, and text similarity.

an advantage that sets nns apart from many learning strate-
gies employed earlier, is their ability to work from raw input data.
where designing features used to be a crucial aspect of newly pro-
posed ir approaches, the focus has shifted to designing network
architectures instead. hence, many architectures and paradigms
have been proposed, such as auto-encoders, recursive networks,
recurrent networks, convolutional networks, various embedding
methods, and deep id23. this tutorial aims to
provide an overview of the main network architectures currently
applied in ir and to show how they relate to previous work.

   slides and other materials will be made available online at http://nn4ir.com.
   corresponding author.

permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. copyrights for third-party components of this work must be honored.
for all other uses, contact the owner/author(s).
wsdm 2018, february 5   9, 2018, marina del rey, ca, usa
   2018 copyright held by the owner/author(s).
acm isbn 978-1-4503-5581-0/18/02.
https://doi.org/10.1145/3159652.3162009

we expect the tutorial to be useful both for academic and indus-
trial researchers and practitioners who either want to develop new
neural models, use them in their own research in other areas or
apply the models described here to improve actual ir systems.

brief outline of the topics to be covered
table 1 gives an overview of the time schedule of the tutorial. the
total time is 6 hours, plus breaks. we bring a team of six lecturers,

table 1: time schedule for nn4ir tutorial
morning

afternoon
45 min. recommender systems
45 min. modeling user behavior
45 min. generating responses
45 min.

industry insights

preliminaries
semantic matching
learning to rank
entities

45 min.
45 min.
45 min.
45 min.

all with their specific areas of specialization. each session will have
two expert lecturers (indicated by their initials below) who will
together present the session.

preliminaries [tk, mdr]. the recent surge of interest in deep
learning has given rise to a myriad of model architectures. different
though the inner structures of nns can be, many building blocks
are shared. in this preliminary session, we focus on key concepts, all
of which will be referred to multiple times in subsequent sessions.
in particular we will cover distributed representations/embeddings
[37], fully-connect layers, convolutional layers [27], recurrent net-
works [36] and sequence-to-sequence models [45].

semantic matching [cvg, bm]. the problem of matching based
on textual descriptions arises in many retrieval systems. the tradi-
tional ir approach involves computing lexical term overlap between
query and document [42]. however, a vocabulary gap occurs when
query and documents use different terms to describe the same con-
cepts [31]. semantic matching methods bridge the vocabulary gap
by matching concepts rather than exact word occurrences. we cover
neural network-based methods that learn to provide a semantic
matching signal supervised fashion [21, 33, 38, 39], semi-supervised
[12, 13], and unsupervised [2, 15, 23, 25, 29, 47   49, 53, 58].

learning to rank [ab, md]. capturing the notion of relevance
for ranking needs to account for different aspects of the query, the
document, and their relationship. neural methods for ranking can
use manually crafted query and document features, and combine
them with regards to a ranking objective. moreover, latent repre-
sentations of the query and document can be learned in situ. we
cover scenarios with different levels of supervision   unsupervised
[43, 47, 48], semi/weakly-supervised [12, 13, 46], or fully-supervised
using labeled data [38] or interaction data [21].

entities [cvg, tk]. entities play a central role in modern ir
systems [14]. we cover neural approaches to solving the basic task
of id39 [8, 10, 28], as well learning representa-
tions in an end-to-end neural model for learning a specific task like
entity ranking for expert finding [48], product search [47] or email
attachment retrieval [50]. furthermore, work related to knowledge
graphs will be covered, such as graph embeddings [5, 55, 57].

recommender systems [mdr, bm]. deep learning has also found
its way into recommender systems. we cover learning of item
(products, users) embeddings [4, 17, 51], as well as deep collabo-
rative filtering using different deep learning techniques and archi-
tectures [7, 54]. furthermore, nn-based feature extraction from
content (such as images, music, text) [3, 34, 40], and session-based
recommendations with id56s [11, 20, 41] will be covered.

modeling user behavior [ab, mdr]. modeling user browsing
behavior plays an important role in modern ir systems. accurately
interpreting user clicks is difficult due to various types of bias. many
click models based on probabilistic id114 (pgms) have
been proposed [9]. recently, it was shown that recurrent neural
networks can learn to account for biases in user clicks directly from
click-through, i.e., without the need for a predefined set of rules as
is customary for pgm-based click models [6].

generating responses [tk, md]. recent inventions such as smart
home devices, voice search, and virtual assistants provide new ways
of accessing information. they require a different response format
than the classic ten blue links. examples are conversational and
id71 [32, 52] or machine reading and id53
tasks, where the model either infers the answer from unstructured
text [18, 19, 24, 26, 44, 56] or generates natural language given
structured data, like data from id13s or from external
memories [1, 16, 30, 35].

industry insights [ab, bm]. where the focus of academic pa-
pers can be on a specific subtask, industry approaches have to
ensure that a system works from start to end. as a result, extra
challenges are involved concerning the user experience. for exam-
ple in google   s smartreply system [22] the neural model at the
core of the system is embedded in a much larger framework of non-
neural methods to make sure quality and efficiency requirements
are met.

[3] t. bansal, d. belanger, and m. a. ask the gru: id72 for deep text

[4] o. barkan and n. koenigstein. item2vec: neural item embedding for collabo-

references
[1] s. ahn et al. a neural knowledge language model. arxiv preprint arxiv:1608.00318,
[2] q. ai et al. improving language estimation with the paragraph vector model for

2016.
ad-hoc retrieval. in sigir, 2016.
recommendations. in recsys, 2016.
rative filtering. in mlsp, 2016.
of knowledge bases. in conference on artificial intelligence, 2011.

[5] a. bordes, j. weston, r. collobert, and y. bengio. learning structured embeddings
[6] a. borisov et al. a neural click model for web search. in www, 2016.
[7] h. cheng et al. wide & deep learning for recommender systems. in dlrs, 2016.
[8] j. p. chiu and e. nichols. id39 with bidirectional lstm-
[9] a. chuklin, i. markov, and m. de rijke. click models for web search. morgan &
[10] r. collobert and j. weston. a unified architecture for natural language processing:

id98s. tacl, 2015.
claypool, 2015.
deep neural networks with multitask learning. in icml, 2008.
generate for session-based query suggestion. in cikm, 2017.
[12] m. dehghani, a. severyn, s. rothe, and j. kamps. avoiding your teacher   s
mistakes: training neural networks with controlled weak supervision. arxiv
preprint arxiv:1711.00313, 2017.
[13] m. dehghani et al. neural ranking models with weak supervision. in sigir, 2017.
[14] l. dietz, a. kotov, and e. meij. utilizing knowledge bases in text-centric infor-

[11] m. dehghani, s. rothe, e. alfonseca, and p. fleury. learning to attend, copy, and

[15] d. ganguly et al. id27 based generalized language model for infor-
[16] a. graves, g. wayne, and i. danihelka. id63s. arxiv preprint

mation retrieval. in ictir, 2016.
mation retrieval. in sigir, 2015.
arxiv:1410.5401, 2014.

in kdd, 2015.

[17] m. grbovic et al. e-commerce in your inbox: product recommendations at scale.
[18] k. m. hermann et al. teaching machines to read and comprehend. in nips, 2015.
[19] d. hewlett et al. wikireading: a novel large-scale language understanding

[20] b. hidasi et al. session-based recommendations with recurrent neural networks.

task over wikipedia. in proc. acl, 2016.
in iclr, 2016.
using clickthrough data. in cikm, 2013.
2016.

[26] t. kenter, l. jones, and d. hewlett. byte-level machine reading across morpho-

[21] p.-s. huang et al. learning deep structured semantic models for web search
[22] a. kannan et al. smart reply: automated response suggestion for email. in kdd,
[23] t. kenter and m. de rijke. short text similarity with id27s. in cikm,
2015.
[24] t. kenter and m. de rijke. attentive memory networks: efficient machine
reading for conversational search. in workshop on conversational approaches to
information retrieval (cair   17) at sigir 2017, 2017.
[25] t. kenter, a. borisov, and m. de rijke. siamese cbow: optimizing word embed-
dings for sentence representations. in proc. acl, 2016.
logically varied languages. in proc. aaai, 2018.
convolutional neural networks. in nips, 2012.
architectures for id39. in naacl-hlt, 2016.
arxiv preprint arxiv:1405.4053, 2014.
[30] r. lebret, d. grangier, and m. auli. neural text generation from structured data
with application to the biography domain. arxiv preprint arxiv:1603.07771, 2016.
[31] h. li, j. xu, et al. semantic matching in search. fntir, 2014.
[32] j. li et al. deep id23 for dialogue generation. in emnlp, 2016.
[33] z. lu and h. li. a deep architecture for matching short texts. in nips, 2013.
[34] j. mcauley et al. image-based recommendations on styles and substitutes. in

[29] q. v. le and t. mikolov. distributed representations of sentences and documents.

[28] g. lample, m. ballesteros, s. subramanian, k. kawakami, and c. dyer. neural

[27] a. krizhevsky, i. sutskever, and g. e. hinton. id163 classification with deep

sigir, 2015.
[35] h. mei, m. bansal, and m. r. walter. what to talk about and how? selective gener-
ation using lstms with coarse-to-fine alignment. arxiv preprint arxiv:1509.00838,
2015.
[36] t. mikolov et al. recurrent neural network based language model. in interspeech,
[37] t. mikolov et al. efficient estimation of word representations in vector space.

[38] b. mitra, f. diaz, and n. craswell. learning to match using local and distributed
[39] b. mitra et al. a dual embedding space model for document ranking. arxiv
[40] a. oord, s. dieleman, and b. schrauwen. deep content-based music recommen-

2010.
arxiv preprint arxiv:1301.3781, 2013.
representations of text for web search. in www    17, 2017.
preprint arxiv:1602.01137, 2016.
dation. in nips, 2013.
chical recurrent neural networks. in recsys, 2017.

[41] m. quadrana et al. personalizing session-based recommendations with hierar-
[42] s. e. robertson et al. okapi at trec-3. nist special publication sp, 1995.
[43] r. salakhutdinov and g. hinton. semantic hashing.

[50] c. van gysel et al. reply with: proactive recommendation of email attachments.

[48] c. van gysel, m. de rijke, and m. worring. unsupervised, efficient and semantic

[49] c. van gysel, m. de rijke, and e. kanoulas. neural vector spaces for unsupervised

[46] m. szummer and e. yilmaz. semi-supervised learning to rank with preference

[47] c. van gysel, m. de rijke, and e. kanoulas. learning latent vector spaces for

international journal of
approximate reasoning, 2009.
[44] i. v. serban et al. generating factoid questions with recurrent neural networks:
the 30m factoid question-answer corpus. arxiv preprint arxiv:1603.06807, 2016.
[45] i. sutskever, o. vinyals, and q. v. le. sequence to sequence learning with neural
networks. in nips, 2014.
id173. in cikm, 2011.
product search. in cikm, 2016.
expertise retrieval. in www, 2016.
information retrieval. arxiv preprint arxiv:1708.02702, 2017.
in cikm, 2017.
recommendations. in recsys, 2016.

[51] f. vasile et al. meta-prod2vec     product embeddings using side-information for
[52] o. vinyals and q. le. a neural conversational model. in icml, 2015.
[53] i. vuli   and m.-f. moens. monolingual and cross-lingual information retrieval
models based on (bilingual) id27s. in proc. sigir, 2015.
[54] h. wang et al. collaborative deep learning for recommender systems. in kdd,
2015.
[55] z. wang, j. zhang, j. feng, and z. chen. id13 embedding by
translating on hyperplanes. in proc. aaai, 2014.
toy tasks. in iclr, 2016.
relatedness with rich information. in ijcai, 2015.

[57] y. zhao, l. zhiyuan, and m. sun. representation learning for measuring entity

[56] j. weston et al. towards ai-complete id53: a set of prerequisite

[58] g. zuccon, b. koopman, p. bruza, and l. azzopardi. integrating and evaluating
neural id27s in information retrieval. in adcs, 2015.

