outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

8

common language

q1: what is information retrieval (ir)?

q2: what is a neural network (nn)?

preliminaries

9

common language

preliminaries

q1: what is information retrieval (ir)?
a1: the purpose of information retrieval systems is to help people    nd the right
(most useful) information in the right (most convenient) format at the right time
(when they need it).

q2: what is a neural network (nn)?

9

common language

preliminaries

q1: what is information retrieval (ir)?
a1: the purpose of information retrieval systems is to help people    nd the right
(most useful) information in the right (most convenient) format at the right time
(when they need it).

q2: what is a neural network (nn)?
a2: a function f (x;   ) with (a large number of) parameters     that maps
an input object x (which can be text, image or arbitrary vector of features)
to an output object y (class label, sequence of class labels, text, image).

9

common language

preliminaries

q1: what is information retrieval (ir)?
a1: the purpose of information retrieval systems is to help people    nd the right
(most useful) information in the right (most convenient) format at the right time
(when they need it).

q2: what is a neural network (nn)?
a2: a function f (x;   ) with (a large number of) parameters     that maps
an input object x (which can be text, image or arbitrary vector of features)
to an output object y (class label, sequence of class labels, text, image).

q3: how to obtain the parameters    ?

q4: what kinds of functions f (x;   ) can be used?

9

common language

preliminaries

q1: what is information retrieval (ir)?
a1: the purpose of information retrieval systems is to help people    nd the right
(most useful) information in the right (most convenient) format at the right time
(when they need it).

q2: what is a neural network (nn)?
a2: a function f (x;   ) with (a large number of) parameters     that maps
an input object x (which can be text, image or arbitrary vector of features)
to an output object y (class label, sequence of class labels, text, image).

q3: how to obtain the parameters    ?
a3: we learn/train the parameters     using back-propagation

q4: what kinds of functions f (x;   ) can be used?

9

common language

preliminaries

q1: what is information retrieval (ir)?
a1: the purpose of information retrieval systems is to help people    nd the right
(most useful) information in the right (most convenient) format at the right time
(when they need it).

q2: what is a neural network (nn)?
a2: a function f (x;   ) with (a large number of) parameters     that maps
an input object x (which can be text, image or arbitrary vector of features)
to an output object y (class label, sequence of class labels, text, image).

q3: how to obtain the parameters    ?
a3: we learn/train the parameters     using back-propagation

q4: what kinds of functions f (x;   ) can be used?
a4: there are many di   erent classes of f (x;   ) , which are called architectures.
we will talk about them throughout the tutorial.

9

outline

morning program
preliminaries

feedforward neural network
distributed representations
recurrent neural networks
sequence-to-sequence models
convolutional neural networks

modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

10

multi-layer id88 a.k.a. feedforward neural network

preliminaries

target: y

^
output/prediction: y

y1
^
y1

y2
^
y2

y3
^
y3

cost function
e.g.: 1/2 (y - y)2

^

xi,j

  : activation function
e.g.: sigmoid

1

1 + e-o

output layer

weights

hidden layer

weights

input: x

x1

x2

j

x3

w1,4
x4

xi-1, 1

xi-1, 2

xi-1, 4

xi-1, 3

node j at level i

11

outline

morning program
preliminaries

feedforward neural network
distributed representations
recurrent neural networks
sequence-to-sequence models
convolutional neural networks

modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

12

distributed representations

i represent units, e.g., words, as vectors
i goal: words that are similar, e.g., in terms of

meaning, should get similar embeddings

cosine similarity to determine how similar two vectors
are:

cosine(~v, ~w) =

=

~v>    ~w
k~vk2k ~wk2
p|v|i=1 vi    wi
qp|v|i=1 v2
iqp|w|

i=1 w2
i

preliminaries

newspaper = <0.08, 0.31, 0.41>

magazine = <0.09, 0.35, 0.36>

biking = <0.59, 0.25, 0.01>

13

distributed representations

how do we get these vectors?

preliminaries

i you shall know a word by the company it keeps [firth, 1957]
i the vector of a word should be similar to the vectors of the words surrounding it

 !all   !you   !need  !is   !love

14

embedding methods

target distribution

vocabulary size probabitity distribution

vocabulary size layer

embedding size hidden layer

vocabulary size inputs

allanswer
a mtrak

is

0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

need
1

0 0 0 0 0 0 0 0 0 0

turn this into a id203 distribution

embedding size    vocabulary size

weight matrix

vocabulary size    embedding size

weight matrix

0
1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0
answer
amtrak
need
all

love

is

0 0 0 0 0 0 0 0 0 0

...

...

...

...

preliminaries

zorro
what
you
0 0 0 0 0 0

0

0
1 0 0 0 0 0
zorro
you

what

15

preliminaries

y: id203 distribution
y: id203 distribution
^
logits

cost
?

...

...

...

...

pground truth(word = vocabulary[i]) log ppredictions(word = vocabulary[i])

id203 distributions

softmax = normalize the logits

elogits[i]

elogits[j]

cost = cross id178 loss

p(x) log   p(x)

=

j=1

p|logits|
=  xx
=  xi
=  xi

yi log   yi

16

outline

morning program
preliminaries

feedforward neural network
distributed representations
recurrent neural networks
sequence-to-sequence models
convolutional neural networks

modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

17

recurrent neural networks

i lots of information is sequential and

requires a memory for successful
processing

i for example, alice helped bob is not

the same as bob helped alice; and
i had my car cleaned is not the same
as i had cleaned my car

i sequences of arbitrary lengths

as input and output

preliminaries

i recurrent neural networks (id56s) are
called recurrent because they perform
same task for every element of
sequence, with output dependent on
previous computations

i id56s have memory that captures
information about what has been
computed so far

i id56s can make use of information in
arbitrarily long sequences     in practice
they are limited to looking back only
few steps

image credits: http://karpathy.github.io/assets/id56/diags.jpeg

18

preliminaries

recurrent neural networks

i id56 being unrolled (or unfolded) into

full network

i unrolling: write out network for

complete sequence

i formulas governing computation:

i xt input at time step t
i st hidden state at time step t     memory of the network, calculated based on previous

hidden state and input at the current step: st = f (u xt + w st 1); f usually
nonlinearity, e.g., tanh or relu; s 1 typically initialized to all zeroes
probabilities across vocabulary: ot = softmax(v st)

i ot output at step t. e.g.,, if we want to predict next word in sentence, a vector of

image credits: nature

19

id38 using id56s

i language model allows us to predict

qm

id203 of observing sentence (in a
given dataset) as: p (w1, . . . , wm) =

i=1 p (wi | w1, . . . , wi 1)

i in id56, set ot = xt+1: we want

output at step t to be actual next word

i input x a sequence of words; each xt

is a single word; we represent each
word as a one-hot vector of size
vocabulary size

i initialize parameters u , v , w to small

random values around 0

preliminaries

20

id38 using id56s

i language model allows us to predict

qm

id203 of observing sentence (in a
given dataset) as: p (w1, . . . , wm) =

i=1 p (wi | w1, . . . , wi 1)

i in id56, set ot = xt+1: we want

output at step t to be actual next word

i input x a sequence of words; each xt

is a single word; we represent each
word as a one-hot vector of size
vocabulary size

i initialize parameters u , v , w to small

random values around 0

preliminaries

i cross-id178 loss as id168
i for n training examples (words in
text) and c classes (the size of our
vocabulary), loss with respect to
predictions o and true labels y is:
l(y, o) =   1

npn2n yn log on

i training id56 similar to training a

traditional nn: id26
algorithm, but with small twist

i parameters shared by all time steps,

so gradient at each output depends on
calculations of previous time steps:
id26 through time

20

vanishing and exploding gradients

i for training id56s, calculate gradients for u ,

v , w     ok for v but for w and u . . .

preliminaries

l0

l1

l2

l3

l4

i gradients for w :

@l3
@w

=

@l3
@o3

@o3
@s3

@s3
@w

i more generally: @l@st

=

@o3
@s3

@s3
@sk

@sk
@w

@l3
@o3

3xk=0
@sm 2              @st+1
@sm 1    @sm 1
= @l@sm    @sm
@st )     1
< 1
< 1
< 1

i gradient contributions from far away steps become zero: state at those steps

doesn   t contribute to what you are learning

image credits: http://www.wildml.com/2015/10/
recurrent-neural-networks-tutorial-part-3-id26-through-time-and-vanishing-gradients/

21

long short term memory [hochreiter and schmidhuber, 1997]

lstms designed to combat vanishing gradients through gating mechanism

i how lstm calculates hidden state st

preliminaries

i =  (xtu i + st 1w i)
f =  (xtu f + st 1w f )
o =  (xtu o + st 1w o)
g = tanh(xtu g + st 1w g)
ct = ct 1   f + g   i
st = tanh(ct)   o

(  is elementwise multiplication)
i id56 computes hidden state as

st = tanh(u xt + w st 1)     an lstm
unit does exact same thing

22

long short term memory [hochreiter and schmidhuber, 1997]

lstms designed to combat vanishing gradients through gating mechanism

preliminaries

i how lstm calculates hidden state st

i =  (xtu i + st 1w i)
f =  (xtu f + st 1w f )
o =  (xtu o + st 1w o)
g = tanh(xtu g + st 1w g)
ct = ct 1   f + g   i
st = tanh(ct)   o

(  is elementwise multiplication)
i id56 computes hidden state as

st = tanh(u xt + w st 1)     an lstm
unit does exact same thing

i i, f , o: input, forget and output gates
i gates optionally let information

through: composed out of sigmoid
neural net layer and pointwise
multiplication operation

i g is a candidate hidden state

computed based on current input and
previous hidden state

i ct is internal memory of lstm unit:

combines previous memory ct 1
multiplied by forget gate, and newly
computed hidden state g, multiplied
by input gate

22

long short term memory [hochreiter and schmidhuber, 1997]

lstms designed to combat vanishing gradients through gating mechanism

preliminaries

i how lstm calculates hidden state st

i =  (xtu i + st 1w i)
f =  (xtu f + st 1w f )
o =  (xtu o + st 1w o)
g = tanh(xtu g + st 1w g)
ct = ct 1   f + g   i
st = tanh(ct)   o

i . . .
i compute output hidden state st by

multiplying memory with output gate
i plain id56s a special case of lstms:

i fix input gate to all 1   s
i fix forget gate to all 0   s (always

forget the previous memory)

i fix output gate to all 1   s (expose the

whole memory)

i additional tanh squashes output

(  is elementwise multiplication)
i id56 computes hidden state as

st = tanh(u xt + w st 1)     an lstm
unit does exact same thing

i gating mechanism allows lstms to

model long-term dependencies

i learn parameters for gates, to learn

how memory should behave

23

id149

preliminaries

i gru layer quite similar to that of lstm layer, as are the equations:

z =  (xtu z + st 1w z)
r =  (xtu r + st 1w r)
h = tanh(xtu h + (st 1   r)w h)
st = (1   z)   h + z   st 1
i gru has two gates: reset gate r and update gate z.

i reset gate determines how to combine new input with previous memory; update

gate de   nes how much of the previous memory to keep around

i set reset to all 1   s and update gate to all 0   s to get plain id56 model

i on many tasks, lstms and grus perform similarly

24

outline

morning program
preliminaries

feedforward neural network
distributed representations
recurrent neural networks
sequence-to-sequence models
convolutional neural networks

modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

25

sequence-to-sequence models

preliminaries

increasingly important: not just retrieval but also generation

i snippets, summaries, small screen versions of search results, spoken results,

chatbots, conversational interfaces, . . . , but also query suggestion, query
correction, . . .

basic sequence-to-sequence (id195) model consists of two id56s: an encoder that
processes input and a decoder that generates output:

each box represents cell of id56 (often gru cell or lstm cell). encoder and decoder
can share weights or, as is more common, use a di   erent set of parameters

image credits: https://www.tensorflow.org/tutorials/id195

26

bidirectional id56s

preliminaries

i bidirectional id56s based on idea that

output at time t may depend on
previous and future elements in
sequence

i example: predict missing word in a

sequence

i bidirectional id56s are two id56s

stacked on top of each other

i output is computed based on hidden

state of both id56s

image credits: http://www.wildml.com/2015/09/
recurrent-neural-networks-tutorial-part-1-introduction-to-id56s/

27

attention

preliminaries

i attention mechanisms come from visual analysis     suppose you want to locate a

speci   c object in a large image; like people, focus on speci   c areas

i first applied to text and nlp in 2015
i basic mechanism behind every attention mechanism

1. read operator: read a    patch    from the input
2. glimpse sensor: extract information from    patch   
3. locator: predict the next location of read operator
4. id56: combine the previous and current responses from glimpse sensor

assume we are at time step t. from t   1 we get next location to which we should pay
attention (produced by locator). move sensor there and extract information, which the
id56 combines with previous outputs. after several iterations, we produce    nal
response, e.g., classi   cation or label.

28

attention

preliminaries

hard attention

soft attention

i read operator:    xed size, but there

may be several of them

i glimpse sensor can be any nn
i locator predicts x and y location of

sensor (images) or words
ahead/previous (text)

i read operator: it only has    xed aspect
ratio; it can zoom into the image and
blur if needed

i glimpse sensor can be any nn
i locator predict more than x and y
parameter (like sigma for blur .etc)

not di   erentiable, so reinforcement
learning is used

di   erentiable

29

sequence-to-sequence models

used for a    traditional information retrieval task   

preliminaries

image credits: sordoni et al. [2015a]

30

outline

morning program
preliminaries

feedforward neural network
distributed representations
recurrent neural networks
sequence-to-sequence models
convolutional neural networks

modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

31

convolutional neural networks

preliminaries

major breakthroughs in image classi   cation     at core of many id161s systems
some initial applications of id98s to problems in text and information retrieval
what is a convolution? intuition: sliding window function applied to a matrix
example: convolution with 3     3    lter

multiply values element-wise with original matrix, then sum. slide over whole matrix.
image credits:
http://deeplearning.stanford.edu/wiki/index.php/feature_extraction_using_convolution

32

visual examples of id98s

preliminaries

averaging each pixel with neighboring
values blurs image:

taking di   erence between pixel and its
neighbors detects edges:

image credits: https://docs.gimp.org/en/plug-in-convmatrix.html

33

convolutional neural networks

preliminaries

i use convolutions over input layer to

compute output

i yields local connections: each region of
input connected to a neuron in output
i each layer applies di   erent    lters and

combines results

i pooling (subsampling) layers
i during training, id98 learns values of

   lters

i image classi   cation a id98 may learn to
detect edges from raw pixels in    rst layer
i then use edges to detect simple shapes in

second layer

i then use shapes to detect higher-level
features, such as facial shapes in higher
layers

i last layer is then a classi   er that uses

high-level features

34

id98s in text

basic intution

preliminaries

i instead of image pixels, input to most nlp tasks are sentences or documents

represented as a matrix. each row of matrix corresponds to one token, typically a
word, but could be a character. that is, each row is vector that represents word.

i typically, these vectors are id27s (low-dimensional representations)

like id97 or glove, but they could also be one-hot vectors that index the
word into a vocabulary.

i for a 10 word sentence using a 100-dimensional embedding we would have a

10     100 matrix as our input.

i that   s our    image   
i typically use    lters that slide over full rows of the matrix (words): the    width    of

our    lters is usually the same as the width of the input matrix. the height, or
region size, may vary, but sliding windows over 2-5 words at a time is typical.

35

id98s in text

example architecture (zhang and wallace, 2015; sentence classi   cation)

preliminaries

36

id98s in text

example uses in ir

preliminaries

i msr: how to learn semantically meaningful representations of sentences that can

be used for information retrieval

i recommending potentially interesting documents to users based on what they are

currently reading

i sentence representations are trained based on search engine log data
i gao et al. modeling interestingness with deep neural networks. emnlp 2014;
shen et al. a latent semantic model with convolutional-pooling structure for
information retrieval. cikm 2014.

37

take aways

preliminaries

1. information retrieval (ir) systems help people    nd the right (most useful)

information in the right (most convenient) format at the right time (when they
need it).

2. neural network (nn) is a function f (x;   ) with (a large number of) parameters

    that maps an input object x (which can be text, image or arbitrary vector of
features) to an output object y (class label, sequence of class labels, text, image).

3. there three main architectures (classes of f (x;   ) ): (i) feed-forward nn (ffnn),

(ii) recurrent nn (id56), (iii) convolutional nn (id98).

4. embeddings are vector representations of objects in a high-dimensional space that

are learned during training. in many practical applications, these vectors re   ect
similarities between objects that are important for solving the task.

5. other stu   , such as id195, vanishing and exploding gradients, lstm and gru,

attention mechanism, softmax, cross-id178.

38

