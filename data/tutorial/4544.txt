outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

121

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

overview & basics
pointwise loss
pairwise loss
listwise loss
di   erent levels of supervision
some recent relevant work
toolkits

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

122

learning to rank (l2r)

learning to rank

de   nition
   ... the task to automatically construct a ranking model using training data, such that
the model can sort new objects according to their degrees of relevance, preference, or
importance.    - liu [2009]

l2r models represent a rankable item   e.g., a document   given some context   e.g., a
user-issued query   as a numerical vector ~x2 rn.
the ranking model f : ~x! r is trained to map the vector to a real-valued score such
that relevant items are scored higher.

we discuss supervised (o   ine) l2r models    rst, but brie   y introduce online l2r later.

123

a problem of historical terminology

learning to rank

with the increasing interest within semantic matching models (ltm), the term
learning to rank has become ambiguous.

training data and objectives can be used to optimize models that speci   cally focus on
text matching (see previous section).

in this tutorial, we use learning to rank to refer to signal-agnostic models. that is,
models that learn to generate rankings from arbitrary matching, importance or recency
signals, amongst others.

124

a problem of historical terminology

learning to rank

semantic matching signals as input to a general-purpose ranker. taken from [li and lu, 2016].

how long will this hierarchical view remain valid?

125

three training objectives

learning to rank

liu [2009] categorizes di   erent l2r approaches based on training objectives:

i pointwise approach: relevance label yq,d is a number   derived from binary or
graded human judgments or implicit user feedback (e.g., ctr). typically, a
regression or classi   cation model is trained to predict yq,d given ~xq,d.

i pairwise approach: pairwise preference between documents for a query (di  q dj)

as label. reduces to binary classi   cation to predict more relevant document.

i listwise approach: directly optimize for rank-based metric, such as

ndcg   di cult because these metrics are often not di   erentiable w.r.t. model
parameters.

126

features

learning to rank

traditional l2r models employ hand-crafted features that encode ir insights

they can often be categorized as:

i query-independent or static features (e.g., incoming link count and document

length)

i query-dependent or dynamic features (e.g., bm25)
i query-level features (e.g., query length)

127

a quick refresher - what is the softmax function?

learning to rank

in neural classi   cation models, the softmax function is popularly used to normalize the
neural network output scores across all the classes

p(zi) =

e zi

pz2z e z

(  is a constant)

(1)

128

a quick refresher - what is cross id178?

learning to rank

the cross id178 between two id203 distributions p and q over a discrete set of
events is given by,

ce(p, q) =  xi

pi log(qi)

(2)

if pcorrect = 1 and pi = 0 for
all other values of i then,

ce(p, q) =   log(qcorrect)

(3)

129

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

overview & basics
pointwise loss
pairwise loss
listwise loss
di   erent levels of supervision
some recent relevant work
toolkits

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

130

pointwise objectives

learning to rank

regression-based or classi   cation-based approaches are popular

regression loss

given hq, di predict the value of yq,d
e.g., square loss for binary or categorical labels,

lsquared = kyq,d   f (~xq,d)k2

(4)

where, yq,d is the one-hot representation [fuhr, 1989] or the actual value [cossock and
zhang, 2006] of the label

131

pointwise objectives

regression-based or classi   cation-based approaches are popular

learning to rank

classi   cation loss

given hq, di predict the class yq,d
e.g., cross-id178 with softmax over categorical labels y [li et al., 2008],

lce(q, d, yq,d) =   log   p(yq,d|q, d)    =   log    e   syq,d
py2y e   sy   

where, syq,d is the model   s score for label yq,d

(5)

132

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

overview & basics
pointwise loss
pairwise loss
listwise loss
di   erent levels of supervision
some recent relevant work
toolkits

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

133

pairwise objectives

pairwise loss minimizes the average
number of inversions in ranking   i.e.,
di  q dj but dj is ranked higher than di
given hq, di, dji, predict the more
relevant document

for hq, dii and hq, dji,
feature vectors: ~xi and ~xj
model scores: si = f (~xi) and sj = f (~xj)

learning to rank

pairwise loss generally has the followingform
[chen et al., 2009],

lpairwise =  (si   sj)

(6)

where,   can be,

i hinge function  (z) = max(0, 1   z)
i exponential function  (z) = e z [freund

[herbrich et al., 2000]

et al., 2003]

i logistic function  (z) = log(1 + e z)

[burges et al., 2005]

i etc.

134

ranknet

learning to rank

ranknet [burges et al., 2005] is a pairwise id168   popular choice for training
neural l2r models and also an industry favourite [burges, 2015]

predicted probabilities: pij = p(si > sj)     e   si

e   si +e   sj =

1

1+e  (si sj )

and pji    

1

1+e  (sj si)

desired probabilities:   pij = 1 and   pji = 0

computing cross-id178 between   p and p,

lranknet =    pij log(pij)     pji log(pji)

=   log(pij)
= log(1 + e  (si sj ))

(7)
(8)

(9)

135

cross id178 (ce) with softmax over documents

learning to rank

an alternative id168 assumes a single relevant document d+ and compares it
against the full collection d

id203 of retrieving d+ for q is given by the softmax function,

the cross id178 loss is then given by,

p(d+|q) =

e   s q,d+ 
pd2d e   s(q,d)
lce(q, d+, d) =   log   p(d+|q)   
=   log    e   s q,d+ 
pd2d e   s(q,d)   

(10)

(11)

(12)

136

notes on cross id178 (ce) loss

learning to rank

i if we consider only a pair of relevant and non-relevant documents in the

denominator, ce reduces to ranknet

i computing the denominator is prohibitively expensive   l2r models typically

consider few negative candidates [huang et al., 2013, mitra et al., 2017, shen
et al., 2014]

i large body of work in nlp to deal with similar issue that may be relevant to

future l2r models

i e.g., hierarchical softmax [goodman, 2001, mnih and hinton, 2009, morin and

bengio, 2005], importance sampling [bengio and sen  ecal, 2008, bengio et al., 2003,
jean et al., 2014, jozefowicz et al., 2016], noise contrastive estimation [gutmann
and hyv  arinen, 2010, mnih and teh, 2012, vaswani et al., 2013], negative sampling
[mikolov et al., 2013], and blackout [ji et al., 2015]

137

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

overview & basics
pointwise loss
pairwise loss
listwise loss
di   erent levels of supervision
some recent relevant work
toolkits

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

138

listwise

learning to rank

blue: relevant

gray: non-relevant

ndcg and err higher for left but pairwise
errors less for right

due to strong position-based discounting in ir
measures, errors at higer ranks are much more
problematic than at lower ranks

but listwise metrics are non-continuous and
non-di   erentiable

[burges, 2010]

139

lambdarank

key observations:

learning to rank

i to train a model we dont need the costs themselves, only the gradients (of the

costs w.r.t model scores)

i it is desired that the gradient be bigger for pairs of documents that produces a

bigger impact in ndcg by swapping positions

lambdarank [burges et al., 2006]
multiply actual gradients with the change in ndcg by swapping the rank positions of
the two documents

 lambdarank =  ranknet    | n dcg|

(13)

140

lambdamart

learning to rank

lambdamart combines lambdarank and mart (multiple additive regression
trees).

i mart is a boosted tree model in which the output of the model is a linear

combination of the outputs of a set of regression trees.

i while mart uses gradient boosted id90 for prediction tasks,

lambdamart uses gradient boosted id90 using a cost function derived
from lambdarank for solving a ranking task.

i lambdamart is able to choose splits and leaf values that may decrease the

utility for some queries, as long as the overall utility increases.

i on experimental datasets, lambdamart has shown better results than

lambdarank and the original ranknet.

141

listnet and listid113

learning to rank

according to the luce model [luce, 2005], given four items {d1, d2, d3, d4} the
id203 of observing a particular rank-order, say [d2, d1, d4, d3], is given by:

p(   |s) =

 (s2)

 (s1) +  (s2) +  (s3) +  (s4)   

 (s1)

 (s1) +  (s3) +  (s4)   

 (s4)

 (s3) +  (s4)

where,     is a particular permutation and   is a transformation (e.g., linear,
exponential, or sigmoid) over the score si corresponding to item di

(14)

142

listnet and listid113

learning to rank

listnet [cao et al., 2007]
compute the id203 distribution over all possible permutations based on model
score and ground-truth labels. the loss is then given by the k-l divergence between
these two distributions.

this is computationally very costly, computing permutations of only the top-k items
makes it slightly less prohibitive

listid113 [xia et al., 2008]
compute the id203 of the ideal permutation based on the ground truth. however,
with categorical labels more than one permutation is possible which makes this di cult.

143

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

overview & basics
pointwise loss
pairwise loss
listwise loss
di   erent levels of supervision
some recent relevant work
toolkits

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

144

training under di   erent levels of supervision

learning to rank

data requirements for training an o   -line l2r system
query/document pairs that encode an ideal ranking given a particular query.

ideal ranking? relevance, preference, importance [liu, 2009], novelty & diversity

[clarke et al., 2008].

what about personalization? triples of user, query and document.
related to evaluation. pairs also used to compute popular o   -line evaluation measures.
graded or binary.    documents may be relevant to a di   erent degree    [j  arvelin and

kek  al  ainen, 2000]

absolute or relative? zheng et al. [2007]

145

how to satisfy data-hungry models?

learning to rank

there are di   erent ways to obtain query/document pairs.

most expensive

1. human judgments

2. explicit user feedback

3. implicit user feedback

least expensive

4. pseudo relevance

146

human judgments

learning to rank

human judges determine the relevance of a document for a given query.

how to determine candidate query/document pairs?

i obtaining human judgments is expensive.
i list of queries: sample of incoming tra c or manually curated.
i use an existing rankers to obtain rankings and pool the outputs [sparck jones and

van rijsbergen, 1976].

i trade-o    between number of queries (shallow) and judgments (deep) [yilmaz and

robertson, 2009].

147

explicit user feedback

learning to rank

when presenting results to the user, ask the user to explicitly judge the documents.

unfortunately, users are only rarely willing to give explicit feedback [joachims et al.,
1997].

148

extracting pairs from click-through data (training)

learning to rank

extract implicit judgments from search engine interactions by users.

i assumption: user clicks ) relevance (or, preference).
i virtually unlimited data at very low cost, but interpretation is more di cult.
i presentation bias: users are more likely to click higher-ranked links.
i how to deal with presentation bias? joachims [2003] suggest to interleave

di   erent rankers and record preference.

i chains of queries (i.e., search sessions) can be identi   ed within logs and more
   ne-grained user preference can be extracted [radlinski and joachims, 2005].

149

extracting pairs from click-through data (evaluation)

learning to rank

clicks can also be used to evaluate di   erent rankers.

i radlinski et al. [2008] discuss how absolute metrics (e.g., abandonment rate) do

not reliable re   ect retrieval quality. however, relative metrics gathered using
interleaving methods, do re   ect retrieval quality.

i carterette and jones [2008] propose a method to predict relevance score of

unjudged documents. allows for comparisons across time and datasets.

150

side-track: online ltr

learning to rank

as mentioned earlier, we focus mostly on o   ine ltr. besides an active learning
set-up, where models are re-trained frequently, neural models have not yet conquered
the online paradigm.

see the sigir   16 tutorial of grotov and de rijke [2016] for an overview.

151

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

overview & basics
pointwise loss
pairwise loss
listwise loss
di   erent levels of supervision
some recent relevant work
toolkits

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

152

irgan [wang et al., 2017]

learning to rank

there are two main way of thinking about modeling retrieval:

i the generative retrieval focusing
on predicting relevant documents
given a query

i the discriminative retrieval

focusing on predicting relevancy
given a query-document pair.

main idea: a theoretical minimax game to iteratively optimize both of these models
based on the idea of gan.

153

two cool, new ideas

learning to rank

i learning to rank with query-level semi-supervised autoencoders [xu et al.,

2017]: besides the reconstruction loss, they introduce extra supervision using a
query-level constraint.

i objectives:

i minimizing the distance between its inputs and output (reconstruction loss)
i minimizing di   erences of the query-level retrieval performance between the inputs and

the outputs.

i alternating pointwise and pairwise learning [lei et al., 2017]

i try to get the best of both worlds: alternating between pointwise and pairwise loss

over pairwise examples

i evaluated on personalized item ranking

154

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

overview & basics
pointwise loss
pairwise loss
listwise loss
di   erent levels of supervision
some recent relevant work
toolkits

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

155

toolkits for o   -line learning to rank

learning to rank

ranklib : https://sourceforge.net/p/lemur/wiki/ranklib
shoelace : https://github.com/rjagerman/shoelace [jagerman et al., 2017]

quickrank : http://quickrank.isti.cnr.it [capannini et al., 2016]

rankpy : https://bitbucket.org/tunystom/rankpy

pyltr : https://github.com/jma127/pyltr

jforests : https://github.com/yasserg/jforests [ganjisa   ar et al., 2011]

xgboost : https://github.com/dmlc/xgboost [chen and guestrin, 2016]
id166rank : https://www.cs.cornell.edu/people/tj/id166_light [joachims,

2006]

so   a-ml : https://code.google.com/archive/p/sofia-ml [sculley, 2009]
pyso   a : https://pypi.python.org/pypi/pysofia

156

