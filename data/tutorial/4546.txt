outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses
recommender systems
industry insights
q & a

193

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses

one-shot dialogues
open-ended dialogues (chit-chat)
goal-oriented dialogues
alternatives to id56s
resources

recommender systems
industry insights
q & a

194

tasks

generating responses

i id53
i summarization
i query suggestion
i reading comprehension / wiki reading
i dialogue systems
i goal-oriented
i chit-chat

195

example scenario for machine reading task

generating responses

sandra went to the kitchen. fred went to the kitchen. sandra picked up the milk.
sandra traveled to the o ce. sandra left the milk. sandra went to the bathroom.

i where is the milk now? a: o ce
i where is sandra? a: bathroom
i where was sandra before the o ce? a: kitchen

196

example scenario for machine reading task

generating responses

sandra went to the kitchen. fred went to the kitchen. sandra picked up the milk.
sandra traveled to the o ce. sandra left the milk. sandra went to the bathroom.

i where is the milk now? a: o ce
i where is sandra? a: bathroom
i where was sandra before the o ce? a: kitchen

i   ll be going to los angeles shortly. i want to book a    ight. i am leaving from
amsterdam. i want the return    ight to be early morning. i don   t have any extra
luggage. i wouldn   t mind extra leg room.

i what does the user want? a: book a    ight
i where is the user    ying from? a: amsterdam
i where is the use going to? a: los angeles

197

what is required?

generating responses

i the model needs to remember the context
i it needs to know what to look for in the context
i given an input, the model needs to know where to look in the context
i it needs to know how to reason using this context
i it needs to handle changes in the context

a possible solution:

i hidden states of id56s have memory: run an id56 on the and get its

representation to map question to answers/response.

this will not scale as id56 states don   t have ability to capture long term dependency:
vanishing gradients, limited state size.

198

teaching machine to read and comprehend

generating responses

[hermann et al., 2015]

199

neural networks with memory

i memory networks

i end2end memnns
i key-value memnns
i id63s
i stack/list/queue augmented id56s

generating responses

200

end2end memory networks [sukhbaatar et al., 2015]

generating responses

i learns which parts of the memory are relevant.
i this is achieved by reading using attention.
i performs multiple lookups to re   ne its guess

about memory relevance.

i only needs supervision at the    nal output.

201

end2end memory networks (multiple hops)

generating responses

i share the input and output embeddings or not
i what to store in memories individual words, word windows, full sentences
i how to represent the memories? bag-of-words? id56 reading of words?

characters?

202

attentive memory networks [kenter and de rijke, 2017]

generating responses

i proposed model: an end-to-end trainable memory networks with a hierarchical

input encoder.

i framing the task of conversational search as a general machine reading task.

203

key-value memory networks

generating responses

example:
for a kb triple [subject, relation, object], key could be [subject,relation] and value
could be [object] or vice versa.

[miller et al., 2016]

204

wikireading [hewlett et al., 2016, kenter et al., 2018]

generating responses

task is based on wikipedia data (datasets available in english, turkish and russian).

i categorical: relatively small number of possible answer (e.g.: instance of, gender,

country).

i relational: rare or totally unique answers (e.g.: date of birth, parent,capital).

205

wikireading

generating responses

i answer classi   cation: encoding document and question, using softmax

classi   er to assign id203 to each of to-50k answers (limited answer vocab) .

i sparse bow baseline, averaged embeddings, paragraph vector, lstm reader,

attentive reader, memory network.

i generally models with id56 and attention work better, especially at relational

properties.

i answer extraction (labeling/pointing) for each word in the document, compute

the id203 that it is part of the answer.

i regardless of the vocabulary so the answer requires being mentioned in the

document.

i id56 labeler: shows a complementary set of strengths, performing better on

relational properties than categorical ones

i sequence to sequence encoding query and document and decoding the answer

as sequences of words or characters.

i basic id195, placeholder id195, basic character id195,
i uni   es the classi   cation and extraction in one model: greater degree of balance

between relational and categorical properties.

206

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses

one-shot dialogues
open-ended dialogues (chit-chat)
goal-oriented dialogues
alternatives to id56s
resources

recommender systems
industry insights
q & a

207

dialogue systems

dialogues/conversational agents/id70

generating responses

open-ended dialogues

goal-oriented dialogues

i eliza
i twitterbots
i alexa/google home/siri/cortana

i restaurant    nding
i hotel reservations
i set an alarm clock
i order a pizza
i play music
i alexa/google home/siri/cortana

is this ir?

208

dialogue systems

chit-id70

user

machine

hello

how are

you

i amfine

thanks

straightforward seq-to-seq [vinyals and
le, 2015].
([sordoni et al., 2015b] is a precursor,
but no id56-to-id56, and no lstm).

generating responses

i am fine

thanks

machine

user

hello

how are

you

same idea, but with attention [shang
et al., 2015]

209

dialogue systems

limitations

i    wrong    optimization criterion
i generic responses
i no way to incorporate world knowledge
i no model of conversation

i inconsistency
i no memory of what was said earlier on

evaluation

generating responses

human: what is your job?
machine: i   m a lawyer.
human: what do you do?
machine: i   m a doctor.
example from [vinyals and le,
2015]

i perplexity?
i blue/meteor?
i nice overview of how not to evaluate your dialogue system [liu et al., 2016].
i open problem....

210

dialogue systems

generating responses

3 solutions

i more consistency in dialogue with hierarchical network
i less generic responses with di   erent optimization function
i more natural responses with gans

211

dialogue systems

generating responses

hierarchical seq-to-seq [serban et al., 2016]. main evaluation metric: perplexity.

212

dialogue systems

generating responses

avoid generic responses
usually: optimize log likelhood of predicted utterance, given previous context:

cll = arg max

ut

log p(ut|context) = arg max

ut

log p(ut|u0 . . . ut 1)

to avoid repetitive/boring answer (i don   t know ), use maximum mutual information
between previous context and predicted utterance [li et al., 2015].

cm m i = arg max

ut

log

p(ut, context)
p(ut)p(context)
= [derivation, next page . . . ]
= arg max

(1    )log p(ut|context) +  log p(context|ut)

ut

213

dialogue systems

bayes rule

generating responses

p(context|ut)p(ut)

log p(ut|context) = log
log p(ut|context) = log p(context|ut) + log p(ut)   log p(context)

p(context)

log p(ut) = log p(ut|context)   log p(context|ut) + log p(context)

cmm i = arg max

ut

= arg max

ut

= arg max

ut

= arg max

ut

= arg max

ut

= arg max

ut

log

log

p(ut, context)
p(ut)p(context)
p(ut|context)

p(ut)

= arg max

ut

log

p(ut|context)p(context)

p(ut)p(context)

log p(ut|context)   log p(ut)   weird, minus language model score.
log p(ut|context)    log p(ut)   introduce  . crucial step! without this it wouldn   t work.
log p(ut|context)    (log p(ut|context)   log p(context|ut) + log p(context))
(1    )log p(ut|context) +  log p(context|ut)

(more is needed to get it to work. see [li et al., 2015] for more details.)

214

generative adversarial network for dialogues

generating responses

i discriminator network

i classi   er: real or generated utterance

i generator network

i generate a realistic utterance

p(    /    )

discriminator

original gan paper [goodfellow et al., 2014].
conditional gans, e.g. [isola et al., 2016].

generator

real data

215

generative adversarial network for dialogues

generating responses

i discriminator network

i classi   er: real or generated utterance

i generator network

i generate a realistic utterance

see [li et al., 2017] for more details.

provided

generated

generator

how are
hello
i amfine

you
thanks

i amfine

thanks

discriminator

p(x = real)
p(x = generated)

hello

howare

you

i amfine

thanks

code available at https://github.com/jiweil/neural-dialogue-generation

216

dialogue systems

open-ended dialogue systems

i very cool, current problem
i very hard
i many problems

i training data
i evaluation
i consistency
i persona
i . . .

generating responses

217

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses

one-shot dialogues
open-ended dialogues (chit-chat)
goal-oriented dialogues
alternatives to id56s
resources

recommender systems
industry insights
q & a

218

goal-oriented

idea

i closed domain

i restaurant reservations
i finding movies

i have a dialogue system
   nd out what the user
wants

generating responses

challenges

i training data
i keeping track of dialogue history
i handling of out-of-domain words or

requests

i going beyond task-speci   c slot    lling
i intermingling live api calls, chit chat,

information requests, etc.

i evaluation

i solve the task
i naturalness
i tone of voice
i speed
i error recovery

219

goal-oriented as id195

memory network [bordes and weston, 2017]

generating responses

i simulated dataset
i finite set of things the bot

can say

i because of the way the

dataset is constructed

i memory networks
i training: next utterance

prediction
i evaluation

i response-level
i dialogue-level

restaurant knowledge base, i.e., a table.
queried by api calls.
each row = restaurant:

i cuisine (10 choices, e.g., french, thai)
i location (10 choices, e.g., london, tokyo)
i price range (cheap, moderate or expensive)
i rating (from 1 to 8)

for words of relevant entity types
i add a trainable entity vector

220

goal-oriented as id23

generating responses

a typical id23
system:

i states s
i actions a
i state transition function:

t : s, a ! s

i reward function:
r : s, a, s ! r
i policy:     : s ! a
a rl system needs an
environment to interact with (e.g.,
real users).

typically [shah et al., 2016]:

i states: agents interpretation of the
environment: distribution over user
intents, dialogue acts and slots and their
values

i intent(buy ticket)
i inform(destination=atlanta)
i ...

i actions: possible communications, and are

usually designed as a combination of
dialogue act tags, slots and possibly slot
values

i request(departure date)
i ...

221

goal-oriented as id23

generating responses

restaurant    nding [wen et al., 2017]:

movie    nding [dhingra et al., 2017]:

i neural belief tracking: distribution

over a possible values of a set of
slots

i delexicalisation: swap slot-values

for generic token (e.g. chinese,
indian, italian ! food type )

i simulated user
i soft attention over database
i neural belief tracking:

i multinomial distribution for
every column over possible
column values

i id56, input is dialogue so far,
output softmax over possible
column values

reward based on    nding the right kb
entry.

222

goal-oriented

generating responses

goal-oriented models

i currently works primarily in very small domains
i how about multiple speakers?
i not clear what kind of architecture is best
i id23 might be the way to go (?)
i open research area...

223

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses

one-shot dialogues
open-ended dialogues (chit-chat)
goal-oriented dialogues
alternatives to id56s
resources

recommender systems
industry insights
q & a

224

alternatives to id56s

generating responses

id56s are:

i well-studied
i robust and tried and trusted method for sequence

tasks

however, id56s have several drawbacks:

i take time to train
i expensive to unroll for many steps
i not too good at catching long-term dependencies

can we do better?

i wavenet
i bytenet
i transformer

225

alternatives to id56s: wavenet

generating responses

wavenet is originally introduced for a text-to-speech task (i.e. generating realistic
audio waves).
we try to model:

p(x) =qt

t=1 p(xt|x1, . . . , xt 1).

i stack of convolutional layers. no pooling layers.
i output of the model has the same time dimensionality as the input.
i output is a categorical distribution over the next value xt with a softmax layer and

it is optimized to maximize the log-likelihood of the data w.r.t. the parameters.

based on the idea of dilated causal convolutions.

[van den oord et al., 2016]

226

227generatingresponsesalternativestoid56s:wavenetcausalconvolutionsinputhidden layerhidden layerhidden layeroutput[vandenoordetal.,2016]228generatingresponsesalternativestoid56s:wavenetdilatedcausalconvolutionsinputhidden layerdilation = 1hidden layerdilation = 2hidden layerdilation = 4outputdilation = 8   attrainingtime,theconditionalpredictionsforalltimestepscanbemadeinparallelbecausealltimestepsofgroundtruthxareknown.whengeneratingwiththemodel,thepredictionsarese-quential:aftereachsampleispredicted,itisfedbackintothenetworktopredictthenextsample.   [vandenoordetal.,2016]alternatives to id56s: bytenet

generating responses

t1

t2

t3

t4

t5

t6

t7

t8

t9

t10

t11 t12 t13 t14 t15 t16 t17

decoder

encoder

t0

t1

t2

t3

t4

t5

t6

t7

t8

t9

t10

t11 t12 t13 t14 t15 t16

s0 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10 s11 s12 s13 s14 s15 s16

[kalchbrenner et al., 2016]

229

alternatives to id56s: transformer

generating responses

i positional encoding added to the input embeddings
i key-value attention
i multi-head self-attention
i the encoder attends over its own states
i the decoder alters between

i attending over its own inputs/states
i attending over encoder states at the same level

[vaswani et al., 2017]

230

alternatives to id56s: transformer

generating responses

...

...

layer n

...

layer 2

layer 1

encoder

decoder

231

outline

morning program
preliminaries
modeling user behavior
semantic matching
learning to rank

afternoon program

entities
generating responses

one-shot dialogues
open-ended dialogues (chit-chat)
goal-oriented dialogues
alternatives to id56s
resources

recommender systems
industry insights
q & a

232

resources: datasets

open-ended dialogue
- opensubtitles [tiedemann, 2009]
- twitter: http://research.microsoft.com/convo/
- weibo:
http://www.noahlab.com.hk/topics/shorttextconversation
- ubuntu dialogue corpus [lowe et al.,
2015]
- switchboard
https://web.stanford.edu/~jurafsky/ws97/
- coarse discourse (google research)
https://research.googleblog.com/2017/05/

coarse-discourse-dataset-for.html

generating responses

goal-oriented dialogues
- misc: a data set of information-seeking
conversations [thomas et al., 2017]
- maluuba frames
http://datasets.maluuba.com/frames
- loqui human-human dialogue corpus
https://academiccommons.columbia.edu/catalog/ac:176612
- babi (facebook research)
https://research.fb.com/downloads/babi/

machine reading
- babi qa (facebook research)
https://research.fb.com/downloads/babi/
- qa corpus [hermann et al., 2015]

https://github.com/deepmind/rc-data/

- wikireading (google research)
https://github.com/google-research-datasets/wiki-reading

233

resources: source code

i end-to-end memory network

https://github.com/facebook/memnn

i attentive memory networks

https://bitbucket.org/tomkenter/attentive-memory-networks-code

i hierarchical nn [serban et al., 2016]

https://github.com/julianser/hed-dlg, https://github.com/julianser/id56-lm

i gan for dialogues

https://github.com/jiweil/neural-dialogue-generation

i rl for dialogue agents [dhingra et al., 2017]

https://github.com/miulab/kb-infobot

i transformer network

https://github.com/tensorflow/tensor2tensor

generating responses

234

