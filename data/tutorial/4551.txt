incorporating structural alignment biases into an attentional neural

translation model

trevor cohn and cong duy vu hoang and ekaterina vymolova

university of melbourne
melbourne, vic, australia

tcohn@unimelb.edu.au and {vhoang2,evylomova}@student.unimelb.edu.au

6
1
0
2

 

n
a
j
 

6

 
 
]
l
c
.
s
c
[
 
 

1
v
5
8
0
1
0

.

1
0
6
1
:
v
i
x
r
a

kaisheng yao

microsoft research
redmond, wa, usa

chris dyer

carnegie mellon university

pittsburgh, pa, usa
cdyer@cmu.edu

gholamreza haffari
monash university

clayton, vic, australia

kaisheng.yao@microsoft.com

gholamreza.haffari@monash.edu

abstract

neural encoder-decoder models of machine
translation have achieved impressive results,
rivalling traditional translation models. how-
ever their modelling formulation is overly
simplistic, and omits several key inductive bi-
ases built into traditional models. in this paper
we extend the attentional neural translation
model to include structural biases from word
based alignment models, including positional
bias, markov conditioning, fertility and agree-
ment over translation directions. we show im-
provements over a baseline attentional model
and standard phrase-based model over sev-
eral language pairs, evaluating on dif   cult lan-
guages in a low resource setting.

introduction

1
recently, models of end-to-end machine translation
based on neural network classi   cation have been
shown to produce excellent translations, rivalling or
in some cases surpassing traditional statistical ma-
chine translation systems (kalchbrenner and blun-
som, 2013; sutskever et al., 2014; bahdanau et al.,
2015). this is despite the neural approaches using
an overall simpler model, with fewer assumptions
about the learning and prediction problem.

broadly, neural approaches are based around the
notion of an encoder-decoder (sutskever et al.,
2014), in which the source language is encoded into
a distributed representation, followed by a decoding
step which generates the target translation. we focus

on the attentional model of translation (bahdanau et
al., 2015) which uses a dynamic representation of
the source sentence while allowing the decoder to
attend to different parts of the source as it gener-
ates the target sentence. the attentional model raises
intriguing opportunities, given the correspondence
between the notions of attention and alignment in
traditional word-based machine translation models
(brown et al., 1993).

in this paper we map modelling biases from word
based translation models into the attentional model,
such that known linguistic elements of translation
can be better captured. we incorporate absolute po-
sitional bias whereby word order tends to be simi-
lar between the source sentence and its translation
(e.g., ibm model 2 and (dyer et al., 2013)), fer-
tility whereby each instance of a source word type
tends to be translated into a consistent number of
target tokens (e.g., ibm models 3, 4, 5), relative po-
sition bias whereby prior preferences for monotonic
alignments/attention can be encouraged (e.g., ibm
model 4, 5 and id48-based alignment (vogel et
al., 1996)), and alignment consistency whereby the
attention in both translation directions are encourged
to agree (e.g. symmetrization heuristics (och and
ney, 2003) or joint modelling (liang et al., 2006;
ganchev et al., 2008)).

we provide an empirical analysis of incorporat-
ing the above structural biases into the attentional
model, considering low resource translation sce-
nario over four language-pairs. our results demon-
strate consistent improvements over vanila encoder-

(cid:20) h   

(cid:21)

i

i

h   

then represented as a pair of hidden states, one from
each id56, ei =
. this encodes not only
the word but also its left and right context, which
can provide important evidence for its translation.
a crucial question is how this dynamic sized ma-
trix e = [e1, e2, . . . , ei ]     ri  h can be used in
the decoder to generate the target sentence. as with
sutskever   s encoder-decoder, the target sentence is
created left-to-right using a id56, while the encoded
source is used to bias the process as an auxiliary in-
put. the mechanism for this bias is by attentional
vectors, i.e. vectors of scores over each source sen-
tence location, which are used to aggregate the dy-
namic source encoding into a    xed length vector.
decoder the decoder operates as a standard id56
over the translation t, formulated as follows

gj = tanh

uj = tanh
tj     softmax

w(th)gj   1 + w(ti)r(t)
(cid:16)
gj + w(uc)cj + w(ui)r(t)
tj   1

w(ou)uj + b(to)(cid:17)

tj   1 + w(ta)cj

(2)

(3)

(4)

(cid:17)

(cid:16)
(cid:16)

(cid:17)

where the decoder id56 is de   ned analogously to
eq 1 but with an additional input, the source atten-
tion component cj     r2h and weighting matrix
w(ta)     rh  2h. the hidden state of the recurrence
is then passed through a single hidden layer2 (eq 3)
in combination with the source attention and target
word using weighting matrices w(uc)     rh  2h
and w(ui)     rh  e. in eq 4 this vector is trans-
formed to be target vocabulary sized, using weight
matrix w(ou)     rvt   h and bias b(to)     rvt , af-
ter which a softmax is taken, and the resulting nor-
malised vector used as the parameters of a categor-
ical distribution in generating the next target word.
the presentation above assumes a simple id56
is used to de   ne the recurrence over hidden states,
however we can easily use alternative formula-
tions of recurrent networks including multiple-
layer id56s, id149 (gru; cho et
al. (2014)), or long short-term memory (lstm;
hochreiter and schmidhuber (1997)) units. these
note that we use a long short term memory unit (hochreiter
and schmidhuber, 1997) in place of the id56, shown here for
simplicity of exposition.

2in bahdanau et al. (2015) they use a max-out layer for this
   nal step, however we found this to be a needless complication,
and instead use a standard hidden layer with tanh activation.

figure 1: attentional model of translation (bah-
danau et al., 2015). the encoder is shown below
the decoder, and the edges connecting the two corre-
sponding to the attention mechanism. heavy edges
denote a higher attention weight, and these values
are also displayed in matrix form, with one row for
each target word.

decoder and attentional model in terms of the per-
plexity and id7 score, e.g. up to 3.5 id7 points
when re-ranking the candiate translations generated
by a state-of-the-art phrase based model.

2 the attentional model of translation
we start by reviewing the attentional model of trans-
lation (bahdanau et al., 2015), as illustrated in
fig. 1, before presenting our extensions in   3.
encoder the encoding of the source sentence is
formulated using a pair of id56s (denoted bi-id56)
one operating left-to-right over the input sequence
and another operating right-to-left,

(cid:17)

w    

h   
i = tanh

i   1, r(s)
si )
i+1, r(s)
si )

h   
i = id56(h   
i = id56(h   
h   
i and h   
i are the id56 hidden states. the
(cid:16)

where h   
left-to-right id56 function is de   ned as
sh h   

(1)
0     rh is a learned parameter vector, as
where h   
sh     rh  h
are r(s)     rvs  e, w    
s     rh, with h the number of hidden units,
and b   
vs the size of the source vocabulary and e the word
embedding dimensionality.1 each source word is
sh    
s     rh are the parameters of the right-to-left id56.

si     rh  e, w    

si + w    

    rh  e, w    

1similarly, h   

i   1 + b   

    rh , w    

rh  h , b   

si r(s)

si

s

0

more advanced methods allow for more ef   cient
learning of more complex concepts, particularly
long distance effects. empirically we found lstms
to be the best performing, and therefore use these
units herein.

the last key detail is the attentional component cj

in eqs 2 and 3, which is de   ned as follows

(cid:17)

w(ae)ei + w(ah)gj   1

(5)

(cid:16)

fji = v(cid:62) tanh
  j = softmax (fj)

(cid:88)

cj =

  jiei

i

with the scalars fji denoting the compatibility be-
tween the target hidden state gj   1 and the source en-
coding ei. this is de   ned as a neural network with
one hidden layer of size a and a single output, pa-
rameterised by w(ae)     ra  2h, w(ah)     ra  h
and v     ra. the softmax then normalises the
scalar compatibility values such that for a given tar-
get word j, the values of   j can be interpreted as
alignment probabilities to each source location. fi-
nally, these alignments are used to to reweight the
source components e to produce a    xed length con-
text representation.

training of this model is done by minimising
the cross-id178 of the target sentence, measured
word-by-word as for a language model. we use
standard stochastic gradient optimisation using the
back-propagation technique for computation of par-
tial derivatives according to the chain rule.

3

incorporating structural biases

the attentional model, as described above, provides
a powerful and elegant model of translation in which
alignments between source and target words are
learned through the implicit conditioning context af-
forded by the attention mechanism. despite its ele-
gance, the attentional model omits several key com-
ponents of a traditional alignment models such as
the ibm models (brown et al., 1993) and vogel   s
hidden markov model (vogel et al., 1996) as imple-
mented in the giza++ toolkit (och and ney, 2003).
combining the strengths of this highly successful
body of research into a neural model of machine
translation holds potential to further improve mod-
elling accuracy of neural techniques. below we out-

line methods for incorporating these factors as struc-
tural biases into the attentional model.

3.1 position bias
first we consider position bias, based on the obser-
vation that a word at a given relative position in the
source tends to align to a word at a similiar rela-
tive position in the target, i
j (ibm 2). related,
alignments tend to occur near the diagonal (dyer et
al., 2013), when considering the alignments as a bi-
nary i    j matrix (illustrated in figure 1), where
the cell at (i, j) denotes whether an alignment exists
between source word i and target word j.

i     j

we include a position bias through rede   ning the

pre-normalised attention scalars fji in eq 5 as:

fji = v(cid:62) tanh(cid:0)w(ae)ei + w(ah)gj   1+

w(ap)  (j, i, i)(cid:1)

where the new component in the input is a simple
feature function of the positions in the source and
target sentences and the source length,

(6)

(cid:21)(cid:62)

(cid:20)

  (j, i, i) =

log(1 + j), log(1 + i), log(1 + i)

and w(ap)     ra  3. we exclude the target length
j as this is unknown during decoding, as a par-
tial translation can have several (in   nite) different
lengths. the use of the log(1+  ) function is to avoid
numerical instabilities from widely varying sentence
lengths. the non-linearity in eq 6 allows for com-
plex functions of these inputs to be learned, such as
relative positions and approximate distance from the
diagonal, as well as their interactions with the other
inputs (e.g., to learn that some words are exceptional
cases where a diagonal bias should not apply).

3.2 markov condition
the id48 model of translation (vogel et al., 1996)
is based on a markov condition over alignment ran-
dom variables, to allow the model to learn local ef-
fects such as when i     j is aligned then it is likely
that i + 1     j + 1 or i     j + 1. these corre-
spond to local diagonal alignments or one-to-many
alignments, respectively. in general, there are many
correlations between the alignments of a word and
the word immediately to its left.

markov conditioning can also be incorporated in
a similar manner to positional bias, by augmenting
the attentional input from eqs 5 and 6 to include:

fji = v(cid:62) tanh

. . . + w(am)  1(  j   1; i)

(7)

(cid:16)

(cid:17)

where . . . abbreviates the ei, gj   1 and    compo-
nents from eq 6, and   1(  j   1) provides a    xed di-
mensional representation of the attention state for
the preceding word. it is not immediately obvious
how to incorporate the previous attention vector as
   is dynamically sized to match the source sentence
length, thus using it directly would not generalise
over sentences of different lengths. for this reason,
we make a simpli   cation by just considering local
moves offset by   k positions, that is,

(cid:21)(cid:62)

(cid:20)

  j   1,i   k, ..,   j   1,i, ..,   j   1,i+k

  1(  j   1; i) =
with w(am)     ra  (2k+1). our approach is
likely to capture the most important alignments pat-
terns forming the backbone of the alignment id48,
namely monotone, 1-to-many, and local inversions.

3.3 fertility
fertility is the propensity for a word to be trans-
lated as a consistent number of words in the other
language, e.g., iseseisvusdeklaratsioon translates as
(the) declaration of independence. fertility is a cen-
tral component in the ibm models 3   5 (brown et
al., 1993). incorporating fertility into the attentional
model is a little more involved, and we present two
techniques for doing so.
local fertility first we consider a feature-based
technique, which includes the following features

      (cid:88)

j(cid:48)<j

(cid:88)

j(cid:48)<j

(cid:88)

j(cid:48)<j

      (cid:62)

  2(  <j; i) =

  j(cid:48),i   k, ..,

  j(cid:48),i, ..,

  j(cid:48),i+1

and the corresponding feature weights, i.e., w(af)    
ra  (2k+1). these sums represent the total align-
ment score for the surrounding source words, simi-
lar to fertility in a traditional latent variable model,
which is the sum over binary alignment random vari-
ables. a word which already has several alignments
can be excluded from participating in more align-
ments, thus combating the garbage collection prob-
lem. conversely words that tend to need high fertil-
ity can be learned through the interactions between

these features and the word and context embeddings
in eq 7.

i

(cid:16)

j   j,i

(cid:17)2

1    (cid:80)

training objective of the form(cid:80)

global fertility a second, more explicit,
tech-
nique for incorporating fertility is to include this
as a modelling constraint.
initially we considered
a soft constraint based on the approach in (xu et
al., 2015), where an image captioning model was
biased to attend to every pixel in the image ex-
actly once.
in our setting, the same idea can be
applied through adding a regularisation term to the
.
however this method is overly restrictive: enforc-
ing that every word is used exactly once is not ap-
propriate in translation where some words are likely
to be dropped (e.g., determiners and other function
words), while others might need to be translated
several times to produce a phrase in the target lan-
guage.3 for this reason we develop an alternative
method, based around a contextual fertility model,

p(fi|s, i) = n(cid:0)  (ei),   2(ei)(cid:1) which scores the fer-
tility of source word i, de   ned as fi =(cid:80)

j   j,i, us-
ing a normal distribution4 parameterised by    and
  2, both positive scalar valued non-linear functions
of the source word encoding ei. this is incorporated
into the training objective as an additional additive
i log p(fi|s, i), for each training sentence.
this formulation allows for greater consistency in
translation, through e.g., learning which words tend
to be omitted from translation, or translate as sev-
eral words. compared to the fertility model in ibm
3   5 (brown et al., 1993), ours uses many fewer pa-
rameters through working over vector embeddings,
and moreover, the biid56 encoding of the source
means that we learn context-dependent fertilities,
which can be useful for dealing with    xed syntac-
tic patterns or multi-word expressions.

term,(cid:80)

3 modern decoders (koehn et al., 2003) often impose the
restriction of each word being translated exactly once, however
this is tempered by their use of phrases as translation units rather
than words, which allow for higher fertility in contiguous trans-
lation chunks.

4the normal distribution is de   cient, as it has support for
all scalar values, despite fi being bounded above and below
(0     fi     j). this could be corrected by using a truncated
normal, or various other choices of distribution.

lang-pair

zh-en
ru-en
et-en
ro-en

# tokens (k)
454
422
1809
1639
1857
1411
1782
1806

145
90
39

# types (k)
3.44

3.12
65
25
24

figure 2: symmetric training with trace bonus, com-
puted as id127,     tr(  s   t  s   t (cid:62)).
dark shading indicates higher values.

3.4 bilingual symmetry
so far we have considered a conditional model of
the target given the source, modelling p(t|s). how-
ever it is well established for latent variable transla-
tion models that the alignments improve if p(s|t) is
also modelled and the id136s of both directional
models are combined     evidenced by the symmetri-
sation heuristics used in most decoders (koehn et al.,
2005), and also by explicit joint agreement training
objectives (liang et al., 2006; ganchev et al., 2008).
the rationale is that both models make somewhat
independent errors, so an ensemble stands to gain
from variance reduction.

joint

we propose a method for

training of
two directional models as pictured in figure 2.
involves optimising
training twinned models
l =     log p(t|s)     log p(s|t) +   b where,
as
before, we consider only a single sentence pair,
for simplicity of notation. this corresponds to a
pseudo-likelihood objective, with the b linking
the two models.5 the b component considers the
alignment (attention) matrices,   s   t     rj  i and
  t   s     ri  j, and attempts to make these close
to one another for both translation directions (see
fig. 2). to achieve this, we use a    trace bonus   ,
inspired by (levinboim et al., 2015), formulated as
b =     tr(  s   t (cid:62)  s   t) =
  s   t
i,j   s   t

(cid:88)

(cid:88)

j,i

.

j

i

as the alignment cells are normalised using the
softmax and thus take values in [0,1], the trace term
is bounded above by min(i, j) which occurs when
the two alignment matrices are transposes of each

5we could share some parameters, e.g., the id27
matrices, however we found this didn   t make much difference
versus using disjoint parameter sets. we set    = 1 herein.

table 1: statistics of the training sets, showing in
each cell the count for the source language (left) and
target language (right).

other, representing perfect one-to-one alignments in
both directions

4 experiments
datasets. we conducted our experiments with
four language pairs, translating between english    
romanian, estonian, russian and chinese. these
languages were chosen to represent a range of trans-
lation dif   culties, including languages with signi   -
cant morphological complexity (estonian, russian).
we focus on a (simulated) low resource setting,
where only a limited amount of training data is avail-
able. this serves to demonstrate the robustness and
generalisation of our model on sparse data     some-
thing that has not yet been established for neural
models with millions of parameters with vast poten-
tial for over-   tting.

table 1 shows the statistics of the training sets.6
for chinese-english, the data comes from the btec
corpus, where the number of training sentence pairs
is 44,016. we used    devset1 2    and    devset 3    as
the development and test sets, respectively, and in
both cases used only the    rst reference for evalua-
tion. for other language pairs, the data come from
the europarl corpus (koehn, 2005), where we used
100k sentence pairs for training, and 3k for devel-
opment and 2k for testing.7
models and baselines. we have implemented our
neural
translation model with linguistic features
in c++ using the id98 library.8 we compared

6for all datasets words were thresholded for training fre-
quency     5, with uncommon training and unseen testing words
replaced by an    unk    symbol.

7the    rst 100k sentence pairs were used for training, while
the development and test were drawn from the last 100k sen-
tence pairs, taking the    rst 2k for testing and the last 3k for
development.

8https://github.com/clab/id98/

con   guration
sutskever encdec
attentional
+align
+align+glofer
+align+glofer-pre
+align+sym
+align+sym+glofer-pre

test
5.35
4.77
4.56
5.20
4.31
4.44
4.43

#param (m)

8.7
15.0
15.0
15.5
15.5
30.1
31.2

table 2: perplexity results for attentional model
variants evaluated on btec zh   en, and number of
model parameters (in millions).

our proposed model against our implementations
of the attentional model (bahdanau et al., 2015)
and encoder-decoder architecture (sutskever et al.,
2014). as the baseline, we used a state-of-the-art
phrase-based id151 model
built using moses (koehn et al., 2007) with the stan-
dard features: relative-frequency and lexical trans-
lation model probabilities in both directions; distor-
tion model; language model and word count. we
used kenlm (hea   eld, 2011) to create 3-gram lan-
guage models with kneser-ney smoothing on the
target side of the bilingual training corpora.

evaluation measures. following previous work
(kalchbrenner and blunsom, 2013; sutskever et al.,
2014; bahdanau et al., 2015; neubig et al., 2015),
we evaluated all neural models using test set per-
plexities and in a re-ranking setting, using id7
(papineni et al., 2002) measure. for re-ranking, we
generated 100-best translations using the baseline
phrase-based model, to which we added log proba-
bility features from our neural models alongside the
features of the underlying phrase-based model.

4.1 analysis of alignment biases
we start by investigating the effect of various lin-
guistic constraints, described in section 3, on the
attentional model. table 2 presents the perplexity
of trained models for chinese   english translation.
for comparison, we report the results of an encoder-
decoder-based neural translation model (sutskever
et al., 2014) as the baseline. all other results are for
the attentional model with a single-layer lstm as
encoder and two-layer lstm as decoder, using 512
embedding, 512 hidden, and 256 alignment dimen-

figure 3: perplexity with training epochs on ro-en
translation, comparing several model variants.

sions. for each model, we also report the number of
its parameters. models are trained using stochastic
gradient, allowing up to 20 epochs. for each model
the best perplexity on the held-out development set
is reported, which was achieved in 5-10 epochs for
most cases.

as expected, the vanilla attentional model greatly
improves over encoder-decoder (perplexity of 4.77
vs. 5.35), clearly making good use of the additional
context. adding the combined positional bias, local
fertility, and markov structure (denoted by +align)
further decreases the perplexity to 4.56. adding the
global fertility (+glofer) is detrimental, however, in-
creases perplexity to 5.20. interestingly, global fer-
tility does helps to reduce the perplexity (to 4.31)
when using with pre-training setting (+align+glofer-
pre).
in this case, it is re   ning an already excel-
lent model from which reliable global fertility es-
timates can be obtained. this    nding is consistent
with the other languages, see figure 3 which shows
typical learning curves of different variants of the
attentional model. note that when global fertility
is added to the vanilla attentional model with align-
ment features, it signi   cantly slows down training
as it limits exploration in early training iterations,
however it does bring a sizeable win when used to
   ne-tune a pre-trained model. finally, the bilin-
gual symmetry also helps to reduce the perplexity
scores when used with the alignment features, how-
ever, does not combine well with global fertility

llllllllllllllllllllllllllllllllllll02468510152025epochsperplexitylvanilla+glofer+align+align +glofer pretrain+align +gloferfigure 4: example development sentence, showing the inferred attention matrix for various models for
et     en. rows correspond to the translation direction and columns correspond to different models: at-
tentional, with alignment features (+align), global fertility (+glofer), and symmetric joint training (+sym).
darker shades denote higher values and white denotes zero.

(+align+sym+glofer-pre). this is perhaps an unsur-
prising result as both methods impose a often-times
similar regularising effect over the attention matrix.
figure 4 illustrates the different attention matri-
ces inferred by the various model variants. note the
difference between the base attentional model and
its variant with alignment features (   +align   ), where
more weight is assigned to diagonal and 1-to-many
alignments. global fertility pushes more attention to
the sentinel symbols    s    and    /s   . determiners and
prepositions in english show much lower fertility
than nouns, while estonian nouns have even higher
fertility. this accords with estonian morphology,
wherein nouns are in   ected with rich case mark-
ing, e.g., n  oukoguga has the cogitative -ga suf   x,
meaning    with   , and thus translates as several en-
glish words (with the council). the right-most col-

umn corresponds to joint symmetric training, with
many more con   dent attention values especially for
consistent 1-to-many alignments (dif   cult in english
and raskeid in estonian, an adjective in partitive case
meaning some dif   cult).

4.2 full results
the perplexity results of the neural models for
the two translation directions across the four lan-
guage pairs are presented in table 3.a and 3.b.
in all cases, our work achieves lower perplexities
compared to the vanilla attentional model and the
encoder-decoder architecture, owing to the linguis-
tic constraints.

table 4 presents the id7 scores for the re-
ranking setting for the translating into english from
our four languages. we compare re-ranking set-

   (cid:84)   (cid:88)(cid:70)(cid:73)(cid:66)(cid:87)(cid:70)(cid:67)(cid:70)(cid:70)(cid:79)(cid:85)(cid:73)(cid:83)(cid:80)(cid:86)(cid:72)(cid:73)(cid:84)(cid:80)(cid:78)(cid:70)(cid:69)(cid:74)(cid:2393)(cid:74)(cid:68)(cid:86)(cid:77)(cid:85)(cid:79)(cid:70)(cid:72)(cid:80)(cid:85)(cid:74)(cid:66)(cid:85)(cid:74)(cid:80)(cid:79)(cid:84)(cid:13)(cid:70)(cid:84)(cid:81)(cid:70)(cid:68)(cid:74)(cid:66)(cid:77)(cid:77)(cid:90)(cid:88)(cid:74)(cid:85)(cid:73)(cid:85)(cid:73)(cid:70)(cid:68)(cid:80)(cid:86)(cid:79)(cid:68)(cid:74)(cid:77)(cid:15)   (cid:16)(cid:84)      (cid:84)   (cid:88)(cid:70)(cid:73)(cid:66)(cid:87)(cid:70)(cid:67)(cid:70)(cid:70)(cid:79)(cid:85)(cid:73)(cid:83)(cid:80)(cid:86)(cid:72)(cid:73)(cid:84)(cid:80)(cid:78)(cid:70)(cid:69)(cid:74)(cid:2393)(cid:74)(cid:68)(cid:86)(cid:77)(cid:85)(cid:79)(cid:70)(cid:72)(cid:80)(cid:85)(cid:74)(cid:66)(cid:85)(cid:74)(cid:80)(cid:79)(cid:84)(cid:13)(cid:70)(cid:84)(cid:81)(cid:70)(cid:68)(cid:74)(cid:66)(cid:77)(cid:77)(cid:90)(cid:88)(cid:74)(cid:85)(cid:73)(cid:85)(cid:73)(cid:70)(cid:68)(cid:80)(cid:86)(cid:79)(cid:68)(cid:74)(cid:77)(cid:15)   (cid:16)(cid:84)      (cid:84)   (cid:80)(cid:77)(cid:70)(cid:78)(cid:70)(cid:81)(cid:74)(cid:69)(cid:66)(cid:79)(cid:86)(cid:69)(cid:87)(cid:164)(cid:72)(cid:66)(cid:83)(cid:66)(cid:84)(cid:76)(cid:70)(cid:74)(cid:69)(cid:77)(cid:164)(cid:67)(cid:74)(cid:83)(cid:164)(cid:164)(cid:76)(cid:74)(cid:78)(cid:74)(cid:84)(cid:74)(cid:13)(cid:70)(cid:83)(cid:74)(cid:85)(cid:74)(cid:79)(cid:181)(cid:86)(cid:76)(cid:80)(cid:72)(cid:86)(cid:72)(cid:66)(cid:15)   (cid:16)(cid:84)      (cid:84)   (cid:80)(cid:77)(cid:70)(cid:78)(cid:70)(cid:81)(cid:74)(cid:69)(cid:66)(cid:79)(cid:86)(cid:69)(cid:87)(cid:164)(cid:72)(cid:66)(cid:83)(cid:66)(cid:84)(cid:76)(cid:70)(cid:74)(cid:69)(cid:77)(cid:164)(cid:67)(cid:74)(cid:83)(cid:164)(cid:164)(cid:76)(cid:74)(cid:78)(cid:74)(cid:84)(cid:74)(cid:13)(cid:70)(cid:83)(cid:74)(cid:85)(cid:74)(cid:79)(cid:181)(cid:86)(cid:76)(cid:80)(cid:72)(cid:86)(cid:72)(cid:66)(cid:15)   (cid:16)(cid:84)      (cid:84)   (cid:80)(cid:77)(cid:70)(cid:78)(cid:70)(cid:81)(cid:74)(cid:69)(cid:66)(cid:79)(cid:86)(cid:69)(cid:87)(cid:164)(cid:72)(cid:66)(cid:83)(cid:66)(cid:84)(cid:76)(cid:70)(cid:74)(cid:69)(cid:77)(cid:164)(cid:67)(cid:74)(cid:83)(cid:164)(cid:164)(cid:76)(cid:74)(cid:78)(cid:74)(cid:84)(cid:74)(cid:13)(cid:70)(cid:83)(cid:74)(cid:85)(cid:74)(cid:79)(cid:181)(cid:86)(cid:76)(cid:80)(cid:72)(cid:86)(cid:72)(cid:66)(cid:15)   (cid:16)(cid:84)      (cid:84)   (cid:80)(cid:77)(cid:70)(cid:78)(cid:70)(cid:81)(cid:74)(cid:69)(cid:66)(cid:79)(cid:86)(cid:69)(cid:87)(cid:164)(cid:72)(cid:66)(cid:83)(cid:66)(cid:84)(cid:76)(cid:70)(cid:74)(cid:69)(cid:77)(cid:164)(cid:67)(cid:74)(cid:83)(cid:164)(cid:164)(cid:76)(cid:74)(cid:78)(cid:74)(cid:84)(cid:74)(cid:13)(cid:70)(cid:83)(cid:74)(cid:85)(cid:74)(cid:79)(cid:181)(cid:86)(cid:76)(cid:80)(cid:72)(cid:86)(cid:72)(cid:66)(cid:15)   (cid:16)(cid:84)   (cid:66)(cid:2171)(cid:70)(cid:79)(cid:85)(cid:74)(cid:80)(cid:79)(cid:66)(cid:77)(cid:12)(cid:66)(cid:77)(cid:74)(cid:72)(cid:79)(cid:12)(cid:66)(cid:77)(cid:74)(cid:72)(cid:79)(cid:12)(cid:72)(cid:77)(cid:80)(cid:71)(cid:70)(cid:83)(cid:12)(cid:66)(cid:77)(cid:74)(cid:72)(cid:79)(cid:12)(cid:72)(cid:77)(cid:80)(cid:71)(cid:70)(cid:83)(cid:12)(cid:84)(cid:90)(cid:78)(cid:70)(cid:85)   (cid:70)(cid:79)(cid:70)(cid:79)   (cid:70)(cid:85)lang. pair
enc-dec
attentional
our work

zh-en
5.35
4.77
4.31

ru-en et-en ro-en
61.9
41.7
39.9

10.3
6.62
5.89

18.2
12.8
11.8

lang. pair
enc-dec
attentional
our work

(a)

en-zh en-ru en-et en-ro

8.60
7.49
6.24

67.3
43.0
40.6

(b)

31.4
19.4
17.0

11.5
7.30
6.35

table 3: perplexity on the test sets for the two trans-
lation directions. our work includes: bidirectional
lstm attentional model combined with positional
bias, markov, local fertility, and global fertility (pre-
trained setting).

lang. pair
phrase-based
enc-dec
attentional
our work

zh-en ru-en et-en ro-en
31.99
40.63
45.21
18.70
32.20
45.36
40.41
18.83
41.16    19.79
32.78
46.83
33.26    46.88
44.14       19.73
table 4: id7 scores on the test sets for re-ranking.
bold: best performance,    : signi   cantly better than
attentional,    : using ensemble of models.

tings using the log probabilities produced by our
model as additional features vs. using log proba-
bilities from the vanilla attentional model and the
encoder-decoder. the re-rankers based on our model
are signi   cantly better than the rest for chinese and
estonian, and on par with the other for russian and
romanian   english. in all cases our model has per-
formance at least 1 id7 point better than the base-
line phrase-based system. it is worth noting that for
chinese-english, our re-ranker leads to an increase
of almost 3 points in the id7 score using an en-
semble of neural models with different con   gura-
tions.9

5 related work

kalchbrenner and blunsom (2013) were the    rst to
propose a full neural model of translation, using a
convolutional network as the source encoder, fol-

lowed by an id56 decoder to generate the target
translation. this was extended in sutskever et al.
(2014), who replaced the source encoder with an
id56 using a long short-term memory (lstm),
and bahdanau et al. (2015) who introduced the no-
tion of    attention    to the model, whereby the source
context can dynamically change during the decod-
ing process to attend to the most relevant parts of
the source sentence luong et al. (2015) re   ned the
attention mechanism to be more local, by constrain-
ing attention to a text span, whose words    repre-
sentations are averaged. to leverage the attention
history, (luong et al., 2015) made use of the atten-
tion vector of the previous position when generat-
ing the attention vector for the next position, simi-
lar in spirit to our method for incorporating align-
ment structural biases. concurrent with our work,
cheng et al. (2015) proposed a similar agreement-
based joint training for bidirectional attention-based
id4, and showed signi   cant
improvement in the id7 score for the large data
french   english translation.
6 conclusion
we have shown that the attentional model of transla-
tion does not capture many well known properties of
traditional word-based translation models, and pro-
posed several ways of imposing these as structural
biases on the model. we show improvements across
several challenging language pairs in a low-resource
setting, both in perplexity and re-ranking evalua-
tions.
in future work we intend to investigate the
model performance on larger datasets, and incorpo-
rate further linguistic information such as morpho-
logical representations.

acknowledgements
the work reported here was started at jsalt 2015
in uw, seattle, and was supported by jhu via grants
from nsf (iis), darpa (lorelei), google, mi-
crosoft, amazon, mitsubishi electric, and merl.
dr cohn was supported by the arc (future fellow-
ship).

9we use the outputs of 6   12 models trained in both direc-
tions, using different alignment and fertility options, and using
a smaller dimensionality than earlier (100 embedding, 100 hid-
den and 50 attention dimensions).

references
[bahdanau et al.2015] dzmitry bahdanau, kyunghyun
2015. neural machine

cho, and yoshua bengio.

translation by jointly learning to align and translate. in
proceedings of the international conference on learn-
ing representations (iclr), san diego, ca.

[brown et al.1993] peter e. brown, stephen a. della
pietra, vincent j. della pietra, and robert l. mercer.
1993. the mathematics of statistical machine trans-
lation: parameter estimation. computational linguis-
tics, 19(2).

[cheng et al.2015] yong cheng, shiqi shen, zhongjun
zhongjun he, wei he, hua wu, maosong sun, and
yang liu. 2015. agreement-based joint training for
bidirectional attention-based neural machine transla-
tion. in arxiv: 1512.04650 [cs.cl].

[cho et al.2014] k. cho, b. van merrienboer, d. bah-
danau, and y. bengio. 2014. on the properties of neu-
ral machine translation. in arxiv:1409.1259 [cs.cl].
[dyer et al.2013] chris dyer, victor chahuneau, and
noah a. smith. 2013. a simple, fast, and effective
in proceedings
reparameterization of ibm model 2.
of the 2013 conference of the north american chap-
ter of the association for computational linguistics:
human language technologies, pages 644   648, at-
lanta, georgia, june. association for computational
linguistics.

[ganchev et al.2008] kuzman ganchev, jo  ao v. grac  a,
2008. better alignments = bet-
and ben taskar.
in proceedings of acl-08: hlt,
ter translations?
pages 986   993, columbus, ohio, june. association
for computational linguistics.

[hea   eld2011] kenneth hea   eld. 2011. kenlm: faster
and smaller language model queries. in proceedings
of the emnlp 2011 sixth workshop on statistical ma-
chine translation, pages 187   197, edinburgh, scot-
land, united kingdom, july.

[hochreiter and schmidhuber1997] s. hochreiter

and
1997. long short-term memory.

j. schmidhuber.
neural computation, 9:1735   1780.

[kalchbrenner and blunsom2013] nal kalchbrenner and
phil blunsom. 2013. recurrent continuous translation
in proceedings of the 2013 conference on
models.
empirical methods in natural language processing,
seattle, october.

[koehn et al.2003] philipp koehn, franz josef och, and
daniel marcu. 2003. statistical phrase-based transla-
tion. in proceedings of north american chapter of the
association for computational linguistics on human
language technology, pages 48   54.

[koehn et al.2005] philipp koehn, amittai axelrod,
alexandra birch, chris callison-burch, miles os-
borne, david talbot, and michael white.
2005.
edinburgh system description for the 2005 iwslt
speech translation evaluation. in iwslt, pages 68   75.
[koehn et al.2007] philipp koehn, hieu hoang, alexan-
dra birch, chris callison-burch, marcello federico,

nicola bertoldi, brooke cowan, wade shen, chris-
tine moran, richard zens, et al. 2007. moses: open
source toolkit for id151.
in
proc. acl interactive poster and demonstration ses-
sions, pages 177   180.

[koehn2005] philipp koehn. 2005. europarl: a parallel
in con-
corpus for id151.
ference proceedings: the tenth machine translation
summit, pages 79   86. aamt.

[levinboim et al.2015] tomer

levinboim,

ashish
vaswani, and david chiang. 2015. model invert-
ibility id173: sequence alignment with or
in proceedings of the north
without parallel data.
american chapter of the association for computa-
tional linguistics (naacl), pages 609   618, denver,
co.

[liang et al.2006] percy liang, ben taskar, and dan
klein. 2006. alignment by agreement. in proceed-
ings of the north american chapter of the association
for computational linguistics (naacl), pages 104   
111, new york, ny.

[luong et al.2015] minh-thang luong, hieu pham, and
christopher d. manning. 2015. effective approaches
to attention-based id4. in pro-
ceedings of the 2015 conference on empirical meth-
ods in natural language processing, pages 1412   
1421, lisbon, portugal, september. association for
computational linguistics.

[neubig et al.2015] graham neubig, makoto morishita,
and satoshi nakamura.
2015. neural reranking
improves subjective quality of machine translation:
naist at wat2015. in proceedings of the 2nd work-
shop on asian translation (wat2015), kyoto, japan,
october.

[och and ney2003] franz josef och and hermann ney.
2003. a systematic comparison of various statis-
tical alignment models. computational linguistics,
29(1):19   51.

[papineni et al.2002] kishore papineni, salim roukos,
todd ward, and wei-jing zhu. 2002. id7: a method
for automatic evaluation of machine translation.
in
proceedings of the 40th annual meeting on associa-
tion for computational linguistics, acl.

[sutskever et al.2014] ilya sutskever, oriol vinyals, and
quoc vv le. 2014. sequence to sequence learning
with neural networks. in neural information process-
ing systems (nips), pages 3104   3112, montr  eal.

[vogel et al.1996] stephan vogel, hermann ney, and
christoph tillmann. 1996. id48-based word align-
in proceedings of the
ment in statistical translation.
international conference on computational linguis-
tics (coling), pages 836   841.

[xu et al.2015] kelvin xu,

jimmy ba, ryan kiros,
kyunghyun cho, aaron courville, ruslan salakhudi-
nov, rich zemel, and yoshua bengio. 2015. show,
attend and tell: neural image id134 with
visual attention. in proceedings of the 32nd interna-
tional conference on machine learning (icml-15),
pages 2048   2057.

