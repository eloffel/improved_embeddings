5
1
0
2

 
r
a

 

m
1
2

 
 
]
l
c
.
s
c
[
 
 

3
v
9
6
3
4

.

2
1
4
1
:
v
i
x
r
a

incorporating both distributional and rela-
tional semantics in word representations

daniel fried   
department of computer science
university of arizona
tucson, arizona, usa
dfried@email.arizona.edu

kevin duh
graduate school of information science
nara institute of science and technology
ikoma, nara, japan
kevinduh@is.naist.jp

abstract

we investigate the hypothesis that word representations ought to incorporate both
distributional and relational semantics. to this end, we employ the alternating
direction method of multipliers (admm), which    exibly optimizes a distribu-
tional objective on raw text and a relational objective on id138. preliminary
results on knowledge base completion, analogy tests, and parsing show that word
representations trained on both objectives can give improvements in some cases.

1

introduction

we are interested in algorithms for learning vector representations of words. recent work has shown
that such representations, also known as id27s, can successfully capture the semantic
and syntactic regularities of words (mikolov et al., 2013a) and improve the performance of various
natural language processing systems, including information extraction (turian et al., 2010; wang
& manning, 2013), parsing (socher et al., 2013a), and id14 (collobert et al., 2011).
although many kinds of representation learning algorithms have been proposed so far, they are all
essentially based on the same premise of id65 (harris, 1954), embodied by j.
r. firth   s dictum:    you shall know a word by the company it keeps.    for example, the models
of (bengio et al., 2003; schwenk, 2007; collobert et al., 2011; mikolov et al., 2013b; mnih &
kavukcuoglu, 2013) train word representations by exploiting the context window around the word.
intuitively, these algorithms learn to map words with similar context to nearby points in vector space.
however, id65 is by no means the only theory of word meaning. relational se-
mantics, exempli   ed by id138 (miller, 1995), de   nes a word by its relation with other words.
relations such as synonymy, hypernymy, and meronymy (cruse, 1986) create a graph that links
words in terms of our world knowledge and psychological predispositions. for example, stating
a relation like    dog is-a mammal    gives a precise hierarchy between the two words, in a way that
is very different from the distributional similarities observable from corpora. arguably, the vec-
tor representation of    dog    ought be close to that of    mammal   , regardless of their distributional
contexts.
we believe both distributional and relational semantics are valuable for word representations. our
goal is to explore how to combine these complementary approaches into a uni   ed learning algorithm.
we thus employ a general representation learning algorithm based on the alternating direction
method of multipliers (admm) (boyd et al., 2011) for jointly optimizing both distributional and
relational objectives. its advantages include (a)    exibility in incorporating arbitrary objectives, and
(b) relative ease of implementation.

   currently at the university of cambridge.

1

in the following, we    rst discuss objectives for independently learning id65
(  2.1) or relational semantics (  2.2). the admm framework that optimizes both objectives is
described in   3 and analyzed in   4. to test whether our embeddings are widely applicable, we
evaluate three speci   c admm instantiations (each using different ways of incorporating relational
semantics) on a wide range of tasks (  5).

2 objectives for representation learning

2.1 id65 objective

a standard way to implement id65 in representation learning is the neural lan-
guage model (nlm) of collobert et al. (2011). each word i in the vocabulary is associated
with a d-dimensional vector wi     rd, the word   s embedding. an n-length sequence of words
(i1, i2, . . . , in) is represented as a vector x by concatenating the vector embeddings for each word,
x = [wi1; wi2 . . . ; win]. this vector x is then scored by feeding it through a two-layer neural
network with h hidden nodes:

(cid:62)

(f (ax + b))

sn lm (x) = u

(1)
where a     rh  (nd) is the weight matrix and b     rh is the bias vector for the hidden layer, u     rh
is the weight vector for the output layer, and f is the sigmoid f (t) = 1/(1 + e   t).
the layer parameters and id27s of this model are trained using noise contrastive es-
timation (smith & eisner, 2005; gutmann & hyv  arinen, 2010; mnih & kavukcuoglu, 2013). a
sequence of text from the training corpus is corrupted by replacing a word in the sequence with
a random word sampled from the vocabulary, providing an implicit negative training example xc.
to train the network so that correct sequences receive a higher score than corrupted sequences, the
hinge id168 is optimized:

ln lm (x, xc) = max(0, 1     sn lm (x) + sn lm (xc))

(2)
the id27s w and network layer parameters a, u, b are trained with id26,
using stochastic id119 (sgd) over id165s in the training corpus. we are concerned with
the learned embeddings and disregard the other network parameters after training.

2.2 relational semantics objective

methods for learning word representations based on relational semantics have only recently been
explored. we    rst present a simple new objective based on id138 graph distance (  2.2.1), then
discuss two recent proposals that directly model relation types (  2.2.2). while our objectives focus
on relational semantics in id138, they are extensible to other kinds of relational data, including
knowledge bases like freebase.

2.2.1 graph distance

in this approach, we aim to train id27s such that the distance between id27s
in the vector space is a function of the distance between corresponding entities in id138. the
primary entities in id138 are synonym sets, or synsets. each synset is a group of words repre-
senting one lexical concept. id138 contains a set of relationships between these synsets, forming
a directed graph where vertices are synsets and relationships are edges. the primary relationship is
formed by the hypernym (is-a) relationship.
by treating these hypernym relationships as undirected edges between synsets, we approximate
semantic relatedness between synsets as the length of the shortest path between two synsets in the
graph. we add a common root node at the base of all hypernym trees so that the synset graph is
connected, and adopt the similarity function of leacock & chodorow (1998):

synsim(si, sj) =     log

len(si, sj)

2    max

s   w ordn et

depth(s)

(3)

where len(si, sj) is the length of the shortest undirected hypernym path between synsets si and sj in
the graph, and depth returns the distance from the root of the hypernym hierarchy to a given synset.

2

since there is a many-to-many relationship between words and id138 synsets, and embeddings
for words, not synsets, are desired, we de   ne the similarity between two words to be the maximum
similarity between their corresponding synsets, if both words have associated synsets, and unde   ned
otherwise:

w ordsim(i, j) =

max

synsim(si, sj)

si   syn(i),sj   syn(j)
where syn(i) is the set of synsets corresponding to word i.
to integrate id138 similarity with id27s, we de   ne the following graph distance
loss, lgd. for a word pair (i, j), we encourage the cosine similarity between their embeddings vi
and vj to match that of a scaled version of w ordsim(i, j):

(4)

lgd(i, j) =

    [a    w ordsim(i, j) + b]

(5)

(cid:19)2

(cid:18) vi    vj

||vi||2||vj||2

where a and b are parameters that scale w ordsim(i, j) to be of the same range as the cosine
similarity between embeddings.
sgd is used to train the id27s as well as the scalar parameters a and b. pairs of words
with de   ned w ordsim (i.e. if both words have synsets) are sampled from the vocabulary and used
as a single training instance. details of the sampling process are presented in   4.

2.2.2 existing relational objectives

we are aware of two recent approaches from the knowledge base literature which, in addition to rep-
resenting words (entities) with vector embeddings, directly represent a knowledge base   s relations
as operations in the vector embedding space. these models both take as input a tuple (vl, r, vr)
representing a possible relationship of type r between words vl and vr, and assign a score to the
relationship.
the transe model of bordes et al. (2013) represents relationships as translations in the vector em-
bedding space. for two words vl and vr, if the relationship r holds, i.e. (vl, r, vr) is true, then
the corresponding embeddings vl, vr     rd should be close after translation by the relation vector
r     rd. the score of a relationship tuple is the similarity between vl + r and vr, measured by the
negative of the residual:

st ranse(vl, r, vr) =    ||vl + r     vr||2

(6)

socher et al. (2013b) introduce a neural tensor network (ntn) model that allows modeling of
the interaction between embeddings using tensors. the ntn model is a two-layer neural network
with h hidden units and a bilinear tensor layer directly relating embeddings. this provides a more
expressive model than transe, but also requires training a larger number of parameters for each
relation. the scoring function for a relation r is

(cid:62)

(cid:62)
l wrvr + vr

f

v

sn t n (vl, r, vr) = u

(7)
where f is the sigmoid non-linearity applied elementwise, u     rh is the weight vector of the
output layer, and wr     rd  d  h, vr     rh  2d and br     rk are a tensor, matrix, and bias vector
respectively for relationship r.
as in the neural language model, embeddings and parameters for these relational models are
trained using contrastive estimation and sgd, using the hinge loss as de   ned in (2), where sn lm
is replaced by either the st ranse or sn t n scoring function on tuples.1

+ br

vr

(cid:20)vl

(cid:21)

(cid:19)

(cid:18)

3

joint objective optimization by admm

we aim to train a set of id27s that, along with the corresponding model parameters,
satisfy both the distributional modeling objective (sec. 2.1) and one of the relational modeling
1as in the graph distance objective, we must map from synsets to words. in each sgd iteration, a relation-
ship tuple (sl, r, sr) is sampled from id138 such that synsets sl and sr contain words in the vocabulary.
one word is sampled for each synset from the set of words in the vocabulary contained in the synset, producing
a tuple (wl, r, wr). this is the correct tuple to be used in training, treating words as entities for the relational
model. to produce the corrupted tuple, one of wl, r, or wr is randomly replaced.

3

objectives (sec. 2.2). we adopt the alternating direction method of multipliers (admm) ap-
proach (boyd et al., 2011). rather than use the same set of embeddings to evaluate the id168s
for both the distributional and relational objectives, the embeddings are split into two sets, one for
each objective, and allowed to vary independently. an augmented lagrangian penalty term is added
to constrain the corresponding embeddings for each word to have minimal difference. the advan-
tage of this approach is that existing methods for optimizing each objective independently can be
re-used, leading to a    exible and easy-to-implement algorithm.
we describe the admm formulation using graph distance as the id138 modeling objective, but
a similar formulation holds when using other relational objectives. let w be the set of word em-
beddings {w1, w2, . . . wn(cid:48)} for the distributional modeling objective, and v be the set of word
embeddings {v1, v2, . . . vn(cid:48)(cid:48)} for the relational modeling objective, where n(cid:48) is the number of
words in the model vocabulary of the corpus, and n(cid:48)(cid:48) is the number of words in the model vocabu-
laries of id138. let i be the set of n words that occur in both the corpus and id138, i.e. the
intersection. then we de   ne a set of vectors y = {y1, y2, . . . yn}, which correspond to lagrange
multipliers, to penalize the difference (wi     vi) between sets of embeddings for each word i in the
joint vocabulary i:

(cid:88)

(cid:16)

i   i

(cid:17)

(cid:32)(cid:88)

i   i

(cid:33)

lp (w, v) =

i (wi     vi)
(cid:62)
y

+

  
2

||wi     vi||2

2

(8)

in the    rst term, y has same dimensionality as w and v, so a scalar penalty is maintained for each
entry in every embedding vector. the second residual penalty term with hyperparameter    is added
to avoid saddle points. later we shall see that    can be viewed as a step-size during the update of y.
finally, this augmented lagrangian term (eq. 8) is added to the sum of the loss terms for each
objective (eq. 2 and eq. 5). let    = (u, a, b) be the parameters of the id38
objective, and    = (a, b) be the parameters of the id138 graph distance objective. the    nal loss
function we optimize becomes:

l = ln lm (w,   ) + lgd(v,   ) + lp (w, v)

(9)

the admm algorithm proceeds by repeating the following three steps until convergence:

1. perform stochastic id119 on w and    to minimize ln lm + lp , with all other

parameters    xed.

2. perform stochastic id119 on v and    to minimize lgd + lp , with all other

parameters    xed.

3. for all embeddings i corresponding to words in both the id165 and relational training sets,

update the constraint vector yi:

yi := yi +   (wi     vi)

(10)

since ln lm and lgd share no parameters, the id119 step for w in step 1 does not
depend on    (the parameters of the relational objective). the derivative of ln lm (w,   )+lp (w, v)
with respect to wi is simply the derivative of ln lm (w,   ) plus yi +    (wi     vi); the second term
acts like a bias term to make wi closer to vi. similarly, the id119 step for v does not
depend on   , so step 2 can be optimized easily. in step 3, a large difference (wi     vi) causes yi to
become large, and therefore increases the constraint for the two sets of embeddings to be similar in
both steps 1 and 2.
we note that it is possible to introduce a weight parameter        [0, 1] into the joint id168 (eq.
9) to prioritize either ln lm or lgd:

l =   ln lm (w,   ) + (1       )lgd(v,   ) + lp (w, v)

empirically we found that while the difference between using joint objective compared to single
objective is large, the exact value of    does not signi   cantly change the results; all experiments here
use equal weighting.

4

(a) mean of joint loss without penalty term, ln lm + lgd

(b) average magnitude of the constraint vectors, y.

(c) magnitude of the embedding residuals, scaled by the embedding magnitudes, averaged across all embed-
dings.

figure 1: analysis of admm behavior as training iteration progresses, for varying values of   .

4 data setup and analysis

the distributional objective ln lm is trained using 5-grams from the google books english corpus,
distributed by uc berkeley in the web 1t format2. this corpus contains over 180 million 5-gram
types and 31 billion 5-gram tokens. in our experiments, 5-grams are preprocessed by lowercasing
all words. the top 50,000 unigrams by frequency are used as the vocabulary. all less-frequently
occurring words are replaced with a token, rare, which has its own vector space embedding. in
each admm iteration, a block of 100,000 id165s is sampled from the corpus. each id165 in the
block (and a corrupted, noise-contrastive version of the id165, see   2.1) is used to perform gradient
descent on the distributional id168 ln lm or its admm equivalent.

2http://tomato.banatao.berkeley.edu:8080/berkeleylm_binaries/

5

02004006008001000training iterations0.000.050.100.150.200.250.300.350.400.45llm+lgdnlm loss + id138 loss02004006008001000training iterations0.00.20.40.60.81.01.21.4mean normmean ||yi||2training data for the relational objective varies depending on whether the graph distance objective
(gd) or one of the two relational objectives (ntn or transe) is used. when the graph distance
objective is used, word pairs with de   ned graph distance (e.g. words that are contained in id138
synsets) are randomly sampled and used as training instances. in each admm iteration, 100,000
words are sampled with replacement from the vocabulary. for each sampled word w, up to 5 other
words v with de   ned graph distance to word w are sampled from the vocabulary. each pair w, v is
then used as a training instance for id119 on the lgd id168 (  2.2.1) or its admm
equivalent.
when either ntn or transe is used as the relational objective, id138 relationship tuples
(s1, r, s2) where synsets s1 and s2 contain words in the vocabulary, are used as the training in-
stances for stochastic id119 using noise contrastive estimation (  2.2.2). for comparison
with the existing work of socher et al. (2013b), we use their dataset, which contains training, devel-
opment, and testing splits for 11 id138 relationships (table 1). the entire training set is presented
to the network in randomized order, one instance at a time, during each iteration of training.
we next provide an analysis of the behavior of the admm joint model (ln lm + lgd) on the
training set in figure 1. figure 1(a) plots the learning curve by training iteration using varying
values of the    hyperparameter. although establishing convergence guarantees for non-convex loss
functions for admm is theoretically still an open question (e.g. ln lm is a non-convex multi-layer
neural net), we empirically observe convergence on our dataset for various values of   . further, in
accordance with previous works (boyd et al., 2011), admm attains a reasonable objective value
relatively quickly in a few iterations; our loss converges around 100 iterations.3 figure 1(b) shows
the change in the mean norms of the lagrange multipliers ||yi||2. the magnitude of these norms
indicates the degree to which the wi and vi vectors are being constrained in the current admm
iteration. the norm gradually increases, indicating the tightening of constraints in each iteration. as
expected, larger values of    lead to faster increases of ||yi||2. finally, figure 1(c) shows the normal-
ized difference between the resulting sets of embeddings w and v, which decreases as desired.4
as we perform sgd on ever-more data, the norms of w and v generally increase. a conventional
solution is to add the l2 norms of w and v as additional regularizers in the objective function.
we found that, on the knowledge base completion task (  5), l2 id173 decreased the per-
formance of all admm models, but slightly increased the performance of the ntn and transe
single objective models; on the analogy test tasks, it decreased the performance of the gd, nlm,
and all admm models, but increased the performance of ntn and transe. since id173
hurt performance for the majority of models, and the evaluation results converge regardless, we use
unregularized models in all experiments reported here.

5 task-specific evaluation

we now compare the embeddings learned with different objectives on three different tasks. for
all experiments, we use 50-dimensional embeddings taken from iteration 1000 of training, with
   = 0.05 for admm.

5.1 knowledge base completion
models trained using either the ntn or transe relational modeling objective (  2.2.2) learn a vector-
space representation of relationships in id138. we use the methodology and datasets of socher
et al. (2013b) to evaluate the models    ability to classify relationship triplets as correct or incorrect.
this relation classi   cation is useful for    completing    or    extending    a knowledge base with new
facts. the testing set consists of correct relationship tuples that are present in id138, and incorrect
tuples created by randomly switching entities from correct tuples. a development set is used to
determine threshold scores tr for each relation that maximize classi   cation accuracy when tuples

3on a 3.3hz xeon cpu, this took about 9 hours for admm, not more than the 7 hours for independent

ln lm and 3 hours for independent lgd objectives combined.

4the reason for the peak around iteration 50 in figure 1(c) is that the embeddings begin with similar random
initializations, so initially differences are small; as admm starts to see more data, w and v diverge, but
converge eventually as y become large.

6

classi   cation accuracies by model on test set

transe + nlm

relationship type
hasinstance
typeof
memberholonym
membermeronym
partof
haspart
domainregion
synsetdomaintopic
subordinateinstanceof
similarto
domaintopic
overall

number of relationships
test
6334
5504
2346
2268
1266
1348
592
622
650
42
116
21088

train
36178
30556
9146
9223
6600
6139
4227
3976
3778
1659
1099
112581

dev
1632
1334
614
522
334
296
168
144
146
4
24
5218

ntn ntn + nlm transe
76.66
79.98
85.35
81.59
90.57
88.44
85.27
81.34
80.72
82.06
76.48
74.40
68.58
70.94
91.15
89.06
94.00
91.53
54.76
64.28
67.24
68.10
80.95
82.87

76.33
83.79
88.61
83.95
80.17
76.33
68.41
90.67
92.15
57.14
62.06
81.27

80.20
84.77
90.36
81.74
83.80
77.52
72.46
90.35
92.46
54.76
72.41
83.10

table 1: knowledge base completion results: counts by id138 relation type and test classi   ca-
tion accuracies for ntn, transe, and joint objectives.

(vl, r, vr) having s(vl, r, vr)     tr are classi   ed as correct, and tuples having a score lower than
tr are classi   ed as incorrect.
models trained using a joint objective (nlm with either transe or ntn)5 are evaluated by taking the
v set of vectors (those learned for the relational objective) and passing these through the relational
objective function to score a given tuple. the test accuracies are shown in table 1. overall, the ntn
baseline achieves 80.95% accuracy6, while the joint objective ntn+nlm improves it to 81.27%.
similarly, transe+nlm (83.10%) outperforms the transe baseline (82.87%). we conclude that
admm can give small albeit noticeable improvement to transe and ntn. table 1 also shows the
accuracies by relation type. we note that transe+nlm performs at least as well as the transe single
objective across all categories except typeof and memberholonym.
we also experimented with using the average of w and v vectors (rather than v itself after admm)
for relation classi   cation. this produced lower overall accuracies for the joint model (75.38% for
ntn+nlm and 71.73% for transe+nlm), implying that differences between w and v may still
be important in actual tasks.

5.2 analogy tests for relational similarity

semeval-2012 task 2 (jurgens et al., 2012) is a relational similarity task similar to sat-style analogy
questions. the task is to score word pairs by the degree to which they belong in a relation class
de   ned by a set of example word pairs. for example, the relation class reverse contains the
example pairs (attack,defend) and (buy,sell). there are 69 testing relation categories,
each with three or four example word pairs. in the evaluation, the model is shown a number of
testing relation pairs in each category and scores each testing pair according to its similarity to the
example relation pairs. these similarity scores are then compared to human similarity judgements.
this is an useful task to test whether the positioning of learned embeddings in vector space leads to
some meaningful semantics.
following zhila et al. (2013), we evaluate the embeddings in this task on their ability to represent
relations as translations in the vector space. a given relation pair (word1, word2) is represented as
the vector difference between the two words, w2     w1, where w1 and w2 are the embeddings of
words word1 and word2. similarities between the example relations and the relations to be scored
are computed using cosine distance of these resulting embedding representations. one evaluation
metric is the spearman   s correlation coef   cient between the similarity scores output by the model
and the scores assigned to pairs by human judges. the second metric is the maxdiff accuracy, which
involves choosing both the most similar and least similar example pairs to a given target pair from a
set of four or    ve pre-de   ned choices.

5gd is not evaluated since it does not model relation types.
6our ntn results differ from those reported in (socher et al., 2013b), likely due to differences in the

optimizer (sgd vs l-bfgs), embedding size, and the use or lack of regularizer.

7

figure 2: detailed comparison of correlation and accuracy for various embeddings on semeval-
2012 task 2, by relationship category.

8

0.00.10.20.30.40.5correlation(  )attributecaserelationscause-purposeclass-inclusioncontrastnon-attributepart-wholereferencesimilarspace-timecategorytranse+nlmntn+nlmgd+nlmtransentnnlmgdembedding
nlm
gd
gd + nlm
ntn
ntn + nlm
transe
transe + nlm

accuracy correlation

0.42
0.41
0.41
0.36
0.41
0.37
0.38

0.25
0.28
0.25
0.12
0.25
0.16
0.18

table 2: analogy test results: comparison of single and joint objective embeddings. for a random
baseline, accuracy=.31 and correlation=.018. (jurgens et al., 2012).

a summary of the results is shown in table 2. we observe:

1. nlm achieves scores competitive with the recurrent neural language models used in (zhila

et al., 2013), which had a maximum accuracy of 0.41 and correlation of 0.23.

2. gd by itself also achieves comparable scores, with 0.42 accuracy and 0.28 correlation. it is
interesting to note that independent distributional and relational objectives achieve similar
results for this task.

3. the joint objective does not appear to help in this task, however. e.g., gd+nlm does not

outperform gd; while ntn+nlm outperforms ntn, it does not outperform nlm.

to analyze this result further, we show the scores by relation category in figure 2. despite nlm,
gd, gd+nlm, and ntn+nlm achieving similar top scores overall, we observe that the scores
by category are considerably varied. we conclude our current objectives, either distribution or rela-
tional, are too coarse-grained to reliably address the analogy test. in particular, the relation categories
for this task do not correspond to the relation types on id138. since it is infeasible to expect a
large id138-like resource that is annotated in the particular categories for this task, an objective
that includes some form of unsupervised relation id91 may be necessary.

5.3 id33

id33 experiments are performed on the sancl2012    parsing the web    data (petrov
& mcdonald, 2012). the setup is to train a parser on news domain (wall street journal) and evaluate
on out-of-domain web text. our goal is to see whether the performance of a standard parser can be
improved by simply using embeddings as additional features. this can be seen as a kind of semi-
supervised id171 (koo et al., 2008).
following (wu et al., 2013), we    rst cluster the embeddings using id116, then incorporate the
cluster ids as features. the reasoning is that discrete cluster ids are easier to incorporate into existing
parsers as conjunctions of features. we use the standard    rst-order mst parser7. for simplicity, we
only attempt to cluster the embeddings into k = 64 clusters and report results on the development
set; a more extensive experiment involving multiple k and model selection is left as future work.
table 3 shows the labeled attachment scores (las), i.e.
syntactic attachment and relation label for each word. we observe:

the accuracy of predicting both correct

1. incorporating embeddings from joint objective training always helps; all of these embed-
dings improve upon the case of no embeddings (none) in all    ve domains. this is a nice
result considering that our embeddings are trained on google books, not sancl, and have
a 9-13% token out-of-vocabulary rate on the data.

2. in contrast, improvements from embeddings trained from a single relational objective are
mixed, and are in general poorer than those trained from a single distributional objective
(nlm). this suggests distributional information may be more effective for this task.

3. the best results are achieved by the joint models nlm+gd (average las of 76.18) and
nlm+ntn (76.14). the improvement over nlm (76.03) is not large, but we believe this is

7sourceforge.net/projects/mstparser/

9

embedding
none
ntn
transe
gd
nlm
nlm + ntn
nlm + transe
nlm + gd

average answers emails newsgroups reviews weblogs

75.83
75.85
75.86
75.90
76.03
76.14
76.01
76.18

73.40
73.56
73.30
73.54
73.65
73.81
73.50
73.78

73.72
73.48
73.69
73.73
73.77
73.87
73.89
73.69

74.88
74.88
75.09
75.03
74.96
75.09
75.09
75.39

75.46
75.53
75.68
75.65
75.81
75.92
75.53
75.76

81.70
81.58
81.74
81.55
81.94
82.01
82.02
82.28

table 3: comparison of parsers using different embedding features. labeled arc score (las) on
   ve web domains and their average are reported.

a promising result nevertheless; it implies that our joint objective does indeed complement
the strong results of id65.

6 conclusions and future work

we advocate for a word representation learning algorithm that jointly implements both distributional
and relational semantics. our    rst contribution is an investigation of the admm algorithm, which
   exibly combines multiple objectives. we show that admm converges quickly and is an effective
method for combining multiple sources of information and linguistic intuitions into word represen-
tations. note that other approaches for combining objectives besides admm are possible, including
direct id119 on the joint objective, or concatenation of word representations individually
optimized on independent objectives. a comparison of various approaches for multi-objective opti-
mization in learning word representations, where the optimization space is riddled with local optima,
is worthwhile as future work.
the second contribution is a preliminary evaluation of three speci   c instantiations of admm, com-
bining the nlm distributional objective with graph distance, transe, or ntn relational objectives.
in both the knowledge base completion and id33 tasks, we demonstrate that the com-
bined objective provides promising minor improvements compared to the single objective case. in
the analogy task, we show that the combined objective is comparable to the single objective, and
learns a very different kind of word representation.
to the best of our knowledge, some recent work (xu et al., 2014; yu & dredze, 2014; faruqui
et al., 2014) explored similar motivations as ours. the main differences are in their optimization
methods (i.e. id119 directly on the joint objective is used in (xu et al., 2014; yu & dredze,
2014), while faruqui et al. (2014) introduces a post-processing graph-based method) as well as
alternate relational objectives, e.g. yu & dredze (2014) formulate a skip-gram objective where word
embeddings are trained to predict other words they share relations with. a detailed comparison on
the same datasets would be bene   cial to understand the impact of these design choices.
compared to the large body of existing work in word representations (e.g. lsa, (deerwester et al.,
1990), esa (gabrilovich & markovitch, 2007), sds (mitchell & lapata, 2008)), the promise of
recent learning-based approaches is that they enable a    exible de   nition of optimization objectives.
as future work, we hope to further explore objectives beyond distributional and relational semantics
and understand what objectives work best for each target task or application.

acknowledgments

this work is supported by a microsoft research core grant and jsps kakenhi grant number
26730121. d.f. was supported by the flinn scholarship during the course of this work. we thank
haixun wang, jun   ichi tsujii, tim baldwin, yuji matsumoto, and several anonymous reviewers for
helpful discussions at various stages of the project.

10

references
bengio, yoshua, ducharme, r  ejean, vincent, pascal, and jauvin, christian. a neural probabilistic

language models. jmlr, 2003.

bordes, antoine, usunier, nicolas, garcia-duran, alberto, weston, jason, and yakhnenko, oksana.
translating embeddings for modeling multi-relational data. in advances in neural information
processing systems, pp. 2787   2795, 2013.

boyd, stephen, parikh, neal, chu, eric, peleato, borja, and eckstein, jonathan. distributed opti-
mization and statistical learning via the alternating direction method of multipliers. foundations
and trends in machine learning, 3(1):1   122, 2011.

collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k., and kuksa, p. natural language
processing (almost) from scratch. journal of machine learning research, 12:2493   2537, 2011.

cruse, alan d. lexical semantics. cambridge univ. press, 1986.

deerwester, scott, dumais, susan t., furnas, george w., landauer, thomas k., and harshman,
richard. indexing by latent semantic analysis. journal of the american society for information
science, 41(6), 1990.

faruqui, manaal, dodge, jesse, jauhar, sujay kumar, dyer, chris, hovy, eduard h., and smith,
noah a. retro   tting word vectors to semantic lexicons. corr, abs/1411.4166, 2014. url
http://arxiv.org/abs/1411.4166.

gabrilovich, e. and markovitch, s. computing semantic relatedness using wikipedia-based explicit

semantic analysis. in ijcai, 2007.

gutmann, michael and hyv  arinen, aapo. noise-contrastive estimation: a new estimation principle
for unnormalized statistical models. in international conference on arti   cial intelligence and
statistics, pp. 297   304, 2010.

harris, zellig. distributional structure. word, 10(23):146   162, 1954.

jurgens, david a, turney, peter d, mohammad, saif m, and holyoak, keith j. semeval-2012 task
2: measuring degrees of relational similarity. in proceedings of the first joint conference on
lexical and computational semantics, pp. 356   364. association for computational linguistics,
2012.

koo, terry, carreras, xavier, and collins, michael. simple semi-supervised id33. in
proceedings of acl-08: hlt, pp. 595   603, columbus, ohio, june 2008. association for compu-
tational linguistics. url http://www.aclweb.org/anthology/p/p08/p08-1068.

leacock, claudia and chodorow, martin. combining local context and id138 similarity for word

sense identi   cation. id138: an electronic lexical database, pp. 265   283, 1998.

mikolov, tomas, yih, wen-tau, and zweig, geoffrey. linguistic regularities in continuous space
in proceedings of the 2013 conference of the north american chapter
word representations.
of the association for computational linguistics: human language technologies, pp. 746   751,
atlanta, georgia, june 2013a. association for computational linguistics. url http://www.
aclweb.org/anthology/n13-1090.

mikolov, tom  a  s, sutskever, ilya, chen, kai, corrado, greg, and dean, jeffrey. distributed repre-

sentations of words and phrases and their compositionality. in nips, 2013b.

miller, george a. id138: a lexical database for english. communications of the acm, 38(11):

39   41, 1995.

mitchell, jeff and lapata, mirella. vector-based models of semantic composition.

236   244, 2008.

in acl, pp.

mnih, andriy and kavukcuoglu, koray.

learning id27s ef   ciently with noise-
contrastive estimation. in advances in neural information processing systems 26 (nips 2013),
2013.

11

petrov, slav and mcdonald, ryan. overview of the 2012 shared task on parsing the web. in notes

of the first workshop on syntactic analysis of non-canonical language (sancl), 2012.

schwenk, holger. continuous space language models. computer speech and language, 21(3):
issn 0885-2308. doi: 10.1016/j.csl.2006.09.003. url http://dx.

492   518, july 2007.
doi.org/10.1016/j.csl.2006.09.003.

smith, noah a and eisner, jason. contrastive estimation: training id148 on unlabeled
data. in proceedings of the 43rd annual meeting of the association for computational linguistics
(acl), pp. 354   362. association for computational linguistics, 2005.

socher, richard, bauer, john, manning, christopher d., and ng, andrew y. parsing with com-
positional vector grammars. in proceedings of the 51st annual meeting of the association for
computational linguistics (volume 1: long papers), pp. 455   465, so   a, bulgaria, august 2013a.
association for computational linguistics. url http://www.aclweb.org/anthology/
p13-1045.

socher, richard, chen, danqi, manning, christopher d., and ng, andrew y. reasoning with neural

tensor networks for knowledge base completion. in nips, 2013b.

turian, joseph, ratinov, lev-arie, and bengio, yoshua. word representations: a simple and general
method for semi-supervise learning. in proceedings of the 48th annual meeting of the associ-
ation for computational linguistics, pp. 384   394, uppsala, sweden, july 2010. association for
computational linguistics. url http://www.aclweb.org/anthology/p10-1040.

wang, mengqiu and manning, christopher d. effect of non-linear deep architecture in sequence
labeling. in proceedings of the sixth international joint conference on natural language pro-
cessing, pp. 1285   1291, nagoya, japan, october 2013. asian federation of natural language
processing. url http://www.aclweb.org/anthology/i13-1183.

wu, xianchao, zhou, jie, sun, yu, liu, zhanyi, yu, dianhai, wu, hua, and wang, haifeng. gen-
eralization of words for chinese id33. in proceedings of the 13th international
conference on parsing technologies (iwpt), 2013.

xu, chang, bai, yanlong, bian, jiang, gao, bin, wang, gang, liu, xiaoguang, and liu, tie-yan.
rc-net: a general framework for incorporating knowledge into word representations. in proceed-
ings of the acm conference on information and knowledge management (cikm), 2014.

yu, mo and dredze, mark. improving lexical embeddings with semantic knowledge. in proceed-
ings of the 52nd annual meeting of the association for computational linguistics (volume 2:
short papers), pp. 545   550, baltimore, maryland, june 2014. association for computational
linguistics. url http://www.aclweb.org/anthology/p14-2089.

zhila, alisa, yih, wen-tau, meek, christopher, zweig, geoffrey, and mikolov, tomas. combining
heterogeneous models for measuring relational similarity. in naacl-hlt, pp. 1000   1009, 2013.

12

