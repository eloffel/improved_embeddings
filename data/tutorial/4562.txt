   searn: search-based id170

   searn ([1]searn.hal3.name) is a generic algorithm for solving
   id170 problems. this page contains papers, software and
   notes about using searn for solving a variety of problems. feel free to
   contact me at hal3 dot name with questions or comments.

    papers

     * practical structured learning techniques for natural language
       processing. [2]hal daum   iii. phd thesis, 2006 (usc).
       this thesis describes an algorithm for solving many of the complex
       prediction problems encountered in nlp applications. the algorithm
       comes with strong theoretical guarentees, is empirically effective
       in applications such as ie and summarization, is efficient and is
       easy to implement.
       [ [3]bib | [4]pdf | [5]ps | [6]html ]
     * search-based id170. [7]hal daum   iii, [8]john
       langford and [9]daniel marcu. submitted to machine learning
       journal, 2006.
       we present searn (for "search-learn"), an algorithm for decomposing
       id170 problems into a set of classification
       problems solvable by standard classification methods. searn works
       for any id168 and any underlying classifier. our algorithm
       satisfies a desirable reduction guarantee: good performance on
       binary prediction problems implies good performance on the original
       problem. searn is also efficient whenever the id168 is
       efficiently approximately optimizable. we test searn and show
       state-of-the-art performance on several diverse structured
       prediction problems.
       [ [10]bib | [11]pdf | [12]ps ]
     * searn in practice. [13]hal daum   iii, [14]john langford and
       [15]daniel marcu. unpublished, 2006.
       we recently introduced an algorithm, searn, for solving hard
       id170 problems. this algorithm enjoys many nice
       properties: efficiency, wide applicability, theoretical
       justification and simplicity. however, under a desire to fit a lot
       of information into the original paper, it may not be so clear how
       simple the technique is. this report is designed to showcase how
       searn can be applied to a wide variety of techniques and what
       really goes on behind the scenes. we will make use of three example
       problems, ranging from simple to complex. these are: (1) sequence
       labeling, (2) parsing and (3) machine translation. (these were
       chosen to be as widely understandable, especially in the nlp
       community, as possible.) in the end, we will come back to discuss
       searn for general problems.
       [ [16]bib | [17]pdf | [18]ps ]

    implementation

   i'm releasing a very simply and stripped down implementation of searn
   (limited to sequence labeling with hamming loss) that should answer
   some questions people have been asking. you can download it here:
   [19]simplesearn.tgz (requires perl).

   i'm also releasing a much more featureful implementation of searn that
   can accomodate pretty much any id170 task. you can
   download it here: [20]searnshell_v0.1.tgz (requires perl). the cool
   thing about searnshell is that you provide your own code for doing
   search and computing features and it automatically connects this to a
   base learning algorithm like megam or libid166.

    frequently asked questions

   here, i'll attempt to answer some questions that i've either been asked
   by email ( me at hal3 dot name ) or in person.

   q: in the searn training algorithm, could you tell me in which step we
   should perform the search process using a kind of search algorithm,
   such as greedy search or id125.
   a: essentially never. i like to think of searn as follow. we have a
   id170 problem, which entails solving an argmax during
   prediction. this argmax is intractable. so, normally, we will use some
   search algorithm to approximate it (beam or greedy or...). whatever
   search algorithm we apply, the search is performed by making a bunch of
   sequential decisions (each decision is a step is the search process).
   the idea behind searn is that instead of learning some sort of global
   model and then searching (as is standard), instead we will simply learn
   a classifier to make each of these decisions optimally. the question
   then becomes how to train such a classifier, and searn is one possible
   solution. in the end, there is no search going on. all you're doing is
   making a bunch of sequential decisions, as if you were performing
   search, but you aren't actually searching. so there is nowhere in the
   training or prediction algorithm where you will run some search
   algorithm.

   q: the searn algorithm takes an optimal policy as input. can i believe
   that actually the optimal policy is an initial classifier? could you
   tell me how to construct the initial classifier?
   a: when you think about searn as described in response to the previous
   question, you see that training a classifier to make incremental
   decisions requires that we get labeled training data for incremental
   decisions. this is essentially where the optimal policy comes in. the
   optimal policy tells us: for a given input (eg., sentence), true output
   (eg., part of speech tags for this sentence) and some location in the
   search space (eg., after 3 words have been tagged), what is the best
   action to take. importantly, this is based on having access to the true
   output sequence. for instance, in sequence labeling, under hamming
   loss, the optimal policy will always choose simply to label the next
   word with its correct label.

   the optimal policy is not a classifier, in the sense that it is not
   learned. it is, essentially, exactly what we want our classifiers to
   learn, since it always makes correct choices. it is up to us as the
   algorithm designer to come up with a way of computing the optimal
   policy on the basis of the true output. essentially, what we need to do
   is to find some way of structuring the search space so that computing
   an optimal policy for our given id168 is easy. i don't know a
   magic method for constructing it, but if you look at the examples for
   sequence labeling, parsing and machine translation, hopefully that will
   give some sort of idea.

    more to come...
     __________________________________________________________________

   last updated 5 may 2006
     __________________________________________________________________

references

   1. http://searn.hal3.name/
   2. http://hal3.name/
   3. http://hal3.name/docs/daume06thesis.bib
   4. http://hal3.name/docs/daume06thesis.pdf
   5. http://hal3.name/docs/daume06thesis.ps
   6. http://hal3.name/thesis.html
   7. http://hal3.name/
   8. http://hunch.net/~jl
   9. http://www.isi.edu/~marcu/
  10. http://hal3.name/docs/daume06searn.bib
  11. http://hal3.name/docs/daume06searn.pdf
  12. http://hal3.name/docs/daume06searn.ps
  13. http://hal3.name/
  14. http://hunch.net/~jl
  15. http://www.isi.edu/~marcu/
  16. http://hal3.name/docs/daume06searn-practice.bib
  17. http://hal3.name/docs/daume06searn-practice.pdf
  18. http://hal3.name/docs/daume06searn-practice.ps
  19. http://users.umiacs.umd.edu/~hal/searn/simplesearn.tgz
  20. http://users.umiacs.umd.edu/~hal/searn/searnshell_v0.1.tgz
