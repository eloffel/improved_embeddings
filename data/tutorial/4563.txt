   #[1]github [2]recent commits to python-machine-learning-book:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]800
     * [35]star [36]9,692
     * [37]fork [38]3,687

[39]rasbt/[40]python-machine-learning-book

   [41]code [42]issues 5 [43]pull requests 1 [44]projects 0 [45]insights
   [46]permalink
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [47]sign up
   branch: master
   [48]python-machine-learning-book/[49]faq/closed-form-vs-gd.md
   [50]find file copy path
   fetching contributors   
   cannot retrieve contributors at this time
   151 lines (65 sloc) 7.27 kb
   [51]raw [52]blame [53]history
   (button) (button)

fitting a model via closed-form equations vs. id119 vs stochastic
id119 vs mini-batch learning. what is the difference?

   in order to explain the differences between alternative approaches to
   estimating the parameters of a model, let's take a look at a concrete
   example: ordinary least squares (ols) id75. the
   illustration below shall serve as a quick reminder to recall the
   different components of a simple id75 model:

   in ordinary least squares (ols) id75, our goal is to find
   the line (or hyperplane) that minimizes the vertical offsets. or, in
   other words, we define the best-fitting line as the line that minimizes
   the sum of squared errors (sse) or mean squared error (mse) between our
   target variable (y) and our predicted output over all samples i in our
   dataset of size n.

   now, we can implement a id75 model for performing ordinary
   least squares regression using one of the following approaches:
     * solving the model parameters analytically (closed-form equations)
     * using an optimization algorithm (id119, stochastic
       id119, newton's method, simplex method, etc.)

1) normal equations (closed-form solution)

   the closed-form solution may (should) be preferred for "smaller"
   datasets -- if computing (a "costly") matrix inverse is not a concern.
   for very large datasets, or datasets where the inverse of x^tx may not
   exist (the matrix is non-invertible or singular, e.g., in case of
   perfect multicollinearity), the gd or sgd approaches are to be
   preferred. the linear function (id75 model) is defined as:

   where y is the response variable, x is an m-dimensional sample vector,
   and w is the weight vector (vector of coefficients). note that w[0]
   represents the y-axis intercept of the model and therefore x[0]=1.
   using the closed-form solution (normal equation), we compute the
   weights of the model as follows:

2) id119 (gd)

   using the gradient decent (gd) optimization algorithm, the weights are
   updated incrementally after each epoch (= pass over the training
   dataset).

   the cost function j(   ), the sum of squared errors (sse), can be written
   as:

   the magnitude and direction of the weight update is computed by taking
   a step in the opposite direction of the cost gradient

   where    is the learning rate. the weights are then updated after each
   epoch via the following update rule:

   where   w is a vector that contains the weight updates of each weight
   coefficient w, which are computed as follows:

   essentially, we can picture gd optimization as a hiker (the weight
   coefficient) who wants to climb down a mountain (cost function) into a
   valley (cost minimum), and each step is determined by the steepness of
   the slope (gradient) and the leg length of the hiker (learning rate).
   considering a cost function with only a single weight coefficient, we
   can illustrate this concept as follows:

3) stochastic id119 (sgd)

   in gd optimization, we compute the cost gradient based on the complete
   training set; hence, we sometimes also call it batch gd. in case of
   very large datasets, using gd can be quite costly since we are only
   taking a single step for one pass over the training set -- thus, the
   larger the training set, the slower our algorithm updates the weights
   and the longer it may take until it converges to the global cost
   minimum (note that the sse cost function is convex).

   in stochastic id119 (sgd; sometimes also referred to as
   iterative or on-line gd), we don't accumulate the weight updates as
   we've seen above for gd:

   instead, we update the weights after each training sample:

   here, the term "stochastic" comes from the fact that the gradient based
   on a single training sample is a "stochastic approximation" of the
   "true" cost gradient. due to its stochastic nature, the path towards
   the global cost minimum is not "direct" as in gd, but may go "zig-zag"
   if we are visualizing the cost surface in a 2d space. however, it has
   been shown that sgd almost surely converges to the global cost minimum
   if the cost function is convex (or pseudo-convex)[1]. furthermore,
   there are different tricks to improve the gd-based learning, for
   example:
     * an adaptive learning rate    choosing a decrease constant d that
       shrinks the learning rate over time:
     * momentum learning by adding a factor of the previous gradient to
       the weight update for faster updates:

a note about shuffling

   there are several different flavors of sgd, which can be all seen
   throughout the literature. let's take a look at the three most common
   variants:

a)

     * randomly shuffle samples in the training set
          + for one or more epochs, or until approx. cost minimum is
            reached
               o for training sample i
                    # compute gradients and perform weight updates

b)

     * for one or more epochs, or until approx. cost minimum is reached
          + randomly shuffle samples in the training set
               o for training sample i
                    # compute gradients and perform weight updates

c)

     * for iterations t, or until approx. cost minimum is reached:
          + draw random sample from the training set
               o compute gradients and perform weight updates

   in scenario a [3], we shuffle the training set only one time in the
   beginning; whereas in scenario b, we shuffle the training set after
   each epoch to prevent repeating update cycles. in both scenario a and
   scenario b, each training sample is only used once per epoch to update
   the model weights.

   in scenario c, we draw the training samples randomly with replacement
   from the training set [2]. if the number of iterations t is equal to
   the number of training samples, we learn the model based on a bootstrap
   sample of the training set.

4) mini-batch id119 (mb-gd)

   mini-batch id119 (mb-gd) a compromise between batch gd and
   sgd. in mb-gd, we update the model based on smaller groups of training
   samples; instead of computing the gradient from 1 sample (sgd) or all n
   training samples (gd), we compute the gradient from 1 < k < n training
   samples (a common mini-batch size is k=50).

   mb-gd converges in fewer iterations than gd because we update the
   weights more frequently; however, mb-gd let's us utilize vectorized
   operation, which typically results in a computational performance gain
   over sgd.

references

     * [1] bottou, l  on (1998). "online algorithms and stochastic
       approximations". online learning and neural networks. cambridge
       university press. isbn 978-0-521-65263-6
     * [2] bottou, l  on. "large-scale machine learning with sgd."
       proceedings of compstat'2010. physica-verlag hd, 2010. 177-186.
     * [3] bottou, l  on. "sgd tricks." neural networks: tricks of the
       trade. springer berlin heidelberg, 2012. 421-436.

   ____________________ (button) go

     *    2019 github, inc.
     * [54]terms
     * [55]privacy
     * [56]security
     * [57]status
     * [58]help

     * [59]contact github
     * [60]pricing
     * [61]api
     * [62]training
     * [63]blog
     * [64]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [65]reload to refresh your
   session. you signed out in another tab or window. [66]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/rasbt/python-machine-learning-book/commits/master.atom
   3. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md
  32. https://github.com/join
  33. https://github.com/login?return_to=/rasbt/python-machine-learning-book
  34. https://github.com/rasbt/python-machine-learning-book/watchers
  35. https://github.com/login?return_to=/rasbt/python-machine-learning-book
  36. https://github.com/rasbt/python-machine-learning-book/stargazers
  37. https://github.com/login?return_to=/rasbt/python-machine-learning-book
  38. https://github.com/rasbt/python-machine-learning-book/network/members
  39. https://github.com/rasbt
  40. https://github.com/rasbt/python-machine-learning-book
  41. https://github.com/rasbt/python-machine-learning-book
  42. https://github.com/rasbt/python-machine-learning-book/issues
  43. https://github.com/rasbt/python-machine-learning-book/pulls
  44. https://github.com/rasbt/python-machine-learning-book/projects
  45. https://github.com/rasbt/python-machine-learning-book/pulse
  46. https://github.com/rasbt/python-machine-learning-book/blob/23853c58a3279a95048245cc8924a48f6316d915/faq/closed-form-vs-gd.md
  47. https://github.com/join?source=prompt-blob-show
  48. https://github.com/rasbt/python-machine-learning-book
  49. https://github.com/rasbt/python-machine-learning-book/tree/master/faq
  50. https://github.com/rasbt/python-machine-learning-book/find/master
  51. https://github.com/rasbt/python-machine-learning-book/raw/master/faq/closed-form-vs-gd.md
  52. https://github.com/rasbt/python-machine-learning-book/blame/master/faq/closed-form-vs-gd.md
  53. https://github.com/rasbt/python-machine-learning-book/commits/master/faq/closed-form-vs-gd.md
  54. https://github.com/site/terms
  55. https://github.com/site/privacy
  56. https://github.com/security
  57. https://githubstatus.com/
  58. https://help.github.com/
  59. https://github.com/contact
  60. https://github.com/pricing
  61. https://developer.github.com/
  62. https://training.github.com/
  63. https://github.blog/
  64. https://github.com/about
  65. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md
  66. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md

   hidden links:
  68. https://github.com/
  69. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md
  70. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md
  71. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md
  72. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#fitting-a-model-via-closed-form-equations-vs-gradient-descent-vs-stochastic-gradient-descent-vs-mini-batch-learning-what-is-the-difference
  73. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/simple_regression.png
  74. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/sse_mse.png
  75. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#1-normal-equations-closed-form-solution
  76. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/linear_model.png
  77. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/closed-form.png
  78. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#2-gradient-descent-gd
  79. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/j.png
  80. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/dw.png
  81. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/w_upd.png
  82. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/w_upd_expl.png
  83. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/ball.png
  84. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#3-stochastic-gradient-descent-sgd
  85. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/iter_gd.png
  86. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/iter_sgd.png
  87. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/adaptive_learning.png
  88. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd/decrease_const.png
  89. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#a-note-about-shuffling
  90. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#a
  91. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#b
  92. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#c
  93. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#4-mini-batch-gradient-descent-mb-gd
  94. https://github.com/rasbt/python-machine-learning-book/blob/master/faq/closed-form-vs-gd.md#references
  95. https://github.com/
