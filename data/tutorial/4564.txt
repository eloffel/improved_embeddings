   #[1]adit deshpande - cs undergrad at ucla ('19)

   [2][pic.jpg]

[3]adit deshpande

   cs undergrad at ucla ('19)

   [4]blog [5]about [6]github [7]projects [8]resume

how i used deep learning to train a chatbot to talk like me (sorta)

   [cover9th.png]

introduction

                     chatbots are    computer programs which conduct
   conversation through auditory or textual methods   . apple   s [9]siri,
   microsoft   s [10]cortana, [11]google assistant, and amazon   s [12]alexa
   are four of the most popular conversational agents today. they can help
   you get directions, check the scores of sports games, call people in
   your address book, and can [13]accidently make you order a $170
   dollhouse.

   these products all have auditory interfaces where the agent converses
   with you through audio messages. in this post, we   ll be looking more at
   chatbots that operate solely on the textual front. facebook has been
   [14]heavily investing in fb messenger bots, which allow small
   businesses and organizations to create bots to help with customer
   support and frequently asked questions. chatbots have been around for a
   decent amount of time (siri released in 2011), but only recently has
   deep learning been the go-to approach to the task of creating realistic
   and effective chatbot interaction.

   in this post, we   ll be looking at how we can use a deep learning model
   to train a chatbot on my past social media conversations in hope of
   getting the chatbot to respond to messages the way that i would.

problem space

                     from a high level, the job of a chatbot is to be able
   to determine the best response for any given message that it receives.
   this    best    response should either (1) answer the sender   s question,
   (2) give the sender relevant information, (3) ask follow-up questions,
   or (4) continue the conversation in a realistic way. this is a pretty
   tall order. the chatbot needs to be able to understand the intentions
   of the sender   s message, determine what type of response message (a
   follow-up question, direct response, etc.) is required, and follow
   correct grammatical and lexical rules while forming the response.

   it   s safe to say that modern chatbots have trouble accomplishing all
   these tasks. for all the progress we have made in the field, we too
   often get chatbot experiences like this.

     "oh no! artificial intelligence is totally going to take over the
     world! because deep learning models neurons!"
     [15]pic.twitter.com/5uepel4k6h
         reza zadeh (@reza_zadeh) [16]august 17, 2016

   chatbots are too often not able to understand our intentions, have
   trouble getting us the correct information, and are sometimes just
   exasperatingly difficult to deal with. as we   ll see in this post, deep
   learning is one of the most effective methods in tackling this tough
   task.

deep learning approach

                     chatbots that use deep learning are almost all using
   some variant of a sequence to sequence (id195) model. in 2014, ilya
   sutskever, oriol vinyals, and quoc le published the seminal work in
   this field with a [17]paper called    sequence to sequence learning with
   neural networks   . this paper showed great results in machine
   translation specifically, but id195 models have grown to encompass a
   variety of nlp tasks.
   [chatbot1.png]

   a sequence to sequence model is composed of 2 main components, an
   encoder id56 and a decoder id56 (if you   re a little shaky on id56s, check
   out my[18] previous blog post for a refresher). from a high level, the
   encoder   s job is to encapsulate the information of the input text into
   a fixed representation. the decoder   s is to take that representation,
   and generate a variable length text that best responds to it.
   [chatbot2.png]

   let   s look at how this works at a more detailed level. as you remember,
   an id56 contains a number of hidden state vectors, which each represent
   information from the previous time steps. for example, the hidden state
   vector at the 3^rd time step will be a function of the first 3 words.
   by this logic, the final hidden state vector of the encoder id56 can be
   thought of as a pretty accurate representation of the whole input text.

   the decoder is another id56, which takes in the final hidden state
   vector of the encoder and uses it to predict the words of the output
   reply. let's look at the first cell. the cell's job is to take in the
   vector representation v, and decide which word in its vocabulary is the
   most appropriate for the output response. mathematically speaking, this
   means that we compute probabilities for each of the words in the
   vocabulary, and choose the argmax of the values.

   the 2nd cell will be a function of both the vector representation v, as
   well as the output of the previous cell. the goal of the lstm is to
   estimate the following id155.
   [chatbot3.png]

   let's deconstruct what that equation means. the left side refers to the
   id203 of the output sequence, conditioned on the given input
   sequence. the right side contains the term p(y[t]|v, y[1],    , y[t-1]),
   which is a vector of probabilities of all the words, conditioned on the
   vector representation and the outputs at the previous time steps. the
   pi notation is simply the multiplication equivalent of sigma (or
   summation). the right hand side can be reduced to p(y[1]|v) * p(y[2]|v,
   y[1]) * p(y[3]|v, y[1, ]y[2]) ... and so on.

   let   s go over a quick example before moving on. let   s take the input
   text we saw in the first image. given the phrase    are you free
   tomorrow?   , let   s think about how most people would answer the
   question. a majority will start with something along the lines of
      yes   ,    yeah   ,    no   , etc. after we   re done training our network, the
   id203 p(y[1]|v) will be a distribution that looks like the
   following.
   [chatbot4.png]

   the second id203 we need to compute, p(y[2]|v, y[1]), will be a
   function of the word this distribution y[1 ]as well as the vector
   representation v. the result of the pi (product) operation will give us
   the most likely sequence of words, which we   ll use as our final
   response.

   one of the most important characteristics of sequence to sequence
   models is the versatility that it provides. when you think of
   traditional ml methods (id75, id166s) and deep learning
   methods like id98s, these models require a fixed size input, and produce
   fixed size outputs as well. the lengths of your inputs must be known
   beforehand. this is a significant limitation to tasks such as machine
   translation, id103, and id53. these are
   tasks where we don't know the size of the input phrase, and we'd also
   like to be able to generate variable length responses, not just be
   constrained to one particular output representation. id195 models
   allow for that flexibility.

   the id195 model has seen numerous improvements since 2014, and you
   can head to the    interesting papers    section of this post to read more
   about them.

dataset selection

   when thinking about applying machine learning to any sort of task, one
   of the first things we need to do is consider the type of dataset that
   we would need to train the model. for sequence to sequence models, we
   need a large number of conversation logs. from a high level, this
   encoder decoder network needs to be able to understand the type of
   responses (decoder outputs) that are expected for every query (encoder
   inputs).  some common datasets are the [19]cornell movie dialog corpus,
   the [20]ubuntu corpus, and [21]microsoft   s social media conversation
   corpus.

   while most people train chatbots to answer company specific information
   or to provide some sort of service, i was more interested in a bit more
   of a fun application. with this particular post, i wanted to see
   whether i could use conversation logs from my own life to train a
   id195 model that learns to respond to messages the way that i would.

where's the data coming from?

   [chatbot5.png]

                     alright id48, let   s see how we can do this. we need to
   create a large dataset of conversations that i   ve had with people
   online. over the course of my time on social media, i   ve used facebook,
   google hangouts, sms, linkedin, twitter, tinder, and slack to stay in
   touch with people.
     * facebook: this is where the bulk of the training data will come
       from. facebook has a [22]cool feature that allows you to download a
       copy of all of your facebook data. this download will contain all
       your messages, your photos, and your all-caps, cringe filled
       statuses that you wrote as a middle schooler.
     * google hangouts: i definitely used this a lot with a close set of
       friends during high school. you can extract of your chat data by
       following the instructions on this [23]fantastic blog post.
     * sms/texting: pretty sure there is a way to get an archive of all
       prior chats (sms backup+ is a good app), but i rarely use text
       anyway, so don   t think it   ll be worth the effort.
     * linkedin: linkedin does provide a tool to get an archive of your
       data [24]here.
     * twitter: not enough private messages for this to be useful.
     * tinder: ummm, yeah let   s just say that these conversations are not
       dataset worthy .
     * slack: just recently started using this, and only having a couple
       private messages, so just planning to manually copy over the
       convos.

dataset creation

                     a big part of machine learning involves dataset
   preprocessing. the data archives from each of these sources comes
   differently formatted, and contains parts that we don   t really need
   (the pictures section of our fb data for example).
   [chatbot6.png]

   as you can see, the hangouts data is formatted a bit differently from
   the facebook data, and the linkedin messages are in a csv format. our
   goal, with all these datasets, is to just create one unified file that
   contains pairs in the form of (friends_message, your_response).

   to do that, i wrote a python script that you can check out [25]here.
   this script will create two different files. one will be a numpy object
   (conversationdictionary.npy) that contains all of the input output
   pairs. the other will be a large txt file (conversationdata.txt) that
   contains these pairs in sentence form, one after the other. normally, i
   love being able to share datasets, but for this specific one, i   m
   keeping it private just because it has a lot of private conversations,
   and i don   t think my friends would be happy if they were just floating
   around on the internet. but here   s a snapshot of how the final dataset
   looks like.
   [chatbot7.png]

word vectors

                     lol. lmao. wtf. these are all words that showed up
   quite frequently in our conversation data file. while they are common
   in the realm of social media, they aren   t in a lot of traditional
   datasets. normally, my first instinct when approaching any nlp task is
   to simply use pre-trained vectors, as they are trained on large
   corpuses for a large number of iterations. however, given that we have
   so many words and acronyms that aren   t in typical pre-trained word
   vector lists, generating our own word vectors is critical to making
   sure that the words get represented properly.

   to generate word vectors, we use the classic approach of a id97
   model. the basic idea is that the model creates word vectors by looking
   at the context with which words appear in sentences. words with similar
   contexts will be placed close together in the vector space. for a more
   detailed overview of how a id97 model is created and trained, check
   out this great [26]blog post by one of my good friends, rohan varma.

   i trained the id97 model in this python script [27]here, which
   saves the word vectors in a numpy object.

   **update: i later learned that the tensorflow id195 function trains
   id27s from scratch, so i don   t end up using these word
   vectors, but it was still good practice **

creating a id195 model with tensorflow

                     now that we   ve created the dataset and generated our
   word vectors, we can move on to coding the id195 model. i created and
   trained the model in this python [28]script. i   ve tried to comment the
   code to the best of my ability, so hopefully you can follow along. the
   crux of the model lies in tensorflow   s embedding_id56_id195()
   function. you can find documentation for it [29]here.

tracking the training progress

   [chatbot8.png]

                     one of the interesting aspects of this project was
   getting a chance to look at how the responses changed as the network
   trained. at different points in the training loop, i tested the network
   on an input string, and outputted all of the non-pad and non-eos tokens
   in the output. at first, you can see that the responses were mainly
   blank, as the network repeatedly outputted padding and eos tokens. this
   is normal since padding tokens are by far the most frequent token in
   the whole dataset. then, you can see that the network starts to output
      lol    for every single input string it is given. this makes sense
   intuitively since    lol    gets used so often these days that it kind of
   is an acceptable response to anything. slowly, you start to see more
   complete thoughts and grammatical structure come up in the responses.
   could be due to a bit of overfitting as well.

setting up the facebook messenger chatbot

                     now that we have a decently trained id195 model,
   let   s look at how to set up a simple fb messenger chatbot. the process
   was not too difficult, as it took me a little less than 30 minutes by
   following all the steps on this [30]great tutorial. the basic idea is
   that we set up a server using a simple express app, host it on heroku,
   and then set up a facebook app/page to connect to it. won   t go into too
   much detail beyond that, since i really thought that the author did a
   great job of explaining everything step by step, but at the end, you
   should have a facebook app like this.
   [chatbot9.png]

   and you should be able to message your bot (this initial behavior is
   just echoing everything it gets sent).
   [chatbot10.png]

deploying our trained tensorflow model

                     so, now it   s time to put everything together. since i
   haven   t found a good interface between tensorflow and node (don   t know
   if there   s an officially supported wrapper), i decided to deploy my
   model using a flask server, and have the chatbot   s express app interact
   with it.

   you can check out the flask server code [31]here and the chatbot   s
   index.js file [32]here.

testing it out!

                     if you   d like to chat with this bot, just go ahead
   and go to this [33]link or go to this [34]facebook page and hit the
   send message button. it might take a while to respond for the first
   time, since the server needs to start up.

   update 6/13/18: fb changed some of their settings with bots so i am not
   sure if the messenger bot works. however, you can also send post
   requests to the server using curl in your command prompt. basically, go
   to your terminal/cmd prompt and type in:

                     curl -i -x post -h 'content-type: application/json'
   -d '{"message": "insert your message here"}'
   https://flask-server-id195-chatbot.herokuapp.com/prediction

   the response will be the answer that my trained chatbot would send.

    it   s probably difficult to judge whether or not the bot actually does
   talk like me (since not a lot of you have talked to me online lol), but
   i   d say it   s doing alright! the grammar is passable, considering social
   media standards. you can cherry pick a couple good results, but most
   are pretty nonsensical. here are some of the ones which help me sleep
   better at night because skynet definitely isn   t happening any time
   soon.
   [chatbot11.png]

   i thought the first one was especially funny because    juju green   
   actually seems to be a combination of [35]juju smith-schuster, a
   steelers wide receiver, and [36]draymond green who is a forward for the
   golden state warriors. interesting combination.

   okay let's be real. the performance is not that great right now to say
   the least. let's think about ways to improve it though!

ways to improve

                     as you can probably tell from interacting with the
   chatbot, there is definitely room for a lot of improvement. after a
   couple of messages, it quickly becomes clear that having a sustained
   conversation simply just isn   t possible. the chabtot iisn   t able to
   connect thoughts together, and some of the responses seem random and
   incoherent. here are some ways that could improve our chatbot   s
   performance.
     * incorporate other datasets to help the network learn from a larger
       conversation corpus. this would remove a bit of the
       "individualness" of the chatbot since it's strictly trained on my
       own conversations right now. however, i believe it would help
       generate more realistic conversations.
     * handling scenarios where the encoder message has nothing to do with
       what the decoder message is. example is when one conversation ends,
       and you start a new one the next day. the topic of conversation
       could be completely unrelated. this could affect the model's
       training.
     * using bidirectional lstms, attention mechanisms, and bucketing.
     * tuning hyperparameters such as number of lstm units, number of lstm
       layers, choice of optimizer, number of training iterations, etc.

   would be curious to hear other suggestions in the comments too!

how you can build your own

                     if you   ve been following along, you should have a
   general idea of what   s needed to create a chatbot that talks like you.
   let   s go over the steps one final time. detailed instructions are
   available in the github repo [37]readme.
    1. find all the online social media sites in which you   ve had a
       conversation with someone, and download a copy of your data.
    2. extract all the (message, response) pairs with createdataset.py or
       your own script.
    3. (optional) generate word vectors for each of the words that show up
       in our conversations through id97.py.
    4. create, train, and save the sequence to sequence model in
       id195.py.
    5. create the facebook chatbot.
    6. create a flask server where you deploy the saved id195 model.
    7. edit index.js file in your express app so it can communicate with
       the flask server.

interesting papers

     * [38]sequence to sequence learning
     * [39]id195 with attention
     * [40]neural conversational model
     * [41]generative id187
     * [42]persona based model
     * [43]deep rl for dialogue generation
     * [44]attention with intention
     * [45]diversity promoting objective functions
     * [46]copying mechanisms in id195

other helpful posts

     * great [47]blog post on id195
     * [48]slides from a tensorflow course at stanford
     * tensorflow id195 [49]documentation
     * helpful [50]video tutorial on using tensorflow   s id195 functions

   shout out to [51]amit tallapragada, [52]arvind sankar, and [53]neil
   chen for helping me out with flask and javascript stuff.

   dueces.
   [54]sources

   [55]tweet

   written on july 25, 2017

   please enable javascript to view the [56]comments powered by disqus.

references

   visible links
   1. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
   2. https://adeshpande3.github.io/adeshpande3.github.io/
   3. https://adeshpande3.github.io/adeshpande3.github.io/
   4. https://adeshpande3.github.io/adeshpande3.github.io/
   5. https://adeshpande3.github.io/adeshpande3.github.io/about
   6. https://github.com/adeshpande3
   7. https://adeshpande3.github.io/adeshpande3.github.io/projects
   8. https://adeshpande3.github.io/adeshpande3.github.io/resume.pdf
   9. https://www.apple.com/ios/siri/
  10. https://www.microsoft.com/en-us/windows/cortana
  11. https://assistant.google.com/
  12. https://developer.amazon.com/alexa
  13. https://qz.com/880541/amazons-amzn-alexa-accidentally-ordered-a-ton-of-dollhouses-across-san-diego/
  14. https://techcrunch.com/2017/04/18/facebook-bot-discovery/
  15. https://t.co/5uepel4k6h
  16. https://twitter.com/reza_zadeh/status/765722701465948160
  17. https://arxiv.org/pdf/1409.3215.pdf
  18. https://adeshpande3.github.io/adeshpande3.github.io/deep-learning-research-review-week-3-natural-language-processing
  19. https://www.cs.cornell.edu/~cristian/cornell_movie-dialogs_corpus.html
  20. http://dataset.cs.mcgill.ca/ubuntu-corpus-1.0/
  21. https://www.microsoft.com/en-us/download/details.aspx?id=52375&from=http://research.microsoft.com/en-us/downloads/6096d3da-0c3b-42fa-a480-646929aa06f1/
  22. https://www.facebook.com/help/131112897028467
  23. https://blog.jay2k1.com/2014/11/10/how-to-export-and-backup-your-google-hangouts-chat-history/
  24. https://www.linkedin.com/psettings/member-data
  25. https://github.com/adeshpande3/facebook-messenger-bot/blob/master/createdataset.py
  26. http://rohanvarma.me/id97/
  27. https://github.com/adeshpande3/facebook-messenger-bot/blob/master/id97.py
  28. https://github.com/adeshpande3/facebook-messenger-bot/blob/master/id195.py
  29. https://www.tensorflow.org/tutorials/id195
  30. https://github.com/jw84/messenger-bot-tutorial
  31. https://github.com/adeshpande3/chatbot-flask-server
  32. https://github.com/adeshpande3/facebook-messenger-bot/blob/master/index.js
  33. https://m.me/adits-fb-chatbot-822517037906771/
  34. https://www.facebook.com/adits-fb-chatbot-822517037906771/
  35. https://en.wikipedia.org/wiki/juju_smith-schuster
  36. https://en.wikipedia.org/wiki/draymond_green
  37. https://github.com/adeshpande3/facebook-messenger-bot/blob/master/readme.md
  38. https://arxiv.org/pdf/1409.3215.pdf
  39. https://arxiv.org/pdf/1409.0473.pdf
  40. https://arxiv.org/pdf/1506.05869.pdf
  41. https://arxiv.org/pdf/1507.04808.pdf
  42. https://arxiv.org/pdf/1603.06155.pdf
  43. https://arxiv.org/pdf/1606.01541.pdf
  44. https://arxiv.org/pdf/1510.08565.pdf
  45. https://arxiv.org/pdf/1510.03055.pdf
  46. https://arxiv.org/pdf/1603.06393.pdf
  47. http://suriyadeepan.github.io/2016-06-28-easy-id195/
  48. http://web.stanford.edu/class/cs20si/lectures/slides_13.pdf
  49. https://www.tensorflow.org/tutorials/id195
  50. https://www.youtube.com/watch?v=elmbrkymxxs
  51. https://github.com/amittallapragada
  52. https://github.com/arvindsankar
  53. https://github.com/nwchen
  54. https://adeshpande3.github.io/assets/sources9.txt
  55. https://twitter.com/share
  56. http://disqus.com/?ref_noscript

   hidden links:
  58. mailto:adeshpande3@g.ucla.edu
  59. https://www.facebook.com/adit.deshpande.5
  60. https://github.com/adeshpande3
  61. https://instagram.com/thejugglinguy
  62. https://www.linkedin.com/in/aditdeshpande
  63. https://adeshpande3.github.io/adeshpande3.github.io/feed.xml
  64. https://www.twitter.com/aditdeshpande3
