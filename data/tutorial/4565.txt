   #[1]data blogger    python deep learning tutorial: create a gru (id56) in
   tensorflow comments feed [2]alternate [3]alternate

   [4]data blogger [logo-300x300.png]

   [ ]
     * [5]write for us!
     * [6]blog
          + [7]do-it-yourself
          + [8]technology
          + [9]web technology
          + [10]data science
          + [11]software science
          + [12]mathematics
          + [13]personal projects
     * [14]english english
          + [15]english english
          + [16]nederlands nederlands
          + [17]       (      )        (      )

   gru.

python deep learning tutorial: create a gru (id56) in tensorflow

   [18]august 27, 2017november 17, 2017 [19]kevin jacobs
   [20]do-it-yourself, [21]data science, [22]software science,
   [23]personal projects

   [24]mlps (multi-layer id88s) are great for many classification
   and regression tasks. however, it is hard for mlps to do classification
   and regression on sequences. in this python deep learning tutorial, a
   gru is implemented in tensorflow. tensorflow is one of the many python
   deep learning libraries.

   by the way, another great article on machine learning is [25]this
   article on machine learning fraud detection. if you are interested in
   another article on id56s, you should definitely read [26]this article on
   the elman id56.

what is a gru or id56?

   a sequence is an ordered set of items and sequences appear everywhere.
   in the stock market, the closing price is a sequence. here, time is the
   ordering. in sentences, words follow a certain ordering. therefore,
   sentences can be viewed as sequences. a gigantic mlp could learn
   parameters based on sequences, but this would be infeasible in terms of
   computation time. the family of recurrent neural networks (id56s) solve
   this by specifying hidden states which do not only depend on the input,
   but also on the previous hidden state. grus are one of the simplest
   id56s. vanilla id56s are even simpler, but these models suffer from the
   [27]vanishing gradient problem.

mathematical gru model

   the key idea of grus is that the gradient chains do not vanish due to
   the length of sequences. this is done by allowing the model to pass
   values completely through the cells. the model is defined as the
   following [1]:
   [ins: :ins]

   $latex z_t = \sigma(w^{(z)} x_t + u^{(z)} h_{t-1} + b^{(z)})$
   $latex r_t = \sigma(w^{(r)} x_t + u^{(r)} h_{t-1} + b^{(r)})$
   $latex \tilde{h}_t = \tanh(w^{(h)} x_t + u^{(h)} h_{t-1} \circ r_t +
   b^{(h)})$
   $latex h_t = (1     z_t) \circ h_{t     1} + z_t \circ \tilde{h}_t$

   i had a hard time understanding this model, but it turns out that it is
   not too hard to understand. in the definitions, $latex \circ$ is used
   as the [28]hadamard product, which is just a fancier name for
   element-wise multiplication. $latex \sigma(x)$ is the sigmoid function
   which is defined as $latex \sigma(x) = \frac{1}{1 + e^{-x}}$. both the
   sigmoid function ($latex \sigma$) and the hyperbolic tangent function
   ($latex \tanh$) are used to squish the values between $latex 0$ and
   $latex 1$.

   $latex z_t$ functions as a filter for the previous state. if $latex
   z_t$ is low (near $latex 0$), then a lot of the previous state is
   reused! the input at the current state ($latex x_t$) does not influence
   the output a lot. if $latex z_t$ is high, then the output at the
   current step is influenced a lot by the current input ($latex x_t$),
   but it is not influenced a lot by the previous state ($latex h_{t-1}$).

   $latex r_t$ functions as forget gate (or reset gate). it allows the
   cell to forget certain parts of the state.
   [ins: :ins]

the task: adding numbers

   in the code example, a simple task is used for testing the gru. given
   two numbers $latex a$ and $latex b$, their sum is computed: $latex c =
   a + b$. the numbers are first converted to reversed bitstrings. the
   reversal is also what most people would do by adding up two numbers.
   you start at the right from the number and if the sum is larger than
   $latex 10$, you carry (memorize) a certain number. the model is capable
   of learning what to carry. as an example, consider the number $latex a
   = 3$ and $latex b = 1$. in bitstrings (of length 3), we have $latex a =
   [0, 1, 1]$ and $latex b = [0, 0, 1]$. in reversed bitstring
   representation, we have that $latex a = [1, 1, 0]$ and $latex b = [1,
   0, 0]$. the sum of these numbers is $latex c = [0, 0, 1]$ in reversed
   bitstring representation. this is $latex [1, 0, 0]$ in normal bitstring
   representation and this is equivalent to $latex 4$. these are all the
   steps which are also done by the code automatically.

the code

   the code is self-explaining. if you have any questions, feel free to
   ask! the code can also be found [29]on github. sharing (or starring) is
   caring :-)!
#%% (0) important libraries
import tensorflow as tf
import numpy as np
from numpy import random
import matplotlib.pyplot as plt
from ipython import display
% matplotlib inline

#%% (1) dataset creation.

def as_bytes(num, final_size):
    """converts an integer to a reversed bitstring (of size final_size).

    arguments
    ---------
    num: int
        the number to convert.
    final_size: int
        the length of the bitstring.

    returns
    -------
    list:
        a list which is the reversed bitstring representation of the given numbe
r.

    examples
    --------
    >>> as_bytes(3, 4)
    [1, 1, 0, 0]
    >>> as_bytes(3, 5)
    [1, 1, 0, 0, 0]
    """
    res = []
    for _ in range(final_size):
        res.append(num % 2)
        num //= 2
    return res

def generate_example(num_bits):
    """generate an example addition.

    arguments
    ---------
    num_bits: int
        the number of bits to use.

    returns
    -------
    a: list
        the first term (represented as reversed bitstring) of the addition.
    b: list
        the second term (represented as reversed bitstring) of the addition.
    c: list
        the addition (a + b) represented as reversed bitstring.

    examples
    --------
    >>> np.random.seed(4)
    >>> a, b, c = generate_example(3)
    >>> a
    [0, 1, 0]
    >>> b
    [0, 1, 0]
    >>> c
    [1, 0, 0]
    >>> # notice that these numbers are represented as reversed bitstrings)
    """
    a = random.randint(0, 2**(num_bits - 1) - 1)
    b = random.randint(0, 2**(num_bits - 1) - 1)
    res = a + b
    return (as_bytes(a,  num_bits),
            as_bytes(b,  num_bits),
            as_bytes(res,num_bits))

def generate_batch(num_bits, batch_size):
    """generates instances of the addition problem.

    arguments
    ---------
    num_bits: int
        the number of bits to use for each number.
    batch_size: int
        the number of examples to generate.

    returns
    -------
    x: np.array
        two numbers to be added represented as bits (in reversed order).
        shape: b, i, n
        where:
            b is bit index from the end.
            i is example idx in batch.
            n is one of [0,1] depending for first and second summand respectivel
y.
    y: np.array
        the result of the addition.
        shape: b, i, n
        where:
            b is bit index from the end.
            i is example idx in batch.
            n is always 0 since there is only one result.
    """
    x = np.empty((batch_size, num_bits, 2))
    y = np.empty((batch_size, num_bits, 1))

    for i in range(batch_size):
        a, b, r = generate_example(num_bits)
        x[i, :, 0] = a
        x[i, :, 1] = b
        y[i, :, 0] = r
    return x, y

# configuration
batch_size = 100
time_size = 5

# generate a test set and a train set containing 100 examples of numbers represe
nted in 5 bits
x_train, y_train = generate_batch(time_size, batch_size)
x_test, y_test = generate_batch(time_size, batch_size)

#%% (2) model definition.

import tensorflow as tf

class gru:
    """implementation of a gated recurrent unit (gru) as described in [1].

    [1] chung, j., gulcehre, c., cho, k., & bengio, y. (2014). empirical evaluat
ion of gated recurrent neural networks on sequence modeling. arxiv preprint arxi
v:1412.3555.

    arguments
    ---------
    input_dimensions: int
        the size of the input vectors (x_t).
    hidden_size: int
        the size of the hidden layer vectors (h_t).
    dtype: obj
        the datatype used for the variables and constants (optional).
    """

    def __init__(self, input_dimensions, hidden_size, dtype=tf.float64):
        self.input_dimensions = input_dimensions
        self.hidden_size = hidden_size

        # weights for input vectors of shape (input_dimensions, hidden_size)
        self.wr = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.input
_dimensions, self.hidden_size), mean=0, stddev=0.01), name='wr')
        self.wz = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.input
_dimensions, self.hidden_size), mean=0, stddev=0.01), name='wz')
        self.wh = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.input
_dimensions, self.hidden_size), mean=0, stddev=0.01), name='wh')

        # weights for hidden vectors of shape (hidden_size, hidden_size)
        self.ur = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.hidde
n_size, self.hidden_size), mean=0, stddev=0.01), name='ur')
        self.uz = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.hidde
n_size, self.hidden_size), mean=0, stddev=0.01), name='uz')
        self.uh = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.hidde
n_size, self.hidden_size), mean=0, stddev=0.01), name='uh')

        # biases for hidden vectors of shape (hidden_size,)
        self.br = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.hidde
n_size,), mean=0, stddev=0.01), name='br')
        self.bz = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.hidde
n_size,), mean=0, stddev=0.01), name='bz')
        self.bh = tf.variable(tf.truncated_normal(dtype=dtype, shape=(self.hidde
n_size,), mean=0, stddev=0.01), name='bh')

        # define the input layer placeholder
        self.input_layer = tf.placeholder(dtype=tf.float64, shape=(none, none, i
nput_dimensions), name='input')

        # put the time-dimension upfront for the scan operator
        self.x_t = tf.transpose(self.input_layer, [1, 0, 2], name='x_t')

        # a little hack (to obtain the same shape as the input matrix) to define
 the initial hidden state h_0
        self.h_0 = tf.matmul(self.x_t[0, :, :], tf.zeros(dtype=tf.float64, shape
=(input_dimensions, hidden_size)), name='h_0')

        # perform the scan operator
        self.h_t_transposed = tf.scan(self.forward_pass, self.x_t, initializer=s
elf.h_0, name='h_t_transposed')

        # transpose the result back
        self.h_t = tf.transpose(self.h_t_transposed, [1, 0, 2], name='h_t')

    def forward_pass(self, h_tm1, x_t):
        """perform a forward pass.

        arguments
        ---------
        h_tm1: np.matrix
            the hidden state at the previous timestep (h_{t-1}).
        x_t: np.matrix
            the input vector.
        """
        # definitions of z_t and r_t
        z_t = tf.sigmoid(tf.matmul(x_t, self.wz) + tf.matmul(h_tm1, self.uz) + s
elf.bz)
        r_t = tf.sigmoid(tf.matmul(x_t, self.wr) + tf.matmul(h_tm1, self.ur) + s
elf.br)

        # definition of h~_t
        h_proposal = tf.tanh(tf.matmul(x_t, self.wh) + tf.matmul(tf.multiply(r_t
, h_tm1), self.uh) + self.bh)

        # compute the next hidden state
        h_t = tf.multiply(1 - z_t, h_tm1) + tf.multiply(z_t, h_proposal)

        return h_t

#%% (3) initialize and train the model.

# the input has 2 dimensions: dimension 0 is reserved for the first term and dim
ension 1 is reverved for the second term
input_dimensions = 2

# arbitrary number for the size of the hidden state
hidden_size = 16

# initialize a session
session = tf.session()

# create a new instance of the gru model
gru = gru(input_dimensions, hidden_size)

# add an additional layer on top of each of the hidden state outputs
w_output = tf.variable(tf.truncated_normal(dtype=tf.float64, shape=(hidden_size,
 1), mean=0, stddev=0.01))
b_output = tf.variable(tf.truncated_normal(dtype=tf.float64, shape=(1,), mean=0,
 stddev=0.01))
output = tf.map_fn(lambda h_t: tf.matmul(h_t, w_output) + b_output, gru.h_t)

# create a placeholder for the expected output
expected_output = tf.placeholder(dtype=tf.float64, shape=(batch_size, time_size,
 1), name='expected_output')

# just use quadratic loss
loss = tf.reduce_sum(0.5 * tf.pow(output - expected_output, 2)) / float(batch_si
ze)

# use the adam optimizer for training
train_step = tf.train.adamoptimizer().minimize(loss)

# initialize all the variables
init_variables = tf.global_variables_initializer()
session.run(init_variables)

# initialize the losses
train_losses = []
validation_losses = []

# perform all the iterations
for epoch in range(5000):
    # compute the losses
    _, train_loss = session.run([train_step, loss], feed_dict={gru.input_layer:
x_train, expected_output: y_train})
    validation_loss = session.run(loss, feed_dict={gru.input_layer: x_test, expe
cted_output: y_test})

    # log the losses
    train_losses += [train_loss]
    validation_losses += [validation_loss]

    # display an update every 50 iterations
    if epoch % 50 == 0:
        plt.plot(train_losses, '-b', label='train loss')
        plt.plot(validation_losses, '-r', label='validation loss')
        plt.legend(loc=0)
        plt.title('loss')
        plt.xlabel('iteration')
        plt.ylabel('loss')
        plt.show()
        print('iteration: %d, train loss: %.4f, test loss: %.4f' % (epoch, train
_loss, validation_loss))

#%% (4) manually evaluate the model.

# define two numbers a and b and let the model compute a + b
a = 1024
b = 16

# the model is independent of the sequence length! now we can test the model on
even longer bitstrings
bitstring_length = 20

# create the feature vectors
x_custom_sample = np.vstack([as_bytes(a, bitstring_length), as_bytes(b, bitstrin
g_length)]).t
x_custom = np.zeros((1,) + x_custom_sample.shape)
x_custom[0, :, :] = x_custom_sample

# make a prediction by using the model
y_predicted = session.run(output, feed_dict={gru.input_layer: x_custom})
# just use a linear class separator at 0.5
y_bits = 1 * (y_predicted > 0.5)[0, :, 0]
# join and reverse the bitstring
y_bitstr = ''.join([str(int(bit)) for bit in y_bits.tolist()])[::-1]
# convert the found bitstring to a number
y = int(y_bitstr, 2)

# print out the prediction
print(y) # yay! this should equal 1024 + 16 = 1040

   [30]gru_tensorflow.py [31]view raw

results


   gru loss.

   gru loss.

   after ~2000 iterations, the model has fully learned how to add 2
   integer numbers!

conclusion (tl;dr)

   this python deep learning tutorial showed how to implement a gru in
   tensorflow. the implementation of the gru in tensorflow takes only ~30
   lines of code! there are some issues with respect to parallelization,
   but these issues can be resolved using the tensorflow api efficiently.
   in this tutorial, the model is capable of learning how to add two
   integer numbers (of any length).

references

   [32][1] chung, j., gulcehre, c., cho, k., & bengio, y. (2014).
   empirical evaluation of gated recurrent neural networks on sequence
   modeling. arxiv preprint arxiv:1412.3555.
   [ins: :ins]

   [33]machine learning [34]neural networks [35]recurrent neural networks
   [36]id56 [37]tensorflow [38]gru

[39]kevin jacobs

   kevin jacobs is a certified data scientist and blog writer for [40]data
   blogger. he is passionate about any project that involves large amounts
   of data and statistical data analysis. kevin can be reached using
   [41]twitter (@kmjjacobs), [42]linkedin or via e-mail:
   [43]kevin8080nl@gmail.com. want to write for our website? then check
   out our [44]write for us page!

post navigation

   [45]what is the blockchain and why should you care?
   [46]pokerbot: create your poker ai bot in python

search

   ____________________ (search) search

subscribe to our newsletter

   please leave this field empty ____________________

   email * ____________________

   subscribe!

   check your inbox or spam folder now to confirm your subscription.

   [ins: :ins]

editors    picks

     * [47]write for us!
     * [48]should you start learning python in 2018 (guide)
     * [49]hands-on: creating neural networks using chainer

categories

     * [50]do-it-yourself (15)
     * [51]services (1)
     * [52]technology (4)
     * [53]web technology (4)
     * [54]book review (2)
     * [55]cryptocurrencies (1)
     * [56]security (2)
     * [57]data science (31)
     * [58]software science (20)
     * [59]mathematics (10)
     * [60]personal projects (13)

awards

   [61]kdnuggets silver blog, july 2017

references

   visible links
   1. https://www.data-blogger.com/2017/08/27/gru-implementation-tensorflow/feed/
   2. https://www.data-blogger.com/wp-json/oembed/1.0/embed?url=https://www.data-blogger.com/2017/08/27/gru-implementation-tensorflow/
   3. https://www.data-blogger.com/wp-json/oembed/1.0/embed?url=https://www.data-blogger.com/2017/08/27/gru-implementation-tensorflow/&format=xml
   4. https://www.data-blogger.com/
   5. https://www.data-blogger.com/write-for-us/
   6. https://www.data-blogger.com/
   7. https://www.data-blogger.com/category/do-it-yourself/
   8. https://www.data-blogger.com/category/technology/
   9. https://www.data-blogger.com/category/web-technology/
  10. https://www.data-blogger.com/category/data-science/
  11. https://www.data-blogger.com/category/software-science/
  12. https://www.data-blogger.com/category/mathematics/
  13. https://www.data-blogger.com/category/personal-projects/
  14. https://www.data-blogger.com/2017/08/27/gru-implementation-tensorflow/#pll_switcher
  15. https://www.data-blogger.com/2017/08/27/gru-implementation-tensorflow/
  16. https://www.data-blogger.com/nl/
  17. https://www.data-blogger.com/zh/
  18. https://www.data-blogger.com/2017/08/27/gru-implementation-tensorflow/
  19. https://www.data-blogger.com/author/admin/
  20. https://www.data-blogger.com/category/do-it-yourself/
  21. https://www.data-blogger.com/category/data-science/
  22. https://www.data-blogger.com/category/software-science/
  23. https://www.data-blogger.com/category/personal-projects/
  24. https://www.data-blogger.com/2017/02/26/artificial-neural-nets-a-gentle-introduction/
  25. https://www.data-blogger.com/2017/06/15/fraud-detection-a-simple-machine-learning-approach/
  26. https://www.data-blogger.com/2017/05/17/elman-id56-implementation-in-tensorflow/
  27. https://en.wikipedia.org/wiki/vanishing_gradient_problem
  28. https://en.wikipedia.org/wiki/hadamard_product_(matrices)
  29. https://gist.github.com/kmjjacobs/eab1e840aecf0ac232cc8370a9be9093
  30. https://gist.github.com/eab1e840aecf0ac232cc8370a9be9093
  31. https://gist.githubusercontent.com/kmjjacobs/eab1e840aecf0ac232cc8370a9be9093/raw/cf2d445fa0893d4f44abf556afd2b667b124c6d4/gru_tensorflow.py
  32. https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf
  33. https://www.data-blogger.com/tag/machine-learning/
  34. https://www.data-blogger.com/tag/neural-networks/
  35. https://www.data-blogger.com/tag/recurrent-neural-networks/
  36. https://www.data-blogger.com/tag/id56/
  37. https://www.data-blogger.com/tag/tensorflow/
  38. https://www.data-blogger.com/tag/gru/
  39. https://www.data-blogger.com/author/admin/
  40. https://www.data-blogger.com/
  41. https://www.twitter.com/kmjjacobs
  42. https://www.linkedin.com/in/kevinjacobs1991/
  43. mailto:kevin8080nl@gmail.com
  44. https://www.data-blogger.com/write-for-us/
  45. https://www.data-blogger.com/2017/08/03/what-is-the-blockchain-and-why-should-you-care/
  46. https://www.data-blogger.com/2017/11/01/pokerbot-create-your-poker-ai-bot-in-python/
  47. https://www.data-blogger.com/write-for-us/
  48. https://www.data-blogger.com/2018/07/16/should-you-start-learning-python-in-2018-guide/
  49. https://www.data-blogger.com/2018/02/15/creating-neural-networks-chainer/
  50. https://www.data-blogger.com/category/do-it-yourself/
  51. https://www.data-blogger.com/category/services/
  52. https://www.data-blogger.com/category/technology/
  53. https://www.data-blogger.com/category/web-technology/
  54. https://www.data-blogger.com/category/book-review/
  55. https://www.data-blogger.com/category/cryptocurrencies/
  56. https://www.data-blogger.com/category/security/
  57. https://www.data-blogger.com/category/data-science/
  58. https://www.data-blogger.com/category/software-science/
  59. https://www.data-blogger.com/category/mathematics/
  60. https://www.data-blogger.com/category/personal-projects/
  61. http://www.kdnuggets.com/2017/07/top-stories-week-0703-0709.html

   hidden links:
  63. https://www.addtoany.com/add_to/flipboard?linkurl=https%3a%2f%2fwww.data-blogger.com%2f2017%2f08%2f27%2fgru-implementation-tensorflow%2f&linkname=python%20deep%20learning%20tutorial%3a%20create%20a%20gru%20%28id56%29%20in%20tensorflow
  64. https://www.addtoany.com/add_to/linkedin?linkurl=https%3a%2f%2fwww.data-blogger.com%2f2017%2f08%2f27%2fgru-implementation-tensorflow%2f&linkname=python%20deep%20learning%20tutorial%3a%20create%20a%20gru%20%28id56%29%20in%20tensorflow
  65. https://www.addtoany.com/add_to/twitter?linkurl=https%3a%2f%2fwww.data-blogger.com%2f2017%2f08%2f27%2fgru-implementation-tensorflow%2f&linkname=python%20deep%20learning%20tutorial%3a%20create%20a%20gru%20%28id56%29%20in%20tensorflow
  66. https://www.addtoany.com/add_to/facebook?linkurl=https%3a%2f%2fwww.data-blogger.com%2f2017%2f08%2f27%2fgru-implementation-tensorflow%2f&linkname=python%20deep%20learning%20tutorial%3a%20create%20a%20gru%20%28id56%29%20in%20tensorflow
  67. https://www.addtoany.com/add_to/copy_link?linkurl=https%3a%2f%2fwww.data-blogger.com%2f2017%2f08%2f27%2fgru-implementation-tensorflow%2f&linkname=python%20deep%20learning%20tutorial%3a%20create%20a%20gru%20%28id56%29%20in%20tensorflow
