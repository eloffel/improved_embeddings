   #[1]machine thoughts    feed [2]machine thoughts    comments feed
   [3]machine thoughts    deep meaning beyond thought vectors comments feed
   [4]the plausibility of near-term machine sentience. [5]vae = em
   [6]alternate [7]alternate [8]machine thoughts [9]wordpress.com

   [10]machine thoughts
   [11]skip to content
     * [12]home
     * [13]about

   [14]    the plausibility of near-term machine sentience.
   [15]vae = em    

[16]deep meaning beyond thought vectors

   posted on [17]september 1, 2017 by [18]mcallester

   i ended my last post by saying that i might write a follow-up post on
   current work that seems to exhibit progress toward natural language
   understanding.   i am going to discuss a couple sampled papers but of
   course these are not the only promising papers.  i just want to give
   some examples to argue that significant progress is being made and that
   there is no obvious limit. i will start with the following quotation
   which has gained considerably currency lately.

      you can   t cram the meaning of a whole %&!$ing sentence into a single
   $&!*ing vector!   

   ray mooney

   this quotation seems to be becoming a legacy similar to that of
   jelinek   s quotation that    every time i fire a linguistic our speech
   recognition error rate improves   . however, while the underlying
   phenomenon pointed to by jelinek     the dominance of learning over
   design     has been controversial over the years, i suspect that there is
   already a large consensus that mooney is correct     we need more than
      thought vectors   .

meaning as a sequence of vectors: attention

   the first step beyond representing a sentence as a single vector is to
   represent a sentence as a sequence of vectors. [19]the attention
   mechanism of machine translation systems essentially does this. it
   takes the meaning of the input sentence to be the sequence of hidden
   state vectors of an id56 (lstm or gru).  as the translation is generated
   the attention mechanism extracts information from interior of the input
   sentence.  there is no clear understanding of what is being
   represented, but the attention mechanism is now central to high
   performance translation systems such as [20]that recently introduced by
   google.  attention is old news so i will say no more.

meaning as a sequence of vectors: graph-lstms

   here i will mention two very recent papers.  the first is a paper that
   appeared at acl this year by nanyun peng, hoifung poon, chris quirk,
   kristina toutanova, wen-tau yih, entitled    [21]cross-sentence n-ary
   id36 with graph lstms   .  this paper is on relation
   extraction      extracting gene-mutation-drug triples from the literature
   on cancer mutations.  however, here i am focused on the representation
   used for the meaning of text.  as in translation, text is converted to
   a sequence of vectors with a vector for each position in the text.
   however, this sequence is not computed by a normal lstm or gru.  rather
   the links of a dependency parse are added as edges between the
   positions in the sentence resulting in a graph that includes both the
   dependency edges and the    next-word    edges as in the following figure
   from the paper. screen shot 2017-08-31 at 3.30.47 pm.png

   the edges are divided into those that go left-to-right and those that
   go right-to-left as shown in the second part of the figure.  each of
   these two edge sets forms a dag (a directed acyclic graph).  [22]tree
   lstms immediately generalize to dags and i might prefer the term
   dag-lstm to the term graph-lstm used in the title.  they then run a
   dag-lstm from left to right over the left-to-right edges and a dag-lstm
   from right-to-left over the right-to-left edges and concatenate the two
   vectors at each position.  the dependency parse is provided by an
   external resource.  importantly, different transition parameters are
   used for different edge types     the    next-word    edge parameters are
   different from the    subject-of    edge parameters which are different
   from the    object-of    edge prameters.

   a startlingly similar but independent paper was posted on arxiv in
   march by bhuwan dhingra, zhilin yang, william w. cohen and ruslan
   salakhutdinov, entitled    [23]linguistic knowledge as memory for
   recurrent neural networks   .  this paper is on reading comprehension but
   here i will again focus on the representation of the meaning of text.
   they also add edges between the positions in the text.  rather than add
   the edges of a dependency parse they add coreference edges and edges
   for hyponym-hypernym relations as in the following figure from the
   paper.

   screen shot 2017-08-31 at 3.45.46 pm.png

   as in the previous paper, they then run a dag-id56 (a dag-gru) from
   left-to-right and also right-to-left.  but they use every edge in both
   directions with different parameters for different types of edges and
   different parameters for the left-to-right and right-to-left directions
   of the same edge type.  again the coreference edges and
   hypernym/hyponym edges are provided by an external resource.

meaning as a sequence of vectors: self-attention

   as a third pass i will consider self attention as developed in    [24]a
   structured self-attentive sentence embedding    by lin et al. (ibm watson
   and the university of montreal) and    [25]attention is all you need   
   by vaswani et al. (google brain, google research and u. of toronto).
   although only a few months old these papers are already well known.
   unlike the graph-lstm papers above, these papers learn graph structure
   on the text without the use of external resources.  they also do not
   use any form of id56 (lstm or gru) but rely entirely on learned graph
   structure. the graph structure is created through self-attention.  i
   will take the liberty of some simplification    i will ignore the
   residual skip connections and various other details.  we start with the
   sequence of input vectors.  for each position we apply three matrices
   to get the three different vectors     a key vector, a query vector, and
   a value vector.  for each position we take the query vector at that
   position inner product the key vector at every other position to get an
   attention weighting (or set of weighted edges) from that position to
   the other positions in the sequence (including itself).  we then then
   weight the value vectors by that weighting and pass the result through
   a  feed forward network to get a vector at that position at the next
   layer.  this is repeated for some number of layers.  the sentence
   representation is the sequence of vectors in the last layer.

   the above description ignores the multi-headed feature described in
   both papers.  rather than just compute one attention graph they compute
   compute several different attention graphs each of which is defined by
   different parameters.  each of these attention graphs might be
   interpreted as a different type of edge, such as the agent semantic
   role relation between event verbs and their agents. but the
   interpretations of deep models is always difficult.  the following
   figure from vaswani et al. shows the edges from the word    making    where
   different colors represent different edge types.  although the sentence
   is shown twice, the edges are actually between the words of a single
   sequence at a single layer of the model.

   screen shot 2017-08-31 at 6.13.14 pm.png the following figure shows one
   particular edge type which is possibly interpretable as a coreference
   relation.

   screen shot 2017-08-31 at 6.21.40 pm.png

   we might call these networks self-attention networks (sans). note that
   they are sans lstms and sans id98s :).

meaning as an embedded id13

   i want to complain at this point that you can   t cram the meaning of a
   bleeping sentence into a bleeping sequence of vectors.  the graph
   structures on the positions in the sentence used in the above models
   should be exposed to the user of the semantic representation.  i would
   take the position that the meaning should be an embedded knowledge
   graph     a graph on embedded entity nodes and typed relations (edges)
   between them.  a node representing an event can be connected through
   edges to entities that fill the semantic roles of the event type (a
   neo-davidsonian representation of events in a graph structure).

   one paper in this direction is    [26]dynamic entity representations in
   neural language models    by yangfeng ji, chenhao tan, sebastian
   martschat, yejin choi and noah a. smith (emnlp 2017). this model
   jointly learns to identify mentions, coreference, and entity embeddings
   (a vector representing the object referred to).

   another related paper is    [27]neural symbolic machines: learning
   semantic parsers on freebase with weak supervision    by chen lian,
   jonathan berant, quoc le, kenneth d. forbus and ni lao which appeared
   at acl this year.  this paper addresses the problem of natural language
   id53 using freebase.  but again i will focus on the
   representation of the input sentence (question in this case). they
   convert the input question to a kind of graph structure where each node
   in the graph denotes a set of freebase entities.  an initial set of
   nodes is constructed by an external linker that links mentions in the
   question, such as    city in the united states   , to a set of freebase
   entities. a sequence-to-sequence model is then used to introduce
   additional nodes each one of which is defined by a relation to
   previously introduced nodes. this    graph    can be viewed as a program
   with instructions of the form n_i = op(r, n_j) where n_i is a newly
   defined node, n_j is a previously defined node, r is a database
   relation such as    born-in    and op is one of    hop   ,    argmax   ,    argmin   ,
   or    filter   .  for example

   n_1 = us-city;  n_2 = hop(born-in, n_1);  n_3 = argmax(population-of,
   n_1)

   defines n_1 as the set of us cities, n_2 as the set of people born in a
   us city and n_3 as the largest us city.  answering the input question
   involves running the generated program on freebase.  the sequence to
   sequence model is trained from question-answer pairs without any gold
   semantic graph output sequences. they use some fancy tricks to get
   reinforce to work. but the point here is that id29 of this
   type converts input text to a kind of graph structure with embedded
   nodes (embedded by the sequence-to-sequence model). another important
   point is that constructing the semantics involves background knowledge
       freebase in this case.  language is highly referential and    meaning   
   must ultimately involve reference resolution     linking to a knowledge
   base.

what   s next?

   a nice survey of semantic formalisms is given in    [28]the state of the
   art in semantic representation    by omri abend and ari rappoport which
   appeared at this acl.  a fundamental issue here is the increasing
   dominance of learning over design.  i believe that in the near term it
   will not be possible to separate semantic formalisms from learning
   architectures.  (in the long term there is the foundations of
   mathematics    ) if the target semantic formalism is a neo-davidsonian
   embedded id13, then this formalism must be unified with some
   learning architecture. the learning should be done in the presence of
   background knowledge     both semantic and episodic long term memory.
   background knowledge could itself be an embedded id13.  the
   paper    [29]node2vec: scalable id171 for networks    by grover
   and leskovec gives a method of embedding a given (unembedded) knowledge
   graph. but it ultimately seems better to generate the embedding jointly
   with acquiring the graph.

   i have long believed that the most promising cognitive (learning)
   architecture is bottom-up logic programming.  bottom-up (forward
   chaining) logic programming has a long history in the form of
   production systems, datalog, and jason eisner   s dyna programming
   language.  there are strong [30]theoretical arguments for the
   centrality of bottom-up logic programming. i have heard rumors that
   there is currently a significant effort at deepmind based on deep
   inductive logic programming (deep ilp) for bottom-up programs and that
   a paper will appear in the next few months.  i have high hopes    
   advertisements

share this:

     * [31]twitter
     * [32]facebook
     *

like this:

   like loading...

related

   this entry was posted in [33]uncategorized. bookmark the [34]permalink.
   [35]    the plausibility of near-term machine sentience.
   [36]vae = em    

1 response to deep meaning beyond thought vectors

    1. pingback: [37]code hamster    an interesting ai blog

leave a reply [38]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *

       iframe: [39]googleplus-sign-in

     *
     *

   [40]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [41]log out /
   [42]change )
   google photo

   you are commenting using your google account. ( [43]log out /
   [44]change )
   twitter picture

   you are commenting using your twitter account. ( [45]log out /
   [46]change )
   facebook photo

   you are commenting using your facebook account. ( [47]log out /
   [48]change )
   [49]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

     * recent posts
          + [50]thoughts from ttic31230: rethinking generalization.
          + [51]thoughts from ttic31230: langevin dynamics and the
            struggle for the soul of sgd.
          + [52]thoughts from ttic31230: hyperparameter conjugacy
          + [53]superintelligence and the truth
          + [54]predictive coding and mutual information
          + [55]rl and the game of mathematics
          + [56]the role of theory in deep learning
          + [57]choice as a natural kind term
          + [58]ctc and the eg algotithm: discrete latent choices without
            id23
          + [59]vae = em
          + [60]deep meaning beyond thought vectors
          + [61]the plausibility of near-term machine sentience.
          + [62]formalism, platonism and mentalese
          + [63]comprehension based id38
          + [64]cognitive architectures
          + [65]architectures and language instincts
          + [66]why we need a new foundation of mathematics
          + [67]the foundations of mathematics.
          + [68]friendly ai and the servant mission
          + [69]ai and free will: when do choices exist?
       advertisements

   [70]machine thoughts
   [71]wordpress.com.


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [72]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [73]cookie policy

   iframe: [74]likes-master

   %d bloggers like this:

references

   visible links
   1. https://machinethoughts.wordpress.com/feed/
   2. https://machinethoughts.wordpress.com/comments/feed/
   3. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/feed/
   4. https://machinethoughts.wordpress.com/2017/07/22/the-plausibility-of-near-term-machine-sentience/
   5. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/&for=wpcom-auto-discovery
   8. https://machinethoughts.wordpress.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://machinethoughts.wordpress.com/
  11. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/#content
  12. https://machinethoughts.wordpress.com/
  13. https://machinethoughts.wordpress.com/about/
  14. https://machinethoughts.wordpress.com/2017/07/22/the-plausibility-of-near-term-machine-sentience/
  15. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  16. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  17. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  18. https://machinethoughts.wordpress.com/author/dmcallester/
  19. https://arxiv.org/abs/1409.0473
  20. https://arxiv.org/abs/1609.08144
  21. https://www.transacl.org/ojs/index.php/tacl/article/view/1028
  22. https://arxiv.org/abs/1503.00075
  23. https://arxiv.org/abs/1703.02620
  24. https://arxiv.org/abs/1703.03130
  25. https://arxiv.org/abs/1706.03762
  26. https://arxiv.org/abs/1708.00781
  27. http://aclanthology.coli.uni-saarland.de/pdf/p/p17/p17-1003.pdf
  28. http://www.aclweb.org/anthology/p/p17/p17-1008.pdf
  29. https://arxiv.org/abs/1607.00653
  30. https://pdfs.semanticscholar.org/4944/2fcd0cc8ad093fc8d3450a2c3d1fbd025541.pdf
  31. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/?share=twitter
  32. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/?share=facebook
  33. https://machinethoughts.wordpress.com/category/uncategorized/
  34. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  35. https://machinethoughts.wordpress.com/2017/07/22/the-plausibility-of-near-term-machine-sentience/
  36. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  37. http://www.codehamster.com/2017/09/15/an-interesting-ai-blog/
  38. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/#respond
  39. https://public-api.wordpress.com/connect/?googleplus-sign-in=https://machinethoughts.wordpress.com&color_scheme=light
  40. https://gravatar.com/site/signup/
  41. javascript:highlandercomments.doexternallogout( 'wordpress' );
  42. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  43. javascript:highlandercomments.doexternallogout( 'googleplus' );
  44. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  45. javascript:highlandercomments.doexternallogout( 'twitter' );
  46. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  47. javascript:highlandercomments.doexternallogout( 'facebook' );
  48. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  49. javascript:highlandercomments.cancelexternalwindow();
  50. https://machinethoughts.wordpress.com/2019/04/03/thoughts-from-ttic21230-rethinking-generalization/
  51. https://machinethoughts.wordpress.com/2019/03/27/thoughts-from-ttic31230-langevin-dynamics-and-the-struggle-for-the-soul-of-sgd/
  52. https://machinethoughts.wordpress.com/2019/03/20/thoughts-from-ttic31230-hyperparameter-conjugacy/
  53. https://machinethoughts.wordpress.com/2018/09/18/superintelligence-and-the-truth/
  54. https://machinethoughts.wordpress.com/2018/08/15/predictive-coding-and-mutual-information/
  55. https://machinethoughts.wordpress.com/2018/04/14/rl-and-the-game-of-mathematics/
  56. https://machinethoughts.wordpress.com/2017/12/08/the-role-of-theory-in-deep-learning/
  57. https://machinethoughts.wordpress.com/2017/11/29/choice-as-a-natural-kind-term/
  58. https://machinethoughts.wordpress.com/2017/11/02/ctc-training-latent-discrete-sequential-decisions-without-rl/
  59. https://machinethoughts.wordpress.com/2017/10/02/vae-em/
  60. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  61. https://machinethoughts.wordpress.com/2017/07/22/the-plausibility-of-near-term-machine-sentience/
  62. https://machinethoughts.wordpress.com/2017/07/17/formalism-platonism-and-mentalese/
  63. https://machinethoughts.wordpress.com/2016/11/15/comprehension-based-language-modeling/
  64. https://machinethoughts.wordpress.com/2016/06/20/cognitive-architectures/
  65. https://machinethoughts.wordpress.com/2016/06/12/architectures-and-language-instincts/
  66. https://machinethoughts.wordpress.com/2016/01/01/why-we-need-a-new-foundation-of-mathematics/
  67. https://machinethoughts.wordpress.com/2015/01/27/the-foundations-of-mathematics-2/
  68. https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/
  69. https://machinethoughts.wordpress.com/2014/08/03/ai-and-free-will-when-do-choices-exist/
  70. https://machinethoughts.wordpress.com/
  71. https://wordpress.com/?ref=footer_custom_com
  72. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/
  73. https://automattic.com/cookies
  74. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  76. https://machinethoughts.wordpress.com/
  77. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/#comment-form-guest
  78. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/#comment-form-load-service:wordpress.com
  79. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/#comment-form-load-service:twitter
  80. https://machinethoughts.wordpress.com/2017/09/01/deep-meaning-beyond-thought-vectors/#comment-form-load-service:facebook
