   #[1]github [2]recent commits to emnlp2017-bilstm-id98-crf:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]54
     * [35]star [36]610
     * [37]fork [38]210

[39]ukplab/[40]emnlp2017-bilstm-id98-crf

   [41]code [42]issues 22 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   bilstm-id98-crf architecture for sequence tagging
     * [47]50 commits
     * [48]1 branch
     * [49]3 releases
     * [50]fetching contributors
     * [51]apache-2.0

    1. [52]python 99.8%
    2. [53]dockerfile 0.2%

   (button) python dockerfile
   branch: master (button) new pull request
   [54]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/u
   [55]download zip

downloading...

   want to be notified of new releases in ukplab/emnlp2017-bilstm-id98-crf?
   [56]sign in [57]sign up

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [59]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [60]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [61]download the github extension for visual studio
   and try again.

   (button) go back
   [62]@nreimers
   [63]nreimers [64]merge pull request [65]#30 [66]from apmoore1/master
   (button)    
fixes [67]#29. thanks for this pr

   latest commit [68]b709f58 sep 19, 2018
   [69]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [70]data [71]initial commit jul 13, 2017
   [72]docker [73]remove pyyaml from requirements -> caused an error for
   pip install jul 24, 2018
   [74]docs [75]update pretrained_models.md jul 2, 2018
   [76]models [77]models folder apr 20, 2018
   [78]neuralnets [79]output of score from best dev epoch updated jul 30,
   2018
   [80]pkl [81]update gitignore apr 20, 2018
   [82]util [83]fixed the reducepretrainedembeddings problem sep 18, 2018
   [84].gitignore
   [85]2017_reimers_optimal hyperparameters for deep lstm-networks.pdf
   [86]updated pdfs aug 18, 2017
   [87]2017_reimers_reporting score distributions makes a difference.pdf
   [88]updated pdfs aug 18, 2017
   [89]license
   [90]notice.txt [91]removed tmp file jul 17, 2017
   [92]readme.md
   [93]runmodel.py
   [94]runmodel_conll_format.py
   [95]save_and_load.py
   [96]train_chunking.py
   [97]train_custom_features.py [98]update code to keras 2.1.5 apr 20,
   2018
   [99]train_multitask.py
   [100]train_multitask_different_levels.py
   [101]train_ner_german.py
   [102]train_pos.py
   [103]hyperparameter_results.csv.gz
   [104]input.conll
   [105]input.txt
   [106]requirements.txt

readme.md

bilstm-id98-crf implementation for sequence tagging

   this repository contains a bilstm-crf implementation that used for nlp
   sequence tagging (for example pos-tagging, chunking, or named entity
   recognition). the implementation is based on keras 2.2.0 and can be run
   with tensorflow 1.8.0 as backend. it was optimized for python 3.5 /
   3.6. it does not work with python 2.7.

   the architecture is described in our papers:
     * [107]reporting score distributions makes a difference: performance
       study of lstm-networks for sequence tagging
     * [108]why comparing single performance scores does not allow to draw
       conclusions about machine learning approaches
     * [109]optimal hyperparameters for deep lstm-networks for sequence
       labeling tasks.

   the implementation is highly configurable, so you can tune the
   different hyperparameters easily. you can use it for single task
   learning as well as different options for id72. you can
   also use it for multilingual learning by using id73
   embeddings.

   this code can be used to run the systems proposed in the following
   papers:
     * [110]huang et al., bidirectional lstm-crf models for sequence
       tagging - you can choose between a softmax and a crf classifier
     * [111]ma and hovy, end-to-end sequence labeling via bi-directional
       lstm-id98s-crf - character based word representations using id98s is
       achieved by setting the parameter charembeddings to id98
     * [112]lample et al, neural architectures for named entity
       recognition - character based word representations using lstms is
       achieved by setting the parameter charembeddings to lstm
     * [113]sogard, goldberg: deep id72 with low level
       tasks supervised at lower layers - train multiple task and
       supervise them on different levels.

   the implementation was optimized for speed: by grouping sentences with
   the same lengths together, this implementation is multiple factors
   faster than the systems by ma et al. or lample et al.

   the training of the network is simple and the neural network can easily
   be trained on new datasets. for an example, see [114]train_pos.py.

   trained models can be stored and loaded for id136. simply execute
   python runmodel.py models/modelname.h5 input.txt. pretrained-models for
   some sequence tagging task using this lstm-crf implementations are
   provided in [115]pretrained models.

   this implementation can be used for id72, i.e. learning
   simultanously several task with non-overlapping datasets. the file
   [116]train_multitask.py depicts an example, how the lstm-crf network
   can be used to learn pos-tagging and chunking simultaneously. the
   number of tasks are not limited. tasks can be supervised at the same
   level or at different output level.

implementation with elmo representations

   the repository [117]elmo-bilstm-id98-crf contains an extension of this
   architecture to work with the elmo representations from allennlp (from
   the paper: peters et al., 2018, [118]deep contextualized word
   representations). elmo representations are computationally expensive to
   compute, but they usually improve the performance by about 1-5
   percentage points f1-measure.

citation

   if you find the implementation useful, please cite the following paper:
   [119]reporting score distributions makes a difference: performance
   study of lstm-networks for sequence tagging
@inproceedings{reimers:2017:emnlp,
  author    = {reimers, nils, and gurevych, iryna},
  title     = {{reporting score distributions makes a difference: performance st
udy of lstm-networks for sequence tagging}},
  booktitle = {proceedings of the 2017 conference on empirical methods in natura
l language processing (emnlp)},
  month     = {09},
  year      = {2017},
  address   = {copenhagen, denmark},
  pages     = {338--348},
  url       = {http://aclweb.org/anthology/d17-1035}
}

   contact person: nils reimers,
   [120]reimers@ukp.informatik.tu-darmstadt.de

   [121]https://www.ukp.tu-darmstadt.de/ [122]https://www.tu-darmstadt.de/

   don't hesitate to send us an e-mail or report an issue, if something is
   broken (and it shouldn't be) or if you have further questions.

     this repository contains experimental software and is published for
     the sole purpose of giving additional background details on the
     respective publication.

setup

   in order to run the code, i recommend python 3.5 or higher. the code is
   based on keras 2.2.0 and as backend i recommend tensorflow 1.8.0. i
   cannot ensure that the code works with different versions for keras /
   tensorflow or with different backends for keras. the code does not work
   with python 2.7.

setup with virtual environment (python 3)

   setup a python virtual environment (optional):
virtualenv --system-site-packages -p python3 env
source env/bin/activate

   install the requirements:
env/bin/pip3 install -r requirements.txt

   if everything works well, you can run python3 train_pos.py to train a
   deep pos-tagger for the pos-tagset from universal dependencies.

setup with docker

   see the [123]docker-folder for more information how to run these
   scripts in a docker container.

running a stored model

   if enabled during the trainings process, models are stored to the
   'models' folder. those models can be loaded and be used to tag new
   data. an example is implemented in runmodel.py:
python runmodel.py models/modelname.h5 input.txt

   this script will read the model models/modelname.h5 as well as the text
   file input.txt. the text will be splitted into sentences and tokenized
   using nltk. the tagged output will be written in a conll format to
   standard out.

training

   see train_pos.py for a simple example how to train the model. more
   details can be found in [124]docs/training.md.

   for training, you specify the datasets you want to train on:
datasets = {
    'unidep_pos':                            #name of the dataset
        {'columns': {1:'tokens', 3:'pos'},   #conll format for the input data. c
olumn 1 contains tokens, column 3 contains pos information
         'label': 'pos',                     #which column we like to predict
         'evaluate': true,                   #should we evaluate on this task? s
et true always for single task setups
         'commentsymbol': none}              #lines in the input data starting w
ith this string will be skipped. can be used to skip comments
}

   and you specify the pass to a pre-trained id27 file:
embeddingspath = 'komninos_english_embeddings.gz'

   the util.preprocessing.py fle contains some methods to read your
   dataset (from the data folder) and to store a pickle file in the pkl
   folder.

   you can then train the network in the following way:
params = {'classifier': ['crf'], 'lstm-size': [100], 'dropout': (0.25, 0.25)}
model = bilstm(params)
model.setmappings(mappings, embeddings)
model.setdataset(datasets, data)
model.modelsavepath = "models/[modelname]_[devscore]_[testscore]_[epoch].h5"
model.fit(epochs=25)

multi-task-learning

   id72 can simply be done by specifying multiple datasets
   (train_multitask.py)
datasets = {
    'unidep_pos':
        {'columns': {1:'tokens', 3:'pos'},
         'label': 'pos',
         'evaluate': true,
         'commentsymbol': none},
    'conll2000_chunking':
        {'columns': {0:'tokens', 2:'chunk_bio'},
         'label': 'chunk_bio',
         'evaluate': true,
         'commentsymbol': none},
}

   here, the networks trains jointly for the pos-task (unidep_pos) and for
   the chunking task (conll2000_chunking).

   you can also train task on different levels. for details, see
   [125]docs/training_multitask.md.

lstm-hyperparameters

   the parameters in the lstm-crf network can be configured by passing a
   parameter-dictionary to the bilstm-constructor: bilstm(params).

   the following parameters exists:
     * dropout: set to 0, for no dropout. for naive dropout, set it to a
       real value between 0 and 1. for variational dropout, set it to a
       two-dimensional tuple or list, with the first entry corresponding
       to output dropout and the second entry to the recurrent dropout.
       default value: [0.5, 0.5]
     * classifier: set to softmax to use a softmax classifier or to crf to
       use a crf-classifier as the last layer of the network. default
       value: softmax
     * lstm-size: list of integers with the number of recurrent units for
       the stacked lstm-network. the list [100,75,50] would create 3
       stacked bilstm-layers with 100, 75, and 50 recurrent units. default
       value: [100]
     * optimizer: available optimizers: sgd, adagrad, adadelta, rmsprop,
       adam, nadam. default value: nadam
     * earlystopping: early stoppig after certain number of epochs, if no
       improvement on the development set was achieved. default value: 5
     * minibatchsize: size (nr. of sentences) for mini-batch training.
       default value: 32
     * addfeaturedimensions: dimension for additional features, that are
       passed to the network. default value: 10
     * charembeddings: available options: [none, 'id98', 'lstm']. if set to
       none, no character-based representations will be used. with id98,
       the approach by [126]ma & hovy using a id98 will be used. with lstm,
       an lstm network will be used to derive the character-based
       representation ([127]lample et al.). default value: none.
          + charembeddingssize: the dimension for characters, if the
            character-based representation is enabled. default value: 30
          + charfiltersize: if the id98 approach is used, this parameters
            defined the filter size, i.e. the output dimension of the
            convolution. default: 30
          + charfilterlength: if the id98 approach is used, this parameters
            defines the filter length. default: 3
          + charlstmsize: if the lstm approach is used, this parameters
            defines the size of the recurrent units. default: 25
     * clipvalue: if non-zero, the gradient will be clipped to this value.
       default: 0
     * clipnorm: if non-zero, the norm of the gradient will be normalized
       to this value. default: 1
     * featurenames: which features the network should use. you can
       specify additinal features that are used, for example, this could
       be pos-tags. see train_custom_features.py for an example. default:
       ['tokens', 'casing']
     * addfeaturedimensions: size of the embedding matrix for all features
       except `tokens'. default: 10

   for id72 scenarios, the following additional parameter
   exists:
     * customclassifier: a dictionary, that maps each dataset an
       individual classifier. for example, the pos tag could use a
       softmax-classifier, while the chunking dataset is trained with a
       crf-classifier.
     * usetaskidentifier: including a task-id as an input feature.
       default: false

documentation

     * [128]docs/training.md: how to train simple, single-task
       architectures
     * [129]docs/training_multitask.md: how to use the architecture for
       id72
     * [130]docs/save_load_models.md: how to save & load models
     * [131]docs/pretrained_models.md: several pretraiend models that can
       be downloaded for common sequence tagging tasks.

acknowledgments

   this code uses the crf-implementation of [132]philipp gross from the
   keras pull request [133]#4621. thank you for contributing this to the
   community.

     *    2019 github, inc.
     * [134]terms
     * [135]privacy
     * [136]security
     * [137]status
     * [138]help

     * [139]contact github
     * [140]pricing
     * [141]api
     * [142]training
     * [143]blog
     * [144]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [145]reload to refresh your
   session. you signed out in another tab or window. [146]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commits/master.atom
   3. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/ukplab/emnlp2017-bilstm-id98-crf
  32. https://github.com/join
  33. https://github.com/login?return_to=/ukplab/emnlp2017-bilstm-id98-crf
  34. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/watchers
  35. https://github.com/login?return_to=/ukplab/emnlp2017-bilstm-id98-crf
  36. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/stargazers
  37. https://github.com/login?return_to=/ukplab/emnlp2017-bilstm-id98-crf
  38. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/network/members
  39. https://github.com/ukplab
  40. https://github.com/ukplab/emnlp2017-bilstm-id98-crf
  41. https://github.com/ukplab/emnlp2017-bilstm-id98-crf
  42. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/issues
  43. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/pulls
  44. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/projects
  45. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commits/master
  48. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/branches
  49. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/releases
  50. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/graphs/contributors
  51. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/license
  52. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/search?l=python
  53. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/search?l=dockerfile
  54. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/find/master
  55. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/archive/master.zip
  56. https://github.com/login?return_to=https://github.com/ukplab/emnlp2017-bilstm-id98-crf
  57. https://github.com/join?return_to=/ukplab/emnlp2017-bilstm-id98-crf
  58. https://desktop.github.com/
  59. https://desktop.github.com/
  60. https://developer.apple.com/xcode/
  61. https://visualstudio.github.com/
  62. https://github.com/nreimers
  63. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commits?author=nreimers
  64. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/b709f580f11c33c0f9951a0afdc3e71a252c93fd
  65. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/pull/30
  66. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/b709f580f11c33c0f9951a0afdc3e71a252c93fd
  67. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/issues/29
  68. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/b709f580f11c33c0f9951a0afdc3e71a252c93fd
  69. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/b709f580f11c33c0f9951a0afdc3e71a252c93fd
  70. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/master/data
  71. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/27d598d087f2400c97878a55a109e735ca3517bf
  72. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/master/docker
  73. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/e6477ddcd4987ea46e5bd68560bd59b42d27eb17
  74. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/master/docs
  75. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/a8e10e7a8310fbfde5637fadc70d2e21182fb8df
  76. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/master/models
  77. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/2916572778c4d191c3d5a0c384a4da5225bfcec1
  78. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/master/neuralnets
  79. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/10d8bb2248b0f5de0d60a4c504a7266b9676bfe5
  80. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/master/pkl
  81. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/349fb2b25484f2dcff4d45fc53c623dd84e8e42e
  82. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/tree/master/util
  83. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/a87d96d5e45f1b44715ea89d70a67eda2b5d3636
  84. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/.gitignore
  85. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/2017_reimers_optimal hyperparameters for deep lstm-networks.pdf
  86. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/6ad58d43d83af6a92e56772c8c8b0efe4494f07d
  87. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/2017_reimers_reporting score distributions makes a difference.pdf
  88. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/6ad58d43d83af6a92e56772c8c8b0efe4494f07d
  89. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/license
  90. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/notice.txt
  91. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/e83ccfd4f4b64e26f5eb0470149867e0b614f5e2
  92. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/readme.md
  93. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/runmodel.py
  94. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/runmodel_conll_format.py
  95. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/save_and_load.py
  96. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_chunking.py
  97. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_custom_features.py
  98. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/commit/11d14dde4568b94daa053d2b3035cdd554e5e8bf
  99. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_multitask.py
 100. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_multitask_different_levels.py
 101. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_ner_german.py
 102. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_pos.py
 103. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/hyperparameter_results.csv.gz
 104. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/input.conll
 105. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/input.txt
 106. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/requirements.txt
 107. https://arxiv.org/abs/1707.09861
 108. https://arxiv.org/abs/1803.09578
 109. https://arxiv.org/abs/1707.06799
 110. https://arxiv.org/abs/1508.01991
 111. https://arxiv.org/abs/1603.01354
 112. https://arxiv.org/abs/1603.01360
 113. http://anthology.aclweb.org/p16-2038
 114. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_pos.py
 115. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docs/pretrained_models.md
 116. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/train_multitask.py
 117. https://github.com/ukplab/elmo-bilstm-id98-crf
 118. http://arxiv.org/abs/1802.05365
 119. https://arxiv.org/abs/1707.09861
 120. mailto:reimers@ukp.informatik.tu-darmstadt.de
 121. https://www.ukp.tu-darmstadt.de/
 122. https://www.tu-darmstadt.de/
 123. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docker
 124. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docs/training.md
 125. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docs/training_multitask.md
 126. https://arxiv.org/abs/1603.01354
 127. https://arxiv.org/abs/1603.01360
 128. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docs/training.md
 129. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docs/training_multitask.md
 130. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docs/save_load_models.md
 131. https://github.com/ukplab/emnlp2017-bilstm-id98-crf/blob/master/docs/pretrained_models.md
 132. https://github.com/phipleg
 133. https://github.com/fchollet/keras/pull/4621
 134. https://github.com/site/terms
 135. https://github.com/site/privacy
 136. https://github.com/security
 137. https://githubstatus.com/
 138. https://help.github.com/
 139. https://github.com/contact
 140. https://github.com/pricing
 141. https://developer.github.com/
 142. https://training.github.com/
 143. https://github.blog/
 144. https://github.com/about
 145. https://github.com/ukplab/emnlp2017-bilstm-id98-crf
 146. https://github.com/ukplab/emnlp2017-bilstm-id98-crf

   hidden links:
 148. https://github.com/
 149. https://github.com/ukplab/emnlp2017-bilstm-id98-crf
 150. https://github.com/ukplab/emnlp2017-bilstm-id98-crf
 151. https://github.com/ukplab/emnlp2017-bilstm-id98-crf
 152. https://help.github.com/articles/which-remote-url-should-i-use
 153. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#bilstm-id98-crf-implementation-for-sequence-tagging
 154. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#implementation-with-elmo-representations
 155. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#citation
 156. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#setup
 157. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#setup-with-virtual-environment-python-3
 158. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#setup-with-docker
 159. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#running-a-stored-model
 160. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#training
 161. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#multi-task-learning
 162. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#lstm-hyperparameters
 163. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#documentation
 164. https://github.com/ukplab/emnlp2017-bilstm-id98-crf#acknowledgments
 165. https://github.com/
