   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

   [1*zwp7zqlr4cxglji2qrmokq.png]

secret sauce behind the beauty of deep learning: beginners guide to
id180

   [16]go to the profile of mate labs
   [17]mate labs (button) blockedunblock (button) followfollowing
   aug 23, 2017

   id180 are functions which take an input signal and
   convert it to an output signal. id180 introduce
   non-linearity to the networks that is why we call them non-linearities.
   neural networks are universal function approximators and deep neural
   networks are trained using backpropapagation which requires
   [18]differentiable id180. backpropapagation uses
   id119 on this function to update the network weights.
   understanding id180 is very important as they play a
   crucial role in the quality of deep neural networks. in this article i
   am listing and describing different id180.
     * identity or linear activation function         identity or linear
       activation function is the simplest activation function of all. it
       applies identity operation on your data and output data is
       proportional to the input data. problem with linear activation
       function is that it   s derivative is a constant and it   s gradient
       will be a constant too and the descent will be on a constant
       gradient.

   [1*gkll4_ewfpxpszfc4spt1g.png]
   equation for identity or linear activation function

   range: (-   , +   )

   examples: f(2) = 2 or f(-4) = -4
   [1*x0lkaicj7i8ut6dis2o6rg.png]
     * heaviside (binary step, 0 or 1, high or low) step function is
       typically only useful within single-layer id88s, an early
       type of neural networks that can be used for classification in
       cases where the input data is linearly separable. these functions
       are useful for binary classification tasks. the output is a certain
       value, a1, if the input sum is above a certain threshold and a0 if
       the input sum is below a certain threshold. the values used by the
       id88 were a1 = 1 and a0 = 0

   [1*lfkvxbfsysfyuwew5yinfg.png]
   equation for heaveside/binary step function (0 or 1, high or low)

   range: {0 or 1} either o or 1

   examples: f(2) = 1, f(-4) = 0, f(0) = 0, f(1) = 1
   [1*lzmnaj3rbuhgmlc68av1sw.png]
   source         [19]https://en.wikipedia.org/wiki/heaviside_step_function
     * sigmoid or logistic activation function(soft step)-it is mostly
       used for binary classification problems (i.e. outputs values that
       range 0   1) . it has problem of vanishing gradients. the network
       refuses to learn or the learning is very slow after certain epochs
       because input(x) is causing very small change in output(y). it is a
       widely used activation function for classification problems, but
       recently. this function is more prone to saturation of the later
       layers, making training more difficult. calculating derivative of
       sigmoid function is very easy.

   for the id26 process in a neural network, your errors will
   be squeezed by (at least) a quarter at each layer. therefore, deeper
   your network is, more knowledge from the data will be    lost   . some
      big    errors we get from the output layer might not be able to affect
   the synapses weight of a neuron in a relatively shallow layer much
   (   shallow    means it   s close to the input layer)         source
   [20]https://github.com/kulbear/deep-learning-nano-foundation/wiki/relu-
   and-softmax-activation-functions
   [1*bhzlu8wmm1ufo8tltorhoq.png]
   sigmoid or logistic activation function
   [1*q_fcmwpcz4f8ionxm9tqcg.png]
   derivative of the sigmoid function

   range: (0, 1)

   examples: f(4) = 0.982, f(-3) = 0.0474, f(-5) = 0.0067
   [1*g1ajzppuf9eoc2ojwr7myw.png]
   source         [21]https://en.wikipedia.org/wiki/logistic_function
   [1*lkk5q2arktxq0m1hjyzqta.png]
   source         [22]https://github.com/kulbear/deep-learning-nano-foundation/w
   iki/relu-and-softmax-activation-functions
     * hyperbolic tangent (tanh)         it looks like a scaled sigmoid
       function. data is centered around zero, so the derivatives will be
       higher. tanh quickly converges than sigmoid and logistic activation
       functions

   [1*ract-asof6gande2fw07iq.png]
   equation for hyperbolic tangent(tanh) activation function

   range: (-1, 1)

   examples: tanh(2) = 0.9640, tanh(-0.567) = -0.5131, tanh(0) = 0
   [1*rtvgvbji7v8cuucac2qm8q.png]
   source         [23]https://commons.wikimedia.org/wiki/file:hyperbolic_tangent
   .svg
     * rectified linear unit(relu)         it trains 6 times faster than tanh.
       output value will be zero when input value is less than zero. if
       input is greater than or equal to zero, output is equal to the
       input. when the input value is positive, derivative is 1, hence
       there will be no squeezing effect which occurs in the case of
       backpropagating errors from the sigmoid function.

   [1*zh-d-nxmq82joihyjocz3w.png]
   equation for rectified linear unit(relu) activation function

   range: [0, x)

   examples: f(-5) = 0, f(0) = 0 & f(5) = 5
   [1*w275sin5bkaiawbaj6zxca.png]
   source         [24]https://en.wikipedia.org/wiki/rectifier_(neural_networks)
   [1*bqereupqhqgvotjlhpeylq.png]
   from andrej karpathy   s cs231n course
     * leaky rectified linear unit (leaky relu)         leaky relus allow a
       small, non-zero gradient when the unit is not active. 0.01 is the
       small non-zero gradient qhere.

   [1*qvrylfchg7yitx5wwx1r7a.png]
   equation for leaky rectified linear unit (leaky relu) activation
   function

   range: (-   , +   )
   [1*w6burqnue62qyxjxmdpdna.png]
   leaky rectified linear unit(leaky relu)
     * parametric rectified linear unit(prelu)         it makes the coefficient
       of leakage into a parameter that is learned along with the other
       neural network parameters. alpha(  ) is the coefficient of leakage
       here.

   for        1 f(x) = max(x,   x)

   range: (-   , +   )
   [1*pz5_jgegdhewstfovfk_2g.png]
   equation for parametric rectified linear unit(prelu)
   [1*w6burqnue62qyxjxmdpdna.png]
     * randomized leaky rectified linear unit(rrelu)

   range: (-   , +   )
   [1*nwoeuo9nn85fzufr9s5tla.png]
   randomized leaky rectified linear unit(rrelu)
   [1*2heeawgnks0wwgz9i0jqag.png]
     * exponential linear unit (elu)         exponential linear units try to
       make the mean activations closer to zero which speeds up learning.
       it has been shown that elus can obtain higher classification
       accuracy than relus.    is a hyper-parameter here and to be tuned
       and the constraint is        0(zero).

   range: (-  , +   )
   [1*rc2g2zim4lrcnt8gwmhjya.png]
   exponential linear unit (elu)
   [1*gfer6eakdzt8hhf2t7u7lw.png]
     * scaled exponential linear unit (selu)

   range: (-    , +   )
   [1*bjs-0xhab_bsjqmdipeujq.png]
   scaled exponential linear unit(selu)
     * s-shaped rectified linear activation unit (srelu)

   range: (-   , +   )
   [1*u35rsj78t6noozjheodmrq.png]
   s-shaped rectified linear activation unit
     * adaptive piecewise linear (apl)

   range: (-   , +   )
   [1*nnue0odrkerdvadearq4pw.png]
     * softplus         the derivative of the softplus function is the logistic
       function. relu and softplus are largely similar, except near
       0(zero) where the softplus is enticingly smooth and differentiable.
       it   s much easier and efficient to compute relu and its derivative
       than for the softplus function which has log(.) and exp(.) in its
       formulation.

   range: (0,    )
   [1*eyvsonpsbrp5fdna-djnbw.png]
   softplus

   derivative of the softplus function is the logistic function.
   [1*d3ykegimpixp1lst0_uljq.png]
   derivative of the softplus function
   [1*w275sin5bkaiawbaj6zxca.png]
   source         [25]https://en.wikipedia.org/wiki/rectifier_(neural_networks)
     * bent identity

   range: (-   , +   )
   [1*vsfd4_mfx5sa7tdhrczhww.png]
   bent identity
     * softmax- softmax functions convert a raw value into a posterior
       id203. this provides a measure of certainty. it squashes the
       outputs of each unit to be between 0 and 1, just like a sigmoid
       function. but it also divides each output such that the total sum
       of the outputs is equal to 1.

   [1*xipilq5ecmqmudxr4pnh_g.png]
   equation for softmax function

   the output of the softmax function is equivalent to a categorical
   id203 distribution, it tells you the id203 that any of the
   classes are true

   conclusion: relu and it   s variants should be preferred over sigmoid or
   tanh id180. as well as relus are faster to train. if
   relu is causing neurons to be dead, use leaky relus or it   s other
   variants. sigmoid and tanh suffers from vanishing gradient problem and
   should not be used in the hidden layers. relus are best for hidden
   layers. id180 which are easily differentiable and easy
   to train should be used.

   references:
    1. [26]https://en.wikipedia.org/wiki/activation_function
    2. [27]https://github.com/kulbear/deep-learning-nano-foundation/wiki/r
       elu-and-softmax-activation-functions
    3. [28]https://en.wikipedia.org/wiki/rectifier_(neural_networks)
     __________________________________________________________________

about

     at [29]mate labs we have built [30]mateverse, a fully automated
     machine learning platform, where you can build customized ml models
     10x faster without writing a single line of code. we make the jobs
     of analysts and data scientists easier, with proprietary
     technologies vis a vis, complex pipelines, big data support,
     automated id174 (missing value imputation using ml
     models, outlier detection, and formatting), automated hyperparameter
     optimization, and [31]much more.

     to help your business adopt machine learning in a way that won   t end
     up wasting your team   s time in data cleaning, and creating effective
     data models, fill up the [32]typeform [33]here, and we will reach
     out to you.

let   s join hands.

     share your thoughts with us on [34]twitter, linkedin

     tell us if you have some new suggestion. our ears and eyes are
     always open for something really exciting.
     __________________________________________________________________

   iframe: [35]/media/7ccc375a11ef548386b2ce5b1a6488ba?postid=a8e23a57d046

previously we   ve shared.

    1. [36]how to train a machine learning model in 5 minutes.
    2. [37]public data sets: use these to train machine learning models on
       mateverse.
    3. [38]mateverse public beta announcement.
    4. [39]why do we need the democratization of machine learning?
    5. a very crisp explanation of all-id98   s implementation. [40]how these
       researchers tried something unconventional to come out with a
       smaller yet better image recognition.
    6. our vision. [41]what everyone is not telling you about artificial
       intelligence

     * [42]machine learning
     * [43]artificial intelligence
     * [44]data science
     * [45]deep learning
     * [46]data science teams

   (button)
   (button)
   (button) 1.2k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [47]go to the profile of mate labs

[48]mate labs

   we   re trying to enable machine learning and deep learning to one and
   all. irrespective of whether a user knows how to code or not.

     (button) follow
   [49]towards data science

[50]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.2k
     * (button)
     *
     *

   [51]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [52]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a8e23a57d046
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_ifc8kbp7gehh---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@matelabs_ai?source=post_header_lockup
  17. https://towardsdatascience.com/@matelabs_ai
  18. https://en.wikipedia.org/wiki/differentiable_function
  19. https://en.wikipedia.org/wiki/heaviside_step_function
  20. https://github.com/kulbear/deep-learning-nano-foundation/wiki/relu-and-softmax-activation-functions
  21. https://en.wikipedia.org/wiki/logistic_function
  22. https://github.com/kulbear/deep-learning-nano-foundation/wiki/relu-and-softmax-activation-functions
  23. https://commons.wikimedia.org/wiki/file:hyperbolic_tangent.svg
  24. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  25. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  26. https://en.wikipedia.org/wiki/activation_function
  27. https://github.com/kulbear/deep-learning-nano-foundation/wiki/relu-and-softmax-activation-functions
  28. https://en.wikipedia.org/wiki/rectifier_(neural_networks)
  29. http://matelabs.in/
  30. https://www.mateverse.com/
  31. http://bit.ly/2ukmo2j
  32. https://matelabs.typeform.com/to/liaau1
  33. http://bit.ly/formcontactus
  34. https://twitter.com/matelabs_ai
  35. https://towardsdatascience.com/media/7ccc375a11ef548386b2ce5b1a6488ba?postid=a8e23a57d046
  36. https://medium.com/@matelabs_ai/how-to-train-a-machine-learning-model-in-5-minutes-c599fa20e7d5
  37. https://medium.com/@matelabs_ai/public-data-sets-use-these-to-train-machine-learning-models-on-mateverse-4dda18a27851
  38. https://medium.com/@matelabs_ai/big-announcement-mateverse-is-in-public-beta-a968e727cfdc
  39. https://medium.com/startup-grind/why-do-we-need-the-democratization-of-machine-learning-80104e43c76f
  40. https://medium.com/@matelabs_ai/how-these-researchers-tried-something-unconventional-to-came-out-with-a-smaller-yet-better-image-544327f30e72
  41. https://medium.com/startup-grind/what-everyone-is-not-telling-you-about-artificial-intelligence-36c8552f3f53
  42. https://towardsdatascience.com/tagged/machine-learning?source=post
  43. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  44. https://towardsdatascience.com/tagged/data-science?source=post
  45. https://towardsdatascience.com/tagged/deep-learning?source=post
  46. https://towardsdatascience.com/tagged/data-science-teams?source=post
  47. https://towardsdatascience.com/@matelabs_ai?source=footer_card
  48. https://towardsdatascience.com/@matelabs_ai
  49. https://towardsdatascience.com/?source=footer_card
  50. https://towardsdatascience.com/?source=footer_card
  51. https://towardsdatascience.com/
  52. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  54. https://medium.com/p/a8e23a57d046/share/twitter
  55. https://medium.com/p/a8e23a57d046/share/facebook
  56. https://medium.com/p/a8e23a57d046/share/twitter
  57. https://medium.com/p/a8e23a57d046/share/facebook
