   #[1]apple machine learning journal

[2]machine learning journal[3]     machine learning journal

inverse text id172 as a labeling problem

   vol. 1, issue 3     august 2017 august two thousand seventeen by siri
   team
     __________________________________________________________________

   siri displays entities like dates, times, addresses and currency
   amounts in a nicely formatted way. this is the result of the
   application of a process called inverse text id172 (itn) to the
   output of a core id103 component. to understand the
   important role itn plays, consider that, without it, siri would display
      october twenty third twenty sixteen    instead of    october 23, 2016   . in
   this work, we show that itn can be formulated as a labelling problem,
   allowing for the application of a statistical model that is relatively
   simple, compact, fast to train, and fast to apply. we demonstrate that
   this approach represents a practical path to a data-driven itn system.

introduction

   in most id103 systems, a core speech recognizer produces a
   spoken-form token sequence which is converted to written form through a
   process called inverse text id172 (itn). itn includes
   formatting entities like numbers, dates, times, and addresses. table 1
   shows examples of spoken-form input and written-form output.

   table 1. examples of spoken-form input and written-form output
   spoken form written form
   one forty one dorchester avenue salem massachusetts 141 dorchester
   ave., salem, ma
   set an alarm for five thirty p.m. set an alarm for 5:30 pm
   add an appointment on september sixteenth twenty seventeen add an
   appointment on september 16, 2017
   twenty percent of fifteen dollars seventy three 20% of $15.73
   what is two hundred seven point three plus six what is 207.3+6

   our goal is to build a data-driven system for itn. in thinking about
   how we   d formulate the problem to allow us to apply a statistical
   model, we could consider naively tokenizing the written-form output by
   segmenting at spaces. if we were to do that, the output token at a
   particular position would not necessarily correspond to the input token
   at that position. indeed, the number of output tokens would not always
   be equal to the number of input tokens. at first glance, this problem
   appears to require a fairly unconstrained sequence-to-sequence model,
   like those often applied for machine translation.

   in fact, it is possible to train a sequence-to-sequence model for this
   task. but, such models are complex. they can be data hungry, and
   training and id136 can be computationally expensive. in addition,
   the lack of constraint in such models can lead to particularly
   egregious and jarring errors when applied. (see [4][1], where a
   sequence-to-sequence model is trained for a related task.) our
   preliminary experiments with this sort of model did not yield promising
   results.

   in this article, we show that with a few tables and simple grammars, we
   can turn the itn modeling problem into one of labeling spoken-form
   input tokens. by doing so, we are able to apply simpler and more
   compact models that achieve good performance with practical amounts of
   training data.

approach

   we begin with two observations:
    1. there is usually a clear correspondence between spoken-form tokens
       and segments of the written-form output string.
    2. most of the time, the segments in the written-form output are in
       the same order as their corresponding spoken-form tokens.

   given those two observations, we formulate itn as the following three
   step process:
    1. label assignment: assign a label to each spoken-form input token. a
       label specifies edits to perform to the spoken-form token string in
       order to obtain its corresponding written-form segment. a label may
       also indicate the start or end of a region where a post-processing
       grammar should be applied.
    2. label application: apply the edits specified by the labels. the
       result will be a written-form string, possibly with regions marked
       for post-processing.
    3. post-processing: apply the appropriate post-processing grammar to
       any regions marked for post-processing.

   the following two tables give examples of the spoken-form token
   sequences with assigned labels, output after label application, and
   final written-form output after application of post-processing
   grammars. for simplicity, we ignore the fields that have default
   values.

   table 2a. the spoken-form token sequence with corresponding labels for
      february 20, 2017   
   spoken form label label applied written form
   february default february february
   twentieth rewriteordinalascardinaldecade_appendcomma 20, 20,
   twenty rewritecardinaldecade 20 20
   seventeen rewritecardinalteen_spaceno 17 17

   table 2b. the spoken-form token sequence with corresponding labels for
      20% of $205   
   spoken form label label applied written form
   twenty rewritecardinaldecade 20 20
   percent rewritepercentsign_spaceno % %
   of default of of
   two rewritecardinal_startmajorcurrency <majorcurrency>2
   hundred rewritemagnitudepop1_spaceno 0
   five rewritecardinal_spaceno 5
   dollars rewritecurrencysymbol_spaceno_endmajorcurrency
   $<\majorcurrency> $205

labels

   a label is a data structure consisting of the following fields:
    1. rewrite. determines if and how the spoken-form token string should
       be re-written. built-in values for this field include capitalize
       and downcase. the default value represents leaving the spoken-form
       token string as it is.
    2. prepend. determines what string, if any, should be prepended to the
       spoken-form token string. the default value represents prepending
       nothing.
    3. append. determines what string, if any, should be appended to the
       spoken-form token string. the default value represents appending
       nothing.
    4. space. determines whether or not a space should be inserted before
       the spoken-form token string. the default value represents
       inserting a space.
    5. post start. determines if this represents the start of a region
       that needs to be post-processed and, if so, the post-processing
       grammar that should apply.
    6. post end. determines if this represents the end of a region that
       needs to be post-processed, and, if so, the post processing grammar
       that should apply.

   for each field, each value corresponds to a string-to-string
   transformation. we use a finite-state-transducer (fst)   a finite-state
   automaton with both input and output symbols   to encode each
   transformation. the process of applying a label to an input token
   consists of applying the fst for the value in each field in the
   sequence.

   we generate the fsts for the rewrite, prepend, and append fields from
   tables, as shown in the next two tables.

   table 3. excerpts from the rewrite table
        option       spoken form written form
      
   cardinal          one         1
   cardinal          two         2
      
   magnitude         hundred     00
   magnitudepop1     hundred     0
      
   magnitudepop1     million     000,00
   magnitudepop2     million     000,0
      
   abbreviatemeasure pounds      lbs.
   abbreviatemeasure zettabytes  zb
      
   currencysymbol    pounds        
      

   table 4. the append table
   option string
   period .
   comma  ,
   colon  :
   hyphen -
   slash  /
   square ^2
   cube   ^3

post-processing

   for most of the itn transformations we observe, label assignment and
   application would be sufficient to produce the correct written-form
   output. however, a few phenomena require additional post-processing.
   first, in currency expressions, we use a post-processing grammar to
   reorder the currency symbol and major currency amount. second, we use a
   post-processing grammar to properly format relative time expressions
   like    twenty five minutes to four.    finally, we use post-processing
   grammars for formatting roman numerals in spoken-form token sequences
   like    roman numeral five hundred seven.    in this last case, the label
   mechanism would be sufficient, but it is straightforward to write a
   language-independent grammar that converts numbers to roman numerals.
   table 5 gives an example for each of these cases. post-processing
   grammars are compiled into fsts.

   table 5. examples of applying post-processing grammar
                    applying label edits                     written form
   you owe me <majorcurrency>3$</majorcurrency>.50         you owe me $3.50
   meet me at <relativetime>25 minutes to 4</relativetime> meet me at 3:35
   super bowl <romannumeral>51</romannumeral>              super bowl li

label id136

   if we are to train a model to predict labels, we need a way to obtain
   label sequences from spoken form, written form pairs. we do this using
   an fst-based approach. first, we construct the following fsts:
    1. e: an expander fst. this takes a spoken-form token sequence as
       input and produces an output fst that allows all possible labels to
       be prepended to each token.
    2. a[1], a[2]   a[f]: a set of applier fsts, one for each field. these
       take as input a token sequence with labels prepended and apply the
       option specified for field f for each token.
    3. r: a renderer fst. this takes as input a token sequence with labels
       prepended after labels have been applied, and then strips the
       labels. the output is the written form, possibly with regions
       marked for post-processing.
    4. p: a post-processor fst. this takes as input a written form string,
       possibly with regions marked for post-processing and does the
       appropriate post processing. the output is the final written-form
       output.

   we apply the series of operations shown in the following equations to
   an fst, s, constructed from the spoken-form input. note that the
   proj[in] operation copies each arc input label to its output label and
   proj[out] does the opposite. w is an fst constructed from the
   written-form output. label sequences compatible with the spoken
   form/written form pair can easily be extracted from s[labeled]. for a
   vast majority of cases in our data, there is only a single label
   sequence. in cases with multiple compatible label sequences, we choose
   one by using a label bigram model trained from unambiguous cases.
   [math: <mtable class="m-equation" displaystyle="true" style="display:
   block; margin-top: 1.0em; margin-bottom: 2.0em"> <mtr> <mtd> <mspace
   width="6.0em" /> </mtd> <mtd columnalign="left"> <mtable
   class="m-array"> <mtr> <mtd columnalign="right"> <msub> <mi>s</mi>
   <mi>exp</mi> </msub> </mtd> <mtd columnalign="center"> <mo>=</mo>
   </mtd> <mtd columnalign="left"> <mi>p</mi> <mi>r</mi> <mi>o</mi> <msub>
   <mi>j</mi> <mrow> <mi>o</mi> <mi>u</mi> <mi>t</mi> </mrow> </msub>
   <mrow> <mo form="prefix">(</mo> <mi>s</mi> <mo>   </mo> <mi>e</mi> <mo
   form="postfix">)</mo> </mrow> </mtd> </mtr> <mtr> <mtd
   columnalign="right"> <msub> <mi>w</mi> <mrow> <mi>p</mi> <mi>r</mi>
   <mi>o</mi> <mi>c</mi> </mrow> </msub> </mtd> <mtd columnalign="center">
   <mo>=</mo> </mtd> <mtd columnalign="left"> <msub> <mi>s</mi>
   <mi>exp</mi> </msub> <mo>   </mo> <msub> <mi>a</mi> <mrow> <mi>r</mi>
   <mi>e</mi> <mi>w</mi> <mi>r</mi> <mi>i</mi> <mi>t</mi> <mi>e</mi>
   </mrow> </msub> <mo>   </mo> <mo>.</mo> <mo>.</mo> <mo>.</mo> <mo>   </mo>
   <msub> <mi>a</mi> <mrow> <mi>p</mi> <mi>o</mi> <mi>s</mi> <mi>t</mi>
   <mi>e</mi> <mi>n</mi> <mi>d</mi> </mrow> </msub> </mtd> </mtr> <mtr>
   <mtd columnalign="right"> <msub> <mi>s</mi> <mrow> <mi>l</mi>
   <mi>a</mi> <mi>b</mi> <mi>e</mi> <mi>l</mi> <mi>e</mi> <mi>d</mi>
   </mrow> </msub> </mtd> <mtd columnalign="center"> <mo>=</mo> </mtd>
   <mtd columnalign="left"> <mi>p</mi> <mi>r</mi> <mi>o</mi> <msub>
   <mi>j</mi> <mrow> <mi>i</mi> <mi>n</mi> </mrow> </msub> <mrow> <mo
   form="prefix">(</mo> <msub> <mi>w</mi> <mrow> <mi>p</mi> <mi>r</mi>
   <mi>o</mi> <mi>c</mi> </mrow> </msub> <mo>   </mo> <mi>r</mi> <mo>   </mo>
   <mi>p</mi> <mo>   </mo> <mi>w</mi> <mo form="postfix">)</mo> </mrow>
   </mtd> </mtr> </mtable> </mtd> </mtr> </mtable> :math]

example: cardinal numbers

   let   s take a look at how we can cast itn as a labeling problem
   specifically for cardinal numbers. table 6 gives the relevant lines
   from the rewrite table. note that the options with popd suffix, where d
   is a digit, represent turning a magnitude into a sequence of zeros with
   commas in the appropriate places and then removing the last d zeros
   (and any interspersed commas.)

   table 6. excerpt of the rewrite table relevant for formatting cardinals
            option          spoken form written form
      
   cardinal                 one         1
   cardinal                 two         2
      
   cardinaltwodigit         ten         10
   cardinaltwodigit         eleven      11
      
   cardinaldecade           twenty      20
   cardinaldecade           thirty      30
      
   cardinaldecadefirstdigit twenty      2
   cardinaldecadefirstdigit thirty      3
      
   cardinalmagnitudepop0    hundred     00
   cardinalmagnitudepop0    thousand    000
   cardinalmagnitudepop0    million     000,000
      
   cardinalmagnitudepop1    million     000,00
   cardinalmagnitudepop1    billion     000,000,00
      
   cardinalmagnitudepop2    million     000,0
   cardinalmagnitudepop2    billion     000,000,0
      
   cardinalmagnitudepop5    million     0
   cardinalmagnitudepop5    billion     000,0
      
   cardinalmagnitudepop11   trillion    0

   to format cardinal numbers, these rewrite options are combined with the
   no-space option. table 7 demonstrates how this works for a non-trivial
   cardinal number.

   table 7. spoken form, labels and written form for a cardinal number
   spoken form label                                   written form
   twenty      rewritecardinaldecadefirstdigit_spaceno 2
   five        rewritecardinal_appendcomma_spaceno     5,
   thousand    rewritedelete_spaceno
   six         rewritecardinal_spaceno                 6
   hundred     rewritecardinalmagnitudepop1_spaceno    0
   and         rewritedelete
   one         rewritecardinal_spaceno                 1

   the rewrite options are designed such that many different numbers will
   share the same label sequence. this is illustrated in table 8. also
   note that the label sequence at the end of table 8 matches the label
   sequence in table 7. this sharing of label sequences across many
   numbers simplifies the modeling task.

   table 8. spoken form, labels and written form for two cardinal numbers
   with the same label sequence
   spoken form a spoken form b label written form a written form b
   one two rewritecardinal_appendcomma_spaceno 1, 2,
   million billion rewritecardinalmagnitudepop5_spaceno 0 000,0
   twenty thirty rewritecardinaldecadefirstdigit_spaceno 2 3
   five six rewritecardinal_appendcomma_spaceno 5, 6,
   thousand thousand rewritedelete_spaceno
   six seven rewritecardinal_spaceno 6 7
   hundred hundred rewritecardinalmagnitudepop1_spaceno 0 0
   and and rewritedelete_spaceno
   one two rewritecardinal_spaceno 1 2

   an alternative approach, also supported in this framework, would be to
   format cardinals using a post-processing grammar. this would require
   writing a complete number grammar. the advantage of the approach we
   describe here is that only the atomic spoken-form token operations need
   to be specified. we can rely on the label prediction model to learn the
   intricacies of the grammar.

modeling and results

   using this approach to cast itn as a labeling problem, we build a
   system using a bi-directional lstm [5][2], [6][3] as the label
   prediction model. we wish to estimate the performance of the system in
   the scenario where a practical amount of training data (in the form of
   spoken form, written form pairs) is available. in lieu of
   hand-transcribed data, we train and evaluate the system using the
   output of a very accurate rule-based system, with the belief that the
   results will generalize to the hand-transcribed data scenario.

   figure 1 gives the accuracies of our system on randomly selected siri
   utterances using 500k, 1m, and 5m training data. if we zoom in on
   utterances where itn performs a significant transformation, including
   those with dates, times and currency expressions, accuracy exceeds 90%
   for nearly all entity types when training with 1m utterances.
   figure 1. accuracy of the models trained using 500k, 1m and 5m
   utterances [7]shows a chart that compares three models. one that uses
   half a million utterances, another that uses one million utterances,
   and a third that uses five million utterances. the accuracy increases
   with the number of utterances. the three accuracies are 99 point forty
   six percent, 99 point 62 percent, and 99 point 85 percent.

   given these results, if you are starting from scratch to develop an itn
   component, we believe this approach represents a practical path to a
   system that can be deployed at scale.

   in our production scenarios, we begin with reasonably accurate
   rule-based systems, and we have access to very large amounts of real
   spoken-form data. we found that by using the rule-based systems to
   generate written-form outputs, selecting our training data carefully,
   and performing some manual correction of the data, we can build systems
   with this approach that achieve better accuracies than our rule-based
   systems. these new systems will allow us to achieve further
   improvements through additional curation of the training data, a
   process requiring less specialized knowledge than rule writing. in
   addition, these new systems can capture transformations that occur in
   long and varied contexts. such transformations are difficult to
   implement with rules.

   you can find a more thorough description of the modeling approach that
   we took for label prediction, as well as more extensive performance
   evaluation in [8][4].

references

   [1] r.sproat and n. jaitly, id56 approaches to text id172: a
   challenge, arxiv preprint arxiv:1611.00068, 2016.
   https://arxiv.org/abs/1611.00068

   [2] m. schuster and k. k. paliwal, bidirectional recurrent neural
   networks, ieee transactions on signal processing, vol. 45, no. 11, pp.
   2673   2681, 1997.

   [3] s. hochreiter and j. schmidhuber, long short-term memory, neural
   computation, vol. 9, no. 8, pp. 1735   1780, 1997.

   [4] e. pusateri, b.r. ambati, e. brooks, o. platek, d. mcallaster, v.
   nagesha, [9]a mostly data-driven approach to inverse text
   id172, interspeech, 2017.

contact us

   [10]send questions or feedback via email

jobs at apple

   [11]apply now for a career at apple

tools for innovation

   [12]apple developer program

   [13]    copyright    2018 apple inc. all rights reserved. copyright two
   thousand seventeen apple inc. all rights reserved.
   [14]subscribe [15]privacy policy [16]terms of use [17]legal

references

   1. https://machinelearning.apple.com/feed.xml
   2. https://machinelearning.apple.com/
   3. https://machinelearning.apple.com/
   4. https://machinelearning.apple.com/2017/08/02/inverse-text-normal.html#1
   5. https://machinelearning.apple.com/2017/08/02/inverse-text-normal.html#2
   6. https://machinelearning.apple.com/2017/08/02/inverse-text-normal.html#3
   7. https://machinelearning.apple.com/images/journals/itn/itn-accuracy.png
   8. https://machinelearning.apple.com/2017/08/02/inverse-text-normal.html#4
   9. http://isca-speech.org/archive/interspeech_2017/abstracts/1274.html
  10. mailto:machine-learning@apple.com
  11. https://www.apple.com/jobs/
  12. https://developer.apple.com/
  13. https://www.apple.com/
  14. https://machinelearning.apple.com/feed.xml
  15. https://www.apple.com/privacy/privacy-policy/
  16. https://www.apple.com/legal/internet-services/terms/site.html
  17. https://www.apple.com/legal/
