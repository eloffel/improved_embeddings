   (button) toggle navigation [1]mlx
     * [2]home
     * [3]archives
     *

the stanford id53 dataset

background, challenges, progress

   by pranav rajpurkar on april 3rd 2017

   id53 is an important nlp task and longstanding milestone
   for artificial intelligence systems. qa systems allow a user to ask a
   question in natural language, and receive the answer to their question
   quickly and succinctly. today, qa systems are used in search engines
   and in phone conversational interfaces, and are pretty good at
   answering simple factoid questions. but on more complex questions,
   these usually only go so far as to return a list of snippets that we
   the users then have to browse through to have our question answered.

   the ability to read a piece of text and then answer questions about it
   is called reading comprehension. reading comprehension is challenging
   for machines, requiring both understanding of natural language and
   knowledge about the world.

   how can we get a machine to make progress on the challenging task of
   reading comprehension? historically, large, realistic datasets have
   played a critical role in driving fields forward     one famous example
   is id163 for visual recognition.

   in reading comprehension, we mainly find two kinds of datasets: those
   that are automatically generated, and those that are manually
   generated. the automatically generated datasets are cloze style, where
   the task is to fill in a missing word or entity, and is a clever way to
   generate datasets that test reading skills. the manually generated
   datasets follow a setup that is closer to the end goal of question
   answering, and other downstream qa applications. however, these
   manually generated datasets are usually small, and insufficient in
   scale for data intensive deep learning methods.

   to address the need for a large and high-quality reading comprehension
   dataset, we introduce the stanford id53 dataset, also
   known as squad. at 100,000 question-answer pairs, it is almost two
   orders of magnitude larger than previous manually labeled reading
   comprehension datasets such as mctest.

the squad setting

   the reading passages in squad are from high-quality wikipedia articles,
   and cover a diverse range of topics across a variety of domains, from
   music celebrities to abstract concepts. a passage is a paragraph from
   an article, and is variable in length. each passage in squad has
   accompanying reading comprehension questions. these questions are based
   on the content of the passage and can be answered by reading through
   the passage. finally, for each question, we have one or more answers.

   one defining characteristic of squad is that the answers to all of the
   questions are
   segments of text, or spans, in the passage. these can be single or
   multiple words, and are not limited to entities     any span is fair
   game.
   answers are spans in the passage answers are spans in the passage

   this is quite a flexible setup, and we find that a diverse range of
   questions can be asked in the span setting. rather than having a list
   of answer choices for each question, systems must select the answer
   from all possible spans in the passage, thus needing to cope with a
   fairly large number of candidates. spans comes with the added bonus
   that they are easy to evaluate.

   in addition, the span-based qa setting is quite natural. for many user
   questions into search engines, open-domain qa systems are often able to
   find the right documents that contain the answer. the challenge is the
   last step of    answer extraction   , which is to find the shortest segment
   of text in the passage or document that answers the question.

   before we dive into the dataset, let   s understand the data collection
   process. squad is a large crowdsourced effort. on each paragraph,
   crowdworkers were tasked with asking and answering several questions on
   the content of that passage. the questions had to be entered in a text
   field, and the answers highlighted in the passage. to guide the
   workers, we had examples of good and bad questions. finally,
   crowdworkers were encouraged to ask questions in their own words,
   without copying word phrases from the passage. the result     a more
   challenging dataset, where simple string matching techniques will often
   fail to find correspondences between passage words and question words.
   squad data collection interface squad data collection interface

a taste of challenges in squad

   because crowdworkers are asked to pose questions in their own words,
   question words are often synonyms of words in the passage     this is
   lexical variation because of synonymy. in a few hundred examples that
   we manually annotated, this case was fairly frequent, necessary in
   about 33% of questions.
   in this example, a qa system would have to recognize that    referred   
   and    call    mean the same thing. in this example, a qa system would have
   to recognize that    referred    and    call    mean the same thing.

   the second type of reasoning we look at is lexical variation that needs
   external knowledge to reason about.
   to answer this question, qa systems have to infer that the european
   parliament and the council of the european union are government bodies.
   such questions are difficult to answer because they go beyond the
   passage. to answer this question, qa systems have to infer that the
   european parliament and the council of the european union are
   government bodies. such questions are difficult to answer because they
   go beyond the passage.

   other than lexical variation, we also have syntactic variation, which
   compares the syntactic structure of the question with the syntactic
   structure of the passage.
   here   s a question which does not require handling of syntactic
   variation. the question and the passage have matching syntactic
   structures    who went to wittenberg   ,    students thronged to wittenberg   
   even though the the question uses the word    went    and the passage uses
   the word    thronged   . questions without syntactic variation are
   relatively easy to answer because the syntactic structure gives all of
   the information needed to answer it. here   s a question which does not
   require handling of syntactic variation. the question and the passage
   have matching syntactic structures    who went to wittenberg   ,    students
   thronged to wittenberg    even though the the question uses the word
      went    and the passage uses the word    thronged   . questions without
   syntactic variation are relatively easy to answer because the syntactic
   structure gives all of the information needed to answer it. here is a
   case which does exhibit syntactic variation. comparing the parse trees
   of the question and the sentence in the passage, we find that their
   structure is fairly different. reasoning about syntactic variation is
   required very frequently, necessary in over 60% of the questions that
   we annotated. here is a case which does exhibit syntactic variation.
   comparing the parse trees of the question and the sentence in the
   passage, we find that their structure is fairly different. reasoning
   about syntactic variation is required very frequently, necessary in
   over 60% of the questions that we annotated.

   finally there is multi-sentence reasoning. for these kind of questions,
   we need to use multiple sentences in the passage to answer them. much
   of the time, this involves conference resolution to identify the entity
   that a pronoun refers to.
   an example case that requires multi-sentence reasoning. an example case
   that requires multi-sentence reasoning.

   now that we   ve looked at the diversity of questions in squad, let   s
   look at the diversity of answers in the dataset. many qa systems
   exploit the expected answer type when answering a question. for
   instance, if there is a    how many    question, a qa system might only
   consider answer candidates which are numbers. in squad, answer types in
   squad are wide-ranging, and often include non-entities and long
   phrases. this makes squad more challenging and more diverse than
   datasets where answers are restricted to be of a certain type.
   diversity of answer types. diversity of answer types.

squad models and results

   squad uses two different metrics to evaluate how well a system does on
   the benchmark. the exact match metric measures the percentage of
   predictions that match any one of the ground truth answers exactly. the
   f1 score metric is a looser metric measures the average overlap between
   the prediction and ground truth answer.

   we first assess human performance on squad. to evaluate human
   performance, we treat the one of the crowdsourced answers as the human
   prediction, and keep the other answers as ground truth answers. the
   resulting human performance score on the test set is 82.3% for the
   exact match metric, and 91.2% f1.
   human performance human performance

   to compare the performance of machines with the performance of humans,
   we implemented a few baselines. our first baseline is a sliding window
   baseline, in we extract a large number of possible answer candidates
   from the passage, and then match a bag of words constructed from the
   question and candidate answer to the text to rank them. using this
   baseline, we get an f1 score of 20.

   compared with human performance on squad, machines seem like a really
   long way with this baseline. but we haven   t yet incorporated any
   learning into our system. and we expect with a large dataset, learning
   can do well.

   to improve upon the sliding window baseline, we implemented a logistic
   regression baseline that scores candidate answers. the logistic
   regression uses a range of features     let   s touch on the features we
   found to be most important, namely the lexicalized features, and
   dependency tree path features.

   let   s first look at lexicalized features.
   question word lemmas are combined with answer word lemmas to form pairs
   like these. question word lemmas are combined with answer word lemmas
   to form pairs like these. we also combine question words with passage
   sentence words that are close to the answer. we also combine question
   words with passage sentence words that are close to the answer.

   next, let   s look at dependency features.
   we use the dependency tree path from the passage sentence words that
   occur in the question to the answer in the passage. this is optionally
   combined with the path from the wh-word to the same question word. we
   use the dependency tree path from the passage sentence words that occur
   in the question to the answer in the passage. this is optionally
   combined with the path from the wh-word to the same question word.

   using these features, we build a id28 model which sits
   between the sliding window baseline and human performance. we note that
   the model is able to select the sentence containing the answer
   correctly with 79.3% accuracy; hence, the bulk of the difficulty lies
   in finding the exact span within the sentence.
   comparing performances comparing performances

   reading comprehension is a challenging task for machines. comprehension
   refers to the ability to go beyond words, to understand the ideas and
   relationships between ideas conveyed in a text. the trec paper, written
   at the start of the millennium, that introduced one of the early qa
   benchmarks, opens by mentioning that a successful evaluation requires a
   task that is neither too easy nor too difficult for the current
   technology. if the task is simple, all systems do well and nothing is
   learned. similarly, if the task is too difficult, all systems do poorly
   and again nothing is learned.

   since our paper came out in july 2016, we have witnessed significant
   improvements from deep learning models, and have had many submissions
   compete to get state of the art results. we expect that the remaining
   gap will be harder to close, but that such efforts will result in
   significant advances in machine comprehension of text.

   you can [4]check out the leaderboard, explore the dataset and visualize
   model predictions. all of the data and experiments are on [5]codalab,
   which we use for official evaluation of models.
     __________________________________________________________________

comments:

   please enable javascript to view the [6]comments powered by disqus.

also read
     __________________________________________________________________

   [7]

id71

   by pranav rajpurkar on august 31st 2017
   [8]

cardiotoxicity prediction

   by jeremy irvin, pranav rajpurkar on october 7th 2017
   [9]

arrhythmia detection

   by pranav rajpurkar on september 13th 2017
     __________________________________________________________________

   [10]#nlp [11]#squad
     __________________________________________________________________

     *
     *
     *

references

   visible links
   1. https://rajpurkar.github.io/mlx/
   2. https://rajpurkar.github.io/mlx/
   3. https://rajpurkar.github.io/mlx/archives
   4. https://stanford-qa.com/
   5. https://worksheets.codalab.org/worksheets/0x62eefc3e64e04430a1a24785a9293fff/
   6. https://disqus.com/?ref_noscript
   7. https://rajpurkar.github.io/mlx/dialog-systems/
   8. https://rajpurkar.github.io/mlx/chemo-cardiotoxicity/
   9. https://rajpurkar.github.io/mlx/arrhythmia-detection/
  10. https://rajpurkar.github.io/mlx/tags/nlp/
  11. https://rajpurkar.github.io/mlx/tags/squad/

   hidden links:
  13. https://github.com/rajpurkar/mlx
  14. https://rajpurkar.github.io/mlx/qa-and-squad/#the-squad-setting
  15. https://rajpurkar.github.io/mlx/qa-and-squad/#a-taste-of-challenges-in-squad
  16. https://rajpurkar.github.io/mlx/qa-and-squad/#squad-models-and-results
  17. https://twitter.com/pranavrajpurkar
  18. https://github.com/rajpurkar/mlx
  19. mailto:pranavsr@cs.stanford.edu
