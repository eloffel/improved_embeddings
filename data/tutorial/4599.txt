   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep learning journal
   [7]deep learning journal
   [8]sign in[9]get started
     __________________________________________________________________

   [1*2r2vbwx60_ycfi2utrp-ew.jpeg]

faster ai: lesson 0         tl;dr version of fast.ai part 1

   [10]go to the profile of kshitiz rimal
   [11]kshitiz rimal (button) blockedunblock (button) followfollowing
   aug 23, 2017
     __________________________________________________________________

   let me start off by saying why i choose to write tl;dr version of
   fast.ai part 1 course. out of so many courses on deep learning on the
   internet, i honestly believe this course should be at the top. there
   are couple of reasons for that.

   one major reason is its free and its not just free but it also provide
   all the materials required for you to follow along the course,
   including [12]lesson notes, [13]exercise files and [14]a dynamic forum
   to ask anything if you ever got stuck.

   another reason is the way the materials are taught. most of the
   courses, including the famous coursera course on deep learning, it is
   taught using what i like to call, a more traditional approach, that is,
   it starts off by explaining elementary things in much details and
   gradually moves up to more larger pieces of the puzzle. i am not saying
   this approach is bad but certainly for me it was not an effective one.

   in this course however, things are little more interesting, you start
   off by implementing an actual image classifier using deep neural
   networks. this direct exposure gives you overall picture of how things
   are moving in grand scale. then slowly the instructor jeremy howard, he
   goes in detail of each pieces of the neural networks and explain things
   along the way. and at the end of the course you will learn the same
   amount of knowledge from any other highly acclaimed courses on deep
   learning but with more confidence on practicality of things with better
   implementation experience.

   this series of tl;dr posts by no means are replacement to this amazing
   course, but instead what i like to achieve here is to reduce some
   shortcomings of this course, that is its length of each videos. each
   video is around 2 hours long, which i think for new learner is bit
   overwhelming even before taking the course and many of you might have
   just drifted away because of that. what i wish is, you follow along
   these tl;dr versions and actually take the entire course after that.
   there is no hard and fast rule to actually learn deep learning and the
   length of those videos might put you off but they actually worth it and
   you will learn a lot, i promise.

   in this [15]lesson 0 of [16]deep learning for coders part 1, jeremy
   starts off by explaining why deep learning is important and its not
   just a fad but a major force of change and how it is changing the shape
   of the society. he gives his gentle introduction, his past experience
   with [17]kaggle as chief scientist there, his [18]medical startup which
   he built around using deep learning and some ideas about what this
   course is going to be.

   neural networks in general are the models which are built with the idea
   of layers stacking up and with input provided at one end, for example:
   images, texts or any data, these inputs are passed through the layers
   and each layer learn something different about the same input. using
   this concept, jeremy tackles a famous image classifying approach using
   [19]mnist dataset and shows us how visually each layer of neural
   network learns the passed image of hand written digit from this
   dataset.

   generally dealing with images, convolutional neural network is used
   mostly because of its better accuracy results. what exactly a
   convolutional neural network is, that is covered in following lessons
   but for this one, he explains it with a more intuitive approach.

   you have this image from dataset
   [1*xnhe4jwt4deoo5wicfjnua.png]

   and suppose you initialize a matrix with these values:
   [1*h07oststr6zwqetltbw8pa.png]

   which visually looks like a strip of black, white and grey. now if you
   correlate these two entities, and correlate in simple terms means to
   multiply each element of that top matrix with that image    7    matrix,
   this is what you get as a result:
   [1*fjgbqhnvh8xg_74wd0ktza.png]

   this same approach is used in convolutional models to detect edges of
   the images, but instead of handcrafting the values of the matrix to
   multiply with the image, these values are generated randomly and there
   are many numbers of these matrices and each detect something different
   from the image, some detect edges, some corners and if you use models
   with many layers deep down, they can also detect part of objects as
   well.

   to explain this visually, he tries out some other values for this top
   matrix basically to detect edges in different angles: horizontal,
   vertical, diagonal and shows us how each of those output looks like
   with different values of the matrix.

   these matrices which is multiplied with images are called kernel or
   filters and in this article they are visually explained:
   [20]http://setosa.io/ev/image-kernels.

   you can read the lesson notes as well for more details:
   [21]http://wiki.fast.ai/index.php/lesson_0

   while explaining the process behind this correlation, jeremy goes
   through different inner steps of a convolutional network. and he
   explained how convolution is also very much similar to correlation with
   minor difference of rotating that filter by 180 degrees. he then goes
   through the concept of max pooling [[22]time: 29:45] and how they are
   used by the neural network to recognize the images.

   in deep learning there are many number of filters and many layers in
   the model and to train a model simply means to optimize these values of
   filters with each iterations and make them able to recognize images
   with better accuracy. he then explained this process in a more simpler
   way with an example of a straight line, and uses famous linear
   regression approach but in deep learning way. [[23]time: 37:00]

   he uses simple equation of the line: y= a*x+b and calculates the values
   of y with a,b and x given. now imagine we have list of x values and y
   values only and now the job of the model is to find the close to real
   values of these a and b. it does so by the process of optimization. in
   each iteration it calculates a loss, which is difference in values of y
   predicted by the model with random values of a and b with the real
   values of y with accurate a and b. so with the help of this loss
   function and through the process of optimization, it recalculates the
   values of a and b in each iteration and by the end of certain number of
   iterations, the values are close to real a and b and so the loss is
   quite low.

   as this lesson 0 is more of an overview session and only conceptual
   part is covered here but following lessons will be more technical and
   each lesson will deal with more complex ideas in deep learning.

   regarding this particular lesson 0, i highly recommend you to watch it
   as it is also very short one (around 45 minutes) compared to other
   lessons. it will give you basic idea what will be covered in whole
   course and how he is going to cover various concepts. besides lesson 0,
   there is also [24]lesson 1 overview video which is generally about what
   the course is going to be, who is this course for and what are the
   differences between this course and others. this overview video is also
   quite short (around 30 minutes), please go through that as well.

   in this lesson he went through all these high level concepts and
   terminologies, the reason for that is to give brief glimpse of what
   this course is about. he went through image recognition using hand
   written digits dataset because later on in the course, we will be
   learning to tackle similar problem but with different dataset of cats
   and dogs and use that to classify between them, using different type of
   convolutional neural network.

   the reason behind going through the concept of straight line and how a
   neural network learns is that, on higher intuitive level, if we can
   understand this simple phenomenon we can understand the workings of
   larger neural networks as well.

   i hope you get the basic idea what i will be writing about in this blog
   series and i hope you will follow along with me. see you on next post.

next post: [25]faster ai: lesson 1

     * [26]machine learning
     * [27]artificial intelligence
     * [28]deep learning

   (button)
   (button)
   (button) 169 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of kshitiz rimal

[30]kshitiz rimal

   ai developer, intel ai student ambassador, co-founder @ ai for
   development: ainepal.org, city ai ambassador: kathmandu

     (button) follow
   [31]deep learning journal

[32]deep learning journal

   deep learning lessons

     * (button)
       (button) 169
     * (button)
     *
     *

   [33]deep learning journal
   never miss a story from deep learning journal, when you sign up for
   medium. [34]learn more
   never miss a story from deep learning journal
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/dd060ae7e319
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-learning-journals?source=avatar-lo_tdiodkj6sa45-aa98d6a0017e
   7. https://medium.com/deep-learning-journals?source=logo-lo_tdiodkj6sa45---aa98d6a0017e
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-0-tl-dr-version-of-fast-ai-part-1-dd060ae7e319&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-0-tl-dr-version-of-fast-ai-part-1-dd060ae7e319&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@kshitizrimal?source=post_header_lockup
  11. https://medium.com/@kshitizrimal
  12. http://wiki.fast.ai/index.php/course_notes
  13. https://github.com/fastai/courses/tree/master/deeplearning1
  14. http://forums.fast.ai/
  15. https://www.youtube.com/watch?v=acu-t9l4_li
  16. https://www.youtube.com/playlist?list=plfyubjixbdts2uqrzyrxmyvhogw0gmlsm
  17. http://kaggle.com/
  18. https://www.enlitic.com/
  19. http://yann.lecun.com/exdb/mnist/
  20. http://setosa.io/ev/image-kernels/
  21. http://wiki.fast.ai/index.php/lesson_0
  22. https://youtu.be/acu-t9l4_li?t=1784
  23. https://youtu.be/acu-t9l4_li?t=2221
  24. https://www.youtube.com/watch?v=kzt3-fhdaem&index=1&list=plfyubjixbdts2uqrzyrxmyvhogw0gmlsm
  25. https://medium.com/deep-learning-journals/faster-ai-lesson-1-tl-dr-version-of-fast-ai-part-1-d63ad6b2993b
  26. https://medium.com/tag/machine-learning?source=post
  27. https://medium.com/tag/artificial-intelligence?source=post
  28. https://medium.com/tag/deep-learning?source=post
  29. https://medium.com/@kshitizrimal?source=footer_card
  30. https://medium.com/@kshitizrimal
  31. https://medium.com/deep-learning-journals?source=footer_card
  32. https://medium.com/deep-learning-journals?source=footer_card
  33. https://medium.com/deep-learning-journals
  34. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  36. https://medium.com/p/dd060ae7e319/share/twitter
  37. https://medium.com/p/dd060ae7e319/share/facebook
  38. https://medium.com/p/dd060ae7e319/share/twitter
  39. https://medium.com/p/dd060ae7e319/share/facebook
