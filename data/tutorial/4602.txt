   #[1]github [2]recent commits to im2markup:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]37
     * [35]star [36]576
     * [37]fork [38]106

[39]harvardnlp/[40]im2markup

   [41]code [42]issues 7 [43]pull requests 1 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   neural model for converting image-to-markup (by yuntian deng
   github.com/da03) [47]http://lstm.seas.harvard.edu/latex
     * [48]25 commits
     * [49]1 branch
     * [50]0 releases
     * [51]fetching contributors
     * [52]mit

    1. [53]lua 62.9%
    2. [54]python 30.3%
    3. [55]javascript 6.8%

   (button) lua python javascript
   branch: master (button) new pull request
   [56]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/h
   [57]download zip

downloading...

   want to be notified of new releases in harvardnlp/im2markup?
   [58]sign in [59]sign up

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [62]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [63]download the github extension for visual studio
   and try again.

   (button) go back
   [64]@da03
   [65]da03 [66]update preprocess_formulas.py
   latest commit [67]2a5b7c1 dec 22, 2016
   [68]permalink
   type       name            latest commit message        commit time
        failed to load latest commit information.
        [69]data/sample [70].                             sep 19, 2016
        [71]scripts     [72]update preprocess_formulas.py dec 23, 2016
        [73]src         [74]delete latex_vocab.lua        dec 17, 2016
        [75]third_party [76]docs                          dec 19, 2016
        [77].gitignore
        [78]license
        [79]readme.md

readme.md

im2markup

   a general-purpose, deep learning-based system to decompile an image
   into presentational markup. for example, we can infer the latex or html
   source from a rendered image.

   [80][687474703a2f2f6c73746d2e736561732e686172766172642e6564752f6c617465
                         782f6e6574776f726b2e706e67]

   an example input is a rendered latex formula:

   [81][687474703a2f2f6c73746d2e736561732e686172766172642e6564752f6c617465
   782f726573756c74732f776562736974652f696d616765732f313139623933613434352
                             d6f7269672e706e67]

   the goal is to infer the latex formula that can render such an image:
 d s _ { 1 1 } ^ { 2 } = d x ^ { + } d x ^ { - } + l _ { p } ^ { 9 } \frac { p _
 { - } } { r ^ { 7 } } \delta ( x ^ { - } ) d x ^ { - } d x ^ { - } + d x _ { 1
} ^ { 2 } + \; \cdots \; + d x _ { 9 } ^ { 2 }

   our model employs a convolutional network for text and layout
   recognition in tandem with an attention-based neural machine
   translation system. the use of attention additionally provides an
   alignment from the generated markup to the original source image:

   [82][687474703a2f2f6c73746d2e736561732e686172766172642e6564752f6c617465
                          782f6d61746865782e706e67]

   see [83]our website for a complete interactive version of this
   visualization over the test set. our paper
   ([84]http://arxiv.org/pdf/1609.04938v1.pdf) provides more technical
   details of this model.
what you get is what you see: a visual markup decompiler
yuntian deng, anssi kanervisto, and alexander m. rush
http://arxiv.org/pdf/1609.04938v1.pdf

prerequsites

   most of the code is written in [85]torch, with python for
   preprocessing.

torch

model

   the following lua libraries are required for the main model.
     * tds
     * class
     * nn
     * nngraph
     * cunn
     * cudnn
     * cutorch

   note that currently we only support gpu since we use cudnn in the id98
   part.

preprocess

   python
     * pillow
     * numpy

   optional: we use node.js and katex for preprocessing [86]installation

pdflatex [87]installaton

   pdflatex is used for rendering latex during evaluation.

imagemagick convert [88]installation

   convert is used for rending latex during evaluation.

webkit2png [89]installation

   webkit2png is used for rendering html during evaluation.

evaluate

   python image-based evaluation
     * python-levenshtein
     * matplotlib
     * distance

wget http://lstm.seas.harvard.edu/latex/third_party/distance-0.1.3.tar.gz

tar zxf distance-0.1.3.tar.gz

cd distance; sudo python setup.py install

perl [90]installation

   perl is used for evaluating id7 score.

usage

   we assume that the working directory is im2markup throught this
   document. the task is to convert an image into its presentational
   markup, so we need to specify a data_base_dir storing the images, a
   label_path storing all labels (e.g., latex formulas). besides, we need
   to specify a data_path for the training (or test) data samples. the
   format of data_path shall look like:
<img_name1> <label_idx1>
<img_name2> <label_idx2>
<img_name3> <label_idx3>
...

   where <label_idx> denotes the line index of the label (starting from
   0).

quick start (math-to-latex toy example)

   to get started with, we provide a toy math-to-latex example. we have a
   larger dataset [91]im2latex-100k-dataset of the same format but with
   much more samples.

preprocess

   the images in the dataset contain a latex formula rendered on a full
   page. to accelerate training, we need to preprocess the images.
python scripts/preprocessing/preprocess_images.py --input-dir data/sample/images
 --output-dir data/sample/images_processed

   the above command will crop the formula area, and group images of
   similar sizes to facilitate batching.

   next, the latex formulas need to be tokenized or normalized.
python scripts/preprocessing/preprocess_formulas.py --mode normalize --input-fil
e data/sample/formulas.lst --output-file data/sample/formulas.norm.lst

   the above command will normalize the formulas. note that this command
   will produce some error messages since some formulas cannot be parsed
   by the katex parser.

   then we need to prepare train, validation and test files. we will
   exclude large images from training and validation set, and we also
   ignore formulas with too many tokens or formulas with grammar errors.
python scripts/preprocessing/preprocess_filter.py --filter --image-dir data/samp
le/images_processed --label-path data/sample/formulas.norm.lst --data-path data/
sample/train.lst --output-path data/sample/train_filter.lst

python scripts/preprocessing/preprocess_filter.py --filter --image-dir data/samp
le/images_processed --label-path data/sample/formulas.norm.lst --data-path data/
sample/validate.lst --output-path data/sample/validate_filter.lst

python scripts/preprocessing/preprocess_filter.py --no-filter --image-dir data/s
ample/images_processed --label-path data/sample/formulas.norm.lst --data-path da
ta/sample/test.lst --output-path data/sample/test_filter.lst

   finally, we generate the vocabulary from training set. all tokens
   occuring less than (including) 1 time will be excluded from the
   vocabulary.
python scripts/preprocessing/generate_latex_vocab.py --data-path data/sample/tra
in_filter.lst --label-path data/sample/formulas.norm.lst --output-file data/samp
le/latex_vocab.txt

train

   for a complete set of parameters, run
th src/train.lua -h

   the most important parameters for training are data_base_dir, which
   specifies where the images live; data_path, the training file;
   label_path, the latex formulas, val_data_path, the validation file;
   vocab_file, the vocabulary file with one token per each line.
th src/train.lua -phase train -gpu_id 1 \
-model_dir model \
-input_feed -prealloc \
-data_base_dir data/sample/images_processed/ \
-data_path data/sample/train_filter.lst \
-val_data_path data/sample/validate_filter.lst \
-label_path data/sample/formulas.norm.lst \
-vocab_file data/sample/latex_vocab.txt \
-max_num_tokens 150 -max_image_width 500 -max_image_height 160 \
-batch_size 20 -beam_size 1

   in the default setting, the log file will be put to log.txt. the log
   file records the training and validation perplexities. model_dir
   speicifies where the models should be saved. the default parameters are
   optimized for the full dataset. in order to overfit on this toy
   example, use flags -learning_rate 0.05, -lr_decay 1.0 and -num_epochs
   30, then after 30 epochs, the training perplexity can reach around 1.1
   and the validation perplexity can only reach around 17.

test

   after training, we can load a model and use it to test on test dataset.
   we provide a model trained on the [92]im2latex-100k-dataset.
mkdir -p model/latex; wget -p model/latex/ http://lstm.seas.harvard.edu/latex/mo
del/latex/final-model

   now we can load the model and test on test set. note that in order to
   output the predictions, a flag -visualize must be set.
th src/train.lua -phase test -gpu_id 1 -load_model -model_dir model/latex -visua
lize \
-data_base_dir data/sample/images_processed/ \
-data_path data/sample/test_filter.lst \
-label_path data/sample/formulas.norm.lst \
-output_dir results \
-max_num_tokens 500 -max_image_width 800 -max_image_height 800 \
-batch_size 5 -beam_size 5

   note that we do not specify a vocabulary file here, since it is already
   included in the model. after a while, the perplexities will be logged,
   and the predictions file results.txt will be put to output_dir. the
   format of the predicitons file is:
<img_name1>\t<label_gold1>\t<label_pred1>\t<score_pred1>\t<score_gold1>
<img_name2>\t<label_gold2>\t<label_pred>2\t<score_pred2>\t<score_gold2>
...

   where \t denotes tab.

evaluate

text metrics

   the test perplexity can be obtained after testing is finished. in order
   to evaluate id7, the following command needs to be executed.
python scripts/evaluation/evaluate_id7.py --result-path results/results.txt --d
ata-path data/sample/test_filter.lst --label-path data/sample/formulas.norm.lst

   note that although the predicions file contains the gold labels, since
   some images (e.g., too large sizes) will be ignored during testing, to
   make the comparison fair, we need to use the test file again and treat
   those that does not appear in predictions file as blank predictions.

   we also provide script for evaluating text id153 (on a token
   level).
python scripts/evaluation/evaluate_text_edit_distance.py --result-path results/r
esults.txt

image metrics

   first, we need to render both the original formulas and the predicted
   formulas. this may take a bit long time. the rendering process depends
   on [93]pdflatex and [94]imagemagick convert.
python scripts/evaluation/render_latex.py --result-path results/results.txt --da
ta-path data/sample/test_filter.lst --label-path data/sample/formulas.lst --outp
ut-dir data/sample/images_rendered --no-replace

   afterwards, based on the rendered images, we evaluate the exact match
   accuracy and the image id153.
python scripts/evaluation/evaluate_image.py --images-dir data/sample/images_rend
ered/

web page-to-html

   another example is to infer the html from an image of a web page. we
   provide a simplified dataset: web pages of size 100x100. (however, in
   the provided dataset, we downsample to 64x64). note that we can use the
   same model parameters as the math-to-latex task, the only difference
   here is the vocabulary.

   first, download the dataset.
wget -p data/ http://lstm.seas.harvard.edu/latex/html/data/html_64_64_100k.tgz

cd data; tar zxf html_64_64_100k.tgz; cd ..

train

   the training parameters is nearly identical to the math-to-latex task.
   however, some parameters such as max_image_width need to be set to a
   different value for memory efficiency and convergence speed.
th src/train.lua -phase train -gpu_id 1 \
-input_feed -prealloc \
-data_base_dir data/html_64_64_100k/data/ \
-data_path data/html_64_64_100k/html_train.txt \
-vocab_file data/html_64_64_100k/html_vocab.txt \
-val_data_path data/html_64_64_100k/html_dev.txt \
-label_path data/html_64_64_100k/html_sources.txt \
-vocab_file data/html_64_64_100k/html_vocab.txt \
-max_num_tokens 100 -max_image_width 64 -max_image_height 64 \
-batch_size 100 -beam_size 1

test

   we provide a trained model as well.
mkdir -p model/html; wget -p model/html/ http://lstm.seas.harvard.edu/latex/mode
l/html/final-model

   now we are ready to test our model.
th src/train.lua -phase test -gpu_id 1 -load_model -model_dir model/html -visual
ize \
-data_base_dir data/html_64_64_100k/data/ \
-data_path data/html_64_64_100k/html_test.txt \
-label_path data/html_64_64_100k/html_sources.txt \
-output_dir results \
-max_num_tokens 400 -max_image_width 64 -max_image_height 64 \
-batch_size 80 -beam_size 5

evaluate

text metrics

   the test perplexity can be obtained after testing is finished. in order
   to evaluate text id153, the following command needs to be
   executed.
python scripts/evaluation/evaluate_text_edit_distance.py --result-path results/r
esults.txt

image metrics

   first, we need to render both the original htmls and the predicted
   htmls. this may take a bit long time. note that rendering is based on
   [95]webkit2png.
python scripts/evaluation/render_html.py --result-path results/results.txt --out
put-dir data/html_64_64_100k/images_rendered --no-replace

   afterwards, based on the rendered images, we evaluate the exact match
   accuracy and the image id153.
python scripts/evaluation/evaluate_image.py --images-dir data/html_64_64_100k/im
ages_rendered/

acknowlegement

   our implementation is based on harvardnlp id4 implementation
   [96]id195-attn. and we would like to thank yoon kim and allen
   schmaltz for helpful discussions during this project.

     *    2019 github, inc.
     * [97]terms
     * [98]privacy
     * [99]security
     * [100]status
     * [101]help

     * [102]contact github
     * [103]pricing
     * [104]api
     * [105]training
     * [106]blog
     * [107]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [108]reload to refresh your
   session. you signed out in another tab or window. [109]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/harvardnlp/im2markup/commits/master.atom
   3. https://github.com/harvardnlp/im2markup#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/harvardnlp/im2markup
  32. https://github.com/join
  33. https://github.com/login?return_to=/harvardnlp/im2markup
  34. https://github.com/harvardnlp/im2markup/watchers
  35. https://github.com/login?return_to=/harvardnlp/im2markup
  36. https://github.com/harvardnlp/im2markup/stargazers
  37. https://github.com/login?return_to=/harvardnlp/im2markup
  38. https://github.com/harvardnlp/im2markup/network/members
  39. https://github.com/harvardnlp
  40. https://github.com/harvardnlp/im2markup
  41. https://github.com/harvardnlp/im2markup
  42. https://github.com/harvardnlp/im2markup/issues
  43. https://github.com/harvardnlp/im2markup/pulls
  44. https://github.com/harvardnlp/im2markup/projects
  45. https://github.com/harvardnlp/im2markup/pulse
  46. https://github.com/join?source=prompt-code
  47. http://lstm.seas.harvard.edu/latex
  48. https://github.com/harvardnlp/im2markup/commits/master
  49. https://github.com/harvardnlp/im2markup/branches
  50. https://github.com/harvardnlp/im2markup/releases
  51. https://github.com/harvardnlp/im2markup/graphs/contributors
  52. https://github.com/harvardnlp/im2markup/blob/master/license
  53. https://github.com/harvardnlp/im2markup/search?l=lua
  54. https://github.com/harvardnlp/im2markup/search?l=python
  55. https://github.com/harvardnlp/im2markup/search?l=javascript
  56. https://github.com/harvardnlp/im2markup/find/master
  57. https://github.com/harvardnlp/im2markup/archive/master.zip
  58. https://github.com/login?return_to=https://github.com/harvardnlp/im2markup
  59. https://github.com/join?return_to=/harvardnlp/im2markup
  60. https://desktop.github.com/
  61. https://desktop.github.com/
  62. https://developer.apple.com/xcode/
  63. https://visualstudio.github.com/
  64. https://github.com/da03
  65. https://github.com/harvardnlp/im2markup/commits?author=da03
  66. https://github.com/harvardnlp/im2markup/commit/2a5b7c11d4ab554fe0c340a85609ce56450afbc5
  67. https://github.com/harvardnlp/im2markup/commit/2a5b7c11d4ab554fe0c340a85609ce56450afbc5
  68. https://github.com/harvardnlp/im2markup/tree/2a5b7c11d4ab554fe0c340a85609ce56450afbc5
  69. https://github.com/harvardnlp/im2markup/tree/master/data/sample
  70. https://github.com/harvardnlp/im2markup/commit/f44f1f009766b0696605f6539cab5ce546b705ce
  71. https://github.com/harvardnlp/im2markup/tree/master/scripts
  72. https://github.com/harvardnlp/im2markup/commit/2a5b7c11d4ab554fe0c340a85609ce56450afbc5
  73. https://github.com/harvardnlp/im2markup/tree/master/src
  74. https://github.com/harvardnlp/im2markup/commit/d9b709bb65f230c8fb2ba0068e56ba198643f685
  75. https://github.com/harvardnlp/im2markup/tree/master/third_party
  76. https://github.com/harvardnlp/im2markup/commit/4e1b6c382267d955ef7087b6f1d84ec79e41086f
  77. https://github.com/harvardnlp/im2markup/blob/master/.gitignore
  78. https://github.com/harvardnlp/im2markup/blob/master/license
  79. https://github.com/harvardnlp/im2markup/blob/master/readme.md
  80. https://camo.githubusercontent.com/d5c6c528cdb25b504b1de298bc34d7109de06aea/687474703a2f2f6c73746d2e736561732e686172766172642e6564752f6c617465782f6e6574776f726b2e706e67
  81. https://camo.githubusercontent.com/8516e379e6ed493de3deba42d66c4a4d620336af/687474703a2f2f6c73746d2e736561732e686172766172642e6564752f6c617465782f726573756c74732f776562736974652f696d616765732f313139623933613434352d6f7269672e706e67
  82. https://camo.githubusercontent.com/5a6350632dd43b69fff7f41928f27b8c8eb9a7d9/687474703a2f2f6c73746d2e736561732e686172766172642e6564752f6c617465782f6d61746865782e706e67
  83. http://lstm.seas.harvard.edu/latex/
  84. http://arxiv.org/pdf/1609.04938v1.pdf
  85. http://torch.ch/
  86. https://nodejs.org/en/
  87. https://www.tug.org/texlive/
  88. http://www.imagemagick.org/script/index.php
  89. http://www.paulhammond.org/webkit2png/
  90. https://www.perl.org/
  91. https://zenodo.org/record/56198#.v2p0ktxt6ea
  92. https://zenodo.org/record/56198#.v2p0ktxt6ea
  93. https://www.tug.org/texlive/
  94. http://www.imagemagick.org/script/index.php
  95. http://www.paulhammond.org/webkit2png/
  96. https://github.com/harvardnlp/id195-attn
  97. https://github.com/site/terms
  98. https://github.com/site/privacy
  99. https://github.com/security
 100. https://githubstatus.com/
 101. https://help.github.com/
 102. https://github.com/contact
 103. https://github.com/pricing
 104. https://developer.github.com/
 105. https://training.github.com/
 106. https://github.blog/
 107. https://github.com/about
 108. https://github.com/harvardnlp/im2markup
 109. https://github.com/harvardnlp/im2markup

   hidden links:
 111. https://github.com/
 112. https://github.com/harvardnlp/im2markup
 113. https://github.com/harvardnlp/im2markup
 114. https://github.com/harvardnlp/im2markup
 115. https://help.github.com/articles/which-remote-url-should-i-use
 116. https://github.com/harvardnlp/im2markup#im2markup
 117. https://github.com/harvardnlp/im2markup#prerequsites
 118. https://github.com/harvardnlp/im2markup#torch
 119. https://github.com/harvardnlp/im2markup#model
 120. https://github.com/harvardnlp/im2markup#preprocess
 121. https://github.com/harvardnlp/im2markup#pdflatex-installaton
 122. https://github.com/harvardnlp/im2markup#imagemagick-convert-installation
 123. https://github.com/harvardnlp/im2markup#webkit2png-installation
 124. https://github.com/harvardnlp/im2markup#evaluate
 125. https://github.com/harvardnlp/im2markup#perl-installation
 126. https://github.com/harvardnlp/im2markup#usage
 127. https://github.com/harvardnlp/im2markup#quick-start-math-to-latex-toy-example
 128. https://github.com/harvardnlp/im2markup#preprocess-1
 129. https://github.com/harvardnlp/im2markup#train
 130. https://github.com/harvardnlp/im2markup#test
 131. https://github.com/harvardnlp/im2markup#evaluate-1
 132. https://github.com/harvardnlp/im2markup#text-metrics
 133. https://github.com/harvardnlp/im2markup#image-metrics
 134. https://github.com/harvardnlp/im2markup#web-page-to-html
 135. https://github.com/harvardnlp/im2markup#train-1
 136. https://github.com/harvardnlp/im2markup#test-1
 137. https://github.com/harvardnlp/im2markup#evaluate-2
 138. https://github.com/harvardnlp/im2markup#text-metrics-1
 139. https://github.com/harvardnlp/im2markup#image-metrics-1
 140. https://github.com/harvardnlp/im2markup#acknowlegement
 141. https://github.com/
