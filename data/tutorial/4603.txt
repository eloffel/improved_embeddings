   #[1]github [2]recent commits to sent-conv-torch:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]48
     * [35]star [36]337
     * [37]fork [38]110

[39]harvardnlp/[40]sent-conv-torch

   [41]code [42]issues 10 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   text classification using a convolutional neural network.
     * [47]112 commits
     * [48]2 branches
     * [49]0 releases
     * [50]fetching contributors
     * [51]mit

    1. [52]lua 56.1%
    2. [53]jupyter notebook 26.8%
    3. [54]python 15.6%
    4. [55]shell 1.5%

   (button) lua jupyter notebook python shell
   branch: master (button) new pull request
   [56]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/h
   [57]download zip

downloading...

   want to be notified of new releases in harvardnlp/sent-conv-torch?
   [58]sign in [59]sign up

launching github desktop...

   if nothing happens, [60]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [62]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [63]download the github extension for visual studio
   and try again.

   (button) go back
   [64]@jeffreyling
   [65]jeffreyling [66]merge branch 'master' of
   github.com:harvardnlp/sent-conv-torch
   latest commit [67]d61ded0 aug 15, 2017
   [68]permalink
   type         name                 latest commit message           commit time
        failed to load latest commit information.
        [69]data            [70]updates readme with cleanup changes feb 25, 2016
        [71]model           [72]requires                            aug 15, 2017
        [73].gitignore
        [74]license         [75]create license                      sep 8, 2015
        [76]readme.md
        [77]tempconv.ipynb
        [78]tempconv.pdf
        [79]get_id97.sh
        [80]main.lua
        [81]preprocess.py
        [82]trainer.lua
        [83]util.lua

readme.md

sentence convolution code in torch

   this code implements kim (2014) sentence convolution code in torch with
   gpus. it replicates the results on existing datasets, and allows
   training of models on arbitrary other text datasets.

quickstart

   to make data in hdf5 format, run the following (with id97 .bin path
   and choice of dataset):
python preprocess.py mr /path/to/id97.bin

   to run training with gpus:
th main.lua -data mr.hdf5 -cudnn 1 -gpuid 1

   results are timestamped and saved to the results/ directory.

dependencies

   the training pipeline requires python hdf5 (the h5py module) and the
   following lua packages:
     * hdf5
     * cudnn

   training on id97 architecture models requires downloading
   [84]id97 and unzipping. simply run the script
./get_id97.sh

creating datasets

   we provide the following datasets: mr, sst1, sst2, subj, trec, cr,
   mpqa. all raw training data is located in the data/ directory. the
   sst1, sst2 data have both test and dev sets, and trec has a test set.

   the data takes id97 embeddings, processes the vocabulary, and
   outputs a data matrix of vocabulary indices for each sentence.

   to create the hdf5 file, run the following with dataset as one of the
   described datasets:
python preprocess.py dataset /path/to/id97.bin

   the script outputs:
     * the dataset.hdf5 file with the data matrix and id97 embeddings
     * a dataset.txt file with a word-index dictionary for the word
       embeddings

training on custom datasets

   we allow training on arbitrary text datasets. they should be formatted
   in the same way as the sample data, with one sentence per line, and the
   first word the class label (0-indexed). our code handles most parsing
   of punctuation, possessives, capitalization, etc.

   example line:
1 no movement , no yuks , not much of anything .

   then run:
python preprocess.py custom /path/to/id97.bin --train /path/to/train/data --
test /path/to/test/data --dev /path/to/dev/data

   the output file's name can be set with the flag --custom_name (default
   is named custom).

running torch

   training is typically done with 10-fold cross-validation and 25 epochs.
   if the data set comes with a test set, we don't do cross validation
   (but split training data 90/10 for the dev set). if the data comes with
   the dev set, we don't do the split for train/dev.

   there are four main model architectures we implemented, as described in
   kim (2014): rand, static, nonstatic, multichannel.
     * rand initializes the id27s randomly and learns them.
     * static initializes the id27s to id97 and keeps the
       weight static.
     * nonstatic also initializes to id97, but allows them to be
       learned.
     * multichannel has two id97 embedding layers, one static and one
       nonstatic. the two layers outputs are summed.

   it is highly recommended that gpus are used during training if possible
   (see results section for timing benchmarks).

   separating out training and testing is easy; use the parameters
   -train_only and -test_only. also, pretrained models at any stage can be
   loaded from a .t7 file with -warm_start_model (see more parameters
   below).

output

   the code outputs a checkpoint .t7 file for every fold with name
   -savefile. the default name is timestamp_results.

   the following are saved as a table:
     * dev_scores with dev scores,
     * test scores with test scores,
     * opt with model parameters,
     * model with best model (as determined by dev score),
     * embeddings with the updated id27s

model augmentations

   a few modifications were made to the model architecture as experiments.
     * we include an option to include highway layers at the final mlp
       step (which increases depth of the model),
     * we also include highway layers at the convolutional step (which
       performs multiple convolutions on the resulting feature maps) as an
       option,
     * we experimented with skip kernels of size 5 (added in parallel with
       the other kernel sizes)

   results from these experiments are described below in the results
   section.

parameters

   the following is a list of complete parameters allowed by the torch
   code.
     * model_type: model architecture, as described above. options: rand,
       static, nonstatic, multichannel
     * data: training dataset to use, including id97 data. this should
       be a .hdf5 file made with preprocess.py.
     * cudnn: use gpus if set to 1, otherwise set to 0
     * seed: random seed, set to -1 for actual randomness
     * folds: number of folds for cross-validation.
     * debug: print debugging info including timing and confusions
     * savefile: name of output .t7 file, which will hold the trained
       model. default is timestamp_results
     * zero_indexing: set to 1 if data is zero indexed
     * warm_start_model: load a .t7 file with pretrained model. should
       contain a table with key 'model'
     * train_only: set to 1 to only train (no testing)
     * test_only: given a .t7 file with model, test on testing data
     * dump_feature_maps_file: filename for dumping feature maps of
       convolution at test time. this will be a .hdf5 file with fields
       feature_maps for the features at each time step and word_idxs for
       the word indexes (aligned with the last word of the filter). this
       currently only works for models with a single filter size. this is
       saved for the best model on fold 1.
     * preds_file: filename for writing predictions (with test_only set to
       1). output is zero indexed.

   training hyperparameters:
     * num_epochs: number of training epochs.
     * optim_method: id119 method. options: adadelta, adam
     * l2s: set l2 norm of final linear layer weights to this.
     * batch_size: batch size for training.

   model hyperparameters:
     * num_feat_maps: number of convolution feature maps.
     * kernels: kernel sizes of different convolutions.
     * dropout_p: dropout id203.
     * highway_mlp: number of highway mlp layers (0 for none)
     * highway_conv_layers: number of highway convolutional layers (0 for
       none)
     * skip_kernel: set 1 to use skip kernels

results

   the following results were collected with the same training setup as in
   kim (2014) (same parameters, 10-fold cross validation if data has no
   test set, 25 epochs).

scores

   dataset rand static nonstatic multichannel
   mr      75.9 80.5   81.3      80.8
   sst1    42.2 44.8   46.7      44.6
   sst2    83.5 85.6   87.0      87.1
   subj    89.2 93.0   93.4      93.2
   trec    88.2 91.8   92.8      91.8
   cr      78.3 83.3   84.4      83.7
   mpqa    84.6 89.6   89.7      89.6

   with 5 trials on sst1, we have a mean nonstatic score of 46.7 with
   standard deviation 1.69.

   with 1 highway layer, sst1 achieves a mean score of mean 47.8, stddev
   0.857, over 5 trials, and with 2 highway layers, mean 47.1, stddev
   1.47, over 10 trials.

timing

   we ran timing benchmarks on sst1, which has train/dev/test data sizes
   of 156817/1101/2210. we used a batch size of 50.

   | non-gpu | gpu --- | --- | --- per epoch | 2475 s | 54.0 s per batch |
   787 ms | 15.6 ms

   from these results, we see that using gpus achieves almost a 50x
   speedup on training. this allows much faster tuning of parameters and
   model experimentation.

relevant publications

   this code is based on kim (2014) and the corresponding theano [85]code.
kim, y. (2014). convolutional neural networks for sentence classification. in pr
oceedings of the 2014 conference on empirical methods in natural language proces
sing (emnlp), pp. 1746   1751, doha, qatar. association for computational linguist
ics.

srivastava, r. k., greff, k., & schmidhuber, j. (2015). training very deep netwo
rks. in advances in neural information processing systems (pp. 2368-2376).

     *    2019 github, inc.
     * [86]terms
     * [87]privacy
     * [88]security
     * [89]status
     * [90]help

     * [91]contact github
     * [92]pricing
     * [93]api
     * [94]training
     * [95]blog
     * [96]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [97]reload to refresh your
   session. you signed out in another tab or window. [98]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/harvardnlp/sent-conv-torch/commits/master.atom
   3. https://github.com/harvardnlp/sent-conv-torch#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/harvardnlp/sent-conv-torch
  32. https://github.com/join
  33. https://github.com/login?return_to=/harvardnlp/sent-conv-torch
  34. https://github.com/harvardnlp/sent-conv-torch/watchers
  35. https://github.com/login?return_to=/harvardnlp/sent-conv-torch
  36. https://github.com/harvardnlp/sent-conv-torch/stargazers
  37. https://github.com/login?return_to=/harvardnlp/sent-conv-torch
  38. https://github.com/harvardnlp/sent-conv-torch/network/members
  39. https://github.com/harvardnlp
  40. https://github.com/harvardnlp/sent-conv-torch
  41. https://github.com/harvardnlp/sent-conv-torch
  42. https://github.com/harvardnlp/sent-conv-torch/issues
  43. https://github.com/harvardnlp/sent-conv-torch/pulls
  44. https://github.com/harvardnlp/sent-conv-torch/projects
  45. https://github.com/harvardnlp/sent-conv-torch/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/harvardnlp/sent-conv-torch/commits/master
  48. https://github.com/harvardnlp/sent-conv-torch/branches
  49. https://github.com/harvardnlp/sent-conv-torch/releases
  50. https://github.com/harvardnlp/sent-conv-torch/graphs/contributors
  51. https://github.com/harvardnlp/sent-conv-torch/blob/master/license
  52. https://github.com/harvardnlp/sent-conv-torch/search?l=lua
  53. https://github.com/harvardnlp/sent-conv-torch/search?l=jupyter-notebook
  54. https://github.com/harvardnlp/sent-conv-torch/search?l=python
  55. https://github.com/harvardnlp/sent-conv-torch/search?l=shell
  56. https://github.com/harvardnlp/sent-conv-torch/find/master
  57. https://github.com/harvardnlp/sent-conv-torch/archive/master.zip
  58. https://github.com/login?return_to=https://github.com/harvardnlp/sent-conv-torch
  59. https://github.com/join?return_to=/harvardnlp/sent-conv-torch
  60. https://desktop.github.com/
  61. https://desktop.github.com/
  62. https://developer.apple.com/xcode/
  63. https://visualstudio.github.com/
  64. https://github.com/jeffreyling
  65. https://github.com/harvardnlp/sent-conv-torch/commits?author=jeffreyling
  66. https://github.com/harvardnlp/sent-conv-torch/commit/d61ded09c99c0199d8c5bca946cd65864de2b296
  67. https://github.com/harvardnlp/sent-conv-torch/commit/d61ded09c99c0199d8c5bca946cd65864de2b296
  68. https://github.com/harvardnlp/sent-conv-torch/tree/d61ded09c99c0199d8c5bca946cd65864de2b296
  69. https://github.com/harvardnlp/sent-conv-torch/tree/master/data
  70. https://github.com/harvardnlp/sent-conv-torch/commit/0ddcecbd4da3b88aa96cc713576e94768fda96d2
  71. https://github.com/harvardnlp/sent-conv-torch/tree/master/model
  72. https://github.com/harvardnlp/sent-conv-torch/commit/6e581882300507485f4c2656a8b3d22075e7a9ae
  73. https://github.com/harvardnlp/sent-conv-torch/blob/master/.gitignore
  74. https://github.com/harvardnlp/sent-conv-torch/blob/master/license
  75. https://github.com/harvardnlp/sent-conv-torch/commit/0b1191f09ffdac803980bf0fdc896eacaec0751f
  76. https://github.com/harvardnlp/sent-conv-torch/blob/master/readme.md
  77. https://github.com/harvardnlp/sent-conv-torch/blob/master/tempconv.ipynb
  78. https://github.com/harvardnlp/sent-conv-torch/blob/master/tempconv.pdf
  79. https://github.com/harvardnlp/sent-conv-torch/blob/master/get_id97.sh
  80. https://github.com/harvardnlp/sent-conv-torch/blob/master/main.lua
  81. https://github.com/harvardnlp/sent-conv-torch/blob/master/preprocess.py
  82. https://github.com/harvardnlp/sent-conv-torch/blob/master/trainer.lua
  83. https://github.com/harvardnlp/sent-conv-torch/blob/master/util.lua
  84. https://code.google.com/p/id97/
  85. https://github.com/yoonkim/id98_sentence/
  86. https://github.com/site/terms
  87. https://github.com/site/privacy
  88. https://github.com/security
  89. https://githubstatus.com/
  90. https://help.github.com/
  91. https://github.com/contact
  92. https://github.com/pricing
  93. https://developer.github.com/
  94. https://training.github.com/
  95. https://github.blog/
  96. https://github.com/about
  97. https://github.com/harvardnlp/sent-conv-torch
  98. https://github.com/harvardnlp/sent-conv-torch

   hidden links:
 100. https://github.com/
 101. https://github.com/harvardnlp/sent-conv-torch
 102. https://github.com/harvardnlp/sent-conv-torch
 103. https://github.com/harvardnlp/sent-conv-torch
 104. https://help.github.com/articles/which-remote-url-should-i-use
 105. https://github.com/harvardnlp/sent-conv-torch#sentence-convolution-code-in-torch
 106. https://github.com/harvardnlp/sent-conv-torch#quickstart
 107. https://github.com/harvardnlp/sent-conv-torch#dependencies
 108. https://github.com/harvardnlp/sent-conv-torch#creating-datasets
 109. https://github.com/harvardnlp/sent-conv-torch#training-on-custom-datasets
 110. https://github.com/harvardnlp/sent-conv-torch#running-torch
 111. https://github.com/harvardnlp/sent-conv-torch#output
 112. https://github.com/harvardnlp/sent-conv-torch#model-augmentations
 113. https://github.com/harvardnlp/sent-conv-torch#parameters
 114. https://github.com/harvardnlp/sent-conv-torch#results
 115. https://github.com/harvardnlp/sent-conv-torch#scores
 116. https://github.com/harvardnlp/sent-conv-torch#timing
 117. https://github.com/harvardnlp/sent-conv-torch#relevant-publications
 118. https://github.com/
