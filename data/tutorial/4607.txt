   #[1]silicon valley data science    feed [2]silicon valley data science   
   comments feed [3]silicon valley data science    ical feed [4]silicon
   valley data science    getting started with deep learning comments feed
   [5]alternate [6]alternate

   [7]silicon valley data science silicon valley data science

     * [8]search

   search ____________________ search
   (button) toggle navigation

   [neurons_header.jpg]

getting started with deep learning

a review of available tools  |  february 15th, 2017

   at svds, our r&d team has been investigating different deep learning
   technologies, from [9]recognizing images of trains to speech
   recognition. we needed to build a pipeline for ingesting data, creating
   a model, and evaluating the model performance. however, when we
   researched what technologies were available, we could not find a
   concise summary document to reference for starting a new deep learning
   project.

   one way to give back to the open source community that provides us with
   tools is to help others evaluate and choose those tools in a way that
   takes advantage of our experience. we offer the chart below, along with
   explanations of the various criteria upon which we based our decisions.

   [deep_learning_ratings_final-1024x563.png]

   these rankings are a combination of our subjective experiences with
   image and id103 applications for these technologies, as
   well as publicly available benchmarking studies. please note that this
   is not an exhaustive list of available deep learning toolkits, more of
   which can be found [10]here. in the coming months, our team is excited
   to checkout [11]deeplearning4j, [12]paddle, [13]chainer, [14]apache
   signa, and [15]dynet. we explain our scoring of the reviewed tools
   below:

   languages: when getting started with deep learning, it is best to use a
   framework that supports a language you are familiar with. for instance,
   caffe (c++) and torch (lua) have python bindings for its codebase, but
   we would recommend that you are proficient with c++ or lua respectively
   if you would like to use those technologies. in comparison, tensorflow
   and mxnet have great multi language support that make it possible to
   utilize the technology even if you are not proficient with c++.

   note: we have not had an opportunity to test out the new python wrapper
   for torch, [16]pytorch, released by facebook ai research (fair) in
   january 2017. this framework was built for python programmers to
   leverage torch   s dynamic construction of neural networks.

   tutorials and training materials: deep learning technologies vary
   dramatically in the quality and quantity of tutorials and getting
   started materials. theano, tensorflow, torch, and mxnet have well
   documented tutorials that are easy to understand and implement. while
   microsoft   s cntk and intel   s nervana neon are powerful tools, we
   struggled to find beginner-level materials. additionally, we   ve found
   that the engagement of the github community is a strong indicator of
   not only a tool   s future development, but also a measure of how
   likely/fast an issue or bug can be solved through searching
   stackoverflow or the repo   s git issues. it is important to note that
   tensorflow is the 800-pound gorilla in the room in regards to quantity
   of tutorials, training materials, and community of developers and
   users.

   [githubcommits_deeplearning.png]

   id98 modeling capability: convolutional neural networks (id98s) are used
   for image recognition, recommendation engines, and natural language
   processing. a id98 is composed of a set of distinct layers that
   transform the initial data volume into output scores of predefined
   class scores (for more information, check out eugenio culurciello   s
   [17]overview of neural network architectures). id98   s can also be used
   for regression analysis, such as models that output of steering angles
   in autonomous vehicles. we consider a technology   s id98 modeling
   capability to include several features. these features include the
   opportunity space to define models, the availability of prebuilt
   layers, and the tools and functions available to connect these layers.
   we   ve seen that theano, caffe, and mxnet all have great id98 modeling
   capabilities. that said, tensorflow   s easy ability to build upon it   s
   [18]inceptionv3 model and torch   s great id98 resources including
   easy-to-use temporal convolution set these two technologies apart for
   id98 modeling capability.

   id56 modeling capability: recurrent neural networks (id56s) are used for
   id103, time series prediction, image captioning, and other
   tasks that require processing sequential information. as prebuilt id56
   models are not as numerous as id98s, it is therefore important if you
   have a id56 deep learning project that you consider what id56 models have
   been previously implemented and open sourced for a specific technology.
   for instance, caffe has minimal id56 resources, while [19]microsoft   s
   cntk and torch have ample id56 tutorials and prebuilt models. while
   vanilla tensorflow has some id56 materials, [20]tflearn and [21]keras
   include many more id56 examples that utilize tensorflow.

   architecture: in order to create and train new models in a particular
   framework, it is critical to have an easy to use and modular front end.
   tensorflow, torch, and mxnet have a straightforward, modular
   architecture that makes development straightforward. in comparison,
   frameworks such as caffe require significant amount of work to create a
   new layer. we   ve found that tensorflow in particular is easy to debug
   and monitor during and after training, as the [22]tensorboard web gui
   application is included.

   speed: torch and nervana have the best documented performance for open
   source convolutional neural network [23]benchmarking tests.
   [24]tensorflow performance was comparable for most tests, while caffe
   and theano lagged behind. microsoft   s cntk claims to have some of the
   fastest id56 training time. [25]another study comparing theano, torch,
   and tensorflow directly for id56 showed that theano performs the best of
   the three.

   multiple gpu support: most deep learning applications require an
   outstanding number of floating point operations (flops). for example,
   baidu   s deepid103 models take [26]10s of exaflops to
   train. that is >10e18 calculations! as leading graphics processing
   units (gpus) such as nvidia   s pascal titanx can execute [27]11e9 flops
   a second, it would take over a week to train a new model on a
   sufficiently large dataset. in order to decrease the time it takes to
   build a model, multiple gpus over multiple machines are needed.
   luckily, most of the technologies outlined above offer this support. in
   particular, [28]mxnet is reported to have one the most optimized
   [29]multi-gpu engine.

   keras compatible: [30]keras is a high level library for doing fast deep
   learning prototyping. we   ve found that it is a great tool for getting
   data scientists comfortable with deep learning. keras currently
   supports two back ends, tensorflow and theano, and will be gaining
   [31]official support in tensorflow in the future. keras is also a good
   choice for a high-level library when considering that its author
   [32]recently expressed that keras will continue to exist as a front end
   that can be used with multiple back ends.

   if you are interested in getting started with deep learning, i would
   recommend evaluating your own team   s skills and your project needs
   first. for instance, for an image recognition application with a
   python-centric team we would recommend tensorflow given its ample
   documentation, decent performance, and great prototyping tools. for
   scaling up an id56 to production with a lua competent client team, we
   would recommend torch for its superior speed and id56 modeling
   capabilities.

   in the future we will discuss some of our challenges in scaling up our
   models. these challenges include optimizing gpu usage over multiple
   machines and adapting open source libraries like [33]cmu sphinx and
   kaldi for our deep learning pipeline.

   [34]sign up for our newsletter to stay in touch

     * matthew rubashkin
    [35]matthew rubashkin
       with a background in optical physics and biomedical research, matt
       has a broad range of experiences in software development, database
       engineering, and data analytics.

share

related blog posts

     * [36]deepdream: accelerating deep learning with hardware
       the deepgramai hackathon has concluded, check out the    
     * [37]understanding ai toolkits
       as well as developing familiarity with ai techniques,    
     * [38]getting started with predictive maintenance models
       in this post, we   ll start to develop an    

   [39]see all blogs

related events

     * mar 13

[40]strata + hadoop world ca 2017
       san jose, ca
     * apr 3

[41]tdwi accelerate boston 2017
       boston, ma
     * apr 2

[42]enterprise data world 2017
       atlanta, ga

   [43]all svds events
   [money_feature-145x118.jpeg]

   previous article

the roi of a modern data strategy

   thank you

   previous article

thank you

   next article

analyzing caltrain delays

   [floppy-disk-2-feature-1-145x118.jpeg]

   next article

welcome to silicon valley data science

      2017 silicon valley data science llc
     * [44]blog
     * [45]projects
     * [46]resources

     * [47]sitemap
     * [48]privacy policy
     * [49]terms of use

sign in

   ____________________ ____________________ log in
     * [50]forgot password?

   [tr?id=1791588387795047&ev=pageview&noscript=1]

references

   visible links
   1. https://www.svds.com/feed/
   2. https://www.svds.com/comments/feed/
   3. https://www.svds.com/events/?ical=1
   4. https://www.svds.com/getting-started-deep-learning/feed/
   5. https://www.svds.com/wp-json/oembed/1.0/embed?url=https://www.svds.com/getting-started-deep-learning/
   6. https://www.svds.com/wp-json/oembed/1.0/embed?url=https://www.svds.com/getting-started-deep-learning/&format=xml
   7. https://www.svds.com/
   8. https://www.svds.com/?s=
   9. http://svds.com/tensorflow-image-recognition-raspberry-pi/
  10. https://en.wikipedia.org/wiki/comparison_of_deep_learning_software
  11. https://deeplearning4j.org/
  12. http://www.paddlepaddle.org/
  13. http://chainer.org/
  14. https://singa.incubator.apache.org/en/index.html
  15. http://dynet.readthedocs.io/en/latest/index.html
  16. http://pytorch.org/
  17. https://culurciello.github.io/tech/2016/06/04/nets.html
  18. https://www.tensorflow.org/tutorials/image_recognition/
  19. https://github.com/microsoft/cntk/wiki/recurrent-neural-networks-with-cntk-and-applications-to-the-world-of-ranking
  20. http://tflearn.org/
  21. https://keras.io/
  22. https://www.tensorflow.org/how_tos/summaries_and_tensorboard/
  23. https://github.com/soumith/convnet-benchmarks/blob/master/readme.md
  24. https://github.com/tobigithub/tensorflow-deep-learning/wiki/tf-benchmarks
  25. https://arxiv.org/abs/1511.06435
  26. https://arxiv.org/abs/1512.02595
  27. https://www.nvidia.com/en-us/geforce/products/10series/titan-x-pascal/
  28. http://mxnet.io/how_to/perf.html
  29. http://www.allthingsdistributed.com/2016/11/mxnet-default-framework-deep-learning-aws.html
  30. https://github.com/fchollet/keras
  31. http://www.fast.ai/2017/01/03/keras/
  32. https://github.com/fchollet/keras/issues/5050#issuecomment-272945570
  33. https://svds.com/open-source-toolkits-speech-recognition/
  34. http://svds.com/newsletter/
  35. https://www.svds.com/team/matthew-rubashkin/
  36. https://www.svds.com/deepdream-accelerating-deep-learning-hardware/
  37. https://www.svds.com/understanding-ai-toolkits/
  38. https://www.svds.com/getting-started-predictive-maintenance-models/
  39. https://www.svds.com/blog/
  40. https://www.svds.com/event/strata-hadoop-world-ca-2017/
  41. https://www.svds.com/event/tdwi-accelerate-boston-2017/
  42. https://www.svds.com/event/enterprise-data-world-2017/
  43. https://www.svds.com/events/
  44. https://www.svds.com/blog/
  45. https://www.svds.com/projects/
  46. https://www.svds.com/resources/
  47. https://www.svds.com/sitemap/
  48. https://www.svds.com/privacy-policy/
  49. https://www.svds.com/terms-of-use/
  50. https://www.svds.com/wp-login.php?action=lostpassword

   hidden links:
  52. https://www.svds.com/roi-modern-data-strategy/
  53. https://www.svds.com/thankyou/
  54. https://www.svds.com/tbt-analyzing-caltrain-delays/
  55. https://www.svds.com/welcome-to-silicon-valley-data-science/
