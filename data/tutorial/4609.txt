   [1]

   smerity.com

id53 on the facebook babi dataset using recurrent neural
networks and 175 lines of python + keras

august 5, 2015

   one of the holy grails of natural language processing is a generic
   system for id53. the [2]facebook babi tasks are a
   synthetic dataset of 20 tasks released by the facebook ai research team
   that help evaluate systems hoping to do just that.

   an example from the second task, two supporting facts (qa2), is below:
   1 john moved to the bedroom.
   2 mary grabbed the football there.
   3 sandra journeyed to the bedroom.
   4 sandra went back to the hallway.
   5 mary moved to the garden.
   6 mary journeyed to the office.
   7 where is the football? office 2 6

   whilst this may seem trivial to you, it can represent quite a challenge
   even for advanced machine learning models. the babi tasks cover far
   more than trivial comprehension however - they're supposed to represent
   a prerequisite towards an ai-complete id53 solution. each
   task aims to require a unique aspect of text and reasoning, testing the
   different capabilities of the learning models. to answer the questions
   correctly, the models must be able to perform induction, deduction,
   fact chaining, and more.

   whilst doing well on this task requires advanced tools, we can
   implement a baseline solution in only a few lines using the keras
   machine learning library. the results are comparable (and occasionally
   superior) to those for the lstm baseline provided in weston et al.'s
   [3]towards ai-complete id53: a set of prerequisite toy
   tasks given only 1000 samples and without any hyperparamater tuning.

try it yourself!

   the id53 code this article refers to is now part of the
   keras distribution: [4]babi_id56.py in the examples directory. when you
   run the example, keras will automatically download the dataset and
   start training.

   best of all, as opposed to most deep learning tasks, training these
   models will only take a few minutes each!

why a synthetic dataset?

   as babi is a synthetic dataset, you may ask why we're interested in
   doing well on it, or why we even created it at all.

   real world data is noisy. rarely does it provide a clear and simple
   answer for you to train on. additionally, even a well curated dataset
   from the real world is littered with nuance, complexities, and errors.

   instead of relying on real world data, we can instead challenge the
   machine learning models using simulations reminiscent of classic text
   adventure games. the tasks are generated using a simulation reminiscent
   of a classic text adventure game. by using an artificial world we know
   the exact state the world is in and the exact set of rules by which it
   runs. thanks to this, generating training and testing data is trivial.

   as opposed to real world material, the data is also well curated. the
   vocabulary (set of words) is constrained, the sentences are always well
   structured (the only noise is the noise we want to challenge the model
   with), and the performance on specific tasks can be tested without
   other tasks interferring. as we know the exact state of the world and
   how it got to that point, we can also provide additional helpful
   information, such as pointing out precisely how the answer can be
   reached (the supporting facts in bold above).

   with the synethetic dataset, all the commonsense knowledge and
   reasoning required for the test set should be contained in the training
   set. that way, if a machine learning model then fails to solve the
   task, we know that the challenge is in the model itself, and not the
   data (or lack of data) it was exposed to.

how do we approach the problem?

   one of the easiest ways to approach a task is to code a basline
   solution. baseline solutions are meant to provide the best "bang for
   the buck" - the minimal amount of work for the best result possible. in
   this situation, a recurrent neural network (id56) is the baseline we can
   turn to.

   recurrent neural networks such as long short term memory (lstm) and the
   gated recurrent unit (gru) are neural networks that can process a
   sequence of inputs, updating the network's internal state as it reads
   more data. this enables it to learn long term dependencies such as
   bracket matching. as we encode words into a vector representation, we
   can consider a sentence as a sequence of words, feeding them one at a
   time into our id56.

   instead of implementing these models ourselves, we can instead use an
   existing implementation.

keras - a deep learning library for python

   [5]keras is a minimalist, highly modular neural network library in the
   spirit of torch, written in python, that uses theano under the hood for
   optimized tensor manipulation on gpu and cpu.

   as an open source project, it has strong documentation, an active
   community, and a good leader. only [6]three hours after submitting my
   pull request for this example code, fran  ois chollet (fchollet) merged
   in the code. the rapid turn around of the project and strong examples
   make it a good library to get going with deep learning. keras also
   leverages the theano library, a python library defining, optimizing,
   and evaluating mathematical expressions involving multi-dimensional
   arrays efficiently.

the challenge at hand

   our idea is as follows: each task has a story component and a query
   component. we will run an id56 over both of these components, converting
   the long sequence of words into a fixed vector representation. this
   fixed vector representation should hopefully encapsulate all of the
   relevant input. finally, we feed these two fixed vector representations
   into a traditional dense neural network, where it can look at the
   encoded query, then at the encoded story, and hopefully answer the
   question correctly.

   [keras_qa_model.svg]

word vectors

   one additional component is the word vector representation. this is
   where hope to convert a word into a fixed vector representation
   encapsulating extra knowledge about it. word vectors hope to capture
   the meaning behind the word, enabling related words to be considered
   similar and thus act in similar fashions.

   this might be important if, for example, we had the two sentences:
   john put down the apple.
   john dropped the apple.
   but are only interested in answering the question "does john have the
   apple?", where the nuance between putting something down and dropping
   it is unimportant.

   whilst we can learn good word vector representations for the small set
   of words in these task, it wouldn't have broader knowledge. for a real
   task, where knowing extra information might be useful (such as frog ~=
   toad), we could use existing word vectors trained on billions of words,
   such as the stanford's [7]glove.

the code

   luckily, the code for this is stunningly simple thanks to keras. you
   can see the full code at [8]babi_id56.py but the relevant recurrent
   network code is quickly and minimally contained below.
sentid56 = sequential()
sentid56.add(embedding(vocab_size, embed_hidden_size, mask_zero=true))
sentid56.add(id56(embed_hidden_size, sent_hidden_size, return_sequences=false))

qid56 = sequential()
qid56.add(embedding(vocab_size, embed_hidden_size))
qid56.add(id56(embed_hidden_size, query_hidden_size, return_sequences=false))

model = sequential()
model.add(merge([sentid56, qid56], mode='concat'))
model.add(dense(sent_hidden_size + query_hidden_size, vocab_size, activation='so
ftmax'))

final results

   in this section, i compare the final results for the keras based
   id53 system with the lstm baseline provided by the
   facebook paper.

   the results are comparable (and occasionally superior) to those for the
   lstm baseline provided in weston et al.'s [9]towards ai-complete
   id53: a set of prerequisite toy tasks given only 1000
   samples and without any hyperparamater tuning. the same model is also
   used across all tasks.

   unfortunately, the baseline is just that. using traditional recurrent
   neural networks, such as the lstm or gru, won't give you substantially
   better performance even if you scale up the network tremendously. for
   better results, new neural network configurations have been suggested
   and used, such as facebook's [10]memory network (further improved in
   the paper presenting the babi dataset), google's [11]neural turing
   machine, and metamind's [12]dynamic memory networks.

   all of these models can take advantage of knowing where the supporting
   facts are, learning where to focus attention in the input, and
   performing multiple "lookups" to track down relevant information. i'm
   hoping to implement a simple version of one of these models in the near
   future.

   for now, however, i'm content with my simple baseline.
           task number          fb lstm baseline keras qa
   qa1 - single supporting fact 50               52.1
   qa2 - two supporting facts   20               37.0
   qa3 - three supporting facts 20               20.5
   qa4 - two arg. relations     61               62.9
   qa5 - three arg. relations   70               61.9
   qa6 - yes/no questions       48               50.7
   qa7 - counting               49               78.9
   qa8 - lists/sets             45               77.2
   qa9 - simple negation        64               64.0
   qa10 - indefinite knowledge  44               47.7
   qa11 - basic coreference     72               74.9
   qa12 - conjunction           74               76.4
   qa13 - compound coreference  94               94.4
   qa14 - time reasoning        27               34.8
   qa15 - basic deduction       21               32.4
   qa16 - basic induction       23               50.6
   qa17 - positional reasoning  51               49.1
   qa18 - size reasoning        52               90.8
   qa19 - path finding          8                9.0
   qa20 - agent's motivations   91               90.7

note: dataset issues - duplication in positional reasoning (qa17) and size
reasoning (qa18)

   the results above show a large performance difference between the
   facebook lstm baseline and the keras qa system on qa18 - jumping from
   52 to 91. whilst investigating i found that there were numerous
   duplicated statements and questions in the qa18 training and testing
   datasets. this is also an issue in qa17 and possibly others. given that
   there are only 1000 train and test data points (which you can confirm
   by running grep "?" tasks_1-20_v1-2/en/qa18_size-reasoning_train.txt |
   wc -l), repetitions could cause serious issues.

   i'll be emailing the maintainers of the dataset once i perform a full
   analysis in the hopes this will be fixed for version 1.3 of the data.
   # qa18 - top of training data
   1 the container fits inside the suitcase.
   2 the container fits inside the suitcase.
   3 the chocolate fits inside the chest.
   4 the box of chocolates fits inside the suitcase.
   5 the chocolate fits inside the box.
   6 the chocolate fits inside the container.
   7 the box of chocolates fits inside the suitcase.
   8 the container fits inside the suitcase.
   9 the chocolate fits inside the box.
   10 the suitcase is bigger than the chocolate.
   11 is the chocolate bigger than the suitcase? no 6 1
   12 does the suitcase fit in the chocolate? no 6 1
   13 does the suitcase fit in the chocolate? no 6 1
   14 does the suitcase fit in the chocolate? no 6 1
   # qa17 - top of training data
   1 the triangle is above the pink rectangle.
   2 the blue square is to the left of the triangle.
   3 is the pink rectangle to the right of the blue square? yes 1 2
   4 is the blue square below the pink rectangle? no 2 1
   5 is the blue square to the right of the pink rectangle? no 2 1
   6 is the blue square below the pink rectangle? no 2 1
   7 is the blue square below the pink rectangle? no 2 1

update - full duplicate analysis

   having finished the duplicate analysis, there are issues in the dataset
   that need to be fixed. luckily the dataset has already been released in
   a versioned state, though it is unfortunate that the papers published
   using the dataset do not report which versions they used, and
   historical versions are not available.

   the duplicate analysis was performed by finding only unique (story,
   query, answer) tuples within the training set and the test set, then
   finding if there were any intersections between those unique tuples.

   the most extreme issue is that one of the tasks, qa4, has about 13% of
   the unique samples present in both training and testing.
   unique samples in qa4 - two arg relations
   train length: 919
   test length: 933
   intersection: 124

   another issue was duplicates within the training and testing sets,
   especially problematic in qa15, qa17, and qa18. this is especially
   important given that the algorithms are trained on only 1000 samples.

   qa15, qa17, and qa18 has numerous duplicates in training and testing
   unique samples in qa15 - basic deduction
   train length: 695
   test length: 678
   intersection: 0
   unique samples in qa17 - positional reasoning
   train length: 627
   test length: 632
   intersection: 11
   unique samples in qa18 - size reasoning
   train length: 654
   test length: 602
   intersection: 0

   these issues become even more extreme when the babi tasks contain
   10,000 samples are used.

   for details, refer to the [13]full results.

popular articles

     * [14]it's ml, not magic: machine learning can be prejudiced
     * [15]it's ml, not magic: the rise of ai-prefix investing
     * [16]in deep learning, architecture engineering is the new feature
       engineering
     * [17]how google sparsehash achieves two bits of overhead per entry
       using sparsetable
     * [18]where did all the http referrers go?

   interested in saying hi? ^_^
   [19]follow @smerity

   i'm stephen merity, better known in most places as smerity.

   salesforce research:
   senior research scientist
   (deep learning)

   part of metamind:
   acquired by salesforce

   harvard university:
   [20]ms in cse

   sydney university:
   [21]bit (university medal + first class honours)

   [22]full about me
   you reached the bottom! reading can be challenging, so i think you
   deserve a reward. i'd offer you [23]cake, but after [24]a certain
   robot-human conflict started due to cake it may not be a wise choice.
   so here:

                               [mini_coin.png]

   take a coin. no, not a bitcoin. do you know how expensive those are?
      _   

references

   visible links
   1. http://smerity.com/
   2. https://research.facebook.com/researchers/1543934539189348
   3. http://arxiv.org/abs/1502.05698
   4. https://github.com/fchollet/keras/blob/master/examples/babi_id56.py
   5. http://keras.io/
   6. https://github.com/fchollet/keras/pull/477
   7. http://nlp.stanford.edu/projects/glove/
   8. https://github.com/fchollet/keras/blob/master/examples/babi_id56.py
   9. http://arxiv.org/abs/1502.05698
  10. http://arxiv.org/pdf/1410.3916.pdf
  11. http://arxiv.org/pdf/1410.5401.pdf
  12. http://arxiv.org/pdf/1506.07285.pdf
  13. https://gist.github.com/smerity/8ceb539c125cbe648bfe
  14. http://smerity.com/articles/2016/algorithms_can_be_prejudiced.html
  15. http://smerity.com/articles/2016/ml_not_magic.html
  16. http://smerity.com/articles/2016/architectures_are_the_new_feature_engineering.html
  17. http://smerity.com/articles/2015/google_sparsehash.html
  18. http://smerity.com/articles/2013/where_did_all_the_http_referrers_go.html
  19. https://twitter.com/smerity
  20. http://www.seas.harvard.edu/programs/graduate/computational-science-and-engineering/master-of-science-in-cse
  21. http://sydney.edu.au/engineering/it/current_students/undergrad/bit.shtml
  22. http://smerity.com/abme.html
  23. http://www.minecraftwiki.net/wiki/cake
  24. http://en.wikipedia.org/wiki/portal_(video_game)

   hidden links:
  26. https://www.twitter.com/smerity/
  27. https://facebook.com/smerity
  28. https://github.com/smerity/
  29. http://au.linkedin.com/in/smerity
  30. mailto:smerity@smerity.com
  31. https://www.twitter.com/smerity/
  32. https://facebook.com/smerity
  33. https://github.com/smerity/
  34. http://au.linkedin.com/in/smerity
  35. mailto:smerity@smerity.com
