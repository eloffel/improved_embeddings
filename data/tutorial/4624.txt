   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

build neural network from scratch         part 2

   [16]go to the profile of david fumo
   [17]david fumo (button) blockedunblock (button) followfollowing
   aug 24, 2017

   a gentle introduction to neural networks series (ginns)         part 2
   [1*hkmjxhe1vv9ldn9l2uadhg.jpeg]

introduction

   in this post, we are going to build a id88 for and logic gate,
   this model we are going to build from scratch using python and numpy.
   here i   m assuming that you read a gentle introduction to neural
   networks series         part1 and that you are already familiar with basic
   concepts of neural networks. if you are new to this topic i strongly
   advise you to start from part 1. are you still here? great, this post
   is going to be one of my shortest post i have written so far, i want to
   keep things simple while you build a good intuition and understanding
   of neural networks, so let   s do this, but first, let me explain why
   build a super dummy neural network from scratchwhy implement a neural
   network from scratch?

   but why implement a neural network from scratch at all? i believe what
   you really want is to create powerful neural networks that can predict
   if the person in your picture is george lucas or luke skywalker? no!,
   oh ok, i don   t blame you.

   now, jokes aside. even if you plan on using neural network libraries
   like tensorflow, keras or another one in the future, implementing a
   network from scratch at least once is an extremely valuable exercise.
   it helps you gain an understanding of how neural networks work, and
   that is essential for designing effective models. so, i hope i
   convinced you to stay here.

single-layer neural network (id88)

   the id88 algorithm is the simplest type of artificial neural
   network, it is inspired by the information processing of a single
   neural cell called a neuron.
   [1*tf1wmeglfw6xzdua9ptc9g.png]
   id88

   in a id88, to compute the output, we will multiply input with
   respective weights and compare with a threshold value, a generalization
   to this is to say that, the weighted sum of the inputs is passed
   through a step/activation function, as shown below:
   [1*63jl4kkh-ltt2rs3peg_5w.png]
   weighted sum passed to an activation function.

activation function

   the activation function of a node defines the output of that node given
   an input or set of inputs. a standard computer chip circuit can be seen
   as a digital network of id180 that can be    on    (1) or
      off    (0), depending on input. for our example, we are going to use the
   binary step function as shown below:
   [1*214_rt524eehevtqqaqhkg.png]
   [1*gnortjgk1jrxzpxem5b2gg.png]
   step function

here is the code!

   iframe: [18]/media/5ddcabfc180e2664c7a8fb0e88a282cc?postid=673ec7cdd89f

   id88
    1. load input (x) and output (y)
    2. initialize weights and other parameters: learning rate is a
       configuration parameter that controls the amount that weights are
       updated, training step (also called epoch) is the single step that
       takes to do forward and backward propagation.
    3. compute dot product between input (x) and weights matrix (w), here
       we use np.dot() to do id127, the dot product is
       also called the inner product, for example, the inner product
       between x and w is x1*w1 + x2*w2 + x3*w3 +     + xn*wn.
    4. apply the activation function on the l1. l1 is actually the output
       of our network.
    5. compute the loss or the errors model made, when we start training
       the network guesses the predictions, and then we compare the
       predicted class and the true one. ideally, what we want when we
       train a neural net, is to find the optimal weights (w) that best
       minimize the error of our model.
    6. compute the change factor, what we called update, in case our model
       predicts the class correctly the error is equal to zero and no
       changes are done on the weights, but if it makes a mistake, then we
       can have a negative (decrease) or positive (increase) update on the
       weights.
    7. we train our model in 100 training steps and using online or
       stochastic learning, which means that we use a single training
       example to make parameter update. here the single training example
       is picked randomly.

   output after training :
   [1*kb_qocmk0uoi7haa8watuq.png]
   y_pred after training

use cases and limitations

   while the id88 classified the instances in our example well, the
   model has limitations, actually, id88 is a linear model and in
   most cases linear models are not enough to learn useful patterns.
   [1*qldlfacxucfknmo4nuel5g.jpeg]
   [1*wnbuyfyzgnmf6io_tvdgdw.jpeg]
   left: and gate, right: not and or logic gates

   linear models like the id88 are not universal function
   approximators, if you were to try to change the problem for xor logic
   gate, the id88 would fail because xor doesn   t have linear
   separability property as shown in the image below:
   [1*h_liooofgvbeggg2hdrfcw.jpeg]
   [19]source

   neural networks are known to be universal function approximators, so
   what are we missing? that   s where hidden layers come at hand and it   s
   what we are going to talk about in the following next.

what   s next?

   we are going to talk about the power of hidden layers and the
   processing they are capable of doing and how we train complex neural
   networks using id26 and id119 based algorithms,
   and if you are willing to go deeper in the study of the inner details
   of neural networks, check out the links below, stay tuned!

references:

     * [20]neural networks and deep learning
     * [21]http://cs231n.github.io/neural-networks-1/
     * [22]implementing a neural network from scratch in python         an
       introduction
     * [23]building a neural network from scratch in python and in
       tensorflow
     * [24]understanding and coding neural networks from scratch in python
       and r
     * [25]a neural network in 11 lines of python (part 1)

before you go!

   if you enjoyed the writings leave your claps      to recommend this
   article so that others can see it.

   with     by david fumo.

   may the force be with you!

     * [26]machine learning
     * [27]data science
     * [28]artificial intelligence
     * [29]neural networks
     * [30]towards data science

   (button)
   (button)
   (button) 345 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of david fumo

[32]david fumo

   love coding, read, write and create. interested in ai, blockchain,
   startups, growth and entrepreneurship.

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 345
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/673ec7cdd89f
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/build-neural-network-from-scratch-part-2-673ec7cdd89f&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/build-neural-network-from-scratch-part-2-673ec7cdd89f&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_mapyuse3qfhb---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@davidfumo?source=post_header_lockup
  17. https://towardsdatascience.com/@davidfumo
  18. https://towardsdatascience.com/media/5ddcabfc180e2664c7a8fb0e88a282cc?postid=673ec7cdd89f
  19. https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781783988365/8/ch08lvl1sec59/limitations-of-the-id88
  20. http://neuralnetworksanddeeplearning.com/index.html
  21. http://cs231n.github.io/neural-networks-1/
  22. http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
  23. https://beckernick.github.io/neural-network-scratch/
  24. https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/
  25. https://iamtrask.github.io/2015/07/12/basic-python-network/
  26. https://towardsdatascience.com/tagged/machine-learning?source=post
  27. https://towardsdatascience.com/tagged/data-science?source=post
  28. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  29. https://towardsdatascience.com/tagged/neural-networks?source=post
  30. https://towardsdatascience.com/tagged/towards-data-science?source=post
  31. https://towardsdatascience.com/@davidfumo?source=footer_card
  32. https://towardsdatascience.com/@davidfumo
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/673ec7cdd89f/share/twitter
  39. https://medium.com/p/673ec7cdd89f/share/facebook
  40. https://medium.com/p/673ec7cdd89f/share/twitter
  41. https://medium.com/p/673ec7cdd89f/share/facebook
