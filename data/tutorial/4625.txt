   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a gentle introduction to neural networks series         part 1

   [16]go to the profile of david fumo
   [17]david fumo (button) blockedunblock (button) followfollowing
   aug 4, 2017

   a gentle introduction to neural networks series (ginns).
   [1*tjclpbcktemilclbph_j4q.jpeg]

introduction

   neural networks and deep learning are big topics in computer science
   and in the technology industry, they currently provide the best
   solutions to many problems in image recognition, id103 and
   natural language processing. recently many papers have been published
   featuring ai that can learn to paint, build 3d models, create user
   interfaces(pix2code), some create images given a sentence and there are
   many more incredible things being done everyday using neural networks.

   i   m writing this series of posts about neural networks and deep
   learning, where i   m going to guide you from learning the basic concepts
   of id158s (ann), show you examples from simple
   network to mimic the and gate, to solving image recognition tasks using
   convolutional neural networks (id98s), recurrent neural networks (id56)
   and more. the code will always be written in python, some times with
   the help of tensorflow (i don   t expect you to be guru using tensorflow
   as i will try to explain the code in details).

agenda

     * introduction to neural networks (this post)
     * and gate neural network (id88) and xor gate feedfoward neural
       network (2 layers).
     * mnist digit recognition with id98
     * mnist digit recognition with id56

neural networks

   the definition of a neural network, more properly referred to as an
   'artificial' neural network (ann), is provided by the inventor of one
   of the first neurocomputers, dr. robert hecht-nielsen. he defines a
   neural network as:

     "...a computing system made up of a number of simple, highly
     interconnected processing elements, which process information by
     their dynamic state response to external inputs."

   or you can also think of id158 as computational
   model that is inspired by the way biological neural networks in the
   human brain process information.

biological motivation and connections

   the basic computational unit of the brain is a neuron. approximately 86
   billion neurons can be found in the human nervous system and they are
   connected with approximately 10              10      synapses. the diagram below
   shows a cartoon drawing of a biological neuron (left) and a common
   mathematical model (right).
   [1*mz0a4eesdjysbvf5m_u-sw.png]
   [1*yf6bwjq0kdhtumero99buq.jpeg]
   biological neuron (left) and a common mathematical model (right)

   the basic unit of computation in a neural network is the neuron , often
   called a node or unit. it receives input from some other nodes, or from
   an external source and computes an output. each input has an associated
   weight (w), which is assigned on the basis of its relative importance
   to other inputs. the node applies a function to the weighted sum of its
   inputs.

   the idea is that the synaptic strengths (the weights w) are learnable
   and control the strength of influence and its direction: excitory
   (positive weight) or inhibitory (negative weight) of one neuron on
   another. in the basic model, the dendrites carry the signal to the cell
   body where they all get summed. if the final sum is above a certain
   threshold, the neuron can fire, sending a spike along its axon. in the
   computational model, we assume that the precise timings of the spikes
   do not matter, and that only the frequency of the firing communicates
   information. we model the firing rate of the neuron with an activation
   function (e.x sigmoid function), which represents the frequency of the
   spikes along the axon.

neural network architecture

     from the above explanation we can conclude that a neural network is
     made of neurons, biologically the neurons are connected through
     synapses where informations flows (weights for out computational
     model), when we train a neural network we want the neurons to fire
     whenever they learn specific patterns from the data, and we model
     the fire rate using an activation function.

   but that   s not everything   
     * input nodes (input layer): no computation is done here within this
       layer, they just pass the information to the next layer (hidden
       layer most of the time). a block of nodes is also called layer.
     * hidden nodes (hidden layer): in hidden layers is where intermediate
       processing or computation is done, they perform computations and
       then transfer the weights (signals or information) from the input
       layer to the following layer (another hidden layer or to the output
       layer). it is possible to have a neural network without a hidden
       layer and i   ll come later to explain this.
     * output nodes (output layer): here we finally use an activation
       function that maps to the desired output format (e.g. softmax for
       classification).
     * connections and weights: the network consists of connections, each
       connection transferring the output of a neuron i to the input of a
       neuron j. in this sense i is the predecessor of j and j is the
       successor of i, each connection is assigned a weight wij.
     * activation function: the activation function of a node defines the
       output of that node given an input or set of inputs. a standard
       computer chip circuit can be seen as a digital network of
       id180 that can be    on    (1) or    off    (0), depending
       on input. this is similar to the behavior of the linear id88
       in neural networks. however, it is the nonlinear activation
       function that allows such networks to compute nontrivial problems
       using only a small number of nodes. in id158s
       this function is also called the transfer function.
     * learning rule: the learning rule is a rule or an algorithm which
       modifies the parameters of the neural network, in order for a given
       input to the network to produce a favored output. this learning
       process typically amounts to modifying the weights and thresholds.

types of neural networks

   there are many classes of neural networks and these classes also have
   sub-classes, here i will list the most used ones and make things simple
   to move on in this journey to learn neural networks.

1. feedforward neural network

   a feedforward neural network is an id158 where
   connections between the units do not form a cycle. in this network, the
   information moves in only one direction, forward, from the input nodes,
   through the hidden nodes (if any) and to the output nodes. there are no
   cycles or loops in the network.

   we can distinguish two types of feedforward neural networks:

1.1. single-layer id88

   this is the simplest feedforward neural network and does not contain
   any hidden layer, which means it only consists of a single layer of
   output nodes. this is said to be single because when we count the
   layers we do not include the input layer, the reason for that is
   because at the input layer no computations is done, the inputs are fed
   directly to the outputs via a series of weights.
   [1*1wvszv59q750l6erccrgcg.gif]
   simple id88

1.2. multi-layer id88 (mlp)

   this class of networks consists of multiple layers of computational
   units, usually interconnected in a feed-forward way. each neuron in one
   layer has directed connections to the neurons of the subsequent layer.
   in many applications the units of these networks apply a sigmoid
   function as an activation function. mlp are very more useful and one
   good reason is that, they are able to learn non-linear representations
   (most of the cases the data presented to us is not linearly separable),
   we will come back to analyse this point in the example i   ll show you in
   the next post.
   [1*-ne4ok759a3uqkzi6mcjwa.jpeg]
   mlp

1.3. convolutional neural network (id98)

   convolutional neural networks are very similar to ordinary neural
   networks, they are made up of neurons that have learnable weights and
   biases. in convolutional neural network (id98, or convnet or shift
   invariant or space invariant) the unit connectivity pattern is inspired
   by the organization of the visual cortex, units respond to stimuli in a
   restricted region of space known as the receptive field. receptive
   fields partially overlap, over-covering the entire visual field. unit
   response can be approximated mathematically by a convolution operation.
   they are variations of multilayer id88s that use minimal
   preprocessing. their wide applications is in image and video
   recognition, recommender systems and natural language processing. id98s
   requires large data to train on.
   [1*n4h1sgwbwid4rrhszm9ejg.png]
   id98 for image classification

2. recurrent neural networks

   in recurrent neural network (id56), connections between units form a
   directed cycle (they propagate data forward, but also backwards, from
   later processing stages to earlier stages). this allows it to exhibit
   dynamic temporal behavior. unlike feedforward neural networks, id56s can
   use their internal memory to process arbitrary sequences of inputs.
   this makes them applicable to tasks such as unsegmented, connected
   handwriting recognition, id103 and other general sequence
   processors.

commonly used id180

   every activation function (or non-linearity) takes a single number and
   performs a certain fixed mathematical operation on it. here are some
   activations functions you will often find in practice:
     * sigmoid
     * tanh
     * relu
     * leaky relu

   follow this [18]id180 post and also this [19]cs231n
   (id98s) course notes for more details.

id158s and the brain

   id158s doesn   t work like our brain, ann are simple
   crude comparison, the connections between biological networks are much
   more complex than those implemented by id158
   architectures, remember, our brain is much more complex and there is
   more we need to learn from it. there are many things we don   t know
   about our brain and this also makes hard to know how we should model an
   artificial brain to reason at human level. whenever we train a neural
   network, we want our model to learn the optimal weights (w) that best
   predicts the desired outcome (y) given the input signals or
   information (x).

what   s next?

   in the next post i will bring some code examples and explain a bit more
   of how we can train neural networks. stay tuned!

references:

     * [20]neural networks and deep learning
     * [21]http://cs231n.github.io/neural-networks-1/

   if you enjoyed the writings leave your claps      to recommend this
   article so that others can see it.

     * [22]machine learning
     * [23]data science
     * [24]artificial intelligence
     * [25]neural networks
     * [26]deep learning

   (button)
   (button)
   (button) 655 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [27]go to the profile of david fumo

[28]david fumo

   love coding, read, write and create. interested in ai, blockchain,
   startups, growth and entrepreneurship.

     (button) follow
   [29]towards data science

[30]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 655
     * (button)
     *
     *

   [31]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [32]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2b90b87795bc
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-gentle-introduction-to-neural-networks-series-part-1-2b90b87795bc&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_i1rjp0bgvacl---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@davidfumo?source=post_header_lockup
  17. https://towardsdatascience.com/@davidfumo
  18. https://medium.com/towards-data-science/activation-functions-in-neural-networks-58115cda9c96
  19. http://cs231n.github.io/neural-networks-1/
  20. http://neuralnetworksanddeeplearning.com/index.html
  21. http://cs231n.github.io/neural-networks-1/
  22. https://towardsdatascience.com/tagged/machine-learning?source=post
  23. https://towardsdatascience.com/tagged/data-science?source=post
  24. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  25. https://towardsdatascience.com/tagged/neural-networks?source=post
  26. https://towardsdatascience.com/tagged/deep-learning?source=post
  27. https://towardsdatascience.com/@davidfumo?source=footer_card
  28. https://towardsdatascience.com/@davidfumo
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/?source=footer_card
  31. https://towardsdatascience.com/
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/2b90b87795bc/share/twitter
  35. https://medium.com/p/2b90b87795bc/share/facebook
  36. https://medium.com/p/2b90b87795bc/share/twitter
  37. https://medium.com/p/2b90b87795bc/share/facebook
