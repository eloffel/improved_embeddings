   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep learning journal
   [7]deep learning journal
   [8]sign in[9]get started
     __________________________________________________________________

   [1*2r2vbwx60_ycfi2utrp-ew.jpeg]

faster ai: lesson 2         tl;dr version of fast.ai part 1

   [10]go to the profile of kshitiz rimal
   [11]kshitiz rimal (button) blockedunblock (button) followfollowing
   aug 26, 2017
     __________________________________________________________________

     in my previous post [12]lesson 1, i talked briefly about how to
     setup software and hardware environment for this course and went
     through fine tuning in brief. if you haven   t read that, please go
     through it before reading this post.

   in this lesson, we are going little deeper with these concepts and
   implement a neural network from scratch while explaining important
   aspects of it.

   to keep things short and to explain them properly, i have divided this
   lesson into 4 parts:
    1. dogs vs cats redux and submitting to kaggle [[13]time: 17:14]
    2. details on id98 features and fine tuning [[14]time: 54:09]
    3. explaining neural network in excel [[15]time: 1:03:30]
    4. a linear model from scratch and how to use that to classify cats
       and dogs [[16]time: 1:14:08]

1. dogs vs cats redux and submitting to kaggle

   in my previous post i talked about dogs vs cats competition and how to
   use that data and use a process called fine tuning on pre-trained vgg16
   model to classify between dogs and cats images.

   [17]dogs vs cats redux is essentially the same with an improvement to
   utilize latest    kernel    features of kaggle. one major difference here
   is the structure of data provided by the competition and the way keras
   handles data to classify between images. basically you move cats images
   inside    cats    folders of    train    and    valid    folders and dogs images to
      dogs    folder, in a same manner.

   after that you do the same thing for    sample    folder as well where
   there are subsets of cats and dogs images to quickly train and test the
   model to ensure if its working properly. to learn how efficiently
   jeremy did it inside jupyter notebook, [18]watch it here [19]or follow
   this notebook.

   after setting up the folder structure, you can simply re-execute the
   same 7 lines of code from [20]previous post to train the model and get
   the results of classification.
   [1*z9mo8pkyz8dhqk0gnri-3g.png]

   in this picture, there is one extra line
   [1*udp2q9irimpecxr6kgkj_a.png]

   in simple terms it saves your trained model in a file and with that you
   can use the same model later on without any sort re-training.

   after successfully training the model, you may want to submit your
   results to the competition. its a very simple and straight forward
   process, you basically need to:

   1. install kaggle-cli tool from pip as [pip install kaggle-cli]
   2. configure your tool for the right competition as
   [kg config -g -u `username` -p `password` -c `competition`]
   3. after your submission is ready submit you csv file as
   [kg submit `entry` -u `username` -p `password` -c `competition` -m
   `message`]

   preparing your submission

   you will need to create a csv file in order to submit to kaggle. this
   csv file includes file names of you test data without the extension as
      id    and id203 of each of that file as    label   . the final
   structure looks like this
   [1*1mmggbdjwrey2b0o86sk4q.png]

   you can get the prediction on each of these test images by this command
   [1*jlc0pgbjosvonjuo5r0ymg.png]

   as these predictions are of 0 or 1, you will need to convert it to
   fractional value of 0.5, 0.95 or something like that, like above . for
   that you can use numpy   s clip function with values between 0.02 and
   0.98 for better results.

   it is very important to know how kaggle evaluates its submissions.
   while explaining how predictions are made in this vgg model, jeremy
   goes on to explain a particular type of id168 called log loss
   or cross id178 loss and its importance on calculating loss and
   generating predictions. it turns out, its the same id168, the
   competition uses to evaluate the results as well. so, before our values
   were like 1s and 0s for prediction and as we are taking log of 0s and
   1s here, the result is not what we desire. so its always a good idea to
   convert your prediction if its zero to 0.02 and if its 1 then to 0.98
   or around that to get proper evaluation results.

   after getting the predictions, you will need to prepare the csv file
   with proper structure of one column as filenames and another column as
   probabilities. for that you can follow this code snippet used by jeremy
   in this lesson
   [1*ow7cvca6in54fiovhwybyg.png]
   [1*cgc_rusgrhs2bbu39tefpg.png]

   here, he is basically getting the probabilities and file names from the
   batches and stacking them on a file called subm98.csv. you can watch
   and follow the [21]whole process here.

   to get better results at these trainings, he recommends to fit them
   more than once and with slightly different learning rates.
   [1*fkly-zoiigtg2dznlzvvla.png]

   you can even visualize and see how your model is performing by looking
   at the results of how many it got right and how many of it got wrong.
   you follow these 5 steps after training the model, to check your model
   performance
   [1*v3qucrmofd1wiqvxk8kg_q.png]

   some of the results from these tests are as:
   [1*nv1o5jjq-yblalh9h8x63a.png]
   correct labels
   [1*l14oocznxejcrfej_hyfvg.png]
   incorrect labels
   [1*cu2vx0no_3wjd6qs3kiuvq.png]

2. details on id98 features and fine tuning

   it is important to know why we should do fine tuning but before that
   lets look at what these layers of convolutional neural network, like
   vgg actually learns when feed images to it.

   [22]one amazing paper by [23]matthew d zeiler, [24]rob fergus explained
   this in much depth. lets try to understand what they mean.

   from their research it is found that the very first layer of a id98
   tends to learn primitive features of an image, like lines. lets look at
   this image
   [1*bb6gfssfhr0o_vorxhfuyg.png]

   on our left hand side is a set of filters on first layer of a id98, by
   looking at it, we can see it recognizes some primitive forms of images
   like diagonal lines and some solid colors.

   when these filters from layer 1 with some more filters continued to
   next layer 2, they learn little bit more of that same image like this
   [1*rf4z6cta5jwuku02mbay3g.png]

   we can see, this layer recognizes shapes like, window or door shapes
   from an image.

   if this process is continued further we get results like this
   [1*fuxilx9rihwur-5exthd9a.png]
   [1*h1vylv-wueiptrwi8iozsg.png]

   at layer 4 and 5 we can see it is recognizing more complex forms like
   face of a dog and eye of a bird.

   there are 2 reasons why this visualization is so important.
    1. now we have intuitive understanding of the model and what they
       might learn at each layer
    2. for fine tuning, we need to decide what layer should we remove and
       what layers should we keep.

   in my previous post, i didn   t cover what fine tuning does under the
   hood. let me go through that quickly.

   generally in fine tuning, we remove the last dense (fully connected)
   layer and replace that one with a new layer which uses our dataset.
   now, this is fine as long as the pre-trained model are trained on
   similar kind of dataset, like ours, as id163 has images of dogs and
   cats as well.

   but lets say you want to classify brain tumors from ct scans. now,
   should we take all the layers of vgg and fine tune like before?

   one way to think of this is, if the ct scans image is totally different
   than id163 images, which it is. you can only take learned layers of
   the model where it can recognize primitive shapes and structure and
   fine tune with that layers only. that way your model will not be
   unnecessarily huge and unpredictable and you can still get benefit of a
   pre-trained model on huge dataset than yours.

   for such applications and intuitions, visualizations like this is very
   important.

3. explaining neural network in excel

   to explain things more simply, jeremy teaches the functioning of a
   neural network by using microsoft excel. which is very unusual, but
   surprisingly very intuitive and effective.

   you can download the [25]excel files from this link and go through it,
   but let me put it briefly, what he does here.

   one way to look at neural network is like a tabular layers of integers
   which are stacked on top of another.

   suppose you have input as integers and for a neural networks you need
   weight values which need to get multiplied with these input integers.
   lets visualize this tabular layers like this in excel.
   [1*ajvg5nziyugf--fjtilfqg.png]
   here, x1, x2, x3 are input values and y1 and y2 are output values and
   randomly generated weights are listed under weights.

   here these weights values are randomly generated, which is what a
   neural network does in real as well, it randomly initializes the weight
   values and when these weights at each layer gets multiplied with input
   of that layer, activations at each layer is what we get.

   but it is very important to consider right kind of initialization for
   these weight values because if they are not at scale of our input
   values, the predicted output will be far off our real output and loss
   will be high, which means the network is not performing properly. for
   that [26]xavier initialization is used, which creates the initial
   weight values with same scale of the input values.

   every now and then jeremy uses this explanation method using excel to
   teach us these concepts behind neural networks, which i think is
   totally awesome and you should definitely check them out in video.

4. a linear model from scratch and how to use that to classify cats and dogs

     you can get the code for this in [27]lesson 2.ipynb file.

   we need to understand how a linear model simple as a model of a
   straight line (y = ax+b) works, in order to understand more
   sophisticated deep neural networks. and after knowing that at intuitive
   higher level, its not so different at all.

   in [28]lesson 0, i briefly talked about about such linear model. in
   this lesson we are going to construct such in keras.

   its a very straight forward process in keras for such model.
   [1*ytb0j2idq25osm-xiglm0w.png]

   you randomly initialize input values of 2 dimensions of 30 entries in a
   vector form. then to create y values, you dot multiply it with [2,3]
   which is a vector and in our case, one of the weights. then add the
   resultant with 1, which is another weight value.

   in such manner you create 30 entries of x, the input and 30 entries of
   y the output. now the job of the model is to figure out the right
   weight values given these xs and ys only. that is, it needs to find
   values of [2,3] vector and [1] given to create these y values.

   you can initialize any such linear layer using dense method in keras.
   and for a neural network to use such layers, you need to wrap around
   these layers by sequential method, which basically means, this is a
   neural model and here dense is one of the layers on it, you can provide
   many type of layers in a array form, in this sequential method.
   [1*xzncnqoe1s5j-unjkeks5a.png]

   and then you compile that model with optimizer, which basically
   corrects the values of randomly initialized weights and make it close
   to real one at each iteration. here loss is mean squared loss which
   measures euclidean distance between real output and predicted output.
   comparing this loss, the weight values are updated using gradient
   descent, which i will cover in following lessons.

   then you train the model, which is by invoking the fit method
   [1*en3v7hpifbdq6yalaua1ng.png]

   after that, when you look at the learned weight values, it should be
   close to our, [2,3] and 1 values.
   [1*o-dggkuf-0t4xdqkagavga.png]

   using this same type of dense(fully connected/linear model) we can also
   use to classify our existing cats and dogs dataset.

   for that what we need to do is, get the predictions of each of our
   images in train and validation set from our original pre-trained vgg
   model, which will give predictions between 1000 image classes, and use
   that prediction of 1000 classes as input to our new linear model with
   input as 1000 dimension and output as 2 for our cats and dogs.
   [1*x4yvg0nsf6nhpt2npjbnuw.png]

   to get the predictions, lets use model.predict on our pre-trained model
   [1*fwhjo5tl6xbqqpwsxclysg.png]

   then lets define our linear model to use these predictions as input
   [1*chxluyw4-jfpbasmerklka.png]

   and lets fit the model
   [1*q_gjb1bbwfbcctfjkv9xaq.png]

   now if we look at our model summary, it shows the linear model we are
   using this model should predict the same way our previous fine tuned
   model on vgg.
   [1*yjqyoug_tpb9apkgc2btea.png]

   one thing that separates neural networks from just linear model is the
   id180. it is present in every neural network and is used
   to activate the neurons on each layer of neural network. what it does
   is, it passes the linear values of any layer of neural network through
   some non linear functions, such as relu, tanh are among many. these non
   linearities is what makes the neural network to solve any kind problem
   approximately.

   this fantastic article called [29]a visual proof that neural nets can
   compute any function explains it in more detail.

   now lets do some fine tuning without any abstractions and see what it
   really means to do fine tuning on a pre-trained model.

   lets take our original pre-trained vgg model and see the layers of it
   [1*kf5soohjifb-ttq0uchcpw.png]
   [1*0kg-vcszmxewylvqh2n4ha.png]

   at the end, like our previous model there is a dense layers which
   outputs the probabilities of 1000 classes, as we only need to predict
   between 2 classes, cats and dogs, lets remove it and lets set the other
   layers of the model to untrainable, because we want to utilize what it
   has learned and add on top of it, not influence it with our dataset.
   [1*1p_wprbknjr3wvokgrvuvg.png]

   now, lets add our new dense layer which will output only two classes.
   [1*-dc-ql0aftuzsfde-8fkva.png]

   now lets setup our optimizers and fit the model with our cat and dog
   dataset.
   [1*vncnxn97mf_xbjku8j_hyw.png]

   thats it, this is what under the hood, fine tuning is. in my previous
   posts, i have showed one overlay function created by jeremy to quickly
   fine tune the model without showing its inner process.

   using this same technique and with new datasets you can train model to
   classify any kind of images. if you follow the notebook file, you can
   also learn to fine tune more than one layer in a model.

   for more details, you can always [30]watch the full video or follow
   along the [31]class notes. you can also view the [32]timeline of this
   lesson and jump to any particular topic you want to watch.

   in the next lesson, i will talk about stochastic id119, max
   pooling, fitting the model and many more.

   see you there.

[33]next post: lesson 3

     * [34]machine learning
     * [35]artificial intelligence
     * [36]deep learning

   (button)
   (button)
   (button) 153 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [37]go to the profile of kshitiz rimal

[38]kshitiz rimal

   ai developer, intel ai student ambassador, co-founder @ ai for
   development: ainepal.org, city ai ambassador: kathmandu

     (button) follow
   [39]deep learning journal

[40]deep learning journal

   deep learning lessons

     * (button)
       (button) 153
     * (button)
     *
     *

   [41]deep learning journal
   never miss a story from deep learning journal, when you sign up for
   medium. [42]learn more
   never miss a story from deep learning journal
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f803218ffe6d
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-learning-journals?source=avatar-lo_nlc225ffvgo9-aa98d6a0017e
   7. https://medium.com/deep-learning-journals?source=logo-lo_nlc225ffvgo9---aa98d6a0017e
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-2-tl-dr-version-of-fast-ai-part-1-f803218ffe6d&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-2-tl-dr-version-of-fast-ai-part-1-f803218ffe6d&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@kshitizrimal?source=post_header_lockup
  11. https://medium.com/@kshitizrimal
  12. https://medium.com/deep-learning-journals/faster-ai-lesson-1-tl-dr-version-of-fast-ai-part-1-d63ad6b2993b
  13. https://youtu.be/e3am6xtekjc?t=1034
  14. https://youtu.be/e3am6xtekjc?t=3249
  15. https://youtu.be/e3am6xtekjc?t=3810
  16. https://youtu.be/e3am6xtekjc?t=4448
  17. https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition
  18. https://youtu.be/e3am6xtekjc?t=1225
  19. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/dogs_cats_redux.ipynb
  20. https://medium.com/deep-learning-journals/faster-ai-lesson-1-tl-dr-version-of-fast-ai-part-1-d63ad6b2993b
  21. https://youtu.be/e3am6xtekjc?t=1368
  22. https://arxiv.org/abs/1311.2901
  23. https://arxiv.org/find/cs/1/au:+zeiler_m/0/1/0/all/0/1
  24. https://arxiv.org/find/cs/1/au:+fergus_r/0/1/0/all/0/1
  25. https://github.com/fastai/courses/tree/master/deeplearning1/excel
  26. https://www.quora.com/what-is-an-intuitive-explanation-of-the-xavier-initialization-for-deep-neural-networks
  27. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson2.ipynb
  28. https://medium.com/deep-learning-journals/faster-ai-lesson-0-tl-dr-version-of-fast-ai-part-1-dd060ae7e319
  29. http://neuralnetworksanddeeplearning.com/chap4.html
  30. https://www.youtube.com/watch?v=e3am6xtekjc&feature=youtu.be
  31. http://wiki.fast.ai/index.php/lesson_2_notes
  32. http://wiki.fast.ai/index.php/lesson_2_video_timeline
  33. https://medium.com/deep-learning-journals/faster-ai-lesson-3-tl-dr-version-of-fast-ai-part-1-d4e76c8c7ab6
  34. https://medium.com/tag/machine-learning?source=post
  35. https://medium.com/tag/artificial-intelligence?source=post
  36. https://medium.com/tag/deep-learning?source=post
  37. https://medium.com/@kshitizrimal?source=footer_card
  38. https://medium.com/@kshitizrimal
  39. https://medium.com/deep-learning-journals?source=footer_card
  40. https://medium.com/deep-learning-journals?source=footer_card
  41. https://medium.com/deep-learning-journals
  42. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  44. https://medium.com/p/f803218ffe6d/share/twitter
  45. https://medium.com/p/f803218ffe6d/share/facebook
  46. https://medium.com/p/f803218ffe6d/share/twitter
  47. https://medium.com/p/f803218ffe6d/share/facebook
