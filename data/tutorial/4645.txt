   #[1]abigail see

[2]abigail see

     * [3]about

four deep learning trends from acl 2017

part one: linguistic structure and id27s

   august 30, 2017

   this is a two-part post. [4]click here for part two.

   [5]click here to read a shortened version of this post translated to
   chinese.

introduction

   [6]   nlp is booming   , declared joakim nivre at the presidential address
   of [7]acl 2017, which i attended in vancouver earlier this month. as
   evidenced by the throngs of attendees, interest in nlp is at an
   all-time high     an increase that is chiefly due to the successes of the
   deep learning renaissance, which recently swept like a tidal wave over
   the field.

   beneath the optimism however, i noticed a tangible anxiety at acl, as
   one field adjusts to its rapid transformation by another. researchers
   asked whether there is anything of the old nlp left     or was it all
   swept away by the tidal wave? are neural networks the only technique we
   need any more? how do we do good science now that experiments are so
   empirical, papers are immediately on arxiv, and access to gpus can
   determine success?
   i don't have money for gpus! is nlp dead? and language? i really like
   my features!

   mirella lapata expresses the community's concerns in her keynote

   though these difficult questions were at the forefront of the
   conference (the presidential address even alluded to [8]a recent
   high-profile debate on the subject), the overall mood was positive
   nonetheless. at acl 2017, the nlp community continued to
   enthusiastically embrace deep learning, though with a healthy
   skepticism. as researchers are starting to reach a clearer view of what
   works and what doesn   t with current neural methods, there is a growing
   trend to consult older nlp wisdom to guide and improve those methods.
   in this post i take a look at what   s happening at this pivotal time for
   nlp research.

about this post

   in this two-part post, i describe four broad research trends that i
   observed at the conference (and its co-located events) through papers,
   presentations and discussions. the content is guided entirely by my own
   research interests; accordingly it   s mostly focused on deep learning,
   sequence-to-sequence models, and adjacent topics. this first part will
   explore two inter-related themes: [9]linguistic structure and [10]word
   representations.

   [11]green links are ordinary hyperlinks, while [12]blue links lead to
   papers, and offer bibliographic information when you hover over them
   (not supported on mobile).

   disclaimer: this recap is by no means exhaustive, as i did not come
   close to reading every paper at acl     consequently i   m sure there are
   many relevant papers that are not mentioned here. secondly, i have done
   my best to accurately understand others    work, but if i   ve
   misrepresented any piece of work, let me know. thirdly, as a person who
   is fairly new to the field, i may lack a longer perspective on some of
   these trends. if you have a more historically-informed perspective, i   d
   be interested to hear it.

trend 1: linguistic structure is back

   the recent deep learning renaissance has emphasized a simple uniform
   paradigm for nlp: language is just sequences of words. according to
   this logic, any further structure is unnecessary     simply train a id56
   end-to-end and stochastic id119 will figure out the rest!
   while this approach has rapidly found enormous popularity and success
   (not least due to the convenience of requiring no feature engineering),
   its limitations are now becoming more apparent. at acl 2017, several
   prominent researchers pushed back against the    language is just
   sequences    zeitgeist and presented reasons, both practical and
   principled, why nlp should re-embrace linguistic structure.

reason 1: reduce the search space

   in her very entertaining [13]keynote talk, mirella lapata questioned
   the hegemony of the id56 sequence-to-sequence framework, [14]asking
   rhetorically whether its dominance means language is dead, and all
   linguistic features should be discarded. instead she concluded that
   [15]structure is coming back, and provided [16]via example one reason
   to embrace its return: linguistic structure reduces the search space of
   possible outputs, making it easier to generate well-formed output.

   for example, code generation involves mapping a natural language
   utterance such as    generate a list of the first 10 square numbers    to a
   corresponding code snippet, e.g.    [x**2 for x in range(10)]    in python.
   this task has been attempted with the standard sequence-to-sequence
   method, which regards the code as simply a sequence of tokens, rather
   than its underlying tree structure. this makes the generation task an
   unconstrained search over the entire output space of all sequences of
   tokens     a search task that is both daunting and prone to generate
   ill-formed output (for example, the decoder may generate code with
   mismatched brackets). in their acl papers, both [17]yin and neubig and
   [18]rabinovich et al. take the id170 approach instead,
   and directly generate the underlying abstract syntax tree. this
   approach restricts the search space to well-formed trees only,
   eliminating ill-formed output and making the search problem more
   manageable.

   though linguistic structure has obvious benefits for tasks with
   highly-formalized output such as code generation and id29,
   it can also help reduce the search space for less obvious tasks, like
   cloze-style reading comprehension. by observing that the correct answer
   is almost always a constituent in the source document   s parse tree,
   [19]xie and xing construct a system that explores only those nodes    
   they argue this is both easier and more effective than exploring all
   possible spans in the document.

reason 2: linguistic scaffolding

   who wants an all-squash diet?

   noah smith

   in his [20]keynote talk, noah smith argued against what he calls the
      all-squash diet        the use of linear transformations + squashing
   functions (a.k.a. neural networks) as the sole model for nlp. instead
   he encouraged the nlp community to think about our models    inductive
   biases     that is, the models    underlying assumptions and how those
   assumptions affect what they learn.

   in particular, smith highlighted the power of id72 as a
   way to incorporate a desirable inductive bias. it is well-known that
   jointly learning a linguistic scaffolding task (such as syntactic
   parsing) with the main task (such as machine translation) tends to
   boost the performance of the main task     most likely because the main
   task is enriched by the useful information contained in the low-level
   shared representations. acl saw several papers successfully take this
   approach     in particular [21]eriguchi et al. and [22]wu et al. designed
   new hybrid decoders for id4 that use shift-reduce algorithms to
   simultaneously generate and parse the target sequence.

   these joint id4+parsing systems, which seem to outperform
   sequence-to-sequence systems, may also benefit from reason 1 (reducing
   the search space). as has been [23]noted, id4 performance is poor for
   long sentences, and (counter-intuitively) larger beam sizes can
   sometimes degrade performance further. if widening the search beam
   causes a drop in performance, this implies that our current methods
   have difficulty identifying the best output when there are more
   candidates to choose from. jointly parsing the output may eliminate
   poor-quality outputs from the search beam, thus allowing id125 to
   choose between better-quality candidates.

reason 3: syntactic recency > sequential recency

   chris dyer also argued for the importance of incorporating linguistic
   structure into deep learning in his conll keynote [24]should neural
   network architecture reflect linguistic structure? like noah smith, he
   drew attention to the inductive biases inherent in the sequential
   approach, [25]arguing that id56s have an inductive bias towards
   sequential recency, while syntax-guided hierarchical architectures
   (such as [26]recursive nns and [27]id56gs) have an inductive bias
   towards syntactic recency. asserting that language is inherently
   hierarchical, dyer concluded that syntactic recency is a preferable
   inductive bias to sequential recency.

   at acl, several papers noted the apparent inability of id56s to capture
   long-range dependencies, and obtained improvements using recursive
   models instead. for example, in [28]improved id4
   with a syntax-aware encoder and decoder, chen et al. find that using a
   recursive encoder improves performance overall, and the improvement is
   greater for longer sentences. the latter may be evidence of the benefit
   of syntactic recency, which can capture long-term dependencies more
   easily than sequential recency.
   syntax tree with long-distance dependency indicated.

   this example from wu et al. shows the difference between syntactic
   recency (red dotted line) and sequential recency.

looking forward

   though linguistic structure is making a comeback, some barriers remain.
   id72 is cumbersome to implement. non-sequential
   architectures are harder to parallelize on gpus (however [29]new
   dynamic libraries provide easier and more efficient implementations).
   supervised learning of id170 tasks can be hindered by a
   lack of parallel data. fortunately, the resurgence of reinforcement
   learning is well-timed; at acl 2017 both [30]liang et al. and [31]iyyer
   et al. use weak supervision to perform id53 via semantic
   parsing, without access to the parses themselves.

   despite these barriers, i think the nlp community will continue to
   (re-)embrace linguistic structure as its benefits become more apparent.
   while the    language is just sequences    paradigm argues that id56s can
   compute anything, researchers are increasingly interested in how the
   inductive biases of the sequential model affect what they do compute.
   on this matter, it seems that a little linguistic structure can go a
   long way.

trend 2: reconsidering id27s

   the number of papers with    id27    in the title fell from ten
   to four this year, perhaps in part due to a shift towards
   sub-word-level representations (more on that below). nonetheless, word
   embeddings remain a standard technique, and the related papers at acl
   this year were very interesting     perhaps precisely because word
   embeddings have passed through the    hype    stage and into the
      thoughtful scrutiny    stage. these papers probed the boundaries of how
   id27s succeed and fail, what they do and don   t capture, and
   how to improve on their weaknesses.

better understanding id27s

   perhaps the most famous and surprising (but often-exaggerated) success
   of id27s is their additive compositional structure, as
   evidenced by word analogies. the cryptically-titled [32]skip-gram -
   zipf + uniform = vector additivity aims to explain this success. the
   authors prove that distributional id27s trained with the
   skip-gram model have additive structure under certain assumptions    
   most notably that the words are uniformly distributed (this is the
   meaning of    -zipf +uniform   ). though training corpora are not uniformly
   distributed, this result may go some way to explain the additivity of
   id27s.

   other papers investigated the limitations of the distributional
   assumption at the heart of id27s. li and gauthier ask [33]are
   distributional representations ready for the real world?, and find that
   while id27s capture certain conceptual features such as    is
   edible   , and    is a tool   , they do not tend to capture perceptual
   features such as    is chewy    and    is curved        potentially because the
   latter are not easily inferred from id65 alone. the
   paper joins a growing call for grounded learning, as evidenced by the
   founding of a new workshop on [34]language grounding for robotics.

   another, more glaring baked-in problem of id27s is that they
   do not account for polysemy, instead assigning exactly one vector per
   surface form. in one approach to this problem, [35]upadhyay et al.
   leverage multi-lingual parallel data to learn multi-sense word
   embeddings     for example, seeing the english word bank translated to
   both the french words banc and banque is evidence that bank is
   polysemous, and helps disentangle its two meanings. in [36]multimodal
   word distributions, athiwaratkun and wilson represent words not by
   single vectors, but by gaussian id203 distributions with multiple
   modes     thus capturing both uncertainty and polysemy. the paper has a
   very impressive [37]tensorboard demo: go to the    embeddings    tab and
   search for a polysemous word like    zip   . you should find that the three
   modes are clustered with related words from the three different senses
   (zip code, clothes zip and zipped file).
   the three senses of 'zip' with their different word clusters

   athiwaratkun et al.

going sub-word

   arguably the most urgent limitation of standard id27s is
   their blindness to morphological information, instead treating each
   surface form as a separate, anonymous unit. this can cause problems
   like an inability to recognize that two words (e.g. walker and walking)
   have the same lemma (walk) and are therefore highly related. this is
   the main reason for the recent shift away from id27s and
   towards sub-word representations, such as characters, character
   id165s, and word pieces. these representations had a strong showing at
   acl 2017, comparing favorably to id27s on both intrinsic
   tasks like [38]word similarity and analogies as well as extrinsic tasks
   like [39]machine translation, [40]id38 and [41]dependency
   parsing. for logographic languages like chinese/japanese/korean, the
   meaning of a character can be [42]composed from the visual features of
   its component parts.
   different characters with similar parts highlighted

   visual compositionality of characters. liu et al.

   with these sub-word representations, and in particular the
   character-id98 emerging as a potential new standard, is morphology
   solved? at least two papers gave a resounding    no   . [43]vania and lopez
   compared the id38 performance of several sub-word
   compositional representations, and found that none of them perform as
   well as a model that has access to gold morphological annotations. this
   result held even when providing the raw input model with ten times the
   training data     suggesting that at best, our current id38
   methods require a very large amount of data to implicitly learn
   morphology, and at worst, no amount of training data can replace
   morphological understanding. in [44]what do id4
   models learn about morphology?, belinkov et al. show that while
   character-based id4 representations are better than word-based
   representations both for id4 and morphological tagging, they have far
   from perfect performance on the latter.

   these results suggest that, if we want truly morphologically-aware word
   representations, we may need a more explicit model of morphology than
   just character composition. in their [45]morph-fitting paper, vuli   et
   al. fine-tune id27s by using some very simple morphological
   rules written by non-linguists (e.g., in english the prefix un-
   indicates an antonym). this results in substantial improvements,
   showing that even a modicum of linguistic knowledge can be very
   effective. meanwhile, [46]cotterell and sch  tze present a more
   comprehensive model of morphology, jointly learning a system that can
   both segment a word into its morphological components (e.g.
   questionably     question+able+ly) and compose the component
   representations back into the word representation. i think this is a
   very worthwhile approach, as any morphological understanding system
   must be able to compose and decompose meaning. though the model
   performs well on the intrinsic evaluation tasks, i   d be interested to
   see how easily and how successfully it transfers to extrinsic tasks
   such as syntactic parsing or id38.
   the id27 is be composed from the embeddings of its
   morphological parts

   cotterell and sch  tze

looking forward

   words are the very basis of language, so our assumptions matter when we
   choose how to model them.

   though id65 has served us well so far, words are
   more than the contexts in which they appear. in the coming years i
   think we will see more grounded, visual and interactive language
   learning to complement distributional representations.

   like the    language is just sequences of words    assumption,    words are
   just anonymous tokens    seems to be on its way out. however, i expect
   the question of    words are just sequences of characters    vs.
      morphological structure is important    will be a matter of future
   debate, both philosophical and practical.
     __________________________________________________________________

next time

   if you   ve enjoyed this post, check out part two, in which i discuss
   interpretability and attention, and find that neither are as easy to
   define as we think they might be.

   [47]click here for part two.

   [48]taming recurrent neural networks for better summarization prev
   [49]four deep learning trends from acl 2017 part two: interpretability
   and attention next

   please enable javascript to view the [50]comments powered by disqus.

   powered by [51]jekyll with [52]type theme

references

   1. http://abigailsee.com/feed.xml
   2. http://www.abigailsee.com/
   3. http://www.abigailsee.com/about/
   4. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html
   5. https://medium.com/@cyeninesky3/   acl-2017-                                 -nlp-         -3163c6a91c50
   6. https://www.slideshare.net/aclanthology/joakim-nivre-2017-presidential-address-acl-2017-challenges-for-acl/4?src=clipshare
   7. http://acl2017.org/
   8. https://medium.com/@yoav.goldberg/an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7
   9. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html#structure
  10. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html#word_emb
  11. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html#!
  12. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html#!
  13. https://www.slideshare.net/aclanthology/mirella-lapata-2017-translating-from-multiple-modalities-to-text-and-back?ref=https://chairs-blog.acl2017.org/2017/08/06/archiving-your-presentations-and-posters/
  14. https://www.slideshare.net/aclanthology/mirella-lapata-2017-translating-from-multiple-modalities-to-text-and-back/23?src=clipshare
  15. https://www.slideshare.net/aclanthology/mirella-lapata-2017-translating-from-multiple-modalities-to-text-and-back/73?src=clipshare
  16. https://www.slideshare.net/aclanthology/mirella-lapata-2017-translating-from-multiple-modalities-to-text-and-back/74?src=clipshare
  17. http://www.aclweb.org/anthology/p/p17/p17-1041.pdf
  18. http://www.aclweb.org/anthology/p/p17/p17-1105.pdf
  19. http://www.aclweb.org/anthology/p/p17/p17-1129.pdf
  20. https://www.slideshare.net/aclanthology/noah-a-smith-2017-squashing-computational-linguistics?ref=https://chairs-blog.acl2017.org/2017/08/06/archiving-your-presentations-and-posters/
  21. http://www.aclweb.org/anthology/p/p17/p17-2012.pdf
  22. http://www.aclweb.org/anthology/p/p17/p17-1065.pdf
  23. http://aclweb.org/anthology/w/w17/w17-3204.pdf
  24. http://www.conll.org/keynotes-2017
  25. https://twitter.com/boknilev/status/893145262473924608
  26. https://cs224d.stanford.edu/lectures/cs224d-lecture10.pdf
  27. https://soundcloud.com/nlp-highlights/04-recurrent-neural-network-grammars-with-chris-dyer
  28. http://www.aclweb.org/anthology/p/p17/p17-1177.pdf
  29. https://github.com/clab/dynet
  30. http://www.aclweb.org/anthology/p/p17/p17-1003.pdf
  31. http://www.aclweb.org/anthology/p/p17/p17-1167.pdf
  32. http://www.aclweb.org/anthology/p/p17/p17-1007.pdf
  33. http://www.aclweb.org/anthology/w/w17/w17-2810.pdf
  34. https://robonlp2017.github.io/
  35. http://aclweb.org/anthology/w/w17/w17-2613.pdf
  36. http://www.aclweb.org/anthology/p/p17/p17-1151.pdf
  37. http://35.161.153.223:6003/
  38. http://www.aclweb.org/anthology/q/q17/q17-1010.pdf
  39. https://arxiv.org/pdf/1610.03017.pdf
  40. http://www.aclweb.org/anthology/p/p17/p17-1137.pdf
  41. http://www.aclweb.org/anthology/p/p17/p17-2106.pdf
  42. http://www.aclweb.org/anthology/p/p17/p17-1188.pdf
  43. http://www.aclweb.org/anthology/p/p17/p17-1184.pdf
  44. http://www.aclweb.org/anthology/p/p17/p17-1080.pdf
  45. http://www.aclweb.org/anthology/p/p17/p17-1006.pdf
  46. https://arxiv.org/pdf/1701.00946.pdf
  47. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html
  48. http://www.abigailsee.com/2017/04/16/taming-id56s-for-better-summarization.html
  49. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html
  50. https://disqus.com/?ref_noscript
  51. http://jekyllrb.com/
  52. https://rohanchandra.github.io/project/type/
