   #[1]abigail see

[2]abigail see

     * [3]about

four deep learning trends from acl 2017

part two: interpretability and attention

   august 30, 2017

   [4]click here for part one.

   this is the second of a two-part post in which i describe four broad
   research trends that i observed at [5]acl 2017. in part one i explored
   the shifting assumptions we make about language, both at the sentence
   and the word level, and how these shifts are prompting both a comeback
   of linguistic structure and a re-evaluation of id27s.

   in this part, i will discuss two more very inter-related themes:
   [6]interpretability and [7]attention.

   throughout, [8]green links are ordinary hyperlinks, while [9]blue links
   lead to papers, and offer bibliographic information when you hover over
   them (not supported on mobile).

trend 3: interpretability

   i   ve been thinking about interpretability a lot recently, and i   m not
   alone     among deep learning practitioners, the dreaded    black box   
   quality of neural networks makes them notoriously hard to control, hard
   to debug and thus hard to develop. from a non-researcher perspective
   however, there is an even more important reason to desire
   interpretability: trust.
   a sinister black box hovering over a spike

   one of many unsettling images used to depict ai in the media. by keith
   rankin.

   the public, the media and some researchers are expressing [10]increased
   anxiety about whether ai can be trusted if it cannot be understood.
   while some of these anxieties are ill-founded (see the [11]   facebook
   chatbots invent their own language    story), others are very real. for
   example, if ai systems absorb [12]unwanted biases present in their
   training data, but we are unable to check the system for those biases,
   then we have a recipe for [13]disaster. second, as ai systems are
   imperfect and [14]sometimes fail, then we must be able to check how
   they make their decisions     especially for more complex tasks. third,
   even if ai systems operate perfectly, humans may always need the
   reassurance of explanation.

   even among researchers,    interpretability    can have many possible
   definitions     for an exploration of those definitions i highly
   recommend zachary lipton   s essay [15]the mythos of model
   interpretability. in particular, lipton identifies two broad approaches
   to interpretability: post-hoc explanations and transparency. post-hoc
   explanations take a learned model and draw some kind of useful insights
   from it; typically these insights provide only a partial or indirect
   explanation of how the model works. transparency asks more directly
      how does the model work?    and seeks to provide some way to understand
   the core mechanisms of the model itself. i think this is a useful
   distinction, so i   ll use it to explore the following acl work.

post-hoc explainability

   at acl, i saw many papers presenting a variety of creative methods to
   gain post-hoc insights into neural systems.

   visualization is probably the most common type of post-hoc
   interpretability, with particular types of visualizations     such as
   saliency maps and 2d projections of id27s     becoming
   standard. these visualizations are certainly useful (and i   m always
   grateful to see them in a paper), but they [16]can be misleading if
   read incorrectly. in [17]visualizing and understanding neural machine
   translation, ding et al. compute relevance scores that quantify how
   much a particular neuron or hidden state contributes to another neuron
   or hidden state. at first glance, the visualizations provided in the
   paper (which essentially give an importance score to each hidden state
   and its associated token) look very similar to the visualizations
   commonly produced from the attention distribution. however, the method
   of computation is very different. the relevance scores are a direct
   measure of how much one neuron affects a downstream neuron, calculated
   post-hoc on the trained model. by contrast the attention distribution
   is learned and computed by the network itself; it is an intermediate
   representation that affects the rest of the computation in complex
   ways. though attention often plays the role of word alignment in id4,
   [18]koehn and knowles note that it learns to play other,
   harder-to-understand roles too; thus it is not always as understandable
   as we might hope. ding et al.   s relevance scores provide a useful
   alternative way to measure word-level relevance in sequence-to-sequence
   models.
   a visualization of the attention distribution, in which it's one step
   shifted from expected word alignment

   koehn and knowles show attention is sometimes off-by-one from word
   alignment. how should we interpret this behavior?

   id21 is another popular post-hoc analysis technique, in
   which the representations learned for task a (typically a high-level
   task) are applied to task b (typically a lower-level task). the degree
   of success at task b indicates how much the task a model has learned
   about task b. at acl 2017, researchers asked what [19]id4 models know
   about morphology, what [20]language models know about ner and chunking,
   and what [21]speech+vision representations know about various semantic
   tasks. these studies, which are often carefully repeated with different
   layers and various configurations of the task a model, can yield useful
   and unexpected insights that guide the development of better models for
   task a. for example, in [22]what do id4 models
   learn about morphology? belinkov et al. find that while attention
   increases the quality of morphological information in the encoder
   representations, it decreases the quality for the decoder
   representations. i was surprised to read about this unintended
   side-effect of attention, and overall i really like how the paper
   thoughtfully and thoroughly addresses its research questions.

   though id21 and attention-like visualizations can tell you
      how much   , they do not tell you    what    or    why   . to answer the latter,
   some researchers directly study the geometry of the representation
   spaces themselves. in [23]emergent predication structure in hidden
   state vectors of neural readers, wang et al. provide evidence that in
   id56-based reading comprehension models, the hidden vector space can be
   decomposed into two orthogonal subspaces: one containing
   representations of entities, and the other containing representations
   of statements (or predicates) about those entities. though it is not a
   focus of the paper, i wonder whether these component parts of the
   hidden state could be further interpreted. in [24]parameter free
   hierarchical graph-based id91 for analyzing continuous word
   embeddings, trost and klakow perform id91 on id27s,
   then cluster those clusters, and so on to obtain a hierarchical
   tree-like structure. judging by the examples provided in the paper, the
   hierarchy could provide a more human-readable way to explore the
   neighborhood structure of id27s.
   a graph showing the hierarchy of words below 'dropped'

   trost and klakow

   another approach to direct post-hoc interpretation is to treat
   interpretation itself as a translation task. in [25]translating
   neuralese, andreas et al. take the vector messages (   neuralese   ) passed
   between two machines trained to perform a collaborative task, and
   translate them into natural language utterances. to overcome the
   absence of neuralese-to-english parallel data, andreas et al. consider
   a pair of messages equivalent if they are used in similar scenarios by
   human and machine agents. the authors raise important questions about
   whether these translations can be trusted. what happens if the
   neuralese messages encode concepts that are impossible to capture in
   english? if humans and machines have different biases about what they
   choose to communicate, how can we be sure that crowdsourced training
   data contains english examples that correspond to the neuralese? in any
   case, this was one of my favorite papers of the conference, and i   m
   excited to see where this research goes next.
   english translations of messages passed in various scenarios

   andreas et al.

transparency

   despite all the work gleaning post-hoc insight from uninterpretable
   neural models, [26]some researchers argue that (notwithstanding finding
   the odd [27]sentiment neuron) staring into the neurons will only get us
   so far. true interpretability requires transparency     models that are
   constructed and trained to be interpretable in themselves.

   linguistically-structured representations are by definition more
   interpretable than unstructured ones     thus [28]trend 1 (structure is
   back) could also be viewed as a move towards more transparent neural
   models. however, a core challenge with these, and other attempts to
   create transparent neural models is the tension between discreteness
   and continuousness. neural networks are powerful because they can learn
   arbitrary continuous representations, but humans find discrete
   information     like language itself     easier to understand.

   we might be concerned that imposing discreteness constraints on our
   neural models diminishes their expressive power     that interpretability
   comes at the price of effectiveness. however, for some types of
   discreteness, like sparsity, the opposite can be true. for example,
   sparsity-inducing id173 is known to improve rather than impair
   neural models, and sparsified id27s can be more effective
   than the original dense ones. in [29]sparse coding of neural word
   embeddings for multilingual sequence labeling, g  bor berend
   demonstrates the effectiveness of sparse id27s for ner and
   pos-tagging, particularly in low-training-data settings. though
   interpretability is not the focus of berend   s paper, he kindly answered
   my questions on the subject and even wrote a follow-up [30]blog post,
   which shows that some of the basis vectors in the sparse representation
   seem to correspond to human-understandable concepts. this is very cool,
   and raises the question: if we have high-performing id27s
   with interpretable dimensions, can we use them to build more complex
   neural systems that are also interpretable?
   closest concepts for several basis vectors.

   g  bor berend

   for ai systems that compute answers to complicated questions,
   transparency is especially important if humans are to trust the
   answers. these systems should ideally produce a proof or derivation of
   the answer     for a id29 id53 system, this
   might be the semantic parse itself, or a relevant excerpt from the
   knowledge base. for a system that solves mathematical problems, the
   proof should be a step-by-step natural language derivation of the final
   answer. this is exactly what ling et al. provide in [31]program
   induction for rationale generation: learning to solve and explain
   algebraic word problems. rather than directly and uninterpretably
   producing the final answer, their system jointly learns to generate the
   underlying sequence of mathematical transformations, and the natural
   language solution that explains it.
   rationale and corresponding chain of mathematical computations to solve
   a mathematical word problem

   ling et al.

looking forward

   i   m unsure which type of interpretability     post hoc explainability or
   transparency     is the right way forward. post hoc techniques tend to
   give restricted explanations that, while fascinating, but are often
   cryptic themselves. i think that more flexible explanation techniques,
   like the translation-based approach, hold a lot of potential     though
   they raise tough new questions about trust. transparency, on the other
   hand, is attractive because interpretability should really be a design
   choice, not an afterthought. though we are far from building neural
   systems that are transparent end-to-end, efforts to make small parts of
   the system transparent are hugely useful     note, for example, how
   useful the attention mechanism has been as a sanity check and debugging
   tool for developing attentional systems. which leads us to trend 4   

trend 4: attention

   widely acknowledged as a game-changer for the sequence-to-sequence
   model, the [32]attention mechanism is quickly becoming a favorite
   technique, and it   s easy to see why. it can be used to bypass
   bottlenecks in information flow, it enables a key-value lookup function
   that can   t be achieved with feed-forward layers, and it provides some
   much-needed interpretability. the attention mechanism had an increased
   presence at acl this year, with fifteen occurrences of    attention    in
   paper titles (an increase from nine the previous year).

more attention everywhere

   the attention mechanism is the most interpretable and therefore most
   manipulable part of the sequence-to-sequence framework. accordingly,
   researchers are finding success by designing increasingly complex
   id12 that aim to solve particular task-specific problems.
   this    mini-industry of model extensions    (as described by alexander
   rush in his [33]id4 workshop keynote) was thriving at acl 2017.

   three papers presented models for id53 that, in addition
   to the usual question-to-document attention, add document-to-question
   attention. of these models ([34]attention-over-attention,
   [35]cross-attention and [36]gated-attention reader), the third also
   incorporates multi-hop attention, which allows the model to iteratively
   attend to different sections before coming to an answer. this seems
   like a core ability, and the appendix of the paper contains several
   examples that demonstrate both the necessity and the effectiveness of
   multi-hop reasoning.
   attention visualizations for each iteration of computation

   over multiple iterations, attention settles on the correct answer.
   dhingra et al.

   attention has also emerged as the standard way to weight and combine
   information from multiple, potentially multi-modal, sources.
   [37]libovicky et al. attend to both text and an image to translate a
   caption, [38]lin et al. attend to multi-lingual data to extract
   relations, and [39]kim et al. attend to the representations from an
   ensemble of domain experts to perform id20 on a
   case-by-case basis. attention is convenient in these cases because it
   offers a general way to obtain a fixed-size representation from an
   arbitrary number of sources.

   others find that applying attention at multiple granularities is useful
   for certain tasks. for example, grammatical error correction requires
   [40]nested attention: word-level attention to detect word order errors,
   and character-level attention to detect spelling errors.

so is attention all you need?

   the enthusiasm for increasingly complex, attention^attention mechanisms
   may seem to confirm recent bold claims that [41]attention is all you
   need. however, at acl i noticed several researchers deliver cautionary
   messages about the potential pitfalls or misapplications of attention.

   for example, there are some scenarios in which attention doesn   t work
   as well as we might hope. [42]tan et al. argue that for abstractive
   document summarization, the attention distribution does not effectively
   model the saliency of source sentences. instead, they find more success
   by using a pre-deep-learning extractive summarization algorithm
   (id95-based sentence ranking) to model saliency. this result serves
   as an important reminder that we should not throw away decades of
   accumulated nlp knowledge     though unfashionable, these techniques may
   provide the key to improving our neural systems.

   second, there may be some scenarios in which attention is redundant.
   [43]bollman et al. find that when they introduce an auxiliary task for
   id72, the addition of an attention mechanism becomes
   harmful rather than helpful. as explanation, they provide evidence that
   the auxiliary task enables the model to learn to focus attention, which
   makes the attention mechanism redundant. though i don   t fully
   understand this interaction between attention and id72,
   we should take note of the phenomenon as it poses a potential pitfall
   for the development of future systems.

   lastly, there are some simpler tasks for which attention may be more
   than you need. [44]aharoni et al. argue that for morphological
   inflection generation, which typically requires focusing on just one
   character at a time, the standard    soft    attention is overkill     they
   find the simpler    hard    attention sufficient.
   2d visualization of embeddings produced by hard and soft attention.

   hard attention produces more clearly-defined clusters than soft
   attention. aharoni et al.

looking forward

   although attention was originally [45]conceived as a fix to the
   bottleneck problem in sequence-to-sequence id4, it has turned out to be
   a much more fundamental and generally useful technique. by thinking
   about why attention is so popular, we might identify some of the
   current needs of the deep learning community     for example the need for
   interpretability, for long distance dependencies, and for dynamic
   structure. i hope that attention is just the first step towards
   achieving these things.

conclusion

   though this is only the second acl i   ve attended, i was really
   impressed by this year   s [46]organizing committee, who did an amazing
   job of being transparent, listening to the community   s concerns, and
   proactively addressing them. this year   s conference featured an
   extensive [47]open discussion of preprint servers and the submission
   process, as well as a strong emphasis on diversity, accessibility and
   ethics. in this era of very empirical-driven work, i was also glad to
   see multiple reminders that we should aspire to do [48]good,
   hypothesis-driven science that is [49]replicable and reproducible.

   just a few years after the deep learning [50]tidal wave, the nlp
   community has reason to feel both excited and anxious     excited about
   where deep learning will lead next, and anxious about whether that   s
   the right direction. but i have confidence in this community to get the
   best out of both deep learning and nlp; to change with the times while
   retaining its collective wisdom. so, no need for hype nor fear. deep
   learning is neither the ultimate solution nor the death of nlp.
     __________________________________________________________________

   thanks to my advisor chris manning who read several drafts of this
   post, as well as the following people who gave me their opinions and/or
   answered my questions about their work: david jurgens, lucy li, g  bor
   berend, yang liu, jiwei tan.

   what themes and trends did you observe at acl 2017? discuss in the
   comments.

   [51]four deep learning trends from acl 2017 part one: linguistic
   structure and id27s prev [52]deep learning, structure and
   innate priors a discussion between yann lecun and christopher manning
   next

   please enable javascript to view the [53]comments powered by disqus.

   powered by [54]jekyll with [55]type theme

references

   1. http://abigailsee.com/feed.xml
   2. http://www.abigailsee.com/
   3. http://www.abigailsee.com/about/
   4. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html
   5. http://acl2017.org/
   6. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html#interpretability
   7. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html#attention
   8. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html#!
   9. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html#!
  10. https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/
  11. http://gizmodo.com/no-facebook-did-not-panic-and-shut-down-an-ai-program-1797414922
  12. http://www.wired.co.uk/article/machine-learning-bias-prejudice
  13. https://qz.com/653084/microsofts-disastrous-tay-experiment-shows-the-hidden-dangers-of-ai/
  14. http://www.techrepublic.com/article/top-10-ai-failures-of-2016/
  15. https://arxiv.org/pdf/1606.03490.pdf
  16. https://distill.pub/2016/misread-tsne/
  17. http://www.aclweb.org/anthology/p/p17/p17-1106.pdf
  18. http://aclweb.org/anthology/w/w17/w17-3204.pdf
  19. http://www.aclweb.org/anthology/p/p17/p17-1080.pdf
  20. http://www.aclweb.org/anthology/p/p17/p17-1161.pdf
  21. http://www.aclweb.org/anthology/p/p17/p17-1057.pdf
  22. http://www.aclweb.org/anthology/p/p17/p17-1080.pdf
  23. http://aclweb.org/anthology/w/w17/w17-2604.pdf
  24. http://aclweb.org/anthology/w/w17/w17-2404.pdf
  25. http://www.aclweb.org/anthology/p/p17/p17-1022.pdf
  26. https://twitter.com/tallinzen/status/893261967074369538
  27. https://blog.openai.com/unsupervised-sentiment-neuron/
  28. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html#structure
  29. https://www.transacl.org/ojs/index.php/tacl/article/view/1063/241
  30. https://begab.github.io/interpretability-of-sparse-reps
  31. http://www.aclweb.org/anthology/p/p17/p17-1015.pdf
  32. https://distill.pub/2016/augmented-id56s/
  33. http://nlp.seas.harvard.edu/slides/id417.pdf
  34. http://www.aclweb.org/anthology/p/p17/p17-1055.pdf
  35. http://www.aclweb.org/anthology/p/p17/p17-1021.pdf
  36. http://www.aclweb.org/anthology/p/p17/p17-1168.pdf
  37. http://www.aclweb.org/anthology/p/p17/p17-2031.pdf
  38. http://www.aclweb.org/anthology/p/p17/p17-1004.pdf
  39. http://www.aclweb.org/anthology/p/p17/p17-1060.pdf
  40. http://www.aclweb.org/anthology/p/p17/p17-1070.pdf
  41. https://arxiv.org/pdf/1706.03762.pdf
  42. http://www.aclweb.org/anthology/p/p17/p17-1108.pdf
  43. http://www.aclweb.org/anthology/p/p17/p17-1031.pdf
  44. http://www.aclweb.org/anthology/p/p17/p17-1183.pdf
  45. https://arxiv.org/pdf/1409.0473.pdf
  46. http://acl2017.org/organization
  47. https://chairs-blog.acl2017.org/2017/05/31/official-acl-survey-preprint-publishing-and-reviewing-make-your-voice-heard/
  48. https://www.slideshare.net/aclanthology/joakim-nivre-2017-presidential-address-acl-2017-challenges-for-acl/68?src=clipshare
  49. https://www.slideshare.net/aclanthology/joakim-nivre-2017-presidential-address-acl-2017-challenges-for-acl/75?src=clipshare
  50. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html/#intro
  51. http://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html
  52. http://www.abigailsee.com/2018/02/21/deep-learning-structure-and-innate-priors.html
  53. https://disqus.com/?ref_noscript
  54. http://jekyllrb.com/
  55. https://rohanchandra.github.io/project/type/
