   #[1]aylien    your first id111 project with python in 3 steps
   comments feed [2]alternate [3]alternate

   [4]products [5]research [6]blog [7]company [8]contact

   [9]blog

   [10]general [11]product [12]data science [13]research [14]get started

   from the blog
   product
   product updates and how-to guides for our text analysis api, news api,
   and text analysis platform
   data science
   using text and image analysis for fun and insightful data science
   projects
   research
   the latest updates from our research team.
   general
   company updates & news
   from the aylien.com

   [15]products [16]research [17]blog [18]company [19]contact

   [20]will gannon
   28 jul, 2017
   data science
   your first id111 project with python in 3 steps

your first id111 project with python in 3 steps

   every day, we generate huge amounts of text online, creating vast
   quantities of data about what is happening in the world and what people
   think. all of this text data is an invaluable resource that can be
   mined in order to generate meaningful business insights for analysts
   and organizations.

   but analyzing all of this content isn   t easy, since converting text
   produced by people into structured information to analyze with a
   machine is a complex task. in recent years though, natural language
   processing and id111 has become a lot more accessible for data
   scientists, analysts, and developers alike.

   there is a massive amount of resources, code libraries, services, and
   apis out there which can all help you embark on your first nlp project.
   for this how-to post, we thought we   d put together a three-step,
   end-to-end guide to your first introductory nlp project. we   ll start
   from scratch by showing you how to build a corpus of language data and
   how to analyze this text, and then we   ll finish by visualizing the
   results.

   we   ve split this post into 3 steps. each of these steps will do two
   things: show a core task that will get you familiar with nlp basics,
   and also introduce you to some common apis and code libraries for each
   of the tasks. the tasks we   ve selected are:
    1.
         1. building a corpus     using tweepy to gather sample text data
            from twitter   s api.
         2. analyzing text     analyzing the sentiment of a piece of text
            with our own sdk.
         3. visualizing results     how to use pandas and matplotlib to see
            the results of your work.


   pythonlogo

step 1. build a corpus

   you can build your corpus from anywhere     maybe you have a large
   collection of emails you want to analyze, a collection of customer
   feedback in nps surveys that you want to dive into, or maybe you want
   to focus on the voice of your customers online. there are lots of
   options open to you, but for the purpose of this post we   re going to
   use twitter as our focus for building a corpus. twitter is a very
   useful source of textual content: it   s easily accessible, it   s public,
   and it offers an insight into a huge volume of text that contains
   public opinion.

   accessing the twitter search api using python is pretty easy. there are
   lots of libraries available, but our favourite option is [21]tweepy. in
   this step, we   re going to use tweepy to ask the twitter api for 500 of
   the most recent tweets that contain our search term, and then we   ll
   write the tweets to a text file, with each tweet on its own line. this
   will make it easy for us to analyze each tweet separately in the next
   step.

   you can install tweepy using pip:
pip install tweepy

   once completed, open a python shell to double-check that it   s been
   installed correctly:
>>> import tweepy

   first, we need to get permission from twitter to gather tweets from the
   [22]search api, so you need to [23]sign up as a developer to get your
   consumer keys and access tokens, which should take you three or four
   minutes. next, you need to build your search query by adding your
   search term to the q =        field. you will also need to add some further
   parameters like the language, the amount of results you want returned,
   and the time period to search in. you can get very specific about what
   you want to search for on twitter; to make a more complicated query,
   take a look at the list of operators you can use the api to search with
   in the [24]search api introduction.

   fill your credentials and your query into this script:
import tweepy, codecs

## fill in your twitter credentials
consumer_key =    your consumer key here   
consumer_secret =    your consumer secret key here   
access_token =    your access token here   
access_token_secret =    your access token secret here   

## let tweepy set up an instance of the rest api
auth = tweepy.oauthhandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.api(auth)

## fill in your search query and store your results in a variable
results = api.search(q = "your search term here", lang = "en", result_type = "re
cent", count = 1000)

## use the codecs library to write the text of the tweets to a .txt file
file = codecs.open("your text file name here.txt", "w", "utf-8")
for result in results:
        file.write(result.text)
        file.write("\n")
file.close()

   you can see in the script that we are writing result.text to a .txt
   file and not simply the result, which is what the api is returning to
   us. apis that return language data from social media or online
   journalism sites usually return lots of metadata along with your
   results. to do this, they format their output in json, which is easy
   for machines to read.

   for example, in the script above, every    result    is its own json
   object, with    text    being just one field     the one that contains the
   tweet text. other fields in the json file contain metadata like the
   location or timestamp of the tweet, which you can extract for a more
   detailed analysis.

   to access the rest of the metadata, we   d need to write to a json file,
   but for this project we   re just going to analyze the text of people   s
   tweets. so in this case, a .txt file is fine, and our script will just
   forget the rest of the metadata once it finishes. if you want to take a
   look at the full json results, print everything the api returns to you
   instead:

   this is also why we used [25]codecs module, to avoid any formatting
   issues when the script reads the json results and writes utf-8 text.

step 2. analyze sentiment

   so once we   ve collected the text of the tweets that you want to
   analyze, we can use more advanced nlp tools to start extracting
   information from it. id31 is a great example of this,
   since it tells us whether people were expressing positive, negative, or
   neutral sentiment in the text that we have.

   for id31, we   re going to use our own aylien text api.
   just like with the twitter search api, you   ll need to [26]sign up for
   the free plan to grab your api key (don   t worry     free means free
   permanently. there   s no credit card required, and we don   t harass you
   with promotional stuff!). this plan gives you 1,000 calls to the api
   per month free of charge.

   again, you can install using pip:
pip install aylien-apiclient

   then make sure the sdk has installed correctly from your python shell:
>>>from aylienapiclient import textapi

   once you   ve got your app key and application id, insert them into the
   code below to get started with your first call to the api from the
   python shell (we also have extensive documentation in [27]7 popular
   languages). our api lets you make your first call to the api with just
   four lines of code:
>>>from aylienapiclient import textapi
>>>client = textapi.client("your_app_id", "your_application_key")
>>>sentiment = client.sentiment({'text': 'enter some of your own text here'})
>>>print(sentiment)

   this will return json results to you with metadata, just like our
   results from the twitter api.

   so now we need to analyze our corpus from step 1. to do this, we need
   to analyze every tweet separately. the script below uses the [28]io
   module to open up a new .csv file and write the column headers    tweet   
   and    sentiment   , and then it opens and reads the .txt file containing
   our tweets. then, for each tweet in the .txt file it sends the text to
   the aylien api, extracts the sentiment prediction from the json that
   the aylien api returns, and writes this to the .csv file beside the
   tweet itself.

   this will give us a .csv file with two columns     the text of a tweet
   and the sentiment of the tweet, as predicted by the aylien api. we can
   look through this file to verify the results, and also visualize our
   results to see some metrics on how people felt about whatever our
   search query was.
from aylienapiclient import textapi
import csv, io

## initialize a new client of aylien text api
client = textapi.client("your_app_id", "your_app_key")

with io.open('trump_tweets.csv', 'w', encoding='utf8', newline='') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(["tweet", "sentiment"])
        with io.open("trump.txt", 'r', encoding='utf8') as f:
            for tweet in f.readlines():
                ## remove extra spaces or newlines around the text
                tweet = tweet.strip()

                ## reject tweets which are empty so you don   t waste your api cre
dits
                if len(tweet) == 0:
                        print('skipped')
                        continue

                print(tweet)

                ## make call to aylien text api
                sentiment = client.sentiment({'text': tweet})

                ## write the sentiment result into csv file
                csv_writer.writerow([sentiment['text'], sentiment['polarity']])

   you might notice on the final line of the script that when the script
   goes to write the tweet text to the file, we   re actually writing the
   tweet as it is returned by the aylien api, rather than the tweet from
   the .txt file. they are both identical pieces of text, but we   ve chosen
   to write the text from the api just to make sure we   re reading the
   exact text that the api analyzed. this is just to make it clearer if
   we   ve made an error somehow.

step 3. visualize your results

   so far we   ve used an api to gather text from twitter, and used our text
   analysis api to analyze whether people were speaking positively or
   negatively in their tweet. at this point, you have a couple of options
   with what you do with the results. you can feed this structured
   information about sentiment into whatever solution you   re building,
   which could be anything from a simple social listening app or a even an
   automated report on the public reaction to a campaign. you could also
   use the data to build informative visualizations, which is what we   ll
   do in this final step.

   for this step, we   re going to use [29]matplotlib to visualize our data
   and [30]pandas to read the .csv file, two python libraries that are
   easy to get up and running. you   ll be able to create a visualization
   from the command line or save it as a .png file.

   install both using pip:
pip install matplotlib
pip install pandas

   the script below opens up our .csv file, and then uses pandas to read
   the column titled    sentiment   . it uses counter to count how many times
   each sentiment appears, and then matplotlib plots counter   s results to
   a color-coded pie chart (you   ll need to enter your search query to the
      yourtext    variable for presentation reasons).
## import the libraries
import matplotlib.pyplot as plt
import pandas as pd
from collections import counter
import csv

## open up your csv file with the sentiment results
with open('your_csv_file_from_step_2.csv', 'r', encoding = 'utf8') as csvfile:
        ## use pandas to read the    sentiment    column,
        df = pd.read_csv(csvfile)
        sent = df["sentiment"]

## use counter to count how many times each sentiment appears
## and save each as a variable
        counter = counter(sent)
        positive = counter['positive']
        negative = counter['negative']
        neutral = counter['neutral']

## declare the variables for the pie chart, using the counter variables for    siz
es   
labels = 'positive', 'negative', 'neutral'
sizes = [positive, negative, neutral]
colors = ['green', 'red', 'grey']
yourtext = "your search query from step 2"

## use matplotlib to plot the chart
plt.pie(sizes, labels = labels, colors = colors, shadow = true, startangle = 90)
plt.title("sentiment of 200 tweets about "+yourtext)
plt.show()

   if you want to save your chart to a .png file instead of just showing
   it, replace plt.show on the last line with savefig(   your chart
   name.png   ). below is the visualization we ended up with (we searched
      trump    in step 1).

   screenshot (261)

   if you run into any issues with these scripts, big or small, please
   leave a comment below and we   ll look into it. we always try to
   anticipate any problems our own users might run into, so be sure to let
   us know!

   that concludes our introductory id111 project with python. we
   hope it gets you up and running with the libraries and apis, and that
   it gives you some ideas about subjects that would interest you. with
   the world producing content on such a large scale, the only obstacle
   holding you back from an interesting project is your own imagination!

   happy coding!

                       [31]text analysis api - sign up

   [32]will gannon
   marketing @ aylien
   a classics graduate from ucd, will handles inbound marketing here at
   aylien. before joining us, will completed a master   s in digital
   humanities at trinity college, where he used nlp methods to index where
   latin terms appear in english literature.
   read next
   [33]sebastian ruder
   [34]learning to select data for id21
   3 aug, 2017
   related articles
   how far can machines go in understanding content?
   [35]mike waldron
   8 apr, 2015
   [36]how far can machines go in understanding content?

   natural language processing, artificial intelligence and machine
   learning are changing how content is discovered, analyzed and shared
   online. more recently, there has been a push to harness the power of
   [   ]
   [37]read    
   [38]will gannon
   21 jun, 2017
   [39]understanding customer frustrations in the airline industry with
   aspect-based id31

   every day, over 100,000 flights carry passengers to and from
   destinations all around the world, and it   s safe to say air travel
   brings out a fairly mixed bag of emotions [   ]
   [40]read    
   [41]sebastian ruder
   3 aug, 2017
   [42]learning to select data for id21

   in machine learning, the traditional assumption is that the data our
   model is applied to is the same as the data we used for training. this
   assumption is proven false [   ]
   [43]read    
   on artificial intellience: branches, applications and challenges
   [44]parsa ghaffari
   20 apr, 2015
   [45]on artificial intellience: branches, applications and challenges

   this blog is an adaptation of a talk,    computer intelligence   ,
   delivered by our founder parsa ghaffari (@parsaghaffari) and trinity
   college, dublin, research fellow and founder of wripl, kevin koidl
   (ph.d., [   ]
   [46]read    

   let's talk

   products

   [47]text analysis api [48]news api [49]text analysis platform
   company

   [50]about us [51]in the news [52]jobs
   content

   [53]resources [54]blog
   get in touch

   [55]contact sales [56]press [57]email us
   stay informed

   [58]privacy policy [59]terms and conditions

   copyright    2018 aylien ltd. all rights reserved.

references

   visible links
   1. http://blog.aylien.com/first-text-mining-project-python-3-steps/feed/
   2. http://blog.aylien.com/wp-json/oembed/1.0/embed?url=http://blog.aylien.com/first-text-mining-project-python-3-steps/
   3. http://blog.aylien.com/wp-json/oembed/1.0/embed?url=http://blog.aylien.com/first-text-mining-project-python-3-steps/&format=xml
   4. https://aylien.com/products
   5. https://aylien.com/research
   6. http://blog.aylien.com/
   7. https://aylien.com/about
   8. https://aylien.com/contact
   9. http://blog.aylien.com/
  10. http://blog.aylien.com/category/general/
  11. http://blog.aylien.com/category/product/
  12. http://blog.aylien.com/category/data-science/
  13. http://blog.aylien.com/category/research/
  14. https://aylien.com/products/
  15. https://aylien.com/products
  16. https://aylien.com/research
  17. http://blog.aylien.com/
  18. https://aylien.com/about
  19. https://aylien.com/contact
  20. http://blog.aylien.com/author/will/
  21. http://www.tweepy.org/
  22. https://dev.twitter.com/rest/public/search
  23. https://dev.twitter.com/resources/signup
  24. https://dev.twitter.com/rest/public/search
  25. https://docs.python.org/2/library/codecs.html
  26. https://developer.aylien.com/signup
  27. http://docs.aylien.com/textapi/sdks/#sdks
  28. https://docs.python.org/2/library/io.html
  29. https://matplotlib.org/
  30. http://pandas.pydata.org/
  31. http://cta-redirect.hubspot.com/cta/redirect/1942801/02bc9816-b470-4328-9625-fad8d92a8811
  32. http://blog.aylien.com/author/will/
  33. http://blog.aylien.com/author/sebastian/
  34. http://blog.aylien.com/learning-select-data-transfer-learning/
  35. http://blog.aylien.com/author/mike/
  36. http://blog.aylien.com/how-far-can-machines-go-in-understanding-content/
  37. http://blog.aylien.com/how-far-can-machines-go-in-understanding-content/
  38. http://blog.aylien.com/author/will/
  39. http://blog.aylien.com/understanding-customer-frustrations-in-the-airline-industry-with-aspect-based-sentiment-analysis/
  40. http://blog.aylien.com/understanding-customer-frustrations-in-the-airline-industry-with-aspect-based-sentiment-analysis/
  41. http://blog.aylien.com/author/sebastian/
  42. http://blog.aylien.com/learning-select-data-transfer-learning/
  43. http://blog.aylien.com/learning-select-data-transfer-learning/
  44. http://blog.aylien.com/author/parsa/
  45. http://blog.aylien.com/on-artificial-intellience-branches-applications/
  46. http://blog.aylien.com/on-artificial-intellience-branches-applications/
  47. https://aylien.com/text-api/
  48. https://aylien.com/news-api/
  49. https://aylien.com/text-analysis-platform/
  50. https://aylien.com/about/
  51. https://aylien.com/in-the-news/
  52. https://aylien.com/jobs/
  53. https://aylien.com/resources/
  54. http://blog.aylien.com/
  55. https://aylien.com/contact/
  56. https://aylien.com/in-the-news/
  57. https://aylien.com/contact/
  58. https://aylien.com/privacy/
  59. https://aylien.com/terms/

   hidden links:
  61. https://aylien.com/
  62. http://blog.aylien.com/?s=
  63. http://blog.aylien.com/?s=
  64. http://blog.aylien.com/category/product/
  65. http://blog.aylien.com/category/data-science/
  66. http://blog.aylien.com/category/research/
  67. http://blog.aylien.com/category/general/
  68. http://blog.aylien.com/first-text-mining-project-python-3-steps/will@aylien.com
  69. https://twitter.com/_aylien
  70. https://www.linkedin.com/company-beta/1195911/
  71. https://www.facebook.com/aylieninc
