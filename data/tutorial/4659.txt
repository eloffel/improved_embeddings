   #[1]github [2]recent commits to
   structured-self-attentive-sentence-embedding:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]10
     * [35]star [36]336
     * [37]fork [38]71

[39]explorerfreda/[40]structured-self-attentive-sentence-embedding

   [41]code [42]issues 5 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   an open-source implementation of the paper ``a structured
   self-attentive sentence embedding'' (lin et al., 2017).
     * [47]14 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors
     * [51]gpl-3.0

    1. [52]python 100.0%

   (button) python
   branch: master (button) new pull request
   [53]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/e
   [54]download zip

downloading...

   want to be notified of new releases in
   explorerfreda/structured-self-attentive-sentence-embedding?
   [55]sign in [56]sign up

launching github desktop...

   if nothing happens, [57]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [58]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [59]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [60]download the github extension for visual studio
   and try again.

   (button) go back
   [61]@explorerfreda
   [62]explorerfreda [63]merge pull request [64]#2 [65]from
   cclauss/patch-2 (button)    
from models import *

   latest commit [66]8e0ace5 jul 28, 2017
   [67]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [68].gitignore [69]add .gitignore jul 24, 2017
   [70]license [71]initial commit jul 24, 2017
   [72]readme.md
   [73]models.py [74]from __future__ import print_function for python 3
   jul 27, 2017
   [75]tokenizer-yelp.py
   [76]train.py
   [77]util.py [78]add model on yelp dataset jul 25, 2017

readme.md

structured-self-attentive-sentence-embedding

   an open-source implementation of the paper ``a structured
   self-attentive sentence embedding'' published by ibm and mila.
   [79]https://arxiv.org/abs/1703.03130

requirments

   pytorch: [80]http://pytorch.org/

   spacy: [81]https://spacy.io/

   please refer to
   [82]https://github.com/pytorch/examples/tree/master/snli for the
   information about obtaining glove model (in pytorch model format .pt).
   typically, the model should be a tuple (dict, torch.floattensor, int),
   where the first element (dict) is a mapping from word to its index, the
   third element (int) is the dimension of the id27s, and the
   second element (torch.floattensor) with the size of word_count * dim
   refers to the id27s.

usage

   id121
python tokenizer-yelp.py --input [yelp dataset] --output [output path, will be a
 json file] --dict [output dictionary path, will be a json file]

   training model
python train.py \
--emsize [id27 size default 300] \
--nhid [hidden layer size, default 300] \
--nlayers [hidden layer numbers in bi-lstm, default 2] \
--attention-unit [attention unit number, d_a in the paper, default 350] \
--attention-hops [hop number, r in the paper, default 1] \
--dropout [dropout ratio, default 0.5] \
--nfc [hidden layer size for mlp in the classifier, default 512] \
--lr [learning rate, default 0.001] \
--epochs [epoch number for training, default 40] \
--seed [initial seed for reproduction, default 1111] \
--log-interval [the interval for reporting training loss, default 200] \
--batch-size [size of a batch in training procedure, default 32] \
--optimizer [type of the optimizer, default adam] \
--penalization-coeff [coefficient of the frobenius norm penalization term, defau
lt 1.0] \
--class-number [number of class for the last step of classification] \
--save [path to save model] \
--dictionary [location of the dictionary generated by the tokenizer] \
--word-vector [location of the initial word vector, e.g. glove, should be a torc
h .pt model] \
--train-data [location of training data, should be in the same format with token
ized productions] \
--val-data [development set] \
--test-data [location of testing dataset] \
--cuda [whether using gpu for training, remove this when using cpu]

differences between the paper and our implementation

    1. for faster python based id121, we used spacy instead of
       stanford tokenizer
       ([83]https://nlp.stanford.edu/software/tokenizer.shtml)
    2. for faster performance, we manually crop the comments in yelp to a
       max length of 500.

example experimental command and result

   we followed lin et al.(2017) to generate the dataset, and obtained the
   following result:
python train.py --train-data "data/train.json" --val-data "data/dev.json" --test
-data "data/test.json" --cuda --emsize 300 --nhid 300 --nfc 300 --dropout 0.5 --
attention-unit 350 --epochs 10 --lr 0.001 --clip 0.5 --dictionary "data/yelp/dat
a/dict.json" --word-vector "data/glove/glove.42b.300d.pt" --save "models/model-m
edium.pt" --batch-size 50 --class-number 5 --optimizer adam --attention-hops 4 -
-penalization-coeff 1.0 --log-interval 100
# test loss (cross id178 loss, without the frobenius norm penalization) 0.7544
# test accuracy: 0.6690

     *    2019 github, inc.
     * [84]terms
     * [85]privacy
     * [86]security
     * [87]status
     * [88]help

     * [89]contact github
     * [90]pricing
     * [91]api
     * [92]training
     * [93]blog
     * [94]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [95]reload to refresh your
   session. you signed out in another tab or window. [96]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commits/master.atom
   3. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/explorerfreda/structured-self-attentive-sentence-embedding
  32. https://github.com/join
  33. https://github.com/login?return_to=/explorerfreda/structured-self-attentive-sentence-embedding
  34. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/watchers
  35. https://github.com/login?return_to=/explorerfreda/structured-self-attentive-sentence-embedding
  36. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/stargazers
  37. https://github.com/login?return_to=/explorerfreda/structured-self-attentive-sentence-embedding
  38. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/network/members
  39. https://github.com/explorerfreda
  40. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding
  41. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding
  42. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/issues
  43. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/pulls
  44. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/projects
  45. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commits/master
  48. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/branches
  49. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/releases
  50. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/graphs/contributors
  51. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/license
  52. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/search?l=python
  53. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/find/master
  54. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/archive/master.zip
  55. https://github.com/login?return_to=https://github.com/explorerfreda/structured-self-attentive-sentence-embedding
  56. https://github.com/join?return_to=/explorerfreda/structured-self-attentive-sentence-embedding
  57. https://desktop.github.com/
  58. https://desktop.github.com/
  59. https://developer.apple.com/xcode/
  60. https://visualstudio.github.com/
  61. https://github.com/explorerfreda
  62. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commits?author=explorerfreda
  63. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commit/8e0ace59ed6538d4f33d4a2af3222545c759d430
  64. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/pull/2
  65. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commit/8e0ace59ed6538d4f33d4a2af3222545c759d430
  66. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commit/8e0ace59ed6538d4f33d4a2af3222545c759d430
  67. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/tree/8e0ace59ed6538d4f33d4a2af3222545c759d430
  68. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/.gitignore
  69. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commit/3cad7a419ab59415d54b4ac6be62dfbb25fa8892
  70. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/license
  71. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commit/1cddadf69c8c9795e18b20d57f62e013caa4dda9
  72. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/readme.md
  73. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/models.py
  74. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commit/3a895205fe06d4a5ff3ae6802ca9058b2df44bda
  75. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/tokenizer-yelp.py
  76. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/train.py
  77. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/blob/master/util.py
  78. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding/commit/1a8d66161b73f0f6906c0b599065a8d98680cf13
  79. https://arxiv.org/abs/1703.03130
  80. http://pytorch.org/
  81. https://spacy.io/
  82. https://github.com/pytorch/examples/tree/master/snli
  83. https://nlp.stanford.edu/software/tokenizer.shtml
  84. https://github.com/site/terms
  85. https://github.com/site/privacy
  86. https://github.com/security
  87. https://githubstatus.com/
  88. https://help.github.com/
  89. https://github.com/contact
  90. https://github.com/pricing
  91. https://developer.github.com/
  92. https://training.github.com/
  93. https://github.blog/
  94. https://github.com/about
  95. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding
  96. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding

   hidden links:
  98. https://github.com/
  99. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding
 100. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding
 101. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding
 102. https://help.github.com/articles/which-remote-url-should-i-use
 103. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding#structured-self-attentive-sentence-embedding
 104. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding#requirments
 105. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding#usage
 106. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding#differences-between-the-paper-and-our-implementation
 107. https://github.com/explorerfreda/structured-self-attentive-sentence-embedding#example-experimental-command-and-result
 108. https://github.com/
