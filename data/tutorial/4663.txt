   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    twitter id31 with gensim
   id97 and keras convolutional networks comments feed [4]alternate
   [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

twitter id31 with gensim id97 and keras convolutional
networks

   [25]08/07/201709/30/2017[26]convnet, [27]deep learning, [28]generic,
   [29]keras, [30]neural networks, [31]nlp, [32]python,
   [33]tensorflow[34]64 comments

   [35]fork

   id97 ([36]https://code.google.com/archive/p/id97/) offers a
   very interesting alternative to classical nlp based on term-frequency
   matrices. in particular, as each word is embedded into a
   high-dimensional vector, it   s possible to consider a sentence like a
   sequence of points that determine an implicit geometry. for this
   reason, the idea of considering 1d convolutional classifiers (usually
   very efficient with images) became a concrete possibility.

   as you know, a convolutional network trains its kernels so to be able
   to capture, initially coarse-grained features (like the orientation),
   and while the kernel-size decreases, more and more detailed elements
   (like eyes, wheels, hands and so forth). in the same way, a 1d
   convolution works on 1-dimensional vectors (in general they are
   temporal sequences), extracting pseudo-geometric features.

   the rationale is provided by the id97 algorithm: as the vectors are
      grouped    according to a semantic criterion so that two similar words
   have very close representations, a sequence can be considered as a
   piecewise function, whose    shape    has a strong relationship with the
   semantic components. in the previous image, two sentences are
   considered as vectorial sums:
     * v1:    good experience   
     * v2:    bad experience   

   as it   s possible to see, the resulting vectors have different
   directions, because the words    good    and    bad    have opposite
   representations. this condition allows    geometrical    language
   manipulations that are quite similar to what happens in an image
   convolutional network, allowing to achieve results that can outperform
   standard bag-of-words methods (like tf-idf).

   to test this approach, i   ve used the twitter id31 dataset
   ([37]http://thinknook.com/wp-content/uploads/2012/09/sentiment-analysis
   -dataset.zip) which is made of about 1.400.000 labeled tweets. the
   dataset is quite noisy and the overall validation accuracy of many
   standard algorithms is always about 75%.

   for the id97 there are some alternative scenarios:
     * gensim (the best choice in the majority of cases)
           [38]https://radimrehurek.com/gensim/index.html
     * custom implementations based on nce (noise contrastive estimation)
       or hierarchical softmax. they are quite easy to implement with
       tensorflow, but they need an extra effort which is often not
       necessary
     * an initial embedding layer. this approach is the simplest, however,
       the training performances are worse because the same network has to
       learn good word representations and, at the same time, optimize its
       weights to minimize the output cross-id178.

   i   ve preferred to train a gensim id97 model with a vector size
   equal to 512 and a window of 10 tokens. the training set is made up of
   1.000.000 tweets and the test set by 100.000 tweets. both sets are
   shuffled before all epochs. as the average length of a tweet is about
   11 tokens (with a maximum of 53), i   ve decided to fix the max length
   equal to 15 tokens (of course this value can be increased, but for the
   majority of tweets the convolutional network input will be padded with
   many blank vectors).

   in the following figure, there   s a schematic representation of the
   process starting from the id27 and continuing with some 1d
   convolutions:

   the whole code (copied into this [39]gist and also available in the
   repository:
   [40]https://github.com/giuseppebonaccorso/twitter_sentiment_analysis_wo
   rd2vec_convnet) is:
   view the code on [41]gist.

   the training has been stopped by the early stopping callback after the
   twelfth iteration when the validation accuracy is about 79.4% with a
   validation loss of 0.44.

   possible improvements and/or experiments i   m going to try are:
     * different word vector size (i   ve already tried with 128 and 256,
       but i   d like to save more memory)
     * embedding layer
     * average and/or max pooling to reduce the dimensionality
     * different architectures

   the previous model has been trained on a gtx 1080 in about 40 minutes.

   see also:

[42]reuters-21578 text classification with gensim and keras     giuseppe
bonaccorso

     reuters-21578 is a collection of about 20k news-lines (see reference
     for more information, downloads and copyright notice), structured
     using sgml and categorized with 672 labels. they are diveded into
     five main categories: however, most of them are unused and, looking
     at the distribution, it   s possible to notice a complete lack of
     homogeneity.

share:

     * [43]click to share on twitter (opens in new window)
     * [44]click to share on facebook (opens in new window)
     * [45]click to share on linkedin (opens in new window)
     * [46]click to share on pocket (opens in new window)
     * [47]click to share on tumblr (opens in new window)
     * [48]click to share on reddit (opens in new window)
     * [49]click to share on pinterest (opens in new window)
     * [50]click to share on skype (opens in new window)
     * [51]click to share on whatsapp (opens in new window)
     * [52]click to share on telegram (opens in new window)
     * [53]click to email this to a friend (opens in new window)
     * [54]click to print (opens in new window)
     *

you can also be interested in these articles:

   [55]convnet[56]keras[57]nlp[58]sentiment
   analysis[59]tensorflow[60]twitter

post navigation

   [61]assessing id91 optimality with instability index
   [62]ml algorithms addendum: mutual information in classification tasks

64 thoughts on    twitter id31 with gensim id97 and keras
convolutional networks   

    1. jack
       10/12/2017 at 18:35
       hey,
       i tried your code on sentiment140 data set with 500,000 tweets for
       training and the rest for testing. i get about the same result as
       you on the validation set but when i use my generated model weights
       for testing, i get about 55% accuracy at best. do you know what
       could be causing it?
       [63]reply
          + [64]giuseppe bonaccorso
            10/12/2017 at 18:40
            in a deep model, the train size should be very large
            (sometimes also 95% of set). i think you   re excluding many
            elements. which is your training accuracy? if it   s quite
            higher than the validation acc, you   re overfitting. try with a
            larger training set and a smaller for testing.
            [65]reply
               o jack
                 10/12/2017 at 22:30
                 hey thanks for your reply! no, my training accuracy is
                 not too high as compared to validation accuracy. here is
                 my testing code [66]https://pastebin.com/cs3vjgeh
                 i just noticed that i am also creating a new id97
                 when tesing. do you think that could be a problem? should
                 i try and save my id97 model while training and reuse
                 it when testing?
                 [67]reply
                    # [68]giuseppe bonaccorso
                      10/13/2017 at 8:55
                      hi,
                      i cannot reproduce your code right now, however you
                      must use the same gensim model. the id27s
                      could be completely different due to the random
                      initializations. moreover you can lose the
                      correspondence between id27 and initial
                      dictionary.
                      [69]reply
                         @ jack
                           10/14/2017 at 13:38
                           yeah, i figured the same. i will give this a
                           shot and get back to you. thanks!
    2. henson
       10/16/2017 at 10:21
       hi giuseppe,
       thanks for making this great post. i have a question    
       on line 76, you create a id97 object by putting in the entire
       tokenized corpus through the function. then, from line 119 you
       perform the train-test split.
       from my understanding, id97 creates word vectors by looking at
       every word in the corpus (which we haven   t split yet). so in
       effect, your model could be biased as it has already    seen    the
       test data, because words that ultimately ended up in the test set
       influenced the ones in the training set.
       can you explain why this is so? please correct me if i   m wrong, but
       i   m a little confused here.
       [70]reply
          + [71]giuseppe bonaccorso
            10/16/2017 at 12:24
            hi henson,
            thanks for your comment.
            the id97 phase, in this case, is a preprocessing stage
            (like tf-idf), which transforms tokens into feature vectors.
            of course, its complexity is higher and the cosine similarity
            of synonyms should be very high. moreover, they are prone to
            be analyzed using 1d convolutions when concatenated into
            sentences.
            however, the model itself (not id97) uses these feature
            vectors to determine if a sentence has whether a positive or
            negative sentiment and this result is determined by many
            factors which work at sentence-level. the initial
            transformation can also be done in the same model (using and
            embedding layer), but the process is slower.
            on the other side, id97 has to    know    also the test words,
            just like any other nlp method, in order to build a complete
            dictionary. if you exclude them, you can   t predict with
            never-seen words. you   re correct when you say that they
            influence each other, but the skip-gram model considers the
            context, not the final classification.
            in other words:    paris   ,    london    and    city    can be close (in
            terms of cosine similarity), but it doesn   t mean that they can
            directly affect the id31. maybe there   s a
            sentence saying:    i love the city of paris    (positive
            sentiment) and another saying    i hate london. it   s a messy
            city    (negative sentiment) (i love both :d).
            so, i don   t think about a bias. maybe the model could be
            improved in terms of capacity, but it doesn   t show either a
            high bias or high variance.
            i hope my viewpoint was clear. it   s a very interesting
            conversation!
            [72]reply
               o henson
                 10/16/2017 at 14:10
                 wow, thanks for the clear explanation. makes perfect
                 sense now.     
                 [73]reply
    3. alessandro
       12/08/2017 at 18:00
       hi giuseppe,
       i have a question, no pre-trained glove model is used on which to
       create the id97 of the whole training set?
       [74]reply
          + [75]giuseppe bonaccorso
            12/09/2017 at 7:22
            hi,
            if i understand your question, the answer is no. the w2v model
            is created directly.
            [76]reply
    4. ayra
       01/20/2018 at 22:00
       hi,i have few questions and since i am new to this they might be
       basic so sorry in advance.
       1-i am getting    memory error    on line 114,is it hardware issue or
       am i doing something wrong in code?
       2-line number 33,what does it refer to?
       3-if i train my model with this dataset and then want to predict
       for the dataset which are still tweets but related to some specific
       brand,would it still make sense in your opinion?
       4-if i want to add lstm (output from the id98 goes into lstm for
       final classification),do you think it can improve results?if
       yes,can you guide a bit how to continue with your code to add that
       part?thanks alot!
       [77]reply
          + [78]giuseppe bonaccorso
            01/21/2018 at 11:22
            hi,
            1. the dataset is huge and you probably don   t have enough free
            memory. try to reduce the train size. all my tests have been
            done with 32gb
            2. a folder where you want to store the gensim model so to
            avoid retraining every time
            3. you should consider the words which are included in the
            production dataset. if they are very specific, it   s better to
            include a set of examples in the training set, or using a
            id97/glove/fasttext pretrained model (there are many based
            on the whole wikipedia corpus).
            4. i   m still working on some improvements, however, in this
            case, the idea is to use the convolutions on the whole
            utterance (which is not considered like an actual sequence
            even if a conv1d formally operates on a sequence), trying to
            discover the    geometrical    relationships that determine the
            semantics. you can easily try adding an lstm layer before the
            dense layers (without flattening). in this case, the input
            will have a shape (batch_size, timesteps, last_num_filters).
            the lstm output can be processed by one or more dense layers.
            [79]reply
               o ayra
                 01/22/2018 at 0:05
                 thanks alot!i am trying to go through line by line to
                 understand the code and i had my concepts build in terms
                 of images so understanding in terms of 1d text is a bit
                 new for me.so i have a few more questions since i am
                 confused a bit:
                 1-as far as i can understand id97 model is trained
                 till like line 87,after that,the separation of training
                 and test data is for id98 ,is my understanding right?
                 2-i wanted to run and see what exactly x_train looks like
                 but i couldnt run it so i am assuming from dry run that
                 its a matrix containing index,words and their
                 corresponding vectors.if my understanding is right,then
                 it means id98 takes 15 words as an input each time(which
                 might or might not be the whole tweet) so when i make
                 predictions how will it make sure that prediction is for
                 one whole tweet?
                 3-i was thinking to use another dataset as well which is
                 similar to one for which i want to make predictions
                 for(e.g.phones) for training id97 since it doesnt
                 need labelled data and it will probably just increase
                 dictionary.but i am concerned that id98 or id98+lstm wont
                 be able to learn anything since i couldnt find any
                 labelled dataset related to phones so if someone says
                 there camera is 2mp vs someone who says 30mp,it wont be
                 able to differentiate that the 2mp one is probably
                 negative sentiment and 30 one is positive.do you think
                 that i should try to make predictions only if i have
                 labelled dataset for that particular domain?
                 4-in lstm timestamp according to me is how many previous
                 steps you would want to consider before making next
                 prediction,which ideally is all the words of one tweet(to
                 see the whole context of the tweet) so in this case would
                 it be 1?since id98 takes 15 words which is almost one
                 tweet.last_num_filters i think is based on feature map or
                 filters that you have used in id98 so e.g. if in your code
                 you did 8,would this be 8?
                 sorry for really lengthy post and hope i make some sense
                 atleast.
                 thanks.
                 [80]reply
    5. kaz
       03/19/2018 at 11:57
       this post is really interesting!
       i am a beginner in the field of machine learning and i   ve been
       trying to understand this code. i would like to know how can we
       predict the sentiment of a fresh tweet/statement using this model.
       it   ll be really helpful if you could attach the code too!
       thanks!
       [81]reply
          + [82]giuseppe bonaccorso
            03/19/2018 at 17:49
            hi,
            thanks a lot for your comment! of course, you can work with
            new tweets. what you should do is similar to this part:
            for i, index in enumerate(indexes):
            for t, token in enumerate(tokenized_corpus[index]):
            if t >= max_tweet_length:
            break
    if token not in x_vecs:
        continue

    if i < train_size:
        x_train[i, t, :] = x_vecs[token]
    else:
        x_test[i - train_size, t, :] = x_vecs[token]

            ...
            in other words, you need first to tokenize the tweet, then
            lookup for the word vectors corresponding to each token.
            however, i   m planning to post a new article based on fasttext
            and i   m going to add a specific section for querying the
            model.
            [83]reply
    6. ahmed
       03/23/2018 at 16:32
       error in line 116
       memoryerror traceback (most recent call last)
       in ()
       2 indexes = set(np.random.choice(len(tokenized_corpus), train_size
       + test_size, replace=false))
       3
          -> 4 x_train = np.zeros((train_size, max_tweet_length,
       vector_size), dtype=k.floatx())
       5 y_train = np.zeros((train_size, 2), dtype=np.int32)
       6 x_test = np.zeros((test_size, max_tweet_length, vector_size),
       dtype=k.floatx())
       memoryerror:
       please hellp
       i tried to reduce trsining and test data to 750000 or even 100000
       and didnot work
       [84]reply
          + [85]giuseppe bonaccorso
            03/25/2018 at 9:41
            hi,
            unfortunately, i can   t help you. you don   t enough free memory.
            try to reset the notebook (if using jupyter) after reducing
            the number of samples. you can also reduce the
            max_tweet_length and the vector size. consider that i worked
            with 32 gb but many people successfully trained the model with
            16 gb.
            [86]reply
    7. ahmed
       03/25/2018 at 9:44
       hi .. it   s worked with 100.000sample but very slow .. i have
       another question .. how can i fed a new review to get it   s
       sentiment predict ?
       [87]reply
    8. zahra
       06/27/2018 at 18:49
       hi,
       i wanna train your model in non english language so i have a couple
       of questions, i would appreciate if you help me,
       1- when i trained your model in my own non english corpus, i got
       unicode error so i tried to fix it with utf8 but it doesn   t work
       anymore, do you have any idea to solve it?
       2- i wanna know whether your id97 model works properly in my
       own english corpus or not is there any code to show id97 output
       vector to me??
       i gonna use id97.save(   file.model   ) but when i open it the file
       contain doesn   t seem meaningful and doesn   t have any vectors
       thanks alot
       [88]reply
          + [89]giuseppe bonaccorso
            06/27/2018 at 22:01
            hi,
            id97 works with any language. if you are experiencing
            issues, they are probably due to the charset. unfortunately, i
            can   t help you, but encode(   utf8   ) and decode(   utf8   ) on the
            strings should solve the problem. alternatively, try other
            charsets, like iso-8859-1.
            the model is binary, so it doesn   t make sense to try and read
            it. if you want to test it (considering the variable names
            used in the example), you can try x_vecs.similarity(   word1   ,
               word2   ) with a couple of test words (e.g.    king    and
               queen   ). instead, the word vectors can be retrieved as in a
            standard dictionary: x_vecs[   word   ]. clearly, check whether
            the term exists before trying to read the vector.
            [90]reply
               o zahra
                 06/30/2018 at 12:03
                 thanks for you   r clear explanation. i completely got it.
                 [91]reply
    9. zahra
       07/01/2018 at 17:18
       hi, i have some questions.
       -in you   re code when i write print(tokens) to see the result of
       tokenized process i face some strange result, say this sentence for
       example:
       .. omgaga. im sooo im gunna cry. i   ve been at this dentist since
       11.. i was suposed 2 just get a crown put on (30mins)   .
       as you know, this is a tweet from you   re corpus and here is the
       result:
       [   omgag   ,    im   ,    sooo   ,    im   ,    gunn   ,    cry   ,    i   ,    ve   ,    been   ,
          at   ,    thi   ,    dent   ,    sint   ,    11   ,    i   ,    was   ,    supos   ,    2   ,
          just   ,    get   ,    a   ,    crown   ,    put   ,    on   ,    30mins   ]
       and when i use nltk for tokenize the result gonna be change, here
       is the result with nltk:
       [   ..   ,    omgaga   ,    .   ,    im   ,    sooo   ,    im   ,    gunna   ,    cry   ,    .   ,    i   ,
             ve   ,    been   ,    at   ,    this   ,    dentist   ,    since   ,    11..   ,    i   ,
          was   ,    suposed   ,    2   ,    just   ,    get   ,    a   ,    crown   ,    put   ,    on   ,
          (   ,    30mins   ,    )   ,          ,    .   ]
       and this is the related code:
       from nltk.tokenize import word_tokenize
       s=         .
       word_tokenize(s)
       i can   t understand why do they have this different and i really get
       confused!
       2-is it that important to have tokenize and stem as a preprocessor
       in id31?? i mean can i train my model without these
       preprocessor, in other words get the corpus directly to the
       id97 model and the result will be passed for training, is it
       possible??
       3-since i   m not that familiar to this field i wanna know after
       training the model is this any code to get my sentences as an input
       and show me the polarity(negative or positive) as an output?? kinda
       test the model i mean,
       thanks alot
       [92]reply
          + [93]giuseppe bonaccorso
            07/01/2018 at 17:44
            when using id97, you can avoid id30 (increasing the
            dictionary size and reducing the generality of the words), but
            tokenizing is always necessary (if you don   t do it explicitly,
            it will be done by the model). the differences are due to
            different approaches (for example, a tokenizer can strip all
            punctuation while another can keep           because of its
            potential meaning). nltk offers different solutions and i
            invite you to check the documentation (this is not
            advertising, but if you are interested in an introduction to
            nlp, there are a couple of chapters in my book machine
            learning algorithms).
            a quick solution to get the polarity is using the vadim
            sentiment analyzer
            ([94]http://www.nltk.org/howto/sentiment.html), which is a
            rule-based algorithm. otherwise, you must:
            1. tokenize the sentence (with the same method employed in the
            training phase)
            2. pad or truncate it (see the code for an example)
            3. create an array containing the vectors for each token
            4. use the method predict(   ) on the keras model to get the
            output
            [95]reply
   10. zahra
       07/01/2018 at 18:04
       thanks alot for your quick answer and valuable suggestions,
       [96]reply
   11. bahara
       07/16/2018 at 12:02
       hi, i run your code in my corpus and everything was ok. but i want
       to know how should i predict sentiment for new tweet, say :    im
       really hungry    for example, since i   m new to this field would you
       please help me to add related code for prediction? thanks
       [97]reply
          + [98]giuseppe bonaccorso
            07/16/2018 at 16:55
            hi,
            i   ve asked this question in other comments. however, you need
            to tokenize your sentence, creating an empty array with the
            maximum length employed during the training, then setting each
            word vector (x_vecs[word]) if present or keep it null if the
            word is not present in the dictionary. at this moment, i   m
            quite busy, but i   m going to create an explicit example soon.
            [99]reply
               o bahara
                 07/16/2018 at 17:08
                 thank you for your instruction- i gonna test it    
                 btw hope to create it soon
                 [100]reply
               o bahara
                 07/16/2018 at 21:29
                 really sorry, but i forgot to ask you if is it right to
                 use    model.predict()    or not, i mean use it after those
                 steps that you recommended before
                 [101]reply
                    # [102]giuseppe bonaccorso
                      07/16/2018 at 21:46
                      of course.
                      [103]reply
                         @ bahara
                           07/18/2018 at 20:43
                           hi, with your instructions i wrote the code for
                           prediction, but i faced a strange problem,since
                           your max_tweet_length is 15 when i get a
                           sentence with for example 3 length,,, say = i   m
                           too hungry for example,,, i faced this error :
                           valueerror: error when checking input: expected
                           conv1d_1_input to have shape (15, 512) but got
                           array with shape (3, 512)
                           to my understanding from the net, there might
                           be something related to input shape, line 143-
                           but i really don   t know how can i fix it, i
                           would appreciate if you help me
                           thanks alot
   12. johm
       07/21/2018 at 9:45
       hi, i want to add neutral sentiment too your code- i added neutral
       tweets with the specific label, 2 , and changed the related code in
       this way:
       if i < train_size:
       if labels[index] == 0 :
       y_train[i, :] = [1.0, 0.0]
       elif labels[index] == 1 :
       y_train[i, :] = [0.0, 1.0]
       else:
       y_train[i, :] = [1.0, 1.0]
       and the same for the testing     all i did was to change what i said
           is it right?
       thanks
       [104]reply
          + [105]giuseppe bonaccorso
            07/21/2018 at 11:02
            hi, you also need to modify the output layer of the network.
            right now it   s a softmax and [1, 1] cannot be accepted. try
            using a sigmoid layer instead. alternatively, you need to
            assign [0.5, 0.5] to the neutral sentiment. softmax must
            represent a valid id203 distribution (so the sum must be
            always equal to 1).
            [106]reply
               o john
                 07/22/2018 at 18:57
                 hi thank you for your clear explanation- i did what you
                 said    
                 i assign in such way: y_test[i     train_size, :] = [0.5,
                 0.5] and i although that i understood in this way i can
                 use softmax , i use sigmoid     all i did was what i said    
                 i didn   t add new neural or anything but the code can   t
                 predict any neutral idea     do you have any suggestion ??
                 [107]reply
                    # [108]giuseppe bonaccorso
                      07/22/2018 at 20:07
                      hi, with (0.5, 0.5) you should use softmax. indeed,
                      any output which is close to (0.5, 0.5) is
                      implicitly a neutral. however, do you have neutral
                      tweets? you should have a dataset made up of 33%
                      positive, 33% negative, and 33% neutral in order to
                      avoid biases.
                      [109]reply
                         @ john
                           07/22/2018 at 20:40
                           yeah my corpus consist only about 10% of
                           neutral     i gonna make my corpus balanced but
                           you know when i put print after this line:
                           y_train[i, :] = [0.5, 0.5]
                           print(y_train[i, :])
                           i see [0 0] in the output !!!!!! do know why??
                         @ [110]giuseppe bonaccorso
                           07/22/2018 at 20:50
                           no    it   s quite strange
               o john
                 07/22/2018 at 19:08
                 and i also want to know do you prefer to assign in the
                 way i mentioned or in this way :
                 y_test[i     train_size, :] = [0.0, 0.0,0.1] consider that
                 i do the same for positive and negative too     honestly i
                 did that but i can   t get the properly result so i want to
                 know whether this might some logical problem or something
                 from my corpus    .
                 with this view i just changed y_train =
                 np.zeros((train_size, 2), dtype=np.int32) to 3 and the
                 same for test and change softmax to sigmoid
                 thank you for your patient
                 [111]reply
   13. shim
       07/22/2018 at 17:28
       great job!
       hi     i   m new in this field so i get confused for a basic issue. why
       doesn   t your model use classifier training method such as training
       and testing the naive bayes classifier?
       is it ok to only choose randomly training and testing data set
       among the corpus??why?
       sorry if i were stupid
       thank you
       [112]reply
          + [113]giuseppe bonaccorso
            07/22/2018 at 17:56
            hi, this is a model based on word vectors that can be more
            efficiently managed using nn or kernel id166. however, you are
            to test any other algorithm (e.g. a gaussian naive bayes) and
            select the solution the best meets your needs.
            in any model, the dataset is supposed to represent a data
            generating process, so randomly sampling from it is the
            optimal way to create two subsets that are close (not exactly
            overlapped) to the original id203 distribution. a
            non-random choice can bias the model, by forcing it to learn
            only some associations while other ones are never presented
            (and, therefore, the relative predictions cannot be reliable).
            [114]reply
   14. john
       07/23/2018 at 21:37
       hi     i have a question     why do you consider 2dim array for y-train
       and y-test?? does it have any problem to define a 1d vector and
       pass it for example 0 for negative and 1 for positive? in such way:
       y_test[i     train_size, :] = [1] for positive
       y_test[i     train_size, :] = [0] for negative
       or for example in such way:
       y_test[i     train_size, :] = [1,1] for positive
       y_test[i     train_size, :] = [0,0] for negative
       does the model of initialing y_test have any effect on the learning
       or what??
       thank you
       [115]reply
   15. neda
       08/28/2018 at 11:08
       hi. i ran your code with my own balanced dataset and when i wanted
       to predict sentences, my model predict all sentences as negative!!!
       i was suffering the internet for days but i can   t fix my problem.
       do you have any idea to help me?
       btw my corpus contain 9000 sentences with equal amount of + and     .
       thank you
       [116]reply
          + [117]giuseppe bonaccorso
            08/28/2018 at 17:33
            have you retrained both the work2vec and the network? if yes,
            you should have seen the validation performances on a test
            set. if you haven   t, it probably means that there are strong
            discrepancies between the two training sets.
            [118]reply
               o neda
                 08/28/2018 at 17:41
                 sorry would you mind explaining more? i think i   m kinda
                 misunderstood since i   m new in this field.
                 how can i should see the validation performance?
                 and about id97 yeah i   ve retrained it.
                 [119]reply
                    # [120]giuseppe bonaccorso
                      08/28/2018 at 18:05
                      this is a generic keras output line:
                      epoch 12/100
                      1000000/1000000 [==============================]    
                      204s     loss: 0.4489     acc: 0.7902     val_loss: 0.4415
                          val_acc: 0.7938
                      as you can see, the validation accuracy (val_acc) is
                      0.7938. this means that the classifier predicts
                      correctly about 80% of labels (considering the test
                      set, which contains samples never seen before).
                      check your validation accuracy. in the case of low
                      values (<0.5, which is a random guess), start
                      increasing the number of epochs. if it doesn   t   
                      work, assuming that your dataset is balanced, try
                      with different architectures (e.g. more dropout and
                      more layers).
                      [121]reply
                         @ neda
                           08/28/2018 at 19:00
                           i gonna do what you said but when i look may
                           val-acc i think the result is kinda strange.do
                           you have any idea?
                           train on 8900 samples, validate on 100 samples
                           you see my balanced corpus contains 9100
                           sentences which i used it as i mentioned above.
                           and this is my result!!!!!!!!!!!!!
                           epoch 3/12
                           (as last one)
                           8900/8900 [==============================]    
                           15s 2ms/step     loss: 0.5896     acc: 0.6330    
                           val_loss: 0.0000e+00     val_acc: 1.0000
                         @ [122]giuseppe bonaccorso
                           08/29/2018 at 7:20
                           the result is not strange at all. it simply
                           shows a mistake: the test set is made up of
                           samples belonging to the same class and, hence,
                           it doesn   t represent the training distribution.
                           it   s clearly impossible to have 0.63 training
                           accuracy and 1.0 validation accuracy. shuffle
                           your dataset before splitting and, possibly,
                           enlarge your test set (e.g. 1000 samples).
   16. neda
       08/29/2018 at 10:15
       thank you for your clear explanation. but i   m kinda
       misunderstood,hope to help me.
       right before splitting we use id97 so when should we shuffle
       our data?? i mean should we shuffle exact tweet or do it after
       using embedding method such as id97?
       thank you
       [123]reply
          + [124]giuseppe bonaccorso
            08/29/2018 at 17:28
            before training the deep model, if your dataset is (x, y), use
            train_test_split from scikit-learn:
            from sklearn.model_selection import train_test_split
            x_train, x_test, y_train, y_test = train_test_split(x, y,
            test_size=0.2, random_state=1000)
            in this way, the sets are shuffled.
            [125]reply
               o neda
                 08/29/2018 at 22:30
                 thanks a lot     i did what you recommended but
                 unfortunately i got a dimension error in line
                 173    
                 valueerror: error when checking input: expected
                 conv1d_1_input to have 3 dimensions, but got array with
                 shape (7254, 1)
                 [126]reply
   17. neda
       08/29/2018 at 23:20
       i don   t know if i think right but in your code i added these, in
       line 140:
       x = corpus
       y = labels
       x_train, x_test, y_train, y_test = train_test_split(x, y,
       test_size=0.2, random_state=1000)
       and as i told you i get this problem:
       valueerror: error when checking input: expected conv1d_1_input to
       have 3 dimensions, but got array with shape (7254, 1)
       ***really sorry for being silly
       [127]reply
          + [128]giuseppe bonaccorso
            08/31/2018 at 18:32
            from your error, i suppose you   re feeding the labels (which
            should be one-hot encoded for a cross-id178 loss, so the
            shape should be (7254, num classes)) as input to the
            convolutional layer. honestly, i don   t know how to help you.
            the input shape should be (num samples, max length, vector
            size), hence check if x has such a shape before splitting.
            moreover, as the output is binary, y should be (num samples,
            2). for example, positive (1.0, 0.0) or negative (0.0, 1.0).
            [129]reply
   18. fer
       08/30/2018 at 9:10
       excuse me why don   t you separate your corpus into 3 parts as
       training testing and validation??
       [130]reply
          + [131]giuseppe bonaccorso
            08/31/2018 at 18:26
            the subdivision into 2 or 3 blocks is a choice with a specific
            purpose. if the dataset is assumed to be sampled from a
            specific data generating process, we want to train a model
            using a subset representing the original distribution and
            validating it using another set of samples (drawn from the
            same process) that have never been used for training. in some
            cases, it   s helpful to have a test set which is employed for
            the hyperparameter tuning and the architectural choices and a
               final    validation set, that is employed only for a pure
            non-biased evaluation. in both scenarios (2 or 3), the goal is
            the same and the only very important condition is that all 2/3
            sets must be drawn from the same distribution. of course, feel
            free to split into 3 sets if you prefer this strategy.
            [132]reply
   19. negar
       09/05/2018 at 23:56
       hi. thanks a lot for your nice explanation- just have a question
       since i   m a beginner     what classifier you use for your
       model?liner? id203? and why? please explain it thank you
       [133]reply
          + [134]giuseppe bonaccorso
            09/06/2018 at 17:54
            it   s a deep convolutional network (1d)
            [135]reply
               o negar
                 09/06/2018 at 19:37
                 thanks a lot
                 [136]reply
   20. sah
       09/07/2018 at 10:51
       nice post     i have a simple question    
       in your architecture how many hidden layer did you use? and the
       number of neuron in each layer is 32?? am i right?
       how can you know which number of layer would be beneficial for your
       model?
       thanks alot
       [137]reply
          + [138]giuseppe bonaccorso
            09/07/2018 at 17:40
            hi,
            in a convolutional network, it doesn   t make sense talking
            about neurons. in this case, there 8 layers (separated by a
            dropout one) with 32 (3  1) kernels (with elu activation),
            followed by 2 dense tanh layers with 256 neurons and a softmax
            output layer with 2 units.
            the number of layers can be analyzed in many ways:
               o experience
               o validation
               o grid-search
            in general, it   s helpful to start with a model with smaller
            models, checking the validation accuracy, overfitting, and so
            on, and making a decision (e.g. adding new layers, increasing
            or decreasing the number of units, adding id173,
            dropout, batch id172,    ). the golden rule (derived
            from the occam   s razor) is to try to find the smallest model
            which achieves the highest validation accuracy. an alternative
            (but more expensive) approach is based on a grid-search. in
            this case, a set of models based on different parameters are
            trained sequentially (or in parallel, if you have enough
            resources) and the optimal configuration (corresponding to the
            highest accuracy/smallest loss) is selected. normally this
            approach requires more iterations because the initial grid is
            coarse-grained and it   s used to determine the sub-space where
            the optimal parameter set is located. then, several zooms are
            performed in order to fine-tune the research.
            [139]reply
   21. natoosha_26571
       09/21/2018 at 18:12
       excuse me sir,
       would you please tell me how many hidden layer did you use in your
       model?
       how can i realize that?
       thank you man
       [140]reply
          + [141]giuseppe bonaccorso
            09/22/2018 at 9:39
            count the number of layers added to the keras model (through
            the method model.add(   )) excluding all    non-structural    ones
            (like dropout, batch id172, flattening/reshaping,
            etc.). in this case, there are 11 layers (considering also the
            output one).
            i highly recommend studying the basic concepts of keras,
            otherwise, it   s impossible to have the minimum awareness
            required to start working with the examples.
            [142]reply
   22. sanam
       10/01/2018 at 12:38
       hi. i wantvto know is it possible to inject some handcrafted
       feature to id98 layers?? i eant to use only convolutional natwork
       nor id166 and     is it possible to combine both kinds of features??
       and i think i should inject hand-crafted features into the fully
       connected layer but i donk know how? can you help me please?
       [143]reply
          + [144]giuseppe bonaccorso
            10/03/2018 at 16:18
            what do you mean with injecting    handcrafted    features? are
            you talking about data-augmented samples?
            [145]reply
   23. ahmed bahaa
       10/25/2018 at 11:16
       hello
       i have this error please t
       [146]https://uploads.disquscdn.com/images/93066cba175391f7263163b9c
       8115ba436eff9332276c412cfe0dcd37e2a9854.png
       [147]reply
          + [148]giuseppe bonaccorso
            10/28/2018 at 11:06
            hi,
            it clearly means that the list/array contains fewer elements
            than the value reached by the index. check the dimensions
            (using x.shape for arrays or len(x) for lists) before starting
            the loops or using indexes.
            [149]reply

leave a reply [150]cancel reply

   iframe: [151]jetpack_remote_comment

follow me

     * [152]linkedin
     * [153]twitter
     * [154]facebook
     * [155]github
     * [156]instagram
     * [157]amazon
     * [158]medium
     * [159]rss

search articles

   ____________________ (button)

latest blog posts

     * [160]machine learning algorithms     second edition 08/28/2018
     * [161]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [162]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [163]a book that every data scientist should read 06/22/2018
     * [164]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [165]my tweets

   copyright    2019 [166]giuseppe bonaccorso. all rights reserved.
   [167]privacy policy - [168]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [169]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/
  26. https://www.bonaccorso.eu/category/machine-learning/deep-learning/convnet/
  27. https://www.bonaccorso.eu/category/machine-learning/deep-learning/
  28. https://www.bonaccorso.eu/category/generic/
  29. https://www.bonaccorso.eu/category/machine-learning/deep-learning/keras/
  30. https://www.bonaccorso.eu/category/machine-learning/neural-networks/
  31. https://www.bonaccorso.eu/category/machine-learning/nlp/
  32. https://www.bonaccorso.eu/category/software/python/
  33. https://www.bonaccorso.eu/category/machine-learning/deep-learning/tensorflow-deep-learning/
  34. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comments
  35. https://github.com/giuseppebonaccorso/twitter_sentiment_analysis_id97_convnet/fork
  36. https://code.google.com/archive/p/id97/
  37. http://thinknook.com/wp-content/uploads/2012/09/sentiment-analysis-dataset.zip
  38. https://radimrehurek.com/gensim/index.html
  39. https://gist.github.com/giuseppebonaccorso/061fca8d0dfc6873619efd8f364bfe89
  40. https://github.com/giuseppebonaccorso/twitter_sentiment_analysis_id97_convnet
  41. https://gist.github.com/giuseppebonaccorso/061fca8d0dfc6873619efd8f364bfe89
  42. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/
  43. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=twitter
  44. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=facebook
  45. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=linkedin
  46. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=pocket
  47. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=tumblr
  48. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=reddit
  49. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=pinterest
  50. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=skype
  51. https://api.whatsapp.com/send?text=twitter id31 with gensim id97 and keras convolutional networks https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/
  52. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=telegram
  53. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/?share=email
  54. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#print
  55. https://www.bonaccorso.eu/tag/convnet/
  56. https://www.bonaccorso.eu/tag/keras/
  57. https://www.bonaccorso.eu/tag/nlp/
  58. https://www.bonaccorso.eu/tag/sentiment-analysis/
  59. https://www.bonaccorso.eu/tag/tensorflow/
  60. https://www.bonaccorso.eu/tag/twitter/
  61. https://www.bonaccorso.eu/2017/08/03/assessing-id91-optimality-instability-index/
  62. https://www.bonaccorso.eu/2017/08/18/ml-algorithms-addendum-mutual-information-in-classification-tasks/
  63. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-50
  64. https://www.bonaccorso.eu/
  65. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-51
  66. https://pastebin.com/cs3vjgeh
  67. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-52
  68. https://www.bonaccorso.eu/
  69. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-53
  70. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-55
  71. https://www.bonaccorso.eu/
  72. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-56
  73. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-57
  74. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-61
  75. https://www.bonaccorso.eu/
  76. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-62
  77. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-72
  78. https://www.bonaccorso.eu/
  79. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-73
  80. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-74
  81. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-78
  82. https://www.bonaccorso.eu/
  83. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-79
  84. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-80
  85. https://www.bonaccorso.eu/
  86. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-81
  87. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-82
  88. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-91
  89. https://www.bonaccorso.eu/
  90. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-92
  91. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-93
  92. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-94
  93. https://www.bonaccorso.eu/
  94. http://www.nltk.org/howto/sentiment.html
  95. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-95
  96. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-96
  97. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-98
  98. https://www.bonaccorso.eu/
  99. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-100
 100. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-101
 101. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-102
 102. https://www.bonaccorso.eu/
 103. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-103
 104. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-105
 105. https://www.bonaccorso.eu/
 106. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-106
 107. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-109
 108. https://www.bonaccorso.eu/
 109. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-111
 110. https://www.bonaccorso.eu/
 111. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-110
 112. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-107
 113. https://www.bonaccorso.eu/
 114. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-108
 115. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-114
 116. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-115
 117. https://www.bonaccorso.eu/
 118. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-116
 119. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-117
 120. https://www.bonaccorso.eu/
 121. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-118
 122. https://www.bonaccorso.eu/
 123. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-121
 124. https://www.bonaccorso.eu/
 125. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-122
 126. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-123
 127. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-124
 128. https://www.bonaccorso.eu/
 129. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-130
 130. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-125
 131. https://www.bonaccorso.eu/
 132. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-129
 133. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-133
 134. https://www.bonaccorso.eu/
 135. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-134
 136. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-135
 137. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-136
 138. https://www.bonaccorso.eu/
 139. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-137
 140. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-139
 141. https://www.bonaccorso.eu/
 142. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-140
 143. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-141
 144. https://www.bonaccorso.eu/
 145. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-142
 146. https://uploads.disquscdn.com/images/93066cba175391f7263163b9c8115ba436eff9332276c412cfe0dcd37e2a9854.png
 147. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-143
 148. https://www.bonaccorso.eu/
 149. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#comment-144
 150. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#respond
 151. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=1080&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=6ed60153e8c7bae0e6e341d116552b21842fd863#parent=https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/
 152. https://www.linkedin.com/in/giuseppebonaccorso/
 153. https://twitter.com/giuseppeb/
 154. https://www.facebook.com/giuseppe.bonaccorso/
 155. https://github.com/giuseppebonaccorso/
 156. https://www.instagram.com/giuseppebonaccorso/
 157. https://www.amazon.com/author/giuseppebonaccorso
 158. https://medium.com/@giuseppe.bonaccorso
 159. https://www.bonaccorso.eu/feed/
 160. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
 161. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
 162. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
 163. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
 164. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
 165. https://twitter.com/giuseppeb
 166. https://www.bonaccorso.eu/
 167. https://www.iubenda.com/privacy-policy/331721
 168. https://www.iubenda.com/privacy-policy/331721/cookie-policy
 169. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/#cancel

   hidden links:
 171. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
