   #[1]giuseppe bonaccorso    feed [2]giuseppe bonaccorso    comments feed
   [3]giuseppe bonaccorso    reuters-21578 text classification with gensim
   and keras comments feed [4]alternate [5]alternate

[6]giuseppe bonaccorso

   artificial intelligence     machine learning     data science
   (button)

     * [7]blog
     * [8]books
     * [9]resume / cv
     * [10]bonaccorso   s law
     * [11]essays
     * [12]contact
     * [13]testimonials
     * [14]gallery
     * [15]disclaimer

     * [16]blog
     * [17]books
     * [18]resume / cv
     * [19]bonaccorso   s law
     * [20]essays
     * [21]contact
     * [22]testimonials
     * [23]gallery
     * [24]disclaimer

reuters-21578 text classification with gensim and keras

   [25]08/02/201606/11/2018[26]artificial intelligence, [27]deep learning,
   [28]generic, [29]keras, [30]machine learning, [31]neural networks,
   [32]nlp, [33]python[34]2 comments

   [35]fork

   reuters-21578 is a collection of about 20k news-lines (see reference
   for more information, downloads and copyright notice), structured using
   sgml and categorized with 672 labels. they are diveded into five main
   categories:
     * topics
     * places
     * people
     * organizations
     * exchanges

   however, most of them are unused and, looking at the distribution, it   s
   possible to notice a complete lack of homogeneity. these are the 20 top
   categories (the prefix is made with the two initial letter of each main
   category) with the number of related news-lines:
   id       name         category    newslines
   161 pl_usa          places        12542
   533 to_earn         topics        3987
   498 to_acq          topics        2448
   158 pl_uk           places        1489
   84  pl_japan        places        1138
   31  pl_canada       places        1104
   571 to_money-fx     topics        801
   526 to_crude        topics        634
   543 to_grain        topics        628
   167 pl_west-germany places        567
   624 to_trade        topics        552
   553 to_interest     topics        513
   56  pl_france       places        469
   185 or_ec           organizations 349
   23  pl_brazil       places        332
   628 to_wheat        topics        306
   606 to_ship         topics        305
   10  pl_australia    places        270
   517 to_corn         topics        254
   37  pl_china        places        223


   in the    experiment    (as jupyter notebook) you can find on this
   [36]github repository, i   ve defined a pipeline for a one-vs-rest
   categorization method, using id97  (implemented by gensim), which
   is much more effective than a standard bag-of-words or tf-idf approach,
   and lstm neural networks (modeled with keras with theano/gpu support    
   see [37]https://goo.gl/ywn4xj for an example written by its author,
   using an embedding layer instead of id97). the pipeline is based on
   the following steps (just like a id31 approach):
    1. category and document acquisition (i suggest to see the full code
       on github). however, i   ve used beautifulsoup in order to parse all
       sgml files, removing all unwanted tags and a simple regex in order
       to strip the ending signature.
    2. tokenizing (with id138 lemmatization and stop-words filtering,
       both implemented by nltk framework)# load stop-words
# load stop-words
stop_words = set(stopwords.words('english'))

# initialize tokenizer
# it's also possible to try with a stemmer or to mix a stemmer and a lemmatizer
tokenizer = regexptokenizer('['a-za-z]+')

# initialize lemmatizer
lemmatizer = id138lemmatizer()

# tokenized document collection
newsline_documents = []

def tokenize(document):
 words = []

 for sentence in sent_tokenize(document):
 tokens = [lemmatizer.lemmatize(t.lower()) for t in tokenizer.tokenize(sentence)
 if t.lower() not in stop_words]
 words += tokens

 return words

    3. id97 training
# create new gensim id97 model
w2v_model = id97(newsline_documents, size=num_features, min_count=1, window=
10, workers=cpu_count())
w2v_model.init_sims(replace=true)
w2v_model.save(data_folder + 'reuters.id97')
    4. id97 conversion
num_categories = len(selected_categories)
x = zeros(shape=(number_of_documents, document_max_num_words, num_features)).ast
ype(float32)
y = zeros(shape=(number_of_documents, num_categories)).astype(float32)

empty_word = zeros(num_features).astype(float32)

for idx, document in enumerate(newsline_documents):
    for jdx, word in enumerate(document):
        if jdx == document_max_num_words:
            break

        else:
            if word in w2v_model:
                x[idx, jdx, :] = w2v_model[word]
            else:
                x[idx, jdx, :] = empty_word

for idx, key in enumerate(document_y.keys()):
    y[idx, :] = document_y[key]
    5. lstm network training
model = sequential()

model.add(lstm(int(document_max_num_words*1.5), input_shape=(document_max_num_wo
rds, num_features)))
model.add(dropout(0.3))
model.add(dense(num_categories))
model.add(activation('sigmoid'))

model.compile(loss='binary_crossid178', optimizer='adam', metrics=['accuracy']
)

# train model
model.fit(x_train, y_train, batch_size=128, nb_epoch=5, validation_data=(x_test,
 y_test))

# evaluate model
score, acc = model.evaluate(x_test, y_test, batch_size=128)

print('score: %1.4f' % score)
print('accuracy: %1.4f' % acc)

   i   ve tested different neural models and i got the best results with an
   lstm layer of 150 cells (1.5 x number of words), a dropout of 0.3-0.4
   and    [38]adam    optimizer. a batch size less than 64 can cause a slow
   down in the learning rate. in my experiments, 128 is a very good value.
   after 5 epochs (they should be incremented in a real-world scenario), i
   have these results:
train on 15104 samples, validate on 6474 samples
epoch 1/5
15104/15104 [==============================] - 38s - loss: 0.5610 - acc: 0.7168
- val_loss: 0.6814 - val_acc: 0.5828
epoch 2/5
15104/15104 [==============================] - 40s - loss: 0.5994 - acc: 0.6597
- val_loss: 0.4435 - val_acc: 0.8230
epoch 3/5
15104/15104 [==============================] - 37s - loss: 0.3949 - acc: 0.8477
- val_loss: 0.3765 - val_acc: 0.8557
epoch 4/5
15104/15104 [==============================] - 38s - loss: 0.3567 - acc: 0.8707
- val_loss: 0.3415 - val_acc: 0.8735
epoch 5/5
15104/15104 [==============================] - 37s - loss: 0.3408 - acc: 0.8761
- val_loss: 0.3269 - val_acc: 0.8837
6474/6474 [==============================] - 5s
score: 0.3269
accuracy: 0.8837


   reference:
     * dataset: [39]https://kdd.ics.uci.edu/databases/reuters21578/
     * dataset info:
       [40]http://kdd.ics.uci.edu/databases/reuters21578/readme.txt
     * gensim id97:
       [41]https://radimrehurek.com/gensim/models/id97.html
     * id97 tutorial:
       [42]http://radimrehurek.com/2014/02/id97-tutorial/
     * id97 and doc2vec models:
       [43]http://arxiv.org/pdf/1405.4053v2.pdf
     * lstm: [44]http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf

   photo credit: [45]old news ! via [46]photopin [47](license)

   see also:

[48]twitter id31 with gensim id97 and keras convolutional
networks     giuseppe bonaccorso

     id97 ( https://code.google.com/archive/p/id97/) offers a
     very interesting alternative to classical nlp based on
     term-frequency matrices. in particular, as each word is embedded
     into a high-dimensional vector, it   s possible to consider a sentence
     like a sequence of points that determine an implicit geometry.

share:

     * [49]click to share on twitter (opens in new window)
     * [50]click to share on facebook (opens in new window)
     * [51]click to share on linkedin (opens in new window)
     * [52]click to share on pocket (opens in new window)
     * [53]click to share on tumblr (opens in new window)
     * [54]click to share on reddit (opens in new window)
     * [55]click to share on pinterest (opens in new window)
     * [56]click to share on skype (opens in new window)
     * [57]click to share on whatsapp (opens in new window)
     * [58]click to share on telegram (opens in new window)
     * [59]click to email this to a friend (opens in new window)
     * [60]click to print (opens in new window)
     *

you can also be interested in these articles:

   [61]deep learning[62]gensim[63]keras[64]lstm[65]nlp[66]nltk[67]id97

post navigation

   [68]fixed-delay smoothing in id48 with numpy
   [69]cifar-10 image classification with keras convnet

2 thoughts on    reuters-21578 text classification with gensim and keras   

    1. peter
       08/29/2017 at 4:09
       hi, bonaccorso
       your    github repository    link is broken?
       [70]http://www.bonaccorso.eu/wp-content/uploads/2016/08/reuters-215
       78-classification1
       thanks
       best regards
       peter
       [71]reply
          + [72]giuseppe bonaccorso
            08/29/2017 at 9:11
            thank you peter! fixed.
            [73]reply

leave a reply [74]cancel reply

   iframe: [75]jetpack_remote_comment

follow me

     * [76]linkedin
     * [77]twitter
     * [78]facebook
     * [79]github
     * [80]instagram
     * [81]amazon
     * [82]medium
     * [83]rss

search articles

   ____________________ (button)

latest blog posts

     * [84]machine learning algorithms     second edition 08/28/2018
     * [85]recommendations and user-profiling from implicit feedbacks
       07/10/2018
     * [86]are recommendations really helpful? a brief non-technical
       discussion 06/29/2018
     * [87]a book that every data scientist should read 06/22/2018
     * [88]mastering machine learning algorithms 05/24/2018

subscribe to this blog

   join 2,190 other subscribers

   email ____________________

   subscribe

follow me on twitter

   [89]my tweets

   copyright    2019 [90]giuseppe bonaccorso. all rights reserved.
   [91]privacy policy - [92]cookie policy

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________ loading send email [93]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

references

   visible links
   1. https://www.bonaccorso.eu/feed/
   2. https://www.bonaccorso.eu/comments/feed/
   3. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/feed/
   4. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/
   5. https://www.bonaccorso.eu/wp-json/oembed/1.0/embed?url=https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/&format=xml
   6. https://www.bonaccorso.eu/
   7. https://www.bonaccorso.eu/blog/
   8. https://www.bonaccorso.eu/books/
   9. https://www.bonaccorso.eu/resume/
  10. https://www.bonaccorso.eu/bonaccorso-law/
  11. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  12. https://www.bonaccorso.eu/contact/
  13. https://www.bonaccorso.eu/testimonials/
  14. https://www.bonaccorso.eu/gallery/
  15. https://www.bonaccorso.eu/disclaimer/
  16. https://www.bonaccorso.eu/blog/
  17. https://www.bonaccorso.eu/books/
  18. https://www.bonaccorso.eu/resume/
  19. https://www.bonaccorso.eu/bonaccorso-law/
  20. https://www.bonaccorso.eu/ai-cognitive-pychology-essays-italian/
  21. https://www.bonaccorso.eu/contact/
  22. https://www.bonaccorso.eu/testimonials/
  23. https://www.bonaccorso.eu/gallery/
  24. https://www.bonaccorso.eu/disclaimer/
  25. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/
  26. https://www.bonaccorso.eu/category/generic/artificial-intelligence/
  27. https://www.bonaccorso.eu/category/machine-learning/deep-learning/
  28. https://www.bonaccorso.eu/category/generic/
  29. https://www.bonaccorso.eu/category/machine-learning/deep-learning/keras/
  30. https://www.bonaccorso.eu/category/machine-learning/
  31. https://www.bonaccorso.eu/category/machine-learning/neural-networks/
  32. https://www.bonaccorso.eu/category/machine-learning/nlp/
  33. https://www.bonaccorso.eu/category/software/python/
  34. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/#comments
  35. https://github.com/giuseppebonaccorso/reuters-21578-classification/fork
  36. https://github.com/giuseppebonaccorso/reuters-21578-classification
  37. https://www.bonaccorso.eu/wp-content/uploads/2016/08/ywn4x1
  38. https://www.bonaccorso.eu/wp-content/uploads/2016/08/stochastic_gradient_descent1
  39. https://kdd.ics.uci.edu/databases/reuters21578/
  40. http://kdd.ics.uci.edu/databases/reuters21578/readme.txt
  41. https://radimrehurek.com/gensim/models/id97.html
  42. http://radimrehurek.com/2014/02/id97-tutorial/
  43. https://arxiv.org/pdf/1405.4053v2.pdf
  44. http://deeplearning.cs.cmu.edu/pdfs/hochreiter97_lstm.pdf
  45. https://www.bonaccorso.eu/wp-content/uploads/2016/08/280194005811
  46. https://www.bonaccorso.eu/wp-content/uploads/2016/08/photopin2.com
  47. https://www.bonaccorso.eu/wp-content/uploads/2016/08/22
  48. https://www.bonaccorso.eu/2017/08/07/twitter-sentiment-analysis-with-gensim-id97-and-keras-convolutional-networks/
  49. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=twitter
  50. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=facebook
  51. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=linkedin
  52. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=pocket
  53. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=tumblr
  54. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=reddit
  55. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=pinterest
  56. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=skype
  57. https://api.whatsapp.com/send?text=reuters-21578 text classification with gensim and keras https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/
  58. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=telegram
  59. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/?share=email
  60. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/#print
  61. https://www.bonaccorso.eu/tag/deep-learning/
  62. https://www.bonaccorso.eu/tag/gensim/
  63. https://www.bonaccorso.eu/tag/keras/
  64. https://www.bonaccorso.eu/tag/lstm/
  65. https://www.bonaccorso.eu/tag/nlp/
  66. https://www.bonaccorso.eu/tag/nltk/
  67. https://www.bonaccorso.eu/tag/id97/
  68. https://www.bonaccorso.eu/2016/07/16/fixed-delay-smoothing-id48-numpy/
  69. https://www.bonaccorso.eu/2016/08/06/cifar-10-image-classification-with-keras-convnet/
  70. https://www.bonaccorso.eu/wp-content/uploads/2016/08/reuters-21578-classification1
  71. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/#comment-34
  72. https://www.bonaccorso.eu/
  73. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/#comment-35
  74. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/#respond
  75. https://jetpack.wordpress.com/jetpack-comment/?blogid=100107841&postid=613&comment_registration=0&require_name_email=1&stc_enabled=1&stb_enabled=1&show_avatars=1&avatar_default=gravatar_default&greeting=leave+a+reply&greeting_reply=leave+a+reply+to+%s&color_scheme=light&lang=en_us&jetpack_version=7.0.1&show_cookie_consent=10&has_cookie_consent=0&sig=97c6b77de8b20ad66d463b829cce8e92c794044d#parent=https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/
  76. https://www.linkedin.com/in/giuseppebonaccorso/
  77. https://twitter.com/giuseppeb/
  78. https://www.facebook.com/giuseppe.bonaccorso/
  79. https://github.com/giuseppebonaccorso/
  80. https://www.instagram.com/giuseppebonaccorso/
  81. https://www.amazon.com/author/giuseppebonaccorso
  82. https://medium.com/@giuseppe.bonaccorso
  83. https://www.bonaccorso.eu/feed/
  84. https://www.bonaccorso.eu/2018/08/28/machine-learning-algorithms-second-edition/
  85. https://www.bonaccorso.eu/2018/07/10/recommendations-user-profiling-implicit-feedbacks/
  86. https://www.bonaccorso.eu/2018/06/29/recommendations-really-helpful-brief-non-technical-discussion/
  87. https://www.bonaccorso.eu/2018/06/22/a-book-that-every-data-scientist-should-read/
  88. https://www.bonaccorso.eu/2018/05/24/mastering-machine-learning-algorithms/
  89. https://twitter.com/giuseppeb
  90. https://www.bonaccorso.eu/
  91. https://www.iubenda.com/privacy-policy/331721
  92. https://www.iubenda.com/privacy-policy/331721/cookie-policy
  93. https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/#cancel

   hidden links:
  95. https://www.bonaccorso.eu/category/machine-learning/machine-learning-algorithms-addenda/
