   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]good audience
     * [9]     what is good audience?
     * [10]    signup for our newsletter
     * [11]       submit a story
     * [12]     get 100 free raven tokens
     __________________________________________________________________

neural networks part 1: a simple proof of the universal approximation theorem

   [13]go to the profile of joe klein
   [14]joe klein (button) blockedunblock (button) followfollowing
   aug 2, 2017

   from this function   
   [1*tidqvdvblrhxsoal3ohy4g.png]

   one neural network can extrapolate the function below. a single layer
   neural network can accurately represent any continuous function
   thinkable.
   [1*55qrlqcjjb0px9ujzjmzpq.png]

     a feedforward network with a single layer is sufficient to represent
     any function, but the layer may be infeasibly large and may fail to
     learn and generalize correctly.

         ian goodfellow, [15]dlb

   a grand statement, declaring any one layer neural network can solve
   almost any problem.
   [1*_7om4rgzytze10fxuzknca.png]

   the universal approximation theorem sparked the potential and
   functionality in neural networks we see today. a simple neural network
   including only a single hidden layer can approximate any continuous
   function. the sample function above, a randomized combination of sine
   functions, can easily be replicated by the neural network on the left,
   when properly configured. by adjusting weights (the lines in the
   diagram) and biases (a variable within each neuron in green on the
   diagram) a neural network can take any input for x and approximate it
   to fit the exact same line as the chaotically wavy line above.
   [1*pva_qfa60ckfvdijtxy-yg.png]

   by including n amount of neurons in a single hidden layer, a neural
   network can approximate an input of x for any function, f(x). this
   function must be continuous, if the function is not continuous a single
   hidden layer neural network does not easily or accurately approximate
   f(x). (we can easily fix this by adding another layer to mimic the
   output of a discontinuous function.)

   in other words, a single hidden layer neural network can approximate
   any continuous function of x to any degree of precision. this theorem
   fostered our understanding of neural networks, giving rise to the
   neural network of today and potentially tomorrow.

   iframe: [16]/media/748a06e9019942f2677bba917ef55804?postid=b7864964dbd3

     * [17]neural networks
     * [18]artificial intelligence
     * [19]machine learning
     * [20]technology
     * [21]deep learning

   (button)
   (button)
   (button) 48 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of joe klein

[23]joe klein

   data scientist and forward thinker

     (button) follow
   [24]good audience

[25]good audience

   the front page of deep tech. don't miss the latest advancements in
   artificial intelligence, machine learning, and blockchain. straight
   from practitioners.

     * (button)
       (button) 48
     * (button)
     *
     *

   [26]good audience
   never miss a story from good audience, when you sign up for medium.
   [27]learn more
   never miss a story from good audience
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://blog.goodaudience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b7864964dbd3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://blog.goodaudience.com/neural-networks-part-1-a-simple-proof-of-the-universal-approximation-theorem-b7864964dbd3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://blog.goodaudience.com/neural-networks-part-1-a-simple-proof-of-the-universal-approximation-theorem-b7864964dbd3&source=--------------------------nav_reg&operation=register
   8. https://blog.goodaudience.com/?source=logo-lo_msqql0vel84l---cb942d4b5d89
   9. https://blog.goodaudience.com/10-reasons-every-early-stage-company-needs-a-social-media-manager-995a96780204
  10. https://blog.goodaudience.com/dont-miss-the-latest-advancements-in-blockchain-crypto-and-artificial-intelligence-bbbf9d34b4a1
  11. https://blog.goodaudience.com/write-for-the-front-page-of-deep-tech-d63e2bc4fc63
  12. http://t.me/ravenprotocol
  13. https://blog.goodaudience.com/@jklein694?source=post_header_lockup
  14. https://blog.goodaudience.com/@jklein694
  15. http://www.deeplearningbook.org/contents/mlp.html
  16. https://blog.goodaudience.com/media/748a06e9019942f2677bba917ef55804?postid=b7864964dbd3
  17. https://blog.goodaudience.com/tagged/neural-networks?source=post
  18. https://blog.goodaudience.com/tagged/artificial-intelligence?source=post
  19. https://blog.goodaudience.com/tagged/machine-learning?source=post
  20. https://blog.goodaudience.com/tagged/technology?source=post
  21. https://blog.goodaudience.com/tagged/deep-learning?source=post
  22. https://blog.goodaudience.com/@jklein694?source=footer_card
  23. https://blog.goodaudience.com/@jklein694
  24. https://blog.goodaudience.com/?source=footer_card
  25. https://blog.goodaudience.com/?source=footer_card
  26. https://blog.goodaudience.com/
  27. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  29. https://medium.com/p/b7864964dbd3/share/twitter
  30. https://medium.com/p/b7864964dbd3/share/facebook
  31. https://medium.com/p/b7864964dbd3/share/twitter
  32. https://medium.com/p/b7864964dbd3/share/facebook
