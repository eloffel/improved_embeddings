   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep learning journal
   [7]deep learning journal
   [8]sign in[9]get started
     __________________________________________________________________

   [1*2r2vbwx60_ycfi2utrp-ew.jpeg]

faster ai: lesson 4    tl;dr version of fast.ai part 1

   [10]go to the profile of kshitiz rimal
   [11]kshitiz rimal (button) blockedunblock (button) followfollowing
   aug 30, 2017

     this is lesson 4 of a series called faster ai. if you haven   t read
     [12]lesson 0, [13]lesson 1, [14]lesson 2 and [15]lesson 3, please go
     through them first.

   in this lesson, we are going to learn about different types of
   optimization algorithms, semi supervised learning and collaborative
   filtering.

   as usual, for the sake of simplicity, i have divided this lesson into 4
   parts:
    1. id119 algorithms in excel [time: [16]11:28]
    2. state farm distracted driver detection competition [time:
       [17]56:57]
    3. pseudo labeling (semi supervised learning) [time: [18]1:23:45]
    4. id185 (path to nlp) [time: [19]1:36:01]

   before diving into these parts, jeremy quickly goes through
   convolutional networks in excel and there he shows how convolution
   works by explaining each part in detail using excel spreadsheets.
   [1*mdv38esm4oxabwy_u9otbw.png]

     [20]link to excel file

1. id119 algorithms in excel

   jeremy goes through all these algorithms in excel and explain to us how
   they work and gives us demos of all these algorithms inside excel.

sgd (stochastic id119)

   like we talked in our [21]previous lesson, sgd basically calculates the
   partial derivatives of the id168 with respect to the weights of
   the system and it is multiplied with a small number like 0.001, 0.0001,
   called learning rate and that resultant value is subtracted from the
   calculated value of the weights. after repeating this same process over
   each iteration the weights will be optimized and we can predict the
   desired value from the system.

momentum

   basically what momentum does is at every step of id119, it
   calculates the average value of each of those steps and this average
   value gives us the sense of direction on where the function is heading.
   this average values when multiplied with some number helps to add
   momentum like aspect to the whole optimization algorithm and speed up
   the process. instead of now just partial derivative this new calculated
   value is used to optimize the weights of the system.

adagrad

   it is often the case when single learning rate for whole system might
   not work, as sometimes it fails to correctly optimize all the
   parameters of the system. while one parameter might update properly,
   others might lag behind and gets slowly updated than required.

   to overcome that, adagrad is introduced. it uses a concept called
   dynamic learning rate, which basically means, provide different
   learning rates to each parameters of the system.

   in keras, when you execute model.summary(), it will display all the
   parameters of the system in each layer, so this adagrad provided
   dynamic learning rate to each of these parameters.

   due to this every parameter will update differently and process is much
   more optimized.

rmsprop

   it is first introduced by geoffrey hinton in his famous [22]coursera
   class. it uses similar concept as momentum does but instead of using
   average of running gradients, it uses square of the average of running
   gradients.

   one nice thing about this algorithm is that, it doesn   t overshoot or
   explodes while optimizing if the learning rate is too high, which is
   often the case with previous algorithms. so, instead of overshooting
   the value, it goes around it and slightly increase or decrease the
   value and keep it under desired range.

   it uses concept of average gradients from momentum and dynamic learning
   rate from adagrad.

adam

   to put it simply, adam is rmsprop + momentum.

   it uses both the concept of average of running gradients and square of
   the average of those gradients as rmsprop and uses momentum.

eve

   it is an addition to adam optimizers. it uses a concept called
   automatic learning rate annealing.

   learning rate annealing is the process of decreasing the learning rate
   as the process tends to reach near the optimum value of the loss
   function.

   eve automates this process by automatically decreasing the learning
   rate.

   it does so by keeping track of loss value from previous epoch, and the
   value before that epoch and calculates the ratio by which they are
   changing and if its too high then, the learning rate is decreased and
   if the loss is somewhat constant, it will increase the learning rate.

adam with annealing

   this is jeremy   s idea based on automatic annealing and adam optimizer.

   instead of using and comparing loss values from the epochs, he uses
   average of sum of squared gradients.

   he then compares the sum of squared gradients from previous epoch and
   current epoch, the gradients should always be decreasing and if it
   happens to be increasing, then the algorithm decreases the learning
   rate.

     the excel file used to demonstrate and explain above concepts is
     [23]available here.

2. state farm distracted driver detection competition

   jeremy here introduces another competition from kaggle, called state
   farm distracted driver detection. here bunch of images are given in
   both train and test sets and each shows the behavior of driver while
   driving. basically, if the driver is looking away from the road and
   looking else where, he is considered to be distracted and if he or she
   is looking forward, he or she is not distracted.

   jeremy then goes on to explain how he tackled this problem and came to
   a solution.

   he followed these steps to solve this problem.
    1. before using your model with real data set, you would want to test
       it on sample small dataset, which will save your time and helps to
       correct your model.
    2. start with simple model of 1 dense layer, make the first layer,
       batchid172 layer, this will automatically normalizes the
       input and no need to calculate all the standard deviations and
       subtract it from the input.
    3. always flatten the layers before the dense layer to put all the
       output from previous layer to a single vector form.
    4. when first training with the small sample data, with default
       learning rate, if it overshoots, decrease the learning rate.
    5. then after decreasing the learning rate substantially for the
       second run, if the accuracy starts to increase, this gives us the
       idea that if the accuracy doesn   t move at first with default
       learning rate, always decreasing it should be the first instinct.
    6. when the validation accuracy of that small sample model is around
       0.5 then its a good sign, else if its below that there is something
       wrong.
    7. if you are solving a id161 problem, obvious thing would
       be to use convolutional model architecture
    8. to avoid overfitting use data augmentation
    9. another best way to reduce overfitting by data augmentation would
       be to try different types of data augmentation one at a time on a
       sample data with big enough validation set and try to find the good
       value of each data augmentation parameters and try combining them
       all together.
   10. id173 cannot be used on sample data it is correlated with
       real data set. when we add more data, we need less id173
       so for a fixed sample data it will not work to check accuracy as
       the real data is more and id173 on that need to be
       reduced.
   11. you can use dropout to reduce overfitting.
   12. its always good idea to use id163 features if your problem
       involves dataset similar from id163.

     state farm code file is [24]available here.

3. pseudo labeling (semi supervised learning)

   particularly in state farm dataset there 80,000 data on test set, which
   are unlabelled. so, we can utilize this huge unlabeled data to our
   advantage by the use of pseudo labeling.

   this is how it works:
    1. use that unlabeled data on some model and predict labels of that
       data. they are now called pseudo labels
    2. now take training labels and concatenate them with pseudo labels.
    3. if you are using convolutional layers, concatenate the pseudo
       labels of convolutional validation set data with training features
    4. use these concatenated data as training data to train a new model.

   this approach increases the accuracy of the model and uses the
   unlabeled data to our advantage.

     more on semi supervised learning [25]here.

4. id185

   while building recommender system, two main concepts are used
    1. meta-data based approach
    2. id185

   by experimentations it is known that meta data based approach adds very
   little value to the whole recommender system and collaborative
   filtering amplifies the results by greater extent.

   in id185, if we are building a movie recommender
   system, it says, find similar people like you and find what they like
   and based on that, the system will assume you will like the same.

   now the lecture breaks down to two parts:
    1. concept

   as said above we need to understand the existing users to better
   recommend movies to another users. when we understand the current
   users, we will know how much similar is the targeted user from the
   existing ones.

   to do that, jeremy creates set of random values vector, these values
   conceptually represents the likeness or character of each user.

   now same approach is done with movies, similar random value vector is
   created to represent character of the movie.

   using these two vectors as weights, and rating of the movie as the real
   value, jeremy uses id119 to calculate his own rating for
   these movies. just like predicted y values from our previous linear
   function method.

   now using id168 and id119, the process is iterated
   and when the predicted rating of the movies are close to the real
   ratings of the movies, the weights are optimum and in our case the
   weights represents the character of users and movies. now based on
   these weights we can compare between users to better recommend movies
   to them using id185.

   2. implementation in keras

   jeremy applied above concept and implemented using pandas to better
   construct the dataset and uses keras to create the model.

   here in this case, he uses something called functional model, instead
   of our previously used sequential model.

   he uses embedding layers to better map the user characteristics with
   user ids and movies characteristics with movies ids.

   what embedding does is, by looking at the user id value from users
   table, it calls and grabs the particular column from the user
   characteristics matrix. if its user id: 1, then the embeddings will
   look up for first column and if its 3 then its the third one.

   then using this concept it is implemented using neural nets and it
   performed better than existing state of the art system on [26]movielens
   dataset. surprisingly, it took like only 10 seconds to train that
   model.

     id185 code file is [27]available here.

     all the notes of this lesson is [28]available here.

     i encourage you to watch the [29]full lecture. you can also jump to
     any particular topic on video by following [30]the video timeline.

     all the codes and excel files are [31]available here.

   in this lesson we went through optimizers and briefly touch the
   functional model in keras, which we will talk further in our next
   lesson.

   see you there.

[32]next post: lesson 5

     * [33]machine learning
     * [34]artificial intelligence
     * [35]deep learning

   (button)
   (button)
   (button) 12 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [36]go to the profile of kshitiz rimal

[37]kshitiz rimal

   ai developer, intel ai student ambassador, co-founder @ ai for
   development: ainepal.org, city ai ambassador: kathmandu

     (button) follow
   [38]deep learning journal

[39]deep learning journal

   deep learning lessons

     * (button)
       (button) 12
     * (button)
     *
     *

   [40]deep learning journal
   never miss a story from deep learning journal, when you sign up for
   medium. [41]learn more
   never miss a story from deep learning journal
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/70124cacd560
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-learning-journals?source=avatar-lo_uoipeiayhstm-aa98d6a0017e
   7. https://medium.com/deep-learning-journals?source=logo-lo_uoipeiayhstm---aa98d6a0017e
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-4-tl-dr-version-of-fast-ai-part-1-70124cacd560&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-4-tl-dr-version-of-fast-ai-part-1-70124cacd560&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@kshitizrimal?source=post_header_lockup
  11. https://medium.com/@kshitizrimal
  12. https://medium.com/deep-learning-journals/faster-ai-lesson-0-tl-dr-version-of-fast-ai-part-1-dd060ae7e319
  13. https://medium.com/deep-learning-journals/faster-ai-lesson-1-tl-dr-version-of-fast-ai-part-1-d63ad6b2993b
  14. https://medium.com/deep-learning-journals/faster-ai-lesson-2-tl-dr-version-of-fast-ai-part-1-f803218ffe6d
  15. https://medium.com/deep-learning-journals/faster-ai-lesson-3-tl-dr-version-of-fast-ai-part-1-d4e76c8c7ab6
  16. https://youtu.be/v2h3iobdvra?t=688
  17. https://youtu.be/v2h3iobdvra?t=3417
  18. https://youtu.be/v2h3iobdvra?t=5025
  19. https://youtu.be/v2h3iobdvra?t=5761
  20. https://github.com/fastai/courses/blob/master/deeplearning1/excel/conv-example.xlsx
  21. https://medium.com/deep-learning-journals/faster-ai-lesson-3-tl-dr-version-of-fast-ai-part-1-d4e76c8c7ab6
  22. https://www.google.com.np/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahukewjlqzdxw__vahumri8khqm8djgqfggmmaa&url=https://www.coursera.org/learn/neural-networks&usg=afqjcnhnathc_b9mj_mqjs5c49a07q69pw
  23. https://github.com/fastai/courses/blob/master/deeplearning1/excel/graddesc.xlsm
  24. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/statefarm.ipynb
  25. http://rinuboney.github.io/2016/01/19/ladder-network.html
  26. https://grouplens.org/datasets/movielens/
  27. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson4.ipynb
  28. http://wiki.fast.ai/index.php/lesson_4_notes
  29. https://youtu.be/v2h3iobdvra
  30. http://wiki.fast.ai/index.php/lesson_4_timeline
  31. https://github.com/fastai/courses/tree/master/deeplearning1
  32. https://medium.com/deep-learning-journals/faster-ai-lesson-5-tl-dr-version-of-fast-ai-part-1-59a6ed2f9b5f
  33. https://medium.com/tag/machine-learning?source=post
  34. https://medium.com/tag/artificial-intelligence?source=post
  35. https://medium.com/tag/deep-learning?source=post
  36. https://medium.com/@kshitizrimal?source=footer_card
  37. https://medium.com/@kshitizrimal
  38. https://medium.com/deep-learning-journals?source=footer_card
  39. https://medium.com/deep-learning-journals?source=footer_card
  40. https://medium.com/deep-learning-journals
  41. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  43. https://medium.com/p/70124cacd560/share/twitter
  44. https://medium.com/p/70124cacd560/share/facebook
  45. https://medium.com/p/70124cacd560/share/twitter
  46. https://medium.com/p/70124cacd560/share/facebook
