   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep learning journal
   [7]deep learning journal
   [8]sign in[9]get started
     __________________________________________________________________

   [1*2r2vbwx60_ycfi2utrp-ew.jpeg]

faster ai: lesson 3         tl;dr version of fast.ai part 1

   [10]go to the profile of kshitiz rimal
   [11]kshitiz rimal (button) blockedunblock (button) followfollowing
   aug 29, 2017
     __________________________________________________________________

     this is lesson 3 of a series called faster ai. if you haven   t read
     [12]lesson 0, [13]lesson 1 and [14]lesson 2, please go through them
     first.

   this lesson goes deeper into the concepts of convolutional neural
   networks and provides some techniques on how to improve your model.

   for the sake of simplicity, i have divided this lesson into 3 parts:
    1. id98 in detail [[15]time: 08:48]
    2. underfitting and overfitting [time: [16]1:12:52]
    3. reducing overfitting and maximizing accuracy of the model [time:
       [17]1:28:42]

1. id98 in detail

   on our [18]last lesson, we went through some visualizations showing
   what each layer of a convolutional neural network can learn from the
   input. in this lesson, jeremy talks about another such tool called,
   [19]deep visualization toolbox. using this tool, you can input your own
   image and go through each layer of a id98 model and see by yourself what
   each layer learned from it.
   [1*hhcrgqmpyrcihq27smpjtw.png]

   the purpose of all these tool is to provide you with some intuitive
   understanding of what a convolutional neural network does.

   to better understand such networks, lets understand what convolution
   means.

   let us take example of this image of letter 7, from mnist dataset.
   [1*xnhe4jwt4deoo5wicfjnua.png]

   previously, we talked about filters. now, these filters are some matrix
   with some numbers on it. in convolutional networks, what they do is,
   these typically 3x3 dimensional matrices, they slide across the input
   image from top to bottom, covering all the parts of the image. so, for
   example, this is one such type of filter
   [1*h07oststr6zwqetltbw8pa.png]

   here, this filter when slide across all of the pixels of above digit 7
   image, each 3x3 part of the pixel value of the image gets matrix
   multiplied with the filter. this process is called convolution.

   here is the resultant image after convolution of above filter with
   digit 7 image.
   [1*fjgbqhnvh8xg_74wd0ktza.png]

   if we zoom at some part of the original digit 7 image, before the
   convolution, this is what we get
   [1*stdey8erz65etdgdzcuhfq.png]

   now, if we look at the pixel value of this part, we see
   [1*7tcls5jp5pydhnxvt5kfvw.png]

   as you can see this value is multiplied with that above filter and the
   resultant output of that particular part is
   [1*tbdb7f0bjlqhi2m1vdkvzq.png]

   in a convolutional neural network, one layer has more than one such
   kind of filters.

   although, in this case jeremy predefined the value of a filter for
   demonstration, it is not the case with an actual neural network.

   in order to find the optimum value of a filter to properly convolve
   across the image and to actually learn something from that convolution,
   these filters are randomly initialized at first. and with each
   iteration of training, the stochastic id119 algorithm,
   optimizes this value of the filter and after certain iterations, we get
   the optimum value of the filters which can successfully understand the
   input image and learns from it.

   you can visit this page for more information about filters and what are
   some popular filters used today: [20]http://setosa.io/ev/image-kernels/

pooling

   pooling is another important part of a convolutional neural network.
   after a convolution, these filters they create something called feature
   maps, like the above digit 7 image after convolution, which are the
   learned outputs. these feature maps, they need to be reduced in
   resolution in order to be processed in a faster way by other layers of
   the network. this process of reducing these feature maps is called
   pooling.

   generally, max pooling is one the famous pooling method used in id98.
   what it does is, suppose this max pool has 5x5 dimension, so like
   filter, it starts at the top of the output image and it selects the
   maximum pixel value out of this 5x5 block on that image it placed on.
   it repeats this process across the whole image or feature map, picking
   only 1 value out of 5x5 block and reducing the resolution
   significantly. and this new matrix after max pool is what feed into
   subsequent layers of the network.

   the reason we do max pooling is because, it is harder to convolve
   across large resolution images on deeper layer of id98, but we also need
   to understand and learn from these images, so what it does is, it
   reduces the resolution on each max pool but, after each max pool, on
   another layer, we double the number of the filters used in this layer
   with respect to the previous layer.

   this results in lowering the resolution but increasing the dimension of
   the feature maps, making the network even better at recognizing and
   learning images, because now with less number of convolution, network
   can learn greater details of an image with the help of this higher
   dimensions.

   another important aspect of max pooling is that, because of max
   pooling, suppose you want to recognize cats in an image, now regardless
   of wherever that cat is placed on your image, that is at top, right,
   bottom or middle, id98 can recognize that, making these networks
   position invariant. and at the same time, certain aspects of the image
   needs to be at fixed position as well, like, in case of a person, eyes
   needs to be closer to the nose. that can also be recognized because of
   max pooling, making networks position invariant as well as fixed
   position learner, depending on input image.

   here is an image showing resolution reduction after max pooling.
   [1*dm3ep-bq39dc_hoshx7htw.png]

padding

   padding in simplest terms means, to add a border of zero around the
   image, making the result after a convolution with same output size as
   the input size.

   suppose we have 3x3 filter. when id98 without padding is used, we end up
   losing 2 pixels at the sides or from edges of the image, each time we
   convolve. to overcome that padding is used.

id98 architectures

   generally, in a convolutional neural network, following hierarchy is
   used:
    1. input layer
    2. convolutional layers
    3. dense (fully connected layers)

   we can see such implementation in keras as:
   [1*_0w7aqn_cj6vkrgtip0umq.png]

   here at third line of code, we are defining input layer of 3 channel,
   and 224x224 resolution or dimension. following that, we have 5
   convolutional blocks with layer number as first argument and number of
   filters as second argument. after convolutional block we have linear
   blocks at the end.

   inside these convblock, this is what we have:
   [1*hi4yffv_fq4yp27hfgz1nw.png]

   here we are adding convolutional layer and zero padding to each layer
   of block. after that at the end, we have max pool layer to pool from
   the last convolution layer.

softmax

   generally, in classification problems involving more than 2 classes,
   softmax function at the last layer of the network is used to give
   highest id203 value to one class and lowest values to others.
   these values when added, sums up to 1.

   this highest value classes is the predicted class of the input image.

stochastic id119 (sgd)

   basic difference between id119 and sgd is that, sgd utilizes
   the batch or mini-batch of neural network, whereas id119 in
   generally are done on whole training set.

   if we remember from our last lesson, and the example of linear problem
   involving a straight line, we went through the concept of gradient
   descent there as well.

   with sgd what we are doing is, we are calculating the derivative of a
   id168 with respect to weights (a and b) and these derivatives
   gets multiplied with small fractional number like 0.1, 0.001, 0.0001
   called learning rates. the resultant small number then is subtracted
   from the randomly initialized weights of the network.

   repeating this process each iteration, gives the optimum value of the
   weight and so the approximate function to calculate the equation of the
   straight line.

   this same concept when applied to a neural network involving hundreds
   of parameters instead of just 2 in our previous case, works the same
   way only the operation is complex. so, to understand it intuitively, we
   have know that the basic function is the same, just parameters are more
   in deep neural networks.

2. underfitting and overfitting

   there are two reasons for a model to be not good.
    1. underfitting
    2. overfitting

   underfitting is when you are using a less powerful model to solve
   certain problem or using less parameters than it needs in a model. for
   example using linear model to classify images.

   overfitting means you are using too many parameters in a model than it
   needs. it results in overfitting when your model is too much trained on
   your training data and fails to perform on your test set. it will learn
   too much details of your training data only and fails to recognize
   general patterns in test data. one way to check it is by looking at
   training accuracy and validation accuracy, if training accuracy is
   more, then its overfitting.

dropout

   dropout is one of the technique to reduce overfitting. what it does is,
   it randomly kills the activation of a layer by certain percentage,
   making the network impossible to overfit on a given task. this
   technique of killing good activation neurons on random, surprisingly
   works fine to increase the accuracy of the network without overfitting
   it.

   dropout is mainly put on last layers because, putting it on early
   layers makes the network to lose information on input data, which is
   not a good sign, instead we can lose some activations later on to
   reduce overfitting.

3. reducing overfitting and maximizing accuracy of the model

   there are five ways other than dropout to reduce overfitting. they are
    1. add more data to the network
    2. add data augmentation
    3. use architecture that generalize well
    4. add id173
    5. reduce architecture complexity

   among them lets talk about data augmentation

data augmentation

   data augmentation means to modify the training data by applying certain
   dynamic function on it, which randomly rotates it, flips it, zoom it,
   modify its colors and other properties so that the model can reduce
   overfitting.

   what type of augmentation should be used, is purely based on intuition
   and experimentation. if applying certain augmentation on data makes
   sense then you should definitely apply and until the right augmentation
   performs better, use different types.

   one example of data augmentation is this, suppose we have a picture of
   a cat
   [1*mjpanq5ygiauu2uct5etwq.png]

   after data augmentation, this is what we will get
   [1*ylzwelne0w6stkztx_wnqq.png]

batch id172

   to normalize any input means to subtract that input by its mean and
   then divide that by its standard deviations. what it does is, it makes
   the input values of a network, of same scale and range, this makes the
   weights of the network uniform and so the activations will be uniform,
   ultimately making network to perform well.

   but id172 on activation layer is also of equal importance as
   sometimes even after normalizing the input, some activations are little
   bit higher than the rest making the subsequent layers to act abnormally
   and which ultimately makes the network fragile.

   so id172 on these activation layers is batch id172. it
   is done in a similar way like input id172 on these layers but
   with 2 extra parameters which are optimizable with id119.
   making the whole id172 process friendly on each iteration and
   provides us with uniform activations.

   another great thing about batch id172 is that it speeds up the
   training process by 10 times and helps in reducing overfitting as well.

   in keras you can use it by simple calling batchid172() after
   the layers of your model. its good if you can put it after every layer
   of your model.

   you can view the code of [21]above implementations here

end to end mnist

   while explaining all these fantastic techniques to reduce overfitting,
   jeremy shows us how to apply them on mnist dataset. he does so my
   sequentially going through each technique one after another on same
   model in following way:
    1. he first tried to classify mnist data on a linear model
    2. then he added one hidden layer to that model and tried to classify
       on that
    3. after that he applied it on convolutional neural network
    4. under convolutional neural network, he applied these techniques to
       reduce overfitting : a) data augmentation b) batch id172 +
       data augmentation c) batch id172 + data augmentation +
       dropout

   you can view the code of [22]above implementation here.

ensembling

   ensembling in simple terms means combining different models to increase
   the overall accuracy, making the network perform well to solve
   particular problem.

   here in this case jeremy took all the code from above with different
   learning rates and wrap it in a function called fit_model().
   [1*liek-5bmuam48a_opwbtaw.png]

   here different learning rates are given to the model after certain
   epochs. now this function is trained 6 times and all the results from
   this six times training is put on a list, called models.

   now let us get the predictions from each of these models as a list
   [1*rv_ie1bdgjdi98-s4ookna.png]

   now, if we check the shape of the prediction, what we get is
   [1*gwznbx_sasertjw0hb7kug.png]

   here, 6 is the number of the models, 1000 is the test images and 10 are
   the outputs

   now, if we take mean of these predictions and calculate accuracy on
   them, we get this
   [1*-eta7q4eqessklybj7sj0w.png]
   [1*squqqkqjecjllghnx1n6-g.png]

   around 99.7% accuracy, which is much better result and we can see it
   doesn   t take much to come to this level of accuracy.

   all the code of [23]above implementation is here.

   here is the [24]link to the actual video

   here are [25]the notes of this lesson

   if you want to jump to any particular topic and watch video from there,
   here is [26]the link to the video timeline

   in this lesson, we went through convolutional neural network in detail
   and ways to improve it. in our next lesson, we will go through
   different types of optimizers and we will learn about collaborative
   filtering.

   see you there.

[27]next post: lesson 4

     * [28]machine learning
     * [29]artificial intelligence
     * [30]deep learning

   (button)
   (button)
   (button) 38 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of kshitiz rimal

[32]kshitiz rimal

   ai developer, intel ai student ambassador, co-founder @ ai for
   development: ainepal.org, city ai ambassador: kathmandu

     (button) follow
   [33]deep learning journal

[34]deep learning journal

   deep learning lessons

     * (button)
       (button) 38
     * (button)
     *
     *

   [35]deep learning journal
   never miss a story from deep learning journal, when you sign up for
   medium. [36]learn more
   never miss a story from deep learning journal
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d4e76c8c7ab6
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-learning-journals?source=avatar-lo_suzogi2zb9df-aa98d6a0017e
   7. https://medium.com/deep-learning-journals?source=logo-lo_suzogi2zb9df---aa98d6a0017e
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-3-tl-dr-version-of-fast-ai-part-1-d4e76c8c7ab6&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-learning-journals/faster-ai-lesson-3-tl-dr-version-of-fast-ai-part-1-d4e76c8c7ab6&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@kshitizrimal?source=post_header_lockup
  11. https://medium.com/@kshitizrimal
  12. https://medium.com/deep-learning-journals/faster-ai-lesson-0-tl-dr-version-of-fast-ai-part-1-dd060ae7e319
  13. https://medium.com/deep-learning-journals/faster-ai-lesson-1-tl-dr-version-of-fast-ai-part-1-d63ad6b2993b
  14. https://medium.com/deep-learning-journals/faster-ai-lesson-2-tl-dr-version-of-fast-ai-part-1-f803218ffe6d
  15. https://youtu.be/6kwqebmandw?t=528
  16. https://youtu.be/6kwqebmandw?t=4372
  17. https://youtu.be/6kwqebmandw?t=5322
  18. https://medium.com/deep-learning-journals/faster-ai-lesson-2-tl-dr-version-of-fast-ai-part-1-f803218ffe6d
  19. http://yosinski.com/deepvis
  20. http://setosa.io/ev/image-kernels/
  21. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson3.ipynb
  22. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/mnist.ipynb
  23. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/mnist.ipynb
  24. https://www.youtube.com/watch?v=6kwqebmandw
  25. http://wiki.fast.ai/index.php/lesson_3_notes
  26. http://wiki.fast.ai/index.php/lesson_3_video_timeline
  27. https://medium.com/deep-learning-journals/faster-ai-lesson-4-tl-dr-version-of-fast-ai-part-1-70124cacd560
  28. https://medium.com/tag/machine-learning?source=post
  29. https://medium.com/tag/artificial-intelligence?source=post
  30. https://medium.com/tag/deep-learning?source=post
  31. https://medium.com/@kshitizrimal?source=footer_card
  32. https://medium.com/@kshitizrimal
  33. https://medium.com/deep-learning-journals?source=footer_card
  34. https://medium.com/deep-learning-journals?source=footer_card
  35. https://medium.com/deep-learning-journals
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/d4e76c8c7ab6/share/twitter
  39. https://medium.com/p/d4e76c8c7ab6/share/facebook
  40. https://medium.com/p/d4e76c8c7ab6/share/twitter
  41. https://medium.com/p/d4e76c8c7ab6/share/facebook
