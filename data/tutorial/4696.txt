   #[1]probabilistic graphical modeling course

   [2]contents [3]class [4]github

                                    contents

   these notes form a concise introductory course on probabilistic
   id114 [ ] probabilistic id114 are a subfield of
   machine learning that studies how to describe and reason about the
   world in terms of probabilities.. they are based on stanford [5]cs228,
   and are written by [6]volodymyr kuleshov and [7]stefano ermon, with the
   [8]help of many students and course staff.     [ ] the notes are still
   under construction! although we have written up most of the material,
   you will probably find several typos. if you do, please let us know, or
   submit a pull request with your fixes to our [9]github repository. you
   too may help make these notes better by submitting your improvements to
   us via [10]github.

   this course starts by introducing probabilistic id114 from
   the very basics and concludes by explaining from first principles the
   [11]variational auto-encoder, an important probabilistic model that is
   also one of the most influential recent results in deep learning.

preliminaries

    1. [12]introduction: what is probabilistic graphical modeling?
       overview of the course.
    2. [13]review of id203 theory: id203 distributions.
       id155. random variables (under construction).
    3. [14]examples of real-world applications: image denoising. rna
       structure prediction. syntactic analysis of sentences. optical
       character recognition (under construction).

representation

    1. [15]id110s: definitions. representations via directed
       graphs. independencies in directed models.
    2. [16]markov random fields: undirected vs directed models.
       independencies in undirected models. id49.

id136

    1. [17]variable elimination the id136 problem. variable
       elimination. complexity of id136.
    2. [18]belief propagation: the junction tree algorithm. exact
       id136 in arbitrary graphs. loopy belief propagation.
    3. [19]map id136: max-sum message passing. graphcuts. linear
       programming relaxations. id209.
    4. [20]sampling-based id136: monte-carlo sampling. forward
       sampling. rejection sampling. importance sampling. markov chain
       monte-carlo. applications in id136.
    5. [21]variational id136: variational lower bounds. mean field.
       marginal polytope and its relaxations.

learning

    1. [22]learning in directed models: id113.
       learning theory basics. maximum likelihood estimators for bayesian
       networks.
    2. [23]learning in undirected models: exponential families. maximum
       likelihood estimation with id119. learning in crfs
    3. [24]learning in latent variable models: latent variable models.
       gaussian mixture models. expectation maximization.
    4. [25]bayesian learning: bayesian paradigm. conjugate priors.
       examples (under construction).
    5. [26]structure learning: chow-liu algorithm. akaike information
       criterion. bayesian information criterion. bayesian structure
       learning (under construction).

bringing it all together

    1. [27]the variational autoencoder: deep generative models. the
       reparametrization trick. learning latent visual representations.
    2. [28]list of further readings: structured support vector machines.
       bayesian non-parametrics.

   contents - volodymyr kuleshov
     __________________________________________________________________

   site created with [29]jekyll using the [30]tufte theme.    2019

references

   1. https://ermongroup.github.io/cs228-notes/feed.xml
   2. https://ermongroup.github.io/cs228-notes/
   3. http://cs228.stanford.edu/
   4. http://github.com/ermongroup/cs228-notes
   5. https://cs228.stanford.edu/
   6. http://www.stanford.edu/~kuleshov
   7. http://cs.stanford.edu/~ermon/
   8. https://github.com/ermongroup/cs228-notes/commits/master
   9. https://github.com/ermongroup/cs228-notes
  10. https://github.com/ermongroup/cs228-notes
  11. https://ermongroup.github.io/cs228-notes/extras/vae
  12. https://ermongroup.github.io/cs228-notes/preliminaries/introduction/
  13. https://ermongroup.github.io/cs228-notes/preliminaries/id203review
  14. https://ermongroup.github.io/cs228-notes/preliminaries/applications
  15. https://ermongroup.github.io/cs228-notes/representation/directed/
  16. https://ermongroup.github.io/cs228-notes/representation/undirected/
  17. https://ermongroup.github.io/cs228-notes/id136/ve/
  18. https://ermongroup.github.io/cs228-notes/id136/jt/
  19. https://ermongroup.github.io/cs228-notes/id136/map/
  20. https://ermongroup.github.io/cs228-notes/id136/sampling/
  21. https://ermongroup.github.io/cs228-notes/id136/variational/
  22. https://ermongroup.github.io/cs228-notes/learning/directed/
  23. https://ermongroup.github.io/cs228-notes/learning/undirected/
  24. https://ermongroup.github.io/cs228-notes/learning/latent/
  25. https://ermongroup.github.io/cs228-notes/learning/bayesian/
  26. https://ermongroup.github.io/cs228-notes/learning/structure/
  27. https://ermongroup.github.io/cs228-notes/extras/vae
  28. https://ermongroup.github.io/cs228-notes/extras/readings
  29. https://jekyllrb.com/
  30. https://github.com/clayh53/tufte-jekyll
