   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a wizard   s guide to adversarial autoencoders: part 2, exploring latent space
with adversarial autoencoders.

   [16]go to the profile of naresh nagabushan
   [17]naresh nagabushan (button) blockedunblock (button) followfollowing
   aug 7, 2017
   [1*kahlz-y-nu86lyxltnus3a.png]

      this article is a continuation from [18]a wizard   s guide to
   autoencoders: part 1, if you haven   t read it but are familiar with the
   basics of autoencoders then continue on. you   ll need to know a little
   bit about id203 theory which can be found [19]here.   

   part 1: [20]autoencoder?

   we left off part 1 by passing a value (0, 0) to our trained decoder
   (which has 2 neurons at the input) and finding its output. it looked
   blurry and didn   t represent a clear digit leaving us with the
   conclusion that the output of the encoder h (also known as the latent
   code) was not distributed evenly in a particular space.
   [1*e6rbh-75bz5eudfvtmdceg.png]

   so, our main aim in this part will be to force the encoder output to
   match a given prior distribution, this required distribution can be a
   normal (gaussian) distribution, uniform distribution, gamma
   distribution    . this should then cause the latent code (encoder output)
   to be evenly distributed over the given prior distribution, which would
   allow our decoder to learn a mapping from the prior to a data
   distribution (distribution of mnist images in our case).

     if you understood absolutely nothing from the above paragraph.

   let   s say you   re in college and have opted to take up machine learning
   (i couldn   t think of another course :p) as one of your courses. now, if
   the course instructor doesn   t provide a syllabus guide or a reference
   book then, what will you study for your finals? (assume your classes
   weren   t helpful).

   you could be asked questions from any subfield of ml, what would you
   do? makeup stuff with what you know??

   this is what happens if we don   t constrain our encoder output to follow
   some distribution, the decoder cannot learn a mapping from any number
   to an image.

   but, if you are given a proper syllabus guide then, you can just go
   through the materials before the exam and you   ll have an idea of what
   to expect.

   similarly, if we force the encoder output to follow a known
   distribution like a gaussian, then it can learn to spread the latent
   code to cover the entire distribution and learn mappings without any
   gap.
   [1*k0vubeegmkcbbap_q_hewq.png]
   good or bad?
     __________________________________________________________________

   we now know that an autoencoder has two parts, each performing a
   completely opposite task.

     two people of similar nature can never get alone, it takes two
     opposites to harmonize.

         ram mohan

   the encoder which is used to get a latent code (encoder output) from
   the input with the constraint that the dimension of the latent code
   should be less than the input dimension and secondly, the decoder that
   takes in this latent code and tries to reconstruct the original image.
   [1*e8lvf2xgg4udsd9wutnd9g.png]
   autoencoder block diagram

   let   s see how the encoder output was distributed when we previously
   implemented our autoencoder (checkout [21]part 1):
   [1*lsykon0k8rjgpm5vz0yysg.png]
   encoder histogram and distribution

   from the distribution graph (which is towards the right) we can clearly
   see that our encoder   s output distribution is all over the place.
   initially, it appears as though the distribution is centred at 0 with
   most of the values being negative. at later stages during training the
   negative samples are distributed farther away from 0 when compared to
   the positive ones (also, we might not even get the same distribution if
   we run the experiment again). this leads to large amounts of gaps in
   the encoder distribution which isn   t a good thing if we want to use our
   decoder as a generative model.

     but, why are these gaps a bad thing to have in our encoder
     distribution?

   if we give an input that falls in this gap to a trained decoder then
   it   ll give weird looking images which don   t represent digits at its
   output (i know, 3rd time).

   another important observation that was made is that training an
   autoencoder gives us latent codes with similar images (for example all
   2s or 3s ..) being far from each other in the euclidean space. this,
   for example, can cause all the 2s in our dataset to be mapped to
   different regions in space. we want the latent code to have a
   meaningful representation by keeping images of similar digits close
   together. some thing like this:
   [1*4rlc6rbln8aod3xdzrjhsa.png]
   a good 2d distribution

   different colored regions represent one class of images, notice how the
   same colored regions are close to one another.

   we now look at adversarial autoencoders that can solve some of the
   above mentioned problems.
     __________________________________________________________________

   an adversarial autoencoder is quite similar to an autoencoder but the
   encoder is trained in an adversarial manner to force it to output a
   required distribution.

   understanding adversarial autoencoders (aaes) requires knowledge of
   id3 (gans), i have written an article on
   gans which can be found here:
   [22]gans n    roses
      this article assumes the reader is familiar with neural networks and
   using tensorflow. if not, we would request you to   medium.com

   if you already know about gans here   s a quick recap (feel free to skip
   this section if you remember the next two points):
   [1*it0rkkxp5yjrqcyczew_kw.png]
   discriminator
   [1*d6j2k23kkughjebgjfth2a.png]
   generator
     * gans have two neural nets, a generator and a discriminator.
     * generator, well generates fake images. we train the discriminator
       to tell apart real images from our dataset with the fake ones
       generated by our generator.
     * the generator initially generates some random noise (because it   s
       weights will be random). after training our discriminator to
       discriminate this random noise and real images, we   ll connect our
       generator to our discriminator and backprop only through the
       generator with the constraint that the discriminator output should
       be 1 (i.e, the discriminator should classify the output of the
       generator as real images).
     * we   ll again train our discriminator to now tell apart the new fake
       images from our generator and the real ones from our database. this
       is followed by training the generator to generate better fake
       looking images.
     * we   ll continue this process until the generator becomes so good at
       generating fake images that the discriminator is no longer able to
       tell real images from fake ones.
     * at the end, we   ll be left with a generator which can produce real
       looking fake images given a random set of numbers as its input.
     __________________________________________________________________

   here   s a block diagram of an adversarial autoencoder:
   [1*nnf4uuq9uuf2l19icyanfg.png]
   aae block diagram
     * x     input image
     * q(z/x)     encoder output given input x
     * z     latent code (fake input), z is drawn from q(z/x)
     * z        real input with the required distribution
     * p(x/z)    decoder output given z
     * d()     discriminator
     * x_    reconstructed image

   again, our main is to force the encoder to output values which have a
   given prior distribution (this can be normal, gamma .. distributions).
   we   ll use the encoder (q(z/x)) as our generator, the discriminator to
   tell if the samples are from a prior distribution (p(z)) or from the
   output of the encoder (z) and the decoder (p(x/z)) to get back the
   original input image.

   to get an understanding of how this architecture could be used to
   impose a prior distribution on the encoder output, lets have a look at
   how we go about training an aae.

   training an aae has 2 phases:
     * reconstruction phase:

   we   ll train both the encoder and the decoder to minimize the
   reconstruction loss (mean squared error between the input and the
   decoder output images, checkout part 1 for more details). forget that
   the discriminator even exists in this phase (i   ve greyed out the parts
   that aren   t required in this phase).
   [1*dkpl7yonx-8fjquhazop-g.png]
   reconstruction phase

   as usual we   ll pass inputs to the encoder which will give us our latent
   code, later, we   ll pass this latent code to the decoder to get back the
   input image. we   ll backprop through both the encoder and the decoder
   weights so that reconstruction loss will be reduced.
     * id173 phase:

   in this phase we   ll have to train the discriminator and the generator
   (which is nothing but our encoder). just forget that the decoder
   exists.
   [1*_pixkcccqjrnmiwtrymjza.png]
   training the discriminator

   first, we train the discriminator to classify the encoder output (z)
   and some random input(z   , this will have our required distribution).
   for example, the random input can be normally distributed with a mean
   of 0 and standard deviation of 5.

   so, the discriminator should give us an output 1 if we pass in random
   inputs with the desired distribution (real values) and should give us
   an output 0 (fake values) when we pass in the encoder output.
   intuitively, both the encoder output and the random inputs to the
   discriminator should have the same size.

   the next step will be to force the encoder to output latent code with
   the desired distribution. to accomplish this we   ll connect the encoder
   output as the input to the discriminator:
   [1*dojesn2lvxpxnvyadrjxww.png]

   we   ll fix the discriminator weights to whatever they are currently
   (make them untrainable) and fix the target to 1 at the discriminator
   output. later, we pass in images to the encoder and find the
   discriminator output which is then used to find the loss (cross-id178
   cost function). we   ll backprop only through the encoder weights, which
   causes the encoder to learn the required distribution and produce
   output which   ll have that distribution (fixing the discriminator target
   to 1 should cause the encoder to learn the required distribution by
   looking at the discriminator weights).
     __________________________________________________________________

   now that the theoretical part of out of the way, let   s have a look at
   how we can implement this using tensorflow.

   here   s the entire code for part 2 (it   s very similar to what we   ve
   discussed in part 1):
   [23]naresh1318/adversarial_autoencoder
   adversarial_autoencoder - a wizard's guide to adversarial
   autoencodersgithub.com

   as usual we have our helper dense() :

   iframe: [24]/media/40914b58fe7c2eb3bbbd3f8bb98ec659?postid=2d53a6f8a4f9

   i haven   t changed the encoder and the decoder architectures:
   [1*hud7t2vly2jip3sxn4wtda.png]
   encoder architecture

   iframe: [25]/media/a1660445a2e9f7c2cbdb9efd386ee3ac?postid=2d53a6f8a4f9

   [1*0t7jrvuqyzg7adqgdjzkrw.png]
   decoder architecture

   iframe: [26]/media/9492214a2d4f09d55c7aa9565a4627aa?postid=2d53a6f8a4f9

   here   s the discriminator architecture:
   [1*df3_l66bezqsqre5i6lzrw.png]
   discriminator architecture

   it   s similar to our encoder architecture, the input shape is z_dim
   (batch_size, z_dim actually) and the output has a shape of 1
   (batch_size, 1 ).

   iframe: [27]/media/a4cba039d81b6c843088cc8ffd6e1e71?postid=2d53a6f8a4f9

   note that i   ve used the prefixes e_ , d_ and dc_ while defining the
   dense layers for the encoder, decoder and discriminator respectively.
   using these notations help us collect the weights to be trained easily:

   iframe: [28]/media/0f2df040c66b57210b8bff6974160c79?postid=2d53a6f8a4f9

   we now know that training an aae has two parts, first being the
   reconstruction phase (we   ll train our autoencoder to reconstruct the
   input) and the id173 phase (first the discriminator is trained
   followed by the encoder).

   we   ll begin the reconstruction phase by connecting our encoder output
   to the decoder input:

   iframe: [29]/media/1898430d8d1b887f896e011318723565?postid=2d53a6f8a4f9

   i   ve used tf.variable_scope(tf.get_variable_scope()) each time i call
   any of our defined architectures as it   ll allow us to share the weights
   among all function calls (this happens only if reuse=true ).

   the id168 as usual is the mean squared error (mse), which we   ve
   come across in [30]part 1.

   iframe: [31]/media/d6c3cd6b23e807df99b52ccde0771ce3?postid=2d53a6f8a4f9

   similar to what we did in part 1, the optimizer (which   ll update the
   weights to reduce the loss[hopefully]) is implemented as follows:

   iframe: [32]/media/71698b762082ba1c44e81d2c8e1b8d21?postid=2d53a6f8a4f9

   iframe: [33]/media/44e2280b73d4f0fa1ec12cf801310969?postid=2d53a6f8a4f9

   i couldn   t help it :p

   that   s it for the reconstruction phase, next we move on to the
   id173 phase:

   we   ll first train the discriminator to distinguish between the real
   distribution samples and the fake ones from the generator (encoder in
   this case).

   iframe: [34]/media/a10bfdd9f4c973ed7f6e0f9b4085b0f4?postid=2d53a6f8a4f9

     * real_distribution is a placeholder which i   ve used to pass in
       values with the required distribution to the discriminator (this
       will be our real input).
     * encoder_output is connected to the discriminator which   ll give us
       our discriminator output for fake inputs d_fake .
     * here reuse=true since we want the same discriminator weights in the
       second call (if this is not specified, then tensorflow creates new
       set of randomly initialized weights [but since i   ve used
       get_variable() to create the weights it   ll through an error]).

   the id168 i   ve used to train the discriminator is:
   [1*y4e01pcqs1shxfdnp9mxyw.png]
   cross id178 cost

   this can easily be implemented in tensorflow as follows:

   iframe: [35]/media/1963ef9a598833e11a2a3b0f499f91c5?postid=2d53a6f8a4f9

   next step will be to train the generator (encoder) to output a required
   distribution. as we have discussed, this requires the discrminator   s
   target to be set to 1 and the d_fake variable (the encoder connected to
   the discriminator [go back and look]). the generator loss is again
   cross id178 cost function.

   iframe: [36]/media/e4c104ee03920689207194ffaaafd94e?postid=2d53a6f8a4f9

   to update only the required weights during training we   ll need to pass
   in all those collected weights to the var_list parameter under
   minimize() . so, i   ve passed in the discriminator variables (dc_var)
   and the generator (encoder) variables (en_var) during their training
   phases.

   iframe: [37]/media/62e778d6bc8b804d6a6a2d860463bc7b?postid=2d53a6f8a4f9

   we   re almost done, all we have left is to pass our mnist images as the
   input and as target along with random numbers of size batch_size, z_dim
   as inputs to the discriminator (this will form the required
   distribution).

   the training part might look intimidating, but stare at it for a while
   you   ll find out that it   s quite intuitive.

   iframe: [38]/media/9b80c9bbdeadb10a70f1f3a59031abcb?postid=2d53a6f8a4f9

   the parameters i   ve used during training are as follows:

   iframe: [39]/media/7358b8d9f460e68910ba5a1281d45762?postid=2d53a6f8a4f9

   i trained the model for 300 epochs with required distribution begin a
   normal (gaussian) having mean 0 and 5 as it   s standard distribution.
   here   s the encoder output and the required distributions along with
   their histograms:
   [1*l0qbcvnntiub5i_nk9uq9q.png]

   the encoder distribution almost matches the required distribution and
   the histogram shows us that it   s centred at zero. great, but what about
   the discriminator, how well has it fared?
   [1*f-w72tq49-_xn9ohzubkwg.png]

   good news, the discriminator loss is increasing (getting worst) which
   tells us it   s having a hard time telling apart real and fake inputs.

   lastly, since we have our z_dim=2 (2d values) we can pass in random
   inputs which fall under the required distribution to our trained
   decoder and use it as a generator (i know, i was calling encoder as the
   generator all this while as it was generating fake inputs to the
   discriminator, but since the decoder has learn   t to map these fake
   inputs to get digits at its output we can call our decoder as a
   generator). i   ve passed values from (-10, -10) to (10, 10) at regular
   intervals to the decoder and stored its outputs here   s how the digits
   have been distributed:
   [0*xhrzt9rjgnbvyo6v.png]

   the above figure shows a clear id91 of digits and their
   transition as we explore values that the decoder is trained at. an aae
   has cause the gaps in the encoder output distribution to get closer
   which allowed us to use the decoder as a generator.

   that   s it!. we   ll focus on how we can use aae to separate image style
   from it   s content in the next part. it   s quite easy to implement it
   since we are done most of the relatively tough parts.
     __________________________________________________________________

   hope you liked this article on aaes. i would openly encourage any
   criticism or suggestions to improve my work.

   if you think this content is worth sharing hit the       , i like the
   notifications it sends me!!

       part 1: [40]autoencoder?

       part 3: [41]disentanglement of style and content.

     * [42]machine learning
     * [43]tensorflow
     * [44]artificial intelligence
     * [45]neural networks

   (button)
   (button)
   (button) 994 claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [46]go to the profile of naresh nagabushan

[47]naresh nagabushan

   master   s student [48]@virginia_tech interested in all things ai.

     (button) follow
   [49]towards data science

[50]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 994
     * (button)
     *
     *

   [51]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [52]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2d53a6f8a4f9
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_gv1cfut0nkht---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@rnaresh.n?source=post_header_lockup
  17. https://towardsdatascience.com/@rnaresh.n
  18. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4
  19. http://www.deeplearningbook.org/contents/prob.html
  20. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4
  21. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4
  22. https://medium.com/towards-data-science/gans-n-roses-c6652d513260
  23. https://github.com/naresh1318/adversarial_autoencoder/blob/master/adversarial_autoencoder.py
  24. https://towardsdatascience.com/media/40914b58fe7c2eb3bbbd3f8bb98ec659?postid=2d53a6f8a4f9
  25. https://towardsdatascience.com/media/a1660445a2e9f7c2cbdb9efd386ee3ac?postid=2d53a6f8a4f9
  26. https://towardsdatascience.com/media/9492214a2d4f09d55c7aa9565a4627aa?postid=2d53a6f8a4f9
  27. https://towardsdatascience.com/media/a4cba039d81b6c843088cc8ffd6e1e71?postid=2d53a6f8a4f9
  28. https://towardsdatascience.com/media/0f2df040c66b57210b8bff6974160c79?postid=2d53a6f8a4f9
  29. https://towardsdatascience.com/media/1898430d8d1b887f896e011318723565?postid=2d53a6f8a4f9
  30. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4
  31. https://towardsdatascience.com/media/d6c3cd6b23e807df99b52ccde0771ce3?postid=2d53a6f8a4f9
  32. https://towardsdatascience.com/media/71698b762082ba1c44e81d2c8e1b8d21?postid=2d53a6f8a4f9
  33. https://towardsdatascience.com/media/44e2280b73d4f0fa1ec12cf801310969?postid=2d53a6f8a4f9
  34. https://towardsdatascience.com/media/a10bfdd9f4c973ed7f6e0f9b4085b0f4?postid=2d53a6f8a4f9
  35. https://towardsdatascience.com/media/1963ef9a598833e11a2a3b0f499f91c5?postid=2d53a6f8a4f9
  36. https://towardsdatascience.com/media/e4c104ee03920689207194ffaaafd94e?postid=2d53a6f8a4f9
  37. https://towardsdatascience.com/media/62e778d6bc8b804d6a6a2d860463bc7b?postid=2d53a6f8a4f9
  38. https://towardsdatascience.com/media/9b80c9bbdeadb10a70f1f3a59031abcb?postid=2d53a6f8a4f9
  39. https://towardsdatascience.com/media/7358b8d9f460e68910ba5a1281d45762?postid=2d53a6f8a4f9
  40. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4
  41. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7
  42. https://towardsdatascience.com/tagged/machine-learning?source=post
  43. https://towardsdatascience.com/tagged/tensorflow?source=post
  44. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  45. https://towardsdatascience.com/tagged/neural-networks?source=post
  46. https://towardsdatascience.com/@rnaresh.n?source=footer_card
  47. https://towardsdatascience.com/@rnaresh.n
  48. http://twitter.com/virginia_tech
  49. https://towardsdatascience.com/?source=footer_card
  50. https://towardsdatascience.com/?source=footer_card
  51. https://towardsdatascience.com/
  52. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  54. https://medium.com/towards-data-science/gans-n-roses-c6652d513260
  55. https://github.com/naresh1318/adversarial_autoencoder/blob/master/adversarial_autoencoder.py
  56. https://medium.com/p/2d53a6f8a4f9/share/twitter
  57. https://medium.com/p/2d53a6f8a4f9/share/facebook
  58. https://medium.com/p/2d53a6f8a4f9/share/twitter
  59. https://medium.com/p/2d53a6f8a4f9/share/facebook
