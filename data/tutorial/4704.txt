   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a wizard   s guide to adversarial autoencoders: part 3, disentanglement of
style and content.

   [16]go to the profile of naresh nagabushan
   [17]naresh nagabushan (button) blockedunblock (button) followfollowing
   aug 19, 2017
   [1*luyz1vdl0q-6h8lyh3ocag.png]

      if you   ve read the previous two parts you   ll feel right at home
   implementing this one.   

       part 2: [18]exploring latent space with adversarial autoencoders.

   parts 1 and 2 were mainly concerned with getting started on
   autoencoders and adversarial autoencoders. we began with a simple ae
   (vanilla one) made some changes to its architecture and training
   algorithm to end up with an aae. this part continues that trend by
   changing aae   s architecture along with a small change in the way it   s
   trained.

   i have created all the python files in my repo such that each one
   required for a part can be obtained from the previous one with some
   minor modifications. i would recommend you to read the theoretical
   aspects from this post and try to modify the code from part 2 to
   implement part 3.

   each and every one of us have our own unique style of writing, be it
   writing a letter or even a signature. the way in which we write certain
   characters, words we use often to create sentences, even the amount of
   pressure one exerts on paper are all characteristics which define a
   unique handwriting. with all these things coming into play, forging a
   handwriting can become very difficult and expensive. let   s try to learn
   the style of writing from the mnist dataset and use that to output
   images which all have the same style. we   ll only focus on the style of
   writing and not on the way sentences are framed or the thought process
   a writer goes through.

   to get a clear picture of what style and content is, look at the image
   below:
   [1*cs35-pfyqwn9sc2-kz7nta.png]
   style and content

   each of the texts has the same content     autoencoder   , but are in
   different styles. we now have the task of separating the style (myriad
   pro, mv boil,   ) and content from the images.

   disentanglement of various features is very important in representation
   learning (more on it [19]here).

   the autoencoder and adversarial autoencoder we have come across are all
   trained in an unsupervised manner (there weren   t any labels used during
   training). using the label info from an image allows an aae to focus on
   learning to extract the style in an image without considering its
   content. making use of image labels makes it a supervised model.

   the architecture we   ll need to accomplish this is very similar to what
   we   ve seen in part 2.
   [1*vgu0rekvre1di7sflu97_g.png]
   aae architecture

   here, instead of directly using the latent code z (output of the
   encoder) to recover the input image (x) at the decoder, we pass in the
   image labels (y) as well. now, the input to the decoder has two parts:
     * the latent code z from the encoder.
     * one hot representation of the image label (let   s call it y).

   we train the aae as usual with a little modification made to the
   reconstruction phase:
     * reconstruction phase: we pass in the input image to the encoder to
       get a latent code z and later, combine the latent code (z, output
       of the encoder) and the image label (y) to get a bigger column
       vector, which is then fed to our decoder. we train the ae to
       minimize the reconstruction loss as usual. since the image label is
       being fed to the decoder, the encoder learns the style of an image
       during training and the decoder uses the content info from the
       label and the style info from the encoder to reconstruct the input
       image.
     * id173 phase: exactly similar to what we   ve seen in
       [20]part 2.
     __________________________________________________________________

   now, let   s just add the image label to our aae and check out the
   results.

   since we need one hot image labels the decoder architecture will have a
   larger number of input neurons [10 + z_dim] (10 cause we have ten
   classes and z_dim is the size of the encoder output, for example, z_dim
   can be 2).
   [1*lzil05qpdy-aetvvh-y1lq.png]
   decoder architecture
     __________________________________________________________________

     now would be the time to stop reading and modify the code from part
     2. come back later to have a look at the code and the results.
     __________________________________________________________________

   modified decoder architecture:

   iframe: [21]/media/0da2fb55800842df0edef77986100de6?postid=89262973a4d7

   we   ll also need to feed in the image labels during training which is
   super easy:

   iframe: [22]/media/257aea95fa43e7b6e9cb1e3abbddcf1e?postid=89262973a4d7

   i   ve just replaced _ with batch_y when calling the
   mnist_train_next_batch() function and used these labels as inputs to
   the y_input placeholder.

   i   ve trained the ae with the following parameters:

   iframe: [23]/media/770582737e1e8c9b9f2e215e69a8c677?postid=89262973a4d7

   note that i have used z_dim=15 instead of 2 unlike the previous
   implementations as the result were visually pleasing.

   similar to part 2, i   ve maintained the standard deviation of the prior
   gaussian distribution to be 5.0 and a mean of 0.

   to generate images having the same style but with a different character
   we   ll pass in a random input as z (z_dim is 15 in this case) and change
   the one hot vector y to the desired value. here i   ve passed in 10
   random values for z and changed y to represent numbers from 0 to 9:

   have a look at generate_image_grid() for it   s implementation.
   [1*j4f9yasq_j0mgbxnni4vea.png]
   style and content disentanglement

   as a sanity check let   s have a look at the encoder output distribution
   to check if it   s still what we want it to be:
   [1*mbkmt0p2mth9icnfjaixmw.png]

     yup, that completes part 3!

   we   ll again build on what we have in this part to classify images using
   a limited number of labeled ones while still disentangling style and
   content.
     __________________________________________________________________

   i   ve reduced the reading time for this article just to check if it
   would affect the read ratio on medium     . leave a comment below if you
   feel shorter articles with condensed content is better or if you have a
   different opinion.

   thank you for reading, i am always looking for ways to improve my work
   your feedback (good or bad) will be of immense help to me. have a nice
   day!

     * [24]machine learning
     * [25]tensorflow
     * [26]neural networks
     * [27]artificial intelligence
     * [28]towards data science

   (button)
   (button)
   (button) 424 claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of naresh nagabushan

[30]naresh nagabushan

   master   s student [31]@virginia_tech interested in all things ai.

     (button) follow
   [32]towards data science

[33]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 424
     * (button)
     *
     *

   [34]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [35]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/89262973a4d7
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_lhmowkylaau4---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@rnaresh.n?source=post_header_lockup
  17. https://towardsdatascience.com/@rnaresh.n
  18. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9
  19. http://www.cl.uni-heidelberg.de/courses/ws14/deepl/bengioetal12.pdf
  20. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9
  21. https://towardsdatascience.com/media/0da2fb55800842df0edef77986100de6?postid=89262973a4d7
  22. https://towardsdatascience.com/media/257aea95fa43e7b6e9cb1e3abbddcf1e?postid=89262973a4d7
  23. https://towardsdatascience.com/media/770582737e1e8c9b9f2e215e69a8c677?postid=89262973a4d7
  24. https://towardsdatascience.com/tagged/machine-learning?source=post
  25. https://towardsdatascience.com/tagged/tensorflow?source=post
  26. https://towardsdatascience.com/tagged/neural-networks?source=post
  27. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  28. https://towardsdatascience.com/tagged/towards-data-science?source=post
  29. https://towardsdatascience.com/@rnaresh.n?source=footer_card
  30. https://towardsdatascience.com/@rnaresh.n
  31. http://twitter.com/virginia_tech
  32. https://towardsdatascience.com/?source=footer_card
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/
  35. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  37. https://medium.com/p/89262973a4d7/share/twitter
  38. https://medium.com/p/89262973a4d7/share/facebook
  39. https://medium.com/p/89262973a4d7/share/twitter
  40. https://medium.com/p/89262973a4d7/share/facebook
