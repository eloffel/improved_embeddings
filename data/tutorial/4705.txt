   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

a wizard   s guide to adversarial autoencoders: part 4, classify mnist using
1000 labels.

   [16]go to the profile of naresh nagabushan
   [17]naresh nagabushan (button) blockedunblock (button) followfollowing
   aug 26, 2017
   [1*6n773d6utdtw_6ld4qayfw.png]

      we   ll apply all that we have learnt in the previous 3 parts to
   classify mnist.

   i know it doesn   t really sound cool, shouldn   t we be working on
   something waaay cooler?   

   as always, if you haven   t read any of the previous parts or want to
   have a look at it again, i can save you some time:

       part 1: [18]autoencoder?

       part 2: [19]exploring the latent space with adversarial autoencoders.

       part 3: [20]disentanglement of style and content.

   we started off with an autoencoder to map images from a higher
   dimension to a lower one, constrained the encoder to output a required
   distribution by training it in an adversarial manner and lastly
   disentangled style from image content. finally, we now look at a way to
   classify mnist digits in a semi-supervised manner using 1000 labeled
   images.

   remember the prerequisite to understand this series?

   yes, classify mnist using tensorflow.

   let   s test that shall we?

   but, before that let me just tell you that (**spoiler alert**) we   ll be
   using the encoder to classify mnist digits and just as a reference to
   compare our results later, we   ll build a classifier (i   ll call it basic
   nn classifier) that has the same architecture (not including the output
   z neurons) as our encoder and train it using 1000 labeled images in a
   supervised manner and test its accuracy using the 10k test data.
   [1*optu8py5kbpbvuzkylgdpa.png]
   basic nn classifier

   if you have come this far, you   ll definitely be able a pro implementing
   the above architecture, so i   ll skip it. but, if you get stuck
   somewhere have a peek at the link below:
   [21]naresh1318/adversarial_autoencoder
   adversarial_autoencoder - a wizard's guide to adversarial
   autoencodersgithub.com

   let   s have a look at the results after training our basic nn classifier
   with the following parameters:

   iframe: [22]/media/632758ed4d3b420d5315d4c2a707f687?postid=2ca08071f95

   [1*ef_uot2b4z6ums6vvxswlg.png]
   accuracy variation

   after about 50 epochs the model seems to be overfitting as the testing
   accuracy has saturated at 87%. note that the model is being shown only
   1000 labeled images, we could have gotten a higher accuracy if we   d
   used all the 50k training images. now, we have the task of improving
   this accuracy using the same 1000 labeled images.
     __________________________________________________________________

     but, how can we use our aae as a classifier?

   we   ll simultaneously improve classification accuracy, reduce
   dimensionality, and disentangle style and content using just a single
   model (yeah, i   m bragging again). this requires some modifications to
   our previous architecture (the one in part 3):
   [1*8ruz8kguluosogodpisguq.png]
   semi-supervised aae block diagram

   it   s similar to the aae in part 3 but with another discriminator added
   at the top (d_cat) and little modification made to the encoder
   architecture (it now outputs both y and z). the encoder   s output can be
   divided into two parts, classification (y) and the latent code (z).
   since there are 10 labels, the one hot vector (y) has 10 values and as
   usual, z depends on the size of the latent code (z_dim) the user
   desires.

   we   ll need to impose a [23]categorical distribution (cat(10)) on y
   since we just want 1 of the 10 possible output neurons to fire for each
   image being feed to it. the discriminator d_cat is meant to do exactly
   that, it   s trained in an adversarial manner along with our generator
   (encoder) to force the encoder to produce one hot vectors at y (we   ll
   still not be able to use the encoder as a classifier as these one hot
   vectors will all be random values since the main aim now is to force a
   categorical distribution at the encoder).

   and as usual, we   ll teach the encoder to produce a gaussian (or any
   other distribution) distribution at z using the discriminator d_gauss
   that   s shown in the above diagram.

   most of the magic happens during the training phase, where we   ll look
   at how the encoder can be used as a classifier.
     __________________________________________________________________

   both the adversarial networks are trained simultaneously using sgd
   (stochastic id119) in three phases:
     * reconstruction phase:

   [1*onlofta8qcfmdgm9ilkdaw.png]
   reconstruction phase

   as seen in part 2 and 3, we   re concerned with reducing the
   reconstruction loss which would be the mean squared error between the
   input and the output images. we   ll backprop only through the encoder
   and decoder networks to reduce the reconstruction loss during this
   phase.
     * id173 phase:

   [1*hkvmbwaxmdag12gjgoc39q.png]
   step 1

   during the id173 phase, the encoder output y should be
   constrained to produce only a categorical distribution which can be
   done in a manner similar to what we did in part 2. we   ll first train
   the discriminator d_cat to discriminate between real categorical
   samples (y   ) and the ones that are obtained from the generator
   (encoder) y. we can do this by passing an image to the encoder which
   produces both y and z, let   s forget z for now (we   ll use it later), the
   fake y from the encoder and y    from the categorical distribution are
   used to train the discriminator. later, the discriminator   s weights are
   fixed (made untrainable) and with the target set to 1, the encoder is
   trained to fool the discriminator.
   [1*qmgbvi2q14lnkgmrx_gmnw.png]
   step 2

   similarly, to impose a gaussian distribution on z, the discriminator
   d_gauss is first trained followed by the encoder just as in part 2.
     * semi-supervised classification phase

   [1*zs7wa7h8thudwkuqgzjt2q.png]

   we   ll finally train the encoder to classify the digits by passing in a
   mini batch of images and their corresponding one hot labels to minimize
   the cross id178 cost (again, we   ll ignore anything that   s obtained at
   z). during the training phase, we   ll only be using the 1000 labeled
   images from the training set and use all of the 10k test set to find
   out how well our model is performing.
     __________________________________________________________________

     i think code speaks clearer than words which is one of the reasons
     why i always try to include snippets.

   to build a semi-supervised adversarial autoencoder using the code from
   part 3, we   ll need to make 3 major changes (yes, there are certain
   subtle changes as well, i   ve explained them under the note sections).
   first, being the encoder architecture:
   [1*qufkkgkupugvykwsmqiv5w.png]
   encoder architecture

   iframe: [24]/media/ab5765e009d4c63767757c1e627dab44?postid=2ca08071f95

   note:
     * latent_variable will be our z .
     * cat_op is the categorical output y .
     * i   ve used an if statement to test the supervised flag. all it does
       is, it checks if the supervised flag is false (which will be the
       case when we directly connect the encoder output as the decoder
       input) and passes the logits (direct output from the architecture
       without passing it through any activation function) through the
       softmax function to get a proper categorical distribution at the
       output. but, when the flag is true, the logits are used as they are
       to train the encoder model in a supervised manner. i   ve done this
       so that i could use softmax_cross_id178_with_logits() that   s
       directly available on tensorflow during the semi-supervised
       training phase.

   the decoder architecture remains the same as the previous one:
   [1*nrf013a75pcdlgot48w0ma.png]
   decoder architecture

   iframe: [25]/media/6c2aef7ad0cc847cd3f258e463908643?postid=2ca08071f95

   both the discriminators have the same architecture with only the input
   dimension changed, this makes it the second major change.
   [1*jzdb_ijhvl74zb0nu03uew.png]
   d_gauss and d_cat architecture

   iframe: [26]/media/5782c00171ddecca6a904c83c87cc5fc?postid=2ca08071f95

   and lastly, the training mechanism has undergone some changes which
   i   ve explained below:

   we   ll start with the reconstruction phase by building the ae:

   iframe: [27]/media/9e53ffd3711411ab859799ee26702cc8?postid=2ca08071f95

   this is later followed by the reconstruction loss as its optimizer:

   iframe: [28]/media/43f4c8f09b9f59ab8ddb51864a361782?postid=2ca08071f95

   to train the encoder and discriminator architectures their
   corresponding weights have to collected just like in part 2 and 3:

   iframe: [29]/media/ca1020fea8c1e35a2ede119b3e9658c0?postid=2ca08071f95

   the id173 phase which first involves training the two
   discriminators (d_cat and d_gauss) is done as follows (well, we still
   have to pass inputs):

   iframe: [30]/media/5ba951d5d9f9db7e7a2b3578ca358398?postid=2ca08071f95

   now, to train the generator (encoder) to output a categorical
   distribution at y and a gaussian distribution at z, we   ll run these
   magical lines:

   iframe: [31]/media/d56ff76d45fe56db1e2c2be080413d79?postid=2ca08071f95

   to train the encoder to classify, i   ve passed in the input images to
   the trained encoder (reuse=true) also notice that supervised=true which
   does not use the softmax function and passes the dense layer output to
   the id168:

   iframe: [32]/media/476a62112f38f837d454c732393c5911?postid=2ca08071f95

   the training stage which requires real categorical distribution is
   produced using np.eye(), the code below trains the model for the given
   number of epochs, saves it every epoch, writes the losses, histograms
   and output images to tensorboard and stores results (losses and
   classification accuracy) in
   the ./results/semi_supervised/<folder_name>/log/log.txt :

   iframe: [33]/media/aea75caa4de6dbc6fd8ce7fe6d0115b8?postid=2ca08071f95

   after training for 1000 epochs using the following parameters:

   iframe: [34]/media/0b0a04c85f78286256c2e149203451a0?postid=2ca08071f95

   the accuracy plot is as follows:
   [1*z_3rv5fc-osppkr5ygft9g.png]
   accuracy variations

   we crossed the 95% accuracy mark without even tuning any of the
   hyperparameters (i was kinda lazy to do that). the model uses the
   features learnt during the unsupervised training phase to achieve a
   higher classification accuracy than its supervised counterpart. this
   clearly shows the potential for the application of unsupervised
   learning, we could collect a lot of freely available unlabeled data on
   the internet and teach a computer to learn features from these
   unlabeled images.

   here   s an example of what an ae can learn in an unsupervised manner:
   [1*phkwexlhrt9xvuvb3cvbpg.png]
   visualizing the features learnt. source:
   [35]http://ufldl.stanford.edu/tutorial/unsupervised/autoencoders/

   if we visualize the features that each of the output neuron in an
   encoder is looking for, we get a visualization that   s similar to what
   is shown in the above diagram. an output neuron may fire only if it
   finds a particular edge or contour at the input (more on this can be
   found [36]here). these features that are learnt in an unsupervised
   manner can be used later during a supervised training task, which is
   exactly what we   ve done in this part.

   and just as a sanity check, let   s visualize the output distributions
   and images:
   [1*ej1erof6l-nmbovaaynncq.png]
   distributions
   [1*rhgylhovn-tmhd7_foxo-a.png]
   images from the decoder

   phew!, real and output distributions match and the output images look
   like images that have been obtained from the mnist dataset.

   so, yup we are done with the final part!
     __________________________________________________________________

     thank you for reading!

   i just wanted people to know how they can go about implementing stuff
   by building on what they know and how starting off with a simple ae we
   could make little modifications at each step to create something
   interesting and fun.

   i wrote this series during my summer break to get a feel of a writer   s
   through process and to appreciate what they go through to express their
   thoughts or share their knowledge for the world to understand. i
   believe people start to admire other   s work and truly understand it
   only when they try doing something similar themselves.

   with that said, if you have come this far, thank you for reading my
   previous posts and showing your appreciating with a     . please let me
   know if i   ve made any blunders or destroyed the internet.

     * [37]machine learning
     * [38]artificial intelligence
     * [39]tensorflow
     * [40]neural networks

   (button)
   (button)
   (button) 432 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [41]go to the profile of naresh nagabushan

[42]naresh nagabushan

   master   s student [43]@virginia_tech interested in all things ai.

     (button) follow
   [44]towards data science

[45]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 432
     * (button)
     *
     *

   [46]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [47]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/2ca08071f95
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/a-wizards-guide-to-adversarial-autoencoders-part-4-classify-mnist-using-1000-labels-2ca08071f95&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_3weceyc2p0dt---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@rnaresh.n?source=post_header_lockup
  17. https://towardsdatascience.com/@rnaresh.n
  18. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-1-autoencoder-d9a5f8795af4
  19. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-2-exploring-latent-space-with-adversarial-2d53a6f8a4f9
  20. https://medium.com/towards-data-science/a-wizards-guide-to-adversarial-autoencoders-part-3-disentanglement-of-style-and-content-89262973a4d7
  21. https://github.com/naresh1318/adversarial_autoencoder/blob/master/basic_nn_classifier.py
  22. https://towardsdatascience.com/media/632758ed4d3b420d5315d4c2a707f687?postid=2ca08071f95
  23. https://en.wikipedia.org/wiki/categorical_distribution
  24. https://towardsdatascience.com/media/ab5765e009d4c63767757c1e627dab44?postid=2ca08071f95
  25. https://towardsdatascience.com/media/6c2aef7ad0cc847cd3f258e463908643?postid=2ca08071f95
  26. https://towardsdatascience.com/media/5782c00171ddecca6a904c83c87cc5fc?postid=2ca08071f95
  27. https://towardsdatascience.com/media/9e53ffd3711411ab859799ee26702cc8?postid=2ca08071f95
  28. https://towardsdatascience.com/media/43f4c8f09b9f59ab8ddb51864a361782?postid=2ca08071f95
  29. https://towardsdatascience.com/media/ca1020fea8c1e35a2ede119b3e9658c0?postid=2ca08071f95
  30. https://towardsdatascience.com/media/5ba951d5d9f9db7e7a2b3578ca358398?postid=2ca08071f95
  31. https://towardsdatascience.com/media/d56ff76d45fe56db1e2c2be080413d79?postid=2ca08071f95
  32. https://towardsdatascience.com/media/476a62112f38f837d454c732393c5911?postid=2ca08071f95
  33. https://towardsdatascience.com/media/aea75caa4de6dbc6fd8ce7fe6d0115b8?postid=2ca08071f95
  34. https://towardsdatascience.com/media/0b0a04c85f78286256c2e149203451a0?postid=2ca08071f95
  35. http://ufldl.stanford.edu/tutorial/unsupervised/autoencoders/
  36. http://ufldl.stanford.edu/tutorial/unsupervised/autoencoders/
  37. https://towardsdatascience.com/tagged/machine-learning?source=post
  38. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  39. https://towardsdatascience.com/tagged/tensorflow?source=post
  40. https://towardsdatascience.com/tagged/neural-networks?source=post
  41. https://towardsdatascience.com/@rnaresh.n?source=footer_card
  42. https://towardsdatascience.com/@rnaresh.n
  43. http://twitter.com/virginia_tech
  44. https://towardsdatascience.com/?source=footer_card
  45. https://towardsdatascience.com/?source=footer_card
  46. https://towardsdatascience.com/
  47. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  49. https://github.com/naresh1318/adversarial_autoencoder/blob/master/basic_nn_classifier.py
  50. https://medium.com/p/2ca08071f95/share/twitter
  51. https://medium.com/p/2ca08071f95/share/facebook
  52. https://medium.com/p/2ca08071f95/share/twitter
  53. https://medium.com/p/2ca08071f95/share/facebook
