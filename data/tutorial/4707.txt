   #[1]github [2]recent commits to awd-lstm-lm:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]57
     * [35]star [36]1,260
     * [37]fork [38]304

[39]salesforce/[40]awd-lstm-lm

   [41]code [42]issues 42 [43]pull requests 7 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   lstm and qid56 language model toolkit for pytorch
   [47]lstm [48]pytorch [49]language-model [50]sgd [51]qid56
     * [52]39 commits
     * [53]1 branch
     * [54]1 release
     * [55]fetching contributors
     * [56]bsd-3-clause

    1. [57]python 97.4%
    2. [58]shell 2.6%

   (button) python shell
   branch: master (button) new pull request
   [59]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/s
   [60]download zip

downloading...

   want to be notified of new releases in salesforce/awd-lstm-lm?
   [61]sign in [62]sign up

launching github desktop...

   if nothing happens, [63]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [64]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [65]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [66]download the github extension for visual studio
   and try again.

   (button) go back
   [67]@keskarnitish
   [68]keskarnitish [69]updating some typos in readme
   latest commit [70]32fcb42 jun 20, 2018
   [71]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [72]data/enwik8 [73]script to process and prepare enwik8 for dataset
   use mar 19, 2018
   [74].gitignore
   [75]license [76]code release aug 18, 2017
   [77]readme.md [78]updating some typos in readme jun 19, 2018
   [79]data.py
   [80]embed_regularize.py
   [81]finetune.py [82]pytorch 0.4 compatible jun 13, 2018
   [83]generate.py [84]generate was broken for qid56 as qid56 didn't flush
   their previously st    nov 26, 2017
   [85]getdata.sh [86]rm duplicate mar 29, 2018
   [87]locked_dropout.py
   [88]main.py [89]removing optional dependency jun 13, 2018
   [90]model.py
   [91]pointer.py
   [92]splitcross.py
   [93]utils.py
   [94]weight_drop.py

readme.md

lstm and qid56 language model toolkit

   this repository contains the code used for two [95]salesforce research
   papers:
     * [96]regularizing and optimizing lstm language models
     * [97]an analysis of neural id38 at multiple scales this
       code was originally forked from the [98]pytorch word level language
       modeling example.

   the model comes with instructions to train:
     * word level language models over the id32 (ptb),
       [99]wikitext-2 (wt2), and [100]wikitext-103 (wt103) datasets
     * id186 over the id32 (ptbc) and
       hutter prize dataset (enwik8)

   the model can be composed of an lstm or a [101]quasi-recurrent neural
   network (qid56) which is two or more times faster than the cudnn lstm in
   this setup while achieving equivalent or better accuracy.
     * install pytorch 0.4
     * run getdata.sh to acquire the id32 and wikitext-2 datasets
     * train the base model using main.py
     * (optionally) finetune the model using finetune.py
     * (optionally) apply the [102]continuous cache pointer to the
       finetuned model using pointer.py

   if you use this code or our results in your research, please cite as
   appropriate:
@article{merityregopt,
  title={{regularizing and optimizing lstm language models}},
  author={merity, stephen and keskar, nitish shirish and socher, richard},
  journal={arxiv preprint arxiv:1708.02182},
  year={2017}
}

@article{merityanalysis,
  title={{an analysis of neural id38 at multiple scales}},
  author={merity, stephen and keskar, nitish shirish and socher, richard},
  journal={arxiv preprint arxiv:1803.08240},
  year={2018}
}

update (june/13/2018)

   the codebase is now pytorch 0.4 compatible for most use cases (a big
   shoutout to [103]https://github.com/shawntan for a fairly comprehensive
   pr [104]https://github.com/salesforce/awd-lstm-lm/pull/43). mild
   readjustments to hyperparameters may be necessary to obtain quoted
   performance. if you desire exact reproducibility (or wish to run on
   pytorch 0.3 or lower), we suggest using an older commit of this
   repository. we are still working on pointer, finetune and generate
   functionalities.

software requirements

   python 3 and pytorch 0.4 are required for the current codebase.

   included below are hyper parameters to get equivalent or better results
   to those included in the original paper.

   if you need to use an earlier version of the codebase, the original
   code and hyper parameters accessible at the [105]pytorch==0.1.12
   release, with python 3 and pytorch 0.1.12 are required. if you are
   using anaconda, installation of pytorch 0.1.12 can be achieved via:
   conda install pytorch=0.1.12 -c soumith.

experiments

   the codebase was modified during the writing of the paper, preventing
   exact reproduction due to minor differences in random seeds or similar.
   we have also seen exact reproduction numbers change when changing
   underlying gpu. the guide below produces results largely similar to the
   numbers reported.

   for data setup, run ./getdata.sh. this script collects the mikolov
   pre-processed id32 and the wikitext-2 datasets and places them
   in the data directory.

   next, decide whether to use the qid56 or the lstm as the underlying
   recurrent neural network model. the qid56 is many times faster than even
   nvidia's cudnn optimized lstm (and dozens of times faster than a naive
   lstm implementation) yet achieves similar or better results than the
   lstm for many word level datasets. at the time of writing, the qid56
   models use the same number of parameters and are slightly deeper
   networks but are two to four times faster per epoch and require less
   epochs to converge.

   the qid56 model uses a qid56 with convolutional size 2 for the first
   layer, allowing the model to view discrete natural language inputs
   (i.e. "new york"), while all other layers use a convolutional size of
   1.

   finetuning note: fine-tuning modifies the original saved model model.pt
   file - if you wish to keep the original weights you must copy the file.

   pointer note: bptt just changes the length of the sequence pushed onto
   the gpu but won't impact the final result.

character level enwik8 with lstm

     * python -u main.py --epochs 50 --nlayers 3 --emsize 400 --nhid 1840
       --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1
       --dropout 0.4 --wdrop 0.2 --wdecay 1.2e-6 --bptt 200 --batch_size
       128 --optimizer adam --lr 1e-3 --data data/enwik8 --save enwik8.pt
       --when 25 35

character level id32 (ptb) with lstm

     * python -u main.py --epochs 500 --nlayers 3 --emsize 200 --nhid 1000
       --alpha 0 --beta 0 --dropoute 0 --dropouth 0.25 --dropouti 0.1
       --dropout 0.1 --wdrop 0.5 --wdecay 1.2e-6 --bptt 150 --batch_size
       128 --optimizer adam --lr 2e-3 --data data/pennchar --save ptbc.pt
       --when 300 400

word level wikitext-103 (wt103) with qid56

     * python -u main.py --epochs 14 --nlayers 4 --emsize 400 --nhid 2500
       --alpha 0 --beta 0 --dropoute 0 --dropouth 0.1 --dropouti 0.1
       --dropout 0.1 --wdrop 0 --wdecay 0 --bptt 140 --batch_size 60
       --optimizer adam --lr 1e-3 --data data/wikitext-103 --save
       wt103.12hr.qid56.pt --when 12 --model qid56

word level id32 (ptb) with lstm

   the instruction below trains a ptb model that without finetuning
   achieves perplexities of approximately 61.2 / 58.8 (validation /
   testing), with finetuning achieves perplexities of approximately 58.8 /
   56.5, and with the continuous cache pointer augmentation achieves
   perplexities of approximately 53.2 / 52.5.
     * python main.py --batch_size 20 --data data/penn --dropouti 0.4
       --dropouth 0.25 --seed 141 --epoch 500 --save ptb.pt
     * python finetune.py --batch_size 20 --data data/penn --dropouti 0.4
       --dropouth 0.25 --seed 141 --epoch 500 --save ptb.pt
     * python pointer.py --data data/penn --save ptb.pt --lambdasm 0.1
       --theta 1.0 --window 500 --bptt 5000

word level id32 (ptb) with qid56

   the instruction below trains a qid56 model that without finetuning
   achieves perplexities of approximately 60.6 / 58.3 (validation /
   testing), with finetuning achieves perplexities of approximately 59.1 /
   56.7, and with the continuous cache pointer augmentation achieves
   perplexities of approximately 53.4 / 52.6.
     * python -u main.py --model qid56 --batch_size 20 --clip 0.2 --wdrop
       0.1 --nhid 1550 --nlayers 4 --emsize 400 --dropouth 0.3 --seed 9001
       --dropouti 0.4 --epochs 550 --save ptb.pt
     * python -u finetune.py --model qid56 --batch_size 20 --clip 0.2
       --wdrop 0.1 --nhid 1550 --nlayers 4 --emsize 400 --dropouth 0.3
       --seed 404 --dropouti 0.4 --epochs 300 --save ptb.pt
     * python pointer.py --model qid56 --lambdasm 0.1 --theta 1.0 --window
       500 --bptt 5000 --save ptb.pt

word level wikitext-2 (wt2) with lstm

   the instruction below trains a ptb model that without finetuning
   achieves perplexities of approximately 68.7 / 65.6 (validation /
   testing), with finetuning achieves perplexities of approximately 67.4 /
   64.7, and with the continuous cache pointer augmentation achieves
   perplexities of approximately 52.2 / 50.6.
     * python main.py --epochs 750 --data data/wikitext-2 --save wt2.pt
       --dropouth 0.2 --seed 1882
     * python finetune.py --epochs 750 --data data/wikitext-2 --save
       wt2.pt --dropouth 0.2 --seed 1882
     * python pointer.py --save wt2.pt --lambdasm 0.1279 --theta 0.662
       --window 3785 --bptt 2000 --data data/wikitext-2

word level wikitext-2 (wt2) with qid56

   the instruction below will a qid56 model that without finetuning
   achieves perplexities of approximately 69.3 / 66.8 (validation /
   testing), with finetuning achieves perplexities of approximately 68.5 /
   65.9, and with the continuous cache pointer augmentation achieves
   perplexities of approximately 53.6 / 52.1. better numbers are likely
   achievable but the hyper parameters have not been extensively searched.
   these hyper parameters should serve as a good starting point however.
     * python -u main.py --epochs 500 --data data/wikitext-2 --clip 0.25
       --dropouti 0.4 --dropouth 0.2 --nhid 1550 --nlayers 4 --seed 4002
       --model qid56 --wdrop 0.1 --batch_size 40 --save wt2.pt
     * python finetune.py --epochs 500 --data data/wikitext-2 --clip 0.25
       --dropouti 0.4 --dropouth 0.2 --nhid 1550 --nlayers 4 --seed 4002
       --model qid56 --wdrop 0.1 --batch_size 40 --save wt2.pt
     * python -u pointer.py --save wt2.pt --model qid56 --lambdasm 0.1279
       --theta 0.662 --window 3785 --bptt 2000 --data data/wikitext-2

speed

   for speed regarding character-level ptb and enwik8 or word-level
   wikitext-103, refer to the relevant paper.

   the default speeds for the models during training on an nvidia quadro
   gp100:
     * id32 (batch size 20): lstm takes 65 seconds per epoch,
       qid56 takes 28 seconds per epoch
     * wikitext-2 (batch size 20): lstm takes 180 seconds per epoch, qid56
       takes 90 seconds per epoch

   the default qid56 models can be far faster than the cudnn lstm model,
   with the speed-ups depending on how much of a bottleneck the id56 is.
   the majority of the model time above is now spent in softmax or
   optimization overhead (see [106]pytorch qid56 discussion on speed).

   speeds are approximately three times slower on a k80. on a k80 or other
   memory cards with less memory you may wish to enable [107]the cap on
   the maximum sampled sequence length to prevent out-of-memory (oom)
   errors, especially for wikitext-2.

   if speed is a major issue, sgd converges more quickly than our
   non-monotonically triggered variant of asgd though achieves a worse
   overall perplexity.

details of the qid56 optimization

   for full details, refer to the [108]pytorch qid56 repository.

details of the lstm optimization

   all the augmentations to the lstm, including our variant of
   [109]dropconnect (wan et al. 2013) termed weight dropping which adds
   recurrent dropout, allow for the use of nvidia's cudnn lstm
   implementation. pytorch will automatically use the cudnn backend if run
   on cuda with cudnn installed. this ensures the model is fast to train
   even when convergence may take many hundreds of epochs.

     *    2019 github, inc.
     * [110]terms
     * [111]privacy
     * [112]security
     * [113]status
     * [114]help

     * [115]contact github
     * [116]pricing
     * [117]api
     * [118]training
     * [119]blog
     * [120]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [121]reload to refresh your
   session. you signed out in another tab or window. [122]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/salesforce/awd-lstm-lm/commits/master.atom
   3. https://github.com/salesforce/awd-lstm-lm#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/salesforce/awd-lstm-lm
  32. https://github.com/join
  33. https://github.com/login?return_to=/salesforce/awd-lstm-lm
  34. https://github.com/salesforce/awd-lstm-lm/watchers
  35. https://github.com/login?return_to=/salesforce/awd-lstm-lm
  36. https://github.com/salesforce/awd-lstm-lm/stargazers
  37. https://github.com/login?return_to=/salesforce/awd-lstm-lm
  38. https://github.com/salesforce/awd-lstm-lm/network/members
  39. https://github.com/salesforce
  40. https://github.com/salesforce/awd-lstm-lm
  41. https://github.com/salesforce/awd-lstm-lm
  42. https://github.com/salesforce/awd-lstm-lm/issues
  43. https://github.com/salesforce/awd-lstm-lm/pulls
  44. https://github.com/salesforce/awd-lstm-lm/projects
  45. https://github.com/salesforce/awd-lstm-lm/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/topics/lstm
  48. https://github.com/topics/pytorch
  49. https://github.com/topics/language-model
  50. https://github.com/topics/sgd
  51. https://github.com/topics/qid56
  52. https://github.com/salesforce/awd-lstm-lm/commits/master
  53. https://github.com/salesforce/awd-lstm-lm/branches
  54. https://github.com/salesforce/awd-lstm-lm/releases
  55. https://github.com/salesforce/awd-lstm-lm/graphs/contributors
  56. https://github.com/salesforce/awd-lstm-lm/blob/master/license
  57. https://github.com/salesforce/awd-lstm-lm/search?l=python
  58. https://github.com/salesforce/awd-lstm-lm/search?l=shell
  59. https://github.com/salesforce/awd-lstm-lm/find/master
  60. https://github.com/salesforce/awd-lstm-lm/archive/master.zip
  61. https://github.com/login?return_to=https://github.com/salesforce/awd-lstm-lm
  62. https://github.com/join?return_to=/salesforce/awd-lstm-lm
  63. https://desktop.github.com/
  64. https://desktop.github.com/
  65. https://developer.apple.com/xcode/
  66. https://visualstudio.github.com/
  67. https://github.com/keskarnitish
  68. https://github.com/salesforce/awd-lstm-lm/commits?author=keskarnitish
  69. https://github.com/salesforce/awd-lstm-lm/commit/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5
  70. https://github.com/salesforce/awd-lstm-lm/commit/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5
  71. https://github.com/salesforce/awd-lstm-lm/tree/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5
  72. https://github.com/salesforce/awd-lstm-lm/tree/master/data/enwik8
  73. https://github.com/salesforce/awd-lstm-lm/commit/a133f7cb005dd89966b685737cb14f1f6d2f27db
  74. https://github.com/salesforce/awd-lstm-lm/blob/master/.gitignore
  75. https://github.com/salesforce/awd-lstm-lm/blob/master/license
  76. https://github.com/salesforce/awd-lstm-lm/commit/acb04a5d86bff8d3867bc1343ce0e4b223b8d5be
  77. https://github.com/salesforce/awd-lstm-lm/blob/master/readme.md
  78. https://github.com/salesforce/awd-lstm-lm/commit/32fcb42562aeb5c7e6c9dec3f2a3baaaf68a5cb5
  79. https://github.com/salesforce/awd-lstm-lm/blob/master/data.py
  80. https://github.com/salesforce/awd-lstm-lm/blob/master/embed_regularize.py
  81. https://github.com/salesforce/awd-lstm-lm/blob/master/finetune.py
  82. https://github.com/salesforce/awd-lstm-lm/commit/457a422eb46e970a6aad659ca815a04b3d074d6c
  83. https://github.com/salesforce/awd-lstm-lm/blob/master/generate.py
  84. https://github.com/salesforce/awd-lstm-lm/commit/9c623587a9c565e43aea8064ac573ad06907b7ea
  85. https://github.com/salesforce/awd-lstm-lm/blob/master/getdata.sh
  86. https://github.com/salesforce/awd-lstm-lm/commit/435f1d7cc35e30c91e4efc22197fc077810600f0
  87. https://github.com/salesforce/awd-lstm-lm/blob/master/locked_dropout.py
  88. https://github.com/salesforce/awd-lstm-lm/blob/master/main.py
  89. https://github.com/salesforce/awd-lstm-lm/commit/f694c0dcef65b6b3db012005e54db7298994dd71
  90. https://github.com/salesforce/awd-lstm-lm/blob/master/model.py
  91. https://github.com/salesforce/awd-lstm-lm/blob/master/pointer.py
  92. https://github.com/salesforce/awd-lstm-lm/blob/master/splitcross.py
  93. https://github.com/salesforce/awd-lstm-lm/blob/master/utils.py
  94. https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py
  95. https://einstein.ai/
  96. https://arxiv.org/abs/1708.02182
  97. https://arxiv.org/abs/1803.08240
  98. https://github.com/pytorch/examples/tree/master/word_language_model
  99. https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset
 100. https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset
 101. https://github.com/salesforce/pytorch-qid56/
 102. https://arxiv.org/abs/1612.04426
 103. https://github.com/shawntan
 104. https://github.com/salesforce/awd-lstm-lm/pull/43
 105. https://github.com/salesforce/awd-lstm-lm/tree/pytorch==0.1.12
 106. https://github.com/salesforce/pytorch-qid56#speed
 107. https://github.com/salesforce/awd-lstm-lm/blob/ef9369d277f8326b16a9f822adae8480b6d492d0/main.py#l131
 108. https://github.com/salesforce/pytorch-qid56
 109. https://cs.nyu.edu/~wanli/dropc/dropc.pdf
 110. https://github.com/site/terms
 111. https://github.com/site/privacy
 112. https://github.com/security
 113. https://githubstatus.com/
 114. https://help.github.com/
 115. https://github.com/contact
 116. https://github.com/pricing
 117. https://developer.github.com/
 118. https://training.github.com/
 119. https://github.blog/
 120. https://github.com/about
 121. https://github.com/salesforce/awd-lstm-lm
 122. https://github.com/salesforce/awd-lstm-lm

   hidden links:
 124. https://github.com/
 125. https://github.com/salesforce/awd-lstm-lm
 126. https://github.com/salesforce/awd-lstm-lm
 127. https://github.com/salesforce/awd-lstm-lm
 128. https://help.github.com/articles/which-remote-url-should-i-use
 129. https://github.com/salesforce/awd-lstm-lm#lstm-and-qid56-language-model-toolkit
 130. https://github.com/salesforce/awd-lstm-lm#update-june132018
 131. https://github.com/salesforce/awd-lstm-lm#software-requirements
 132. https://github.com/salesforce/awd-lstm-lm#experiments
 133. https://github.com/salesforce/awd-lstm-lm#character-level-enwik8-with-lstm
 134. https://github.com/salesforce/awd-lstm-lm#character-level-penn-treebank-ptb-with-lstm
 135. https://github.com/salesforce/awd-lstm-lm#word-level-wikitext-103-wt103-with-qid56
 136. https://github.com/salesforce/awd-lstm-lm#word-level-penn-treebank-ptb-with-lstm
 137. https://github.com/salesforce/awd-lstm-lm#word-level-penn-treebank-ptb-with-qid56
 138. https://github.com/salesforce/awd-lstm-lm#word-level-wikitext-2-wt2-with-lstm
 139. https://github.com/salesforce/awd-lstm-lm#word-level-wikitext-2-wt2-with-qid56
 140. https://github.com/salesforce/awd-lstm-lm#speed
 141. https://github.com/salesforce/awd-lstm-lm#details-of-the-qid56-optimization
 142. https://github.com/salesforce/awd-lstm-lm#details-of-the-lstm-optimization
 143. https://github.com/
