   #[1]yerevann

[2]yerevann blog on neural networks

automatic id68 with lstm

   09 sep 2016

   by [3]tigran galstyan, [4]hrayr harutyunyan and [5]hrant khachatrian.

   many languages have their own non-latin alphabets but the web is full
   of content in those languages written in latin letters, which makes it
   inaccessible to various nlp tools (e.g. automatic translation).
   id68 is the process of converting the romanized text back to
   the original writing system. in theory every language has a strict set
   of romanization rules, but in practice people do not follow the rules
   and most of the romanized content is hard to transliterate using rule
   based algorithms. we believe this problem is solvable using the state
   of the art nlp tools, and we demonstrate a high quality solution for
   armenian based on recurrent neural networks. we invite everyone to
   [6]adapt our system for more languages.

contents

     * [7]problem description
     * [8]data processing
          + [9]source of the data
          + [10]romanization rules
          + [11]geographic dependency
          + [12]filtering out large non-armenian chunks
     * [13]network architecture
          + [14]encoding the characters
          + [15]aligning
          + [16]bidirectional lstm with residual-like connections
     * [17]results
     * [18]future work

problem description

   since early 1990s computers became widespread in many countries, but
   the operating systems did not fully support different alphabets out of
   the box. most keyboards had only latin letters printed on them, and
   people started to invent romanization rules for their languages. every
   language has its own story, and these stories are usually not known
   outside their own communities. in case of armenian, [19]some solutions
   have been developed, but even those who knew how to write in armenian
   characters, were not sure that the readers (r.g. the recipient of the
   email) would be able to read that.
    armenian alphabet in the unicode space. source: wikipedia
   armenian alphabet in the unicode space. source: [20]wikipedia

   in the unicode era all major oses started to support displaying
   [21]armenian characters. but the lack of keyboard layouts was still a
   problem. in late 2000s mobile internet penetration exploded in armenia,
   and most of the early mobile phones did not support writing in
   armenian. for example, ios doesn   t include armenian keyboard and
   started to officially support custom keyboards [22]only in 2014! the
   result was that lots of people entered the web (mostly through social
   networks) without having access to armenian letters. so everyone
   started to use some sort of romanization (obviously no one was aware
   that there are fixed standards for the [23]romanization of armenian).

   currently there are many attempts to fight romanized armenian on forums
   and social networks. armenian keyboard layouts are developed for every
   popular platform. but still lots of content is produced in non-armenian
   letters (maybe only facebook knows the exact scale of the problem), and
   such content remains inaccessible for search indexing, automated
   translation, text-to-speech, etc. recently the problem started to flow
   outside the web, people use romanized armenian on the streets.
   romanized armenian on the street. source: vkontakte social network
   romanized armenian on the street. source: vkontakte social network

   there are some online tools that correctly transliterate romanized
   armenian if its written using strict rules. [24]hayeren.am is the most
   famous example. facebook   s search box also recognizes some
   romanizations (but not all). but for many practical cases these tools
   do not give a reasonable output. the algorithm must be able to use the
   context to correctly predict the armenian character.
           facebook's search box recognizes some romanized armenian
   facebook   s search box recognizes some romanized armenian. note that the
   spelling suggestion is not for armenian.

   finally, there are debates whether these tools actually help fighting
   the    translit    problem. some argue that people will not be forced to
   use armenian keyboard if there are very good tools to transliterate. we
   believe that the goal of making this content available for the nlp
   tools is extremely important, as no one will (and should) develop, say,
   language translation tools for romanized alphabets.

   wikipedia has similar stories for [25]greek, [26]persian and
   [27]cyrillic alphabets. the problem exists for many writing systems and
   is mostly overlooked by the nlp community, although it   s definitely not
   the hardest problem in nlp. we hope that the solution we develop for
   armenian might become helpful for other languages as well.

data processing

   we are using a recurrent neural network that takes a sequence of
   characters (romanized armenian) at its input and outputs a sequence of
   armenian characters. in order to train such a system we take a lot of
   text in armenian, romanize it using probabilistic rules and give them
   to the network.

source of the data

   we chose armenian wikipedia as the easiest available large corpus of
   armenian text. the dumps are available [28]here. these dumps are in a
   very complicated xml format, but they can be parsed by the
   [29]wikiextractor tool. the details are in the [30]readme file of the
   repository we released today.

   the disadvantage of wiki is that it doesn   t contain very diverse texts.
   for example, it doesn   t contain any dialogs or non formal speech (while
   social networks are full of them). on the other hand it   s very easy to
   parse and it   s quite large (356mb). we splitted this into training
   (284mb), validation (36mb) and test (36mb) sets, but then we understood
   that the overlap between training and validation sets can be very high.
   finally we decided to use some [31]fiction text with lots of dialogs as
   a validation set.

romanization rules

   to generate the input sequences for the network we need to romanize the
   texts. we use probabilistic rules, as different people prefer different
   romanizations. armenian alphabet has 39 characters, while latin has
   only 26. some of the armenian letters are romanized in a unique way,
   like   -a,   -b,   -d,   -i,   -m,   -n. some letters require a combination
   of two latin letters:   -sh,   -zh,   -kh. the latter is also romanized to
   gh or even x (because this one looks like russian    which is pronounced
   the same way as armenian   ).

   but the main obstacle is that the same latin character can correspond
   to different armenian letters. for example c can come from both    and
     , t can come from both    and   , and so on. this is what the network
   has to learn to infer from the context.

   we have created a probabilistic mapping, so that each armenian letter
   is romanized according to the given probabilities. for example,    is
   replaced by ts in 60% of cases, c in 30% of cases, and & in 10% of
   cases. the full set of rules are here and can be browsed [32]here.
   some of the romanization rules for armenian
   some of the romanization rules for armenian

geographic dependency

   the romanization rules vary a lot in different countries. for example,
   armenian letter    is mostly romanized as sh, but armenians in germany
   prefer sch, armenians in france sometimes use ch, and armenians in
   russia use w (because w is visually similar to russian    which sounds
   like sh). there are many other similar differences that might require
   separate analysis.

   finally, armenian language has two branches: eastern and western
   armenian. these branches have crucial differences in romanization
   rules. here we focus only on the rules for eastern armenian and those
   that are commonly used in armenia.

filtering out large non-armenian chunks

   wikidumps contain some large regions where there are no armenian
   characters. we noticed that these regions were confusing the network.
   so now when generating a chunk to give to the system we drop the ones
   that do not contain at least 33% armenian characters.

   this is a difficult decision, as one might want the system to recognize
   english words in the text and leave them without id68. for
   example, the word you tube should not be transliterated to armenian. we
   hope that such small cases of english words/names will remain in the
   training set.

network architecture

   our search for a good network architecture started from [33]lasagne
   implementation of [34]karpathy   s popular char-id56 network. char-id56 is
   a language model, it predicts the next character given the previous
   ones and is based on 2 layers of lstms going from left to right. the
   context from the right is also important in our case, so we replaced
   simple lstms with [35]bidirectional lstms (introduced [36]here back in
   1995).

   we have also added a shortcut connection from the input to the output
   of the 2nd bilstm layer. this should help to learn the    easy   
   id68 rules on this short way and leave lstms for the complex
   stuff.

   just like char-id56, our network works on character level data and has
   no access to dictionaries.

encoding the characters

   first we define the set of possible characters (   vocabularies   ) for the
   input and the output. the input    vocabulary    contains all the
   characters that appear in the right hand sides of the romanization
   rules, the digits and some punctuation (that can provide useful
   context). then a special program runs over the entire corpus, generates
   the romanized version, and every symbol outside the input vocabulary is
   replaced by some placeholder symbol (#) in both original and romanized
   versions. the symbols that are left in the original version form the
      output vocabulary   .

   all symbols are encoded as one-hot vectors and are passed to the
   network. in our case the input vectors are 72 dimensional and the
   output vectors are 152 dimensional.

aligning

   after some experiments we noticed that lstms are really struggling when
   the characters are not aligned in inputs and outputs. as one armenian
   character can be replaced by 2 or 3 latin characters, the input and
   output sequences usually have different lengths, and the network has to
      remember    by how many characters the romanized sequence is ahead of
   the armenian sequence in order to print the next character in the
   correct place. this turned to be extremely difficult, and we decided to
   explicitly align the armenian sequence by [37]adding some placeholder
   symbols after those characters that are romanized to multi-character
   latin.
   character level alignment of armenian text with the romanization
   character level alignment of armenian text with the romanization

   also there is one exceptional case in armenian: the latin letter    u   
   should be transliterated to 2 armenian symbols:     . this is another
   source of misalignment. we [38]explicitly replace all      pairs with
   some placeholder symbol to avoid the problem.

bidirectional lstm with residual-like connections

   lstm network expects a sequence of vectors at its input. in our case it
   is a sequence of one-hot vectors, and the sequence length is a
   hyperparameter. we used --seq_len 30 for the final model. this means
   that the network reads 30 characters in armenian, transforms to latin
   characters (it usually becomes a bit longer than 30), then crops up to
   the latest whitespace before the 30th symbol. the remaining cells are
   filled with another placeholder symbol. this ensures that the words are
   not split in the middle.
                         network architecture
   network architecture. green boxes encapsulate all the magic inside
   lstm. grey trapezoids denote dense connections. dotted line is an
   identity connection without trainable parameters.

   these 30 one-hot vectors are passed to the first layer of bidirectional
   lstm. basically it is a combination of two separate lstms, first one is
   passing over the sequence from left to right, and the other is passing
   from right to left. we use 1024 neurons in all lstms. both lstms output
   some 1024-dimensional vectors at every position. these outputs are
   [39]concatenated into a 2048 dimensional vector and are passed through
   another dense layer that outputs a 1024 dimensional vector. that   s what
   we call one layer of a bidirectional lstm. the number of such layers is
   another hyperparameter (--depth). our experiments showed that 2 layers
   learn better than 1 or 3 layers.

   at every position the output of the last bidirectional lstm is
   [40]concatenated with the one-hot vector of the input forming a 1096
   dimensional vector. then it is densely connected to the final layer
   with 152 neurons on which softmax is applied. the total loss is the
   mean of the cross id178 losses of the current sequence.

   the concatenation of the input vector to the output of the lstm is
   similar to the residual connections introduced in [41]deep residual
   networks. some of the id68 rules are very easy and
   deterministic, so they can be learned by a diagonal-like matrix between
   input and output vectors. for more complex rules the output of lstms
   will become important. one important difference from deep residual
   networks is that instead of adding the input vector to the output of
   lstms, we just concatenate them. also, our residual connections do not
   help fighting the vanishing/exploding gradient problem, we have lstm
   for that.

results

   we have trained this network using adagrad algorithm with gradient
   clipping (learning rate was set to 0.01 and was not modified). training
   is not very stable and it   s very hard to wait until it overfits on our
   hardware (nvidia gtx 980). we use --batch_size 350 and it consumes more
   than 2gb of gpu memory.
python -u train.py --hdim 1024 --depth 2 --seq_len 30 --batch_size 350 &> log

   the model we got for armenian was trained for 42 hours. here are the
   plots of training and validation sets:
                            id168s
   id168s. green is the validation loss, blue is the training
   loss.

   the loss quickly drops in the first quarter of the first epoch, then
   continues to slowly decrease. we stopped after 5.1 epochs. the
   [42]levenshtein distance between the original armenian text and the
   output of the network on the validation test is 405 (the length is
   36694). for example, hayeren.am   s converter output has more than 2500
   id153.

   here are some results.
   romanized snippet from wikipedia (test set) id68 by
   translit-id56
   belgiayi gyuxatntesutyuny belgiayi tntesutyan jyuxeric mekn e  
   gyuxatntesutyany bnorosh e bardzr intyensivutyune, sakayn myec che nra
   der@ erkri tntesutyan mej   byelgian manr ev mijin agrarayin
   tntesutyunneri erkir e   gyuxatntyesutyan mej ogtagortsvox
   hoghataracutyan mot kese patkanum e 5-ic 20 ha unecox fermernerin,
   voronq masnagitacats yen qaxaknerin mterqner matakararelu gorcum, talis
   en apranqayin artadranqi himnakan zangvatse                   
                                                                                                                  
                                                                                                   ,                                
                                                                                                               
                                                                                                                    
                                                                        5-     20                                           ,
                                                                                                                           ,           
                                                                                     

   id153 between this output and the original text is 0. next we
   try some legal text in armenian:
   romanized snippet from armenian constitution id68 by
   translit-id56
   zhoghovurdn ir ishkhanutyunn irakanatsnum e azat yntrutyunneri,
   hanraqveneri, inchpyes naev sahmanadrutyamb naghatesvac petakan ev
   teghakan inqnakaravarman marminnyeri u pashtonatar anzanc midjocov:
                                                                                                                           ,
                           ,                                                                                                           
                                                                                                            :

   there is only one error here. the first word should start by    and not
     . the possible reason for this is that the network doesn   t have a
   left-side context for that character.

   an interesting feature of this system is that it also tries to learn
   when the latin letters should not be converted to armenian. next
   example comes from a random facebook group:
   random post from a facebook group id68 by translit-id56
   aysor aravotyan jamy 10;40   11;00 ynkac hatvacum 47 hamari yertuxayini
   miji txa,vor qez pahecir txamardavari u vori hamar mersi.,xndrum em ete
   kardas pm gri. p.s.anlurj, animast u antexi commentner chgreq,karevor e
   u lurj.                                        10;40   11;00                               47             
                                         ,                                                                          
         si.,                                                   . p.s.              ,                                 
                                  ,                               .

   it is interesting that the sequence p.s. is not transliterated. also it
   decided to leave half of the letters of mersi in latin which is
   probably because it   s written in all caps (wikipedia doesn   t contain a
   lot of text in all caps, maybe except some abbreviations). also, the
   word commentner is transliterated as                      (instead of                   ),
   because it   s not really a romanized armenian word, it just includes the
   english word comment (and it definitely doesn   t appear in wiki).

future work

   first we plan to understand what the system actually learned by
   visualizing its behavior on different cases. it is interesting to see
   how the residual connection performed and also if the network managed
   to discover some rules known from armenian orthography.

   next, we want to bring this tool to the web. we will have to make much
   smaller/faster model, translate it to javascript, and probably wrap it
   in a chrome extension.

   finally, we would like to see this tool applied to more languages. we
   have released all the code in the [43]translit-id56 repository and
   prepared instructions on how to add a new language. basically a large
   corpus and probabilistic romanization rules are required.

   we would like to thank adam mathias bittlingmayer for many valuable
   discussions.
   recurrent neural networks
   natural language processing
   armenian

related posts

     * [44]challenges of reproducing r-net neural network using keras 25
       aug 2017
     * [45]interpreting neurons in an lstm network 27 jun 2017
     * [46]announcing yerevann non-profit foundation 17 oct 2016

   please enable javascript to view the [47]comments powered by disqus.

references

   1. http://yerevann.github.io/atom.xml
   2. http://yerevann.github.io/
   3. https://github.com/tigrangalstyan
   4. https://github.com/harhro94
   5. https://github.com/hrant-khachatrian
   6. https://github.com/yerevann/translit-id56
   7. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#problem-description
   8. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#data-processing
   9. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#source-of-the-data
  10. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#romanization-rules
  11. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#geographic-dependency
  12. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#filtering-out-large-non-armenian-chunks
  13. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#network-architecture
  14. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#encoding-the-characters
  15. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#aligning
  16. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#bidirectional-lstm-with-residual-like-connections
  17. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#results
  18. http://yerevann.github.io/2016/09/09/automatic-id68-with-lstm/#future-work
  19. https://en.wikipedia.org/wiki/armscii
  20. https://en.wikipedia.org/wiki/armenian_alphabet#character_encodings
  21. https://en.wikipedia.org/wiki/armenian_alphabet
  22. http://www.theverge.com/2014/6/2/5773504/developers-already-at-work-on-alternate-ios-8-keyboards/in/6116530
  23. https://en.wikipedia.org/wiki/romanization_of_armenian
  24. https://hayeren.am/?p=convertor
  25. https://en.wikipedia.org/wiki/greeklish
  26. https://en.wikipedia.org/wiki/fingilish
  27. https://en.wikipedia.org/wiki/translit
  28. https://dumps.wikimedia.org/hywiki/
  29. https://github.com/attardi/wikiextractor
  30. https://github.com/yerevann/translit-id56
  31. http://grapaharan.org/index.php/        
  32. http://jsoneditoronline.org/?id=ef9f135c1a0b4f3ad4724f5fa628fb00
  33. https://github.com/lasagne/recipes/blob/master/examples/lstm_text_generation.py
  34. https://github.com/karpathy/char-id56
  35. http://www.cs.toronto.edu/~graves/asru_2013.pdf
  36. ftp://ftp.idsia.ch/pub/juergen/nn_2005.pdf
  37. https://github.com/yerevann/translit-id56/blob/master/utils.py#l227-l232
  38. https://github.com/yerevann/translit-id56/blob/master/utils.py#l160-l166
  39. https://github.com/yerevann/translit-id56/blob/master/utils.py#l283
  40. https://github.com/yerevann/translit-id56/blob/master/utils.py#l292
  41. https://arxiv.org/abs/1512.03385
  42. https://en.wikipedia.org/wiki/levenshtein_distance
  43. https://github.com/yerevann/translit-id56
  44. http://yerevann.github.io/2017/08/25/challenges-of-reproducing-r-net-neural-network-using-keras/
  45. http://yerevann.github.io/2017/06/27/interpreting-neurons-in-an-lstm-network/
  46. http://yerevann.github.io/2016/10/17/announcing-yerevann-non-profit-foundation/
  47. https://disqus.com/?ref_noscript
