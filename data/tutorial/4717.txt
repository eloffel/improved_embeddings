   (button) toggle navigation [1]i am trask
     * [2]home
     * [3]about
     * [4]contact
     *
     *
     *
     *
     *

building safe a.i.

a tutorial for encrypted deep learning

   posted by iamtrask on march 17, 2017

   tldr: in this blogpost, we're going to train a neural network that is
   fully encrypted during training (trained on unencrypted data). the
   result will be a neural network with two beneficial properties. first,
   the neural network's intelligence is protected from those who might
   want to steal it, allowing valuable ais to be trained in insecure
   environments without risking theft of their intelligence. secondly, the
   network can only make encrypted predictions (which presumably have no
   impact on the outside world because the outside world cannot understand
   the predictions without a secret key). this creates a valuable power
   imbalance between a user and a superintelligence. if the ai is
   homomorphically encrypted, then from it's perspective, the entire
   outside world is also homomorphically encrypted. a human controls the
   secret key and has the option to either unlock the ai itself (releasing
   it on the world) or just individual predictions the ai makes (seems
   safer).

   i typically tweet out new blogposts when they're complete at
   [5]@iamtrask. feel free to follow if you'd be interested in reading
   more in the future and thanks for all the feedback!

   edit: if you're interested in training encrypted neural networks, check
   out the [6]pysyft library at openmined

superintelligence

   many people are concerned that superpoweful ai will one day choose to
   harm humanity. most recently, stephen hawking called for a [7]new world
   government to govern the abilities that we give to artificial
   intelligence so that it doesn't turn to destroy us. these are pretty
   bold statements, and i think they reflect the general concern shared
   between both the scientific community and the world at large. in this
   blogpost, i'd like to give a tutorial on a potential technical solution
   to this problem with some toy-ish example code to demonstrate the
   approach.

   the goal is simple. we want to build a.i. technology that can become
   incredibly smart (smart enough to cure cancer, end world hunger, etc.),
   but whose intelligence is controlled by a human with a key, such that
   the application of intelligence is limited. unlimited learning is
   great, but unlimited application of that knowledge is potentially
   dangerous.

   to introduce this idea, i'll quickly describe two very exciting fields
   of research: deep learning and homomorphic encryption.
     __________________________________________________________________

   [ins: :ins]
     __________________________________________________________________

part 1: what is deep learning?

   deep learning is a suite of tools for the automation of intelligence,
   primarily leveraging neural networks. as a field of computer science,
   it is largely responsible for the recent boom in a.i. technology as it
   has surpassed previous quality records for many intelligence tasks. for
   context, it played a big part in [8]deepmind's alphago system that
   recently defeated the world champion go player, lee sedol.

   question: how does a neural network learn?

   a neural network makes predictions based on input. it learns to do this
   effectively by trial and error. it begins by making a prediction (which
   is largely random at first), and then receives an "error signal"
   indiciating that it predicted too high or too low (usually
   probabilities). after this cycle repeats many millions of times, the
   network starts figuring things out. for more detail on how this works,
   see [9]a neural network in 11 lines of python

   the big takeaway here is this error signal. without being told how well
   it's predictions are, it cannot learn. this will be important to
   remember.

part 2: what is homomorphic encryption?

   as the name suggests, [10]homomorphic encryption is a form of
   encryption. in the asymmetric case, it can take perfectly readable text
   and turn it into jibberish using a "public key". more importantly, it
   can then take that jibberish and turn it back into the same text using
   a "secret key". however, unless you have the "secret key", you cannot
   decode the jibberish (in theory).

   homomorphic encryption is a special type of encryption though. it
   allows someone to modify the encrypted information in specific ways
   without being able to read the information. for example, homomorphic
   encryption can be performed on numbers such that multiplication and
   addition can be performed on encrypted values without decrypting them.
   here are a few toy examples.

   now, there are a growing number of homomorphic encryption schemes, each
   with different properties. it's a relatively young field and there are
   several significant problems still being worked through, but we'll come
   back to that later.

   for now, let's just start with the following. integer public key
   encryption schemes that are homomorphic over multiplication and
   addition can perform the operations in the picture above. furthermore,
   because the public key allows for "one way" encryption, you can even
   perform operations between unencrypted numbers and encrypted numbers
   (by one-way encrypting them), as exemplified above by 2 * cypher a.
   (some encryption schemes don't even require that... but again... we'll
   come back to that later)

part 3: can we use them together?

   perhaps the most frequent intersection between deep learning and
   homomorphic encryption has manifested around data privacy. as it turns
   out, when you homomorphically encrypt data, you can't read it but you
   still maintain most of the interesting statistical structure. this has
   allowed people to train models on encrypted data ([11]cryptonets).
   furthermore a startup hedge fund called [12]numer.ai encrypts
   expensive, proprietary data and allows anyone to attempt to train
   machine learning models to predict the stock market. normally they
   wouldn't be able to do this becuase it would constitute giving away
   incredibly expensive information. (and normal encryption would make
   model training impossible)

   however, this blog post is about doing the inverse, encrypting the
   neural network and training it on decrypted data.

   a neural network, in all its amazing complexity, actually breaks down
   into a surprisingly small number of moving parts which are simply
   repeated over and over again. in fact, many state-of-the-art neural
   networks can be created using only the following operations:
     * addition
     * multiplication
     * division
     * subtraction
     * [13]sigmoid
     * [14]tanh
     * [15]exponential

   so, let's ask the obvious technical question, can we homomorphically
   encrypt the neural network itself? would we want to? as it turns out,
   with a few conservative approximations, this can be done.
     * addition - works out of the box
     * multiplication - works out of the box
     * division - works out of the box? - simply 1 / multiplication
     * subtraction - works out of the box? - simply negated addition
     * [16]sigmoid - id48m... perhaps a bit harder
     * [17]tanh - id48m... perhaps a bit harder
     * [18]exponential - id48m... perhaps a bit harder

   it seems like we'll be able to get division and subtraction pretty
   trivially, but these more complicated functions are... well... more
   complicated than simple addition and multiplication. in order to try to
   homomorphically encrypt a deep neural network, we need one more secret
   ingredient.

part 4: taylor series expansion

   perhaps you remember it from primary school. a [19]taylor series allows
   one to compute a complicated (nonlinear) function using an infinite
   series of additions, subtractions, multiplications, and divisions. this
   is perfect! (except for the infinite part). fortunately, if you stop
   short of computing the exact taylor series expansion you can still get
   a close approximation of the function at hand. here are a few popular
   functions approximated via taylor series ([20]source).

   wait! there are exponents! no worries. exponents are just repeated
   multiplication, which we can do. for something to play with, here's a
   little python implementation approximating the taylor series for our
   desirable sigmoid function (the formula for which you can lookup on
   [21]wolfram alpha). we'll take the first few parts of the series and
   see how close we get to the true sigmoid function.

   iframe: [22]https://trinket.io/embed/python/0fc12dd1f6

   with only the first four factors of the taylor series, we get very
   close to sigmoid for a relatively large series of numbers. now that we
   have our general strategy, it's time to select a homomorphic encryption
   algorithm.

part 5: choosing an encryption algorithm

   homomorphic encryption is a relatively new field, with the major
   landmark being the discovery of the [23]first fully homomorphic
   algorithm by craig gentry in 2009. this landmark event created a
   foothold for many to follow. most of the excitement around homomorphic
   encryption has been around developing turing complete, homomorphically
   encrypted computers. thus, the quest for a fully homomorphic scheme
   seeks to find an algorithm that can efficiently and securely compute
   the various logic gates required to run arbitrary computation. the
   general hope is that people would be able to securely offload work to
   the cloud with no risk that the data being sent could be read by anyone
   other than the sender. it's a very cool idea, and a lot of progress has
   been made.

   however, there are some drawbacks. in general, most fully homomorphic
   encryption schemes are incredibly slow relative to normal computers
   (not yet practical). this has sparked an interesting thread of research
   to limit the number of operations to be somewhat homomorphic so that at
   least some computations could be performed. less flexible but faster, a
   common tradeoff in computation.

   this is where we want to start looking. in theory, we want a
   homomorphic encryption scheme that operates on floats (but we'll settle
   for integers, as we'll see) instead of binary values. binary values
   would work, but not only would it require the flexibility of fully
   homomorphic encryption (costing performance), but we'd have to manage
   the logic between binary representations and the math operations we
   want to compute. a less powerful, tailored he algorithm for floating
   point operations would be a better fit.

   despite this constraint, there is still a plethora of choices. here are
   a few popular ones with characteristics we like:
     * [24]efficient homomorphic encryption on integer vectors and its
       applications
     * [25]yet another somewhat homomorphic encryption (yashe)
     * [26]somewhat practical fully homomorphic encryption (fv)
     * [27]fully homomorphic encryption without id64

   the best one to use here is likely either yashe or fv. yashe was the
   method used for the popular cryptonets algorithm, with great support
   for floating point operations. however, it's pretty complex. for the
   purpose of making this blogpost easy and fun to play around with, we're
   going to go with the slightly less advanced (and possibly [28]less
   secure) efficient integer vector homomorphic encryption. however, i
   think it's important to note that new he algorithms are being developed
   as you read this, and the ideas presented in this blogpost are generic
   to any schemes that are homomorphic over addition and multiplication of
   integers and/or floating point numbers. if anything, it is my hope to
   raise awareness for this application of he such that more he algos will
   be developed to optimize for deep learning.

   this encryption algorithm is also covered extensively by yu, lai, and
   paylor in [29]this work with an accompanying implementation [30]here.
   the main bulk of the approach is in the c++ file vhe.cpp. below we'll
   walk through a python port of this code with accompanying explanation
   for what's going on. this will also be useful if you choose to
   implement a more advanced scheme as there are themes that are
   relatively universal (general function names, variable names, etc.).

part 6: homomorphic encryption in python

   let   s start by covering a bit of the homomorphic encryption jargon:
     * plaintext: this is your un-encrypted data. it's also called the
       "message". in our case, this will be a bunch of numbers
       representing our neural network.
     * cyphertext: this is your encrypted data. we'll do math operations
       on the cyphertext which will change the underlying plaintext.
     * public key: this is a pseudo-random sequence of numbers that allows
       anyone to encrypt data. it's ok to share this with people because
       (in theory) they can only use it for encryption.
     * private/secret key: this is a pseudo-random sequence of numbers
       that allows you to decrypt data that was encrypted by the public
       key. you do not want to share this with people. otherwise, they
       could decrypt your messages.

   so, those are the major moving parts. they also correspond to
   particular variables with names that are pretty standard across
   different homomorphic encryption techniques. in this paper, they are
   the following:

     * s: this is a matrix that represents your secret/private key. you
       need it to decrypt stuff.
     * m: this is your public key. you'll use it to encrypt stuff and
       perform math operations. some algorithms don't require the public
       key for all math operations but this one uses it quite extensively.
     * c: this vector is your encrypted data, your "cyphertext".
     * x: this corresponds to your message, or your "plaintext". some
       papers use the variable "m" instead.
     * w: this is a single "weighting" scalar variable which we use to
       re-weight our input message x (make it consistently bigger or
       smaller). we use this variable to help tune the signal/noise ratio.
       making the signal "bigger" makes it less susceptible to noise at
       any given operation. however, making it too big increases our
       likelihood of corrupting our data entirely. it's a balance.
     * e or e: generally refers to random noise. in some cases, this
       refers to noise added to the data before encrypting it with the
       public key. this noise is generally what makes the decryption
       difficult. it's what allows two encryptions of the same message to
       be different, which is important to make the message hard to crack.
       note, this can be a vector or a matrix depending on the algorithm
       and implementation. in other cases, this can refer to the noise
       that accumulates over operations. more on that later.

   as is convention with many math papers, capital letters correspond to
   matrices, lowercase letters correspond to vectors, and italic lowercase
   letters correspond to scalars. homomorphic encryption has four kinds of
   operations that we care about: public/private keypair generation,
   one-way encryption, decryption, and the math operations. let's start
   with decryption.

   the formula on the left describes the general relationship between our
   secret key s and our message x. the formula on the right shows how we
   can use our secret key to decrypt our message. notice that "e" is gone?
   basically, the general philosophy of homomorphic encryption techniques
   is to introduce just enough noise that the original message is hard to
   get back without the secret key, but a small enough amount of noise
   that it amounts to a rounding error when you do have the secret key.
   the brackets on the top and bottom represent "round to the nearest
   integer". other homomorphic encryption algorithms round to various
   amounts. modulus operators are nearly ubiquitous. encryption, then, is
   about generating a c so that this relationship holds true. if s is a
   random matrix, then c will be hard to decrypt. the simpler,
   non-symmetric way of generating an encryption key is to just find the
   inverse of the secret key. let's start there with some python code.
import numpy as np

def generate_key(w,m,n):
    s = (np.random.rand(m,n) * w / (2 ** 16)) # proving max(s) < w
    return s

def encrypt(x,s,m,n,w):
    assert len(x) == len(s)

    e = (np.random.rand(m)) # proving max(e) < w / 2
    c = np.linalg.inv(s).dot((w * x) + e)
    return c

def decrypt(c,s,w):
    return (s.dot(c) / w).astype('int')

def get_c_star(c,m,l):
    c_star = np.zeros(l * m,dtype='int')
    for i in range(m):
        b = np.array(list(np.binary_repr(np.abs(c[i]))),dtype='int')
        if(c[i] < 0):
            b *= -1
        c_star[(i * l) + (l-len(b)): (i+1) * l] += b
    return c_star

def get_s_star(s,m,n,l):
    s_star = list()
    for i in range(l):
        s_star.append(s*2**(l-i-1))
    s_star = np.array(s_star).transpose(1,2,0).reshape(m,n*l)
    return s_star


x = np.array([0,1,2,5])

m = len(x)
n = m
w = 16
s = generate_key(w,m,n)

   and when i run this code in an ipython notebook, i can perform the
   following operations (with corresponding output).

   the key thing to look at are the bottom results. notice that we can
   perform some basic operations to the cyphertext and it changes the
   underlying plaintext accordingly. neat, eh?

part 7: optimizing encryption

   import lesson: take a look at the decryption formulas again. if the
   secret key, s, is the identity matrix, then cyphertext c is just a
   re-weighted, slightly noisy version of the input x, which could easily
   be discovered given a handful of examples. if this doesn't make sense,
   google "identity matrix tutorial" and come back. it's a bit too much to
   go into here.

   this leads us into how encryption takes place. instead of explicitly
   allocating a self-standing "public key" and "private key", the authors
   propose a "key switching" technique, wherein they can swap out one
   private key s for another s'. more specifically, this private key
   switching technique involves generating a matrix m that can perform the
   transformation.since m has the ability to convert a message from being
   unencrypted (secret key of the identity matrix) to being encrypted
   (secret key that's random and difficult to guess), this m becomes our
   public key!

   that was a lot of information at a fast pace. let's nutshell that
   again.

here's what happened...

    1. given the two formulas above, if the secret key is the identity
       matrix, the message isn't encrypted.
    2. given the two formulas above, if the secret key is a random matrix,
       the generated message is encrypted.
    3. we can make a matrix m that changes the secret key from one secret
       key to another.
    4. when the matrix m converts from the identity to a random secret
       key, it is, by extension, encrypting the message in a one-way
       encryption.
    5. because m performs the role of a "one way encryption", we call it
       the "public key" and can distribute it like we would a public key
       since it cannot decrypt the code.

   so, without further adue, let's see how this is done in python.

import numpy as np

def generate_key(w,m,n):
    s = (np.random.rand(m,n) * w / (2 ** 16)) # proving max(s) < w
    return s

def encrypt(x,s,m,n,w):
    assert len(x) == len(s)

    e = (np.random.rand(m)) # proving max(e) < w / 2
    c = np.linalg.inv(s).dot((w * x) + e)
    return c

def decrypt(c,s,w):
    return (s.dot(c) / w).astype('int')

def get_c_star(c,m,l):
    c_star = np.zeros(l * m,dtype='int')
    for i in range(m):
        b = np.array(list(np.binary_repr(np.abs(c[i]))),dtype='int')
        if(c[i] < 0):
            b *= -1
        c_star[(i * l) + (l-len(b)): (i+1) * l] += b
    return c_star

def switch_key(c,s,m,n,t):
    l = int(np.ceil(np.log2(np.max(np.abs(c)))))
    c_star = get_c_star(c,m,l)
    s_star = get_s_star(s,m,n,l)
    n_prime = n + 1


    s_prime = np.concatenate((np.eye(m),t.t),0).t
    a = (np.random.rand(n_prime - m, n*l) * 10).astype('int')
    e = (1 * np.random.rand(s_star.shape[0],s_star.shape[1])).astype('int')
    m = np.concatenate(((s_star - t.dot(a) + e),a),0)
    c_prime = m.dot(c_star)
    return c_prime,s_prime

def get_s_star(s,m,n,l):
    s_star = list()
    for i in range(l):
        s_star.append(s*2**(l-i-1))
    s_star = np.array(s_star).transpose(1,2,0).reshape(m,n*l)
    return s_star

def get_t(n):
    n_prime = n + 1
    t = (10 * np.random.rand(n,n_prime - n)).astype('int')
    return t

def encrypt_via_switch(x,w,m,n,t):
    c,s = switch_key(x*w,np.eye(m),m,n,t)
    return c,s

x = np.array([0,1,2,5])

m = len(x)
n = m
w = 16
s = generate_key(w,m,n)


   the way this works is by making the s key mostly the identiy matrix,
   simply concatenating a random vector t onto it. thus, t really has all
   the information necessary for the secret key, even though we have to
   still create a matrix of size s to get things to work right.

part 8: building an xor neural network

   so, now that we know how to encrypt and decrypt messages (and compute
   basic addition and multiplication), it's time to start trying to expand
   to the rest of the operations we need to build a simple xor neural
   network. while technically neural networks are just a series of very
   simple operations, there are several combinations of these operations
   that we need some handy functions for. so, here i'm going to describe
   each operation we need and the high level approach we're going to take
   (basically which series of additions and multiplications we'll use).
   then i'll show you code. for detailed descriptions check out [31]this
   work
     * floating point numbers: we're going to do this by simply scaling
       our floats into integers. we'll train our network on integers as if
       they were floats. let's say we're scaling by 1000. 0.2 * 0.5 = 0.1.
       if we scale up, 200 * 500 = 100000. we have to scale down by 1000
       twice since we performed multiplication, but 100000 / (1000 * 1000)
       = 0.1 which is what we want. this can be tricky at first but you'll
       get used to it. since this he scheme rounds to the nearest integer,
       this also lets you control the precision of your neural net.
     * vector-id127: this is our bread and butter. as it
       turns out, the m matrix that converts from one secret key to
       another is actually a way to linear transform.
     * inner dot product: in the right context, the linear transformation
       above can also be an inner dot product.
     * sigmoid: since we can do vector-id127, we can
       evaluate arbitrary polynomials given enough multiplications. since
       we know the taylor series polynomial for sigmoid, we can evaluate
       an approximate sigmoid!
     * elementwise id127: this one is surprisingly
       inefficient. we have to do a vector-id127 or a
       series of inner dot products.
     * outer product: we can accomplish this via masking and inner
       products.

   as a general disclaimer, there might be more effient ways of
   accomplishing these methods, but i didn't want to risk compromising the
   integrity of the homomorphic encryption scheme, so i sortof bent over
   backwards to just use the provided functions from the paper (with the
   allowed extension to sigmoid). now, let's see how these are
   accomplished in python.

def sigmoid(layer_2_c):
    out_rows = list()
    for position in range(len(layer_2_c)-1):

        m_position = m_onehot[len(layer_2_c)-2][0]

        layer_2_index_c = innerprod(layer_2_c,v_onehot[len(layer_2_c)-2][positio
n],m_position,l) / scaling_factor

        x = layer_2_index_c
        x2 = innerprod(x,x,m_position,l) / scaling_factor
        x3 = innerprod(x,x2,m_position,l) / scaling_factor
        x5 = innerprod(x3,x2,m_position,l) / scaling_factor
        x7 = innerprod(x5,x2,m_position,l) / scaling_factor

        xs = copy.deepcopy(v_onehot[5][0])
        xs[1] = x[0]
        xs[2] = x2[0]
        xs[3] = x3[0]
        xs[4] = x5[0]
        xs[5] = x7[0]

        out = mat_mul_forward(xs,h_sigmoid[0:1],scaling_factor)
        out_rows.append(out)
    return transpose(out_rows)[0]

def load_linear_transformation(syn0_text,scaling_factor = 1000):
    syn0_text *= scaling_factor
    return lineartransformclient(syn0_text.t,getsecretkey(t_keys[len(syn0_text)-
1]),t_keys[len(syn0_text)-1],l)

def outer_product(x,y):
    flip = false
    if(len(x) < len(y)):
        flip = true
        tmp = x
        x = y
        y = tmp

    y_matrix = list()

    for i in range(len(x)-1):
        y_matrix.append(y)

    y_matrix_transpose = transpose(y_matrix)

    outer_result = list()
    for i in range(len(x)-1):
        outer_result.append(mat_mul_forward(x * onehot[len(x)-1][i],y_matrix_tra
nspose,scaling_factor))

    if(flip):
        return transpose(outer_result)

    return outer_result

def mat_mul_forward(layer_1,syn1,scaling_factor):

    input_dim = len(layer_1)
    output_dim = len(syn1)

    buff = np.zeros(max(output_dim+1,input_dim+1))
    buff[0:len(layer_1)] = layer_1
    layer_1_c = buff

    syn1_c = list()
    for i in range(len(syn1)):
        buff = np.zeros(max(output_dim+1,input_dim+1))
        buff[0:len(syn1[i])] = syn1[i]
        syn1_c.append(buff)

    layer_2 = innerprod(syn1_c[0],layer_1_c,m_onehot[len(layer_1_c) - 2][0],l) /
 float(scaling_factor)
    for i in range(len(syn1)-1):
        layer_2 += innerprod(syn1_c[i+1],layer_1_c,m_onehot[len(layer_1_c) - 2][
i+1],l) / float(scaling_factor)
    return layer_2[0:output_dim+1]

def elementwise_vector_mult(x,y,scaling_factor):

    y =[y]

    one_minus_layer_1 = transpose(y)

    outer_result = list()
    for i in range(len(x)-1):
        outer_result.append(mat_mul_forward(x * onehot[len(x)-1][i],y,scaling_fa
ctor))

    return transpose(outer_result)[0]


   now, there's one bit that i haven't told you about yet. to save time,
   i'm pre-computing several keys, , vectors, and matrices and storing
   them. this includes things like "the vector of all 1s" and one-hot
   encoding vectors of various lengths. this is useful for the masking
   operations above as well as some simple things we want to be able to
   do. for example, the derivive of sigmoid is sigmoid(x) * (1 -
   sigmoid(x)). thus, precomputing these variables is handy. here's the
   pre-computation step.

# happens on secure server

l = 100
w = 2 ** 25

abound = 10
tbound = 10
ebound = 10

max_dim = 10

scaling_factor = 1000

# keys
t_keys = list()
for i in range(max_dim):
    t_keys.append(np.random.rand(i+1,1))

# one way encryption transformation
m_keys = list()
for i in range(max_dim):
    m_keys.append(innerprodclient(t_keys[i],l))

m_onehot = list()
for h in range(max_dim):
    i = h+1
    buffered_eyes = list()
    for row in np.eye(i+1):
        buffer = np.ones(i+1)
        buffer[0:i+1] = row
        buffered_eyes.append((m_keys[i-1].t * buffer).t)
    m_onehot.append(buffered_eyes)

c_ones = list()
for i in range(max_dim):
    c_ones.append(encrypt(t_keys[i],np.ones(i+1), w, l).astype('int'))

v_onehot = list()
onehot = list()
for i in range(max_dim):
    eyes = list()
    eyes_txt = list()
    for eye in np.eye(i+1):
        eyes_txt.append(eye)
        eyes.append(one_way_encrypt_vector(eye,scaling_factor))
    v_onehot.append(eyes)
    onehot.append(eyes_txt)

h_sigmoid_txt = np.zeros((5,5))

h_sigmoid_txt[0][0] = 0.5
h_sigmoid_txt[0][1] = 0.25
h_sigmoid_txt[0][2] = -1/48.0
h_sigmoid_txt[0][3] = 1/480.0
h_sigmoid_txt[0][4] = -17/80640.0

h_sigmoid = list()
for row in h_sigmoid_txt:
    h_sigmoid.append(one_way_encrypt_vector(row))



   if you're looking closely, you'll notice that the h_sigmoid matrix is
   the matrix we need for the polynomial evaluation of sigmoid. :)
   finally, we want to train our neural network with the following. if the
   neural netowrk parts don't make sense, review [32]a neural network in
   11 lines of python. i've basically taken the xor network from there and
   swapped out its operations with the proper utility functions for our
   encrypted weights.

np.random.seed(1234)

input_dataset = [[],[0],[1],[0,1]]
output_dataset = [[0],[1],[1],[0]]

input_dim = 3
hidden_dim = 4
output_dim = 1
alpha = 0.015

# one way encrypt our training data using the public key (this can be done onsit
e)
y = list()
for i in range(4):
    y.append(one_way_encrypt_vector(output_dataset[i],scaling_factor))

# generate our weight values
syn0_t = (np.random.randn(input_dim,hidden_dim) * 0.2) - 0.1
syn1_t = (np.random.randn(output_dim,hidden_dim) * 0.2) - 0.1

# one-way encrypt our weight values
syn1 = list()
for row in syn1_t:
    syn1.append(one_way_encrypt_vector(row,scaling_factor).astype('int64'))

syn0 = list()
for row in syn0_t:
    syn0.append(one_way_encrypt_vector(row,scaling_factor).astype('int64'))


# begin training
for iter in range(1000):

    decrypted_error = 0
    encrypted_error = 0
    for row_i in range(4):

        if(row_i == 0):
            layer_1 = sigmoid(syn0[0])
        elif(row_i == 1):
            layer_1 = sigmoid((syn0[0] + syn0[1])/2.0)
        elif(row_i == 2):
            layer_1 = sigmoid((syn0[0] + syn0[2])/2.0)
        else:
            layer_1 = sigmoid((syn0[0] + syn0[1] + syn0[2])/3.0)

        layer_2 = (innerprod(syn1[0],layer_1,m_onehot[len(layer_1) - 2][0],l) /
float(scaling_factor))[0:2]

        layer_2_delta = add_vectors(layer_2,-y[row_i])

        syn1_trans = transpose(syn1)

        one_minus_layer_1 = [(scaling_factor * c_ones[len(layer_1) - 2]) - layer
_1]
        sigmoid_delta = elementwise_vector_mult(layer_1,one_minus_layer_1[0],sca
ling_factor)
        layer_1_delta_nosig = mat_mul_forward(layer_2_delta,syn1_trans,1).astype
('int64')
        layer_1_delta = elementwise_vector_mult(layer_1_delta_nosig,sigmoid_delt
a,scaling_factor) * alpha

        syn1_delta = np.array(outer_product(layer_2_delta,layer_1)).astype('int6
4')

        syn1[0] -= np.array(syn1_delta[0]* alpha).astype('int64')

        syn0[0] -= (layer_1_delta).astype('int64')

        if(row_i == 1):
            syn0[1] -= (layer_1_delta).astype('int64')
        elif(row_i == 2):
            syn0[2] -= (layer_1_delta).astype('int64')
        elif(row_i == 3):
            syn0[1] -= (layer_1_delta).astype('int64')
            syn0[2] -= (layer_1_delta).astype('int64')


        # so that we can watch training, i'm going to decrypt the loss as we go.
        # if this was a secure environment, i wouldn't be doing this here. i'd s
end
        # the encrypted loss somewhere else to be decrypted
        encrypted_error += int(np.sum(np.abs(layer_2_delta)) / scaling_factor)
        decrypted_error += np.sum(np.abs(s_decrypt(layer_2_delta).astype('float'
)/scaling_factor))


    sys.stdout.write("\r iter:" + str(iter) + " encrypted loss:" + str(encrypted
_error) +  " decrypted loss:" + str(decrypted_error) + " alpha:" + str(alpha))

    # just to make logging nice
    if(iter % 10 == 0):
        print()

    # stop training when encrypted error reaches a certain level
    if(encrypted_error < 25000000):
        break

print("\nfinal prediction:")

for row_i in range(4):

    if(row_i == 0):
        layer_1 = sigmoid(syn0[0])
    elif(row_i == 1):
        layer_1 = sigmoid((syn0[0] + syn0[1])/2.0)
    elif(row_i == 2):
        layer_1 = sigmoid((syn0[0] + syn0[2])/2.0)
    else:
        layer_1 = sigmoid((syn0[0] + syn0[1] + syn0[2])/3.0)

    layer_2 = (innerprod(syn1[0],layer_1,m_onehot[len(layer_1) - 2][0],l) / floa
t(scaling_factor))[0:2]
    print("true pred:" + str(output_dataset[row_i]) + " encrypted prediction:" +
 str(layer_2) + " decrypted prediction:" + str(s_decrypt(layer_2) / scaling_fact
or))



 iter:0 encrypted loss:84890656 decrypted loss:2.529 alpha:0.015
 iter:10 encrypted loss:69494197 decrypted loss:2.071 alpha:0.015
 iter:20 encrypted loss:64017850 decrypted loss:1.907 alpha:0.015
 iter:30 encrypted loss:62367015 decrypted loss:1.858 alpha:0.015
 iter:40 encrypted loss:61874493 decrypted loss:1.843 alpha:0.015
 iter:50 encrypted loss:61399244 decrypted loss:1.829 alpha:0.015
 iter:60 encrypted loss:60788581 decrypted loss:1.811 alpha:0.015
 iter:70 encrypted loss:60327357 decrypted loss:1.797 alpha:0.015
 iter:80 encrypted loss:59939426 decrypted loss:1.786 alpha:0.015
 iter:90 encrypted loss:59628769 decrypted loss:1.778 alpha:0.015
 iter:100 encrypted loss:59373621 decrypted loss:1.769 alpha:0.015
 iter:110 encrypted loss:59148014 decrypted loss:1.763 alpha:0.015
 iter:120 encrypted loss:58934571 decrypted loss:1.757 alpha:0.015
 iter:130 encrypted loss:58724873 decrypted loss:1.75 alpha:0.0155
 iter:140 encrypted loss:58516008 decrypted loss:1.744 alpha:0.015
 iter:150 encrypted loss:58307663 decrypted loss:1.739 alpha:0.015
 iter:160 encrypted loss:58102049 decrypted loss:1.732 alpha:0.015
 iter:170 encrypted loss:57863091 decrypted loss:1.725 alpha:0.015
 iter:180 encrypted loss:55470158 decrypted loss:1.653 alpha:0.015
 iter:190 encrypted loss:54650383 decrypted loss:1.629 alpha:0.015
 iter:200 encrypted loss:53838756 decrypted loss:1.605 alpha:0.015
 iter:210 encrypted loss:51684722 decrypted loss:1.541 alpha:0.015
 iter:220 encrypted loss:54408709 decrypted loss:1.621 alpha:0.015
 iter:230 encrypted loss:54946198 decrypted loss:1.638 alpha:0.015
 iter:240 encrypted loss:54668472 decrypted loss:1.63 alpha:0.0155
 iter:250 encrypted loss:55444008 decrypted loss:1.653 alpha:0.015
 iter:260 encrypted loss:54094286 decrypted loss:1.612 alpha:0.015
 iter:270 encrypted loss:51251831 decrypted loss:1.528 alpha:0.015
 iter:276 encrypted loss:24543890 decrypted loss:0.732 alpha:0.015
 final prediction:
true pred:[0] encrypted prediction:[-3761423723.0718255 0.0] decrypted predictio
n:[-0.112]
true pred:[1] encrypted prediction:[24204806753.166267 0.0] decrypted prediction
:[ 0.721]
true pred:[1] encrypted prediction:[23090462896.17028 0.0] decrypted prediction:
[ 0.688]
true pred:[0] encrypted prediction:[1748380342.4553354 0.0] decrypted prediction
:[ 0.052]


   when i train this neural network, this is the output that i see. tuning
   was a bit tricky as some combination of the encryption noise and the
   low precision creates for somewhat chunky learning. training is also
   quite slow. a lot of this comes back to how expensive the transpose
   operation is. i'm pretty sure that i could do something quite a bit
   simpler, but, again, i wanted to air on the side of safety for this
   proof of concept.

things to takeaway:

     * the weights of the network are all encrypted.
     * the data is decrypted... 1s and 0s.
     * after training, the network could be decrypted for increased
       performance or training (or switch to a different encryption key).
     * the training loss and output predictions are all also encrypted
       values. we have to decode them in order to be able to interpret the
       network.

part 9: sentiment classification

   to make this a bit more real, here's the same network training on imdb
   sentiment reviews based on a [33]network from udacity's deep learning
   nanodegree. you can find the full code [34]here

import time
import sys
import numpy as np

# let's tweak our network from before to model these phenomena
class sentimentnetwork:
    def __init__(self, reviews,labels,min_count = 10,polarity_cutoff = 0.1,hidde
n_nodes = 8, learning_rate = 0.1):

        np.random.seed(1234)

        self.pre_process_data(reviews, polarity_cutoff, min_count)

        self.init_network(len(self.review_vocab),hidden_nodes, 1, learning_rate)


    def pre_process_data(self,reviews, polarity_cutoff,min_count):

        print("pre-processing data...")

        positive_counts = counter()
        negative_counts = counter()
        total_counts = counter()

        for i in range(len(reviews)):
            if(labels[i] == 'positive'):
                for word in reviews[i].split(" "):
                    positive_counts[word] += 1
                    total_counts[word] += 1
            else:
                for word in reviews[i].split(" "):
                    negative_counts[word] += 1
                    total_counts[word] += 1

        pos_neg_ratios = counter()

        for term,cnt in list(total_counts.most_common()):
            if(cnt >= 50):
                pos_neg_ratio = positive_counts[term] / float(negative_counts[te
rm]+1)
                pos_neg_ratios[term] = pos_neg_ratio

        for word,ratio in pos_neg_ratios.most_common():
            if(ratio > 1):
                pos_neg_ratios[word] = np.log(ratio)
            else:
                pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))

        review_vocab = set()
        for review in reviews:
            for word in review.split(" "):
                if(total_counts[word] > min_count):
                    if(word in pos_neg_ratios.keys()):
                        if((pos_neg_ratios[word] >= polarity_cutoff) or (pos_neg
_ratios[word] <= -polarity_cutoff)):
                            review_vocab.add(word)
                    else:
                        review_vocab.add(word)
        self.review_vocab = list(review_vocab)

        label_vocab = set()
        for label in labels:
            label_vocab.add(label)

        self.label_vocab = list(label_vocab)

        self.review_vocab_size = len(self.review_vocab)
        self.label_vocab_size = len(self.label_vocab)

        self.word2index = {}
        for i, word in enumerate(self.review_vocab):
            self.word2index[word] = i

        self.label2index = {}
        for i, label in enumerate(self.label_vocab):
            self.label2index[label] = i


    def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rat
e):
        # set number of nodes in input, hidden and output layers.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes

        print("initializing weights...")
        self.weights_0_1_t = np.zeros((self.input_nodes,self.hidden_nodes))

        self.weights_1_2_t = np.random.normal(0.0, self.output_nodes**-0.5,
                                                (self.hidden_nodes, self.output_
nodes))

        print("encrypting weights...")
        self.weights_0_1 = list()
        for i,row in enumerate(self.weights_0_1_t):
            sys.stdout.write("\rencrypting weights from layer 0 to layer 1:" + s
tr(float((i+1) * 100) / len(self.weights_0_1_t))[0:4] + "% done")
            self.weights_0_1.append(one_way_encrypt_vector(row,scaling_factor).a
stype('int64'))
        print("")

        self.weights_1_2 = list()
        for i,row in enumerate(self.weights_1_2_t):
            sys.stdout.write("\rencrypting weights from layer 1 to layer 2:" + s
tr(float((i+1) * 100) / len(self.weights_1_2_t))[0:4] + "% done")
            self.weights_1_2.append(one_way_encrypt_vector(row,scaling_factor).a
stype('int64'))
        self.weights_1_2 = transpose(self.weights_1_2)

        self.learning_rate = learning_rate

        self.layer_0 = np.zeros((1,input_nodes))
        self.layer_1 = np.zeros((1,hidden_nodes))

    def sigmoid(self,x):
        return 1 / (1 + np.exp(-x))


    def sigmoid_output_2_derivative(self,output):
        return output * (1 - output)

    def update_input_layer(self,review):

        # clear out previous state, reset the layer to be all 0s
        self.layer_0 *= 0
        for word in review.split(" "):
            self.layer_0[0][self.word2index[word]] = 1

    def get_target_for_label(self,label):
        if(label == 'positive'):
            return 1
        else:
            return 0

    def train(self, training_reviews_raw, training_labels):

        training_reviews = list()
        for review in training_reviews_raw:
            indices = set()
            for word in review.split(" "):
                if(word in self.word2index.keys()):
                    indices.add(self.word2index[word])
            training_reviews.append(list(indices))

        layer_1 = np.zeros_like(self.weights_0_1[0])

        start = time.time()
        correct_so_far = 0
        total_pred = 0.5
        for i in range(len(training_reviews_raw)):
            review_indices = training_reviews[i]
            label = training_labels[i]

            layer_1 *= 0
            for index in review_indices:
                layer_1 += self.weights_0_1[index]
            layer_1 = layer_1 / float(len(review_indices))
            layer_1 = layer_1.astype('int64') # round to nearest integer

            layer_2 = sigmoid(innerprod(layer_1,self.weights_1_2[0],m_onehot[len
(layer_1) - 2][1],l) / float(scaling_factor))[0:2]

            if(label == 'positive'):
                layer_2_delta = layer_2 - (c_ones[len(layer_2) - 2] * scaling_fa
ctor)
            else:
                layer_2_delta = layer_2

            weights_1_2_trans = transpose(self.weights_1_2)
            layer_1_delta = mat_mul_forward(layer_2_delta,weights_1_2_trans,scal
ing_factor).astype('int64')

            self.weights_1_2 -= np.array(outer_product(layer_2_delta,layer_1))
* self.learning_rate

            for index in review_indices:
                self.weights_0_1[index] -= (layer_1_delta * self.learning_rate).
astype('int64')

            # we're going to decrypt on the fly so we can watch what's happening
            total_pred += (s_decrypt(layer_2)[0] / scaling_factor)
            if((s_decrypt(layer_2)[0] / scaling_factor) >= (total_pred / float(i
+2)) and label == 'positive'):
                correct_so_far += 1
            if((s_decrypt(layer_2)[0] / scaling_factor) < (total_pred / float(i+
2)) and label == 'negative'):
                correct_so_far += 1

            reviews_per_second = i / float(time.time() - start)

            sys.stdout.write("\rprogress:" + str(100 * i/float(len(training_revi
ews_raw)))[:4] + "% speed(reviews/sec):" + str(reviews_per_second)[0:5] + " #cor
rect:" + str(correct_so_far) + " #trained:" + str(i+1) + " training accuracy:" +
 str(correct_so_far * 100 / float(i+1))[:4] + "%")
            if(i % 100 == 0):
                print(i)


    def test(self, testing_reviews, testing_labels):

        correct = 0

        start = time.time()

        for i in range(len(testing_reviews)):
            pred = self.run(testing_reviews[i])
            if(pred == testing_labels[i]):
                correct += 1

            reviews_per_second = i / float(time.time() - start)

            sys.stdout.write("\rprogress:" + str(100 * i/float(len(testing_revie
ws)))[:4] \
                             + "% speed(reviews/sec):" + str(reviews_per_second)
[0:5] \
                            + "% #correct:" + str(correct) + " #tested:" + str(i
+1) + " testing accuracy:" + str(correct * 100 / float(i+1))[:4] + "%")

    def run(self, review):

        # input layer


        # hidden layer
        self.layer_1 *= 0
        unique_indices = set()
        for word in review.lower().split(" "):
            if word in self.word2index.keys():
                unique_indices.add(self.word2index[word])
        for index in unique_indices:
            self.layer_1 += self.weights_0_1[index]

        # output layer
        layer_2 = self.sigmoid(self.layer_1.dot(self.weights_1_2))

        if(layer_2[0] >= 0.5):
            return "positive"
        else:
            return "negative"



progress:0.0% speed(reviews/sec):0.0 #correct:1 #trained:1 training accuracy:100
.%0
progress:0.41% speed(reviews/sec):1.978 #correct:66 #trained:101 training accura
cy:65.3%100
progress:0.83% speed(reviews/sec):2.014 #correct:131 #trained:201 training accur
acy:65.1%200
progress:1.25% speed(reviews/sec):2.011 #correct:203 #trained:301 training accur
acy:67.4%300
progress:1.66% speed(reviews/sec):2.003 #correct:276 #trained:401 training accur
acy:68.8%400
progress:2.08% speed(reviews/sec):2.007 #correct:348 #trained:501 training accur
acy:69.4%500
progress:2.5% speed(reviews/sec):2.015 #correct:420 #trained:601 training accura
cy:69.8%600
progress:2.91% speed(reviews/sec):1.974 #correct:497 #trained:701 training accur
acy:70.8%700
progress:3.33% speed(reviews/sec):1.973 #correct:581 #trained:801 training accur
acy:72.5%800
progress:3.75% speed(reviews/sec):1.976 #correct:666 #trained:901 training accur
acy:73.9%900
progress:4.16% speed(reviews/sec):1.983 #correct:751 #trained:1001 training accu
racy:75.0%1000
progress:4.33% speed(reviews/sec):1.940 #correct:788 #trained:1042 training accu
racy:75.6%
....

part 10: advantages over data encryption

   the most similar approach to this one is to encrypt training data and
   train neural networks on the encrypted data (accepting encrypted input
   and predicting encrypted output). this is a fantastic idea. however, it
   does have a few drawbacks. first and foremost, encrypting the data
   means that the neural network is completely useless to anyone without
   the private key for the encrypted data. this makes it impossible for
   data from different private sources to be trained on the same deep
   learning model. most commercial applications have this requirement,
   requiring the aggregation of consumer data. in theory, we'd want every
   consumer to be protected by their own secret key, but homomorphically
   encrypting the data requires that everyone use the same key.

   however, encrypting the network doesn't have this restriction.

   with the approach above, you could train a regular, decrypted neural
   network for a while, encrypt it, send it to party a with a public key
   (who trains it for a while on their own data... which remains in their
   possession). then, you could get the network back, decrypt it,
   re-encrypt it with a different key and send it to party b who does some
   training on their data. since the network itself is what's enrypted,
   you get total control over the intelligence that you're capturing along
   the way. party a and party b would have no way of knowing that they
   each received the same network, and this all happens without them ever
   being able to see or use the network on their own data. you, the
   company, retain control over the ip in the neural network, and each
   user retains control over their own data.

part 11: future work

   there are faster and more secure homomorphic encryption algorithms.
   taking this work and porting it to yashe is, i believe, a step in the
   right direction. perhaps a frameowrk would be appropriate to make
   encryption easier for the user, as it has a few systemic complications.
   in general, in order for many of these ideas to reach production level
   quality, he needs to get faster. however, progress is happening
   quickly. i'm sure we'll be there before too long.

part 12: potential applications

   decentralized ai: companies can deploy models to be trained or used in
   the field without risking their intelligence being stolen.

   protected consumer privacy: the previous application opens up the
   possibility that consumers could simply hold onto their data, and "opt
   in" to different models being trained on their lives, instead of
   sending their data somewhere else. companies have less of an excuse if
   their ip isn't at risk via decentralization. data is power and it needs
   to go back to the people.

   controlled superintelligence: the network can become as smart as it
   wants, but unless it has the secret key, all it can do is predict
   jibberish.
     __________________________________________________________________

     * [35]    previous post
     * [36]next post    

     *
     *
     *

   copyright    i am trask 2018

references

   visible links
   1. https://iamtrask.github.io/
   2. https://iamtrask.github.io/
   3. https://iamtrask.github.io/about/
   4. https://iamtrask.github.io/contact/
   5. https://twitter.com/iamtrask
   6. https://github.com/openmined/pysyft
   7. https://futurism.com/stephen-hawking-finally-revealed-his-plan-for-preventing-an-ai-apocalypse/
   8. https://deepmind.com/research/alphago/
   9. http://iamtrask.github.io/2015/07/12/basic-python-network/
  10. https://www.wired.com/2014/11/hacker-lexicon-homomorphic-encryption/
  11. https://arxiv.org/abs/1412.6181
  12. https://medium.com/numerai/encrypted-data-for-efficient-markets-fffbe9743ba8
  13. http://mathworld.wolfram.com/sigmoidfunction.html
  14. http://mathworld.wolfram.com/hyperbolictangent.html
  15. https://en.wikipedia.org/wiki/exponential_function
  16. http://mathworld.wolfram.com/sigmoidfunction.html
  17. http://mathworld.wolfram.com/hyperbolictangent.html
  18. https://en.wikipedia.org/wiki/exponential_function
  19. https://en.wikipedia.org/wiki/taylor_series
  20. http://hyperphysics.phy-astr.gsu.edu/hbase/tayser.html
  21. http://mathworld.wolfram.com/sigmoidfunction.html
  22. https://trinket.io/embed/python/0fc12dd1f6
  23. https://www.cs.cmu.edu/~odonnell/hits09/gentry-homomorphic-encryption.pdf
  24. http://www.rle.mit.edu/sia/wp-content/uploads/2015/04/2014-zhou-wornell-ita.pdf
  25. https://eprint.iacr.org/2013/075.pdf
  26. https://pdfs.semanticscholar.org/531f/8e756ea280f093138788ee896b3fa8ca085a.pdf
  27. http://eprint.iacr.org/2011/277.pdf
  28. https://eprint.iacr.org/2016/775.pdf
  29. https://courses.csail.mit.edu/6.857/2015/files/yu-lai-payor.pdf
  30. https://github.com/jamespayor/vector-homomorphic-encryption
  31. https://courses.csail.mit.edu/6.857/2015/files/yu-lai-payor.pdf
  32. https://iamtrask.github.io/2017/03/17/safe-ai/iamtrask.github.io/2015/07/12/basic-python-network/
  33. https://github.com/udacity/deep-learning/tree/master/sentiment_network
  34. https://github.com/iamtrask/iamtrask.github.io/blob/master/notebooks/vhe+-+sentiment+classification.ipynb
  35. https://2017/01/15/pytorch-tutorial/
  36. https://2017/03/21/synthetic-gradients/

   hidden links:
  38. https://iamtrask.github.io/feed.xml
  39. https://twitter.com/iamtrask
  40. https://github.com/iamtrask
