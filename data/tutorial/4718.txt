   my year at brain

   colin raffel

   august 23rd, 2017

   [1]colinraffel.com/blog

   i've spent the past year doing the [2]google brain residency program.
   it's been a great opportunity for the first year out of my phd, and in
   this blog post i'll describe what the residency was like, what i worked
   on while here, and what i'm doing next.


                                 the residency

   throughout the year i've had a lot of people ask me what the residency
   actually is, since this is the first year brain is running it. the most
   accurate description i've heard is that it's like spending a year in a
   phd, except you're at brain, not at a university. if you're new to
   machine learning, it might be like the first year of a phd (except
   without any classes!). if you already have a lot of experience in ml,
   doing research, writing papers, etc., it may be more like one of the
   later years of a phd. that said, if you do the residency for four or
   five years you won't be able to make people call you    doctor   .

   i think this is an apt description because my day-to-day routine and
   responsibilities felt pretty unchanged compared to what i was doing
   during my phd. specifically, i'd normally spend an hour or two each
   morning reading papers, then i'd spend the day writing code to run
   experiments. from time to time, i'd attend talks by brain researchers
   or people visiting. i had various people serving as my    advisor   , who
   i'd report my research progress to and bounce ideas off of. and, as
   usual, around paper deadlines i'd often be trying to cram my
   experimental results into the page limit for some conference or
   another.

   of course, there have been some substantial differences too, mostly
   id30 from the fact that brain is a very large research lab
   (hundreds of people), whereas most academic labs are considerably
   smaller. what this means in practice is that if you have a question
   about some specific research topic, there's probably someone at brain
   who has worked on it, or more concretely if you're reading some random
   recent machine learning paper there's a good chance one of the authors
   is sitting within a few hundred feet from you. also, it makes for many
   more opportunities for collaboration - maybe too many! i had never had
   to turn down/put off working on promising research with smart people
   before coming to brain.

   another significant difference is that you have a pretty crazy amount
   of computation at your disposal. it is generally not a big deal to spin
   up experiments on dozens or even hundreds of gpus. this does have a
   concrete effect on the type of work you do - there's a lot more room to
   try lots of different ideas, hyperparameter configurations, etc. it
   also means you can be a bit more careless about getting everything just
   right/highly optimized before kicking off an experiment, for better or
   worse. i remember during my phd i spent a considerable amount of time
   benchmarking different theano ops so that i could squeeze out maximal
   performance from my single gpu. at google, there's much less emphasis
   on fine-grain efficiency; instead, the focus is on scaling up.


                                what i worked on

   my goals for the residency were basically 1. do foundational (as
   opposed to application-specific) ml research, 2. learn stuff, 3. take
   advantage of the fact that i was at brain. my background coming in
   mostly involved machine learning models for sequential data (my phd was
   mostly focused on ml applied to music), so this gave me a heavy bias
   when choosing which problems i was going to work on (though i did get a
   chance to branch out a bit).


                             subsampling sequences

   my first project was an attempt to create a model which was able to
   discover hierarchical structure in sequences on its own. many sequences
   can be naturally decomposed into a hierarchy (e.g. document    
   paragraphs     sentences     words     characters), so we might expect a
   model which can discover this hierarchy to be particularly powerful or
   at least more easily interpretable. to attack this problem, i decided
   to focus on a relatively simple mechanism, which can be described as
   follows: say you have an input sequence, and you want to construct a
   shorter output sequence by making a binary yes/no decision as to
   whether you're going to include each entry of the input sequence in the
   output sequence. effectively, you're just    subsampling    the input
   sequence to produce a new shorter sequence. the decision of whether to
   include each input sequence entry in the output can be adaptive, i.e.
   based on the input sequence itself. here's a diagram which illustrates
   this idea:

   [3][subsampling_mechanism.png]
   schematic of the subsampling mechanism. for each entry in the input
   sequence (bottom row,    a, b, c, d, ...   ), an emission id203 is
   produced (middle row, shown in circles). the output sequence (top row)
   is produced by sampling elements in order from the input sequence
   according to the emission probabilities.

   you can imagine that processing a sequence with an id56 and applying
   this    subsampling mechanism    multiple times might encourage the model
   to learn a hierarchical representation, where you can think of each
   element in each successively shorter sequence as representing a coarser
   concept present in the input sequence. fortunately, it's possible to
   derive an expression for the id203 that a given input sequence
   element is included in the output, so you can train a model using this
   mechanism with standard id26. unfortunately, i wasn't able
   to get good results with this approach on any tasks except for a simple
   toy problem. for example, i was only able to get a model using this
   mechanism to achieve a ~30% phone error rate on timit (state of the art
   is around 17%). i nevertheless wrote up the idea in an iclr workshop
   extended abstract [[4]raffel2017a] to make people aware that training a
   subsampling mechanism in expectation is possible.


                              monotonic attention

   one reason i thought the subsampling mechanism might not be working
   well in practice was that the    subsampling probabilities    had to be
   computed all at once and prior to producing any output. to remedy this,
   i made a connection to sequence-to-sequence models with an attention
   mechanism, which can transduce an input sequence into a new output
   sequence. as each element in the output is generated, it's allowed to
      attend    back to entries in the input which helps it more directly
   condition the output generation process. while attention has proven
   extremely effective, its    vanilla    form has a few drawbacks: first,
   these models can't be used online - that is, they can't produce the
   output as the input sequence is being generated; and second, they have
   a quadratic-time complexity, because for each element of the output
   sequence the model must re-process the entire input sequence. however,
   by replacing standard attention with the subsampling mechanism we can
   actually get an online and linear-time sequence-to-sequence model. this
   gives us a sort of    monotonic attention   , where once the model attends
   to a given entry in the input, it can't attend to anything that came
   before it in subsequent output timesteps. as with the subsampling
   mechanism, there is a simple way to train this mechanism in expectation
   without resorting to gradient estimation. schematically, monotonic
   attention looks something like the following:

   [5][monotonic_process.png]
   schematic of the monotonic attention mechanism. at each output
   timestep, the decoder inspects memory entries (indicated in gray) from
   left-to-right starting from where it left off at the previous output
   timestep and chooses a single one (indicated in black). white nodes
   indicate that a particular input-output alignment was not considered
   because it violates monotonicity. arrows indicate the order of
   processing and dependence between memory entries and output timesteps.

   one nice thing about this project was that i was able to leverage
   existing expertise and codebases within google to accelerate
   experiments. unsurprisingly, there were many teams and projects in
   google using sequence-to-sequence models with attention, and i had
   formulated this    monotonic attention    in such a way that hot-swapping
   it with standard attention was relatively straightforward. as a result,
   i implemented this approach in four different codebases; this breadth
   gave me a lot of insight into whether the model was applicable across
   tasks. the culmination of the project was an icml paper
   [[6]raffel2017b]. i also wrote up a more in-depth blog post about it
   [7]here.


                            hard alignments with rl

   an unsurprising (but still unexpected to me) result of working at a lab
   as large as brain (or a company as big as google/alphabet) is that
   there can be multiple groups of people working on very similar ideas in
   parallel. this was the case for the monotonic attention work - some of
   my fellow residents and other colleagues at brain were simultaneously
   working on a very similar model. however, instead of training in
   expectation with id26, they instead focused on training the
   model with hard alignments (i.e. keeping the decision of    what to
   attend to    discrete). this precludes the use of id26, so
   they carried out a thorough investigation of gradient estimation
   techniques (reinforce [[8]williams1992], nvil [[9]mnih2014], vimco
   [[10]mnih2016]) to use instead. the nice thing about this approach is
   that it makes the training and test-time model behavior the same, which
   among other things means that training is linear-time too. the
   difficult part is that variance of the gradient estimates can make
   training hard to get right without some care.

   [11][hard_alignments.png]
   schematic of the online hard-alignment sequence-to-sequence model. the
   model (shown in grey) processes the input sequence x, and at each
   timestep decides to either emit a new token to include in the output
   sequence y, or continue ingesting inputs. this binary ingest/emit
   sequence is denoted b.

   in the beginning, we mostly shared results and insight at a high-level,
   but eventually i ended up trying out their idea on a sentence
   summarization benchmark i had been working with. this was mainly to see
   whether it worked in a setting other than id103, which had
   been their primary testbed. unfortunately it wasn't able to beat a
   simple attention-free sequence-to-sequence baseline, which suggested
   that the model was not really taking advantage of the attention
   mechanism. this could be at least in part because getting the model to
   work on id103 required some id173 which didn't
   generalize well to text data. despite the negative result, this gave us
   some interesting insight into the importance of the different methods
   being used to train the model. after running this experiment, i mostly
   helped out on a high-level again, and they've recently posted a nice
   paper on arxiv about this approach [[12]lawson2017].


                           direct feedback alignment

   sometime during my residency, i was chatting with my fellow resident
   [13]cinjon about synthetic gradients [[14]jaderberg2016] when [15]ben
   (then an intern) came up and said    oh, you think synthetic gradients
   are weird? just wait until you hear about direct feedback alignment.   
   sure enough, direct feedback alignment (dfa) [[16]nokland2016] is a
   pretty weird idea: in normal id26, the network's    error   
   (gradient of loss w.r.t. output nonlinearity pre-activations) is
   backpropagated to earlier layers via multiplication against each
   layer's weight matrix, transposed. in dfa, we instead take the
   network's error and multiply it against a fixed random matrix to
   compute the updates for each layer's parameters. surprisingly, this
   seems to work ok.

   [17][dfa_lafs.png]
   a comparison of direct feedback alignment and    lafs   , our proposed
   layer-wise training method which behaves very similarly to dfa. in dfa,
   the error at the final (top) layer of the network is directly
   propagated to each layer via the fixed random matrices    b   . in lafs, we
   instead compute a separate layer-wise error by projecting each layer's
   activation through the fixed random matrices. figure modified from an
   original version by my co-author [18]justin.

   reading this paper and trying to understand why dfa works led to some
   long discussions with brain team members, some people at deepmind, and
   the paper's author. the eventual culmination of all of this was an iclr
   extended abstract [[19]gilmer2017a], where we show that dfa can be seen
   as extremely similar to layer-wise training, where the layer-wise
   predictions are made through fixed random matrices. the tl;dr version
   being that, under this view, it's not that surprising that dfa works!


                                    magenta

   a nice thing about working at brain for me personally was that, while i
   could learn about new research areas and do foundational ml research,
   there was a group within brain which was partially focused on music:
   [20]magenta. i never did substantial work under the magenta umbrella,
   but they were nice enough to let me come to their meetings and even
   asked me for my opinion on different research ideas from time to time.
   i also got the chance to go to moogfest to represent magenta, where i
   helped lead a tutorial on some of the tools they've developed.


                              what i'm doing next

   my initial post-phd plan was (fittingly) to do a postdoc. after
   deciding to do the residency instead, i still planned to do a postdoc
   after the residency. a postdoc sounds really nice to me, in terms of
   where i'm at in my research career - i still want good top-down advice
   from an advisor, but i've also really enjoyed my mentorship
   opportunities in the past, and a postdoc seems like an ideal
   combination of these dynamics. but, i've ultimately decided to stay at
   brain as a research scientist (and fortunately google is on board with
   this decision). here's why: i think if you asked your average ml
   researcher to describe their ideal research lab, they'd say something
   like    i'd like to be able to work on whatever i want, be surrounded by
   smart people, share my results freely, and have effectively unlimited
   resources.    incidentally, i think you could describe brain this way.

   i honestly don't know of a lab anywhere where researchers have the same
   freedom that they do at brain; i think i realized this when i was
   talking to my fellow resident [21]sam and he said something along the
   lines of    look, i just wrote a paper on chemistry [[22]gilmer2017b].
   what ml-focused research labs would let me do something like that?    as
   far as i can tell, researchers at brain are never required, or even
   asked, to work on research that would benefit some specific product
   (though i get the impression it wouldn't hurt if they did).

   in terms of colleagues, as i mentioned above you are basically
   surrounded by leading ml researchers. in analogy with the postdoc, a
   new crop of residents has just arrived and there are always interns
   around, which provides excellent potential mentorship opportunities. i
   also have come to realize that a huge part of a lab's culture comes
   from its leadership, and i think brain's great culture is largely
   thanks to this. the senior staff at brain are uniformly approachable,
   friendly, and sharing-focused. of course, the more junior people are
   also extremely knowledgeable and hard-working.

   brain also has a comparatively strong bias towards sharing work - in
   particular, i think the fact that tensorflow is open-source and is
   developed here pushes for this a lot. at an academic lab, publishing
   and sharing is basically your job description; the exact same is true
   for research scientists at brain. in other words, there is no
   discernible difference in my job description and deliverables compared
   to (for example) a postdoc's, except that i'm not going to be teaching
   any formal courses.

   so, i'm staying at brain. concretely, i'm working in ian goodfellow's
   group, which means i'll be devoting a lot of my time to ml security,
   adversarial training, and un/semi-supervised learning. i'm looking
   forward to being directly supervised by an expert, while having the
   opportunity to mentor residents and interns who want to work on
   projects in this domain. of course, i'll probably continue to devote a
   bit of time to ml models for sequential data and music, because old
   habits die hard!

   references

   [ raffel2017a]    training a subsampling mechanism in expectation    by
   colin raffel and dieterich lawson (arxiv:1702.06914).

   [ raffel2017b]    online and linear-time attention by enforcing monotonic
   alignments    by colin raffel, minh-thang luong, peter j. liu, ron j.
   weiss, and douglas eck (arxiv:1704.00784).

   [ williams1992]    simple statistical gradient-following algorithms for
   connectionist id23    by ronald j. williams.

   [ mnih2014]    neural variational id136 and learning in belief
   networks " by andriy mnih and karol gregor (arxiv:1402.0030).

   [ mnih2016]    variational id136 for monte carlo objectives " by
   andriy mnih and danilo j. rezende (arxiv:1602.06725).

   [ lawson2017]    learning hard alignments with variational id136    by
   dieterich lawson, george tucker, chung-cheng chiu, colin raffel, kevin
   swersky, and navdeep jaitly (arxiv:1705.05524).

   [ jaderberg2016]    decoupled neural interfaces using synthetic
   gradients    by max jaderberg, wojciech marian czarnecki, simon osindero,
   oriol vinyals, alex graves, david silver and koray kavukcuoglu
   (arxiv:1608.05343).

   [ nokland2016]    direct feedback alignment provides learning in deep
   neural networks    by arild n  kland (arxiv:1609.01596).

   [ gilmer2017a]    explaining the learning dynamcis of direct feedback
   alignment    by justin gilmer, colin raffel, samuel s. schoenholtz,
   maithra raghu and jascha sohl-dickstein.

   [ gilmer2017b]    neural message passing for quantum chemistry    by justin
   gilmer, samuel s. schoenholz, patrick f. riley, oriol vinyals and
   george e. dahl (arxiv:1704.01212).

   formatted by [23]markdeep 1.03
      

references

   1. http://colinraffel.com/blog
   2. https://research.google.com/teams/brain/residency/
   3. https://colinraffel.com/blog/images/residency/subsampling_mechanism.png
   4. https://colinraffel.com/blog/my-year-at-brain.html#citation-raffel2017a
   5. https://colinraffel.com/blog/images/residency/monotonic_process.png
   6. https://colinraffel.com/blog/my-year-at-brain.html#citation-raffel2017b
   7. http://colinraffel.com/blog/online-and-linear-time-attention-by-enforcing-monotonic-alignments.html
   8. https://colinraffel.com/blog/my-year-at-brain.html#citation-williams1992
   9. https://colinraffel.com/blog/my-year-at-brain.html#citation-mnih2014
  10. https://colinraffel.com/blog/my-year-at-brain.html#citation-mnih2016
  11. https://colinraffel.com/blog/images/residency/hard_alignments.png
  12. https://colinraffel.com/blog/my-year-at-brain.html#citation-lawson2017
  13. https://www.linkedin.com/in/cinjonresnick
  14. https://colinraffel.com/blog/my-year-at-brain.html#citation-jaderberg2016
  15. http://cs.stanford.edu/~poole/
  16. https://colinraffel.com/blog/my-year-at-brain.html#citation-nokland2016
  17. https://colinraffel.com/blog/images/residency/dfa_lafs.png
  18. https://www.linkedin.com/in/jmgilmer
  19. https://colinraffel.com/blog/my-year-at-brain.html#citation-gilmer2017a
  20. http://magenta.tensorflow.org/
  21. https://samschoenholz.wordpress.com/
  22. https://colinraffel.com/blog/my-year-at-brain.html#citation-gilmer2017b
  23. http://casual-effects.com/markdeep
