   #[1]rss

[2]fast.ai

   making neural nets uncool again

   [3]home [4]about [5]our mooc [6]posts by topic

      fast.ai 2019. all rights reserved.

notes on state of the art techniques for id38

   written: 25 aug 2017 by jeremy howard

   edit one day later    much to my surprise a lot of people shared this on
   twitter, and much to my delight there were some very helpful and
   interesting comments from people i respect   so check out the [7]thread
   here.

   i cleverly trapped [8]smerity in a twitter dm conversation while he was
   trapped on a train with nothing better to do than answer my dumb
   questions, and i managed to get a download of ~0.001% of what he knows
   about id38. it should be enough to keep me busy for a few
   months    the background of this conversation is that for    version 2    of
   our [9]deep learning course at usf we   re curating and implementing in a
   consistent api the most important best practices in a range of deep
   learning applications, including id161, text, and
   id126s. unfortunately, for text applications the best
   practices are not really collected anywhere, hence the need for the
   smerity-brain-dump.

   i figured i   d make my notes on the conversation into a little blog post
   in case other people find this useful too. i   m assuming people are
   familiar with the topics covered in parts 1 & 2 of [10]our mooc, such
   as id56s, dropout, attentional models, and neural translation. i   ve
   spent the day scouring the internet for other resources too and i   m
   incorporating some of my own research here, so if you see anything dumb
   it   ll almost certainly be my fault, not smerity   s.

pytorch code examples

   smerity pointed to two excellent repositories that seemed to contain
   examples of all the techniques we discussed:
     * [11]awd-lstm language model, which is a very recent release that
       shows substantial improvements in state of the art for language
       modeling, using techniques that are likely to be useful across a
       range of nlp problems. doesn   t work on the latest pytorch, although
       might not need too much tweaking to fix
     * [12]word-level id38 id56, a simpler and really ancient
       (over 6 months old!) id38 example, but still some
       useful example code

   two other interesting libraries to be aware of are:
     * [13]practical pytorch has some nice simple tutorial examples,
       although there are some significant problems around both approach
       (e.g. no test sets!), performance, and occassionally clunky code.
       also it doesn   t take advantage of the torchtext library, which
       makes for some redudent code. but really nicely chosen problems and
       clear descriptions.
     * [14]torchtext is a small but convenient library for some basic text
       processing tasks, and also provides convenient access to a few
       datasets.

techniques to get state of the art (sota) results

   in part 2 of the course we got pretty close to sota in neural
   translation by showing how to use attentional models, dynamic teacher
   forcing, and of course stacked bidirectional lstms. but there have been
   some interesting approaches that have come to the fore since we
   developed that course, and smerity suggested that combining the
   following should get the biggest wins across a range of nlp tasks
   without much additional complexity:
     * two of my favorite researchers, dyer and blunsom, and the
       extraordinary kaggle, computer olympiad, and google ai winner gabor
       melis, published [15]on the state of the art of evaluation in
       neural language models, which curates a few best practices and uses
       hyper-parameter optimization to show great results from lstms
     * smerity et al also recently released a paper curating and combining
       recent nlp techniques and got a big jump in sota on language
       modeling, in [16]regularizing and optimizing lstm language models
     * perhaps the most important addition in this paper is through using
       id193, which both smerity et al ([17]pointer sentinel)
       and grave et al ([18]continuous cache pointer) have extended to
       id38. smerity uses the continuous cache for his most
       recent work and describes it as    the pointer is really really
       really simple! you could apply that to any lm model output and get
       similar gains. it   s more engineering though. all it is: store
       hidden state of lm as history (2000 timesteps), when generating a
       word calculate attention over that history using current lm hidden
       state, your word distribution is based on the next word according
       to where your hidden state is most similar in the past   .
     * also shown as important in both the melis and smerity summary
       papers above is thoughtful use of id173. smerity found
       that [19]dropconnect (which doesn   t really help id98s much over
       regular dropout) actually works very nicely for id56s, which makes
       intuitive sense when you think about it    in his words    given an
       id56, h = wx + uh_t-1 + b, you apply dropout to the recurrent weight
       matrix u and have it the same over the entire forward + backward.
       simple, fast, and prevents overfitting from h to h to h to        he
       calls this the weight-dropped lstm, and he implemented it in a very
       brief class called [20]weightdrop in the recent id38
       project..
     * something that seems kinda obvious but actually only got written up
       a year ago is weight-tying, described in [21]using the output
       embedding to improve language models. it simply means that the
       output weights and the input embedding weights are the same matrix,
       and it   s handled in the basic language model example above with a
       single line of code (self.decoder.weight = self.encoder.weight)
     * finally, smerity suggested adding    activation id173 (add
       l2 of lstm output as loss) and temporal activation id173
       (add l2 of ht - h_t-1 to loss). that is like three lines of change
       and gets you near old sota with weight tying.    the approach is
       described in [22]this paper.

problems to solve

   interesting problems to solve in the nlp world, with meaningful
   benchmarks, include:
     * id31, or similar standard classification or
       regression tasks, are great for learning since they   re very similar
       to other classification and regression problems, so you can focus
       on the text analysis issues rather than the problem description.
       and they   re widely useful. there   s a [23]huge dataset of amazon
       reviews that can be used, although i   m not aware of good
       benchmarks. the [24]imdb review dataset has been widely studied so
       there   s plenty of good benchmarks (we already use this dataset in
       our mooc). any group of documents that have some kind of categories
       can be used too, such as the [25]20 newsgroup dataset that we used
       in our [26]compuational id202 course. (btw i don   t
       recommend using stanford   s sentiment dataset, since it contains
       phrase annotations as well as sentence annotations, which seems
       rather artifical and designed to make the researchers    tree-based
       techniques look more impressive.)
     * id38, which basically means trying to predict the next
       word given the previous words in a sentence. the standard benchmark
       for this is    ptb    (id32), which is available in the
       [27]pytorch id38 example repo.
     * language translation, for which there are many datasets and
       tutorials around nowadays, although i like to think that our
       dataset and approach from part 2 of our course isn   t too bad   
     * id123, which is best explained by the examples on
       [28]stanford   s natural language id136 (snli) corpus page. you
       get pairs of sentences, and have to say if they   re in agreement or
       they contradict. e.g.    a man inspects the uniform of a figure in
       some east asian country    and    the man is sleeping    would be labeled
       as a contradiction. as well as the above corpus, the excellent sam
       bowman and friends have also recently released [29]the multi-genre
       nli corpus; the two datasets can be combined. it   s a cool
       demonstration of the power of deep learning, although it   s not
       entirely clear (to me at least) whether it   s actually useful   
     * text summarization is an area of active with some recent
       significant advances, although i   m not yet clear on whether it   s
       useful in practice
     * a lot of language workers spend their time on sub-tasks, such as
       part-of-speech tagging and entity recognition, which can then be
       used in larger applications. these particular tasks are part of the
       area of sequence labeling, about which nlp research yoav goldberg
       tweeted:    tons of information extraction tasks can be modeled as
       seq labeling. it   s a killer app.   

   this post is tagged: [ [30]technical ] (click a tag for more posts in
   that category).

related posts

     * [31]fast.ai embracing swift for deep learning 06 mar 2019
     * [32]a conversation about tech ethics with the new york times chief
       data scientist 04 mar 2019
     * [33]dairy farming, solar panels, and diagnosing parkinson's
       disease: what can you do with deep learning? 21 feb 2019

references

   1. https://www.fast.ai/2017/08/25/language-modeling-sota/atom.xml
   2. https://www.fast.ai/
   3. https://www.fast.ai/
   4. https://www.fast.ai/about/
   5. http://course.fast.ai/
   6. https://www.fast.ai/topics
   7. https://twitter.com/jeremyphoward/status/901292853053145088
   8. https://twitter.com/smerity
   9. https://www.usfca.edu/data-institute/certificates/deep-learning-part-one
  10. https://course.fast.ai/
  11. https://github.com/salesforce/awd-lstm-lm
  12. https://github.com/pytorch/examples/tree/master/word_language_model
  13. https://github.com/spro/practical-pytorch/blob/master/char-id56-classification/char-id56-classification.ipynb
  14. https://github.com/pytorch/text
  15. https://arxiv.org/abs/1707.05589
  16. https://arxiv.org/abs/1708.02182
  17. http://slides.com/smerity/stanford-pointer-sentinel#/
  18. https://arxiv.org/abs/1612.04426
  19. http://cs.nyu.edu/~wanli/dropc/
  20. https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py
  21. https://arxiv.org/abs/1608.05859
  22. https://www.fast.ai/2017/08/25/language-modeling-sota/arxiv.org/abs/1708.01009
  23. http://jmcauley.ucsd.edu/data/amazon/
  24. http://ai.stanford.edu/~amaas/data/sentiment/
  25. http://qwone.com/~jason/20newsgroups/
  26. http://www.fast.ai/2017/07/17/num-lin-alg/
  27. https://github.com/pytorch/examples/tree/master/word_language_model
  28. https://nlp.stanford.edu/projects/snli/
  29. https://www.nyu.edu/projects/bowman/multinli/
  30. https://www.fast.ai/tag/technical
  31. https://www.fast.ai/2019/03/06/fastai-swift/
  32. https://www.fast.ai/2019/03/04/ethics-framework/
  33. https://www.fast.ai/2019/02/21/dl-projects/
