   #[1]rss

[2]fast.ai

   making neural nets uncool again

   [3]home [4]about [5]our mooc [6]posts by topic

      fast.ai 2019. all rights reserved.

new fast.ai course: computational id202

   written: 17 jul 2017 by rachel thomas

   i am thrilled to release fast.ai   s newest free course, computational
   id202, including an [7]online textbook and a [8]series of
   videos, and covering applications (using python) such as how to
   identify the foreground in a surveillance video, how to categorize
   documents, the algorithm powering google   s search, how to reconstruct
   an image from a ct scan, and more.

   jeremy and i developed this material for a numerical id202
   course we taught in the [9]university of san francisco   s masters of
   analytics program, and it is the first ever numerical id202
   course, to our knowledge, to be completely centered around practical
   applications and to use cutting edge algorithms and tools, including
   [10]pytorch, [11]numba, and [12]randomized svd. it also covers
   foundational numerical id202 concepts such as floating point
   arithmetic, machine epsilon, singular value decomposition, eigen
   decomposition, and qr decomposition.

what is numerical id202?

      what exactly is numerical id202?    you may be wondering. it is
   all about getting computers to do matrix math with speed and with
   acceptable accuracy, and way more awesome than the somewhat dull name
   suggests. who cares about how computers do matrix math? all of you,
   probably. data science is largely about manipulating matrices since
   almost all data can be represented as a matrix: time-series, structured
   data, anything that fits in a spreadsheet or sql database, images, and
   language (often represented as [13]id27s).

   a typical first id202 course focuses on how to solve matrix
   problems by hand, for instance, spending time using gaussian
   elimination with pencil and paper to solve a small system of equations
   manually. however, it turns out that the methods and concerns for
   solving larger matrix problems via a computer are often drastically
   different:
     * speed: when you have big matrices, matrix computations can get very
       slow. there are different ways of addressing this:
          + different algorithms: there may be a less intuitive way to
            solve the problem that still gets the right answer, and does
            it faster.
          + vectorizing or parallelizing your code.
          + locality: traditional runtime computations focus on big o, the
            number of operations computed. however, for modern computing,
            moving data around in memory can be very time-consuming, and
            you need ways to minimize how much memory movement you do.
     * accuracy: computers represent numbers (which are continuous and
       infinite) in a discrete and finite way, which means that they have
       limited accuracy. rounding errors can add up, particularly if you
       are iterating! furthermore, some math problems are not very stable,
       meaning that if you vary the input a little, you get a vastly
       different output. this isn   t a problem with rounding or with the
       computer, but it can still have a big impact on your results.
     * memory use: for matrices that have lots of zero entries, or that
       have a particular structure, there are more efficient ways to store
       these in memory.
     * scalability: in many cases you are interested in working with more
       data than you have space in memory for.

   this course also includes some very modern approaches that we don   t
   know of any other numerical id202 courses covering (and we
   have done a lot of researching of numerical id202 courses and
   syllabi), such as:
     * [14]randomized algorithms
     * [15]numba: a library that compiles python to optimized c code
     * [16]pytorch: an alternative to numpy that runs on the gpu (also, an
       excellent deep learning framework, although we are using it just as
       a numpy alternative here to speed up our code)

   basically, we considered anything that could speed up your matrix
   computations as fair game to cover here!

teaching methodology

   this course uses the same top down, code first, application centered
   teaching method as we used in our [17]practical deep learning for
   coders course, and which i describe in more detail in [18]this blog
   post and [19]this talk i gave at the san francisco machine learning
   meetup. basically, our goal is to get students working on interesting
   applications as quickly as possible, and then to dig into the
   underlying details later as time goes on.

   this approach is very different from how most math courses operate:
   typically, math courses first introduce all the separate components you
   will be using, and then you gradually build them into more complex
   structures. the problems with this are that students often lose
   motivation, don   t have a sense of the    big picture   , and don   t know
   which pieces they   ll even end up needing. we have been inspired by
   harvard professor [20]david perkin   s baseball analogy. we don   t require
   kids to memorize all the rules of baseball and understand all the
   technical details before we let them have fun and play the game.
   rather, they start playing with a just general sense of it, and then
   gradually learn more rules/details as time goes on. all that to say,
   don   t worry if you don   t understand everything at first! you   re not
   supposed to. we will start using some    black boxes    or matrix
   decompositions that haven   t yet been explained, and then we   ll dig into
   the lower level details later.

   i love math (i even have a math phd!), but when i   m trying to solve a
   practical problem, code is more useful than theory. also, one of the
   tests of whether you truly understand something is if you can code it,
   so this course if much more code-centered than a typical numerical
   id202 course.

the details

   the primary resource for this course is the [21]free online textbook of
   jupyter notebooks, available on github. they are full of explanations,
   code samples, pictures, interesting links, and exercises for you to
   try. anyone can view the notebooks online by clicking on the links in
   the readme [22]table of contents. however, to really learn the
   material, you need to interactively run the code, which requires
   [23]installing anaconda on your computer (or an equivalent set up of
   the python scientific libraries) and you will need to be able to clone
   or download the git repo.

   accompanying the notebooks is a [24]playlist of lecture videos,
   available on youtube. if you are ever confused by a lecture or it goes
   too quickly, check out the beginning of the next video, where i review
   concepts from the previous lecture, often explaining things from a new
   perspective or with different illustrations.

   you can ask questions or share your thoughts and resources using the
   [25]computational id202 category on our fast.ai discussion
   forums.

   this course assumes more background than our [26]practical deep
   learning for coders course. it was originally taught during the final
   term before graduation of the [27]usf master of science in analytics
   program, and all students had already participated in a    id202
   bootcamp   . if you are new to id202, i recommend you watch the
   beautiful [28]3blue 1brown essence of id202 video series as
   preparation.

   there is no official registration for the course; rather it is up to
   you to work through the materials at your own pace. you can get started
   now, by watching the 1st video:

   iframe:
   [29]https://www.youtube.com/embed/8igzbmboa0i?list=pltmwhnx-gukic92m1k0
   p6bionzb-mg0hy

applications covered

   this list is not exhaustive, but here are a few of the applications
   covered:
     * how to identify the foreground in a surveillance video, by
       decomposing a matrix into the sum of two other matrices [pcp.png]

                         photo = background + people

     * how to reconstruct an image from a ct scan using the angles of the
       x-rays and the readings (spoiler: it   s surprisingly simple and we
       use id75!), covered in [30]lesson 4

   [ct_scan.png]
     * how to identify topics in a corpus of documents, covered in
       [31]lesson 2.

   [nmf_doc.png]

                       [32]image source: nmf tutorial

     * the algorithm behind google   s id95, used to rank the relative
       importance of different web pages

can   t i just use sci-kit learn?

   many (although certainly not all) of the algorithms covered in this
   course are already implemented in scientific python libraries such as
   numpy, scipy, and scikit learn, so you may be wondering why it   s
   necessary to learn what   s going on underneath the hood. knowing how
   these algorithms are implemented will allow you to better combine and
   utilize them, and will make it possible for you to customize them if
   needed. in at least one case, we show how to get a sizable speedup over
   sci-kit learn   s implementation of a method. several of the topics we
   cover are areas of active research, and there is recent research that
   has not yet been added to existing libraries.
   this post is tagged: [ [33]courses ] (click a tag for more posts in
   that category).

related posts

     * [34]fast.ai embracing swift for deep learning 06 mar 2019
     * [35]a conversation about tech ethics with the new york times chief
       data scientist 04 mar 2019
     * [36]dairy farming, solar panels, and diagnosing parkinson's
       disease: what can you do with deep learning? 21 feb 2019

references

   1. https://www.fast.ai/2017/07/17/num-lin-alg/atom.xml
   2. https://www.fast.ai/
   3. https://www.fast.ai/
   4. https://www.fast.ai/about/
   5. http://course.fast.ai/
   6. https://www.fast.ai/topics
   7. https://github.com/fastai/numerical-linear-algebra/blob/master/readme.md
   8. https://www.youtube.com/playlist?list=pltmwhnx-gukic92m1k0p6bionzb-mg0hy
   9. https://www.usfca.edu/arts-sciences/graduate-programs/analytics
  10. http://pytorch.org/
  11. https://numba.pydata.org/
  12. https://research.fb.com/fast-randomized-svd/
  13. https://www.youtube.com/watch?v=25nc0n9erq4
  14. https://research.fb.com/fast-randomized-svd/
  15. https://numba.pydata.org/
  16. http://pytorch.org/
  17. http://course.fast.ai/
  18. http://www.fast.ai/2016/10/08/teaching-philosophy/
  19. https://vimeo.com/214233053
  20. https://www.amazon.com/making-learning-whole-principles-transform/dp/0470633719
  21. https://github.com/fastai/numerical-linear-algebra
  22. https://github.com/fastai/numerical-linear-algebra/blob/master/readme.md
  23. https://docs.continuum.io/anaconda/
  24. https://www.youtube.com/playlist?list=pltmwhnx-gukic92m1k0p6bionzb-mg0hy
  25. http://forums.fast.ai/c/lin-alg
  26. http://course.fast.ai/
  27. https://www.usfca.edu/arts-sciences/graduate-programs/analytics
  28. https://www.youtube.com/playlist?list=plzhqobowtqdpd3mizzm2xvfitgf8he_ab
  29. https://www.youtube.com/embed/8igzbmboa0i?list=pltmwhnx-gukic92m1k0p6bionzb-mg0hy
  30. https://github.com/fastai/numerical-linear-algebra/blob/master/nbs/4. compressed sensing of ct scans with robust regression.ipynb
  31. https://github.com/fastai/numerical-linear-algebra/blob/master/nbs/2. id96 with nmf and svd.ipynb
  32. http://perso.telecom-paristech.fr/~essid/teach/nmf_tutorial_icme-2014.pdf
  33. https://www.fast.ai/tag/courses
  34. https://www.fast.ai/2019/03/06/fastai-swift/
  35. https://www.fast.ai/2019/03/04/ethics-framework/
  36. https://www.fast.ai/2019/02/21/dl-projects/
