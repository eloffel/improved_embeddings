   #[1]github [2]recent commits to deeprlhacks:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]44
     * [35]star [36]810
     * [37]fork [38]98

[39]williamfalcon/[40]deeprlhacks

   [41]code [42]issues 0 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   hacks for training rl systems from john schulman's lecture at deep rl
   bootcamp (aug 2017)
     * [47]15 commits
     * [48]1 branch
     * [49]0 releases
     * [50]fetching contributors

   branch: master (button) new pull request
   [51]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/w
   [52]download zip

downloading...

   want to be notified of new releases in williamfalcon/deeprlhacks?
   [53]sign in [54]sign up

launching github desktop...

   if nothing happens, [55]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [56]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [57]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [58]download the github extension for visual studio
   and try again.

   (button) go back
   fetching latest commit   
   cannot retrieve the latest commit at this time.
   [59]permalink
   type      name     latest commit message commit time
        failed to load latest commit information.
        [60]readme.md

readme.md

deeprlhacks

   from a talk given by [61]john schulman titled "the nuts and bolts of
   deep rl research" (aug 2017)
   these are tricks written down while attending summer [62]deep rl
   bootcamp at uc berkeley.

   update: rl bootcamp just released the [63]video and the rest of the
   [64]lectures.

tips to debug new algorithm

    1. simplify the problem by using a low dimensional state space
       environment.
          + john suggested to use the [65]pendulum problem because the
            problem has a 2-d state space (angle of pendulum and
            velocity).
          + easy to visualize what the value function looks like and what
            state the algorithm should be in and how they evolve over
            time.
          + easy to visually spot why something isn't working (aka, is the
            value function smooth enough and so on).
    2. to test if your algorithm is reasonable, construct a problem you
       know it should work on.
          + ex: for hierarchical id23 you'd construct a
            problem with an obvious hierarchy it should learn.
          + can easily see if it's doing the right thing.
          + warning: don't over fit method to your toy problem (realize
            it's a toy problem).
    3. familiarize yourself with certain environments you know well.
          + over time, you'll learn how long the training should take.
          + know how rewards evolve, etc...
          + allows you to set a benchmark to see how well you're doing
            against your past trials.
          + john uses the hopper robot where he knows how fast learning
            should take, and he can easily spot odd behaviors.

tips to debug a new task

    1. simplify the task
          + start simple until you see signs of life.
          + approach 1: simplify the feature space:
               o for example, if you're learning from images (huge
                 dimensional space), then maybe hand engineer features
                 first. example: if you think your function is trying to
                 approximate a location of something, use the x,y location
                 as features as step 1.
               o once it starts working, make the problem harder until you
                 solve the full problem.
          + approach 2: simplify the reward function.
               o formulate so it can give you fast feedback to know
                 whether you're doing the right thing or not.
               o ex: have reward for robot when it hits the target (+1).
                 hard to learn because maybe too much happens in between
                 starting and reward. reformulate as distance to target
                 instead which will increase learning and allow you to
                 iterate faster.

tips to frame a problem in rl

   maybe it's unclear what the features are and what the reward should be,
   or if it's feasible at all.
    1. first step: visualize a random policy acting on this problem.
          + see where it takes you.
          + if random policy on occasion does the right thing, then high
            chance rl will do the right thing.
               o policy gradient will find this behavior and make it more
                 likely.
          + if random policy never does the right thing, rl will likely
            also not.
    2. make sure observations usable:
          + see if you could control the system by using the same
            observations you give the agent.
               o example: look at preprocessed images yourself to make
                 sure you don't remove necessary details or hinder the
                 algorithm in a certain way.
    3. make sure everything is reasonably scaled.
          + rule of thumb:
               o observations: make everything mean 0, standard deviation
                 1.
               o reward: if you control it, then scale it to a reasonable
                 value.
                    # do it across all your data so far.
          + look at all observations and rewards and make sure there
            aren't crazy outliers.
    4. have good baseline whenever you see a new problem.
          + it's unclear which algorithm will work, so have a set of
            baselines (from other methods)
               o cross id178 method
               o id189
               o some kind of id24 method (checkout [66]openai
                 baselines as a starter or [67]rllab)

reproducing papers

   sometimes (often), it's hard to reproduce results from papers. some
   tricks to do that:
    1. use more samples than needed.
    2. policy right... but not exactly
          + try to make it work a little bit.
          + then tweak hyper parameters to get up to the public
            performance.
          + if want to get it to work at all, use bigger batch sizes.
               o if batch size is too small, noisy will overpower signal.
               o example: trpo, john was using too tiny of a batch size
                 and had to use 100k time steps.
               o for id25, best hyperparams: 10k time steps, 1mm frames in
                 replay buffer.

guidelines on-going training process

   sanity check that your training is going well.
    1. look at sensitivity of every hyper parameter
          + if algo is too sensitive, then not robust and should not be
            happy with it.
          + sometimes it happens that a method works one way because of
            funny dynamics but not in general.
    2. look for indicators that the optimization process is healthy.
          + varies
          + look at whether value function is accurate.
               o is it predicting well?
               o is it predicting returns well?
               o how big are the updates?
          + standard diagnostics from deep networks
    3. have a system for continuously benchmarking code.
          + needs discipline.
          + look at performance across all previous problems you tried.
               o sometimes it'll start working on one problem but mess up
                 performance in others.
               o easy to over fit on a single problem.
          + have a battery of benchmarks you run occasionally.
    4. think your algorithm is working but you're actually seeing random
       noise.
          + example: graph of 7 tasks with 3 algorithms and looks like 1
            algorithm might be doing best on all problems, but turns out
            they're all the same algorithm with different random seeds.
    5. try different random seeds!!
          + run multiple times and average.
          + run multiple tasks on multiple seeds.
               o if not, you're likely to over fit.
    6. additional algorithm modifications might be unnecessary.
          + most tricks are actually normalizing something in some way or
            improving your optimization.
          + a lot of tricks also have the same effect... so you can remove
            some of them and simplify your algorithm (very key).
    7. simplify your algorithm
          + will generalize better
    8. automate your experiments
          + don't spend your whole day watching your code spit out
            numbers.
          + launch experiments on cloud services and analyze results.
          + frameworks to track experiments and results:
               o mostly use ipython notebooks.
               o dbs seem unnecessary to store results.

general training strategies

    1. whiten and standardize data (for all seen data since the
       beginning).
          + observations:
               o do it by computing a running mean and standard deviation.
                 then z-transform everything.
               o over all data seen (not just the recent data).
                    # at least it'll scale down over time how fast it's
                      changing.
                    # might trip up the optimizer if you keep changing the
                      objective.
                    # rescaling (by using recent data) means your
                      optimizer probably didn't know about that and
                      performance will collapse.
          + rewards:
               o scale and don't shift.
                    # affects agent's will to live.
                    # will change the problem (aka, how long you want it
                      to survive).
          + standardize targets:
               o same way as rewards.
          + pca whitening?
               o could help.
               o starting to see if it actually helps with neural nets.
               o huge scales (-1000, 1000) or (-0.001, 0.001) certainly
                 make learning slow.
    2. parameters that inform discount factors.
          + determines how far you're giving credit assignment.
          + ex: if factor is 0.99, then you're ignoring what happened 100
            steps ago... means you're shortsighted.
               o better to look at how that corresponds to real time
                    # intuition, in rl we're usually discretizing time.
                    # aka: are those 100 steps 3 seconds of actual time?
                    # what happens during that time?
          + if td methods for policy gradient of value fx estimation,
            gamma can be close to 1 (like 0.999)
               o algo becomes very stable.
    3. look to see that problem can actually be solved in the discretized
       level.
          + example: in game if you're doing frame skip.
               o as a human, can you control it or is it impossible?
               o look at what random exploration looks like
                    # discretization determines how far your brownian
                      motion goes.
                    # if do many actions in a row, then tend to explore
                      further.
                    # choose your time discretization in a way that works.
    4. look at episode returns closely.
          + not just mean, look at min and max.
               o the max return is something your policy can hone in
                 pretty well.
               o is your policy ever doing the right thing??
          + look at episode length (sometimes more informative than
            episode reward).
               o if on game you might be losing every time so you might
                 never win, but... episode length can tell you if you're
                 losing slower.
               o might see an episode length improvement in the beginning
                 but maybe not reward.

policy gradient diagnostics

    1. look at id178 really carefully
          + id178 in action space
               o care more about id178 in state space, but don't have
                 good methods for calculating that.
          + if going down too fast, then policy becoming deterministic and
            will not explore.
          + if not going down, then policy won't be good because it is
            really random.
          + can fix by:
               o kl penalty
                    # keep id178 from decreasing too quickly.
               o add id178 bonus.
          + how to measure id178.
               o for most policies can compute id178 analytically.
                    # if continuous, it's usually a gaussian, so can
                      compute differential id178.
    2. look at kl divergence
          + look at size of updates in terms of kl divergence.
          + example:
               o if kl is .01 then very small.
               o if 10 then too much.
    3. baseline explained variance.
          + see if value function is actually a good predictor or a
            reward.
               o if negative it might be overfitting or noisy.
                    # likely need to tune hyper parameters
    4. initialize policy
          + very important (more so than in supervised learning).
          + zero or tiny final layer to maximize id178
               o maximize random exploration in the beginning

id24 strategies

    1. be careful about replay buffer memory usage.
          + you might need a huge buffer, so adapt code accordingly.
    2. play with learning rate schedule.
    3. if converges slowly or has slow warm-up period in the beginning
          + be patient... id25 converges very slowly.

bonus from [68]andrej karpathy:

    1. a good feature can be to take the difference between two frames.
          + this delta vector can highlight slight state changes otherwise
            difficult to distinguish.

     *    2019 github, inc.
     * [69]terms
     * [70]privacy
     * [71]security
     * [72]status
     * [73]help

     * [74]contact github
     * [75]pricing
     * [76]api
     * [77]training
     * [78]blog
     * [79]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [80]reload to refresh your
   session. you signed out in another tab or window. [81]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/williamfalcon/deeprlhacks/commits/master.atom
   3. https://github.com/williamfalcon/deeprlhacks#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/williamfalcon/deeprlhacks
  32. https://github.com/join
  33. https://github.com/login?return_to=/williamfalcon/deeprlhacks
  34. https://github.com/williamfalcon/deeprlhacks/watchers
  35. https://github.com/login?return_to=/williamfalcon/deeprlhacks
  36. https://github.com/williamfalcon/deeprlhacks/stargazers
  37. https://github.com/login?return_to=/williamfalcon/deeprlhacks
  38. https://github.com/williamfalcon/deeprlhacks/network/members
  39. https://github.com/williamfalcon
  40. https://github.com/williamfalcon/deeprlhacks
  41. https://github.com/williamfalcon/deeprlhacks
  42. https://github.com/williamfalcon/deeprlhacks/issues
  43. https://github.com/williamfalcon/deeprlhacks/pulls
  44. https://github.com/williamfalcon/deeprlhacks/projects
  45. https://github.com/williamfalcon/deeprlhacks/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/williamfalcon/deeprlhacks/commits/master
  48. https://github.com/williamfalcon/deeprlhacks/branches
  49. https://github.com/williamfalcon/deeprlhacks/releases
  50. https://github.com/williamfalcon/deeprlhacks/graphs/contributors
  51. https://github.com/williamfalcon/deeprlhacks/find/master
  52. https://github.com/williamfalcon/deeprlhacks/archive/master.zip
  53. https://github.com/login?return_to=https://github.com/williamfalcon/deeprlhacks
  54. https://github.com/join?return_to=/williamfalcon/deeprlhacks
  55. https://desktop.github.com/
  56. https://desktop.github.com/
  57. https://developer.apple.com/xcode/
  58. https://visualstudio.github.com/
  59. https://github.com/williamfalcon/deeprlhacks/tree/c111306ac3d42129bc28d28264d78703aa107d2c
  60. https://github.com/williamfalcon/deeprlhacks/blob/master/readme.md
  61. http://joschu.net/
  62. https://www.deepbootcamp.io/
  63. https://www.youtube.com/watch?v=8ecdack9kaq&feature=youtu.be
  64. https://sites.google.com/view/deep-rl-bootcamp/lectures
  65. https://gym.openai.com/envs/pendulum-v0
  66. https://github.com/openai/baselines
  67. https://github.com/rll/rllab
  68. http://cs.stanford.edu/people/karpathy/
  69. https://github.com/site/terms
  70. https://github.com/site/privacy
  71. https://github.com/security
  72. https://githubstatus.com/
  73. https://help.github.com/
  74. https://github.com/contact
  75. https://github.com/pricing
  76. https://developer.github.com/
  77. https://training.github.com/
  78. https://github.blog/
  79. https://github.com/about
  80. https://github.com/williamfalcon/deeprlhacks
  81. https://github.com/williamfalcon/deeprlhacks

   hidden links:
  83. https://github.com/
  84. https://github.com/williamfalcon/deeprlhacks
  85. https://github.com/williamfalcon/deeprlhacks
  86. https://github.com/williamfalcon/deeprlhacks
  87. https://help.github.com/articles/which-remote-url-should-i-use
  88. https://github.com/williamfalcon/deeprlhacks#deeprlhacks
  89. https://github.com/williamfalcon/deeprlhacks#tips-to-debug-new-algorithm
  90. https://github.com/williamfalcon/deeprlhacks#tips-to-debug-a-new-task
  91. https://github.com/williamfalcon/deeprlhacks#tips-to-frame-a-problem-in-rl
  92. https://github.com/williamfalcon/deeprlhacks#reproducing-papers
  93. https://github.com/williamfalcon/deeprlhacks#guidelines-on-going-training-process
  94. https://github.com/williamfalcon/deeprlhacks#general-training-strategies
  95. https://github.com/williamfalcon/deeprlhacks#policy-gradient-diagnostics
  96. https://github.com/williamfalcon/deeprlhacks#id24-strategies
  97. https://github.com/williamfalcon/deeprlhacks#bonus-from-andrej-karpathy
  98. https://github.com/
