   #[1]github [2]recent commits to magenta:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]820
     * [35]star [36]12,817
     * [37]fork [38]2,476

[39]tensorflow/[40]magenta

   [41]code [42]issues 158 [43]pull requests 18 [44]insights
   [45]permalink
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   branch: master
   [47]magenta/[48]magenta/[49]models/[50]sketch_id56/readme.md
   [51]find file copy path
   [52]@proppy [53]proppy [54]readme: fix typo in jupyter notebook link
   ([55]#908[56]) [57]6f67133 oct 6, 2017
   6 contributors
   (button)

users who have contributed to this file

     * [58]hardmaru
     * [59]cghawthorne
     * [60]proppy
     * [61]silky
     * [62]douglaseck
     * [63]adarob

   [64]@hardmaru [65]@cghawthorne [66]@proppy [67]@silky [68]@douglaseck
   [69]@adarob
   161 lines (110 sloc) 14 kb
   [70]raw [71]blame [72]history
   (button) (button)

sketch-id56: a generative model for vector drawings

   before jumping in on any code examples, please first set up your
   [73]magenta environment.

   [74]example images

   examples of vector images produced by this generative model.

   this repo contains the tensorflow code for sketch-id56, the recurrent
   neural network model described in [75]teaching machines to draw and
   [76]a neural representation of sketch drawings. we've also provided a
   jupyter notebook [77]sketch_id56.ipynb in our [78]magenta demos
   repository which demonstrates many of the examples discussed here.

overview of model

   sketch-id56 is a sequence-to-sequence variational autoencoder. the
   encoder id56 is a bi-directional id56, and the decoder is an
   autoregressive mixture-density id56. you can specify the type of id56
   cell to use, and the size of the id56 using the settings enc_model,
   dec_model, enc_size, dec_size.

   the encoder will sample a latent code z, a vector of floats with a
   dimension of z_size. like in the vae, we can enforce a gaussian iid
   distribution to z, and the strength of the kl divergence loss term is
   controlled using kl_weight. there will be a tradeoff between kl
   divergence loss and the reconstruction loss. we also allow some room
   for the latent code to store information, and not be pure gaussian iid.
   once the kl loss term gets below kl_tolerance, we will stop optimizing
   for this term.

   [79]model schematic

   for small to medium sized datasets, dropout and data augmentation is
   very useful technique to avoid overfitting. we have provided options
   for input dropout, output dropout, and [80]recurrent dropout without
   memory loss. in practice, we only use recurrent dropout, and usually
   set it to between 65% to 90% depending on the dataset. [81]layer
   id172 and [82]recurrent dropout can be used together, forming a
   powerful combination for training recurrent neural nets on a small
   dataset.

   there are two data augmentation techniques provided. the first one is a
   random_scale_factor to randomly scale the size of training images. the
   second augmentation technique (not used in the sketch-id56 paper) is
   dropping out random points in a line stroke. given a line segment with
   more than 2 points, we can randomly drop points inside the line
   segments with a small id203 of augment_stroke_prob, and still
   maintain a similar-looking vector image. this type of data augmentation
   is very powerful when used on small datasets, and is unique to vector
   drawings, since it is difficult to dropout random characters or notes
   in text or midi data, and also not possible to dropout random pixels
   without causing large visual differences in pixel image data. we
   usually set both data augmentation parameters to 10% to 20%. if there
   is virtually no difference for a human audience when they compare an
   augmented example compared to a normal example, we apply both data
   augmentation techniques regardless of the size of the training dataset.

   using dropout and data augmentation effectively will avoid overfitting
   to a small training set.

training a model

   to train the model you first need a dataset containing
   train/validation/test examples. we have provided links to the
   aaron_sheep dataset and the model will use this lightweight dataset by
   default.

example usage:

sketch_id56_train --log_root=checkpoint_path --data_dir=dataset_path --hparams="d
ata_set=[dataset_filename.npz]"

   we recommend you create subdirectories inside models and datasets to
   hold your own data and checkpoints. the [83]tensorboard logs will be
   stored inside checkpoint_path for viewing training curves for the
   various losses on train/validation/test datasets.

   here is a list of full options for the model, along with the default
   settings:
data_set=['aaron_sheep.npz'],  # our dataset. can be list of multiple .npz sets.
num_steps=10000000,            # total number of training set. keep large.
save_every=500,                # number of batches per checkpoint creation.
dec_id56_size=512,              # size of decoder.
dec_model='lstm',              # decoder: lstm, layer_norm or hyper.
enc_id56_size=256,              # size of encoder.
enc_model='lstm',              # encoder: lstm, layer_norm or hyper.
z_size=128,                    # size of latent vector z. recommend 32, 64 or 12
8.
kl_weight=0.5,                 # kl weight of loss equation. recommend 0.5 or 1.
0.
kl_weight_start=0.01,          # kl start weight when annealing.
kl_tolerance=0.2,              # level of kl loss at which to stop optimizing fo
r kl.
batch_size=100,                # minibatch size. recommend leaving at 100.
grad_clip=1.0,                 # gradient clipping. recommend leaving at 1.0.
num_mixture=20,                # number of mixtures in gaussian mixture model.
learning_rate=0.001,           # learning rate.
decay_rate=0.9999,             # learning rate decay per minibatch.
kl_decay_rate=0.99995,         # kl annealing decay rate per minibatch.
min_learning_rate=0.00001,     # minimum learning rate.
use_recurrent_dropout=true,    # recurrent dropout without memory loss. recomend
ed.
recurrent_dropout_prob=0.90,   # id203 of recurrent dropout keep.
use_input_dropout=false,       # input dropout. recommend leaving false.
input_dropout_prob=0.90,       # id203 of input dropout keep.
use_output_dropout=false,      # output droput. recommend leaving false.
output_dropout_prob=0.90,      # id203 of output dropout keep.
random_scale_factor=0.15,      # random scaling data augmention proportion.
augment_stroke_prob=0.10,      # point dropping augmentation proportion.
conditional=true,              # if false, use decoder-only model.

   here are some options you may want to use to train the model on a very
   large dataset spanning three .npz files, and use [84]hyperlstm as the
   id56 cells. for small datasets of less than 10k training examples, lstm
   with layer id172 (layer_norm for both enc_model and dec_model)
   works best.
sketch_id56_train --log_root=models/big_model --data_dir=datasets/big_dataset --h
params="data_set=[class1.npz,class2.npz,class3.npz],dec_model=hyper,dec_id56_size
=2048,enc_model=layer_norm,enc_id56_size=512,save_every=5000,grad_clip=1.0,use_re
current_dropout=0"

   we have tested this model on tensorflow 1.0 and 1.1 for python 2.7.

datasets

   due to size limitations, this repo does not contain any datasets.

   we have prepared many datasets that work out of the box with
   sketch-id56. the google [85]quickdraw dataset is a collection of 50m
   vector sketches across 345 categories. in the repo of
   [86]quickdraw-dataset, there is a section called sketch-id56 quickdraw
   dataset that describes the pre-processed datafiles that can be used
   with this project. each category class is stored in its own file, such
   as cat.npz, and contains training/validation/test set sizes of
   70000/2500/2500 examples.

   you download the .npz datasets from [87]google cloud for local use. we
   recommend you create a sub directory called datasets/quickdraw, and
   save these .npz files in this sub directory.

   in addition to the quickdraw dataset, we have also tested this model on
   smaller datasets. in the [88]sketch-id56-datasets repo, there are 3
   other datasets: aaron koblin sheep market, kanji, and omniglot. we
   recommend you create a sub directory for each of these dataset, such as
   datasets/aaron_sheep, if you wish to use them locally. as mentioned
   before, recurrent dropout and data augmentation should be used when
   training models on small datasets to avoid overfitting.

creating your own dataset

   please create your own interesting datasets and train this algorithm on
   them! getting your hands dirty and creating new datasets is part of the
   fun. why settle on existing pre-packaged datasets when you are
   potentially sitting on an interesting dataset of vector line drawings?
   in our experiments, a dataset size consisting of a few thousand
   examples was sufficient to produce some meaningful results. here, we
   describe the format of the dataset files the model expects to see.

   each example in the dataset is stored as list of coordinate offsets:
      x,    y, and a binary value representing whether the pen is lifted away
   from the paper. this format, we refer to as stroke-3, is described in
   this [89]paper. note that the data format described in the paper has 5
   elements (stroke-5 format), and this conversion is done automatically
   inside the dataloader. below is an example sketch of a turtle using
   this format:

   [90]example training sketches figure: a sample sketch, as a sequence of
   (   x,    y, binary pen state) points and in rendered form. in the rendered
   sketch, the line color corresponds to the sequential stroke ordering.

   in our datasets, each example in the list of examples is represented as
   a np.array with np.int16 datatypes. you can store them as np.int8 if
   you can get away with it to save storage space. if your data must be in
   floating-point format, np.float16 also works. np.float32 can be a waste
   of storage space. in our data, the    x and    y offsets are represented in
   pixel locations, which are larger than the range of numbers a neural
   network model likes to see, so there is a id172 scaling process
   built into the model. when we load the training data, the model will
   automatically convert to np.float and normalize accordingly before
   training.

   if you want to create your own dataset, you must create three lists of
   examples for training/validation/test sets, to avoid overfitting to the
   training set. the model will handle the early stopping using the
   validation set. for the aaron_sheep dataset, we used a split of
   7400/300/300 examples, and put each inside python lists called
   train_data, validation_data, and test_data. afterwards, we created a
   subdirectory called datasets/aaron_sheep and we use the built-in
   savez_compressed method to save a compressed version of the dataset in
   a aaron_sheep.npz file. in all of our experiments, the size of each
   dataset is an exact multiple of 100, and use a batch_size of 100.
   deviate at your own peril.
filename = os.path.join('datasets/your_dataset_directory', 'your_dataset_name.np
z')
np.savez_compressed(filename, train=train_data, valid=validation_data, test=test
_data)

   we also performed simple stroke simplification to preprocess the data,
   called [91]ramer-douglas-peucker. there is some easy-to-use open source
   code for applying this algorithm [92]here. in practice, we can set the
   epsilon parameter to a value between 0.2 to 3.0, depending on how
   aggressively we want to simply the lines. in the paper we used an
   epsilon parameter of 2.0. we suggest you build a dataset where the
   maximum sequence length is less than 250.

   if you have a large set of simple svg images, there are some available
   [93]libraries to convert subsets of svgs into line segments, and you
   can then apply rdp on the line segments before converting the data to
   stroke-3 format.

pre-trained models

   we have provided pre-trained models for the aaron_sheep dataset, for
   both conditional and unconditional training mode, using vanilla lstm
   cells and lstm cells with [94]layer id172. these models will be
   downloaded by running the jupyter notebook. they are stored in:
     * /tmp/sketch_id56/models/aaron_sheep/lstm
     * /tmp/sketch_id56/models/aaron_sheep/lstm_uncond
     * /tmp/sketch_id56/models/aaron_sheep/layer_norm
     * /tmp/sketch_id56/models/aaron_sheep/layer_norm_uncond

   in addition, we have provided pre-trained models for selected quickdraw
   datasets:
     * /tmp/sketch_id56/models/owl/lstm
     * /tmp/sketch_id56/models/flamingo/lstm_uncond
     * /tmp/sketch_id56/models/catbus/lstm
     * /tmp/sketch_id56/models/elephantpig/lstm

using a model with jupyter notebook

   [95]example images

   let's get the model to interpolate between a cat and a bus!

   we've included a simple [96]jupyter notebook to show you how to load a
   pre-trained model and generate vector sketches. you will be able to
   encode, decode, and morph between two vector images, and also generate
   new random ones. when sampling images, you can tune the temperature
   parameter to control the level of uncertainty.

citation

   if you find this project useful for academic purposes, please cite it
   as:
@article{sketchid56,
  author          = {{ha}, david and {eck}, douglas},
  title           = "{a neural representation of sketch drawings}",
  journal         = {arxiv e-prints},
  archiveprefix   = "arxiv",
  eprinttype      = {arxiv},
  eprint          = {1704.03477},
  primaryclass    = "cs.ne",
  keywords        = {computer science - neural and evolutionary computing, compu
ter science - learning, statistics - machine learning},
  year            = 2017,
  month           = apr,
}

   ____________________ (button) go

     *    2019 github, inc.
     * [97]terms
     * [98]privacy
     * [99]security
     * [100]status
     * [101]help

     * [102]contact github
     * [103]pricing
     * [104]api
     * [105]training
     * [106]blog
     * [107]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [108]reload to refresh your
   session. you signed out in another tab or window. [109]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/tensorflow/magenta/commits/master.atom
   3. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md
  32. https://github.com/join
  33. https://github.com/login?return_to=/tensorflow/magenta
  34. https://github.com/tensorflow/magenta/watchers
  35. https://github.com/login?return_to=/tensorflow/magenta
  36. https://github.com/tensorflow/magenta/stargazers
  37. https://github.com/login?return_to=/tensorflow/magenta
  38. https://github.com/tensorflow/magenta/network/members
  39. https://github.com/tensorflow
  40. https://github.com/tensorflow/magenta
  41. https://github.com/tensorflow/magenta
  42. https://github.com/tensorflow/magenta/issues
  43. https://github.com/tensorflow/magenta/pulls
  44. https://github.com/tensorflow/magenta/pulse
  45. https://github.com/tensorflow/magenta/blob/c7886f95525b0c3a2c51e0f9086bc1f8c699da9b/magenta/models/sketch_id56/readme.md
  46. https://github.com/join?source=prompt-blob-show
  47. https://github.com/tensorflow/magenta
  48. https://github.com/tensorflow/magenta/tree/master/magenta
  49. https://github.com/tensorflow/magenta/tree/master/magenta/models
  50. https://github.com/tensorflow/magenta/tree/master/magenta/models/sketch_id56
  51. https://github.com/tensorflow/magenta/find/master
  52. https://github.com/proppy
  53. https://github.com/proppy
  54. https://github.com/tensorflow/magenta/commit/6f67133802d7f5e84c16e8af297217d9cee2b39a
  55. https://github.com/tensorflow/magenta/pull/908
  56. https://github.com/tensorflow/magenta/commit/6f67133802d7f5e84c16e8af297217d9cee2b39a
  57. https://github.com/tensorflow/magenta/commit/6f67133802d7f5e84c16e8af297217d9cee2b39a
  58. https://github.com/hardmaru
  59. https://github.com/cghawthorne
  60. https://github.com/proppy
  61. https://github.com/silky
  62. https://github.com/douglaseck
  63. https://github.com/adarob
  64. https://github.com/tensorflow/magenta/commits/master/magenta/models/sketch_id56/readme.md?author=hardmaru
  65. https://github.com/tensorflow/magenta/commits/master/magenta/models/sketch_id56/readme.md?author=cghawthorne
  66. https://github.com/tensorflow/magenta/commits/master/magenta/models/sketch_id56/readme.md?author=proppy
  67. https://github.com/tensorflow/magenta/commits/master/magenta/models/sketch_id56/readme.md?author=silky
  68. https://github.com/tensorflow/magenta/commits/master/magenta/models/sketch_id56/readme.md?author=douglaseck
  69. https://github.com/tensorflow/magenta/commits/master/magenta/models/sketch_id56/readme.md?author=adarob
  70. https://github.com/tensorflow/magenta/raw/master/magenta/models/sketch_id56/readme.md
  71. https://github.com/tensorflow/magenta/blame/master/magenta/models/sketch_id56/readme.md
  72. https://github.com/tensorflow/magenta/commits/master/magenta/models/sketch_id56/readme.md
  73. https://github.com/tensorflow/magenta/blob/master/readme.md
  74. https://camo.githubusercontent.com/fabcb6371ffc73aa2420507ef954b200446bc624/68747470733a2f2f63646e2e7261776769742e636f6d2f74656e736f72666c6f772f6d6167656e74612f6d61737465722f6d6167656e74612f6d6f64656c732f736b657463685f726e6e2f6173736574732f736b657463685f726e6e5f6578616d706c65732e737667
  75. https://research.googleblog.com/2017/04/teaching-machines-to-draw.html
  76. https://arxiv.org/abs/1704.03477
  77. https://github.com/tensorflow/magenta-demos/blob/master/jupyter-notebooks/sketch_id56.ipynb
  78. https://github.com/tensorflow/magenta-demos
  79. https://camo.githubusercontent.com/1d108741379d5a6fc1cfcaa071103aaecd376369/68747470733a2f2f63646e2e7261776769742e636f6d2f74656e736f72666c6f772f6d6167656e74612f6d61737465722f6d6167656e74612f6d6f64656c732f736b657463685f726e6e2f6173736574732f736b657463685f726e6e5f736368656d617469632e737667
  80. https://arxiv.org/abs/1603.05118
  81. https://arxiv.org/abs/1607.06450
  82. https://arxiv.org/abs/1603.05118
  83. https://www.tensorflow.org/get_started/summaries_and_tensorboard
  84. https://arxiv.org/abs/1609.09106
  85. https://quickdraw.withgoogle.com/data
  86. https://github.com/googlecreativelab/quickdraw-dataset
  87. https://console.cloud.google.com/storage/quickdraw_dataset/sketchid56
  88. https://github.com/hardmaru/sketch-id56-datasets
  89. https://arxiv.org/abs/1308.0850
  90. https://camo.githubusercontent.com/dffe251cefd373f49a2cf6c0dd79b76b81816d24/68747470733a2f2f63646e2e7261776769742e636f6d2f74656e736f72666c6f772f6d6167656e74612f6d61737465722f6d6167656e74612f6d6f64656c732f736b657463685f726e6e2f6173736574732f646174615f666f726d61742e737667
  91. https://en.wikipedia.org/wiki/ramer   douglas   peucker_algorithm
  92. https://github.com/fhirschmann/rdp
  93. https://pypi.python.org/pypi/svg.path
  94. https://arxiv.org/abs/1607.06450
  95. https://camo.githubusercontent.com/3bec492b1b85a71834226e771b3d5cd1a043d240/68747470733a2f2f63646e2e7261776769742e636f6d2f74656e736f72666c6f772f6d6167656e74612f6d61737465722f6d6167656e74612f6d6f64656c732f736b657463685f726e6e2f6173736574732f6361746275732e737667
  96. https://github.com/tensorflow/magenta-demos/blob/master/jupyter-notebooks/sketch_id56.ipynb
  97. https://github.com/site/terms
  98. https://github.com/site/privacy
  99. https://github.com/security
 100. https://githubstatus.com/
 101. https://help.github.com/
 102. https://github.com/contact
 103. https://github.com/pricing
 104. https://developer.github.com/
 105. https://training.github.com/
 106. https://github.blog/
 107. https://github.com/about
 108. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md
 109. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md

   hidden links:
 111. https://github.com/
 112. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md
 113. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md
 114. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md
 115. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#sketch-id56-a-generative-model-for-vector-drawings
 116. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#overview-of-model
 117. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#training-a-model
 118. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#example-usage
 119. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#datasets
 120. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#creating-your-own-dataset
 121. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#pre-trained-models
 122. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#using-a-model-with-jupyter-notebook
 123. https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_id56/readme.md#citation
 124. https://github.com/
