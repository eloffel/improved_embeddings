   #[1]adventures in machine learning    python gensim id97 tutorial
   with tensorflow and keras comments feed [2]alternate [3]alternate

   menu

     * [4]home
     * [5]about
     * [6]coding the deep learning revolution ebook
     * [7]contact
     * [8]ebook / newsletter sign-up

   search: ____________________

python gensim id97 tutorial with tensorflow and keras

   by [9]admin | [10]gensim

     * you are here:
     * [11]home
     * [12]keras
     * [13]python gensim id97 tutorial with tensorflow and keras

   sep 01
   [14]7
   gensim id97 - nearest words

   i   ve been dedicating quite a bit of time recently to id97 tutorials
   because of the importance of the id97 concept for natural language
   processing (nlp) and also because i   ll soon be presenting some
   tutorials on recurrent neural networks and lstms for sequence
   prediction/nlp (update: i   ve completed a comprehensive tutorial on
   these topics     [15]recurrent neural networks and lstm tutorial in
   python and tensorflow).  there are also some very interesting ideas
   floating around such as [16]thought vectors which require an
   understanding of the id97 concept.  my two id97 tutorials
   are [17]id97 id27 tutorial in python and
   tensorflow and [18]a id97 keras tutorial showing the concepts of
   id97 and implementing in tensorflow and keras, respectively.  in
   this tutorial, i am going to show you how you can use the original
   google id97 c code to generate word vectors, using the python
   gensim library which wraps this cod,e and apply the results to
   tensorflow and keras.

   the gensim id97 implementation is very fast due to its c
   implementation     but to use it properly you will first need to install
   the [19]cython library. in this tutorial, i   ll show how to load the
   resulting embedding layer generated by gensim into tensorflow and keras
   embedding implementations.  because of gensim   s blazing fast c wrapped
   code, this is a good alternative to running native id97 embeddings
   in tensorflow and keras.
     __________________________________________________________________

   recommended online course: if you are more of a video course learner,
   check out this inexpensive udemy course: [20]natural language
   processing with deep learning in python
   [show?id=jbc0n5zkdzk&amp;bids=323058.918390&amp;type=2&amp;subid=0]
     __________________________________________________________________

id97 and gensim

   i   ve devoted plenty of words to explaining id97 in my previous
   tutorials ([21]here and [22]here) so i   ll only briefly introduce the
   id97 concepts here.  for further details, check out those
   tutorials. here   s the (relatively) quick version     for each text data
   set that we create, we have to create a vocabulary. the vocabulary is
   the list of unique words within the text.  often it is >10,000 words
   for serious data sets.  machine learning models generally can   t take
   raw word inputs, so we first need to convert our data set into some
   number format     generally a list of unique integers.

   neural network based models like vector inputs. we, therefore, need to
   convert the integers into vectors.  a naive way of converting integers
   into vectors is to convert them into one-hot vectors     these are
   vectors where all of the values are set to zero, except for one i.e.
   [0, 0, 0,    , 1,    , 0, 0].  the    one-hot    value is located at the array
   index which matches the unique integer representation of the word.
   therefore, our input one-hot vector must be at least the size of the
   vocabulary in length     i.e. >10,000 words.

   there are two main problems with this type of representation of words    
   the first is that it is inefficient. each word is represented by a
   10,000 word plus vector, which for neural networks means a heck of a
   lot of associated weights between the input layer and the first hidden
   layer (generally millions).  the second is that it loses all contextual
   meaning of the words.  we need a way of representing words that is both
   efficient and yet retains some of the original meaning of the word and
   its relation to other words. enter id27 and id97.

id27 and id97

   id27 involves creating better vector representations of words
       both in terms of efficiency and maintaining meaning. for instance, a
   id27 layer may involve creating a 10,000 x 300 sized matrix,
   whereby we look up a 300 length vector representation for each of the
   10,000 words in our vocabulary.  this new, 300 length vector is
   obviously a lot more efficient than a 10,000 length one-hot
   representation.  but we also need to create this 300 length vector in
   such a way as to preserve some semblance of the meaning of the word.

   id97 does this by taking the context of words surrounding
   the target word.  so, if we have a context window of 2, the context of
   the target word    sat    in the sentence    the cat sat on the mat    is the
   list of words [   the   ,    cat   ,    on   ,    the   ]. in id97, the meaning of
   a word is roughly translatable to context     and it basically works.
   target words which share similar common context words often have
   similar meanings. the way id97 trains the embedding vectors is via
   a neural network of sorts     the neural network, given a one-hot
   representation of a target word, tries to predict the most likely
   context words.  for an introduction to neural networks, see [23]this
   tutorial.

   here   s a naive way of performing the neural network training using an
   output softmax layer:
   gensim id27 softmax trainer

   a id27 softmax trainer

   in this network, the 300 node hidden layer weights are training by
   trying to predict (via a softmax output layer) genuine, high
   id203 context words.  once the training is complete, the output
   softmax layer is discarded and what is of real value is the 10,000 x
   300 weight matrix connecting the input to the hidden layer. this is our
   embedding matrix, and we can look up any member of our 10,000-word
   vocabulary and get it   s 300 length vector representation.

   it turns out that this softmax way of training the embedding layer is
   very inefficient, due to the millions of weights that need to be
   involved in updating and calculating the softmax values. therefore, a
   concept called negative sampling is used in the real id97, which
   involves training the layer with real context words and a few negative
   samples which are chosen randomly from outside the context.  for more
   details on this, see my [24]id97 keras tutorial.

   now we understand what id97 training of embedding layers involves,
   let   s talk about the gensim id97 module.

a gensim id97 tutorial

   gensim id97 - nearest words

   nearest words by cosine similarity

   this section will give a brief introduction to the gensim id97
   module.  the gensim library is an open-source python library that
   specializes in vector space and id96.  it can be made very
   fast with the use of the cython python model, which allows c code to be
   run inside the python environment. this is good for our purposes, as
   the [25]original google id97 implementation is written in c, and
   gensim has a wrapper for this code, which will be explained below.

   for this tutorial, we are going to use the text8 corpus sourced from
   [26]here for our text data. all the code for this tutorial can be found
   on this site   s [27]github repository.

   first off, we need to download the text8.zip file (if required) and
   extract it:
url = 'http://mattmahoney.net/dc/'
filename = maybe_download('text8.zip', url, 31344016)
root_path = "c:\\users\andy\pycharmprojects\\adventures-in-ml-code\\"
if not os.path.exists((root_path + filename).strip('.zip')):
    zipfile.zipfile(root_path+filename).extractall()

   this is all fairly straightforward python file handling, downloading
   and zip file manipulation, so i won   t go into it here.

   the next step that is required is to create an iterator for gensim to
   extract its data from.  we can cheat a little bit here and use a
   supplied iterator that gensim provides for the text8 corpus:
sentences = id97.text8corpus((root_path + filename).strip('.zip'))

   the required input to the gensim id97 module is an [28]iterator
   object, which sequentially supplies sentences from which gensim will
   train the embedding layer. the line above shows the supplied gensim
   iterator for the text8 corpus, but below shows another generic form
   that could be used in its place for a different data set (not actually
   implemented in the code for this tutorial), where the data set also
   contains multiple files:
class mysentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

   this capability of gensim is great, as it means you can setup iterators
   which cycle through the data without having to load the entire data set
   into memory.  this is vital, as some text data sets are huge  i.e. tens
   of gb.

   after we   ve setup the iterator object, it is dead simple to train our
   word vectors:
logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s', level=lo
gging.info)
model = id97.id97(sentences, iter=10, min_count=10, size=300, workers=4)

   the first line just lets us see the info logging that gensim provides
   as it trains. the second line will execute the training on the
   provided sentences iterator.  the first optional argument iter
   specifies how many times the training code will run through the data
   set to train the neural network (kind of like the number of training
   epochs). the gensim training code will actually run through all the
   data iter+1 time, as the first pass involves collecting all the unique
   words, creating dictionaries etc.  the next argument, min_count,
   specifies the minimum amount of times that the word has to appear in
   the corpus before it is included in the vocabulary     this allows us to
   easily eliminate rare words and reduce our vocabulary size.  the third
   argument is the size of the resultant word vector     in this case, we
   set it to 300. in other words, each word in our vocabulary, after
   training, will be represented by a 300 length word vector. finally, if
   we are using cython, we can specify how many parallel workers we would
   like to work on the data     this will speed up the training process.
   there are [29]lots of other arguments, but these are the main ones to
   consider.

   let   s examine our results and see what else gensim can do.
# get the word vector of "the"
print(model.wv['the'])

   this returns a 300 length numpy vector     as you can see, each word
   vector can be retrieved from the model via a dictionary key i.e. a word
   within our vocabulary.
# get the most common words
print(model.wv.index2word[0], model.wv.index2word[1], model.wv.index2word[2])

   the word vectors are also arranged within the wv object with indexes    
   the lowest index (i.e. 0) represents the most common word, the highest
   (i.e. the length of the vocabulary minus 1) the least common word.  the
   above code returns:    the of and   , which is unsurprising, as these are
   very common words.
# get the least common words
vocab_size = len(model.wv.vocab)
print(model.wv.index2word[vocab_size - 1], model.wv.index2word[vocab_size - 2],
model.wv.index2word[vocab_size - 3])

   the discovered vocabulary is found in model.wv.vocab     by taking the
   length of this dictionary, we can determine the vocabulary size (in
   this case, it is 47,134 elements long). the code above returns:
      zanetti markschies absentia        rare words indeed.
# find the index of the 2nd most common word ("of")
print('index of "of" is: {}'.format(model.wv.vocab['of'].index))

   we can also go the other way i.e. retrieve the index of a word we
   supply.  in this case, we are getting the index of the second most
   common word    of   . as expected the above code returns    index of    of    is:
   1   .
# some similarity fun
print(model.wv.similarity('woman', 'man'), model.wv.similarity('man', 'elephant'
))

   we can also easily extract similarity measures between word vectors
   (gensim uses [30]cosine similarity). the above code returns    0.6599
   0.2955   , which again makes sense given the context such words are
   generally used in.
# what doesn't fit?
print(model.wv.doesnt_match("green blue red zebra".split()))

   this fun function determines which word doesn   t match the context of
   the others     in this case,    zebra    is returned.

   we also want to able to convert our data set from a list of words to a
   list of integer indexes, based on the vocabulary developed by gensim.
   to do so, we can use the following code:
# convert the input data into a list of integer indexes aligning with the wv ind
exes
# read the data into a list of strings.
def read_data(filename):
    """extract the first file enclosed in a zip file as a list of words."""
    with zipfile.zipfile(filename) as f:
        data = f.read(f.namelist()[0]).split()
    return data

def convert_data_to_index(string_data, wv):
    index_data = []
    for word in string_data:
        if word in wv:
            index_data.append(wv.vocab[word].index)
    return index_data

str_data = read_data(root_path + filename)
index_data = convert_data_to_index(str_data, model.wv)
print(str_data[:4], index_data[:4])

   the first function, read_data simply extracts the zip file data and
   returns a list of strings in the same order as our original text data
   set.  the second function loops through each word in the data set,
   determines if it is in the vocabulary*, and if so, adds the matching
   integer index to a list.  the code above returns:    [   anarchism   ,
      originated   ,    as   ,    a   ] [5237, 3080, 11, 5]   .

   * remember that some words in the data set will be missing from the
   vocabulary if they are very rare in the corpus.

   we can also save and reload our trained word vectors/embeddings by the
   following simple code:
# save and reload the model
model.save(root_path + "mymodel")
model = gensim.models.id97.load(root_path + "mymodel")

   finally, i   ll show you how we can extract the embedding weights from
   the gensim id97 embedding layer and store it in a numpy array,
   ready for use in tensorflow and keras.
# convert the wv word vectors into a numpy matrix that is suitable for insertion
# into our tensorflow and keras models
embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))
for i in range(len(model.wv.vocab)):
    embedding_vector = model.wv[model.wv.index2word[i]]
    if embedding_vector is not none:
        embedding_matrix[i] = embedding_vector

   in this case, we first create an appropriately sized numpy zeros array.
    then we loop through each word in the vocabulary, grabbing the word
   vector associated with that word by using the wv dictionary.  we then
   add the word vector into our numpy array.

   so there we have it     gensim id97 is a great little library that
   can execute the id27 process very quickly, and also has a
   host of other useful functionality.

   now i will show how you can use pre-trained gensim embedding layers in
   our tensorflow and keras models.

using gensim id97 embeddings in tensorflow

   for this application, we   ll setup a dummy tensorflow network with an
   embedding layer and measure the similarity between some words.  if
   you   re not up to speed with tensorflow, i suggest you check out my
   [31]tensorflow tutorial or this online course [32]data science:
   practical deep learning in theano + tensorflow
   [show?id=jbc0n5zkdzk&amp;bids=323058.772462&amp;type=2&amp;subid=0] .
   also, it   s probably a good idea to check out my [33]id97 tensorflow
   tutorial to understand how the embedding layer works.

   the first step is to select some random words from the top 100 most
   common words in our text data set.
valid_size = 16  # random set of words to evaluate similarity on.
valid_window = 100  # only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=false)
valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

   the last line saves the array of 16 random words into a tensorflow
   constant valid_dataset.

   for the next step, we take the embedding matrix from our gensim
   id97 simulation and    implant it    into a tensorflow variable which
   we use as our embedding layer.
# embedding layer weights are frozen to avoid updating embeddings while training
saved_embeddings = tf.constant(embedding_matrix)
embedding = tf.variable(initial_value=saved_embeddings, trainable=false)

   note that in the second line above for the tensorflow variable
   declaration, i   ve set the trainable argument to false. if we were using
   this layer in, say, training a recurrent neural network, if we didn   t
   set this argument to false our embedding layer would be trained in
   tensorflow with negative performance impacts. it   s probably not an
   overall bad strategy, i.e. starting with a gensim embedding matrix and
   then training further using something like a recurrent nn, but if you
   want your embedding layer fixed for performance reasons, you need to
   set trainable to false.

   the next chunk of code calculates the similarity between each of the
   word vectors using the [34]cosine similarity measure. it is explained
   more fully in my [35]id97 tensorflow tutorial, but basically it
   calculates the norm of all the embedding vectors, then performs a dot
   product between the validation words and all other word vectors.
# create the cosine similarity operations
norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=true))
normalized_embeddings = embedding / norm
valid_embeddings = tf.nn.embedding_lookup(
      normalized_embeddings, valid_dataset)
similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=true
)

   now we can run our tensorflow session and sort the eight words which
   are closest to our validation example words.  again, this code is
   explained in more detail in the previously mentioned tutorial.
# add variable initializer.
init = tf.global_variables_initializer()

with tf.session() as sess:
    sess.run(init)
    # call our similarity operation
    sim = similarity.eval()
    # run through each valid example, finding closest words
    for i in range(valid_size):
        valid_word = wv.index2word[i]
        top_k = 8  # number of nearest neighbors
        nearest = (-sim[i, :]).argsort()[1:top_k + 1]
        log_str = 'nearest to %s:' % valid_word
            for k in range(top_k):
            close_word = wv.index2word[nearest[k]]
            log_str = '%s %s,' % (log_str, close_word)
        print(log_str)

   this code will produce lines like:

     nearest to two: three, five, zero, four, six, one, seven, eight

   as you can see, our id97 embeddings produced by gensim have the
   expected results     in this example, we have number words being grouped
   together in similarity which makes sense.

   next up, let   s see how we can use the gensim id97 embeddings in
   keras.

using gensim id97 embeddings in keras

   we can perform similar steps with a keras model. in this case,
   following the example code previously shown in [36]the keras id97
   tutorial, our model takes two single word samples as input and finds
   the similarity between them.  the top 8 closest words loop is therefore
   slightly different than the previous example:
valid_size = 16  # random set of words to evaluate similarity on.
valid_window = 100  # only pick dev samples in the head of the distribution.
valid_examples = np.random.choice(valid_window, valid_size, replace=false)
# input words - in this case we do sample by sample evaluations of the similarit
y
valid_word = input((1,), dtype='int32')
other_word = input((1,), dtype='int32')
# setup the embedding layer
embeddings = embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding
_matrix.shape[1],
                      weights=[embedding_matrix])
embedded_a = embeddings(valid_word)
embedded_b = embeddings(other_word)
similarity = merge([embedded_a, embedded_b], mode='cos', dot_axes=2)
# create the keras model
k_model = model(input=[valid_word, other_word], output=similarity)

def get_sim(valid_word_idx, vocab_size):
    sim = np.zeros((vocab_size,))
    in_arr1 = np.zeros((1,))
        in_arr2 = np.zeros((1,))
    in_arr1[0,] = valid_word_idx
    for i in range(vocab_size):
        in_arr2[0,] = i
        out = k_model.predict_on_batch([in_arr1, in_arr2])
        sim[i] = out
    return sim

# now run the model and get the closest words to the valid examples
for i in range(valid_size):
    valid_word = wv.index2word[valid_examples[i]]
    top_k = 8  # number of nearest neighbors
    sim = get_sim(valid_examples[i], len(wv.vocab))
    nearest = (-sim).argsort()[1:top_k + 1]
    log_str = 'nearest to %s:' % valid_word
    for k in range(top_k):
        close_word = wv.index2word[nearest[k]]
        log_str = '%s %s,' % (log_str, close_word)
    print(log_str)

   as you can see when i setup the embeddings layer (using
   keras    dedicated embedding() layer), all we need to do is specify the
   input and output dimensions (vocabulary size and embedding vector
   length, respectively) and then assign the gensim embedding_matrix to
   the weights argument. all the remaining logic is a copy from the
   [37]keras id97 tutorial, so check that post out for more details.

   the code produces lines like:

     nearest to when: unless, if, where, whenever, then, before, once,
     finally

   here we can see that [38]subordinating conjunction word types have been
   grouped together     which is a good, expected result.

   so that wraps up the tutorial     in this post, i   ve shown you how to use
   gensim to create id97 id27s in a quick and efficient
   fashion.  i then gave an overview of how to    upload    these learned
   embeddings into tensorflow and keras.  i hope it has been helpful.

     __________________________________________________________________

   recommended online course: if you are more of a video course learner,
   check out this inexpensive udemy course: [39]natural language
   processing with deep learning in python
   [show?id=jbc0n5zkdzk&amp;bids=323058.918390&amp;type=2&amp;subid=0]
     __________________________________________________________________


about the author

     dimitris says:
   [40]september 15, 2017 at 2:18 pm

   hey andy! great article! thanks for publishing. btw why you are
   initializing:

   `embedding_matrix = np.zeros((len(model.wv.vocab) + 1, vector_dim))`
   and not
   `embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))`. it
   doesnt seem there is a reason to add one more row.
     * andy says:
       [41]september 15, 2017 at 8:10 pm
       thanks dimitris     good pickup, that is an error. i   ve corrected it

     mark says:
   [42]november 9, 2017 at 8:01 pm

   hey andy, incredibly clear and concise tutorial. was able to follow
   along without even running code to visual results.

   i had some free time though and gave it a go to play around a bit.
   i can   t seem to get any results though when executing the model built
   with tensorflow.

   it   s an error that name    wv    is not defined. it   s picking it up after
   that first for statement during tf.session().

   any idea what   s wrong?
   thanks.
     * andy says:
       [43]november 10, 2017 at 10:11 pm
       hi mark, thanks very much for your kind comments. unfortunately i
       am unable to replicate this error, so i can   t think of why this may
       be occurring

     hyunjae says:
   [44]november 26, 2017 at 11:36 pm

   hi andy!.
   i saw your good tutorial.
   i got one problem.

   embedding_matrix = np.zeros((len(model.wv.vocab), vector_dim))

   what does vector_dim mean?

   i don   t find about vector_dim explanation.

   can you help me?

   thanks.
     * olivier paalvast says:
       [45]january 22, 2018 at 11:18 pm
       hey, i ran into this problem as well at first. if you look back to
       when the model was created an option size=300 was added in :
       model = id97.id97(sentences, iter=10, min_count=10,
       size=300, workers=4)
       this 300 is the effectively the size of your model so when
       embedding the words your vector_dim has to be equal to 300 as well.

   ____________________ (button)

   recent posts
     * [46]an introduction to id178, cross id178 and kl divergence in
       machine learning
     * [47]google colaboratory introduction     learn how to build deep
       learning systems in google colaboratory
     * [48]keras, eager and tensorflow 2.0     a new tf paradigm
     * [49]introduction to tensorboard and tensorflow visualization
     * [50]tensorflow eager tutorial

   recent comments
     * andry on [51]neural networks tutorial     a pathway to deep learning
     * sandipan on [52]keras lstm tutorial     how to easily build a
       powerful deep learning language model
     * andy on [53]neural networks tutorial     a pathway to deep learning
     * martin on [54]neural networks tutorial     a pathway to deep learning
     * uri on [55]the vanishing gradient problem and relus     a tensorflow
       investigation

   archives
     * [56]march 2019
     * [57]january 2019
     * [58]october 2018
     * [59]september 2018
     * [60]august 2018
     * [61]july 2018
     * [62]june 2018
     * [63]may 2018
     * [64]april 2018
     * [65]march 2018
     * [66]february 2018
     * [67]november 2017
     * [68]october 2017
     * [69]september 2017
     * [70]august 2017
     * [71]july 2017
     * [72]may 2017
     * [73]april 2017
     * [74]march 2017

   categories
     * [75]amazon aws
     * [76]cntk
     * [77]convolutional neural networks
     * [78]cross id178
     * [79]deep learning
     * [80]gensim
     * [81]gpus
     * [82]keras
     * [83]id168s
     * [84]lstms
     * [85]neural networks
     * [86]nlp
     * [87]optimisation
     * [88]pytorch
     * [89]recurrent neural networks
     * [90]id23
     * [91]tensorboard
     * [92]tensorflow
     * [93]tensorflow 2.0
     * [94]weight initialization
     * [95]id97

   meta
     * [96]log in
     * [97]entries rss
     * [98]comments rss
     * [99]wordpress.org

   copyright text 2019 by adventures in machine learning.   -  designed by
   [100]thrive themes | powered by [101]wordpress

   (button) close dialog

   session expired

   [102]please log in again. the login page will open in a new tab. after
   logging in you can close it and return to this page.

   >

   we use cookies to ensure that we give you the best experience on our
   website. if you continue to use this site we will assume that you are
   happy with it.[103]ok

references

   visible links
   1. https://adventuresinmachinelearning.com/gensim-id97-tutorial/feed/
   2. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/gensim-id97-tutorial/
   3. https://adventuresinmachinelearning.com/wp-json/oembed/1.0/embed?url=https://adventuresinmachinelearning.com/gensim-id97-tutorial/&format=xml
   4. https://www.adventuresinmachinelearning.com/
   5. https://adventuresinmachinelearning.com/about/
   6. https://adventuresinmachinelearning.com/coding-deep-learning-ebook/
   7. https://adventuresinmachinelearning.com/contact/
   8. https://adventuresinmachinelearning.com/ebook-newsletter-sign/
   9. https://adventuresinmachinelearning.com/author/admin/
  10. https://adventuresinmachinelearning.com/category/nlp/gensim/
  11. https://adventuresinmachinelearning.com/
  12. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  13. https://adventuresinmachinelearning.com/gensim-id97-tutorial/
  14. http://adventuresinmachinelearning.com/gensim-id97-tutorial/#comments
  15. https://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/
  16. https://deeplearning4j.org/thoughtvectors
  17. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  18. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  19. http://cython.org/
  20. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.918390&type=2&murl=https://www.udemy.com/natural-language-processing-with-deep-learning-in-python/
  21. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  22. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  23. https://adventuresinmachinelearning.com/neural-networks-tutorial/
  24. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  25. https://code.google.com/archive/p/id97/
  26. http://mattmahoney.net/dc/
  27. https://github.com/adventuresinml/adventures-in-ml-code
  28. http://pymbook.readthedocs.io/en/latest/igd.html
  29. https://radimrehurek.com/gensim/models/id97.html
  30. https://en.wikipedia.org/wiki/cosine_similarity
  31. https://adventuresinmachinelearning.com/python-tensorflow-tutorial/
  32. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.772462&type=2&murl=https://www.udemy.com/data-science-deep-learning-in-theano-tensorflow/
  33. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  34. https://en.wikipedia.org/wiki/cosine_similarity
  35. https://adventuresinmachinelearning.com/id97-tutorial-tensorflow/
  36. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  37. https://adventuresinmachinelearning.com/id97-keras-tutorial/
  38. https://en.wikipedia.org/wiki/conjunction_(grammar)
  39. https://click.linksynergy.com/link?id=jbc0n5zkdzk&offerid=323058.918390&type=2&murl=https://www.udemy.com/natural-language-processing-with-deep-learning-in-python/
  40. https://adventuresinmachinelearning.com/gensim-id97-tutorial/#comments/5077
  41. https://adventuresinmachinelearning.com/gensim-id97-tutorial/#comments/5078
  42. https://adventuresinmachinelearning.com/gensim-id97-tutorial/#comments/5079
  43. https://adventuresinmachinelearning.com/gensim-id97-tutorial/#comments/5080
  44. https://adventuresinmachinelearning.com/gensim-id97-tutorial/#comments/5083
  45. https://adventuresinmachinelearning.com/gensim-id97-tutorial/#comments/5088
  46. https://adventuresinmachinelearning.com/cross-id178-kl-divergence/
  47. https://adventuresinmachinelearning.com/introduction-to-google-colaboratory/
  48. https://adventuresinmachinelearning.com/keras-eager-and-tensorflow-2-0-a-new-tf-paradigm/
  49. https://adventuresinmachinelearning.com/introduction-to-tensorboard-and-tensorflow-visualization/
  50. https://adventuresinmachinelearning.com/tensorflow-eager-tutorial/
  51. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/139
  52. https://adventuresinmachinelearning.com/keras-lstm-tutorial/#comments/5153
  53. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/136
  54. https://adventuresinmachinelearning.com/neural-networks-tutorial/#comments/135
  55. https://adventuresinmachinelearning.com/vanishing-gradient-problem-tensorflow/#comments/5233
  56. https://adventuresinmachinelearning.com/2019/03/
  57. https://adventuresinmachinelearning.com/2019/01/
  58. https://adventuresinmachinelearning.com/2018/10/
  59. https://adventuresinmachinelearning.com/2018/09/
  60. https://adventuresinmachinelearning.com/2018/08/
  61. https://adventuresinmachinelearning.com/2018/07/
  62. https://adventuresinmachinelearning.com/2018/06/
  63. https://adventuresinmachinelearning.com/2018/05/
  64. https://adventuresinmachinelearning.com/2018/04/
  65. https://adventuresinmachinelearning.com/2018/03/
  66. https://adventuresinmachinelearning.com/2018/02/
  67. https://adventuresinmachinelearning.com/2017/11/
  68. https://adventuresinmachinelearning.com/2017/10/
  69. https://adventuresinmachinelearning.com/2017/09/
  70. https://adventuresinmachinelearning.com/2017/08/
  71. https://adventuresinmachinelearning.com/2017/07/
  72. https://adventuresinmachinelearning.com/2017/05/
  73. https://adventuresinmachinelearning.com/2017/04/
  74. https://adventuresinmachinelearning.com/2017/03/
  75. https://adventuresinmachinelearning.com/category/amazon-aws/
  76. https://adventuresinmachinelearning.com/category/deep-learning/cntk/
  77. https://adventuresinmachinelearning.com/category/deep-learning/convolutional-neural-networks/
  78. https://adventuresinmachinelearning.com/category/loss-functions/cross-id178/
  79. https://adventuresinmachinelearning.com/category/deep-learning/
  80. https://adventuresinmachinelearning.com/category/nlp/gensim/
  81. https://adventuresinmachinelearning.com/category/deep-learning/gpus/
  82. https://adventuresinmachinelearning.com/category/deep-learning/keras/
  83. https://adventuresinmachinelearning.com/category/loss-functions/
  84. https://adventuresinmachinelearning.com/category/deep-learning/lstms/
  85. https://adventuresinmachinelearning.com/category/deep-learning/neural-networks/
  86. https://adventuresinmachinelearning.com/category/nlp/
  87. https://adventuresinmachinelearning.com/category/optimisation/
  88. https://adventuresinmachinelearning.com/category/deep-learning/pytorch/
  89. https://adventuresinmachinelearning.com/category/deep-learning/recurrent-neural-networks/
  90. https://adventuresinmachinelearning.com/category/reinforcement-learning/
  91. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorboard/
  92. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/
  93. https://adventuresinmachinelearning.com/category/deep-learning/tensorflow/tensorflow-2-0/
  94. https://adventuresinmachinelearning.com/category/deep-learning/weight-initialization/
  95. https://adventuresinmachinelearning.com/category/nlp/id97/
  96. https://adventuresinmachinelearning.com/wp-login.php
  97. https://adventuresinmachinelearning.com/feed/
  98. https://adventuresinmachinelearning.com/comments/feed/
  99. https://wordpress.org/
 100. https://www.thrivethemes.com/
 101. http://www.wordpress.org/
 102. https://adventuresinmachinelearning.com/wp-login.php
 103. http://adventuresinmachinelearning.com/gensim-id97-tutorial/

   hidden links:
 105. https://adventuresinmachinelearning.com/author/admin/
