   #[1]hashrocket - all posts [2]hashrocket - all posts [3]publisher

[4]hashrocket

   [5]menu

     * [6]work
     * [7]team
     * [8]services
     * [9]blog
     * [10]contact

   [11]1-877-885-8846

   heading image for post: a friendly introduction to convolutional neural
   networks

a friendly introduction to convolutional neural networks

   [12]profile picture of ifu aniemeka

   by ifu aniemeka on august 22, 2017
   [13]

introduction

   convolutional neural networks (or convnets for short) are used in
   situations where data can be expressed as a "map" wherein the proximity
   between two data points indicates how related they are. an image is
   such a map, which is why you so often hear of convnets in the context
   of image analysis. if you take an image and randomly rearrange all of
   its pixels, it is no longer recognizable. the relative position of the
   pixels to one another, that is, the order, is significant.

   convnets are commonly used to categorize things in images, so that's
   the context in which we'll discuss them.

   a convnet takes an image expressed as an array of numbers, applies a
   series of operations to that array and, at the end, returns the
   id203 that an object in the image belongs to a particular class
   of objects. for instance, a convnet can let you know the id203
   that a photo you took contains a building or a horse or what have you.
   it might be used to distinguish between very similar instances of
   something. for example, you could use a convnet to go through a
   collection of images of skin lesions and classify the lesions as benign
   or malignant.

   convnets contain one or more of each of the following layers:
     * convolution layer
     * relu (rectified linear units) layer
     * pooling layer
     * fully connected layer
     * loss layer (during the training process)

   we could also consider the initial inputs i.e. the pixel values of the
   image, to be a layer. however, since no operation occurs at this point,
   i've excluded it from the list.
   [14]

convolution layer

   the architecture of a convnet is modeled after the mammalian visual
   cortex, the part of the brain where visual input is processed. within
   the visual cortex, specific neurons fire only when particular phenomena
   are in the field of vision. one neuron might fire only when you are
   looking at a left-sloping diagonal line and another only when a
   horizontal line is in view.

   our brains process images in layers of increasing complexity. the first
   layer distinguishes basic attributes like lines and curves. at higher
   levels, the brain recognizes that a configuration of edges and colors
   is, for instance, a house or a bird.

   in a similar fashion, a convnet processes an image using a matrix of
   weights called filters (or features) that detect specific attributes
   such as diagonal edges, vertical edges, etc. moreover, as the image
   progresses through each layer, the filters are able to recognize more
   complex attributes.

   to a computer, an image is just an array of numbers. an image using the
   full spectrum of colors is represented using a 3-dimensional matrix of
   numbers.

   alt text

   the depth of the matrix corresponds to rgb values. for the sake of
   simplicity, we'll initially consider only grayscale images. each pixel
   in a grayscale image can be represented using a single value that
   indicates the intensity of the pixel. these values lie between 0 and
   255, where 0 is black and 255 is white.

   the convolution layer is always the first step in a convnet. let's say
   we have a 10 x 10 pixel image, here represented by a 10 x 10 x 1 matrix
   of numbers:

   alt text

   i've stuck to zeroes and ones to make the math simpler. you'll see what
   i'm talking about in a bit.

   the output of a convolution layer is something called a feature map (or
   activation map).

   in order to generate a feature map, we take an array of weights (which
   is just an array of numbers) and slide it over the image, taking the
   dot product of the smaller array and the pixel values of the image as
   we go. this operation is called convolution. the array of weights is
   referred to as a filter or a feature. below, we have a 3 x 3 filter (as
   with the image, we've used 1s and 0s for simplicity).

   alt text

   we then use the filter to generate a feature map

   alt text

   the image shows the calculation for the first dot product taken.
   imagine the filter overlaid on the upper left-hand corner of the image.
   each weight in the filter is multiplied by the number beneath it. we
   then sum those products and put them in the upper left hand corner of
   what will be the feature map. we slide the filter to the right by 1
   pixel and repeat the operation, placing the sum of the products in the
   next slot in the feature map. when we reach the end of the row, we
   shift the filter down by 1 pixel and repeat.

   filters are used to find where in an image details such as horizontal
   lines, curves, colors, etc. occur. the filter above finds right-sloping
   diagonal lines.

   note that higher values in the feature map are in roughly the same
   location as the diagonal lines in the image. regardless of where a
   feature appears in an image, the convolution layer will detect it. if
   you know that your convnet can identify the letter 'a' when it is in
   the center of an image, then you know that it can also find it when it
   is moved to the right-hand side of the image. any shift in the 'a' is
   reflected in the feature maps. this property of the convolution layer
   is called translation equivariance.

   thus far, we've been discussing the convolutional layer in the context
   of a grayscale image with a 2-dimensional filter. it's also possible
   that you would be dealing with a color image, the representation of
   which is a 3-dimensional matrix, as well as multiple filters per
   convolutional layer, each of which is also 3-dimensional. this does
   require a little more calculation, but the math is still basically the
   same.

   alt text

   you'll see that we're still taking the dot product, it's just that this
   time around we need to add the products along the depth dimension as
   well.

   in the above example, we convolved two 3 x 3 x 3 filters with a 6 x 6 x
   3 image and the result was a 3 x 3 x 2 feature map. if we had used
   three filters, the feature map would have been 3 x 3 x 3. if there were
   4, then the size would have been 3 x 3 x 4, and so on.

   to get the dimensions of the feature map is fairly straightforward.

   our input image has a width of 6, a height of 6, and a depth of 3, i.e.
   wi= 6, hi = 6, di = 3. each of our filters has a width of 3, a height
   of 3 and a depth of 3 i.e. wf = 3, hf = 3, df = 3.

   the stride is the number of pixels you move the filter between each dot
   product operation. in our example, we would take the dot product, move
   the filter over by one pixel, and repeat. when we get to the edge of
   the image, we move the filter down by one pixel. hence, the stride in
   our example convolution layer is 1.

   the width and height of the feature map are calculated like so:

   w[m] = (w[i] - w[f])/s + 1

   h[m] = (h[i] - h[f])/s + 1

   where s is the stride.

   let   s break that down a bit. if you have a filter that   s 2 x 2 and your
   image is 10 x 10, then when you overlay your filter on your image,
   you   ve already taken up 2 spaces across, i.e. along the width. that
   counts for 1 spot in the feature map (that   s the    +1    part). to get the
   number of spots left, you subtract the filter   s width from the total
   width. in this case, the result is 8. if your stride is 1, you have 8
   spots left, i.e. 8 more dot products to take until you get to the edge
   of the image. if your stride is 2, then you have 4 more dot products to
   take.

   the depth of the feature map is always equal to the number of filters
   used; in our case, 2.
   [15]

relu layer

   the relu (short for rectified linear units) layer commonly follows the
   convolution layer. the addition of the relu layer allows the neural
   network to account for non-linear relationships, i.e. the relu layer
   allows the convnet to account for situations in which the relationship
   between the pixel value inputs and the convnet output is not linear.
   note that the convolution operation is a linear one. the output in the
   feature map is just the result of multiplying the weights of a given
   filter by the pixel values of the input and adding them up:

   y = w[1]x[1] +w[2]x[2] + w[3]x[3] + ...

   where w is a weight value and x is a pixel value.

   the relu function takes a value x and returns 0 if x is negative and x
   if x is positive.

   f(x) = max(0,x)

   alt text

   as you can see from the graph, the relu function is nonlinear. in this
   layer, the relu function is applied to each point in the feature map.
   the result is a feature map without negative values.

   alt text

   other functions such as tanh or the sigmoid function can be used to add
   non-linearity to the network, but relu generally works better in
   practice.
   [16]

pooling layer

   the pooling layer also contributes towards the ability of the convnet
   to locate features regardless of where they are in the image. in
   particular, the pooling layer makes the convnet less sensitive to small
   changes in the location of a feature, i.e. it gives the convnet the
   property of translational invariance in that the output of the pooling
   layer remains the same even when a feature is moved a little. pooling
   also reduces the size of the feature map, thus simplifying computation
   in later layers.

   there are a number of ways to implement pooling, but the most effective
   in practice is max pooling. to perform max pooling, imagine a window
   sliding across the feature map. as the window moves across the map, we
   grab the largest value in the window and discard the rest.

   alt text

   as mentioned earlier, the output indicates the general region where a
   feature is present, as opposed to the precise location, which isn't
   really important. in the above diagram, the result of the pooling
   operation indicates that the feature can be found in the upper
   left-hand corner of the feature map, and, thus, in the upper left hand
   corner of the image. we don   t need to know that the feature is exactly,
   say 100 pixels down and 50 pixels to the right relative to the top-left
   corner.

   as an example, if we're trying to discern if an image contains a dog,
   we don't care if one of the dog's ears is flopped slightly to the
   right.

   the most common implementation of max pooling, and the one used in the
   example image, uses a 2 x 2 pixel window and a stride of 2, i.e. we
   take the largest value in the window, move the window over by 2 pixels,
   and repeat.

   the operation is basically the same for 3d feature maps as well. the
   dimensions of a 3d feature map are only reduced along the x and y axes.
   the depth of the pooling layer output is equal to the depth of the
   feature map.
   [17]

the fully-connected and loss layers

   the fully-connected layer is where the final "decision" is made. at
   this layer, the convnet returns the id203 that an object in a
   photo is of a certain type.

   the convolutional neural networks we've been discussing implement
   something called supervised learning. in supervised learning, a neural
   network is provided with labeled training data from which to learn.
   let's say you want your convnet to tell you if an image is of a cat or
   of a dog. you would provide your network with a large set of pictures
   of cats and dogs, where pictures of cats are labeled 'cat' and pictures
   of dogs are labeled 'dog'. this is called the training set. then, based
   on the difference between its guesses and the actual values, the
   network adjusts itself such that it becomes more accurate each time you
   run a test image through it.

   you confirm that your network is in fact able to properly classify
   photos of cats and dogs in general (as opposed to just being able to
   classify photos in the training set you provided) by running it against
   an unlabeled collection of images. this collection is called the test
   set.

   in this example, the fully-connected layer might return an output like
   "0.92 dog, 0.08 cat" for a specific image, indicating that the image
   likely contains a dog.

   the fully-connected layer has at least 3 parts - an input layer, a
   hidden layer, and an output layer. the input layer is the output of the
   preceding layer, which is just an array of values.

   alt text

   you'll note in the image, there are lines extending from the inputs
   (x[a] to x[e]) to nodes (y[a] to y[d]) that represent the hidden layer
   (so called because they   re sandwiched between the input and output
   layers and, thus,    invisible   ). the input values are assigned different
   weights w[xy] per connection to a node in the hidden layer (in the
   image, only the weights for the value xa are labeled). each of the
   circles in the hidden layer is an instance of computation. such
   instances are often called neurons. each neuron applies a function to
   the sum of the product of a weight and its associated input value.

   the neurons in the output layer correspond to each of the possible
   classes the convnet is looking for. similar to the interaction between
   the input and hidden layer, the output layer takes in values (and their
   corresponding weights) from the hidden layer, applies a function and
   puts out the result. in the example above, there are two classes under
   consideration - cats and dogs.

   following the fully-connected layer is the loss layer, which manages
   the adjustments of weights across the network. before the training of
   the network begins, the weights in the convolution and fully-connected
   layers are given random values. then during training, the loss layer
   continually checks the fully-connected layer's guesses against the
   actual values with the goal of minimizing the difference between the
   guess and the real value as much as possible. the loss layer does this
   by adjusting the weights in both the convolution and fully-connected
   layers.
   [18]

hyperparameters

   it should be said at this point that each of these layers (with the
   exception of the loss layer) can be multiply stacked on one another.
   you may very well have a network that looks like this:

   convolutional layer >> relu >> pooling >> convolutional layer >>
   pooling >> fully-connected layer >> convolutional layer >>
   fully-connected layer >> loss layer

   there are other parameters in addition to the order and number of the
   layers of a convnet that an engineer can modify. the parameters that
   are adjusted by a human agent are referred to as hyperparameters. this
   is where an engineer gets to be creative.

   other hyperparameters include the size and number of filters in the
   convolution layer and the size of the window used in the max pooling
   layer.
   [19]

overview

   so, let's run through an entire convolutional neural network from start
   to finish just to clarify things. we start with an untrained convnet in
   which we have determined all of the hyperparameters. we initialize the
   weights in the convolution and fully-connected layers with random
   inputs. we then feed it images from our training set. let   s say we have
   one instance of each layer in an example network. each image is
   processed first in the convolution layer, then in the relu layer, then
   in the pooling layer. the fully-connected layer receives inputs from
   the pooling layer and uses these inputs to return a guess as to the
   contents of the image. the loss layer compares the guess to the actual
   value and figures out by how much to adjust the weights to bring the
   guess closer to the actual value. for instance, if the convnet returns
   that there is an 87% chance that an image contains a dog, and the image
   does indeed contain a dog, the guess is off by 13% and the weights are
   adjusted to bring that guess closer to 100%.
   [20]

other resources

   [21]understanding neural networks through deep visualization by jason
   yosinksi, jeff clune, ang nguyen, et al.

   [22]how do convolutional neural networks work? by brandon rohrer

   [23]a quick introduction to neural networks by ujjwal karn

   [24]convolutional neural networks for visual recognition by andrej
   karpathy

   photo by [25]nasa on [26]unsplash

   was this post helpful? share it with others.
     * [27]twitter logo tweet
     * [28]facebook logo share
     * [29]linkedin logo post

   [30]heading image for keep anaconda from constricting your homebrew
   installs up next keep anaconda from constricting your homebrew installs

   more posts about [31]data science [32]convolutional neural networks
   [33]machine learning [34]neural networks

subscribe today!

   stay ahead of the curve. receive valuable blog posts, resources and
   event notices right to your inbox.

   email address
          ____________________ sign up

   thanks for signing up for our newsletter.

     * [35]home
     * [36]work
     * [37]team
     * [38]blog
     * [39]contact

     * [40]services
     * [41]careers
     * [42]brand
     * [43]today i learned (til)

     * [44]

    jacksonville beach
       320 1st street n #714
       jacksonville beach, fl 32250
     * [45]

    chicago
       661 w lake st. suite 3ne
       chicago, il 60661

   [46]1-877-885-8846

     * [47]facebook
     * [48]twitter
     * [49]github

      2019 hashrocket

references

   1. https://hashrocket.com/blog.rss
   2. https://hashrocket.com/blog.json
   3. https://plus.google.com/+hashrocket
   4. https://hashrocket.com/
   5. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks
   6. https://hashrocket.com/work
   7. https://hashrocket.com/team
   8. https://hashrocket.com/services
   9. https://hashrocket.com/blog
  10. https://hashrocket.com/contact
  11. tel:1-877-885-8846
  12. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks
  13. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#introduction
  14. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#convolution-layer
  15. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#relu-layer
  16. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#pooling-layer
  17. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#the-fully-connected-and-loss-layers
  18. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#hyperparameters
  19. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#overview
  20. https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks#other-resources
  21. http://yosinski.com/deepvis
  22. https://brohrer.github.io/how_convolutional_neural_networks_work.html
  23. https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/
  24. http://cs231n.github.io/convolutional-networks/
  25. https://unsplash.com/collections/235/best-of-nasa
  26. https://unsplash.com/
  27. https://twitter.com/share?text=a+friendly+introduction+to+convolutional+neural+networks&url=https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks
  28. https://www.facebook.com/sharer/sharer.php?u=https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks
  29. https://www.linkedin.com/sharearticle?mini=true&title=a+friendly+introduction+to+convolutional+neural+networks&url=https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks
  30. https://hashrocket.com/blog/posts/keep-anaconda-from-constricting-your-homebrew-installs
  31. https://hashrocket.com/blog/tags/data-science
  32. https://hashrocket.com/blog/tags/convolutional-neural-networks
  33. https://hashrocket.com/blog/tags/machine-learning
  34. https://hashrocket.com/blog/tags/neural-networks
  35. https://hashrocket.com/
  36. https://hashrocket.com/work
  37. https://hashrocket.com/team
  38. https://hashrocket.com/blog
  39. https://hashrocket.com/contact
  40. https://hashrocket.com/services
  41. https://hashrocket.com/careers
  42. https://hashrocket.com/brand
  43. https://til.hashrocket.com/
  44. https://www.google.com/maps/place/hashrocket/@30.2918842,-81.3905494,15z/data=!4m12!1m6!3m5!1s0x0:0xab9af841d2073454!2shashrocket!8m2!3d30.2918842!4d-81.3905494!3m4!1s0x0:0xab9af841d2073454!8m2!3d30.2918842!4d-81.3905494
  45. https://www.google.com/maps/place/hashrocket/@41.8853796,-87.6473097,17z/data=!3m1!4b1!4m13!1m7!3m6!1s0x880e2cc58813a5d7:0x968e8408bab03d30!2shashrocket!3b1!8m2!3d41.8853796!4d-87.645121!3m4!1s0x880e2cc58813a5d7:0x968e8408bab03d30!8m2!3d41.8853796!4d-87.645121
  46. tel:1-877-885-8846
  47. https://www.facebook.com/hashrocket
  48. https://twitter.com/hashrocket
  49. https://github.com/hashrocket
