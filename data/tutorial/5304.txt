introductory course at 
esslli 
bolzano, italia  
august 2016  

id104  

linguistic datasets 

lecture 4 

chris biemann 

biem@cs.tu-darmstadt.de 

lesson 4: a few sample projects 

projects were selected to span a wide range of nlp tasks and whether they 
discuss id104-related issues 
coming up next: 
      part-of-speech tagging 
      id39 and classification 
      prepositional phrase attachment 
      word alignment 
      id36 
      question rating 
      image annotation 
since id104 has become a commodity, there are less and less papers that 
specifically discuss id104 practices for nlp. 
 
many examples from the naacl 2010 workshop on creating speech and 
language data with amazon   s mechanical turk: 24 participants were granted $100 
each to promote id104 in nlp 

2 

use of manually acquired data in nlp 

     resource creation  
      putting together a dictionary for human or machine use 
      includes: wikipedia, wiktionary, id138 
     training data acquisition 
      create training / development / test data for machine learning 
      includes: treebanking, text annotation, translation, document class labeling, 
marking as spam 

     evaluation 
      have system output manually checked 
      post-hoc evaluations for all sorts of nlp systems 

all of the above can be crowdsourced, but pose different challenges     
mostly related to the missing expertise of the average crowdworker, as well 
as quality control in light of the vagueness/variety of language. 

3 

crowdsourced re-annotation of id52 data 

     task: assign parts of speech to twitter data 
q/noun :/. hay/prt justin/noun screeeeeeeem/prt !!!!!!!/. i/pron 
luv/verb u/pron omg/prt !!!!!!!!!/. i/verb did/verb a/det quiz/noun 
ubout/adp if/adp me/pron and/conj u/pron wer/verb thu/det only/adj 
ones/pron o/adp http://www.society.me/q/29910/view/x 
     motivation: language change on twitter is rapid, thus models fall out of 
use quickly 

crowdflower interface 

hovy, d., plank, b., s  gaard, a. 
(2014): experiments with 
crowdsourced re-annotation of a 
id52 data set. 
proceedings of the 52nd annual 
meeting of the association for 
computational linguistics (short 
papers), pages 377   382, 
baltimore, md, usa 

4 

crowdsourced re-annotation of id52 data ii 

     crowd setup on crowdflower: 
      only trusted crowdworkers: need to pass 4 test items 
      reward: $0.05 for 10 tokens / 5 annotations per token, thus 2.5 cents / token 
      full dataset: 14,619 tokens, took 10 days to complete 
      high satisfaction of crowdworkers with the task 
     aggregation: comparing majority voting (mv) with mace 
      mv: treat all annotators equally and choose the label that most annotators 
supply 
      mace: treat annotator competence and true label as hidden variables and 
estimate both with expectation maximization (hovy et al., 2013) 

     evaluation: 
      compare to gold standard labels from expert annotators 
      compare ml model quality 
      compare impact on a downstream tasks, here: chunking and ner 

hovy, d. berg-kirkpatrick, t., vaswani, a., hovy, e. (2013). learning whom to trust with mace. proceedings of naacl-2013, atlanta, ga, usa. 

5 

crowdsourced re-annotation of id52 data iii 

     over 10% of tokens never received 
gold label, mostly related to 
punctuation and pronouns 
     mace scheme helps a little, filtering 
with wiktionary helps more 
     impact on downstream: yes for 
chunking, no for ner 

6 

id39 with id104 

     task: mark name spans in text and assign a class label 
     challenge for id104: 
      standard interface does not 
support the marking of spans 
      payment scheme encourages  
low recall if we pay    per paragraph    

     solution:  
      custom interface 
      bonus system using command line tools 

lawson, n., eustice, k., perkowitz, m. (2010): annotating large email datasets for id39 with mechanical turk. proceedings of 
the naacl hlt 2010 workshop on creating speech and language data with amazon   s mechanical turk, pages 71   79, los angeles, ca, usa 

7 

id39 with id104 ii 

     web-based gui that supports highlighting/marking of tokens, written in 
javascript 
     annotation of 20,609 email messages of 400 characters on average 
     looking for three types person, organization and location separately: 
in each task, only one type is sought for 
      for per, workers also annotated unnamed mentions like    my mom   , thus a 
separate class of these was included, just to discard its contents for ner 

     pricing scheme on amazon mturk 
      $0.01 for each hit     regardless of the number of entities found 
      $0.01 / $0.02 bonus for each entity found  
      bonus only paid if the majority of annotators found the respective entity 
     setup on mturk 
      batches of 100     1000 emails: larger batches completed faster 
      798 workers in total, only 10 scammers that never marked any entity 

8 

id39 with id104 iii 

     different types have different recall levels: 
need more workers to catch all locs and 
orgs, fewer to catch pers 
     bonus system seems to work: most productive 
workers tend to have a high recall 
     using annotations that at least 2 workers 
marked produced best tagging results (more: 
recall too little; less: precision issues) 

the eager beavers 

9 

id39 with id104 iv 

     alternative interface 
using standard forms 
(generated per hit) 
     more complex, does 
not handle overlapping 
annotations 
     was tested only on 
small batches, hence 
unclear how scammers 
should be handled 
when scaling up  

finin, t., murnane, w., karandikar, a., keller, n., martineau, j., dredze, m. (2010): annotating named entities in twitter data with id104. procee   
dings of the naacl hlt 2010 workshop on creating speech and language data with amazon   s mechanical turk, pages 80   88, los angeles, ca, usa 

10 

pp attachment: major issue for phrase structure grammars 

s
np
vp
pp
n
v
p
d

     np vp 
     n | d n | np pp 
     v np | v np pp 
     p np 
     i | elephant | pajamas 
     shot 
     in 
     my | an | a | the 

     syntax trees can (almost) be modeled with 
context-free languages 

 
 

np 

s 

vp 

np 

n 
i 

v 
shot 

d 
an 

n 

elephant 

p 
in 

pp 

np 

n 

pajamas 

d 
my 

11 

pp attachment: major issue for phrase structure grammars 

     syntax trees can (almost) be modeled with 
context-free languages 
     one surface sentence can have several 
derivations 
s 

s
np
vp
pp
n
v
p
d

     np vp 
     n | d n | np pp 
     v np | v np pp 
     p np 
     i | elephant | pajamas 
     shot 
     in 
     my | an | a | the 

 
 

np 

vp 

np 

np 

n 
i 

v 
shot 

d 
an 

n 

elephant 

p 
in 

how he got into my 
pajamas i'll never know! 
- groucho marx 

pp 

np 

n 

pajamas 

d 
my 

12 

id104 for pp attachment i 

     motivation: pp attachment bias is different for different genres 
     need semantic knowledge to disambiguate pp attachment ambiguities 
     setup:  
      generate possible attachments from pos tag sequences and chunks 
      generate id104 questions to decide the correct attachment 

jha, m., andreas, j., thadani, k., rosenthal, s., mckeown, k. (2010): corpus creation for new genres: a crowdsourced approach to pp attachment. 
proceedings of the naacl hlt 2010 workshop on creating speech and language data with amazon   s mechanical turk, pages 13   20, los angeles, 
ca, usa 

13 

id104 for pp attachment ii 

     id104 setup on mturk: 
      show sentence with pp highlighted, allow to pick best option to attach it 
      exits: workers can type additional options, indicate problems with hit 
      1000 hits, 5 workers per hit, $0.04 per question 
     results 
      typical accuracy/multiplicity tradeoff 
      about 5% loss due to chunker errors     these were often identified with the    exit    
option 

14 

id104 word alignment 

     motivation  
      machine translation systems learn from parallel data, usually from parallel 
sentences 
      word alignment is usually done automatically, but results in noise 
     solution: use id104 for word alignment 
     specialized interface on top of google web kit (javascript)  

gao, q. and vogel, s. (2010): consensus versus expertise : a case study of word alignment with mechanical turk. proceedings of the naacl hlt 
2010 workshop on creating speech and language data with amazon   s mechanical turk, pages 30   34, los angeles, ca, usa 

15 

id104 word alignment ii 

     collecting and accepting alignments with majority vote leads to partial 
alignments in presence of worker noise 
     information from partial alignments: a) we get pairs of aligned words and 
b) we know which words they are not aligned to 

     using this information to constrain an automatic 
aligner reduces overall alignment error 

     other observation: lack of chinese- 
speaking crowdworkers: task went slow,  
even after raising the price considerably.  

16 

crowds 4 id36  

     motivation 
      relation annotation (e.g. born in, plays  
for ..) in text is expensive 
      distant supervision: use a knowledge  
base to find patterns in which known relations occur helps but is error-prone 
      can use id104 to manually correct wrong extractions  
     setup 
      show 10 sentences with relations (from 17 relations between persons) and have 
crowdworkers assign one of three options above 
      7 are automatically generated, 
3 control items 
      $0.05 per hit, 5 workers/hit 

gorid113y, m.r., gerber, a., harper, m., dredze, m. (2010): 
non-expert correction of automatically generated 
relation annotations. proceedings of the naacl hlt 
2010 workshop on creating speech and language data 
with amazon   s mechanical turk, pages 204   207, los 
angeles, ca, usa 

17 

crowds 4 id36 ii 

     inter-annotator-agreement: measures how much people provide the 
same labels for the same task. 
commonly used: cohen   s kappa 
     agreement often perceived as an 
upper bound for learning algorithms 
     here: expert annotators (e1/e2)  
show higher agreement than 
expert vs. majority vote (m); control 
questions seem    easier    
     filtering bad workers: by control items and by time (too short is bad) 

conger, a.j. (1980): integration and generalization of kappas for multiple 
raters. psychological bulletin, 88(2):322   328. 
landis, j. r. and koch, g. g. (1977): the measurement of observer 
agreement for categorical data. biometrics, 33(1):159-74. 

18 

question rating with id104 

     goal: automatically generate reading comprehension questions 
     why? because authors work for the educational testing service     hands 
up: who participated in: gre? toefl? pisa? 
     approach: overgenerate-and-rank paradigm: generate as many questions 
as possible, then pick the    best    by statistical ranking 
     ranker (any ranker!) needs to be trained on manually judgments 

     setup on mturk: 
      $0.05 per rating, 5 workers/hit 
      hourly wage: $5-$10 / hour 
      using default qualifications and manual 
filtering of bad workers 

heilman, m., smith, n.a. (2010): rating computer-generated questions with 
mechanical turk. proceedings of the naacl hlt 2010 workshop on creating 
speech and language data with amazon   s mechanical turk, pages 35-40, 
los angeles, ca, usa 

19 

question rating with id104 ii 

     results: averaging over 3-7 crowdworkers 
achieves the performance of a 
computational linguist, as measured by 
ranking correlation 
     when using this data for training (linear 
regression on a set of 326 numerical 
features), data shows a very positive trend 

20 

image annotation for  
many purposes 

     goal: produce simple, full-sentence  
descriptions of images 
     motivations: scene understanding, generation of paraphrases, training an 
image labeler, ...  

     generate-verify setup: 
      ask for descriptions of 1000 images, 10 per hit, $0.10 per hit, 5 workers/hit 
      judge for grammaticality/spelling without showing the picture: 5 per hit (1 
control), $0.08 per hit, 3 workers/hit 

     assessing the impact of a qualification test required to be able to work on 
the task: 
      grammar/spelling: detect whether there is an error 
      image content: choose the better description 

rashtchian, c., young, p., hodosh, m., hockenmaier, j. (2010): collecting image annotations using amazon   s mechanical turk. proceedings of the 
naacl hlt 2010 workshop on creating speech and language data with amazon   s mechanical turk, pages 139   147, los angeles, ca, usa 

21 

image annotation qualification test for grammar/spelling 

     d 

22 

image annotation for many purposes ii 

     qualification test results in much higher worker quality: unqualified 
contained nonsensical responses and a lot of grammar errors 
     verification not needed for qualified workers: simple pre-screening 
improves results a lot.  

23 

a video is worth 25 pictures per second... 
http://www.cs.utexas.edu/users/ml/clamp/videodescription/  

     msrvid corpus: same idea, but 
describing what can be seen in 2089 
(short) videos 
     this elicits descriptions of actions, 
rather than situations 
     data was used in the semeval tasks 
on short text similarity from 2012 
     works in any language: 
 

 85550 
english
serbian
 3420 
macedonian  1915 
french
 1226 

 hindi
 tamil
 spanish
 italian

 6245 
 2789 
 1883 
 953 

 romanian  3998 
 2735 
 dutch
 gujarati
 1437 
 georgian  907 

 slovene
 german
 russian
 polish

 3584 
 2326 
 1243 
 544 

chen, d. l. and dolan, w.b. (2011): collecting highly parallel data for paraphrase evaluation. in the proceedings of the 49th annual 
meetings of the association for computational linguistics (acl), portland, or, usa  

24 

let   s crowdsource!  
find all the spelling and grammar errors 

     voters all over europe have lost face in the eu because of its 
meddling in there lives, the eu comission president said in strassbourg. 
public support have collapsed right across the eus    28 member nations. 
     in a astonishing confesion of failure, he added:    we are no longer 
respected in our countrys when we emphazise the need to give priority  
to the eu.   . 
     his remarks were being seen as recognition of public revolsion at the eu 
ahead of britains    in-or-out referendum, on june 23 says the express. 

hands up     how many errors?  

 

25 

let   s crowdsource!  
find all the spelling and grammar errors 

     voters all over europe have lost faith in the eu because of its meddling 
in their lives, the eu commission president said in strasbourg.  
public support has collapsed right across the eu   s 28 member nations. 
     in an astonishing confession of failure, he added:    we are no longer 
respected in our countries when we emphasise the need to give priority 
to the eu.   . 
     his remarks were being seen as recognition of public revulsion at the eu 
ahead of britain   s in-or-out referendum on june 23, says the express. 

13 errors!  

http://gibraltarpanorama.gi/15209/191118/a/eu-faces-ruin-voters-all-over-europe-lose-faith-in-the-eu-says-european-commissi 

26 

id104 translations 
(materials from chris callison-burch   s tutorial)   

     motivation:  
      train a machine translation system 
      existing parallel data does not cover all languages and domains 
     solution 
      use id104 for translation 

 
      zaidan&callison-burch   11 setup on mturk: 
      $0.10 to translate a sentence 
      $0.25 for post-editing 10 sentences 
      $0.06 to rank 4 translation groups 

zaidan, o. f. and callison-burch, c. (2011): id104 translation: professional quality from non-professionals. in proceedings of the 49th annual 
meeting of the association for computational linguistics: human language technologies - volume 1 (hlt '11), vol. 1. pp. 1220-1229, portland, or, usa 

27 

translation interface on mturk 
(slide by chris callison-burch     task: translation into english) 

     d 

28 

translation verification interface on mturk 
(slide by chris callison-burch     task: translation into english) 

     s 

29 

quality control model for translation 
(slide by chris callison-burch     task: translation into english) 

     sentence features  
      language model id203 
      ratio of source / target sentence lengths  
      web id165 match percentage  
      translation edit rate to other translators 
     worker features 
      aggregate of sentence feature scores  
      self-reported language abilities (is native speaker? how long speaking?)  
      worker location (pakistan? india?) 
     ranking features (based on second pass vote)  
     calibration feature (id7 against professionals) 

30 

id7 translation evaluation metric 

reference (human) translation:   

the u.s. island of guam is 
maintaining a high state of alert 
after the guam airport and its 
offices both received an e-mail 
from someone calling himself the 
saudi arabian osama bin laden 
and threatening a biological/
chemical attack against public 
places such as the airport . 

machine translation:   

the american [?] international 
airport and its the office all 
receives one calls self the sand 
arab rich business [?] and so on 
electronic mail , which sends out ;  
the threat will be able after public 
place and so on the airport to start 
the biochemistry attack , [?] highly 
alerts after the maintenance. 

id74 formula  
    (counts id165s up to length 4) 
 
exp (1.0 * log p1 + 
        0.5 * log p2 + 
        0.25 * log p3 + 
        0.125 * log p4      
        max(words-in-reference /  
                words-in-machine     1, 0) ) 
 
p1 = 1-gram precision 
p2 = 2-gram precision 
p3 = 3-gram precision 
p4 = 4-gram precision 

31 

crowds approaching professional quality 
(slide by chris callison-burch     task: translation into english) 

     s 

32 

crowds not approaching professional   s costs 
(slide by chris callison-burch     task: translation into english) 

     d 

33 

translator availability on mturk 
(slide by chris callison-burch     task: translation into english) 

     s 

34 

translation speed of mturk for different languages 
(slide by chris callison-burch     task: translation into english) 

     d 

35 

duolingo commercial model 

     incentive for crowdworkers: learn a language!  
     founder: luis von ahn, see also: esp game, recaptcha 
     language learners translate sentences according to their level. 
     more advanced learners correct these.  
     also: collection of speech corpora 
     translations are aggregated and sold as a service 

http://www.slideshare.net/katfish2008/duolingo-powerpoint?qid=4c2767b8-9f8a-4381-98e8-2251eb364560&v=&b=&from_search=1 

36 

crowdsourced translation for  
emergency response 
     2010 haiti earthquake 
     text messaging is the only 
popular and working communication channel 
     aid personnel does not speak creole 
        mission 4636    launched in under 2 days, 
both volunteer and paid crowdwork 

http://www.slideshare.net/wwrob/realtime-crowdsourced-translation-for-emergency-response-
37 
and-beyond?qid=7d7e3002-ebe4-4afb-9872-be1b0c45edd7&v=&b=&from_search=1 

in a nutshell: learned in lesson 4 

     many sample projects for nlp tasks 
     introduction to many nlp problems 
     different quality control mechanisms in practice 

38 

