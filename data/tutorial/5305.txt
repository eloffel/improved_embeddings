introductory course at 
esslli 
bolzano, italia  
august 2016  

id104  

linguistic datasets 

lecture 5  

chris biemann 

biem@cs.tu-darmstadt.de 

lesson 5: aggregation strategies and ethical 
considerations 

     beyond majority vote: using knowledge about crowdworkers and tasks to 
improve the aggregation 
     gold mine or coal mine: working conditions and ethical considerations 
     voices of the crowds 

2 

aggregation of multiple signals 
(materials taken from alessandro bozzon / luca galli)  

     majority voting (mv) is a simple mechanism that does not take many 
factors into account 
     now: what signals can we use to improve mv? 

what is truth?  
     objective truth: exists freely and independently from subjects. e.g. 
number of birds in a picture 
     cultural truth: shared belief of a group of people, often biased by their 
cultural background. e.g. how negative is    garbage    on a scale from 1     
5?  

natural language is subjective and cultural by definition, thus we should not 
expect to have an objective truth available for most non-trivial tasks. 

3 

latent class model 
(materials taken from alessandro bozzon / luca galli)   

     observed: hit outputs 
     latent: truth, user experience, task difficulty, worker quality 
     desired:    solution    that matches the truth 

     often, the matrix is incomplete 
     ground truth might never be known     see also http://crowdtruth.org/
framework 

4 

majority vote as latent model 
(materials taken from alessandro bozzon / luca galli)  

     mv: ask multiple workers, keep majority as the 
   true    label 
     assumptions: 
      the output of each worker depends on the true 
answer 
      outputs are independent of the worker 
      outputs are independent of the (relative) difficulty 
within the task 

     quality: id203 of obtaining the correct label 
directly depends on id203 of correct 
observation  

5 

majority vote for different levels of single worker quality 
(materials taken from alessandro bozzon / luca galli)  

     mv needs a minimum quality of 50%, otherwise it is harmful!  

6 

hidden factors 
(materials taken from alessandro bozzon / luca galli)  
     majority vote works best when workers have a similar quality 
     workers could still randomly guess and agree by chance 
     early experiments by amazon to reject workers that deviated too much 
from the average (e.g. 5-point relevance scale) were discontinued 

     worker characteristics 
      expertise (e.g. spelling skills) 
      bias (cultural, age, education) 
      physical condition (e.g. fatigue)  
     task characteristics 
      quality variance  
(e.g. longer/shorter text, grammar 
 mistakes) 
      difficulty (e.g. transcription of  
native vs. non-native speech) 

7 

incorporating worker quality 
(materials taken from alessandro bozzon / luca galli)  

     example: medical diagnosis by 
doctors 

     model: doctors have different 
rates and types of errors 
        jl
(k) defines the id203 of 
doctor k to declare a patient in 
state l when the true state is j 
        il
(k) is the number of times the 
doctor k gets response l from 
patient i 

8 

incorporating worker quality 
(materials taken from alessandro bozzon / luca galli)  

     solving the equations: expectation maximization (em) algorithm 

     estimate the confusion matrix and the true state simultaneously using 
em, which 
      estimates the true states of each object by weighting the label votes 
according to their current estimates of label quality (from confusion matrix) 
      re-estimate the confusion matrices based on the current beliefs about the 
true states of each object  

dawid, a.p. and skene, a.m. (1979): id113 of observer error-rates using the em algorithm. applied statistics, 28(1):20   28 
dempster, a. p.; laird, n.; rubin, d. b. (1977): maximum likelihood from incomplete data via the em algorithm, journal of the royal statistical society, 
series b 39 (1): 1   38 

9 

incorporating task difficulty 
(materials taken from alessandro bozzon / luca galli)  

     example: label images containing 
at least one    duck    
     competency varies with image 
(blurry, b/w, background features) 

10 

comparing crowd consensus methods 

     offline consensus: given multiple noisy labels per example, how do we 
infer the best consensus label 
     comparative evaluation on 5 nlp datasets and 4 non-nlp datasets 
     all methods rely on  
em in one way or  
another 
     mean worker  
accuracy is varied  
from 0.55 to 0.95 

sheshadri, a. and lease, m. (2013): square: a benchmark for research on computing crowd consensus. in proceedings of the 1st aaai 
conference on human computation (hcomp), pp. 156-164, palm springs, ca, usa  

11 

comparing crowd consensus methods ii 

     mv was often outperformed by some other method.  
     classic ds (em) and version that adds priors fares remarkably well 
     each method was seen to outperform every other method in some 
condition  

12 

pick-a crowd: select set of workers based on their 
preferences 

     social media profiles 
from workers are 
correlated to quality 

difallah, d.e., demartini, g., cudr  -mauroux, p. (2013): pick-a-crowd: tell me 
what you like, and i'll tell you what to do. in proceedings of the 22nd 
international conference on world wide web (www '13). acm, new york, 
ny, usa, 367-374 

13 

summary on modeling agreement beyond mv 

      takeaways:  
      other signals can be employed that tell us about the worker   s quality 
      methods based on expectation maximization improve label quality 
      main idea: high-quality workers get a larger vote (implemented in crowdflower   s 
auto-aggregate) 
      caveat: only pays off for high volume tasks / requesters, as all these parameters 
have to be estimated reliably 

some more references: 
      demartini, g., difallah, d. e., cudr  -mauroux, p. (2012): zencrowd: leveraging probabilistic reasoning and id104 

techniques for large-scale entity linking. in www, pages 469-478, new york, ny, usa 

      hovy, d., berg-kirkpatrick, t., vaswani, a., hovy, e. (2013): learning whom to trust with mace. in: proceedings of naacl-

     

hlt 2013, pages 1120   1130, atlanta, ga, usa 
inel, o., aroyo, l., welty, c., sips, r.-j. (2013): domain-independent quality measures for crowd truth disagreement. 
detection, representation, and exploitation of events in the semantic web 2 (derive 2013), sydney, australia 

      wang, j., ipeirotis, p.g., provost, f. (2011): managing id104 workers.  winter conference on business intelligence, 

2011  

      jagabathula, s., subramanian, l., venkataraman, a. (2014): reputation-based worker filtering in id104. in 

proceedings of the 27th international conference on neural information processing systems (nips'14). mit press, cambridge, 
ma, usa, 2492-2500.  

      zhuang, h., parameswaran, a., roth, d., han, j. (2015): debiasing crowdsourced batches. in proceedings of the 21th acm 

sigkdd international conference on knowledge discovery and data mining (kdd '15). acm, new york, ny, usa, 1593-1602.  

 

14 

let   s crowdsource!  
id52 with upos 

     read instructions on the right, then: 
     you have 5 seconds per word 

15 

 
 
!
e
c
r
u
o
s
d
w
o
r
c
 
s
   
t
e
l

 
 
y
c
n
e
d
n
e
p
e
d

 

g
n
i
s
r
a
p

     do this as quickly as possible!  

16 

how much should parsing short sentences pay?  

 
 
? 

17 

crowdworking on assembly lines 

in 1911, the american engineer frederick taylor delivered 
a paper in which he announced that workers    natural 
laziness and propensity for underworking was    the 
greatest evil which the working-people of both england 
and america are now afflicted.    his solution was a system 
of    scientific management,    wherein work would be 
divided into the smallest repeatable tasks and 
assigned a time limit. the aggregate of these tasks would 
then become the baseline for the workday, and    those 
who fail to rise to a certain standard are discharged 
and a fresh supply of carefully selected men are given 
work in their place.    
almost a century later, amazon hit upon a similar 
approach to worker productivity. yet, whereas taylor   s 
genius was in super-charging the assembly line by 
reducing all skilled work to tiny micro-tasks, the genius of 
mechanical turk is in creating virtual assembly lines.   

https://www.thenation.com/article/how-crowdworkers-became-ghosts-digital-machine/ 

18 

moshe z. marvit 

home stories 2014 

     over the last nine years, milland has completed more than 830,000 tasks 
on mechanical turk, earning an average of 20 cents for each.    i hope our 
letters make jeff bezos realise that there are living, breathing human 
beings who rely on this service he provides to feed and shelter 
themselves and their families,    she says.    i am a human being, not an 
algorithm, and yet [employers] seem to think i am there just to serve 
their bidding,    writes milland in her letter to bezos. amazon does not set 
minimum rates for work, which can pay less than $2 a hour, and takes a 
10% commission from every transaction. employers can even refuse to 
pay for work altogether, with no legal consequences. 
        in the last four years, i have had so many problems getting my cheques 
that i have considered just quitting,    writes manish, a turker from india. 
   there have been numerous cases of workers losing their accounts 
without any fault of their own. workers are left with little to no 
support from amazon.    

https://www.theguardian.com/technology/2014/dec/03/amazon-mechanical-turk-workers-protest-jeff-bezos 

19 

virtual online sweatshops 

     costello describes full-time turking as    feast or famine,    but years of 
turking have diminished her view of the feast. in february 2013, she 
worked approximately sixty hours a week searching for and performing 
hits and made approximately $150 per week   and that was the feast. 
the next month, she was unable to find as many    good-paying    hits and 
earned only about $50 per week. she describes how she often stays up 
all night with the mechanical turk screen open, because when people 
post a good batch of hits, they go quickly. 
     as moshe marvit pointed out in    the nation   , cloud labor can spawn a 
new breed of sweatshop. the casualization of labor, in which there   s no 
longer such a thing as a steady job, much less a career, feeds neoliberal 
capitalist growth by allowing corporations to skirt regulations that ensure 
fair pay, guard against discrimination and provide workers with benefits 
and income supports if the company decides to shift to, say, a cheaper 
labor market abroad.  
https://www.thenation.com/article/id104-bad-workers/ 
https://www.thenation.com/article/how-crowdworkers-became-ghosts-digital-machine/ 

20 

commonly voiced issues by crowdworkers 

     unfair rejection 
     slow payment 
     low pay  
     lack of communication 
     threat of suspension 
     requester scams 
     badly designed tasks 
     information asymmetry 
     lack/imbalance of power 
     lack of search tools/user configuration  

21 

qualitative study of turker nation 

     turking is working     primarily motivated by earning money 
     considerable variation in earnings but it is low wage work 
      highest earners $15-16k per year (~ equivalent to 40 hours/per week, us 
minimum wage - $7.25per hour)     peak at $30,000 

     workers generally aspire to earning $7-10 per hour 
      newbies do lower paid easy work to increase their reputation and ranking 
      lower wages off-set against search time, amount of concentration required etc. 
     turkers have preferences and skills 
      e.g. high volume grinding, writing, professional tasks, some multi-skilled 
     amt as a compromise     problems accessing the regular job market or 
need to supplement income. 
      some housebound, others are in difficult circumstances 

 

fort, k., gilles a. and k. bretonnel cohen.    amazon mechanical turk: gold mine or coal mine?.    computational linguistics 37 (2011): 413-420. 

22 

qualitative study of indian turkers 

     family and community collaboration 
      word of mouth, facebook groups etc. 
      sharing accounts, market in trading accounts, training, cs companies 
      minimum english and some keyboard skills required 
      lower skilled do simple and intuitive tasks; higher skilled can earn a good wage 
by indian levels 
      danger of misunderstandings 
     infrastructure challenges, bricolage and back-ups 
     flexibility and turk-life-balance 
      organise life around turking and are often helped by family 
     accounts/blocking/suspension, getting paid:  many of the participants no 
longer have accounts 
     cultural questions 
      some operate on a basis of accepted = allowed 

23 

otey vs. crowdflower class-action lawsuit 

     crowdflower settled a class action over minimum wage, overtime, and 
other fair labor standards act violations, paying out $585,000 to us 
workers (oct 2014) . 
     crux of this dispute: whether the workers are actually employees entitled 
to minimum wage or independent contractors. 
     line of argumentation: crowdflower tasks systematically undercut the 
minimum wage in the us 
     notable: 
      crowdwork relevant for this settlement was all done on amazon   s mechanical 
turk under the requester name    crowdflower    
      pricing is usually set by crowdflower   s customers 
      it is still possible to undercut the minimum wage on both platforms 

christopher otey, et al. v. crowdflower, inc., et al., no. 12-5524, n.d. calif.; 2015 u.s. dist. lexis 86712), http://wtf.tw/ref/otey.pdf 
http://www.nelp.org/content/uploads/rights-on-demand-report.pdf 

24 

crowdworker forums and online resources 

     crowdworkers are people. they talk about their work. 
     some forums to watch: 
      http://turkernation.com/ turker nation 
      http://www.turkalert.com/ turk alert, also see the blog 
      http://www.mturkforum.com/ mturk forum 
      https://turkopticon.ucsd.edu/  turkopticon 
      https://www.reddit.com/r/mturk  reddit thread 
     advertise your tasks 
     get feedback about your tasks 
     notice issues early     don   t get yourself into the hall of shame!  

25 

turkopticon 
   turkers strike back!     

default worker view on mturk 

turkopticon extensions 

26 

turkopticon        turkers strike back    

turkopticon plugin offers ratings on requesters, by fellow turkers, along 
these dimensions:  
     communicativity: how responsive is the requester, how does the 
requester handle raised issues?  
     generosity: how well are the hits of this requester paid? 
     fairness: how transparent is the rejection behavior of this requester? 
     promptness: how quickly are the hits approved/paid after their 
completion? 

also: comments about requesters.  

default worker view on mturk 

irani, l.c., silberman, m.s. (2013): turkopticon: 
interrupting worker invisibility in amazon mechanical 
turk. chi 2013: changing perspectives, paris, france 

27 

     d 

28 

slide stolen from chris callison-burch 

     d 

slide stolen from chris callison-burch 

29 

quotes from the requester hall of fame/shame 

a big thumbs up to this hit!! it was fun and 
actually rewarding. i wish they would 
contact me with further studies they need 
help with. 

these guys are dishonest, thieving 
cowards who have no respect what so ever 
for turkers, do yourself a favor and avoid 
them at all costs. your better off 
doing xxx hits all day, at least you'll get 
paid. 
 
xxx inc. == bad people with no 
respect for you  

i loved doing hits for this 
requester until i got a bunch of 
rejections in a row. about 3 of 
the rejections were legit the rest 
made no sense to me. the 
rejection claimed i got a 0 on the 
test questions, ... 

30 

a final $1 mturk experiment just for this tutorial 

     s 

31 

what crowdworkers would like to tell you 

     take the task seriously and workers will do the same. when i come 
across obvious typos--when it's clear the requestor didn't put in effort to 
check their own work--i do not take the work seriously either. it's 
disrespectful to the workers and the task at hand when the survey itself is 
of low quality. 
     pay a fair wage for the work that you are requiring someone to do. at 
bare minimum, a pay rate to equal to the minimum wage standard. be 
available to communicate with the turk workers. most of us want to do 
good work for you and sometimes we need a little extra guidance so 
we ask questions. responding to us promptly will help ensure we 
give you what you are looking for which makes all of us happy. 
     communicate and clarify. the best way to get usable results is to 
provide clear instructions and respond when a worker asks for 
clarification. value all feedback and remember that there are no stupid 
questions; what might seem clear to you isn't necessarily so for 
everyone. 

32 

what crowdworkers would like to tell you ii 

     honestly, i think the most important thing for requesters to remember is that 
crowdworkers are also people who are just trying to make some money by 
working like anyone else, except for a myriad of reasons we do it this way 
instead of going to an office. those of us who have been doing this long 
enough understand the problems requesters face: the relatively small 
number of scamming workers that cause a tremendous amount of problems, 
etc. i think the good requesters are the ones who understand that workers 
also deal on a regular basis with requesters who use our work without paying 
us, who believe we should be grateful to spend our time working for less than 
half of minimum wage, or who generally insult and belittle us if contacted 
(often assuming we are all 18-year-old college students with all the bias that 
entails). pay us as fairly as possible within your budget, treat us with 
respect when we contact you respectfully, and try to make your 
instructions as clear and coherent as possible to help us understand 
what you're looking for. the vast majority of us really do want this to be a 
mutually beneficial relationship, as do the majority of requesters. 

33 

what crowdworkers would like to tell you iii 

     the most important thing a requester can do it to pay a fair amount of 
money for each hit. this should be at least 20 cents per minute 
     the most important thing for requesters to know is that crowdworkers 
are actual people and as such should be treated with respect and fair 
pay. remember that the crowdworkers choose to do your work, and they 
can just as easily choose not to. many crowdworkers are highly educated 
individuals who for one reason or another may not be employed in the 
traditional workplace. quite a few are actually employed and do 
crowdwork for extra income. few crowdworkers are doing crowdwork just 
for amusement or to kill time while waiting to meet with the prime 
minister. one final thing, which probably should have been first - always 
give clear instructions, and have a system in place for clarifications 
when needed, and they will be needed! 

34 

what crowdworkers would like to tell you vi 

     you should treat me as someone who deserves respect. i think you 
should also treat me nicely. 
     in my experience, workers are looking for a few key things in terms of 
id104 work. first, workers want and need clearly written, 
precise instructions that are not verbose. second, the pay should be 
at least ten cents per minute. lastly, workers are typically working for 
extra money, and want a quick approval process. in the unlikely event, 
work is rejected for some reason, there should be a clear reason why 
the work is rejected. the vast majority of the workers do high quality 
work. thank you for the opportunity to participate, it is greatly 
appreciated.  

35 

other tutorials and materials (incomplete) 

     2016 upenn class on    id104 and human computation   , chris 
callison-burch, see http://id104-class.org/ 
     2016 icwsm tutorial    how to use id104 effectively for social 
media & web science research    by ujwal gadiraju, gianluca demartni, 
djellel eddine difallah, michele catasta  
     2015 naacl-hlt tutorial    id104 for nlp    by chris callison-
burch,  lyle ungar and ellie pavlick  
     2013 icwe tutorial    an introduction to human computation and games 
with a purpose"" by alessandro bozzon and luca galli 
     2011 wsdm tutorial    id104 101: putting the wsdm of crowds 
to work for you    by omar alonso and matthew lease 
     2010 naacl-hlt workshop    creating speech and language data with  
amazon   s mechanical turk    by chris callison-burch and mark dredze 

36 

in a nutshell: learned in lesson 5 

statistical quality control 
     methods to improve over majority voting 
     approaches based on expectation maximization work well 
     no    best method   , still ongoing research 

ethics of id104  
     id104 platforms can be viewed as sweatshops 
     after initial euphoria, crowdworkers demand rights 
     there are tools that support the choice of genuine/trustworthy 
requesters 
     build your reputation as a requester 
     don   t underpay, don   t reject deliberately 
     check the forums! 

37 

in a nutshell: lessons learned overall   

     keep hits extremely simple 
     provide redundant instructions 
     crowdworkers are mostly not linguists 
     never trust a single worker. trust the crowd. 
     watch out for scammers:  
      use qualification requirements 
      use test items 
     iterate on small batches, watch closely 
     watch the forums 
     always give crowdworkers an exit and a room for comments 
     be a crowdworker yourself for a while     you will notice that they are 
human beings, and you will treat them as such. 

38 

the end 

the crowds are waiting for you!  

http://www.greenbookblog.org/wp-content/uploads/2013/11/crowd.jpg 

39 

