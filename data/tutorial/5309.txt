computational 
semantics: day 3 

johan bos 
university of groningen 
www.rug.nl/staff/johan.bos 

computational semantics 

       day 1: exploring models 
       day 2: meaning representations 
       day 3: computing meanings 
       day 4: drawing id136s 
       day 5: meaning banking 

questions after yesterday   s lecture 
       quantifier scope 
       the i function 

questions: quantifier scope 

       is there a difference between    
   x   y love(x,y) and    y   x love(x,y)  ? 

the satisfaction definition 

questions: ig
f 

f         (can   t even typeset it properly in ppt) 

       the horrible ig
       this is a function from terms to entities in the domain 
       recall that terms can be variables or contants 
       so basically this function catches two birds with one 
stone:   

suppose t is a term.    
if t is a variable, then    
    we use the assignment function g: i(t)=g(t)   
if t is a constant, then   
    we use the interpretation function f: i(t)=f(t)   

   
semantic analysis pipeline 

segmentation 

tokenised text 

pos-tagging 

parts of speech 

morphological parsing 

syntactic parsing 

syntactic structure 

id29 

semantic representation 

id136 

natural language descriptions 
  true descriptions 
       a white rabbit is eating a carrot. 
       a rabbit with a carrot. 
       a rabbit is nibbling on a carrot. 
       a rabbit holding a carrot in its mouth. 
       a carrot is being eaten by a rabbit. 
 
  false descriptions 
       a rabbit without a carrot. 
       a brown rabbit is eating an orange carrot. 
       two rabbits are sharing a carrot. 
       a carrot is holding a white rabbit. 
       a rabbit with orange flowers. 

natural language descriptions 
  true descriptions 
       ................ 
       ................ 

  false descriptions 
       ................. 
       ................. 

natural language descriptions 
  true descriptions 
       ................ 
       ................ 

  false descriptions 
       ................. 
       ................. 

description guidelines 
       try to include at least two entities in your description 
       only describe the situation, not what is around it 
i.e., not    a girl is looking into a camera    
       don   t use relative positional information 
i.e., not    a cat is standing left of a dog    
 

goal 
       build first-order meaning representations from natural 
language descriptions, using the vocabulary of non-logical 
symbols used in the models 
       we assume that we need syntax to give structure to the 
descriptions, providing us means for a compositional way 
of constructing meaning representation 

goal 
       build first-order meaning representations from natural 
language descriptions, using the vocabulary of non-logical 
symbols used in the models 
       we assume that we need syntax to give structure to the 
descriptions, providing us means for a compositional way 
of constructing meaning representation 
       note:  
recent attempts with neural networks skip syntactic 
analysis entirely! 

goal 
       build first-order meaning representations from natural 
language descriptions, using the vocabulary of non-logical 
symbols used in the models 
       we assume that we need syntax to give structure to the 
descriptions, providing us means for a compositional way 
of constructing meaning representation 
       we will have a closer look at two grammar formalisms: 

       phrase structure grammar (dcg) 
       id35 (id35)  tomorrow 

ordinary clauses in prolog! 
terminals are in square brackets. 
left-recursive rules not allowed. 

definite clause grammars (prolog) 
s	
   -     -     >	
   np,	
   vp.	
   
np	
   -     -     >	
   det,	
   n.	
   
vp	
   -     -     >	
   tv,	
   np.	
   
vp	
   -     -     >	
   iv.	
   
vp	
   -     -     >	
   av,	
   vp.	
   
	
   
det	
   -     -     >	
   [a].	
   det	
   -     -     >	
   [the].	
   det	
   -     -     >	
   [every].	
   
np	
   -     -     >	
   [someone].	
   np	
   -     -     >	
   [somebody].	
   
av	
   -     -     >	
   [is].	
   av	
   -     -     >	
   [are].	
   
n	
   -     -     >	
   [cat].	
   n	
   -     -     >	
   [dog].	
   
tv	
   -     -     >	
   [eats].	
   tv	
   -     -     >	
   [eating].	
   

adding constraints 
       aspectual features (vp):  

       prp (present participle) 
       pap (past participle) 
       inf (infinitival) 
       pss (passive) 

       mood features (s):  

       dcl (declarative) 
       int (interrogative)  

       agreement features (np): 

       sg (singular) 
       pl (plural) 

definite clause grammars with features 

here we use lists to be able to add 
more features. order is important! 

s	
   -     -     >	
   np,	
   vp.	
   
np	
   -     -     >	
   det,	
   n.	
   
vp([f])	
   -     -     >	
   tv([f]),	
   np.	
   
vp([f])	
   -     -     >	
   iv([f]).	
   
vp([m])	
   -     -     >	
   av([m,a]),	
   vp([a]).	
   
	
   
det	
   -     -     >	
   [a].	
   det	
   -     -     >	
   [the].	
   det	
   -     -     >	
   [every].	
   
np	
   -     -     >	
   [someone].	
   np	
   -     -     >	
   [somebody].	
   
av([dcl,prp])	
   -     -     >	
   [is].	
   av([dcl,prp])	
   -     -     >	
   [are].	
   
n	
   -     -     >	
   [cat].	
   n	
   -     -     >	
   [dog].	
   
tv([dcl])	
   -     -     >	
   [eats].	
   tv([prp])	
   -     -     >	
   [eating].	
   

eliminating left-recursive rules 
       dcg can   t handle left-recursive grammars  
(because of prolog   s top-down search strategy it risks  
 to go in an infinite loop) 
       the simple cases of left recursion (direct left recursion) 
can be eliminated from a dcg 
       these cases are of the form (x is a non-terminal, y and z 
are terminal or non-terminal categories): 
 
 
  x	
   -     -     >	
   x,	
   y.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x	
   -     -     >	
   z,	
   x   .	
   
 
	
   x	
   -     -     >	
   z.	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x   	
   -     -     >	
   [].	
   
 
       	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   x   	
   -     -     >	
   y,	
   x   .	
   
 
left-recursive dcg schema 

 
 
 
 
 
left-recursion eliminated by 
introducing new category and 
empty production 

	
   

example: italian 

np	
   -     -     >	
   det,	
   n.	
   
n	
   -     -     >	
   n,	
   adj.	
   
n	
   -     -     >	
   adj,	
   n.	
   
	
   
det	
   -     -     >	
   [una].	
   	
   	
   det	
   -     -     >	
   [la].	
   
n	
   -     -     >	
   [casa].	
   	
   	
   	
   n	
   -     -     >	
   [cosa].	
   
adj	
   -     -     >	
   [bella].	
   adj	
   -     -     >	
   [nuova].	
   

provide dcg analyses 
  true descriptions 
       a white rabbit is eating a carrot. 
       a rabbit with a carrot. 
       a rabbit is nibbling on a carrot. 
       a rabbit holding a carrot in its mouth. 
       a carrot is being eaten by a rabbit. 
 
  false descriptions 
       a rabbit without a carrot. 
       a brown rabbit is eating an orange carrot. 
       two rabbits are sharing a carrot. 
       a carrot is holding a white rabbit. 
       a rabbit with orange flowers. 

not sure it is 

a parser i 

want to use 

you get a  
parser  
for free    

with  

prolog! 

non-logical symbols 

       concepts (id138) 
       relations (spatial relations only)   

part of       ->  s_part_of   
touch         ->  s_touch   
near           ->  s_near   
support      ->  s_support   
 
       id136s 

       support implies touch 
       near implies not touch and not part of 
       touch  implies not part of 

   
the big question 

       how can we associate a natural language description 
like    every cat is drinking milk    with its first-order 
translation:    
      x[n_cat_1(x)          y [n_milk_1(y) & s_near(x,y)]]? 

       moreover: how can we do this in a systematic way?   
we want to make our method scalable to other kinds of 
natural language expressions, including those that we 
have never seen before! 

another example 

someone is holding a melon.   

   x [n_person_1(x) &    
         y [n_melon_2(y) &    
               z [n_hand_1(z) &    
                  s_part_of(z,x) &    
                  s_supports(z,y)]]] 

 

   
   
   
next 

       we will have a look at dcg the again 
       but now we will specify the lexical semantics 
       and we show how composition works 
       but first, more about compositionality 

compositionality 
       we assume that the meaning representation of a 
sentence is composed out of the (partial) meaning 
representations of its parts (i.e., the words) 
       this principle is known as compositionality, often 
misattributed to frege [janssen 2012] 

frege 

compositionality 
       we assume that the meaning representation of a 
sentence is composed out of the (partial) meaning 
representations of its parts (i.e., the words) 
       this principle is known as compositionality, often 
misattributed to frege [janssen 2012] 

frege 

carnap 

compositionality 

       generally speaking, the motivation for compositionality 
is not for principled, but for practical reasons 
       this follows an old wisdom, often attributed to julius 
caesar, but probably from philippus of macedonia 
(father of alexander the great): compositionality 
implements the rule divide et impera [janssen 2012] 

caesar 

philippus 

decomposing 

   every cat is drinking milk           
   x[cat(x)          y [milk(y) & near(x,y)]] 

   every        ... 
   cat        ... 
   is        ... 
   drinking        ... 
   milk        ... 

decomposing 

   every cat is drinking milk           
   x[cat(x)          y [milk(y) & near(x,y)]] 

   every        ... 
   cat        cat(x) 
   is        ... 
   drinking        near(x,y) 
   milk        milk(y) 

decomposing 

   every cat is drinking milk           
   x[cat(x)          y [milk(y) & near(x,y)]] 

   every           x [ ...(x)        ...(x) ]  
   cat        cat(x) 
   is        ... 
   drinking        near(x,y) 
   milk        milk(y) 

decomposing 

   every cat is drinking milk           
   x[cat(x)          y [milk(y) & near(x,y)]] 

   every           x [ ...(x)        ...(x) ]  
   cat        cat(x) 
   is        nothing? 
   drinking        near(x,y) 
   milk        milk(y) 

decomposing 

   every cat is drinking milk           
   x[cat(x)          y [milk(y) & near(x,y)]] 

   every           x [ ...(x)        ...(x) ]  
   cat        cat(x) 
   is        nothing? 
   drinking        near(x,y) 
   milk        milk(y) 

decomposing 

   every cat is drinking milk           
   x[cat(x)          y [milk(y) & near(x,y)]] 

   every           x [ ...(x)        ...(x) ]  
   cat        cat(x) 
   is        nothing? 
   drinking        near(x,y) 
   milk           y [milk(y) & ...(y) ] 

what do we observe? 

       open spaces for formulas (the ...),   
sometimes more than one! 
       variables need to be correctly bound,   
sometimes more than one! 
       some lexical items seem to have no    
   semantic contribution    

partial formulas 
we will add a couple of new operators to describe partial 
formulas:  
 
                  @ 
 

       the lambda operator    signals missing information 

the lambda binds variables (like the quantifiers) and is placed in 
front of a formula (like the quantifiers) 

       the application operator @ indicates that two pieces of information 

need to be combined 

adding lambdas and applications 

   every cat is drinking milk           
   x[cat(x)          y [milk(y) & near(x,y)]] 

   every              p  q   x [ (p@x)       (q@x) ]  
   cat                  x cat(x) 
   is                    f f 
   drinking          y   x near(x,y) 
   milk                p   y [milk(y) & (p@y) ] 

higher order logic 

       lambda-bound variables can also range over non-
entities (i.e. properties and formulas) 
       this means that we have left the (relatively safe) domain 
of id85 
       we will use the lambdas purely as a device to construct 
formulas from smaller parts 
       it will provide us a way to control free and bound 
variables 

with a little help of syntactic structure  

       syntax (dcg, id35, or something else) helps us to    
find out what combines with what 
       consider the following (simplified) dcg   

s       np vp                                     det       [every]   
np       det n                                    n       [cat]   
np       n                                          n       [milk]   
vp       tv np                                     av       [is]   
vp       av vp                                    tv       [drinking]   
 
       next step: add semantics 

   
the semantics in the lexicon 

det [sem:   p  q   x[(p@x)       (q@x)]]       [every]   
n [sem:   x cat(x)]      [cat]   
n [sem:   x milk(x)]      [milk]   
av [sem:   f f]      [is]   
tv [sem:   x  y near(x,y)]      [drinking]   
 
 

the semantics in the rules 

s[sem: (x@y)]       np[sem:x] vp[sem:y]   
np[sem: (x@y)]      det[sem:x] n[sem:y]   
np [sem:    x(y@x)]      n[sem:y]   
vp [sem:   x(y@(x@x))]      tv[sem:x] np[sem:y]   
vp [sem: (x@y)]      av[sem:x] vp[sem:y]   
 
 

one picture says more than a thousand 
words variables 

butch on his chopper 

np       det: n 

np 

det 

   every    

n 

   cat    

np:[  @  ]       det:   n:   

[  p  q   x[(p@x)       (q@x)] @   y cat(y)] 
np: 
 

det: 

  p  q   x[(p@x)       (q@x)] 

n: 

  y cat(y) 

   every    

   cat    

  -conversion 
       consider the application: (  x  @  ) 

       here the functor is:        x   
       and the argument is:      

       the process of  replacing every free occurrence of x in    
by    is called  
           -conversion  
         (or   -reduction, or   -conversion)   

 

np:[  @  ]       det:   n:   

  q   x[(  y cat(y)@x)       (q@x)] 
np: 
 

det: 

  p  q   x[(p@x)       (q@x)] 

n: 

  y cat(y) 

   every    

   cat    

np:[  @  ]       det:   n:   

  q   x[cat(x)       (q@x)] 
np: 
 

det: 

  p  q   x[(p@x)       (q@x)] 

n: 

  y cat(y) 

   every    

   cat    

demo 
       ~/doc/tea/computationalsemantics % cat esslligrammar.pl 
       ~/doc/tea/computationalsemantics % cat lexicon.pl 
       ~/doc/tea/computationalsemantics % cat semdcg.pl 

       [semdcg], s(sem,[a,man,rides,a,bicycle],[]). 

exercise 1 
       not only sentences, also noun phrases. 

exercise 2 
       look at the natural language statements associated with 
the images in grim 
       pick a frequently occurring verb that is not in the lexicon 
already 
       specify the lexical semantics of this verb in 
 
a) no events (pre-davidsonian) 
b) davidsonian 
c) neo-davidsonian 
d) the spatial relations only 

the big picture 

today 

semantic 
semantic 
parsing 
parsing 

natural 
language 
statement 

true 

or 

false 

model 

extraction 

monday 

meaning 

model 

checking 

model 

planet semantics 

representations 

models 

proofs 

planet semantics 

studies relation between  
meanings and situations 

model-theoretic 

semantics 

proof-theoretic 

semantics 

studies relation between  
meanings and meanings 

representation of  

semantics 

studies relation between  
natural language and meanings 

proof-theoretical semantics 

models 

lexical  
semantics 

compositional 

semantics 

discourse 
semantics 

proofs 

inductive id136 

abductive 
id136 

deductive 
id136 

computational semantics 

       day 1: exploring models 
       day 2: meaning representations 
       day 3: computing meanings with dcg 
       day 4: computing meanings with id35 
       day 5: drawing id136s and meaning banking 

