efficient methods and hardware for 

deep learning 

song han

stanford university

may 25, 2017 

intro

song han
phd candidate
stanford

bill dally
chief scientist

nvidia
professor
stanford

2

deep learning is changing our lives

self-driving	car

machine	translation

this image is licensed under cc-by 2.0

this image is in the public domain

this image is in the public domain

this image is licensed under cc-by 2.0

3

alphago

smart	robots

3

models are getting larger

image recognition

id103

16x

model

8 layers
1.4 gflop
~16% error

2012
alexnet

152 layers
22.6 gflop
~3.5%  error

10x

training ops

80 gflop

7,000 hrs of data

~8% error

465 gflop

12,000 hrs of data

~5% error

2015
resnet

2014

deep speech 1

2015

deep speech 2

microsoft

baidu

dally, nips   2016 workshop on efficient methods for deep neural networks

4

the first challenge: model size

hard to distribute large models through over-the-air update

app icon is in the public domain 

phone image is licensed under cc-by 2.0

this image is licensed under cc-by 2.0

5

the second challenge: speed

resnet18: 
resnet50: 
resnet101: 
resnet152:

error rate
10.76% 
7.02%   
6.21% 
6.16%

training time
2.5 days 
5 days 
1 week 
1.5 weeks

such long training time limits ml researcher   s productivity

training time benchmarked with fb.resnet.torch using four m40 gpus

6

the third challenge: energy efficiency

alphago: 1920 cpus and 280 gpus,   
                  $3000 electric bill per game 

this image is in the public domain

this image is in the public domain

on mobile: drains battery
on data-center: increases tco

phone image is licensed under cc-by 2.0

this image is licensed under cc-by 2.0

7

where is the energy consumed?

larger model => more memory reference => more energy

8

where is the energy consumed?

larger model => more memory reference => more energy

relative energy cost 
relative energy cost

relative energy cost 

operation
32 bit int add
32 bit    oat add
32 bit register file
32 bit int mult
32 bit    oat mult
32 bit sram cache
32 bit dram memory

energy [pj] relative cost
operation
0.1
1
32 bit int add
0.9
9
32 bit    oat add
1
10
32 bit register file
3.1
31
32 bit int mult
3.7
37
32 bit    oat mult
32 bit sram cache
50
5
32 bit dram memory
640
6400

energy [pj] relative cost
0.1
0.9
1
3.1
3.7
5
640

1
9
10
31
37
50
6400
1

10

100

1
1000

10
10000

100

1000

10000

figure 1: energy table for 45nm cmos process [7]. memory access is 3 orders of magnitude more
energy expensive than simple arithmetic.

figure 1: energy table for 45nm cmos process [7]. memory access is 3 orders of magnitude more
energy expensive than simple arithmetic.

= 1000

1

this image is in the public domain

to achieve this goal, we present a method to prune network connections in a manner that preserves the
original accuracy. after an initial training phase, we remove all connections whose weight is lower
than a threshold. this pruning converts a dense, fully-connected layer to a sparse layer. this    rst
phase learns the topology of the networks     learning which connections are important and removing

to achieve this goal, we present a method to prune network connections in a manner that preserves the
original accuracy. after an initial training phase, we remove all connections whose weight is lower
than a threshold. this pruning converts a dense, fully-connected layer to a sparse layer. this    rst
phase learns the topology of the networks     learning which connections are important and removing

9

where is the energy consumed?

larger model => more memory reference => more energy

relative energy cost 
relative energy cost

relative energy cost 

operation
32 bit int add
32 bit    oat add
32 bit register file
32 bit int mult
32 bit    oat mult
32 bit sram cache
32 bit dram memory

energy [pj] relative cost
operation
0.1
1
32 bit int add
0.9
9
32 bit    oat add
1
10
32 bit register file
3.1
31
32 bit int mult
3.7
37
32 bit    oat mult
32 bit sram cache
50
5
32 bit dram memory
640
6400

energy [pj] relative cost
0.1
0.9
1
3.1
3.7
5
640

1
9
10
31
37
50
6400
1

figure 1: energy table for 45nm cmos process [7]. memory access is 3 orders of magnitude more
energy expensive than simple arithmetic.

figure 1: energy table for 45nm cmos process [7]. memory access is 3 orders of magnitude more
energy expensive than simple arithmetic.

10
10000
how to make deep learning more efficient?

1
1000

100

10

100

1000

10000

image 1, image 2, image 2, image 4

battery images are in the public domain 

to achieve this goal, we present a method to prune network connections in a manner that preserves the
original accuracy. after an initial training phase, we remove all connections whose weight is lower
than a threshold. this pruning converts a dense, fully-connected layer to a sparse layer. this    rst
phase learns the topology of the networks     learning which connections are important and removing

to achieve this goal, we present a method to prune network connections in a manner that preserves the
original accuracy. after an initial training phase, we remove all connections whose weight is lower
than a threshold. this pruning converts a dense, fully-connected layer to a sparse layer. this    rst
phase learns the topology of the networks     learning which connections are important and removing

10

improve the efficiency of deep learning 

by algorithm-hardware co-design 

11

application as a black box

algorithm

hardware

spec 2006

this image is in 
the public domain

cpu

this image is in 
the public domain

12

open the box before hardware design

algorithm

hardware

?

this image is in 
the public domain

this image is in 
the public domain

?pu

breaks the boundary between algorithm and hardware 

13

agenda

id136

training

agenda

algorithm

id136

training

hardware

agenda

algorithm

id136

training

hardware

agenda

algorithm

algorithms for    
ef   cient id136

algorithms for
ef   cient training

id136

training

hardware for   

ef   cient id136 

hardware for   

ef   cient training 

hardware

hardware 101: the family

hardware

general purpose*

specialized hw

cpu
latency 
 oriented

gpu

fpga

throughput 
 oriented

programmable    

logic

asic
   xed  
logic

* including gpgpu

hardware 101: number representation

s

(-1)  x (1.m) x 2

e

8
e

5
e

10
m

23
m

31
m

15
m

7
m

i

f

radix point

fp32 

fp16 

int32 

int16 

int8

fixed point

1
s

1
s

1
s

1
s

1
s

s

range 

accuracy 

10-38 - 1038 

.000006% 

6x10-5 - 6x104 

.05% 

0     2x109 

0     6x104 

0     127 

   

   

     

-                           -

dally, high performance hardware for machine learning, nips   2015

hardware 101: number representation

operation:
8b add
16b add
32b add
16b fp add
32b fp add
8b mult
32b mult
16b fp mult
32b fp mult
32b sram read (8kb)
32b dram read

energy (pj)

area (  m2)

0.03
0.05
0.1
0.4
0.9
0.2
3.1
1.1
3.7
5
640

36
67
137
1360
4184
282
3495
1640
7700
n/a
n/a

energy numbers are from mark horowitz    computing   s energy problem (and what we can do about it)   , isscc 2014
area numbers are from synthesized result using design compiler under tsmc 45nm tech node. fp units used designware library.

agenda

algorithm

algorithms for    
ef   cient id136

algorithms for
ef   cient training

id136

training

hardware for   

ef   cient id136 

hardware for   

ef   cient training 

hardware

part 1: algorithms for efficient id136

    1. pruning
    2. weight sharing 
    3. quantization 
    4. low rank approximation
    5. binary / ternary net
    6. winograd transformation

part 1: algorithms for efficient id136

    1. pruning
    2. weight sharing 
    3. quantization 
    4. low rank approximation
    5. binary / ternary net
    6. winograd transformation

pruning neural networks

[lecun et al. nips   89]
[han et al. nips   15]

pruning

trained	quantization

huffman	coding

24

pruning neural networks

[han et al. nips   15]

-0.01x +x+1

2

60 million

10x	less	connections

6m

pruning

trained	quantization

huffman	coding

25

pruning neural networks

[han et al. nips   15]

s
s
o
l
 
y
c
a
r
u
c
c
a

0.5%
0.0%
-0.5%
-1.0%
-1.5%
-2.0%
-2.5%
-3.0%
-3.5%
-4.0%
-4.5%

40%

50%

70%

60%
80%
parameters pruned away

90%

100%

pruning

trained	quantization

huffman	coding

26

pruning neural networks

[han et al. nips   15]

pruning

s
s
o
l
 
y
c
a
r
u
c
c
a

0.5%
0.0%
-0.5%
-1.0%
-1.5%
-2.0%
-2.5%
-3.0%
-3.5%
-4.0%
-4.5%

40%

50%

70%

60%
80%
parameters pruned away

90%

100%

pruning

trained	quantization

huffman	coding

27

retrain to recover accuracy

[han et al. nips   15]

pruning

pruning+retraining

s
s
o
l
 
y
c
a
r
u
c
c
a

0.5%
0.0%
-0.5%
-1.0%
-1.5%
-2.0%
-2.5%
-3.0%
-3.5%
-4.0%
-4.5%

40%

50%

70%

60%
80%
parameters pruned away

90%

100%

pruning

trained	quantization

huffman	coding

28

iteratively retrain to recover accuracy

[han et al. nips   15]

pruning

pruning+retraining

iterative pruning and retraining

s
s
o
l
 
y
c
a
r
u
c
c
a

0.5%
0.0%
-0.5%
-1.0%
-1.5%
-2.0%
-2.5%
-3.0%
-3.5%
-4.0%
-4.5%

40%

50%

70%

60%
80%
parameters pruned away

90%

100%

pruning

trained	quantization

huffman	coding

29

pruning id56 and lstm

[han et al. nips   15]

image captioning

explain images with multimodal recurrent neural networks, mao et al.
deep visual-semantic alignments for generating image descriptions, karpathy and fei-fei
show and tell: a neural image caption generator, vinyals et al.
long-term recurrent convolutional networks for visual recognition and description, donahue et al.
learning a recurrent visual representation for image id134, chen and zitnick

*karpathy et al, "deep visual-
semantic alignments for generating 
image descriptions   , 2015.    
lecture 10 -
lecture 10 -

figure copyright ieee, 2015; reproduced for educational purposes. 

fei-fei li & andrej karpathy & justin johnson
fei-fei li & andrej karpathy & justin johnson

51

8 feb 2016
8 feb 2016

pruning

trained	quantization

huffman	coding

30

pruning id56 and lstm

[han et al. nips   15]

    original:  a  basketball  player  in  a  white  uniform  is 

playing with a ball

    pruned 90%: a basketball player in a white uniform is 

playing with a basketball

    original : a brown dog is running through a grassy field
    pruned 90%: a brown dog is running through a grassy 

area

    original : a man is riding a surfboard on a wave
    pruned 90%: a man in a wetsuit is riding a wave on a 

beach

    original : a soccer player in red is running in the field
    pruned 95%: a man in a red shirt and black and white 

black shirt is running through a field

90%

90%

90%

95%

pruning

trained	quantization

huffman	coding

31

pruning happens in human brain

1000 trillion
synapses  

50 trillion
synapses  

500 trillion
synapses  

this image is in the public domain

newborn

this image is in the public domain

1 year old

this image is in the public domain

adolescent 

christopher a walsh. peter huttenlocher (1931-2013). nature, 502(7470):172   172, 2013.    

pruning

trained	quantization

huffman	coding

32

pruning changes weight distribution

[han et al. nips   15]

before pruning

after pruning

after retraining

conv5 layer of alexnet. representative for other network layers as well.

pruning

trained	quantization

huffman	coding

33

part 1: algorithms for efficient id136

    1. pruning
    2. weight sharing 
    3. quantization 
    4. low rank approximation
    5. binary / ternary net
    6. winograd transformation

trained quantization

[han et al. iclr   16]

2.09,  2.12,  1.92,  1.87

2.0

pruning

trained	quantization

huffman	coding

35

pruning: less number of weights

trained quantization
quantization: less bits per weight

[han et al. iclr   16]

huffman encoding

train connectivity

   same 
2.09, 2.12, 1.92, 1.87
accuracy

cluster the weights

generate code book

prune connections

train weights

2.0

   9x-13x 
reduction

quantize the weights 
with code book

retrain code book

   same 
accuracy

  27x-31x 
reduction

encode weights

encode index

figure 1: the three stage compression pipeline: pruning, quantization and huffman coding. pruning
reduces the number of weights by 10   , while quantization further improves the compression rate:
between 27    and 31   . huffman coding gives more compression: between 35    and 49   . the
compression rate already included the meta-data for sparse representation. the compression scheme

8x	less	memory	footprint

trained	quantization

huffman	coding

pruning

4bit

36

32 bit

figure 2: representing the matrix sparsity with relative index. padding    ller zero to prevent over   ow.

trained quantization

[han et al. iclr   16]

figure 3: weight sharing by scalar quantization (top) and centroids    ne-tuning (bottom).

pruning

trained	quantization

huffman	coding

37

2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.49-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:figure 2: representing the matrix sparsity with relative index. padding    ller zero to prevent over   ow.

trained quantization

[han et al. iclr   16]

figure 3: weight sharing by scalar quantization (top) and centroids    ne-tuning (bottom).

pruning

trained	quantization

huffman	coding

38

2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.49-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:figure 2: representing the matrix sparsity with relative index. padding    ller zero to prevent over   ow.

trained quantization

[han et al. iclr   16]

figure 3: weight sharing by scalar quantization (top) and centroids    ne-tuning (bottom).

pruning

trained	quantization

huffman	coding

39

2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.49-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:figure 2: representing the matrix sparsity with relative index. padding    ller zero to prevent over   ow.

trained quantization

[han et al. iclr   16]

figure 3: weight sharing by scalar quantization (top) and centroids    ne-tuning (bottom).

pruning

trained	quantization

huffman	coding

40

2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.49-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:figure 2: representing the matrix sparsity with relative index. padding    ller zero to prevent over   ow.

trained quantization

[han et al. iclr   16]

figure 3: weight sharing by scalar quantization (top) and centroids    ne-tuning (bottom).

pruning

trained	quantization

huffman	coding

41

2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.49-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:figure 2: representing the matrix sparsity with relative index. padding    ller zero to prevent over   ow.

trained quantization

[han et al. iclr   16]

figure 3: weight sharing by scalar quantization (top) and centroids    ne-tuning (bottom).

pruning

trained	quantization

huffman	coding

42

2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.49-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:figure 2: representing the matrix sparsity with relative index. padding    ller zero to prevent over   ow.

trained quantization

[han et al. iclr   16]

figure 3: weight sharing by scalar quantization (top) and centroids    ne-tuning (bottom).

pruning

trained	quantization

huffman	coding

43

2.09-0.981.480.090.05-0.14-1.082.12-0.911.920-1.031.8701.531.49-0.03-0.010.030.02-0.010.01-0.020.12-0.010.020.040.01-0.07-0.020.01-0.020.040.020.04-0.03-0.030.120.02-0.070.030.010.02-0.010.010.04 -0.01-0.02-0.010.01cluster   weights (32 bit float)centroidsgradient3021110303103122cluster index  (2 bit uint)2.001.500.00-1.00-0.02-0.02group byfine-tuned centroidsreduce1.961.48-0.04-0.971:lr0:2:3:before trained quantization:   

continuous weight 

[han et al. iclr   16]

t

n
u
o
c

weight value

pruning

trained	quantization

huffman	coding

44

after trained quantization:   

discrete weight 

[han et al. iclr   16]

t

n
u
o
c

weight value

pruning

trained	quantization

huffman	coding

45

after trained quantization:   
discrete weight after training 

[han et al. iclr   16]

t

n
u
o
c

weight value

pruning

trained	quantization

huffman	coding

46

how many bits do we need?

[han et al. iclr   16]

pruning

trained	quantization

huffman	coding

47

how many bits do we need?

[han et al. iclr   16]

pruning

trained	quantization

huffman	coding

48

pruning + trained quantization work together

[han et al. iclr   16]

pruning

trained	quantization

huffman	coding

49

pruning + trained quantization work together

[han et al. iclr   16]

alexnet on id163

pruning

trained	quantization

huffman	coding

50

huffman coding

[han et al. iclr   16]

huffman encoding

accuracy

  27x-31x 
reduction

encode weights

encode index

   same 
accuracy

  35x-49x 
reduction

figure 1: the three stage compression pipeline: pruning, quantization and huffman coding. pruning
reduces the number of weights by 10   , while quantization further improves the compression rate:
between 27    and 31   . huffman coding gives more compression: between 35    and 49   . the
compression rate already included the meta-data for sparse representation. the compression scheme

    in-frequent weights: use more bits to represent
    frequent weights: use less bits to represent

pruning

trained	quantization

huffman	coding

51

summary of deep compression

published as a conference paper at iclr 2016

[han et al. iclr   16]

pruning: less number of weights

quantization: less bits per weight

huffman encoding

original 
network

original
   size

train connectivity

prune connections

train weights

   same 
accuracy

   9x-13x 
reduction

cluster the weights

generate code book

quantize the weights 
with code book

retrain code book

   same 
accuracy

  27x-31x 
reduction

encode weights

encode index

   same 
accuracy

  35x-49x 
reduction

figure 1: the three stage compression pipeline: pruning, quantization and huffman coding. pruning
reduces the number of weights by 10   , while quantization further improves the compression rate:
between 27    and 31   . huffman coding gives more compression: between 35    and 49   . the
compression rate already included the meta-data for sparse representation. the compression scheme
doesn   t incur any accuracy loss.

trained	quantization

huffman	coding

pruning

52

results: compression ratio

[han et al. iclr   16]

network

lenet-300
lenet-5
alexnet
vggnet
googlenet
resnet-18

original 

size
1070kb
1720kb
240mb
550mb
28mb
44.6mb

compressed 

size
27kb
44kb
6.9mb
11.3mb
2.8mb
4.0mb

compression   

ratio
40x
39x
35x
49x
10x
11x

original 
accuracy
98.36%
99.20%
80.27%
88.68%
88.90%
89.24%

compressed 
accuracy
98.42%
99.26%
80.30%
89.09%
88.92%
89.28%

can we make compact models to begin with?

compression

acceleration

id173

53

squeezenet

input

64
1x1 conv   
squeeze

16

1x1 conv   
expand

3x3 conv   
expand

64
64
output 

concat/eltwise

128

iandola et al,     squeezenet: alexnet-level accuracy with 50x fewer parameters and <0.5mb model size   , arxiv 2016

vanilla fire module

compression

acceleration

id173

54

compressing squeezenet

network

approach

size

ratio

alexnet

alexnet

alexnet

-

svd

deep 

compression

squeezenet

-

squeezenet

deep 

compression

240mb

48mb

6.9mb

4.8mb

1x

5x

35x

50x

top-1 
accuracy

top-5 
accuracy

57.2%

80.3%

56.0%

79.4%

57.2%

80.3%

57.5%

80.3%

0.47mb

510x

57.5%

80.3%

iandola et al,     squeezenet: alexnet-level accuracy with 50x fewer parameters and <0.5mb model size   , arxiv 2016

compression

acceleration

id173

55

results: speedup

average

cpu

gpu

0.6x
mgpu

compression

acceleration

id173

56

results: energy efficiency

average

cpu

gpu

mgpu

compression

acceleration

id173

57

deep compression applied to industry

deep   

compression

f8 

compression

acceleration

id173

58

part 1: algorithms for efficient id136

    1. pruning
    2. weight sharing 
    3. quantization 
    4. low rank approximation
    5. binary / ternary net
    6. winograd transformation

quantizing the weight and activation

    train with float
    quantizing the weight and 
activation:
    gather the statistics for 
weight and activation
    choose proper radix point 
position

    fine-tune in float format
    convert to fixed-point format

qiu et al.  going deeper with embedded fpga platform for convolutional neural network, fpga   16

quantization result

(cid:13)(cid:23)(cid:23)(cid:20)(cid:14)(cid:18)(cid:15)(cid:18)(cid:26)

(cid:16)(cid:13)(cid:13)(cid:1)(cid:4)(cid:9)

(cid:4)
(cid:3)(cid:2)(cid:12)
(cid:3)(cid:2)(cid:11)
(cid:3)(cid:2)(cid:10)
(cid:3)(cid:2)(cid:9)
(cid:3)(cid:2)(cid:8)
(cid:3)(cid:2)(cid:7)
(cid:3)(cid:2)(cid:6)
(cid:3)(cid:2)(cid:5)
(cid:3)(cid:2)(cid:4)
(cid:3)

(cid:4)
(cid:3)(cid:2)(cid:12)
(cid:3)(cid:2)(cid:11)
(cid:3)(cid:2)(cid:10)
(cid:3)(cid:2)(cid:9)
(cid:3)(cid:2)(cid:8)
(cid:3)(cid:2)(cid:7)
(cid:3)(cid:2)(cid:6)
(cid:3)(cid:2)(cid:5)
(cid:3)(cid:2)(cid:4)
(cid:3)

(cid:19)(cid:24)(cid:6)(cid:5)

(cid:19)(cid:21)(cid:28)(cid:18)(cid:17)(cid:4)(cid:9)

(cid:19)(cid:21)(cid:28)(cid:18)(cid:17)(cid:11)

(cid:19)(cid:21)(cid:28)(cid:18)(cid:17)(cid:9)

(cid:19)(cid:24)(cid:6)(cid:5)

(cid:19)(cid:21)(cid:28)(cid:18)(cid:17)(cid:4)(cid:9)

(cid:19)(cid:21)(cid:28)(cid:18)(cid:17)(cid:11)

(cid:19)(cid:21)(cid:28)(cid:18)(cid:17)(cid:9)

(cid:26)(cid:23)(cid:24)(cid:4)(cid:1)(cid:23)(cid:25)(cid:21)(cid:20)(cid:21)(cid:22)
(cid:26)(cid:23)(cid:24)(cid:8)(cid:1)(cid:23)(cid:25)(cid:21)(cid:20)(cid:21)(cid:22)

(cid:26)(cid:23)(cid:24)(cid:4)(cid:1)(cid:19)(cid:21)(cid:22)(cid:18)(cid:1)(cid:26)(cid:27)(cid:22)(cid:18)
(cid:26)(cid:23)(cid:24)(cid:8)(cid:1)(cid:19)(cid:21)(cid:22)(cid:18)(cid:1)(cid:26)(cid:27)(cid:22)(cid:18)

(cid:26)(cid:23)(cid:24)(cid:4)(cid:1)(cid:23)(cid:25)(cid:21)(cid:20)(cid:21)(cid:22)
(cid:26)(cid:23)(cid:24)(cid:8)(cid:1)(cid:23)(cid:25)(cid:21)(cid:20)(cid:21)(cid:22)

(cid:26)(cid:23)(cid:24)(cid:4)(cid:1)(cid:19)(cid:21)(cid:22)(cid:18)(cid:1)(cid:26)(cid:27)(cid:22)(cid:18)
(cid:26)(cid:23)(cid:24)(cid:8)(cid:1)(cid:19)(cid:21)(cid:22)(cid:18)(cid:1)(cid:26)(cid:27)(cid:22)(cid:18)

qiu et al.  going deeper with embedded fpga platform for convolutional neural network, fpga   16

part 1: algorithms for efficient id136

    1. pruning
    2. weight sharing 
    3. quantization 
    4. low rank approximation
    5. binary / ternary net
    6. winograd transformation

low rank approximation for conv

zhang et al efficient and accurate approximations of nonlinear convolutional networks cvpr   15

low rank approximation for conv

zhang et al efficient and accurate approximations of nonlinear convolutional networks cvpr   15

low rank approximation for fc

novikov et al tensorizing neural networks, nips   15

part 1: algorithms for efficient id136

    1. pruning
    2. weight sharing 
    3. quantization 
    4. low rank approximation
    5. binary / ternary net
    6. winograd transformation

binary / ternary net: motivation

train on sparse (s)

recover zero weights

train on dense (d)

6400

normalized    

full precision weight

4800

t
n
u
o
c

3200

1600

0

t

0

   0.05

-t

0.05

weight value

6400

4800

intermediate ternary weight

t
n
u
o
c

3200

=>

quantize

1600

0.05

0

   0.05

-1

0

weight value

0.05

0

1

weight value

0

1
(d)

(e)

gradient1

figure 2: weight distribution of the original googlenet (a), pruned googlenet (b), after retraining
the sparsity-constrained googlenet (c), ignoring the sparisty constraint and recovering the zero
weights (d), and after retraining the dense network (e).

id136 time

trained ternary quantization

full precision weight

normalized    

full precision weight

intermediate ternary weight

normalize

quantize

trained 

quantization

scale

wn

wp

final ternary weight

-1

0

1

-1

-t

0

t

1

-1

0

1

-wn

0

wp

feed forward

back propagate

id136 time

gradient1

gradient2

loss

zhu, han, mao, dally. trained ternary quantization, iclr   17

pruning

trained quantization

huffman coding

 l = t     max(|   w|)

(9)

weight evolution during training

and 2) maintain a constant sparsity r for all layers throughout training. by adjusting the hyper-
parameter r we are able to obtain ternary weight networks with various sparsities. we use the    rst
method and set t to 0.05 in experiments on cifar-10 and id163 dataset and use the second one
to explore a wider range of sparsities in section 5.1.1.

res1.0/conv1/wn

res1.0/conv1/wp

res3.2/conv2/wn

res3.2/conv2/wp

linear/wn

linear/wp

3
2
1
0
-1
-2
-3

 
e
u
a
v

l

 
t

i

h
g
e
w
 
y
r
a
n
r
e
t

100%
75%
50%
25%
0%

negatives

zeros

positives

negatives

zeros

positives

negatives

zeros

positives

0

50

100

150

0

50

100

150

0

50

100

150

epochs

 
t

i

h
g
e
w
 
y
r
a
n
r
e
t

e
g
a

t

n
e
c
r
e
p

figure 2: ternary weights value (above) and distribution (below) with iterations for different layers
of resnet-20 on cifar-10.

zhu, han, mao, dally. trained ternary quantization, iclr   17

4

visualization of the ttq kernels

zhu, han, mao, dally. trained ternary quantization, iclr   17

pruning

trained quantization

huffman coding

error rate on id163

zhu, han, mao, dally. trained ternary quantization, iclr   17

pruning

trained quantization

huffman coding

part 1: algorithms for efficient id136

    1. pruning
    2. weight sharing 
    3. quantization 
    4. low rank approximation
    5. binary / ternary net
    6. winograd transformation

image tensor

0

3

6

1

4

7

2

5

8

3x3 direct convolutions

compute bound

filter

8

5

2

7

4

1

6

3

0

   

9xc fmas/output: math intensive

8

5

2

1

4

7

7

4

1

2

5

8

9xk fmas/input: good data reuse

0

3

6

4

4

4

1

4

7

4

4

4

6

3

0

2

5

8

   

0

3

6

4

4

4

direct convolution: we need 9xcx4 = 36xc fmas for 4 outputs

julien demouth, convolution optimization: winograd, nvidia

73

3x3 winograd convolutions

transform data to reduce math intensity

4x4 tile

filter

data transform

filter transform

    over c

point-wise 
multiplication

output transform

direct convolution: we need 9xcx4 = 36xc fmas for 4 outputs 
winograd convolution: we need 16xc fmas for 4 outputs: 2.25x fewer fmas

see a. lavin & s. gray,    fast algorithms for convolutional neural networks

julien demouth, convolution optimization: winograd, nvidia

74

speedup of winograd convolution

vgg16, batch size 1     relative performance

cudnn 3

cudnn 5

2.00

1.50

1.00

0.50

0.00

conv 1.1

conv 1.2

conv 2.1

conv 2.2

conv 3.1

conv 3.2

conv 4.1

conv 4.2

conv 5.0

measured on maxwell titan x

julien demouth, convolution optimization: winograd, nvidia

75

agenda

algorithm

algorithms for    
ef   cient id136

algorithms for
ef   cient training

id136

training

hardware for   

ef   cient id136 

hardware for   

ef   cient training 

hardware

hardware for efficient id136
a common goal: minimize memory access

sparse matrix read unit. the sparse-matrix read unit
uses pointers pj and pj+1 to read the non-zero elements (if
any) of this pe   s slice of column ij from the sparse-matrix
sram. each entry in the sram is 8-bits in length and
contains one 4-bit element of v and one 4-bit element of x.
for ef   ciency (see section vi) the pe   s slice of encoded
sparse matrix i is stored in a 64-bit-wide sram. thus eight
entries are fetched on each sram read. the high 13 bits
of the current pointer p selects an sram row, and the low
3-bits select one of the eight entries in that row. a single
(v, x) entry is provided to the arithmetic unit each cycle.

dadiannao
cas
edram

tpu
google

arithmetic unit. the arithmetic unit receives a (v, x)
entry from the sparse matrix read unit and performs the
multiply-accumulate operation bx = bx + v     aj. index
x is used to index an accumulator array (the destination
activation registers) while v is multiplied by the activation
value at the head of the activation queue. because v is stored
in 4-bit encoded form, it is    rst expanded to a 16-bit    xed-
point number via a table look up. a bypass path is provided
to route the output of the adder to its input if the same
accumulator is selected on two adjacent cycles.

8-bit integer

activation read/write. the activation read/write unit
contains two activation register    les that accommodate the
source and destination activation values respectively during
a single round of fc layer computation. the source and
destination register    les exchange their role for next layer.
thus no additional data transfer is needed to support multi-
layer feed-forward computation.

   this unit is designed for dense 
matrices. sparse architectural 
support was omitted for time-to-
deploy reasons. sparsity will have 
high priority in future designs    
each activation register    le holds 64 16-bit activations.
to accommodate 4k activation vectors
this is suf   cient
across 64 pes. longer activation vectors can be accommo-
dated with the 2kb activation sram. when the activation
vector has a length greater than 4k, the m   v will be
completed in several batches, where each batch is of length
4k or less. all the local reduction is done in the register

id173

spmat

ptr_even

act_0

act_1

arithm

ptr_odd

spmat
eie

table ii

figure 5. layout of one pe in eie under tsmc 45nm process.

the implementation results of one pe in eie and the

breakdown by component type (line 3-7), by module (line

8-13). the critical path of eie is 1.15 ns

compression/

stanford
sparsity

power
(mw)
9.157
5.416
1.874
1.026
0.841

(59.15%)
(20.46%)
(11.20%)
(9.18%)

(%)

area
(  m2)
638,024
594,786
866
9,465
8,946
23,961
758
121,849
469,412
3,110
18,934
23,961

(%)

(93.22%)
(0.14%)
(1.48%)
(1.40%)
(3.76%)
(0.12%)
(19.10%)
(73.57%)
(0.49%)
(2.97%)
(3.76%)

total
memory
clock network
register
combinational
   ller cell
act queue
ptrread
spmatread
arithmunit
actrw
   ller cell

0.112
1.807
4.955
1.162
1.122

(1.23%)
(19.73%)
(54.11%)
(12.68%)
(12.25%)

central unit: i/o and computing. in the i/o mode, all of
the pes are idle while the activations and weights in every
pe can be accessed by a dma connected with the central
unit. this is one time cost. in the computing mode, the
ccu repeatedly collects a non-zero value from the lnzd
quadtree and broadcasts this value to all pes. this process
continues until the input length is exceeded. by setting the

77

eyeriss   
mit

rs data   ow

compression

acceleration

google tpu

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

78

google tpu

    the matrix unit: 65,536 (256x256) 

8-bit multiply-accumulate units

    700 mhz clock rate
    peak: 92t operations/second 

    65,536 * 2 * 700m

    >25x as many macs vs gpu
    >100x as many macs vs cpu
    4 mib of on-chip accumulator 

memory

    24 mib of on-chip unified buffer 

(activation memory)

    3.5x as much on-chip memory 

    two 2133mhz ddr3 dram 

    8 gib of off-chip weight dram 

vs gpu

channels

memory

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

79

google tpu

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

80

id136 datacenter workload

name loc

layers

fc conv vector pool total

mlp0 0.1k 5
4
mlp1 1k
24
lstm0 1k

lstm1 1.5k 37
id980 1k
id981 1k

4

16
72

34

19

nonlinear 
function weights
relu 20m
5m
relu
sigmoid, 
52m

tanh

tpu ops / 

weight 
byte
200
168
64

tpu 
batch 
size
200
168
64

5
4
58

56
16
89

13

sigmoid, 

96
tanh
relu
2888
relu 100m 1750

34m
8m

96
8
32

% 

deployed

61%

29%

5%

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

81

roofline model: identify performance bottleneck

samuel williams, andrew waterman, and david patterson. "roofline: an insightful visual 
performance model for multicore architectures."communications of the acm 52.4 (2009): 65-76.

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

82

tpu roofline

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

83

log rooflines for cpu, gpu, tpu

star = tpu
triangle = gpu
circle = cpu

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

84

linear rooflines for cpu, gpu, tpu

star = tpu
triangle = gpu
circle = cpu

david patterson and the google tpu team, in-data center performance analysis of a tensor processing unit

85

why so far below rooflines?

low latency requirement => can   t batch more => low ops/byte

how to solve this?

less memory footprint => need compress the model

challenge:

hardware that can infer on compressed model

86

eie: the first dnn accelerator for   

 sparse, compressed model

[han et al. isca   16]

compression

acceleration

id173

87

eie: the first dnn accelerator for   

 sparse, compressed model

[han et al. isca   16]

0 * a = 0

w * 0 = 0

2.09, 1.92=> 2

sparse	weight	
90%	static	sparsity

sparse	activation	
70%	dynamic	sparsity

weight	sharing	

4-bit	weights

10x	less	computation

3x	less	computation

5x	less	memory	footprint

8x	less	memory	footprint

compression

acceleration

id173

88

eie: reduce memory access by compression

logically

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

physically

virtual	weight w0,0 w0,1 w4,2 w0,3 w4,3

relative	index

column	pointer

0

0

1

1

2

2

0

3

0

han et al.    eie: efficient id136 engine on compressed deep neural network   , isca 2016, hotchips 2016

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

90

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

91

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

92

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

93

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

94

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

95

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

96

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

97

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

98

dataflow

[han et al. isca   16]

p e0
p e1
p e2
p e3

~a  0 a1

0

   

a3  

w0,0w0,1 0 w0,3

0 w1,2 0

0
0 w2,1 0 w2,3
0
0

0
0 w4,2w4,3

0

0

w5,0 0
0

0
0 w7,1 0

0

0
0 w6,3

0

0bbbbbbbbbbbbb@

=

0bbbbbbbbbbbbb@

b0
b1
 b2
b3
 b4
b5
b6
 b7

1ccccccccccccca

relu)

~b
b0
b1
0
b3
0
b5
b6
0

0bbbbbbbbbbbbb@

1ccccccccccccca

1ccccccccccccca

rule of thumb:   

0 * a = 0   w * 0 = 0

compression

acceleration

id173

99

mark a. horowitz    william j. dally      

   stanford university,    nvidia

eie architecture

{songhan,xyl,huizi,jingpu,perdavan,horowitz,dally}@stanford.edu

[han et al. isca   16]

alu

mem

prediction
result

compressed	
dnn	model

input	
image

encoded	weight	
relative	index	
sparse	format	

weight decode
4-bit	   

virtual	weight

weight		
look-up

16-bit		

real	weight

4-bit	   

relative	index

index		
accum

16-bit	   

absolute	index

figure 1. ef   cient id136 engine that works on the compressed deep
neural network model for machine learning applications.

address accumulate

rule of thumb:

word, or speech sample. for embedded mobile applications,
2.09, 1.92=> 2
these resource demands become prohibitive. table i shows
compression
the energy cost of basic arithmetic and memory operations

w * 0 = 0

0 * a = 0

id173

acceleration

100

micro architecture for each pe

[han et al. isca   16]

act	value

act	index

act	queue

act	index

even	ptr	sram	bank

odd	ptr	sram	bank

act	value

encoded	
weight

act	
sram

leading	
nzero	
detect

col	
start/
end	
addr

sparse	
matrix	
sram

regs

weight	
decoder

bypass

dest	
act	
regs

src	
act	
regs

relu

pointer	read

sparse	matrix	access

act	r/w

relative	index

address	
accum
																						arithmetic	unit

absolute	address

sram

regs

comb

compression

acceleration

id173

101

figure 6. speedups of gpu, mobile gpu and eie compared with cpu running uncompressed dnn model. there is no batching in all cases.

nt-we

spmat

cpu dense (baseline)

mgpu compressed

ptr_odd

arithm

248x

507x

115x

210x

14x

56x

25x

14x

24x

94x

21x

5x

9x

5x

act_0

act_1

ptr_even

cpu compressed

8x

2x

1x

3x

0.6x

1x

1.1x

1x

1x

1x

0.5x

1x

1.0x

spmat

gpu dense
1018x

618x

135x

22x

10x

9x

1x

1.0x

gpu compressed

eie

92x

10x

16x

1x

1x

1x

0.3x

mgpu dense

mgpu compressed

eie

63x

34x

9x

2x

3x

0.5x

98x

33x

15x

3x

1x

2x

0.5x

60x

25x

9x

2x

1x

1x

0.5x

189x

48x

15x

3x

1x

3x

0.6x

1x

p
u
d
e
e
p
s

1000x
100x
10x
1x
0.1x

figure 5. layout of one pe in eie under tsmc 45nm process.

alex-6

alex-7

alex-8

vgg-6

vgg-7

vgg-8

nt-we

nt-wd

nt-lstm

geo mean

table ii

figure 6. speedups of gpu, mobile gpu and eie compared with cpu running uncompressed dnn model. there is no batching in all cases.

the implementation results of one pe in eie and the

breakdown by component type (line 3-7), by module (line

8-13). the critical path of eie is 1.15 ns

98x

power
(mw)
9.157
5.416
1.874
1.026
0.841

0.112
1.807
4.955
1.162
1.122

(%)

(59.15%)
(20.46%)
(11.20%)
(9.18%)

(1.23%)
(19.73%)
(54.11%)
(12.68%)
(12.25%)

60x

gpu dense
119,797x

cpu dense (baseline)

cpu compressed

gpu compressed

mgpu dense

mgpu compressed

14,826x

61,533x

34,522x

25x

(%)
100000x
(93.22%)
10000x
(0.14%)
(1.48%)
1000x
(1.40%)
100x
(3.76%)
(0.12%)
10x
(19.10%)
(73.57%)
1x
(0.49%)
(2.97%)
(3.76%)

area
(  m2)
638,024
y
594,786
c
n
866
e
9,465
i
c
8,946
i
f
23,961
f
e
758
 
y
121,849
g
469,412
r
e
3,110
n
18,934
e
23,961
figure 7. energy ef   ciency of gpu, mobile gpu and eie compared with cpu running uncompressed dnn model. there is no batching in all cases.

geo mean

15x

48x

1x
geo mean

1x
nt-lstm

3x

9x

3x

8x
nt-we

7x
nt-wd

5x
vgg-8

10x
alex-6

7x
alex-8

vgg-6

vgg-7

alex-7

76,784x

10,904x

11,828x

9,485x

8,053x

101x

102x

26x

37x

17x

20x

12x

10x

10x

37x

10x

78x

61x

15x

13x

59x

18x

25x

14x

15x

23x

14x

14x

39x

25x

20x

36x

1x

1x

1x

1x

5x

2x

6x

6x

4x

6x

7x

5x

6x

6x

5x

7x

8x

7x

9x

1x

1x

1x

1x

9x

3x

7x

eie
24,207x

memory
clock network
register
combinational
   ller cell
act queue
ptrread
spmatread
arithmunit
actrw
   ller cell

2x

189x

2x

1x

1x

0.6x

1x

central unit: i/o and computing. in the i/o mode, all of
the pes are idle while the activations and weights in every
pe can be accessed by a dma connected with the central
unit. this is one time cost. in the computing mode, the
ccu repeatedly collects a non-zero value from the lnzd
quadtree and broadcasts this value to all pes. this process
continues until the input length is exceeded. by setting the
input length and starting address of pointer array, eie is
instructed to execute different layers.

v. evaluation methodology

0.5x

corner. we placed and routed the pe using the synopsys ic
compiler (icc). we used cacti [25] to get sram area and
energy numbers. we annotated the toggle rate from the rtl
simulation to the gate-level netlist, which was dumped to
switching activity interchange format (saif), and estimated
the power using prime-time px.

simulator, rtl and layout. we implemented a custom
cycle-accurate c++ simulator for the accelerator aimed to
the rtl behavior of synchronous circuits. each
hardware module is abstracted as an object that implements
two abstract methods: propagate and update, corresponding
to combination logic and the    ip-   op in rtl. the simulator
is used for design space exploration. it also serves as a
checker for rtl veri   cation.

to measure the area, power and critical path delay, we
implemented the rtl of eie in verilog. the rtl is veri   ed
against the cycle-accurate simulator. then we synthesized
eie using the synopsys design compiler (dc) under the
tsmc 45nm gp standard vt library with worst case pvt

compression

comparison baseline. we compare eie with three dif-
ferent off-the-shelf computing units: cpu, gpu and mobile
gpu.

nt-lstm

1) cpu. we use intel core i-7 5930k cpu, a haswell-e
class processor, that has been used in nvidia digits deep
learning dev box as a cpu baseline. to run the benchmark
on cpu, we used mkl cblas gemv to implement the

acceleration

9%

alex-8

alex-7

layer
alex-6

size
9216,
4096
4096,
4096
4096,
25%
cpu gpu mgpu eie
1000
25088, 4%
4096
4096,
4096
4096,
1000
id173
4096,
600
600,

geo mean

vgg-8

vgg-6

vgg-7

23%

10%

4%

benchmark from state-of-the-art dnn models

table iii

weight% act% flop% description
9%

35.1% 3%

35.3% 3%

37.5% 10%

18.3% 1%

37.5% 2%

41.1% 9%

100% 10%

compressed
alexnet [1] for
large scale image
classi   cation

compressed
vgg-16 [3] for
large scale image
classi   cation and
id164
compressed
neuraltalk [7]
with id56 and

102

speedup on eie

[han et al. isca   16]

figure 6. speedups of gpu, mobile gpu and eie compared with cpu running uncompressed dnn model. there is no batching in all cases.

vgg-6

vgg-8

vgg-7

alex-7

alex-6

alex-8

nt-we

nt-wd

nt-lstm

figure 6. speedups of gpu, mobile gpu and eie compared with cpu running uncompressed dnn model. there is no batching in all cases.

nt-wd

cpu dense (baseline)

nt-lstm

507x

248x

94x

56x

25x

cpu compressed

gpu dense
1018x

geo mean

gpu compressed

618x

92x

210x

135x

115x

mgpu dense

mgpu compressed

eie

63x

34x

98x

33x

60x

[han et al. isca   16]

189x

48x

15x

14x

2x

3x

1x

0.6x

5x

5x

21x

14x

energy efficiency on eie

1.1x

0.5x

1.0x

1.0x

0.3x

0.5x

0.5x

10x

22x

10x

15x

16x

1x

1x

1x

1x

9x

1x

8x

9x

1x

1x

1x

1x

1x

1x

2x

3x

2x

9x

9x

1x

3x

2x

24x

25x

1x

0.5x

p
u
d
e
e
p
s

1000x
100x
10x
1x
0.1x

table ii

figure 7. energy ef   ciency of gpu, mobile gpu and eie compared with cpu running uncompressed dnn model. there is no batching in all cases.

gpu dense
119,797x

gpu compressed

76,784x

mgpu dense

mgpu compressed

9,485x

10,904x

8,053x

102x

61x

20x

10x

14x

1x

vgg-7

39x

11,828x

8x

5x

2x

14x

25x

eie
24,207x

5x
vgg-8

8x
nt-we

1x

1x

6x

6x

25x

14x

6x

6x

1x

7x
nt-wd

15x

20x

6x

4x

5x

7x

1x
nt-lstm

9x

1x
geo mean

benchmark from state-of-the-art dnn models

table iii

weight% act% flop% description
9%

35.1% 3%

35.3% 3%

37.5% 10%

geo mean

18.3% 1%

ptr_even

spmat

cpu dense (baseline)

cpu compressed

34,522x

61,533x

y
c
n
ptr_odd
e
c

i

act_1

act_0

arithm

mgpu compressed

spmat

i
f
f

26x

37x

17x

12x

10x

37x

10x

78x

59x

18x

5x

7x

7x

9x

14,826x

101x

1x

10x
alex-6

15x

1x

alex-7

3x

1x

7x
alex-8

1x

13x

vgg-6

100000x
10000x
1000x
100x
10x
1x

 

e
y
g
r
e
n
e

figure 5. layout of one pe in eie under tsmc 45nm process.

the implementation results of one pe in eie and the

breakdown by component type (line 3-7), by module (line

8-13). the critical path of eie is 1.15 ns

10,904x

(%)

8,053x

power
(mw)
9.157
5.416
1.874
1.026
0.841

0.112
1.807
4.955
1.162
1.122

(59.15%)
(20.46%)
(11.20%)
(9.18%)

(1.23%)
(19.73%)
(54.11%)
(12.68%)
(12.25%)

memory
clock network
register
combinational
   ller cell
act queue
ptrread
spmatread
arithmunit
actrw
   ller cell

20x

15x

area
(  m2)
638,024
594,786
866
9,465
8,946
23,961
758
121,849
469,412
3,110
18,934
23,961

(%)

corner. we placed and routed the pe using the synopsys ic
compiler (icc). we used cacti [25] to get sram area and
energy numbers. we annotated the toggle rate from the rtl
simulation to the gate-level netlist, which was dumped to
switching activity interchange format (saif), and estimated
the power using prime-time px.

comparison baseline. we compare eie with three dif-
ferent off-the-shelf computing units: cpu, gpu and mobile
gpu.

(93.22%)
(0.14%)
(1.48%)
(1.40%)
(3.76%)
(0.12%)
(19.10%)
(73.57%)
(0.49%)
(2.97%)
(3.76%)

7x

5x

4x

1x
nt-lstm

1) cpu. we use intel core i-7 5930k cpu, a haswell-e
7x
class processor, that has been used in nvidia digits deep
learning dev box as a cpu baseline. to run the benchmark
on cpu, we used mkl cblas gemv to implement the
original dense model and mkl spblas csrmv for the
compressed sparse model. cpu socket and dram power
are as reported by the pcm-power utility provided by intel.
2) gpu. we use nvidia geforce gtx titan x gpu,
a state-of-the-art gpu for deep learning as our baseline
using nvidia-smi utility to report the power. to run
the benchmark, we used cublas gemv to implement
the original dense layer. for the compressed sparse layer,
we stored the sparse matrix in in csr format, and used
cusparse csrmv kernel, which is optimized for sparse

acceleration

9%

alex-8

alex-7

vgg-6

layer
alex-6

36x

23x

size
9216,
4096
4096,
4096
4096,
25%
1000
25088, 4%
4096
4096,
4096
4096,
1000
4096,
9x
600
600,
nt-wd
8791
ntlstm 1201,
2400
cpu gpu mgpueie

1x
geo mean

6x

vgg-8

vgg-7

nt-we

23%

10%

11%

10%

4%

25x

central unit: i/o and computing. in the i/o mode, all of
the pes are idle while the activations and weights in every
pe can be accessed by a dma connected with the central
unit. this is one time cost. in the computing mode, the
ccu repeatedly collects a non-zero value from the lnzd
quadtree and broadcasts this value to all pes. this process
continues until the input length is exceeded. by setting the
input length and starting address of pointer array, eie is
instructed to execute different layers.

7x
nt-wd

v. evaluation methodology

simulator, rtl and layout. we implemented a custom
cycle-accurate c++ simulator for the accelerator aimed to
the rtl behavior of synchronous circuits. each
hardware module is abstracted as an object that implements
two abstract methods: propagate and update, corresponding
to combination logic and the    ip-   op in rtl. the simulator
is used for design space exploration. it also serves as a
checker for rtl veri   cation.

to measure the area, power and critical path delay, we
implemented the rtl of eie in verilog. the rtl is veri   ed
against the cycle-accurate simulator. then we synthesized
eie using the synopsys design compiler (dc) under the
tsmc 45nm gp standard vt library with worst case pvt

compression

37.5% 2%

41.1% 9%

100% 10%

100% 11%

100% 11%

the uncompressed dnn model
is obtained from caffe
model zoo [28] and neuraltalk model zoo [7]; the com-
pressed dnn model is produced as described in [16], [23].
the benchmark networks have 9 layers in total obtained
from alexnet, vggnet, and neuraltalk. we use the image-
net dataset [29] and the caffe [28] deep learning framework

103

3x

1x

3x

0.6x

geo mean

eie
24,207x

36x

23x

7x

compressed
alexnet [1] for
large scale image
classi   cation

compressed
vgg-16 [3] for
large scale image
classi   cation and
id164
compressed
neuraltalk [7]
with id56 and
lstm for
automatic
image captioning

figure 7. energy ef   ciency of gpu, mobile gpu and eie compared with cpu running uncompressed dnn model. there is no batching in all cases.

id173

comparison: throughput

[han et al. isca   16]

eie

asic

throughput (layers/s in log scale)

asic

asic

asic

gpu

cpu

mgpu

fpga

core-i7 5930k   

22nm    
cpu

titanx   
28nm    
gpu

tegra k1   
28nm   
mgpu

a-eye    
28nm    
fpga

dadiannao    

28nm   
asic

truenorth   

28nm   
asic

eie   
45nm   
asic   
64pes

eie   
28nm   
asic   
256pes

1e+06

1e+05

1e+04

1e+03

1e+02

1e+01

1e+00

compression

acceleration

id173

104

comparison: energy efficiency

[han et al. isca   16]

energy ef   ciency (layers/j in log scale)

eie

asic asic

asic asic

gpu mgpu

core-i7 5930k   

cpu
22nm    
cpu

titanx   
28nm    
gpu

tegra k1   
28nm   
mgpu

fpga
a-eye    
28nm    
fpga

dadiannao    

28nm   
asic

truenorth   

28nm   
asic

eie   
45nm   
asic   
64pes

eie   
28nm   
asic   
256pes

1e+06

1e+05

1e+04

1e+03

1e+02

1e+01

1e+00

compression

acceleration

id173

105

agenda

algorithm

algorithms for    
ef   cient id136

algorithms for
ef   cient training

id136

training

hardware for   

ef   cient id136 

hardware for   

ef   cient training 

hardware

part 3: efficient training     algorithms

    1. parallelization 
    2. mixed precision with fp16 and fp32
    3. model distillation
    4. dsd: dense-sparse-dense training

part 3: efficient training     algorithms

    1. parallelization 
    2. mixed precision with fp16 and fp32
    3. model distillation
    4. dsd: dense-sparse-dense training

moore   s law made cpus 300x faster than in 1990   

but its over   

c moore, data processing in exascale-classcomputer systems, salishan, april 2011

data parallel     run multiple inputs in parallel

dally, high performance hardware for machine learning, nips   2015

data parallel     run multiple inputs in parallel

    doesn   t affect latency for one input 
    requires p-fold larger batch size 
    for training requires coordinated weight update

dally, high performance hardware for machine learning, nips   2015

parameter update

one method to achieve scale is parallelization

parameter server

p    = p +    p

   p

p   

model!
workers

data!
shards

large scale distributed deep networks, jeff dean et al., 2013

large scale distributed deep networks 
j dean et al (2012)

model parallel    

split up the model     i.e. the network

dally, high performance hardware for machine learning, nips   2015

model-parallel convolution     by output region (x,y)

kernels 
multiple 3d 
kuvkj

x

aijaijaxyk

bxyj bxyj
bxyj bxyj

bxyj
bxyj

bxyj bxyj
bxyj bxyj

input maps 

axyk

output maps 

bxyj

6d loop 
forall output map j 
  for each input map k 
  for each pixel x,y 
 
 
 
 
 

  for each kernel element u,v 
 

  bxyj += a(x-u)(y-v)k x kuvkj

dally, high performance hardware for machine learning, nips   2015

model-parallel convolution     by output map j 

(filter)

kernels 
multiple 3d 
kuvkj

x

6d loop 
forall output map j 
  for each input map k 
  for each pixel x,y 
 
 
 
 
 

  for each kernel element u,v 
 

  bxyj += a(x-u)(y-v)k x kuvkj

aijaijaxyk

aij
aijaijbxyj

input maps 

axyk

output maps 

bxyj

dally, high performance hardware for machine learning, nips   2015

model parallel fully-connected layer (m x v)

bi

=

wij

x

aj

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

weight matrix i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

dally, high performance hardware for machine learning, nips   2015

model parallel fully-connected layer (m x v)

bi

bi

o
u

t

p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

=

wij

wij

x

aj

weight matrix i

n
p
u

t
 

a
c
t
i
v
a

t
i

o
n
s

dally, high performance hardware for machine learning, nips   2015

hyper-parameter parallel   

try many alternative networks in parallel

dally, high performance hardware for machine learning, nips   2015

summary of parallelism

    lots of parallelism in dnns

    16m independent multiplies in one fc layer
    limited by overhead to exploit a fraction of this

    data parallel

    run multiple training examples in parallel
    limited by batch size

    model parallel

    split model over multiple processors
    by layer
    conv layers by map region
    fully connected layers by output activation

    easy to get 16-64 gpus training one model in parallel
dally, high performance hardware for machine learning, nips   2015

part 3: efficient training     algorithms

    1. parallelization 
    2. mixed precision with fp16 and fp32
    3. model distillation
    4. dsd: dense-sparse-dense training

mixed precision

https://devblogs.nvidia.com/parallelforall/cuda-9-features-revealed/

mixed precision training
volta training method 
volta training method 
volta training method 
volta training method 
volta training method 
volta training method 
w (f16) 
f16 
w (f16) 
w (f16) 
w (f16) 
f16 
f16 
f16 

w (f16) 
w (f16) 

f16 
f16 

w 
w 
w 
w 
w 
w 
actv 
actv 
actv 
actv 
actv 
actv 

f16 
f16 
f16 
f16 

f16 
f16 
f16 
f16 
f16 
f16 
f16 
f16 

fwd 
fwd 

fwd 
fwd 
fwd 
fwd 

actv 
actv 

actv 
actv 
actv 
actv 

actv grad 
actv grad 

actv grad 
actv grad 
actv grad 
actv grad 

f16 
f16 

f16 
f16 
f16 
f16 

bwd-a 
bwd-a 

bwd-a 
bwd-a 
bwd-a 
bwd-a 

w grad 
w grad 

w grad 
f16 
f16 
w grad 
w grad 
w grad 

f16 
f16 
f16 
f16 

bwd-w 
bwd-w 

bwd-w 
bwd-w 
bwd-w 
bwd-w 

f16 
f16 

f16 
f16 
f16 
f16 

f16 
f16 
f16 
f16 

f16 
f16 
f16 
f16 
w 
w 
f16 
f16 
f16 
f16 
actv grad 
actv grad 

w 
w 
w 
w 
actv grad 
actv grad 
actv grad 
actv grad 

f16 
f16 
f16 
f16 

f16 
f16 
f16 
f16 
f16 
f16 
f16 
f16 

actv 
actv 
actv 
actv 
actv 
actv 
actv grad 
actv grad 
actv grad 
actv grad 
actv grad 
actv grad 

master-w (f32) 
master-w (f32) 

master-w (f32) 
master-w (f32) 
master-w (f32) 
master-w (f32) 

f32 
f32 

f32 
f32 
f32 
f32 

weight update 
weight update 

weight update 
weight update 
weight update 
weight update 

f32 
f32 

f32 
f32 
f32 
f32 

updated master-w 
updated master-w 

updated master-w 
updated master-w 
updated master-w 
updated master-w 
5  
5  

5  
5  
5  
5  

boris ginsburg, sergei nikolaev, paulius micikevicius,    training with mixed precision   , nvidia gtc 2017

inception v1
inception v1 

boris ginsburg, sergei nikolaev, paulius micikevicius,    training with mixed precision   , nvidia gtc 2017

12  12  

resnet
resnet50 

boris ginsburg, sergei nikolaev, paulius micikevicius,    training with mixed precision   , nvidia gtc 2017

13  13  

alexnet : comparison of results 

alexnet

top1 

top5  

accuracy, % 

accuracy, % 

mode 
fp32 

mixed precision training 

58.62 
resnet results 
58.12 

fp16 training, loss scale = 1000 

fp16 training 

inception v3

no scale of id168      
54.89 
top1 
57.76 

mode 
fp32 
inception-v3 results 

accuracy, % 

71.75 

81.25 

80.71 

78.12 
top5  
80.76 

accuracy, % 

90.52 

nvcaffe-0.16, dgx-1, sgd with momentum, 100 epochs, batch=1024, no augmentation, 1 crop, 1 model  

30  

mixed precision training 

90.10 

scale id168 by 100x     

resnet-50

fp16 training, loss scale = 1 
fp16 training, loss scale = 1, 
fp16 master weight storage 

accuracy, % 

90.33 
top5  
90.14 

accuracy, % 

73.85 

91.44 

mode 
fp32 

71.17 

71.17 
top1 
70.53 

nvcaffe-0.16, dgx-1, sgd with momentum, 100 epochs, batch=512, no augmentation, 1 crop, 1 model  

41  

mixed precision training 

73.6 

91.11 

boris ginsburg, sergei nikolaev, paulius micikevicius,    training with mixed precision   , nvidia gtc 2017

fp16 training 

90.84 

71.36 

fp16 training, loss scale = 100 

74.13 

91.51 

part 3: efficient training algorithm

    1. parallelization 
    2. mixed precision with fp16 and fp32
    3. model distillation
    4. dsd: dense-sparse-dense training

model distillation

teacher	model	1	

teacher	model	2	

(googlenet)

(vggnet)

teacher	model	3	

(resnet)

knowledge

knowledge

knowledge

student	
model

student model has much smaller model size

softened outputs reveal the dark knowledge

hinton et al. dark knowledge / distilling the knowledge in a neural network

softened outputs reveal the dark knowledge

   method: divide score by a    temperature    to get a much softer 
distribution    
   result: start with a trained model that classifies 58.9% of the 
test frames correctly. the new model converges to 57.0% 
correct even when it is only trained on 3% of the data

hinton et al. dark knowledge / distilling the knowledge in a neural network

part 3: efficient training algorithm

    1. parallelization 
    2. mixed precision with fp16 and fp32
    3. model distillation
    4. dsd: dense-sparse-dense training

dsd: dense sparse dense training

under review as a conference paper at iclr 2017

figure 1: dense-sparse-dense training flow. the sparse training regularizes the model, and the    nal
dense training restores the pruned weights (red), increasing the model capacity without over   tting.

dsd  produces  same  model  architecture  but  can  find  better  optimization  solution, 
arrives at better local minima, and achieves higher prediction accuracy across a wide 
range of deep neural networks on id98s / id56s / lstms.

algorithm 1: work   ow of dsd training
initialization: w (0) with w (0)     n (0,    )
output :w (t).
                                                 initial dense phase                                                 
while not converged do

han et al.    dsd: dense-sparse-dense training for deep neural networks   , iclr 2017

densepruningsparsity constraintsparseincrease model capacity re-densedensedsd: intuition

learn the trunk first

then learn the leaves

han et al.    dsd: dense-sparse-dense training for deep neural networks   , iclr 2017

dsd is general purpose:  

vision, speech, natural language

[han et al. iclr 2017]

network

domain

dataset

type

baseline

googlenet

vgg-16

resnet-18

resnet-50

vision id163 id98
vision id163 id98
vision id163 id98
vision id163 id98

31.1%
31.5%
30.4%
24.0%

dsd
30.0%
27.2%
29.3%
23.2%

abs.   
rel.   
imp.
imp.
1.1%
3.6%
4.3% 13.7%
3.7%
1.1%
3.5%
0.9%

open sourced dsd model zoo: https://songhan.github.io/dsd

the beseline results of alexnet, vgg16, googlenet, squeezenet are from caffe model zoo. resnet18, resnet50 are from fb.resnet.torch.

compression

acceleration

id173

133

dsd is general purpose:  

vision, speech, natural language

[han et al. iclr 2017]

network

domain

dataset

type

baseline

vgg-16

googlenet

vision id163 id98
vision id163 id98
vision id163 id98
vision id163 id98
resnet-50
neuraltalk caption flickr-8k lstm

resnet-18

31.1%
31.5%
30.4%
24.0%
16.8

dsd
30.0%
27.2%
29.3%
23.2%
18.5

abs.   
rel.   
imp.
imp.
1.1%
3.6%
4.3% 13.7%
3.7%
1.1%
3.5%
0.9%
10.1%
1.7

open sourced dsd model zoo: https://songhan.github.io/dsd

the beseline results of alexnet, vgg16, googlenet, squeezenet are from caffe model zoo. resnet18, resnet50 are from fb.resnet.torch.

compression

acceleration

id173

134

dsd is general purpose:  

vision, speech, natural language

[han et al. iclr 2017]

network

domain

dataset

type

baseline

vgg-16

resnet-18

googlenet

vision id163 id98
vision id163 id98
vision id163 id98
vision id163 id98
resnet-50
neuraltalk caption flickr-8k lstm
id56
deepspeech speech wsj   93
id56
deepspeech-2 speech wsj   93

31.1%
31.5%
30.4%
24.0%
16.8
33.6%
14.5%

dsd
30.0%
27.2%
29.3%
23.2%
18.5
31.6%
13.4%

abs.   
rel.   
imp.
imp.
1.1%
3.6%
4.3% 13.7%
3.7%
1.1%
3.5%
0.9%
10.1%
1.7
5.8%
2.0%
7.4%
1.1%

open sourced dsd model zoo: https://songhan.github.io/dsd

the beseline results of alexnet, vgg16, googlenet, squeezenet are from caffe model zoo. resnet18, resnet50 are from fb.resnet.torch.

compression

acceleration

id173

135

https://songhan.github.io/dsd

dsd on id134

baseline:    a  boy 
in  a  red  shirt  is 
climbing  a  rock 
wall. 
sparse:  a  young 
girl is jumping off 
a tree. 

dsd: a young girl 
in  a  pink  shirt  is 
swinging  on  a 
swing.

 

 

baseline: 
  a     
basketball  player  in 
a  red  uniform  is 
playing with a ball. 
sparse: a basketball 
player  in  a  blue 
uniform  is  jumping 
over the goal. 
dsd:  a  basketball 
player  in  a  white 
uniform  is  trying  to 
make a shot.

baseline: 
  two 
dogs  are  playing 
together in a field. 

baseline: a man and 
a  woman  are  sitting 
on a bench. 

sparse:   two dogs 
are  playing  in  a 
field. 

dsd: two dogs are 
p l a y i n g  i n  t h e 
grass.

sparse:  a  man  is 
sitting  on  a  bench 
with his hands in the 
air. 
dsd: a man is sitting 
on  a  bench  with  his 
arms folded.

baseline:   a person in 
a red jacket is riding a  
b i k e  t h r o u g h  t h e   
woods. 
sparse:  a  car  drives 
through a mud puddle. 

dsd:  a  car  drives 
through a forest.

figure 3: visualization of dsd training improves the performance of image captioning.

baseline model: andrej karpathy, neural talk model zoo.
han et al.    dsd: dense-sparse-dense training for deep neural networks   , iclr 2017

the forest from the background. the good performance of dsd training generalizes beyond these
examples, more image caption results generated by dsd training is provided in the supplementary
material.

137

a appendix: more examples of dsd training improves the captions

a. supplementary material: more examples of dsd training improves the performance of 
neuraltalk auto-caption system

generated by neuraltalk (images from flickr-8k test set)
dsd on id134

baseline:  a boy is swimming in a pool. 
sparse: a small black dog is jumping 
into a pool. 
dsd: a black and white dog is swimming 
in a pool.

baseline:   a group of people are 
standing in front of a building. 
sparse: a group of people are standing 
in front of a building. 
dsd: a group of people are walking in a 
park.

baseline:  two girls in bathing suits are 
playing in the water. 
sparse:  two children are playing in the 
sand. 
dsd: two children are playing in the 
sand.

baseline: a man in a red shirt and 
jeans is riding a bicycle down a street. 
sparse: a man in a red shirt and a 
woman in a wheelchair. 
dsd: a man and a woman are riding on 
a street.

baseline:  a group of people sit on a 
bench in front of a building. 
sparse: a group of people are 
standing in front of a building. 
dsd: a group of people are standing 
in a fountain.

baseline: a man in a black jacket and a 
black jacket is smiling. 
sparse: a man and a woman are standing 
in front of a mountain. 
dsd: a man in a black jacket is standing 
next to a man in a black shirt.

baseline:  a group of football players in 
red uniforms. 
sparse:  a group of football players in a 
field. 
dsd: a group of football players in red 
and white uniforms.

baseline: a dog runs through the grass. 
sparse: a dog runs through the grass. 
dsd: a white and brown dog is running 
through the grass.

baseline model: andrej karpathy, neural talk model zoo.

agenda

algorithm

algorithms for    
ef   cient id136

algorithms for
ef   cient training

id136

training

hardware for   

ef   cient id136 

hardware for   

ef   cient training 

hardware

cpus for training

cpus are targeting deep learning 

intel knights landing (2016) 

       7 tflops fp32 
       16gb mcdram    400 gb/s 
       245w tdp 
       29 gflops/w (fp32) 
       14nm process 

knights mill: next gen xeon phi    optimized for deep learning     

intel announced the addition of new vector instructions for deep learning 

(avx512-4vnniw and avx512-4fmaps), october 2016 
slide source: sze et al survey of dnn hardware,  micro   16 tutorial.   
image source: intel, data source: next platform 

image source: intel, data source: next platform 

2 

gpus for training

gpus are targeting deep learning 

nvidia pascal gp100 (2016) 

       10/20 tflops fp32/fp16 
       16gb hbm     750 gb/s 
       300w tdp 
       67 gflops/w (fp16) 
       16nm process 
       160gb/s nv link  

slide source: sze et al survey of dnn hardware,  micro   16 tutorial.   
data source: nvidia

source: nvidia 

3 

gpus for training

nvidia volta gv100 (2017)

    15 fp32 tflops
    120 tensor tflops
    16gb hbm2 @ 900gb/s
    300w tdp
    12nm process
    21b transistors
    die size: 815 mm2
    300gb/s nvlink

data source: nvidia

tensor core 4x4x4 matrix-multiply acc 

what   s new in volta: tensor core

a new instruction that performs 4x4x4 fma mixed-precision operations per clock    
12x increase in throughput for the volta v100 compared to the pascal p100

8  

https://devblogs.nvidia.com/parallelforall/cuda-9-features-revealed/

pascal v.s. volta

tesla v100 tensor cores and cuda 9 deliver up to 9x higher performance for gemm operations.

https://devblogs.nvidia.com/parallelforall/cuda-9-features-revealed/

pascal v.s. volta

left: tesla v100 trains the resnet-50 deep neural network 2.4x faster than tesla p100.    
right: given a target latency per image of 7ms, tesla v100 is able to perform id136 using the 
resnet-50 deep neural network 3.7x faster than tesla p100.

https://devblogs.nvidia.com/parallelforall/cuda-9-features-revealed/

the gv100 sm is partitioned 
into four processing blocks, 
each with:

    8 fp64 cores
    16 fp32 cores
    16 int32 cores
    two of the new mixed-precision 
tensor cores for deep learning
    a new l0 instruction cache
    one warp scheduler
    one dispatch unit
    a 64 kb register file.

https://devblogs.nvidia.com/parallelforall/

cuda-9-features-revealed/

tesla product
gpu

tesla k40
gk110 (kepler) gm200 (maxwell) gp100 (pascal) gv100 (volta)

tesla p100

tesla v100

tesla m40

810/875 mhz

gpu boost clock
peak fp32 tflop/s* 5.04
peak tensor core 
tflop/s*
memory interface
memory size

-

1114 mhz
6.8
-

1480 mhz
10.6
-

1455 mhz
15
120

384-bit gddr5 384-bit gddr5
up to 12 gb

up to 24 gb

4096-bit hbm2
16 gb

4096-bit hbm2
16 gb

tdp
transistors
gpu die size
manufacturing 
process

235 watts
7.1 billion
551 mm  
28 nm

250 watts
8 billion
601 mm  
28 nm

300 watts
15.3 billion
610 mm  
16 nm finfet+ 12 nm ffn

300 watts
21.1 billion
815 mm  

https://devblogs.nvidia.com/parallelforall/cuda-9-features-revealed/

gpu / tpu

https://blogs.nvidia.com/blog/2017/04/10/ai-drives-rise-accelerated-computing-datacenter/

build and train machine learning models on our new google cloud tpus

5/22/17, 8)20 pm

google cloud tpu

cloud tpu delivers up to 180 tera   ops to train and run machine learning models. 

source: google blog

our new cloud tpu delivers up to 180 teraeops to train and run machine learning

149

in an afternoon using just one eighth of a tpu pod.
google cloud tpu

a    tpu pod    built with 64 second-generation tpus delivers up to 11.5 
peta   ops of machine learning acceleration. 
   one of our new large-scale translation models used to take a full day to train 
a    tpu pod    built with 64 second-generation tpus delivers up to 11.5 petaeops of
on 32 of the best commercially-available gpus   now it trains to the same 
machine learning acceleration.
accuracy in an afternoon using just one eighth of a tpu pod.       google blog
introducing cloud tpus
we   re bringing our new tpus to 

google compute engine

cloud

 as 

150

wrap-up

algorithm

algorithms for    
ef   cient id136

algorithms for
ef   cient training

id136

training

hardware for   

ef   cient id136 

hardware for   

ef   cient training 

hardware

future

smart

low latency

privacy

mobility

energy-ef   cient

152

outlook: the focus for computation

pc	era																						mobile-first	era															ai-first	era

computing

mobile	   
computing

brain-inspired   
cognitive		
computing

sundar pichai, google io, 2016

153

thank you!
stanford.edu/~songhan

