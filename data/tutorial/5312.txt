adversarial examples 
and adversarial training

ian goodfellow, sta    research scientist, google brain 

cs 231n, stanford university, 2017-05-30

overview

    what are adversarial examples? 

    why do they happen? 

    how can they be used to compromise machine learning 

systems? 

    what are the defenses? 

    how to use adversarial examples to improve machine 

learning, even when there is no adversary

(goodfellow 2016)

since 2013, deep neural networks have 
matched human performance at...

...recognizing objects 
and faces   .

(szegedy et al, 2014)

(taigmen et al, 2013)

...solving captchas and 
reading addresses...

(goodfellow et al, 2013)

(goodfellow et al, 2013)

and other tasks...

(goodfellow 2016)

adversarial examples

timeline: 
   adversarial classi   cation    dalvi et al 2004: fool spam    lter 
   evasion attacks against machine learning at test time    
biggio 2013: fool neural nets 
szegedy et al 2013: fool id163 classi   ers imperceptibly 
goodfellow et al 2014: cheap, closed form attack

(goodfellow 2016)

turning objects into    airplanes   

(goodfellow 2016)

attacking a linear model

(goodfellow 2016)

not just for neural nets

    linear models 

    id28 

    softmax regression 

    id166s 

    id90 

    nearest neighbors

(goodfellow 2016)

adversarial examples from over   tting

o

x

x

x

x

o

o

o

(goodfellow 2016)

adversarial examples from 

excessive linearity

o

o

x

o

x

x

x

o

o

(goodfellow 2016)

modern deep nets are very 
modern deep nets are very (piecewise) linear

piecewise linear

rectified linear unit 
rectified linear unit

maxout 
maxout

carefully tuned sigmoid
carefully tuned sigmoid

lstm
lstm

google proprietary

(goodfellow 2016)

nearly linear responses in practice

x
a
m

t
f
o
s
 
o
t
 
t
n
e
m
u
g
r
a

(goodfellow 2016)

small inter-class distances

clean 
example

perturbation corrupted 

example

perturbation changes the true 
class

random perturbation does not 
change the class

perturbation changes the input 
to    rubbish class   

all three perturbations have l2 norm 3.96
this is actually small. we typically use 7!

(goodfellow 2016)

the fast gradient sign method

(goodfellow 2016)

maps of adversarial and random 

cross-sections

(collaboration with david warde-farley and nicolas papernot)

(goodfellow 2016)

maps of adversarial cross-sections

(goodfellow 2016)

maps of random cross-sections

adversarial examples 

are not noise

(collaboration with david warde-farley and nicolas papernot)

(goodfellow 2016)

estimating the subspace 

dimensionality

(tram  r et al, 2017)

(goodfellow 2016)

clever hans

(   clever hans, 

clever 

algorithms,    
bob sturm)

(goodfellow 2016)

wrong almost everywhere

(goodfellow 2016)

adversarial examples for rl

(huang et al., 2017)

(goodfellow 2016)

high-dimensional linear models

clean examples

adversarial 

weights

signs of weights

(goodfellow 2016)

linear models of id163

(andrej karpathy,    breaking linear classifiers on id163   )

(goodfellow 2016)

rbfs behave more intuitively

(goodfellow 2016)

cross-model, cross-dataset 

generalization

(goodfellow 2016)

cross-technique transferability

(papernot 2016)

(goodfellow 2016)

transferability attack

target model with 
unknown weights, 
machine learning 
algorithm, training 
set; maybe non-
di   erentiable
deploy adversarial 
examples against the 
target; transferability 
property results in them 

succeeding

train your 
own model

substitute model 
mimicking target 
model with known, 
di   erentiable function

adversarial crafting 
against substitute

adversarial 
examples

(goodfellow 2016)

cross-training data transferability

strong

weak

intermediate

(papernot 2016)

(goodfellow 2016)

enhancing transfer with 

ensembles

(liu et al, 2016)

(goodfellow 2016)

adversarial examples in the 

human brain

these are 
concentric 
circles, 

not 

intertwined 

spirals. 

(pinna and gregory, 2002)

(goodfellow 2016)

practical attacks

    fool real classi   ers trained by remotely hosted api 

(metamind, amazon, google) 

    fool malware detector networks 

    display adversarial examples in the physical world 

and fool machine learning systems that perceive 
them through a camera

(goodfellow 2016)

adversarial examples in the 

physical world

(kurakin et al, 2016)

(goodfellow 2016)

failed defenses

generative 
pretraining

adding noise 
at test time

removing perturbation 
with an autoencoder

ensembles

con   dence-reducing 

perturbation at test time

error correcting 

codes

multiple glimpses

weight decay

various 

non-linear units

double backprop
dropout

adding noise 
at train time

(goodfellow 2016)

generative modeling is not 
su   cient to solve the problem

(goodfellow 2016)

universal approximator 

universal approximator theorem

theorem

neural nets can represent either function:

neural nets can represent either function:

maximum likelihood doesn   t cause them to 
learn the right function. but we can fix that...

maximum likelihood doesn   t cause them to learn 
the right function. but we can fix that...

(goodfellow 2016)

google proprietary

training on adversarial examples

100

10 1

10 2

e
t
a
r

n
o
i
t
a
c
   
i
s
s
a
l
c
s
i

m

t
s
e
t

train=clean, test=clean
train=clean, test=adv
train=adv, test=clean
train=adv, test=adv

0

50

100

150

200

250

300

training time (epochs)

(goodfellow 2016)

adversarial training of other 

models

    linear models: id166 / id75 cannot learn 
a step function, so adversarial training is less useful, 
very similar to weight decay 

    id92: adversarial training is prone to over   tting. 

    takeway: neural nets can actually become more 
secure than other models. adversarially trained 
neural nets have the best empirical success rate on 
adversarial examples of any machine learning model.

(goodfellow 2016)

weaknesses persist

(goodfellow 2016)

adversarial training

labeled as bird

still has same label (bird)

decrease 
id203 
of bird class

(goodfellow 2016)

virtual adversarial training
new guess should 
unlabeled; model 
match old guess 
guesses it   s probably 
a bird, maybe a plane

(probably bird, maybe plane)

adversarial 
perturbation 
intended to 

change the guess

(goodfellow 2016)

text classi   cation with vat

rcv1 misclassi   cation rate

8.00

7.50

7.00

6.50

6.00

earlier sota

sota

our baseline adversarial

zoomed in for legibility

virtual 
adversarial

both

both +  

bidirectional model

(goodfellow 2016)

6.686.977.057.127.407.207.70universal engineering machine (model-based optimization)

make new inventions 
by finding input 
that maximizes 
model   s predicted 
performance

training data extrapolation

(goodfellow 2016)

conclusion

    attacking is easy 

    defending is di   cult 

    adversarial training provides id173 and 

semi-supervised learning 

    the out-of-domain input problem is a bottleneck for 

model-based optimization generally

(goodfellow 2016)

cleverhans

open-source library available at: 

https://github.com/openai/cleverhans 

built on top of tensorflow (theano support anticipated) 
standard implementation of attacks, for adversarial training 
and reproducible benchmarks

(goodfellow 2016)

