5
1
0
2

 

n
u
j
 

4
2

 
 
]

a
n
.
s
c
[
 
 

1
v
0
4
5
7
0

.

6
0
5
1
:
v
i
x
r
a

globally optimal positively homogeneous factorizations

global optimality in tensor factorization, deep learning, and

benjamin d. haeffele
ren  e vidal
department of biomedial engineering
johns hopkins university
baltimore, md 21218, usa

beyond

abstract

bhaeffele@jhu.edu
rvidal@jhu.edu

techniques involving factorization are found in a wide range of applications and have enjoyed
signi   cant empirical success in many    elds. however, common to a vast majority of these problems
is the signi   cant disadvantage that the associated optimization problems are typically non-convex
due to a multilinear form or other convexity destroying transformation. here we build on ideas from
convex relaxations of id105s and present a very general framework which allows for
the analysis of a wide range of non-convex factorization problems - including id105,
tensor factorization, and deep neural network training formulations. we derive suf   cient conditions
to guarantee that a local minimum of the non-id76 problem is a global minimum
and show that if the size of the factorized variables is large enough then from any initialization
it is possible to    nd a global minimizer using a purely local descent algorithm. our framework
also provides a partial theoretical justi   cation for the increasingly common use of recti   ed linear
units (relus) in deep neural networks and offers guidance on deep network architectures and
id173 strategies to facilitate ef   cient optimization.

1. introduction

models involving factorization or decomposition are ubiquitous across a wide variety of technical
   elds and application areas. as a simple example relevant to machine learning, various forms of
id105 are used in classical id84 techniques such as principle
component analysis (pca) and in more recent methods like non-negative id105 or
dictionary learning (lee and seung, 1999; aharon et al., 2006; mairal et al., 2010).
in a typical
id105 problem, we might seek to    nd matrices (u, v ) such that the product u v t
closely approximates a given data matrix y while at the same time requiring that u and v satisfy
certain properties (e.g., non-negativity, sparseness, etc.). this naturally leads to an optimization
problem of the form

   (y, u v t ) +   (u, v )

min
u,v

(1)

where     is some function that measures how closely y is approximated by u v t and    is a regu-
larization function to enforce the desired properties in u and v . unfortunately, aside from a few
special cases (e.g., pca), a vast majority of id105 models suffer from the signi   -
cant disadvantage that the associated optimization problems are non-convex and very challenging
to solve. for example, in (1) even if we choose   (u, v ) to be jointly convex in (u, v ) and    (y, x)
to be a convex function in x, the optimization problem is still typically a non-convex problem in
(u, v ) due to the composition with the bilinear form x = u v t .

given this challenge, a common approach is to relax the non-convex factorization problem into
a problem which is convex on the product of the factorized matrices, x = u v t . as a concrete

1

haeffele and vidal

example, in low-rank id105, one might be interested in solving a problem of the form

   (y, u v t ) subject to rank(u v t )     r

min
u,v

(2)

where the rank constraint can be easily enforced by limiting the number of columns in the u and v
matrices to be less than or equal to r. however, aside from a few special choices of    , solving (2)
is a np-hard problem in general. instead, one can relax (2) into a fully convex problem by using a
convex id173 that promotes low-rank solutions, such as the nuclear norm kxk   , and then
solve
(3)

min
x

   (y, x) +   kxk   

which can be done ef   ciently if    (y, x) is convex with respect to x (cai et al., 2008; recht et al.,
2010). given a solution to (3), xopt, it is then simple to    nd a low-rank factorization u v t = xopt
via a singular value decomposition. unforunately, however, while the nuclear norm provides a nice
convex relaxation for low-rank id105 problems, nuclear norm relaxation does not
capture the full generality of problems such as (1) as it does not necessarily ensure that xopt can
be    ef   ciently    factorized as xopt = u v t for some (u, v ) pair which has the desired properties
encouraged by   (u, v ) (sparseness, non-negativity, etc.), nor does it provide a means to    nd the
desired factors. to address these issues, in this paper we consider the task of solving non-convex
optimization problems directly in the factorized space and use ideas inspired from the convex re-
laxation of id105s as a means to analyze the non-convex factorization problem. our
framework includes problems such as (1) as a special case but also applies much more broadly to a
wide range of non-id76 problems; several of which we describe below.

1.1 generalized factorization

more generally, tensor factorization models provide a natural extension to id105
and have been employed in a wide variety of applications (cichocki et al., 2009; kolda and bader,
2009). the resulting optimization problem is similar to id105, with the difference that
we now consider more general factorizations which decompose a multidimensional tensor y into
a set of k different factors (x 1, . . . , x k ), where each factor is also possibly a multidimensional
tensor. these factors are then combined via an arbitrary multilinear mapping   (x 1, . . . , x k )     y ;
i.e.,    is a linear function in each x i term if the other x j, i 6= j terms are held constant. this model
then typically gives optimization problems of the form

min

x 1,...,xk

   (y,   (x 1, . . . , x k )) +   (x 1, . . . , x k )

(4)

where again     might measure how closely y is approximated by the tensor   (x 1, . . . , x k ) and   
encourages the factors (x 1, . . . , x k ) to satisfy certain requirements. clearly, (4) is a generalization
of (1) by taking (x 1, x 2) = (u, v ) and   (u, v ) = u v t , and similar to id105, the
optimization problem given by (4) will typically be non-convex regardless of the choice of    and    
functions due to the multilinear mapping   .

while the tensor factorization framework is very general with regards to the dimensionalities of
the data and the factors, a tensor factorization usually implies the assumption that the mapping   
from the factorized space to the output space (the codomain of   ) is multilinear. however, if we
consider more general mappings from the factorized space into the output space (i.e.,    mappings

2

globally optimal positively homogeneous factorizations

which are not restricted to be multilinear) then we can capture a much broader array of models
in the    factorized model    family. for example, in deep neural network training the output of the
network is typically generated by performing an alternating series of a linear function followed by
a non-linear function. more concretely, if one is given training data consisting of n data points of d
dimensional data, v     rn  d, and an associated vector of desired outputs y     rn , the goal then is
to    nd a set of network parameters (x 1, . . . , x k ) by solving an optimization problem of the form
(4) using a mapping

  (x 1, . . . , x k ) =   k (  k   1(. . .   2(  1(v x 1)x 2) . . . x k   1)x k )

(5)

where each x i factor is an appropriately sized matrix and the   i(  ) functions apply some form of
non-linearity after each id127, e.g., a sigmoid function, recti   cation, max-pooling.
note that although here we have shown the linear operations to be simple id127s
for notational simplicity, this is easily generalized to other linear operators (e.g., in a convolutional
network each linear operator could be a set of convolutions with a group of various kernels with
parameters contained in the x i variables).

1.2 paper contributions

our primary contribution is to extend ideas from convex id105 and present a general
framework which allows for a wide variety of factorization problems to be analyzed within a convex
formulation. speci   cally, using this convex framework we are able to show that local minima of the
non-convex factorization problem achieve the global minimum if they satisfy a simple condition.
further, we also show that if the factorization is done with factorized variables of suf   cient size,
then from any initialization it is always possible to reach a global minimizer using purely local
descent search strategies.

two concepts are key to our analysis framework: 1) the size of the factorized elements is not
constrained, but instead    t to the data through id173 (for example, the number of columns
in u and v is allowed to change in id105) 2) we require that the mapping from the
factorized elements to the    nal output,   , satis   es a positive homogeneity property. interestingly,
the deep learning    eld has increasingly moved to using non-linearities such as recti   ed linear
units (relu) and max-pooling, both of which satisfy the positive homogeneity property, and it has
been noted empirically that both the speed of training the neural network and the overall perfor-
mance of the network is increased signi   cantly when relu non-linearities are used instead of the
more traditional hyperbolic tangent or sigmoid non-linearities (dahl et al., 2013; maas et al., 2013;
krizhevsky et al., 2012; zeiler et al., 2013). we suggest that our framework provides a partial the-
oretical explanation to this phenomena and also offers directions of future research which might be
bene   cial in improving the performance of multilayer neural networks.

2. prior work

despite the signi   cant empirical success and wide ranging applications of the models discussed
above (and many others not discussed), as we have mentioned, a vast majority of the above tech-
niques models suffer from the signi   cant disadvantage that the associated optimization problems
are non-convex and very challenging to solve. as a result, the numerical optimization algorithms
often used to solve factorization problems     including (but certainly not limited to) alternating mini-
mization, id119, stochastic id119, block coordinate descent, back-propagation,

3

haeffele and vidal

and quasi-id77s     are typically only guaranteed to converge to a critical point or local
minimum of the objective function (mairal et al., 2010; rumelhart et al., 1988; ngiam et al., 2011;
wright and nocedal, 1999; xu and yin, 2013). the nuclear norm relaxation of low-rank matrix
factorization discussed above provides a means to solve factorization problems with reglarization
promoting low-rank solutions1, but it fails to capture the full generality of problems such as (1)
as it does not allow one to    nd factors, (u, v ), with the desired properties encouraged by   (u, v )
(sparseness, non-negativity, etc.). to address this issue, several studies have explored a more general
convex relaxation via the matrix norm given by

r

kxku,v     inf
r   n+

inf

u,v :u v t =x

    inf
r   n+

inf

u,v :u v t =x

r

xi=1
xi=1

kuikukvikv

(6)

1

2 (kuik2

u + kvik2
v)

where (ui, vi) denotes the i   th columns of u and v , k    ku and k    kv are arbitrary vector norms,
and the number of columns (r) in the u and v matrices is allowed to be variable (bach et al.,
2008; bach, 2013; haeffele et al., 2014). the norm in (6) has appeared under multiple names in
the literature, including the projective tensor norm, decomposition norm, and atomic norm, and
by replacing the column norms in (6) with gauge functions the formulation can be generalized to
incorporate additional id173 on (u, v ), such as non-negativity, while still being a convex
function of x (bach, 2013). further, it is worth noting that for particular choices of the k    ku
and k    kv vector norms, kxku,v reverts to several well known matrix norms and thus provides a
generalization of many commonly used regularizers. notably, when the vector norms are both l2
norms, kxk2,2 = kxk   , and the form in (6) is the well known variational de   nition of the nuclear
norm.
the k    ku,v norm has the appealing property that by an appropriate choice of vector norms k    ku
and k    kv (or more generally gauge functions), one can promote desired properties in the factorized
matrices (u, v ) while still working with a problem which is convex w.r.t. the product x = u v t .
based on this concept, several studies have explored optimization problems over factorized matrices
(u, v ) of the form

   (u v t ) +   ku v t ku,v

min
u,v

(7)

even though the problem is still non-convex w.r.t. the factorized matrices (u, v ), it can be shown
using ideas from burer and monteiro (2005) on factorized semide   nite programming that, subject
to a few general conditions, then local minima of (7) will be global minima (bach et al., 2008;
haeffele et al., 2014), which can signi   cantly reduce the dimensionality of some large scale opti-
mization problems. unfortunately, aside from a few special cases, the norm de   ned by (6) (and
related id173 functions such as those discussed by bach (2013)) cannot be evaluated ef   -
ciently, much less optimized over, due to the complicated and non-convex nature of the de   nition.
as a result, in practice one is often forced to replace (7) by the closely related problem

   (u v t ) +   

min
u,v

r

xi=1

kuikukvikv = min
u,v

   (u v t ) +   

r

xi=1

1

2 (kuik2

u + kvik2
v)

(8)

1. similar convex relaxation techniques have also been proposed for low-rank tensor factorizations, but in the case
of tensors    nding a    nal factorization xopt =   (x 1, . . . , x k ) from a low-rank tensor can still be a challenging
problem (tomioka et al., 2010; gandy et al., 2011)

4

globally optimal positively homogeneous factorizations

however, (7) and (8) are not equivalent problems, due to the fact that solutions to (7) include
any factorization (u, v ) such that their product equals the optimal solution, u v t = xopt, while
in (7) one is speci   cally searching for a factorization (u, v ) that achieves the in   mum in (6); in
brief, solutions to (8) will be solutions to (7), but the converse is not true. as a consequence,
results guaranteeing that local minima of the form (7) will be global minima cannot be applied to
the formulation in (8), which is typically more useful in practice. here we focus our analysis on
the more commonly used family of problems, such as (8), and show that similar guarantees can be
provided regarding the global optimality of local minima. additionally, we show that these ideas can
be signi   cantly extended to a very wide range of non-convex models and id173 functions,
with applications such as tensor factorization and certain forms of neural network training being
additional special cases of our framework.

in the context of neural networks, bengio et al. (2005) showed that for neural networks with
a single hidden layer, if the number of neurons in the hidden layer is not    xed, but instead    t to
the data through a sparsity inducing id173, then the process of training a globally optimal
neural network is analgous to selecting a    nite number of hidden units from the in   nite dimensional
space of all possible hidden units and taking a weighted summation of these units to produce the
output. further, these ideas have very recently been used to analyze the generalization performance
of such networks (bach, 2014). here, our results take a similar approach and extend these ideas
to certain forms of multi-layer neural networks. additionally, our framework provides suf   cient
conditions on the network architecture to guarantee that from any intialization a globally optimal
solution can be found by performing purely local descent on the network weights.

3. preliminaries

before we present our main results, we    rst describe our notation system and recall a few de   nitions.

3.1 notation

x     rd for d = d1    . . .    dn ; we also denote the cardinality of d as card(d) = qn

our formulation is fairly general in regards to the dimensionality of the data and factorized variables.
as a result, to simplify the notation, we will use capital letters as a shorthand for a set of dimensions,
and individual dimensions will be denoted with lower case letters. for example, x     rd1  ...  dn    
i=1 di.
similarly, x     rd  r     x     rd1  ...  dn  r1  ...  rm for d = d1   . . .  dn and r = r1   . . .  rm .
given an element from a tensor space, we will use a subscript to denote a slice of the tensor
along the last dimension. for example, given a matrix x     rd1  r, then xi     rd
1, i     {1, . . . , r},
denotes the i   th column of x. similarly, given a cube x     rd1  d2  r then xi     rd1  d2, i    
{1 . . . , r}, denotes the i   th slice along the third dimension. further, given two tensors with matching
dimensions except for the last dimension, x     rd  rx and y     rd  ry, we will use [x y ]    
rd  (rx+ry) to denote the concatenation of the two tensors along the last dimension.

we denote the dot product between two elements from a tensor space (x     rd, y     rd) as
hx, yi = vec(x)t vec(y), where vec(  ) denotes    attening the tensor into a vector. for a function
  (x), we denote its image as im(  ) and its fenchel dual as      (x)     supz hx, zi     (z). the gradient
of a differentiable function   (x) is denoted      (x), and the subgradient of a convex (but possibly
non-differentiable) function   (x) is denoted      (x). for a differentiable function with multiple
variables   (x1, . . . , xk), we will use    xi  (x1, . . . , xk) to denote the portion of the gradient corre-

5

haeffele and vidal

sponding to xi. the space of non-negative real numbers is denoted r+, and the space of positive
integers is denoted n+.

3.2 de   nitions

we now make/recall a few general de   nitions and well known facts which will be used in our
analysis.
de   nition 1 a size-r set of k factors (x 1, . . . , x k )r is de   ned to be a set of k tensors where the
   nal dimension of each tensor is equal to r. this is to be interpreted (x 1, . . . , x k )r     r(d1  r)   
. . .    r(dk  r).
de   nition 2 the indicator function of a set c is de   ned as

  c (x) =(cid:26) 0

x     c
    x /    c

(9)

de   nition 3 a function    : rd1
  (  x1, . . . ,   xn ) =   p  (x1, . . . , xn ),           0.

   . . .    rdn     rd is positively homogeneous with degree p if

note that this de   nition also implies that   (0, . . . , 0) = 0 for p 6= 0.

de   nition 4 a function    : rd1
and   (x1, . . . , xn )     0,    (x1, . . . , xn ).

   . . .    rdn     r+ is positive semide   nite if   (0, . . . , 0) = 0

de   nition 5 the one-sided directional derivative of a function   (x) at a point x in the direction z
is denoted   (x)(z) and de   ned as d  (x)(z)     lim    0 (  (x +   z)       (x))     1.
also, recall that for a differentiable function   (x), d  (x)(z) = h     (x), zi.

4. problem formulation

returning to the motivating example from the introduction (4), we now de   ne the family of mapping
functions from the factors into the output space and the family of id173 functions on the
factors (   and   , respectively) which we will study in our framework.

4.1 factorization mappings

in this paper, we consider mappings    which are based on a sum of what we refer to as an elemental
mapping. speci   cally, if we are given a size-r set of k factors (x 1, . . . , x k )r, the elemental
mapping    : rd1    . . .    rdk     rd takes a slice along the last dimension from each tensor in
the set of factors and maps it into the output space. we then de   ne the full mapping to be the sum
of these elemental mappings along each of the r slices in the set of factors. the only requirement
we impose on the elemental mapping is that it must be positively homogeneous. more formally,
de   nition 6 an elemental mapping,    : rd1    . . .    rdk     rd is any mapping which is
positively homogeneous with degree p 6= 0. the r-element factorization mapping   r : r(d1  r)   
. . .    r(dk  r)     rd is de   ned as

  r(x 1, . . . , x k ) =

r

xi=1

6

  (x 1

i , . . . , x k

i ).

(10)

globally optimal positively homogeneous factorizations

as we do not place any restrictions on the elemental mapping,   , beyond the requirement that it
must be positively homogeneous, there are a wide range of problems that can be captured by a
mapping with form (10). several example problems which can be placed in this framework include:

id105: the elemental mapping,    : rd1    rd2     rd1  d2

  (u, v) = uvt

(11)

is positively homogeneous with degree 2 and   r(u, v ) = pr

multiplication for matrices with r columns.

tensor decomposition - candecomp/parafac (cp): slightly more generally, the elemen-

tal mapping    : rd1    . . .    rdk     rd1  ...  dk

  (x1, . . . , xk) = x1                  xk

(12)

i=1 uiv t

i = u v t is simply matrix

(where     denotes the tensor outer product) results in   r(x 1, . . . , x k ) being the mapping used in
the rank-r candecomp/parafac (cp) tensor decomposition model (kolda and bader, 2009).
further, instead of choosing    to be a simple outer product, we can also generalize this to be any
multilinear function of the factors (x 1

i , . . . , x k

i )2.

neural networks with recti   ed linear units (relu): let   +(x)     max{x, 0} be the linear
recti   cation function, which is applied element-wise to a tensor x of arbitrary dimension. then if
we are given a matrix of training data v     rn  d1, the elemental mapping   (x1, x2) : rd1    rd2    
rn  d2

  (x1, x2) =   +(v x1)(x2)t

(13)

results in a mapping   r(x 1, x 2) =   +(v x 1)(x 2)t , which can be interpreted as producing the d2
outputs of a 3 layer neural network with r hidden units in response to the input of n data points of
d1 dimensional data, v . the hidden units have a relu non-linearity; the other units are linear; and
the (x 1, x 2)     rd1  r    rd2  r matrices contain the connection weights from the input-to-hidden
and hidden-to-output layers, respectively.

by utilizing more complicated de   nitions of   , it is possible to consider a broad range of neural
network architectures. as a simple example of networks with multiple hidden layers, an elemental
mapping such as    : rd1  d2    rd2  d3    rd3  d4    rd4  d5     rn  d5

  (x1, x2, x3, x4) =   +(  +(  +(v x1)x2)x3)x4

(14)

gives a   r(x 1, x 2, x 3, x 4) mapping which is the output of a 5 layer neural network in response
to the inputs in the v     rn  d1 matrix with relu non-linearities on all of the hidden layer units.
in this case, the network has the architecture that there are r, 4 layer fully-connected subnetworks,
with each subnetwork having the same number of units in each layer as de   ned by the dimensions
{d2, d3, d4}. the r subnetworks are all then fed into a fully connected linear layer to produce the
output.

more general still, since any positively homogenous transformation is a potential elemental
mapping, by an appropriate de   nition of   , one can describe neural networks with very general

2. we note that more general tensor decompositions, such as the general form of the tucker decomposition, do not
explicitly    t inside the framework we describe here; however, by using similar arguments to the ones we develop
here, it is possible to show analogous results to those we derive in this paper for more general tensor decompositions,
which we do not show for clarity of presentation.

7

haeffele and vidal

architectures, provided the non-linearities in the network are compatible with positive homogene-
ity. note that max-pooling and recti   cation are both positively homogeneous and thus fall within
our framework. for example, the well-known id163 network from (krizhevsky et al., 2012),
which consists of a series of convolutional layers, linear-recti   cation, max-pooling layers, response
id172 layers, and fully connected layers, can be described by taking r = 1 and de   ning
   to be the entire transformation of the network (with the removal of the response id172
layers, which are not positively homogenous). note, however, that our results will rely on r poten-
tially changing size or being initialized to be suf   ciently large, which limits the applicability of our
results to current state-of-the-art network architectures (see discussion).

here we have provided a few examples of common factorization mappings that can be cast
in form (10), but certainly there are a wide variety of other problems for which our framework is
relevant. additionally, while all of the mappings described above are positively homogeneous with
degree equal to the degree of the factorization (k), this is not a requirement; p 6= 0 is suf   cient. for
example, non-linearities such as a recti   cation followed by raising each element to a non-zero power
are positively homogeneous but of a possibly different degree. what will turn out to be essential,
however, is that we require p to match the degree of positive homogeneity used to regularize the
factors, which we will discuss in the next section.

4.2 factorization id173

inspired by the ideas from structured convex id105, instead of trying to analyze the
optimization over a size-r set of k factors (x 1, . . . , x k )r for a    xed r, we instead consider the
optimization problem where r is possibly allowed to vary and adapted to the data through regular-
ization. to do so, we will de   ne a id173 function similar to the k    ku,v norm discussed
in id105 which is convex with respect to the output tensor but which still allows for
id173 to be placed on the factors. similar to our de   nition in (10), we will begin by    rst
   . . .    rdk     r+         which takes as in-
de   ning an elemental id173 function g : rd1
put slices of the factorized tensors along the last dimension and returns a non-negative number. the
requirements we place on g are that it must be positively homogeneous and positive semide   nite.
formally,

de   nition 7 we de   ne an elemental id173 function g : rd1    . . .    rdk     r+        ,
to be any function which is positive semide   nite and positively homogeneous.

again, due to the generality of the framework, there are a wide variety of possible elemental
id173 functions. we highlight two positive semide   nite, positively homogeneous functions
which are commonly used and note that functions can be composed with summations, multiplica-
tions, and raising to non-zero powers to change the degree of positive homogeneity and combine
various functions.

norms: any norm kxk is positively homogeneous with degree 1. note that because we make
no requirement of convexity on g, this framework can also include functions such as the lq pseudo-
norms for q     (0, 1).

conic indicators: the indicator function   c(x) of any conic set c is positively homogeneous
for all degrees. recall that a conic set, c, is simply any set such that if x     c then   x     c,           0.
a few popular conic sets which can be of interest include the non-negative orthant rd
+, the kernel
of a linear operator {x : ax = 0}, inequality constraints for a linear operator {x : ax     0},

8

globally optimal positively homogeneous factorizations

and the set of positive semide   nite matrices. constraints on the non-zero support of x are also
typically conic sets. for example, the set {x : kxk0     n} is a conic set, where kxk0 is simply the
number of non-zero elements in x and n is a positive integer. more abstractly, conic sets can also
be used to enforce invariances w.r.t. positively homogeneous transformations. for example, given
two positively homogeneous functions   (x),      (x) with equal degrees of positive homogeneity, the
sets {x :   (x) =      (x)} and {x :   (x)          (x)} are also conic sets.

a few typical formulations of a g which are positively homogeneous with degree k might

include:

k

g(x1, . . . , xk ) =

kxik(i)

g(x1, . . . , xk ) = 1
k

kxikk
(i)

k

yi=1
xi=1
yi=1

k

g(x1, . . . , xk ) =

(kxik(i) +   ci(xi))

(15)

(16)

(17)

where all of the norms, k    k(i), are arbitrary. forms (15) and (16) can be shown to be equivalent, in
the sense that they give rise to the same      ,g function, for all of the example mappings    we have
discussed here and by an appropriate choice of norm can induce various properties in the factorized
elements (such as sparsity), while form (17) is similar but additionally constrains each factor to be
an element of a conic set ci (see bach et al., 2008; bach, 2013; haeffele et al., 2014, for examples
from id105).

to de   ne our id173 function on the output tensor, x =   (x 1, . . . , x k ), it will be
necessary that the elemental id173 function, g, and the elemental mapping,   , satisfy a few
properties to be considered    compatible    for the de   nition of our id173 function. speci   -
cally, we will require the following de   nition.
de   nition 8 given an elemental mapping    and an elemental id173 function g, will we say
that (  , g) are a nondegenerate pair if 1) g and    are both positively homogeneous with degree p, for
some p 6= 0 and 2)    x     im(  )\0,           (0,    ] and (  z1, . . . ,   zk ) such that   (  z1, . . . ,   zk ) = x,
g(  z1, . . . ,   zk ) =   , and g(z1, . . . , zk )        for all (z1, . . . , zk ) such that   (z1, . . . , zk ) = x.3

from this, we now de   ne our main id173 function:

de   nition 9 given an elemental mapping    and an elemental id173 function g such that
(  , g) are a nondegenerate pair, we de   ne the factorization id173 function,      ,g(x) :
rd     r+         to be

r

     ,g(x)     inf
r   n+

inf

(x 1,...,xk )r

g(x 1

i , . . . , x k
i )

(18)

xi=1

subject to   r(x 1, . . . , x k ) = x

3. property 1 from the de   nition of a nondegenerate pair will be critical to our formulation. several of our results can
be shown without property 2, but property 2 is almost always satis   ed for most interesting choices of (  , g) and is
designed to avoid    pathological         ,g functions (such as      ,g(x) = 0    x). for example, in id105
with   (u, v) = uvt , taking g(u, v) =   c (u)kvk2 for any arbitrary norm and conic set c satis   es property 1 but not
property 2, as we can always reduce the value of g(u, v) by scaling v by a constant        (0, 1) and scaling u by      1
without changing the value of   (u, v).

9

haeffele and vidal

with the additional condition that      ,g(x) =     if x /   sr im(  r).

we will show that      ,g is a convex function of x and that in general the in   mum in (18) can
always be achieved with a    nitely sized factorization (i.e., r does not need to approach    )4. while
     ,g suffers from many of the practical issues associated with the matrix norm k    ku,v discussed
earlier (namely that in general it cannot be evaluated in polynomial time due to the complicated
de   nition), because      ,g(x) is a convex function on x, this allows us to use      ,g purely as an
analysis tool to derive results for a more tractable factorized formulation.

4.3 problem de   nition

to build our analysis, we will start by de   ning the convex (but typically non-tractable) problem,
given by

min
x,q

f (x, q) =    (x, q) +        ,g(x) + h(q).

(19)

here x     rd is the output of the factorization mapping x =   (x 1, . . . , x k ) as we have been
discussing, and the q term is an optional additional set of non-factorized variables which can be
helpful in modeling some problems (for example, to add intercept terms or to model outliers in the
data). for our analysis we will assume the following:

assumption 1    (x, q) is once differentiable and jointly convex in (x, q)

assumption 2 h(q) is convex (but possibly non-differentiable)

assumption 3 (  , g) are a nondegenerate pair as de   ned by de   nition 8;      ,g(x) is as de   ned
by (18); and    > 0

assumption 4 the minimum of f (x, q) exists =        6= arg minx,q f (x, q).

as noted above, it is typically impractical to optimize over functions involving      ,g(x), and,
even if one were given an optimal solution to (19), xopt, one would still need to solve the problem
given in (18) to recover the desired (x 1, . . . , x k ) factors. therefore, we use (19) merely as analysis
tool and instead tailor our results to the non-id76 problem given by

min

fr(x 1, . . . , x k , q)    

(x 1,...,xk )r,q

   (  r(x 1, . . . , x k ), q) +   

r

xi=1

g(x 1

i , . . . , x k

i ) + h(q).

(20)

we will show in the next section that any local minima of (20) is a global minima if it satis   es
the condition that one slice from each of the factorized tensors is all zero. further, we will also
show that if r is taken to be large enough then from any initialization we can always    nd a global
minimum of (20) by doing an optimization based purely on local descent.

4. in particular, the largest r needs to be is card(d), and we note that card(d) is a worst case upper bound on the
size of the factorization. in certain cases the bound can be shown to be lower. as an example,      ,g(x) = kxk   
when   (u, v) = uvt and g(u, v) = kuk2kvk2. in this case the in   mum can be achieved with r     rank(x)    
min{card(u), card(v)}.

10

globally optimal positively homogeneous factorizations

5. main analysis

we begin our analysis by    rst showing a few simple properties and lemmas relevant to our frame-
work.

5.1 preliminary results

first, from the de   nition of   r it is easy to verify that if    is positively homogeneous with degree
p, then   r is also positively homogeneous with degree p and satis   es the following proposition

proposition 10 given a size-rx set of k factors, (x 1, . . . , x k )rx, and a size-ry set of k factors,
(y 1, . . . , y k)ry , then           0,        0

  (rx+ry)([  x 1   y 1], . . . , [  x k   y k]) =   p  rx(x 1, . . . , x k ) +   p  ry (y 1, . . . , y k )

(21)

where recall, [x y ] denotes the concatenation of x and y along the    nal dimension of the tensor.

further,      ,g(x) satis   es the following proposition:

proposition 11 the function      ,g : rd     r         as de   ned in (18) has the properties

1.      ,g(0) = 0 and      ,g(x) > 0    x 6= 0.

2.      ,g is positively homogeneous with degree 1.

3.      ,g(x + y )          ,g(x) +      ,g(y )    (x, y )

4.      ,g(x) is convex w.r.t. x     rd.

5. the in   mum in (18) can be achieved with r     card(d)    x s.t.      ,g(x) <    .

proof proposition 11 many of these properties can be shown in a similar fashion to results from
the k    ku,v norm discussed previously (bach et al., 2008; yu et al., 2014).

1) by de   nition and the fact that g is positive semide   nite, we always have      ,g(x)     0    x.
trivially,      ,g(0) = 0 since we can always take (x 1, . . . , x k ) = (0, . . . , 0) to achieve the in   -
i ) > 0 for any
(x 1, . . . , x k )r s.t.   r(x 1, . . . , x k ) = x and r    nite. property 5) shows that the in   mum can
be achieved with r    nite, completing the result.

mum. for x 6= 0, because (  , g) is a non-degenerate pair thenpr

i , . . . , x k

i=1 g(x 1

i=1 g(x 1

       ,g,(x).

i , . . . , x k

i=1 g(  1/px 1

2) for all        0 and any (x 1, . . . , x k )r such that x =   r(x 1, . . . , x k ), note that from
i ) =
i ). applying this fact to the de   nition of      ,g gives that      ,g(  x) =

positive homogeneity   r(  1/px 1, . . . ,   1/px k ) =   x and pr
  pr

3) if either      ,g(x) =     or      ,g(y ) =     then the inequality is trivially satis   ed. considering
any (x, y ) pair such that      ,g is    nite for both x and y , for any    > 0 let (x 1, . . . , x k )rx be an
i )    
     ,g(x)+  . similarly, let (y 1, . . . , y k)ry be an    optimal factorization of y . from proposition 10
i ) +

   optimal factorization of x. speci   cally,   rx(x 1, . . . , x k ) = x and prx
we have   rx+ry ([x 1 y 1], . . . , [x k y k ]) = x + y , so      ,g(x + y )    prx
pry

j=1 g(y 1
4) convexity is given by the combination of properties 2 and 3. further, note that properties 2

j )          ,g(x) +      ,g(y ) + 2  . letting    tend to 0 completes the result.

i , . . . ,   1/px k

and 3 also show that {x     rd :      ,g(x) <    } is a convex set.

j , . . . , x k

i , . . . , x k

i , . . . , x k

i=1 g(x 1

i=1 g(x 1

11

haeffele and vidal

5) let        rd be de   ned as

   = {x :    (x1, . . . , xk ),   (x1, . . . , xk) = x, g(x1, . . . , xk )     1}

(22)

note that because (  , g) is a nondegenerate pair, for any non-zero x        there exists        [1,    )
such that   x is on the boundary of   , so    and its convex hull are compact sets.

further, note that    contains the origin by de   nition of    and g, so as a result,      ,g is equivalent

to a gauge function on the convex hull of   

     ,g(x) = inf
  

{   :        0, x        conv(  )}

(23)

since the in   mum w.r.t.    is linear and constrained to a compact set, it must be achieved. therefore,
i , . . . , z k
i ) :
i ) and      ,g(x) =   opt.
the result as we can take

there must exist   opt     0, {       rcard(d) :   i     0    i, pcard(d)

i )       }
i=1
combined with positive homogeneity,
i , . . . , (  opt  i)1/pz k
i ) = ((  opt  i)1/pz 1

such that x =   optpcard(d)

i , . . . , z k
this,
i , . . . , x k

  i = 1}, and {(z 1

  i  (z 1
completes

i ), which gives

i , . . . , z k

card(d)
i=1

  (z 1

(x 1

i=1

  opt =      ,g(x)    

card(d)

xi=1

g(x 1

i , . . . , x k

i ) =   opt

card(d)

xi=1

  ig(z 1

i , . . . , z k

i )       opt

card(d)

xi=1

  i =   opt

(24)

and shows that a factorization of size-card(d) which achieves the in   mum must exist.

we next derive the fenchel dual of      ,g, which will provide a useful characterization of the

subgradient of      ,g.

proposition 12 the fenchel dual of      ,g(x) is given by

        ,g(w ) =(cid:26) 0         ,g(w )     1

    otherwise

where

        ,g(w )     sup

(z1,...,zk )(cid:10)w,   (z1, . . . , zk )(cid:11)

subject to g(z1, . . . , zk )     1

(25)

(26)

proof recall,         ,g(w )     supz hw, zi          ,g(z), so for z to approach the supremum we must

have z    sr im(  r). as result, the problem is equivalent to

        ,g(w ) = sup
r   n+

sup

(z 1,...,z k )r(cid:10)w,   r(z 1, . . . , z k )(cid:11)    

r

= sup
r   n+

sup

(z 1,...,z k )r

xi=1(cid:2)(cid:10)w,   (z 1

i , . . . , z k

r

g(z 1

xi=1
i )(cid:11)     g(z 1

i , . . . , z k
i )

i , . . . , z k

i )(cid:3)

(27)

(28)

if         ,g(w )     1 then all the terms in the summation of (28) will be non-positive, so taking
(z 1, . . . , z k) = (0, . . . , 0) will achieve the supremum. conversely, if         ,g(w ) > 1, then

12

globally optimal positively homogeneous factorizations

   (z1, . . . , zk ) such that (cid:10)w,   (z1, . . . , zk )(cid:11) > g(z1, . . . , zk ). this result, combined with the

positive homogeneity of    and g gives that (28) is unbounded by considering (  z1, . . . ,   zk ) as
          .

we brie   y note that the optimization problem associated with (26) is typically referred to as the
polar problem and is a generalization of the concept of a dual norm. in practice solving the polar
can still be very challenging and is often the limiting factor in applying our results in practice (see
bach, 2013; zhang et al., 2013, for further information).

with the above derivation of the fenchel dual, we now recall that if      ,g(x) <     then the sub-
gradient of      ,g(x) can be characterized by         ,g(x) = {w : hx, w i =      ,g(x) +         ,g(w )}.
this forms the basis for the following lemma which will be used in our main results

lemma 13 given a factorization x =   r(x 1, . . . , x k ) and a id173 function      ,g(x),
then the following conditions are equivalent:

i=1 g(x 1

i ) =      ,g(x)

i , . . . , x k
i=1 g(x 1

1. (x 1, . . . , x k ) is an optimal factorization of x; i.e.,pr
2.    w such that         ,g(w )     1 and(cid:10)w,   r(x 1, . . . , x k )(cid:11) =pr
3.    w such that         ,g(w )     1 and    i     {1, . . . , r},(cid:10)w,   (x 1

i , . . . , x k
i )
further, any w which satis   es condition 2 or 3 satis   es both conditions 2 and 3 and w    
        ,g(x).
proof 2        3) 3 trivially implies 2 from the de   nition of   r. for the opposite direction, because
i )    i. taking the sum over i, we
can only achieve equality in 2 if we have equality    i in condition 3. this also shows that any w
which satis   es condition 2 or 3 must also satisfy the other condition.

        ,g(w )     1 we have (cid:10)w,   (x 1

i )(cid:11)     g(x 1

i )(cid:11) = g(x 1

i , . . . , x k

i , . . . , x k

i , . . . , x k

i , . . . , x k

i ),

2/3 and the de   nition of      ,g, we have      ,g(x)     pr

we next show that if w satis   es conditions 2/3 then w             ,g(x). first, from condition
i ) = hw, xi <    . thus,
recall that because      ,g(x) is convex and    nite at x, we have hw, xi          ,g(x) +         ,g(w )
with equality iff w             ,g(x). now, by contradiction assume w satis   es conditions 2/3 but
w /            ,g(x). from condition 2/3 we have         ,g(w ) = 0, so      ,g(x) =      ,g(x) +         ,g(w ) >

i , . . . , x k

i=1 g(x 1

i , . . . , x k

i ) which contradicts the de   nition of      ,g(x).

=    2) any w             ,g(x) satis   es hx, w i =      ,g(x) +         ,g(w ) =
i , . . . , x k

1
i=1 g(x 1
2 =    1) by contradiction, assume (x 1, . . . , x k )r was not an optimal factorization of x.
i ) = hw, xi =      ,g(x) +         ,g(w ) =      ,g(x),

i , . . . , x k

i=1 g(x 1

i ).

i=1 g(x 1

hx, w i =pr
pr
this gives,      ,g(x) < pr

producing the contradiction.

finally, we show one additional lemma before presenting our main results.

lemma 14 if (x 1, . . . , x k , q) is a local minimum of fr(x 1, . . . , x k , q) as given in (20), then
for any        rr

*    1

     x   (  r(x 1, . . . , x k ), q),

r

xi=1

  i  (x 1

i , . . . , x k

i )+ =

  ig(x 1

i , . . . , x k
i )

(29)

r

xi=1

13

(30)

(31)

g(x 1

xi=1
i ), q! +   

haeffele and vidal

proof

let (z 1

i , . . . , z k

i ) = (  ix 1

i ) for all i     {1 . . . r} and let    =
i ). from positive homogeneity and the fact that we have a local minimum,

i , . . . ,   ix k

i , . . . , x k

i=1   i  (x 1

then       > 0 such that           (0,   ) we must have

pr

fr(x 1, . . . , x k , q)     fr(x 1 +   z 1, . . . , x k +   z k, q) =   

r

   (  r(x 1, . . . , x k ), q) +   

i , . . . , x k

i ) + h(q)    

     r
xi=1

(1 +     i)p  (x 1

i , . . . , x k

r

(1 +     i)pg(x 1

i , . . . , x k

i ) + h(q)

xi=1

taking the    rst order approximation (1 +     i)p = 1 + p    i + o(  2) and rearranging the terms of
(31), we arrive at

0       (cid:0)  r(x 1, . . . , x k ) + p     + o(  2), q(cid:1)        (  r(x 1, . . . , x k ), q)

r

+ p    

  ig(x 1

i , . . . , x k

i ) + o(  2)

xi=1

(32)

taking lim    0[ (32)
derivative d   (  r(x 1, . . . , x k ), q)(p  , 0), thus from the differentiability of     we get

   ], we note that the difference in the    (  ,   ) terms gives the one-sided directional

0    (cid:10)   x   (  r(x 1, . . . , x k ), q), p  (cid:11) + p  

r

xi=1

  ig(x 1

i , . . . , x k
i )

(33)

noting that for    > 0 but suf   ciently small, we also must have fr(x 1, . . . , x k , q)     fr(x 1    
  z 1, . . . , x k       z k), using identical steps as before and taking the    rst order approximation (1    
    i)p = 1     p    i + o(  2), we get

0       (  r(x 1, . . . , x k )     p     + o(  2), q)        (  r(x 1, . . . , x k ), q)

r

    p    

xi=1
and taking the limit lim    0[ (34)

   ], we arrive at

  ig(x 1

i , . . . , x k

i ) + o(  2)

(34)

0    (cid:10)   x   (  r(x 1, . . . , x k ), q),    p  (cid:11)     p  

r

xi=1

combining (33) and (35) and rearranging terms gives the result.

  ig(x 1

i , . . . , x k
i )

(35)

5.2 main results

based on the above preliminary results, we are now ready to state our main results and several
immediate corollaries.

14

globally optimal positively homogeneous factorizations

theorem 15 given a function fr(x 1, . . . , x k , q) of the form given in (20), any local minimizer
of the optimization problem

min

(x 1,...,xk )r ,q

fr(x 1, . . . , x k , q)    

   (  r(x 1, . . . , x k ), q) +   

r

xi=1

g(x 1

i , . . . , x k

i ) + h(q)

(36)

such that (x 1

i0 , . . . , x k

i0 ) = (0, . . . , 0) for some i0     {1, . . . , r} is a global minimizer.

proof theorem 15

we begin by noting that from the de   nition of      ,g(x),

  r(x 1, . . . , x k )

for any factorization x =

f (x, q) =    (x, q) +        ,g(x) + h(q)    

   (  r(x 1, . . . , x k ), q) +   

r

xi=1

g(x 1

i , . . . , x k

i ) + h(q) = fr(x 1, . . . , x k , q)

(37)

with equality at any factorization which achieves the in   mum in (18). we will show that a local
minimum of fr(x 1, . . . , x k , q) satisfying the conditions of the theorem also satis   es the condi-
tions for (  r(x 1, . . . , x k ), q) to be a global minimum of the convex function f (x, q), which
implies a global minimum of fr(x 1, . . . , x k , q) due to the global bound in (37).

first, because (19) is a convex function, a simple subgradient condition gives that (x, q) is a

global minimum of f (x, q) iff the following two conditions are satis   ed

    1

      x   (x, q)             ,g(x)
      q   (x, q)        h(q)

(38)
(39)

where    x   (x, q) and    q   (x, q) denote the portions of the gradient of    (x, q) corresponding to
x and q, respectively. if (x 1, . . . , x k , q) is a local minimum of fr(x 1, . . . , x k , q), then (39)
must be satis   ed at (x, q) = (  r(x 1, . . . , x k ), q), as this is implied by the    rst order optimality
condition for a local minimum (rockafellar and wets, 1998, chap. 10), so we are left to show that
(38) is also satis   ed.

turning to the factorization objective,

if (x 1, . . . , x k , q) is a local minimum of
fr(x 1, . . . , x k , q), then    (z 1, . . . , z k)r there exists    > 0 such that           (0,   ) we have
fr(x 1 +   1/pz 1, . . . , x k +   1/pz k, q)     fr(x 1, . . . , x k , q). if we now consider search di-
rections (z 1, . . . , z k)r of the form

(z 1

j , . . . , z k

j ) =(cid:26) (0, . . . , 0)

(z1, . . . , zk )

j 6= i0
j = i0

(40)

15

haeffele and vidal

where i0 is the index such that (x 1

i0 , . . . , x k

i0 ) = (0, . . . , 0), then for        (0,   ), we have

r

   (  r(x 1, . . . , x k ), q) +   

g(x 1

i , . . . , x k

i ) + h(q)    

xi=1

   (  r(x 1 +   1/pz 1, . . . , x k +   1/pz k), q)+

g(x 1

i +   1/pz 1

i , . . . , x k

i +   1/pz k

i ) + h(q) =

  

r

xi=1

   (  r(x 1, . . . , x k ) +     (z1, . . . , zk ), q)+

  

r

xi=1

g(x 1

i , . . . , x k

i ) +     g(z1, . . . , zk ) + h(q).

(41)

(42)

(43)

the equality between (42) and (43) comes from the special form of z given by (40) and the positive
homogeneity of    and g. rearranging terms, we now have

     1[   (  r(x 1, . . . , x k ) +     (z1, . . . , zk ), q)        (  r(x 1, . . . , x k ), q)]    

      g(z1, . . . , zk ).

(44)

taking the limit lim       0 of (44), we note that the left side of the inequality is simply
the de   nition of the one-sided directional derivative of    (  r(x 1, . . . , x k ), q) in the direction
(  (z1, . . . , zk ), 0), which combined with the differentiability of    (x, q), gives

(cid:10)  (z1, . . . , zk ),    x   (  r(x 1, . . . , x k ), q)(cid:11)          g(z1, . . . , zk ).

because (z1, . . . , zk ) was arbitrary, we have established that

(cid:10)  (z1, . . . , zk ),     1

        ,g(    1

      x   (  r(x 1, . . . , x k ), q)(cid:11)     g(z1, . . . , zk )    (z1, . . . , zk )       

      x   (  r(x 1, . . . , x k ), q))     1

further, if we choose    to be vector of all ones in lemma 14, we get

r

xi=1

g(x 1

i , . . . , x k

i ) =(cid:10)  r(x 1, . . . , x k ),     1

      x   (  r(x 1, . . . , x k ), q)(cid:11)

(45)

(46)

(47)

which, combined with (46) and lemma 13, shows that     1
        ,g(  r(x 1, . . . , x k )), completing the result.

      x   (  r(x 1, . . . , x k ), q)    

from this result, we can then test the global optimality of any local minimum (regardless of

whether it has an all-zero slice or not) from the immediate corollary:

corollary 16 given a function fr(x 1, . . . , x k , q) of the form given in (20), any local minimizer
of the optimization problem

min

(x 1,...,xk )r,q

fr(x 1, . . . , x k , q)

(48)

is a global minimizer if fr+1([x 1 0], . . . , [x k 0], q) is a local minimizer of fr+1.

16

globally optimal positively homogeneous factorizations

from the results of theorem 15, we are now also able to show that if we let the size of the
factorized variables (r) become large enough, then from any initialization we can always    nd a
global minimizer of fr(x 1, . . . , x k , q) using a purely local descent strategy. speci   cally, we
have the following result.

theorem 17 given a function fr(x 1, . . . , x k , q) as de   ned by (20), if r > card(d) then from
any point (z 1, . . . , z k , q) such that fr(z 1, . . . , z k , q) <     there must exist a non-increasing
path from (z 1, . . . , z k, q) to a global minimizer of fr(x 1, . . . , x k , q).

proof theorem 17

clearly if (z 1, . . . , z k, q) is not a local minimum, then we can follow a decreasing path
until we reach a local minimum. having arrived at a local minimum, (   x 1, . . . ,   x k ,   q), if
(   x 1
i ) = (0, . . . , 0) for any i     {1, . . . , r} then from theorem 15 we must be at a global
minimum. we are left to show that a non-increasing path to a global minimizer must exist from any
local minima such that (   x 1

i ) 6= (0, . . . , 0) for all i     {1, . . . , r}.

i , . . . ,   x k

i , . . . ,   x k

i=1   i  (   x 1

let us de   ne the set s = {pr
r > card(d) there must exist          rr such that      6= 0 and pr
that pr
d    1
     x   (  r(x 1, . . . , x k ), q),pr

i , . . . ,   x k
i )

14 we must

from lemma

i , . . . , x k

    i  (x 1

further,

i )e = 0. because g(   x 1

0,    i     {1, . . . , r} this implies that at least one entry of      must be strictly less than zero.

i , . . . ,   x k
i , . . . ,   x k
i )
i , . . . ,   x k

i=1
    ig(   x 1

       rr}.

because
i ) =
=
i ) >

without loss of generality, scale      so that mini     i =    1. now, for all (  , i)     {[0, 1]}   

    i  (   x 1

have

i=1

i=1

0.

:

{1, . . . , r}, let us de   ne

(r1

i (  ), . . . , rk

i (  ))     ((1 +        i)1/p   x 1

i , . . . , (1 +        i)1/p   x k
i )

(49)

where p is the degree of positive homogeneity of (  , g).
note that by construction
(r1(0), . . . , rk (0)) = (   x 1, . . . ,   x k ) and that for    = 1 there must exist i0     {1, . . . , r} such
that (r1

i0 (1)) = (0, . . . , 0).

i0(1), . . . , rk

further, from the positive homogeneity of (  , g) we have           [0, 1]

fr(r1(  ), . . . , rk (  ), q) =     r
xi=1
xi=1

    

r

  (x 1

i , . . . , x k

i ) +   

    i  (x 1

i , . . . , x k

i ), q! +

    ig(x 1

i , . . . , x k

i ) +   

g(x 1

i , . . . , x k

i ) + h(q)

=   (  r(x 1, . . . , x k ), q) +   

=fr(x 1, . . . , x k , q)

g(x 1

i , . . . , x k

i ) + h(q)

r

r

xi=1
xi=1
xi=1

r

(50)

(51)

(52)

    ig(x 1

i , . . . , x k

where the equality between (50) and (51) is seen by recalling thatpr
pr

i=1
as a result, as    goes from 0     1 we can traverse a path from (x 1, . . . , x k , q)    
(r1(1), . . . , rk (1), q) without changing the value of fr. also recall that by construction
i0 (1)) = (0, . . . , 0), so if (r1(1), . . . , rk(1), q) is a local minimizer of fr then it
(r1

i0(1), . . . , rk

i ) = 0 and

i , . . . , x k

i ) = 0.

    i  (x 1

i=1

17

haeffele and vidal

algorithm 1 (local descent meta-algorithm)
input p - degree of positive homogeneity for (  , g)
input {(x 1, . . . , x k )r, q} - initialization for variables

while not converged do

perform local descent on variables {(x 1, . . . , x k ), q} until arriving at a local minimum
{(   x 1, . . . ,   x k ),   q}
if    i0     {1, . . . , r} such that (   x 1

i0 ) = (0, . . . , 0) then

i0, . . . ,   x k

{(   x 1, . . . ,   x k ),   q} is a global minimum. return.

else

if           rr\0 such thatpr

scale    so that mini   i =    1
set (x 1

i , . . . , x k

else

i=1   i  (x 1

i , . . . , x k

i ) = 0 then

i ) = ((1       i)1/p   x 1

i , . . . , (1       i)1/p   x k

i ),    i     {1, . . . , r}

increase size of factorized variables by appending an all zero slice
(x 1, . . . , x k )r+1 = ([   x 1 0], . . . , [   x k 0])

end if
set q =   q
continue loop

end if

end while

must be a global minimizer due to theorem 15. if (r1(1), . . . , rk(1), q) is not a local minimizer
then there must exist a descent direction and we can iteratively apply this result until we reach a
global minimizer, completing the proof.

we note that from this proof we have also described a meta-algorithm (outlined in algorithm
1) which can be used with any local-descent optimization strategy to guarantee convergence to a
global minimum. while in general the size of the factorization (r) might increase as the algorithm
proceeds, as a worst case, it is guaranteed that a global minimum can be found with a    nite r never
growing larger than card(d) + 1. also note that this is a worst case upper bound on r for the most
general form of our framework and that for speci   c choices of    and g the bound on the maximum
r required can be signi   cantly lowered.

corollary 18 algorithm 1 will    nd a global minimum of fr(x 1, . . . , x k , q) as de   ned in (20). if
r is intialized to be greater than card(d), then the size of the factorized variables will not increase.
otherwise, the algorithm will terminate with r     card(d) + 1.

6. discussion and conclusions

we begin the discussion of our results with a cautionary note; namely, these results can be chal-
lenging to apply in practice. in particular, many algorithms based on alternating minimization can
typically only guarantee convergence to a critical point, and with the inherent non-convexity of the
problem, verifying whether a given critical point is also a local minima can be a challenging prob-
lem on its own. nevertheless, we emphasize that our results guarantee that global minimizers can

18

globally optimal positively homogeneous factorizations

be found from purely local descent if the optimization problem falls within the general framework
we have described here. as a result, even if the particular local descent strategy one chooses for
a speci   c problem does not come with guaranteed convergence to a local minimum, the scope of
the problem is still vastly reduced from a full global optimization. there is no need, in theory, to
consider multiple initializations or more complicated (and much larger scale) techniques to explore
the entire search space.

6.1 balanced degrees of homogeneity

in addition to the above points, our analysis analysis also provides a few insights into the behavior of
factorization problems and offers simple guidance on the design of such problems. the    rst is that
balancing the degree of positive homogeneity between the id173 function and the mapping
function is crucial. here we have analyzed a mapping    with the particular form given in (10).
we conjecture our results can likely be generalized to include additional factorization mappings
(which we save for future work), but even for more general mappings and id173 functions,
requiring the degrees of positive homogeneity to match between the id173 function and the
mapping function will be critical to showing results similar to those we present here. in general,
if the degrees of positive homogeneity do not match between the factorization mapping and the
id173 function, then it either becomes impossible to make guarantees regarding the global
optimality of a local minimum, or the id173 function does nothing to limit the size of the
factorization, so the degrees of freedom in the model are largely determined by the user de   ned
choice of r. as a demonstration of these phenomena,    rst consider the case where we have a
general mapping   (x 1, . . . , x k ) which is positively homogeneous with degree p (but which is
not assumed to have form (10)). now, consider a general id173 function g(x 1, . . . , x k )
which is positively homogeneous with degree p    < p, then the following proposition provides a
simple counter-example demonstrating that in general it is not possible to guarantee that a global
minimum can be found from local descent.

proposition 19 let     : rd     r be a convex function with       (0) 6=    ; let    : rd1
   . . .   
rdk     rd be a positively homogeneous mapping with degree p; and let g : rd1    . . .    rdk    
r+ be a positively homogeneous function with degree p    < p such that g(0, . . . , 0) = 0 and
g(x 1, . . . , x k ) > 0    {(x 1, . . . , x k ) :   (x 1, . . . , x k ) 6= 0}. then, the optimization problem
given by

min

(x 1,...,xk )

  f (x 1, . . . , x k ) =    (  (x 1, . . . , x k )) + g(x 1, . . . , x k )

(53)

will always have a local minimum at (x 1, . . . , x k ) = (0, . . . , 0). additionally,    (x 1, . . . , x k )
such that   (x 1, . . . , x k ) 6= 0 there exists a neighborhood such that   f (  x 1, . . . ,   x k ) >
  f (0, . . . , 0) for    > 0 and suf   ciently small.

proof consider   f (  x 1, . . . ,   x k )       f (0, . . . , 0). this gives

   (  (  x 1, . . . ,   x k )) + g(  x 1, . . . ,   x k )        (0)     g(0, . . . , 0) =
   (  p  (x 1, . . . , x k ))        (0) +   p   

g(x 1, . . . , x k )    
g(x 1, . . . , x k )

  p(cid:10)      (0),   (x 1, . . . , x k )(cid:11) +   p   

19

(54)

(55)

(56)

xi=1

haeffele and vidal

recall that p > p    and   (x 1, . . . , x k ) 6= 0 =    g(x 1, . . . , x k ) > 0, so    (x 1, . . . , x k ),
  f (  x 1, . . . ,   x k )       f (0, . . . , 0)     0 for    > 0 and suf   ciently small, with equality iff
g(x 1, . . . , x k ) = 0 =      (x 1, . . . , x k ) = 0, giving the result.

the above proposition shows that unless we have the special case where (x 1, . . . , x k ) =
(0, . . . , 0) happens to be a global minimizer, then there will always exist a local minimum at the
origin, and from the origin it will always be necessary to take an increasing path to escape the local
minimum. the case described above, where p > p   , is arguably the more common situation for
mismatched degrees of homogeneity (as opposed to p < p   ), and a typical example might be an
objective function such as

k

   (  (x 1, . . . , x k )) +   

kx ikp   

(57)

where    is a positively homogeneous mapping with degree k > 2 (e.g., the mapping of a deep
neural network) but p    is typcially taken to be only 1 or 2 depending on the particular choice of
norm.

conversely, in the situation where p    > p, then it is often the case that the id173 function
is not suf   cient to    limit    the size of the factorization, in the sense that the objective function can
always be decreased by allowing the size of the factors to grow. as a simple example, consider the
case of id105 with the objective function

   (u v t ) +   (ku kp   

+ kv kp   

(58)

   2
2 )p   

(k[u u ]kp   

+ k[v v ]kp   

   2
2 v ]t = u v t ,
if the size of the factorization doubles, then we can always take [
+ kv kp   , then the objective function can always be
so if (
decreased by simply duplicating and scaling the existing factorization. it is easily veri   ed that the
above inequality is satis   ed for many choices of norm (for example, all the lq norms with q     1)
when p    > 2. as a result, this implies that the degrees of freedom in the model will be largely
dependent on the initial choice of the number of columns in (u, v ), since in general the objective
function is typically decreased by having all entries of (u, v ) be non-zero.

   2
2 u ][

) < ku kp   

   2
2 v

)
   2
2 u

6.2 implications for neural networks

examining our results speci   cally as they apply to deep neural networks, we    rst note that from
our analysis we have shown that neural networks which are based on positively homogeneous map-
pings can be regularized in the way we have outlined in our framework so that the optimization
problem of training the network can be analyzed from a convex framework. further, we suggest
that our results provide a partial explanation to the recently observed empirical phenomenon where
replacing the traditional sigmoid or hyperbolic tangent non-linearities with positively homogeneous
non-linearities, such as recti   cation and max-pooling, signi   cantly boosts the speed of optimization
and the performance of the network (dahl et al., 2013; maas et al., 2013; krizhevsky et al., 2012;
zeiler et al., 2013). namely, by using a positively homogeneous network mapping, the problem
then becomes a convex function of the network outputs. additionally, we have also shown that if
the size of the network is allowed to be large enough then for any initialization a global minimizer
can be found from purely local descent, and thus local minima are all equivalent. this is a similar
conclusion to the work of choromanska et al. (2014), who analyzed the problem from a statistical

20

globally optimal positively homogeneous factorizations

standpoint and showed that with suf   ciently large networks and appropriate assumptions about the
distributions of the data and network weights, then with high id203 any family of networks
learned with different initializations will have similar objective values, but we note that our results
allow for a well de   ned set of conditions which will be suf   cient to guarantee the property. fi-
nally, many modern large scale networks do not use traditional id173 on the network weigh
parameters such as an l1 or l2 norms during training and instead rely on alternative forms of reg-
ularization such as dropout as it tends to achieve better performance in practice(srivastava et al.,
2014; krizhevsky et al., 2012; wan et al., 2013). given our commentary above regarding the crit-
ical importance of balancing the degree of homogeneity between the mapping and the regularizer,
an immediate prediction of our analysis is that simply ensuring that the degrees of homogeneity are
balanced could be a signi   cant factor in improving the performance of deep networks.

we conclude by noting that the main limitation of our current framework in the context of the
analysis of currently existing state-of-the-art neural networks is that the form of the mapping we
study here (10) implies that the network architecture must consist of r parallel subnetworks, where
each subnetwork has a particular architecture de   ned by the elemental mapping   . previously, we
mentioned as an example that the well known id163 network from (krizhevsky et al., 2012)
can be described by our framework by taking r = 1 and using an appropriate de   nition of   ;
however, to apply corollary 16 to then test for global optimality, we must test whether it is possible
to reduce the objective function by adding an entire network with the same architecture in parallel
to the given network. clearly, this is a signi   cant limitation for the application of these results and
suggests two possibilities for future work. the    rst is that simply implementing neural networks
with a highly parallel network architecture and relatively simple subnetwork architectures could
be advantageous and worthy of experimental study. in fact, the id163 network already has a
certain degree of parallelization as the initial convolutional layers of the network operate largely in
parallel on separate gpu units. more generally, here we have focused on mappings with form (10)
as it is conducive to analysis, but we believe that many of the results we have presented here can be
generalized to more general mappings (and thus more general network architectures) using many of
the principles and analysis techniques we have presented here; an extension we reserve for future
work.

6.3 conclusions

here we have presented a general framework which allows for a wide variety of non-convex fac-
torization problems to be analyzed with tools from convex analysis. in particular, we have shown
that for problems which can be placed in our framework, any local minimum can be guaranteed to
be a global minimum of the non-convex factorization problem if one slice of the factorized tensors
is all zero. additionally, we have shown that if the non-convex factorization problem is done with
factors of suf   cient size, then from any feasible initialization it is always possible to    nd a global
minimizer using a purely local descent algorithm.

references

m. aharon, m. elad, and a. m. bruckstein. k-svd: an algorithm for designing overcomplete
ieee trans. on signal processing, 54(11):4311   4322,

dictionaries for sparse representation.
2006.

21

haeffele and vidal

f. bach. convex relaxations of structured id105s. arxiv:1309.3117v1, 2013.

f. bach, j. mairal, and j. ponce. convex sparse id105s. arxiv:0812.1869v1, 2008.

francis bach. breaking the curse of dimensionality with convex neural networks. arxiv preprint

arxiv:1412.8690, 2014.

yoshua bengio, nicolas l roux, pascal vincent, olivier delalleau, and patrice marcotte. convex

neural networks. in advances in neural information processing systems, pages 123   130, 2005.

s. burer and r. d. c. monteiro. local minima and convergence in low-rank semide   nite program-

ming. mathematical programming, series a, (103):427   444, 2005.

j-f cai, e. j. cand  es, and z. shen. a singular value thresholding algorithm for matrix completion.

siam journal of optimization, 20(4):1956   1982, 2008.

anna choromanska, mikael henaff, michael mathieu, g  erard ben arous, and yann lecun. the

loss surface of multilayer networks. arxiv preprint arxiv:1412.0233, 2014.

andrzej cichocki, rafal zdunek, anh huy phan, and shun-ichi amari. nonnegative matrix and
tensor factorizations: applications to exploratory multi-way data analysis and blind source sep-
aration. john wiley & sons, 2009.

george e dahl, tara n sainath, and geoffrey e hinton. improving deep neural networks for lvcsr
using recti   ed linear units and dropout. in acoustics, speech and signal processing (icassp),
2013 ieee international conference on, pages 8609   8613. ieee, 2013.

silvia gandy, benjamin recht, and isao yamada. tensor completion and low-n-rank tensor recovery

via id76. inverse problems, 27(2):025010, 2011.

b. haeffele, e. young, and r. vidal. structured low-rank id105: optimality, algo-
rithm, and applications to image processing. in international conference on machine learning,
2014.

tamara g kolda and brett w bader. tensor decompositions and applications. siam review, 51(3):

455   500, 2009.

alex krizhevsky, ilya sutskever, and geoffrey e hinton. id163 classi   cation with deep convolu-
tional neural networks. in advances in neural information processing systems, pages 1097   1105,
2012.

daniel d lee and h sebastian seung. learning the parts of objects by non-negative matrix factor-

ization. nature, 401(6755):788   791, 1999.

andrew l maas, awni y hannun, and andrew y ng. recti   er nonlinearities improve neural

network acoustic models. in proc. icml, volume 30, 2013.

julien mairal, francis bach, jean ponce, and guillermo sapiro. online learning for matrix factor-

ization and sparse coding. the journal of machine learning research, 11:19   60, 2010.

22

globally optimal positively homogeneous factorizations

jiquan ngiam, adam coates, ahbik lahiri, bobby prochnow, quoc v le, and andrew y ng. on
optimization methods for deep learning. in proceedings of the 28th international conference on
machine learning (icml-11), pages 265   272, 2011.

b. recht, m. fazel, and p. parrilo. guaranteed minimum-rank solutions of linear matrix equations

via nuclear norm minimization. siam review, 52(3):471   501, 2010.

r tyrrell rockafellar and roger j-b wets. variational analysis, volume 317. springer, 1998.

david e rumelhart, geoffrey e hinton, and ronald j williams. learning representations by back-

propagating errors. cognitive modeling, 5, 1988.

nitish srivastava, geoffrey hinton, alex krizhevsky, ilya sutskever, and ruslan salakhutdinov.
dropout: a simple way to prevent neural networks from over   tting. the journal of machine
learning research, 15(1):1929   1958, 2014.

ryota tomioka, kohei hayashi, and hisashi kashima. estimation of low-rank tensors via convex

optimization. arxiv preprint arxiv:1010.0789, 2010.

li wan, matthew zeiler, sixin zhang, yann l cun, and rob fergus. id173 of neural
networks using dropconnect. in proceedings of the 30th international conference on machine
learning (icml-13), pages 1058   1066, 2013.

stephen j wright and jorge nocedal. numerical optimization, volume 2. springer new york, 1999.

yangyang xu and wotao yin. a block coordinate descent method for regularized multiconvex
optimization with applications to nonnegative tensor factorization and completion. siam journal
on imaging sciences, 6(3):1758   1789, 2013.

yaoliang yu, xinhua zhang, and dale schuurmans. generalized conditional gradient for sparse

estimation. arxiv preprint arxiv:1410.4828, 2014.

matthew d zeiler, m ranzato, rajat monga, m mao, k yang, quoc viet le, patrick nguyen,
a senior, vincent vanhoucke, jeffrey dean, et al. on recti   ed linear units for speech processing.
in acoustics, speech and signal processing (icassp), 2013 ieee international conference on,
pages 3517   3521. ieee, 2013.

xinhua zhang, yao-liang yu, and dale schuurmans. polar operators for structured sparse estima-

tion. in advances in neural information processing systems, pages 82   90, 2013.

23

