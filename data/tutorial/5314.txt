mathematics of deep learning

cvpr tutorial, honolulu, usa, july 21st 2017   
raja giryes (tel aviv university), ren   vidal (hopkins)

   
learning deep image feature hierarchies
    deep learning gives ~ 10% improvement on id163 

    1.2m images 
    1000 categories 
    60 million   
parameters

[1] krizhevsky, sutskever and hinton. id163 classification with deep convolutional neural networks, nips   12. 
[2] sermanet, eigen, zhang, mathieu, fergus, lecun. overfeat: integrated recognition, localization and detection using convolutional networks. iclr   14. 
[3] donahue, jia, vinyals, hoffman, zhang, tzeng, darrell. decaf: a deep convolutional activation feature for generic visual recognition. icml   14.

impact of deep learning in id161
id98 
    2012-2014 classification results in id163 

non-id98

    2015 results: msr under 3.5% error using 150 layers!

slide from yann lecun   s cvpr   15 plenary and iccv   15 tutorial intro by joan bruna

stockholm, sweden

person

plane bike bird boat

bus
71.7
77.5
75.0
72.5
73.4

car
83.6
88.8
77.5
85.3
86.7

boat
72.1
76.1
73.4
82.0
84.1

dog
51.1
51.3
74.6
77.8
84.0

cat
66.5
btl
69.1
79.2
81.6
85.4

chair
52.5
bus
62.2
46.2
59.9
61.3

sheep
55.3
61.7
61.0
71.1
76.7

person
86.5
91.1
91.1
90.2
92.0

plant
36.4
48.1
53.9
54.8
56.9

chair cow table dog horse moto pers plant sheep sofa train

abstract

sheep
55.3
61.7
61.0
71.1
76.7

cow table
62.8
57.5
car
cat
64.2
61.8
62.7
41.4
66.5
58.5
67.6
69.6

horse mbike
71.5
81.4
80.2
85.4
85.0
76.8
78.8
81.8
85.4
80.0

sofa
plant
36.4
60.6
67.7
48.1
64.0 85.9 36.3
53.9
67.5
76.8 91.1 53.9
80.0 95.6 60.8
54.8
62.6
56.9
67.3

aero
76.7
ghm[8]
82.2
ags[11]
82.5
nus[39]
id98-id166
88.5
id98aug-id166 90.1

train
80.6
tv map
86.3
44.7 50.6 79.2 53.2 59.4
83.6
61.0 67.5 83.6 70.6 70.5
76.8 58.0 90.4 77.9 77.7
87.2
89.1

{razavian,azizpour,sullivan,stefanc}@csc.kth.se

map
64.7
chair cow table dog horse moto pers plant sheep sofa train
71.1
70.5
73.9
77.2

transfer from id163 to other datasets
    id98s + smvs [1] 

bird
bike
53.8
74.7
plane bike bird boat
58.4
83.0
inria [32]
77.5 63.6 56.1 71.9 33.1 60.6 78.0 58.8 53.5 42.6 54.9 45.8 77.5
79.6
64.8
nus-psl [44] 82.5 79.6 64.8 73.4 54.2 75.0 77.5 79.2 46.2 62.7 41.4 74.6 85.0
88.5 81.5 87.9 82.0 47.5 75.5 90.1 87.2 61.6 75.7 67.3 85.5 83.5
pre-1000c
83.5
81.0
84.4
86.5

77.5 63.6 56.1 71.9 33.1 60.6 78.0 58.8 53.5 42.6 54.9 45.8 77.5
nus-psl [44] 82.5 79.6 64.8 73.4 54.2 75.0 77.5 79.2 46.2 62.7 41.4 74.6 85.0
88.5 81.5 87.9 82.0 47.5 75.5 90.1 87.2 61.6 75.7 67.3 85.5 83.5

tv
bottle
57.8
40.4
56.4
70.9
70.6
54.2
71.8
42.0
74.9
48.4
table 1: per-class results for object classi   cation on the voc2007 test set (average precision %).
table 1: pascal voc 2007 image classi   cation results compared to other methods which also use training data outside voc. the id98 representation
chair
bird
tv
bike
horse mbike
dog
bottle
sofa
recent results indicate that the generic descriptors ex-
tv map
car
bus
is not tuned for the pascal voc dataset. however, ghm [8] learns from voc a joint representation of bag-of-visual-words and contextual information.
52.5
53.8
57.8
74.7
81.4
71.5
51.1
40.4
60.6
nus-psl [49] 97.3 84.2 80.8 85.3 60.8 89.9 86.8 89.3 75.4 77.8 75.1 83.0 87.5
79.2 73.4 94.5 80.7 82.2
tracted from the convolutional neural networks are very
ags [11] learns a second layer of representation by id91 the voc data into subcategories. nus [39] trains a codebook for the sift, hog and lbp
no pretrain 85.2 75.0 69.4 66.2 48.8 82.1 79.5 79.8 62.4 61.9 49.8 75.9 71.4
69.7 49.3 80.0 76.7 70.9
62.2
80.2
85.4
56.4
67.7
58.4
70.9
83.0
51.3
descriptors from the voc dataset. oquab et al. [29] adapt the id98 classi   cation layers and achieves better results (77.7) indicating
93.5 78.4 87.7 80.9 57.3 85.0 81.6 89.4 66.9 73.8 62.0 89.5 83.2
pre-1000c
79.0 54.3 88.0 78.3 78.7
46.2
64.8
70.6
79.6
76.8
85.0
74.6
54.2
67.5
powerful [10, 29, 48]. this paper adds to the mount-
the potential to boost the performance by further adaptation of the representation to the target task/dataset.
93.2 77.9 83.8 80.0 55.8 82.7 79.0 84.3 66.2 71.7 59.5 83.4 81.4
pre-1000r
74.9 52.9 83.8 75.7 76.3
94.6 82.9 88.2 84.1 60.3 89.0 84.4 90.7 72.1 86.8 69.0 92.1 93.4
86.6 62.3 91.1 79.8 82.8
pre-1512
59.9
83.5
71.8
81.0
81.8
78.8
77.8
42.0
62.6
ing evidence that this is indeed the case. we report on
100
86.5
74.9
84.4
85.4
84.0
61.3
80.0
48.4
67.3
method
table 2: per-class results for object classi   cation on the voc2012 test set (average precision %).
tv map
cat
plane bike bird boat
btl
car
chair cow table dog horse moto pers plant sheep sofa train
a series of experiments conducted for different recogni-
roi + gist[36]
26.1
boat
bus
person
bottle
horse mbike
car
cat
dog
train
aero
map
tv
sofa
cow table
bike
80
44.7 50.6 79.2 53.2 59.4
77.5 63.6 56.1 71.9 33.1 60.6 78.0 58.8 53.5 42.6 54.9 45.8 77.5
inria [32]
table 1: pascal voc 2007 image classi   cation results compared to other methods which also use training data outside voc. the id98 representation
table 1: pascal voc 2007 image classi   cation results compared to other methods which also use training data outside voc. the id98 representation
dpm[30]
30.4
jump phon instr read bike horse run phot compwalk map
action
action recognition task, and inspired by [6], our last re-
tion tasks using the publicly available code and model of
61.0 67.5 83.6 70.6 70.5
nus-psl [44] 82.5 79.6 64.8 73.4 54.2 75.0 77.5 79.2 46.2 62.7 41.4 74.6 85.0
86.5
66.5
83.6
72.1
71.5
81.4
51.1
80.6
71.7
40.4
76.7
ghm[8]
64.7
57.8
60.6
62.8
57.5
74.7
object bank[24]
37.6
60
stanford [1] 75.7 44.8 66.6 44.4 93.2 94.2 87.6 38.4 70.6 75.6 69.1
sults were obtained by training the target task id98 with-
88.8
85.4
80.2
56.4
77.5
67.7
91.1
69.1
76.1
51.3
86.3
82.2
ags[11]
71.1
70.9
61.8
83.0
64.2
is not tuned for the pascal voc dataset. however, ghm [8] learns from voc a joint representation of bag-of-visual-words and contextual information.
is not tuned for the pascal voc dataset. however, ghm [8] learns from voc a joint representation of bag-of-visual-words and contextual information.
mean ap
rbow[31]
37.9
76.8 58.0 90.4 77.9 77.7
88.5 81.5 87.9 82.0 47.5 75.5 90.1 87.2 61.6 75.7 67.3 85.5 83.5
pre-1000c
77.0 50.4 65.3 39.5 94.1 95.9 87.7 42.7 68.6 74.5 69.6
oxford [1]
the overfeat network which was trained to perform ob-
91.1
79.2
54.2
77.5
73.4
76.8
85.0
74.6
75.0
83.6
82.5
nus[39]
70.5
41.4
62.7
70.6
67.5
79.6
1
out freezing the fc6 weights. more precisely, we copy
bop[21]
46.1
40
no pretrain 43.2 30.6 50.2 25.0 76.8 80.7 75.2 22.2 37.9 55.6 49.7
ags [11] learns a second layer of representation by id91 the voc data into subcategories. nus [39] trains a codebook for the sift, hog and lbp
ags [11] learns a second layer of representation by id91 the voc data into subcategories. nus [39] trains a codebook for the sift, hog and lbp
the id163-trained weights of layers c1. . . c5, fc6 and
miid166[25]
46.4
table 1: per-class results for object classi   cation on the voc2007 test set (average precision %).
73.4 44.8 74.8 43.2 92.1 94.3 83.4 45.7 65.5 66.8 68.4
pre-1512
0.8
ject classi   cation on ilsvrc13. we use features extracted
90.2
81.6
85.3
82.0
78.8
81.8
77.8
72.5
42.0
id98-id166
88.5
87.2
73.9
66.5
58.5
71.8
62.6
81.0
descriptors from the voc dataset. oquab et al. [29] adapt the id98 classi   cation layers and achieves better results (77.7) indicating
descriptors from the voc dataset. oquab et al. [29] adapt the id98 classi   cation layers and achieves better results (77.7) indicating
scene
sculptures
scene
d-parts[40]
51.4
paris
bird
pre-1512u 74.8 46.0 75.6 45.3 93.5 95.0 86.5 49.3 66.7 69.5 70.2
fc7, we append the adaptation layers fca and fcb, and
92.0
85.4
84.1
84.0
85.4
89.1
id98aug-id166 90.1
77.2
67.6
84.4
74.9
69.6
86.7
80.0
48.4
73.4
67.3
xford
bject
bject
0.6
ifv[21]
60.8
from the overfeat network as a generic image represen-
we retrain layers fc6, fca, and fcb on the action recog-
uildings
tv map
cat
bus
chair cow table dog horse moto pers plant sheep sofa train
btl
car
bcategorization
table 3: pascal voc 2012 action classi   cation results (ap %).
classi   cation
attrib
the potential to boost the performance by further adaptation of the representation to the target task/dataset.
the potential to boost the performance by further adaptation of the representation to the target task/dataset.
attrib
mlrep[9]
64.0
uildings
ecognition
0.4
table 1: pascal voc 2007 image classi   cation results compared to other methods which also use training data outside voc. the id98 representation
tv map
chair cow table dog horse moto pers plant sheep sofa train
nition data. this strategy increases the performance on all
tv map
plane bike bird boat
car
ute
79.2 73.4 94.5 80.7 82.2
90.1 95.0 57.8
90.1 95.0 57.8
nus-psl [49] 97.3 84.2 80.8 85.3 60.8 89.9 86.8 89.3 75.4 77.8 75.1 83.0 87.5
tation to tackle the diverse range of recognition tasks of
ute
id98-id166
58.4
0.2
etrieval
action categories (row pre-1512u in table 3), yielding, to
is not tuned for the pascal voc dataset. however, ghm [8] learns from voc a joint representation of bag-of-visual-words and contextual information.
44.7 50.6 79.2 53.2 59.4
19
 7
11
 3
23
15
inria [32]
77.5 63.6 56.1 71.9 33.1 60.6 78.0 58.8 53.5 42.6 54.9 45.8 77.5
44.7 50.6 79.2 53.2 59.4
64.0 85.9 36.3
sessment of localization results, we compute an output map
etection
etection
level
69.7 49.3 80.0 76.7 70.9
82.7 93.1 59.1
no pretrain 85.2 75.0 69.4 66.2 48.8 82.1 79.5 79.8 62.4 61.9 49.8 75.9 71.4
82.7 93.1 59.1
69.0
id98aug-id166
mean accuracy
method
object image classi   cation, scene recognition,    ne grained
the best of our knowledge, the best average result published
61.0 67.5 83.6 70.6 70.5
for each category by averaging the scores of all the testing
nus-psl [44] 82.5 79.6 64.8 73.4 54.2 75.0 77.5 79.2 46.2 62.7 41.4 74.6 85.0
61.0 67.5 83.6 70.6 70.5
76.8 91.1 53.9
ags [11] learns a second layer of representation by id91 the voc data into subcategories. nus [39] trains a codebook for the sift, hog and lbp
id98(alexconvnet)+multiscale pooling [16]
68.9
79.0 54.3 88.0 78.3 78.7
87.6 95.8 61.4
pre-1000c
87.6 95.8 61.4
93.5 78.4 87.7 80.9 57.3 85.0 81.6 89.4 66.9 73.8 62.0 89.5 83.2
(a)
on the pascal voc 2012 action recognition task.
patches covering a given pixel of the test image. examples
descriptors from the voc dataset. oquab et al. [29] adapt the id98 classi   cation layers and achieves better results (77.7) indicating
76.8 58.0 90.4 77.9 77.7
88.5 81.5 87.9 82.0 47.5 75.5 90.1 87.2 61.6 75.7 67.3 85.5 83.5
80.0 95.6 60.8
76.8 58.0 90.4 77.9 77.7
pre-1000c
recognition, attribute detection and id162 applied
26.1
roi + gist[36]
74.9 52.9 83.8 75.7 76.3
84.8 95.2 59.8
93.2 77.9 83.8 80.0 55.8 82.7 79.0 84.3 66.2 71.7 59.5 83.4 81.4
pre-1000r
84.8 95.2 59.8
to demonstrate that we can also localize the action in the
table 2: mit-67 indoor scenes dataset. the mlrep [9] has a    ne
of such output maps are given in figures 1 and 5 as well
the potential to boost the performance by further adaptation of the representation to the target task/dataset.
figure 2: a) evolution of the mean image classi   cation ap over pas-
86.6 62.3 91.1 79.8 82.8
88.6 96.1 64.3
94.6 82.9 88.2 84.1 60.3 89.0 84.4 90.7 72.1 86.8 69.0 92.1 93.4
88.6 96.1 64.3
pre-1512
30.4
dpm[30]
to a diverse set of datasets. we selected these tasks and
figure 1: top) id98 representation replaces pipelines of s.o.a methods
image, we train the network in a sliding window manner, as
figure 2: transferring parameters of a id98. first, the network is trained on the source task (id163 classi   cation, top row) with
as on the project webpage [2]. this visualization clearly
tuned pipeline which takes weeks to select and train various part detectors.
cal voc 2007 classes as we use a deeper representation from the
a large amount of available labelled images. pre-trained parameters of the internal layers of the network (c1-fc7) are then transferred to
object bank[24]
37.6
described in section 3. in particular, we use the ground truth
demonstrates that the system knows the size and locations
furthermore, improved fisher vector (ifv) representation has dimension-
overfeat id98 trained on the ilsvrc dataset. overfeat considers
and achieve better results. e.g. dpd [50].
the target tasks (pascal voc object or action classi   cation, bottom row). to compensate for the different image statistics (type of objects,
datasets as they gradually move further away from the orig-
method
mean accuracy
37.9
rbow[31]
person bounding boxes during training, but do not use the
tv map
plane bike bird boat
car
cat
chair cow table dog horse moto pers plant sheep sofa train
of target objects within the image. addressing the detection
tv map
typical viewpoints, imaging conditions) of the source and target data we add an adaptation layer (fully connected layers fca and fcb) and
convolution, max pooling, nonlinear activations, etc. as separate layers.
ality larger than 200k. [16] has very recently tuned a multi-scale orderless
bottom) augmented id98 representation with linear id166 consistently
1
train them on the labelled data of the target task.
ground truth person bounding boxes at test time. example
the re-occurring decreases in the plot is of the activation function layer
46.1
bop[21]
inal task and data the overfeat network was trained to
task seems within reach.
roi + gist[36]
26.1
90.1 95.0 57.8
79.2 73.4 94.5 80.7 82.2
79.2 73.4 94.5 80.7 82.2
pooling of id98 features (off-the-shelf) suitable for certain tasks. with this
which loses information by half rectifying the signal. b) confusion matrix
outperforms s.o.a. on multiple tasks. specialized id98 refers to other
output maps shown in    gure 6 clearly demonstrate that the
dpm[30]
30.4
46.4
miid166[25]
for target tasks (pascal voc object and action classi   ca-
(here object and action classi   cation in pascal voc), as il-
82.7 93.1 59.1
69.7 49.3 80.0 76.7 70.9
simple modi   cation they achieved signi   cant average classi   cation accu-
0.8
69.7 49.3 80.0 76.7 70.9
action recognition. the pascal voc 2012 action recog-
for the mit-67 indoor dataset. some of the off-diagonal confused classes
solve. astonishingly, we report consistent superior results
network provides an estimate of the action location in the
lustrated in figure 2. however, this is dif   cult as the la-
tion) we wish to design a network that will output scores for
37.6
object bank[24]
works which speci   cally designed the id98 for their task
51.4
d-parts[40]
racy of 68.88.
have been annotated, these particular cases could be hard even for a human
87.6 95.8 61.4
79.0 54.3 88.0 78.3 78.7
nition task consists of 4588 training images and 4569 test
79.0 54.3 88.0 78.3 78.7
bels and the distribution of images (type of objects, typical
target categories, or background if none of the categories
image.
rbow[31]
37.9
0.6
compared to the highly tuned state-of-the-art systems in
to distinguish.
60.8
ifv[21]
images featuring people performing actions among ten cate-
viewpoints, imaging conditions, etc.) in the source and tar-
are present in the image. however, the object labels in the
84.8 95.2 59.8
74.9 52.9 83.8 75.7 76.3
1
74.9 52.9 83.8 75.7 76.3
bop[21]
46.1
get datasets can be very different, as illustrated in figure 3.
source task can be very different from the labels in the tar-
3.3. id164
gories such as jumping, phoning, playing instrument
64.0
mlrep[9]
86.6 62.3 91.1 79.8 82.8
88.6 96.1 64.3
0.4
all the visual classi   cation tasks on various datasets. for
86.6 62.3 91.1 79.8 82.8
large amount of labelled data to train my own network to
miid166[25]
46.4
get task (also called a    label bias    [49]). for example, the
to address these challenges we (i) design an architecture
0.8
last 2 layers the performance increases. we observed the
this    ne-grained task differs from the
or reading.
source network is trained to recognize different breeds of
that explicitly remaps the class labels between the source
d-parts[40]
51.4
unfortunately, we have not conducted any experiments for
same trend in the individual class plots. the subtle drops in
58.4
id98-id166
instance retrieval it consistently outperforms low memory
0.2
and target tasks (section 3.1), and (ii) develop training and
dogs such as husky dog or australian terrier, but the
quickly    nd out the answer   . but when the convolutional
object classi   cation task because it entails recognizing
60.8
ifv[21]
 7
 3
using id98 off-the-shelf features for the task of object de-
target task contains only one label dog. the problem be-
test procedures, inspired by sliding window detectors, that
the mid layers (e.g. 4, 8, etc.) is due to the    relu    layer
69.0
id98aug-id166
   ne differences in human poses (e.g.
running v.s.
mlrep[9]
64.0
footprint methods except for sculptures dataset. the results
comes even more evident for the target task of action classi-
explicitly deal with different distributions of object sizes,
0.4
tection. but it is worth mentioning that girshick et al. [15]
neural network overfeat [38] was recently made pub-
which half-recti   es the signals. although this will help the
walking) and subtle interactions with objects (phoning
id98(alexconvnet)+multiscale pooling [16]
68.9
id98(alexconvnet)+multiscale pooling [16]
   cation. what object categories in id163 are related to
locations and scene clutter in source and target tasks (sec-
[1] razavian, azizpour, sullivan, carlsson, id98 features off-the-shelf: an astounding baseline for recognition. cvprw   14. 
have reported remarkable numbers on pascal voc 2007
jump phon instr read bike horse run phot compwalk map
action
id98-id166
58.4
non-linearity of the trained model in the id98, it does not
0.2
(b)
the target actions reading or running ?
tions 3.2 and 3.3).
are achieved using a linear id166 classi   er (or l2 distance
or taking photo). training samples with multiple simul-
licly available1 it allowed for some experimentation.
[2] oquab, bottou, laptev, sivic. learning and transferring mid-level image representations using convolutional neural networks cvpr   14 
in
23
15
11
using off-the-shelf features from caffe code. we repeat
level
69.0
id98aug-id166
stanford [1] 75.7 44.8 66.6 44.4 93.2 94.2 87.6 38.4 70.6 75.6 69.1
help if immediately used for classi   cation.
[3] taigman, yang, ranzato, wolf. deepface: closing the gap to human-level performance in face verification. cvpr   14 
in order to achieve the transfer, we remove the output
3.1. network architecture
taneous actions are excluded from our training set.
in case of retrieval) applied to a feature representation of
their relevant results here. using off-the-shelf features they
particular we wondered now, not whether one could train
layer fc8 of the pre-trained network and add an adaptation
id98(alexconvnet)+multiscale pooling [16]
68.9
77.0 50.4 65.3 39.5 94.1 95.9 87.7 42.7 68.6 74.5 69.6
oxford [1]
for the source task, we use the network architec-
layer formed by two fully connected layers fca and fcb
achieve a map of 46.2 which already outperforms state
ture of krizhevsky et al. [24]. the network takes as
no pretrain 43.2 30.6 50.2 25.0 76.8 80.7 75.2 22.2 37.9 55.6 49.7
size 4096 extracted from a layer in the net. the representa-
(see figure 2, bottom) that use the output vector y7 of the
a deep network speci   cally for a given task, but if the fea-
input a square 224    224 pixel rgb image and pro-
of the art by about 10%. this adds to our evidences of

cow table
57.5
62.8
90.1 95.0 57.8
82.7 93.1 59.1
64.2
61.8
87.6 95.8 61.4
41.4
62.7
84.8 95.2 59.8
88.6 96.1 64.3
58.5
66.5
67.6
69.6
mean accuracy
chair cow table dog horse moto pers plant sheep sofa train
sheep
plant
55.3
36.4
48.1
61.7
61.0
53.9
71.1
54.8
flo
56.9
76.7
u
chair cow table dog horse moto pers plant sheep sofa train

nus-psl [49] 97.3 84.2 80.8 85.3 60.8 89.9 86.8 89.3 75.4 77.8 75.1 83.0 87.5
jump phon instr read bike horse run phot compwalk map
action
no pretrain 85.2 75.0 69.4 66.2 48.8 82.1 79.5 79.8 62.4 61.9 49.8 75.9 71.4
stanford [1] 75.7 44.8 66.6 44.4 93.2 94.2 87.6 38.4 70.6 75.6 69.1
93.5 78.4 87.7 80.9 57.3 85.0 81.6 89.4 66.9 73.8 62.0 89.5 83.2
pre-1000c
mean ap
77.0 50.4 65.3 39.5 94.1 95.9 87.7 42.7 68.6 74.5 69.6
oxford [1]
p
pre-1000r
93.2 77.9 83.8 80.0 55.8 82.7 79.0 84.3 66.2 71.7 59.5 83.4 81.4
a
no pretrain 43.2 30.6 50.2 25.0 76.8 80.7 75.2 22.2 37.9 55.6 49.7
94.6 82.9 88.2 84.1 60.3 89.0 84.4 90.7 72.1 86.8 69.0 92.1 93.4
pre-1512
73.4 44.8 74.8 43.2 92.1 94.3 83.4 45.7 65.5 66.8 68.4
pre-1512
pre-1512u 74.8 46.0 75.6 45.3 93.5 95.0 86.5 49.3 66.7 69.5 70.2
15
level
table 3: pascal voc 2012 action classi   cation results (ap %).

action recognition task, and inspired by [6], our last re-
sults were obtained by training the target task id98 with-
out freezing the fc6 weights. more precisely, we copy
the id163-trained weights of layers c1. . . c5, fc6 and
fc7, we append the adaptation layers fca and fcb, and
we retrain layers fc6, fca, and fcb on the action recog-
nition data. this strategy increases the performance on all
action categories (row pre-1512u in table 3), yielding, to
the best of our knowledge, the best average result published

(a)
sessment of localization results, we compute an output map
table 2: mit-67 indoor scenes dataset. the mlrep [9] has a    ne
for each category by averaging the scores of all the testing
tuned pipeline which takes weeks to select and train various part detectors.

action recognition task, and inspired by [6], our last re-
sults were obtained by training the target task id98 with-
out freezing the fc6 weights. more precisely, we copy
the id163-trained weights of layers c1. . . c5, fc6 and
fc7, we append the adaptation layers fca and fcb, and
we retrain layers fc6, fca, and fcb on the action recog-
nition data. this strategy increases the performance on all
action categories (row pre-1512u in table 3), yielding, to
the best of our knowledge, the best average result published

nus-psl [49] 97.3 84.2 80.8 85.3 60.8 89.9 86.8 89.3 75.4 77.8 75.1 83.0 87.5
no pretrain 85.2 75.0 69.4 66.2 48.8 82.1 79.5 79.8 62.4 61.9 49.8 75.9 71.4
93.5 78.4 87.7 80.9 57.3 85.0 81.6 89.4 66.9 73.8 62.0 89.5 83.2
93.2 77.9 83.8 80.0 55.8 82.7 79.0 84.3 66.2 71.7 59.5 83.4 81.4
94.6 82.9 88.2 84.1 60.3 89.0 84.4 90.7 72.1 86.8 69.0 92.1 93.4

failure modes. top-ranked false positives in figure 5
correspond to samples closely resembling target object
classes. resolving some of these errors may require high-
level scene interpretation. our method may also fail to
recognize spatially co-occurring objects (e.g., person on a
chair) since patches with multiple objects are currently ex-
cluded from training. this issue could be addressed by
changing the training objective to allow multiple labels per
sample. recognition of very small or very large objects

bject
age
chair cow table dog horse moto pers plant sheep sofa train
etrieval
etrieval

sheep
55.3
61.7
61.0
71.1
76.7
44.7 50.6 79.2 53.2 59.4
61.0 67.5 83.6 70.6 70.5
76.8 58.0 90.4 77.9 77.7

action recognition task, and inspired by [6], our last re-
sults were obtained by training the target task id98 with-
out freezing the fc6 weights. more precisely, we copy

action recognition task, and inspired by [6], our last re-
sults were obtained by training the target task id98 with-
out freezing the fc6 weights. more precisely, we copy

aero
map
train
ghm[8]
76.7
64.7
80.6
82.2
ags[11]
71.1
86.3
82.5
nus[39]
70.5
83.6
88.5
73.9
id98-id166
87.2
89.1
id98aug-id166 90.1
77.2
bird
53.8
58.4
64.8
83.5
86.5

table 2: mit-67 indoor scenes dataset. the mlrep [9] has a    ne
tuned pipeline which takes weeks to select and train various part detectors.

plant
36.4
48.1
53.9
54.8
56.9
64.0 85.9 36.3
76.8 91.1 53.9
80.0 95.6 60.8

79.2 73.4 94.5 80.7 82.2
69.7 49.3 80.0 76.7 70.9
79.0 54.3 88.0 78.3 78.7
74.9 52.9 83.8 75.7 76.3
86.6 62.3 91.1 79.8 82.8

    retrain top-layer [2] 

table 2: per-class results for object classi   cation on the voc2012 test set (average precision %).

table 1: per-class results for object classi   cation on the voc2007 test set (average precision %).

table 2: per-class results for object classi   cation on the voc2012 test set (average precision %).

table 1: per-class results for object classi   cation on the voc2007 test set (average precision %).

table 2: per-class results for object classi   cation on the voc2012 test set (average precision %).

table 2: per-class results for object classi   cation on the voc2012 test set (average precision %).

table 1: per-class results for object classi   cation on the voc2007 test set (average precision %).

figure 2: a) evolution of the mean image classi   cation ap over pas-

    deep face [3]

to evaluate how our transfer method performs on this
very different target task, we use a network pre-trained

chair cow table dog horse moto pers plant sheep sofa train

boat
btl
72.1
76.1
73.4
82.0
84.1
chair
52.5
62.2
46.2
59.9
61.3

64.0 85.9 36.3
76.8 91.1 53.9
80.0 95.6 60.8

3.2.3 results of mit 67 scene classi   cation

person
86.5
91.1
91.1
90.2
92.0

plane bike bird boat
btl

classi   cation

bus
cat
71.7
77.5
75.0
72.5
73.4

cat
66.5
69.1
79.2
81.6
85.4

car
83.6
88.8
77.5
85.3
86.7

instance
etrieval

mean accuracy

etrieval

id98 o   -the-shelf + augmentation

mean ap

best state of the art

id98 o   -the-shelf

specialized id98

wers

(b)

bus

bus

(a)

bus

cat

p
a

btl

(b)

im

11

19

23

an

0.6

p
a

19

 3

 7

s

u

b

b

r

r

r

r

r

r

o

o

o

o

d

d

h

m

8
.
1
6

9
.
4
7

9
.
1
8

4
.
5
4

9
.
5
6

5
.
9
7

3
.
4
8

4
.
7
6

3
.
9
8

5
.
8
4

6
.
4
6

3
.
6
7

3
.
2
4

1
.
1
9

2
.
0
8

8
.
6
5

9
.
3
7

4
.
8
5

7
.
4
7

2
.
7
7

8
.
6
8

7
.
7
7

9
.
8
6

1
.
1
7

7
.
0
8

9
.
9
6

5
.
9
8

3
.
3
5

8
.
0
7

4
.
1
9

8
6

4
6

9
8

9
6

3
7

5
6

9
7

c1-c2-c3-c4-c5fc 6fc 7fc 8african elephantwall clockgreen snakeyorkshire terriersource tasktraining imagessliding patchesfcafcbchairbackgroundpersontv/monitorconvolutional layersfully-connected layerssource task labelstarget task labelstransfer parameters1 : id1712 : feature transfer3 : classifier learningc1-c2-c3-c4-c5fc 6fc 74096 or 6144-dimvector4096 or 6144-dimvectortarget tasktraining images9216-dimvector4096 or 6144-dimvectornew adaptation layers trained on target task(cid:44)(cid:80)(cid:68)(cid:74)(cid:72)(cid:51)(cid:68)(cid:85)(cid:87)(cid:3)(cid:36)(cid:81)(cid:81)(cid:82)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)(cid:47)(cid:72)(cid:68)(cid:85)(cid:81)(cid:3)(cid:49)(cid:82)(cid:85)(cid:80)(cid:68)(cid:79)(cid:76)(cid:93)(cid:72)(cid:71)(cid:3)(cid:51)(cid:82)(cid:86)(cid:72)(cid:40)(cid:91)(cid:87)(cid:85)(cid:68)(cid:70)(cid:87)(cid:3)(cid:41)(cid:72)(cid:68)(cid:87)(cid:88)(cid:85)(cid:72)(cid:86)(cid:53)(cid:42)(cid:37)(cid:15)(cid:3)(cid:74)(cid:85)(cid:68)(cid:71)(cid:76)(cid:72)(cid:81)(cid:87)(cid:15)(cid:3)(cid:47)(cid:37)(cid:51)(cid:3)(cid:38)(cid:49)(cid:49)(cid:3)(cid:53)(cid:72)(cid:83)(cid:85)(cid:72)(cid:86)(cid:72)(cid:81)(cid:87)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:54)(cid:57)(cid:48)(cid:54)(cid:87)(cid:85)(cid:82)(cid:81)(cid:74)(cid:3)(cid:39)(cid:51)(cid:48)ross girshick jeff donahue trevor darrell jitendra malik
transfer from classification to other tasks
{rbg,jdonahue,trevor,malik}@eecs.berkeley.edu
    id98s + id166s for id164 [1,2] 

uc berkeley

chair cow table dog horse mbike person plant sheep sofa train

tv map
34.2 20.7 43.8 38.3 33.4
41.1 31.8 47.0 44.8 35.1
43.9 32.6 54.0 45.9 39.7
38.7 35.0 52.8 43.1 40.4
56.5 38.1 52.8 50.2 50.2
59.4 39.3 61.2 52.4 53.7
table 1: detection average precision (%) on voc 2010 test. r-id98 is most directly comparable to uva and regionlets since all
methods use selective search region proposals. bounding-box regression (bb) is described in section c. at publication time, segdpm
was the top-performer on the pascal voc leaderboard.    dpm and segdpm use context rescoring not used by the other methods.

abstract

r-id98: regions with id98 features
car

cat

aeroplane? no.

warped region

1. input 
image

2. extract region 
proposals (~2k)

47.7
...
32.9
person? yes.
...
43.5
47.1
53.6
regions
58.1

chair cow table dog horse mbike person plant sheep sofa train

51.2
52.9
55.7
54.4
tvmonitor? no.
4. classify 
65.9
69.0

voc 2010 test aero bike bird boat bottle bus
34.2 20.7 43.8 38.3 33.4
49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4
dpm v5 [20]   
id164 performance, as measured on the
41.1 31.8 47.0 44.8 35.1
56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5
uva [39]
canonical pascal voc dataset, has plateaued in the last
43.9 32.6 54.0 45.9 39.7
regionlets [41] 65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2
id98
38.7 35.0 52.8 43.1 40.4
segdpm [18]    61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3
few years. the best-performing methods are complex en-
56.5 38.1 52.8 50.2 50.2
r-id98
67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8
semble systems that typically combine multiple low-level
59.4 39.3 61.2 52.4 53.7
71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0
r-id98 bb
image features with high-level context.
in this paper, we
propose a simple and scalable detection algorithm that im-
table 1: detection average precision (%) on voc 2010 test. r-id98 is most directly comparable to uva and regionlets since all
methods use selective search region proposals. bounding-box regression (bb) is described in section c. at publication time, segdpm
proves mean average precision (map) by more than 30%
was the top-performer on the pascal voc leaderboard.    dpm and segdpm use context rescoring not used by the other methods.
relative to the previous best result on voc 2012   achieving
a map of 53.3%. our approach combines two key insights:
(1) one can apply high-capacity convolutional neural net-
works (id98s) to bottom-up region proposals in order to
localize and segment objects and (2) when labeled training
data is scarce, supervised pre-training for an auxiliary task,
22.6%
followed by domain-speci   c    ne-tuning, yields a signi   cant
20.9%
performance boost. since we combine region proposals
with id98s, we call our method r-id98: regions with id98
19.4%
features. we also compare r-id98 to overfeat, a recently
proposed sliding-window detector based on a similar id98
architecture. we    nd that r-id98 outperforms overfeat

figure 1: id164 system overview. our system (1)
    id98s for pose estimation [3] and semantic segmentation [4]
takes an input image, (2) extracts around 2000 bottom-up region
proposals, (3) computes features for each proposal using a large
convolutional neural network (id98), and then (4) classi   es each
region using class-speci   c linear id166s. r-id98 achieves a mean
average precision (map) of 53.7% on pascal voc 2010. for
comparison, [39] reports 35.1% map using the same region pro-
posals, but with a spatial pyramid and bag-of-visual-words ap-
proach. the popular deformable part models perform at 33.4%.
on the 200-class ilsvrc2013 detection dataset, r-id98   s
map is 31.4%, a large improvement over overfeat [34], which
had the previous best result at 24.3%.

[1] girshick, donahue, darrell and malik. rich feature hierarchies for accurate id164 and semantic segmentation, cvpr   14 
[2] sermanet, eigen, zhang, mathieu, fergus, lecun. overfeat: integrated recognition, localization and detection using convolutional networks. iclr 
[3] tompson, goroshin, jain, lecun, bregler. efficient object localization using convolutional networks. cvpr   15 
[4] pinheiro, collobert, dollar. learning to segment object candidates. nips   15

archical, multi-stage processes for computing features that
are even more informative for visual recognition.

ilsvrc2013 detection test set class ap box plots

ilsvrc2013 detection test set class ap box plots

3. compute 
id98 features

10.8
15.3
14.3
14.8
26.7
29.5

ilsvrc2013 detection test set map

(
 
n
o
i
s
i
c
e
r
p
 
e
g
a
r
e
v
a

uva   euvision 

sysu_vision 

*overfeat (1) 

*overfeat (2) 

%
n
i
 
)
p
a

*r   id98 bb 

*nec   mu 

toronto a 

10.5%

11.5%

24.3%

31.4%

100

20

30

10

40

50

60

70

80

90

 

 

why these improvements in performance?
    features are learned rather than hand-crafted 
1

mean ap

    more layers capture more invariances [1]  

0.8

    more data to train deeper networks 

    more computing (gpus) 

    better id173: dropout 

    new nonlinearities 

    max pooling, rectified linear units (relu) 

p
a

0.6

0.4

0.2

 3

 7

11

15
level

19

23

(a)

    theoretical understanding of deep networks remains shallow

[1] razavian, azizpour, sullivan, carlsson, id98 features off-the-shelf: an astounding baseline for recognition. cvprw   14.

figure 2: a) evolution of the mean image classi   cation ap over pas-
cal voc 2007 classes as we use a deeper representation from the
overfeat id98 trained on the ilsvrc dataset. overfeat considers
convolution, max pooling, nonlinear activations, etc. as separate layers.
the re-occurring decreases in the plot is of the activation function layer
which loses information by half rectifying the signal. b) confusion matrix
for the mit-67 indoor dataset. some of the off-diagonal confused classes
have been annotated, these particular cases could be hard even for a human
to distinguish.

last 2 layers the performance increases. we observed the

theoretical results on deep learning 
    approximation, depth, width, and invariance theory 
    id88s and multilayer feedforward networks are universal 
approximators: cybenko    89, hornik    89, hornik    91, barron    93 

    scattering networks are deformation stable for lipschitz non-

linearities: bruna-mallat    13, wiatowski    15, mallat    16 

    generalization and id173 theory 

    # training examples grows exponentially with network size: barlett    03 
    distance and margin-preserving embeddings: giryes    15, sokolik    16 
    geometry, generalization bounds and depth efficiency: montufar    15, 

neyshabur    15, shashua    14    15    16

[1] cybenko. approximations by superpositions of sigmoidal functions, mathematics of control, signals, and systems, 2 (4), 303-314, 1989. 
[2] hornik, stinchcombe and white. multilayer feedforward networks are universal approximators, neural networks, 2(3), 359-366, 1989. 
[3] hornik. approximation capabilities of multilayer feedforward networks, neural networks, 4(2), 251   257, 1991. 
[4] barron. universal approximation bounds for superpositions of a sigmoidal function. ieee transactions on id205, 39(3):930   945, 1993. 
[5] bruna and mallat. invariant scattering convolution networks. trans. pami, 35(8):1872   1886, 2013.  
[6] wiatowski, b  lcskei. a mathematical theory of deep convolutional neural networks for feature extraction. arxiv 2015. 
[7] mallat. understanding deep convolutional networks. phil. trans. r. soc. a, 374(2065), 2016.  

[8] bartlett and maass. vapnik-chervonenkis dimension of neural nets. the handbook of brain theory and neural networks, pages 1188    1192, 2003.  
[9] giryes, sapiro, a bronstein. deep neural networks with random gaussian weights: a universal classification strategy? arxiv:1504.08291. 
[10] sokolic. margin preservation of deep neural networks, 2015 
[11] montufar. geometric and combinatorial perspectives on deep neural networks, 2015. 
[12] neyshabur. the geometry of optimization and generalization in neural networks: a path-based approach, 2015.

theoretical results on deep learning 
    earlier work on optimization theory 

    no spurious local optima for linear networks (baldi & hornik    89) 
    id26 fails to converge for nonlinear networks (brady    89) 
    back propagation converges for linearly separable data (gori & tesi 

   91    92), but it get stuck in other cases (frasconi    97) 

    recent work on optimization theory 

    convex neural networks in infinite number of variables: bengio    05 
    networks with many hidden units can learn polynomials: andoni   14 
    the loss surface of multilayer networks: choromanska    15 
    attacking the saddle point problem: dauphin    14 
    effect of gradient noise on the energy landscape: chaudhari    15 
    guaranteed training of nns using tensor methods: janzamin    15 
    guarantees of global optimality in neural network training: haeffele    15   

motivations and goals of this tutorial
    motivation: deep networks have led to dramatic 

improvements in performance for many tasks, but the 
mathematical reasons for this success remain unclear.  

    goal: review very recent work that aims at understanding 

the mathematical reasons for the success of deep networks. 

    what we will do: study theoretical questions such as 

    can we ensure that the learned representations are globally optimal? 
    what properties of images are being captured/exploited by dnns? 
    how should dnns be regularized to ensure generalization properties? 

    what we will not do: show x% improvement in performance 

for a particular application. 

tutorial schedule
    09:00-09:15: ren   vidal - introduction 

    09:15-10:00: ren   vidal - global optimality and 

id173 in deep learning 

    10:00-10:30: coffee break 

    10:30-11:15: raja giryes - data structure based theory of 

deep learning 

    11:15-12:00: raja giryes- generalization bounds for deep 

learning 

    12:00-12:15: discussion

